	query	feed_id	candidates
0	{'doc_id': '221106365', 'title': 'Validation of an electronic coding algorithm to identify the primary indication of orthopedic surgeries from administrative data', 'abstract': 'Background Determining the primary indication of a surgical procedure can be useful in identifying patients undergoing elective surgery where shared decision-making is recommended. The purpose of this study was to develop and validate an algorithm to identify patients receiving the following combinations of surgical procedure and primary indication as part of a study to promote shared decision-making: (1) knee arthroplasty to treat knee osteoarthritis (KOA); (2) hip arthroplasty to treat hip osteoarthritis (HOA); (3) spinal surgery to treat lumbar spinal stenosis (SpS); and (4) spinal surgery to treat lumbar herniated disc (HD). Methods Consecutive surgical procedures performed by participating spine, hip, and knee surgeons at four sites within an integrated care network were included. Study staff reviewed electronic medical records to ascertain a “gold standard” determination of the procedure and primary indication status. Electronic algorithms consisting of ICD-10 and CPT codes for each combination of procedure and indication were then applied to records for each case. The primary measures of validity for the algorithms were the sensitivity and specificity relative to the gold standard review. Results Participating surgeons performed 790 procedures included in this study. The sensitivity of the algorithms in determining whether a surgical case represented one of the combinations of procedure and primary indication ranged from 0.70 (HD) to 0.92 (KOA). The specificity ranged from 0.94 (SpS) to 0.99 (HOA, KOA). Conclusion The electronic algorithm was able to identify all four procedure/primary indication combinations of interest with high specificity. Additionally, the sensitivity for the KOA cases was reasonably high. For HOA and the spine conditions, additional work is needed to improve the sensitivity of the algorithm to identify the primary indication for each case.', 'corpus_id': 221106365}	17794	"[{'doc_id': '235168000', 'title': 'Validation of Claims Data for the Identification of Intraoperative Transesophageal Echocardiography During Cardiac Surgery.', 'abstract': 'OBJECTIVE\nThe goal of this study was to assess the validity of Current Procedural Terminology (CPT) claims data for the identification of intraoperative transesophageal echocardiography (TEE) during cardiac surgery.\n\n\nDESIGN\nThis study was a retrospective, cohort analysis.\n\n\nSETTING\nThis study used data from electronic medical records (EMRs), in combination with CPT billing claims data, from two hospitals within the Penn Medicine Health System-Penn Presbyterian Medical Center and the Hospital of the University of Pennsylvania.\n\n\nPARTICIPANTS\nThe cohort consisted of adult patients, aged ≥18 years, undergoing open cardiac valve surgery (repair or replacement), coronary artery bypass graft surgery, or aortic surgery between April 1 and October 31, 2019.\n\n\nINTERVENTIONS\nAgreement between TEE identified using CPT billing code(s) (93312-8 with or without 93320-1 or 93325) and TEE identified by manual EMR review.\n\n\nMEASUREMENTS AND MAIN RESULTS\nAs identified by a reference standard (ie, EMR review) of the 873 cases that met inclusion criteria, 867 (99.31%) cases were performed with TEE and six cases were performed without TEE (<1%). Of the 867 cases performed with TEE, CPT code(s) correctly identified 866 cases, as indicated by having at least one of the CPT codes (93312-8 with or without 93320-1 or 93325). These CPT codes identified intraoperative TEE with a 99.88% sensitivity, 100.00% specificity, 100.00% positive predictive value, and 85.71% negative predictive value. When billing claims for TEE were restricted to the CPT code 93312 alone, the results were identical.\n\n\nCONCLUSIONS\nBilling claims using CPT code(s) identified true intraoperative TEE with a high sensitivity, specificity, excellent positive predictive value, and moderate negative predictive value. These results demonstrated that claims data are a valuable data source from which to study the effect of TEE in cardiac surgical patients.', 'corpus_id': 235168000, 'score': 1}, {'doc_id': '234787208', 'title': 'A new prediction model for patient satisfaction after total knee arthroplasty and the roles of different scoring systems: a retrospective cohort study', 'abstract': 'Background Although total knee arthroplasty (TKA) is an efficacious treatment for end-stage osteoarthritis, ~20% of patients are dissatisfied with the results. We determined which factors contribute to patient satisfaction and compared the various scoring systems before and after surgery. Methods In this retrospective cohort study, 545 patients were enrolled and evaluated preoperatively and 1 year postoperatively. Patient demographics, as well as scores for the Western Ontario and McMaster Universities Osteoarthritis Index (WOMAC), Short Form (SF)-12, and 1989 Knee Society Clinical Rating System (1989 KSS), were recorded preoperatively and postoperatively. The possible predictors were introduced into a prediction model. Scores for overall satisfaction and the 2011 Knee Society Score (2011 KSS) were also assessed after TKA to identify the accuracy and agreement of the systems. Results There were 134 male patients and 411 female patients, with an overall prevalence of satisfaction of 83.7% 1 year after surgery. A history of surgery ( p < 0.001) and the 1989 KSS and SF-12 were of the utmost importance in the prediction model, whereas the WOMAC score had a vital role postoperatively (change in WOMAC pain score, p < 0.001; change in WOMAC physical function score, p < 0.001; postoperative WOMAC pain score, p = 0.004). C-index of model was 0.898 > 0.70 (95% confidence interval (CI): 0.86-0.94). The Hosmer-Lemeshow test showed a p value of 0.586, and the AUC of external cohort was 0.953 (sensitivity=0.87, specificity=0.97). The agreement between the assessment of overall satisfaction and the 2011 KSS satisfaction assessment was general (Kappa=0.437 > 0.4, p < 0.001). Conclusion A history of surgery, the preoperative 1989 KSS, and the preoperative SF-12 influenced patient satisfaction after primary TKA. We recommend the WOMAC (particularly the pain subscale score) to reflect overall patient satisfaction postoperatively.', 'corpus_id': 234787208, 'score': 1}, {'doc_id': '233989500', 'title': 'Accuracy of soft tissue balancing in total knee arthroplasty using surgeon-defined assessment versus a gap-balancer or electronic sensor', 'abstract': 'Background Soft tissue balancing is essential for the success of total knee arthroplasty (TKA) and is mainly dependent on surgeon-defined assessment (SDA) or a gap-balancer (GB). However, an electronic sensor has been developed to objectively measure the gap pressure. This study aimed to evaluate the accuracy of soft tissue balancing using SDA and GB compared with a sensor. Methods Forty-eight patients undergoing TKA (60 knees) were prospectively enrolled. Soft tissue balancing was sequentially performed using SDA, a GB, and an electronic sensor. We compared the SDA, GB, and sensor data to calculate the sensitivity, specificity, and accuracy at 0°, 45°, 90°, and 120° flexion. Cumulative summation (CUSUM) analysis was performed to assess the surgeon’s performance during the sensor introductory phase. Results The sensitivity of SDA was 63.3%, 68.3%, 80.0%, and 80.0% at 0°, 45°, 90°, and 120°, respectively. The accuracy of the GB compared with sensor data was 76.7% and 71.7% at 0° and 90°, respectively. Cohen’s kappa coefficient for the accuracy of the GB was 0.406 at 0° (moderate agreement) and 0.227 at 90° (fair agreement). The CUSUM 0° line achieved good prior performance at case 45, CUSUM 90° and 120° showed a trend toward good prior performance, while CUSUM 45° reached poor prior performance at case 8. Conclusion SDA was a poor predictor of knee balance. GB improved the accuracy of soft tissue balancing, but was still less accurate than the sensor, particularly for unbalanced knees. SDA improved with ongoing use of the sensor, except at 45° flexion.', 'corpus_id': 233989500, 'score': 0}, {'doc_id': '235272231', 'title': 'The short-term effectiveness and safety of second-generation patellofemoral arthroplasty and total knee arthroplasty on isolated patellofemoral osteoarthritis: a systematic review and meta-analysis', 'abstract': 'We aimed to compare second-generation patellofemoral arthroplasty (2G PFA) with total knee arthroplasty (TKA) in treating isolated patellofemoral osteoarthritis (PFOA) by assessing the percentages of revisions, complications, and patient-reported outcome measures (PROMs). Studies that compared the outcomes of 2G PFA and TKA in the treatment of isolated PFOA were searched in electronic databases, including MEDLINE, Embase, and Web of Science. Two researchers independently identified eligible studies, extracted the data, and evaluated the quality of the literature. Pooled risk ratios (RRs) or weighted mean differences with 95% confidence intervals were calculated using either fixed or random effects models. Descriptive analysis was used when data could not be pooled. A total of six studies were included in the review. For the revision percentage and complications, there were no significant differences between 2G PFA and TKA (RR = 2.29, 95% CI 0.69–7.58, P = 0.17; RR = 0.56, 95% CI 0.23–1.40, P = 0.22, respectively). Second, the results demonstrated that the differences in the Oxford Knee Score (OKS) and the University of California, Los Angeles (UCLA) activity score between 2G PFA and TKA were not significant (WMD −4.68, 95% CI −16.32 to 6.97, p = 0.43; WMD 0.16, 95% CI −1.21 to 1.53, P = 0.82). The Knee Injury and Osteoarthritis Outcome Score (KOOS), the American Knee Society Score (AKSS), and the Western Ontario and McMaster Universities Osteoarthritis Index (WOMAC) were presented in a narrative form due to methodological heterogeneity. For isolated PFOA, 2G PFA demonstrated similar results to TKA with respect to the percentages of revisions, complications, and PROMs.', 'corpus_id': 235272231, 'score': 0}, {'doc_id': '233303808', 'title': 'Identifying factors predicting prolonged rehabilitation after simultaneous bilateral total knee arthroplasty: a retrospective observational study', 'abstract': 'Background Rehabilitation is an effective procedure for promoting functional recovery after simultaneous bilateral total knee arthroplasty (TKA); however, it has been cited as a significant economic burden of medical care. We hypothesized that preoperative factors, including age, sex, body mass index, living alone, the knee society function score (KSS), the American society of anesthesiologists (ASA) class, hemoglobin (Hb), albumin level, mean range of motion, and the Kellgren–Lawrence grade, would predict prolonged rehabilitation utilization. Methods In total, 191 patients undergoing simultaneous bilateral TKA in a single hospital were enrolled. The successful compliance group included patients who completed their rehabilitation program and could return to their residence within 3\u2009weeks after surgery ( n \u2009=\u2009132), whereas the delayed group included the remaining patients ( n \u2009=\u200959). Logistic regression analysis was performed using preoperative factors. A prediction scoring system was created using the regression coefficients from the logistic regression model. Results Logistic regression analysis revealed that age (β\u2009=\u2009−\u20090.0870; P \u2009<\u2009\xa00.01) and Hb (β\u2009=\u20090.34; P \u2009<\u2009\xa00.05) were significantly associated with prolonged rehabilitation programs, whereas body mass index, living alone, KSS score, and ASA class were not significantly associated with successful completion of rehabilitation programs; however, these factors contributed to the prediction scoring formula, which was defined as follows: Score = 10 - 0.09 × age - 0.09 × body mass index - ( 0.56 × living alone [ alone : 1 , others : 0 ] ) + 0.03 × KSS stairs + 0.34 × Hb - 1.1 × ASA class . $$ {\\displaystyle \\begin{array}{l}\\mathrm{Score}=10-\\left(0.09\\times \\mathrm{age}\\right)-\\left(0.09\\times \\mathrm{body}\\ \\mathrm{mass}\\ \\mathrm{index}\\right)-\\left(0.56\\times \\mathrm{living}\\ \\mathrm{alone}\\ \\right[\\mathrm{alone}:1,\\\\ {}\\mathrm{others}:0\\left]\\right)+\\left(0.03\\times \\mathrm{KSS}\\ \\mathrm{stairs}\\right)+\\left(0.34\\times \\mathrm{Hb}\\right)-\\left(1.1\\times \\mathrm{ASA}\\ \\mathrm{class}\\right).\\end{array}} $$ The C-statistic for the scoring system was 0.748 (95% confidence interval, 0.672–0.824). The positive and negative likelihood ratios were 2.228 (95% CI, 1.256–3.950) and 0.386 (95% CI, 0.263–0.566), respectively. These results showed an increase of 15–20% and a decrease of 20–25% in the risk of prolonged rehabilitation. The optimal cutoff point for balancing sensitivity and specificity was 3.5, with 66.6% sensitivity and 78.0% specificity. Conclusions Older age and lower preoperative Hb were significantly associated with prolonged rehabilitation programs. We defined a new scoring formula using preoperative patient factors to predict prolonged rehabilitation utilization in patients undergoing simultaneous bilateral TKA.', 'corpus_id': 233303808, 'score': 0}, {'doc_id': '234494104', 'title': 'Predictive Models for Clinical Outcomes in Total Knee Arthroplasty: A Systematic Analysis', 'abstract': 'Background Predictive modeling promises to improve our understanding of what variables influence patient satisfaction after total knee arthroplasty (TKA). The purpose of this article was to systematically review the relevant literature using predictive models of clinical outcomes after TKA. The aim was to identify the predictor strategies used for systematic data collection with the highest likelihood of success in predicting clinical outcomes. Methods A Preferred Reporting Items for Systematic Reviews and Meta-Analyses protocol systematic review was conducted using 3 databases (MEDLINE, EMBASE, and PubMed) to identify all clinical studies that had used predictive models or that assessed predictive features for outcomes after TKA between 1996 and 2020. The ROBINS-I tool was used to evaluate the quality of the studies and the risk of bias. Results A total of 75 studies were identified of which 48 met our inclusion criteria. Preoperative predictive factors strongly associated with postoperative clinical outcomes were knee pain, knee-specific Patient-Reported Outcome Measure (PROM) scores, and mental health scores. Demographic characteristics, pre-existing comorbidities, and knee alignment had an inconsistent association with outcomes. The outcome measures that correlated best with the predictive models were improvement of PROM scores, pain scores, and patient satisfaction. Conclusions Several algorithms, based on PROM improvement, patient satisfaction, or pain after TKA, have been developed to improve decision-making regarding both indications for surgery and surgical strategy. Functional features such as preoperative pain and PROM scores were highly predictive for clinical outcomes after TKA. Some variables such as demographics data or knee alignment were less strongly correlated with TKA outcomes. Level of evidence Systematic review – Level III.', 'corpus_id': 234494104, 'score': 1}, {'doc_id': '3812196', 'title': 'Validation of an International Statistical Classification of Diseases and Related Health Problems 10th Revision Coding Algorithm for Hospital Encounters with Hypoglycemia.', 'abstract': ""OBJECTIVES\nTo determine the positive predictive value and sensitivity of an International Statistical Classification of Diseases and Related Health Problems, 10th Revision, coding algorithm for hospital encounters concerning hypoglycemia.\n\n\nMETHODS\nWe carried out 2 retrospective studies in Ontario, Canada. We examined medical records from 2002 through 2014, in which older adults (mean age, 76) were assigned at least 1 code for hypoglycemia (E15, E160, E161, E162, E1063, E1163, E1363, E1463). The positive predictive value of the algorithm was calculated using a gold-standard definition (blood glucose value <4\u2009mmol/L or physician diagnosis of hypoglycemia). To determine the algorithm's sensitivity, we used linked healthcare databases to identify older adults (mean age, 77) with laboratory plasma glucose values <4\u2009mmol/L during a hospital encounter that took place between 2003 and 2011. We assessed how frequently a code for hypoglycemia was present. We also examined the algorithm's performance in differing clinical settings (e.g. inpatient vs. emergency department, by hypoglycemia severity).\n\n\nRESULTS\nThe positive predictive value of the algorithm was 94.0% (95% confidence interval 89.3% to 97.0%), and its sensitivity was 12.7% (95% confidence interval 11.9% to 13.5%). It performed better in the emergency department and in cases of more severe hypoglycemia (plasma glucose values <3.5\u2009mmol/L compared with ≥3.5\u2009mmol/L).\n\n\nCONCLUSIONS\nOur hypoglycemia algorithm has a high positive predictive value but is limited in sensitivity. Although we can be confident that older adults who are assigned 1 of these codes truly had a hypoglycemia event, many episodes will not be captured by studies using administrative databases."", 'corpus_id': 3812196, 'score': 1}, {'doc_id': '233474300', 'title': 'From Outpatient Procedures? Patients More Likely to Have Optimal Immediate Recovery Outcomes of Same-Day Orthopedic Surgery: Are Spine', 'abstract': 'Background: Spinal surgery is associated with an inherently elevated risk profile, and thus far there has been limited discussion about how these outpatient spine patients are benefiting from these same-day procedures against other typical outpatient orthopedic surgeries. Methods: Orthopedic patients who received either inpatient or outpatient surgery were isolated in the American College of Surgeons National Surgical Quality of Improvement Program (2005–2016). Patients were stratified by type of orthopedic surgery received (spine, knee, ankle, shoulder, or hip). Mean comparisons and chi-squared tests assessed basic demographics. Perioperative complications were analyzed via regression analyses in regard to their principal inpatient or outpatient orthopedic surgery received. Results: This study included 729 480 surgical patients: 32.5% received spinal surgery, 36.5% knee, 24.1% hip, 4.9% shoulder, and 1.7%ankle. Of those who received a spinal procedure, 74.7% were inpatients (IN), and 25.3% were outpatients (OUT): knee: 96.1% IN, 3.9% OUT; hip:98.9% IN, 1.1% OUT; ankle: 29% IN, 71% OUT; and shoulder: 52.6% IN, 47.6% OUT. Hip patients were the oldest, and knee patients had the highest body mass index out of the orthopedic groups (P , .00). Spine IN patients experienced more complications than the other orthopedic groups and had the lowest OUT complications(both P , .05). This same trend of having higher IN complications than OUT complications was identified for hip, shoulder, and knee. However, ankle procedures had greater OUT procedure complications than IN (P , .05). After controlling for age, body mass index, and Charlson Comorbidity Index, IN procedures, such as knee, hip, spine, and shoulder, were significantly associated with experiencing postoperative complications. From 2006 to 2016, IN and OUT surgeries were significantly different among complications experienced for all of the orthopedic groups (P , .05) with complications decreasing for IN and OUT patients by 2016. Conclusions: Over the past decade, spine surgery has decreased in complications for IN and OUT procedures along with IN/OUT knee, ankle, hip, and shoulder procedures, reflecting greater tolerance for risk in an outpatient setting. Level of Evidence: 3. Clinical Relevance: Despite the increase in riskier spine procedures, complications have decreased over the years. Surgeons should aim to continue to decrease inpatient spine complications to the level of other orthopedic surgeries.', 'corpus_id': 233474300, 'score': 0}, {'doc_id': '234774031', 'title': 'Prognostic models for knee osteoarthritis: a protocol for systematic review, critical appraisal, and meta-analysis', 'abstract': 'Background Osteoarthritis is the most common degenerative joint disease. It is associated with significant socioeconomic burden and poor quality of life, mainly due to knee osteoarthritis (KOA), and related total knee arthroplasty (TKA). Since early detection method and disease-modifying drug is lacking, the key of KOA treatment is shifting to disease prevention and progression slowing. The prognostic prediction models are called for to guide clinical decision-making. The aim of our review is to identify and characterize reported multivariable prognostic models for KOA about three clinical concerns: (1) the risk of developing KOA in the general population, (2) the risk of receiving TKA in KOA patients, and (3) the outcome of TKA in KOA patients who plan to receive TKA. Methods The electronic datasets (PubMed, Embase, the Cochrane Library, Web of Science, Scopus, SportDiscus, and CINAHL) and gray literature sources (OpenGrey, British Library Inside, ProQuest Dissertations & Theses Global, and BIOSIS preview) will be searched from their inception onwards. Title and abstract screening and full-text review will be accomplished by two independent reviewers. The multivariable prognostic models that concern on (1) the risk of developing KOA in the general population, (2) the risk of receiving TKA in KOA patients, and (3) the outcome of TKA in KOA patients who plan to receive TKA will be included. Data extraction instrument and critical appraisal instrument will be developed before formal assessment and will be modified during a training phase in advance. Study reporting transparency, methodological quality, and risk of bias will be assessed according to the TRIPOD statement, CHARMS checklist, and PROBAST tool, respectively. Prognostic prediction models will be summarized qualitatively. Quantitative metrics on the predictive performance of these models will be synthesized with meta-analyses if appropriate. Discussion Our systematic review will collate evidence from prognostic prediction models that can be used through the whole process of KOA. The review may identify models which are capable of allowing personalized preventative and therapeutic interventions to be precisely targeted at those individuals who are at the highest risk. To accomplish the prediction models to cross the translational gaps between an exploratory research method and a valued addition to precision medicine workflows, research recommendations relating to model development, validation, or impact assessment will be made. Systematic review registration PROSPERO CRD42020203543', 'corpus_id': 234774031, 'score': 0}, {'doc_id': '20769472', 'title': 'Validation of an International Classification of Disease, Ninth Revision coding algorithm to identify decompressive craniectomy for stroke', 'abstract': 'BackgroundAlthough International Classification of Disease, Ninth Revision, Clinical Modification (ICD9-CM) coding is the basis of administrative claims data, no study has validated an ICD9-CM algorithm to identify patients undergoing decompressive craniectomy for space-occupying supratentorial infarction.MethodsPatients who underwent decompressive craniectomy for stroke at our institution were retrospectively identified and their associated ICD9-CM codes were extracted from billing data. An ICD9-CM algorithm was generated and its accuracy compared against physician review.ResultsA total of 10,925 neurosurgical operations were performed from December 2008 to March 2015, of which 46 (0.4%) were decompressive craniectomy for space-occupying stroke. The ICD9-CM procedure code for craniectomy (01.25) was only encoded in 67.4% of patients, while craniotomy (01.24) was used in 19.6% and lobectomy (01.39, 01.53, 01.59) in 13.1%. The ICD-9-CM algorithm included patients with a diagnosis codes for cerebral infarction (433.11, 434.01, 434.11, and 434.91) and a procedure code for craniotomy, craniectomy, or lobectomy. Patients were excluded with an ICD9-CM diagnosis code for brain tumor, intracranial abscess, subarachnoid hemorrhage, vertebrobasilar infarction, intracranial aneurysm, Moyamoya disease, intracranial venous sinus thrombosis, vertebral artery dissection, congenital cerebrovascular anomaly, head trauma or an ICD9-CM procedure code for laminectomy. This algorithm had a sensitivity of 97.8%, specificity of 99.9%, positive predictive value of 88.2%, and negative predictive value of 99.9%. The majority of false-positive results were patients who underwent evacuation of a primary intracerebral hematoma.ConclusionAn ICD-9-CM algorithm based on diagnosis and procedure codes can effectively identify patients undergoing decompressive craniectomy for supratentorial stroke.', 'corpus_id': 20769472, 'score': 1}]"
1	"{'doc_id': '2934115', 'title': 'Memory influences on hippocampal and striatal neural codes: Effects of a shift between task rules', 'abstract': ""Interactions with neocortical memory systems may facilitate flexible information processing by hippocampus. We sought direct evidence for such memory influences by recording hippocampal neural responses to a change in cognitive strategy. Well-trained rats switched (within a single recording session) between the use of place and response strategies to solve a plus maze task. Maze and extramaze environments were constant throughout testing. Place fields demonstrated (in-field) firing rate and location-based reorganization [Leutgeb, S., Leutgeb, J. K., Barnes, C. A., Moser, E. I., McNaughton, B. L., & Moser, M. B. (2005). Independent codes for spatial and episodic memory in hippocampal neuronal ensembles. Science, 309, 619-623] after a task switch, suggesting that hippocampus encoded each phase of testing as a different context, or episode. The task switch also resulted in qualitative and quantitative changes to discharge that were correlated with an animal's velocity or acceleration of movement. Thus, the effects of a strategy switch extended beyond the spatial domain, and the movement correlates were not passive reflections of the current behavioral state. To determine whether hippocampal neural responses were unique, striatal place and movement-correlated neurons were simultaneously recorded with hippocampal neurons. Striatal place and movement cells exhibited a response profile that was similar, but not identical, to that observed for hippocampus after a strategy switch. Thus, retrieval of a different memory led both neural systems to represent a different context. However, hippocampus may play a special (though not exclusive) role in flexible spatial processing since correlated firing amongst cell pairs was highest when rats successfully switched between two spatial tasks. Correlated firing by striatal cell pairs increased following any strategy switch, supporting the view that striatum codes change in reinforcement contingencies."", 'corpus_id': 2934115}"	19444	"[{'doc_id': '235652018', 'title': 'A nonlinear hidden layer enables actor-critic agents to learn multiple paired association navigation', 'abstract': 'Navigation to multiple cued reward locations has been increasingly used to study rodent learning. Though deep reinforcement learning agents have been shown to be able to learn the task, they are not biologically plausible. Biologically plausible classic actor-critic agents have been shown to learn to navigate to single reward locations, but which biologically plausible agents are able to learn multiple cue-reward location tasks has remained unclear. In this computational study, we show versions of classic agents that learn to navigate to a single reward location, and adapt to reward location displacement, but are not able to learn multiple paired association navigation. The limitation is overcome by an agent in which place cell and cue information are first processed by a feedforward nonlinear hidden layer with synapses to the actor and critic subject to temporal difference error-modulated plasticity. Faster learning is obtained when the feedforward layer is replaced by a recurrent reservoir network. Introduction Navigation to remembered locations is important for many animals. Tasks like the Barnes maze and the Morris water maze requiring navigation to a single reward location are often used to study rodent learning. More recently, there has been increasing use of a multiple paired association navigation task for rodents involving more than one reward location. The multiple paired association task takes place in an arena where the reward is hidden. Each trial starts with the animal in one of several positions at the arena boundary, where the animal receives one of several sensory cues, such as a particular odor. Each sensory cue consistently represents a possible reward location, and indicates where the animal must go to obtain a reward. Deep reinforcement learning algorithms have progressed considerably to show human level performance in computer games and other remarkable capabilities, and provide useful frameworks for interpreting brain function. However, deep reinforcement learning uses', 'corpus_id': 235652018, 'score': 0}, {'doc_id': '235748269', 'title': 'A Machine Learning Approach for Detecting Vicarious Trial and Error Behaviors', 'abstract': 'Vicarious trial and error behaviors (VTEs) indicate periods of indecision during decision-making, and have been proposed as a behavioral marker of deliberation. In order to understand the neural underpinnings of these putative bridges between behavior and neural dynamics, researchers need the ability to readily distinguish VTEs from non-VTEs. Here we utilize a small set of trajectory-based features and standard machine learning classifiers to identify VTEs from non-VTEs for rats performing a spatial delayed alternation task (SDA) on an elevated plus maze. We also show that previously reported features of the hippocampal field potential oscillation can be used in the same types of classifiers to separate VTEs from non-VTEs with above chance performance. However, we caution that the modest classifier success using hippocampal population dynamics does not identify many trials where VTEs occur, and show that combining oscillation-based features with trajectory-based features does not improve classifier performance compared to trajectory-based features alone. Overall, we propose a standard set of features useful for trajectory-based VTE classification in binary decision tasks, and support previous suggestions that VTEs are supported by a network including, but likely extending beyond, the hippocampus.', 'corpus_id': 235748269, 'score': 1}, {'doc_id': '235821357', 'title': 'Dentate granule cells encode auditory decisions after reinforcement learning in rats', 'abstract': 'Auditory-cued goal-oriented behaviors requires the participation of cortical and subcortical brain areas, but how neural circuits associate sensory-based decisions with goal locations through learning remains poorly understood. The hippocampus is critical for spatial coding, suggesting its possible involvement in transforming sensory inputs to the goal-oriented decisions. Here, we developed an auditory discrimination task in which rats learned to navigate to goal locations based on the frequencies of auditory stimuli. Using in vivo calcium imaging in freely behaving rats over the course of learning, we found that dentate granule cells became more active, spatially tuned, and responsive to task-related variables as learning progressed. Furthermore, only after task learning, the activity of dentate granule cell ensembles represented the navigation path and predicts auditory decisions as early as when rats began to approach the goals. Finally, chemogenetic silencing of dentate gyrus suppressed task learning. Our results demonstrate that dentate granule cells gain task-relevant firing pattern through reinforcement learning and could be a potential link of sensory decisions to spatial navigation.', 'corpus_id': 235821357, 'score': 0}, {'doc_id': '12748644', 'title': 'Conflict between place and response navigation strategies: effects on vicarious trial and error (VTE) behaviors.', 'abstract': 'Navigation can be accomplished through multiple decision-making strategies, using different information-processing computations. A well-studied dichotomy in these decision-making strategies compares hippocampal-dependent ""place"" and dorsal-lateral striatal-dependent ""response"" strategies. A place strategy depends on the ability to flexibly respond to environmental cues, while a response strategy depends on the ability to quickly recognize and react to situations with well-learned action-outcome relationships. When rats reach decision points, they sometimes pause and orient toward the potential routes of travel, a process termed vicarious trial and error (VTE). VTE co-occurs with neurophysiological information processing, including sweeps of representation ahead of the animal in the hippocampus and transient representations of reward in the ventral striatum and orbitofrontal cortex. To examine the relationship between VTE and the place/response strategy dichotomy, we analyzed data in which rats were cued to switch between place and response strategies on a plus maze. The configuration of the maze allowed for place and response strategies to work competitively or cooperatively. Animals showed increased VTE on trials entailing competition between navigational systems, linking VTE with deliberative decision-making. Even in a well-learned task, VTE was preferentially exhibited when a spatial selection was required, further linking VTE behavior with decision-making associated with hippocampal processing.', 'corpus_id': 12748644, 'score': 1}, {'doc_id': '236962002', 'title': 'Lateral Habenula Inactivation Alters Willingness to Exert Physical Effort Using a Maze Task in Rats', 'abstract': 'An impairment in willingness to exert physical effort in daily activities is a noted aspect of several psychiatric conditions. Previous studies have supported an important role for the lateral habenula (LHb) in dynamic decision-making, including decisions associated with discounting costly high value rewards. It is unknown whether a willingness to exert physical effort to obtain higher rewards is also mediated by the LHb. It also remains unclear whether the LHb is critical to monitoring the task contingencies generally as they change, or whether it also mediates choices in otherwise static reward environments. The present study indicates that the LHb might have an integrative role in effort-based decision-making even when no alterations in choice contingencies occur. Specifically, pharmacological inactivation of the LHb showed differences in motivational behavior by reducing choices for the high effort (30cm barrier) high reward (2 pellets) choice versus the low effort (0 cm) low reward (1 pellet) choice. In sessions where the barrier was removed, rats demonstrated a similar preference for the high reward arm under both control and LHb inactivation. Further, no differences were observed when accounting for sex as a biological variable. These results support that effort to receive a high-value reward is considered on a trial-by-trial basis and the LHb is part of the circuit responsible for integrating this information during decision-making. Therefore, it is likely that previously observed changes in the LHb may be a key contributor to changes in a willingness to exert effort in psychiatric conditions.', 'corpus_id': 236962002, 'score': 0}, {'doc_id': '24687509', 'title': 'Extensive training and hippocampus or striatum lesions: Effect on place and response strategies', 'abstract': 'The hippocampus has been linked to spatial navigation and the striatum to response learning. The current study focuses on how these brain regions continue to interact when an animal is very familiar with the task and the environment and must continuously switch between navigation strategies. Rats were trained to solve a plus maze using a place or a response strategy on different trials within a testing session. A room cue (illumination) was used to indicate which strategy should be used on a given trial. After extensive training, animals underwent dorsal hippocampus, dorsal lateral striatum or sham lesions. As expected hippocampal lesions predominantly caused impairment on place but not response trials. Striatal lesions increased errors on both place and response trials. Competition between systems was assessed by determining error type. Pre-lesion and sham animals primarily made errors to arms associated with the wrong (alternative) strategy, this was not found after lesions. The data suggest a qualitative change in the relationship between hippocampal and striatal systems as a task is well learned. During acquisition the two systems work in parallel, competing with each other. After task acquisition, the two systems become more integrated and interdependent. The fact that with extensive training (as something becomes a ""habit""), behaviors become dependent upon the dorsal lateral striatum has been previously shown. The current findings indicate that dorsal lateral striatum involvement occurs even when the behavior is spatial and continues to require hippocampal processing.', 'corpus_id': 24687509, 'score': 1}, {'doc_id': '235725216', 'title': 'The Ventral Midline Thalamus Mediates Successful Deliberation by Coordinating Prefrontal and Hippocampal Neural Activity', 'abstract': 'When faced with difficult choices, the possible outcomes are considered through a process known as deliberation. In rats, deliberation can be reflected by pause-and-reorienting behaviors, better known as Vicarious Trial and Errors (VTEs). While VTEs are thought to require medial prefrontal cortex (mPFC) and dorsal hippocampal (dHPC) interactions, no work has demonstrated such a dual requirement. The nucleus reuniens (Re) of the ventral midline thalamus is anatomically connected with both the mPFC and dHPC, is required for HPC-dependent spatial memory tasks, and is critical for mPFC-dHPC neural synchronization. Currently, it is unclear if, or how, the Re is involved in deliberation. Therefore, by examining the role of the Re on VTE behaviors, we can better understand the anatomical and physiological mechanisms supporting deliberation. Here, we examined the impact of Re suppression on VTE behaviors and mPFC-dHPC theta synchrony during well-learned performance of a HPC-dependent delayed alternation (DA) task. Pharmacological suppression of the Re increased VTE behaviors that resulted in erroneous choices. These errors were best characterized as perseverative behaviors, where at the decision-point, some rats repeatedly turned in the direction that previously yielded no reward, while simultaneously deliberating. These ‘failed’ deliberations were associated with a reduction in mPFC-dHPC theta coherence at the choice-point. Importantly, the reduction in mPFC-dHPC theta synchrony observed during VTE behaviors was almost entirely driven by Re-suppression induced changes to the mPFC theta oscillation. Our findings suggest that the Re is important for successful deliberation, and mPFC-dHPC interactions, by coordinating local theta oscillations in the mPFC.', 'corpus_id': 235725216, 'score': 1}, {'doc_id': '235649977', 'title': 'Communication between the mediodorsal thalamus and prelimbic cortex regulates timing performance in rats', 'abstract': 'Predicting when future events will occur and adjusting behavior accordingly is critical to adaptive behavior. Despite this, little is known about the brain networks that encode time and how this ultimately impacts decision-making. One established finding is that the prefrontal cortex (PFC) and its non-human analogues (e.g., the rodent prelimbic cortex; PL) mediate timing. This provides a starting point for exploring the networks that support temporal processing by identifying areas that interact with the PFC during timing tasks. For example, substantial work has explored the role of frontostriatal circuits in timing. However, other areas are undoubtedly involved. The mediodorsal nucleus of the thalamus (MD) is an excellent candidate region. It shares dense, reciprocal connections with PFC-areas in both humans and non-human species and is implicated in cognition. However, causal data implicating MD-PFC interactions in cognition broadly is still sparse, and their role in timing specifically is currently unknown. To address this, we trained male rats on a time-based, decision-making task referred to as the ‘peak-inter- val’ procedure. During the task, presentation of a cue instructed the rats to respond after a specific interval of time elapsed (e.g., tone-8 seconds). We incorporated two cues; each requiring a response after a distinct time-interval (e.g., tone-8 seconds / light-16 seconds). We tested the effects of either reversibly inactivating the MD or PL individually or functionally disconnecting them on performance. All manipulations caused a comparable timing deficit. Specifically, responses showed little organization in time, as if primarily guided by motivational systems. These data expand our understanding of the networks that support timing and suggest that MD-PL interactions specifically are a core component. More broadly, our results suggest that timing tasks provide a reliable assay for characterizing the role of MD-PL interactions in cognition using rodents, which has been difficult to establish in the past.', 'corpus_id': 235649977, 'score': 0}, {'doc_id': '5678947', 'title': 'Rat Prefrontal Cortical Neurons Selectively Code Strategy Switches', 'abstract': 'Multiple memory systems are distinguished by different sets of neuronal circuits and operating principles optimized to solve different problems across mammalian species (Tulving and Schacter, 1994). When a rat selects an arm in a plus maze, for example, the choice can be guided by distinct neural systems (White and Wise, 1999) that encode different relationships among perceived stimuli, actions, and reward. Thus, egocentric or stimulus–response associations require striatal circuits, whereas spatial or episodic learning requires hippocampal circuits (Packard et al., 1989). Although these memory systems function in parallel (Packard and McGaugh, 1996), they can also interact competitively or synergistically (Kim and Ragozzino, 2005). The neuronal mechanisms that coordinate these multiple memory systems are not fully known, but converging evidence suggests that the prefrontal cortex (PFC) is central. The PFC is crucial for abstract, rule-guided behavior in primates and for switching rapidly between memory strategies in rats. We now report that rat medial PFC neuronal activity predicts switching between hippocampus- and caudate-dependent memory strategies. Prelimbic (PL) and infralimbic (IL) neuronal activity changed as rats switched memory strategies even as the rats performed identical behaviors but did not change when rats learned new contingencies using the same strategy. PL dynamics anticipated learning performance whereas IL lagged, suggesting that the two regions help initiate and establish new strategies, respectively. These neuronal dynamics suggest that the PFC contributes to the coordination of memory strategies by integrating the predictive relationships among stimuli, actions, and reward.', 'corpus_id': 5678947, 'score': 1}, {'doc_id': '235249388', 'title': 'Motivated semantic control: Exploring the effects of extrinsic reward and self-reference on semantic retrieval in semantic aphasia', 'abstract': 'Recent insights show increased motivation can benefit executive control, but this effect has not been explored in relation to semantic cognition. Patients with deficits of controlled semantic retrieval in the context of semantic aphasia (SA) after stroke may benefit from this approach since their deficits tend to be accompanied by deficits of cognitive control. We assessed the effect of both extrinsic and intrinsic motivation in healthy controls and semantic aphasia patients. Experiment 1 manipulated extrinsic reward using high or low levels of points for correct responses during a semantic association task. Experiment 2 manipulated the intrinsic value of items using self-reference; allocating pictures of items to the participant (‘self’) or researcher (‘other’) in a shopping game before people retrieved their semantic associations. These experiments revealed that patients, but not controls, showed better performance when given an extrinsic reward, consistent with the view that increased external motivation may help to ameliorate patients’ semantic control deficits. However, while self-reference was associated with better episodic memory, there was no effect on semantic retrieval. We conclude that semantic control deficits can be reduced when extrinsic rewards are anticipated; this enhanced motivational state is expected to support proactive control, for example, through the maintenance of task representations. It may be possible to harness this modulatory impact of reward to combat the control demands of semantic tasks in SA patients.', 'corpus_id': 235249388, 'score': 0}]"
2	{'doc_id': '220128138', 'title': 'Does the Whole Exceed its Parts? The Effect of AI Explanations on Complementary Team Performance', 'abstract': 'Many researchers motivate explainable AI with studies showing that human-AI team performance on decision-making tasks improves when the AI explains its recommendations. However, prior studies observed improvements from explanations only when the AI, alone, outperformed both the human and the best team. Can explanations help lead to complementary performance, where team accuracy is higher than either the human or the AI working solo? We conduct mixed-method user studies on three datasets, where an AI with accuracy comparable to humans helps participants solve a task (explaining itself in some conditions). While we observed complementary improvements from AI augmentation, they were not increased by explanations. Rather, explanations increased the chance that humans will accept the AI’s recommendation, regardless of its correctness. Our result poses new challenges for human-centered AI: Can we develop explanatory approaches that encourage appropriate trust in AI, and therefore help generate (or improve) complementary performance?', 'corpus_id': 220128138}	20506	"[{'doc_id': '237619843', 'title': 'Explainable Recommendations and Calibrated Trust: Two Systematic User Errors', 'abstract': 'The increased adoption of collaborative human-artificial intelligence decision-making tools triggered a need to explain recommendations for safe and effective collaboration. We explore how users interact with explanations and why trust-calibration errors occur, taking clinical decision-support systems as a case study.', 'corpus_id': 237619843, 'score': 1}, {'doc_id': '85429250', 'title': 'Uptake and interconversion of plasma unesterified 14C linoleic acid by gastrointestinal tract and blood forming tissues : an experimental study in the rat', 'abstract': 'Abstract The origin of the arachidonic acid (20:4, n-6) in the gastrointestinal tract and blood forming tissues is unknown. This study examines the rate of uptake and interconversion of unesterified 14 C linoleic acid (18:2, n-6) by the liver, gastrointestinal tract, bone marrow, and spleen. Albumin-bound unesterified 14 C-18:2 was injected intravenously into young male rats. The clearance rate of the 14 C-18:2 and the mass concentration of unesterified 18:2 in plasma were measured. After 5 and 10 min, the radioactivity of the tissue lipids and the degree of interconversion of 14 C-18:2 were determined. The rate of retention and interconversion of plasma unesterified 18:2 in different tissues could then be calculated. The retention of 14 C in liver lipids (3.6 to 4.5%/g of tissue) was several-fold higher than in the stomach (0.4%/g), small intestine (0.3%/g), colon (0.2%/g), and spleen (0.2%/g). Higher proportions of the 14 C were in phospholipids (PL) in the extrahepatic tissues (50 to 80%) than in the liver (42 to 44%). The main part of the PL radioactivity was in phosphatidylcholine. The percent of labeling in cardiolipin was 4 to 10 fold higher in the gastrointestinal tract and 8 to 20 fold higher in the spleen and bone marrow than in the liver. The percent of interconversion of retained 14 C-18:2 to δ6 desaturase products was higher in colon (16%), spleen (18%), and bone marrow (15 to 16%) than in liver (10 to 11%). The rate of 20:4 formation from plasma-free 18:2 was 14,200 pmol/min in liver, 106 pmol/min in the stomach, 751 pmol/min in the small intestine, and 159 pmol/min in the colon. This study indicates that a large part of the 20:4 pools in the gastrointestinal tract and blood-forming tissues can be formed by local interconversion reactions.', 'corpus_id': 85429250, 'score': 0}, {'doc_id': '210023849', 'title': 'Effect of confidence and explanation on accuracy and trust calibration in AI-assisted decision making', 'abstract': ""Today, AI is being increasingly used to help human experts make decisions in high-stakes scenarios. In these scenarios, full automation is often undesirable, not only due to the significance of the outcome, but also because human experts can draw on their domain knowledge complementary to the model's to ensure task success. We refer to these scenarios as AI-assisted decision making, where the individual strengths of the human and the AI come together to optimize the joint decision outcome. A key to their success is to appropriately calibrate human trust in the AI on a case-by-case basis; knowing when to trust or distrust the AI allows the human expert to appropriately apply their knowledge, improving decision outcomes in cases where the model is likely to perform poorly. This research conducts a case study of AI-assisted decision making in which humans and AI have comparable performance alone, and explores whether features that reveal case-specific model information can calibrate trust and improve the joint performance of the human and AI. Specifically, we study the effect of showing confidence score and local explanation for a particular prediction. Through two human experiments, we show that confidence score can help calibrate people's trust in an AI model, but trust calibration alone is not sufficient to improve AI-assisted decision making, which may also depend on whether the human can bring in enough unique knowledge to complement the AI's errors. We also highlight the problems in using local explanation for AI-assisted decision making scenarios and invite the research community to explore new approaches to explainability for calibrating human trust in AI."", 'corpus_id': 210023849, 'score': 1}, {'doc_id': '237635290', 'title': 'Discovering and Validating AI Errors With Crowdsourced Failure Reports', 'abstract': 'AI systems can fail to learn important behaviors, leading to real-world issues like safety concerns and biases. Discovering these systematic failures often requires significant developer attention, from hypothesizing potential edge cases to collecting evidence and validating patterns. To scale and streamline this process, we introduce crowdsourced failure reports, end-user descriptions of how or why a model failed, and show how developers can use them to detect AI errors. We also design and implement Deblinder, a visual analytics system for synthesizing failure reports that developers can use to discover and validate systematic failures. In semi-structured interviews and think-aloud studies with 10 AI practitioners, we explore the affordances of the Deblinder system and the applicability of failure reports in real-world settings. Lastly, we show how collecting additional data from the groups identified by developers can improve model performance.', 'corpus_id': 237635290, 'score': 0}, {'doc_id': '237492896', 'title': 'The Effect of Explanations on Trust in an Assistance System for Public Transport Users and the Role of the Propensity to Trust', 'abstract': 'The present study aimed to investigate whether explanations increase trust in an assistance system. Moreover, we wanted to take the role of the individual propensity to trust in technology into account. We conducted an empirical study in a virtual reality environment where 40 participants interacted with a specific assistance system for public transport users. The study was in a 2x2 mixed design with the within-subject factor assistance system feature (trip planner and connection request) and the between-subject factor explanation (with or without). We measured trust as explicit trust via a questionnaire and as implicit trust via an operationalization of the participants’ behavior. The results showed that trust propensity predicted explicit trust, and explanations increased explicit trust significantly. This was not the case for implicit trust, though, suggesting that explicit and implicit trust do not necessarily coincide. In conclusion, our results complement the literature on explainable artificial intelligence and trust in automation and provide topics for future research regarding the effect of explanations on trust in assistance systems or other technologies.', 'corpus_id': 237492896, 'score': 1}, {'doc_id': '236956490', 'title': 'Left, Right, and Gender: Exploring Interaction Traces to Mitigate Human Biases', 'abstract': 'Human biases impact the way people analyze data and make decisions. Recent work has shown that some visualization designs can better support cognitive processes and mitigate cognitive biases (i.e., errors that occur due to the use of mental ""shortcuts""). In this work, we explore how visualizing a user\'s interaction history (i.e., which data points and attributes a user has interacted with) can be used to mitigate potential biases that drive decision making by promoting conscious reflection of one\'s analysis process. Given an interactive scatterplot-based visualization tool, we showed interaction history in real-time while exploring data (by coloring points in the scatterplot that the user has interacted with), and in a summative format after a decision has been made (by comparing the distribution of user interactions to the underlying distribution of the data). We conducted a series of in-lab experiments and a crowd-sourced experiment to evaluate the effectiveness of interaction history interventions toward mitigating bias. We contextualized this work in a political scenario in which participants were instructed to choose a committee of 10 fictitious politicians to review a recent bill passed in the U.S. state of Georgia banning abortion after 6 weeks, where things like gender bias or political party bias may drive one\'s analysis process. We demonstrate the generalizability of this approach by evaluating a second decision making scenario related to movies. Our results are inconclusive for the effectiveness of interaction history (henceforth referred to as interaction traces) toward mitigating biased decision making. However, we find some mixed support that interaction traces, particularly in a summative format, can increase awareness of potential unconscious biases.', 'corpus_id': 236956490, 'score': 0}, {'doc_id': '237492114', 'title': 'Perceptions of Fairness and Trustworthiness Based on Explanations in Human vs. Automated Decision-Making', 'abstract': 'Automated decision systems (ADS) have become ubiquitous in many high-stakes domains. Those systems typically involve sophisticated yet opaque artificial intelligence (AI) techniques that seldom allow for full comprehension of their inner workings, particularly for affected individuals. As a result, ADS are prone to deficient oversight and calibration, which can lead to undesirable (e.g., unfair) outcomes. In this work, we conduct an online study with 200 participants to examine people’s perceptions of fairness and trustworthiness towards ADS in comparison to a scenario where a human instead of an ADS makes a high-stakes decision— and we provide thorough identical explanations regarding decisions in both cases. Surprisingly, we find that people perceive ADS as fairer than human decisionmakers. Our analyses also suggest that people’s AI literacy affects their perceptions, indicating that people with higher AI literacy favor ADS more strongly over human decision-makers, whereas low-AI-literacy people exhibit no significant differences in their perceptions.', 'corpus_id': 237492114, 'score': 1}, {'doc_id': '11183066', 'title': 'Combined measures toward restoration of temporal information from offline handwritten text', 'abstract': 'The research described in this paper intends to restore temporal information out of offline handwritten words. Such a system can be a backbone to a number of other more complicated systems such as signature verification, and handwriting analysis. We combined energetic measure with vision rules to estimate the original handwriting. Experimental results indicate a 28% improvement in the estimation accuracy of the combined measure over that of the energetic measure alone.', 'corpus_id': 11183066, 'score': 0}, {'doc_id': '237576342', 'title': 'From SAE-Levels to Cooperative Task Distribution:An Efficient and Usable Way to Deal with System Limitations?', 'abstract': 'Automated driving seems to be a promising approach to increase traffic safety, efficiency, and driver comfort. The defined automation capability levels (SAE) recommend a distinct takeover of the vehicle’s control from the human driver. This implies that if the system reaches a system boundary, the control falls back to the human. However, another possibility might be the cooperative approach of task distribution: The driver provides the missing information to the automation, which will stay activated. In a driving simulator study, we compared both a classical and a cooperative approach (N = 18). An automated car was driving on a rural road when a slower leading vehicle made it impossible for the automation to overtake. The participants could either initiate the overtake by providing the missing information cooperatively or fully taking over the vehicle’s control. Results showed that the cooperative approach has a higher usage and reduces workload. Therefore, the suggested cooperative approach seems to be more promising.', 'corpus_id': 237576342, 'score': 0}, {'doc_id': '236965949', 'title': 'Examining correlation between trust and transparency with explainable artificial intelligence', 'abstract': 'Trust between humans and artificial intelligence(AI) is an issue which has implications in many fields of human computer interaction. The current issue with artificial intelligence is a lack of transparency into its decision making, and literature shows that increasing transparency increases trust. Explainable artificial intelligence has the ability to increase transparency of AI, which could potentially increase trust for humans. This paper attempts to use the task of predicting yelp review star ratings with assistance from an explainable and non explainable artificial intelligence to see if trust is increased with increased transparency. Results show that for these tasks, explainable artificial intelligence provided significant increase in trust as a measure of influence.', 'corpus_id': 236965949, 'score': 1}]"
3	{'doc_id': '198984511', 'title': 'Forecasting species range dynamics with process-explicit models: matching methods to applications.', 'abstract': 'Knowing where species occur is fundamental to many ecological and environmental applications. Species distribution models (SDMs) are typically based on correlations between species occurrence data and environmental predictors, with ecological processes captured only implicitly. However, there is a growing interest in approaches that explicitly model processes such as physiology, dispersal, demography and biotic interactions. These models are believed to offer more robust predictions, particularly when extrapolating to novel conditions. Many process-explicit approaches are now available, but it is not clear how we can best draw on this expanded modelling toolbox to address ecological problems and inform management decisions. Here, we review a range of process-explicit models to determine their strengths and limitations, as well as their current use. Focusing on four common applications of SDMs - regulatory planning, extinction risk, climate refugia and invasive species - we then explore which models best meet management needs. We identify barriers to more widespread and effective use of process-explicit models and outline how these might be overcome. As well as technical and data challenges, there is a pressing need for more thorough evaluation of model predictions to guide investment in method development and ensure the promise of these new approaches is fully realised.', 'corpus_id': 198984511}	10905	"[{'doc_id': '235233696', 'title': 'A quantitative review of abundance-based species distribution models', 'abstract': 'The contributions of species to ecosystem functions or services depend not only on their presence in a given community, but also on their local abundance. Progress in predictive spatial modelling has largely focused on species occurrence, rather than abundance. As such, limited guidance exists on the most reliable methods to explain and predict spatial variation in abundance. We analysed the performance of 68 abundance-based species distribution models fitted to 800,000 standardised abundance records for more than 800 terrestrial bird and reef fish species. We found high heterogeneity in performance of abundance-based models. While many models performed poorly, a subset of models consistently reconstructed range-wide abundance patterns. The best predictions were obtained using random forests for frequently encountered and abundant species, and for predictions within the same environmental domain as model calibration. Extending predictions of species abundance outside of the environmental conditions used in model training generated poor predictions. Thus, interpolation of abundances between observations can help improve understanding of spatial abundance patterns, but extrapolated predictions of abundance, e.g. under climate change, have a much greater uncertainty. Our synthesis provides a roadmap for modelling abundance patterns, a key property of species’ distributions that underpins theoretical and applied questions in ecology and conservation.', 'corpus_id': 235233696, 'score': 1}, {'doc_id': '235200653', 'title': ""Can dynamic occupancy models improve predictions of species' range dynamics? A test using Swiss birds."", 'abstract': ""Predictions of species' current and future ranges are needed to effectively manage species under environmental change. Species ranges are typically estimated using correlative species distribution models (SDMs), which have been criticized for their static nature. In contrast, dynamic occupancy models explicitily describe temporal changes in species' occupancy via colonisation and local extinction probabilities, estimated from time series of occurrence data. Yet, tests of whether these models improve predictive accuracy under current or future conditions are rare. Using a long-term dataset on 69 Swiss birds, we tested whether dynamic occupancy models improve predictions of distribution changes over time compared to SDMs. We evaluated the accuracy of spatial predictions and their ability to detect population trends. We also explored how predictions differed when we accounted for imperfect detection and parameterised models using calibration datasets of different time series lengths. All model types had high spatial predictive performance when assessed across all sites (mean AUC > 0.8), with flexible machine-learning SDM algorithms outperforming parametric static and dynamic occupancy models. However, none of the models performed well at identifying sites where range changes are likely to occur. In terms of estimating population trends, dynamic occupancy models performed best, particularly for species with strong population changes and when fit with sufficient data, while static SDMs performed very poorly. Overall, our study highlights the importance of considering what aspects of performance matter most when selecting a modelling method for a particular application and the need for further research to improve model utility. While dynamic occupancy models show promise for capturing range dynamics and inferring population trends when fitted with sufficient data, computational constraints on variable selection and model fitting can lead to reduced spatial accuracy of predictions, an area warranting more attention."", 'corpus_id': 235200653, 'score': 1}, {'doc_id': '221878982', 'title': 'Optimal Sampling Regimes for Estimating Population Dynamics', 'abstract': 'Ecologists are interested in modeling the population growth of species in various ecosystems. Studying population dynamics can assist environmental managers in making better decisions for the environment. Traditionally, the sampling of species and tracking of populations have been recorded on a regular time frequency. However, sampling can be an expensive process due to available resources, money and time. Limiting sampling makes it challenging to properly track the growth of a population. Thus, we propose a new and novel approach to designing sampling regimes based on the dynamics associated with population growth models. This design study minimizes the amount of time ecologists spend in the field, while maximizing the information provided by the data.', 'corpus_id': 221878982, 'score': 1}, {'doc_id': '225094269', 'title': 'Projecting the optimal control strategy on invasive plants combining effects of herbivores and native plants resistance', 'abstract': ""Understanding how to limit biological invasion is critical, especially in the context of accelerating anthropogenic ecological changes. Although biological invasion success could be explained by the lack of natural enemies in new regions, recent studies have revealed that resident herbivores often do have a substantial effect on both native and invasive plants. Very few studies have included consideration of native plant resistance while estimating methods of controlling invasion; hence, it is unclear to what extent the interactive effects of controlling approaches and native plants' resistance could slow down or even inhibit biological invasion. We developed a spatial modeling framework, using a paired logistic equation model, with considerations of the dispersal processes, to capture the dynamics change of native and invasive plants under various strategies of control. We found that when biocontrol agents could have a strong effect on invasive plant, that could almost completely limit the invasion, together with a high native plant resistance. However, a high application frequency is needed make an efficient impact, whereas, a low frequency treatment leads to nearly the same outcome as the no treatment case. Lastly, we showed that evenly controlling a larger area with a weaker effect still lead to a better outcome than focusing on small patches with a stronger effect. Overall, this study has some management implications, such as how to determine the optimal allocation strategy."", 'corpus_id': 225094269, 'score': 0}, {'doc_id': '233310309', 'title': 'Convolutional neural networks improve species distribution modelling by capturing the spatial structure of the environment', 'abstract': 'Convolutional Neural Networks (CNNs) are statistical models suited for learning complex visual patterns. In the context of Species Distribution Models (SDM) and in line with predictions of landscape ecology and island biogeography, CNN could grasp how local landscape structure affects prediction of species occurrence in SDMs. The prediction can thus reflect the signatures of entangled ecological processes. Although previous machine-learning based SDMs can learn complex influences of environmental predictors, they cannot acknowledge the influence of environmental structure in local landscapes (hence denoted “punctual models”). In this study, we applied CNNs to a large dataset of plant occurrences in France (GBIF), on a large taxonomical scale, to predict ranked relative probability of species (by joint learning) to any geographical position. We examined the way local environmental landscapes improve prediction by performing alternative CNN models deprived of information on landscape heterogeneity and structure (“ablation experiments”). We found that the landscape structure around location crucially contributed to improve predictive performance of CNN-SDMs. CNN models can classify the predicted distributions of many species, as other joint modelling approaches, but they further prove efficient in identifying the influence of local environmental landscapes. CNN can then represent signatures of spatially structured environmental drivers. The prediction gain is noticeable for rare species, which open promising perspectives for biodiversity monitoring and conservation strategies. Therefore, the approach is of both theoretical and practical interest. We discuss the way to test hypotheses on the patterns learnt by CNN, which should be essential for further interpretation of the ecological processes at play.', 'corpus_id': 233310309, 'score': 1}, {'doc_id': '221672958', 'title': 'Introduction to Modern Climate Change. Andrew E. Dessler: Cambridge University Press, 2011, 252 pp, ISBN-10: 0521173159.', 'abstract': 'Climate change is the variability of the climate system that includes the atmosphere, the biogeochemical cycles (Carbon cycle, Nitrogen cycle and Hydrological cycle), the land surface, ice and the biotic and abiotic components of the planet earth. Significant impact of climate change is seen in the form of rise in temperature called as global warming. Carbon dioxide (CO2), methane (CH4), and nitrous oxide (N2O) are the primary greenhouse gases (GHGs) mainly responsible for the global warming and climate change. These GHGs have drawn lot of attention due to their significant role in the global warming potential. Intergovernmental Panel on Climate Change (IPCC) suggested to stop global warming at 1.5oC above preindustrial levels as warming beyond this level might lead to heat extremes, alter insect and plant phenology (Phenological shifts) and more occurrence of vector borne diseases. Climate change is the topic of interest in all fields of life starting from social science and going to the applied science. Global climate cycles and world food production systems are under threat due to the recent climate extreme events. These events include heat waves and change in the rainfall patterns. Thus, risk reduction intervention in the form of mitigation and adaptation is required to minimize the impacts of climate change. Mitigation option includes understanding the present and future components of the climate system and interaction among them through coupled modeling system i.e. Global Circulation Model (GCM). Finally, global issue of climate change could be addressed by taking worldwide cooperation and action and adopting sustainable measures like use of alternative energy sources. The visible benefit on recovery of climate has been seen recently through global lockdown against coronavirus disease 2019 (COVID-19) pandemic.', 'corpus_id': 221672958, 'score': 0}, {'doc_id': '127157909', 'title': 'Forecasting species range shifts: a Hierarchical Bayesian framework for estimating process-based models of range dynamics', 'abstract': ""Shifts of species ranges have been widely observed as ‘fingerprints’ of climate change and more drastic shifts are expected in the coming decades. Current studies projecting range shifts in response to climate change arepredominantly based on phenomenological models of potential climate space (climate envelope models). These models assume that species distributions are at equilibrium with climate, both at present and in the future. A more reliable projection of range dynamics under environmental change requires process-based models that can be fitted to distribution data and permit a more comprehensive assessment of forecast uncertainties [1]. To achieve this goal, we develop a Hierarchical Bayesian framework [2] that utilizes models of local population dynamics and regional dispersal to link data on species distribution and abundance to explanatory environmental variables. In a simulation study we investigate the performance of this approach in relation to the biological characteristics of the target species and the quantity and quality of biological information available. We use R to implement an integrated routine that combines grida-based ecological simulation model and a ‘virtual ecologist’ with efficient MCMC algorithms (e.g. DRAM [3]) for sampling from the full posterior distribution of model parameters and derived predictions of spatially distributed abundances under prescribed climatic changes. This enables us to run a range of virtual scenarios differing in both ecological assumptions and sampling design in order to examine how forecast uncertainty depends on a species' ecology as well as on data quality and quantity."", 'corpus_id': 127157909, 'score': 1}, {'doc_id': '224783495', 'title': 'Introduction to Modern Climate Change. Andrew E. Dessler: Cambridge University Press, 2011, 252 pp, ISBN-10: 0521173159.', 'abstract': 'Climate change is the variability of the climate system that includes the atmosphere, the biogeochemical cycles (Carbon cycle, Nitrogen cycle and Hydrological cycle), the land surface, ice and the biotic and abiotic components of the planet earth. Significant impact of climate change is seen in the form of rise in temperature called as global warming. Carbon dioxide (CO2), methane (CH4), and nitrous oxide (N2O) are the primary greenhouse gases (GHGs) mainly responsible for the global warming and climate change. These GHGs have drawn lot of attention due to their significant role in the global warming potential. Intergovernmental Panel on Climate Change (IPCC) suggested to stop global warming at 1.5oC above preindustrial levels as warming beyond this level might lead to heat extremes, alter insect and plant phenology (Phenological shifts) and more occurrence of vector borne diseases. Climate change is the topic of interest in all fields of life starting from social science and going to the applied science. Global climate cycles and world food production systems are under threat due to the recent climate extreme events. These events include heat waves and change in the rainfall patterns. Thus, risk reduction intervention in the form of mitigation and adaptation is required to minimize the impacts of climate change. Mitigation option includes understanding the present and future components of the climate system and interaction among them through coupled modeling system i.e. Global Circulation Model (GCM). Finally, global issue of climate change could be addressed by taking worldwide cooperation and action and adopting sustainable measures like use of alternative energy sources. The visible benefit on recovery of climate has been seen recently through global lockdown against coronavirus disease 2019 (COVID-19) pandemic.', 'corpus_id': 224783495, 'score': 0}, {'doc_id': '222290900', 'title': 'Regional Flood Risk Projections under Climate Change', 'abstract': 'Flood-related risks to people and property are expected to increase in the future due to environmental and demographic changes. It is important to quantify and effectively communicate flood hazards and exposure to inform the design and implementation of flood risk management strategies. Here we develop an integrated modeling framework to assess projected changes in regional riverine flood inundation risks. The framework samples climate model outputs to force a hydrologic model and generate streamflow projections. Together with a statistical and hydraulic model, we use the projected streamflow to map the uncertainty of flood inundation projections for extreme flood events. We implement the framework for rivers across the state of Pennsylvania, United States. Our projections suggest that flood hazards and exposure across Pennsylvania are overall increasing with future climate change. Specific regions, including the main stem Susquehanna River, lower portion of the Allegheny basin and central portion of Delaware River basin, demonstrate higher flood inundation risks. In our analysis, the climate uncertainty dominates the overall uncertainty surrounding the flood inundation projection chain. The combined hydrologic and hydraulic uncertainties can account for as much as 37% of the total uncertainty. We discuss how this framework can provide regional and dynamic flood-risk assessments and help to inform the design of risk-management strategies.', 'corpus_id': 222290900, 'score': 0}, {'doc_id': '222217038', 'title': 'Introduction to Modern Climate Change. Andrew E. Dessler: Cambridge University Press, 2011, 252 pp, ISBN-10: 0521173159.', 'abstract': 'Climate change is the variability of the climate system that includes the atmosphere, the biogeochemical cycles (Carbon cycle, Nitrogen cycle and Hydrological cycle), the land surface, ice and the biotic and abiotic components of the planet earth. Significant impact of climate change is seen in the form of rise in temperature called as global warming. Carbon dioxide (CO2), methane (CH4), and nitrous oxide (N2O) are the primary greenhouse gases (GHGs) mainly responsible for the global warming and climate change. These GHGs have drawn lot of attention due to their significant role in the global warming potential. Intergovernmental Panel on Climate Change (IPCC) suggested to stop global warming at 1.5oC above preindustrial levels as warming beyond this level might lead to heat extremes, alter insect and plant phenology (Phenological shifts) and more occurrence of vector borne diseases. Climate change is the topic of interest in all fields of life starting from social science and going to the applied science. Global climate cycles and world food production systems are under threat due to the recent climate extreme events. These events include heat waves and change in the rainfall patterns. Thus, risk reduction intervention in the form of mitigation and adaptation is required to minimize the impacts of climate change. Mitigation option includes understanding the present and future components of the climate system and interaction among them through coupled modeling system i.e. Global Circulation Model (GCM). Finally, global issue of climate change could be addressed by taking worldwide cooperation and action and adopting sustainable measures like use of alternative energy sources. The visible benefit on recovery of climate has been seen recently through global lockdown against coronavirus disease 2019 (COVID-19) pandemic.', 'corpus_id': 222217038, 'score': 0}]"
4	{'doc_id': '212747499', 'title': 'Neuroevolution of self-interpretable agents', 'abstract': 'Inattentional blindness is the psychological phenomenon that causes one to miss things in plain sight. It is a consequence of the selective attention in perception that lets us remain focused on important parts of our world without distraction from irrelevant details. Motivated by selective attention, we study the properties of artificial agents that perceive the world through the lens of a self-attention bottleneck. By constraining access to only a small fraction of the visual input, we show that their policies are directly interpretable in pixel space. We find neuroevolution ideal for training self-attention architectures for vision-based reinforcement learning (RL) tasks, allowing us to incorporate modules that can include discrete, non-differentiable operations which are useful for our agent. We argue that self-attention has similar properties as indirect encoding, in the sense that large implicit weight matrices are generated from a small number of key-query parameters, thus enabling our agent to solve challenging vision based tasks with at least 1000x fewer parameters than existing methods. Since our agent attends to only task critical visual hints, they are able to generalize to environments where task irrelevant elements are modified while conventional methods fail.1', 'corpus_id': 212747499}	20117	"[{'doc_id': '237194840', 'title': 'Global Pooling, More than Meets the Eye: Position Information is Encoded Channel-Wise in CNNs', 'abstract': 'In this paper, we challenge the common assumption that collapsing the spatial dimensions of a 3D (spatial-channel) tensor in a convolutional neural network (CNN) into a vector via global pooling removes all spatial information. Specifically, we demonstrate that positional information is encoded based on the ordering of the channel dimensions, while semantic information is largely not. Following this demonstration, we show the real world impact of these findings by applying them to two applications. First, we propose a simple yet effective data augmentation strategy and loss function which improves the translation invariance of a CNN’s output. Second, we propose a method to efficiently determine which channels in the latent representation are responsible for (i) encoding overall position information or (ii) region-specific positions. We first show that semantic segmentation has a significant reliance on the overall position channels to make predictions. We then show for the first time that it is possible to perform a ‘region-specific’ attack, and degrade a network’s performance in a particular part of the input. We believe our findings and demonstrated applications will benefit research areas concerned with understanding the characteristics of CNNs. Code is available at: https://github.com/islamamirul/PermuteNet.', 'corpus_id': 237194840, 'score': 1}, {'doc_id': '235806856', 'title': 'Training BatchNorm and Only BatchNorm: On the Expressivity of Random Features in CNNs', 'abstract': 'Batch normalization (BatchNorm) has become an indispensable tool for training deep neural networks, yet it is still poorly understood. Although previous work has typically focused on studying its normalization component, BatchNorm also adds two per-feature trainable parameters—a coefficient and a bias—whose role and expressive power remain unclear. To study this question, we investigate the performance achieved when training only these parameters and freezing all others at their random initializations. We find that doing so leads to surprisingly high performance. For example, sufficiently deep ResNets reach 82% (CIFAR-10) and 32% (ImageNet, top-5) accuracy in this configuration, far higher than when training an equivalent number of randomly chosen parameters elsewhere in the network. BatchNorm achieves this performance in part by naturally learning to disable around a third of the random features. Not only do these results highlight the under-appreciated role of the affine parameters in BatchNorm, but—in a broader sense—they characterize the expressive power of neural networks constructed simply by shifting and rescaling random features.', 'corpus_id': 235806856, 'score': 1}, {'doc_id': '236924338', 'title': 'Generalization in Multimodal Language Learning from Simulation', 'abstract': 'Neural networks can be powerful function approximators, which are able to model high-dimensional feature distributions from a subset of examples drawn from the target distribution. Naturally, they perform well at generalizing within the limits of their target function, but they often fail to generalize outside of the explicitly learned feature space. It is therefore an open research topic whether and how neural network-based architectures can be deployed for systematic reasoning. Many studies have shown evidence for poor generalization, but they often work with abstract data or are limited to single-channel input. Humans, however, learn and interact through a combination of multiple sensory modalities, and rarely rely on just one. To investigate compositional generalization in a multimodal setting, we generate an extensible dataset with multimodal input sequences from simulation. We investigate the influence of the underlying training data distribution on compostional generalization in a minimal LSTM-based network trained in a supervised, time continuous setting. We find compositional generalization to fail in simple setups while improving with the number of objects, actions, and particularly with a lot of color overlaps between objects. Furthermore, multimodality strongly improves compositional generalization in settings where a pure vision model struggles to generalize.', 'corpus_id': 236924338, 'score': 0}, {'doc_id': '237304042', 'title': 'Glimpse-Attend-and-Explore: Self-Attention for Active Visual Exploration', 'abstract': 'Active visual exploration aims to assist an agent with a limited field of view to understand its environment based on partial observations made by choosing the best viewing directions in the scene. Recent methods have tried to address this problem either by using reinforcement learning, which is difficult to train, or by uncertainty maps, which are task-specific and can only be implemented for dense prediction tasks. In this paper, we propose the Glimpse-Attendand-Explore model which: (a) employs self-attention to guide the visual exploration instead of task-specific uncertainty maps; (b) can be used for both dense and sparse prediction tasks; and (c) uses a contrastive stream to further improve the representations learned. Unlike previous works, we show the application of our model on multiple tasks like reconstruction, segmentation and classification. Our model provides encouraging results while being less dependent on dataset bias in driving the exploration. We further perform an ablation study to investigate the features and attention learned by our model. Finally, we show that our self-attention module learns to attend different regions of the scene by minimizing the loss on the downstream task. Code: https://github.com/ soroushseifi/glimpse-attend-explore.', 'corpus_id': 237304042, 'score': 0}, {'doc_id': '49653092', 'title': 'An Intriguing Failing of Convolutional Neural Networks and the CoordConv Solution', 'abstract': 'Few ideas have enjoyed as large an impact on deep learning as convolution. For any problem involving pixels or spatial representations, common intuition holds that convolutional neural networks may be appropriate. In this paper we show a striking counterexample to this intuition via the seemingly trivial coordinate transform problem, which simply requires learning a mapping between coordinates in (x,y) Cartesian space and one-hot pixel space. Although convolutional networks would seem appropriate for this task, we show that they fail spectacularly. We demonstrate and carefully analyze the failure first on a toy problem, at which point a simple fix becomes obvious. We call this solution CoordConv, which works by giving convolution access to its own input coordinates through the use of extra coordinate channels. Without sacrificing the computational and parametric efficiency of ordinary convolution, CoordConv allows networks to learn either complete translation invariance or varying degrees of translation dependence, as required by the end task. CoordConv solves the coordinate transform problem with perfect generalization and 150 times faster with 10--100 times fewer parameters than convolution. This stark contrast raises the question: to what extent has this inability of convolution persisted insidiously inside other tasks, subtly hampering performance from within? A complete answer to this question will require further investigation, but we show preliminary evidence that swapping convolution for CoordConv can improve models on a diverse set of tasks. Using CoordConv in a GAN produced less mode collapse as the transform between high-level spatial latents and pixels becomes easier to learn. A Faster R-CNN detection model trained on MNIST showed 24% better IOU when using CoordConv, and in the RL domain agents playing Atari games benefit significantly from the use of CoordConv layers.', 'corpus_id': 49653092, 'score': 1}, {'doc_id': '145047837', 'title': 'Deconstructing Lottery Tickets: Zeros, Signs, and the Supermask', 'abstract': 'The recent ""Lottery Ticket Hypothesis"" paper by Frankle & Carbin showed that a simple approach to creating sparse networks (keeping the large weights) results in models that are trainable from scratch, but only when starting from the same initial weights. The performance of these networks often exceeds the performance of the non-sparse base model, but for reasons that were not well understood. In this paper we study the three critical components of the Lottery Ticket (LT) algorithm, showing that each may be varied significantly without impacting the overall results. Ablating these factors leads to new insights for why LT networks perform as well as they do. We show why setting weights to zero is important, how signs are all you need to make the reinitialized network train, and why masking behaves like training. Finally, we discover the existence of Supermasks, masks that can be applied to an untrained, randomly initialized network to produce a model with performance far better than chance (86% on MNIST, 41% on CIFAR-10).', 'corpus_id': 145047837, 'score': 1}, {'doc_id': '236923189', 'title': 'SLOT MACHINES: DISCOVERING WINNING COMBINA-', 'abstract': 'In contrast to traditional weight optimization in a continuous space, we demonstrate the existence of effective random networks whose weights are never updated. By selecting a weight among a fixed set of random values for each individual connection, our method uncovers combinations of random weights that match the performance of traditionally-trained networks of the same capacity. We refer to our networks as “slot machines” where each reel (connection) contains a fixed set of symbols (random values). Our backpropagation algorithm “spins” the reels to seek “winning” combinations, i.e., selections of random weight values that minimize the given loss. Quite surprisingly, we find that allocating just a few random values to each connection (e.g., 8 values per connection) yields highly competitive combinations despite being dramatically more constrained compared to traditionally learned weights. Moreover, finetuning these combinations often improves performance over the trained baselines. A randomly initialized VGG19 with 8 values per connection contains a combination that achieves 90% test accuracy on CIFAR-10. Our method also achieves an impressive performance of 98.1% on MNIST for neural networks containing only random weights.', 'corpus_id': 236923189, 'score': 0}, {'doc_id': '237413485', 'title': 'Discovering Weight Initializers with Meta Learning', 'abstract': 'Deep neural network training largely depends on the choice of initial weight distribution. However, this choice can often be nontrivial. Existing theoretical results for this problem mostly cover simple architectures, e.g., feedforward networks with ReLU activations. The architectures used for practical problems are more complex and often incorporate many overlapping modules, making them challenging for theoretical analysis. Therefore, practitioners have to use heuristic initializers with questionable optimality and stability. In this study, we propose a task-agnostic approach that discovers initializers for specific network architectures and optimizers by learning the initial weight distributions directly through the use of Meta-Learning. In several supervised and unsupervised learning scenarios, we show the advantage of our initializers in terms of both faster convergence and higher model performance. The PyTorch implementation of our algorithm is available online.', 'corpus_id': 237413485, 'score': 0}, {'doc_id': '234742218', 'title': 'Pay Attention to MLPs', 'abstract': 'Transformers [1] have become one of the most important architectural innovations in deep learning and have enabled many breakthroughs over the past few years. Here we propose a simple attention-free network architecture, gMLP, based solely on MLPs with gating, and show that it can perform as well as Transformers in key language and vision applications. Our comparisons show that self-attention is not critical for Vision Transformers, as gMLP can achieve the same accuracy. For BERT, our model achieves parity with Transformers on pretraining perplexity and is better on some downstream tasks. On finetuning tasks where gMLP performs worse, making the gMLP model substantially larger can close the gap with Transformers. In general, our experiments show that gMLP can scale as well as Transformers over increased data and compute.', 'corpus_id': 234742218, 'score': 1}, {'doc_id': '235795569', 'title': 'CoBERL: Contrastive BERT for Reinforcement Learning', 'abstract': 'Many reinforcement learning (RL) agents require a large amount of experience to solve tasks. We propose Contrastive BERT for RL (COBERL), an agent that combines a new contrastive loss and a hybrid LSTM-transformer architecture to tackle the challenge of improving data efficiency. COBERL enables efficient, robust learning from pixels across a wide range of domains. We use bidirectional masked prediction in combination with a generalization of recent contrastive methods to learn better representations for transformers in RL, without the need of hand engineered data augmentations. We find that COBERL consistently improves performance across the full Atari suite, a set of control tasks and a challenging 3D environment.', 'corpus_id': 235795569, 'score': 0}]"
5	{'doc_id': '45229749', 'title': 'Polarized Subtyping for Sized Types', 'abstract': 'We present an algorithm for deciding polarized higher-order subtyping without bounded quantification. Constructors are identified not only modulo β, but also η. We give a direct proof of completeness, without constructing a model or establishing a strong normalization theorem. Inductive and coinductive types are enriched with a notion of size and the subtyping calculus is extended to account for the arising inclusions between the sized types.', 'corpus_id': 45229749}	4876	"[{'doc_id': '235742698', 'title': 'A Framework for Proof-carrying Logical Transformations', 'abstract': 'In various provers and deductive verification tools, logical transformations are used extensively in order to reduce a proof task into a number of simpler tasks. Logical transformations are often part of the trusted base of such tools. In this paper, we develop a framework to improve confidence in their results. We follow a modular and skeptical approach: transformations are instrumented independently of each other and produce certificates that are checked by a third-party tool. Logical transformations are considered in a higher-order logic, with type polymorphism and built-in theories such as equality and integer arithmetic. We develop a language of proof certificates for them and use it to implement the full chain of certificate generation and certificate verification.', 'corpus_id': 235742698, 'score': 0}, {'doc_id': '218531097', 'title': 'The Duality of Subtyping', 'abstract': 'Subtyping is a concept frequently encountered in many programming languages and calculi. Various forms of subtyping exist for different type system features, including intersection types, union types or bounded quantification. Normally these features are designed independently of each other, without exploiting obvious similarities (or dualities) between features. This paper proposes a novel methodology for designing subtyping relations that exploits duality between features. At the core of our methodology is a generalization of subtyping relations, which we call Duotyping. Duotyping is parameterized by the mode of the relation. One of these modes is the usual subtyping, while another mode is supertyping (the dual of subtyping). Using the mode it is possible to generalize the usual rules of subtyping to account not only for the intended behaviour of one particular language construct, but also of its dual. Duotyping brings multiple benefits, including: shorter specifications and implementations, dual features that come essentially for free, as well as new proof techniques for various properties of subtyping. To evaluate a design based on Duotyping against traditional designs, we formalized various calculi with common OOP features (including union types, intersection types and bounded quantification) in Coq in both styles. Our results show that the metatheory when using Duotyping does not come at a significant cost: the metatheory with Duotyping has similar complexity and size compared to the metatheory for traditional designs. However, we discover new features as duals to well-known features. Furthermore, we also show that Duotyping can significantly simplify transitivity proofs for many of the calculi studied by us. 2012 ACM Subject Classification Software and its engineering → Object oriented languages', 'corpus_id': 218531097, 'score': 1}, {'doc_id': '2747372', 'title': 'Unifying typing and subtyping', 'abstract': 'In recent years dependent types have become a hot topic in programming language research. A key reason why dependent types are interesting is that they allow unifying types and terms, which enables both additional expressiveness and economy of concepts. Unfortunately there has been much less work on dependently typed calculi for object-oriented programming. This is partly because it is widely acknowledged that the combination between dependent types and subtyping is particularly challenging. This paper presents λ I≤, which is a dependently typed generalization of System F≤. The resulting calculus follows the style of Pure Type Systems, and contains a single unified syntactic sort that accounts for expressions, types and kinds. To address the challenges posed by the combination of dependent types and subtyping, λ I≤ employs a novel technique that unifies typing and subtyping. In λ I≤ there is only a judgement that is akin to a typed version of subtyping. Both the typing relation, as well as type well-formedness are just special cases of the subtyping relation. The resulting calculus has a rich metatheory and enjoys of several standard and desirable properties, such as subject reduction, transitivity of subtyping, narrowing as well as standard substitution lemmas. All the metatheory of λ I≤ is mechanically proved in the Coq theorem prover. Furthermore, (and as far as we are aware) λ I≤ is the first dependently typed calculus that completely subsumes System F≤, while preserving various desirable properties.', 'corpus_id': 2747372, 'score': 1}, {'doc_id': '18904380', 'title': 'An Interval-Based Inference of Variant Parametric Types', 'abstract': 'Variant parametric types represent the successful integration of subtype and parametric polymorphism to support a more flexible subtyping for Java like languages. A key feature that helps strengthen this integration is the use-site variance. Depending on how the fields are used, each variance denotes a covariant, a contravariant, an invariant or a bivariant subtyping. By annotating variance properties on each type argument to a parametric class, programmers can choose various desirable variance properties for each use of the parametric class. Although Java library classes have been successfully refactored to use variant parametric types, these mechanisms are often criticized, due to the difficulty of choosing appropriate variance annotations. Several algorithms have been proposed for automatically refactoring legacy Java code to use generic libraries, but none can support the full flexibility of the use-site variance-based subtyping. This paper addresses this difficulty by proposing a novel interval-based approach to inferring both the variance annotations and the type arguments. Each variant parametric type is regarded as an interval type with two type bounds, a lower bound for writing and an upper bound for reading. We propose a constraint-based inference algorithm that works on a per method basis, as a summary-based analysis.', 'corpus_id': 18904380, 'score': 1}, {'doc_id': '232334124', 'title': 'Comparing Haskell and Scala support for generic programming', 'abstract': 'Datatype-generic programming (DGP) involves parametrization of programs by the shape of data, in the form of type constructors such as ‘list of’. Most approaches to DGP are developed in pure functional programming languages such as Haskell. We argue that the functional object-oriented language Scala is in many ways a better choice. Not only does Scala provide equivalents of all the necessary functional programming features (such as parametric polymorphism, higher-order functions, higher-kinded type operations, and typeand constructor-classes), but it also provides the most useful features of objectoriented languages (such as subtyping, overriding, traditional single inheritance, and multiple inheritance in the form of traits). Common Haskell techniques for DGP can be conveniently replicated in Scala, whereas the extra expressivity provides some important additional benefits in terms of extensibility and reuse. We illustrate this by comparing two simple approaches in Haskell, pointing out their limitations and showing how equivalent approaches in Scala address some of these limitations. Finally, we present three case studies on how to implement in Scala real DGP approaches from the literature: Hinze’s ‘Generics for the Masses’, Lämmel and Peyton Jones’s ‘Scrap your Boilerplate with Class’, and Gibbons’s ‘Origami Programming’.', 'corpus_id': 232334124, 'score': 0}, {'doc_id': '235632529', 'title': 'CPS Semantics: Smoother Nondeterminism in Operational Semantics', 'abstract': 'This paper introduces the CPS-big-step and CPS-small-step judgments. These two judgments describe operational semantics by relating starting states to sets of outcomes rather than to individual outcomes. A single derivation of these semantics for a particular starting state and program describes all possible nondeterministic executions, whereas in traditional small-step and big-step semantics, each derivation only talks about one single execution. We demonstrate how this restructuring allows for straightforward modeling of languages featuring both nondeterminism and undefined behavior. Specifically, our semantics inherently assert safety, i.e. they guarantee that none of the execution branches gets stuck, while traditional semantics need either a separate judgment or additional error markers to specify safety in the presence of nondeterminism. Applications presented include proofs of type soundness for lambda calculi, mechanical derivation of reasoning rules for program verification, and a forward proof of compiler correctness for terminating but potentially nondeterministic programs. All results in this paper have been formalized in Coq.', 'corpus_id': 235632529, 'score': 0}, {'doc_id': '125541714', 'title': 'Higher-Order Subtyping with Type Intervals', 'abstract': ""Modern, statically typed programming languages provide various abstraction facilities at both the term- and type-level. Common abstraction mechanisms for types include parametric polymorphism -- a hallmark of functional languages -- and subtyping -- which is pervasive in object-oriented languages. Additionally, both kinds of languages may allow parametrized (or generic) datatype definitions in modules or classes. When several of these features are present in the same language, new and more expressive combinations arise, such as (1) bounded quantification, (2) bounded operator abstractions and (3) translucent type definitions. An example of such a language is Scala, which features all three of the aforementioned type-level constructs. This increases the expressivity of the language, but also the complexity of its type system. From a theoretical point of view, the various abstraction mechanisms have been studied through different extensions of Girard's higher-order polymorphic lambda-calculus F-omega. Higher-order subtyping and bounded polymorphism (1 and 2) have been formalized in F-omega-sub and its many variants; type definitions of various degrees of opacity (3) have been formalized through extensions of F-omega with singleton types. In this dissertation, I propose type intervals as a unifying concept for expressing (1--3) and other related constructs. In particular, I develop an extension of F-omega with interval kinds as a formal theory of higher-order subtyping with type intervals, and show how the familiar concepts of higher-order bounded quantification, bounded operator abstraction and singleton kinds can all be encoded in a semantics-preserving way using interval kinds. Going beyond the status quo, the theory is expressive enough to also cover less familiar constructs, such as lower-bounded operator abstractions and first-class, higher-order inequality constraints. I establish basic metatheoretic properties of the theory: I prove that subject reduction holds for well-kinded types w.r.t. full beta-reduction, that types and kinds are weakly normalizing, and that the theory is type safe w.r.t. its call-by-value operational reduction semantics. Key to this metatheoretic development is the use of hereditary substitution and the definition of an equivalent, canonical presentation of subtyping, which involves only normal types and kinds. The resulting metatheory is entirely syntactic, i.e. does not involve any model constructions, and has been fully mechanized in Agda. The extension of F-omega with interval kinds constitutes a stepping stone to the development of a higher-order version of the calculus of Dependent Object Types (DOT) -- the theoretical foundation of Scala's type system. In the last part of this dissertation, I briefly sketch a possible extension of the theory toward this goal and discuss some of the challenges involved in adapting the existing metatheory to that extension."", 'corpus_id': 125541714, 'score': 1}, {'doc_id': '232110795', 'title': 'Contextual modal types for algebraic effects and handlers', 'abstract': 'Programming languages with algebraic effects often track the computations’ effects using type-and-effect systems. In this paper, we propose to view an algebraic effect theory of a computation as a variable context; consequently, we propose to track algebraic effects of a computation with contextual modal types. We develop ECMTT, a novel calculus which tracks algebraic effects by a contextualized variant of the modal □ (necessity) operator, that it inherits from Contextual Modal Type Theory (CMTT). Whereas type-and-effect systems add effect annotations on top of a prior programming language, the effect annotations in ECMTT are inherent to the language, as they are managed by programming constructs corresponding to the logical introduction and elimination forms for the □ modality. Thus, the type-and-effect system of ECMTT is actually just a type system. Our design obtains the properties of local soundness and completeness, and determines the operational semantics solely by β-reduction, as customary in other logic-based calculi. In this view, effect handlers arise naturally as a witness that one context (i.e., algebraic theory) can be reached from another, generalizing explicit substitutions from CMTT. To the best of our knowledge, ECMTT is the first system to relate algebraic effects to modal types. We also see it as a step towards providing a correspondence in the style of Curry and Howard that may transfer a number of results from the fields of modal logic and modal type theory to that of algebraic effects.', 'corpus_id': 232110795, 'score': 0}, {'doc_id': '219532033', 'title': 'Explicit effect subtyping', 'abstract': 'Abstract As popularity of algebraic effects and handlers increases, so does a demand for their efficient execution. Eff, an ML-like language with native support for handlers, has a subtyping-based effect system on which an effect-aware optimising compiler could be built. Unfortunately, in our experience, implementing optimisations for Eff is overly error-prone because its core language is implicitly typed, making code transformations very fragile. To remedy this, we present an explicitly typed polymorphic core calculus for algebraic effect handlers with a subtyping-based type-and-effect system. It reifies appeals to subtyping in explicit casts with coercions that witness the subtyping proof, quickly exposing typing bugs in program transformations. Our typing-directed elaboration comes with a constraint-based inference algorithm that turns an implicitly typed Eff-like language into our calculus. Moreover, all coercions and effect information can be erased in a straightforward way, demonstrating that coercions have no computational content. Additionally, we present a monadic translation from our calculus into a pure language without algebraic effects or handlers, using the effect information to introduce monadic constructs only where necessary.', 'corpus_id': 219532033, 'score': 1}, {'doc_id': '232478760', 'title': 'Idris 2: Quantitative Type Theory in Practice', 'abstract': 'Dependent types allow us to express precisely what a function is intended to do. Recent work on Quantitative Type Theory (QTT) extends dependent type systems with linearity, also allowing precision in expressing when a function can run. This is promising, because it suggests the ability to design and reason about resource usage protocols, such as we might find in distributed and concurrent programming, where the state of a communication channel changes throughout program execution. As yet, however, there has not been a full-scale programming language with which to experiment with these ideas. Idris 2 is a new version of the dependently typed language Idris, with a new core language based on QTT, supporting linear and dependent types. In this paper, we introduce Idris 2, and describe how QTT has influenced its design. We give examples of the benefits of QTT in practice including: expressing which data is erased at run time, at the type level; and, resource tracking in the type system leading to type-safe concurrent programming with session types. 2012 ACM Subject Classification Software and its engineering → Functional languages', 'corpus_id': 232478760, 'score': 0}]"
6	"{'doc_id': '19024384', 'title': 'Surveillance of Dengue Fever Virus: A Review of Epidemiological Models and Early Warning Systems', 'abstract': ""Dengue fever affects over a 100 million people annually hence is one of the world's most important vector-borne diseases. The transmission area of this disease continues to expand due to many direct and indirect factors linked to urban sprawl, increased travel and global warming. Current preventative measures include mosquito control programs, yet due to the complex nature of the disease and the increased importation risk along with the lack of efficient prophylactic measures, successful disease control and elimination is not realistic in the foreseeable future. Epidemiological models attempt to predict future outbreaks using information on the risk factors of the disease. Through a systematic literature review, this paper aims at analyzing the different modeling methods and their outputs in terms of acting as an early warning system. We found that many previous studies have not sufficiently accounted for the spatio-temporal features of the disease in the modeling process. Yet with advances in technology, the ability to incorporate such information as well as the socio-environmental aspect allowed for its use as an early warning system, albeit limited geographically to a local scale."", 'corpus_id': 19024384}"	13848	[{'doc_id': '231621305', 'title': 'Malaria in the USA: How Vulnerable Are We to Future Outbreaks?', 'abstract': 'Purpose of Review Malaria poses a threat to nearly half of the world’s population, and recent literature in the USA is lacking regarding understanding risk for local outbreaks. This article aims to review Anopheles mosquito data, vector-borne disease outbreak preparedness, and human travel data from large international gateway cities in an effort to examine risk for localized outbreaks. Recent Findings The majority of vector control organizations are widely unprepared for a vector-borne disease outbreak, and multiple mosquito species capable of transmitting malaria continue to persist throughout the USA. Summary Despite the lack of recent autochthonous cases in the USA, multiple risk factors suggest that local malaria outbreaks in the USA will continue to pose a public health threat due to large numbers of international travelers from endemic areas, multiple Anopheles spp. capable of transmitting the parasite, and unsatisfactory vector-borne disease outbreak preparedness. Climate conditions and recent changes in travel patterns will influence malaria across the globe.', 'corpus_id': 231621305, 'score': 0}, {'doc_id': '230582776', 'title': 'Development and dissemination of infectious disease dynamic transmission models during the COVID-19 pandemic: what can we learn from other pathogens and how can we move forward?', 'abstract': '\n The current COVID-19 pandemic has resulted in the unprecedented development and integration of infectious disease dynamic transmission models into policy making and public health practice. Models offer a systematic way to investigate transmission dynamics and produce short-term and long-term predictions that explicitly integrate assumptions about biological, behavioural, and epidemiological processes that affect disease transmission, burden, and surveillance. Models have been valuable tools during the COVID-19 pandemic and other infectious disease outbreaks, able to generate possible trajectories of disease burden, evaluate the effectiveness of intervention strategies, and estimate key transmission variables. Particularly given the rapid pace of model development, evaluation, and integration with decision making in emergency situations, it is necessary to understand the benefits and pitfalls of transmission models. We review and highlight key aspects of the history of infectious disease dynamic models, the role of rigorous testing and evaluation, the integration with data, and the successful application of models to guide public health. Rather than being an expansive history of infectious disease models, this Review focuses on how the integration of modelling can continue to be advanced through policy and practice in appropriate and conscientious ways to support the current pandemic response.\n', 'corpus_id': 230582776, 'score': 1}, {'doc_id': '227070795', 'title': 'Climatic and socio-economic factors supporting the co-circulation of dengue, Zika and chikungunya in three different ecosystems in Colombia', 'abstract': 'Dengue, Zika and chikungunya are diseases of global health significance caused by arboviruses and transmitted by the mosquito Aedes aegypti of worldwide circulation. The arrival of the Zika and chikungunya viruses to South America increased the complexity of transmission and morbidity caused by these viruses co-circulating in the same vector mosquito species. Here we present an integrated analysis of the reported arbovirus cases between 2007 and 2017 and local climate and socio-economic profiles of three distinct Colombian municipalities (Bello, Cucuta and Moniquira). These locations were confirmed as three different ecosystems given their contrasted geographic, climatic and socio-economic profiles. Correlational analyses were conducted with both generalised linear models and generalised additive models for the geographical data. Average temperature and wind speed were strongly correlated with disease incidence. The transmission of Zika during the 2016 epidemic appeared to decrease circulation of dengue in Cucuta, an area of sustained high incidence of dengue. Socio-economic factors such as barriers to health and childhood services, inadequate sanitation and poor water supply suggested an unfavourable impact on the transmission of dengue, Zika and chikungunya in all three ecosystems. Socio-demographic influencers were also discussed including the influx of people to Cucuta, fleeing political and economic instability from neighbouring Venezuela. Aedes aegypti is expanding its range and increasing the global threat of these diseases. It is therefore vital that we learn from the epidemiology of these arboviruses and translate it into an actionable knowledge base. This is even more acute given the recent historical high of dengue cases in the Americas in 2019, preceding the COVID-19 pandemic, which is itself hampering mosquito control efforts.', 'corpus_id': 227070795, 'score': 1}, {'doc_id': '231760473', 'title': 'The value of infectious disease modeling and trend assessment: a public health perspective', 'abstract': 'ABSTRACT Introduction Disease outbreaks of acquired immunodeficiency syndrome, severe acute respiratory syndrome, pandemic H1N1, H7N9, H5N1, Ebola, Zika, Middle East respiratory syndrome, and recently COVID-19 have raised the attention of the public over the past half-century. Revealing the characteristics and epidemic trends are important parts of disease control. The biological scenarios including transmission characteristics can be constructed and translated into mathematical models, which can help to predict and gain a deeper understanding of diseases. Areas covered This review discusses the models for infectious diseases and highlights their values in the field of public health. This information will be of interest to mathematicians and clinicians, and make a significant contribution toward the development of more specific and effective models. Literature searches were performed using the online database of PubMed (inception to August 2020). Expert opinion Modeling could contribute to infectious disease control by means of predicting the scales of disease epidemics, indicating the characteristics of disease transmission, evaluating the effectiveness of interventions or policies, and warning or forecasting during the pre-outbreak of diseases. With the development of theories and the ability of calculations, infectious disease modeling would play a much more important role in disease prevention and control of public health.', 'corpus_id': 231760473, 'score': 1}, {'doc_id': '231303818', 'title': 'Weather Variability and COVID-19 Transmission: A Review of Recent Research', 'abstract': 'Weather and climate play a significant role in infectious disease transmission, through changes to transmission dynamics, host susceptibility and virus survival in the environment. Exploring the association of weather variables and COVID-19 transmission is vital in understanding the potential for seasonality and future outbreaks and developing early warning systems. Previous research examined the effects of weather on COVID-19, but the findings appeared inconsistent. This review aims to summarize the currently available literature on the association between weather and COVID-19 incidence and provide possible suggestions for developing weather-based early warning system for COVID-19 transmission. Studies eligible for inclusion used ecological methods to evaluate associations between weather (i.e., temperature, humidity, wind speed and rainfall) and COVID-19 transmission. The review showed that temperature was reported as significant in the greatest number of studies, with COVID-19 incidence increasing as temperature decreased and the highest incidence reported in the temperature range of 0–17 °C. Humidity was also significantly associated with COVID-19 incidence, though the reported results were mixed, with studies reporting positive and negative correlation. A significant interaction between humidity and temperature was also reported. Wind speed and rainfall results were not consistent across studies. Weather variables including temperature and humidity can contribute to increased transmission of COVID-19, particularly in winter conditions through increased host susceptibility and viability of the virus. While there is less indication of an association with wind speed and rainfall, these may contribute to behavioral changes that decrease exposure and risk of infection. Understanding the implications of associations with weather variables and seasonal variations for monitoring and control of future outbreaks is essential for early warning systems.', 'corpus_id': 231303818, 'score': 0}, {'doc_id': '230784497', 'title': 'Spatiotemporal Distribution of Zika Virus and Its Spatially Heterogeneous Relationship with the Environment', 'abstract': 'Infectious diseases have caused some of the most feared plagues and greatly harmed human health. However, despite the qualitative understanding that the occurrence and diffusion of infectious disease is related to the environment, the quantitative relations are unknown for many diseases. Zika virus (ZIKV) is a mosquito-borne virus that poses a fatal threat and has spread explosively throughout the world, impacting human health. From a geographical perspective, this study aims to understand the global hotspots of ZIKV as well as the spatially heterogeneous relationship between ZIKV and environmental factors using exploratory special data analysis (ESDA) model. A geographically weighted regression (GWR) model was used to analyze the influence of the dominant environmental factors on the spread of ZIKV at the continental scale. The results indicated that ZIKV transmission had obvious regional and seasonal heterogeneity. Population density, GDP per capita, and landscape fragmentation were the dominant environmental factors affecting the spread of ZIKV, which indicates that social factors had a greater influence than natural factors on the spread of it. As SARS-CoV-2 is spreading globally, this study can provide methodological reference for fighting against the pandemic.', 'corpus_id': 230784497, 'score': 1}, {'doc_id': '7447366', 'title': 'DVE Creation through MAS and ES: A Case Study', 'abstract': None, 'corpus_id': 7447366, 'score': 0}, {'doc_id': '231138663', 'title': 'COVID-19 epidemic prediction and the impact of public health interventions: A review of COVID-19 epidemic models', 'abstract': '\n The coronavirus disease outbreak of 2019 (COVID-19) has been spreading rapidly to all corners of the word, in a very complex manner. A key research focus is in predicting the development trend of COVID-19 scientifically through mathematical modelling. We conducted a systematic review of epidemic prediction models of COVID-19 and the public health intervention strategies by searching the Web of Science database. 55 studies of the COVID-19 epidemic model were reviewed systematically. It was found that the COVID-19 epidemic models were different in the model type, acquisition method, hypothesis and distribution of key input parameters. Most studies used the gamma distribution to describe the key time period of COVID-19 infection, and some studies used the lognormal distribution, the Erlang distribution, and the Weibull distribution. The setting ranges of the incubation period, serial interval, infectious period and generation time were 4.9–7 days, 4.41–8.4 days, 2.3–10 days and 4.4–7.5 days, respectively, and more than half of the incubation periods were set to 5.1 or 5.2 days. Most models assumed that the latent period was consistent with the incubation period. Some models assumed that asymptomatic infections were infectious or pre-symptomatic transmission was possible, which overestimated the value of R0. For the prediction differences under different public health strategies, the most significant effect was in travel restrictions. There were different studies on the impact of contact tracking and social isolation, but it was considered that improving the quarantine rate and reporting rate, and the use of protective face mask were essential for epidemic prevention and control. The input epidemiological parameters of the prediction models had significant differences in the prediction of the severity of the epidemic spread. Therefore, prevention and control institutions should be cautious when formulating public health strategies by based on the prediction results of mathematical models.\n', 'corpus_id': 231138663, 'score': 0}, {'doc_id': '229652718', 'title': 'Spatial Risk Assessment of Mosquito-Borne Viral Diseases – Research at the Intersection of Ecology and Epidemiology.', 'abstract': 'The arrival and rapid spread of the mosquito-borne viral disease Chikungunya across the Americas is one of the most significant public health developments of recent years, preceding and mirroring the subsequent spread of Zika. Globalization in trade and travel can lead to the importation of these viruses, but climatic conditions strongly affect the efficiency of transmission in local settings. In order to direct preparedness for future outbreaks, it is necessary to anticipate global regions that could become suitable for Chikungunya transmission. Here, we present global correlative niche models for autochthonous Chikungunya transmission. These models were used as the basis for projections under the representative concentration pathway (RCP) 4.5 and 8.5 climate change scenarios. In a further step, hazard maps, which account for population densities, were produced. The baseline models successfully delineate current areas of active Chikungunya transmission. Projections under the RCP 4.5 and 8.5 scenarios suggest the likelihood of expansion of transmission-suitable areas in many parts of the world, including China, sub-', 'corpus_id': 229652718, 'score': 1}, {'doc_id': '140207287', 'title': 'PGE2 suppresses NK activity in vivo directly and through adrenal hormones: Effects that cannot be reflected by ex vivo assessment of NK cytotoxicity', 'abstract': 'Surgery can suppress in vivo levels of NK cell cytotoxicity (NKCC) through various mechanisms, including catecholamine-, glucocorticoid (CORT)-, and prostaglandin (PG)-mediated responses. However, PGs are synthesized locally following tissue damage, driving proinflammatory and CORT responses, while their systemic levels are often unaffected. Thus, we herein studied the role of adrenal factors in mediating in vivo effects of PGs on NKCC, using adrenalectomized and sham-operated F344 rats subjected to surgery or PGE(2) administration. In vivo and ex vivo approaches were employed, based on intravenous administration of the NK-sensitive MADB106 tumor line, and based on ex vivo assessment of YAC-1 and MADB106 target-line lysis. Additionally, in vitro studies assessed the kinetics of the impact of epinephrine, CORT, and PGE(2) on NKCC. The results indicated that suppression of NKCC by epinephrine and PGE(2) are short lasting, and cannot be evident when these compounds are removed from the in vitro assay milieu, or in the context of ex vivo assessment of NKCC. In contrast, the effects of CORT are long-lasting and are reflected in both conditions even after its removal. Marginating-pulmonary NKCC was less susceptible to suppression than circulating NKCC, when tested against the xenogeneic YAC-1 target line, but not against the syngeneic MADB106 line, which seems to involve different cytotoxicity mechanisms. Overall, these findings indicate that elevated systemic PG levels can directly suppress NKCC in vivo, but following laparotomy adrenal hormones mediate most of the effects of endogenously-released PGs. Additionally, the ex vivo approach seems limited in reflecting the short-lasting NK-suppressive effects of catecholamines and PGs.', 'corpus_id': 140207287, 'score': 0}]
7	"{'doc_id': '52003073', 'title': 'Defining the Rhythmogenic Elements of Mammalian Breathing.', 'abstract': ""Breathing's remarkable ability to adapt to changes in metabolic, environmental, and behavioral demands stems from a complex integration of its rhythm-generating network within the wider nervous system. Yet, this integration complicates identification of its specific rhythmogenic elements. Based on principles learned from smaller rhythmic networks of invertebrates, we define criteria that identify rhythmogenic elements of the mammalian breathing network and discuss how they interact to produce robust, dynamic breathing."", 'corpus_id': 52003073}"	19197	"[{'doc_id': '235690151', 'title': 'Conservation of locomotion-induced oculomotor activity through evolution in higher tetrapods', 'abstract': 'Efference copies are neural replicas of motor outputs used to anticipate the sensory consequences of a self-generated motor action or to coordinate neural networks involved in distinct motor behaviors1. An established example of this motor-to-motor coupling is the efference copy of the propulsive motor command that supplements classical visuo-vestibular reflexes to ensure gaze stabilization during amphibian larval locomotion2. Such feedforward replica from spinal pattern-generating circuits produces a spino-extraocular motor coupled activity that evokes eye movements, spatio-temporally coordinated to tail undulation independently of any sensory signal3,4. Exploiting the evolutionary-development characteristic of the frog1, studies in metamorphing Xenopus demonstrated the persistence of this spino-extraocular motor command in adults, and its developmental adaptation to tetrapodal locomotion5,6. Here, we demonstrate for the first time the existence of a comparable locomotor-to-ocular motor coupling in the mouse. In neonates, ex vivo nerve recordings from brainstem-spinal cord preparation reveals a spino-extraocular motor coupled activity similar to the one described in Xenopus. In adult mice, trans-synaptic rabies injection in lateral rectus eye muscle labels cervical spinal cord neurons projecting directly to abducens motor neurons. Finally, treadmill-elicited locomotion in decerebrated preparations7 evokes rhythmic eye movements in synchrony with the limb gait pattern. Overall, our data are evidence for the conservation of locomotor-induced eye movements in higher tetrapods. Thus, in mammals as in amphibians, during locomotion CPG-efference copy feedforward signals might interact with sensory feedback to ensure efficient gaze control. Highlights Spino-extraocular motor coupling is evidenced from newborn mice ex vivo preparations Adult decerebrated mice exhibit conjugated rhythmic eye movements during treadmill locomotion Locomotor-induced oculomotor activity occurs in absence of visuo-vestibular inputs Conserved CPG-based efference copy signal in vertebrates with common features. eTOC blurb We report a functional coupling between spinal locomotor and oculomotor networks in the mouse, similar to the one previously described in Amphibians. This is the first evidence for the direct contribution of locomotor networks to gaze control in mammals, suggesting a conservation of the spino-extraocular coupling in higher tetrapods during sustained locomotion.', 'corpus_id': 235690151, 'score': 0}, {'doc_id': '235708536', 'title': 'Computational Modeling of Spinal Locomotor Circuitry in the Age of Molecular Genetics', 'abstract': 'Neuronal circuits in the spinal cord are essential for the control of locomotion. They integrate supraspinal commands and afferent feedback signals to produce coordinated rhythmic muscle activations necessary for stable locomotion. For several decades, computational modeling has complemented experimental studies by providing a mechanistic rationale for experimental observations and by deriving experimentally testable predictions. This symbiotic relationship between experimental and computational approaches has resulted in numerous fundamental insights. With recent advances in molecular and genetic methods, it has become possible to manipulate specific constituent elements of the spinal circuitry and relate them to locomotor behavior. This has led to computational modeling studies investigating mechanisms at the level of genetically defined neuronal populations and their interactions. We review literature on the spinal locomotor circuitry from a computational perspective. By reviewing examples leading up to and in the age of molecular genetics, we demonstrate the importance of computational modeling and its interactions with experiments. Moving forward, neuromechanical models with neuronal circuitry modeled at the level of genetically defined neuronal populations will be required to further unravel the mechanisms by which neuronal interactions lead to locomotor behavior.', 'corpus_id': 235708536, 'score': 0}, {'doc_id': '5171968', 'title': 'Respiratory rhythm generation: triple oscillator hypothesis', 'abstract': 'Breathing is vital for survival but also interesting from the perspective of rhythm generation. This rhythmic behavior is generated within the brainstem and is thought to emerge through the interaction between independent oscillatory neuronal networks. In mammals, breathing is composed of three phases – inspiration, post-inspiration, and active expiration – and this article discusses the concept that each phase is generated by anatomically distinct rhythm-generating networks: the preBötzinger complex (preBötC), the post-inspiratory complex (PiCo), and the lateral parafacial nucleus (pF L), respectively. The preBötC was first discovered 25 years ago and was shown to be both necessary and sufficient for the generation of inspiration. More recently, networks have been described that are responsible for post-inspiration and active expiration. Here, we attempt to collate the current knowledge and hypotheses regarding how respiratory rhythms are generated, the role that inhibition plays, and the interactions between the medullary networks. Our considerations may have implications for rhythm generation in general.', 'corpus_id': 5171968, 'score': 1}, {'doc_id': '235378252', 'title': 'Comodulation of h- and Na+/K+ Pump Currents Expands the Range of Functional Bursting in a Central Pattern Generator by Navigating between Dysfunctional Regimes', 'abstract': ""Central pattern generators (CPGs), specialized oscillatory neuronal networks controlling rhythmic motor behaviors such as breathing and locomotion, must adjust their patterns of activity to a variable environment and changing behavioral goals. Neuromodulation adjusts these patterns by orchestrating changes in multiple ionic currents. In the medicinal leech, the endogenous neuromodulator myomodulin speeds up the heartbeat CPG by reducing the electrogenic Na+/K+ pump current and increasing h-current in pairs of mutually inhibitory leech heart interneurons (HNs), which form half-center oscillators (HN HCOs). Here we investigate whether the comodulation of two currents could have advantages over a single current in the control of functional bursting patterns of a CPG. We use a conductance-based biophysical model of an HN HCO to explain the experimental effects of myomodulin. We demonstrate that, in the model, comodulation of the Na+/K+ pump current and h-current expands the range of functional bursting activity by avoiding transitions into nonfunctional regimes, such as asymmetric bursting and plateau-containing seizure-like activity. We validate the model by finding parameters that reproduce temporal bursting characteristics matching experimental recordings from HN HCOs under control, three different myomodulin concentrations, and Cs+ treated conditions. The matching cases are located along the border of an asymmetric regime away from the border with more dangerous seizure-like activity. We found a simple comodulation mechanism with an inverse relation between the pump and h-currents makes a good fit of the matching cases and comprises a general mechanism for the robust and flexible control of oscillatory neuronal networks. SIGNIFICANCE STATEMENT Rhythm-generating neuronal circuits adjust their oscillatory patterns to accommodate a changing environment through neuromodulation. In different species, chemical messengers participating in such processes may target two or more membrane currents. In medicinal leeches, the neuromodulator myomodulin speeds up the heartbeat central pattern generator by reducing Na+/K+ pump current and increasing h-current. In a computational model, we show that this comodulation expands the range of central pattern generator's functional activity by navigating the circuit between dysfunctional regimes resulting in a much wider range of cycle period. This control would not be attainable by modulating only one current, emphasizing the synergy of combined effects. Given the prevalence of h-current and Na+/K+ pump current in neurons, similar comodulation mechanisms may exist across species."", 'corpus_id': 235378252, 'score': 0}, {'doc_id': '39622908', 'title': 'Phylogenetic trends in respiratory rhythmogenesis: Insights from ectothermic vertebrates', 'abstract': 'Understanding the neural substrate driving breathing has puzzled physiologists for more than a century. The discovery of the pre-Bötzinger complex (preBötC) in newborn rodents as a structure with a unique physiological function in respiratory rhythm generation was an important progress in respiratory neurobiology that stimulated much research. Owing to the extensive literature describing the location, organisation, and function of the preBötC mainly in newborn rodents, this structure has become the point of reference in studies addressing respiratory rhythm generation in other mammals and various classes of vertebrates. This paper reviews recent progress made in non-mammalian vertebrates in our understanding of the location and function of the neural networks driving respiratory activity. As in newborn rodents, data from lampreys, air breathing fish, and amphibians show that the production of eupnea is the result of interactions between multiple (at least two) rhythmogenic networks. These networks are located in anatomically distinct areas and show different functional properties in terms of their ability to produce (or not) bursting activity in the absence of synaptic inputs (e.g. pacemaker neurons) and their sensitivity to specific neuromodulators such as substance P, somatostatin, and opioids. Current data indicate that respiratory rhythmogenesis is a phylogenetically ancient function that was highly conserved throughout evolution and that a comparative approach remains important to derive broader biological principles and a more comprehensive view.', 'corpus_id': 39622908, 'score': 1}, {'doc_id': '3519925', 'title': 'Microcircuits in respiratory rhythm generation: commonalities with other rhythm generating networks and evolutionary perspectives', 'abstract': 'Rhythmicity is critical for the generation of rhythmic behaviors and higher brain functions. This review discusses common mechanisms of rhythm generation, including the role of synaptic inhibition and excitation, with a focus on the mammalian respiratory network. This network generates three phases of breathing and is highly integrated with brain regions associated with numerous non-ventilatory behaviors. We hypothesize that during evolution multiple rhythmogenic microcircuits were recruited to accommodate the generation of each breathing phase. While these microcircuits relied primarily on excitatory mechanisms, synaptic inhibition became increasingly important to coordinate the different microcircuits and to integrate breathing into a rich behavioral repertoire that links breathing to sensory processing, arousal, and emotions as well as learning and memory.', 'corpus_id': 3519925, 'score': 1}, {'doc_id': '236165231', 'title': 'Astrocytic contribution to glutamate-related central respiratory chemoreception in vertebrates', 'abstract': 'Central respiratory chemoreceptors play a key role in the respiratory homeostasis by sensing CO2 and H+ in brain and activating the respiratory neural network. This ability of specific brain regions to respond to acidosis and hypercapnia is based on neuronal and glial mechanisms. Several decades ago, glutamatergic transmission was proposed to be involved as a main mechanism in central chemoreception. However, a complete identification of mechanism has been elusive. At the rostral medulla, chemosensitive neurons of the retrotrapezoid nucleus (RTN) are glutamatergic and they are stimulated by ATP released by RTN astrocytes in response to hypercapnia. In addition, recent findings show that caudal medullary astrocytes in brainstem can also contribute as CO2 and H+ sensors that release D-serine and glutamate, both gliotransmitters able to activate the respiratory neural network. In this review, we describe the mammalian astrocytic glutamatergic contribution to the central respiratory chemoreception trying to trace in vertebrates the emergence of several components involved in this process.', 'corpus_id': 236165231, 'score': 0}, {'doc_id': '6189549', 'title': 'Evolution of lung breathing from a lungless primitive vertebrate', 'abstract': 'Air breathing was critical to the terrestrial radiation and evolution of tetrapods and arose in fish. The vertebrate lung originated from a progenitor structure present in primitive boney fish. The origin of the neural substrates, which are sensitive to metabolically produced CO2 and which rhythmically activate respiratory muscles to match lung ventilation to metabolic demand, is enigmatic. We have found that a distinct periodic centrally generated rhythm, described as ""cough"" and occurring in lamprey in vivo and in vitro, is modulated by central sensitivity to CO2. This suggests that elements critical for the evolution of breathing in tetrapods, were present in the most basal vertebrate ancestors prior to the evolution of the lung. We propose that the evolution of breathing in all vertebrates occurred through exaptations derived from these critical basal elements.', 'corpus_id': 6189549, 'score': 1}, {'doc_id': '236209099', 'title': 'Modeling Post-Scratching Locomotion with Two Rhythm Generators and a Shared Pattern Formation', 'abstract': 'Simple Summary Post-scratching locomotion in cats refers to the spontaneous occurrence of an episode of locomotion generated after an event of scratching. This phenomenon suggests the potential existence of shared neurons in the spinal cord mediating the transition from one rhythmic motor task to another. Here, we examine this possibility with a mathematical model, reproducing the experimental observations. Our findings reveal a possible mechanism in which the central nervous system could share neuronal circuits from two central pattern generators to produce a sequence of different rhythmic motor actions. Abstract This study aimed to present a model of post-scratching locomotion with two intermixed central pattern generator (CPG) networks, one for scratching and another for locomotion. We hypothesized that the rhythm generator layers for each CPG are different, with the condition that both CPGs share their supraspinal circuits and their motor outputs at the level of their pattern formation networks. We show that the model reproduces the post-scratching locomotion latency of 6.2 ± 3.5 s, and the mean cycle durations for scratching and post-scratching locomotion of 0.3 ± 0.09 s and 1.7 ± 0.6 s, respectively, which were observed in a previous experimental study. Our findings show how the transition of two rhythmic movements could be mediated by information exchanged between their CPG circuits through routes converging in a common pattern formation layer. This integrated organization may provide flexible and effective connectivity despite the rigidity of the anatomical connections in the spinal cord circuitry.', 'corpus_id': 236209099, 'score': 0}, {'doc_id': '3267690', 'title': 'Phenotypic specification of hindbrain rhombomeres and the origins of rhythmic circuits in vertebrates.', 'abstract': 'This essay considers the ontogeny and phylogeny of the cranial neural circuitry producing rhythmic behaviors in vertebrates. These behaviors are characterized by predictable temporal patterns established by a neuronal network variously referred to as either a pacemaker, neural oscillator or central pattern generator. Comparative vertebrate studies have demonstrated that the embryonic hindbrain is divided into segmented compartments called rhombomeres, each of which gives rise to a distinct complement of cranial motoneurons and, as yet, unidentified populations of interneurons. We now propose that novel rhythmic circuits were innovations associated with the adoption of cardiac and respiratory pumps during the protochordate-vertebrate transition. We further suggest that the pattern-generating circuits of more recent innovations, such as the vocal, electromotor and extraocular systems, have originated from the same Hox gene-specified compartments of the embryonic hindbrain (rhombomeres 7-8) that gave rise to rhythmically active cardiac and respiratory circuits. Lastly, we propose that the capability for pattern generation by neurons originating from rhombomeres 7 and 8 is due to their electroresponsive properties producing pacemaker oscillations, as best typified by the inferior olive which also has origins from these same hindbrain compartments and has been suggested to establish rhythmic oscillations coupled to sensorimotor function throughout the neuraxis of vertebrates.', 'corpus_id': 3267690, 'score': 1}]"
8	{'doc_id': '13466955', 'title': '[Narcolepsy in horses].', 'abstract': 'Narcolepsy is an incurable non-progressive disease of the central nervous system. In humans, narcolepsy causes excessive drowsiness during the day (sometimes a sleep-attack occurs), cataplexy (sudden loss of muscle tone), hallucinations, and sleep paralysis. In the horse and other mammals cataplexy is the most frequently observed symptom. Excessive drowsiness can occur but is harder to observe. Cataplexy is caused by a fragmentation of the REM sleep. The etiology of narcolepsy is still subject to debate, partly because normal sleeping patterns are poorly understood. In humans and certain breeds of dogs a hereditary background has been demonstrated. In Shetland ponies the disease runs in certain families. The role of trauma and infection is the subject of debate. Cataplexy (which can be induced by physostigmine injection) confirms the diagnosis. Several drugs are available for the treatment of narcolepsy in humans. However there are a few data on the results of treatment of narcolepsy in the horse.', 'corpus_id': 13466955}	2681	[{'doc_id': '31292490', 'title': 'Narcolepsy in a horse.', 'abstract': None, 'corpus_id': 31292490, 'score': 1}, {'doc_id': '214786936', 'title': 'Dealing with sleep problems during home confinement due to the COVID‐19 outbreak: Practical recommendations from a task force of the European CBT‐I Academy', 'abstract': 'In the current global home confinement situation due to the COVID‐19 outbreak, most individuals are exposed to an unprecedented stressful situation of unknown duration. This may not only increase daytime stress, anxiety and depression levels, but also disrupt sleep. Importantly, because of the fundamental role that sleep plays in emotion regulation, sleep disturbance can have direct consequences upon next day emotional functioning. In this paper, we summarize what is known about the stress−sleep link and confinement as well as effective insomnia treatment. We discuss those effects of the current home confinement situation that can disrupt sleep but also those that could benefit sleep quality. We suggest adaptions of cognitive behavioural therapy elements that are feasible to implement for those facing changed work schedules and requirements, those with health anxiety and those handling childcare and home‐schooling, whilst also recognizing the general limitations imposed on physical exercise and social interaction. Managing sleep problems as best as possible during home confinement can limit stress and possibly prevent disruptions of social relationships.', 'corpus_id': 214786936, 'score': 0}, {'doc_id': '7730471', 'title': 'Narcolepsy in a horse.', 'abstract': None, 'corpus_id': 7730471, 'score': 1}, {'doc_id': '21743106', 'title': 'Seizures in horses: diagnosis and classification', 'abstract': 'Seizures are a diverse and very common set of chronic neurologic disorders in humans and dogs but are less common in horses. Seizures refer to a specific clinical event (described as sudden and severe) regardless of the etiology, which includes both intracranial and extracranial causes. Therefore, after briefly reviewing some definitions, this article aims to describe the use of a standardized classification, which could facilitate a logical approach for the clinician to establish a diagnosis, as well as to use a consistent mode of communication. For instance, seizures can be classified by type (ie, focal vs generalized) or etiology (ie, reactive, symptomatic, cryptogenic, idiopathic). In particular, epilepsy, a brain disorder characterized by recurrent seizures can be classified as primary (ie, genetic origin) or secondary (ie, acquired). This review further discusses the limitations associated with the clinical workup of horses with seizures. This is germane to the fact that the identification of the underlying cause remains challenging due to the technical limitations of imaging the equine adult brain. Indeed, as in man and dogs, epilepsies of unknown cause (ie, cryptogenic) account for the majority of all epilepsies. Therefore, although electroencephalography and advanced brain imaging techniques (eg, computed tomography and magnetic resonance imaging) are becoming increasingly available, information obtained from the history, physical, and neurologic examinations and progression of clinical signs and response to treatment remain essential in the workup of horses with seizures.', 'corpus_id': 21743106, 'score': 1}, {'doc_id': '7453081', 'title': 'Retrospective evaluation of episodic collapse in the horse in a referred population: 25 cases (1995-2009).', 'abstract': 'BACKGROUND\nEpisodic collapse in horses has equine welfare and human safety implications. There are, however, no published case series describing this syndrome.\n\n\nOBJECTIVES\nTo characterize the cause and outcomes for horses referred for investigation of episodic collapse.\n\n\nANIMALS\nTwenty-five horses referred for investigation of single or multiple episodes of collapse.\n\n\nMETHODS\nRetrospective study. Clinical records from the Dick Vet Equine Hospital, University of Edinburgh from November 1995 to July 2009 were searched using the following keywords: collapse, collapsing, fall, syncope. Collapse was defined as an incident in which the horse lost postural tone with or without progression to recumbency and with or without loss of consciousness. Long-term follow-up information was obtained by telephone conversation with the owner.\n\n\nRESULTS\nA final diagnosis was reached in 11 cases, namely cardiac arrhythmia (4), right-sided heart failure (1), hypoglycemia (2), generalized seizures (2), and sleep disorder (2). A presumptive diagnosis was reached in 8 cases, namely neurocardiogenic syncope (5), exercise-induced pulmonary hemorrhage (2), and generalized seizures (1). No diagnosis was reached in 6 cases despite comprehensive investigations. Three horses were euthanized at presentation. Treatment was attempted in 9 horses with 6 cases having successful outcome before discharge. Follow-up information was available for 14 of 19 horses discharged from the hospital. Only 1 of these horses was observed to collapse after discharge.\n\n\nCONCLUSIONS AND CLINICAL IMPORTANCE\nDefinitive diagnosis was more likely to be reached in cases with multiple episodes of collapse. Horses in which 1 episode of collapse occurred did not necessarily collapse again.', 'corpus_id': 7453081, 'score': 1}, {'doc_id': '31714223', 'title': 'Pharmacokinetics of imipramine in narcoleptic horses.', 'abstract': 'OBJECTIVE\nTo validate use of high-performance liquid chromatography (HPLC) in determining imipramine concentrations in equine serum and to determine pharmacokinetics of imipramine in narcoleptic horses.\n\n\nANIMALS\n5 horses with adult-onset narcolepsy.\n\n\nPROCEDURE\nBlood samples were collected before (time 0) and 3, 5, 10, 15, 20, 30, and 45 minutes and 1, 2, 3, 4, 6, 8, 12, and 24 hours after IV administration of imipramine hydrochloride (2 or 4 mg/kg of body weight). Serum was analyzed, using HPLC, to determine imipramine concentration. The serum concentration-versus-time curve for each horse was analyzed separately to estimate pharmacokinetic values.\n\n\nRESULTS\nAdverse effects (muscle fasciculations, tachycardia, hyperresponsiveness to sound, and hemolysis) were detected in most horses when serum imipramine concentrations were high, and these effects were most severe in horses receiving 4 mg of imipramine/kg. Residual adverse effects were not apparent. Value (mean +/- SD) for area under the curve was 3.9 +/- 0.7 h X microg/ml, whereas volume of distribution was 584 +/- 161.7 ml/kg, total body clearance was 522 +/- 102 ml/kg/h, and mean residence time was 1.8 +/- 0.6 hours. One horse had signs of narcolepsy 6 and 12 hours after imipramine administration; corrresponding serum imipramine concentrations were less than the therapeutic range.\n\n\nCONCLUSIONS AND CLINICAL RELEVANCE\nPotentially serious adverse effects may be seen in horses administered doses of imipramine that exceed a dosage of 2 mg/kg. Total body clearance of imipramine in horses is slower than that in humans; thus, the interval between subsequent doses should be longer in horses.', 'corpus_id': 31714223, 'score': 1}, {'doc_id': '215406516', 'title': '[COVID-19 andcardiovascular diseases].', 'abstract': 'COVID-2019 disease mainly affects the respiratory tract and can progress in severe cases to pneumonia, acute respiratory distress syndrome and multi-organ failure. Patients with prior cardiovascular disease are at higher risk of developing an infection and progressing to a severe form of the disease. Also, due to the growing number of infected cases, it is clear that, in addition to the typical respiratory symptoms caused by the infection, some patients suffer from cardiovascular damage. This condition can, in fact, cause significant myocardial damage, which worsens the disease and affects the prognosis. Based on the results of currently published research, it seems important to discuss the manifestations and characteristics of myocardial damage induced by COVID-19 and its impact on patient prognosis.', 'corpus_id': 215406516, 'score': 0}, {'doc_id': '212712971', 'title': 'coronavlrus n ec on : a paradigm for virus-induced demyel inat ing disease', 'abstract': 'C urrent hypotheses to explain the etiology and pathogenesis of demyelinating diseases in humans include the idea that an infectious agent encountered early in life may prime or trigger a disease process that manifests later in life as white-matter demyelinationL Although epidemiological evidence points to an infectious etiology, a single agent has never been linked convincingly with human diseases such as multiple sclerosis (MS), a chronic central nervous system A variety of neurological diseases in humans, including multiple sderosis (MS), have been postulated to have a viral etiology. The use of animal models provides insights into potential mechanism(s) involved in the disease process. The murine coronavirus-induced demyelinating disease in rodents is one such model for demyelinating disease in humans.', 'corpus_id': 212712971, 'score': 0}, {'doc_id': '210989030', 'title': 'Health problems in geriatric rats', 'abstract': 'Figure 1. Chromodacryorrhea in a rat. Porphyrin staining around the eye is a non-specific sign of stress that may indicate an underlying disease or a painful state, warranting further investigations. Image: Wendy Bament. They can be excellent children’s pets if handled from a young age and kept in social pairs or groups, and their natural inquisitiveness and dexterity make them fascinating pets that can respond well to training and human interaction. A survey showed approximately 100,000 rats are kept as pets in the UK (www.pfma.org.uk/pet-population-2014). Rodent owners are increasingly expecting high-quality veterinary care for these much-valued pet animals. The lifespan of a domestic rat is approximately two-and-a-half years to three years (Brown and Donnelly, 2012) and improved preventive and interventional health care is certainly leading to increasing numbers of geriatric pets. The aged rat is susceptible to various diseases, many of which are analogous to geriatric canine', 'corpus_id': 210989030, 'score': 0}, {'doc_id': '214730067', 'title': 'Effects of progressive muscle relaxation on anxiety and sleep quality in patients with COVID-19', 'abstract': '\n               Abstract\n               \n                  Background\n                  Patients with Coronavirus Disease 2019(COVID-19) will experience high levels of anxiety and low sleep quality due to isolation treatment. Some sleep-improving drugs may inhibit the respiratory system and worsen the condition. Prolonged bedside instruction may increase the risk of medical infections.\n               \n               \n                  Objective\n                  To investigate the effect of progressive muscle relaxation on anxiety and sleep quality of COVID-19.\n               \n               \n                  Methods\n                  In this randomized controlled clinical trial, a total of 51 patients who entered the isolation ward were included in the study and randomly divided into experimental and control groups. The experimental group used progressive muscle relaxation (PMR) technology for 30\xa0min per day for 5 consecutive days. During this period, the control group received only routine care and treatment. Before and after the intervention, the Spielberger State-Trait Anxiety Scale (STAI) and Sleep State Self-Rating Scale (SRSS) were used to measure and record patient anxiety and sleep quality. Finally, data analysis was performed using SPSS 25.0 software.\n               \n               \n                  Results\n                  The average anxiety score (STAI) before intervention was not statistically significant (P\xa0=\xa00.730), and the average anxiety score after intervention was statistically significant (P\xa0<\xa00.001). The average sleep quality score (SRSS) of the two groups before intervention was not statistically significant (P\xa0=\xa00.838), and it was statistically significant after intervention (P\xa0<\xa00.001).\n               \n               \n                  Conclusion\n                  Progressive muscle relaxation as an auxiliary method can reduce anxiety and improve sleep quality in patients with COVID-19.\n               \n            ', 'corpus_id': 214730067, 'score': 0}]
9	{'doc_id': '214803107', 'title': 'Light3DPose: Real-time Multi-Person 3D Pose Estimation from Multiple Views', 'abstract': 'We present an approach to perform 3D pose estimation of multiple people from a few calibrated camera views. Our architecture, leveraging the recently proposed unprojection layer, aggregates feature-maps from a 2D pose estimator backbone into a comprehensive representation of the 3D scene. Such intermediate representation is then elaborated by a fully-convolutional volumetric network and a decoding stage to extract 3D skeletons with sub-voxel accuracy. Our method achieves state of the art MPJPE on the CMU Panoptic dataset using a few unseen views and obtains competitive results even with a single input view. We also assess the transfer learning capabilities of the model by testing it against the publicly available Shelf dataset obtaining good performance metrics. The proposed method is inherently efficient: as a pure bottom-up approach, it is computationally independent of the number of people in the scene. Furthermore, even though the computational burden of the 2D part scales linearly with the number of input views, the overall architecture is able to exploit a very lightweight 2D backbone which is orders of magnitude faster than the volumetric counterpart, resulting in fast inference time. The system can run at 6 FPS, processing up to 10 camera views on a single 1080Ti GPU.', 'corpus_id': 214803107}	3926	"[{'doc_id': '214714313', 'title': 'MetaFuse: A Pre-trained Fusion Model for Human Pose Estimation', 'abstract': 'Cross view feature fusion is the key to address the occlusion problem in human pose estimation. The current fusion methods need to train a separate model for every pair of cameras making them difficult to scale. In this work, we introduce MetaFuse, a pre-trained fusion model learned from a large number of cameras in the Panoptic dataset. The model can be efficiently adapted or finetuned for a new pair of cameras using a small number of labeled images. The strong adaptation power of MetaFuse is due in large part to the proposed factorization of the original fusion model into two parts—(1) a generic fusion model shared by all cameras, and (2) lightweight camera-dependent transformations. Furthermore, the generic model is learned from many cameras by a meta-learning style algorithm to maximize its adaptation capability to various camera poses. We observe in experiments that MetaFuse finetuned on the public datasets outperforms the state-of-the-arts by a large margin which validates its value in practice.', 'corpus_id': 214714313, 'score': 1}, {'doc_id': '211572845', 'title': '4D Association Graph for Realtime Multi-Person Motion Capture Using Multiple Video Cameras', 'abstract': ""his paper contributes a novel realtime multi-person motion capture algorithm using multiview video inputs. Due to the heavy occlusions and closely interacting motions in each view, joint optimization on the multiview images and multiple temporal frames is indispensable, which brings up the essential challenge of realtime efficiency. To this end, for the first time, we unify per-view parsing, cross-view matching, and temporal tracking into a single optimization framework, i.e., a 4D association graph that each dimension (image space, viewpoint and time) can be treated equally and simultaneously. To solve the 4D association graph efficiently, we further contribute the idea of 4D limb bundle parsing based on heuristic searching, followed with limb bundle assembling by proposing a bundle Kruskal's algorithm. Our method enables a realtime motion capture system running at 30fps using 5 cameras on a 5-person scene. Benefiting from the unified parsing, matching and tracking constraints, our method is robust to noisy detection due to severe occlusions and close interacting motions, and achieves high-quality online pose reconstruction quality. The proposed method outperforms state-of-the-art methods quantitatively without using high-level appearance information."", 'corpus_id': 211572845, 'score': 1}, {'doc_id': '153312868', 'title': 'Learnable Triangulation of Human Pose', 'abstract': 'We present two novel solutions for multi-view 3D human pose estimation based on new learnable triangulation methods that combine 3D information from multiple 2D views. The first (baseline) solution is a basic differentiable algebraic triangulation with an addition of confidence weights estimated from the input images. The second, more complex, solution is based on volumetric aggregation of 2D feature maps from the 2D backbone followed by refinement via 3D convolutions that produce final 3D joint heatmaps. Crucially, both of the approaches are end-to-end differentiable, which allows us to directly optimize the target metric. We demonstrate transferability of the solutions across datasets and considerably improve the multi-view state of the art on the Human3.6M dataset.', 'corpus_id': 153312868, 'score': 1}, {'doc_id': '215745329', 'title': 'A Novel Pose Proposal Network and Refinement Pipeline for Better Object Pose Estimation', 'abstract': ""In this paper, we present a novel deep learning pipeline for 6D object pose estimation and refinement from RGB inputs. The first component of the pipeline leverages a region proposal framework to estimate multi-class single-shot 6D object poses directly from an RGB image and through a CNN-based encoder multi-decoders network. The second component, a multi-attentional pose refinement network (MARN), iteratively refines the estimated pose. MARN takes advantage of both visual and flow features to learn a relative transformation between an initially predicted pose and a target pose. MARN is further augmented by a spatial multi-attention block that emphasizes objects' discriminative feature parts. Experiments on three benchmarks for 6D pose estimation show that the proposed pipeline outperforms state-of-the-art RGB-based methods with competitive runtime performance."", 'corpus_id': 215745329, 'score': 0}, {'doc_id': '233864855', 'title': 'PoseAug: A Differentiable Pose Augmentation Framework for 3D Human Pose Estimation', 'abstract': 'Existing 3D human pose estimators suffer poor generalization performance to new datasets, largely due to the limited diversity of 2D-3D pose pairs in the training data. To address this problem, we present PoseAug, a new auto-augmentation framework that learns to augment the available training poses towards a greater diversity and thus improve generalization of the trained 2D-to-3D pose estimator. Specifically, PoseAug introduces a novel pose augmentor that learns to adjust various geometry factors (e.g., posture, body size, view point and position) of a pose through differentiable operations. With such differentiable capacity, the augmentor can be jointly optimized with the 3D pose estimator and take the estimation error as feedback to generate more diverse and harder poses in an online manner. Moreover, PoseAug introduces a novel part-aware Kinematic Chain Space for evaluating local joint-angle plausibility and develops a discriminative module accordingly to ensure the plausibility of the augmented poses. These elaborate designs enable PoseAug to generate more diverse yet plausible poses than existing offline augmentation methods, and thus yield better generalization of the pose estimator. PoseAug is generic and easy to be applied to various 3D pose estimators. Extensive experiments demonstrate that PoseAug brings clear improvements on both intra-scenario and cross-scenario datasets. Notably, it achieves 88.6% 3D PCK on MPI-INF-3DHP under cross-dataset evaluation setup, improving upon the previous best data augmentation based method by 9.1%. Code can be found at: this https URL.', 'corpus_id': 233864855, 'score': 0}, {'doc_id': '212747986', 'title': '3D Crowd Counting via Multi-View Fusion with 3D Gaussian Kernels', 'abstract': 'Crowd counting has been studied for decades and a lot of works have achieved good performance, especially the DNNs-based density map estimation methods. Most existing crowd counting works focus on single-view counting, while few works have studied multi-view counting for large and wide scenes, where multiple cameras are used. Recently, an end-to-end multi-view crowd counting method called multi-view multi-scale (MVMS) has been proposed, which fuses multiple camera views using a CNN to predict a 2D scene-level density map on the ground-plane. Unlike MVMS, we propose to solve the multi-view crowd counting task through 3D feature fusion with 3D scene-level density maps, instead of the 2D ground-plane ones. Compared to 2D fusion, the 3D fusion extracts more information of the people along z-dimension (height), which helps to solve the scale variations across multiple views. The 3D density maps still preserve the 2D density maps property that the sum is the count, while also providing 3D information about the crowd density. We also explore the projection consistency among the 3D prediction and the ground-truth in the 2D views to further enhance the counting performance. The proposed method is tested on 3 multi-view counting datasets and achieves better or comparable counting performance to the state-of-the-art.', 'corpus_id': 212747986, 'score': 0}, {'doc_id': '70350026', 'title': 'Self-Supervised Learning of 3D Human Pose Using Multi-View Geometry', 'abstract': 'Training accurate 3D human pose estimators requires large amount of 3D ground-truth data which is costly to collect. Various weakly or self supervised pose estimation methods have been proposed due to lack of 3D data. Nevertheless, these methods, in addition to 2D ground-truth poses, require either additional supervision in various forms (e.g. unpaired 3D ground truth data, a small subset of labels) or the camera parameters in multiview settings. To address these problems, we present EpipolarPose, a self-supervised learning method for 3D human pose estimation, which does not need any 3D ground-truth data or camera extrinsics. During training, EpipolarPose estimates 2D poses from multi-view images, and then, utilizes epipolar geometry to obtain a 3D pose and camera geometry which are subsequently used to train a 3D pose estimator. We demonstrate the effectiveness of our approach on standard benchmark datasets (i.e. Human3.6M and MPI-INF-3DHP) where we set the new state-of-the-art among weakly/self-supervised methods. Furthermore, we propose a new performance measure Pose Structure Score (PSS) which is a scale invariant, structure aware measure to evaluate the structural plausibility of a pose with respect to its ground truth. Code and pretrained models are available at https://github.com/mkocabas/EpipolarPose', 'corpus_id': 70350026, 'score': 1}, {'doc_id': '97283264', 'title': 'An Engineering Model for the Scale-Up and Design of Photocatalytic Reactors', 'abstract': 'Work is presented on the modeling and design of fixed-film photocatalytic reactors based on reaction kinetics data from a simple flat-plate photoreactor, local mass transfer rates and local radiation energy absorption rates on the surfaces of catalyst films, and an engineering model. The degradation of 4-chlorophenol in the flat plate reactor was examined experimentally for the purpose of model fitting, using fluorescent lamps as the illumination source. Using this information, the performance of several solar illuminated corrugated plate reactors with different structural parameters was calculated with the model.', 'corpus_id': 97283264, 'score': 0}, {'doc_id': '3144902', 'title': 'Dental Care with Manual Toothbrushes during Fixed Orthodontic Treatment—a New Testing Procedure', 'abstract': 'Aim:The aim of this investigation was to employ a new in-vitro testing system for manual toothbrushes in order to distinguish the more effective from those less so for dental care during fixed appliance treatment.Materials and Methods:The testing apparatus consisted of a sliding carriage able to execute a horizontal brushing movement, and a row of artificial teeth upon which the various toothbrushes were manipulated. The artificial row of teeth was fixed on a sensor that recorded in all three dimensions the forces and moments caused by the toothbrushes on the toothbrush field. All the tests were executed with a weight of 110 g on a tooth field with a multibracket appliance. Tests were also carried out with five toothbrushes having weights of 200 g, 250 g and 300 g. Here, the decisive target values were 1) the degree of exertion necessary in the brushing direction to move a brush over the artificial teeth, and 2) the maximum force occurring in the brushing direction. High target values indicated high interaction between toothbrush bristles and the surfaces being brushed.Results:From testing five toothbrushes with four different weights, we have established profiles confirming the beneficial and less beneficial properties of certain toothbrushes involving various high contact forces.ZusammenfassungZiel:Das Ziel dieser Untersuchung war, ein neues In-vitro-Testsystem für manuelle Zahnbürsten zu nutzen, um günstige von weniger günstigen Beborstungen für die Pflege von Zähnen mit Multibracketapparaturen zu unterscheiden.Material und Methoden:Die Testapparatur bestand aus einem Schlitten, der eine horizontale Bürstbewegung ausführen konnte, und einer künstlichen Zahnreihe, auf der die verschiedenen Zahnbürsten bewegt wurden. Die künstliche Zahnreihe war auf einem Sensor befestigt, der Kräfte und Drehmomente, die durch die Zahnbürsten auf dem Zahnbürstenfeld verursacht wurden, in allen Raumebenen aufzeichnete. Alle Tests wurden mit einem Auflagegewicht von 110 g auf einem Zahnfeld mit einer Multibracketapparatur durchgeführt. Mit fünf Bürsten erfolgten zusätzliche Tests mit Gewichten von 200 g, 250 g und 300 g. Die ausschlaggebenden Zielgrößen waren dabei 1) die Arbeit, die in Putzrichtung geleistet werden musste, um eine Bürste über die künstlichen Zähne zu bewegen, und 2) die maximal auftretenden Kräfte in Putzrichtung. Hohe Zielwerte lassen auf eine hohe Interaktion zwischen Zahnbürstenborsten und den zu reinigenden Flächen schließen.Ergebnisse:Aus den Tests der fünf Zahnbürsten mit vier unterschiedlichen Gewichten ließen sich Profile erstellen, die einer Zahnbürste günstige oder ungünstige Eigenschaften bei verschieden hohen Anpresskräften bescheinigen.', 'corpus_id': 3144902, 'score': 0}, {'doc_id': '212736938', 'title': 'Weakly-Supervised 3D Human Pose Learning via Multi-View Images in the Wild', 'abstract': 'One major challenge for monocular 3D human pose estimation in-the-wild is the acquisition of training data that contains unconstrained images annotated with accurate 3D poses. In this paper, we address this challenge by proposing a weakly-supervised approach that does not require 3D annotations and learns to estimate 3D poses from unlabeled multi-view data, which can be acquired easily in in-the-wild environments. We propose a novel end-to-end learning framework that enables weakly-supervised training using multi-view consistency. Since multi-view consistency is prone to degenerated solutions, we adopt a 2.5D pose representation and propose a novel objective function that can only be minimized when the predictions of the trained model are consistent and plausible across all camera views. We evaluate our proposed approach on two large scale datasets (Human3.6M and MPII-INF-3DHP) where it achieves state-of-the-art performance among semi-/weakly-supervised methods.', 'corpus_id': 212736938, 'score': 1}]"
10	{'doc_id': '14746303', 'title': 'Nematode Spatial and Ecological Patterns from Tropical and Temperate Rainforests', 'abstract': 'Large scale diversity patterns are well established for terrestrial macrobiota (e.g. plants and vertebrates), but not for microscopic organisms (e.g. nematodes). Due to small size, high abundance, and extensive dispersal, microbiota are assumed to exhibit cosmopolitan distributions with no biogeographical patterns. This assumption has been extrapolated from local spatial scale studies of a few taxonomic groups utilizing morphological approaches. Recent molecularly-based studies, however, suggest something quite opposite. Nematodes are the most abundant metazoans on earth, but their diversity patterns are largely unknown. We conducted a survey of nematode diversity within three vertical strata (soil, litter, and canopy) of rainforests at two contrasting latitudes in the North American meridian (temperate: the Olympic National Forest, WA, U.S.A and tropical: La Selva Biological Station, Costa Rica) using standardized sampling designs and sample processing protocols. To describe nematode diversity, we applied an ecometagenetic approach using 454 pyrosequencing. We observed that: 1) nematode communities were unique without even a single common species between the two rainforests, 2) nematode communities were unique among habitats in both rainforests, 3) total species richness was 300% more in the tropical than in the temperate rainforest, 4) 80% of the species in the temperate rainforest resided in the soil, whereas only 20% in the tropics, 5) more than 90% of identified species were novel. Overall, our data provided no support for cosmopolitanism at both local (habitats) and large (rainforests) spatial scales. In addition, our data indicated that biogeographical patterns typical of macrobiota also exist for microbiota.', 'corpus_id': 14746303}	16790	[{'doc_id': '73533380', 'title': 'Forest floor microarthropod abundance and oribatid mite (Acari: Oribatida) composition following partial and clear-cut harvesting in the mixedwood boreal forest', 'abstract': 'The effects of partial and clear-cut harvesting on abundance and community composition of forest floor microarthropods and oribatid mites were investigated in conifer and deciduous stands of the mi...', 'corpus_id': 73533380, 'score': 1}, {'doc_id': '233544031', 'title': 'Does litter decomposition affect mite communities (Acari, Mesostigmata)? A five-year litterbag experiment with 14 tree species in mixed forest stands growing on a post-industrial area', 'abstract': 'Abstract Decomposition and topsoil microclimate, mainly humidity and soil temperature, affects the availability of nutrients, as well as the edaphon structure, including soil mites, springtails, nematodes, insects and oligochaetes. Soil arthropod decomposers are the food base for predators in the soil trophic chains, including mesostigmatid (gamasid) mites. The aim of our study was to describe the succession of mesostigmatid assemblages on decomposed litter of 14 tree species in mixed stands growing on a reclaimed post-mining site. We hypothesized that litter species would significantly affect the gamasid abundance, species richness and diversity. Moreover, we hypothesized that mesostigmatid abundance, species richness and diversity would be higher in deciduous litter compared to pioneer Scots pine litter. Additionally, along with the decomposition process, the gamasid diversity, species richness and abundance would be significantly affected by soil temperature and differ among collection dates, with the lowest values at the beginning and the end, and with the highest values in the middle of the study period. In December 2011, 1389 litterbags (mesh size\xa0=\xa01\xa0mm) containing leaf litter (initial dry mass\xa0=\xa08.004–10.772\xa0g) were placed on research plots and collected after 3, 6, 9, 13, 19, 25, 31, 37, 43 and 58\xa0months after the experiment started. We determined the percentage litter mass loss for each sample and mean soil temperature on each research plot. Soil microarthropods were extracted from litterbag samples using Berlese-Tullgren funnels, and mesostigmatid mites were selected using a stereomicroscope. The mites were identified to genus and/or species level including developmental stages, using a microscope with high magnification and acarological keys. In total, 19,296 gamasid mites were selected and classified into 52 taxa. The dominant species were Trachytes aegrota (C.L. Koch) (49.9% of all gamasid mites found), Veigaia nemorensis (C.L. Koch) (8.9%) and Gamasellodes bicolor (Berlese) (7.7%). We found that time of litterbag sampling did not affect abundance, species richness and diversity of gamasid mites. Moreover, litter species affected species richness and diversity of gamasid mite assemblages, however, abundance was affected when calculated per sample, but not when calculated per dry litter mass. Additionally, we found that mean soil temperature and percentage litter mass loss also significantly affected abundance and species richness, however, the impact on diversity was insignificant. Our results may help to better understand of the importance of soil fauna that has a decisive impact on soil-forming processes on degraded areas. Results could also help to choose the right tree species to improve revitalization of degraded areas by creating better conditions for edaphic fauna, including species not directly involved in the decomposition process.', 'corpus_id': 233544031, 'score': 0}, {'doc_id': '233268296', 'title': 'Soil mites communities ( Acari : Oribatida , Mesostigmata ) in Kokorycz', 'abstract': 'Soil fauna is an important reservoir of biodiversity in forest ecosystems and plays an essential role in these ecosystems. The relationship between soil fauna groups from different trophic levels reflects well the conditions of the ecosystem, which is crucial especially for protected areas such as nature reserves. The aim of the study was to examine for the first time the soil mite (Oribatida and Mesostigmata) community (e.g. abundance, richness, and diversity) in the soil of the Kokorycz Nature Reserve located in the Lower Silesia Region.  In 2012, a  total of 50  soil  samples were collected using a  soil  corer  (10 cm2)  in  flood plain  forest  (40\xad120 y.o.), dominated by common oak and ash. Overall, 125 species (84 moss mites and 41 predators) were recorded in our study. The most abundant Oribatida were Conchogneta willmanni, Lauroppia fallax, Oppiella (O.) nova, and Rhinoppia subpectinata. Interestingly, ten oribatid  species were  recorded  from  the Lower  Silesia Region  for  the  first  time. Among Mesostigmata,  the most  common were Oodinychus ovalis, Paragamasus runcatellus, Paragamasus vagabundus, and Rhodacarus coronatus. Our study reported high soil fauna species diversity of this reserve and noted that the community is dominated by species typical for mature deciduous forests.', 'corpus_id': 233268296, 'score': 0}, {'doc_id': '54645094', 'title': 'Nested patterns of community assembly in the colonisation of artificial canopy habitats by oribatid mites', 'abstract': 'An observed species–area relationship (SAR) in assemblages of oribatid mites inhabiting natural canopy habitats (suspended soils) led to an experimental investigation of how patch size, height in canopy and moisture influence the species richness, abundance and community composition of arboreal oribatid mites. Colonisation by oribatid mites on 90 artificial canopy habitats (ACHs) of three sizes placed at each of three heights on the trunks of ten western redcedar trees was recorded over a 1-year period. Fifty-nine oribatid mite species colonised the ACHs, and richness increased with the moisture content and size of the habitat patch. Oribatid mite species richness and abundance, and ACH moisture content decreased with increasing ACH height in the canopy. Patterns in the species richness and community composition of ACHs were non-random and demonstrated a significant nested pattern. Correlations of patch size, canopy height and moisture content with community nestedness suggest that species-specific environmental tolerances combined with the differential dispersal abilities of species contributed to the non-random patterns of composition in these habitats. In line with the prediction that niche-selection filters out species from the regional pool that cannot tolerate environmental harshness, moisture-stressed ACHs in the high canopy had lower community variability than ACHs in the lower canopy. Colonising source pools to ACHs were almost exclusively naturally-occurring canopy sources, but low levels of colonisation from the forest floor were apparent at low heights within the ACH system. We conclude that stochastic dispersal dynamics within the canopy are crucial to understanding oribatid mite community structure in suspended soils, but that the relative importance of stochastic dispersal assembly may be dependent on a strong deterministic element to the environmental tolerances of individual species which drives non-random patterns of community assembly.', 'corpus_id': 54645094, 'score': 1}, {'doc_id': '83800931', 'title': 'Oribatid mite communities and foliar litter decomposition in canopy suspended soils and forest floor habitats of western redcedar forests, Vancouver Island, Canada', 'abstract': 'Litter decomposition and changes in oribatid mite community composition were studied for 2 years in litterbags collected from arboreal organic matter accumulations (canopy suspended soils) and forest floors associated with western redcedar trees on Vancouver Island, British Columbia. We tested the hypotheses that lower rates of mass loss, higher nutrient levels, and different patterns of oribatid mite richness and abundance in decomposing western redcedar litter would be observed in litterbags associated with canopy suspended soils compared to forest floors. Decomposition, measured by mass loss of cedar litter in litterbags, was not significantly different in canopy and forest floor habitats, although reduced in the canopy. Abundance and richness of oribatid mites inhabiting litterbags were significantly greater on the forest floor compared to the canopy suspended soils. Canopy suspended soils had higher levels of total nitrogen, available phosphorus and potassium than the forest floor, but moisture content was significantly lower in the suspended soils. Higher nutrient levels in the canopy system are attributed to differences in coarse woody debris input (but not foliar litter), combined with reduced nutrient uptake by roots and lower mobilisation rates of nutrients by detritivorous and fungivorous microarthropods. Moisture limitation in the canopy system possibly contributed to lower mass loss in litterbags, and lower abundance and richness of oribatid mites in litterbags placed on canopy suspended soils. Patterns of oribatid mite community composition were related to mite communities associated with the underlying substrate (forest floor or canopy suspended soil) which act as source pools for individuals colonising litterbags. Successional and seasonal trends in oribatid mite communities were confounded by moisture limitation at 24 months, particularly within the canopy habitat.', 'corpus_id': 83800931, 'score': 1}, {'doc_id': '233984096', 'title': 'Relative importance of tree species richness, tree functional type, and microenvironment for soil macrofauna communities in European forests.', 'abstract': 'Soil fauna communities are major drivers of many forest ecosystem processes. Tree species diversity and composition shape soil fauna communities, but their relationships are poorly understood, notably whether or not soil fauna diversity depends on tree species diversity. Here, we characterized soil macrofauna communities from forests composed of either one or three tree species, located in four different climate zones and growing on different soil types. Using multivariate analysis and model averaging we investigated the relative importance of tree species richness, tree functional type (deciduous vs. evergreen), litter quality, microhabitat and microclimatic characteristics as drivers of soil macrofauna community composition and structure. We found that macrofauna communities in mixed forest stands were represented by a higher number of broad taxonomic groups that were more diverse and more evenly represented. We also observed a switch from earthworm-dominated to predator-dominated communities with increasing evergreen proportion in forest stands, which we interpreted as a result of a lower litter quality and a higher forest floor mass. Finally, canopy openness was positively related to detritivore abundance and biomass, leading to higher predator species richness and diversity probably through trophic cascade effects. Interestingly, considering different levels of taxonomic resolution in the analyses highlighted different facets of macrofauna response to tree species richness, likely a result of both different ecological niche range and methodological constraints. Overall, our study supports the positive effects of tree species richness on macrofauna diversity and abundance through multiple changes in resource quality and availability, microhabitat, and microclimate modifications.', 'corpus_id': 233984096, 'score': 0}, {'doc_id': '233304528', 'title': 'Floristic Variation of Tree Communities In Island Forests of Pulau Tuba and Gunung Raya Forest Reserve, Langkawi', 'abstract': 'Island forests are among forest habitats that are vulnerable to natural and anthropogenic disturbances, whereby the disturbances would influence the survival of biological species of the ecosystems. Langkawi Archipelago contains many small island forests and rapid development of tourism industry within this archipelago might contribute impacts to the tree flora of the forest communities on the small islands. Hence, in this study the species richness and floristic variation pattern of tree communities of two selected island forests in the Langkawi Archipelago were explored, and data gathered are anticipated to be used for management of island forests in Langkawi. Tree survey was carried out in 10 study plots of 20m x 25m each, at island forests of Pulau Tuba Forest Reserve (PTB) and Gunung Raya Forest Reserve (GRFR), making the total of 20 study plots. All trees with diameter at breast height (dbh) of 5.0 cm and above were enumerated and tree species were identified. Species data were analyzed for diversity and richness using the Shannon and Margalef indices; whilst Detrended Correspondence Analysis (DCA) was used to determine floristic pattern. A total of 1062 trees were recorded from all study plots which comprised of 49 families, 134 genera and 213 tree species. The GRFR exhibited the highest species number of 135 tree species, followed by the PTB (106 tree species). Species accumulation curves showed that the curves were far from reaching the asymptote even when the whole dataset were combined. The DCA ordination diagram clearly grouped the study plots by their geological formation that indicated a gradient of species change in GRFR and PTB sites.', 'corpus_id': 233304528, 'score': 0}, {'doc_id': '234685719', 'title': 'Title: Communities of mites (Acari) in litter and soil under the invasive red oak (Quercus rubra L.) and native pedunculate oak (Q. robur L.) Author:', 'abstract': 'Because of thoughtless decisions or unintentional introduction, alien species disturb native ecosystems. red oak (Quercus rubra), among other alien woody plants, is still used to rehabilitate degraded land because of its better resistance to pollution and faster growth, as compared to native tree species. Soil mites, especially oribatida, are good bioindicators of ecosystem disturbance, so the main goal of this study was to explore the influence of invasive and native oaks on mite communities. forest stands dominated by 40-year-old Q. rubra or 35-year-old Q. robur were compared. over 2300 soil mites were extracted from 20 soil and 20 litter samples. mite densities in the communities were higher in red oak litter, which is probably a result of the thicker layer of shed leaves. changes in species composition of oribatid communities were observed in litter, in contrast to a lack of differences in soil. These observations are consistent with other researches on invasive woody plants. we expect that over time these changes will also be noticeable in the soil and will increase in litter.', 'corpus_id': 234685719, 'score': 0}, {'doc_id': '30957087', 'title': 'Spatial and environmental factors contributing to patterns in arboreal and terrestrial oribatid mite diversity across spatial scales', 'abstract': 'Understanding the conditions under which species traits, species–environment relationships, and the spatial structure of the landscape interact to shape local communities requires quantifying the relative contributions of space and the environment on community composition. Using analogous sampling of arboreal and terrestrial oribatid mite communities across a large spatial scale in a temperate rainforest, we quantified the variation in oribatid mite community structure relating to environmental and spatial factors, and tested whether terrestrial and arboreal communities demonstrated a difference in their patterns of community composition based on the assumption of differences in dispersal potential. The expectation that terrestrial oribatid mite communities are spatially structured while arboreal communities are environmentally structured was supported by our analyses at the level of variation in beta diversity, but not by assessing beta diversity itself. We found that terrestrial oribatid mite communities with active, cursorial dispersal demonstrate spatial constraint consistent with reduced long-distance dispersal opportunities and high environmental dissimilarity among sites. Arboreal communities, which potentially disperse long distances via passive aerial vectors, show a spatial signature associated with patterns in beta diversity and a correlation with environmental dissimilarities among sites. In the arboreal community, moisture content of the substrate, total tree height, and average sampled branch height were significant factors explaining beta diversity patterns. For ground-dwelling species, predator abundance and soil type were important local determinants of community variability. Both communities showed clear spatial structuring, suggesting that dispersal limitation continues to influence community composition across multiple forest watershed locations. Our results provide evidence of dispersal-maintained diversity patterns in response to local environmental factors in arboreal and terrestrial communities. The relative importance of stochastic dispersal assembly may be dependent on strong deterministic effects associated with micro-site and macro-site environmental variation, particularly across large spatial scales.', 'corpus_id': 30957087, 'score': 1}, {'doc_id': '84654135', 'title': 'A comparison of microarthropod assemblages with emphasis on oribatid mites in canopy suspended soils and forest floors associated with ancient western redcedar trees', 'abstract': 'Summary Microarthropod abundance, oribatid mite species richness and community composition were assessed in the high canopy (ca. 35\xa0m) of an ancient temperate rainforest and compared with microarthropod communities of the forest floor. Microarthropods were extracted from 72 core samples of suspended soils and 72 core samples from forest floors associated with six western redcedar trees in the Walbran Valley on the southwest coast of Vancouver Island, Canada. Total microarthropod abundances, mesostigmatid and astigmatid mites, Collembola and other microarthropod abundances were significantly greater in forest floors compared to canopy habitats. Oribatid and prostigmatid mite abundance were not significantly different between habitats. The relative abundances of all microarthropod groups considered in this study differed significantly between habitats. Eighty-eight species of oribatid mites were identified from the study area. Eighteen of the 53 species observed in suspended soils were unique to the canopy. Cluster analysis indicates that the arboreal oribatid mite community is distinct and not a taxonomic subset of the forest floor assemblage, however, canopy oribatid mite communities are more heterogeneous in species composition than in the forest floor.', 'corpus_id': 84654135, 'score': 1}]
11	"{'doc_id': '216562809', 'title': 'Optimizing AI for Teamwork', 'abstract': ""In many high-stakes domains such as criminal justice, finance, and healthcare, AI systems may recommend actions to a human expert responsible for final decisions, a context known as AI-advised decision making. When AI practitioners deploy the most accurate system in these domains, they implicitly assume that the system will function alone in the world. We argue that the most accurate AI team-mate is not necessarily the em best teammate; for example, predictable performance is worth a slight sacrifice in AI accuracy. So, we propose training AI systems in a human-centered manner and directly optimizing for team performance. We study this proposal for a specific type of human-AI team, where the human overseer chooses to accept the AI recommendation or solve the task themselves. To optimize the team performance we maximize the team's expected utility, expressed in terms of quality of the final decision, cost of verifying, and individual accuracies. Our experiments with linear and non-linear models on real-world, high-stakes datasets show that the improvements in utility while being small and varying across datasets and parameters (such as cost of mistake), are real and consistent with our definition of team utility. We discuss the shortcoming of current optimization approaches beyond well-studied loss functions such as log-loss, and encourage future work on human-centered optimization problems motivated by human-AI collaborations."", 'corpus_id': 216562809}"	5637	"[{'doc_id': '3415066', 'title': 'Predict Responsibly: Increasing Fairness by Learning To Defer', 'abstract': ""Machine learning systems, which are often used for high-stakes decisions, suffer from two mutually reinforcing problems: unfairness and opaqueness. Many popular models, although generally accurate, cannot express uncertainty about their predictions. Even in regimes where a model is inaccurate, users may trust the model's predictions too fully, and allow its biases to reinforce the user's own. \nIn this work, we explore models that learn to defer. In our scheme, a model learns to classify accurately and fairly, but also to defer if necessary, passing judgment to a downstream decision-maker such as a human user. We further propose a learning algorithm which accounts for potential biases held by decision-makers later in a pipeline. Experiments on real-world datasets demonstrate that learning to defer can make a model not only more accurate but also less biased. Even when operated by highly biased users, we show that deferring models can still greatly improve the fairness of the entire pipeline."", 'corpus_id': 3415066, 'score': 1}, {'doc_id': '75956106', 'title': 'Study on Interleukin-13 -1112 C>T gene Polymorphisms in Patients with Atopic Dermatitis in Comtonese Han Nationality', 'abstract': 'Objective To investigate the distribution of Interleukin-13 -1112 CT genes in a Guangdong population and to explore the association of these genes with atopic dermatitis. Methods One hundred and Seven unrelated healthy individuals of Cantonese Han nationality and 75 patients with atopic dermatitis who were from the same geographic region were enrolled in the study. The Interleukin-13 -1112 CT was genotyped by PCR-RFLP technique. Results There was no significant difference between IL-13 -1112 CT and CC genetypes. There was no significant difference in IL-13 -1112 between AD group and control group. Conclusion Polymorphisms of IL13-1112 CT were no associated with atopic dermatitis of Cantonese Han nationality.', 'corpus_id': 75956106, 'score': 0}, {'doc_id': '148206486', 'title': 'Бюджетирование, ориентированное на результат, и новое правовое положение государственных (муниципальных) предприятий', 'abstract': 'В статье рассматриваются положения Федерального закона от 8 мая 2010 года № 83-ФЗ «О внесении изменений в отдельные законодательные акты Российской Федерации в связи с совершенствованием правового положения государственных (муниципальных) учреждений», направленные на дальнейшее внедрение элементов программно-целевого финансирования в рамках перехода к принципам бюджетирования, ориентированного на результат. Анализируются возможные последствия реализации данного закона, а также обозначаются общие перспективные направления совершенствования финансирования государственных (муниципальных) учреждений.', 'corpus_id': 148206486, 'score': 0}, {'doc_id': '229210866', 'title': 'Learning Prediction Intervals for Model Performance', 'abstract': 'Understanding model performance on unlabeled data is a fundamental challenge of developing, deploying, and maintaining AI systems. Model performance is typically evaluated using test sets or periodic manual quality assessments, both of which require laborious manual data labeling. Automated performance prediction techniques aim to mitigate this burden, but potential inaccuracy and a lack of trust in their predictions has prevented their widespread adoption. We address this core problem of performance prediction uncertainty with a method to compute prediction intervals for model performance. Our methodology uses transfer learning to train an uncertainty model to estimate the uncertainty of model performance predictions. We evaluate our approach across a wide range of drift conditions and show substantial improvement over competitive baselines. We believe this result makes prediction intervals, and performance prediction in general, significantly more practical for real-world use.', 'corpus_id': 229210866, 'score': 0}, {'doc_id': '219260564', 'title': 'Consistent Estimators for Learning to Defer to an Expert', 'abstract': ""Learning algorithms are often used in conjunction with expert decision makers in practical scenarios, however this fact is largely ignored when designing these algorithms. In this paper we explore how to learn predictors that can either predict or choose to defer the decision to a downstream expert. Given only samples of the expert's decisions, we give a procedure based on learning a classifier and a rejector and analyze it theoretically. Our approach is based on a novel reduction to cost sensitive learning where we give a consistent surrogate loss for cost sensitive learning that generalizes the cross entropy loss. We show the effectiveness of our approach on a variety of experimental tasks."", 'corpus_id': 219260564, 'score': 1}, {'doc_id': '227334816', 'title': 'Learning Interpretable Concept-Based Models with Human Feedback', 'abstract': ""Machine learning models that first learn a representation of a domain in terms of human-understandable concepts, then use it to make predictions, have been proposed to facilitate interpretation and interaction with models trained on high-dimensional data. However these methods have important limitations: the way they define concepts are not inherently interpretable, and they assume that concept labels either exist for individual instances or can easily be acquired from users. These limitations are particularly acute for high-dimensional tabular features. We propose an approach for learning a set of transparent concept definitions in high-dimensional tabular data that relies on users labeling concept features instead of individual instances. Our method produces concepts that both align with users' intuitive sense of what a concept means, and facilitate prediction of the downstream label by a transparent machine learning model. This ensures that the full model is transparent and intuitive, and as predictive as possible given this constraint. We demonstrate with simulated user feedback on real prediction problems, including one in a clinical domain, that this kind of direct feedback is much more efficient at learning solutions that align with ground truth concept definitions than alternative transparent approaches that rely on labeling instances or other existing interaction mechanisms, while maintaining similar predictive performance."", 'corpus_id': 227334816, 'score': 0}, {'doc_id': '8943607', 'title': 'Principles of mixed-initiative user interfaces', 'abstract': 'Recent debate has centered on the relative promise of focusinguser-interface research on developing new metaphors and tools thatenhance users abilities to directly manipulate objects versusdirecting effort toward developing interface agents that provideautomation. In this paper, we review principles that show promisefor allowing engineers to enhance human-computer interactionthrough an elegant coupling of automated services with directmanipulation. Key ideas will be highlighted in terms of the Lookoutsystem for scheduling and meeting management.', 'corpus_id': 8943607, 'score': 1}, {'doc_id': '227344215', 'title': 'Bringing Cognitive Augmentation to Web Browsing Accessibility', 'abstract': 'In this paper we explore the opportunities brought by cognitive augmentation to provide a more natural and accessible web browsing experience. We explore these opportunities through \\textit{conversational web browsing}, an emerging interaction paradigm for the Web that enables blind and visually impaired users (BVIP), as well as regular users, to access the contents and features of websites through conversational agents. Informed by the literature, our previous work and prototyping exercises, we derive a conceptual framework for supporting BVIP conversational web browsing needs, to then focus on the challenges of automatically providing this support, describing our early work and prototype that leverage heuristics that consider structural and content features only.', 'corpus_id': 227344215, 'score': 0}, {'doc_id': '218486980', 'title': 'Learning to Complement Humans', 'abstract': 'A rising vision for AI in the open world centers on the development of systems that can complement humans for perceptual, diagnostic, and reasoning tasks. To date, systems aimed at complementing the skills of people have employed models trained to be as accurate as possible in isolation. We demonstrate how an end-to-end learning strategy can be harnessed to optimize the combined performance of human-machine teams by considering the distinct abilities of people and machines. The goal is to focus machine learning on problem instances that are difficult for humans, while recognizing instances that are difficult for the machine and seeking human input on them. We demonstrate in two real-world domains (scientific discovery and medical diagnosis) that human-machine teams built via these methods outperform the individual performance of machines and people. We then analyze conditions under which this complementarity is strongest, and which training methods amplify it. Taken together, our work provides the first systematic investigation of how machine learning systems can be trained to complement human reasoning.', 'corpus_id': 218486980, 'score': 1}, {'doc_id': '231698316', 'title': 'Leveraging Expert Consistency to Improve Algorithmic Decision Support', 'abstract': 'Due to their promise of superior predictive power relative to human assessment, machine learning models are increasingly being used to support high-stakes decisions. However, the nature of the labels available for training these models often hampers the usefulness of predictive models for decision support. In this paper, we explore the use of historical expert decisions as a rich–yet imperfect–source of information, and we show that it can be leveraged to mitigate some of the limitations of learning from observed labels alone. We consider the problem of estimating expert consistency indirectly when each case in the data is assessed by a single expert, and propose influence functions based methodology as a solution to this problem. We then incorporate the estimated expert consistency into the predictive model meant for decision support through an approach we term label amalgamation. This allows the machine learning models to learn from experts in instances where there is expert consistency, and learn from the observed labels elsewhere. We show how the proposed approach can help mitigate common challenges of learning from observed labels alone, reducing the gap between the construct that the algorithm optimizes for and the construct of interest to experts. After providing intuition and theoretical results, we present empirical results in the context of child maltreatment hotline screenings. Here, we find that (1) there are high-risk cases whose risk is considered by the experts but not wholly captured in the target labels used to train a deployed model, and (2) the proposed approach improves recall for these cases.', 'corpus_id': 231698316, 'score': 1}]"
12	{'doc_id': '170078913', 'title': 'What Can Neural Networks Reason About?', 'abstract': 'Neural networks have succeeded in many reasoning tasks. Empirically, these tasks require specialized network structures, e.g., Graph Neural Networks (GNNs) perform well on many such tasks, but less structured networks fail. Theoretically, there is limited understanding of why and when a network structure generalizes better than others, although they have equal expressive power. In this paper, we develop a framework to characterize which reasoning tasks a network can learn well, by studying how well its computation structure aligns with the algorithmic structure of the relevant reasoning process. We formally define this algorithmic alignment and derive a sample complexity bound that decreases with better alignment. This framework offers an explanation for the empirical success of popular reasoning models, and suggests their limitations. As an example, we unify seemingly different reasoning tasks, such as intuitive physics, visual question answering, and shortest paths, via the lens of a powerful algorithmic paradigm, dynamic programming (DP). We show that GNNs align with DP and thus are expected to solve these tasks. On several reasoning tasks, our theory is supported by empirical results.', 'corpus_id': 170078913}	20653	"[{'doc_id': '237532584', 'title': 'Deep Algorithmic Question Answering: Towards a Compositionally Hybrid AI for Algorithmic Reasoning', 'abstract': 'An important aspect of artificial intelligence (AI) is the ability to reason in a step-by-step “algorithmic” manner that can be inspected and verified for its correctness. This is especially important in the domain of question answering (QA). We argue that the challenge of algorithmic reasoning in QA can be effectively tackled with a “systems” approach to AI which features a hybrid use of symbolic and sub-symbolic methods including deep neural networks. Additionally, we argue that while neural network models with end-to-end training pipelines perform well in narrow applications such as image classification and language modelling, they cannot, on their own, successfully perform algorithmic reasoning, especially if the task spans multiple domains. We discuss a few notable exceptions and point out how they are still limited when the QA problem is widened to include other intelligence-requiring tasks. However, deep learning, and machine learning in general, do play important roles as components in the reasoning process. We propose an approach to algorithm reasoning for QA, Deep Algorithmic Question Answering (DAQA), based on three desirable properties: interpretability, generalizability and robustness which such an AI system should possess and conclude that they are best achieved with a combination of hybrid and compositional AI.', 'corpus_id': 237532584, 'score': 1}, {'doc_id': '237100535', 'title': 'Compositional Neural Logic Programming', 'abstract': 'This paper introduces Compositional Neural Logic Programming (CNLP), a framework that integrates neural networks and logic programming for symbolic and sub-symbolic reasoning. We adopt the idea of compositional neural networks to represent first-order logic predicates and rules. A voting backward-forward chaining algorithm is proposed for inference with both symbolic and sub-symbolic variables in an argument-retrieval style. The framework is highly flexible in that it can be constructed incrementally with new knowledge, and it also supports batch reasoning in certain cases. In the experiments, we demonstrate the advantages of CNLP in discriminative tasks and generative tasks.', 'corpus_id': 237100535, 'score': 1}, {'doc_id': '236981984', 'title': 'Review of Algorithms for Artificial Intelligence on Low Memory Devices', 'abstract': 'The aim of the article is to conceptualise a more compact and efficient version of algorithms for artificial intelligence (AI). The core objective is to construct the design for a self-optimising and self-adapting autonomous artificial intelligence (AutoAI) that can be applied for edge analytics using real-time data. The methodology is based on synthesising existing knowledge on AI (i.e., knowledge modelling, symbolic reasoning, modal logic), with novel concepts from neuromorphic engineering in combination with deep learning algorithms (i.e., reinforcement learning, neural networks, evolutionary algorithms) and data science (i.e., statistics, linear regression, Bayesian methods). Far-reaching implications are expected from the unique integration of approaches in neuromorphic engineering and edge analytics.', 'corpus_id': 236981984, 'score': 0}, {'doc_id': '237430720', 'title': 'Relational Graph Reasoning for Knowledge-Augmented Question Answering', 'abstract': 'Pretrained language models have been widely used in various Natural Language Processing (NLP) tasks and achieved remarkable success. However, there are two shortcomings of such methods: (1) while language models do well in encoding word sequence based on its semantic meanings, it can’t introduce knowledge from other sources, which limits its performance on knowledge-guided NLP tasks; (2) language models understand semantics based on co-occurrence in the training corpus, which makes it hard to do complex reasoning. In this paper, we focus on the question answering task where external knowledge is necessary for both understanding the context and identifying the correct answer. Inspired by Relation Network (Santoro et al., 2017), we propose a framework to incorporate relevant facts from knowledge graph and do reasoning. Experiments on CommonsenseQA dataset demonstrate the effectiveness of our method and the value of external knowledge.', 'corpus_id': 237430720, 'score': 0}, {'doc_id': '233864602', 'title': 'Neural algorithmic reasoning', 'abstract': 'We present neural algorithmic reasoning—the art of building neural networks that are able to execute algorithmic computation—and provide our opinion on its transformative potential for running classical algorithms on inputs previously considered inaccessible to them.', 'corpus_id': 233864602, 'score': 1}, {'doc_id': '236980370', 'title': 'From Deep Learning to Deep Reasoning', 'abstract': 'The rise of big data and big compute has brought modern neural networks to many walks of digital life, thanks to the relative ease of constructing large models that scale to the real world. Current successes of Transformers and self-supervised pretraining on massive data have led some to believe that deep neural networks will be able to do almost everything once we have sufficient data and computational resources. However, neural networks are fast to exploit surface statistics but fail miserably to generalize to novel combinations. This is because they are not designed for deliberate reasoning -- the capacity to deliberately deduce new knowledge out of the contextualized data. This tutorial reviews recent developments to extend the capacity of neural networks to ""learning-to-reason\'\' from data, where the task is to determine if the data entails a conclusion. This capacity opens up new ways to generate insights from data through arbitrary compositional querying without the need of predefining a narrow set of tasks. The tutorial consists of four parts. The first part covers the learning-to-reason framework, and explains how neural networks can serve as a strong backbone for reasoning through its natural operations such as binding, attention & dynamic computational graphs. The second part goes into more detail on how neural networks perform reasoning over unstructured and structured data, and across modalities. The third part reviews neural memories and their role in reasoning. The last part discusses generalization to novel combinations, under less supervision and with more knowledge.', 'corpus_id': 236980370, 'score': 1}, {'doc_id': '236956827', 'title': 'Knowledge accumulating: The general pattern of learning', 'abstract': 'Artificial Intelligence has been developed for decades with the achievement of great progress. Recently, deep learning shows its ability to solve many real world problems, e.g. image classification and detection, natural language processing, playing GO. Theoretically speaking, an artificial neural network can fit any function and reinforcement learning can learn from any delayed reward. But in solving real world tasks, we still need to spend a lot of effort to adjust algorithms to fit task unique features. This paper proposes that the reason of this phenomenon is the sparse feedback feature of the nature, and a single algorithm, no matter how we improve it, can only solve dense feedback tasks or specific sparse feedback tasks. This paper first analyses how sparse feedback affects algorithm perfomance, and then proposes a pattern that explains how to accumulate knowledge to solve sparse feedback problems.', 'corpus_id': 236956827, 'score': 1}, {'doc_id': '237431232', 'title': 'Advances in QA Models II: Machine Reading Comprehension and Multi-hop Reasoning', 'abstract': 'Machine reading comprehension (MRC) answers a query about a given context, which usually requires modeling complex interactions between the context and the query. Some harder questions require explicit modeling of multi-hop reasoning process. In this lecture, we will discuss advances in machine reading comprehension and multi-hop reasoning.', 'corpus_id': 237431232, 'score': 0}, {'doc_id': '237433348', 'title': 'Commonsense Question Answering: A Survey', 'abstract': 'When humans use their languages to communicate with each other, they often rely on broad implicit assumptions. Humans learn and use this kind of assumptions in everyday life, which makes their language concise without lacking precision. However, machines by nature don’t have such background knowledge. Machine learning models can’t accumulate human’s commonsense through interacting with the environment. Therefore, empowering Natural Language Processing (NLP) techniques with commonsense knowledge is one of the major long-term goals for Artificial Intelligence (AI). Question Answering (QA) is a Natural Language Understanding (NLU) task requiring both language processing and knowledge reasoning. When commonsense knowledge outside the given text is needed to answer the question, the task is called Commonsense Question Answering. Therefore, the main focus of the commonsense question answering task is how to incorporate commonsense knowledge and conduct reasoning.', 'corpus_id': 237433348, 'score': 0}, {'doc_id': '237012256', 'title': 'TERN OF LEARNING', 'abstract': 'Artificial Intelligence has been developed for decades with the achievement of great progress. Recently, deep learning shows its ability to solve many real world problems, e.g. image classification and detection, natural language processing, playing GO. Theoretically speaking, an artificial neural network can fit any function and reinforcement learning can learn from any delayed reward. But in solving real world tasks, we still need to spend a lot of effort to adjust algorithms to fit task unique features. This paper proposes that the reason of this phenomenon is the sparse feedback feature of the nature, and a single algorithm, no matter how we improve it, can only solve dense feedback tasks or specific sparse feedback tasks. This paper first analyses how sparse feedback affects algorithm perfomance, and then proposes a pattern that explains how to accumulate knowledge to solve sparse feedback problems.', 'corpus_id': 237012256, 'score': 0}]"
13	{'doc_id': '198736387', 'title': 'Animal abuse: A close relationship with domestic violence', 'abstract': 'This article aims to address domestic violence and its relation to animal abuse, and to propose alternative solutions. A close relation has been found between domestic violence and animal maltreatment. It is verified that the majority of the aggressors belong to the masculine gender and the most effective way to break the cycle of abuse is education, with the consequent awareness of respect for life in all its forms.', 'corpus_id': 198736387}	16744	"[{'doc_id': '55035394', 'title': 'Pets in danger: exploring the link between domestic violence and animal abuse', 'abstract': ""Abstract Previous research has found that domestic violence (DV) victims who seek refuge in DV shelters often report the abuse of companion animals as a form of psychological control. However, these studies have mainly involved the use of interviews and questionnaires which restrict the quality and depth of data collected (e.g. these methods increase the probability that victims will withhold information due to embarrassment or ethical constraints). The current study utilized a novel method previously overlooked in the literature on companion animal abuse in an attempt to overcome these problems; domestic violence victims' stories of companion animal abuse were obtained from online forums where victims voluntarily shared their experiences. Seventy-four stories were analyzed using thematic analysis and four key themes were identified: The Victim - Companion Animal Bond ; Companion Animals Used to Control Victims ; Victims ' Perceptions of Abusers ' Behavior ; and Support for Victims and Companion Animals . A number of DV victims reported that companion animals were one of their main sources of support, and many chose to stay in an abusive relationship because DV shelters did not have the facilities to house their pets. Findings have policy implications for police, DV shelters, child protection organizations, and animal welfare organizations."", 'corpus_id': 55035394, 'score': 1}, {'doc_id': '233705472', 'title': 'Domestic violence and animal abuse', 'abstract': 'The renaissance of interest in the links between animal abuse and other forms of family violence has been accepted widely by the field of animal care and control and, to a lesser degree, by domestic violence prevention and child protection. The growing interest in the ""link"" is not meant to imply that animals are more important than people. It does imply, however, that no forms of family violence should be tolerated and that when any member of the family is abused, others are at risk. A coordinated, multi-disciplinary approach shows great promise in helping remove significant obstacles that prevent battered women from leaving abusive relationships.', 'corpus_id': 233705472, 'score': 1}, {'doc_id': '157722086', 'title': 'Awareness Can Change a Society: The Link Between Animal Abuse and Domestic Violence in the Netherlands', 'abstract': 'There is a growing awareness of the relationship between animal abuse and domestic violence. In the Netherlands, the topic was unknown until research in 2009 (Enders-Slegers & Jansen) revealed that 60 % of the interviewed veterinarians for pet animals (N = 108) in the Netherlands noticed animal abuse in their practices. In one-third of the cases they supposed or were sure that other forms of violence occurred in the family as well (child abuse, partner abuse). A second research project (Garnier and Enders-Slegers, Huiselijk geweld en dierenmishandeling in Nederland. Rapport Kadera aanpak huiselijk geweld. Retrieved September 10, 2015, from http://www.kadera.nl, 2012) with female pet owners in the Dutch general population (N = 111) and with female pet owners in women’s shelters (N = 51) affirmed these findings. The results show that animal cruelty occurs significantly more often among battered women compared to women in the general population. One-third of the women reported that the partner (or ex-partner) threatened to hurt the pet and 55 % reported that the partner had hurt or killed the pet. The research findings of both studies are discussed in this chapter, as are the social developments following upon the growing awareness in Dutch society of this relationship of violence. Such developments include, for instance, the formal obligation of veterinarians to report animal abuse, the development of foster care for animals from violent families and animal police, and the embedding of “animal abuse” in risk assessment questionnaires.', 'corpus_id': 157722086, 'score': 1}, {'doc_id': '46767568', 'title': 'Intimate Partner Violence and Animal Abuse in an Immigrant-Rich Sample of Mother–Child Dyads Recruited From Domestic Violence Programs', 'abstract': 'We examined rates of animal abuse in pet-owning families experiencing intimate partner violence (IPV). We also examined whether higher levels of IPV (as measured by subscales from the Conflict Tactics Scales) predicted increased risk for partner-perpetrated animal abuse. Our sample included 291 mother–child dyads, where the mothers sought services from domestic violence agencies. Nearly half the sample is comprised of Mexican immigrants. Mothers reported that 11.7% of partners threatened to harm a pet and 26.1% actually harmed a pet, the latter of which represents a lower rate than in similar studies. When examining animal abuse by “Hispanic status,” follow-up analyses revealed significant omnibus differences between groups, in that non-Hispanic U.S.-born partners (mostly White) displayed higher rates of harming pets (41%) than either U.S.-born or Mexican-born Hispanic groups (27% and 12.5%, respectively). Differences in rates for only threatening (but not harming) pets were not significant, possibly due to a small number of partners (n = 32) in this group. When examining whether partners’ IPV predicted only threatening to harm pets, no IPV subscale variables (Physical Assault, Psychological Aggression, Injury, or Sexual Coercion) were significant after controlling for income, education, and Hispanic status. When examining actual harm to pets, more Psychological Aggression and less Physical Assault significantly predicted slightly higher risk of harm. However, Mexican-born partners had nearly 4 times lower risk of harming a pet. Overall, these results suggest that Hispanic men who are perpetrators of IPV are less likely to harm pets than non-Hispanic perpetrators of IPV, particularly if Mexican-born. Considering that the United States has a significant proportion of Mexican immigrants, it may be worthwhile to explore the topics of IPV and animal abuse within this group.', 'corpus_id': 46767568, 'score': 1}, {'doc_id': '232323377', 'title': 'Firearm-related Abuse and Protective Order Requests Among Intimate Partner Violence Victims.', 'abstract': ""Firearms play a critical role in the murder of intimate partner violence (IPV) victims and there is evidence that laws prohibiting protective order (PO) respondents from possessing a firearm reduce IPV fatalities. However, little research has compared specific abuse tactics involving firearms among victims who have and have not sought a PO against an abuser. This study investigates IPV victims' experiences with a range of firearm-related abuse tactics across victim race/ethnicity, in addition to the relationship between firearm IPV and PO requests, above and beyond IPV not involving firearms. Questionnaires were administered to 215 female victims recruited from six domestic violence shelters in Texas. Over one-half of victims who sought a PO were threatened to be shot by their abuser and victims who experienced high levels of firearm abuse incurred a 302% increase in the odds of requesting a PO. There were no significant differences between White, Black, and Hispanic victims regarding firearm IPV tactics. The results shed light on the magnitude of risk IPV victims can experience when seeking a PO against an abusive partner."", 'corpus_id': 232323377, 'score': 0}, {'doc_id': '233451946', 'title': 'Effects of Abuse on Female Offenders', 'abstract': 'Between 1995 and 2005, the number of female offenders increased significantly. However, studies show that most female offenders do not commit violent crimes. Researchers have established that women that have experienced some form of abuse causes them to offend. Although women do not commit violent crimes, they still receive severe punishments. Incarceration is not a solution for reform and courts should consider the effects of abuse on female offenders. This paper illustrates how the effects of abuse correlates with female offenders, describes the effects of abuse on male offenders and how it relates to female offenders, and provides additional risk factors that can lead to a woman’s pathway towards criminality. Additionally, this paper will provide policy implications for women offenders that have experienced abuse in their lifetime.', 'corpus_id': 233451946, 'score': 0}, {'doc_id': '49230000', 'title': 'The Perpetration of Adulthood Animal Abuse and Intimate Partner Violence in Men and Women Arrested for Domestic Violence', 'abstract': 'Intimate partner violence (IPV) occurs at devastatingly high rates in the United States. The current interventions for perpetrators of IPV are limited in their effectiveness. Research regarding characteristics of perpetrators of IPV may provide needed insights about their aggression in order to inform more effective treatments. This cross-sectional study employed the newly developed Interactions with Animals Scale, an original measure of a form of aggression that lacks comprehensive examination despite its demonstrated association with IPV, adulthood animal abuse (AAA). The prevalence, frequency, initiation, motivation, type of animal victimized, and recency of AAA was obtained from a sample of men (N= 157) and women (N= 41) arrested for domestic violence. This study also examined whether AAA accounts for unique variance in IPV perpetration beyond antisocial characteristics, and whether those IPV perpetrators who engaged in AAA differed from those who did not on other characteristics common to perpetrators of IPV. Comparisons by sex were made where appropriate. AAA perpetration was endorsed at significantly higher rates than in nationwide community samples. Men endorsed significantly more AAA overall, as well as physical and threatening acts of AAA than women. It was more common for both sexes to initiate animal abuse perpetration after age 15 than before age 15, beyond the age at which animal abuse is typically considered a sign of future psychopathology. AAA was not uniquely associated with IPV perpetration beyond antisocial personality characteristics. Compared to those individuals who denied AAA perpetration, men who reported AAA perpetration endorsed higher rates of antisocial personality characteristics and difficulties with emotional clarity, while women who reported AAA perpetration were not significantly different from their counterparts.', 'corpus_id': 49230000, 'score': 1}, {'doc_id': '235409216', 'title': 'Perpetrator Blame Attribution in Heterosexual Intimate Partner Violence: The Role of Gender and Perceived Injury', 'abstract': 'Gender asymmetry in intimate partner violence (IPV) is a well-supported phenomenon in research and clinical work. However few studies examine the influence of gender and perceived injury on blame attribution in third-party observers. Partner violence resulting in physical injury is thought to be more serious, and therefore, men are blamed more than women for perpetrating the same offence, as they are often perceived to be stronger and more capable of inflicting injury. The current vignette study used a 2x2x3 mixed-model design in order to examine the influence of perpetrator and observer gender, and weapon presence on observer blame. Participants were randomly assigned the male or female perpetrator condition. They were then given vignettes depicting an IPV scenario, which included either no weapon, a bottle, or a gun. A split-plot analysis of variance produced a significant main effect of perpetrator gender and an interaction effect of perpetrator gender and weapon presence. Strengths and limitations of the study are examined along with possible avenues for future exploration. The work done in the present study is important as it contributes to the understanding of community attitudes toward IPV, which in turn drive policy work and education ensuring that social perceptions are in line with clinical realities.', 'corpus_id': 235409216, 'score': 0}, {'doc_id': '233284608', 'title': 'Psychological consequences of domestic violence on children', 'abstract': 'Domestic violence is a universal phenomenon and a lot of families suffer in all over the world. Most children are exposed to domestic violence in each country and societies. And is has a horrible and disagreeable psychological, emotional, physical, and social effects on children. This article has been written to study psychological impacts of domestic violence on children. We have analyzed lots of prestigious books, articles, and researches in the context of domestic violence and its psychological effects on children. Our research findings show that children who exposed to domestic violence may have depression, anxiety, panic attacks, posttraumatic stress disorder, sadness, anger, powerlessness, and other psychological distress. Moreover, they are usually weaker at performing development activities and have problems of remembering and using new information likewise Children who exposed domestic violence may be faced with a lots of emotional problems (fear, low self-esteem, insecurity), cognitive functioning, and behavioral problems such as: aggression and introversion, and social problems (weak social skills in their lives and social isolation) and they will show other signs of emotional problems such as: self-harm, weight-loss and bed-wetting. Effect of domestic violence can be different long term and short term; the short term effects may start from primary beginning of domestic violence and may continue till or after last event. But if the domestic violence is very deep the impact can be long term. And the connection between long term effect and short term effect may related to different factors such as: the length of time of violence, exposure of child, age of child and intensity of violence and finally the degree of level to which child has accessibility to some help, if the child has strong source of help the effects of trauma will be reduced significantly.', 'corpus_id': 233284608, 'score': 0}, {'doc_id': '235593915', 'title': 'The Influence of Media Violence on Intimate Partner Violence Perpetration: An Examination of Inmates’ Domestic Violence Convictions and Self-Reported Perpetration', 'abstract': 'Research suggests that the representation of violence against women in the media has resulted in an increased acceptance of attitudes favoring domestic violence. While prior work has investigated the relationship between violent media exposure and violent crime, there has been little effort to empirically examine the relationship between specific forms of violent media exposure and the perpetration of intimate partner violence. Using data collected from a sample of 148 inmates, the current study seeks to help fill these gaps in the literature by examining the relationship between exposure to various forms of pleasurable violent media and the perpetration of intimate partner violence (i.e., conviction and self-reported). At the bivariate level, results indicate a significant positive relationship between exposure to pleasurable television violence and self-reported intimate partner abuse. However, this relationship is reduced to insignificant levels in multivariable modeling. Endorsement of domestic violence beliefs and victimization experience were found to be the strongest predictors of intimate partner violence perpetration. Potential policy implications based on findings are discussed within.', 'corpus_id': 235593915, 'score': 0}]"
14	{'doc_id': '6102161', 'title': 'Cortical Folding Patterns and Predicting Cytoarchitecture', 'abstract': 'The human cerebral cortex is made up of a mosaic of structural areas, frequently referred to as Brodmann areas (BAs). Despite the widespread use of cortical folding patterns to perform ad hoc estimations of the locations of the BAs, little is understood regarding 1) how variable the position of a given BA is with respect to the folds, 2) whether the location of some BAs is more variable than others, and 3) whether the variability is related to the level of a BA in a putative cortical hierarchy. We use whole-brain histology of 10 postmortem human brains and surface-based analysis to test how well the folds predict the locations of the BAs. We show that higher order cortical areas exhibit more variability than primary and secondary areas and that the folds are much better predictors of the BAs than had been previously thought. These results further highlight the significance of cortical folding patterns and suggest a common mechanism for the development of the folds and the cytoarchitectonic fields.', 'corpus_id': 6102161}	3993	"[{'doc_id': '3222772', 'title': 'Predicting the location of entorhinal cortex from MRI', 'abstract': ""Entorhinal cortex (EC) is a medial temporal lobe area critical to memory formation and spatial navigation that is among the earliest parts of the brain affected by Alzheimer's disease (AD). Accurate localization of EC would thus greatly facilitate early detection and diagnosis of AD. In this study, we used ultra-high resolution ex vivo MRI to directly visualize the architectonic features that define EC rostrocaudally and mediolaterally, then applied surface-based registration techniques to quantify the variability of EC with respect to cortical geometry, and made predictions of its location on in vivo scans. The results indicate that EC can be localized quite accurately based on cortical folding patterns, within 3 mm in vivo, a significant step forward in our ability to detect the earliest effects of AD when clinical intervention is most likely to be effective."", 'corpus_id': 3222772, 'score': 1}, {'doc_id': '212747887', 'title': 'Segmentation of brain tumor on magnetic resonance imaging using a convolutional architecture', 'abstract': 'The brain is a complex organ controlling cognitive process and physical functions. Tumors in the brain are accelerated cell growths affecting the normal function and processes in the brain. MRI scans provides detailed images of the body being one of the most common tests to diagnose brain tumors. The process of segmentation of brain tumors from magnetic resonance imaging can provide a valuable guide for diagnosis, treatment planning and prediction of results. Here we consider the problem brain tumor segmentation using a Deep learning architecture for use in tumor segmentation. Although the proposed architecture is simple and computationally easy to train, it is capable of reaching $IoU$ levels of 0.95.', 'corpus_id': 212747887, 'score': 0}, {'doc_id': '214723743', 'title': 'A new sulcal landmark identifying anatomical and functional gradients in human lateral prefrontal cortex', 'abstract': 'Understanding the relationship between anatomy and function in portions of human cortex that are expanded compared to other mammals such as lateral prefrontal cortex (LPFC) is of major interest in cognitive neuroscience. Implementing a multi-modal approach and the manual definition of nearly 800 cortical indentations, or sulci, in 72 hemispheres, we report a new sulcal landmark in human LPFC: the posterior middle frontal sulcus (pmfs). The pmfs is a shallow tertiary sulcus with three components that differ in their myelin content, resting state connectivity profiles, and engagement across meta-analyses of 83 cognitive tasks. These findings support a classic, largely unconsidered anatomical theory that tertiary sulci serve as landmarks in association cortices, as well as a modern cognitive neuroscience theory proposing a functional hierarchy in LPFC. As there is a growing need for computational tools that automatically define tertiary sulci throughout cortex, we share pmfs probabilistic sulcal maps with the field.', 'corpus_id': 214723743, 'score': 1}, {'doc_id': '216036143', 'title': 'Self-Supervised Feature Extraction for 3D Axon Segmentation', 'abstract': 'Existing learning-based methods to automatically trace axons in 3D brain imagery often rely on manually annotated segmentation labels. Labeling is a labor-intensive process and is not scalable to whole-brain analysis, which is needed for improved understanding of brain function. We propose a self-supervised auxiliary task that utilizes the tube-like structure of axons to build a feature extractor from unlabeled data. The proposed auxiliary task constrains a 3D convolutional neural network (CNN) to predict the order of permuted slices in an input 3D volume. By solving this task, the 3D CNN is able to learn features without ground-truth labels that are useful for downstream segmentation with the 3D U-Net model. To the best of our knowledge, our model is the first to perform automated segmentation of axons imaged at subcellular resolution with the SHIELD technique. We demonstrate improved segmentation performance over the 3D U-Net model on both the SHIELD PVGPe dataset and the BigNeuron Project, single neuron Janelia dataset.', 'corpus_id': 216036143, 'score': 0}, {'doc_id': '14629765', 'title': 'Accurate prediction of V1 location from cortical folds in a surface coordinate system', 'abstract': ""Previous studies demonstrated substantial variability of the location of primary visual cortex (V1) in stereotaxic coordinates when linear volume-based registration is used to match volumetric image intensities [Amunts, K., Malikovic, A., Mohlberg, H., Schormann, T., and Zilles, K. (2000). Brodmann's areas 17 and 18 brought into stereotaxic space-where and how variable? Neuroimage, 11(1):66-84]. However, other qualitative reports of V1 location [Smith, G. (1904). The morphology of the occipital region of the cerebral hemisphere in man and the apes. Anatomischer Anzeiger, 24:436-451; Stensaas, S.S., Eddington, D.K., and Dobelle, W.H. (1974). The topography and variability of the primary visual cortex in man. J Neurosurg, 40(6):747-755; Rademacher, J., Caviness, V.S., Steinmetz, H., and Galaburda, A.M. (1993). Topographical variation of the human primary cortices: implications for neuroimaging, brain mapping, and neurobiology. Cereb Cortex, 3(4):313-329] suggested a consistent relationship between V1 and the surrounding cortical folds. Here, the relationship between folds and the location of V1 is quantified using surface-based analysis to generate a probabilistic atlas of human V1. High-resolution (about 200 microm) magnetic resonance imaging (MRI) at 7 T of ex vivo human cerebral hemispheres allowed identification of the full area via the stria of Gennari: a myeloarchitectonic feature specific to V1. Separate, whole-brain scans were acquired using MRI at 1.5 T to allow segmentation and mesh reconstruction of the cortical gray matter. For each individual, V1 was manually identified in the high-resolution volume and projected onto the cortical surface. Surface-based intersubject registration [Fischl, B., Sereno, M.I., Tootell, R.B., and Dale, A.M. (1999b). High-resolution intersubject averaging and a coordinate system for the cortical surface. Hum Brain Mapp, 8(4):272-84] was performed to align the primary cortical folds of individual hemispheres to those of a reference template representing the average folding pattern. An atlas of V1 location was constructed by computing the probability of V1 inclusion for each cortical location in the template space. This probabilistic atlas of V1 exhibits low prediction error compared to previous V1 probabilistic atlases built in volumetric coordinates. The increased predictability observed under surface-based registration suggests that the location of V1 is more accurately predicted by the cortical folds than by the shape of the brain embedded in the volume of the skull. In addition, the high quality of this atlas provides direct evidence that surface-based intersubject registration methods are superior to volume-based methods at superimposing functional areas of cortex and therefore are better suited to support multisubject averaging for functional imaging experiments targeting the cerebral cortex."", 'corpus_id': 14629765, 'score': 1}, {'doc_id': '8366133', 'title': ""Predicting the location of human perirhinal cortex, Brodmann's area 35, from MRI"", 'abstract': ""The perirhinal cortex (Brodmann's area 35) is a multimodal area that is important for normal memory function. Specifically, perirhinal cortex is involved in the detection of novel objects and manifests neurofibrillary tangles in Alzheimer's disease very early in disease progression. We scanned ex vivo brain hemispheres at standard resolution (1 mm × 1 mm × 1 mm) to construct pial/white matter surfaces in FreeSurfer and scanned again at high resolution (120 μm × 120 μm × 120 μm) to determine cortical architectural boundaries. After labeling perirhinal area 35 in the high resolution images, we mapped the high resolution labels to the surface models to localize area 35 in fourteen cases. We validated the area boundaries determined using histological Nissl staining. To test the accuracy of the probabilistic mapping, we measured the Hausdorff distance between the predicted and true labels and found that the median Hausdorff distance was 4.0mm for the left hemispheres (n=7) and 3.2mm for the right hemispheres (n=7) across subjects. To show the utility of perirhinal localization, we mapped our labels to a subset of the Alzheimer's Disease Neuroimaging Initiative dataset and found decreased cortical thickness measures in mild cognitive impairment and Alzheimer's disease compared to controls in the predicted perirhinal area 35. Our ex vivo probabilistic mapping of the perirhinal cortex provides histologically validated, automated and accurate labeling of architectonic regions in the medial temporal lobe, and facilitates the analysis of atrophic changes in a large dataset for earlier detection and diagnosis."", 'corpus_id': 8366133, 'score': 1}, {'doc_id': '214551088', 'title': 'Predicting brain function from anatomy using geometric deep learning', 'abstract': 'Whether it be in a single neuron or a more complex biological system like the human brain, form and function are often directly related. The functional organization of human visual cortex, for instance, is tightly coupled with the underlying anatomy. This is seen in properties such as cortical magnification (i.e., there is more cortex dedicated to processing foveal vs. peripheral information) as well as in the presence, placement, and connectivity of multiple visual areas - which is critical for the hierarchical processing underpinning the rich experience of human vision. Here we developed a geometric deep learning model capable of exploiting the actual structure of the cortex to learn the complex relationship between brain function and anatomy in human visual cortex. We show that our neural network was not only able to predict the functional organization throughout the visual cortical hierarchy, but that it was also able to predict nuanced variations across individuals. Although we demonstrate its utility for modeling the relationship between structure and function in human visual cortex, geometric deep learning is flexible and well-suited for a range of other applications involving data structured in non-Euclidean spaces.', 'corpus_id': 214551088, 'score': 1}, {'doc_id': '211126796', 'title': 'What is the function of inter-hemispheric inhibition?.', 'abstract': ""It is widely supposed that following unilateral brain injury, there arises an asymmetry in inter-hemispheric inhibition which has an adverse influence upon motor control. I argue that this 'inter-hemispheric imbalance' model arises from a fundamental misunderstanding of the roles played by inter-hemispheric (callosal) projections in mammalian brains. Drawing upon a large body of empirical data, derived largely from animal models, and associated theoretical modeling, it is demonstrated that inter-hemispheric projections perform contrast enhancing and integrative functions via mechanisms such as surround/lateral inhibition. The principal functional unit of callosal influence comprises a facilitatory centre and a depressing peripheral zone, that together shape the influence of converging inputs to pyramidal neurons. Inter-hemispheric inhibition is an instance of a more general feature of mammalian neural systems, whereby inhibitory interneurons act not simply to prevent over-excitation but to sculpt the output of specific circuits. The narrowing of the excitatory focus that occurs through crossed surround inhibition is a highly conserved motif of transcallosal interactions in mammalian sensory and motor cortices. A case is presented that the notion of 'inter-hemispheric imbalance' has been sustained, and clinical interventions derived from this model promoted, by erroneous assumptions concerning that revealed by investigative techniques such as transcranial magnetic stimulation (TMS). The alternative perspective promoted by the present analysis, also permits the basis of positive (e.g. post stroke) associations between the structural integrity of transcallosal projections and motor capability to be better understood."", 'corpus_id': 211126796, 'score': 0}, {'doc_id': '211204840', 'title': 'The Fluidity of Concept Representations in Human Brain Signals', 'abstract': 'Cognitive theories of human language processing often distinguish between concrete and abstract concepts. In this work, we analyze the discriminability of concrete and abstract concepts in fMRI data using a range of analysis methods. We find that the distinction can be decoded from the signal with an accuracy significantly above chance, but it is not found to be a relevant structuring factor in clustering and relational analyses. From our detailed comparison, we obtain the impression that human concept representations are more fluid than dichotomous categories can capture. We argue that fluid concept representations lead to more realistic models of human language processing because they better capture the ambiguity and underspecification present in natural language use.', 'corpus_id': 211204840, 'score': 0}, {'doc_id': '214693237', 'title': 'Planning with Brain-inspired AI', 'abstract': 'This article surveys engineering and neuroscientific models of planning as a cognitive function, which is regarded as a typical function of fluid intelligence in the discussion of general intelligence. It aims to present existing planning models as references for realizing the planning function in brain-inspired AI or artificial general intelligence (AGI). It also proposes themes for the research and development of brain-inspired AI from the viewpoint of tasks and architecture.', 'corpus_id': 214693237, 'score': 0}]"
15	{'doc_id': '53569842', 'title': 'Capitalizing on the heterogeneous effects of CFTR nonsense and frameshift variants to inform therapeutic strategy for cystic fibrosis', 'abstract': 'CFTR modulators have revolutionized the treatment of individuals with cystic fibrosis (CF) by improving the function of existing protein. Unfortunately, almost half of the disease-causing variants in CFTR are predicted to introduce premature termination codons (PTC) thereby causing absence of full-length CFTR protein. We hypothesized that a subset of nonsense and frameshift variants in CFTR allow expression of truncated protein that might respond to FDA-approved CFTR modulators. To address this concept, we selected 26 PTC-generating variants from four regions of CFTR and determined their consequences on CFTR mRNA, protein and function using intron-containing minigenes expressed in 3 cell lines (HEK293, MDCK and CFBE41o-) and patient-derived conditionally reprogrammed primary nasal epithelial cells. The PTC-generating variants fell into five groups based on RNA and protein effects. Group A (reduced mRNA, immature (core glycosylated) protein, function <1% (n = 5)) and Group B (normal mRNA, immature protein, function <1% (n = 10)) variants were unresponsive to modulator treatment. However, Group C (normal mRNA, mature (fully glycosylated) protein, function >1% (n = 5)), Group D (reduced mRNA, mature protein, function >1% (n = 5)) and Group E (aberrant RNA splicing, mature protein, function > 1% (n = 1)) variants responded to modulators. Increasing mRNA level by inhibition of NMD led to a significant amplification of modulator effect upon a Group D variant while response of a Group A variant was unaltered. Our work shows that PTC-generating variants should not be generalized as genetic ‘nulls’ as some may allow generation of protein that can be targeted to achieve clinical benefit.', 'corpus_id': 53569842}	5532	"[{'doc_id': '219172832', 'title': 'Clinical efficacy, speed of improvement and safety of apremilast for the treatment of adult Psoriasis during COVID‐19 pandemic', 'abstract': ""Time to improvement is a crucial characteristic for effective treatments of chronic inflammatory conditions, such as psoriasis. Apremilast is a recently approved drug, belonging to the small molecule phosphodiesterase 4 inhibitors, whose optimal safety and efficacy profile is somewhat affected by slow activity rate in clinical trials. Real world case series are suggesting a more consistent improvement, and with this additional personal investigation on 48 patients, we signal that 58% of patients achieved Psoriasis Area and Severity Index (PASI) 50, and 19% PASI 75 improvement in the first 8\u2009weeks of treatment. Results at 16‐week are remarkable, with overall 55% of patients achieving PASI 75, 21% PASI 90 and 14% PASI 100. Only 8 patients (18, 6%) had slightly improved, although satisfied with the regimen, and determined to continue. Noteworthy, our population was rather problematic in terms of comorbidities (86%), and resistance to other treatments, with only 28% naïve to systemics, including biologics. Moreover, the observation period includes the Italian outbreak of COVID‐19 epidemic, and further information on apremilast safety are provided, no one of the patients having stopped treatment. In such a critical period, the apremilast satisfactory speed of therapeutic response in a real‐world setting has further strengthens patient's compliance to remain safely at home, which is the best strategy to limit contagion."", 'corpus_id': 219172832, 'score': 0}, {'doc_id': '215782277', 'title': 'Hydroxychloroquine in patients with COVID-19: an open-label, randomized, controlled trial', 'abstract': 'Abstract Objectives To assess the efficacy and safety of hydroxychloroquine (HCQ) plus standard-of-care (SOC) compared with SOC alone in adult patients with COVID-19. Design Multicenter, open-label, randomized controlled trial. Setting 16 government-designated COVID-19 treatment centers in China through 11 to 29 in February 2020. Participants 150 patients hospitalized with COVID-19. 75 patients were assigned to HCQ plus SOC and 75 were assigned to SOC alone (control group). Interventions HCQ was administrated with a loading dose of 1, 200 mg daily for three days followed by a maintained dose of 800 mg daily for the remaining days (total treatment duration: 2 or 3 weeks for mild/moderate or severe patients, respectively). Main outcome measures The primary endpoint was the 28-day negative conversion rate of SARS-CoV-2. The assessed secondary endpoints were negative conversion rate at day 4, 7, 10, 14 or 21, the improvement rate of clinical symptoms within 28-day, normalization of C-reactive protein and blood lymphocyte count within 28-day. Primary and secondary analysis was by intention to treat. Adverse events were assessed in the safety population. Results The overall 28-day negative conversion rate was not different between SOC plus HCQ and SOC group (Kaplan-Meier estimates 85.4% versus 81.3%, P=0.341). Negative conversion rate at day 4, 7, 10, 14 or 21 was also similar between the two groups. No different 28-day symptoms alleviation rate was observed between the two groups. A significant efficacy of HCQ on alleviating symptoms was observed when the confounding effects of anti-viral agents were removed in the post-hoc analysis (Hazard ratio, 8.83, 95%CI, 1.09 to 71.3). This was further supported by a significantly greater reduction of CRP (6.986 in SOC plus HCQ versus 2.723 in SOC, milligram/liter, P=0.045) conferred by the addition of HCQ, which also led to more rapid recovery of lymphopenia, albeit no statistical significance. Adverse events were found in 8.8% of SOC and 30% of HCQ recipients with two serious adverse events. The most common adverse event in the HCQ recipients was diarrhea (10%). Conclusions The administration of HCQ did not result in a higher negative conversion rate but more alleviation of clinical symptoms than SOC alone in patients hospitalized with COVID-19 without receiving antiviral treatment, possibly through anti-inflammatory effects. Adverse events were significantly increased in HCQ recipients but no apparently increase of serious adverse events. Trial registration ChiCTR2000029868.', 'corpus_id': 215782277, 'score': 0}, {'doc_id': '219122390', 'title': 'FDA Approves Phase III Clinical Trial of Tocilizumab for COVID-19 Pneumonia', 'abstract': 'The FDA has approved a phase III clinical trial to assess the safety and efficacy of intravenous tocilizumab (Actemra) plus standard of care in hospitalized adult patients with severe COVID-19 pneumonia.', 'corpus_id': 219122390, 'score': 0}, {'doc_id': '219177273', 'title': 'A Systematic Review of Mutations Associated with Isoniazid Resistance Points to Lower Diagnostic Sensitivity for Common Mutations and Increased Incidence of Uncommon Mutations in Clinical Strains of Mycobacterium tuberculosis', 'abstract': 'Molecular testing is rapidly becoming integral to the global tuberculosis (TB) control effort. Uncommon mechanisms of resistance can escape detection by these platforms and lead to the development of Multi-Drug Resistant (MDR) strains. This article is a systematic review of published articles that reported isoniazid (INH) resistance-conferring mutations between September-2013 and December-2019. The aims were to catalogue mutations associated with INH resistance, estimate their global prevalence and co-occurrence, and their utility in molecular diagnostics. The genes commonly associated with INH resistance, katG, inhA, fabG1, and the intergenic region oxyR-ahpC were considered in this review. In total, 52 articles were included describing 5,632 INHR clinical isolates from 31 countries. The three most frequently mutated loci continue to be katG315 (4,100), inhA-15 (786), and inhA-8 (105). However, the diagnostic value of inhA-8 is far lower than previously thought, only appearing in 25 (0.4%) INHR isolates that lacked a mutation at the first two loci. Importantly, of the four katG loci recommended by the previous systematic review for diagnostics, only katG315 was observed in our INHR isolates. This indicates continued evolution and regional differences in INH resistance. We have identified 58 loci (common to both systematic reviews) in three genomic regions as a reliable basis for molecular diagnostics. We also catalogue mutations at 49 new loci associated with INH resistance. Including all observed mutations provides a cumulative sensitivity of 85.1%. The most disconcerting is the remaining 14.9% of isolates that harbor an unknown mechanism of resistance, will escape molecular detection, and likely convert to MDR-TB, further complicating treatment. Integrating the information cataloged in this and other similar studies into current diagnostic tools is essential for combating the emergence of MDR-TB. Exclusion of this information will lead to an unnatural selection which will result in eradication of the common but propagation of the uncommon mechanisms of resistance, leading to ineffective global treatment policy and a need for region-specific regiments. Finally, the observance of many low-frequency resistance-conferring mutations point to an advantage of platforms that consider regions rather than specific loci for detection of resistance.', 'corpus_id': 219177273, 'score': 0}, {'doc_id': '210924482', 'title': 'Efficacy and safety of ataluren in patients with nonsense-mutation cystic fibrosis not receiving chronic inhaled aminoglycosides: The international, randomized, double-blind, placebo-controlled Ataluren Confirmatory Trial in Cystic Fibrosis (ACT CF).', 'abstract': ""BACKGROUND\nAtaluren was developed for potential treatment of nonsense-mutation cystic fibrosis (CF). A previous phase 3 ataluren study failed to meet its primary efficacy endpoint, but post-hoc analyses suggested that aminoglycosides may have interfered with ataluren's action. Thus, this subsequent trial (NCT02139306) was designed to assess the efficacy and safety of ataluren in patients with nonsense-mutation CF not receiving aminoglycosides.\n\n\nMETHODS\nEligible subjects with nonsense-mutation CF (aged ≥6 years; percent predicted (pp) FEV1 ≥40 and ≤90) from 75 sites in 16 countries were randomly assigned in double-blinded fashion to receive oral ataluren or matching placebo thrice daily for 48 weeks. The primary endpoint was absolute change in average ppFEV1 from baseline to the average of Weeks 40 and 48.\n\n\nFINDINGS\n279 subjects were enrolled; 138 subjects in the ataluren arm and 136 in the placebo arm were evaluable for efficacy. Absolute ppFEV1 change from baseline did not differ significantly between the ataluren and placebo groups at Week 40 (-0.8 vs -1.8) or Week 48 (-1.7 vs -2.4). Average ppFEV1 treatment difference from baseline to Weeks 40 and 48 was 0.6 (95% CI -1.3, 2.5; p\xa0=\xa00.54). Pulmonary exacerbation rate per 48 weeks was not significantly different (ataluren 0.95 vs placebo 1.13; rate ratio p\xa0=\xa00.40). Safety was similar between groups. No life-threatening adverse events or deaths were reported.\n\n\nINTERPRETATION\nNeither ppFEV1 change nor pulmonary exacerbation rate over 48 weeks were statistically different between ataluren and placebo groups. Development of a nonsense-mutation CF therapy remains elusive."", 'corpus_id': 210924482, 'score': 1}, {'doc_id': '216054616', 'title': 'Genetic Profiles in Pharmacogenes Indicate Personalized Drug Therapy for COVID-19', 'abstract': 'Background: The coronavirus disease 2019 (COVID-19) has become a global pandemic currently. Many drugs showed potential for COVID-19 therapy. However, genetic factors which can lead to different drug efficiency and toxicity among populations are still undisclosed in COVID-19 therapy. Methods: We selected 67 potential drugs for COVID-19 therapy (DCTs) from clinical guideline and clinical trials databases. 313 pharmaco-genes related to these therapeutic drugs were included. Variation information in 125,748 exomes were collected for racial differences analyses. The expression level of pharmaco-genes in single cell resolution was evaluated from single-cell RNA sequencing (scRNA-seq) data of 17 healthy adults. Results: Pharmacogenes, including CYP3A4, ABCB1, SLCO1B1, ALB, CYP3A5, were involved in the process of more than multi DCTs. 224 potential drug-drug interactions (DDI) of DCTs were predicted, while 112 of them have been reported. Racial discrepancy of common nonsynonymous mutations was found in pharmacogenes including: VDR, ITPA, G6PD, CYP3A4 and ABCB1 which related to DCTs including ribavirin, -interferon, chloroquine and lopinavir. Moreover, ACE2, the target of 2019-nCoV, was only found in parts of lung cells, which makes drugs like chloroquine that prevent virus binding to ACE2 more specific than other targeted drugs such as camostat mesylate. Conclusions: At least 17 drugs for COVID-19 therapy with predictable pharmacogenes should be carefully utilized in risk races which are consisted of more risk allele carriers. At least 29 drugs with potential of DDIs are reported to be affected by other DDIs, they should be replaced by similar drugs without interaction if it is possible. Drugs which specifically targeted to infected cells with ACE2 such as chloroquine are preferred in COVID-19 therapy.', 'corpus_id': 216054616, 'score': 0}, {'doc_id': '80736732', 'title': 'WS01.3 Translational read-through of CFTR nonsense mutations and inducement of cystic fibrosis transmembrane conductance regulator (CFTR) function by ELX-02 treatment', 'abstract': None, 'corpus_id': 80736732, 'score': 1}, {'doc_id': '90731227', 'title': 'Testing CFTR repair in cystic fibrosis patients carrying nonsense and channel gating mutations', 'abstract': 'Background. In order to measure CFTR activity in a minimal invasive manner, we set up a method to assay functional CFTR and focussed on leukocytes from healthy and CF donors. Leukocytes are recognized in the scientific literature as a key component of the pathogenetic events associated to cystic fibrosis and represent an easily accessible source of primary cells that might be exploited to monitor CFTR expression and activity.', 'corpus_id': 90731227, 'score': 1}, {'doc_id': '49339292', 'title': 'A G542X cystic fibrosis mouse model for examining nonsense mutation directed therapies', 'abstract': 'Nonsense mutations are present in 10% of patients with CF, produce a premature termination codon in CFTR mRNA causing early termination of translation, and lead to lack of CFTR function. There are no currently available animal models which contain a nonsense mutation in the endogenous Cftr locus that can be utilized to test nonsense mutation therapies. In this study, we create a CF mouse model carrying the G542X nonsense mutation in Cftr using CRISPR/Cas9 gene editing. The G542X mouse model has reduced Cftr mRNA levels, demonstrates absence of CFTR function, and displays characteristic manifestations of CF mice such as reduced growth and intestinal obstruction. Importantly, CFTR restoration is observed in G542X intestinal organoids treated with G418, an aminoglycoside with translational readthrough capabilities. The G542X mouse model provides an invaluable resource for the identification of potential therapies of CF nonsense mutations as well as the assessment of in vivo effectiveness of these potential therapies targeting nonsense mutations.', 'corpus_id': 49339292, 'score': 1}, {'doc_id': '190873359', 'title': 'WS10-2 Investigating differences in outcomes in people with cystic fibrosis with a nonsense mutation, compared to the total cystic fibrosis patient population and patients homozygous for F508del', 'abstract': None, 'corpus_id': 190873359, 'score': 1}]"
16	{'doc_id': '56057207', 'title': 'A new governance approach for multi-firm projects: Lessons from Olkiluoto 3 and Flamanville 3 nuclear power plant projects', 'abstract': 'We analyze governance in two contemporary nuclear power plant projects: Olkiluoto 3 (Finland) and Flamanville 3 (France). We suggest that in the governance of large multi-firm projects, any of the prevalent governance approaches that rely on market, hierarchy, or hybrid forms, is not adequate as such. This paper opens up avenues towards a novel theory of governance in large projects by adopting a project network view with multiple networked firms within a single project, and by simultaneously going beyond organizational forms that cut across the traditional firm–market dichotomy. Our analysis suggests four changes in the prevailing perspective towards the governance of large projects. First, there should be a shift from viewing multi-firm projects as hierarchical contract organizations to viewing them as supply networks characterized by a complex and networked organizational structure. Second, there should be a shift in the emphasis of the predominant modes of governance, market and hierarchy towards novel governance approaches that emphasize network-level mechanisms such as self-regulation within the project. Third, there should be a shift from viewing projects as temporary endeavors to viewing projects as short-term events or episodes embedded in the long-term sphere of shared history and expected future activities among the involved actors. Fourth, there should be a shift from the prevailing narrow view of a hierarchical project management system towards an open system view of managing in complex and challenging institutional environments.', 'corpus_id': 56057207}	20756	[{'doc_id': '235789136', 'title': 'Towards Sustainable Innovative Business Models', 'abstract': 'This paper addresses two research questions: (1) How do firms innovate their business models to deal with the economic, environmental and social aspects of sustainability, and their interconnections? (2) How do managers design the process toward more sustainable innovative business models? Starting from the triple-layered canvas as a theoretical framework, a pattern matching technique is used to compare that theoretical pattern to the empirical pattern observed. The study is based on qualitative methods and data from a Spanish company in the wine sector. The findings indicate that the triple-layered canvas is applicable and useful for micro firms, although the process to design sustainable innovative business models might be even more important. A new theoretical model is inferred and proposed to incorporate the perspective of the process of the business model innovations for sustainability, and to add several relevant aspects to make the process more successful. Besides this, non-family firms introducing sustainable business model innovations in their economic, ecological, and social aspects move closer to family firms’ distinctive behavior. Finally, the implications and future lines of research are summarized.', 'corpus_id': 235789136, 'score': 0}, {'doc_id': '237552257', 'title': 'The Interrelationship between Corporate Social Responsibility and Strategic Innovation In Aveiro-based Startups', 'abstract': 'The environmental, social and technological developments of the past few years have increased the research on Corporate Social Responsibility (CSR) and Strategic Innovation. Both influence the company’s reputation, its employees’ motivation, its customers’ perceptions as well as other stakeholder’s decisions. However, most academic research is conducted in large firms or small and medium-sized enterprises. This article focuses on the relationship between CSR and strategic innovation in the startup context. As a result, this paper analyses to what extent does CSR influence strategic innovation and how do startup CEOs and founders perceive this interrelation? Furthermore, what effects does CSR have on employees’ motivation, company reputation and performance? In a full state of emergency over COVID-19, we conducted 15 semistructured interviews with founders and CEOs of startups based at the University of Aveiro Incubator. The contacts were established through digital platforms, such as Hangouts and Zoom, and through cellphone calls. All of the interviewees had an idea of what CSR is, namely it is about the idea of contributing to society. Eleven of the interviewees agreed that innovation can be a motivating factor for practicing CSR. If a company combines CSR with innovation, it can differentiate itself and become more competitive, the interviewees revealed. All of the entrepreneurs defended that CSR can indeed improve employee motivation and performance. We also, herein, propose a model and perceive benefits from having startups report on their CSR efforts, which should be measured, including in relation to their innovation impact – and regarding the innovation culture and capacity for innovation of the firm.', 'corpus_id': 237552257, 'score': 0}, {'doc_id': '237407086', 'title': 'Critical Success Factors for Digitalization Projects', 'abstract': 'Our paper provides insights into which critical success factors (CSFs) for digitalization projects are seen as important from the companies’ perspective based on an online survey. The results presented in this paper show that CSFs of the dimensions of Corporate organization and Technology are considered to be of particular relevance, as stated by the companies, with Corporate culture, Top management support, and a Unified digital corporate strategy / vision as the three most important CSFs. Therefore, this paper contributes to the CSF research regarding digital transformation and enables the development of practice-oriented recommendations for action and assistance in shaping digital transformation.', 'corpus_id': 237407086, 'score': 0}, {'doc_id': '109017979', 'title': 'What is project governance and what are its origins', 'abstract': 'Abstract Although there is an ever-increasing discussion on governance in recent project research, the concept of project governance and its main origins remains ambiguous. In this paper, we examine project governance literature and contrast it to general governance literature published outside the domain of project research. Our analysis revealed the existence of two distinct and relatively independent streams of research. One of these streams addresses project governance as a phenomenon external to any specific project, while the other views project governance as internal to a specific project. Our results further indicate that while project governance literature bases most of its argumentation on established project research it also, to a significant extent, draws from the transaction cost economics literature. Based on our findings, we argue that there exists considerable potential for bridging project governance literature and general governance literature further.', 'corpus_id': 109017979, 'score': 1}, {'doc_id': '107515513', 'title': 'Industrial Megaprojects: Concepts, Strategies, and Practices for Success', 'abstract': 'Foreword JAMES B. PORTER, JR. vii Acknowledgments xi Introduction Why Megaprojects Fail So Often: Seven Key Mistakes 1 Part One Understanding the Projects Chapter 1 Megaprojects Creators and Destroyers of Capital 11 Chapter 2 Data and Methods 23 Chapter 3 Project Outcomes 37 Part Two Making the Right Business Decisions Before You Commit Chapter 4 The Opportunity-Shaping Process 53 Chapter 5 Devising the Shaping Strategy 91 Chapter 6 Megaprojects and Co rporate Governance 123 Part Three Making the Right Project Decisions Chapter 7 Basic Data Are Basic: Get Them Right Before You Start to Design 137 Chapter 8 Megaproject Teams: People Do Projects 159 Chapter 9 Organizing Megaproject Teams 185 Chapter 10 Project Defi nition: Getting the Front End Right 199 Chapter 11 Contracting 253 Chapter 12 The Control of Execution Risk 305 Chapter 13 Focus on Success 333 Glossary and Abbreviations 341 Notes 343 Index 347', 'corpus_id': 107515513, 'score': 1}, {'doc_id': '116419285', 'title': 'Editorial: Delivering value in projects and project-based business', 'abstract': 'Introduction Project-related research has increasingly treated projects and project-based operations as vehicles for defining, creating and delivering value (Laursen and Svejvig, 2016). Value is dominantly perceived as the “worth” of the project or its deliverables, dealing both with the immediate outputs of the project, the consequent outcomes (i.e., lifecycle benefits and sacrifices from using the project deliverable over time; Ahola et al., 2008; Zwikael and Smyrk, 2012), and the buyer’s willingness to pay for the deliverable (Bowman and Ambrosini, 2000). A moral or social perspective of values has also been acknowledged in the context of projects (Aliakbarlou et al., 2016): that is, treating values as abstract ideals and beliefs of what is good and right (Rokeach, 1973). Public and private organizations and their managers espouse their beliefs of what is important to them and attempt to influence the actions of other stakeholders, thereby drawing attention to the sense-making and framing processes and power and politics in project settings. The multidimensional nature of value is well understood, appearing in various economic, social, and environmental (Martinsuo and Killen, 2014; Kivilä et al., 2017) as well as symbolic and political dimensions of value (Eskerod and Ang, 2017; Flyvbjerg, 2017).', 'corpus_id': 116419285, 'score': 1}, {'doc_id': '235789141', 'title': 'Project Management in the Development of Dynamic Capabilities for an Open Innovation Era', 'abstract': 'The aim of the research is to explain how Project Management (PM) ensures the accumulation, integration, utilization, and reconfiguration of the capabilities and knowledge acquired in projects in order to build dynamic capabilities (DCs). This study also gives insight into how PM can develop DCs through the identification and implementation of project management opportunities. The result of 22 semi-structured interviews with 22 participants from 9 companies of different industries are detailed and framed within theoretical dimensions of DCs: knowledge accumulation, integration, utilization, reconfiguration, sensing, and seizing. As a result, we present the best practices, techniques, and PM tools that allow leveraging DCs in organizations. This qualitative study contributes to a theoretical and empirical discussion about how PM transforms knowledge acquired in projects into routines and learning practices that allow organizations to develop or reshape capabilities.', 'corpus_id': 235789141, 'score': 0}, {'doc_id': '212479021', 'title': 'Measuring and improving information systems agility through the balanced scorecard approach', 'abstract': 'Facing an environment increasingly complex, uncertain and changing, even in crisis, organizations are driven to be agile in order to survive. Agility, at the core heart of business strategy, represents the ability to grow in a competitive environment of continuous and unpredictable changes with information systems perceived as one of its main enablers. In other words, to be agile, organizations must be able to rely on agile enterprise information systems/information technology (IT/IS). Since, the agility needs are not the same among stakeholders, the objective of this research is to develop a conceptual model for the achievement and assessment of IT/IS agility from balanced perspectives to support agile organizations. Several researches have indicated that the IT balanced scorecard (BSC) approach is an appropriate technique for evaluating IT performance. This paper provides a balanced-scorecard based framework to evaluate the IS agility through four perspectives: business contribution, user orientation, operation excellence and innovation and competitiveness. The proposed framework, called IS Agility BSC, propose a three layer structure for each of the four perspectives: mission, key success factors, and agility evaluation criteria. According to this conceptual model, enterprise information systems agility is measured according to 14 agility key success factors, over the four BSC perspectives, using 42 agility evaluation criteria that are identified based on literature survey methodology. This paper explores agility in the broader context of the enterprise information systems. The findings will provide, for both researchers and practitioners, a practical approach for achieving and measuring IS agility performance to support organizations in attempt to become agile as a new condition of surviving in the new business world.', 'corpus_id': 212479021, 'score': 0}, {'doc_id': '219445681', 'title': 'Inter-organizational collaboration challenges and preconditions in industrial engineering projects', 'abstract': 'The purpose of this paper is to achieve an understanding of the challenges and preconditions for inter-organizational collaborative project practices in industrial engineering projects. A framework for identifying the challenges and preconditions for inter-organizational collaboration is presented.,The adopted research method is qualitative, and empirical data were collected from the industrial engineering project sector in Finland. The literature related to industrial engineering projects and inter-organizational collaborative project management practices is summarized, informing the qualitative design of the study.,By analyzing empirical data from industrial engineering projects, the challenges for inter-organizational collaboration are identified in each industrial engineering project stage. A framework of preconditions for inter-organizational collaboration is identified, in which investors are advised to pay attention when deciding on the use of collaborative project management methods.,The findings of this study help practitioners deal effectively with mechanisms aimed at fostering and hindering inter-organizational collaborative practices. The identified preconditions for inter-organizational collaboration provide support for decision-making in every phase of an engineering project and can be used as guidelines throughout the process.,Inter-organizational collaborative project management practices have recently been attracting attention in the industrial engineering project setting. This research is an attempt to identify the underlying forces supporting and preventing inter-organizational collaboration in industrial engineering projects. This study offers a framework that can help academics and project management practitioners deal with the challenges affecting inter-organizational collaboration at each project stage and consider preconditions for inter-organizational collaboration in industrial engineering project settings.', 'corpus_id': 219445681, 'score': 1}, {'doc_id': '234428218', 'title': 'Preparing for Successful Collaborative Contracts', 'abstract': 'Preparing well before entering a contract is always vital, independent of the characteristics of the project and type of contract. However, as projects become larger and more complex, and value for stakeholders and society becomes the dominating perspective on success, the need for well-developed collaboration is becoming more and more critical. In this paper, we investigate how the parties should prepare for a collaborative project. The purpose is to help owners secure the success of the project for its key stakeholders. We choose to address the issues as an active risk mitigation strategy that serves as a vehicle to reduce uncertainty, avoid unnecessary risks, and utilize opportunities as a project owner. We look at the project mainly through a project owner perspective, but on key points, we contrast this with contractor perspectives. The research was performed in Australia in 2020 and includes public and private sector investment projects. The methodology is qualitative case studies and includes primarily in-depth interviews supplemented with document studies and two workshop group discussions. The paper highlights the difference between being collaborative in a contract and using a collaborative contract. The results document significant differences in preparations depending on the degree of complexity of the projects. One major difference is illustrated in the different levels of precision in terms and definitions used in communication. Complex projects require freedom of interpretation only gained by allowing wide and less precise expressions.', 'corpus_id': 234428218, 'score': 1}]
17	{'doc_id': '2670798', 'title': 'Mindfulness-Based Interventions in Context: Past, Present, and Future', 'abstract': 'studies from the Center for Mindfulness in Medicine, Health Care, and Society not reviewed by Baer but which raise a number of key questions about clinical applicability, study design, and mechanism of action, and (7) current opportunities for professional training and development in mindfulness and its clinical applications.', 'corpus_id': 2670798}	12955	"[{'doc_id': '12955342', 'title': 'Mindfulness : A proposed operational definition', 'abstract': 'There has been substantial interest in mindfulness as an approach to reduce cognitive vulnerability to stress and emotional distress in recent years. However, thus far mindfulness has not been defined operationally. This paper describes the results of recent meetings held to establish a consensus on mindfulness and to develop conjointly a testable operational definition. We propose a two-component model of mindfulness and specify each component in terms of specific behaviors, experiential manifestations, and implicated psychological processes. We then address issues regarding temporal stability and situational specificity and speculate on the conceptual and operational distinctiveness of mindfulness. We conclude this paper by discussing implications for instrument development and briefly describing our own approach to measurement.', 'corpus_id': 12955342, 'score': 1}, {'doc_id': '231652404', 'title': 'Editorial for special issue on neglect rehabilitation.', 'abstract': ""It is clear already that in current and future years more people will suffer from stroke, whether related to COVID-19 or not, and given its prevalence, many more people's lives will be affected by neglect. Here we hope to have contributed to its possible amelioration with highlights of the latest thinking on neglect diagnosis, prevalence and treatment."", 'corpus_id': 231652404, 'score': 0}, {'doc_id': '204538683', 'title': 'What Is Meditation? Proposing an Empirically Derived Classification System', 'abstract': 'Meditation is an umbrella term, which subsumes a huge number of diverse practices. It is still unclear how these practices can be classified in a reasonable way. Earlier proposals have struggled to do justice to the diversity of meditation techniques. To help in solving this issue, we used a novel bottom-up procedure to develop a comprehensive classification system for meditation techniques. In previous studies, we reduced 309 initially identified techniques to the 20 most popular ones. In the present study, 100 experienced meditators were asked to rate the similarity of the selected 20 techniques. Using multidimensional scaling, we found two orthogonal dimensions along which meditation techniques could be classified: activation and amount of body orientation. These dimensions emphasize the role of embodied cognition in meditation. Within these two dimensions, seven main clusters emerged: mindful observation, body-centered meditation, visual concentration, contemplation, affect-centered meditation, mantra meditation, and meditation with movement. We conclude there is no “meditation” as such, but there are rather different groups of techniques that might exert diverse effects. These groups call into question the common division into “focused attention” and “open-monitoring” practices. We propose a new embodied classification system and encourage researchers to evaluate this classification system through comparative studies.', 'corpus_id': 204538683, 'score': 1}, {'doc_id': '231646379', 'title': 'Robert S. Wallerstein Lecture in Psychoanalysis and Psychotherapy', 'abstract': 'Each year, the UCSF Department of Psychiatry and Behavioral Sciences invites a distinguished scholar to speak on campus as part of the Robert S. Wallerstein Visiting Lectureship in Psychoanalysis and Psychotherapy. This lecture series is held in honor of the late Robert S. Wallerstein, MD, and focuses on showcasing psychoanalytic knowledge and clinical expertise that influence psychiatry, psychotherapy, and psychoanalysis.', 'corpus_id': 231646379, 'score': 0}, {'doc_id': '231639566', 'title': 'Real-Time Telehealth Treatment Team Consultation for Self-Injury by Individuals with Autism Spectrum Disorder', 'abstract': 'Objectives Self-injurious behavior (SIB) refers to any repeated self-directed, non-suicidal, behavior that may cause or has the potential to cause physical harm to the person’s body. Behavioral interventions provide the standard evidence-based treatments for SIB by people with autism spectrum disorder (ASD) and intellectual disabilities (ID). Translating the proven effectiveness of behavioral interventions to treatment of self-injury in community settings by clinicians and caregivers has not been totally successful. The aim of the present study was to advance translational research by providing real-time telehealth consultation to a treatment team at a community-based mental health agency that provided inpatient and outpatient services to individuals with ASD and ID. Method The participants of this single-case experimental study were three adolescents with ASD who had been referred for services because of their increasingly unmanageable SIB both at home and at school. The telehealth consultant provided real-time assistance to the treatment team within a translational model of care in the development and implementation of a behavior support plan and an informal mindfulness-based Soles of the Feet (SoF) program. Results Both visual and statistical analyses demonstrated reductions in the frequency of SIB for all three adolescents, with overall clinically significant reductions only with the SoF intervention. Conclusion The results of this translational study suggest that telehealth consultation might be a viable technological alternative in situations which preclude face-to-face consultation. Telehealth consultation could be one method of supporting people with behavioral difficulties during pandemics, such as COVID-19.', 'corpus_id': 231639566, 'score': 0}, {'doc_id': '2755919', 'title': 'Mindfulness: Theoretical Foundations and Evidence for its Salutary Effects', 'abstract': 'Interest in mindfulness and its enhancement has burgeoned in recent years. In this article, we discuss in detail the nature of mindfulness and its relation to other, established theories of attention and awareness in day-to-day life. We then examine theory and evidence for the role of mindfulness in curtailing negative functioning and enhancing positive outcomes in several important life domains, including mental health, physical health, behavioral regulation, and interpersonal relationships. The processes through which mindfulness is theorized to have its beneficial effects are then discussed, along with proposed directions for theoretical development and empirical research.', 'corpus_id': 2755919, 'score': 1}, {'doc_id': '22194749', 'title': 'The varieties of contemplative experience: A mixed-methods study of meditation-related challenges in Western Buddhists', 'abstract': 'Buddhist-derived meditation practices are currently being employed as a popular form of health promotion. While meditation programs draw inspiration from Buddhist textual sources for the benefits of meditation, these sources also acknowledge a wide range of other effects beyond health-related outcomes. The Varieties of Contemplative Experience study investigates meditation-related experiences that are typically underreported, particularly experiences that are described as challenging, difficult, distressing, functionally impairing, and/or requiring additional support. A mixed-methods approach featured qualitative interviews with Western Buddhist meditation practitioners and experts in Theravāda, Zen, and Tibetan traditions. Interview questions probed meditation experiences and influencing factors, including interpretations and management strategies. A follow-up survey provided quantitative assessments of causality, impairment and other demographic and practice-related variables. The content-driven thematic analysis of interviews yielded a taxonomy of 59 meditation-related experiences across 7 domains: cognitive, perceptual, affective, somatic, conative, sense of self, and social. Even in cases where the phenomenology was similar across participants, interpretations of and responses to the experiences differed considerably. The associated valence ranged from very positive to very negative, and the associated level of distress and functional impairment ranged from minimal and transient to severe and enduring. In order to determine what factors may influence the valence, impact, and response to any given experience, the study also identified 26 categories of influencing factors across 4 domains: practitioner-level factors, practice-level factors, relationships, and health behaviors. By identifying a broader range of experiences associated with meditation, along with the factors that contribute to the presence and management of experiences reported as challenging, difficult, distressing or functionally impairing, this study aims to increase our understanding of the effects of contemplative practices and to provide resources for mediators, clinicians, meditation researchers, and meditation teachers.', 'corpus_id': 22194749, 'score': 1}, {'doc_id': '231644467', 'title': 'Perceptions, Experiences, and Challenges of Physicians Involved in Dementia Care During the COVID-19 Lockdown in India: A Qualitative Study', 'abstract': 'Introduction: With 5.3 million people living with dementia in India and the pandemic wreaking havoc, dementia care has faced unique challenges during the outbreak, with reduced healthcare access, travel restriction, long-term lockdown and fear of hospitalization. We explored the experiences and barriers faced by the physicians involved in dementia care during the lockdown period. Methods: A qualitative approach was used with purposive sampling. After an initial pilot, 148 physicians were included in the study. They were virtually interviewed in-depth based on a pre-designed semi-structured questionnaire, in areas related to tele-consultations, attributes related to dementia care, challenges faced and way forward. Interviews were recorded, transcribed and thematically analyzed using Nvivo-10 software. Triangulation, peer debriefing and respondent validation were used to ensure rigor. Results: The overarching categories that emerged were “Tele-medicine as the future of dementia care in India,” “people living with dementia being uniquely susceptible to the pandemic with a triple burden of: age, ageism and lack of autonomy” and “markedly reduced healthcare access in this population with significant mental health burden of caregivers.” The experiences of the physicians were categorized into their challenges during the lockdown period and perceptions related to specific facets of dementia care during the crisis. The general physicians expressed special “unmet needs” of dementia-specific training and specialist collaboration. Most of the participants perceived ambiguity related to the newly released telepsychiatry guidelines. Conclusion: Resource constraints and pandemic burden are currently high. This study looks at the “voices” of those actively providing dementia care during the ongoing crisis and to the best of our knowledge, is the first one from India to do so. Concurring with their experiences, PwD and their families are exposed to multiple vulnerabilities during COVID-19, need tailored care, especially at the primary healthcare level which includes general physicians. These relevant “voices” are discussed in light of the new tele-psychiatry guidelines and further optimization of dementia care in an aging India.', 'corpus_id': 231644467, 'score': 0}, {'doc_id': '204811481', 'title': 'PROMISE: A Model of Insight and Equanimity as the Key Effects of Mindfulness Meditation', 'abstract': 'In a comprehensive meta-analysis on the effects of mindfulness meditation, Eberth and Sedlmeier (2012) identified a multitude of positive effects that covered a wide range of psychological variables, such as heightened mindfulness as measured through contemporary mindfulness scales, reduced negative emotions, increased positive emotions, changes in self-concept, enhanced attention, perception, and wellbeing, improved interpersonal abilities, and a reduction of negative personality traits. The present research aimed at developing and testing a comprehensive model explaining the wide range of mindfulness meditation effects and their temporal and causal relationships. In Study 1, interviews with meditators at different levels of experience were analyzed using a grounded theory procedure. The resulting model was triangulated and refined by concepts from both Western research and ancient Buddhist scriptures. The model developed highlights equanimity (reduction in emotional reactivity) and insight (alteration of cognitions) as the two key effects of mindfulness meditation that eventually lead to increased wellbeing. The model was pilot-tested with a large sample of meditators and non-meditators in Study 2. Data showed an acceptable fit with the model and indicated that meditators and non-meditators score significantly differently on the model’s core categories.', 'corpus_id': 204811481, 'score': 1}, {'doc_id': '231646746', 'title': 'CORRELATION OF DYSPNEA WITH AGE AND SPO2 LEVELS IN COVID-19 AND EFFECTIVENESS OF NEUROPHYSIOLOGICAL FACILITATION IN THE MANAGEMENT OF DYSPNEA-A RANDOMIZED CLINICAL CONTROL TRAIL', 'abstract': 'Suraj kuma, Gowrishankar Pottur, Sangh Mitra, Ajay Kumar Kushwaha, Pramod Kuma, Shailendra Pal Singh 1 Department of Physiotherapy, Faculty of Paramedical sciences, Uttar Pradesh University of Medical Sciences, Saifai, Etawah, U.P, India 2 MBBS, DGO. Awantibai Women Hospital, Lucknow, Uttar Pradesh, India 3 Department of Physiotherapy, Uttar Pradesh University of Medical Sciences, Saifai, Etawah, U.P, India 4 Physiotherapist, Department of General Surgery, Uttar Pradesh University of Medical Sciences, Saifai, Etawah, U.P, India', 'corpus_id': 231646746, 'score': 0}]"
18	{'doc_id': '73601156', 'title': 'General purpose technologies in theory, application and controversy: a review', 'abstract': 'Distinguishing characteristics of General Purpose Technologies (GPTs) are identified and definitions discussed. Our definition includes multipurpose and single-purpose technologies, defining them according to their micro-technological characteristics, not their macro-economic effects. Identifying technologies as GPTs requires recognizing their evolutionary nature, and accepting possible uncertainties concerning marginal cases. Many of the existing ‘tests’ of whether particular technologies are GPTs are based on misunderstandings either of what GPT theory predicts or what such tests can establish. The development of formal GPT theories is outlined, showing that only the early theories predicted the inevitability of GPT-induced showdown and surges. More recent GPT theories, designed to model the characteristics of GPTs, do not imply the necessity of specific macro effects. We show that GPTs can rejuvenate the growth process without causing slowdowns or surges. We conclude that existing criticisms of GPT theory can be resolved and that the concept remains useful for economic theory.', 'corpus_id': 73601156}	12279	"[{'doc_id': '229504355', 'title': 'De-Globalisation? Global Value Chains in the Post-COVID-19 Age', 'abstract': 'This paper evaluates the extent to which the world economy has entered a phase of de-globalisation, and it offers some speculative thoughts on the future of global value chains in the post-COVID-19 age. Although the growth of international trade flows relative to that of GDP has slowed down since the Great Recession, this paper finds little systematic evidence indicating that the world economy has already entered an era of de-globalisation. Instead, the observed slowdown in globalisation is a natural sequel to the unsustainable increase in globalisation experienced in the late 1980s, 1990s and early 2000s. I offer a description of the mechanisms leading to that earlier expansionary phase, together with a discussion of why these forces might have run out of steam, and of the extent to which they may be reversible. I conclude that the main challenge for the future of globalisation is institutional and political in nature rather than technological, although new technologies might aggravate the trends in inequality that have created the current political backlash against globalisation. Zooming in on the COVID-19 global pandemic, I similarly conclude that the current health crisis may further darken the future of globalisation if it aggravates policy tensions across countries.', 'corpus_id': 229504355, 'score': 0}, {'doc_id': '222131019', 'title': 'The transformation of jobs and working conditions: Towards a policy response', 'abstract': 'The introduction of new technologies and their impact on workers is not a new topic among scholars; innovation is perceived as an embedded feature of capitalism and necessary for capital renewal (Hall 2010). However, there are some aspects which make the current changes different from previous waves of technological revolutions: the speed of innovation and its destructive potential regarding technologies currently in use but which are quickly becoming obsolete (Komlos 2016); their association with jobless growth (Brynjolfsson and McAfee 2012); and their facilitation of new business models that reach across the globe with minimum physical capital and with a very low number of employed workers, which is especially relevant in the IT sector (Soete 2018). In production areas, new technologies are used to reduce costs by limiting the input of labour while preserving or even increasing production levels. The transformation of working conditions and the reduction in job opportunities are consequences of the deployment of new technologies in the production process which deserve researchers’ attention.', 'corpus_id': 222131019, 'score': 0}, {'doc_id': '112062027', 'title': 'General Purpose Technologies', 'abstract': 'This chapter selectively surveys the literature on general purpose technologies (GPTs), focusing on incentives and aggregate growth implications. The literature on classical GPTs (steam, electricity, computers) and on classical great economic transformations (industrial revolutions, the information age) are linked to the theoretical and empirical literatures. The implications of GPT analysis for understanding the history of productivity growth in the late twentieth century are taken up on the concluding remarks.', 'corpus_id': 112062027, 'score': 1}, {'doc_id': '229359460', 'title': 'ADBI Working Paper Series DIGITAL TRANSFORMATION: SOME IMPLICATIONS FOR FINANCIAL AND MACROECONOMIC STABILITY', 'abstract': 'Digital transformation is changing how and by whom financial services are provided, how payments are made within an economy and across borders, and how and where goods and services are produced in a globalized economy. These transformations bring significant benefits in the form of greater variety and convenience of financial services, faster speed of payment transactions, and more efficient production processes. But there are also potential costs. Traditional commercial banking models are challenged by unregulated FinTech and BigTech firms, possibly threatening systemic financial stability. Globalization of production processes has led to greater spillovers of economic fluctuations across borders, thereby complicating macroeconomic policy decisions. This paper reviews how digital transformation is likely to impact financial stability, payment systems, and macroeconomic stability, and discusses the need for changes in regulatory and macroeconomic policies to mitigate the associated risks. The paper ends with reflections on the possible consequences of the current Coronavirus pandemic for the analysis and conclusions.', 'corpus_id': 229359460, 'score': 0}, {'doc_id': '169653681', 'title': 'Ai as the Next Gpt: A Political-Economy Perspective', 'abstract': 'History suggests that dismal prophecies regarding the impact of great technological advances rarely come to pass. Yet, as many occupations will indeed vanish with the advent of AI as the new General Purpose Technology (GPT), we should search for ways to ameliorate the detrimental effects of AI, and enhance its positive ones, particularly in: (1) education and skills development: need to move away from the centuries-old ""factory model"" of education, and develop instead skills relevant for an AI-based economy â€“ analytical, creative, interpersonal, and emotional. (2) The professionalization of personal care occupations, particularly in healthcare and education; these are to provide the bulk of future employment growth, yet as performed today involve little training and technology, and confer low wages. New, higher standards and academic requirements should be set for these occupations, which would enable AI to benefit both providers and users. (3) Affect the direction of technical advance â€“ we distinguish between ""human-enhancing innovations"" (HEI), that magnify and enhance sensory, motoric, and other such human capabilities, and ""human-replacing innovations"" (HRI), which replace human intervention, and often leave for humans mostly ""dumb"" jobs. AI-based HEI\'s have the potential to unleash a new wave of creativity and productivity, particularly in services, whereas HRI\'s might just decrease employment and give rise to unworthy jobs.', 'corpus_id': 169653681, 'score': 1}, {'doc_id': '109734952', 'title': 'A Time to Sow and a Time to Reap: Growth Based on General Purpose Technologies', 'abstract': ""We develop a model of growth driven by successive improvements in 'General Purpose Technologies' (GPT's), such as the steam engine, electricity, or micro-electronics. Each new generation of GPT's prompts investments in complementary inputs, and impacts the economy after enough such compatible inputs become available. The long-run dynamics take the form of recurrent cycles: during the first phase of each cycle output and productivity grow slowly or even decline, and it is only in the second phase that growth starts in earnest. The historical record of productivity growth associated with electrification, and perhaps also of computerization lately, may offer supportive evidence for this pattern. In lieu of analytical comparative dynamics, we conduct simulations of the model over a wide range of parameters, and analyze the results statistically. We extend the model to allow for skilled and unskilled labor, and explore the implications for the behavior over time of their relative wages. We also explore diffusion in the context of a multi-sector economy."", 'corpus_id': 109734952, 'score': 1}, {'doc_id': '109412507', 'title': 'Diffusion of General Purpose Technologies', 'abstract': ""History and theory alike suggest that General Purpose Technologies (GPT's), such as the steam engine or electricity, may play a key role in economic growth. In a previous paper (Helpman and Trajtenberg, 1994) we incorporated this notion into a Grossman-Helpman growth model, and explored the economy-wide dynamics that a GPT generates. The present paper deals with the diffusion of the GPT over heterogeneous final-good sectors. We show that the gradual adoption of the GPT by each user sector generates a sequence of two-phased cycles, culminating in a bringing about a spell of sustained growth. We also analyze the welfare implications of the order of adoption, by way of numerical simulations. As a diffusion of the transistor (the first embodiment of semiconductors, the dominant GPT of our era), and seek to characterize both the early adopters and the laggards in terms of the parameters of the model."", 'corpus_id': 109412507, 'score': 1}, {'doc_id': '221989035', 'title': 'General Purpose Technologies ""Engines of Growth?""', 'abstract': 'Whole eras of technical progress and economic growth appear to be driven by a few key technologies, which we call General Purpose Technologies (GPT\'s). Thus the steam engine and the electric motor may have played such a role in the past, whereas semiconductors and computers may be doing as much in our era. GPT\'s are characterized by pervasiveness (they are used as inputs by many downstream sectors), inherent potential for technical improvements, and innovational complementarities\', meaning that the productivity of RD on the other hand this phenomenon makes it difficult for a decentralized economy to fully exploit the growth opportunities offered by evolving GPT\'s. In particular; if the relationship between the GPT and its users is limited to arms-length market transactions, there will be ""too little, too late"" innovation in both sectors. Likewise, difficulties in forecasting the technological developments of the other side may lower the rate of technical advance of all sectors. Lastly, we show that the analysis of GPT\'s has testable implications in the context of R&D and productivity equations, that can in principle be estimated.', 'corpus_id': 221989035, 'score': 1}, {'doc_id': '222177052', 'title': 'Cyclical phenomena in technological change.', 'abstract': 'The process of technological change can be regarded as a non-deterministic system governed by factors of a cumulative nature that generate cyclical phenomena. In this context, the process of growth and decline of technology can be systematically analyzed to design best practices for technology management of firms and innovation policy of nations. In this perspective, this study focuses on the evolution of technologies in the U.S. recorded music industry. Empirical findings reveal that technological change in the sector under study here has recurring fluctuations of technological innovations. In particular, cycle of technology has up wave phase longer than down wave phase in the process of evolution in markets before it is substituted by a new technology. Results suggest that radical innovation is one of the main sources of cyclical phenomena for industrial and corporate change, and as a consequence, economic and social change.', 'corpus_id': 222177052, 'score': 0}, {'doc_id': '223669721', 'title': 'The conceptual basis for the empirical estimation is a multiplicative production function of new innovation in country i in technology j along the lines of the one specified in Acemoglu and others', 'abstract': 'The most closely related paper to this analysis is Johnstone and others (2010). Similar to our paper, it analyses in a cross-country setup the effect of broad policy measures on climatechange mitigating innovation. Our analysis however benefits from a much more recent sample, a more precise technological classification and more standardized policy indicators, namely the environmental policy stringency (EPS) indicator published by the OECD. This allows us to better capture the dramatic increase in clean innovation of the early 2000s, but also the flattening and partial reversal since 2010.4 Our analysis relies on the environment-related technology (ERT) classification proposed by Haščič and Migotto (2015). However, rather than relying on all ERT technologies, we focus on the climate change mitigation technologies related to energy. These are among the technologies with the biggest potential for emissions reductions and most closely targeted by climate-related policies. Unlike the technologies investigated by Johnstone and others (2010), they include not only renewable energy, but also technologies related to improved efficiency in energy generation, transmission and distribution. In addition, we use a technological specification proposed by Dechelepretre and others (2017) to look more closely at technologies related to electricity. The classification has the advantage of not only identifying clean technologies, but also dirty as well gray one, where the latter are innovation that improve the environmental impact of dirty technologies (e.g. biofuel, waste incineration plants). This allows us to study the relative benefits from tightening environmental policies for these different types of technologies, as well as the impact on electricity innovation overall.', 'corpus_id': 223669721, 'score': 0}]"
19	{'doc_id': '53047569', 'title': 'RLgraph: Flexible Computation Graphs for Deep Reinforcement Learning', 'abstract': 'Reinforcement learning (RL) tasks are challenging to implement, execute and test due to algorithmic instability, hyper-parameter sensitivity, and heterogeneous distributed communication patterns. We argue for the separation of logical component composition, backend graph definition, and distributed execution. To this end, we introduce RLgraph, a library for designing and executing high performance RL computation graphs in both static graph and define-by-run paradigms. The resulting implementations yield high performance across different deep learning frameworks and distributed backends.', 'corpus_id': 53047569}	2702	[{'doc_id': '211506335', 'title': 'DLSpec: A Deep Learning Task Exchange Specification', 'abstract': 'Deep Learning (DL) innovations are being introduced at a rapid pace. However, the current lack of standard specification of DL tasks makes sharing, running, reproducing, and comparing these innovations difficult. To address this problem, we propose DLSpec, a model-, dataset-, software-, and hardware-agnostic DL specification that captures the different aspects of DL tasks. DLSpec has been tested by specifying and running hundreds of DL tasks.', 'corpus_id': 211506335, 'score': 1}, {'doc_id': '211532776', 'title': 'Review, Analyze, and Design a Comprehensive Deep Reinforcement Learning Framework', 'abstract': 'Reinforcement learning (RL) has emerged as a standard approach for building an intelligent system, which involves multiple self-operated agents to collectively accomplish a designated task. More importantly, there has been a great attention to RL since the introduction of deep learning that essentially makes RL feasible to operate in high-dimensional environments. However, current research interests are diverted into different directions, such as multi-agent and multi-objective learning, and human-machine interactions. Therefore, in this paper, we propose a comprehensive software architecture that not only plays a vital role in designing a connect-the-dots deep RL architecture but also provides a guideline to develop a realistic RL application in a short time span. By inheriting the proposed architecture, software managers can foresee any challenges when designing a deep RL-based system. As a result, they can expedite the design process and actively control every stage of software development, which is especially critical in agile development environments. For this reason, we designed a deep RL-based framework that strictly ensures flexibility, robustness, and scalability. Finally, to enforce generalization, the proposed architecture does not depend on a specific RL algorithm, a network configuration, the number of agents, or the type of agents.', 'corpus_id': 211532776, 'score': 0}, {'doc_id': '25224027', 'title': 'Over-expression of a LEA gene in rice improves drought resistance under the field conditions', 'abstract': 'Late embryogenesis abundant (LEA) proteins have been implicated in many stress responses of plants. In this report, a LEA protein gene OsLEA3-1 was identified and over-expressed in rice to test the drought resistance of transgenic lines under the field conditions. OsLEA3-1 is induced by drought, salt and abscisic acid (ABA), but not by cold stress. The promoter of OsLEA3-1 isolated from the upland rice IRAT109 exhibits strong activity under drought- and salt-stress conditions. Three expression constructs consisting of the full-length cDNA driven by the drought-inducible promoter of OsLEA3-1 (OsLEA3-H), the CaMV 35S promoter (OsLEA3-S), and the rice Actin1 promoter (OsLEA3-A) were transformed into the drought-sensitive japonica rice Zhonghua 11. Drought resistance pre-screening of T1 families at anthesis stage revealed that the over-expressing families with OsLEA3-S and OsLEA3-H constructs had significantly higher relative yield (yield under drought stress treatment/yield under normal growth conditions) than the wild type under drought stress conditions, although a yield penalty existed in T1 families under normal growth conditions. Nine homozygous families, exhibiting over-expression of a single-copy of the transgene and relatively low yield penalty in the T1 generation, were tested in the field for drought resistance in the T2 and T3 generations and in the PVC pipes for drought tolerance in the T2 generation. Except for two families (transformed with OsLEA3-A), all the other families (transformed with OsLEA3-S and OsLEA3-H constructs) had higher grain yield than the wild type under drought stress in both the field and the PVC pipes conditions. No significant yield penalty was detected for these T2 and T3 families. These results indicate that transgenic rice with significantly enhanced drought resistance and without yield penalty can be generated by over-expressing OsLEA3-1 gene with appropriate promoters and following a bipartite (stress and non-stress) in-field screening protocol.', 'corpus_id': 25224027, 'score': 0}, {'doc_id': '212725640', 'title': 'Deep Deterministic Portfolio Optimization', 'abstract': 'Can deep reinforcement learning algorithms be exploited as solvers for optimal trading strategies? The aim of this work is to test reinforcement learning algorithms on conceptually simple, but mathematically non-trivial, trading environments. The environments are chosen such that an optimal or close-to-optimal trading strategy is known. We study the deep deterministic policy gradient algorithm and show that such a reinforcement learning agent can successfully recover the essential features of the optimal trading strategies and achieve close-to-optimal rewards.', 'corpus_id': 212725640, 'score': 1}, {'doc_id': '215238626', 'title': 'Uniform State Abstraction For Reinforcement Learning', 'abstract': 'Potential Based Reward Shaping combined with a potential function based on appropriately defined abstract knowledge has been shown to significantly improve learning speed in Reinforcement Learning. MultiGrid Reinforcement Learning (MRL) has further shown that such abstract knowledge in the form of a potential function can be learned almost solely from agent interaction with the environment. However, we show that MRL faces the problem of not extending well to work with Deep Learning. In this paper we extend and improve MRL to take advantage of modern Deep Learning algorithms such as Deep Q-Networks (DQN). We show that DQN augmented with our approach perform significantly better on continuous control tasks than its Vanilla counterpart and DQN augmented with MRL.', 'corpus_id': 215238626, 'score': 1}, {'doc_id': '211082837', 'title': 'fastai: A Layered API for Deep Learning', 'abstract': 'fastai is a deep learning library which provides practitioners with high-level components that can quickly and easily provide state-of-the-art results in standard deep learning domains, and provides researchers with low-level components that can be mixed and matched to build new approaches. It aims to do both things without substantial compromises in ease of use, flexibility, or performance. This is possible thanks to a carefully layered architecture, which expresses common underlying patterns of many deep learning and data processing techniques in terms of decoupled abstractions. These abstractions can be expressed concisely and clearly by leveraging the dynamism of the underlying Python language and the flexibility of the PyTorch library. fastai includes: a new type dispatch system for Python along with a semantic type hierarchy for tensors; a GPU-optimized computer vision library which can be extended in pure Python; an optimizer which refactors out the common functionality of modern optimizers into two basic pieces, allowing optimization algorithms to be implemented in 4-5 lines of code; a novel 2-way callback system that can access any part of the data, model, or optimizer and change it at any point during training; a new data block API; and much more. We have used this library to successfully create a complete deep learning course, which we were able to write more quickly than using previous approaches, and the code was more clear. The library is already in wide use in research, industry, and teaching. NB: This paper covers fastai v2, which is currently in pre-release at this http URL', 'corpus_id': 211082837, 'score': 0}, {'doc_id': '214713757', 'title': 'Agent57: Outperforming the Atari Human Benchmark', 'abstract': 'Atari games have been a long-standing benchmark in the reinforcement learning (RL) community for the past decade. This benchmark was proposed to test general competency of RL algorithms. Previous work has achieved good average performance by doing outstandingly well on many games of the set, but very poorly in several of the most challenging games. We propose Agent57, the first deep RL agent that outperforms the standard human benchmark on all 57 Atari games. To achieve this result, we train a neural network which parameterizes a family of policies ranging from very exploratory to purely exploitative. We propose an adaptive mechanism to choose which policy to prioritize throughout the training process. Additionally, we utilize a novel parameterization of the architecture that allows for more consistent and stable learning.', 'corpus_id': 214713757, 'score': 1}, {'doc_id': '211677359', 'title': 'Contextual Policy Reuse using Deep Mixture Models', 'abstract': 'Reinforcement learning methods that consider the context, or current state, when selecting source policies for transfer have been shown to outperform context-free approaches. However, existing work typically tailors the approach to a specific learning algorithm such as Q-learning, and it is often difficult to interpret and validate the knowledge transferred between tasks. In this paper, we assume knowledge of estimated source task dynamics and policies, and common goals between tasks. We introduce a novel deep mixture model formulation for learning a state-dependent prior over source task dynamics that matches the target dynamics using only state trajectories obtained while learning the target policy. The mixture model is easy to train and interpret, is compatible with most reinforcement learning algorithms, and complements existing work by leveraging knowledge of source dynamics rather than Q-values. We then show how the trained mixture model can be incorporated into standard policy reuse frameworks, and demonstrate its effectiveness on benchmarks from OpenAI-Gym.', 'corpus_id': 211677359, 'score': 0}, {'doc_id': '212717766', 'title': 'Sample Efficient Reinforcement Learning through Learning from Demonstrations in Minecraft', 'abstract': 'Sample inefficiency of deep reinforcement learning methods is a major obstacle for their use in real-world applications. In this work, we show how human demonstrations can improve final performance of agents on the Minecraft minigame ObtainDiamond with only 8M frames of environment interaction. We propose a training procedure where policy networks are first trained on human data and later fine-tuned by reinforcement learning. Using a policy exploitation mechanism, experience replay and an additional loss against catastrophic forgetting, our best agent was able to achieve a mean score of 48. Our proposed solution placed 3rd in the NeurIPS MineRL Competition for Sample-Efficient Reinforcement Learning.', 'corpus_id': 212717766, 'score': 0}, {'doc_id': '173126080', 'title': 'Послевузовское и дополнительное профессиональное образование в России', 'abstract': 'Автор отмечает стратегическую значимость поствузовского образования для модернизации России, полагая, что формирование структур поствузовского образования должно строиться на базе крупнейших университетов и научных центров. Выделяются три направления дополнительного профессионального образования: 1) получение нового высшего образования, 2) получение новой квалификации, 3) профессиональная переподготовка. Обсуждаются проблемы и основные задачи системы поствузовского образования. Приводятся многочисленные примеры из практики Института по переподготовке и повышению квалификации преподавателей гуманитарных и социальных наук (ИППК) Ростовского государственного университета (РГУ), директором которого и является автор статьи.', 'corpus_id': 173126080, 'score': 0}]
20	{'doc_id': '195733125', 'title': 'Mathematical Modeling of the Size-structured Growth of Microalgae Dividing by Multiple Fission', 'abstract': 'A novel mathematical model to simulate the size-structured growth of microalgal strains dividing by multiple fission is proposed. The model is validated by comparison with literature experimental data. Then, the implications of such division mode on crucial aspects of microalgae cultivation and processing, such as time for steady state achievement, biomass productivity and flocculant dosage, were assessed through proper numerical simulations. The model well captured the experimental results and thus represents a suitable tool for the simulation of the growth of microalgae strains dividing by multiple fission.', 'corpus_id': 195733125}	17132	[{'doc_id': '97001121', 'title': 'A novel mathematical model to simulate the size-structured growth of microalgae strains dividing by multiple fission', 'abstract': 'Abstract Several microalgae strains are capable to divide by multiple fission, namely they can give rise to variable number of daughter cells after cytokinesis. Such behavior may have implications on the overall growth and productivities of microalgal cultures that are difficult to infer intuitively. Consequently, a novel mathematical model to simulate the dynamics of the size-structured growth of microalgal strains characterized by multiple fission, is proposed in this work. The model relies on the use of population balance equations (PBEs) to describe the evolution of the size distribution of microalgae cells during growth and permits to decouple the single cell growth phase, which is known to take place in the light, from the division one, that on the contrary is assumed to occur under dark conditions according to well corroborated experimental observations. Moreover, the effect of light intensity, photoperiod and nutrients concentration on the continuous growth of the cells, are suitably accounted for by the model. Furthermore, in order to describe the partition of newborn cells after division, a new approach, which relies on suitable experimental observations, is developed to formulate a novel birth term related to PBEs which takes into account the possibility of multiple fission to take place. Model results and literature experimental data pertaining a strain capable to divide by multiple fission, are successfully compared in terms of biomass concentration evolution, thus highlighting a good predictive capability of the model. Subsequently, specific numerical experiments are performed in order to examine the potential improvements arising from this model with respect to the ones currently available in the literature. Finally, suitable simulation-based inferences are formulated about the potential implications of multiple fission on photobioreactor’s productivity.', 'corpus_id': 97001121, 'score': 1}, {'doc_id': '198914347', 'title': 'Growth and the cell cycle in green algae dividing by multiple fission', 'abstract': 'Most cells divide into two daughter cells; however, some green algae can have different division patterns in which a single mother cell can sometimes give rise to up to thousands of daughter cells. Although such cell cycle patterns can be very complex, they are governed by the same general concepts as the most common binary fission. Moreover, cell cycle progression appears to be connected with size, since cells need to ensure that their size after division will not drop below the limit required for survival. Although the exact mechanism that lets cells measure cell size remains largely unknown, there have been several prominent hypotheses that try to explain it.', 'corpus_id': 198914347, 'score': 1}, {'doc_id': '234599845', 'title': 'Cell size regulation in microorganisms', 'abstract': 'Various rod-shaped bacteria such as the canonical gram negative Escherichia coli or the wellstudied gram positive Bacillus subtilis divide symmetrically after they approximately double their volume. Their size at division is not constant, but is typically distributed over a narrow range. Here, we propose an analytically tractable model for cell size control, and calculate the cell size and interdivision time distributions. We suggest ways of extracting the model parameters from experimental data. Existing data for E. coli supports partial size control, and a particular explanation: a cell attempts to add a constant volume from the time of initiation of DNA replication to the next initiation event. This hypothesis explains how bacteria control their tight size distributions and accounts for the experimentally observed correlations between parents and daughters as well as the exponential dependence of size on growth rate.', 'corpus_id': 234599845, 'score': 0}, {'doc_id': '233328847', 'title': 'Asynchronous nuclear cycles in multinucleated Plasmodium falciparum enable rapid proliferation', 'abstract': 'Malaria-causing parasites proliferate within erythrocytes through schizogony, forming multinucleated stages before cellularization. Nuclear multiplication does not follow a strict geometric 2n progression and each proliferative cycle produces a heterogeneous number of progeny. Here, by tracking nuclei and DNA replication, we show that individual nuclei replicate their DNA at different times, despite residing in a shared cytoplasm. Extrapolating from experimental data using mathematical modeling, we demonstrate that a limiting factor must exist that slows down the nuclear multiplication rate. Indeed, our data show that temporally overlapping DNA replication events were significantly slower than partially or non-overlapping events. Our findings suggest an evolutionary pressure that selects for asynchronous DNA replication, balancing available resources with rapid pathogen proliferation.', 'corpus_id': 233328847, 'score': 1}, {'doc_id': '233472094', 'title': 'Threshold accumulation of a constitutive protein explains E. coli cell-division behavior in nutrient upshifts', 'abstract': 'Significance The mechanism leading to cell division in the bacterium Escherichia coli is unknown, but we know that it results in adding a roughly constant size every cell cycle, regardless of size at birth. While most available studies try to infer information on cell division from steadily dividing cells in constant nutrient conditions, this study leverages a high-resolution device to monitor single-cell growth division upon nutrient changes. Comparing these data with different mathematical models, we are able to discriminate among fundamentally different mechanisms of cell-division control, and we show that the data support a model where an unregulated protein accumulates to a threshold and triggers division. Despite a boost of recent progress in dynamic single-cell measurements and analyses in Escherichia coli, we still lack a mechanistic understanding of the determinants of the decision to divide. Specifically, the debate is open regarding the processes linking growth and chromosome replication to division and on the molecular origin of the observed “adder correlations,” whereby cells divide, adding roughly a constant volume independent of their initial volume. In order to gain insight into these questions, we interrogate dynamic size-growth behavior of single cells across nutrient upshifts with a high-precision microfluidic device. We find that the division rate changes quickly after nutrients change, much before growth rate goes to a steady state, and in a way that adder correlations are robustly conserved. Comparison of these data to simple mathematical models falsifies proposed mechanisms, where replication–segregation or septum completions are the limiting step for cell division. Instead, we show that the accumulation of a putative constitutively expressed “P-sector divisor” protein explains the behavior during the shift.', 'corpus_id': 233472094, 'score': 0}, {'doc_id': '233245996', 'title': 'Emergent Spatiotemporal Population Dynamics with Cell-Length Control of Synthetic Microbial Consortia', 'abstract': 'Increased complexity of engineered microbial biocircuits highlights the need for distributed cell functionality due to concomitant increases of metabolic and regulatory burdens imposed on single-strain topologies. Distributed systems, however, introduce additional challenges since consortium composition and spatiotemporal dynamics of constituent strains must be robustly controlled to achieve desired circuit behaviors. Here, we address these challenges with a modeling-based investigation of emergent spatiotemporal population dynamics that result from cell-length control of monolayer, two-strain bacterial consortia. We demonstrate that with dynamic control of a strain’s division length, nematic cell alignment in close-packed monolayers can be destabilized. We found this destabilization conferred an emergent, competitive advantage on smaller-length strains—but by mechanisms that differed depending on the spatial patterns of the population. We used complementary modeling approaches to elucidate underlying mechanisms: an agent-based model to simulate detailed mechanical and signaling interactions between the competing strains and a reductive, stochastic lattice model to represent cell-cell interactions with a single rotational parameter. Our modeling suggests that spatial strain-fraction oscillations can be generated when cell-length control is coupled to quorum-sensing signaling in negative feedback topologies. Our research employs novel methods of population control and points the way to programming strain fraction dynamics in consortial synthetic biology. Engineered microbial collectives are more versatile and robust than single strain populations. However, the function of such collectives is sensitive to their spatiotemporal organization. Here, we demonstrate control of the spatiotemporal composition of synthetic microbial consortia by dynamically modulating the average cell length of constituent strains. Such modulation confers an emergent “mechanical fitness” advantage upon the shorter length strain. We used both a biophysically realistic agent-based model to test the impact of cell shape on spatiotemporal dynamics and a conceptually simpler stochastic lattice model to explain the essential mechanisms driving the dynamics.', 'corpus_id': 233245996, 'score': 0}, {'doc_id': '233206086', 'title': 'The Modular Circuitry of Apicomplexan Cell Division Plasticity', 'abstract': 'The close-knit group of apicomplexan parasites displays a wide variety of cell division modes, which differ between parasites as well as between different life stages within a single parasite species. The beginning and endpoint of the asexual replication cycles is a ‘zoite’ harboring the defining apical organelles required for host cell invasion. However, the number of zoites produced per division round varies dramatically and can unfold in several different ways. This plasticity of the cell division cycle originates from a combination of hard-wired developmental programs modulated by environmental triggers. Although the environmental triggers and sensors differ between species and developmental stages, widely conserved secondary messengers mediate the signal transduction pathways. These environmental and genetic input integrate in division-mode specific chromosome organization and chromatin modifications that set the stage for each division mode. Cell cycle progression is conveyed by a smorgasbord of positively and negatively acting transcription factors, often acting in concert with epigenetic reader complexes, that can vary dramatically between species as well as division modes. A unique set of cell cycle regulators with spatially distinct localization patterns insert discrete check points which permit individual control and can uncouple general cell cycle progression from nuclear amplification. Clusters of expressed genes are grouped into four functional modules seen in all division modes: 1. mother cytoskeleton disassembly; 2. DNA replication and segregation (D&S); 3. karyokinesis; 4. zoite assembly. A plug-and-play strategy results in the variety of extant division modes. The timing of mother cytoskeleton disassembly is hard-wired at the species level for asexual division modes: it is either the first step, or it is the last step. In the former scenario zoite assembly occurs at the plasma membrane (external budding), and in the latter scenario zoites are assembled in the cytoplasm (internal budding). The number of times each other module is repeated can vary regardless of this first decision, and defines the modes of cell division: schizogony, binary fission, endodyogeny, endopolygeny.', 'corpus_id': 233206086, 'score': 0}, {'doc_id': '190874494', 'title': 'A single light-responsive sizer can control multiple-fission cycles in Chlamydomonas', 'abstract': 'Proliferating cells need to coordinate cell division and growth to maintain size homeostasis. Any systematic deviation from a balance between growth and division results in progressive changes of cell size over subsequent generations. While most eukaryotic cells execute binary division after a mass doubling, the photosynthetic green alga Chlamydomonas can grow more than eight-fold during daytime before undergoing rapid cycles of DNA replication, mitosis and cell division at night, which produce up to 16 daughter cells. Here, we propose a mechanistic model for multiple fission and size control in Chlamydomonas. The model comprises a light-sensitive and size-dependent biochemical toggle switch that acts as a sizer and guards transitions into and exit from a phase of cell-division cycle oscillations. We show that this simple ‘sizer-oscillator’ arrangement reproduces the experimentally observed features of multiple-fission cycles and the response of Chlamydomonas cells to different light-dark regimes. Our model also makes testable predictions about the dynamical properties of the biochemical network that controls these features and about the network’s makeup. Collectively, these results provide a new perspective on the concept of a ‘commitment point’ during the growth of Chlamydomonas cells and hint at an intriguing continuity of cell-size control in different eukaryotic lineages. Graphical abstract G1-sizer and S/M-oscillator can give rise to multiple-fission cycles in Chlamydomonas Light-responsive bistable switch may guard transition between G1 and S/M-cycles Illumination increases S/M-entry threshold, causing multiple-fission cycles Dark shift lowers S/M-entry threshold, allowing small cells to commit to fewer divisions', 'corpus_id': 190874494, 'score': 1}, {'doc_id': '233874168', 'title': 'How Many Is Enough? - Challenges of Multinucleated Cell Division in Malaria Parasites', 'abstract': 'Regulating the number of progeny generated by replicative cell cycles is critical for any organism to best adapt to its environment. Classically, the decision whether to divide further is made after cell division is completed by cytokinesis and can be triggered by intrinsic or extrinsic factors. Contrarily, cell cycles of some species, such as the malaria-causing parasites, go through multinucleated cell stages. Hence, their number of progeny is determined prior to the completion of cell division. This should fundamentally affect how the process is regulated and raises questions about advantages and challenges of multinucleation in eukaryotes. Throughout their life cycle Plasmodium spp. parasites undergo four phases of extensive proliferation, which differ over three orders of magnitude in the amount of daughter cells that are produced by a single progenitor. Even during the asexual blood stage proliferation parasites can produce very variable numbers of progeny within one replicative cycle. Here, we review the few factors that have been shown to affect those numbers. We further provide a comparative quantification of merozoite numbers in several P. knowlesi and P. falciparum parasite strains, and we discuss the general processes that may regulate progeny number in the context of host-parasite interactions. Finally, we provide a perspective of the critical knowledge gaps hindering our understanding of the molecular mechanisms underlying this exciting and atypical mode of parasite multiplication.', 'corpus_id': 233874168, 'score': 1}, {'doc_id': '233460971', 'title': 'The Consequences of Budding versus Binary Fission on Adaptation and Aging in Primitive Multicellularity', 'abstract': 'Early multicellular organisms must gain adaptations to outcompete their unicellular ancestors, as well as other multicellular lineages. The tempo and mode of multicellular adaptation is influenced by many factors including the traits of individual cells. We consider how a fundamental aspect of cells, whether they reproduce via binary fission or budding, can affect the rate of adaptation in primitive multicellularity. We use mathematical models to study the spread of beneficial, growth rate mutations in unicellular populations and populations of multicellular filaments reproducing via binary fission or budding. Comparing populations once they reach carrying capacity, we find that the spread of mutations in multicellular budding populations is qualitatively distinct from the other populations and in general slower. Since budding and binary fission distribute age-accumulated damage differently, we consider the effects of cellular senescence. When growth rate decreases with cell age, we find that beneficial mutations can spread significantly faster in a multicellular budding population than its corresponding unicellular population or a population reproducing via binary fission. Our results demonstrate that basic aspects of the cell cycle can give rise to different rates of adaptation in multicellular organisms.', 'corpus_id': 233460971, 'score': 0}]
21	{'doc_id': '19155898', 'title': 'Injuries at the myotendinous junction.', 'abstract': 'The complete role of the myotendinous junction is discussed in this article. The morphology and function of the junction, typical injuries occurring at this region, delayed-onset muscle soreness, and muscle strain injury are described. Muscle strain injury is covered in detail, including characterization, treatment, and prevention. Clinical information is conveyed as well as basic science study results pertinent to the clinical situation.', 'corpus_id': 19155898}	10257	[{'doc_id': '216336157', 'title': 'Basic Muscle Physiology in Relation to Hamstring Injury and Repair', 'abstract': 'A hamstring strain injury has long been considered as a skeletal muscle injury, and the connective tissue associated with the muscle, i.e., the tendon/aponeurosis, has not been addressed sufficiently. A hamstring strain is a traumatic injury which very often occurs at the myotendinous junction (MTJ), which is the interface between the muscle and the tendon. The MTJ is formed during development and animal research clearly shows the interdependence and interaction between the muscle-derived and the connective tissue-derived cells during developmental processes. Additionally, several key molecules are indispensable for the MTJ formation. Although the MTJ is the most affected site after a strain injury, the research in adaptations to loading, unloading, and regeneration of the human MTJ is scarce. Skeletal muscle has a pronounced healing capacity, but the connective tissue in contrast is associated with a long repair period and incomplete repair. The differences in tissue healing and regeneration may complicate MTJ repair after hamstring strain injuries and might be the underlying factor why these sports injuries have a high recurrence rate. Re-injuries might occur as the repaired tissues have inferior mechanical properties as it is often described when scar tissue forms following tissue damage. Prolonged inflammation has been tightly associated with scar formation in several tissues, and recent data on human strain injuries support the idea that inflammation is present for an extended time after strain injuries.', 'corpus_id': 216336157, 'score': 1}, {'doc_id': '222341252', 'title': 'Media Dysfunction is a Key Initiator in Pathogenesis of Atherosclerosis', 'abstract': 'Atherosclerosis, a chronic lesion of vascular wall, remains a leading cause of death and loss of life years. Classical hypotheses for atherosclerosis are long-standing mainly to explain atherogenesis. Unfortunately, these hypotheses may not explain the variation in the susceptibility to atherosclerosis. These issues are controversial over the past 150 years. Atherosclerosis from human coronary arteries was examined and triangle of media was found to be a true portraiture of cells injury in the media, and triangle of intima was a true portraiture of myofibroblast proliferation, extracellular matrix (ECM) secretion, collagen fiber formation and intimal thickening to repair media dysfunction. Myofibroblasts, ECM and lumen (intima)/vasa vasorum (VV) (adventitia) constitute granulation tissue repair. With granulation tissue hyperplasia, lots of collagen fibers (normal or denatured), foam cells and new capillaries formed. Thus, the following theory was postulated: Risk factors induce smooth muscle cells (SMCs) injury/loss, and fibrosis or structure destruction could be developed in the media, which lead to media dysfunction. Media dysfunction prompts disturbed mechanical properties of blood vessels, resulting in bigger pressure buildup in the intima and adventitia. Granulation tissues in the intima/adventitia develop to repair the injured media. Atherosclerosis, stiffening or aneurysm develops depending on media dysfunction severity and granulated tissue repair mode/degree. Nearly all characteristics of clinical atherosclerosis could be ideally interpreted with the theory and Bernoulli equation. We believe that media dysfunction is a key initiator in the pathogenesis of atherosclerosis. It is expected that media dysfunction theory of atherosclerosis, should offer better understanding of the etiology for atherosclerosis.', 'corpus_id': 222341252, 'score': 0}, {'doc_id': '221293088', 'title': 'Evaluating the Effect of Crutch-using on Trunk Muscle Loads', 'abstract': 'As a traditional tool of external assistance, crutches play an important role in society. They have a wide range of applications to help either the elderly and disabled to walk or to treat certain illnesses or for post-operative rehabilitation. But there are many different types of crutches, including shoulder crutches and elbow crutches. How to choose has become an issue that deserves to be debated. Because while crutches help people walk, they also have an impact on the body. Inappropriate choice of crutches or long-term misuse can lead to problems such as scoliosis. Previous studies were mainly experimental measurements or the construction of dynamic models to calculate the load on joints with crutches. These studies focus only on the level of the joints, ignoring the role that muscles play in this process. Although some also take into account the degree of muscle activation, there is still a lack of quantitative analysis. The traditional dynamic model can be used to calculate the load on each joint. However, due to the activation of the muscle, this situation only causes part of the load transmitted to the joint, and the work of the chair will compensate the other part of the load. Analysis at the muscle level allows a better understanding of the impact of crutches on the body. By comparing the levels of activation of the trunk muscles, it was found that the use of crutches for walking, especially a single crutch, can cause a large difference in the activation of the back muscles on the left and right sides, and this difference will cause muscle degeneration for a long time, leading to scoliosis. In this article taking scoliosis as an example, by analyzing the muscles around the spine, we can better understand the pathology and can better prevent diseases. The objective of this article is to analyze normal walking compared to walking with one or two crutches using OpenSim software to obtain the degree of activation of different muscles in order to analyze the impact of crutches on the body.', 'corpus_id': 221293088, 'score': 0}, {'doc_id': '224801130', 'title': 'Medial Injury/Dysfunction Induced Granulation Tissue Repair is the Pathogenesis of Atherosclerosis.', 'abstract': 'Atherosclerosis, a chronic lesion of vascular wall, remains a leading cause of death and loss of life years. Classical hypotheses for atherosclerosis are long-standing mainly to explain atherogenesis. Unfortunately, these hypotheses may not explain the variation in the susceptibility to atherosclerosis. These issues are controversial over the past 150 years. Atherosclerosis from human coronary arteries was examined and triangle of media was found to be a true portraiture of cells injury in the media, and triangle of intima was a true portraiture of myofibroblast proliferation, extracellular matrix (ECM) secretion, collagen fiber formation and intimal thickening to repair media dysfunction. Myofibroblasts, ECM and lumen (intima)/vasa vasorum (VV) (adventitia) constitute granulation tissue repair. With granulation tissue hyperplasia, lots of collagen fibers (normal or denatured), foam cells and new capillaries formed. Thus, the following theory was postulated: Risk factors induce smooth muscle cells (SMCs) injury/loss, and fibrosis or structure destruction could be developed in the media, which lead to media dysfunction. Media dysfunction prompts disturbed mechanical properties of blood vessels, resulting in bigger pressure buildup in the intima and adventitia. Granulation tissues in the intima/adventitia develop to repair the injured media. Atherosclerosis, stiffening or aneurysm develops depending on media dysfunction severity and granulated tissue repair mode/degree. Nearly all characteristics of clinical atherosclerosis could be ideally interpreted with the theory. We believe that media dysfunction is a key initiator in the pathogenesis of atherosclerosis. It is expected that media dysfunction theory of atherosclerosis, should offer better understanding of the etiology for atherosclerosis.', 'corpus_id': 224801130, 'score': 0}, {'doc_id': '3780760', 'title': 'The single-leg Roman chair hold is more effective than the Nordic hamstring curl in improving hamstring strength-endurance in Gaelic footballers with previous hamstring injury.', 'abstract': 'Poor hamstring strength-endurance is a risk factor for hamstring injuries. This study investigated the effectiveness of the single-leg Roman hold and Nordic hamstring curl in improving hamstring strength-endurance. Twelve Gaelic footballers (mean ± standard deviation age, height and mass were 25.17 ± 3.46 years, 179.25 ± 5.88 cm, 85.75 ± 4.75 kilo) with a history of hamstring injury were randomized into 2 groups that performed 6 weeks of either Nordic hamstring curl, or single-leg Roman chair hold training. The single-leg hamstring bridge (SLHB) was measured pre- and post- intervention. The Roman chair group showed a very likely moderate magnitude improvement on SLHB performance for both legs (23.7% for the previously injured leg [90% confidence interval 9.6% to 39.6%] and 16.9% for the non-injured leg [6.2% to 28.8%]). The Nordic curl group showed a likely trivial change in SLHB performance for the non-injured leg (-2.1% [-6.7% to 2.6%]) and an unclear, but possibly trivial change for the previously injured leg (0.3% [-5.6% to 6.6%]). The Roman chair group improved very likely more with a moderate magnitude in both the non-injured (19.5% [8.0% to 32.2%]) and the previously injured leg (23.3% [8.5% to 40.0%]) compared to the Nordic curl group. This study demonstrated that 6-weeks single-leg Roman chair training substantially improved SLHB performance, suggesting that it may be an efficacious strategy to mitigate hamstring (re-) injury risk. Conversely, 6-weeks Nordic curl training did not substantially improve SLHB performance, suggesting this may not be the intervention of choice for modifying this risk factor.', 'corpus_id': 3780760, 'score': 1}, {'doc_id': '25594242', 'title': 'Behavior of fascicles and the myotendinous junction of human medial gastrocnemius following eccentric strength training', 'abstract': 'This study is the first in which measurements of thickness, fascicle angle and length, and tendon elongation were combined to examine the impact of eccentric strength training on both muscle architecture and tendinous structures. Eighteen healthy male subjects were divided into an eccentric strength training group (n = 10) and a control group (n = 8). The training program consisted of 18 sessions of eccentric exercises over a 7‐week period. All subjects were tested at baseline and after the last training session. Using ultrasound imaging, the fascicle angle and length and thickness of the medial gastrocnemius (MG) were analyzed at rest (i.e., θp, Flp, and tp, respectively), at 50% of maximal voluntary contraction (MVC) (i.e., θ50, Fl50, and t50, respectively), and during MVC (i.e., θ100, Fl100, and t100, respectively). Tendon elongation (TE) was measured by tracking the proximal displacement of the myotendinous junction of the MG during ramp isometric contraction. During ramp isometric contraction, the slope of the load–deformation relationship of the gastrocnemius tendon above 50% MVC was defined as an index of stiffness. After training, muscle thickness and fascicle angle increased significantly at rest and during contraction, whereas fascicle length increased at rest and did not change during contraction. Furthermore, the stiffness of the gastrocnemius tendon increased significantly. The results suggest that the behavior of muscle architecture and tendon mechanical properties are affected differently by strength training. Muscle Nerve, 2009', 'corpus_id': 25594242, 'score': 1}, {'doc_id': '4972525', 'title': 'Remodeling of muscle fibers approaching the human myotendinous junction', 'abstract': 'The myotendinous junction (MTJ) is at high risk of strain injuries, due to high amounts of energy that is transferred through this structure. The risk of strain injury is significantly reduced by heavy resistance training (HRT), indicating a remodeling capacity of MTJ. We investigated the degree of remodeling of muscle fibers near the human MTJ. In 8 individuals, samples were taken from the semitendinosus and gracilis MTJ and they were stained immunohistochemically for myonuclei (DAPI), fibroblasts (TCF7L2), and satellite cells (CD56). A high portion of the muscle fibers adjacent to the MTJ contained a centrally located myonucleus (47 ± 8%, mean ± SD) and half of the muscle fibers were CD56 positive. The number of satellite cells and fibroblasts were not higher than what has previously been reported from muscle bellies. The immunohistochemical findings suggest that the rate of remodeling of muscle fibers near the MTJ is very high. The finding that there was no increased number of satellite cells and fibroblasts could be explained as a dynamic phenomenon. The effect of HRT should be evaluated in a randomized setting.', 'corpus_id': 4972525, 'score': 1}, {'doc_id': '209261871', 'title': 'Lower leg compression and its biomechanical effects on the soft tissues of the leg', 'abstract': 'Abstract Elastic compression of the lower leg is the traditional preventive and curative treatment of venous insufficiency. After presenting the medical strategies related to compression therapies, this chapter develops current advances in clinics as well as in engineering and outlines the most important knowledge arising from this review. Compression hosiery acts by providing pressure to the leg. Pressure generation using socks mainly depends not only on the stiffness and the size of the socks and the size of the leg but also on the leg morphology. In the case of bandages, the role of friction must be outlined, not only as the main factor in maintaining the bandage wrapped around the leg but also as an influencing factor in pressure generation. Besides generated pressure, response of superficial veins to compression also depends on the vein size and the fat stiffness. But mechanical assessments should not mask the importance of other factors such as muscular contraction or nurse formation. An important impact of these results would head toward an improved personalization of compression treatment.', 'corpus_id': 209261871, 'score': 0}, {'doc_id': '58664338', 'title': 'Razor hamstring curl and Nordic hamstring exercise architectural adaptations: Impact of exercise selection and intensity', 'abstract': 'To investigate knee flexor strength and biceps femoris long head (BFlh) architectural adaptations following two different Nordic hamstring exercise (NHE) interventions and one razor hamstring curl (RHC) intervention.', 'corpus_id': 58664338, 'score': 1}, {'doc_id': '222927136', 'title': 'Do Different Pathologies Affect the Relationship Between the Stiffness of the Plantar Fascia and the Function of the MTP Joint', 'abstract': 'DO DIFFERENT PATHOLOGIES AFFECT THE RELATIONSHIP BETWEEN THE STIFFNESS OF THE PLANTAR FASCIA AND THE FUNCTION OF THE MTP JOINT? Madeline Ryan Pauley Old Dominion University, 2020 Director: Dr. Stacie Ringleb Compared to healthy individuals, individuals with plantar fasciitis and diabetes experience material and structural property changes to soft tissues in the feet. The purpose of this study was to compare the relationship between material properties, power absorption, and energy storage characteristics to metatarsal power between healthy, plantar fasciitis symptomatic and asymptomatic, and diabetic participants. Investigating material change differences as well as energy storage and transfer trends in different pathology groups can lead to a better overall understanding of power transfer at the metatarsophalangeal joint (MTP). Participants were recruited for kinematic gait analysis and lower extremity shear wave elastography analysis and fell into subgroups of either having plantar fasciitis and having symptoms (PFS, n=11), plantar fasciitis without having symptoms (PFA, n=5), diabetic type 1 or 2 (DT1, n=7/DT2, n=8), or age-matched healthy controls (n=16). There was no significant difference between subgroups at either the plantar fascia (PF) proximal or distal region. PFS presented statistically significant (p=.02) reductions in the total range of motion consistent with prior literature. Insignificant differences in the Redistribution Ratio between subgroups, which is the ratio of total positive work performed by MTP joint musculature to the proximal joint musculature, suggests that work is performed about the MTP similarly in both eccentric and concentric motions. PFA was found to have a positive relationship between eccentric peak power and the PF proximal (r=.897, p=.003), as well as a negative relationship between concentric peak power and the PF distal stiffness (r=-.72, p=.044). These observations suggest that there may be an altered mechanism of moment execution in the plantarflexion propulsion movement in a PFA population.', 'corpus_id': 222927136, 'score': 0}]
22	{'doc_id': '2614028', 'title': 'Heuristic evaluation for games: usability principles for video game design', 'abstract': 'Most video games require constant interaction, so game designers must pay careful attention to usability issues. However, there are few formal methods for evaluating the usability of game interfaces. In this paper, we introduce a new set of heuristics that can be used to carry out usability inspections of video games. The heuristics were developed to help identify usability problems in both early and functional game prototypes. We developed the heuristics by analyzing PC game reviews from a popular gaming website, and the review set covered 108 different games and included 18 from each of 6 major game genres. We analyzed the reviews and identified twelve common classes of usability problems seen in games. We developed ten usability heuristics based on the problem categories, and they describe how common game usability problems can be avoided. A preliminary evaluation of the heuristics suggests that they help identify game-specific usability problems that can easily be overlooked otherwise.', 'corpus_id': 2614028}	4683	"[{'doc_id': '215744827', 'title': 'Computers in Secondary Schools: Educational Games', 'abstract': 'This entry introduces educational games in secondary schools. Educational games include three main types of educational activities with a playful learning intention supported by digital technologies: educational serious games, educational gamification, and learning through game creation. Educational serious games are digital games that support learning objectives. Gamification is defined as the use of ""game design elements and game thinking in a non-gaming context"" (Deterding et al. 2011, p. 13). Educational gamification is not developed through a digital game but includes game elements for supporting the learning objectives. Learning through game creation is focused on the process of designing and creating a prototype of a game to support a learning process related to the game creation process or the knowledge mobilized through the game creation process. Four modalities of educational games in secondary education are introduced in this entry to describe educational games in secondary education: educational purpose of entertainment games, serious games, gamification, and game design.', 'corpus_id': 215744827, 'score': 0}, {'doc_id': '215786140', 'title': 'Open and Cultural Data Games for Learning', 'abstract': 'Educators often seek ways to introduce gaming in the classroom in order to break the usual teaching routine, expand the usual course curriculum with additional knowledge, but mostly as a means to motivate students and increase their engagement with the course content. Even though the vast majority of students find gaming to be appealing and a welcome change to the usual teaching practice, many educators and parents doubt their educational value; in this paper, we discuss a card game designed to teach environmental matters to early elementary school students, using open data. We present a comparative study of how the game increased the students’ interest for the subject, as well as their performance and engagement to the course, compared with conventional teaching and a Prezi presentation used to teach the same content to other student groups.', 'corpus_id': 215786140, 'score': 1}, {'doc_id': '211519195', 'title': 'Benefits and Pitfalls of Using Capture the Flag Games in University Courses', 'abstract': ""The concept of Capture the Flag (CTF) games for practicing cybersecurity skills is widespread in informal educational settings and leisure-time competitions. However, it is not much used in university courses. This paper summarizes our experience from using jeopardy CTF games as homework assignments in an introductory undergraduate course. Our analysis of data describing students' in-game actions and course performance revealed four aspects that should be addressed in the design of CTF tasks: scoring, scaffolding, plagiarism, and learning analytics capabilities of the used CTF platform. The paper addresses these aspects by sharing our recommendations. We believe that these recommendations are useful for cybersecurity instructors who consider using CTF games for assessment in university courses and developers of CTF game frameworks."", 'corpus_id': 211519195, 'score': 1}, {'doc_id': '54104725', 'title': 'Fundamentals of Game Design', 'abstract': 'Now in its third edition, the classic book on game design has been completely revised to include the latest developments in the game industry. Readers will learn all the fundamentals of concept development, gameplay design, core mechanics, user interfaces, storytelling, and balancing. Theyll be introduced to designing for mobile devices and touch screens, as well as for the Kinect and motion-capture gameplay. Theyll learn how indie developers are pushing the envelope and how new business models such as free-to-play are influencing design. In an easy-to-follow approach, Adams offers a first-hand look into the process of designing a game, from initial concept to final tuning. This in-depth resource also comes with engaging end-of-chapter exercises, design worksheets, and case studies.', 'corpus_id': 54104725, 'score': 1}, {'doc_id': '216868139', 'title': 'Design and Implementation of Air Selection based Augmented Reality Serious Game for Learning Capability Analysis', 'abstract': 'Rising advancements and ICT have changed the way of life of society, every single logical zone are exploiting innovation to get a genuine improvement. Specialists understand the advantages of utilizing genuine games as a dependable device in psychoanalyst. Hence, the exploration looks at important issues in regards to Dyspraxia issue in youngsters and presents a similar report in the treatments strategies by utilizing a non autonomous riddle and by utilizing the game, a Serious Game created in the intension of helping kids suffering from Dyspraxia to enhance their engine aptitudes and deftness through innovation. The investigation of information results indicated that exist a critical distinction among the two strategies, demonstrating that youngsters spending time with Serious Game got little schedule in the movement running and furthermore enhanced execution.', 'corpus_id': 216868139, 'score': 0}, {'doc_id': '218486854', 'title': ""Discussion of digital gaming's impact on players' well-being during the COVID-19 lockdown"", 'abstract': ""This research discusses how to utilise digital gaming to support the well-being of its users and sustain their physical and mental health during the COVID-19 lockdown in which people's activities are limited. The published academic literature that is written in English and available for access on online databases was reviewed to develop key take-aways and a framework for discussing how to enhance people's well-being in the COVID-19 lockdown. Interaction with other players in virtual communities has been found to have a positive influence on the mental health of those suffering from a lack of societal connection. A framework for further research has also been developed that focuses on the critical situation of the COVID-19 lockdown,as this is an urgent topic with a huge impact on our health.Some gaming service providers have been proactive in redesigning game programming to be suitable for the lockdown situation, and this enables players to enjoy physical activities even at home."", 'corpus_id': 218486854, 'score': 1}, {'doc_id': '218684555', 'title': 'Towards Friendly Mixed Initiative Procedural Content Generation: Three Pillars of Industry', 'abstract': 'While the games industry is moving towards procedural content generation (PCG) with tools available under popular platforms such as Unreal, Unity or Houdini, and video game titles like No Man’s Sky and Horizon Zero Dawn taking advantage of PCG, the gap between academia and industry is as wide as it has ever been, in terms of communication and sharing methods. The authors have worked on both sides of this gap and in an effort to shorten it and increase the synergy between the two sectors have identified three design pillars for PCG using mixed-initiative interfaces. The three pillars are respect designer control, respect the creative process and respect existing work processes. Respecting designer control is about creating a tool that gives enough control to bring out the designer’s vision. Respecting the creative process concerns itself with having a feedback loop that is short enough, that the creative process is not disturbed. Respecting existing work processes means that a PCG tool should plug in easily to existing asset pipelines. As academics and communicators, it is surprising that publications often do not describe ways for developers to use our work or lack considerations for how a piece of work might fit into existing content pipelines.', 'corpus_id': 218684555, 'score': 0}, {'doc_id': '214802840', 'title': 'Generative Forensics: Procedural Generation and Information Games', 'abstract': 'Procedural generation is used across game design to achieve a wide variety of ends, and has led to the creation of several game subgenres by injecting variance, surprise or unpredictability into otherwise static designs. Information games are a type of mystery game in which the player is tasked with gathering knowledge and developing an understanding of an event or system. Their reliance on player knowledge leaves them vulnerable to spoilers and hard to replay. In this paper we introduce the notion of generative forensics games, a subgenre of information games that challenge the player to understand the output of a generative system. We introduce information games, show how generative forensics develops the idea, report on two prototype games we created, and evaluate our work on generative forensics so far from a player and a designer perspective.', 'corpus_id': 214802840, 'score': 0}, {'doc_id': '214728424', 'title': 'Baba is Y’all: Collaborative Mixed-Initiative Level Design', 'abstract': 'We present a collaborative mixed-initiative system for building levels for the puzzle game ""Baba is You"". Unlike previous mixed-initiative systems, Baba is Y’all is designed for collaborative asynchronous creation by multiple users over the internet. The system includes several AI-assisted features to help designers, including a level evolver and an automated player for playtesting. The level archives catalogues levels according to which mechanics are implemented and not implemented, allowing the system to ask users to design levels with specific combinations of mechanics. We describe the operation of the system and the results of small-scale informal user test, and discuss future development paths for this system as well as for collaborative mixed-initiative systems in general.', 'corpus_id': 214728424, 'score': 0}, {'doc_id': '214802710', 'title': 'Software Engineering For Automated Game Design', 'abstract': ""As we develop more assistive and automated game design systems, the question of how these systems should be integrated into game development workflows, and how much adaptation may be required, becomes increasingly important. In this paper we explore the impact of software engineering decisions on the ability of an automated game design system to understand a game's codebase, generate new game code, and evaluate its work. We argue that a new approach to software engineering may be required in order for game developers to fully benefit from automated game designers."", 'corpus_id': 214802710, 'score': 0}]"
23	"{'doc_id': '219322800', 'title': 'Side by side comparison of three fully automated SARS-CoV-2 antibody assays with a focus on specificity', 'abstract': ""Background: In the context of the COVID-19 pandemic, numerous new serological test systems for the detection of anti-SARS-CoV-2 antibodies have become available quickly. However, the clinical performance of many of them is still insufficiently described. Therefore we compared three commercial, CE-marked, SARS-CoV-2 antibody assays side by side. Methods: We included a total of 1,154 specimens from pre-COVID-19 times and 65 samples from COVID-19 patients ([≥]14 days after symptom onset) to evaluate the test performance of SARS-CoV-2 serological assays by Abbott, Roche, and DiaSorin. Results: All three assays presented with high specificities: 99.2% (98.6-99.7) for Abbott, 99.7% (99.2-100.0) for Roche, and 98.3% (97.3-98.9) for DiaSorin. In contrast to the manufacturers' specifications, sensitivities only ranged from 83.1% to 89.2%. Although the three methods were in good agreement (Cohen's Kappa 0.71-0.87), McNemar's test revealed significant differences between results obtained from Roche and DiaSorin. However, at low seroprevalences, the minor differences in specificity resulted in profound discrepancies of positive predictability at 1% seroprevalence: 52.3% (36.2-67.9), 77.6% (52.8-91.5), and 32.6% (23.6-43.1) for Roche, Abbott, and DiaSorin, respectively. Conclusion: We find diagnostically relevant differences in specificities for the anti-SARS-CoV-2 antibody assays by Abbott, Roche, and DiaSorin that have a significant impact on the positive predictability of these tests."", 'corpus_id': 219322800}"	11427	"[{'doc_id': '227070713', 'title': 'Multicenter evaluation of the Panbio COVID-19 Rapid Antigen-Detection Test for the diagnosis of SARS-CoV-2 infection', 'abstract': 'The standard RT-PCR assay for COVID-19 is laborious and time-consuming, limiting the availability of testing. Rapid antigen-detection tests are faster and less expensive; however, the reliability of these tests must be validated before they can be used widely. The objective of this study was to determine the reliability of the PanbioTM COVID-19 Ag Rapid Test Device (PanbioRT) (Abbott) for SARS-CoV-2 in nasopharyngeal swab specimens. This was a prospective multicenter study in ten Spanish university hospitals of patients from hospital units with clinical symptoms or epidemiological criteria for COVID-19. Patients whose onset of symptoms or exposure was more than 7 days earlier were excluded. Two nasopharyngeal exudate samples were taken to perform the PanbioRT and a diagnostic RT-PCR test. Among the 958 patients studied, 359 (37.5%) were positive by RT-PCR and 325 (33.9%) were also positive by the PanbioRT. Agreement was 95.7% (kappa score: 0.90). All 34 false-negative PanbioRT results were in symptomatic patients, with 23.5% of them at 6-7 days since the onset of symptoms and 58.8% presenting CT >30 values for RT-PCR, indicating a low viral load. Overall sensitivity and specificity for the PanbioRT were 90.5% and 98.8%, respectively. The PanbioRT provides good clinical performance as a point-of-care test, with even more reliable results for patients with a shorter clinical course of the disease or a higher viral load. While this study has had a direct impact on the national diagnostic strategy for COVID-19 in Spain, the results must be interpreted based on the local epidemiological context.', 'corpus_id': 227070713, 'score': 0}, {'doc_id': '226981672', 'title': 'Comparison of SARS-CoV-2 serological tests with different antigen targets', 'abstract': '\n                  Background\n                  These last months, dozens of SARS-CoV-2 serological tests have become available with varying performances. A major effort was completed to compare 17 serological tests available in April 2020 in Switzerland.\n               \n                  Methods\n                  In a preliminary phase, we compared 17 IgG, IgM, IgA and pan Ig serological tests including ELISA, LFA, CLIA and ECLIA on a panel of 182 sera, comprising 113 sera from hospitalized patients with a positive RT-PCR, and 69 sampled before 1\u2009st November 2019, expected to give a positive and negative results, respectively. In a second phase, the five best performing and most available tests were further evaluated on a total of 582 sera (178 and 404 expected positive and negative, respectively), allowing the assessment of 20 possible cross-reactions with other virus.\n               \n                  Results\n                  In the preliminary phase, among eight IgG/pan-Ig ELISA or CLIA/ECLIA tests, five had a sensitivity and specificity above 90% and 98% respectively, and on six IgM/IgA tests, only one was acceptable. Only one LFA test on three showed good performances for both IgG and IgM. For all the tests IgM and IgG aroused concomitantly. In the second phase, no tests showed particular cross-reaction. We observed an important heterogeneity in the development of the antibody response.\n               \n                  Conclusions\n                  The majority of the evaluated tests exhibited high performances of IgG/pan-Ig sensitivity and specificity to detect the serological response of moderately to critically ill hospitalized patients. The IgM and IgA tests showed mostly insufficient performance with no added value for the early diagnostic on the cohort tested in this study.\n               ', 'corpus_id': 226981672, 'score': 1}, {'doc_id': '226273936', 'title': 'Increasing both specificity and sensitivity of SARS-CoV-2 antibody tests by using an adaptive orthogonal testing approach', 'abstract': ""Background SARS-CoV-2 antibody tests have undergone a remarkable improvement in performance. However, due to the low seroprevalence in several areas, very high demands are made on their specificity. Furthermore, the low antibody-response in some individuals requires high test sensitivity to avoid underestimating true seroprevalence. Optimization of testing has been reported through lowering manufacturer cut-offs to improve SARS-CoV-2 assay sensitivity or by combining two tests to improve specificity at the cost of sensitivity. However, these strategies have thus far been used in isolation of each other. Methods To increase sensitivity, cut-offs of three commercially available SARS-CoV-2 automated assays (Roche, Abbott, and DiaSorin) were reduced according to published values in a pre-pandemic specificity cohort (n=1117) and a SARS-CoV-2 positive cohort (n=64). All three testing systems were combined in an orthogonal approach with a confirmatory test, which was one of the remaining automated assays or one of two commercial ELISAs directed against the spike protein receptor binding-domain (RBD) or the nucleocapsid antigen (NP). Results The modified orthogonal test strategy resulted in an improved specificity of at least 99.8%, often even 100%, in all 12 tested combinations with no significant decline in sensitivity. In our cohort, regardless of whether the assays were used for screening or confirmation, combining Roche and Abbott delivered the best overall performance (+~10% sensitivity compared to the single tests and 100% specificity). Conclusion Here we propose a novel orthogonal assay strategy that approaches 100% specificity while maintaining or even significantly improving the screening test's sensitivity."", 'corpus_id': 226273936, 'score': 1}, {'doc_id': '227060736', 'title': 'Clinical performance evaluation of a SARS-CoV-2 Rapid Antibody Test for determining past exposure to SARS-CoV-2', 'abstract': '\n                  Objectives\n                  The true prevalence and seropositivity of SARS-CoV-2 infection remains unknown, due to the number of asymptomatic infections and limited access to high-performance antibody tests. To fill this gap, the clinical performance of a point-of-care SARS-CoV-2 Rapid Antibody Assay, a chromatographic immunoassay for detection of IgM/IgG antibodies, in near-patient settings was assessed.\n               \n                  Methods\n                  42 Anti-SARS-Cov-2 positive (CoV+) and 92 Anti-SARS-Covid-2 negative (CoV-) leftover samples from before December 2019 were assessed, using the Elecsys® Anti-SARS-CoV-2 as the reference assay. Analytical specificity was tested using leftover samples collected before December 2019 from patients with common cold symptoms.\n               \n                  Results\n                  The SARS-CoV-2 Rapid Antibody Test was 100.0% (95% CI 91.59–100.00) sensitive and 96.74% (95% CI 90.77–99.32) specific, with 0.00% assay failure rate. No cross-reactivity was observed against the common cold panel. Method comparison was additionally conducted by two external laboratories, using 100 CoV+/275 CoV- samples, also comparing whole blood versus plasma matrix. The comparison demonstrated 96.00% positive/96.36% negative percent agreement for plasma with the Elecsys Anti-SARS-CoV-2 and 99.20% percent overall agreement between whole blood and EDTA plasma.\n               \n                  Conclusion\n                  The SARS-CoV-2 Rapid Antibody Test demonstrated similar performance to the manufacturer’s data and a centralized automated immunoassay, with no cross-reactivity with common cold panels.\n               ', 'corpus_id': 227060736, 'score': 0}, {'doc_id': '226399225', 'title': 'Comparison of seven commercial SARS-CoV-2 rapid Point-of-Care Antigen tests', 'abstract': 'Background Antigen point of care tests (AgPOCT) can accelerate SARS-CoV-2 testing. As first AgPOCT are becoming available, there is a growing interest in their utility and performance. Methods Here we compare AgPOCT products by seven suppliers: the Abbott Panbio COVID-19 Ag Rapid Test; the RapiGEN BIOCREDIT COVID-19 Ag; the Healgen Coronavirus Ag Rapid Test Cassette (Swab); the Coris Bioconcept Covid.19 Ag Respi-Strip; the R-Biopharm RIDA QUICK SARS-CoV-2 Antigen; the NAL von minden NADAL COVID19-Ag Test; and the Roche/SD Biosensor SARS-CoV Rapid Antigen Test. Tests were evaluated on recombinant nucleoprotein, cultured endemic and emerging coronaviruses, stored clinical samples with known SARS-CoV-2 viral loads (n=138), stored samples from patients with respiratory agents other than SARS-CoV-2 (n=100), as well as self-sampled swabs from healthy volunteers (n=35). Findings Limits of detection in six of seven tested products ranged between 2.08 X 106 and 2.88 X 107 copies per swab, the outlier at 1.58 X 1010 copies per swab. Specificities ranged between 98.53% and 100% in five products, with two outliers at 94.85% and 88.24%. False positive results were not associated with any specific respiratory agent. As some of the tested AgPOCT were early production lots, the observed issues with specificity are unlikely to persist. Interpretation The sensitivity range of most AgPOCT overlaps with viral load figures typically observed during the first week of symptoms, which marks the infectious period in the majority patients. AgPOCTs with a limit of detection that approximates the virus concentration above which patients are infectious may enable shortcuts in decision-making in various areas of healthcare and public health.', 'corpus_id': 226399225, 'score': 0}, {'doc_id': '226255252', 'title': 'Characteristics of three different chemiluminescence assays for testing for SARS-CoV-2 antibodies', 'abstract': 'Several tests based on chemiluminescence immunoassay techniques have become available to test for SARS CoV 2 antibodies. There is currently insufficient data on serology assay performance beyond 35 days after symptoms onset. We aimed to evaluate SARS CoV 2 antibody tests on three widely used platforms. A chemiluminescent microparticle immunoassay (CMIA; Abbott Diagnostics, USA), a luminescence immunoassay (LIA; Diasorin, Italy), and an electrochemiluminescence immunoassay (ECLIA; Roche Diagnostics, Switzerland) were investigated. In a multigroup study, sensitivity was assessed in a group of participants with confirmed SARS CoV 2 (n=145), whereas specificity was determined in two groups of participants without evidence of COVID 19 (i.e. healthy blood donors, n=191, and healthcare workers, n=1002). Receiver operating characteristic (ROC) curves, multilevel likelihood ratios (LR), and positive (PPV) and negative (NPV) predictive values were characterized. Finally, analytical specificity was characterized in samples with evidence of Epstein Barr virus (EBV) (n=9), cytomegalovirus (CMV) (n=7) and endemic common cold coronavirus infections (n=12) taken prior to the current SARS CoV 2 pandemic. The diagnostic accuracy was comparable in all three assays (AUC 0.98). Using the manufacturers cutoffs, the sensitivities were 90%, 95% confidence interval,[84,94] (LIA), 93% [88,96] (CMIA), and 96% [91,98] (ECLIA). The specificities were 99.5% [98.9,99.8]( CMIA) 99.7% [99.3,99,9] (LIA) and 99.9% [99.5,99.98] (ECLIA). The LR at half of the manufacturers cutoffs were 60 (CMIA), 82 (LIA), and 575 (ECLIA) for positive and 0.043 (CMIA) and 0.035 (LIA, ECLIA) for negative results. ECLIA had higher PPV at low pretest probabilities than CMIA and LIA. No interference with EBV or CMV infection was observed, whereas endemic coronavirus in some cases provided signals in LIA and/or CMIA. Although the diagnostic accuracy of the three investigated assays is comparable, their performance in low prevalence settings is different. Introducing gray zones at half of the manufacturers cutoffs is suggested, especially for orthogonal testing approaches that use a second assay for confirmation.', 'corpus_id': 226255252, 'score': 1}, {'doc_id': '226985026', 'title': 'Clinical performance of three fully automated anti‐SARS‐CoV‐2 immunoassays targeting the nucleocapsid or spike proteins', 'abstract': 'This study assesses the clinical performance of three anti‐SARS‐CoV‐2 assays, namely EUROIMMUN anti‐SARS‐CoV‐2 nucleocapsid (IgG) ELISA, Elecsys anti‐SARS‐CoV‐2 nucleocapsid (total antibodies) assay, and LIAISON anti‐SARS‐CoV‐2 spike proteins S1 and S2 (IgG) assay. One hundred and thirty‐seven coronavirus disease 2019 (COVID‐19) samples from 96 reverse‐transcription polymerase chain reaction confirmed patients were chosen to perform the sensitivity analysis. Non‐SARS‐CoV‐2 sera (n\u2009=\u2009141) with a potential cross‐reaction to SARS‐CoV‐2 immunoassays were included in the specificity analysis. None of these tests demonstrated a sufficiently high clinical sensitivity to diagnose acute infection. Fourteen days since symptom onset, we did not find any significant difference between the three techniques in terms of sensitivities. However, Elecsys performed better in terms of specificity. All three anti‐SARS‐CoV‐2 assays had equivalent sensitivities 14 days from symptom onset to diagnose past‐COVID‐19 infection. We also confirmed that anti‐SARS‐CoV‐2 determination before Day 14 is of less clinical interest.', 'corpus_id': 226985026, 'score': 1}, {'doc_id': '227224310', 'title': 'Evaluation of four commercial, fully automated SARS-CoV-2 antibody tests suggests a revision of the Siemens SARS-CoV-2 IgG assay', 'abstract': ""Objectives: Serological tests detect antibodies against Severe Acute Respiratory Syndrome Coronavirus 2 (SARS-CoV-2) in the ongoing coronavirus disease-19 (COVID-19) pandemic. Independent external clinical validation of performance characteristics is of paramount importance. Methods: Four fully automated assays, Roche Elecsys Anti-SARS-CoV-2, Abbott SARS-CoV-2 IgG, Siemens SARS-CoV-2 total (COV2T) and SARS-CoV-2 IgG (COV2G) were evaluated using 350 pre-pandemic samples and 700 samples from 245 COVID-19 patients (158 hospitalized, 87 outpatients). Results: All tests showed very high diagnostic specificity. Sensitivities in samples collected at least 14 days after disease onset were slightly lower than manufacturers' claims for Roche (93.04%), Abbott (90.83%), and Siemens COV2T (90.26%), and distinctly lower for Siemens COV2G (78.76%). Concordantly negative results were enriched for immunocompromised patients. ROC curve analyses suggest a lowering of the cut-off index for the Siemens COV2G assay. Finally, the combination of two anti-SARS-CoV-2 antibody assays is feasible when considering borderline reactive results. Conclusions: Thorough on-site evaluation of commercially available serologic tests for detection of antibodies against SARS-CoV-2 remains imperative for laboratories. The potentially impaired sensitivity of the Siemens COV2G necessitates a switch to the company's newly filed SARS-CoV-2 IgG assay (sCOVG) for follow-up studies. A combination of tests could be considered in clinical practice."", 'corpus_id': 227224310, 'score': 1}, {'doc_id': '226999598', 'title': 'Accuracy of Rapid Point-of-Care Antibody Test in patients with suspected or confirmed COVID-19', 'abstract': 'Objectives: To assess the real-world diagnostic accuracy of the Livzon point-of-care rapid test for antibodies to SARS-COV-2 Design: Prospective cohort study Setting: District general hospital in England Participants: 173 Patients and 224 hospital staff with a history of COVID-19 symptoms, and who underwent PCR and/or reference antibody testing for COVID-19. Interventions: The Livzon point-of-care (POC) lateral flow immunoassay rapid antibody test (IgM and IgG) was conducted at least 7 days after onset of symptoms and compared to the composite reference standard of PCR for SARS-COV-2 plus reference laboratory testing for antibodies to SARS-COV-2. The SARS-CoV-2 RT-PCR was tested using the available molecular technology during the study time (PHE laboratories, GeneXpert(R) system Xpert, Xpress SARS-CoV-2 and Source bioscience laboratory). All molecular platforms/assays were PHE/NHSE approved. The reference antibody test was the Elecsys Anti-SARS-CoV-2 assay (Roche diagnostics GmBH). Main outcome measures: Sensitivity and specificity of the rapid antibody test Results: The reference antibody test was positive in 190/268 (70.9%) of participants with a history of symptoms suggestive of COVID-19; in the majority (n=312) the POC test was taken 35 days or more after onset of symptoms. The POC antibody test had an overall sensitivity of 90.1% (292/328, 95% CI 86.3 - 93.1) and specificity of 100% (68/68, 95% CI 94.7 - 100) for confirming prior SARS-CoV-2 infection when compared to the composite reference standard. Sensitivity was 97.8% (89/92, 95% CI 92.3% to 99.7%) in participants who had been admitted to hospital and 84.4% (124/147, 95% CI 77.5% to 89.8%) in those with milder illness who had never been seen in hospital. Conclusions: The Livzon point-of-care antibody test had comparable sensitivity and specificity to the reference laboratory antibody test, so could be used in clinical settings to support decision-making about patients presenting with more than 10 days of symptoms of COVID-19.', 'corpus_id': 226999598, 'score': 0}, {'doc_id': '227216961', 'title': 'Automated Western immunoblotting detection of anti-SARS-CoV-2 serum antibodies.', 'abstract': ""ELISA and chemiluminescence serological assays for COVID-19 are currently incorporating only one or two SARS-CoV-2 antigens. We developed an automated Western immunoblotting as a complementary serologic assay for COVID-19. The Jess Simple Western system, an automated capillary-based assay was used, incorporating an inactivated SARS-CoV-2 lineage 20a strain as antigen, and IgT detection. In total, 602 sera were tested including 223 from RT-PCR-confirmed COVID-19 patients, 76 from patients diagnosed with seasonal HCoVs and 303 from coronavirus-negative control sera. We also compared this assay with the EUROIMMUN(R) SARS-CoV-2 IgG ELISA kit. Among 223 sera obtained from RT-PCR-confirmed COVID-19 patients, 180/223 (81%) exhibited reactivity against the nucleocapsid and 70/223 (31%) against the spike protein. Nucleocapsid reactivity was further detected in 9/76 (14%) samples collected from patients diagnosed with seasonal HCoVs and in 15/303 (5%) coronavirus-negative control samples. In the subset of sera collected more than 2 weeks after the onset of symptoms, the sensitivity was 94% and the specificity 93%, the latter value probably reflecting cross-reactivity of SARS-CoV-2 with other coronaviruses. The automated Western immunoblotting presented a substantial agreement (90%) with the compared ELISA (Cohen's Kappa=0.64). Automated Western immunoblotting may be used as a second line test to monitor exposition of people to HCoVs including SARS-CoV-2."", 'corpus_id': 227216961, 'score': 0}]"
24	{'doc_id': '219118986', 'title': 'Epidemiology of COVID-19 Among Children in China', 'abstract': 'This study examined the epidemiological characteristics and transmission patterns of 2135 pediatric patients with COVID-19 using a retrospective analytical approach. OBJECTIVE: To identify the epidemiological characteristics and transmission patterns of pediatric patients with the 2019 novel coronavirus disease (COVID-19) in China. METHODS: Nationwide case series of 2135 pediatric patients with COVID-19 reported to the Chinese Center for Disease Control and Prevention from January 16, 2020, to February 8, 2020, were included. The epidemic curves were constructed by key dates of disease onset and case diagnosis. Onset-to-diagnosis curves were constructed by fitting a log-normal distribution to data on both onset and diagnosis dates. RESULTS: There were 728 (34.1%) laboratory-confirmed cases and 1407 (65.9%) suspected cases. The median age of all patients was 7 years (interquartile range: 2–13 years), and 1208 case patients (56.6%) were boys. More than 90% of all patients had asymptomatic, mild, or moderate cases. The median time from illness onset to diagnoses was 2 days (range: 0–42 days). There was a rapid increase of disease at the early stage of the epidemic, and then there was a gradual and steady decrease. The disease rapidly spread from Hubei province to surrounding provinces over time. More children were infected in Hubei province than any other province. CONCLUSIONS: Children of all ages appeared susceptible to COVID-19, and there was no significant sex difference. Although clinical manifestations of children’s COVID-19 cases were generally less severe than those of adult patients, young children, particularly infants, were vulnerable to infection. The distribution of children’s COVID-19 cases varied with time and space, and most of the cases were concentrated in Hubei province and surrounding areas. Furthermore, this study provides strong evidence of human-to-human transmission.', 'corpus_id': 219118986}	11978	"[{'doc_id': '224805730', 'title': 'COVID-19 infection prevalence in pediatric population: Etiology, clinical presentation, and outcome', 'abstract': '\n Novel COVID-19 infections caused major morbidity and mortality globally in the adult age group. Likewise, SARS-COV-2 infections in children are highly risky in the selected patient population. We performed a focused literature search of published reports from December 1, 2019, till August 20, 2020. The aim was to explore the etiology, clinical presentations, and outcome of pediatric COVID-19 patients. Viral respiratory infections are associated with high societal costs for children. In addition, children with asymptomatic SARS-COV-2 infections can be a source of COVID-19 spread to parents and caregivers. The major reported risk factors for pediatric COVID-19 cases were close contact with a SARS-COV-2 positive family member, a history of travel, and/or living in endemic areas. Children with COVID-19 who required ICU care had various comorbidities, such as malignancy. As the pandemic evolved, multiple cases of multisystem inflammatory syndrome in children and adolescents temporarily related to covid-19 (MIS-C) were reported. A unique population is neonates born to COVID-19 affected mothers, as there is an urgent need to optimize their management and outcome during this rapidly evolving pandemic. The early identification of SARS-COV-2 infection in infants and children has important direct management effects in these children and public health implications because of the effects on disease transmission control measures.\n', 'corpus_id': 224805730, 'score': 0}, {'doc_id': '229207643', 'title': 'COVID 19: Children should be Treated Even in Absence of Symptoms', 'abstract': 'The current pandemic by COVID 19 leaves new teachings at every moment. One of them is that children (especially those from early childhood) have a viral load of COVID 19 up to 10 times higher than adults, even though they are, in their vast majority, asymptomatic. This is of enormous sanitary importance, since they are ""healthy"" carriers, who can transmit the disease. For this reason, the authors emphasize the need to treat this age group with nasal and oral carrageenan, in order to cut the chain of contagion.', 'corpus_id': 229207643, 'score': 1}, {'doc_id': '222216733', 'title': 'COVID-19 in children: current evidence and key questions', 'abstract': 'Purpose of review SARS-CoV-2 infection in children has been less well characterized than in adults, primarily due to a significantly milder clinical phenotype meaning many cases have gone undocumented by health professionals or researchers. This review outlines the current evidence of the epidemiology of infection in children, the clinical manifestations of disease, the role of children in transmission of the virus and the recently described hyperinflammatory syndrome observed later during the first phase of the pandemic. Recent findings International seroprevalence studies have found younger children to have lower prevalence of antibodies to SARS-CoV-2, indicating they have not been infected as much as adults. This may be due to shielding by school closures, or by a reduced susceptibility to infection, as indicated by a significantly lower attack rate in children than adults in household contact tracing studies. The most well recognized symptoms in adults of cough, fever, anosmia and ageusia are less frequent in children, who may often present with mild and nonspecific symptoms, or with gastrointestinal symptoms alone. Risk factors for severe disease in children include chronic lung, cardiac or neurological disease, and malignancy. However, the absolute risk still appears very low for these cohorts. A new hyperinflammatory syndrome has emerged with an apparent immune cause. Summary Important questions remain unanswered regarding why children have mild disease compared with adults; how children of different ages contribute to asymptomatic community transmission of the virus; and the pathophysiology of and most appropriate investigation and treatment strategies for the novel hyperinflammatory syndrome.', 'corpus_id': 222216733, 'score': 1}, {'doc_id': '227172135', 'title': 'COVID-19 in Pediatric Patients: A Focus on CHD Patients', 'abstract': 'Coronavirus disease 2019 (COVID-19) is a global pandemic caused by SARS-CoV-2 virus. As of the 30th of September 2020, around 34,000,000 cases have been reported globally. Pediatrics with underlying congenital heart disease represent a small yet a critical proportion of these patients. In general, the majority of infected children experience mild to moderate disease with significant interindividual variability in laboratory and radiographic findings. Nevertheless, in healthy children with COVID-19, cardiac involvement has been documented and is attributed to various causes. Myocarditis, arrhythmias, cardiogenic shock, and serious multisystem inflammatory syndrome in children are all encountered. Since COVID-19 is a recent novel disease and based on previous experience with respiratory infections, children with underlying congenital heart disease should be given special attention. To date, little data is available about COVID-19 presentation, complications, and appropriate treatment in this population. However, variable and inconsistent disease presentation and severity have been observed. This paper discusses COVID-19 course of illness in pediatric population with a special emphasis on the cardiac manifestations of the disease in healthy population and also on the disease course in congenital heart disease patients in particular.', 'corpus_id': 227172135, 'score': 0}, {'doc_id': '225050243', 'title': 'Clinical and Epidemiologic Analysis of COVID-19 Children Cases in Colombia PEDIACOVID', 'abstract': 'Objective: The COVID pandemic has affected Colombia with a high number of cases and deceases; however, no studies have been published regarding pediatric population. An epidemiologic analysis of the nationwide COVID register, therefore, is necessary to outline and describe the impact in such population. Methods: A retrospective analysis was made of the characteristics of a cohort of 5062 patients <18 years of age, until June 16, 2020, reported at the National Institute of Health—INS (https://www.ins.gov.co/News./Pages/Coronavirus.aspx), through the national public access database, with all subjects confirmed with COVID-19 or severe acute respiratory syndrome-CoV-2. Results: Reviewed on June 16, 2020, a total of 54,971 confirmed cases were reported nationwide for COVID-19, of which 5062 (9.2%) are cases in patients under 18 years of age. There was a statistically significant difference between groups; age was statistically significantly higher in the asymptomatic, compared with: deceased, severe and moderate cases; moreover, age was statistically significantly higher in the mild, compared with: deceased, severe and moderate. Statistically significant difference determined with one-way ANOVA was found between groups (F = 16.08, P < 0.001). Post hoc analysis reveals significant differences between groups, the age of patients at home (9.39 years) and those recovered (9.3 years) being significantly higher than those in intensive care unit (4.9 years), in hospital (6.1 years), or than the deceased (2.9 years). Conclusion: The results of this study show that, at the nationwide level, patients in more severe states (deceased, severe and moderate), are significantly younger than those in the milder state (asymptomatic and mild).', 'corpus_id': 225050243, 'score': 1}, {'doc_id': '225138420', 'title': 'CHARACTERISTICS and considerations in the medical treatment of COVID‐19 in children', 'abstract': 'It is rare for children to be in serious condition or die from coronavirus disease 2019 (COVID‐19) caused by the 2019 novel coronavirus (severe acute respiratory syndrome coronavirus 2 [SARS‐CoV‐2]) except for those with underlying diseases such as chronic lung disease (including asthma), cardiovascular disease, and immunosuppressive disease. Recently, patients with hyperinflammatory shock have been identified among children who are confirmed to have or are suspected of having SARS‐CoV‐2 infection. The presenting signs and symptoms are characterized by prolonged fever, abdominal pain, and cardiac involvement without any signs of pneumonia on chest computed tomography. However, it is uncertain at this time whether SARS‐CoV‐2 infection affects this syndrome. Compared with adults, quite a few children are asymptomatic even when infected with SARS‐CoV‐2, which could make these children serious sources of infection at home or in medical institutions. Considering these characteristics, it is important to take appropriate precautions during medical examinations and perform infection control in emergency departments to save the lives of both the children and adult patients. Most healthy children are suffering from huge stress due to restrictions against going outside and school closures as social means to control infection. It is possible that children are socially isolated when they come to the emergency department, and they might require mental or social support even if they are only complaining about their physical condition. Health‐care providers are required to examine the children’s circumstances carefully and cooperate with workers in other professions appropriately.', 'corpus_id': 225138420, 'score': 0}, {'doc_id': '222238071', 'title': 'Children and Adolescents With SARS-CoV-2 Infection', 'abstract': 'Background: There is limited information on severe acute respiratory syndrome virus 2 (SARS-CoV-2) infection in children. Methods: We retrieved data from the national database on SARS-CoV-2 infections. We studied in-family transmission. The level of viral load was categorized as high, moderate, or low based on the cycle threshold values. Results: We studied 203 SARS-CoV-2-infected children (median age: 11 years; range: 6 days to 18.4 years); 111 (54.7%) had an asymptomatic infection. Among the 92 children (45.3%) with coronavirus disease 2019 (COVID-19), 24 (26.1%) were hospitalized. Infants <1 year were more likely to develop COVID-19 (19.5% of all COVID-19 cases) (P-value = 0.001). There was no significant difference between viral load and age, sex, underlying condition, fever and hospitalization, as well as between type of SARS-CoV-2 infection and age, sex, underlying condition and viral load. Transmission from a household member accounted for 132 of 178 (74.2%) children for whom the source of infection was identified. An adult member with COVID-19 was the first case in 125 (66.8%) family clusters. Child-to-adult transmission was found in one occasion only. Conclusions: SARS-CoV-2 infection is mainly asymptomatic or mild during childhood. Adults appear to play a key role in spread of the virus in families. Most children have moderate or high viral loads regardless of age, symptoms or severity of infection. Further studies are needed to elucidate the role of children in the ongoing pandemic and particularly in light of schools reopening and the need to prioritize groups for vaccination, when COVID-19 vaccines will be available.', 'corpus_id': 222238071, 'score': 1}, {'doc_id': '222352864', 'title': 'COVID-19, children and non-communicable diseases: translating evidence into action', 'abstract': 'The world faces an existential, once in a lifetime pandemic due to a novel coronavirus (SARS-CoV-2) which has to date infected over 25\u2009million people across the world, with nearly 850\u2009000 deaths.1 The disease, labelled COVID-19 by the WHO, has now spread to almost all the countries of the world and crippled the global economy. While high-income countries have been able to tap into their resources and reserves, for many low-income and middle-income countries, rising unemployment, population lock downs and closure of businesses have inflicted crippling damage on fragile economies, with rising inequalities and worsening poverty.\n\nWhile early reports of the infection2 3 suggested that the infection may be generally mild in children with COVID-19, with general case fatality rate less than 1%, there are increasing reports of complications among children and adolescents.4 In addition, a recent series of cases with multisystem inflammatory response merits reconsideration of these risks.5 There are also clear signals of predictors for adverse outcomes from COVID-19 infections. The disease has disproportionately taken a toll among the elderly population in long-term care facilities, with many dying without even being tested for COVID-19 infection.6 There is clear evidence of excess mortality in subgroups, especially those with comorbidities, most commonly related to non-communicable diseases (NCDs), such as diabetes, hypertension, obesity, heart disease and cancer.7 The same appears to be true among paediatric COVID-19 infections. A systematic review analysed a total of 7780 paediatric COVID-19 positive cases globally, and found that patients with information on underlying conditions (n=655) included the following comorbidities: immunosuppression (30.5%), respiratory conditions (20%) and cardiovascular disease (14%).8 A recent report from the UK of 651 hospitalised children with COVID-19 from 260 hospitals identified comorbidities in 42% (276/651) of cases.9 Comorbidities most commonly associated with …', 'corpus_id': 222352864, 'score': 1}, {'doc_id': '225068415', 'title': 'Coronavirus-2019 Disease (COVID-19) in Children', 'abstract': 'Abstract COVID-19 disease affects all ages, but severe cases of the disease and mortality are very rarely seen among children. In most cases, they acquire the virus from their parents or from an another infected person. The exact reasons why the disease has a milder course in children is unknown but high numbers of Angiotensin Converting Enzyme-2 (ACE2) receptors, underdeveloped immune responses, cross-reaction with other viruses, protective effect of fetal hemoglobin and fewer outdoor activities as well as journeys, and nonexposure to air pollution, and smoking. Although many cases are asymptomatic, they can still shed the virus. Materno-fetal vertical transmission has not been shown so far. In symptomatic cases, clinical findings include fever and respiratory symptoms, followed by diarrhea and vomiting. There are signs indicating a possible association between Kawasaki disease and COVID-19. Clinical findings and diagnostic procedures in newborns, and older children are similar. Supportive therapy is essential and antiviral agents are not required in most cases. During cytokine storm, anti-inflammatory treatments may be tried. There is no evidence for transmission through breastmilk; therefore infected mothers should breastfeed their infants by taking all precautions. Routine immunizations of children should not be deferred during COVID-19 outbreak period. Psychological support for children who need to stay at home and for healthcare personnel should be provided.', 'corpus_id': 225068415, 'score': 0}, {'doc_id': '226496140', 'title': 'SARS-CoV-2 infection in children/ Çocuklarda sars-cov-2 enfeksiyonu', 'abstract': 'SARS-CoV-2, a RNA virus that emerged in December 2019 in the city of Wuhan in China and took hold of the whole world, affects children as well as all age groups. In our country, we started to observe the first cases by March 2020. SARS-CoV-2, which is transmitted by droplets and by way of contact with surfaces contaminated by these droplets, is generally transmitted to children from adults through close contact. There is no proven information about other transmission routes such as fecal-oral transmission. Similar to adults, the primary symptoms at presentation include fever, cough, sore throat, malaise, nasal dis-charge, and rarely, vomiting and diarrhea in children. Although the majority of pediatric patients are asymptomatic or have a mild clinical course, severe cases have been reported in children with underlying chronic diseases. There is currently no specific antiviral treatment against the SARS-CoV-2 virus. Supportive treatment is recommended in children with a mild course, and some treatments are recommended in children with comorbidities or in children who are observed to have a more severe course. Asymptomatic pediatric patients or pediatric patients who have a mild course constitute an important group in terms of transmission of the infection to the advanced age group who carry high risk. Prevention of infection is very important in terms of reducing new cases and alleviating the load on the health-care system. In order to prevent transmission of SARS-CoV-2, hygienic rules should be pursued in the community, social distancing should be observed, and the family members and contacts of patients who have been diagnosed should be screened and isolated.', 'corpus_id': 226496140, 'score': 0}]"
25	{'doc_id': '222124337', 'title': 'Gender Differences in Tertiary Education: What Explains STEM Participation?', 'abstract': 'The share of women achieving tertiary education has increased rapidly over time and now exceeds that of men in most OECD countries. However, women are severely under-represented in mathsintensive science fields, which are generally referred to as STEM (science, technology, engineering, and maths). The under-representation of women in these subject areas has received a great deal of attention. This is because these fields are seen to be especially important for productivity and economic growth and are associated with occupations that have higher earnings. Subject of degree is an important part of the explanation for the gender wage gap. The aim of this paper is to review evidence on explanations for the STEM gap in tertiary education. This starts with statistics about background context and evidence on how well-prepared male and female students may be for studying STEM at a later stage. I then discuss what the literature has to say about the role of personal attributes: namely confidence, self-efficacy and competitiveness and the role of preferences and expectations. I go on to discuss features of the educational context thought to be important for influencing attributes and preferences (or mediating their effects): peers; teachers; role models; and curriculum. I then briefly discuss broader cultural influences. I use the literature reviewed to discuss policy implications.', 'corpus_id': 222124337}	9969	"[{'doc_id': '207943355', 'title': 'Increasing gender diversity in the STEM research workforce', 'abstract': ""Policies must address harassment and bias Women experience substantial, gender-specific barriers that can impede their advancement in research careers. These include unconscious biases that negatively influence the perception of women's abilities, as well as social and cultural factors like those that lead to an unequal distribution of domestic labor (1, 2). Additionally, sexual and gender-based harassment is a widespread and pernicious impediment to the retention and advancement of women in many science, technology, engineering, and mathematics (STEM)–related fields (3). Although there is substantial evidence documenting systemic barriers that women face in scientific careers, less is known about how research institutions and funding agencies can best address these problems (see references below and in the supplementary materials). We outline here specific, potentially high-impact policy changes that build upon existing mechanisms for research funding and governance and that can be rapidly implemented to counteract barriers facing women in science. These approaches must be coupled to vigorous and continuous outcomes-based monitoring, so that the most successful strategies can be disseminated and widely implemented. Though our professional focus is primarily academic biomedical research in U.S. institutions, we suggest that some of the approaches that we discuss may be broadly useful across STEM disciplines and outside of academia as well."", 'corpus_id': 207943355, 'score': 1}, {'doc_id': '216898013', 'title': 'SPOC-supported course for promoting gender diversity in ICT careers', 'abstract': 'The gender imbalance in Information and Communication Technology (ICT) careers is an enduring issue (European Commission 2013). The gender gap in ICT is mainly due to the masculine culture in education and work environments within the field (Faulkner 2011, Margolis&Fisher 2003) and to the persistent gender stereotypes that depict women as technologically incompetent (Clayton et al. 2012). Previous research conducted on our campus, which hosts a business and an engineering school (Telecom Ecole de Management and Telecom SudParis), has shown that despite initiatives for equality, the ICT & gender stereotype is still prevalent among students. However, our results also give cause for optimism in so far as mechanisms to alter the status quo exist and are mobilised (McDonnell&Morley 2015,Morley&McDonnell 2016). With a view to deconstructing the ICT & gender stereotype in our students‘representations, we have developed an experimental online course, and more specifically a SPOC (Small_Private_Online_Course). Our students come from varied cultural and social backgrounds, and differ widely concerning customs, religion, family values and personal experience. We thus wanted to design a course that would take into account individual differences, in particular those concerning gender knowledge and personal ability to get involved in classroom discussions. Our ten-week course, entitled ""Feminine-Masculine in the digital world - a journey of discovery"", is in three parts. The first helps students discover the little known role of women in the history of computer science (1950-1970) in the United States and Europe, and comprehend when and why women were removed from the IT field. In the second part, students are led to question the gendering of a profession while discovering women\'s involvement in IT occupations today (CIOs, CEOs and entrepreneurs in the digital sector etc.) and the gender balance in ICT jobs in Malaysia. The third part of the course focuses on stereotypes in the ICT world (how they work, what are the effects, what has changed today...), and students learn how to recognize and deconstruct these stereotypes. The course provides a framework for analysing stereotyped representations of ICT (in advertisements, posters, etc.) from a gender perspective. The online lectures were delivered via videos, and each week, students were assigned a writing activity to be completed either individually, in small groups, or in a forum. At the end of ten online sessions, two face-to-face sessions were organised, during the first students presented their final group assignment (stereotype analysis of a document dealing with ICT), and during the second did the final written exam. The course was launched in January 2016, and a second session in September, with about 40 students attending each session. Both have been greatly appreciated by students. The communication presents the results of our study of this experimental course aimed at reducing gender inequality using an innovative teaching technique. We have first assessed our pedagogy against the recommendations issued by researchers working on community of inquiry for an effective commitment of learners (Garrison&al. 2009, Pelz 2010), and we examine if and how the online course has been inclusive. Then, we have tried to identify whether our learning goals have been achieved. Our central research questions were: Has the course contributed to weakening the gender stereotype that links technology and masculinity in the participating students\' representations? Do female participants feel more empowered in the IT field? Have male participants become more aware of gender stereotypes in IT? Have students developed critical thinking on gender issues? Finally, we discuss the contribution of an online course to teaching gender issues, compared to face-to-face teaching, and the specific role of online teachers.', 'corpus_id': 216898013, 'score': 1}, {'doc_id': '74262967', 'title': 'Professor, member of the Academy of (Medical) Sciences, Igor Dmitrievich Kirpatovsky and his scientific heritage', 'abstract': 'Academician Igor Dmitrievich Kirpatovsky created a scientific school at the Department of Operative Surgery at the Russian People’s Friendship University. Unique studies have been conducted in various areas of medicine and science: vascular and abdominal surgery; microsurgery; traumatology and orthopedics; clinical anatomy and relief anatomy; nervous and endocrine transplantation; andrology transplantation; experiments in the area of renal transplantation, small intestine and limb transplantation; transplantation immunology.', 'corpus_id': 74262967, 'score': 0}, {'doc_id': '67774390', 'title': 'Gender Diversity in STEM Disciplines: A Multiple Factor Problem', 'abstract': 'Lack of diversity, and specifically, gender diversity, is one of the key problems that both technological companies and academia are facing these days. Moreover, recent studies show that the number of female students enrolled in science, technology, engineering and mathematics (STEM) related disciplines have been decreasing in the last twenty years, while the number of women resigning from technological job positions remains unacceptably high. As members of a higher education institution, we foresee that working towards increasing and retaining the number of female students enrolled in STEM disciplines can help to alleviate part of the challenges faced by women in STEM fields. In this paper, we first review the main barriers and challenges that women encounter in their professional STEM careers through different age stages. Next, we focus on the special case of the information theory field, discussing the potential of gendered innovation, and whether it can be applied in the Information Theory case. The working program developed by the School of Engineering at the University of Valencia (ETSE-UV), Spain, which aims at decreasing the gender diversity gap, is then presented and recommendations for practice are given. This program started in 2011 and it encompasses Bachelor, Master and PhD levels. Four main actions are implemented: Providing institutional encouragement and support, increasing the professional support network, promoting and supporting the leadership, and increasing the visibility of female role models. To assess the impact of these actions, a chi-square test of independence is included to evaluate whether there is a significant effect on the percentage of enrolled female students. The percentage of graduated female students in the information and Communications Technology Field is also positioned with respect to other universities and the Spanish reference value. This analysis establishes that, in part, this program has helped to achieve higher female graduation rates, especially among Bachelor students, as well as increasing the number of top-decision positions held by faculty women.', 'corpus_id': 67774390, 'score': 1}, {'doc_id': '222508352', 'title': 'Pre-Service Mathematics Teachers’ Levels of Academic Procrastination and Online Learning Readiness', 'abstract': 'This article aims to examine the relationship between the online learning readiness and academic procrastination behaviors of the pre-service mathematics teachers. In line with this research purpose, it is examined whether the online learning readiness and academic procrastination differentiate with regard to demographic variables such as gender, grade levels...etc.; and as well as that to technical problems occurs during the online learning process, the last minute course and the instructors’ impact during the course study process. This research has been conducted with 314 pre-service mathematics teachers that currently attending the Faculty of Education of different universities in Turkey. The relevant analyses revealed that there is a low-level significant relationship between the academic procrastination tendency and online learning readiness. Besides, the scores of academic procrastination tendency and online learning readiness are found higher among males, first graders, those who have access problems and those who think an instructor is a determinant. The research findings are discussed within the light of related literature.', 'corpus_id': 222508352, 'score': 0}, {'doc_id': '100765549', 'title': 'Evolution and effects of surface chemistry of activated coke during combined SO_2/NO_xremoval process', 'abstract': 'The evolution of surface chemistry of activated coke during combined SO2/NOx removal process can be used to elucidate the mechanism of flue gas purification process.The experiments on combined desulfurization/denitration of the flue gas were conducted in a micro reactor.The effect of trace SO2 left in the flue gas after desulfurization on removal of NO by the selective catalytic reduction(SCR),as well as the influence of denitration on followed desulfurization were investigated.The X-ray photoelectron spectroscopy(XPS) and the scanning electron microscope(SEM) were used to characterize the surface properties and morphology of activated coke before and after desulfurization/denitration.The results show that the depleting adsorbed NH3 consumed by reaction with SO2 is the main reason for decreasing NO conversion.The desulfurization efficiency of activated coke are strengthened because of the decreasing acidic surface functional groups resulting from the decrease of oxygen content,delocalization of the π-electron of carbon,and the introduction of N-containing compounds onto the surface of activated coke resulting in the enhancement of basic surface functional groups.', 'corpus_id': 100765549, 'score': 0}, {'doc_id': '221806683', 'title': 'Advancing Enterprise Education for Women in Science, Technology, Engineering and Mathematics (STEM) among MENA Countries-G20 Insights', 'abstract': 'This Policy Brief is offered to the Saudi T20 process, as a recommendation to the G20 in 2020. In 2020, Saudi Arabia is host of the G20 Summit during the unprecedented COVID-19 pandemic and calls for social and political transformation among MENA countries. Calls for reform include the need to strengthen enterprise or entrepreneurship education for women in science, technology, engineering, and mathematics (STEM). This need is evidenced in below average performance of MENA countries among 80 countries surveyed in the OECD (2018) Program for International Student Assessment. The assessment measured 15year-old students’ reading, mathematics and science literacy, as well as problem solving and skills to ‘meet real-life challenges.’ With respect to enterprise education, an UNESCO (2013, p. 18) study of Arab countries recommended, “a national strategy for the integration of entrepreneurship education in the educational and training systems and curricula during general education, technical or professional education and higher education.” Similar recommendations are advanced by the World Economic Forum (2010). Challenges facing women in STEM are also reported. While agencies, such as UNESCO (2017) and the World Bank (2020), report that the engagement of women in STEM across MENA countries is high (relative to more developed countries) few women graduates pursue STEM careers. Thirteen of 15 countries with the lowest rate of female participation in the workforce are Arab countries (World Bank, 2020). This reflects ‘lost’ economic opportunity and return on investment from STEM education. In recognizing the importance of STEM credentials as the foundation for employment in the digital economy, associated challenges are twofold. First, is a need to imbed enterprise education in STEM curricula to help students develop entrepreneurial attitudes, skills and competencies, and to improve their abilities to meet ‘real life challenges’. Second, entrepreneurship education within STEM disciplines will encourage women to pursue business startup as a career option, and enhance their ability to scale and growth their businesses. Such measures will contribute to women’s economic empowerment and economic growth across the MENA region. To this extent, this Policy Brief addresses the lack of: Formalized enterprise curricula in MENA educational and training systems Enterprise education within STEM disciplines within most MENA states Small business support services beyond startup within most MENA states Small business support services for startups operating within MENA’s STEM sectors Mentors and role models for women STEM entrepreneurs in MENA states. The policy brief informs an issue of significant importance to the G20 economies, and in particular, MENA states by advancing strategies to: Educate and train a billion people in the next decade. The focus on entrepreneurship and STEM will prepare young women for careers in the Fourth Industrial Revolution. Build capacity and mobilize knowledge needed to create businesses to drive economic growth in the Fourth Industrial Revolution.', 'corpus_id': 221806683, 'score': 0}, {'doc_id': '210119153', 'title': 'A Theory of Change for Improving Children’s Perceptions, Aspirations and Uptake of STEM Careers', 'abstract': 'There is concern about the low numbers and diversity of young people choosing careers and study subjects in science, technology, engineering and maths (STEM) at university and beyond. Many interventions aimed at addressing this issue have focused on young people aged 14+ years old. However, these interventions have resulted in little improvement in the numbers and diversity of young people progressing into STEM careers. The aim of this study is to ask “What are the affordances of a Theory of Change (ToC) for increasing the diversity and number of young people choosing a career in STEM post-18?” An innovative ToC is introduced which provides the theoretical underpinnings and context for the complex mix of interventions necessary to lead to a significant change in the number and diversity of those choosing STEM careers. Case studies of interventions developed using the ToC are presented. This approach, and associated ToC, is widely applicable across STEM, education and public engagement fields.', 'corpus_id': 210119153, 'score': 1}, {'doc_id': '221506642', 'title': ""The Impact Of The Secondary School Students' Integrationin Distance Learning With Its Various Platforms On Their Achievement From Their Point Of View In Na'ourdistrict Directorate"", 'abstract': '2020 The study aimed to identify the impact of the integration of secondary school students in distance learning with its various platforms on their achievement from their point of view in Na\'ourDistrict Directorate. To achieve the goal of the study, the descriptive approach was used by constructing a questionnaire, the validity and reliability of the study were verified, and the study sampleconsisted of 918 male and female students from the first and second secondary students.The study concluded the following results: there is a statistically significant effect of the level of integration of secondary school students through distance learning on the total scorein academic achievement, and integration accounts for (59.2%) of achievement. As for the domains level, the results showed that there was a statistically significant effect of the dimensions related to the student and the ones related to the teacher in academic achievement and the absence ofthe effect of the technical related dimension. The results also showed that the extent of secondary stage students\'integration in distance learning with its various platforms in Na\'our District Directorate wasvery high, the student field occupied ""the first place, and in the second place came technical – aspect related dimension."", in the third and last place came the aspects related to teacher dimension"", the results also showed that there are statistically significant differences regardingthe extent of the integration of secondary students taught through distance learning at the total score as well as fields according to variables (gender, class, educational platform).The differences were in favor of females, the second secondary class , Noorspace platform. In the light of the results, a number of recommendations were presented, the most important of which are: the necessity of encouraging secondary stage students, especially the second secondary students to follow educational platforms to improve achievement, to understand academic subjects, and to be less dependent on costlyprivate lessons. Another recommendation isteacher training on the optimal use of synchronous and asynchronous distance learning.', 'corpus_id': 221506642, 'score': 0}, {'doc_id': '28577905', 'title': ""Why do women opt out? Sense of belonging and women's representation in mathematics."", 'abstract': ""Sense of belonging to math-one's feelings of membership and acceptance in the math domain-was established as a new and an important factor in the representation gap between males and females in math. First, a new scale of sense of belonging to math was created and validated, and was found to predict unique variance in college students' intent to pursue math in the future (Studies 1-2). Second, in a longitudinal study of calculus students (Study 3), students' perceptions of 2 factors in their math environment-the message that math ability is a fixed trait and the stereotype that women have less of this ability than men-worked together to erode women's, but not men's, sense of belonging in math. Their lowered sense of belonging, in turn, mediated women's desire to pursue math in the future and their math grades. Interestingly, the message that math ability could be acquired protected women from negative stereotypes, allowing them to maintain a high sense of belonging in math and the intention to pursue math in the future."", 'corpus_id': 28577905, 'score': 1}]"
26	"{'doc_id': '4564962', 'title': 'Gamification. using game-design elements in non-gaming contexts', 'abstract': '""Gamification"" is an informal umbrella term for the use of video game elements in non-gaming systems to improve user experience (UX) and user engagement. The recent introduction of \'gamified\' applications to large audiences promises new additions to the existing rich and diverse research on the heuristics, design patterns and dynamics of games and the positive UX they provide. However, what is lacking for a next step forward is the integration of this precise diversity of research endeavors. Therefore, this workshop brings together practitioners and researchers to develop a shared understanding of existing approaches and findings around the gamification of information systems, and identify key synergies, opportunities, and questions for future research.', 'corpus_id': 4564962}"	4681	"[{'doc_id': '218719915', 'title': 'User Attention and Behaviour in Virtual Reality Art Encounter', 'abstract': 'With the proliferation of consumer virtual reality (VR) headsets and creative tools, content creators have started to experiment with new forms of interactive audience experience using immersive media. Understanding user attention and behaviours in virtual environment can greatly inform creative processes in VR. We developed an abstract VR painting and an experimentation system to study audience encounters through eye gaze and movement tracking. The data from a user experiment with 35 participants reveal a range of user activity patterns in art exploration. Deep learning models are used to study the connections between behavioural data and audience background. New integrated methods to visualise user attention as part of the artwork are also developed as a feedback loop to the content creator.', 'corpus_id': 218719915, 'score': 0}, {'doc_id': '219721421', 'title': 'On the environment-destructive probabilistic trends: a perceptual and behavioral study on video game players', 'abstract': ""Currently, gaming is the world's favorite form of entertainment. Various studies have shown how games impact players' perceptions and behaviors, prompting opportunities for purposes beyond entertainment. This study uses Animal Crossing: New Horizons (ACNH), a real-time life-simulation game, as a unique case study of how video games can affect humans' environmental perceptions. A dataset of 584 observations from a survey of ACNH players and the Hamiltonian MCMC technique has enabled us to explore the relationship between in-game behaviors and perceptions. The findings indicate a probabilistic trend towards exploiting the in-game environment despite players' perceptions, suggesting that the simplification of commercial game design may overlook opportunities to engage players in pro-environmental activities."", 'corpus_id': 219721421, 'score': 1}, {'doc_id': '219955919', 'title': 'Reflection in Game-Based Learning: A Survey of Programming Games', 'abstract': 'Reflection is a critical aspect of the learning process. However, educational games tend to focus on supporting learning concepts rather than supporting reflection. While reflection occurs in educational games, the educational game design and research community can benefit from more knowledge of how to facilitate player reflection through game design. In this paper, we examine educational programming games and analyze how reflection is currently supported. We find that current approaches prioritize accuracy over the individual learning process and often only support reflection post-gameplay. Our analysis identifies common reflective features, and we develop a set of open areas for future work. We discuss these promising directions towards engaging the community in developing more mechanics for reflection in educational games.', 'corpus_id': 219955919, 'score': 0}, {'doc_id': '215814147', 'title': 'SportsXR - Immersive Analytics in Sports', 'abstract': 'We wish to thank Coach Kathy Delaney-Smith, Mike Roux, Mark Kaliris, and Lindsay Werner at Harvard Women’s Basketball, and Mike Sotsky and Casey Brinn at Harvard Men’s Basketball for their time and expertise. This research is supported in part by King Abdullah University of Science and Technology (KAUST) and the KAUST Office of Sponsored Research (OSR) award OSR-2015-CCF-2533-01.', 'corpus_id': 215814147, 'score': 0}, {'doc_id': '220438671', 'title': 'HCI in Games: Second International Conference, HCI-Games 2020, Held as Part of the 22nd HCI International Conference, HCII 2020, Copenhagen, Denmark, July 19–24, 2020, Proceedings', 'abstract': 'General game-playing artificial intelligence (AI) has recently seen important advances due to the various techniques known as ‘deep learning’. However, in terms of human-computer interaction, the advances conceal a major limitation: these algorithms do not incorporate any sense of what human players find meaningful in games. I argue that adaptive game AI will be enhanced by a generalised player model, because games are inherently human artefacts which require some encoding of the human perspective in order to respond naturally to individual players. The player model provides constraints on the adaptive AI, which allow it to encode aspects of what human players find meaningful. I propose that a general player model requires parameters for the subjective experience of play, including: player psychology, game structure, and actions of play. I argue that such a player model would enhance efficiency of per-game solutions, and also support study of game-playing by allowing (within-player) comparison between games, or (within-game) comparison between players (human and AI). Here we detail requirements for functional adaptive AI, arguing from first-principles drawn from games research literature, and propose a formal specification for a generalised player model based on our ‘Behavlets’ method for psychologically-derived player modelling.', 'corpus_id': 220438671, 'score': 1}, {'doc_id': '219955784', 'title': '”And then they died”: Using Action Sequences for Data Driven, Context Aware Gameplay Analysis', 'abstract': 'Many successful games rely heavily on data analytics to understand players and inform design. Popular methodologies focus on machine learning and statistical analysis of aggregated data. While effective in extracting information regarding player action, much of the context regarding when and how those actions occurred is lost. Qualitative methods allow researchers to examine context and derive meaningful explanations about the goals and motivations behind player behavior, but are difficult to scale. In this paper, we build on previous work by combining two existing methodologies: Interactive Behavior Analytics (IBA) [2] and sequence analysis (SA), in order to create a novel, mixed methods, human-in-the-loop data analysis methodology that uses behavioral labels and visualizations to allow analysts to examine player behavior in a way that is context sensitive, scalable, and generalizable. We present the methodology along with a case study demonstrating how it can be used to analyze behavioral patterns of teamwork in the popular multiplayer game Defense of the Ancients 2 (DotA 2).', 'corpus_id': 219955784, 'score': 1}, {'doc_id': '219955873', 'title': 'Data-Driven Game Development: Ethical Considerations', 'abstract': 'In recent years, the games industry has made a major move towards data-driven development, using data analytics and player modeling to inform design decisions. Data-driven techniques are beneficial as they allow for the study of player behavior at scale, making them very applicable to modern digital game development. However, with this move towards data driven decision-making comes a number of ethical concerns. Previous work in player modeling [45] as well as work in the fields of AI and machine learning [9, 53] have demonstrated several ways in which algorithmic decision-making can be flawed due to data or algorithmic bias or lack of data from specific groups. Further, black box algorithms create a trust problem due to lack of interpretability and transparency of the results or models developed based on the data, requiring blind faith in the results. In this position paper, we discuss several factors affecting the use of game data in the development cycle. In addition to issues raised by previous work, we also raise issues with algorithms marginalizing certain player groups and flaws in the resulting models due to their inability to reason about situational factors affecting players’ decisions. Further, we outline some work that seeks to address these problems and identify some open problems concerning ethics and game data science.', 'corpus_id': 219955873, 'score': 1}, {'doc_id': '215744827', 'title': 'Computers in Secondary Schools: Educational Games', 'abstract': 'This entry introduces educational games in secondary schools. Educational games include three main types of educational activities with a playful learning intention supported by digital technologies: educational serious games, educational gamification, and learning through game creation. Educational serious games are digital games that support learning objectives. Gamification is defined as the use of ""game design elements and game thinking in a non-gaming context"" (Deterding et al. 2011, p. 13). Educational gamification is not developed through a digital game but includes game elements for supporting the learning objectives. Learning through game creation is focused on the process of designing and creating a prototype of a game to support a learning process related to the game creation process or the knowledge mobilized through the game creation process. Four modalities of educational games in secondary education are introduced in this entry to describe educational games in secondary education: educational purpose of entertainment games, serious games, gamification, and game design.', 'corpus_id': 215744827, 'score': 0}, {'doc_id': '219573376', 'title': 'When Science is a Game', 'abstract': ""What happens when scientists are, at certain points in a field's development, playing a game? I present a framework for such an analysis that draws on the theory of games provided by the historian Johan Huizinga. Huizinga gives five conditions for a social practice to become a game: free engagement, disconnection, boundedness in time and arena, the order-creation of rules, and the presence of tension. Application of this theory to scientific practice predicts patterns of behavior that can be tested by quantitative analysis: the emergence of hard boundaries between disciplines, the closure of loopholes in theory creation, resistance to certain innovations in journal publication, and the ways in which scientists fail to prosecute colleagues who engage in questionable research practices."", 'corpus_id': 219573376, 'score': 1}, {'doc_id': '215768748', 'title': 'Embracing Companion Technologies', 'abstract': 'As an increasing number of interactive devices offer human-like assistance, there is a growing need to understand our experience of interactive agents. When interactive artefacts become intertwined in our everyday experience, we need to make sure that they assume the right roles and contribute to our wellbeing. In this theoretical exploration, we propose a reframing of our understanding of the experience of interactions with everyday technologies by proposing the metaphor of companion technologies. We employ theory in the philosophy of empathy to propose a framework for understanding how users develop relationships with digital agents. The experiential framework for companion technologies provides connections between the users’ psychological needs and companion features of interactive systems. Our work provides a theoretical basis for rethinking the user experience of everyday artefacts with an empathy-oriented mindset and poses future challenges for HCI.', 'corpus_id': 215768748, 'score': 0}]"
27	{'doc_id': '221494492', 'title': 'Survey on RNN and CRF models for de-identification of medical free text', 'abstract': 'The increasing reliance on electronic health record (EHR) in areas such as medical research should be addressed by using ample safeguards for patient privacy. These records often tend to be big data, and given that a significant portion is stored as free (unstructured) text, we decided to examine relevant work on automated free text de-identification with recurrent neural network (RNN) and conditional random field (CRF) approaches. Both methods involve machine learning and are widely used for the removal of protected health information (PHI) from free text. The outcome of our survey work produced several informative findings. Firstly, RNN models, particularly long short-term memory (LSTM) algorithms, generally outperformed CRF models and also other systems, namely rule-based algorithms. Secondly, hybrid or ensemble systems containing joint LSTM-CRF models showed no advantage over individual LSTM and CRF models. Thirdly, overfitting may be an issue when customized de-identification datasets are used during model training. Finally, statistical validation of performance scores and diversity during experimentation were largely ignored. In our comprehensive survey, we also identify major research gaps that should be considered for future work.', 'corpus_id': 221494492}	16783	"[{'doc_id': '232414659', 'title': 'De-identifying Spanish medical texts - named entity recognition applied to radiology reports', 'abstract': 'Background Medical texts such as radiology reports or electronic health records are a powerful source of data for researchers. Anonymization methods must be developed to de-identify documents containing personal information from both patients and medical staff. Although currently there are several anonymization strategies for the English language, they are also language-dependent. Here, we introduce a named entity recognition strategy for Spanish medical texts, translatable to other languages. Results We tested 4 neural networks on our radiology reports dataset, achieving a recall of 97.18% of the identifying entities. Alongside, we developed a randomization algorithm to substitute the detected entities with new ones from the same category, making it virtually impossible to differentiate real data from synthetic data. The three best architectures were tested with the MEDDOCAN challenge dataset of electronic health records as an external test, achieving a recall of 69.18%. Conclusions The strategy proposed, combining named entity recognition tasks with randomization of entities, is suitable for Spanish radiology reports. It does not require a big training corpus, thus it could be easily extended to other languages and medical texts, such as electronic health records.', 'corpus_id': 232414659, 'score': 1}, {'doc_id': '174778554', 'title': 'De-identification of French medical narratives', 'abstract': 'In this work, a rule-based method for the de-identification of French free-text medical data using natural language processing (NLP) tools is presented.', 'corpus_id': 174778554, 'score': 1}, {'doc_id': '218974523', 'title': 'A Semi-supervised Approach for De-identification of Swedish Clinical Text', 'abstract': 'An abundance of electronic health records (EHR) is produced every day within healthcare. The records possess valuable information for research and future improvement of healthcare. Multiple efforts have been done to protect the integrity of patients while making electronic health records usable for research by removing personally identifiable information in patient records. Supervised machine learning approaches for de-identification of EHRs need annotated data for training, annotations that are costly in time and human resources. The annotation costs for clinical text is even more costly as the process must be carried out in a protected environment with a limited number of annotators who must have signed confidentiality agreements. In this paper is therefore, a semi-supervised method proposed, for automatically creating high-quality training data. The study shows that the method can be used to improve recall from 84.75% to 89.20% without sacrificing precision to the same extent, dropping from 95.73% to 94.20%. The model’s recall is arguably more important for de-identification than precision.', 'corpus_id': 218974523, 'score': 1}, {'doc_id': '232369347', 'title': 'AI-NLM exploration of the Acronym Identification Shared Task at SDU@AAAI-21', 'abstract': 'National Library of Medicine has developed systems for recognition of named entities in biomedical and clinical text. The systems are primarily leveraging the Unified Medical Language System (UMLS) to recognize the terms and link them to the terminology part of the UMLS (Metathesaurus.) Biomedical and clinical texts are rife with acronyms and abbreviations. Acronym identification and disambiguation play, therefore, an important role in processing of the text using the UMLS-based approaches. To test the existing rule-based approaches developed at NLM and to explore the state-ofthe-art DL approaches, we participated in the SDU Acronym Identification shared task. Not surprisingly, our existing rulebased approach achieved high precision (over 96%), but had very low recall, whereas, the LSTM and BERT-based approaches had almost equal recall and precision and achieved F1 scores in the low 90s.', 'corpus_id': 232369347, 'score': 0}, {'doc_id': '233742517', 'title': 'Extracting Drug Names and Associated Attributes From Discharge Summaries: Text Mining Study', 'abstract': 'Background Drug prescriptions are often recorded in free-text clinical narratives; making this information available in a structured form is important to support many health-related tasks. Although several natural language processing (NLP) methods have been proposed to extract such information, many challenges remain. Objective This study evaluates the feasibility of using NLP and deep learning approaches for extracting and linking drug names and associated attributes identified in clinical free-text notes and presents an extensive error analysis of different methods. This study initiated with the participation in the 2018 National NLP Clinical Challenges (n2c2) shared task on adverse drug events and medication extraction. Methods The proposed system (DrugEx) consists of a named entity recognizer (NER) to identify drugs and associated attributes and a relation extraction (RE) method to identify the relations between them. For NER, we explored deep learning-based approaches (ie, bidirectional long-short term memory with conditional random fields [BiLSTM-CRFs]) with various embeddings (ie, word embedding, character embedding [CE], and semantic-feature embedding) to investigate how different embeddings influence the performance. A rule-based method was implemented for RE and compared with a context-aware long-short term memory (LSTM) model. The methods were trained and evaluated using the 2018 n2c2 shared task data. Results The experiments showed that the best model (BiLSTM-CRFs with pretrained word embeddings [PWE] and CE) achieved lenient micro F-scores of 0.921 for NER, 0.927 for RE, and 0.855 for the end-to-end system. NER, which relies on the pretrained word and semantic embeddings, performed better on most individual entity types, but NER with PWE and CE had the highest classification efficiency among the proposed approaches. Extracting relations using the rule-based method achieved higher accuracy than the context-aware LSTM for most relations. Interestingly, the LSTM model performed notably better in the reason-drug relations, the most challenging relation type. Conclusions The proposed end-to-end system achieved encouraging results and demonstrated the feasibility of using deep learning methods to extract medication information from free-text data.', 'corpus_id': 233742517, 'score': 0}, {'doc_id': '214676863', 'title': 'A Comparative Analysis of Speed and Accuracy for Three Off-the-Shelf De-Identification Tools.', 'abstract': ""A growing quantity of health data is being stored in Electronic Health Records (EHR). The free-text section of these clinical notes contains important patient and treatment information for research but also contains Personally Identifiable Information (PII), which cannot be freely shared within the research community without compromising patient confidentiality and privacy rights. Significant work has been invested in investigating automated approaches to text de-identification, the process of removing or redacting PII. Few studies have examined the performance of existing de-identification pipelines in a controlled comparative analysis. In this study, we use publicly available corpora to analyze speed and accuracy differences between three de-identification systems that can be run off-the-shelf: Amazon Comprehend Medical PHId, Clinacuity's CliniDeID, and the National Library of Medicine's Scrubber. No single system dominated all the compared metrics. NLM Scrubber was the fastest while CliniDeID generally had the highest accuracy."", 'corpus_id': 214676863, 'score': 1}, {'doc_id': '232352634', 'title': 'Benchmarking Modern Named Entity Recognition Techniques for Free-text Health Record De-identification', 'abstract': 'Electronic Health Records (EHRs) have become the primary form of medical data-keeping across the United States. Federal law restricts the sharing of any EHR data that contains protected health information (PHI). De-identification, the process of identifying and removing all PHI, is crucial for making EHR data publicly available for scientific research. This project explores several deep learning-based named entity recognition (NER) methods to determine which method(s) perform better on the de-identification task. We trained and tested our models on the i2b2 training dataset, and qualitatively assessed their performance using EHR data collected from a local hospital. We found that 1) BiLSTM-CRF represents the best-performing encoder/decoder combination, 2) character-embeddings and CRFs tend to improve precision at the price of recall, and 3) transformers alone under-perform as context encoders. Future work focused on structuring medical text may improve the extraction of semantic and syntactic information for the purposes of EHR deidentification.', 'corpus_id': 232352634, 'score': 1}, {'doc_id': '232283498', 'title': 'A BERT-BiGRU-CRF Model for Entity Recognition of Chinese Electronic Medical Records', 'abstract': 'Because of difficulty processing the electronic medical record data of patients with cerebrovascular disease, there is little mature recognition technology capable of identifying the named entity of cerebrovascular disease. Excellent research results have been achieved in the field of named entity recognition (NER), but there are several problems in the pre processing of Chinese named entities that have multiple meanings, of which neglecting the combination of contextual information is one. +erefore, to extract five categories of key entity information for diseases, symptoms, body parts, medical examinations, and treatment in electronic medical records, this paper proposes the use of a BERT-BiGRU-CRF named entity recognition method, which is applied to the field of cerebrovascular diseases. +e BERT layer first converts the electronic medical record text into a low-dimensional vector, then uses this vector as the input to the BiGRU layer to capture contextual features, and finally uses conditional random fields (CRFs) to capture the dependency between adjacent tags. +e experimental results show that the F1 score of the model reaches 90.38%.', 'corpus_id': 232283498, 'score': 0}, {'doc_id': '233744760', 'title': 'A Hybrid Model for Named Entity Recognition on Chinese Electronic Medical Records', 'abstract': 'Electronic medical records (EMRs) contain valuable information about the patients, such as clinical symptoms, diagnostic results, and medications. Named entity recognition (NER) aims to recognize entities from unstructured text, which is the initial step toward the semantic understanding of the EMRs. Extracting medical information from Chinese EMRs could be a more complicated task because of the difference between English and Chinese. Some researchers have noticed the importance of Chinese NER and used the recurrent neural network or convolutional neural network (CNN) to deal with this task. However, it is interesting to know whether the performance could be improved if the advantages of the RNN and CNN can be both utilized. Moreover, RoBERTa-WWM, as a pre-training model, can generate the embeddings with word-level features, which is more suitable for Chinese NER compared with Word2Vec. In this article, we propose a hybrid model. This model first obtains the entities identified by bidirectional long short-term memory and CNN, respectively, and then uses two hybrid strategies to output the final results relying on these entities. We also conduct experiments on raw medical records from real hospitals. This dataset is provided by the China Conference on Knowledge Graph and Semantic Computing in 2019 (CCKS 2019). Results demonstrate that the hybrid model can improve performance significantly.', 'corpus_id': 233744760, 'score': 0}, {'doc_id': '233421290', 'title': 'Ensemble of deep masked language models for effective named entity recognition in multi-domain corpora', 'abstract': 'The health and life science domains are well-known for their wealth of entities. These entities are presented as free text in large corpora, such as biomedical scientific and electronic health records. To enable the secondary use of these corpora and unlock their value, named entity recognition (NER) methods are proposed. Inspired by the success of deep masked language models, we present an ensemble approach for NER using these models. Results show statistically significant improvement of the ensemble models over baselines based on individual models in multiple domains - chemical, clinical and wet lab - and languages - English and French. The ensemble model achieves an overall performance of 79.2% macro F1-score, a 4.6 percentage point increase upon the baseline in multiple domains and languages. These results suggests that ensembles are a more effective strategy for tackling NER. We further perform a detailed analysis of their performance based on a set of entity properties.', 'corpus_id': 233421290, 'score': 0}]"
28	"{'doc_id': '5276660', 'title': 'Neural Module Networks', 'abstract': 'Visual question answering is fundamentally compositional in nature-a question like where is the dog? shares substructure with questions like what color is the dog? and where is the cat? This paper seeks to simultaneously exploit the representational capacity of deep networks and the compositional linguistic structure of questions. We describe a procedure for constructing and learning neural module networks, which compose collections of jointly-trained neural ""modules"" into deep networks for question answering. Our approach decomposes questions into their linguistic substructures, and uses these structures to dynamically instantiate modular networks (with reusable components for recognizing dogs, classifying colors, etc.). The resulting compound networks are jointly trained. We evaluate our approach on two challenging datasets for visual question answering, achieving state-of-the-art results on both the VQA natural image dataset and a new dataset of complex questions about abstract shapes.', 'corpus_id': 5276660}"	5226	"[{'doc_id': '214612170', 'title': 'Linguistically Driven Graph Capsule Network for Visual Question Reasoning', 'abstract': 'Recently, studies of visual question answering have explored various architectures of end-to-end networks and achieved promising results on both natural and synthetic datasets, which require explicitly compositional reasoning. However, it has been argued that these black-box approaches lack interpretability of results, and thus cannot perform well on generalization tasks due to overfitting the dataset bias. In this work, we aim to combine the benefits of both sides and overcome their limitations to achieve an end-to-end interpretable structural reasoning for general images without the requirement of layout annotations. Inspired by the property of a capsule network that can carve a tree structure inside a regular convolutional neural network (CNN), we propose a hierarchical compositional reasoning model called the ""Linguistically driven Graph Capsule Network"", where the compositional process is guided by the linguistic parse tree. Specifically, we bind each capsule in the lowest layer to bridge the linguistic embedding of a single word in the original question with visual evidence and then route them to the same capsule if they are siblings in the parse tree. This compositional process is achieved by performing inference on a linguistically driven conditional random field (CRF) and is performed across multiple graph capsule layers, which results in a compositional reasoning process inside a CNN. Experiments on the CLEVR dataset, CLEVR compositional generation test, and FigureQA dataset demonstrate the effectiveness and composition generalization ability of our end-to-end model.', 'corpus_id': 214612170, 'score': 0}, {'doc_id': '215548225', 'title': 'Injecting Numerical Reasoning Skills into Language Models', 'abstract': 'Large pre-trained language models (LMs) are known to encode substantial amounts of linguistic information. However, high-level reasoning skills, such as numerical reasoning, are difficult to learn from a language-modeling objective only. Consequently, existing models for numerical reasoning have used specialized architectures with limited flexibility. In this work, we show that numerical reasoning is amenable to automatic data generation, and thus one can inject this skill into pre-trained LMs, by generating large amounts of data, and training in a multi-task setup. We show that pre-training our model, GenBERT, on this data, dramatically improves performance on DROP (49.3 –> 72.3 F1), reaching performance that matches state-of-the-art models of comparable size, while using a simple and general-purpose encoder-decoder architecture. Moreover, GenBERT generalizes well to math word problem datasets, while maintaining high performance on standard RC tasks. Our approach provides a general recipe for injecting skills into large pre-trained LMs, whenever the skill is amenable to automatic data augmentation.', 'corpus_id': 215548225, 'score': 1}, {'doc_id': '215745291', 'title': 'Toward Subgraph Guided Knowledge Graph Question Generation with Graph Neural Networks', 'abstract': 'Knowledge graph question generation (QG) aims to generate natural language questions from KG and target answers. Most previous works mainly focusing on the simple setting are to generate questions from a single KG triple. In this work, we focus on a more realistic setting, where we aim to generate questions from a KG subgraph and target answers. In addition, most of previous works built on either RNN-based or Transformer-based models to encode a KG sugraph, which totally discard the explicit structure information contained in a KG subgraph. To address this issue, we propose to apply a bidirectional Graph2Seq model to encode the KG subgraph. In addition, we enhance our RNN decoder with node-level copying mechanism to allow directly copying node attributes from the input graph to the output question. We also explore different ways of initializing node/edge embeddings and handling multi-relational graphs. Our model is end-to-end trainable and achieves new state-of-the-art scores, outperforming existing methods by a significant margin on the two benchmarks.', 'corpus_id': 215745291, 'score': 0}, {'doc_id': '36396995', 'title': 'Tensor Product Generation Networks for Deep NLP Modeling', 'abstract': 'We present a new approach to the design of deep networks for natural language processing (NLP), based on the general technique of Tensor Product Representations (TPRs) for encoding and processing symbol structures in distributed neural networks. A network architecture — the Tensor Product Generation Network (TPGN) — is proposed which is capable in principle of carrying out TPR computation, but which uses unconstrained deep learning to design its internal representations. Instantiated in a model for image-caption generation, TPGN outperforms LSTM baselines when evaluated on the COCO dataset. The TPR-capable structure enables interpretation of internal representations and operations, which prove to contain considerable grammatical content. Our caption-generation model can be interpreted as generating sequences of grammatical categories and retrieving words by their categories from a plan encoded as a distributed representation.', 'corpus_id': 36396995, 'score': 1}, {'doc_id': '216553434', 'title': 'A Heterogeneous Graph with Factual, Temporal and Logical Knowledge for Question Answering Over Dynamic Contexts', 'abstract': 'We study question answering over a dynamic textual environment. Although neural network models achieve impressive accuracy via learning from input-output examples, they rarely leverage various types of knowledge and are generally not interpretable. In this work, we propose a graph-based approach, where a heterogeneous graph is automatically built with factual knowledge of the context, temporal knowledge of the past states, and logical knowledge that combines human-curated knowledge bases and rule bases. We develop a graph neural network over the constructed graph, and train the model in an end-to-end manner. Experimental results on a benchmark dataset show that the injection of various types of knowledge improves a strong neural network baseline. An additional benefit of our approach is that the graph itself naturally serves as a rational behind the decision making.', 'corpus_id': 216553434, 'score': 1}, {'doc_id': '214727958', 'title': 'Code Prediction by Feeding Trees to Transformers', 'abstract': 'Code prediction, more specifically autocomplete, has become an essential feature in modern IDEs. Autocomplete is more effective when the desired next token is at (or close to) the top of the list of potential completions offered by the IDE at cursor position. This is where the strength of the underlying machine learning system that produces a ranked order of potential completions comes into play. We advance the state-of-the-art in the accuracy of code prediction (next token prediction) used in autocomplete systems. Our work uses Transformers as the base neural architecture. We show that by making the Transformer architecture aware of the syntactic structure of code, we increase the margin by which a Transformer-based system outperforms previous systems. With this, it outperforms the accuracy of several state-of-the-art next token prediction systems by margins ranging from 14% to 18%. We present in the paper several ways of communicating the code structure to the Transformer, which is fundamentally built for processing sequence data. We provide a comprehensive experimental evaluation of our proposal, along with alternative design choices, on a standard Python dataset, as well as on Facebook internal Python corpus. Our code and data preparation pipeline will be available in open source.', 'corpus_id': 214727958, 'score': 1}, {'doc_id': '216080851', 'title': 'Syntactic Structure from Deep Learning', 'abstract': 'Modern deep neural networks achieve impressive performance in engineering applications that require extensive linguistic skills, such as machine translation. This success has sparked interest in probing whether these models are inducing human-like grammatical knowledge from the raw data they are exposed to, and, consequently, whether they can shed new light on long-standing debates concerning the innate structure necessary for language acquisition. In this article, we survey representative studies of the syntactic abilities of deep networks, and discuss the broader implications that this work has for theoretical linguistics.', 'corpus_id': 216080851, 'score': 0}, {'doc_id': '216553210', 'title': 'Semantic Graphs for Generating Deep Questions', 'abstract': 'This paper proposes the problem of Deep Question Generation (DQG), which aims to generate complex questions that require reasoning over multiple pieces of information about the input passage. In order to capture the global structure of the document and facilitate reasoning, we propose a novel framework that first constructs a semantic-level graph for the input document and then encodes the semantic graph by introducing an attention-based GGNN (Att-GGNN). Afterward, we fuse the document-level and graph-level representations to perform joint training of content selection and question decoding. On the HotpotQA deep-question centric dataset, our model greatly improves performance over questions requiring reasoning over multiple facts, leading to state-of-the-art performance. The code is publicly available at https://github.com/WING-NUS/SG-Deep-Question-Generation.', 'corpus_id': 216553210, 'score': 0}, {'doc_id': '218487314', 'title': 'Visual Question Answering with Prior Class Semantics', 'abstract': ""We present a novel mechanism to embed prior knowledge in a model for visual question answering. The open-set nature of the task is at odds with the ubiquitous approach of training of a fixed classifier. We show how to exploit additional information pertaining to the semantics of candidate answers. We extend the answer prediction process with a regression objective in a semantic space, in which we project candidate answers using prior knowledge derived from word embeddings. We perform an extensive study of learned representations with the GQA dataset, revealing that important semantic information is captured in the relations between embeddings in the answer space. Our method brings improvements in consistency and accuracy over a range of question types. Experiments with novel answers, unseen during training, indicate the method's potential for open-set prediction."", 'corpus_id': 218487314, 'score': 0}, {'doc_id': '195346132', 'title': 'Tensor Product Generation Networks', 'abstract': 'We present a new tensor product generation network (TPGN) that generates natural language descriptions for images. The model has a novel architecture that instantiates a general framework for encoding and processing symbolic structure through neural network computation. This framework is built on Tensor Product Representations (TPRs). We evaluated the proposed TPGN on the MS COCO image captioning task. The experimental results show that the TPGN outperforms the LSTM based state-of-the-art baseline with a significant margin. Further, we show that our caption generation model can be interpreted as generating sequences of grammatical categories and retrieving words by their categories from a plan encoded as a distributed representation.', 'corpus_id': 195346132, 'score': 1}]"
29	{'doc_id': '215864996', 'title': 'Algorithmic Effects on the Diversity of Consumption on Spotify', 'abstract': 'On many online platforms, users can engage with millions of pieces of content, which they discover either organically or through algorithmically-generated recommendations. While the short-term benefits of recommender systems are well-known, their long-term impacts are less well understood. In this work, we study the user experience on Spotify, a popular music streaming service, through the lens of diversity—the coherence of the set of songs a user listens to. We use a high-fidelity embedding of millions of songs based on listening behavior on Spotify to quantify how musically diverse every user is, and find that high consumption diversity is strongly associated with important long-term user metrics, such as conversion and retention. However, we also find that algorithmically-driven listening through recommendations is associated with reduced consumption diversity. Furthermore, we observe that when users become more diverse in their listening over time, they do so by shifting away from algorithmic consumption and increasing their organic consumption. Finally, we deploy a randomized experiment and show that algorithmic recommendations are more effective for users with lower diversity. Our work illuminates a central tension in online platforms: how do we recommend content that users are likely to enjoy in the short term while simultaneously ensuring they can remain diverse in their consumption in the long term?', 'corpus_id': 215864996}	13742	"[{'doc_id': '211483360', 'title': 'A Formative Study on Designing Accurate and Natural Figure Captioning Systems', 'abstract': 'Automatic figure captioning is widely useful for improving the readability and accessibility of figures. Despite recent advances in figure question answering and parsing figure elements that enable machines to accurately read information from figures, the machine learning community still lacks sufficient understanding of this problem, on what contents are important to include in a caption and how to make it sound natural. In this work, we crawled, annotated, and analyzed a corpus of real-world human-written figure captions. Our study results show that real-world captions usually consist of a finite set of caption units and that automatic figure captioning should be formulated as a multi-stage task. The first stage is to generate caption units with high accuracy and the second is to stitch together the units with diverse stitching patterns, to form a natural caption.', 'corpus_id': 211483360, 'score': 1}, {'doc_id': '221761685', 'title': 'Self-Supervised Meta-Learning for Few-Shot Natural Language Classification Tasks', 'abstract': 'Self-supervised pre-training of transformer models has revolutionized NLP applications. Such pre-training with language modeling objectives provides a useful initial point for parameters that generalize well to new tasks with fine-tuning. However, fine-tuning is still data inefficient -- when there are few labeled examples, accuracy can be low. Data efficiency can be improved by optimizing pre-training directly for future fine-tuning with few examples; this can be treated as a meta-learning problem. However, standard meta-learning techniques require many training tasks in order to generalize; unfortunately, finding a diverse set of such supervised tasks is usually difficult. This paper proposes a self-supervised approach to generate a large, rich, meta-learning task distribution from unlabeled text. This is achieved using a cloze-style objective, but creating separate multi-class classification tasks by gathering tokens-to-be blanked from among only a handful of vocabulary terms. This yields as many unique meta-training tasks as the number of subsets of vocabulary terms. We meta-train a transformer model on this distribution of tasks using a recent meta-learning framework. On 17 NLP tasks, we show that this meta-training leads to better few-shot generalization than language-model pre-training followed by finetuning. Furthermore, we show how the self-supervised tasks can be combined with supervised tasks for meta-learning, providing substantial accuracy gains over previous supervised meta-learning.', 'corpus_id': 221761685, 'score': 1}, {'doc_id': '231813775', 'title': 'The Disagreement Deconvolution: Bringing Machine Learning Performance Metrics In Line With Reality', 'abstract': 'Machine learning classifiers for human-facing tasks such as comment toxicity and misinformation often score highly on metrics such as ROC AUC but are received poorly in practice. Why this gap? Today, metrics such as ROC AUC, precision, and recall are used to measure technical performance; however, human-computer interaction observes that evaluation of human-facing systems should account for people’s reactions to the system. In this paper, we introduce a transformation that more closely aligns machine learning classification metrics with the values and methods of user-facing performance measures. The disagreement deconvolution takes in any multi-annotator (e.g., crowdsourced) dataset, disentangles stable opinions from noise by estimating intra-annotator consistency, and compares each test set prediction to the individual stable opinions from each annotator. Applying the disagreement deconvolution to existing social computing datasets, we find that current metrics dramatically overstate the performance of many human-facing machine learning tasks: for example, performance on a comment toxicity task is corrected from .95 to .73 ROC AUC.', 'corpus_id': 231813775, 'score': 1}, {'doc_id': '230799347', 'title': 'Did Aristotle Use a Laptop? A Question Answering Benchmark with Implicit Reasoning Strategies', 'abstract': 'Abstract A key limitation in current datasets for multi-hop reasoning is that the required steps for answering the question are mentioned in it explicitly. In this work, we introduce StrategyQA, a question answering (QA) benchmark where the required reasoning steps are implicit in the question, and should be inferred using a strategy. A fundamental challenge in this setup is how to elicit such creative questions from crowdsourcing workers, while covering a broad range of potential strategies. We propose a data collection procedure that combines term-based priming to inspire annotators, careful control over the annotator population, and adversarial filtering for eliminating reasoning shortcuts. Moreover, we annotate each question with (1) a decomposition into reasoning steps for answering it, and (2) Wikipedia paragraphs that contain the answers to each step. Overall, StrategyQA includes 2,780 examples, each consisting of a strategy question, its decomposition, and evidence paragraphs. Analysis shows that questions in StrategyQA are short, topic-diverse, and cover a wide range of strategies. Empirically, we show that humans perform well (87%) on this task, while our best baseline reaches an accuracy of ∼ 66%.', 'corpus_id': 230799347, 'score': 0}, {'doc_id': '231749675', 'title': 'NBSearch: Semantic Search and Visual Exploration of Computational Notebooks', 'abstract': 'Code search is an important and frequent activity for developers using computational notebooks (e.g., Jupyter). The flexibility of notebooks brings challenges for effective code search, where classic search interfaces for traditional software code may be limited. In this paper, we propose, NBSearch, a novel system that supports semantic code search in notebook collections and interactive visual exploration of search results. NBSearch leverages advanced machine learning models to enable natural language search queries and intuitive visualizations to present complicated intra- and inter-notebook relationships in the returned results. We developed NBSearch through an iterative participatory design process with two experts from a large software company. We evaluated the models with a series of experiments and the whole system with a controlled user study. The results indicate the feasibility of our analytical pipeline and the effectiveness of NBSearch to support code search in large notebook collections.', 'corpus_id': 231749675, 'score': 0}, {'doc_id': '67771800', 'title': 'SearchLens: composing and capturing complex user interests for exploratory search', 'abstract': 'Whether figuring out where to eat in an unfamiliar city or deciding which apartment to live in, consumer generated data (i.e. reviews and forum posts) are often an important influence in online decision making. To make sense of these rich repositories of diverse opinions, searchers need to sift through a large number of reviews to characterize each item based on aspects that they care about. We introduce a novel system, SearchLens, where searchers build up a collection of ""Lenses"" that reflect their different latent interests, and compose the Lenses to find relevant items across different contexts. Based on the Lenses, SearchLens generates personalized interfaces with visual explanations that promotes transparency and enables deeper exploration. While prior work found searchers may not wish to put in effort specifying their goals without immediate and sufficient benefits, results from a controlled lab study suggest that our approach incentivized participants to express their interests more richly than in a baseline condition, and a field study showed that participants found benefits in SearchLens while conducting their own tasks.', 'corpus_id': 67771800, 'score': 1}, {'doc_id': '231639356', 'title': 'Situation and Behavior Understanding by Trope Detection on Films', 'abstract': 'The human ability of deep cognitive skills is crucial for the development of various real-world applications that process diverse and abundant user generated input. While recent progress of deep learning and natural language processing have enabled learning system to reach human performance on some benchmarks requiring shallow semantics, such human ability still remains challenging for even modern contextual embedding models, as pointed out by many recent studies [9, 10, 22, 24, 32]. Existing machine comprehension datasets assume sentence-level input, lack of casual or motivational inferences, or can be answered with question-answer bias. Here, we present a challenging novel task, trope detection on films, in an effort to create a situation and behavior understanding for machines. Tropes are frequently used storytelling devices for creative works. Comparing to existing movie tag prediction tasks, tropes are more sophisticated as they can vary widely, from a moral concept to a series of circumstances, and embedded with motivations and cause-and-effects. We introduce a new dataset, Tropes in Movie Synopses (TiMoS), with 5623 movie synopses and 95 different tropes collecting from a Wikipedia-style database, TVTropes. We present a multi-stream comprehension network (MulCom) leveraging multi-level attention of words, sentences, and role relations. Experimental result demonstrates that modern models including BERT contextual embedding, movie tag prediction systems, and relational networks, perform at most 37% of human performance (23.97/64.87) in terms of F1 score. Our MulCom outperforms all modern baselines, by 1.5 to 5.0 F1 score and 1.5 to 3.0 mean of average precision (mAP) score. We also provide a detailed analysis and human evaluation to pave ways for future research.', 'corpus_id': 231639356, 'score': 0}, {'doc_id': '51665899', 'title': 'Transient voltages on bonded cable sheaths', 'abstract': 'Discussion of a paper by Herman Halperin, J. E. Clem, and K. W. Miller published in the January 1935 issue, pages 73–82, and presented for oral discussion at the cables session of the winter convention, New York, N. Y., January 24,1935.', 'corpus_id': 51665899, 'score': 0}, {'doc_id': '222066998', 'title': 'Augmenting Scientific Papers with Just-in-Time, Position-Sensitive Definitions of Terms and Symbols', 'abstract': 'Despite the central importance of research papers to scientific progress, they can be difficult to read. Comprehension is often stymied when the information needed to understand a passage resides somewhere else—in another section, or in another paper. In this work, we envision how interfaces can bring definitions of technical terms and symbols to readers when and where they need them most. We introduce ScholarPhi, an augmented reading interface with four novel features: (1) tooltips that surface position-sensitive definitions from elsewhere in a paper, (2) a filter over the paper that “declutters” it to reveal how the term or symbol is used across the paper, (3) automatic equation diagrams that expose multiple definitions in parallel, and (4) an automatically generated glossary of important terms and symbols. A usability study showed that the tool helps researchers of all experience levels read papers. Furthermore, researchers were eager to have ScholarPhi’s definitions available to support their everyday reading.', 'corpus_id': 222066998, 'score': 1}, {'doc_id': '231861515', 'title': 'Civil Rephrases Of Toxic Texts With Self-Supervised Transformers', 'abstract': 'Platforms that support online commentary, from social networks to news sites, are increasingly leveraging machine learning to assist their moderation efforts. But this process does not typically provide feedback to the author that would help them contribute according to the community guidelines. This is prohibitively time-consuming for human moderators to do, and computational approaches are still nascent. This work focuses on models that can help suggest rephrasings of toxic comments in a more civil manner. Inspired by recent progress in unpaired sequence-to-sequence tasks, a self-supervised learning model is introduced, called CAE-T5. CAE-T5 employs a pre-trained text-to-text transformer, which is fine tuned with a denoising and cyclic auto-encoder loss. Experimenting with the largest toxicity detection dataset to date (Civil Comments) our model generates sentences that are more fluent and better at preserving the initial content compared to earlier text style transfer systems which we compare with using several scoring systems and human evaluation.', 'corpus_id': 231861515, 'score': 0}]"
30	{'doc_id': '171473076', 'title': 'Look, A White!: Philosophical Essays on Whiteness', 'abstract': None, 'corpus_id': 171473076}	13605	"[{'doc_id': '230577791', 'title': 'These Stories Must Be Told: Preliminary Observations by a Black Scholar Practitioner on Silences in the Archive', 'abstract': 'As a scholar practitioner, a trained philosophical theologian, Methodist clergywoman, and social enterprise founder who is conducting oral histories as part of my doctoral internship in the IUPUI Arts and Humanities Institute, my scholarly lens and methodological skills are being defined as I interrogate the COVID-19 archive. In this article I attempt to offer some preliminary reflections on my oral history curation focused on how Black and brown artists and activists, primarily based in Indianapolis, IN, frame their lived experiences of death, dying, mourning, and bereavement in the wake of COVID-19 utilizing critical archival practices: those practices that take seriously the methods of critical race theory, critical gender theory, Womanist, mujerista, and feminist methodologies, to name a few. The COVID-19 archive is a collection of oral histories, stories and artifacts depicting the times in which we are living, through the lenses of storytellers grappling with the pandemics of systemic racism, COVID-19, distrust in government, and various relics representing the idea of the United States of America in 2020, as such, I conclude with a brief exploration of how art emerges as both an outlet for creators and a mode of illumination for consumers.', 'corpus_id': 230577791, 'score': 0}, {'doc_id': '191605977', 'title': 'Whiteness and “The Canon”', 'abstract': ""It is a great tragedy that all things in this society, including history, pedagogy, and the pursuit of knowledge, must struggle under the asphyxiating sludge of race, which is the legacy of the myth of whiteness. I grew up in a different society where color has no meaning outside the painter's palette, and my early training was such that I developed a healthy admiration for the work of artists as varied as Francisco de Goya, Gustave Courbet, Kathe Kollwitz, Charles White, Jacob Lawrence, and Ben Shahn, without consciousness of their color.3 I also had considerable exposure to a history of world art without prejudice toward the contributions ofmy own culture to that heritage. I knew one language of art history; a race coding of that language would have appeared ludicrous. However, having since taught in the academy on three continents, I must testify to the cogency of the issues that you raise. A cursory look at the curricula of art history programs across the United States quickly reveals a methodical blindness to all that is not rooted in Western civilization, which is as troubling as it is enduring. This predilection is as evident in course plans and program leanings as it is in faculty hiring practices and student recruitment. My experience was not entirely different in Britain. There is truth, therefore, to the notion of a race-specific pedagogical system and environment so suffused with “absences of vital presences” as to alienate those who may not find themselves or their backgrounds reflected.4"", 'corpus_id': 191605977, 'score': 1}, {'doc_id': '231742773', 'title': 'BLACK LIVES MATTER IN THE UNITED STATES OF AMERICA', 'abstract': ""This paper examines affiliation with the Black Lives Matter (BLM) movement using the constructivism theory. The main finding presented in the paper is that the discrimination experienced by African Americans in the United States in the past two decades. The BLM movement's history was a response to the death of two black teenagers, Trayvon Martin and Michael Brown, who were both unarmed and shot and killed. The most famous one happened this year, the death of George Floyd for the brutal police action by pressing the victim's neck with his leg until Floyd died. The second key finding is that BLM organizations generated more to frame the movement as a struggle for individual rights. Still, many youths assume that this movement is just a trend on social media. Finally, social media's influence where the spread of news, content, videos is the important point of the black lives matter movement in the US. Keyword: BlackLivesMatter, Social Media, Constructivism, Discrimination"", 'corpus_id': 231742773, 'score': 0}, {'doc_id': '147407071', 'title': 'White Women, Race Matters: The Social Construction of Whiteness', 'abstract': 'Traditional debates concerning racially hierarchical societies have tended to focus on the experience of being black. White Women, Race Matters breaks with this tradition by focusing on the particular ecperiences of white women in a racially hierarchical society. By considering the ways in which their experience not only contributes to but challenges the reproduction of racism, the work offers a rigorous examination of existing methodologies, practices and assumptions concerning racism and gender relations. Supported by extracts from in-depth life history interviews, White Women, Race Matters provides valuable course material.', 'corpus_id': 147407071, 'score': 1}, {'doc_id': '231830644', 'title': 'Racial Security: The Unobserved Threat in IR', 'abstract': 'Since the development of the modern state in the seventeenth century, race has directly influenced human understanding of peace and order. Racist perceptions of anarchy associated non-Western and non-white communities with savagery. In turn, a racist cycle of global knowledge upon which the international community is based has developed. Additionally, imperialistic reign over the following centuries dictated the inability for society to challenge racist ideals and norms assumed by the powers that be. Unfortunately, the concepts of race and security have run counter-intuitively in respect to human development. Using the supporting research to construct a preliminary definition of Racial Security and its implications, this essay aims to show how race has compromised the theoretical understanding of international relations in its applicability to the fields of security and strategy.', 'corpus_id': 231830644, 'score': 0}, {'doc_id': '151937310', 'title': 'Black Bodies, White Gazes: The Continuing Significance of Race in America', 'abstract': '149 Black Bodies, White Gazes: The Continuing Significance of Race George Yancy Rowman & Littlefield Publishers, Inc. ISBN: 978-0-7425-5298-2 In the text, Black Bodies White Gazes: the Continuing Significance of Race, George Yancy provides a powerful narrative concerning what he identifies in his introduction as the historical ontology of blackness as this applies to the fluid presencing of human embodiment within the context of anti-black racism. More importantly, Yancy explores the way in which whiteness becomes constructed as a “’transcendental norm,” which is never “raced”, but whose “privilege’ is always implicated in the problematic presencing of the black body. However, he also argues that the historical objectification of blackness is never sufficient to completely silence those potentialities for black embodiment which seek to construct an alternative meaning(s) for blackness that is not beholden to this transcendental normative process. As such, the social presencing of the Black body is never totally captured by the ongoing historical project of anti-black racism and is therefore always capable of constructing an alternative meaning for blackness that is free from the objectifying restrictions imposed upon it.', 'corpus_id': 151937310, 'score': 1}, {'doc_id': '231834773', 'title': 'Colonial Patronage: Evolutions in the Critique of Sartre’s “Orphée noir”', 'abstract': 'One of the most interesting and controversial episodes in the history of the Négritude literary and philosophical movement came when two white, French authors prefaced the texts of two of the movement’s most significant authors. Jean-Paul Sartre’s “Orphée noir” is one of these texts in question, and it served as the preface for Léopold Sédar Senghor’s Anthologie de le nouvelle poésie nègre et malgache de langue française. In one sense, one might characterize Sartre as a friend to the Négritude movement, exposing it to the francophone mainstream and thereby helping it gain traction in Western academia. Viewed a different way, however, and Sartre was intruding into a dialogue in a way he did not truly understand and limiting the movement he sought to help by defining it within his own definition of Blackness. In this project, I propose to investigate the larger implications and perspectives surrounding Sartre’s essay in order to extract the most important criticisms against it as well as the most optimistic takes on what good can be salvaged from his work.', 'corpus_id': 231834773, 'score': 1}, {'doc_id': '229935062', 'title': 'On Holding Various Truths to (Not) Be Self-Evident: Leading During the Dual Pandemics of 2020 as a Racialized Body', 'abstract': 'In this critical autoethnography, the author counter/narrates how she has navigated the dual pandemics of COVID-19 and structural racism predicated on over four centuries of racial oppression that reached a zenith on May 25, 2020, when George Floyd was murdered by a White police officer. As an Asian American woman dean of education at a White-dominated regional university, she weaves in between the past and present to discuss the multilayered intersections between the lives and livelihoods of Asian Americans and Black Americans by theorizing contemporary meanings of the historic slogan from the 1960s: “Yellow Peril Supports Black Power.” She reflects on a few critical moments during the dual pandemics where she has navigated predominantly White spaces that have attempted to center tired-old platitudes around justice for George Floyd despite the daily persistence of blatantly and subtly racist practices against individuals who are Black, Indigenous, and People of Color.', 'corpus_id': 229935062, 'score': 0}, {'doc_id': '190340996', 'title': 'What White Looks Like: African-American Philosophers on the Whiteness Question', 'abstract': ""Contributors Introduction: Fragments of a Social Ontology of Whiteness, George Yancy 1. Racial Exploitation and the Wages of Whiteness, Charles W. Mills 2. The Bad Faith of Whiteness, Robert E. Birt 3. The Impairment of Empathy in Goodwill Whites for African Americans, Janine Jones 4. Deligitimizing the Normativity of 'Whiteness': A Critical Africana Philosophical Study of the Metaphoricity of 'Whiteness', Clevis Headley 5. A Foucauldian (Genealogical) Reading Of Whiteness: The Production Of The Black Body/Self And The Racial Deformation. Of Pecola Breedlove In Toni Morrison's The Bluest Eye, George Yancy 6. Whiteness Visible: Enlightenment Racism and the Structure of Racialized Consciousness, Arnold Farr 7. Rehabilitate Racial Whiteness?, Lucius T. Outlaw 8. Critical Reflections on Three Popular Tropes in the Study of Whiteness, Lewis Gordon 9. Whiteness and Africana Phenomenology, Paget Henry 10. On the Nature of Whiteness and the Ontology of Race: Toward a Dialectical Materialist Analysis, John H. McClendon 11. Silence and Sympathy: Dewey's Whiteness, Paul C. Taylor 12. Whiteness and Feminism: Deja vu Discourses, What's Next?, Blanche Radford Curry 13. The Academic Addict: Mainlining White Supremacy (WS), Joy James"", 'corpus_id': 190340996, 'score': 1}, {'doc_id': '230620229', 'title': 'World War II and American Racial Politics: Public Opinion, the Presidency, and Civil Rights Advocacy. By Steven White. New York: Cambridge University Press, 2019. 216p. $99.99 cloth.', 'abstract': 'how forms of civic engagement other than voting are subject to the noncognitive skills thesis, because the overall goal is to make better democratic citizens. Second, there is a tendency in the book (perhaps unintentionally) to treat young people monolithically. Formative characteristics and experiences vary across racial, ethnic, and even generational groups that likely contribute to the development of noncognitive skills. These factors should be more systematically considered. Finally, an important remaining question is how much electoral reforms can counter the necessity for improved noncognitive skills (or vice versa) in closing the youth voting gap. The book argues for investment in both, but in the real world where tradeoffs exist, it is important to have a better understanding of the potential relative success in outcomes. Yet anyone interested in increasing youth civic engagement should heed the call to explore the role of noncognitive skills in the participatory process, with Making Young Voters serving as a vital roadmap in the investigation.', 'corpus_id': 230620229, 'score': 0}]"
31	{'doc_id': '211069171', 'title': 'StickyPillars: Robust feature matching on point clouds using Graph Neural Networks', 'abstract': 'StickyPillars introduces a sparse feature matching method on point clouds. It is the first approach applying Graph Neural Networks on point clouds to stick points of interest. The feature estimation and assignment relies on the optimal transport problem, where the cost is based on the neural network itself. We utilize a Graph Neural Network for context aggregation with the aid of multihead self and cross attention. In contrast to image based feature matching methods, the architecture learns feature extraction in an end-to-end manner. Hence, the approach does not rely on handcrafted features. Our method outperforms state-of-the art matching algorithms, while providing real-time capability.', 'corpus_id': 211069171}	1586	[{'doc_id': '3911724', 'title': 'Incremental-Segment-Based Localization in 3-D Point Clouds', 'abstract': 'Localization in 3-D point clouds is a highly challenging task due to the complexity associated with extracting information from 3-D data. This letter proposes an incremental approach addressing this problem efficiently. The presented method first accumulates the measurements in a dynamic voxel grid and selectively updates the point normals affected by the insertion. An incremental segmentation algorithm, based on region growing, tracks the evolution of single segments, which enables an efficient recognition strategy using partitioning and caching of geometric consistencies. We show that the incremental method can perform global localization at 10\xa0Hz in an urban driving environment, a speedup of $\\times$7.1 over the compared batch solution. The efficiency of the method makes it suitable for applications where real-time localization is required and enables its usage on cheaper low-energy systems. Our implementation is available open source along with instructions for running the system. (The implementation is available at  https://github.com/ethz-asl/segmatch and a video demonstration is available at https://youtu.be/cHfs3HLzc2Y .)', 'corpus_id': 3911724, 'score': 1}, {'doc_id': '212628458', 'title': 'D3Feat: Joint Learning of Dense Detection and Description of 3D Local Features', 'abstract': 'A successful point cloud registration often lies on robust establishment of sparse matches through discriminative 3D local features. Despite the fast evolution of learning-based 3D feature descriptors, little attention has been drawn to the learning of 3D feature detectors, even less for a joint learning of the two tasks. In this paper, we leverage a 3D fully convolutional network for 3D point clouds, and propose a novel and practical learning mechanism that densely predicts both a detection score and a description feature for each 3D point. In particular, we propose a keypoint selection strategy that overcomes the inherent density variations of 3D point clouds, and further propose a self-supervised detector loss guided by the on-the-fly feature matching results during training. Finally, our method achieves state-of-the-art results in both indoor and outdoor scenarios, evaluated on 3DMatch and KITTI datasets, and shows its strong generalization ability on the ETH dataset. Towards practical use, we show that by adopting a reliable feature detector, sampling a smaller number of features is sufficient to achieve accurate and fast point cloud alignment.', 'corpus_id': 212628458, 'score': 0}, {'doc_id': '210714034', 'title': 'Review: deep learning on 3D point clouds', 'abstract': 'Point cloud is point sets defined in 3D metric space. Point cloud has become one of the most significant data format for 3D representation. Its gaining increased popularity as a result of increased availability of acquisition devices, such as LiDAR, as well as increased application in areas such as robotics, autonomous driving, augmented and virtual reality. Deep learning is now the most powerful tool for data processing in computer vision, becoming the most preferred technique for tasks such as classification, segmentation, and detection. While deep learning techniques are mainly applied to data with a structured grid, point cloud, on the other hand, is unstructured. The unstructuredness of point clouds makes use of deep learning for its processing directly very challenging. Earlier approaches overcome this challenge by preprocessing the point cloud into a structured grid format at the cost of increased computational cost or lost of depth information. Recently, however, many state-of-the-arts deep learning techniques that directly operate on point cloud are being developed. This paper contains a survey of the recent state-of-the-art deep learning techniques that mainly focused on point cloud data. We first briefly discussed the major challenges faced when using deep learning directly on point cloud, we also briefly discussed earlier approaches which overcome the challenges by preprocessing the point cloud into a structured grid. We then give the review of the various state-of-the-art deep learning approaches that directly process point cloud in its unstructured form. We introduced the popular 3D point cloud benchmark datasets. And we also further discussed the application of deep learning in popular 3D vision tasks including classification, segmentation and detection.', 'corpus_id': 210714034, 'score': 0}, {'doc_id': '210928259', 'title': 'Online LiDAR-SLAM for Legged Robots with Robust Registration and Deep-Learned Loop Closure', 'abstract': 'In this paper, we present a 3D factor-graph LiDAR-SLAM system which incorporates a state-of-the-art deeply learned feature-based loop closure detector to enable a legged robot to localize and map in industrial environments. Point clouds are accumulated using an inertial-kinematic state estimator before being aligned using ICP registration. To close loops we use a loop proposal mechanism which matches individual segments between clouds. We trained a descriptor offline to match these segments. The efficiency of our method comes from carefully designing the network architecture to minimize the number of parameters such that this deep learning method can be deployed in real-time using only the CPU of a legged robot, a major contribution of this work. The set of odometry and loop closure factors are updated using pose graph optimization. Finally we present an efficient risk alignment prediction method which verifies the reliability of the registrations. Experimental results at an industrial facility demonstrated the robustness and flexibility of our system, including autonomous following paths derived from the SLAM map.', 'corpus_id': 210928259, 'score': 1}, {'doc_id': '201127695', 'title': 'DeepPCO: End-to-End Point Cloud Odometry through Deep Parallel Neural Network', 'abstract': 'Odometry is of key importance for localization in the absence of a map. There is considerable work in the area of visual odometry (VO), and recent advances in deep learning have brought novel approaches to VO, which directly learn salient features from raw images. These learning-based approaches have led to more accurate and robust VO systems. However, they have not been well applied to point cloud data yet. In this work, we investigate how to exploit deep learning to estimate point cloud odometry (PCO), which may serve as a critical component in point cloud-based downstream tasks or learning-based systems. Specifically, we propose a novel end-to-end deep parallel neural network called DeepPCO, which can estimate the 6-DOF poses using consecutive point clouds. It consists of two parallel sub-networks to estimate 3D translation and orientation respectively rather than a single neural network. We validate our approach on KITTI Visual Odometry/SLAM benchmark dataset with different baselines. Experiments demonstrate that the proposed approach achieves good performance in terms of pose accuracy.', 'corpus_id': 201127695, 'score': 1}, {'doc_id': '19233296', 'title': 'SegMap: 3D Segment Mapping using Data-Driven Descriptors', 'abstract': 'When performing localization and mapping, working at the level of structure can be advantageous in terms of robustness to environmental changes and differences in illumination. This paper presents SegMap: a map representation solution to the localization and mapping problem based on the extraction of segments in 3D point clouds. In addition to facilitating the computationally intensive task of processing 3D point clouds, working at the level of segments addresses the data compression requirements of real-time single- and multi-robot systems. While current methods extract descriptors for the single task of localization, SegMap leverages a data-driven descriptor in order to extract meaningful features that can also be used for reconstructing a dense 3D map of the environment and for extracting semantic information. This is particularly interesting for navigation tasks and for providing visual feedback to end-users such as robot operators, for example in search and rescue scenarios. These capabilities are demonstrated in multiple urban driving and search and rescue experiments. Our method leads to an increase of area under the ROC curve of 28.3% over current state of the art using eigenvalue based features. We also obtain very similar reconstruction capabilities to a model specifically trained for this task. The SegMap implementation will be made available open-source along with easy to run demonstrations at this http URL. A video demonstration is available at this https URL.', 'corpus_id': 19233296, 'score': 1}, {'doc_id': '211677268', 'title': 'Triangle-Net: Towards Robustness in Point Cloud Classification', 'abstract': '3D object recognition is becoming a key desired capability for many computer vision systems such as autonomous vehicles, service robots and surveillance drones to operate more effectively in unstructured environments. These real-time systems require effective classification methods that are robust to sampling resolution, measurement noise, and pose configuration of the objects. Previous research has shown that sparsity, rotation and positional variance of points can lead to a significant drop in the performance of point cloud based classification techniques. In this regard, we propose a novel approach for 3D classification that takes sparse point clouds as input and learns a model that is robust to rotational and positional variance as well as point sparsity. To this end, we introduce new feature descriptors which are fed as an input to our proposed neural network in order to learn a robust latent representation of the 3D object. We show that such latent representations can significantly improve the performance of object classification and retrieval. Further, we show that our approach outperforms PointNet and 3DmFV by 34.4% and 27.4% respectively in classification tasks using sparse point clouds of only 16 points under arbitrary SO(3) rotation.', 'corpus_id': 211677268, 'score': 0}, {'doc_id': '214775247', 'title': 'Monocular Camera Localization in Prior LiDAR Maps with 2D-3D Line Correspondences', 'abstract': 'Light-weight camera localization in existing maps is essential for vision-based navigation. Currently, visual and visual-inertial odometry (VO&VIO) techniques are well-developed for state estimation but with inevitable accumulated drifts and pose jumps upon loop closure. To overcome these problems, we propose an efficient monocular camera localization method in prior LiDAR maps using direct 2D-3D line correspondences. To handle the appearance differences and modality gaps between LiDAR point clouds and images, geometric 3D lines are extracted offline from LiDAR maps while robust 2D lines are extracted online from video sequences. With the pose prediction from VIO, we can efficiently obtain coarse 2D-3D line correspondences. Then the camera poses and 2D-3D correspondences are iteratively optimized by minimizing the projection error of correspondences and rejecting outliers. Experimental results on the EurocMav dataset and our collected dataset demonstrate that the proposed method can efficiently estimate camera poses without accumulated drifts or pose jumps in structured environments.', 'corpus_id': 214775247, 'score': 0}, {'doc_id': '3516878', 'title': 'IMLS-SLAM: Scan-to-Model Matching Based on 3D Data', 'abstract': 'The Simultaneous Localization And Mapping (SLAM) problem has been well studied in the robotics community, especially using mono, stereo cameras or depth sensors. 3D depth sensors, such as Velodyne LiDAR, have proved in the last 10 years to be very useful to perceive the environment in autonomous driving, but few methods exist that directly use these 3D data for odometry. We present a new low-drift SLAM algorithm based only on 3D LiDAR data. Our method relies on a scan-to-model matching framework. We first have a specific sampling strategy based on the LiDAR scans. We then define our model as the previous localized LiDAR sweeps and use the Implicit Moving Least Squares (IMLS) surface representation. We show experiments with the Velodyne HDL32 with only 0.40% drift over a 4 km acquisition without any loop closure (i.e., 16 $m$ drift after 4 km). We tested our solution on the KITTI benchmark with a Velodyne HDL64 and ranked among the best methods (against mono, stereo and LiDAR methods) with a global drift of only 0.69%.', 'corpus_id': 3516878, 'score': 1}, {'doc_id': '211677612', 'title': '3D Point Cloud Processing and Learning for Autonomous Driving', 'abstract': 'We present a review of 3D point cloud processing and learning for autonomous driving. As one of the most important sensors in autonomous vehicles, light detection and ranging (LiDAR) sensors collect 3D point clouds that precisely record the external surfaces of objects and scenes. The tools for 3D point cloud processing and learning are critical to the map creation, localization, and perception modules in an autonomous vehicle. While much attention has been paid to data collected from cameras, such as images and videos, an increasing number of researchers have recognized the importance and significance of LiDAR in autonomous driving and have proposed processing and learning algorithms to exploit 3D point clouds. We review the recent progress in this research area and summarize what has been tried and what is needed for practical and safe autonomous vehicles. We also offer perspectives on open issues that are needed to be solved in the future.', 'corpus_id': 211677612, 'score': 0}]
32	{'doc_id': '203667650', 'title': 'Effect of community-initiated kangaroo mother care on survival of infants with low birthweight: a randomised controlled trial', 'abstract': 'BACKGROUND\nCoverage of kangaroo mother care remains very low despite WHO recommendations for its use for babies with low birthweight in health facilities for over a decade. Initiating kangaroo mother care at the community level is a promising strategy to increase coverage. However, knowledge of the efficacy of community-initiated kangaroo mother care is still lacking. We aimed to assess the effect of community-initiated kangaroo mother care provided to babies weighing 1500-2250 g on neonatal and infant survival.\n\n\nMETHODS\nIn this randomised controlled, superiority trial, undertaken in Haryana, India, we enrolled babies weighing 1500-2250 g at home within 72 h of birth, if not already initiated in kangaroo mother care, irrespective of place of birth (ie, home or health facility) and who were stable and feeding. The first eligible infants in households were randomly assigned (1:1) to the intervention (community-initiated kangaroo mother care) or control group by block randomisation using permuted blocks of variable size. Twins were allocated to the same group. For second eligible infants in the same household as an enrolled infant, if the first infant was assigned to the intervention group the second infant was also assigned to this group, whereas if the first infant was assigned to the control group the second infant was randomly assigned (1:1) to the intervention or control group. Mothers and infants in the intervention group were visited at home (days 1-3, 5, 7, 10, 14, 21, and 28) to support kangaroo mother care (ie, skin-to-skin contact and exclusive breastfeeding). The control group received routine care. The two primary outcomes were mortality between enrolment and 28 days and between enrolment and 180 days. Analysis was by intention to treat and adjusted for clustering within households. The effect of the intervention on mortality was assessed with person-time in the denominator using Cox proportional hazards model. This study is registered with ClinicalTrials.gov, NCT02653534 and NCT02631343, and is now closed to new participants.\n\n\nFINDINGS\nBetween July 30, 2015, and Oct 31, 2018, 8402 babies were enrolled, of whom 4480 were assigned to the intervention group and 3922 to the control group. Most births (6837 [81·4%]) occurred at a health facility, 36·2% (n=3045) had initiated breastfeeding within 1 h of birth, and infants were enrolled at an average of about 30 h (SD 17) of age. Vital status was known for 4470 infants in the intervention group and 3914 in the control group at age 28 days, and for 3653 in the intervention group and 3331 in the control group at age 180 days. Between enrolment and 28 days, 73 infants died in 4423 periods of 28 days in the intervention group and 90 deaths in 3859 periods of 28 days in the control group (hazard ratio [HR] 0·70, 95% CI 0·51-0·96; p=0·027). Between enrolment and 180 days, 158 infants died in 3965 periods of 180 days in the intervention group and 184 infants died in 3514 periods of 180 days in the control group (HR 0·75, 0·60-0·93; p=0·010). The risk ratios for death were almost the same as the HRs (28-day mortality 0·71, 95% CI 0·52- 0·97; p=0·032; 180-day mortality 0·76, 0·60-0·95; p=0·017).\n\n\nINTERPRETATION\nCommunity-initiated kangaroo mother care substantially improves newborn baby and infant survival. In low-income and middle-income countries, incorporation of kangaroo mother care for all infants with low birthweight, irrespective of place of birth, could substantially reduce neonatal and infant mortality.\n\n\nFUNDING\nResearch Council of Norway and University of Bergen.', 'corpus_id': 203667650}	20595	"[{'doc_id': '236991276', 'title': 'Placental growth factor in assessment of women with suspected pre-eclampsia to reduce maternal morbidity: a stepped wedge cluster randomised control trial (PARROT Ireland)', 'abstract': ""Abstract Objective To determine whether the addition of placental growth factor (PlGF) measurement to current clinical assessment of women with suspected pre-eclampsia before 37 weeks' gestation would reduce maternal morbidity without increasing neonatal morbidity. Design Stepped wedge cluster randomised control trial from 29 June 2017 to 26 April 2019. Setting National multisite trial in seven maternity hospitals throughout the island of Ireland Participants Women with a singleton pregnancy between 20+0 to 36+6 weeks’ gestation, with signs or symptoms suggestive of evolving pre-eclampsia. Of the 5718 women screened, 2583 were eligible and 2313 elected to participate. Intervention Participants were assigned randomly to either usual care or to usual care plus the addition of point-of-care PlGF testing based on the randomisation status of their maternity hospital at the time point of enrolment. Main outcomes measures Co-primary outcomes of composite maternal morbidity and composite neonatal morbidity. Analysis was on an individual participant level using mixed-effects Poisson regression adjusted for time effects (with robust standard errors) by intention-to-treat. Results Of the 4000 anticipated recruitment target, 2313 eligible participants (57%) were enrolled, of whom 2219 (96%) were included in the primary analysis. Of these, 1202 (54%) participants were assigned to the usual care group, and 1017 (46%) were assigned the intervention of additional point-of-care PlGF testing. The results demonstrate that the integration of point-of-care PlGF testing resulted in no evidence of a difference in maternal morbidity—457/1202 (38%) of women in the control group versus 330/1017 (32%) of women in the intervention group (adjusted risk ratio (RR) 1.01 (95% CI 0.76 to 1.36), P=0.92)—or in neonatal morbidity—527/1202 (43%) of neonates in the control group versus 484/1017 (47%) in the intervention group (adjusted RR 1.03 (0.89 to 1.21), P=0.67). Conclusions This was a pragmatic evaluation of an interventional diagnostic test, conducted nationally across multiple sites. These results do not support the incorporation of PlGF testing into routine clinical investigations for women presenting with suspected preterm pre-eclampsia, but nor do they exclude its potential benefit. Trial registration ClinicalTrials.gov NCT02881073."", 'corpus_id': 236991276, 'score': 0}, {'doc_id': '237392173', 'title': 'Effects of Intermittent Kangaroo Mother Care in Preterm Low Birth Weight Babies: A Randomized Controlled Trial', 'abstract': 'Background: Prematurity is the largest cause of neonatal mortality. They need incubators or radiant warmers which are expensive and very difficult to arrange in a resource constraint country. Kangaroo mother care (KMC) had been proposed as an alternative to conventional neonatal care for low birthweight (LBW) babies. Objectives: To observe the benefits of Kangaroo mother care in preterm low birth weight babies. Methods: This randomized controlled trial was conducted over 6 months in Dhaka Shishu Hospital. Neonates who were <1800 gm and hemodynamically stable were enrolled. Total 80 neonates were enrolled and divided into 2 groups: Kangaroo mother care group and conventional method care group (incubator/warmer). The mother or caregiver were taught for KMC, supervised by trained nurses round the clock. KMC was given at least 2 hours at a time and at least 12 hours in a day. When the baby was not in KMC at that time the baby was placed in cot with adequate coverings. During hospital stay both the groups were monitored. Results: In KMC group 25% and conventional care group 40% neonates became hypothermic. Among the study population 35% neonates in KMC and 65% neonates in conventional care groups developed sepsis (p= 0.007). More KMC babies were exclusively breastfed at the end of the study (95% vs 60%). The KMC babies had shown better growth: weight gain per day (18.35±7.81 grams vs 13.55±4.89 p<0.001) and length (0.99±0.70 vs 0.71±0.44 cm, p = 0.03). KMC babies were discharged earlier than conventional care baby. Conclusion: KMC provides significant improvement in exclusive breast feeding, reduction of infection, decrease hospital stay and gaining weight of the babies. It also helps in maintaining temperature better than conventional care.', 'corpus_id': 237392173, 'score': 1}, {'doc_id': '237426135', 'title': 'Saving babies’ lives (SBL) – a programme to reduce neonatal mortality in rural Cambodia: study protocol for a stepped-wedge cluster-randomised trial', 'abstract': 'Background Neonatal mortality remains unacceptably high. Many studies successful at reducing neonatal mortality have failed to realise similar gains at scale. Effective implementation and scale-up of interventions designed to tackle neonatal mortality is a global health priority. Multifaceted programmes targeting the continuum of neonatal care, with sustainability and scalability built into the design, can provide practical insights to solve this challenge. Cambodia has amongst the highest neonatal mortality rates in South-East Asia, with rural areas particularly affected. The primary objective of this study is the design, implementation, and assessment of the Saving Babies’ Lives programme, a package of interventions designed to reduce neonatal mortality in rural Cambodia. Methods This study is a five-year stepped-wedge cluster-randomised trial conducted in a rural Cambodian province with an estimated annual delivery rate of 6615. The study is designed to implement and evaluate the Saving Babies’ Lives programme, which is the intervention. The Saving Babies’ Lives programme is an iterative package of neonatal interventions spanning the continuum of care and integrating into the existing health system. The Saving Babies’ Lives programme comprises two major components: participatory learning and action with community health workers, and capacity building of primary care facilities involving facility-based mentorship. Standard government service continues in control arms. Data collection covering the whole study area includes surveillance of all pregnancies, verbal and social autopsies, and quality of care surveys. Mixed methods data collection supports iteration of the complex intervention, and facilitates impact, outcome, process and economic evaluation. Discussion Our study uses a robust study design to evaluate and develop a holistic, innovative, contextually relevant and sustainable programme that can be scaled-up to reduce neonatal mortality. Trial registration ClinicalTrials.gov: NCT04663620 . Registered on 11th December 2020, retrospectively registered.', 'corpus_id': 237426135, 'score': 0}, {'doc_id': '237443116', 'title': 'Maternal experience of intermittent kangaroo mother care for late preterm infants: a mixed-methods study in four postnatal wards in China', 'abstract': 'Objective To describe how mothers of late preterm infants experienced the provision of intermittent kangaroo mother care (KMC) in four postnatal wards in different hospitals in China, under a pilot KMC project. Design A concurrent mixed-methods approach incorporating quantitative maternal questionnaires and qualitative semistructured interviews. Setting Four postnatal wards in level-III hospitals based in different provinces of Southeast and Northwest China. Participants All 752 mothers who provided intermittent KMC to their late preterm newborns in the four participating postnatal wards consented to participate in the study (quantitative component), as well as six nurses, two obstetricians and two mothers from two of the participating postnatal wards (qualitative component). Outcome measures Maternal KMC experiences during a hospital stay, patients’ perceptions of KMC initiation, processes, benefits and challenges. Results Most mothers had not heard of KMC before being introduced to it in the postnatal ward. On average, mothers and newborns stayed in postnatal wards for 3.6 days; during their stay, mothers provided an average of 3.5 KMC sessions, which is an average of 1.1 sessions a day. Each KMC session lasted an average of 68 min, though there was much variation in the length of a session. Common reasons given for discontinuing a KMC session included restroom use, infant crying and perceived time limitations. Some mothers would have preferred to provide KMC for longer periods of time and nurses encouraged this. Most mothers experienced no difficulty providing KMC, received support from family and medical staff and intended to continue with KMC postdischarge. Conclusion In order to improve the maternal experience of KMC, it is recommended that raising awareness of KMC should be included in antenatal care and after birth. Longer periods of KMC provision should be encouraged, greater privacy should be provided for mothers providing KMC in postnatal wards and family members should be encouraged to support KMC.', 'corpus_id': 237443116, 'score': 1}, {'doc_id': '237090324', 'title': 'Impact of early kangaroo mother care versus standard care on survival of mild-moderately unstable neonates <2000 grams: A randomised controlled trial', 'abstract': 'Background Understanding the effect of early kangaroo mother care on survival of mild-moderately unstable neonates <2000 g is a high-priority evidence gap for small and sick newborn care. Methods This non-blinded pragmatic randomised clinical trial was conducted at the only teaching hospital in The Gambia. Eligibility criteria included weight <2000g and age 1–24 h with exclusion if stable or severely unstable. Neonates were randomly assigned to receive either standard care, including KMC once stable at >24 h after admission (control) versus KMC initiated <24 h after admission (intervention). Randomisation was stratified by weight with twins in the same arm. The primary outcome was all-cause mortality at 28 postnatal days, assessed by intention to treat analysis. Secondary outcomes included: time to death; hypothermia and stability at 24 h; breastfeeding at discharge; infections; weight gain at 28d and admission duration. The trial was prospectively registered at www.clinicaltrials.gov (NCT03555981). Findings Recruitment occurred from 23rd May 2018 to 19th March 2020. Among 1,107 neonates screened for participation 279 were randomly assigned, 139 (42% male [n = 59]) to standard care and 138 (43% male [n = 59]) to the intervention with two participants lost to follow up and no withdrawals. The proportion dying within 28d was 24% (34/139, control) vs. 21% (29/138, intervention) (risk ratio 0·84, 95% CI 0·55 – 1·29, p = 0·423). There were no between-arm differences for secondary outcomes or serious adverse events (28/139 (20%) for control and 30/139 (22%) for intervention, none related). One-third of intervention neonates reverted to standard care for clinical reasons. Interpretation The trial had low power due to halving of baseline neonatal mortality, highlighting the importance of implementing existing small and sick newborn care interventions. Further mortality effect and safety data are needed from varying low and middle-income neonatal unit contexts before changing global guidelines.', 'corpus_id': 237090324, 'score': 1}, {'doc_id': '237396959', 'title': 'Birthweight measurement processes and perceived value: a qualitative study in Temeke Hospital, Tanzania', 'abstract': ""Background: Globally an estimated 20.5 million liveborn babies are low birthweight (LBW) each year, weighing less than 2500 g. LBW babies have increased risk of mortality even beyond the neonatal period, with an ongoing risk of stunting and non-communicable diseases. LBW is a priority global health indicator. Now almost 80% of births are in facilities, yet birthweight data are lacking in most high-mortality burden countries and are of poor quality, notably with heaping especially on values ending in 00. We aimed to undertake qualitative research in a regional hospital in Dar es Salaam, Tanzania, observing birthweight practices, exploring barriers and enablers to weighing at birth as well as perceived value of birthweight data to health workers, women and stakeholders. Methods: Observations were undertaken on type of birthweight scale availability in hospital wards. In-depth semistructured interviews (n = 21) were conducted with three groups: women in postnatal and kangaroo mother care wards, health workers involved in birthweight measurement/recording, and with stakeholders involved in data aggregation in Temeke Hospital, Tanzania, a site in the EN-BIRTH study. An inductive thematic analysis was undertaken of translated interview transcripts. Results: Of five wards that were expected to have scales, three had functional scales, and only one of the functional scales was digital. The Labour ward weighed the most newborns using an analogue scale which was not consistently zeroed. Hospital birthweight data were aggregated monthly for reporting into the health management information system. Birthweight measurement was highly valued by all respondents, notably families and healthcare workers, and local use of data was considered an enabler. Perceived barriers to high quality birthweight data included: gaps in availability of precise weighing equipment, adequate health workers and imprecise measurement practices. (Continued on next page) © The Author(s). 2020 Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/. The Creative Commons Public Domain Dedication waiver (http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated in a credit line to the data. * Correspondence: joy.lawn@lshtm.ac.uk Joy E Lawn is senior author Miriam E Gladstonea and Nahya Salim are joint first authors Centre for Maternal, Adolescent, Reproductive, & Child Health (MARCH), London School of Hygiene and Tropical Medicine (LSHTM), Keppel Street, London WC1E 7HT, UK Full list of author information is available at the end of the article"", 'corpus_id': 237396959, 'score': 0}, {'doc_id': '237329145', 'title': 'Enablers and barriers for enteral feeding with mother`s own milk in preterm very low birth weight infants in a tertiary care neonatal intensive care unit.', 'abstract': 'BACKGROUND\nThe management of lactation in preterm mothers is a real challenge for Neonatal Intensive Care Unit (NICU) care, providers. The study aimed to evaluate the enablers and barriers for enteral feeding with mothers` own milk (MOM) in preterm very low birth weight (VLBW) infants in a tertiary care neonatal unit.\n\n\nMETHODS\nThis prospective observational study took place at a tertiary level NICU of a high-risk obstetric unit in a private hospital. All VLBW infants and mothers were incorporated into the study. Data on enablers and barriers were gathered from mother-baby dyads at the time of birth, at the end of the 7th day, and then weekly till the discharge of the baby from the unit.\n\n\nRESULTS\nWe studied 87 mother-baby dyads. Mean (SD) maternal age, gestation age and birth weight were 29.3 (4.7) years, 30.8 (2.0) weeks, and 1196 (196) grams respectively. We categorized our data into 2 groups based on outcome estimates done during the entire hospital stay or pre-discharge (48 hours before the discharge). On comparison of perinatal and post-natal factors, the enablers were maternal dwelling from the rural locality, number of milk expression son day 1 after the birth, number of night expressions in the first week postnatally, and MOM volume till day 3, day 7, and 2 weeks postnatally. The enablers of MOM in the pre-discharge group were the number of expressions in the first 3 days, the number of night expressions in week 1, mother`s visit, and the number of maternal visits on day 1 to NICU and MOM volume expressed from day 1 until the second week after birth. The main barriers for MOM (48 hours pre-discharge) were extremely low birth weight (ELBW) and intrauterine growth-restricted infants (IUGR).\n\n\nCONCLUSIONS\nELBW infants and IUGR infants are susceptible to low MOM feeding. The total of milk expressions in the first 3 days, number of night expressions in the first week, maternal visits on day 1 and the average MOM amount in the first 2 weeks are enablers for MOM feeding.', 'corpus_id': 237329145, 'score': 0}, {'doc_id': '237505283', 'title': 'Scaling up Kangaroo Mother Care in Ethiopia and India: a multi-site implementation research study', 'abstract': 'Objectives Kangaroo Mother Care (KMC), prolonged skin-to-skin care of the low birth weight baby with the mother plus exclusive breastfeeding reduces neonatal mortality. Global KMC coverage is low. This study was conducted to develop and evaluate context-adapted implementation models to achieve improved coverage. Design This study used mixed-methods applying implementation science to develop an adaptable strategy to improve implementation. Formative research informed the initial model which was refined in three iterative cycles. The models included three components: (1) maximising access to KMC-implementing facilities, (2) ensuring KMC initiation and maintenance in facilities and (3) supporting continuation at home postdischarge. Participants 3804 infants of birth weight under 2000\u2009g who survived the first 3\u2009days, were available in the study area and whose mother resided in the study area. Main outcome measures The primary outcomes were coverage of KMC during the 24 hours prior to discharge and at 7\u2009days postdischarge. Results Key barriers and solutions were identified for scaling up KMC. The resulting implementation model achieved high population-based coverage. KMC initiation reached 68%–86% of infants in Ethiopian sites and 87% in Indian sites. At discharge, KMC was provided to 68% of infants in Ethiopia and 55% in India. At 7\u2009days postdischarge, KMC was provided to 53%–65% of infants in all sites, except Oromia (38%) and Karnataka (36%). Conclusions This study shows how high coverage of KMC can be achieved using context-adapted models based on implementation science. They were supported by government leadership, health workers’ conviction that KMC is the standard of care, women’s and families’ acceptance of KMC, and changes in infrastructure, policy, skills and practice. Trial registration numbers ISRCTN12286667; CTRI/2017/07/008988; NCT03098069; NCT03419416; NCT03506698.', 'corpus_id': 237505283, 'score': 1}, {'doc_id': '237630601', 'title': 'A randomised comparative study of coconut oil massage for effect on weight change in low birth weight neonates', 'abstract': 'Background: Low birth weight (LBW) babies have more the risk of the neurological complications, physiological problems and mental retardation. Topical massage with natural oil is routinely practiced in India. The positive effects of massage are weight gain, improved sleep/wake pattern, decreased the stress, early discharge from the neonatal intensive care unit (NICU), improve the skin integrity and enhanced parent’s infant bonding.Methods:This prospective interventional randomised comparative study was conducted among 64 LBW babies at Paediatric department of Swami Dayanand hospital (SDH), Delhi. Out of which 31 were in intervention group and 33 were in control group.\xa0 In the intervention group, mothers were encouraged to massage their babies with 10 ml of coconut oil for 15 min, twice a day until 10 days of life. Those allocated to the control group were received care as usual. Weight and head circumference was measured at enrolment and on day 11 in both the groups.Results: Basic characteristics of neonates of intervention and control group were almost similar. Mean weight gain in intervention group was 352.26±101.05 g while it was 209.70±124.66 g in control group (p=0.0001). Similarly mean weight gain velocity was significantly higher in intervention group (32.02±19.19 g/day) as compared to control group (19.09±11.33 g/day, p=0.0001).Conclusions:The present study supports significant increase in weight gain in LBW preterm and term neonates with coconut oil massage. Coconut oil is easily available in the market and it should be recommended to LBW babies for their better weight gain.', 'corpus_id': 237630601, 'score': 0}, {'doc_id': '237510451', 'title': 'Effect of Daily Intermittent Kangaroo Mother Care on Vital Physiological Parameters of Low Birth Weight Newborns', 'abstract': 'Objective: To determine the effect of daily intermittent Kangaroo Mother Care (KMC) on vital physiological parameters of low birth weight newborns. Methods: This Quasi-experimental study was conducted in the Department of Neonatal Paediatrics, King Edward Medical University / Mayo Hospital Lahore from July 2017 to March 2018. Total of 84 low birth weight (< 2500gms) neonates were recruited by non-probability convenient sampling. Kangaroo mother care was provided for 3 consecutive hours a day, for 3 consecutive days. Vital physiological parameters including temperature, heart rate, respiratory rate and oxygen saturation of every baby were recorded immediately before and after KMC. Data was analyzed through SPSS 20.0. Median values of all four vital parameters (pre and post KMC) were compared by Wilcoxon signed-rank test after applying KolmogrovSmirnov test for normality distribution. Results: In 84 newborns, there was no change in median body temperature after KMC on day 1, while it was + 0.25 0C on day 2 and + 0.4 0C on day 3(p-values < 0.001). There was no change in median respiratory rate after KMC on day 1 (p-value = 0.412), while it reduced by 2 breaths/min on day 2 (p-value = 0.01) and 2 breaths/min on day 3 (p-value < 0.001). There was no change in median heart rate after KMC on day 1 (p-value = 0.765), but it decreased favorably by 5 beats/min (p-value = 0.008) and 4 beats/min (p-values< 0.001) on day 2 and 3 respectively. The median oxygen saturation after KMC increased by 1% on all 3 days ( p-values < 0.001). Conclusion: Except respiratory rate and heart rate on day 1, all vital physiological parameters in low birth weight babies showed statistical improvement after KMC. Corresponding Author | Dr. Sadia Shabir, Department of Paediatrics, KEMU/ Mayo Hospital, Lahore, Email: drsadiahussain@yahoo.com', 'corpus_id': 237510451, 'score': 1}]"
33	"{'doc_id': '160413762', 'title': 'Tsuchigumo soshi : The Emergence of a Shape-Shifting Killer Female Spider', 'abstract': 'Tsuchigumo sōshi is a fascinating otogizōshi story of a haunted house full of strange creatures that extols the prowess of Minamoto no Raikō (or Yorimitsu, 948―1021), recounting how he vanquished a gigantic earth spider. Although spiders appear in ancient Japanese texts, this picture scroll is the oldest extant work in which a spider is portrayed as a supernatural creature. I speculate that the spider\'s transformation to an evil, uncanny creature is due to its association with an oni (demon, ogre). During the early modern period, the earth spider was notorious in literature and theatrical performance as a shape-shifting killer. The Nihon shoki and the ""Swords chapter"" of Heike monogatari are widely recognized sources for an influential Noh play, Tsuchigumo, which is rightfully cited as a work that greatly influenced later tsuchigumo literature and performance. I believe Tsuchigumo sōshi should be regarded as a source for the Noh play and for the emergence of an image of tsuchigumo as a killer shape-shifter. A complete translation of Tsuchigumo sōshi accompanies this article.', 'corpus_id': 160413762}"	2963	[{'doc_id': '211138181', 'title': 'Shamkhani phones Ukrainian counterpart over January plane crash investigations Zarif to Trump : Time to abandon your delusions', 'abstract': 'Bahman 22nd (February 11) is one of the greatest moments in human history. The victory of the Islamic Revolution in Iran not only led to overthrow the tyrannical Pahlavi regime, it also cut the hands of arrogant countries, above all the U.S., out of the country. The Islamic Revolution and the subsequent establishment of the Islamic Republic led to major changes in regional and international developments. The 1979 Revolution was like a sapling that has become a strong tree over the past 40 years and is still growing. The Islamic Republic of Iran has confronted and challenged the U.S. and the Zionist regime’s imperialist policies in the region and the larger world. And Iran’s strategic influence in the region has become the greatest concern of the U.S. and Tel Aviv. This strategic influence derives from the very nature of the Islamic Revolution that attracts the liberal and oppressed nations of the world. It is a popular revolution originated from pure human nature, which does not accept oppression, aggression and violence. One of the most important effects of the Islamic Revolution was to disrupt the balance of power between the East and the West. We have seen the collapse of the Eastern bloc led by the Soviet Union, as predicted by Imam Khomeini. Now, the Leader of the Islamic Revolution Ayatollah Khamenei has promised the decline of the American domination. Despite all U.S. military, political, economic investments and soft war in the region, President Donald Trump has admitted that Americans have failed and were not able to carry out their plans. It was the Islamic Revolution that, with strong logic and modern interpretation, presented a new discourse, according to the needs of Islamic communities and the Arab environment. 3 “Walnut Tree”: story of enemies’ sustained evil', 'corpus_id': 211138181, 'score': 0}, {'doc_id': '211237479', 'title': '‘ Shahi Snan ’ , procession of ‘ Naga Sadhus ’ held', 'abstract': 'D ahead of his India visit, US President Donald Trump on Thursday said the two countries could make a “tremendous” trade deal. “We’re going to India, and we may make a tremendous deal there,” Trump said in his commencement address at the Hope for Prisoners Graduation Ceremony in Las Vegas. Trump, accompanied by First Lady Melania Trump, is scheduled to travel to Ahmedabad, Agra and New Delhi on February 24 and 25. Ahead of the visit, there have been talks about India and the United States agreeing on a trade package as a precursor to a major trade deal. During his commencement address, Trump indicated that the talks on this might slowdown if he did not get a good deal. “Maybe we’ll slow down. We’ll do it after the election. I think that could happen too. So, we’ll see what happens... But we’re only making deals if they’re good deals because we’re putting America first. Whether people like it or not, we’re putting America first,” Trump said. Bilateral India-US trade in goods and services is about three per cent of the US’ world trade. In a recent report, the Congressional Research Service (CRS) said the trading relationship is more consequential for India — in 2018 the United States was its second largest goods export market (16.0 per cent share) after the European Union (EU, 17.8 per cent), and third largest goods import supplier (6.3 per cent) after China (14.6 per cent) and the EU 28 (10.2 per cent). “The Trump Administration takes issue with the US trade deficit with India, and has criticised India for a range of ‘unfair’ trading practices,” the CRS said. “Indian Prime Minister Modi’s first term fell short of many observers’ expectations, as India did not move forward with anticipated market opening reforms, and instead increased tariffs and trade restrictions,” it said. “Modi’s strong electoral mandate may embolden the Indian Government to press ahead with its reform agenda with greater vigour. Slowing economic growth in India raises concerns about its business environment,” CRS said. As per a fact sheet issued by the Council on Foreign Relations (CFR), trade in goods and services between the two countries from 1999 to 2018 surged from $16 billion to $142 billion. India is now the United States’ eighth-largest trading partner in goods and services and is among the world’s largest economies. India’s trade with the United States now resembles, in terms of volume, the US’ trade with South Korea ($167 billion in 2018) or France ($129 billion), said Alyssa Ayres from CFR. “The United States for two years now has set out in stone pretty clearly the things that they wanted to see to try to get an agreement, and it’s basically then on India’s doorstep on whether they want to take those steps,” said Rick Rossow, Wadhwani Chair in US-India Policy Studies at the Center for Strategic and International Studies think-tank. “The list of US asks has been pretty static all throughout. Not to say that any of these things are easy for India to do, but the United States to my knowledge didn’t change the goalposts just because we now consider India to be a middleincome country. The things that we wanted to see happen to get this trade agreement have been pretty static all throughout, no matter how difficult they are,” he said.', 'corpus_id': 211237479, 'score': 0}, {'doc_id': '192895043', 'title': 'Monstrous Maternity: Folkloric Expressions of the Feminine in Images of the Ubume', 'abstract': None, 'corpus_id': 192895043, 'score': 1}, {'doc_id': '211268501', 'title': 'Undertrial escapes from police custody in Hoshiarpur', 'abstract': 'In his first comment on provocative statements by leaders of the Bharatiya Janata Party, Home Minister Amit Shah on Thursday said it was possible that such statements could have contributed to the party’s defeat in national capital Delhi. “They should not have given such statements,” Shah said at the Times Now Summit to questions on hate statements such as the “goli maro” slogan and comparing the Delhi election to a India vs Pak match. Shah said the BJP had distanced itself from these remarks and should not be construed as the party’s position. “There are all kinds of people in the fray,” he said. Union minister of state Anurag Thakur, BJP’s West Delhi MP Parvesh Verma and the BJP’s Model Town candidate Kapil Mishra were banned from campaigning by the election commission for these inflammatory slogans and statements. While referring to protesters at Shaheen Bagh, Thakur had led party supporters to chant the “goli maro” slogan and Verma had said that the protesters will “rape and kill your daughters and sisters”. Mishra, who called the election as a contest between “India and Pakistan” had also called the anti-CAA protest at Shaheen Bagh a “mini-Pakistan”. Asked about his speech where he asked voters to press the EVM buttons so hard that the Shaheen Bagh protesters would be jolted by its electric current, Amit Shah said people figured that he did not mean that people would be electrocuted. “It was in a manner of speaking to explain to people that victory (in the election) is linked to an ideology,” he said, adding that he counted it as a certificate that the opposition had only criticised one of his statements. Shah said it was impossible to outline the factors which go into people deciding to vote a particular way but it was quite possible that such statements had a negative fallout. Shah, however, underlined that the election result could not be counted as a mandate on the protest against the amended citizenship law at Delhi’s Shaheen Bagh. Elections are not won on a single issue, he said. The Shaheen Bagh protest, he said, was still an issue. “How should protests be held?... In a democratic country, what should be definition of a protest and what are the issues on which protests should be held... just as people of Shaheen Bagh have the right to articulate their view, so do we.,” he said.', 'corpus_id': 211268501, 'score': 0}, {'doc_id': '163381146', 'title': 'Otogi-Zoshi and Nara-Ehon: A Field of Study in Flux', 'abstract': None, 'corpus_id': 163381146, 'score': 1}, {'doc_id': '156049995', 'title': 'Japanese Demon Lore', 'abstract': None, 'corpus_id': 156049995, 'score': 1}, {'doc_id': '210999557', 'title': 'Speaker of Shura Council reiterates Qatar ’ s support of Palestinians ’ rights', 'abstract': 'His Highness the Amir Sheikh Tamim bin Hamad al-Thani yesterday attended the graduation ceremony of the 15th batch of students of Ahmed Bin Mohamed Military College. The graduation ceremony was attended by HE the Prime Minister and Minister of Interior Sheikh Khalid bin Khalifa bin Abdulaziz al-Thani, ministers, offi cials of military colleges from brotherly and friendly countries, guests and parents of the graduates. Upon the Amir’s arrival, the Qatari national anthem was played. The Amir then inspected a guard of honour by 119 graduates from Qatar and several sisterly countries, such as Kuwait, Jordan, Sudan, Tunisia, Libya and Mauritania. The Amir then honoured ten outstanding graduates. The 15th batch handed over the fl ag to the 16th batch, and the order for promotions was then read out. Commander of the Military College Staff Colonel Abdulhadi Mohamed al-Hajri said the graduating batch felt honoured by the Amir’s presence. The Qatari graduates, he said, would join the Armed Forces, Amiri Guard, Internal Security Force (Lekhwiya), State Security Bureau and others would join their countries’ military wings. They have carried out internal and external field training and exercises with determination and courage in order to follow the path of previous generations, Staff Colonel al-Hajri added. He said as a continuation of the work plan and achievements in the College, the third and fourth courses of the undergraduate candidates diploma graduated on Wednesday. Last September, the College received the 19th batch, he said. Staff Colonel al-Hajri noted that, in pursuit of the Amir’s vision, the College will be a meeting place to enhance brotherhood and friendship with brotherly and friendly countries, and that members from Iraq and Rwanda joined for the fi rst time with this batch. He added that an exercise entitled ‘Oryx Endeavour’ was carried out by cadets of the Royal Military Academy Sandhurst in co-operation with the Ahmed Bin Mohamed Military College in November 2019 where the imHis Highness the Amir Sheikh Tamim bin Hamad al-Thani attends the graduation ceremony of the 15th batch of students of Ahmed Bin Mohamed Military College yesterday.', 'corpus_id': 210999557, 'score': 0}, {'doc_id': '214619981', 'title': 'Assassination of Iranian hero activates end of U . S . presence in region “ The Orientalist ” on Henry Corbin to premiere in Tehran 12 Recent crucial events determine course of history', 'abstract': 'The millions-strong protest in Baghdad on Friday was a continuation of the “hard revenge” process over the assassination of senior commanders of the resistance movement. This is while that over the past three weeks and after the approval of a resolution calling for the expulsion of U.S. troops by Iraq’s parliament, the Western media outlets had been claiming that the resolution is not backed by people. The Iraqi people, in a historic rally reminiscent of the pullout of British troops from the country in 1920, positively responded to the referendum on the expulsion of American terrorists. They also showed that hatred for the Americans had nothing to do with the views of other countries, including Iran. The anti-U.S. demonstration, which also received full support from Iraq’s religious authority, was reflected widely in various media outlets around the world. The clear message of the Iraqi people in the Friday demonstration in support of the parliamentary resolution and explicit opposition to the presence of American terrorists in their country were in fact a response to two weeks of hostile U.S. efforts to undermine the resolution. The protest was of great importance because the U.S. officials had claimed that Iraqis have no objection to the U.S. military presence in their country, but their objection is to the presence of others. This is a significant message as over the past four months the hegemonic system has used all its potential by launching a plan based on hybrid warfare model as one of the most modern fourth-generation warfare to properly carry out the project of “regional partition”. In this model, violent protests using symbols effective at inciting the youth to riot like the Joker, full media-intelligence support, and even using some ISIS elements are among the intended operations aimed at creating power vacuum and disrupting order in the countries classified as the axis of resistance. 3 A president’s childish wishes and the lies that damage the American people’s psyche', 'corpus_id': 214619981, 'score': 0}, {'doc_id': '214662256', 'title': 'A qualitative look into the importance of singing ability in Japanese and Finnish karaoke', 'abstract': 'This research was conducted to gain an understanding into how singers of the Japanese-invented form of singing entertainment known as karaoke view the importance of singing ability in karaoke, and what they consider to be the respective roles of the singer and the listener. A total of 6 semi-structured interviews were conducted on 3 Japanese informants and 3 Finnish informants to form 6 separate life history case studies, which were then thematically analyzed, and common themes identified. The study found that greater self-rated singing ability of a respondent correlated with higher perceived importance of singing ability, and that a singer is expected to consider the listeners’ preferences and the mood of the occasion when deciding on which song to sing, but that lower singing ability is generally not a role conflict. It was also found that a Finnish-style karaoke bar is actually a low pressure singing environment when compared to the more intimate Japanese forms of karaoke, due to it being common and acceptable for listeners to not pay attention to the singer, while in Japanese karaoke social cohesion is higher.', 'corpus_id': 214662256, 'score': 1}, {'doc_id': '162901090', 'title': 'Warrior/Monk, Demon/Saint: Humor and Parody in the Late Medieval Tale of Benkei', 'abstract': 'The late medieval tale known as Benkei monogatari 弁慶物語 (The Tale of Benkei) recounts and embellishes the life and adventures of Saitō no Musashibō Benkei 西塔武蔵坊弁慶 (?–1189) from his birth through his engagement as the trusted retainer of the hero Minamoto no Yoshitsune 源義経 (1159–1189). In comparison with Yoshitsune, whose historical existence is solidly documented, not much is known about the real Benkei; other than a brief mention in Azuma kagami 吾妻鏡 (Mirror of the East), his name does not appear in any nonliterary documents.1 Though factual information about Benkei is scarce, stories about him abound, forming a rich and varied corpus of works produced in multiple historical periods and different regions of Japan—a corpus that I refer to as the Benkei legend.2 Benkei monogatari, a tale existing in numerous variants, encompasses what appears to be the first comic renderings of the legend; the variants of the tale have been only briefly treated in Western languages.3 This article explores one of these versions, a latesixteenth-century text known as Musashibō e-engi 武蔵坊絵縁起 (The Illustrated Story', 'corpus_id': 162901090, 'score': 1}]
34	{'doc_id': '96435050', 'title': 'Convergent regulatory evolution and loss of flight in paleognathous birds', 'abstract': 'All roads lead to regulation Species from widely divergent taxa can experience similar changes in traits. What underlying genetic drivers cause these parallel changes remains an open question. Sackton et al. looked across groups of birds that have repeatedly lost flight, the ratites and tinamous, and found that there is convergence in the regulatory regions associated with genes related to flight, but not within the protein coding regions. Changes within these regulatory regions influenced limb development and may represent quick paths toward convergent change across taxa. Science, this issue p. 74 Changes in regulatory regions led to the evolution of flightlessness in birds. A core question in evolutionary biology is whether convergent phenotypic evolution is driven by convergent molecular changes in proteins or regulatory regions. We combined phylogenomic, developmental, and epigenomic analysis of 11 new genomes of paleognathous birds, including an extinct moa, to show that convergent evolution of regulatory regions, more so than protein-coding genes, is prevalent among developmental pathways associated with independent losses of flight. A Bayesian analysis of 284,001 conserved noncoding elements, 60,665 of which are corroborated as enhancers by open chromatin states during development, identified 2355 independent accelerations along lineages of flightless paleognaths, with functional consequences for driving gene expression in the developing forelimb. Our results suggest that the genomic landscape associated with morphological convergence in ratites has a substantial shared regulatory component.', 'corpus_id': 96435050}	16303	"[{'doc_id': '220309142', 'title': 'A fully-automated method discovers loss of mouse-lethal and human-monogenic disease genes in 58 mammals', 'abstract': 'Abstract Gene losses provide an insightful route for studying the morphological and physiological adaptations of species, but their discovery is challenging. Existing genome annotation tools focus on annotating intact genes and do not attempt to distinguish nonfunctional genes from genes missing annotation due to sequencing and assembly artifacts. Previous attempts to annotate gene losses have required significant manual curation, which hampers their scalability for the ever-increasing deluge of newly sequenced genomes. Using extreme sequence erosion (amino acid deletions and substitutions) and sister species support as an unambiguous signature of loss, we developed an automated approach for detecting high-confidence gene loss events across a species tree. Our approach relies solely on gene annotation in a single reference genome, raw assemblies for the remaining species to analyze, and the associated phylogenetic tree for all organisms involved. Using human as reference, we discovered over 400 unique human ortholog erosion events across 58 mammals. This includes dozens of clade-specific losses of genes that result in early mouse lethality or are associated with severe human congenital diseases. Our discoveries yield intriguing potential for translational medical genetics and evolutionary biology, and our approach is readily applicable to large-scale genome sequencing efforts across the tree of life.', 'corpus_id': 220309142, 'score': 1}, {'doc_id': '233328739', 'title': 'Convergent selection on juvenile hormone signaling is associated with the evolution of eusociality in bees', 'abstract': 'Life’s most dramatic innovations, from the emergence of self-replicating molecules to highly-integrated societies, often involve increases in biological complexity. Some groups traverse different levels of complexity, providing a framework to identify key factors shaping these evolutionary transitions. Halictid bees span the transition from individual to group reproduction, with repeated gains and losses of eusociality. We generated chromosome-length genome assemblies for 17 species and searched for genes that both experienced positive selection when eusociality arose and relaxed selection when eusociality was secondarily lost. Loci exhibiting these complementary evolutionary signatures are predicted to carry costs outweighed by their importance for traits in eusocial lineages. Strikingly, these loci included two proteins that bind and transport juvenile hormone (JH) – a key regulator of insect development and reproduction. Though changes in JH abundance are frequently associated with polymorphisms, the mechanisms coupling JH to novel phenotypes are not well understood. Our results suggest novel links between JH and eusociality arose in halictids by altering transport and availability of JH in a tissue-specific manner, including in the brain. Through genomic comparisons of species encompassing both the emergence and breakdown of eusociality, we provide insights into the mechanisms targeted by selection to shape a key evolutionary transition.', 'corpus_id': 233328739, 'score': 0}, {'doc_id': '203622006', 'title': 'A functional enrichment test for molecular convergent evolution finds a clear protein-coding signal in echolocating bats and whales', 'abstract': 'Significance Echolocation is a prime example of convergent evolution, the independent gain of similar features in species of different lineages. Is phenotypic convergence driven by underlying molecular convergence? If so, could molecular convergence include contributions from highly constrained, often-pleotropic, coding regions? We develop a generalizable test that offers a resounding “yes” to both extensively debated questions. Our test highlights molecular convergence in genes regulating the cochlear ganglion of echolocating bats and whales, the skin of aquatic mammals, and the lung of high-altitude mammals. Importantly, the approach correctly dismisses confounding convergence-like patterns, such as those from sequence decay of vision genes in blind subterranean species, and is readily applicable to the thousands of genomes sequenced across the tree of life. Distantly related species entering similar biological niches often adapt by evolving similar morphological and physiological characters. How much genomic molecular convergence (particularly of highly constrained coding sequence) contributes to convergent phenotypic evolution, such as echolocation in bats and whales, is a long-standing fundamental question. Like others, we find that convergent amino acid substitutions are not more abundant in echolocating mammals compared to their outgroups. However, we also ask a more informative question about the genomic distribution of convergent substitutions by devising a test to determine which, if any, of more than 4,000 tissue-affecting gene sets is most statistically enriched with convergent substitutions. We find that the gene set most overrepresented (q-value = 2.2e-3) with convergent substitutions in echolocators, affecting 18 genes, regulates development of the cochlear ganglion, a structure with empirically supported relevance to echolocation. Conversely, when comparing to nonecholocating outgroups, no significant gene set enrichment exists. For aquatic and high-altitude mammals, our analysis highlights 15 and 16 genes from the gene sets most affected by molecular convergence which regulate skin and lung physiology, respectively. Importantly, our test requires that the most convergence-enriched set cannot also be enriched for divergent substitutions, such as in the pattern produced by inactivated vision genes in subterranean mammals. Showing a clear role for adaptive protein-coding molecular convergence, we discover nearly 2,600 convergent positions, highlight 77 of them in 3 organs, and provide code to investigate other clades across the tree of life.', 'corpus_id': 203622006, 'score': 1}, {'doc_id': '232485270', 'title': 'Genomic and anatomical comparisons of skin support independent adaptation to life in water by cetaceans and hippos', 'abstract': 'The macroevolutionary transition from terra firma to obligatory inhabitance of the marine hydrosphere has occurred twice in the history of Mammalia: Cetacea and Sirenia. In the case of Cetacea (whales, dolphins, and porpoises), molecular phylogenies provide unambiguous evidence that fully aquatic cetaceans and semiaquatic hippopotamids (hippos) are each other\'s closest living relatives. Ancestral reconstructions suggest that some adaptations to the aquatic realm evolved in the common ancestor of Cetancodonta (Cetacea\xa0+ Hippopotamidae). An alternative hypothesis is that these adaptations evolved independently in cetaceans and hippos. Here, we focus on the integumentary system and evaluate these hypotheses by integrating new histological data for cetaceans and hippos, the first genome-scale data for pygmy hippopotamus, and comprehensive genomic screens and molecular evolutionary analyses for protein-coding genes that have been inactivated in hippos and cetaceans. We identified eight skin-related genes that are inactivated in both cetaceans and hippos, including genes that are related to sebaceous glands, hair follicles, and epidermal differentiation. However, none of these genes exhibit inactivating mutations that are shared by cetaceans and hippos. Mean dates for the inactivation of skin genes in these two clades serve as proxies for phenotypic changes and suggest that hair reduction/loss, the loss of sebaceous glands, and changes to the keratinization program occurred ∼16 Ma earlier in cetaceans (∼46.5 Ma) than in hippos (∼30.5 Ma). These results, together with histological differences in the integument and prior analyses of oxygen isotopes from stem hippopotamids (""anthracotheres""), support the hypothesis that aquatic skin adaptations evolved independently in hippos and cetaceans.', 'corpus_id': 232485270, 'score': 1}, {'doc_id': '232313442', 'title': 'ActiveDriverDB: Interpreting Genetic Variation in Human and Cancer Genomes Using Post-translational Modification Sites and Signaling Networks (2021 Update)', 'abstract': 'Deciphering the functional impact of genetic variation is required to understand phenotypic diversity and the molecular mechanisms of inherited disease and cancer. While millions of genetic variants are now mapped in genome sequencing projects, distinguishing functional variants remains a major challenge. Protein-coding variation can be interpreted using post-translational modification (PTM) sites that are core components of cellular signaling networks controlling molecular processes and pathways. ActiveDriverDB is an interactive proteo-genomics database that uses more than 260,000 experimentally detected PTM sites to predict the functional impact of genetic variation in disease, cancer and the human population. Using machine learning tools, we prioritize proteins and pathways with enriched PTM-specific amino acid substitutions that potentially rewire signaling networks via induced or disrupted short linear motifs of kinase binding. We then map these effects to site-specific protein interaction networks and drug targets. In the 2021 update, we increased the PTM datasets by nearly 50%, included glycosylation, sumoylation and succinylation as new types of PTMs, and updated the workflows to interpret inherited disease mutations. We added a recent phosphoproteomics dataset reflecting the cellular response to SARS-CoV-2 to predict the impact of human genetic variation on COVID-19 infection and disease course. Overall, we estimate that 16-21% of known amino acid substitutions affect PTM sites among pathogenic disease mutations, somatic mutations in cancer genomes and germline variants in the human population. These data underline the potential of interpreting genetic variation through the lens of PTMs and signaling networks. The open-source database is freely available at www.ActiveDriverDB.org.', 'corpus_id': 232313442, 'score': 0}, {'doc_id': '232283165', 'title': 'Ecological correlates of gene family size: the draft genome of the redheaded', 'abstract': None, 'corpus_id': 232283165, 'score': 0}, {'doc_id': '232224270', 'title': 'Complementary evolution of coding and noncoding sequence underlies mammalian hairlessness', 'abstract': 'Body hair is a defining mammalian characteristic, but several mammals, such as whales, naked mole-rats, and humans, have notably less hair than others. To find the genetic basis of reduced hair quantity, we used our evolutionary-rates-based method, RERconverge, to identify coding and noncoding sequences that evolve at significantly different rates in so-called hairless mammals compared to hairy mammals. Using RERconverge, we performed an unbiased, genome-wide scan over 62 mammal species using 19,149 genes and 343,598 conserved noncoding regions to find genetic elements that evolve at significantly different rates in hairless mammals compared to hairy mammals. We show that these rate shifts resulted from relaxation of evolutionary constraint on hair-related sequences in hairless species. In addition to detecting known and potential novel hair-related genes, we also discovered hundreds of putative hair-related regulatory elements. Computational investigation revealed that genes and their associated noncoding regions show different evolutionary patterns and influence different aspects of hair growth and development. Many genes under accelerated evolution are associated with the structure of the hair shaft itself, while evolutionary rate shifts in noncoding regions also included the dermal papilla and matrix regions of the hair follicle that contribute to hair growth and cycling. Genes that were top-ranked for coding sequence acceleration included known hair and skin genes KRT2, KRT35, PKP1, and PTPRM that surprisingly showed no signals of evolutionary rate shifts in nearby noncoding regions. Conversely, accelerated noncoding regions are most strongly enriched near regulatory hair-related genes and microRNAs, such as mir205, ELF3, and FOXC1, that themselves do not show rate shifts in their protein-coding sequences. Such dichotomy highlights the interplay between the evolution of protein sequence and regulatory sequence to contribute to the emergence of a convergent phenotype.', 'corpus_id': 232224270, 'score': 1}, {'doc_id': '233015559', 'title': 'Single individual structural variant detection uncovers widespread hemizygosity in molluscs', 'abstract': 'The advent of complete genomic sequencing has opened a window into genomic phenomena obscured by fragmented assemblies. A good example of these is the existence of hemizygous regions of autosomal chromosomes, which can result in marked differences in gene content between individuals within species. While these hemizygous regions, and presence/absence variation of genes that can result, are well known in plants, firm evidence has only recently emerged for their existence in metazoans. Here, we use recently published, complete genomes from wild-caught molluscs to investigate the prevalence of hemizygosity across a well-known and ecologically important clade. We show that hemizygous regions are widespread in mollusc genomes, not clustered in individual chromosomes, and often contain genes linked to transposition, DNA repair and stress response. With targeted investigations of HSP70-12 and C1qDC, we also show how individual gene families are distributed within pan-genomes. This work suggests that extensive pan-genomes are widespread across the conchiferan Mollusca, and represent useful tools for genomic evolution, allowing the maintenance of additional genetic diversity within the population. As genomic sequencing and re-sequencing becomes more routine, the prevalence of hemizygosity, and its impact on selection and adaptation, are key targets for research across the tree of life. This article is part of the Theo Murphy meeting issue ‘Molluscan genomics: broad insights and future directions for a neglected phylum’.', 'corpus_id': 233015559, 'score': 0}, {'doc_id': '233246520', 'title': 'Intron Losses and Gains in Nematodes: Not Eccentric at All', 'abstract': 'The evolution of spliceosomal introns has been widely studied among various eukaryotic groups. Researchers nearly reached the consensuses on the pattern and the mechanisms of intron losses and gains across eukaryotes. However, according to previous studies that analyzed a few genes or genomes of nematodes, Nematoda seem to be an eccentric group. Taking advantage of the recent accumulation of sequenced genomes, we carried out an extensive analysis on the intron losses and gains using 104 nematodes genomes across all the five Clades of the phylum. Nematodes have a wide range of intron density, from less than one to more than nine per 1kbp coding sequence. The rates of intron losses and gains exhibit significant heterogeneity both across different nematode lineages and across different evolutionary stages of the same lineage. The frequency of intron losses far exceeds that of intron gains. Five pieces of evidence supporting the model of cDNA-mediated intron loss have been observed in ten Caenorhabditis species, the dominance of the precise intron losses, frequent loss of adjacent introns, and high-level expression of the intron-lost genes, preferential losses of short introns, and the preferential losses of introns close to 3′-ends of genes. Like studies in most eukaryotic groups, we cannot find the source sequences for the limited number of intron gains detected in the Caenorhabditis genomes. All the results indicate that nematodes are a typical eukaryotic group rather than an outlier in intron evolution.', 'corpus_id': 233246520, 'score': 0}, {'doc_id': '232200338', 'title': 'Novel regulators of growth identified in the evolution of fin proportion in flying fish', 'abstract': 'Identifying the genetic foundations of trait variation and evolution is challenging as it is often difficult to parse meaningful signals from confounding signatures such as drift and epistasis. However, identification of the genetic loci underlying morphological and physiological traits can be honed through the use of comparative and complementary genetic approaches, whereby shared sets of genes that are repeatedly implicated across large evolutionary time periods as under selection can illuminate important pathways and epistatic relationships that function as novel regulators of trait development. Here we intersect comparative genomic analyses with unbiased mutagenesis screens in distantly related species to define the control of proportional growth, as changes in the size and relative proportions of tissues underlie a large degree of the variant forms seen in nature. Through a phylogenomic analysis of genome-wide variation in 35 species of flying fishes and relatives, we identify genetic signatures in both coding and regulatory regions underlying the convergent evolution of increased paired fin size and aerial gliding behaviors, key innovations for flying fishes and flying halfbeaks. To refine our analysis, we intersected convergent phylogenomic signatures with mutants identified in distantly related zebrafish with altered fin size. Through these paired approaches, we identify a surprising role for an L-type amino acid transporter, lat4a, and the potassium channel, kcnh2a, in the regulation of fin proportion. We show that specific epistatic interaction between these genetic loci in zebrafish closely phenocopies the observed fin proportions of flying fishes. The congruence of experimental and phylogenomic findings point to a conserved, non-canonical signaling interaction that integrates bioelectric cues and amino acid transport in the establishment of relative size in development and evolution.', 'corpus_id': 232200338, 'score': 0}]"
35	{'doc_id': '169838937', 'title': 'The Flash Crash: High-Frequency Trading in an Electronic Market', 'abstract': 'We study intraday market intermediation in an electronic market before and during a period of large and temporary selling pressure. On May 6, 2010, U.S. financial markets experienced a systemic intraday event, known as the Flash Crash, when a large automated sell program was rapidly executed in the E-mini S&P 500 stock index futures market. Using audit trail transaction-level data for the E-mini on May 6 and the previous three days, we find that the trading pattern of the most active non-designated intraday intermediaries (classified as High Frequency Traders) did not change when prices fell during the Flash Crash.', 'corpus_id': 169838937}	13907	"[{'doc_id': '228097218', 'title': 'Exposure to the COVID-19 Stock Market Crash and Its Effect on Household Expectations', 'abstract': 'We survey a representative sample of US households to study how exposure to the COVID-19 stock market crash affects expectations and planned behavior. Wealth shocks are associated with upward adjustments of expectations about retirement age, desired working hours, and household debt, but have only small effects on expected spending. We provide correlational and experimental evidence that beliefs about the duration of the stock market recovery shape households’ expectations about their own wealth and their planned investment decisions and labor market activity. Our findings shed light on the implications of household exposure to stock market crashes for expectation formation.', 'corpus_id': 228097218, 'score': 0}, {'doc_id': '166886563', 'title': 'The stock market flash crash of 2010', 'abstract': 'Preface Findings Regarding the Market Events of May 6, 2010: Report of the Staffs of the CFTC & SEC to the Joint Advisory Committee on Emerging Regulatory Issues Preliminary Findings Regarding the Market Events of May 6, 2010: Report of the Staffs of the CFTC & SEC to the Joint Advisory Committee on Emerging Regulatory Issues Index.', 'corpus_id': 166886563, 'score': 1}, {'doc_id': '220481362', 'title': 'A dual thiourea-appended perylenebisimide ""turn-on"" fluorescent chemosensor with high selectivity and sensitivity for Hg2+ in living cells.', 'abstract': ""Sensing heavy metal ions particularly for the most toxic Hg2+ is a long-term pursuit for chemists because of its obvious and extreme harmfulness to both the environment and human health. Herein, a novel 'turn-on' perylenebisimide-thiourea fluorescent probe PBI-BTB is achieved for rapid detection of Hg2+ in a DMSO/H2O (5/1, v/v) solution through a typical Hg2+-promoting desulfurization reaction, which has been investigated through Job's plot titration, FT-IR, 1H NMR and HRMS analysis. A remarkable fluorescence emission enhancement at 540 and 580\xa0nm is observed in the presence of Hg2+, which is visible to the naked eye with high selectivity and sensitivity. Moreover, probe PBI-BTB combined strong anti-interference recognition with short response time (< 1\xa0min). The rapid fluorescence response with low limit of detection (0.35\xa0μM) in a wide pH range of 3.0-11.0 makes PBI-BTB a promising candidate for detection of Hg2+ without any buffer system. Furthermore, the practicability of probe PBI-BTB upon the Hg2+ recognition in human liver cancer cells (HepG-2) has been studied through fluorescent live cell imaging which reveals the probe's low toxicity to organism as well as the favorable cell permeability of PBI-BTB for detecting Hg2+ in biological systems."", 'corpus_id': 220481362, 'score': 0}, {'doc_id': '219047976', 'title': 'Stock Returns and Roughness Extreme Variations: A New Model for Monitoring 2008 Market Crash and 2015 Flash Crash', 'abstract': 'We use Student’s t -copula to study the extreme variations in the bivariate kinematic time series of log–return and log–roughness of the S&P 500 index during two market crashes, the financial crisis in 2008 and the flash crash on Monday August 24, 2015. The stable and small values of the tail dependence index observed for some months preceding the market crash of 2008 indicate that the joint distribution of daily return and roughness was close to a normal one. The volatility of the tail and degree of freedom indices as determined by Student’s t -copula falls down substantially after the stock market crash of 2008. The number of degrees of freedom in the empirically observed distributions falls while the tail coefficient of the copula increases, indicating the long memory effect of the market crash of 2008. A significant change in the tail and degree of freedom indices associated with the intraday price of S&P 500 index is observed before, during, and after the flash crash on August 24, 2015. The long memory effect of the stock market flash crash of August 2015 is indicated by the number of degrees of freedom in the empirically observed distributions fall while the tail coefficient of the joint distribution increases after the flash crash. The small and stable value of degrees of freedom preceding the flash crash provides evidence that the joint distribution for intraday data of return and roughness is heavy-tailed. Time-varying long-range dependence in mean and volatility as well as the Chow and Bai-Perron tests indicate non-stability of the stock market in this period.', 'corpus_id': 219047976, 'score': 1}, {'doc_id': '231653479', 'title': 'The joint dynamics of investor beliefs and trading during the COVID-19 crash', 'abstract': 'Significance We analyze how investor expectations about economic growth and stock returns changed during the February−March 2020 stock market crash induced by the COVID-19 pandemic, as well as during the subsequent partial stock market recovery. Our results provide guidance for the design of macro and finance models and related economic policies. We analyze how investor expectations about economic growth and stock returns changed during the February−March 2020 stock market crash induced by the COVID-19 pandemic, as well as during the subsequent partial stock market recovery. We surveyed retail investors who are clients of Vanguard at three points in time: 1) on February 11–12, around the all-time stock market high, 2) on March 11–12, after the stock market had collapsed by over 20%, and 3) on April 16–17, after the market had rallied 25% from its lowest point. Following the crash, the average investor turned more pessimistic about the short-run performance of both the stock market and the real economy. Investors also perceived higher probabilities of both further extreme stock market declines and large declines in short-run real economic activity. In contrast, investor expectations about long-run (10-y) economic and stock market outcomes remained largely unchanged, and, if anything, improved. Disagreement among investors about economic and stock market outcomes also increased substantially following the stock market crash, with the disagreement persisting through the partial market recovery. Those respondents who were the most optimistic in February saw the largest decline in expectations and sold the most equity. Those respondents who were the most pessimistic in February largely left their portfolios unchanged during and after the crash.', 'corpus_id': 231653479, 'score': 1}, {'doc_id': '229248829', 'title': 'Time-varying dependence between stock markets and oil prices during COVID-19: The case of net oil-exporting countries', 'abstract': 'This article provides an empirical investigation of the time-varying dependence between oil prices and stock markets in the top ten net oil-exporting countries. Using daily data focusing on COVID-19 period, we implement the DCC-GARCH to identify the dynamic dependence. Then, we apply structural break techniques to detect the shift in the dependence structure. We find that there exists a positive time-varying dependence between oil returns and stock returns during the ongoing COVID-19 pandemic wherein the breakpoints mostly coincided with the emergence of oil price war and global stock market crash. Overall, results imply that declining oil prices lead to a fall in stock returns due to lower future earnings for oil companies, exhibiting a signal of reduction in aggregate demand and economic activity in oil-exporting countries. Thus, the high positive co-movement may have ill-effects on portfolio diversification, as the latter will be less effective if the asset returns are highly correlated.', 'corpus_id': 229248829, 'score': 0}, {'doc_id': '227247852', 'title': 'A Study on the Efficiency of the Indian Stock Market', 'abstract': 'The efficiency of the stock market has a significant impact on the potential return on investment. An efficient market eliminates the possibility of arbitrage and unexploited profit opportunities. This study analyzes the weak form efficiency of the Indian Stock market based on the two major Indian stock exchanges, viz., BSE and NSE. The daily closing values of Sensex and Nifty indices for the period from April 2010 to March 2019 are used to perform the Runs test, the Autocorrelation test, and the Autoregression test. The study confirms that the Indian Stock market is weak form inefficient and can thus be outperformed.', 'corpus_id': 227247852, 'score': 0}, {'doc_id': '214181927', 'title': 'Sistem Pakar Penanganan Kasus Sengketa Tanah Menggunakan Metode Backward Chaining', 'abstract': 'Land dispute is a conflict between two or more parties that have different interests in one or several objects of land rights that can result in legal consequences for both. Problems to explain government regulations and legislation relating to land dispute cases in a simple way that is, explained the rules that correspond to the problems that occur. This makes it difficult for a lawyer to deal with many clients and different cases. The purpose of this study is to design the Expert System for Handling Land Dispute Cases using the Backward Chaining Method. The study was conducted to analyze the existing needs, collect data, design, implement the results of the design into the application that is used, after that will be tested and maintained the system. The results of the study show that the existence of this expert system, it provides very meaningful benefits for the client, namely facilitating the client to carry out consultations regarding the handling of land dispute cases.', 'corpus_id': 214181927, 'score': 0}, {'doc_id': '231573393', 'title': 'The ‘COVID’ crash of the 2020 U.S. Stock market', 'abstract': 'Abstract We employed the log-periodic power law singularity (LPPLS) methodology to systematically investigate the 2020 stock market crash in the U.S. equities sectors with different levels of total market capitalizations through four major U.S. stock market indexes, including the Wilshire 5000 Total Market index, the SP the 2020 U.S. stock market crash originated from a bubble that had begun to form as early as September 2018; and the bubble profiles for stocks with different levels of total market capitalizations have distinct temporal patterns. This study not only sheds new light on the makings of the 2020 U.S. stock market crash but also creates a novel pipeline for future real-time crash detection and mechanism dissection of any financial market and/or economic index.', 'corpus_id': 231573393, 'score': 1}, {'doc_id': '229415998', 'title': 'Trust and stock market volatility during the COVID-19 crisis', 'abstract': 'Abstract We investigate if trust affects global stock market volatility during the COVID-19 pandemic. Using a sample of 47 national stock markets, we find the stock markets’ volatility to be significantly lower in high-trust countries (in reaction to COVID-19 case announcements). Both trust in fellow citizens as well as in the countries’ governments are of significant importance.', 'corpus_id': 229415998, 'score': 1}]"
36	"{'doc_id': '3813029', 'title': 'Real-world evidence in the treatment of ovarian cancer', 'abstract': ""Introduction\n'Real-world evidence (RWE)' refers to information on the utilization and outcome of new therapies and technologies in clinical practice. RWE may include single institution cohort studies, population-based health services studies, or (inter)national data on survival and mortality. This paper reviews RWE on the impact of treatment in ovarian cancer.\n\n\nMaterials and methods\nA literature review of publications addressing population level survival outcomes of new surgical and systemic treatment interventions in ovarian cancer was undertaken. In addition, literature and international cancer registry trends in ovarian cancer survival, mortality and incidence rates were compiled. These latter were utilized to make inferences on the relative impact of new treatments as well as changing incidence rates on observed mortality trends.\n\n\nResults\nThe last four decades have seen new systemic and surgical treatments introduced into practice for ovarian cancer based on randomized trial evidence. However, there has been little published on population level uptake and survival outcomes of those interventions. Exceptions were population studies on intraperitoneal chemotherapy and neoadjuvant chemotherapy. One paper demonstrated modest uptake of intraperitoneal chemotherapy and evidence of improved survival. Cancer registry statistics revealed falling incidence rates (∼1%-2% per year) for ovarian cancer across Europe, North America and elsewhere over the last three to four decades. Mortality rates also declined by ∼1%-2% per year over this period. Population 5-year relative survival estimates also improved over this period [from 33.7% in 1975 to 46.2% in 2008 (SEER data)].\n\n\nConclusions\nThere are few RWE studies of specific treatments in ovarian cancer. Trends in relative survival and population mortality have shown improvements. Mortality changes can be explained in part by reductions in ovarian cancer incidence rates (speculated to be due to use of oral contraceptives and reduction in postmenopausal hormone use). However, it is plausible that at least some of the mortality reduction is related to improved survival of patients with the introduction of effective new treatments."", 'corpus_id': 3813029}"	17827	[{'doc_id': '235205365', 'title': 'Management of advanced ovarian cancer in Spain: an expert Delphi consensus', 'abstract': 'Background To determine the state of current practice and to reach a consensus on recommendations for the management of advanced ovarian cancer using a Delphi survey with a group of Spanish gynecologists and medical oncologists specially dedicated to gynecological tumors. Methods The questionnaire was developed by the byline authors. All questions but one were answered using a 9-item Likert-like scale with three types of answers: frequency, relevance and agreement. We performed two rounds between December 2018 and July 2019. A consensus was considered reached when at least 75% of the answers were located within three consecutive points of the Likert scale. Results In the first round, 32 oncologists and gynecologists were invited to participate, and 31 (96.9%) completed the online questionnaire. In the second round, 27 (87.1%) completed the online questionnaire. The results for the questions on first-line management of advanced disease, treatment of patients with recurrent disease for whom platinum might be the best option, and treatment of patients with recurrent disease for whom platinum might not be the best option are presented. Conclusions This survey shows a snapshot of current recommendations by this selected group of physicians. Although the majority of the agreements and recommendations are aligned with the recently published ESMO-ESGO consensus, there are some discrepancies that can be explained by differences in the interpretation of certain clinical trials, reimbursement or accessibility issues.', 'corpus_id': 235205365, 'score': 0}, {'doc_id': '234471422', 'title': 'Drug development in the era of precision medicine in oncology-Clues beyond randomization.', 'abstract': 'The primary objective of any treatment in oncology is the improve patients’ overall survival (OS) and/or quality of life (QoL). Patients with solid tumors may often be cured thanks to local treatments including surgery and radiotherapy when then are free of distant metastases. In this setting, anticancer drugs may improve cure rates when combined to local treatments. In the recurrent and/ or metastatic setting, drugs represent the main treatment option, while surgery and radiotherapy might still be used in a palliative intent in most cases. With the exception of germline tumors and lymphoma, drugs have a limited ability to cure patients in this setting, and patients most often need to receive sequential treatments for life. In the ancestral paradigm of drug development in oncology, drugs used to be developed per cancer type following a well-established path that well suited chemotherapeutic agents for which antitumor activity largely depended on cancer types in preclinical models (Table 1). The first evaluation of new drugs in patients in phase I clinical trials was usually performed in patients who had exhausted standard of care, and not in healthy volunteers given the toxic nature of the drugs and their narrow therapeutic index. Dose escalation used to be open to patients with any type of cancer however, in order not to miss a serendipitous antitumor activity in unexpected cancer types. Phase I trials were not randomized, and enabled to establish the schedule and the recommended phase II dose that was the highest dose to be considered safe for further evaluation. Based on antitumor activity observed in preclinical models and during phase I trials, preliminary drug efficacy was then assessed in a specific cancer type and setting (usually in the recurrent and/or metastatic setting) in single-arm or randomized phase II clinical trials. Surrogate endpoints such as the objective response rate (ORR) or progression-free survival (PFS) were commonly used to have a rapid read-out of the efficacy. These surrogate endpoints were assessed by following over time the tumor burden, formerly the sum of the product of the two diameters of the target lesions (WHO criteria) (1), and more recently the sum of the largest diameter of up to five target lesions (RECIST) (2). Randomized phase III clinical trials were performed in a similar patient population in order to demonstrate an improvement in OS and/or QoL of the new treatment over the current standard of care. Randomization is the gold-standard approach for market access in order to avoid selection biases. Recently, seamless drug development led to increasingly replace phase II clinical trials with large expansion cohorts performed during phase I trials, in order to accelerate drug development (3). This paradigm based on the randomization for drug approval nicely fitted to drugs developed in specific but common cancer types, but is less suited to rare cancer types. A better understanding of cancer biology led to the development of molecularly targeted agents (MTAs) that were specifically designed to modulate a molecular pathway in the tumor cells or their microenvironment, and Editorial', 'corpus_id': 234471422, 'score': 1}, {'doc_id': '234489208', 'title': 'Chemotherapy-induced neutropenia and treatment efficacy in advanced non-small-cell lung cancer: a pooled analysis of 6 randomized trials', 'abstract': 'Background Chemotherapy-induced neutropenia (CIN) has been demonstrated to be a prognostic factor in several cancer conditions. We previously found a significant prognostic value of CIN on overall survival (OS), in a pooled dataset of patients with advanced non-small-cell lung cancer (NSCLC) receiving first line chemotherapy from 1996 to 2001. However, the prognostic role of CIN in NSCLC is still debated. Methods We performed a post hoc analysis pooling data prospectively collected in six randomized phase 3 trials in NSCLC conducted from 2002 to 2016. Patients who never started chemotherapy and those for whom toxicity data were missing were excluded. Neutropenia was categorized on the basis of worst grade during chemotherapy: absent (grade 0), mild (grade 1–2), or severe (grade 3–4). The primary endpoint was OS. Multivariable Cox model was applied for statistical analyses. In the primary analysis, a minimum time (landmark) at 180\u2009days from randomization was applied in order to minimize the time-dependent bias. Results Overall, 1529 patients, who received chemotherapy, were eligible; 572 of them (who received 6\u2009cycles of treatment) represented the landmark population. Severe CIN was reported in 143 (25.0%) patients and mild CIN in 135 (23.6%). At multivariable OS analysis, CIN was significantly predictive of prognosis although its prognostic value was entirely driven by severe CIN (hazard ratio [HR] of death 0.71; 95%CI: 0.53–0.95) while it was not evident with mild CIN (HR 1.21; 95%CI: 0.92–1.58). Consistent results were observed in the out-of-landmark group (including 957 patients), where both severe and mild CIN were significantly associated with a reduced risk of death. Conclusion The pooled analysis of six large trials of NSCLC treatment shows that CIN occurrence is significantly associated with a longer overall survival, particularly in patients developing severe CIN, confirming our previous findings.', 'corpus_id': 234489208, 'score': 0}, {'doc_id': '234484217', 'title': 'Two-year survival with nivolumab in previously treated advanced non-small-cell lung cancer: A real-world pooled analysis of patients from France, Germany, and Canada.', 'abstract': 'OBJECTIVES\nImmune checkpoint inhibitors have become the standard of care for metastatic non-small-cell lung cancer (NSCLC) progressing during or after platinum-based chemotherapy. Real-world clinical practice tends to represent more diverse patient characteristics than randomized clinical trials. We sought to evaluate overall survival (OS) outcomes in the total study population and in key subsets of patients who received nivolumab for previously treated advanced NSCLC in real-world settings in France, Germany, or Canada.\n\n\nMATERIALS AND METHODS\nData were pooled from two prospective observational cohort studies, EVIDENS and ENLARGE, and a retrospective registry in Canada. Patients included in this analysis were aged ≥18 years, had stage IIIB/IV NSCLC, and received nivolumab after at least one prior line of systemic therapy. OS was estimated in the pooled population and in various subgroups using the Kaplan-Meier method. Timing of data collection varied across cohorts (2015-2019).\n\n\nRESULTS\nOf the 2585 patients included in this analyses, 1235 (47.8 %) were treated in France, 881 (34.1 %) in Germany, and 469 (18.1 %) in Canada. Median OS for the total study population was 11.3 months (95 % CI: 10.5-12.2); this was similar across France, Germany, and Canada. The OS rate was 49 % at 1\u202fyear and 28 % at 2 years for the total study population. In univariable Cox analyses, the presence of epidermal growth factor receptor mutations in nonsquamous disease, liver, or bone metastases were associated with significantly shorter OS, whereas tumor programmed death ligand 1 expression and Eastern Cooperative Oncology Group performance status 0-1 were associated with significantly prolonged OS. Similar OS was noted across subgroups of age and prior lines of therapy.\n\n\nCONCLUSION\nOS rates in patients receiving nivolumab for previously treated advanced NSCLC in real-world clinical practice closely mirrored those in phase 3 studies, suggesting similar effectiveness of nivolumab in clinical trials and clinical practice.', 'corpus_id': 234484217, 'score': 1}, {'doc_id': '233988691', 'title': 'Retrospective analysis of real-world treatment patterns and clinical outcomes in patients with advanced non-small cell lung cancer starting first-line systemic therapy in the United Kingdom', 'abstract': 'Background The treatment landscape for advanced non-small cell lung cancer (aNSCLC) has evolved rapidly since immuno-oncology (IO) therapies were introduced. This study used recent data to assess real-world treatment patterns and clinical outcomes in aNSCLC in the United Kingdom. Methods Electronic prescribing records of treatment-naive patients starting first-line (1\u2009L) treatment for aNSCLC between June 2016 and March 2018 (follow-up until December 2018) in the United Kingdom were assessed retrospectively. Patient characteristics and treatment patterns were analyzed descriptively. Outcomes assessed included overall survival (OS), time to treatment discontinuation, time to next treatment, and real-world tumor response. Results In all, 1003 patients were evaluated (median age, 68\u2009years [range, 28–93\u2009years]; 53.9% male). Use of 1\u2009L IO monotherapy (0–25.9%) and targeted therapy (11.8–15.9%) increased during the study period, but chemotherapy remained the most common 1\u2009L treatment at all time points (88.2–58.2%). Median OS was 9.5\u2009months (95% CI, 8.8–10.7\u2009months) for all patients, 8.1\u2009months (95% CI, 7.4–8.9\u2009months) with chemotherapy, 14.0\u2009months (95% CI, 10.7–20.6\u2009months) with IO monotherapy, and 20.2\u2009months (95% CI, 16.0–30.5\u2009months) with targeted therapy. In the 28.6% of patients who received second-line treatment, IO monotherapy was the most common drug class (used in 51.6%). Conclusions Although use of 1\u2009L IO monotherapy for aNSCLC increased in the United Kingdom during the study period, most patients received 1\u2009L chemotherapy. An OS benefit for first-line IO monotherapy vs chemotherapy was observed but was numerically smaller than that reported in clinical trials. Targeted therapy was associated with the longest OS, highlighting the need for improved treatment options for tumors lacking targetable mutations.', 'corpus_id': 233988691, 'score': 1}, {'doc_id': '235355345', 'title': 'ICI plus chemotherapy prolonged survival over ICI alone in patients with previously treated advanced NSCLC', 'abstract': 'Immune checkpoint inhibitors (ICI) monotherapy was standard of care in second-line treatment of patients with advance non-small cell lung cancer (NSCLC). This study aims to investigate the efficacy of ICI plus chemotherapy in patients with previously treated advanced NSCLC. An investigator-initiated trial (IIT) aiming to evaluate the efficacy and safety of ICI in combination with chemotherapy as second line and beyond for patients with advanced NSCLC was undergone at Shanghai Pulmonary Hospital (ChiCTR1900026203). Patients who received ICI monotherapy as second or later line setting during the same period were also collected as a comparator. From April 2018 to June 2019, 31 patients were included into this IIT study, simultaneously 51 patients treated with ICI monotherapy were selected as a comparator. ICI plus chemotherapy showed a significantly higher ORR (35.5% vs. 15.7%, p=0.039), prolonged PFS (median: 5.6 vs. 2.5 months, p = 0.013) and OS (median: NE vs. 12.6 months, p = 0.038) compared with ICI alone. In the subgroup of negative PD-L1 expression (9 patients in combination group and 12 patients in monotherapy group), ICI plus chemotherapy also had a favorable ORR (44.4% vs. 8.3%, p = 0.119), longer PFS (median: 6.5 vs 3.0 months, p < 0.05) and OS (median: NE vs. 8.2 months, p = 0.117). Meanwhile, the addition of chemotherapy did not increase immune-related adverse events. ICI plus chemotherapy showed superior ORR, PFS and OS than ICI alone patients with previous treated advanced NSCLC. These findings warrant further investigation.', 'corpus_id': 235355345, 'score': 0}, {'doc_id': '122248068', 'title': 'Analytical Solutions for the Pencil-Beam Equation with Energy Loss and Straggling', 'abstract': 'In this article, we derive equations approximating the Boltzmann equation for charged particle transport under the continuous slowing down assumption. The objective is to obtain analytical expressions that approximate the solution to the Boltzmann equation. The analytical expressions found are based on the Fermi-Eyges solution, but include correction factors to account for energy loss and spread. Numerical tests are also performed to investigate the validity of the approximations.', 'corpus_id': 122248068, 'score': 0}, {'doc_id': '233405261', 'title': 'Using Electronic Health Records to Derive Control Arms for Early Phase SingleArm Lung Cancer Trials: ProofofConcept in Randomized Controlled Trials', 'abstract': 'Oncology drug development increasingly relies on single-arm clinical trials. External controls (ECs) derived from electronic health record (EHR) databases may provide additional context. Patients from a US-based oncology EHR database were aligned with patients from randomized controlled trials (RCTs) and trial-specific eligibility criteria were applied to the EHR dataset. Overall survival (OS) in the EC-derived control arm was compared with OS in the RCT experimental arm. The primary outcome was OS, defined as time from randomization or treatment initiation (EHR) to death. Cox regression models were used to obtain effect estimates using EHR data. EC-derived hazard ratio estimates aligned closely with those from the corresponding RCT with one exception. Comparing log HRs among all RCT and EC results gave a Pearson correlation coefficient of 0.86. Properly selected control arms from contemporaneous EHR data could be used to put single-arm trials of OS in advanced non-small cell lung cancer into context.', 'corpus_id': 233405261, 'score': 1}, {'doc_id': '233487087', 'title': 'Trends in the prescription of systemic anticancer therapy and mortality among patients with advanced non-small cell lung cancer: a real-world retrospective observational cohort study from the I-O optimise initiative', 'abstract': 'Objectives To assess how a decade of developments in systematic anticancer therapy (SACT) for advanced non-small cell lung cancer (NSCLC) affected overall survival (OS) in a large UK University Hospital. Design Real-world retrospective observational cohort study using existing data recorded in electronic medical records. Setting A large National Health Service (NHS) university teaching hospital serving 800 000 people living in a diverse metropolitan area of the UK. Participants 2119 adults diagnosed with advanced NSCLC (tumour, node, metastasis stage IIIB or IV) between 2007 and 2017 at Leeds Teaching Hospitals NHS Trust. Main outcomes and measures OS following diagnosis and the analysis of factors associated with receiving SACT. Results Median OS for all participants was 2.9 months, increasing for the SACT-treated subcohort from 8.4 months (2007–2012) to 9.1 months (2013–2017) (p=0.02); 1-year OS increased from 33% to 39% over the same period for the SACT-treated group. Median OS for the untreated subcohort was 1.6 months in both time periods. Overall, 30.6% (648/2119) patients received SACT; treatment rates increased from 28.6% (338/1181) in 2007–2012 to 33.0% (310/938) in 2013–2017 (p=0.03). Age and performance status were independent predictors for SACT treatment; advanced age and higher performance status were associated with lower SACT treatment rates. Conclusion Although developments in SACT during 2007–2017 correspond to some changes in survival for treated patients with advanced NSCLC, treatment rates remain low and the prognosis for all patients remains poor.', 'corpus_id': 233487087, 'score': 1}, {'doc_id': '235360451', 'title': 'Effectiveness of neoadjuvant chemotherapy on the survival outcomes of patients with resectable non-small-cell lung cancer: A meta-analysis of randomized controlled trials.', 'abstract': 'PURPOSE\nTo determine the effectiveness of neoadjuvant chemotherapy (NACT) versus primary surgery on survival outcomes for resectable non-small-cell lung cancer (NSCLC) using an approach based on a meta-analysis.\n\n\nMETHODS\nThe PubMed, EmBase, Cochrane library, and CNKI databases were systematically browsed to identify randomized controlled trials (RCTs) which met a set of predetermined inclusion criteria throughout January 2020. Hazard ratios (HRs) were applied for the pooled overall survival (OS) and progression-free survival (PFS) values, and the pooled survival rates at 1-year and 3-year were used as the relative risk (RR). All the pooled effect estimates with 95% confidence intervals (CIs) were calculated using the random-effects model.\n\n\nRESULTS\nNineteen RCTs contained a total of 4372 NSCLC at I-III stages was selected for final meta-analysis. We noted NACT was significantly associated with an improvement in OS (HR: 0.87; 95%CI: 0.81-0.94; P\xa0<\xa00.001) and PFS (HR: 0.86; 95%CI: 0.78-0.96; P\xa0=\xa00.005). Moreover, the survival rate at 1-year (RR: 1.07; 95%CI: 1.02-1.12; P\xa0=\xa00.007) and 3-year (RR: 1.16; 95%CI: 1.06-1.27; P\xa0=\xa00.001) in the NACT group was significantly higher than the survival rate for the primary surgery group. Finally, the treatment effects of NACT versus primary surgery on survival outcomes might be different when stratified by the mean age of patients and the tumor stages.\n\n\nCONCLUSIONS\nNACT could improve survival outcomes for patients with resectable NSCLC, suggesting its suitable future applicability for clinical practice. However, large-scale RCT should be conducted to assess the chemotherapy regimen on the prognosis of resectable NSCLC.', 'corpus_id': 235360451, 'score': 0}]
37	{'doc_id': '225213456', 'title': 'A Survey of Mental Modeling Techniques in Human–Robot Teaming', 'abstract': 'As robots become increasingly prevalent and capable, the complexity of roles and responsibilities assigned to them as well as our expectations for them will increase in kind. For these autonomous systems to operate safely and efficiently in human-populated environments, they will need to cooperate and coordinate with human teammates. Mental models provide a formal mechanism for achieving fluent and effective teamwork during human–robot interaction by enabling awareness between teammates and allowing for coordinated action. Much recent research in human–robot interaction has made use of standardized and formalized mental modeling techniques to great effect, allowing for a wider breadth of scenarios in which a robotic agent can act as an effective and trustworthy teammate. This paper provides a structured overview of mental model theory and methodology as applied to human–robot teaming. Also discussed are evaluation methods and metrics for various aspects of mental modeling during human–robot interaction, as well as recent emerging applications and open challenges in the field.', 'corpus_id': 225213456}	13594	"[{'doc_id': '231839673', 'title': '""I Don\'t Think So"": Disagreement-Based Policy Summaries for Comparing Agents', 'abstract': 'With Artificial Intelligence on the rise, human interaction with autonomous agents becomes more frequent. Effective human-agent collaboration requires that the human understands the agent’s behavior, as failing to do so may lead to reduced productiveness, misuse, frustration and even danger. Agent strategy summarization methods are used to describe the strategy of an agent to its destined user through demonstration. The summary’s purpose is to maximize the user’s understanding of the agent’s aptitude by showcasing its behaviour in a set of world states, chosen by some importance criteria. While shown to be useful, we show that these methods are limited in supporting the task of comparing agent behavior, as they independently generate a summary for each agent. In this paper, we propose a novel method for generating contrastive summaries that highlight the differences between agent’s policies by identifying and ranking states in which the agents disagree on the best course of action. We conduct a user study in which participants face an agent selection task. Our results show that the novel disagreement-based summaries lead to improved user performance compared to summaries generated using HIGHLIGHTS, a previous strategy summarization algorithm.', 'corpus_id': 231839673, 'score': 0}, {'doc_id': '86389282', 'title': 'Will You Accept an Imperfect AI?: Exploring Designs for Adjusting End-user Expectations of AI Systems', 'abstract': 'AI technologies have been incorporated into many end-user applications. However, expectations of the capabilities of such systems vary among people. Furthermore, bloated expectations have been identified as negatively affecting perception and acceptance of such systems. Although the intelligibility of ML algorithms has been well studied, there has been little work on methods for setting appropriate expectations before the initial use of an AI-based system. In this work, we use a Scheduling Assistant - an AI system for automated meeting request detection in free-text email - to study the impact of several methods of expectation setting. We explore two versions of this system with the same 50% level of accuracy of the AI component but each designed with a different focus on the types of errors to avoid (avoiding False Positives vs. False Negatives). We show that such different focus can lead to vastly different subjective perceptions of accuracy and acceptance. Further, we design expectation adjustment techniques that prepare users for AI imperfections and result in a significant increase in acceptance.', 'corpus_id': 86389282, 'score': 1}, {'doc_id': '15512333', 'title': 'Tell me more?: the effects of mental model soundness on personalizing an intelligent agent', 'abstract': ""What does a user need to know to productively work with an intelligent agent? Intelligent agents and recommender systems are gaining widespread use, potentially creating a need for end users to understand how these systems operate in order to fix their agent's personalized behavior. This paper explores the effects of mental model soundness on such personalization by providing structural knowledge of a music recommender system in an empirical study. Our findings show that participants were able to quickly build sound mental models of the recommender system's reasoning, and that participants who most improved their mental models during the study were significantly more likely to make the recommender operate to their satisfaction. These results suggest that by helping end users understand a system's reasoning, intelligent agents may elicit more and better feedback, thus more closely aligning their output with each user's intentions."", 'corpus_id': 15512333, 'score': 1}, {'doc_id': '231796001', 'title': 'Supporting an Online Investigation of User Interaction with an XAIP Agent', 'abstract': 'Human interaction relies on a wide range of signals, including non-verbal cues. In order to develop effective Explainable Planning (XAIP) agents it is important that we understand the range and utility of these communication channels. Our intention is to develop an interactive agent, whose behaviour is conditioned on the affective measures of the user (i.e., explicitly incorporating the user’s affective state within the planning model). Accurate prediction of user affective state relies on real-time analysis of various predictors, which can require specialist equipment and calibration. However, the worldwide COVID-19 lockdown has meant that many intended lab-based experiments have now been moved online, making such real-time analysis impractical. As a result, we have developed a website to support a data gathering experiment, including a video stream (for facial expression analysis) with access to mouse positions and task performance, providing rich observations of the users as they interact with the agent and its plan. Underlying this system is the agent’s behaviour strategy, which must be computed in advance and captured efficiently. This paper describes the built system and the challenges we faced getting it ready for deployment.', 'corpus_id': 231796001, 'score': 0}, {'doc_id': '36908206', 'title': 'Cause of late postoperative death in patients with early gastric cancer with special reference to recurrence and the incidence of metachronous primary cancer in other organs.', 'abstract': 'Among 452 patients who underwent operation for early gastric cancer, 101 were late deaths. The cause of death in these patients was studied with special reference to cancer recurrence and the occurrence of metachronous cancers in other organs. Of these 101 patients, 63 died of noncancerous diseases, the other 38 of cancer. Among the latter, 15 patients died of recurrence more than 5 years after operation; macroscopically, the protruded or elevated type combined with the depressed or excavated type were found more frequently in the primary lesions. Differentiated adenocarcinoma was a characteristic histologic findings for these lesions, and hematogenic metastasis was the most frequent recurrence pattern. Ten patients with early gastric cancer who underwent operation died of metachronous cancer in other organs. Our findings suggest that in the long-term follow-up of patients with early gastric cancer who undergo operation, the occurrence of metachronous cancer in other organs should be considered in addition to recurrence of cancer.', 'corpus_id': 36908206, 'score': 0}, {'doc_id': '210927566', 'title': ""Effects of a Social Robot's Self-Explanations on How Humans Understand and Evaluate Its Behavior"", 'abstract': ""Social robots interacting with users in real-life environments will often show surprising or even undesirable behavior. In this paper we investigate whether a robot's ability to self-explain its behavior affects the users' perception and assessment of this behavior. We propose an explanation model based on humans' folk-psychological concepts and test different explanation strategies in specifically designed HRI scenarios with robot behaviors perceived as intentional, but differently surprising or desirable. All types of explanation strategies increased the understandability and desirability of the behaviors. While merely stating an action had similar effects as giving a reason for it (an intention or need), combining both in a causal explanation helped the robot to better justify its behavior and to increase its understandability and desirability to a larger extent."", 'corpus_id': 210927566, 'score': 1}, {'doc_id': '229349386', 'title': 'Are We On The Same Page? Hierarchical Explanation Generation for Planning Tasks in Human-Robot Teaming using Reinforcement Learning', 'abstract': 'Providing explanations is considered an imperative ability for an AI agent in a human-robot teaming framework. The right explanation provides the rationale behind an AI agent’s decision making. However, to maintain the human teammate’s cognitive demand to comprehend the provided explanations, prior works have focused on providing explanations in a specific order or intertwining the explanation generation with plan execution. These approaches, however, do not consider the degree of details they share throughout the provided explanations. In this work, we argue that the explanations, especially the complex ones, should be abstracted to be aligned with the level of details the teammate desires to maintain the cognitive load of the recipient. The challenge here is to learn a hierarchical model of explanations and details the agent requires to yield the explanations as an objective. Moreover, the agent needs to follow a high-level plan in a task domain such that the agent can transfer learned teammate preferences to a scenario where lower-level control policies are different, while the high-level plan remains the same. Results confirmed our hypothesis that the process of understanding an explanation was a dynamic hierarchical process. The human preference that reflected this aspect corresponded exactly to creating and employing abstraction for knowledge assimilation hidden deeper in our cognitive process. We showed that hierarchical explanations achieved better task performance and behavior interpretability while reduced cognitive load. These results shed light on designing explainable agents utilizing reinforcement learning and planning across various domains.', 'corpus_id': 229349386, 'score': 0}, {'doc_id': '14287365', 'title': 'An Embodied Cognition Approach to Mindreading Skills for Socially Intelligent Robots', 'abstract': ""Future applications for personal robots motivate research into developing robots that are intelligent in their interactions with people. Toward this goal, in this paper we present an integrated socio-cognitive architecture to endow an anthropomorphic robot with the ability to infer mental states such as beliefs, intents, and desires from the observable behavior of its human partner. The design of our architecture is informed by recent findings from neuroscience and embodies cognition that reveals how living systems leverage their physical and cognitive embodiment through simulation-theoretic mechanisms to infer the mental states of others. We assess the robot's mindreading skills on a suite of benchmark tasks where the robot interacts with a human partner in a cooperative scenario and a learning scenario. In addition, we have conducted human subjects experiments using the same task scenarios to assess human performance on these tasks and to compare the robot's performance with that of people. In the process, our human subject studies also reveal some interesting insights into human behavior."", 'corpus_id': 14287365, 'score': 1}, {'doc_id': '227014686', 'title': 'Explainable AI for System Failures: Generating Explanations that Improve Human Assistance in Fault Recovery', 'abstract': ""With the growing capabilities of intelligent systems, the integration of artificial intelligence (AI) and robots in everyday life is increasing. However, when interacting in such complex human environments, the failure of intelligent systems, such as robots, can be inevitable, requiring recovery assistance from users. In this work, we develop automated, natural language explanations for failures encountered during an AI agents' plan execution. These explanations are developed with a focus of helping non-expert users understand different point of failures to better provide recovery assistance. Specifically, we introduce a context-based information type for explanations that can both help non-expert users understand the underlying cause of a system failure, and select proper failure recoveries. Additionally, we extend an existing sequence-to-sequence methodology to automatically generate our context-based explanations. By doing so, we are able develop a model that can generalize context-based explanations over both different failure types and failure scenarios."", 'corpus_id': 227014686, 'score': 1}, {'doc_id': '85502531', 'title': ""People's Explanations of Robot Behavior Subtly Reveal Mental State Inferences"", 'abstract': ""It has long been assumed that when people observe robots they intuitively ascribe mind and intentionality to them, just as they do to humans. However, much of this evidence relies on experimenter-provided questions or self-reported judgments. We propose a new way of investigating people's mental state ascriptions to robots by carefully studying explanations of robot behavior. Since people's explanations of human behavior are deeply grounded in assumptions of mind and intentional agency, explanations of robot behavior can reveal whether such assumptions similarly apply to robots. We designed stimulus behaviors that were representative of a variety of robots in diverse contexts and ensured that people saw the behaviors as equally intentional, desirable, and surprising across both human and robot agents. We provided 121 participants with verbal descriptions of these behaviors and asked them to explain in their own words why the agent (human or robot) had performed them. To systematically analyze the verbal data, we used a theoretically grounded classification method to identify core explanation types. We found that people use the same conceptual toolbox of behavior explanations for both human and robot agents, robustly indicating inferences of intentionality and mind. But people applied specific explanatory tools at somewhat different rates and in somewhat different ways for robots, revealing specific expectations people hold when explaining robot behaviors."", 'corpus_id': 85502531, 'score': 1}]"
38	{'doc_id': '201830604', 'title': 'End-to-End Learning Framework for IMU-Based 6-DOF Odometry', 'abstract': 'This paper presents an end-to-end learning framework for performing 6-DOF odometry by using only inertial data obtained from a low-cost IMU. The proposed inertial odometry method allows leveraging inertial sensors that are widely available on mobile platforms for estimating their 3D trajectories. For this purpose, neural networks based on convolutional layers combined with a two-layer stacked bidirectional LSTM are explored from the following three aspects. First, two 6-DOF relative pose representations are investigated: one based on a vector in the spherical coordinate system, and the other based on both a translation vector and an unit quaternion. Second, the loss function in the network is designed with the combination of several 6-DOF pose distance metrics: mean squared error, translation mean absolute error, quaternion multiplicative error and quaternion inner product. Third, a multi-task learning framework is integrated to automatically balance the weights of multiple metrics. In the evaluation, qualitative and quantitative analyses were conducted with publicly-available inertial odometry datasets. The best combination of the relative pose representation and the loss function was the translation and quaternion together with the translation mean absolute error and quaternion multiplicative error, which obtained more accurate results with respect to state-of-the-art inertial odometry techniques.', 'corpus_id': 201830604}	2638	"[{'doc_id': '119111419', 'title': 'AI-IMU Dead-Reckoning', 'abstract': 'In this paper, we propose a novel accurate method for dead-reckoning of wheeled vehicles based only on an Inertial Measurement Unit (IMU). In the context of intelligent vehicles, robust and accurate dead-reckoning based on the IMU may prove useful to correlate feeds from imaging sensors, to safely navigate through obstructions, or for safe emergency stops in the extreme case of exteroceptive sensors failure. The key components of the method are the Kalman filter and the use of deep neural networks to dynamically adapt the noise parameters of the filter. The method is tested on the KITTI odometry dataset, and our dead-reckoning inertial method based only on the IMU accurately estimates 3D position, velocity, orientation of the vehicle and self-calibrates the IMU biases. We achieve on average a 1.10% translational error and the algorithm competes with top-ranked methods which, by contrast, use LiDAR or stereo vision.', 'corpus_id': 119111419, 'score': 1}, {'doc_id': '220363628', 'title': 'TLIO: Tight Learned Inertial Odometry', 'abstract': 'In this letter we propose a tightly-coupled Extended Kalman Filter framework for IMU-only state estimation. Strap-down IMU measurements provide relative state estimates based on IMU kinematic motion model. However the integration of measurements is sensitive to sensor bias and noise, causing significant drift within seconds. Recent research by Yan et al. (RoNIN) and Chen et al. (IONet) showed the capability of using trained neural networks to obtain accurate 2D displacement estimates from segments of IMU data and obtained good position estimates from concatenating them. This letter demonstrates a network that regresses 3D displacement estimates and its uncertainty, giving us the ability to tightly fuse the relative state measurement into a stochastic cloning EKF to solve for pose, velocity and sensor biases. We show that our network, trained with pedestrian data from a headset, can produce statistically consistent measurement and uncertainty to be used as the update step in the filter, and the tightly-coupled system outperforms velocity integration approaches in position estimates, and AHRS attitude filter in orientation estimates. Video materials and code can be found on our project page: http://cathias.github.io/TLIO/.', 'corpus_id': 220363628, 'score': 1}, {'doc_id': '111115048', 'title': 'Extreme Response Estimate of Steel Catenary Risers Using L-Moments', 'abstract': 'The steel catenary risers (SCR) respond non-linearly to FPU motions, especially in extreme environments such as 100-year and 1000-year hurricanes. One of the design challenges is an accurate estimate of SCR response based on limited realizations. Due to the nonlinear nature of SCR response and the lack of enough sample data, traditional methods did not produce optimum extrema estimation. L-moments have been applied successfully in the statistical analysis of data, because they are linear combinations of probability weighted moments inherently and are less sensitive to outlying data values. Therefore, L-moments have been introduced in this paper to determine the candidate distribution of SCR responses and estimate extrema. Three typical SCRs connected to semi-submersible FPU have been analyzed using this methodology. Simulations confirmed L-moments can accurately determine the SCR responses distribution. Moreover, the extreme values predicted in this way were consistent and reliable as compared with current industry practice. Those observations have demonstrated that L-moments are robust and efficient methods in the design of steel catenary risers.Copyright © 2010 by ASME', 'corpus_id': 111115048, 'score': 0}, {'doc_id': '220363761', 'title': 'Preintegrated IMU Features For Efficient Deep Inertial Odometry', 'abstract': ""MEMS Inertial Measurement Units (IMUs) are inexpensive and effective sensors that provide proprioceptive motion measurements for many robots and consumer devices. However, their noise characteristics and manufacturing imperfections lead to complex ramifications in classical fusion pipelines. While deep learning models provide the required flexibility to model these complexities from data, they have higher computation and memory requirements, making them impractical choices for low-power and embedded applications. This paper attempts to address the mentioned conflict by proposing a computationally, efficient inertial representation for deep inertial odometry. Replacing the raw IMU data in deep Inertial models, preintegrated features improves the model's efficiency. The effectiveness of this method has been demonstrated for the task of pedestrian inertial odometry, and its efficiency has been shown through its embedded implementation on a microcontroller with restricted resources."", 'corpus_id': 220363761, 'score': 1}, {'doc_id': '221266488', 'title': 'Towards Resilient Autonomous Navigation of Drones', 'abstract': 'Robots and particularly drones are especially useful in exploring extreme environments that pose hazards to humans. To ensure safe operations in these situations, usually perceptually degraded and without good GNSS, it is critical to have a reliable and robust state estimation solution. The main body of literature in robot state estimation focuses on developing complex algorithms favoring accuracy. Typically, these approaches rely on a strong underlying assumption: the main estimation engine will not fail during operation. In contrast, we propose an architecture that pursues robustness in state estimation by considering redundancy and heterogeneity in both sensing and estimation algorithms. The architecture is designed to expect and detect failures and adapt the behavior of the system to ensure safety. To this end, we present HeRO (Heterogeneous Redundant Odometry): a stack of estimation algorithms running in parallel supervised by a resiliency logic. This logic carries out three main functions: a) perform confidence tests both in data quality and algorithm health; b) re-initialize those algorithms that might be malfunctioning; c) generate a smooth state estimate by multiplexing the inputs based on their quality. The state and quality estimates are used by the guidance and control modules to adapt the mobility behaviors of the system. The validation and utility of the approach are shown with real experiments on a flying robot for the use case of autonomous exploration of subterranean environments, with particular results from the STIX event of the DARPA Subterranean Challenge.', 'corpus_id': 221266488, 'score': 0}, {'doc_id': '170079303', 'title': 'RoNIN: Robust Neural Inertial Navigation in the Wild: Benchmark, Evaluations, & New Methods', 'abstract': 'This paper sets a new foundation for data-driven inertial navigation research, where the task is the estimation of horizontal positions and heading direction of a moving subject from a sequence of IMU sensor measurements from a phone. In contrast to existing methods, our method can handle varying phone orientations and placements.More concretely, the paper presents 1) a new benchmark containing more than 40 hours of IMU sensor data from 100 human subjects with ground-truth 3D trajectories under natural human motions; 2) novel neural inertial navigation architectures, making significant improvements for challenging motion cases; and 3) qualitative and quantitative evaluations of the competing methods over three inertial navigation benchmarks. We share the code and data to promote further research. (http://ronin.cs.sfu.ca).', 'corpus_id': 170079303, 'score': 1}, {'doc_id': '221266081', 'title': 'Towards Autonomous Driving: a Multi-Modal 360$^{\\circ}$ Perception Proposal', 'abstract': 'In this paper, a multi-modal 360$^{\\circ}$ framework for 3D object detection and tracking for autonomous vehicles is presented. The process is divided into four main stages. First, images are fed into a CNN network to obtain instance segmentation of the surrounding road participants. Second, LiDAR-to-image association is performed for the estimated mask proposals. Then, the isolated points of every object are processed by a PointNet ensemble to compute their corresponding 3D bounding boxes and poses. Lastly, a tracking stage based on Unscented Kalman Filter is used to track the agents along time. The solution, based on a novel sensor fusion configuration, provides accurate and reliable road environment detection. A wide variety of tests of the system, deployed in an autonomous vehicle, have successfully assessed the suitability of the proposed perception stack in a real autonomous driving application.', 'corpus_id': 221266081, 'score': 0}, {'doc_id': '221186591', 'title': 'DronePose: Photorealistic UAV-Assistant Dataset Synthesis for 3D Pose Estimation via a Smooth Silhouette Loss', 'abstract': 'In this work we consider UAVs as cooperative agents supporting human users in their operations. In this context, the 3D localisation of the UAV assistant is an important task that can facilitate the exchange of spatial information between the user and the UAV. To address this in a data-driven manner, we design a data synthesis pipeline to create a realistic multimodal dataset that includes both the exocentric user view, and the egocentric UAV view. We then exploit the joint availability of photorealistic and synthesized inputs to train a single-shot monocular pose estimation model. During training we leverage differentiable rendering to supplement a state-of-the-art direct regression objective with a novel smooth silhouette loss. Our results demonstrate its qualitative and quantitative performance gains over traditional silhouette objectives. Our data and code are available at this https URL', 'corpus_id': 221186591, 'score': 0}, {'doc_id': '221245995', 'title': 'Fifteen Years of Progress at Zero Velocity: A Review', 'abstract': 'Fifteen years have passed since the publication of Foxlin’s seminal paper “Pedestrian tracking with shoe-mounted inertial sensors”. In addition to popularizing the zero-velocity update, Foxlin also hinted that the optimal parameter tuning of the zero-velocity detector is dependent on, for example, the user’s gait speed. As demonstrated by the recent influx of related studies, the question of how to properly design a robust zero-velocity detector is still an open research question. In this review, we first recount the history of foot-mounted inertial navigation and characterize the main sources of error, thereby motivating the need for a robust solution. Following this, we systematically analyze current approaches to robust zero-velocity detection, while categorizing public code and data. The article concludes with a discussion on commercialization along with guidance for future research.', 'corpus_id': 221245995, 'score': 1}, {'doc_id': '73299117', 'title': 'Obstrução intestinal por melanoma metastático: Relato de caso', 'abstract': 'Melanoma is a highly malignant tumor and represents 3% of all cancers. Malignant melanoma is the most common metastatic tumor involving the gastrointestinal tract, the small bowel being the localization most frequently involved. Its clinical presentation is inespecific and an obstructive acute abdomen syndrome could be its first manifestation. The article is about a case of a 36 years-old patient, which had been submitted to resection of melanoma in the dorsal region in 2009. The patient developed acute intestinal obstruction and laparotomy showed intestinal intussusceptions caused by metastatic melanoma at 50 cm from the angle of Treitz. The specimen was sent for pathological, which confirmed the presence of metastatic melanoma in the small bowel. The treatment of metastatic melanoma involves systemic treatment and treatment of complications. Intestinal metastasis must be considered in any patient with gastrointestinal symptoms and previous history of melanoma and the surgical treatment should always be considered because it can occasionally provide long-term survival. Key-words: Melanoma. Metastasis. Intestinal obstruction. Intussesception. Surgery.', 'corpus_id': 73299117, 'score': 0}]"
39	{'doc_id': '6242341', 'title': 'Location management in mobile network: A survey', 'abstract': 'Location management is an important area of mobile computing. Location management in mobile network deals with location registration and tracking of mobile terminals. The location registration process is called location update and the searching process is called paging. Various types of location management methods exist such as mobility based location management, data replication based location management, signal attenuation based location tracking, time, zone and distance based location update etc. In this paper, existing location management schemes are discussed and compared with respect to their cost consumption in terms of bytes. Finally the key issues are addressed in the context of location management for future generation mobile network. Different types of location management schemes for mobile network are discussed.The location management cost in terms of message is calculated for these schemes.Comparative analysis is performed between the methods based on cost.Future scopes of location management are also explored.', 'corpus_id': 6242341}	20271	[{'doc_id': '54502491', 'title': 'Effects of Language on CATS Performance', 'abstract': 'The University of Puerto Rico, Mayaguez (UPRM) is an officially bilingual university where engineering classes may be taught in Spanish, English, or a combination of both languages. Spanish is the home language of 91% of undergraduates at UPRM. Because of low performance on the Concept Assessment Tool for Statics (CATS) (around 29% compared to the results of other institutions, which ranged from 30-70%), a Spanish version of CATS (CATS-S) was developed. Ten Hispanic senior civil engineering students were recruited and divided into two groups, control and experimental. The control group was assigned the original CATS version, while the experimental group received the CATS-S. For both instruments, questions were given in open-ended format. The control group scored an average of 51% on CATS, and the experimental group scored an average of 24% on CATS-S. That is, native-Spanish speakers did worse on CATS-S than CATS. This study seeks to understand the role of language in the performance of these students by comparing their performance on CATS and CAT-S, answering the following questions: 1. What are the linguistic discrepancies between CATS and CATS-S? How might these discrepancies lead to differences in performance? 2. Can the poor performance of students on CATS-S be explained by the linguistic discrepancies found in question 1? Linguistic analysis took place by looking at syntactic and semantic differences between the English on CATS and the Spanish in CATS-S. This analysis led to the identification of the following linguistic discrepancies: (1) semantic assonance, (2) inconsistent terminology, and (3) changed information. Performance by the control group on each item of CATS was compared with performance by the experimental group on each item of CATS-S. We deduced that where there was a major discrepancy in performance between the control and experimental groups, language could be playing a role in performance. If both the control and experimental group did poorly on a question, we deduced that this was likely because of a lack of conceptual knowledge, not the language of the question. This analysis determined that lack of conceptual knowledge was the main reason for poor performance on both CATS and CATS-S, but that language did play a role in poor performance on certain CATS-S questions. Suggestions for a revised version of CATS-S and a next phase of testing are made.', 'corpus_id': 54502491, 'score': 0}, {'doc_id': '237492039', 'title': 'RWP+: A New Random Waypoint Model for High-Speed Mobility', 'abstract': 'In this letter, we emulate real-world statistics for mobility patterns on road systems. We then propose modifications to the assumptions of the random waypoint (RWP) model to better represent high-mobility profiles. We call the model under our new framework as RWP+. Specifically, we show that the lengths of the transitions which constitute a trip, are best represented by a lognormal distribution, and that the velocities are best described by a linear combination of normal distributions with different mean values. Compared to the assumptions used in the literature for mobile cellular networks, our modeling provides mobility metrics, such as handoff rates, that better characterize actual emulated trips from the collected statistics.', 'corpus_id': 237492039, 'score': 1}, {'doc_id': '235829415', 'title': 'On the Analysis of Adaptive-Rate Applications in Data-Centric Wireless Ad-Hoc Networks', 'abstract': 'Adapting applications’ data rates in multi-hop wireless ad-hoc networks is inherently challenging. Packet collision, channel contention, and queue buildup contribute to packet loss but are difficult to manage in conventional TCP/IP architecture. This work explores a data-centric approach based on Name Data Networking (NDN) architecture, which is considered more suitable for wireless ad-hoc networks. We show that the default NDN transport offers better performance in linear topologies but struggles in more extensive networks due to high collision and contention caused by excessive Interests from out-of-order data retrieval and redundant data transmission from improper Interest lifetime setting as well as in-network caching. To fix these, we use round-trip hop count to limit Interest rate and Dynamic Interest Lifetime to minimize the negative effect of improper Interest lifetime. Finally, we analyze the effect of innetwork caching on transport performance and which scenarios may benefit or suffer from it.', 'corpus_id': 235829415, 'score': 0}, {'doc_id': '216024688', 'title': 'Location management in mobile network', 'abstract': 'Location management is an important area of mobile computing. Location management in mobile network deals with location registration and tracking of mobile terminals. The location registration proc...', 'corpus_id': 216024688, 'score': 1}, {'doc_id': '237420576', 'title': 'The fallacy of the closest antenna: Towards an adequate view of device location in the mobile network', 'abstract': 'The partition of the Mobile Phone Network (MPN) service area into the cell towers’ Voronoi polygons (VP) may serve as a coordinate system for representing the location of the mobile phone devices. This view is shared by numerous papers that exploit mobile phone data for studying human spatial mobility. We investigate the credibility of this view by comparing volunteers’ locational data of two kinds: (1) Cell towers’ that served volunteers’ connections and (2) The GPS tracks of the users at the time of connection. In more than 60% of connections, user’s mobile device was found outside the VP of the cell tower that served for the connection. We demonstrate that the area of possible device’s location is many times larger than the area of the cell tower’s VP. To comprise 90% of the possible locations of the devices that may be connected to the cell tower one has to consider the tower’s VP together with the two rings of the VPs adjacent to the tower’s VPs. An additional, third, ring of the adjacent VPs is necessary to comprise 95% of possible locations of the devices that can be connected to a cell tower. The revealed location uncertainty is in the nature of the MPN structure and service and entail essential overlap between the cell towers’ service areas. We discuss the far-reaching consequences of this uncertainty in regards to the estimating of locational privacy and urban mobility. Our results undermine today’s dominant opinion that an adversary, who obtains the access to the database of the Call Detail Records maintained by the MPN operator, can identify a mobile device without knowing its number based on a very short sequence of time-stamped field observations of the user’s connection.', 'corpus_id': 237420576, 'score': 1}, {'doc_id': '237353453', 'title': 'Performance Evaluation of Ad Hoc Multicast Routing Protocols to Facilitate Video Streaming in VANETS', 'abstract': 'Vehicular Ad Hoc Network (VANET) is a type of mobile ad hoc network (MANET) that facilitates communication among vehicles. VANET provides inter-vehicular communications to serve for the application like road traffic safety and traffic efficiency. Infotainment service has been an anticipating trend in VANETs, and video streaming has a high potential in VANET. Although, this emerging technology is trending, there are still some issues like QoS provisions, decentralized medium access control, node coverage area, and finding and maintaining routes due to highly dynamic topology. These issues make multicast communication difficult in VANETs. Numerous routing protocols and routing strategies have been projected to cope with these issues. Lots of work has taken place to assess and measure the performances of these protocols in VANETs but these protocols are rarely analyzed for performance under stress of real time video multicast. In this study two different multicast routing protocols viz. Multicast Ad hoc On Demand Distance Vector (MAODV) and Protocol for Unified Multicasting through Announcements (PUMA) are evaluated for facilitating video streaming in VANETS. The protocols are examined against the QoS parameters such as Network Throughput, Packet Delivery Ratio (PDR), Average end to end Delay, and Normalized Routing Load (NRL). Variable Bit Rate (VBR) traffic is used to evaluate the performances of protocol. PUMA, at the end, showed better performance against different QoS provisions in different scenarios.', 'corpus_id': 237353453, 'score': 0}, {'doc_id': '206437266', 'title': 'On optimum timer value of area and timer-based location registration scheme', 'abstract': 'In typical mobile communication systems, mobile station (MS) location information is updated when the MS crosses the location area boundary or the registration timer is expired. When a call attempt occurs, sequential paging rather than blanket paging is used to reduce the paging cost. We propose a new location update scheme in which to increase the paging accuracy, timer-based location update is performed within a location area. In this work, the optimum timer value of the area and timer-based location registration scheme with intelligent paging is derived. In case of a fixed location area, the optimum registration timer value depends on the speed and call arrival rate of the MS. If the speed or call arrival rate of the MS is high, location registration based on timer value should be performed frequently. Otherwise, location registration based on the crossing of location area is sufficient.', 'corpus_id': 206437266, 'score': 1}, {'doc_id': '21194474', 'title': 'Thy-1 antigen is specific to ganglion cells in chicks', 'abstract': 'The cellular localization of Thy-1 in the chick retina was investigated by selectively destroying certain populations of neurons with toxins. In control retinae four weeks after intravitreal injection of vehicle, there was strong immunoreactivity for Thy-1 in the nerve fibre layer, ganglion cell layer and inner plexiform layer. By contrast, 4 weeks after intraocular injection with 1.25 nmol of colchicine, virtually all ganglion cells had been destroyed, but most amacrine cells remained. Very little Thy-1 immunoreactivity was evident in these retinae. Four weeks after intraocular injection of 2 mumol of N-methyl-D-aspartic acid (NMDA), a large proportion of amacrine cells had been destroyed, but most ganglion cells remained. In these retinae Thy-1 immunoreactivity was present in the nerve fibre, ganglion cell and inner plexiform layers, in the latter with greater intensity than in controls. We conclude that in chicks the Thy-1 antigen is principally, if not exclusively restricted to ganglion cells.', 'corpus_id': 21194474, 'score': 0}, {'doc_id': '237350733', 'title': 'Efficient Handoff for QoS Enhancement in Heterogeneous Wireless Networks (UMTS/WLAN Interworking)', 'abstract': 'Today’s Wireless Communications technologies prove us that wireless communications will in the long run be composed of different communication networks as a way to benefit from each other. This can however be achieved from cellular networks and wireless local area networks that show some compatible characteristics that enable them be integrated. Scenarios typically behind these integrations is the UMTS and WLAN interworking where UMTS network is known for its wide area of coverage and nearly roaming however, known for lack of enough data rate. This is contrary with WLAN which is known for high data rate and cheaper compared to UMTS. WLAN however has a small area of coverage and lacks roaming. This in regard brings the idea that the two different networks being integrated could provide the means for mobile users to be gratified with a supported coverage and quality at anywhere and anytime with seamless', 'corpus_id': 237350733, 'score': 1}, {'doc_id': '235742722', 'title': 'Integrated Satellite-HAP-Terrestrial Networks for Dual-Band Connectivity', 'abstract': 'The recent development of high-altitude platforms (HAPs) has attracted increasing attention since they can serve as a promising communication method to assist satellite-terrestrial networks. In this paper, we consider an integrated three-layer satellite-HAP-terrestrial network where the HAP support dual-band connectivity. Specifically, the HAP can not only communicate with terrestrial users over Cband directly, but also provide backhaul services to terrestrial user terminals over Ka-band. We formulate a sum-rate maximization problem and then propose a fractional programming based algorithm to solve the problem by optimizing the bandwidth and power allocation iteratively. The closed-form optimal solutions for bandwidth allocation and power allocation in each iteration are also derived. Simulation results show the capacity enhancement brought by the dual-band connectivity of the HAP. The influence of the power of the HAP and the power of the satellite is also discussed.', 'corpus_id': 235742722, 'score': 0}]
40	{'doc_id': '23892230', 'title': 'Challenges in Data-to-Document Generation', 'abstract': 'Recent neural models have shown significant progress on the problem of generating short descriptive texts conditioned on a small number of database records. In this work, we suggest a slightly more difficult data-to-text generation task, and investigate how effective current approaches are on this task. In particular, we introduce a new, large-scale corpus of data records paired with descriptive documents, propose a series of extractive evaluation methods for analyzing performance, and obtain baseline results using current neural generation methods. Experiments show that these models produce fluent text, but fail to convincingly approximate human-generated documents. Moreover, even templated baselines exceed the performance of these neural models on some metrics, though copy- and reconstruction-based extensions lead to noticeable improvements.', 'corpus_id': 23892230}	10160	[{'doc_id': '233296604', 'title': 'Learning to Reason for Text Generation from Scientific Tables', 'abstract': 'In this paper, we introduce SciGen, a new challenge dataset for the task of reasoningaware data-to-text generation consisting of tables from scientific articles and their corresponding descriptions. Describing scientific tables goes beyond the surface realization of the table content and requires reasoning over table values. The unique properties of SciGen are that (1) tables mostly contain numerical values, and (2) the corresponding descriptions require arithmetic reasoning. SciGen is therefore the first dataset that assesses the arithmetic reasoning capabilities of generation models on complex input structures, i.e., tables from scientific articles. We study the effectiveness of state-of-the-art data-to-text generation models on SciGen and evaluate the results using common metrics as well as human evaluation. Our results and analyses show that (a) while humans like to reason for describing scientific tables, the ability of stateof-the-art models is severely limited on this task, (b) while adding more training data improves the results, it is not the solution for reasoning-aware text generation, and (c) one of the main bottlenecks for this task is the lack of proper automatic evaluation metrics. The data, code, and annotations for human evaluation will be available at https://github. com/UKPLab/SciGen. SciGen opens new avenues for future research in reasoning-aware text generation and evaluation.', 'corpus_id': 233296604, 'score': 1}, {'doc_id': '233189589', 'title': 'Expanding, Retrieving and Infilling: Diversifying Cross-Domain Question Generation with Flexible Templates', 'abstract': 'Sequence-to-sequence based models have recently shown promising results in generating high-quality questions. However, these models are also known to have main drawbacks such as lack of diversity and bad sentence structures. In this paper, we focus on question generation over SQL database and propose a novel framework by expanding, retrieving, and infilling that first incorporates flexible templates with a neural-based model to generate diverse expressions of questions with sentence structure guidance. Furthermore, a new activation/deactivation mechanism is proposed for template-based sequence-to-sequence generation, which learns to discriminate template patterns and content patterns, thus further improves generation quality. We conduct experiments on two large-scale cross-domain datasets. The experiments show that the superiority of our question generation method in producing more diverse questions while maintaining high quality and consistency under both automatic evaluation and human evaluation.', 'corpus_id': 233189589, 'score': 0}, {'doc_id': '232320803', 'title': 'SAFEval: Summarization Asks for Fact-based Evaluation', 'abstract': 'Summarization evaluation remains an open research problem: current metrics such as ROUGE are known to be limited and to correlate poorly with human judgments. To alleviate this issue, recent work has proposed evaluation metrics which rely on question answering models to assess whether a summary contains all the relevant information in its source document. Though promising, the proposed approaches have so far failed to correlate better than ROUGE with human judgments. In this paper, we extend previous approaches and propose a unified framework, named SAFEval. In contrast to established metrics such as ROUGE or BERTScore, SAFEval does not require any ground-truth reference. Nonetheless, SAFEval substantially improves the correlation with human judgments over four evaluation dimensions (consistency, coherence, fluency, and relevance), as shown in the extensive experiments we report.', 'corpus_id': 232320803, 'score': 0}, {'doc_id': '233004503', 'title': 'TAPAS at SemEval-2021 Task 9: Reasoning over tables with intermediate pre-training', 'abstract': 'We present the TAPAS contribution to the Shared Task on Statement Verification and Evidence Finding with Tables (SemEval 2021 Task 9, Wang et al. (2021)). SEM TAB FACT Task A is a classification task of recognizing if a statement is entailed, neutral or refuted by the content of a given table. We adopt the binary TAPAS model of Eisenschlos et al. (2020) to this task. We learn two binary classification models: A first model to predict if a statement is neutral or non-neutral and a second one to predict if it is entailed or refuted. As the shared task training set contains only entailed or refuted examples, we generate artificial neutral examples to train the first model. Both models are pre-trained using a MASKLM objective, intermediate counter-factual and synthetic data (Eisenschlos et al., 2020) and TABFACT (Chen et al., 2020), a large table entailment dataset. We find that the artificial neutral examples are somewhat effective at training the first model, achieving 68.03 test F1 versus the 60.47 of a majority baseline. For the second stage, we find that the pre-training on the intermediate data and TABFACT improves the results over MASKLM pre-training (68.03 vs 57.01).', 'corpus_id': 233004503, 'score': 1}, {'doc_id': '233268388', 'title': 'Have Your Text and Use It Too! End-to-End Neural Data-to-Text Generation with Semantic Fidelity', 'abstract': 'End-to-end neural data-to-text (D2T) generation has recently emerged as an alternative to pipeline-based architectures. However, it has faced challenges generalizing to new domains and generating semantically consistent text. In this work, we present DATATUNER, a neural, end-to-end data-to-text generation system that makes minimal assumptions about the data representation and target domain. We take a two-stage generation-reranking approach, combining a fine-tuned language model with a semantic fidelity classifier. Each component is learnt end-toend without needing dataset-specific heuristics, entity delexicalization, or post-processing. We show that DATATUNER achieves state of the art results on automated metrics across four major D2T datasets (LDC2017T10, WebNLG, ViGGO, and Cleaned E2E), with fluency assessed by human annotators as nearing or exceeding the human-written reference texts. Our generated text has better semantic fidelity than the state of the art on these datasets. We further demonstrate that our model-based semantic fidelity scorer is a better assessment tool compared to traditional heuristic-based measures of semantic accuracy.', 'corpus_id': 233268388, 'score': 1}, {'doc_id': '233189560', 'title': 'Discourse Understanding and Factual Consistency in Abstractive Summarization', 'abstract': 'We introduce a general framework for abstractive summarization with factual consistency and distinct modeling of the narrative flow in an output summary. Our work addresses current limitations of models for abstractive summarization that often hallucinate information or generate summaries with coherence issues. To generate abstractive summaries with factual consistency and narrative flow, we propose Cooperative Generator-Discriminator Networks (Co-opNet), a novel transformer-based framework where the generator works with a discriminator architecture to compose coherent long-form summaries. We explore four different discriminator objectives which each capture a different aspect of coherence, including whether salient spans of generated abstracts are hallucinated or appear in the input context, and the likelihood of sentence adjacency in generated abstracts. We measure the ability of Co-opNet to learn these objectives with arXiv scientific papers, using the abstracts as a proxy for gold long-form scientific article summaries. Empirical results from automatic and human evaluations demonstrate that Co-opNet learns to summarize with considerably improved global coherence compared to competitive baselines.', 'corpus_id': 233189560, 'score': 0}, {'doc_id': '233189556', 'title': 'Structural Encoding and Pre-training Matter: Adapting BERT for Table-Based Fact Verification', 'abstract': 'Growing concern with online misinformation has encouraged NLP research on fact verification. Since writers often base their assertions on structured data, we focus here on verifying textual statements given evidence in tables. Starting from the Table Parsing (TAPAS) model developed for question answering (Herzig et al., 2020), we find that modeling table structure improves a language model pre-trained on unstructured text. Pre-training language models on English Wikipedia table data further improves performance. Pre-training on a question answering task with column-level cell rank information achieves the best performance. With improved pre-training and cell embeddings, this approach outperforms the state-of-the-art Numerically-aware Graph Neural Network table fact verification model (GNN-TabFact), increasing statement classification accuracy from 72.2% to 73.9% even without modeling numerical information. Incorporating numerical information with cell rankings and pre-training on a question-answering task increases accuracy to 76%. We further analyze accuracy on statements implicating single rows or multiple rows and columns of tables, on different numerical reasoning subtasks, and on generalizing to detecting errors in statements derived from the ToTTo table-to-text generation dataset.', 'corpus_id': 233189556, 'score': 1}, {'doc_id': '233240996', 'title': 'Planning with Entity Chains for Abstractive Summarization', 'abstract': 'Pre-trained transformer-based sequence-tosequence models have become the go-to solution for many text generation tasks, including summarization. However, the results produced by these models tend to contain significant issues such as hallucinations and irrelevant passages. One solution to mitigate these problems is to incorporate better content planning in neural summarization. We propose to use entity chains (i.e., chains of entities mentioned in the summary) to better plan and ground the generation of abstractive summaries. In particular, we augment the target by prepending it with its entity chain. We experimented with both pre-training and finetuning with this content planning objective. When evaluated on CNN/DailyMail, SAMSum and XSum, models trained with this objective improved on entity correctness and summary conciseness, and achieved state-of-the-art performance on ROUGE for SAMSum and XSum.', 'corpus_id': 233240996, 'score': 0}, {'doc_id': '233025430', 'title': 'Inference Time Style Control for Summarization', 'abstract': 'How to generate summaries of different styles without requiring corpora in the target styles, or training separate models? We present two novel methods that can be deployed during summary decoding on any pre-trained Transformer-based summarization model. (1) Decoder state adjustment instantly modifies decoder final states with externally trained style scorers, to iteratively refine the output against a target style. (2) Word unit prediction constrains the word usage to impose strong lexical control during generation. In experiments of summarizing with simplicity control, automatic evaluation and human judges both find our models producing outputs in simpler languages while still informative. We also generate news headlines with various ideological leanings, which can be distinguished by humans with a reasonable probability.', 'corpus_id': 233025430, 'score': 0}, {'doc_id': '232478685', 'title': 'FeTaQA: Free-form Table Question Answering', 'abstract': 'Existing table question answering datasets contain abundant factual questions that primarily evaluate the query and schema comprehension capability of a system, but they fail to include questions that require complex reasoning and integration of information due to the constraint of the associated short-form answers. To address these issues and to demonstrate the full challenge of table question answering, we introduce FeTaQA, a new dataset with 10K Wikipediabased {table, question, free-form answer, supporting table cells} pairs. FeTaQA yields a more challenging table question answering setting because it requires generating free-form text answers after retrieval, inference, and integration of multiple discontinuous facts from a structured knowledge source. Unlike datasets of generative QA over text in which answers are prevalent with copies of short text spans from the source, answers in our dataset are human-generated explanations involving entities and their high-level relations. We provide two benchmark methods for the proposed task: a pipeline method based on semantic parsingbased QA systems and an end-to-end method based on large pretrained text generation models, and show that FeTaQA poses a challenge for both methods.', 'corpus_id': 232478685, 'score': 1}]
41	{'doc_id': '221559642', 'title': 'Development of a Peer-Reviewed Open-Access Undergraduate Research Journal†', 'abstract': 'Dissemination of results is a fundamental aspect of the scientific process and requires an avenue for publication that is specifically designed to suit the nature of the research being communicated. Undergraduate research journals provide a unique forum for students to report scientific findings and ideas while learning about the complete scientific process. We have developed a peer-reviewed, open-access, international undergraduate research journal that is linked to a course-based undergraduate research experience. We reflect on lessons learned and recommend effective approaches for the implementation and operation of a successful undergraduate research journal.', 'corpus_id': 221559642}	20854	[{'doc_id': '158128148', 'title': 'Running a Student Journal: Best Practices for Success and Sustainability', 'abstract': 'The presentation was given at the Third Annual Student Journal Forum organized by the University of Toronto Libraries in January 2018.', 'corpus_id': 158128148, 'score': 1}, {'doc_id': '238000559', 'title': 'Advances in Mobile Learning Educational Research (A.M.L.E.R.): Mobile learning as an educational reform', 'abstract': 'The journal Advances in Educational Research and Evaluation is a peer-reviewed openaccess journal aimed to be a medium for discussing a wide range of international educational experiences and assessment techniques. The journal intends to publish high-quality articles, the scope of which includes but is not limited to topics mentioned in this editorial. With the support of an international team of educational scholars who kindly volunteered to serve on the editorial board, the journal is set to adhere to the highest publishing ethics standards.', 'corpus_id': 238000559, 'score': 0}, {'doc_id': '238009872', 'title': 'Impacting and Influencing the System to Support Student Career Readiness, Voice, and Efficacy', 'abstract': 'This chapter provides context for the design of an experiential, service-learning-based capstone course that provides students with meaningful service-based learning experiences. Through experiential learning and publication opportunities, students develop transferable career development, communication, writing, and critical thinking skills. It is a related goal that students leave this course with improved self and collective efficacy and a fundamentally heightened awareness of their own potential to create positive change in their community.', 'corpus_id': 238009872, 'score': 0}, {'doc_id': '210362922', 'title': 'Reflections Of A (Former) Student Journal Director', 'abstract': 'The presentation was given at the Fourth Annual Student Journal Forum organized by the University of Toronto Libraries in January 2019.', 'corpus_id': 210362922, 'score': 1}, {'doc_id': '198131840', 'title': 'Publication in a medical student journal predicts short- and long-term academic success: a matched-cohort study', 'abstract': 'BackgroundMedical student journals play a critical role in promoting academic research and publishing amongst medical students, but their impact on students’ future academic achievements has not been examined. We aimed to evaluate the short- and long-term effects of publication in the New Zealand Medical Student Journal (NZMSJ) through examining rates of post-graduation publication, completion of higher academic degrees, and pursuing an academic career.MethodsStudent-authored original research publications in the NZMSJ during the period 2004–2011 were retrospectively identified. Gender-, university- and graduation year-matched controls were identified from publicly available databases in a 2:1 ratio (two controls for each student authors). Date of graduation, current clinical scope of practice, completion of higher academic degrees, and attainment of an academic position for both groups were obtained from Google searches, New Zealand graduate databases, online lists of registered doctors in New Zealand and Australia, and author affiliation information from published articles. Pre- and post-graduation PubMed®-indexed publications were identified using standardised search criteria.ResultsFifty publications authored by 49 unique students were identified. The median follow-up period after graduation was 7.0\u2009years (range 2–12\u2009years). Compared with controls, student-authors were significantly more likely to publish in PubMed®-indexed journals (OR 3.09, p\xa0=\u20090.001), obtain a PhD (OR 9.21, p\xa0=\u20090.004) or any higher degree (OR 2.63, p\xa0=\u20090.007), and attain academic positions (OR 2.90, p\xa0=\u20090.047) following graduation.ConclusionPublication in a medical student journal is associated with future academic achievement and contributes to develop a clinical academic workforce. Future work should aim to explore motivators and barriers associated with these findings.', 'corpus_id': 198131840, 'score': 1}, {'doc_id': '230451426', 'title': 'Experiential Learning in Business Communication: Starting a Peer-Reviewed Student Journal and Podcast', 'abstract': 'Ancient Greeks such as Plato, Socrates, and Aristotle realized the need for combining intellectual, theoretical learning with practical, real-world experiences. Modern educational theorists continue to make similar pleas for the need of a more holistic and experiential view of education. To promote experiential learning in business communication, Brigham Young University – Provo (BYU) started a peer-reviewed student journal and podcast for business students. The students’ experiences mirror Kolb’s experiential learning theory cycle (1984) and Mezirow’s theories on critical reflection and transformative learning, (1990, 1998, 2000). Students’ reflective comments were reviewed using Morris’ (2019) five characteristics of concrete learning experiences. Additionally, student ratings for this course are compared to other business management courses at BYU. These comparisons illustrate the high rating students give experiential learning courses. As the journal and podcast continue to grow in popularity, the opportunities for students have also grown because the students are gaining practical experience for future careers.', 'corpus_id': 230451426, 'score': 1}, {'doc_id': '237565694', 'title': 'Student Plagiarism: Never-Ending Challenges and Possibilities for Faculty', 'abstract': 'Student plagiarism is a never-ending challenge for faculty. This column introduction shares faculty experiences as well as some successful interventions. The author grounds faculty student plagiarism struggles with the humanbecoming teaching-learning module and reminds faculty to address the issue for the benefit of the student and the discipline.', 'corpus_id': 237565694, 'score': 0}, {'doc_id': '237261765', 'title': 'How to Become Involved in JME? Joining the Peer-Review Process', 'abstract': '“How does one become an editor?” is a question that many of our nonacademic friends and family members have been asking us the past few months as we trained and transitioned into this role. This made us realize that there is a proverbial “black box” in terms of professional development and promotion for those outside our field, such that most people have little understanding of how we transition into different roles in academic settings. Indeed, looking back to our first few years in academia, we did not really know all the “ins and outs” of journal hierarchy and processes. Hence, we continue the editorial tradition of providing further clarity and transparency in the editorial review process by focusing on all the opportunities available at JME, many of which are available in other journals. This editorial aims to provide specific action steps and information for novice academics who are keenly interested in management education and the scholarship of teaching and learning. This information is particularly relevant to early career researchers and academics without previous coaching and training on standard editorial roles. We will provide information about how to become a reviewer and suggestions for reviewers seeking to move onto the Editorial Board and into Associate Editor roles. Our guidance for those interested in getting involved in the Editorial process at JME focuses on the four E’s: Entering, Engaging, Extending, and Exiting.', 'corpus_id': 237261765, 'score': 0}, {'doc_id': '158804885', 'title': 'Publishing a student journal? The library can help!', 'abstract': 'The presentation was given at the Third Annual Student Journal Forum organized by the University of Toronto Libraries in January 2018.', 'corpus_id': 158804885, 'score': 1}, {'doc_id': '238030809', 'title': 'Virtues, Character Strengths, and Graduate Student Organizations', 'abstract': 'Research in the area of student life has shown that student engagement in student organizations improves a number of undergraduate and graduate student outcomes. Because of the critical importance that student organizations play in student development, continued research is needed to understand the elements that make such organizations successful. This chapter will utilize the reflections of three faculty advisors and two student presidents to explore how virtues and character strengths played a critical role in the continued success of a student organization within the context of a college of education.', 'corpus_id': 238030809, 'score': 0}]
42	{'doc_id': '218665668', 'title': 'Think Too Fast Nor Too Slow: The Computational Trade-off Between Planning And Reinforcement Learning', 'abstract': 'Planning and reinforcement learning are two key approaches to sequential decision making. Multi-step approximate real-time dynamic programming, a recently successful algorithm class of which AlphaZero [Silver et al., 2018] is an example, combines both by nesting planning within a learning loop. However, the combination of planning and learning introduces a new question: how should we balance time spend on planning, learning and acting? The importance of this trade-off has not been explicitly studied before. We show that it is actually of key importance, with computational results indicating that we should neither plan too long nor too short. Conceptually, we identify a new spectrum of planning-learning algorithms which ranges from exhaustive search (long planning) to model-free RL (no planning), with optimal performance achieved midway.', 'corpus_id': 218665668}	5139	"[{'doc_id': '218516818', 'title': 'Learning Adaptive Exploration Strategies in Dynamic Environments Through Informed Policy Regularization', 'abstract': 'We study the problem of learning exploration-exploitation strategies that effectively adapt to dynamic environments, where the task may change over time. While RNN-based policies could in principle represent such strategies, in practice their training time is prohibitive and the learning process often converges to poor solutions. In this paper, we consider the case where the agent has access to a description of the task (e.g., a task id or task parameters) at training time, but not at test time. We propose a novel algorithm that regularizes the training of an RNN-based policy using informed policies trained to maximize the reward in each task. This dramatically reduces the sample complexity of training RNN-based policies, without losing their representational power. As a result, our method learns exploration strategies that efficiently balance between gathering information about the unknown and changing task and maximizing the reward over time. We test the performance of our algorithm in a variety of environments where tasks may vary within each episode.', 'corpus_id': 218516818, 'score': 0}, {'doc_id': '219176823', 'title': 'Invariant Policy Optimization: Towards Stronger Generalization in Reinforcement Learning', 'abstract': 'A fundamental challenge in reinforcement learning is to learn policies that generalize beyond the operating domains experienced during training. In this paper, we approach this challenge through the following invariance principle: an agent must find a representation such that there exists an action-predictor built on top of this representation that is simultaneously optimal across all training domains. Intuitively, the resulting invariant policy enhances generalization by finding causes of successful actions. We propose a novel learning algorithm, Invariant Policy Optimization (IPO), that implements this principle and learns an invariant policy during training. We compare our approach with standard policy gradient methods and demonstrate significant improvements in generalization performance on unseen domains for linear quadratic regulator and grid-world problems, and an example where a robot must learn to open doors with varying physical properties.', 'corpus_id': 219176823, 'score': 1}, {'doc_id': '145022099', 'title': 'A novel deep residual network-based incomplete information competition strategy for four-players Mahjong games', 'abstract': 'The game theory is widely acknowledged to benefit a lot from recent advances in deep learning, and intelligent competition strategies have been proposed for both complete information games and incomplete information games in recent years. In this paper, the four-players Chinese Mahjong game, which is a typical incomplete information game, is emphasized, a low-level semantic pseudo image generated based on game related prior knowledge and a novel deep residual network-based competition strategy are introduced to play the Chines Mahjong game. Technically, the deep learning within this new competition strategy is realized by a series of “GoBlock”, which is a new deep learning model structure introduced in this paper. Also, the “GoBlock” is further made up of several “Inception+” sub-structures, which is novel as well. Comprehensive experiments are conducted to reveal the superiority of this new competition strategy. A great number of the Chinese Mahjong game data have been collected from an online Chinese Mahjong company to construct the dataset in this study, and the newly proposed competition strategy has been compared with several shallow learning-based methods as well as deep learning-based methods. Both qualitative and quantitative analysis have been conducted based on outcomes obtained by all compared methods, and the superiority of the new competition strategy over others are suggested. Furthermore, an interesting competition among the new AI competition strategy and three real senior players are also introduced in this paper. The effectiveness and efficiency of the new competition strategy over real senior human players are also revealed by quantitative analysis based on four measures, from the statistical point of view. It is also necessary to point out that, this work is the first attempt to tackle the Mahjong game, which is a typical incomplete information game, from the deep learning perspective.', 'corpus_id': 145022099, 'score': 1}, {'doc_id': '219721048', 'title': 'Learning What to Defer for Maximum Independent Sets', 'abstract': 'Designing efficient algorithms for combinatorial optimization appears ubiquitously in various scientific fields. Recently, deep reinforcement learning (DRL) frameworks have gained considerable attention as a new approach: they can automate the design of a solver while relying less on sophisticated domain knowledge of the target problem. However, the existing DRL solvers determine the solution using a number of stages proportional to the number of elements in the solution, which severely limits their applicability to large-scale graphs. In this paper, we seek to resolve this issue by proposing a novel DRL scheme, coined learning what to defer (LwD), where the agent adaptively shrinks or stretch the number of stages by learning to distribute the element-wise decisions of the solution at each stage. We apply the proposed framework to the maximum independent set (MIS) problem, and demonstrate its significant improvement over the current state-of-the-art DRL scheme. We also show that LwD can outperform the conventional MIS solvers on large-scale graphs having millions of vertices, under a limited time budget.', 'corpus_id': 219721048, 'score': 0}, {'doc_id': '219177434', 'title': 'Manipulating the Distributions of Experience used for Self-Play Learning in Expert Iteration', 'abstract': 'Expert Iteration (ExIt) is an effective framework for learning game-playing policies from self-play. ExIt involves training a policy to mimic the search behaviour of a tree search algorithm -— such as Monte-Carlo tree search -— and using the trained policy to guide it. The policy and the tree search can then iteratively improve each other, through experience gathered in self-play between instances of the guided tree search algorithm. This paper outlines three different approaches for manipulating the distribution of data collected from self-play, and the procedure that samples batches for learning updates from the collected data. Firstly, samples in batches are weighted based on the durations of the episodes in which they were originally experienced. Secondly, Prioritized Experience Replay is applied within the ExIt framework, to prioritise sampling experience from which we expect to obtain valuable training signals. Thirdly, a trained exploratory policy is used to diversify the trajectories experienced in self-play. This paper summarises the effects of these manipulations on training performance evaluated in fourteen different board games. We find major improvements in early training performance in some games, and minor improvements averaged over fourteen games.', 'corpus_id': 219177434, 'score': 1}, {'doc_id': '212644574', 'title': 'Convex Hull Monte-Carlo Tree Search', 'abstract': 'This work investigates Monte-Carlo planning for agents in stochastic environments, with multiple objectives. We propose the Convex Hull Monte-Carlo Tree-Search (CHMCTS) framework, which builds upon Trial Based Heuristic Tree Search and Convex Hull Value Iteration (CHVI), as a solution to multi-objective planning in large environments. Moreover, we consider how to pose the problem of approximating multiobjective planning solutions as a contextual multi-armed bandits problem, giving a principled motivation for how to select actions from the view of contextual regret. This leads us to the use of Contextual Zooming for action selection, yielding Zooming CHMCTS. We evaluate our algorithm using the Generalised Deep Sea Treasure environment, demonstrating that Zooming CHMCTS can achieve a sublinear contextual regret and scales better than CHVI on a given computational budget.', 'corpus_id': 212644574, 'score': 1}, {'doc_id': '219635852', 'title': 'Mutual Information Based Knowledge Transfer Under State-Action Dimension Mismatch', 'abstract': 'Deep reinforcement learning (RL) algorithms have achieved great success on a wide variety of sequential decision-making tasks. However, many of these algorithms suffer from high sample complexity when learning from scratch using environmental rewards, due to issues such as credit-assignment and high-variance gradients, among others. Transfer learning, in which knowledge gained on a source task is applied to more efficiently learn a different but related target task, is a promising approach to improve the sample complexity in RL. Prior work has considered using pre-trained teacher policies to enhance the learning of the student policy, albeit with the constraint that the teacher and the student MDPs share the state-space or the action-space. In this paper, we propose a new framework for transfer learning where the teacher and the student can have arbitrarily different state- and action-spaces. To handle this mismatch, we produce embeddings which can systematically extract knowledge from the teacher policy and value networks, and blend it into the student networks. To train the embeddings, we use a task-aligned loss and show that the representations could be enriched further by adding a mutual information loss. Using a set of challenging simulated robotic locomotion tasks involving many-legged centipedes, we demonstrate successful transfer learning in situations when the teacher and student have different state- and action-spaces.', 'corpus_id': 219635852, 'score': 0}, {'doc_id': '219177064', 'title': 'PlanGAN: Model-based Planning With Sparse Rewards and Multiple Goals', 'abstract': 'Learning with sparse rewards remains a significant challenge in reinforcement learning (RL), especially when the aim is to train a policy capable of achieving multiple different goals. To date, the most successful approaches for dealing with multi-goal, sparse reward environments have been model-free RL algorithms. In this work we propose PlanGAN, a model-based algorithm specifically designed for solving multi-goal tasks in environments with sparse rewards. Our method builds on the fact that any trajectory of experience collected by an agent contains useful information about how to achieve the goals observed during that trajectory. We use this to train an ensemble of conditional generative models (GANs) to generate plausible trajectories that lead the agent from its current state towards a specified goal. We then combine these imagined trajectories into a novel planning algorithm in order to achieve the desired goal as efficiently as possible. The performance of PlanGAN has been tested on a number of robotic navigation/manipulation tasks in comparison with a range of model-free reinforcement learning baselines, including Hindsight Experience Replay. Our studies indicate that PlanGAN can achieve comparable performance whilst being around 4-8 times more sample efficient.', 'corpus_id': 219177064, 'score': 1}, {'doc_id': '218538147', 'title': 'Learning, transferring, and recommending performance knowledge with Monte Carlo tree search and neural networks', 'abstract': 'Making changes to a program to optimize its performance is an unscalable task that relies entirely upon human intuition and experience. In addition, companies operating at large scale are at a stage where no single individual understands the code controlling its systems, and for this reason, making changes to improve performance can become intractably difficult. In this paper, a learning system is introduced that provides AI assistance for finding recommended changes to a program. Specifically, it is shown how the evaluative feedback, delayed-reward performance programming domain can be effectively formulated via the Monte Carlo tree search (MCTS) framework. It is then shown that established methods from computational games for using learning to expedite tree-search computation can be adapted to speed up computing recommended program alterations. Estimates of expected utility from MCTS trees built for previous problems are used to learn a sampling policy that remains effective across new problems, thus demonstrating transferability of optimization knowledge. This formulation is applied to the Apache Spark distributed computing environment, and a preliminary result is observed that the time required to build a search tree for finding recommendations is reduced by up to a factor of 10x.', 'corpus_id': 218538147, 'score': 0}, {'doc_id': '216562627', 'title': 'Image Augmentation Is All You Need: Regularizing Deep Reinforcement Learning from Pixels', 'abstract': ""We propose a simple data augmentation technique that can be applied to standard model-free reinforcement learning algorithms, enabling robust learning directly from pixels without the need for auxiliary losses or pre-training. The approach leverages input perturbations commonly used in computer vision tasks to regularize the value function. Existing model-free approaches, such as Soft Actor-Critic (SAC), are not able to train deep networks effectively from image pixels. However, the addition of our augmentation method dramatically improves SAC's performance, enabling it to reach state-of-the-art performance on the DeepMind control suite, surpassing model-based (Dreamer, PlaNet, and SLAC) methods and recently proposed contrastive learning (CURL). Our approach can be combined with any model-free reinforcement learning algorithm, requiring only minor modifications. An implementation can be found at this https URL."", 'corpus_id': 216562627, 'score': 0}]"
43	{'doc_id': '198406103', 'title': 'Reinterpreting Archaeobotany in Mainland Southeast Asia', 'abstract': None, 'corpus_id': 198406103}	17286	"[{'doc_id': '183938136', 'title': 'Die Maske der Einsamkeit', 'abstract': '„Ich lerne sehen.“ Während seiner Streifzüge durch Paris verzeichnet der Protagonist Szenen des Lebens – vor allem aber des Sterbens. Den Schreckensbildern armer Menschen, die unbekannt und zahllos vergehen, hält Malte die Erinnerungen an eine andere Zeit entgegen ...', 'corpus_id': 183938136, 'score': 0}, {'doc_id': '225996328', 'title': 'Agricultural trajectories in Yunnan, southwest China : a comparative analysis of archaeobotanical remains from the Neolithic to the Bronze Age', 'abstract': 'This dissertation investigates the emergence and development of agricultural practices in the southwest Chinese province of Yunnan, between the 3rd and 1st millennia BC. Drawing from previously unstudied archaeobotanical remains from the sites of Baiyangcun, Haimenkou, and Dayingzhuang; this research analyses compositional and chronological changes in the crop assemblage from each site. These sites are located in the strategic region of sanjiang, at the crossroads of three main Asian rivers: Yangzi, Mekong, and Salween. Local and regional developments of agricultural systems are explored through the comparison of these new material with other published datasets from Yunnan, the surrounding provinces of Sichuan, Tibet, Chongqing, Guangxi, and mainland Southeast Asian countries. The main research questions addressed in this dissertation are: -What was the basis of early agriculture in Yunnan? -Given that the first attested agricultural systems in Southwest China appear 3000 to 2000-years later than those associated with domestication centres in North China and along the Yangzi River, to what extent can agricultural practices in Yunnan be derived entirely from migrating farmers, or did adoption (acculturation) by local forager populations play a role? -What role did native wild plants play in Yunnan Neolithic and Post-Neolithic subsistence, and were there any local processes of domestication underway? -With regards to rice, what was the ecology of rice cultivation? Did this differ either from source regions along the Yangzi, or from the early systems in Southeast Asia, which have sometimes been suggested to have origins in Yunnan? The results contained in this thesis provide archaeological evidence that was until now lacking to evaluate the validity of the language/farming dispersal hypothesis in the context of the Austroasiatic languages dispersal, as well as laying an important archaeological and chronological framework for studying of the emergence of a settled agricultural lifestyle in Yunnan.', 'corpus_id': 225996328, 'score': 1}, {'doc_id': '232215337', 'title': 'Two-season agriculture and irrigated rice during the Dian: radiocarbon dates and archaeobotanical remains from Dayingzhuang, Yunnan, Southwest China', 'abstract': 'Historical sources describe irrigation and intensive agriculture being practiced in lowland Yunnan from at least the first century AD, but so far archaeobotanical remains allowing investigation of this issue have been scarce. Here, we present new archaeobotanical evidence, including macro-botanical and phytoliths results, from the Dian settlement site of Dayingzhuang, with direct AMS radiocarbon dates on two wheat grains falling between 750 and 390 BC. We compare these results with contemporary Dian sites and analyse the agricultural systems in Central Yunnan between the eight and fourth centuries BC. We propose that agriculture was intensified toward the end of the Dian through both multiple cropping seasons and increased evidence for irrigated rice fields.', 'corpus_id': 232215337, 'score': 1}, {'doc_id': '140336780', 'title': 'Method for the treatment of fibrotic disease', 'abstract': 'The invention generally A method for treating fibrosis, more specifically, relates to a CSF-1R activity inhibitor for the treatment and / or prevention of fibrotic diseases, wherein the inhibitor is a small chemical entities (NCE), a nucleic acid or an antibody or a functional thereof It includes active fragments or derivatives thereof. The invention also in the manufacture of a medicament for the treatment and / or prevention of fibrotic diseases, relates to the use of CSF-1R inhibitors.', 'corpus_id': 140336780, 'score': 0}, {'doc_id': '165734627', 'title': 'Between foraging and farming: strategic responses to the Holocene Thermal Maximum in Southeast Asia', 'abstract': 'Large, ‘complex’ pre-Neolithic hunter-gatherer communities thrived in southern China and northern Vietnam, contemporaneous with the expansion of farming. Research at Con Co Ngua in Vietnam suggests that such hunter-gatherer populations shared characteristics with early farming communities: high disease loads, pottery, complex mortuary practices and access to stable sources of carbohydrates and protein. The substantive difference was in the use of domesticated plants and animals—effectively representing alternative responses to optimal climatic conditions. The work here suggests that the supposed correlation between farming and a decline in health may need to be reassessed.', 'corpus_id': 165734627, 'score': 1}, {'doc_id': '233191063', 'title': 'Zaraa Uul: An archaeological record of Pleistocene-Holocene palaeoecology in the Gobi Desert', 'abstract': 'Environmentally-based archaeological research at Zaraa Uul, including zooarchaeology, phytolith analysis, and radiocarbon dating, is the first of its kind in Mongolia and presents critical new insight on the relationship between periods of occupational intensity and climatic amelioration from the earliest anatomically modern humans to the adoption of pastoralism. The palaeoenvironmental and faunal record of Zaraa Uul show that Early-Middle Holocene hydrology and species distributions were distinct from all other periods of human occupation. Holocene hunter-gatherers inhabited an ecosystem characterized by extensive marshes, riparian shrub and arboreal vegetation along the hill slopes and drainages. The exploitation of species associated with riparian and wetland settings supports the hypothesis of, but suggests an earlier timing for, oasis-based logistical foraging during the Early-Middle Holocene of arid Northeast Asia. The onset of wetter conditions at 8500 cal BP agrees with other regional studies, but multiple lines of evidence present the first integrated field- and laboratory-based record of human-environment relationships in arid East Asia during the Holocene Climatic Optimum. We compare it to Late Pleistocene climatic amelioration, and highlight specific responses of the hydrological, vegetative and faunal communities to climate change in arid Northeast Asia.', 'corpus_id': 233191063, 'score': 0}, {'doc_id': '234786270', 'title': 'Agriculture, the Environment, and Social Complexity From the Early to Late Yangshao Periods (5000–3000 BC): Insights From Macro-Botanical Remains in North-Central China', 'abstract': 'In northern China, the Yangshao cultural period (5000–3000 BC) was a critical timespan in the establishment of agricultural economies and the emergence of social complexity. We present the results of archeobotanical analysis from 58 soil samples collected from 12 recently investigated sites located in the Luoyang Basin, and recovered 5290 carbonized plant remains from 9 sites dating to the Late Yangshao period. We compared our novel dataset with previous archeobotanical date, compiling a total of 196 samples from 58 sites in central and western Henan Province. During the Early Yangshao period (5000–4200 BC), a nascent, extensive agricultural economy based primarily on broomcorn millet, with lesser foxtail millet and rice, was developing in small settlements (<0.2 km2) in the loess tablelands and valleys of western Henan province. However, the population pressure—rather than environmental degradation—drove the “foxtail millet-broomcorn millet substitution” during the Middle Yangshao period (4200–3500BC). The intensive agriculture based mainly on foxtail millet facilitated the development of social complexity in the region, as demonstrated by the emergence of size-graded agricultural settlements of medium (0.2–0.6 km2) and large (> 0.6 km2) scale. Notably, millets tend to be less ubiquitous in these larger settlements compared to smaller ones, with differences in millet ubiquity between sites increasing over time. The local surface hydrology influenced by paleoclimatic changes prompted the spread of agriculture from higher loess tablelands and valleys during the early Yangshao period into more marginal loess tablelands and plains by the Middle and Late Yangshao periods. Rice cultivation is concentrated in valley areas and appears to have been closely tied to environments with better hydrothermal conditions. Our research shows that climatic conditions during the Holocene fostered the development of agriculture during the Yangshao Culture period and that the distribution of settlements throughout this time was influenced by highly localized geomorphologic environments delimiting the distribution of crops. The rise of agriculture promoted the formation of complex and stratified economies in the Yangshao Culture period and it was the intensification and elaboration of these new economic and social systems that led to later transformation in agricultural structures and settlement sizes.', 'corpus_id': 234786270, 'score': 0}, {'doc_id': '24848246', 'title': 'Dental perspectives on the population history of Southeast Asia.', 'abstract': 'This article uses metric and nonmetric dental data to test the ""two-layer"" or immigration hypothesis whereby Southeast Asia was initially occupied by an ""Australo-Melanesian"" population that later underwent substantial genetic admixture with East Asian immigrants associated with the spread of agriculture from the Neolithic period onwards. We examined teeth from 4,002 individuals comprising 42 prehistoric and historic samples from East Asia, Southeast Asia, Australia, and Melanesia. For the odontometric analysis, dental size proportions were compared using factor analysis and Q-mode correlation coefficients, and overall tooth size was also compared between population samples. Nonmetric population affinities were estimated by Smith\'s distances, using the frequencies of 16 tooth traits. The results of both the metric and nonmetric analyses demonstrate close affinities between recent Australo-Melanesian samples and samples representing early Southeast Asia, such as the Early to Middle Holocene series from Vietnam, Malaysia, and Flores. In contrast, the dental characteristics of most modern Southeast Asians exhibit a mixture of traits associated with East Asians and Australo-Melanesians, suggesting that these populations were genetically influenced by immigrants from East Asia. East Asian metric and/or nonmetric traits are also found in some prehistoric samples from Southeast Asia such as Ban Kao (Thailand), implying that immigration probably began in the early Neolithic. Much clearer influence of East Asian immigration was found in Early Metal Age Vietnamese and Sulawesi samples. Although the results of this study are consistent with the immigration hypothesis, analysis of additional Neolithic samples is needed to determine the exact timing of population dispersals into Southeast Asia.', 'corpus_id': 24848246, 'score': 1}, {'doc_id': '90527003', 'title': 'Early agriculture at the crossroads of China and Southeast Asia: Archaeobotanical evidence and radiocarbon dates from Baiyangcun, Yunnan', 'abstract': ""We report archaeobotanical results from systematic flotation at what is presently the earliest Neolithic site with hard evidence for crop cultivation in the Southwestern Chinese province of Yunnan, at the site of Baiyangcun. Direct AMS dates on rice and millet seeds, included together in a Bayesian model, suggests that sedentary agricultural occupation began ca. 2650 BCE, with cultivation of already domesticated rice (Oryza sativa), broomcorn millet (Panicum miliaceum), and foxtail millet (Setaria italica). Soybean (Glycine cf. max) was also present and presumably cultivated, although it still resembles its wild progenitor in terms of seed size. Additional possible cultivars include melon (Cucumis melo) and an unknown Vigna pulse, while wild gathered resources include fruits and nuts, including hawthorn (Crateagus) and aquatic foxnut (Euryale ferox). Weed flora suggests at least some rice was cultivated in wet (flooded or irrigated fields), while dryland weeds may derive from millet fields. This subsistence system persisted throughout the site's occupation, up to ca. 2050 BCE. These data provide secure evidence for the spread of Chinese Neolithic crops to Yunnan, and provide new evidence for reconstructing possible sources of cereal agriculture in mainland Southeast Asia."", 'corpus_id': 90527003, 'score': 1}, {'doc_id': '233489400', 'title': 'Le Néolithique du Nord de la France dans son contexte européen : habitat et économie aux 4e et 3e millénaires avant notre ère', 'abstract': 'The rich evidence derived from excavations in the wetlands of the Rhine/meue delta has revealed that the neolithisation of the Lower Rhine Basin, to the north of the loess, was a long lasting and phased process. Between 5300 and 3400 cal BC three main phases can be identified, characterised here as a material, an economic/ideological and a social stage. schipluiden represents the start of the last stage, in which a local community of several households settled permanently for many generations at one place, while fences and an extensive cluster of short lived wells demonstrate collective activities of the entire local group. in subsistence and mode of life the best of the « old » and the « new » were combined. inspiration came from the south, from the spiere and michelsberg communities. microregional divergent trajectories in neolithisation are seen as reflecting variation in agency on a local level.', 'corpus_id': 233489400, 'score': 0}]"
44	"{'doc_id': '226232135', 'title': 'Predicting dairy cattle heat stress using machine learning techniques.', 'abstract': ""The objectives of the study were to use a heat stress scoring system to evaluate the severity of heat stress on dairy cows using different heat abatement techniques. The scoring system ranged from 1 to 4, where 1 = no heat stress; 2 = mild heat stress; 3 = severe heat stress; and 4 = moribund. The accuracy of the scoring system was then predicted using 3 machine learning techniques: logistic regression, Gaussian naïve Bayes, and random forest. To predict the accuracy of the scoring system, these techniques used factors including temperature-humidity index, respiration rate, lying time, lying bouts, total steps, drooling, open-mouth breathing, panting, location in shade or sprinklers, somatic cell score, reticulorumen temperature, hygiene body condition score, milk yield, and milk fat and protein percent. Three different treatments, namely, portable shade structure, portable polyvinyl chloride pipe sprinkler system, or control with no heat abatement, were considered, where each treatment was replicated 3 times with 3 second-trimester lactating cows. Results indicate that random forest outperformed the other 2 methods, with respect to both accuracy and precision, in predicting the sprinkler group's score. Both logistic regression and random forest were consistent in predicting scores for control, shade, and combined groups. The mean probability of predicting non-heat-stressed cows was highest for cows in the sprinkler group. Finally, the logistic regression method worked best for predicting heat-stressed cows in control, shade, and combined. The insights gained from these results could aid dairy producers to detect heat stress before it becomes severe, which could decrease the negative effects of heat stress, such as milk loss."", 'corpus_id': 226232135}"	16386	"[{'doc_id': '233419647', 'title': 'The Effect of Heat Stress on Milk Yield, Milk Fat Rate and Rectal Temperature in Holstein-Friesian Dairy Cattle', 'abstract': ""Heat stress is an environmental factor that negatively affects the morphological and physiological properties of dairy cattle. The aim of this study is to investigate the relationship between heat stress and milk yield, milk fat ratio and body temperature in Holstein-Friesian dairy cattle. Data Southeastern Anatolia Region of Turkey's Siirt province was obtained from a special Kurtalan Farm. Milk yield and content of 13 head Holstein-Friesian dairy cattle were recorded in March, April, May, June, July, August and September. In addition, temperature and humidity records were recorded in the farm and in the parlor to be used for calculating the temperature humidity index value. In the analysis of data, correlation and regression methods were used in SAS 9.4 program. As a result, positive correlation (P <0.001), milk yield and milk fat ratio and negative correlation (P <0.05, P <0.01) were detected between heat stress and body temperature. In addition, a significant negative relationship was observed between rectal temperature and milk yield (P <0.01)."", 'corpus_id': 233419647, 'score': 0}, {'doc_id': '233388460', 'title': 'Recent Advances on Early Detection of Heat Strain in Dairy Cows Using Animal-Based Indicators: A Review', 'abstract': 'Simple Summary In the dairy industry, heat stress and its induced heat strain result in huge economic loss every year. To better manage heat strain in dairy cows, it is more sensible to advance the detection by using more sensitive indicators so that cooling measures can be implemented in time. With the development of sensor technologies and wireless transmission technologies, body surface temperature and respiration rate can be measured automatically through wearable devices. Lots of efforts have been made recently to develop meaningful thresholds on both physiological and environmental sides. However, the existing thresholds should be used carefully considering the differences in experimental conditions and animal information. Further studies are required to evaluate and customize thresholds based on different influencing factors. A comprehensive early detection system for heat strain based on both animal- and environment-based indicators is expected. Abstract In pursuit of precision livestock farming, the real-time measurement for heat strain-related data has been more and more valued. Efforts have been made recently to use more sensitive physiological indicators with the hope to better inform decision-making in heat abatement in dairy farms. To get an insight into the early detection of heat strain in dairy cows, the present review focuses on the recent efforts developing early detection methods of heat strain in dairy cows based on body temperatures and respiratory dynamics. For every candidate animal-based indicator, state-of-the-art measurement methods and existing thresholds were summarized. Body surface temperature and respiration rate were concluded to be the best early indicators of heat strain due to their high feasibility of measurement and sensitivity to heat stress. Future studies should customize heat strain thresholds according to different internal and external factors that have an impact on the sensitivity to heat stress. Wearable devices are most promising to achieve real-time measurement in practical dairy farms. Combined with internet of things technologies, a comprehensive strategy based on both animal- and environment-based indicators is expected to increase the precision of early detection of heat strain in dairy cows.', 'corpus_id': 233388460, 'score': 1}, {'doc_id': '233740622', 'title': 'Random Forest Modelling of Milk Yield of Dairy Cows under Heat Stress Conditions', 'abstract': 'Simple Summary Sustainability is a necessary goal for animal-derived products due to the mounting pressure on the livestock sector to meet the growing demand of an increasing population with rising incomes and the need to reduce the exploitation of resources and environmental impact, while safeguarding animal welfare. We found that by considering a precision livestock farming approach to feeding, advanced numerical methods could represent a reliable and viable tool for the evaluation of future productive scenarios of dairy cows in the presence of changing climate conditions. We believe that the model proposed here could help to develop and improve decision support for farmers to increase both milk yield and animal welfare and, on the other hand, to reduce the resources needed, hence increasing sustainability of the dairy sector. Abstract Precision Livestock Farming (PLF) relies on several technological approaches to acquire, in the most efficient way, precise and real-time data concerning production and welfare of individual animals. In this regard, in the dairy sector, PLF devices are being increasingly adopted, automatic milking systems (AMSs) are becoming increasingly widespread, and monitoring systems for animals and environmental conditions are becoming common tools in herd management. As a consequence, a great amount of daily recorded data concerning individual animals are available for the farmers and they could be used effectively for the calibration of numerical models to be used for the prediction of future animal production trends. On the other hand, the machine learning approaches in PLF are nowadays considered an extremely promising solution in the research field of livestock farms and the application of these techniques in the dairy cattle farming would increase sustainability and efficiency of the sector. The study aims to define, train, and test a model developed through machine learning techniques, adopting a Random Forest algorithm, having the main goal to assess the trend in daily milk yield of a single cow in relation to environmental conditions. The model has been calibrated and tested on the data collected on 91 lactating cows of a dairy farm, located in northern Italy, and equipped with an AMS and thermo-hygrometric sensors during the years 2016–2017. In the statistical model, having seven predictor features, the daily milk yield is evaluated as a function of the position of the day in the lactation curve and the indoor barn conditions expressed in terms of daily average of the temperature-humidity index (THI) in the same day and its value in each of the five previous days. In this way, extreme hot conditions inducing heat stress effects can be considered in the yield predictions by the model. The average relative prediction error of the milk yield of each cow is about 18% of daily production, and only 2% of the total milk production.', 'corpus_id': 233740622, 'score': 0}, {'doc_id': '233282236', 'title': 'Effects of heat stress on milk yield of primiparous Holstein cows at regional scale using large data bases', 'abstract': 'With the prospects of global warming, heat stress, the depressive (summer) heat effect on milk yield, has become a high priority research problem in temperate zones. The effect of summer present day heat and lag heat effects on milk yield of first lactation grazing Holstein cows was assessed through the temperature and humidity index (THI). Additionally, THI thresholds were calculated. Daily air temperature and humidity data from three locations for six summer seasons (DecemberMarch in years 2001 – 2006) were used. Data of 35500 monthly test days from 8875 cows in 54 farms within the influence zones of the respective meteorological stations were analyzed. Mixed linear models were adjusted, considering the animal as random effect and location, farm, days in milking, age at calving, year of calving and THI as fixed effects. Four measures per animal were taken into account and modelled as repeated measures. A significant depressing heat effect on milk yield was found for the present day (THI) and also for one-day and two-days before (THI1 and THI2). Significant interactions between THI and days in milk, farm and year were found. The lag heat effects explained more variability on milk yield than the heat effect for the present day. Threshold THI-values were different depending on the considered day: 75, 75 and 72 were estimated for THI, THI1 and THI2, respectively. Heat stress caused a decrease in milk yield of 1.3%, 1.9%, and 0.9% of average daily production (per THI unit increase above threshold), depending on the THImeasure used.', 'corpus_id': 233282236, 'score': 0}, {'doc_id': '232775847', 'title': 'Alterations in TNF-α and its receptors expression in cows undergoing heat stress.', 'abstract': 'Heat stress is one of the environmental factors that most severely affects milk industry, as it has impact on production, immune responses and reproductive performance. The present study was conducted with high-performance Holando-Argentino cows. Our objective was to study TNF-α and its receptors pattern expression in cows from a region characterized by extreme climatic seasonality. Animals were evaluated in three periods: spring (n\u202f=\u202f15), summer (n\u202f=\u202f14) and autumn (n\u202f=\u202f11). Meteorological records from a local station were used to estimate the temperature and humidity index (THI) by means of an equation previously defined. A THI higher than 68 is indicative of stressing conditions. During the summer period, the animals were exposed to 8.5\u202f±\u202f1.09\u202fh of heat stress, or THI\u202f>\u202f68. In spring, stress hours were reduced to 1.4\u202f±\u202f0.5 every day, while during the autumn, there were no recorded heat stress events. Expression of TNF-α, and its receptors was determined by qPCR. During the summer, TNF-α and its receptors expression diminished drastically compared to the rest of the year, when stressful conditions were infrequent. We conclude that animals that are not physiologically prepared to resist high temperatures might have a less efficient immune response, reinforcing the need to develop new strategies to improve animal welfare.', 'corpus_id': 232775847, 'score': 1}, {'doc_id': '233424273', 'title': 'Thermoregulatory and Feeding Behavior under Different Management and Heat Stress Conditions in Heifer Water Buffalo (Bubalus bubalis) in the Tropics', 'abstract': 'Simple Summary Silvopastoral systems can modulate the thermoregulatory behavior of buffaloes decreasing the heat stress and improving the animal welfare in the tropics. The objective of this study was to compare the behavior of heifer buffaloes in a silvopastoral systems with Leucahena leucocephala trees and a conventional system without trees under two heat stress condition (intense heat stress and moderate heat stress) in Cuba. The results show that despite intense heat stress conditions, the animals spent more time feeding in the silvopastoral system than in the conventional system. Besides, the silvopastoral system reduced the use of water in the wallowing areas. We conclude that pastures with trees increase fodder offer while improve grazing behavior and animal welfare for buffalo farming in tropical conditions. Abstract In the wake of climate change and global warming, the production systems of water buffaloes (Bubalus bubalis) are receiving increasing attention in the tropics, where the silvopastoral systems can improve animal welfare and production conditions. The objective of this study was to characterize the behavior of heifer buffaloes in a silvopastoral system (SPS) with Leucaena leucocephala (600 trees/ha) and in a conventional system (CVS), under intense heat stress and moderate heat stress in Cuba. We observed nine animals, with an average weight of 167.9 kg at the beginning of the study, during the daylight period, from 6:00 to 18:00 h, at 10 min intervals, for 12 days. Activities recorded were grazing, ingestion of tree leaves, rumination, water intake, walking, lying, standing, sheltering in the shade of trees, and wallowing. Sheltering in the shade of trees and wallowing were collectively considered as thermoregulatory behavior (TB). TB was different in both systems and conditions of heat stress (p < 0.05), with 4.06 in CVS and 3.81 h in SPS in the intense heat stress period, while it was 2.91 and 1.08 h for SPS and CVS, respectively, during the moderate heat stress period. The wallowing activity showed statistically significant differences (p < 0.05) in the intense heat stress season with 1.18 and 2.35 h for SPS and CVS, respectively. Time spent on feeding behavior was highest in the SPS system (p < 0.05). Longer times of thermoregulatory and feeding behavior indicate the importance of trees in animal welfare for this species in tropical conditions, thus supporting avoided deforestation and the replanting of trees in existing production systems and landscapes.', 'corpus_id': 233424273, 'score': 0}, {'doc_id': '225276626', 'title': 'Thermal imaging combined with predictive machine learning based model for the development of thermal stress level classifiers', 'abstract': 'Abstract Thermal stress in dairy cows has been studied to improve production efficiency and animal welfare. Several authors have verified the potential of infrared thermography (IRT) as noninvasive tool for monitoring the surface temperature of animals. In this study, machine learning-based models for the individual assessment of thermal stress levels in dairy cows (Holstein) were established and evaluated considering both weather and animal factors. An experiment was performed with 26 lactating cows, which were monitored during summer and winter (40 nonconsecutive days in total; three times a day) to acquire the weather and physiological data including the rectal temperature (RT), respiration rate (RR), and body surface temperature (BST), measured by IRT in different body area. The data were analyzed with the Pearson correlation coefficient to determine the ideal body part (front, ocular area, rib, and flank regions) for computational modeling. In addition, a data analysis with linear regression was performed to enhance the parameters choices. The models based on artificial neural networks (ANNs) were developed based on the defined weather and physiological variables. The ANN-based models for predicting the RR (ANN-RR) and RT (ANN-RT) were established with Perceptron, feedforward, and multi-layered architectures. The model responses were used as classifiers for the thermal stress levels (comfort, alert, danger, and emergency). The classification efficiency was assessed with metrics extracted from the confusion matrices (accuracy) and compared with the results of traditional classification methods for thermal stress levels: the Temperature–Humidity Index (THI) and Black Globe-Humidity Index (BGHI). The ANN models provided better predictions of the RR and RT with coefficient of determination (R2) of 0.74 (ANN-RR) and 0.71 (ANN-RT) than the linear regression models. With regard to thermal stress levels, the ANN-based models demonstrated a good predictive ability compared with the BGHI and THI classifications. The best thermal stress accuracy predictions with ANN-RR and ANN-RT were 83% and 84%, respectively; the best accuracies of the BGHI- and THI-based classifiers were 68% and 55%, respectively. In addition, the ANN-based classifier enables an individual assessment of the thermal stress levels of animals.', 'corpus_id': 225276626, 'score': 1}, {'doc_id': '233744052', 'title': 'Delineation of temperature-humidity index (THI) as indicator of heat stress in riverine buffaloes (Bubalus bubalis) of a sub-tropical Indian region.', 'abstract': 'The erstwhile developed temperature-humidity index (THI) has been popularly used to indicate heat stress in dairy cattle and often in buffaloes. However, scientific literature suggests differences in thermotolerance and physiological responses to heat stress between cattle and buffalo. Therefore, THI range used to indicate degree of heat stress (mild, moderate, and severe) in cattle should be recalibrated for indicating heat stress in buffaloes. The present study was carried out to delineate THI range to indicate onset and severity of heat stress in buffaloes based on physiological, biochemical, and expression profiling of heat shock response (HSR) genes in animals at different THI. The result indicated early onset of heat stress in buffaloes as compared to cattle. Physiological and biochemical parameters indicated onset of mild signs of heat stress in buffaloes at THI 68-69. Significant deviation in these parameters was again observed at THI range 73-76. At THI 77-80, the physiological and biochemical responses of animals were further intensified indicating extreme alteration in homeostasis. The in vivo expression profiling of HSR genes indicated that members of Hsp70 gene family are expressed in a temporal pattern over different THIs, whereas expressions of Hsf genes were evident during intense heat stress. Overall, the study established that amplitude of heat shock response and THI range for indicating severity of thermal stress for buffaloes are not in unison to cattle. The study also suggests skin temperature of the poll region could be used as non-invasive tool for monitoring heat stress in dairy buffaloes.', 'corpus_id': 233744052, 'score': 1}, {'doc_id': '233278750', 'title': 'Spatiotemporal variations on infrared temperature as a thermal comfort indicator for cattle under agroforestry systems.', 'abstract': ""With the expanding use of thermal assessment techniques in beef cattle, infrared thermography has become a promising tool for assessing the environment for animal thermal comfort. Goals of this study were: (1) to evaluate cattle thermal comfort in agroforestry systems with different shade availability (2) to verify the spatiotemporal variations of infrared temperature inside agroforestry systems, and; (3) to test infrared thermography as a potential tool to assess animal thermal comfort indices in agroforestry systems. A trial was carried out between June 2015 and February 2016, covering Central-Brazil's dry winter and rainy summer seasons, respectively. The experimental area of Embrapa Beef Cattle is located in Campo Grande (Mato Grosso do Sul), coordinates 20°24'53″ S, 54°42'26″ W and 558\xa0m altitude. The 12\xa0ha plot has two agroforestry systems varying shade availability. Traditional Black Globe Temperature and Humidity Index, Heat Load Index and Radiation Thermal Load were determined, from measurements using digital thermo-hygrometers, with datalogger. Surface temperature and humidity of tree canopies and pasture were determined using an infrared thermographic camera. Results show spatiotemporal variations in infrared temperature. This means that the environment inside agroforestry systems is not homogeneously comfortable for cattle, and the system with the lowest shade availability has the greatest heat accumulation area. Weak to strong associations were identified between infrared variables and thermal comfort indices (0.08\xa0=\xa0r\xa0≤\xa00.75). Positive relationships were also obtained and equally well explained by the Black Globe Temperature and Humidity Index and Heat Load Index (0.55\xa0=\xa0R2\xa0≤\xa00.94). We conclude that infrared thermography can be used as a tool to assess thermal comfort indices in agroforestry systems and to determine onset of animal thermal stress from environment and heat body accumulation."", 'corpus_id': 233278750, 'score': 1}, {'doc_id': '233668437', 'title': 'Heat stress effects on somatic cell score of Holstein cattle in tropical environment', 'abstract': 'ABSTRACT Considering the importance of dairy farming and negative effects of heat stress, the objective of this study was to investigate the effect of heat stress via temperature-humidity index (THI) and diurnal temperature variation (DTV) for somatic cell score (SCS) of Holstein dairy cattle, using random regression models. Data were a total of 52,012 test-day records for SCS of 9,765 first parity Holstein cows from Brazil, collected from 1997 to 2013, along with weather records (THI and DTV) from 18 weather stations. Least square linear regression models were used to determine THI and DTV thresholds for SCS increase caused by heat stress. In addition to the standard model (SM; without bioclimatic variables), THI and DTV were combined in various ways and tested for different days, totaling 21 models. Thresholds of THI and DTV for SCS increase was 70 (0.09 score unit/THI) and 9 (0.03 score unit/DTV), respectively. The model that included THI and DTV as fixed effects, considering the two days average, presented better fit (AIC, BIC and -2logL). Estimated breeding values (EBVs) and reliability of EBVs improved when using this model. Changes on SCS may be an early indicator of heat stress in Holstein cattle reared in tropical conditions. The sires are re-ranked when bioclimatic variables are included in the model. More significant reclassifications of sires were observed when we increased selection pressure. This study provides strong evidence of a genotype by environment interaction on SCS. Genetic evaluation using average of two days of THI and DTV as fixed effects, improves EBVs and reliability of EBVs.', 'corpus_id': 233668437, 'score': 0}]"
45	{'doc_id': '207988304', 'title': 'Dual targeting of IGF-1R and ErbB3 as a potential therapeutic regimen for ovarian cancer', 'abstract': 'Therapeutically targeting receptor tyrosine kinases has proven to be paramount to overcoming chemotherapy resistance in several cancer indications, improving patient outcomes. Insulin-Like Growth Factor Receptor 1 (IGF-1R) and Epidermal Growth Factor Receptor 3 (ErbB3) have been implicated as two such drivers of resistance, however their simultaneous role in ovarian cancer chemotherapy resistance remains poorly elucidated. The aim of this work is to determine the effects of dual IGF-1R/ErbB3 inhibition on ovarian cancer cell signaling, growth, and in vivo efficacy. Assessment of in vitro chemotherapy response across a panel of ovarian cancer cell lines revealed that increased IGF-1R cell surface expression correlates with decreased sensitivity to chemotherapy, and that growth induced by IGF-1R and ErbB3 ligands is blocked by the tetravalent bispecific antibody targeting IGF-1R and ErbB3, istiratumab. In vitro chemotherapy treatment increased ovarian cancer cell line capacity to activate prosurvival PI3K signaling in response to ligand, which could be prevented with istiratumab treatment. Furthermore, in vivo efficacy of standard of care chemotherapies using a xenograft model of ovarian cancer was potentiated with istiratumab. Our results suggest a role for IGF-1R and ErbB3 in driving chemotherapy resistance of ovarian cancer.', 'corpus_id': 207988304}	4467	"[{'doc_id': '216532933', 'title': 'The role of growth factor receptors in viral infections: An opportunity for drug repurposing against emerging viral diseases such as COVID‐19?', 'abstract': 'Growth factor receptors are known to be involved in the process of viral infection. Many viruses not only use growth factor receptors to physically attach to the cell surface and internalize, but also divert receptor tyrosine kinase signaling in order to replicate. Thus, repurposing drugs that have initially been developed to target growth factor receptors and their signaling in cancer may prove to be a fast track to effective therapies against emerging new viral infections, including the coronavirus disease 19 (COVID‐19).', 'corpus_id': 216532933, 'score': 0}, {'doc_id': '208017856', 'title': 'Kinase activity of ERBB3 contributes to intestinal organoids growth and intestinal tumorigenesis', 'abstract': 'As a member of the epidermal growth factor receptor (EGFR) family, ERBB3 plays an essential role in development and disease independent of inherently inactive kinase domain. Recently, ERBB3 has been found to bind to ATP and has catalytic activity in vitro. However, the biological function of ERBB3 kinase activity remains elusive in vivo. Here we have identified the physiological function of inactivated ERBB3 kinase activity by creating Erbb3‐K740M knockin mice in which ATP cannot bind to ERBB3. Unlike Erbb3 knockout mice, kinase‐inactive Erbb3K740M homozygous mice were born in Mendelian ratios and showed normal development. After dextran sulfate sodium‐induced colitis, the kinase‐inactive Erbb3 mutant mice showed normal recovery. However, the outgrowth of ileal organoids by neuregulin‐1 treatment was more attenuated in Erbb3 mutant mice than in WT mice. Moreover, in combination with the ApcMin mouse, the proportion of polyps less than 1 mm in diameter in mutant mice was higher than in control mice and an increase in the number of apoptotic cells was observed in polyps from mutant mice compared with polyps from control mice. Taken together, the ERBB3 kinase activity contributes to the outgrowth of ileal organoids and intestinal tumorigenesis, and the development of ERBB3 kinase inhibitors, including epidermal growth factor receptor family members, can be a potential way to target colorectal cancer.', 'corpus_id': 208017856, 'score': 1}, {'doc_id': '202570351', 'title': 'ERBB3 mutations in cancer: biological aspects, prevalence and therapeutics', 'abstract': 'HER3, a member of the EGFR family of receptor tyrosine kinases coded by the ERBB3 gene, plays an important role in cancer, despite its lack of intrinsic kinase activity. As with genes coding for potential heterodimeric partners of HER3, EGFR, and HER2, oncogenic mutations of ERBB3 have been explored by several studies. In this review, we discuss the evidence presenting ERBB3 somatic mutations as potential tumoral drivers. We then show that ERBB3 mutations are not uncommon in many cancer types. Finally, we present the recent results of several studies evaluating different therapeutic approaches for treating patients with oncogenic ERBB3 mutations.', 'corpus_id': 202570351, 'score': 1}, {'doc_id': '216557143', 'title': 'Neoadjuvant endocrine therapy for luminal breast cancer treatment: a first-choice alternative in times of crisis such as the COVID-19 pandemic', 'abstract': 'The epidemiological emergency caused by CoV-2 (COVID-19) has changed priorities in breast cancer management. In those places where the pandemic has had the greatest effect, it is of paramount importance for most patients to be at home, reducing or postponing their attendance at clinics, as well as avoiding surgeries. In this scenario, neoadjuvant endocrine treatment could be an appropriate alternative treatment for hormone receptor positive breast cancer (luminal-like tumours) in order to minimise hospital admissions and to delay elective surgeries. Accordingly, we present a simple protocol that can be applied to most cases of luminal-like breast cancer and is appropriate for the majority of secondary or tertiary medical centres, or even primary care.', 'corpus_id': 216557143, 'score': 0}, {'doc_id': '212725583', 'title': 'A Novel Bis-Coumarin Targets Multiple Tyrosine Kinases of Key Signaling Pathways in Melanoma and Inhibits Melanoma Cell Survival, Proliferation, and Migration', 'abstract': 'Melanoma is one of the most dangerous skin malignancies due to its high metastatic tendency and high mortality. Activation of key signaling pathways enforcing melanoma progression depends on phosphorylation of tyrosine kinases, and oxidative stress. We here investigated the effect of the new bis-coumarin derivative (3,5-DCPBC) on human melanoma cell survival, growth, proliferation, migration, and intracellular redox state, and deciphered associated signal pathways. This novel derivative was found to be toxic for melanoma cells, and non-toxic for their benign counterparts, melanocytes and fibroblasts. 3,5-DCPBC inhibited cell survival, migration and proliferation of different metastatic, and non-metastatic melanoma cell lines through the profound suppression of phosphorylation of the Epidermal Growth Factor receptor, and related downstream pathways. Suppression of phosphorylation of key downstream transcription factors and different tyrosine kinases comprise JAK/STAT, SRC kinases, ERK and MAP kinases (p38alpha), all involved in melanoma progression. Simultaneous and specific targeting of multiple tyrosine kinases and corresponding key genes in melanoma cells makes 3,5-DCPBC a highly interesting anti-melanoma, and anti-metastatic drug candidate which may in the long term hold promise in the therapy of advanced melanoma.', 'corpus_id': 212725583, 'score': 0}, {'doc_id': '33235643', 'title': 'The ErbB2/ErbB3 heterodimer functions as an oncogenic unit: ErbB2 requires ErbB3 to drive breast tumor cell proliferation', 'abstract': ""ErbB2 is a receptor tyrosine kinase whose activity in normal cells depends on dimerization with another ligand-binding ErbB receptor. In contrast, amplification of c-erbB2 in tumors results in dramatic overexpression and constitutive activation of the receptor. Breast cancer cells overexpressing ErbB2 depend on its activity for proliferation, because treatment of these cells with ErbB2-specific antagonistic antibodies or kinase inhibitors blocks tumor cells in the G1 phase of the cell cycle. Intriguingly, loss of ErbB2 signaling is accompanied by a decrease in the phosphotyrosine content of ErbB3. On the basis of these results, it has been proposed that ErbB3 might be a partner for ErbB2 in promoting cellular transformation. To test this hypothesis and directly examine the role of the “kinase dead” ErbB3, we specifically ablated its expression with a designer transcription factor (E3). By infection of ErbB2-overexpressing breast cancer cells with a retrovirus expressing E3, we show that ErbB3 is an essential partner in the transformation process. Loss of functional ErbB2 or ErbB3 has similar effects on cell proliferation and cell cycle regulators. Furthermore, expression of constitutively active protein kinase B rescues the proliferative block induced as a consequence of loss of ErbB2 or ErbB3 signaling. These results demonstrate that ErbB2 overexpression and activity alone are insufficient to promote breast tumor cell division. Furthermore, we identify ErbB3's role, which is to couple active ErbB2 to the phosphatidylinositol 3-kinase/protein kinase B pathway. Thus, the ErbB2/ErbB3 dimer functions as an oncogenic unit to drive breast tumor cell proliferation."", 'corpus_id': 33235643, 'score': 1}, {'doc_id': '23254145', 'title': 'MET Amplification Leads to Gefitinib Resistance in Lung Cancer by Activating ERBB3 Signaling', 'abstract': 'The epidermal growth factor receptor (EGFR) kinase inhibitors gefitinib and erlotinib are effective treatments for lung cancers with EGFR activating mutations, but these tumors invariably develop drug resistance. Here, we describe a gefitinib-sensitive lung cancer cell line that developed resistance to gefitinib as a result of focal amplification of the MET proto-oncogene. inhibition of MET signaling in these cells restored their sensitivity to gefitinib. MET amplification was detected in 4 of 18 (22%) lung cancer specimens that had developed resistance to gefitinib or erlotinib. We find that amplification of MET causes gefitinib resistance by driving ERBB3 (HER3)–dependent activation of PI3K, a pathway thought to be specific to EGFR/ERBB family receptors. Thus, we propose that MET amplification may promote drug resistance in other ERBB-driven cancers as well.', 'corpus_id': 23254145, 'score': 1}, {'doc_id': '85529451', 'title': 'Cross-Talk between Receptor Tyrosine Kinases AXL and ERBB3 Regulates Invadopodia Formation in Melanoma Cells.', 'abstract': 'The invasive phenotype of metastatic cancer cells is accompanied by the formation of actin-rich invadopodia, which adhere to the extracellular matrix and degrade it. In this study, we explored the role of the tyrosine kinome in the formation of invadopodia in metastatic melanoma cells. Using a microscopy-based siRNA screen, we identified a series of regulators, the knockdown of which either suppresses (e.g., TYK2, IGFR1, ERBB3, TYRO3, FES, ALK, PTK7) or enhances (e.g., ABL2, AXL, CSK) invadopodia formation and function. Notably, the receptor tyrosine kinase AXL displayed a dual regulatory function, where both depletion or overexpression enhanced invadopodia formation and activity. This apparent contradiction was attributed to the capacity of AXL to directly stimulate invadopodia, yet its suppression upregulates the ERBB3 signaling pathway, which can also activate core invadopodia regulators and enhance invadopodia function. Bioinformatic analysis of multiple melanoma cell lines points to an inverse expression pattern of AXL and ERBB3. High expression of AXL in melanoma cells is associated with high expression of invadopodia components and an invasive phenotype. These results provide new insights into the complexity of metastasis-promoting mechanisms and suggest that targeting of multiple invadopodia signaling networks may serve as a potential anti-invasion therapy in melanoma. SIGNIFICANCE: These findings uncover a unique interplay between AXL and ERBB3 in invadopodia regulation that points to the need for combined therapy in order to prevent invadopodia-mediated metastasis in melanoma.', 'corpus_id': 85529451, 'score': 1}, {'doc_id': '216071085', 'title': 'The protective and pathogenic roles of CXCL17 in human health and disease: Potential in respiratory medicine', 'abstract': '\n Abstract\n \n C-X-C motif chemokine 17 (CXCL-17) is a novel chemokine that plays a functional role maintaining homeostasis at distinct mucosal barriers, including regulation of myeloid-cell recruitment, angiogenesis, and control of microorganisms. Particularly, CXCL17 is produced along the epithelium of the airways both at steady state and under inflammatory conditions. While increased CXCL17 expression is associated with disease progression in pulmonary fibrosis, asthma, and lung/hepatic cancer, it is thought to play a protective role in pancreatic cancer, autoimmune encephalomyelitis and viral infections. Thus, there is emerging evidence pointing to both a harmful and protective role for CXCL17 in human health and disease, with therapeutic potential for translational applications. In this review, we provide an overview of the discovery, characteristics and functions of CXCL17 emphasizing its clinical potential in respiratory disorders.\n \n', 'corpus_id': 216071085, 'score': 0}, {'doc_id': '214802243', 'title': 'Tau affects P53 function and cell fate during the DNA damage response', 'abstract': 'Cells are constantly exposed to DNA damaging insults. To protect the organism, cells developed a complex molecular response coordinated by P53, the master regulator of DNA repair, cell division and cell fate. DNA damage accumulation and abnormal cell fate decision may represent a pathomechanism shared by aging-associated disorders such as cancer and neurodegeneration. Here, we examined this hypothesis in the context of tauopathies, a neurodegenerative disorder group characterized by Tau protein deposition. For this, the response to an acute DNA damage was studied in neuroblastoma cells with depleted Tau, as a model of loss-of-function. Under these conditions, altered P53 stability and activity result in reduced cell death and increased cell senescence. This newly discovered function of Tau involves abnormal modification of P53 and its E3 ubiquitin ligase MDM2. Considering the medical need with vast social implications caused by neurodegeneration and cancer, our study may reform our approach to disease-modifying therapies. Martina Sola, Claudia Magrin et al. study the relation between Tau and P53 in response to DNA damage. They uncover an important role for Tau in regulating the stability, and activity of P53 post translationally. Their findings provide insights to potentially common pathways in neurodegenerative disease and cancer.', 'corpus_id': 214802243, 'score': 0}]"
46	"{'doc_id': '831608', 'title': 'Using big data to study the link between human mobility and socio-economic development', 'abstract': 'Big Data offer nowadays the potential capability of creating a digital nervous system of our society, enabling the measurement, monitoring and prediction of relevant aspects of socio-economic phenomena in quasi real time. This potential has fueled, in the last few years, a growing interest around the usage of Big Data to support official statistics in the measurement of individual and collective economic well-being. In this work we study the relations between human mobility patterns and socioeconomic development. Starting from nation-wide mobile phone data we extract a measure of mobility volume and a measure of mobility diversity for each individual. We then aggregate the mobility measures at municipality level and investigate the correlations with external socio-economic indicators independently surveyed by an official statistics institute. We find three main results. First, aggregated human mobility patterns are correlated with these socio-economic indicators. Second, the diversity of mobility, defined in terms of entropy of the individual users\' trajectories, exhibits the strongest correlation with the external socio-economic indicators. Third, the volume of mobility and the diversity of mobility show opposite correlations with the socioeconomic indicators. Our results, validated against a null model, open an interesting perspective to study human behavior through Big Data by means of new statistical indicators that quantify and possibly ""nowcast"" the socio-economic development of our society.', 'corpus_id': 831608}"	4237	[{'doc_id': '216080960', 'title': 'Quantifying the Economic Impact of Extreme Shocks on Businesses using Human Mobility Data: a Bayesian Causal Inference Approach', 'abstract': 'In recent years, extreme shocks, such as natural disasters, are increasing in both frequency and intensity, causing significant economic loss to many cities around the world. Quantifying the economic cost of local businesses after extreme shocks is important for post-disaster assessment and pre-disaster planning. Conventionally, surveys have been the primary source of data used to quantify damages inflicted on businesses by disasters. However, surveys often suffer from high cost and long time for implementation, spatio-temporal sparsity in observations, and limitations in scalability. Recently, large scale human mobility data (e.g. mobile phone GPS) have been used to observe and analyze human mobility patterns in an unprecedented spatio-temporal granularity and scale. In this work, we use location data collected from mobile phones to estimate and analyze the causal impact of hurricanes on business performance. To quantify the causal impact of the disaster, we use a Bayesian structural time series model to predict the counterfactual performances of affected businesses (what if the disaster did not occur?), which may use performances of other businesses outside the disaster areas as covariates. The method is tested to quantify the resilience of 635 businesses across 9 categories in Puerto Rico after Hurricane Maria. Furthermore, hierarchical Bayesian models are used to reveal the effect of business characteristics such as location and category on the long-term resilience of businesses. The study presents a novel and more efficient method to quantify business resilience, which could assist policy makers in disaster preparation and relief processes.', 'corpus_id': 216080960, 'score': 0}, {'doc_id': '215745299', 'title': 'Socio-economic, built environment, and mobility conditions associated with crime: a study of multiple cities', 'abstract': 'Nowadays, 23% of the world population lives in multi-million cities. In these metropolises, criminal activity is much higher and violent than in either small cities or rural areas. Thus, understanding what factors influence urban crime in big cities is a pressing need. Seminal studies analyse crime records through historical panel data or analysis of historical patterns combined with ecological factor and exploratory mapping. More recently, machine learning methods have provided informed crime prediction over time. However, previous studies have focused on a single city at a time, considering only a limited number of factors (such as socio-economical characteristics) and often at large in a single city. Hence, our understanding of the factors influencing crime across cultures and cities is very limited. Here we propose a Bayesian model to explore how violent and property crimes are related not only to socio-economic factors but also to the built environmental (e.g. land use) and mobility characteristics of neighbourhoods. To that end, we analyse crime at small areas and integrate multiple open data sources with mobile phone traces to compare how the different factors correlate with crime in diverse cities, namely Boston, Bogotá, Los Angeles and Chicago. We find that the combined use of socio-economic conditions, mobility information and physical characteristics of the neighbourhood effectively explain the emergence of crime, and improve the performance of the traditional approaches. However, we show that the socio-ecological factors of neighbourhoods relate to crime very differently from one city to another. Thus there is clearly no “one fits all” model.', 'corpus_id': 215745299, 'score': 0}, {'doc_id': '211506320', 'title': 'Measuring Spatial Subdivisions in Urban Mobility with Mobile Phone Data', 'abstract': 'Urban population grows constantly. By 2050 two thirds of the world population will reside in urban areas. This growth is faster and more complex than the ability of cities to measure and plan for their sustainability. To understand what makes a city inclusive for all, we define a methodology to identify and characterize spatial subdivisions: areas with over- and under-representation of specific population groups, named hot and cold spots respectively. Using aggregated mobile phone data, we apply this methodology to the city of Barcelona to assess the mobility of three groups of people: women, elders, and tourists. We find that, within the three groups, cold spots have a lower diversity of amenities and services than hot spots. Also, cold spots of women and tourists tend to have lower population income. These insights apply to the floating population of Barcelona, thus augmenting the scope of how inclusiveness can be analyzed in the city.', 'corpus_id': 211506320, 'score': 1}, {'doc_id': '218501695', 'title': 'AN INTERACTIVE COVID-19 MOBILITY IMPACT AND SOCIAL DISTANCING ANALYSIS PLATFORM', 'abstract': 'The research team has utilized privacy-protected mobile device location data, integrated with COVID-19 case data and census population data, to produce a COVID-19 impact analysis platform that can inform users about the effects of COVID-19 spread and government orders on mobility and social distancing. The platform is being updated daily, to continuously inform decision-makers about the impacts of COVID-19 on their communities using an interactive analytical tool. The research team has processed anonymized mobile device location data to identify trips and produced a set of variables including social distancing index, percentage of people staying at home, visits to work and non-work locations, out-of-town trips, and trip distance. The results are aggregated to county and state levels to protect privacy and scaled to the entire population of each county and state. The research team are making their data and findings, which are updated daily and go back to January 1, 2020, for benchmarking, available to the public in order to help public officials make informed decisions. This paper presents a summary of the platform and describes the methodology used to process data and produce the platform metrics.', 'corpus_id': 218501695, 'score': 1}, {'doc_id': '214612148', 'title': 'Entropy as a Measure of Attractiveness and Socioeconomic Complexity in Rio de Janeiro Metropolitan Area', 'abstract': 'Defining and measuring spatial inequalities across the urban environment remains a complex and elusive task which has been facilitated by the increasing availability of large geolocated databases. In this study, we rely on a mobile phone dataset and an entropy-based metric to measure the attractiveness of a location in the Rio de Janeiro Metropolitan Area (Brazil) as the diversity of visitors’ location of residence. The results show that the attractiveness of a given location measured by entropy is an important descriptor of the socioeconomic status of the location, and can thus be used as a proxy for complex socioeconomic indicators.', 'corpus_id': 214612148, 'score': 1}, {'doc_id': '216080438', 'title': 'Mobile phone data analytics against the COVID-19 epidemics in Italy: flow diversity and local job markets during the national lockdown', 'abstract': 'Understanding collective mobility patterns is crucial to plan the restart of production and economic activities, which are currently put in stand-by to fight the diffusion of the epidemics. In this report, we use mobile phone data to infer the movements of people between Italian provinces and municipalities, and we analyze the incoming, outcoming and internal mobility flows before and during the national lockdown (March 9th, 2020) and after the closure of non-necessary productive and economic activities (March 23th, 2020). The population flow across provinces and municipalities enable for the modelling of a risk index tailored for the mobility of each municipality or province. Such an index would be a useful indicator to drive counter-measures in reaction to a sudden reactivation of the epidemics. Mobile phone data, even when aggregated to preserve the privacy of individuals, are a useful data source to track the evolution in time of human mobility, hence allowing for monitoring the effectiveness of control measures such as physical distancing. We address the following analytical questions: How does the mobility structure of a territory change? Do incoming and outcoming flows become more predictable during the lockdown, and what are the differences between weekdays and weekends? Can we detect proper local job markets based on human mobility flows, to eventually shape the borders of a local outbreak?', 'corpus_id': 216080438, 'score': 0}, {'doc_id': '214794894', 'title': 'On the importance of trip destination for modelling individual human mobility patterns', 'abstract': 'Obtaining insights into human mobility patterns and being able to reproduce them accurately is of the utmost importance in a wide range of applications from public health, to transport and urban planning. Still the relationship between the effort individuals will invest in a trip and the importance of its purpose is not taken into account in individual mobility models that can be found in the recent literature. Here, we address this issue by introducing a model hypothesizing a relation between the importance of a trip and the distance travelled. In most practical cases, quantifying such importance is undoable. We overcome this difficulty by focusing on shopping trips (for which we have empirical data) and by taking the price of items as a proxy. Our model is able to reproduce the long-tailed distribution in travel distances empirically observed and to explain the scaling relationship between distance travelled and item value found in the data.', 'corpus_id': 214794894, 'score': 0}, {'doc_id': '215548659', 'title': 'Mapping county-level mobility pattern changes in the United States in response to COVID-19', 'abstract': 'To contain the Coronavirus disease (COVID-19) pandemic, one of the non-pharmacological epidemic control measures in response to the COVID-19 outbreak is reducing the transmission rate of SARS-COV-2 in the population through (physical) social distancing. An interactive web-based mapping platform that provides timely quantitative information on how people in different counties and states reacted to the social distancing guidelines was developed with the support of the National Science Foundation (NSF). It integrates geographic information systems (GIS) and daily updated human mobility statistical patterns derived from large-scale anonymized and aggregated smartphone location big data at the county-level in the United States, and aims to increase risk awareness of the public, support governmental decision-making, and help enhance community responses to the COVID-19 outbreak.', 'corpus_id': 215548659, 'score': 1}, {'doc_id': '212726019', 'title': 'Flow descriptors of human mobility networks', 'abstract': 'Mobile phone data has enabled the timely and fine-grained study human mobility. Call Detail Records, generated at call events, allow building descriptions of mobility at different resolutions and with different spatial, temporal and social granularity. Individual trajectories are the basis for long-term observation of mobility patterns and identify factors of human dynamics. Here we propose a systematic analysis to characterize mobility network flows and topology and assess their impact into individual traces. Discrete flow-based descriptors are used to classify and understand human mobility patterns at multiple scales. This framework is suitable to assess urban planning, optimize transportation, measure the impact of external events and conditions, monitor internal dynamics and profile users according to their movement patterns.', 'corpus_id': 212726019, 'score': 0}, {'doc_id': '214693410', 'title': 'Mobile phone data and COVID-19: Missing an opportunity?', 'abstract': 'This paper describes how mobile phone data can guide government and public health authorities in determining the best course of action to control the COVID-19 pandemic and in assessing the effectiveness of control measures such as physical distancing. It identifies key gaps and reasons why this kind of data is only scarcely used, although their value in similar epidemics has proven in a number of use cases. It presents ways to overcome these gaps and key recommendations for urgent action, most notably the establishment of mixed expert groups on national and regional level, and the inclusion and support of governments and public authorities early on. It is authored by a group of experienced data scientists, epidemiologists, demographers and representatives of mobile network operators who jointly put their work at the service of the global effort to combat the COVID-19 pandemic.', 'corpus_id': 214693410, 'score': 1}]
47	{'doc_id': '1809816', 'title': 'Good Question! Statistical Ranking for Question Generation', 'abstract': 'We address the challenge of automatically generating questions from reading materials for educational practice and assessment. Our approach is to overgenerate questions, then rank them. We use manually written rules to perform a sequence of general purpose syntactic transformations (e.g., subject-auxiliary inversion) to turn declarative sentences into questions. These questions are then ranked by a logistic regression model trained on a small, tailored dataset consisting of labeled output from our system. Experimental results show that ranking nearly doubles the percentage of questions rated as acceptable by annotators, from 27% of all questions to 52% of the top ranked 20% of questions.', 'corpus_id': 1809816}	706	"[{'doc_id': '209521357', 'title': 'Neural Question Generation', 'abstract': 'Question generation attempts to create natural questions from a body of text. An important application of this technology is related to education: we can use question generation to provide readily-available reading comprehension material given any corpus of text. Successful question generation can also used to generate non-trivial annotated datasets for question answering given a piece of text. Furthermore, question generation can be used as a submodule in chatbot systems to prompt certain answers from users based on the responses that the user has given so far. This feature could be integrated in a wide-ranging variety of disciplines, such as in commercial chatbots or in healthcare-based chatbots.', 'corpus_id': 209521357, 'score': 1}, {'doc_id': '210157114', 'title': 'Open Domain Question Answering Using Web Tables', 'abstract': 'Tables extracted from web documents can be used to directly answer many web search queries. Previous works on question answering (QA) using web tables have focused on factoid queries, i.e., those answerable with a short string like person name or a number. However, many queries answerable using tables are non-factoid in nature. In this paper, we develop an open-domain QA approach using web tables that works for both factoid and non-factoid queries. Our key insight is to combine deep neural network-based semantic similarity between the query and the table with features that quantify the dominance of the table in the document as well as the quality of the information in the table. Our experiments on real-life web search queries show that our approach significantly outperforms state-of-the-art baseline approaches. Our solution is used in production in a major commercial web search engine and serves direct answers for tens of millions of real user queries per month.', 'corpus_id': 210157114, 'score': 0}, {'doc_id': '211010963', 'title': 'Asking Questions the Human Way: Scalable Question-Answer Generation from Text Corpus', 'abstract': 'The ability to ask questions is important in both human and machine intelligence. Learning to ask questions helps knowledge acquisition, improves question-answering and machine reading comprehension tasks, and helps a chatbot to keep the conversation flowing with a human. Existing question generation models are ineffective at generating a large amount of high-quality question-answer pairs from unstructured text, since given an answer and an input passage, question generation is inherently a one-to-many mapping. In this paper, we propose Answer-Clue-Style-aware Question Generation (ACS-QG), which aims at automatically generating high-quality and diverse question-answer pairs from unlabeled text corpus at scale by imitating the way a human asks questions. Our system consists of: i) an information extractor, which samples from the text multiple types of assistive information to guide question generation; ii) neural question generators, which generate diverse and controllable questions, leveraging the extracted assistive information; and iii) a neural quality controller, which removes low-quality generated data based on text entailment. We compare our question generation models with existing approaches and resort to voluntary human evaluation to assess the quality of the generated question-answer pairs. The evaluation results suggest that our system dramatically outperforms state-of-the-art neural question generation models in terms of the generation quality, while being scalable in the meantime. With models trained on a relatively smaller amount of data, we can generate 2.8 million quality-assured question-answer pairs from a million sentences found in Wikipedia.', 'corpus_id': 211010963, 'score': 1}, {'doc_id': '52176706', 'title': 'Improving Neural Question Generation using Answer Separation', 'abstract': 'Neural question generation (NQG) is the task of generating a question from a given passage with deep neural networks. Previous NQG models suffer from a problem that a significant proportion of the generated questions include words in the question target, resulting in the generation of unintended questions. In this paper, we propose answer-separated seq2seq, which better utilizes the information from both the passage and the target answer. By replacing the target answer in the original passage with a special token, our model learns to identify which interrogative word should be used. We also propose a new module termed keyword-net, which helps the model better capture the key information in the target answer and generate an appropriate question. Experimental results demonstrate that our answer separation method significantly reduces the number of improper questions which include answers. Consequently, our model significantly outperforms previous state-of-the-art NQG models.', 'corpus_id': 52176706, 'score': 1}, {'doc_id': '211258652', 'title': 'Training Question Answering Models from Synthetic Data', 'abstract': 'Question and answer generation is a data augmentation method that aims to improve question answering (QA) models given the limited amount of human labeled data. However, a considerable gap remains between synthetic and human-generated question-answer pairs. This work aims to narrow this gap by taking advantage of large language models and explores several factors such as model size, quality of pretrained models, scale of data synthesized, and algorithmic choices. On the SQuAD1.1 question answering task, we achieve higher accuracy using solely synthetic questions and answers than when using the SQuAD1.1 training set questions alone. Removing access to real Wikipedia data, we synthesize questions and answers from a synthetic corpus generated by an 8.3 billion parameter GPT-2 model. With no access to human supervision and only access to other models, we are able to train state of the art question answering networks on entirely model-generated data that achieve 88.4 Exact Match (EM) and 93.9 F1 score on the SQuAD1.1 dev set. We further apply our methodology to SQuAD2.0 and show a 2.8 absolute gain on EM score compared to prior work using synthetic data.', 'corpus_id': 211258652, 'score': 1}, {'doc_id': '214611567', 'title': 'Fast Cross-domain Data Augmentation through Neural Sentence Editing', 'abstract': 'Data augmentation promises to alleviate data scarcity. This is most important in cases where the initial data is in short supply. This is, for existing methods, also where augmenting is the most difficult, as learning the full data distribution is impossible. For natural language, sentence editing offers a solution - relying on small but meaningful changes to the original ones. Learning which changes are meaningful also requires large amounts of training data. We thus aim to learn this in a source domain where data is abundant and apply it in a different, target domain, where data is scarce - cross-domain augmentation. \nWe create the Edit-transformer, a Transformer-based sentence editor that is significantly faster than the state of the art and also works cross-domain. We argue that, due to its structure, the Edit-transformer is better suited for cross-domain environments than its edit-based predecessors. We show this performance gap on the Yelp-Wikipedia domain pairs. Finally, we show that due to this cross-domain performance advantage, the Edit-transformer leads to meaningful performance gains in several downstream tasks.', 'corpus_id': 214611567, 'score': 0}, {'doc_id': '214611659', 'title': 'E2EET: From Pipeline to End-to-end Entity Typing via Transformer-Based Embeddings', 'abstract': 'Entity Typing (ET) is the process of identifying the semantic types of every entity within a corpus. In contrast to Named Entity Recognition, where each token in a sentence is labelled with zero or one class label, ET involves labelling each entity mention with one or more class labels. Existing entity typing models, which operate at the mention level, are limited by two key factors: they do not make use of recently-proposed context-dependent embeddings, and are trained on fixed context windows. They are therefore sensitive to window size selection and are unable to incorporate the context of the entire document. In light of these drawbacks we propose to incorporate context using transformer-based embeddings for a mention-level model, and an end-to-end model using a Bi-GRU to remove the dependency on window size. An extensive ablative study demonstrates the effectiveness of contextualised embeddings for mention-level models and the competitiveness of our end-to-end model for entity typing.', 'corpus_id': 214611659, 'score': 0}, {'doc_id': '214611797', 'title': 'Pairwise Multi-Class Document Classification for Semantic Relations between Wikipedia Articles', 'abstract': 'Many digital libraries recommend literature to their users considering the similarity between a query document and their repository. However, they often fail to distinguish what is the relationship that makes two documents alike. In this paper, we model the problem of finding the relationship between two documents as a pairwise document classification task. To find the semantic relation between documents, we apply a series of techniques, such as GloVe, Paragraph Vectors, BERT, and XLNet under different configurations (e.g., sequence length, vector concatenation scheme), including a Siamese architecture for the Transformer-based systems. We perform our experiments on a newly proposed dataset of 32,168 Wikipedia article pairs and Wikidata properties that define the semantic document relations. Our results show vanilla BERT as the best performing system with an F1-score of 0.93, which we manually examine to better understand its applicability to other domains. Our findings suggest that classifying semantic relations between documents is a solvable task and motivates the development of a recommender system based on the evaluated techniques. The discussions in this paper serve as first steps in the exploration of documents through SPARQL-like queries such that one could find documents that are similar in one aspect but dissimilar in another.', 'corpus_id': 214611797, 'score': 0}, {'doc_id': '214612170', 'title': 'Linguistically Driven Graph Capsule Network for Visual Question Reasoning', 'abstract': 'Recently, studies of visual question answering have explored various architectures of end-to-end networks and achieved promising results on both natural and synthetic datasets, which require explicitly compositional reasoning. However, it has been argued that these black-box approaches lack interpretability of results, and thus cannot perform well on generalization tasks due to overfitting the dataset bias. In this work, we aim to combine the benefits of both sides and overcome their limitations to achieve an end-to-end interpretable structural reasoning for general images without the requirement of layout annotations. Inspired by the property of a capsule network that can carve a tree structure inside a regular convolutional neural network (CNN), we propose a hierarchical compositional reasoning model called the ""Linguistically driven Graph Capsule Network"", where the compositional process is guided by the linguistic parse tree. Specifically, we bind each capsule in the lowest layer to bridge the linguistic embedding of a single word in the original question with visual evidence and then route them to the same capsule if they are siblings in the parse tree. This compositional process is achieved by performing inference on a linguistically driven conditional random field (CRF) and is performed across multiple graph capsule layers, which results in a compositional reasoning process inside a CNN. Experiments on the CLEVR dataset, CLEVR compositional generation test, and FigureQA dataset demonstrate the effectiveness and composition generalization ability of our end-to-end model.', 'corpus_id': 214612170, 'score': 0}, {'doc_id': '21706647', 'title': 'Translating Web Search Queries into Natural Language Questions', 'abstract': 'Users often query a search engine with a specific question in mind and often these queries are keywords or sub-sentential fragments. For example, if the users want to know the answer for ""What\'s the capital of USA"", they will most probably query ""capital of USA"" or ""USA capital"" or some keyword-based variation of this. For example, for the user entered query ""capital of USA"", the most probable question intent is ""What\'s the capital of USA?"". In this paper, we are proposing a method to generate well-formed natural language question from a given keyword-based query, which has the same question intent as the query. Conversion of keyword-based web query into a well-formed question has lots of applications, with some of them being in search engines, Community Question Answering (CQA) website and bots communication. We found a synergy between query-to-question problem with standard machine translation(MT) task. We have used both Statistical MT (SMT) and Neural MT (NMT) models to generate the questions from the query. We have observed that MT models perform well in terms of both automatic and human evaluation.', 'corpus_id': 21706647, 'score': 1}]"
48	{'doc_id': '10657033', 'title': 'Bacteriocins active against plant pathogenic bacteria.', 'abstract': 'Gram-negative phytopathogens cause significant losses in a diverse range of economically important crop plants. The effectiveness of traditional countermeasures, such as the breeding and introduction of resistant cultivars, is often limited by the dearth of available sources of genetic resistance. An alternative strategy to reduce loss to specific bacterial phytopathogens is to use narrow-spectrum protein antibiotics such as colicin-like bacteriocins as biocontrol agents. A number of colicin-like bacteriocins active against phytopathogenic bacteria have been described previously as have strategies for their application to biocontrol. In the present paper, we discuss these strategies and our own recent work on the identification and characterization of candidate bacteriocins and how these potent and selective antimicrobial agents can be effectively applied to the control of economically important plant disease.', 'corpus_id': 10657033}	8548	"[{'doc_id': '221402862', 'title': 'Probiotics: Versatile Bioactive Components in Promoting Human Health', 'abstract': 'The positive impact of probiotic strains on human health has become more evident than ever before. Often delivered through food, dietary products, supplements, and drugs, different legislations for safety and efficacy issues have been prepared. Furthermore, regulatory agencies have addressed various approaches toward these products, whether they authorize claims mentioning a disease’s diagnosis, prevention, or treatment. Due to the diversity of bacteria and yeast strains, strict approaches have been designed to assess for side effects and post-market surveillance. One of the most essential delivery systems of probiotics is within food, due to the great beneficial health effects of this system compared to pharmaceutical products and also due to the increasing importance of food and nutrition. Modern lifestyle or various diseases lead to an imbalance of the intestinal flora. Nonetheless, as the amount of probiotic use needs accurate calculations, different factors should also be taken into consideration. One of the novelties of this review is the presentation of the beneficial effects of the administration of probiotics as a potential adjuvant therapy in COVID-19. Thus, this paper provides an integrative overview of different aspects of probiotics, from human health care applications to safety, quality, and control.', 'corpus_id': 221402862, 'score': 0}, {'doc_id': '792798', 'title': 'Bacteriocins from the rhizosphere microbiome – from an agriculture perspective', 'abstract': 'Bacteria produce and excrete a versatile and dynamic suit of compounds to defend against microbial competitors and mediate local population dynamics. These include a wide range of broad-spectrum non-ribosomally synthesized antibiotics, lytic enzymes, metabolic by-products, proteinaceous exotoxins, and ribosomally produced antimicrobial peptides (bacteriocins). Most bacteria produce at least one bacteriocin. Bacteriocins are of interest in the food industry as natural preservatives and in the probiotics industry, leading to extensive studies on lactic acid bacteria (colicin produced by Escherichia coli is a model bacteriocin). Recent studies have projected use of bacteriocins in veterinary medicine and in agriculture, as biostimulants of plant growth and development and as biocontrol agents. For example, bacteriocins such as Cerein 8A, Bac-GM17, putidacin, Bac 14B, amylocyclicin have been studied for their mechanisms of anti-microbial activity. Bac IH7 promotes tomato and musk melon plant growth. Thuricin 17 (Th17) is the only bacteriocin studied extensively for plant growth promotion, including at the molecular level. Th17 functions as a bacterial signal compound, promoting plant growth in legumes and non-legumes. In Arabidopsis thaliana and Glycine max Th17 increased phytohormones IAA and SA at 24 h post treatment. At the proteome level Th17 treatment of 3-week-old A. thaliana rosettes led to >2-fold changes in activation of the carbon and energy metabolism pathway proteins, 24 h post treatment. At 250 mM NaCl stress, the control plants under osmotic-shock shut down most of carbon-metabolism and activated energy-metabolism and antioxidant pathways. Th17 treated plants, at 250 mM NaCl, retained meaningful levels of the light harvesting complex, photosystems I and II proteins and energy and antioxidant pathways were activated, so that rosettes could better withstand the salt stress. In Glycine max, Th17 helped seeds germinate in the presence of NaCl stress, and was most effective at 100 mM NaCl. The 48 h post germination proteome suggested efficient and speedier partitioning of storage proteins, activation of carbon, nitrogen and energy metabolisms in Th17 treated seeds both under optimal and 100 mM NaCl. This review focuses on the bacteriocins produced by plant-rhizosphere colonizers and plant-pathogenic bacteria, that might have uses in agriculture, veterinary, and human medicine.', 'corpus_id': 792798, 'score': 1}, {'doc_id': '220629959', 'title': 'Artemisia annua, a Traditional Plant Brought to Light', 'abstract': 'Traditional remedies have been used for thousand years for the prevention and treatment of infectious diseases, particularly in developing countries. Of growing interest, the plant Artemisia annua, known for its malarial properties, has been studied for its numerous biological activities including metabolic, anti-tumor, anti-microbial and immunomodulatory properties. Artemisia annua is very rich in secondary metabolites such as monoterpenes, sesquiterpenes and phenolic compounds, of which the biological properties have been extensively studied. The purpose of this review is to gather and describe the data concerning the main chemical components produced by Artemisia annua and to describe the state of the art about the biological activities reported for this plant and its compounds beyond malaria.', 'corpus_id': 220629959, 'score': 0}, {'doc_id': '210871744', 'title': 'Antimicrobials for food and feed; a bacteriocin perspective.', 'abstract': 'Bacteriocins are natural antimicrobials that have been consumed via fermented foods for millennia and have been the focus of renewed efforts to identify novel bacteriocins, and their producing microorganisms, for use as food biopreservatives and other applications. Bioengineering bacteriocins or combining bacteriocins with multiple modes of action (hurdle approach) can enhance their preservative effect and reduces the incidence of antimicrobial resistance. In addition to their role as food biopreservatives, bacteriocins are gaining credibility as health modulators, due to their ability to regulate the gut microbiota, which is strongly associated with human wellbeing. Indeed the strengthening link between the gut microbiota and obesity make bacteriocins ideal alternatives to Animal Growth Promoters (AGP) in animal feed also. Here we review recent advances in bacteriocin research that will contribute to the development of functional foods and feeds as a consequence of roles in food biopreservation and human/animal health.', 'corpus_id': 210871744, 'score': 1}, {'doc_id': '221728087', 'title': 'Strategies of Plant Biotechnology to Meet the Increasing Demand of Food and Nutrition in India', 'abstract': ""A groundbreaking application of biotechnology research during the recent past has been improvement of crop health and production. India being one of the most rapidly developing countries with an enormous population and remarkable biodiversity, plant biotechnology promises significant potential to contribute to characterization and conservation of the biodiversity, increasing its usefulness. However, India’s green revolution was noted to be insufficient to feed the country's teeming millions. Therefore, novel approaches in crop biotechnology had to be aimed at ensuring better productivity and quality of cultivars. This paper provides a comprehensive review of research undertaken mainly in the last couple of decades along with potential strategies in plant biotechnology focusing on specific grain and seed crops of key agricultural as well as dietary importance to meet the growing demand of food and nutrition in India, while also proposing potential application of relevant global research findings in the Indian context. The analysis would help address the ever-increasing worldwide socioeconomic necessity for greater food security, particularly during times of crisis such as the recent Coronavirus Infectious Disease 2019 (COVID-19) pandemic."", 'corpus_id': 221728087, 'score': 0}, {'doc_id': '221691975', 'title': 'Impact of Dietary Modification on Microbiome:Exploring Therapeutic Implications', 'abstract': 'The Human Gut Microbiome: The gastro-intestinal tract and various other organs harbour large and diverse communities of bacteria, viruses, and other microscopic life. In the human gut, there inhabit microbial members as residents (autochthonous), while others (allochthonous) are from ingested food, water and other components of the environment. The adult human gut microbiota is dominated by mainly two bacteria, the Bacteroidetes and Firmicutes and an archaea, Metanobrevibacter smithii.', 'corpus_id': 221691975, 'score': 0}, {'doc_id': '92671064', 'title': 'Bacteriocins active against plant pathogenic bacteria. Biochem Soc Trans', 'abstract': None, 'corpus_id': 92671064, 'score': 1}, {'doc_id': '58553652', 'title': 'Bacteriocins: Classification, synthesis, mechanism of action and resistance development in food spoilage causing bacteria.', 'abstract': 'Huge demand of safe and natural preservatives has opened new area for intensive research on bacteriocins to unravel the novel range of antimicrobial compounds that could efficiently fight off the food-borne pathogens. Since food safety has become an increasingly important international concern, the application of bacteriocins from lactic acid bacteria that target food spoilage/pathogenic bacteria without major adverse effects has received great attention. Different modes of actions of these bacteriocins have been suggested and identified, like pore-forming, inhibition of cell-wall/nucleic acid/protein synthesis. However, development of resistance in the food spoilage and pathogenic bacteria against these bacteriocins is a rising concern. Emergence and spread of mutant strains resistant to bacteriocins is hampering food safety. It has spurred an interest to understand the bacteriocin resistance phenomenon displayed by the food pathogens, which will be helpful in mitigating the resistance problem. Therefore, present review is focused on the different resistance mechanisms adopted by food pathogens to overcome bacteriocin.', 'corpus_id': 58553652, 'score': 1}, {'doc_id': '37563756', 'title': 'Bacteriocins — a viable alternative to antibiotics?', 'abstract': 'Solutions are urgently required for the growing number of infections caused by antibiotic-resistant bacteria. Bacteriocins, which are antimicrobial peptides produced by certain bacteria, might warrant serious consideration as alternatives to traditional antibiotics. These molecules exhibit significant potency against other bacteria (including antibiotic-resistant strains), are stable and can have narrow or broad activity spectra. Bacteriocins can even be produced in situ in the gut by probiotic bacteria to combat intestinal infections. Although the application of specific bacteriocins might be curtailed by the development of resistance, an understanding of the mechanisms by which such resistance could emerge will enable researchers to develop strategies to minimize this potential problem.', 'corpus_id': 37563756, 'score': 1}, {'doc_id': '221507911', 'title': 'Investigation of the Cyprus donkey milk bacterial diversity by 16SrDNA high-throughput sequencing in a Cyprus donkey farm', 'abstract': 'The interest in milk originating from donkeys is growing worldwide due to its claimed functional and nutritional properties, especially for sensitive population groups, such as infants with cow milk protein allergy. The current study aimed to assess the microbiological quality of donkey milk produced in a donkey farm in Cyprus using culture-based and high-throughput sequencing techniques. The culture-based microbiological analysis showed very low microbial counts, whereas important food-borne pathogens were not detected in any sample. In addition, high-throughput sequencing was applied to characterize the bacterial communities of donkey milk samples. Donkey milk mostly composed of gram-negative Proteobacteria, including Sphingomonas, Pseudomonas, Mesorhizobium, and Acinetobacter; lactic acid bacteria, including Lactobacillus and Streptococcus; the endospores forming Clostridium; and the environmental genera Flavobacterium and Ralstonia, detected in lower relative abundances. The results of the study support existing findings that donkey milk contains mostly gram-negative bacteria. Moreover, it raises questions regarding the contribution of (1) antimicrobial agents (i.e., lysozyme, peptides) in shaping the microbial communities and (2) bacterial microbiota to the functional value of donkey milk.', 'corpus_id': 221507911, 'score': 0}]"
49	{'doc_id': '215754706', 'title': 'When to Update Systematic Literature Reviews in Software Engineering', 'abstract': '[Context] Systematic Literature Reviews (SLRs) have been adopted by the Software Engineering (SE) community for approximately 15 years to provide meaningful summaries of evidence on several topics. Many of these SLRs are now potentially outdated, and there are no systematic proposals on when to update SLRs in SE. [Objective] The goal of this paper is to provide recommendations on when to update SLRs in SE. [Method] We evaluated, using a three-step approach, a third-party decision framework (3PDF) employed in other fields, to decide whether SLRs need updating. First, we conducted a literature review of SLR updates in SE and contacted the authors to obtain their feedback relating to the usefulness of the 3PDF within the context of SLR updates in SE. Second, we used these authors feedback to see whether the framework needed any adaptation; none was suggested. Third, we applied the 3PDF to the SLR updates identified in our literature review. [Results] The 3PDF showed that 14 of the 20 SLRs did not need updating. This supports the use of a decision support mechanism (such as the 3PDF) to help the SE community decide when to update SLRs. [Conclusions] We put forward that the 3PDF should be adopted by the SE community to keep relevant evidence up to date and to avoid wasting effort with unnecessary updates.', 'corpus_id': 215754706}	3460	[{'doc_id': '214612459', 'title': 'Rapid Reviews in Software Engineering', 'abstract': 'Integrating research evidence into practice is one of the main goals of evidence-based software engineering (EBSE). Secondary studies, one of the main EBSE products, are intended to summarize the “best” research evidence and make them easily consumable by practitioners. However, recent studies show that some secondary studies lack connections with software engineering practice. In this chapter, we present the concept of Rapid Reviews, which are lightweight secondary studies focused on delivering evidence to practitioners in a timely manner. Rapid reviews support practitioners in their decision-making, and should be conducted bounded to a practical problem, inserted into a practical context. Thus, Rapid Reviews can be easily integrated in a knowledge/technology transfer initiative. After describing the basic concepts, we present the results and experiences of conducting two Rapid Reviews. We also provide guidelines to help researchers and practitioners who want to conduct Rapid Reviews, and we finally discuss topics that may concern the research community about the feasibility of Rapid Reviews as an evidence-based method. In conclusion, we believe Rapid Reviews might be of interest to researchers and practitioners working on the intersection of software engineering research and practice.', 'corpus_id': 214612459, 'score': 1}, {'doc_id': '212676032', 'title': 'Analyzing the Impact of Automated User Assistance Systems: A Systematic Review', 'abstract': 'Context: User assistance is generally defined as the guided assistance to a user of a software system in order to help accomplish tasks and enhance user experience. Automated user assistance systems are equipped with online help system that provides information to the user in an electronic format and which can be opened directly in the application. Various different automated user assistance approaches have been proposed in the literature. However, there has been no attempt to systematically review and report the impact of automated user assistance systems. Objective: The overall objective of this systematic review is to identify the state of art in automated user assistance systems, and describe the reported evidence for automated user assistance. Method: A systematic literature review is conducted by a multiphase study selection process using the published literature since 2002. Results: We reviewed 575 papers that are discovered using a well-planned review protocol, and 31 of them were assessed as primary studies related to our research questions. Conclusions: Our study shows that user assistance systems can provide important benefits for the user but still more research is required in this domain.', 'corpus_id': 212676032, 'score': 0}, {'doc_id': '211020564', 'title': 'The Four Pillars of Research Software Engineering', 'abstract': 'We present four elements we believe are key to providing a comprehensive and sustainable support for research software engineering: software development, community, training, and policy. We also show how the wider developer community can learn from, and engage with, these activities.', 'corpus_id': 211020564, 'score': 1}, {'doc_id': '211677517', 'title': 'A systematic literature review of modern software visualization', 'abstract': 'Abstract We report on the state-of-the-art of software visualization. To ensure reproducibility, we adopted the Systematic Literature Review methodology. That is, we analyzed 1440 entries from IEEE Xplore and ACM Digital Library databases. We selected 105 relevant full papers published in 2013–2019, which we classified based on the aspect of the software system that is supported (i.e., structure, behavior, and evolution). For each paper, we extracted main dimensions that characterize software visualizations, such as software engineering tasks, roles of users, information visualization techniques, and media used to display visualizations. We provide researchers in the field an overview of the state-of-the-art in software visualization and highlight research opportunities. We also help developers to identify suitable visualizations for their particular context by matching software visualizations to development concerns and concrete details to obtain available visualization tools. Graphic abstract', 'corpus_id': 211677517, 'score': 1}, {'doc_id': '5263776', 'title': 'Preliminary Guidelines for Empirical Research in Software Engineering', 'abstract': 'Empirical software engineering research needs research guidelines to improve the research and reporting processes. We propose a preliminary set of research guidelines aimed at stimulating discussion among software researchers. They are based on a review of research guidelines developed for medical researchers and on our own experience in doing and reviewing software engineering research. The guidelines are intended to assist researchers, reviewers, and meta-analysts in designing, conducting, and evaluating empirical studies. Editorial boards of software engineering journals may wish to use our recommendations as a basis for developing guidelines for reviewers and for framing policies for dealing with the design, data collection, and analysis and reporting of empirical studies.', 'corpus_id': 5263776, 'score': 1}, {'doc_id': '215908486', 'title': 'On the Performance of Hybrid Search Strategies for Systematic Literature Reviews in Software Engineering', 'abstract': 'Abstract Context When conducting a Systematic Literature Review (SLR), researchers usually face the challenge of designing a search strategy that appropriately balances result quality and review effort. Using digital library (or database) searches or snowballing alone may not be enough to achieve high-quality results. On the other hand, using both digital library searches and snowballing together may increase the overall review effort. Objective The goal of this research is to propose and evaluate hybrid search strategies that selectively combine database searches with snowballing. Method We propose four hybrid search strategies combining database searches in digital libraries with iterative, parallel, or sequential backward and forward snowballing. We simulated the strategies over three existing SLRs in SE that adopted both database searches and snowballing. We compared the outcome of digital library searches, snowballing, and hybrid strategies using precision, recall, and F-measure to investigate the performance of each strategy. Results Our results show that, for the analyzed SLRs, combining database searches from the Scopus digital library with parallel or sequential snowballing achieved the most appropriate balance of precision and recall. Conclusion We put forward that, depending on the goals of the SLR and the available resources, using a hybrid search strategy involving a representative digital library and parallel or sequential snowballing tends to represent an appropriate alternative to be used when searching for evidence in SLRs.', 'corpus_id': 215908486, 'score': 1}, {'doc_id': '215827822', 'title': 'Code Review in the Classroom', 'abstract': 'This paper presents a case study to examine the affinity of the code review process among young developers in an academic setting. Code review is indispensable considering the positive outcomes it generates. However, it is not an individual activity and requires substantial interaction among stakeholders, deliverance, and acceptance of feedback, timely actions upon feedback as well as the ability to agree on a solution in the wake of diverse viewpoints. Young developers in a classroom setting provide a clear picture of the potential favourable and problematic areas of the code review process. Their feedback suggests that the process has been well received with some points to better the process. This paper can be used as guidelines to perform code reviews in the classroom.', 'corpus_id': 215827822, 'score': 0}, {'doc_id': '211146636', 'title': 'Formal Methods: From Academia to Industrial Practice. A Travel Guide', 'abstract': 'For many decades, formal methods are considered to be the way forward to help the software industry to make more reliable and trustworthy software. However, despite this strong belief and many individual success stories, no real change in industrial software development seems to be occurring. In fact, the software industry itself is moving forward rapidly, and the gap between what formal methods can achieve and the daily software-development practice does not appear to be getting smaller (and might even be growing). \nIn the past, many recommendations have already been made on how to develop formal-methods research in order to close this gap. This paper investigates why the gap nevertheless still exists and provides its own recommendations on what can be done by the formal-methods-research community to bridge it. Our recommendations do not focus on open research questions. In fact, formal-methods tools and techniques are already of high quality and can address many non-trivial problems; we do give some technical recommendations on how tools and techniques can be made more accessible. To a greater extent, we focus on the human aspect: how to achieve impact, how to change the way of thinking of the various stakeholders about this issue, and in particular, as a research community, how to alter our behaviour, and instead of competing, collaborate to address this issue.', 'corpus_id': 211146636, 'score': 0}, {'doc_id': '211171624', 'title': 'How Interaction Designers Use Tools to Manage Ideas', 'abstract': 'This article presents a grounded theory analysis based on a qualitative study of professional interaction designers (n = 20) with a focus on how they use tools to manage design ideas. Idea management can be understood as a subcategory of the field personal information management, which includes the activities around the capture, organization, retrieval, and use of information. Idea management pertains to the management and use of ideas, a particular type of information, as part of creative activities. The article identifies tool-supported idea management strategies and needs of professional interaction designers, and discusses the context and consequences of these strategies. Based on our analysis, we identify a conceptual framework of 10 strategies which are supported by tools: saving, externalizing, advancing, exploring, archiving, clustering, extracting, browsing, verifying, and collaborating. Finally, we discuss how this framework can be used to characterize and analyze existing and novel idea management tools.', 'corpus_id': 211171624, 'score': 0}, {'doc_id': '211082994', 'title': 'Measurement of Interpersonal Trust in Global Software Development: SLR Protocol', 'abstract': 'The purpose of this protocol is to be useful to identify, evaluate and synthesize reported knowledge about the measurement of interpersonal trust (IpT) in virtual software teams. To achieve this goal we applied a research technique known as Systematic Literature Review (SLR). The aim of a SLR is to be as objective, analytical, and repeatable as possible.', 'corpus_id': 211082994, 'score': 0}]
50	{'doc_id': '134211005', 'title': 'Past large earthquakes on the Alpine Fault: paleoseismological progress and future directions', 'abstract': 'ABSTRACT Paleoseismology has been making an important contribution to understanding the Alpine Fault and the hazard it poses to society. However, evidence of past earthquakes comes from a wide variety of sources and publication of the evidence has been somewhat fragmented. Here, we review physical evidence for past large to great earthquakes on the Alpine Fault to summarise current understanding, illustrate progress and highlight future directions. Paleoseismic evidence has been derived from tree disturbance, landscape features and trenches across the fault. These records have been supplemented and extended back in time with sedimentary evidence of Alpine Fault earthquakes from fault-proximal lakes and wetlands. In this review, we update radiocarbon analyses using recent calibration curves and modern Bayesian statistical methods where necessary to enable comparison between on-fault, fault-proximal and off-fault earthquake records. Over recent decades, Alpine Fault paleoseismology has progressed from playing an important role in demonstrating that large surface-rupturing earthquakes occur, to enabling estimates of earthquake recurrence behaviour, shaking intensities, rupture extents, landscape response durations and likelihood of the next earthquake.', 'corpus_id': 134211005}	12540	"[{'doc_id': '227254542', 'title': 'Numerical computation of stress-permeability relationships of fracture networks in a shale rock', 'abstract': 'We present stress-sensitive permeability relationships for two-dimensional fracture networks in the Opalinus Clay from the Mont Terri underground rock laboratory. These relationships may be used as a proxy for fracture network permeability in numerical models that resolve large spatial scales and are used in a variety of GeoEnergy applications involving flow in shaly rocks. To obtain these relationships we present a numerical procedure that uses experimentally determined stress-permeability relationships to numerically compute the effective permeability of the network. The material discontinuities stemming from the fractures are treated by a simple contact-interaction algorithm that accounts for normal interaction between fracture walls, allowing us to calculate the permeability of a fracture network under different stress conditions. We apply the procedure to four fracture networks digitized from two galleries of the Mont Terri rock laboratory. These fracture networks are mapped from the damage zone of the Main Fault that intersects the Opalinus Clay. The networks show a maximum variation of four orders of magnitude when stress ranges from 1MPa to 20 MPa. Our numerical procedure not only establishes representative stress-permeability relationships for a fractured rock mass under stress, but also provides a proxy for fracture network permeability for simulation in fractured formations.', 'corpus_id': 227254542, 'score': 0}, {'doc_id': '132516649', 'title': 'A plate boundary earthquake record from a wetland adjacent to the Alpine fault in New Zealand refines hazard estimates', 'abstract': ""Abstract Discovery and investigation of millennial-scale geological records of past large earthquakes improve understanding of earthquake frequency, recurrence behaviour, and likelihood of future rupture of major active faults. Here we present a ∼2000 year-long, seven-event earthquake record from John O'Groats wetland adjacent to the Alpine fault in New Zealand, one of the most active strike-slip faults in the world. We linked this record with the 7000 year-long, 22-event earthquake record from Hokuri Creek (20 km along strike to the north) to refine estimates of earthquake frequency and recurrence behaviour for the South Westland section of the plate boundary fault. Eight cores from John O'Groats wetland revealed a sequence that alternated between organic-dominated and clastic-dominated sediment packages. Transitions from a thick organic unit to a thick clastic unit that were sharp, involved a significant change in depositional environment, and were basin-wide, were interpreted as evidence of past surface-rupturing earthquakes. Radiocarbon dates of short-lived organic fractions either side of these transitions were modelled to provide estimates for earthquake ages. Of the seven events recognised at the John O'Groats site, three post-date the most recent event at Hokuri Creek, two match events at Hokuri Creek, and two events at John O'Groats occurred in a long interval during which the Hokuri Creek site may not have been recording earthquakes clearly. The preferred John O'Groats–Hokuri Creek earthquake record consists of 27 events since ∼6000 BC for which we calculate a mean recurrence interval of 291 ± 23 years , shorter than previously estimated for the South Westland section of the fault and shorter than the current interseismic period. The revised 50-year conditional probability of a surface-rupturing earthquake on this fault section is 29%. The coefficient of variation is estimated at 0.41. We suggest the low recurrence variability is likely to be a feature of other strike-slip plate boundary faults similar to the Alpine fault."", 'corpus_id': 132516649, 'score': 1}, {'doc_id': '134456106', 'title': 'Focal mechanisms and inter-event times of low-frequency earthquakes reveal quasi-continuous deformation and triggered slow slip on the deep Alpine Fault', 'abstract': 'Abstract Characterising the seismicity associated with slow deformation in the vicinity of the Alpine Fault may provide constraints on the stresses acting on a major transpressive margin prior to an anticipated great (≥M8) earthquake. Here, we use recently detected tremor and low-frequency earthquakes (LFEs) to examine how slow tectonic deformation is loading the Alpine Fault late in its typical ∼300-yr seismic cycle. We analyse a continuous seismic dataset recorded between 2009 and 2016 using a network of 10–13 short-period seismometers, the Southern Alps Microearthquake Borehole Array. Fourteen primary LFE templates are used in an iterative matched-filter and stacking routine, allowing the detection of similar signals corresponding to LFE families sharing common locations. This yields an 8-yr catalogue containing 10,000 LFEs that are combined for each of the 14 LFE families using phase-weighted stacking to produce signals with the highest possible signal-to-noise ratios. We show that LFEs occur almost continuously during the 8-yr study period and highlight two types of LFE distributions: (1) discrete behaviour with an inter-event time exceeding 2 min; (2) burst-like behaviour with an inter-event time below 2 min. We interpret the discrete events as small-scale frequent deformation on the deep extent of the Alpine Fault and LFE bursts (corresponding in most cases to known episodes of tremor or large regional earthquakes) as brief periods of increased slip activity indicative of slow slip. We compute improved non-linear earthquake locations using a 3-D velocity model. LFEs occur below the seismogenic zone at depths of 17–42 km, on or near the hypothesised deep extent of the Alpine Fault. The first estimates of LFE focal mechanisms associated with continental faulting, in conjunction with recurrence intervals, are consistent with quasi-continuous shear faulting on the deep extent of the Alpine Fault.', 'corpus_id': 134456106, 'score': 1}, {'doc_id': '222282997', 'title': 'Selection of Ground Motion Models for Probabilistic Seismic Hazard Analysis in Iran', 'abstract': 'Back-projection has proven useful to image large earthquake rupture processes. It utilizes array techniques to estimate the spatial and temporal evolution of earthquake rupture over time, and can help us identify interesting earthquake phenomena like supershear rupture. However, the method does not directly solve an inverse problem and has difficulty in quantifying epistemic uncertainties, which can be caused by seismic array configurations, structural heterogeneities in the Earth’s crust, unknown seismic phases, and variations in the focal mechanism. These uncertainties may cause erroneous interpretations of earthquake physics, which is particularly challenging to distinguish for complex earthquake rupture processes.', 'corpus_id': 222282997, 'score': 0}, {'doc_id': '134795563', 'title': 'Evidence for a pre-Eocene proto-Alpine Fault through Zealandia', 'abstract': 'ABSTRACT For many decades, there has been speculation about when the Alpine Fault first became a major regional structure. This paper presents a summary of recently published data, especially U–Pb ages, that supports the existence of a Late Cretaceous to Paleogene proto-Alpine Fault in the form of a thermotectonic corridor between North and South Zealandia. The evidence takes the form of fault-controlled sedimentary basins, ductile shear zones, Alpine Schist regional metamorphism and igneous rocks. Much critical evidence has been removed or strongly modified by Neogene shortening and the total lateral displacement history of the proto-Alpine Fault cannot yet be unambiguously determined.', 'corpus_id': 134795563, 'score': 1}, {'doc_id': '226236714', 'title': 'Earthquake and Electrochemistry: Unraveling the Unpredictable', 'abstract': ""Earthquakes are measured using well defined seismic parameters such as seismic moment (Mo), moment magnitude (Mw), and released elastic energy(E). How this tremendous amount of energy is accumulated silently deep inside the earth's crust? The most obvious question in seismic research remains unanswered. We found an inherent and intriguing connection between the released energy in an earthquake and electrochemical potential induced in an ultra-thin metal oxide electrode immersed in an aqueous pH solution, which leads us to understand the origin of the energy accumulation process in an earthquake. A huge electrochemical potential is accumulated from numerous electrochemical cells formed in a unique layer structure of hydrated clay minerals (predominantly smectite), which resulted in a lightning-like discharge in the lithosphere (hypocenter). The subsequent thunder-like massive shockwave is produced, which initiates tectonic plate movement along a fault line, probably through acoustic fluidization (AF), and resulting seismic energy is transmitted as primary wave (P-wave), secondary wave (S-wave), and surface waves. The presence of electrical voltage in the hypocenter directly supports the seismic electric signal (SES), further strengthening the VAN method of earthquake prediction. Our finding is supported by a plethora of research and observation devoted to seismic science. This study will indeed find its significance if immediate action is implemented to monitor the evolution of electrochemical potential, seismic electrical signal (SES), and ionic activity in the fault zone at lithosphere as well as in the ionosphere for predicting an impending earthquake for saving human lives as early as possible."", 'corpus_id': 226236714, 'score': 0}, {'doc_id': '134068443', 'title': 'Frictional properties and 3-D stress analysis of the southern Alpine Fault, New Zealand', 'abstract': ""Abstract New Zealand's Alpine Fault (AF) ruptures quasi-periodically in large-magnitude earthquakes. Paleoseismological evidence suggests that about half of all recognized AF earthquakes terminated at the boundary between the Central and South Westland sections of the fault. There, fault geometry and the polarity of uplift change. The South Westland AF exhibits oblique-normal fault motion on a structure oriented 052°/82°SE that, for at least 35\u202fkm along strike, contains saponite-rich principal slip zone gouges. New hydrothermal friction experiments reveal that the saponite fault gouge is frictionally weak, exhibiting friction coefficients between μ\u202f=\u202f0.12 and μ\u202f=\u202f0.16 for a range of temperatures (T\u202f=\u202f25–210\u202f°C) and effective normal stresses (σn'\u202f=\u202f31.2–93.6\u202fMPa). The saponite gouge is rate-strengthening in all velocity steps performed at velocities between 0.01 and 3.0\u202fμm/s, behavior conducive to aseismic creep. A three-dimensional stress analysis shows that the South Westland AF is favorably oriented with respect to the regional stress field for slip within the frictionally weak saponite fault gouge. Geometrically, the fault is severely misoriented for slip in any fault-forming materials with friction coefficients exceeding μ∼0.5. The combination of weak gouges prone to aseismic creep, strong asperities, and low resolved shear stress may impede earthquake rupture propagation along the South Westland Alpine Fault."", 'corpus_id': 134068443, 'score': 1}, {'doc_id': '224821702', 'title': 'Crustal deformation rates in Kashmir valley and adjoining regions from continuous GPS measurements from 2008 to 2019', 'abstract': 'We present GPS velocities in Kashmir valley and adjoining regions from continuous Global Positioning System (cGPS) network during 2008 to 2019. Results indicate total arc normal shortening rates of\u2009~\u200914 mm/year across this transect of Himalaya that is comparable to the rates of\u2009~\u200910 to 20 mm/year reported else-where in the 2500 km Himalaya Arc. For the first time in Himalayas, arc-parallel extension rate of\u2009~\u20097 mm/year was recorded in the Kashmir valley, pointing to oblique deformation. Inverse modeling of the contemporary deformation rates in Kashmir valley indicate oblique slip of\u2009~\u200916 mm/year along the decollement with locking depth of\u2009~\u200915 km and width of\u2009~\u2009145 km. This result is consistent with the recorded micro-seismicity and low velocity layer at a depth of 12 to 16 km beneath the Kashmir valley obtained from collocated broadband seismic network. Geodetic strain rates are consistent with the dislocation model and micro-seismic activity, with high strain accumulation (~\u20097e−08 maximum compression) to the north of Kashmir valley and south of Zanskar ranges. Assuming the stored energy was fully released during 1555 earthquake, high geodetic strain rate since then and observed micro-seismicity point to probable future large earthquakes of Mw\u2009~\u20097.7 in Kashmir seismic gap.', 'corpus_id': 224821702, 'score': 0}, {'doc_id': '59331719', 'title': 'The Alpine Fault Hangingwall Viewed From Within: Structural Analysis of Ultrasonic Image Logs in the DFDP‐2B Borehole, New Zealand', 'abstract': ""Ultrasonic image logs acquired in the DFDP‐2B borehole yield the first continuous, subsurface description of the transition from schist to mylonite in the hangingwall of the Alpine Fault, New Zealand, to a depth of 818 m below surface. Three feature sets are delineated. One set, comprising foliation and foliation‐parallel veins and fractures, has a constant orientation. The average dip direction of 145° is subparallel to the dip direction of the Alpine Fault, and the average dip magnitude of 60° is similar to nearby outcrop observations of foliation in the Alpine mylonites that occur immediately above the Alpine Fault. We suggest that this foliation orientation is similar to the Alpine Fault plane at ∼1 km depth in the Whataroa valley. The other two auxiliary feature sets are interpreted as joints based on their morphology and orientation. Subvertical joints with NW‐SE (137°) strike occurring dominantly above ∼500 m are interpreted as being formed during the exhumation and unloading of the Alpine Fault's hangingwall. Gently dipping joints, predominantly observed below ∼500 m, are interpreted as inherited hydrofractures exhumed from their depth of formation. These three fracture sets, combined with subsidiary brecciated fault zones, define the fluid pathways and anisotropic permeability directions. In addition, high topographic relief, which perturbs the stress tensor, likely enhances the slip potential and thus permeability of subvertical fractures below the ridges, and of gently dipping fractures below the valleys. Thus, DFDP‐2B borehole observations support the inference of a large zone of enhanced permeability in the hangingwall of the Alpine Fault."", 'corpus_id': 59331719, 'score': 1}, {'doc_id': '226965171', 'title': 'On the correlation of earthquake occurrence among major fault zones in the eastern margin of the Tibetan Plateau by Big Data Analysis', 'abstract': 'The subsequent series of responses to big events may exhibit a synchronicity of event number, frequency and energy release in different fault zones. This synchronicity is a reliable source for probing non-intuitive geological structures, assessing regional seismicity hazard map and even predicting the next big events. The synchronicity of main faults in the eastern margin of the Qinghai-Tibetan Plateau is still unknown to us. We propose to examine the correlation of earthquake occurrence among different fault zones to indicate this synchronicity, and to obtain a preliminary understanding of geodynamics processes and the unrecognized characteristics of deep evolution in the eastern margin of the Qinghai-Tibetan Plateau. We estimate temporal changes of completeness level, frequency seismicity, and intensity seismicity, referring respectively to Mc, Z, and E values, of 21 main fault zones, using a seismic catalogue from 1970 to 2015. Our results reveal that six fault zone pairs of fault zones exhibit relative high correlation (>0.6) by all three indicators, while four fault zone pairs are non-adjacent with close internal affinity offsetting the limit of spatial distance, such as the pair of Rongjing-mabian fault and Minjiang-huya fault. Most strikingly, some fault zone pairs showing typical high correlation (>0.8) of seismicity frequency or seismicity intensity, the faults surprisingly belong to neither the same seismic belt nor the same geological block, exhibiting a regional scale remote triggering pattern of earthquakes or structures. An embryonic pattern to predict the next possible events will also be presented. This correlation analysis discovers a previously unrecognized strong coupling relationship among main faults with high earthquake risk in the eastern margin of the Qinghai-Tibetan Plateau.', 'corpus_id': 226965171, 'score': 0}]"
51	{'doc_id': '7126677', 'title': 'Principles of Smart Home Control', 'abstract': 'Seeking to be sensitive to users, smart home researchers have focused on the concept of control. They attempt to allow users to gain control over their lives by framing the problem as one of end-user programming. But families are not users as we typically conceive them, and a large body of ethnographic research shows how their activities and routines do not map well to programming tasks. End-user programming ultimately provides control of devices. But families want more control of their lives. In this paper, we explore this disconnect. Using grounded contextual fieldwork with dual-income families, we describe the control that families want, and suggest seven design principles that will help end-user programming systems deliver that control.', 'corpus_id': 7126677}	7014	"[{'doc_id': '220407868', 'title': 'Mothers, childcare duties, and remote working under COVID-19 lockdown in Italy: Cultivating communities of care', 'abstract': 'Drawing on a virtual ethnography, we explore how the increase in remote working has created unequal domestic rearrangements of parenting duties with respect to gender relations during the COVID-19 lockdown in Italy. We also discuss the resources that mothers have mobilized to create a network of social support in the organization of care.', 'corpus_id': 220407868, 'score': 0}, {'doc_id': '14480787', 'title': 'Home automation in the wild: challenges and opportunities', 'abstract': 'Visions of smart homes have long caught the attention of researchers and considerable effort has been put toward enabling home automation. However, these technologies have not been widely adopted despite being available for over three decades. To gain insight into this state of affairs, we conducted semi-structured home visits to 14 households with home automation. The long term experience, both positive and negative, of the households we interviewed illustrates four barriers that need to be addressed before home automation becomes amenable to broader adoption. These barriers are high cost of ownership, inflexibility, poor manageability, and difficulty achieving security. Our findings also provide several directions for further research, which include eliminating the need for structural changes for installing home automation, providing users with simple security primitives that they can confidently configure, and enabling composition of home devices.', 'corpus_id': 14480787, 'score': 1}, {'doc_id': '67781466', 'title': 'Benefits and risks of smart home technologies', 'abstract': 'Smart homes are a priority area of strategic energy planning and national policy. The market adoption of smart home technologies (SHTs) relies on prospective users perceiving clear benefits with acceptable levels of risk. This paper characterises the perceived benefits and risks of SHTs from multiple perspectives.', 'corpus_id': 67781466, 'score': 1}, {'doc_id': '2394201', 'title': 'Sabbath day home automation: ""it\'s like mixing technology and religion""', 'abstract': ""We present a qualitative study of 20 American Orthodox Jewish families' use of home automation for religious purposes. These lead users offer insight into real-life, long-term experience with home automation technologies. We discuss how automation was seen by participants to contribute to spiritual experience and how participants oriented to the use of automation as a religious custom. We also discuss the relationship of home automation to family life. We draw design implications for the broader population, including surrender of control as a design resource, home technologies that support long-term goals and lifestyle choices, and respite from technology."", 'corpus_id': 2394201, 'score': 1}, {'doc_id': '167784161', 'title': 'Who uses smart home technologies? Representations of users by the smart home industry', 'abstract': 'Through ambient intelligence and automated control systems, smart homes have been presented as a key means by which households can optimize their use of energy-consuming appliances in order to save energy and money. Whilst the adoption of smart home technologies and their appropriation within everyday domestic lives is critical to the overall success of smart homes, to date visions of smart homes have been strongly driven by technology push and have not been based on a clear understanding of user-centric benefits, nor have users been engaged with in any clear or systematic way. There is thus an important need to understand how smart home users are being represented and understood within these technology-driven visions. The paper presents the results of a content analysis of industry-produced smart home marketing materials that focussed on representations of the technology itself, its users, and of technology-user interactions. The content analysis was based on a coding template derived from a systematic review of the academic literature on smart homes and their users. Key findings from the content analysis include: • differences in opinion around whether user practices are predominantly stable, routine and predictable or involve substantial variability and unpredictability; • consensus around the modular development of smart homes within existing homes through additional and integrated (rather than replacement) technologies; • a widespread lack of attention to within household interactions and the possibility of multiple users with divergent technology preferences; • an implicit assumption that user decision-making is mainly rational, centred on information-processing; • strong consensus on the design of user interfaces as mobile, familiar, intuitive, and visible devices; • ambiguity regarding the potential tension between control and empowerment as opposed to automation. The paper concludes that industry visions of smart homes are more convergent than academic research suggests, particularly around issues of user decisions and interaction, trust and confidentiality, and control and automation.', 'corpus_id': 167784161, 'score': 1}, {'doc_id': '219300570', 'title': 'Impact of Covid-19 on consumer behavior: Will the old habits return or die?', 'abstract': '\n Abstract\n \n The COVID-19 pandemic and the lockdown and social distancing mandates have disrupted the consumer habits of buying as well as shopping. Consumers are learning to improvise and learn new habits. For example, consumers cannot go to the store, so the store comes to home. While consumers go back to old habits, it is likely that they will be modified by new regulations and procedures in the way consumers shop and buy products and services. New habits will also emerge by technology advances, changing demographics and innovative ways consumers have learned to cope with blurring the work, leisure, and education boundaries.\n \n', 'corpus_id': 219300570, 'score': 0}, {'doc_id': '221881991', 'title': 'Smart cities as a platform for technological and social innovation in productivity, sustainability, and livability: A conceptual framework', 'abstract': '\n Despite a great deal of attention paid to smart cities, the conceptual framework for understanding them has been partial at best. This chapter establishes a holistic framework to define and evaluate smart cities through three core objectives that any city wants to improve—productivity, sustainability, and livability. Although smartness includes a wide range of aspects within a city, it should tackle the complexity of urban challenges internally and externally generated. Thus, adaptive capacity is becoming more and more important, requiring timely innovation. The chapter asserts cities are and should be a platform for technological and social innovation to enhance these three urban cores. Creating smart cities via innovation is not a one-way process, but reciprocal. Innovation can create smart built environments, and, in turn, smart cities engender innovation. There are many successful evidences and documented examples of both technology-oriented initiatives and social innovation strategies worldwide. However, there is limited understanding of the combined view on technological innovation or social innovation that can contribute to meeting urban challenges. Furthermore, how the urban future might benefit from interdependency and interactions of the elements in these two concepts has not been fully explored. The research will set an agenda for measurement of cities’ performance in productivity, sustainability, and livability from both technological and social innovation perspectives.\n', 'corpus_id': 221881991, 'score': 0}, {'doc_id': '219691177', 'title': 'Using online technologies to improve diversity and inclusion in cognitive interviews with young people', 'abstract': 'Background We aimed to assess the feasibility of using multiple technologies to recruit and conduct cognitive interviews among young people across the United States to test items measuring sexual and reproductive empowerment. We sought to understand whether these methods could achieve a diverse sample of participants. With more researchers turning to approaches that maintain social distancing in the context of COVID-19, it has become more pressing to refine these remote research methods. Methods We used several online sites to recruit for and conduct cognitive testing of survey items. To recruit potential participants we advertised the study on the free online bulletin board, Craigslist, and the free online social network, Reddit. Interested participants completed an online Qualtrics screening form. To maximize diversity, we purposefully selected individuals to invite for participation. We used the video meeting platform, Zoom, to conduct the cognitive interviews. The interviewer opened a document with the items to be tested, shared the screen with the participant, and gave them control of the mouse and keyboard. After the participant self-administered the survey, the\xa0interviewer asked about interpretation and comprehension. After completion of the interviews we sent participants a follow-up survey about their impressions of the research methods and technologies used. We describe the processes, the advantages and disadvantages, and offer recommendations for researchers. Results We recruited and interviewed 30 young people from a range of regions, gender identities, sexual orientations, ages, education, and experiences with sexual activity. These methods allowed us to recruit a purposefully selected diverse sample in terms of race/ethnicity and region. It also may have offered potential participants a feeling of safety and anonymity leading to greater participation from gay, lesbian, and transgender people who would not have agreed to participate in-person. Conducting the interviews using video chat may also have facilitated the inclusion of individuals who would not volunteer for in-person meetings. Disadvantages of video interviewing included participant challenges to finding a private space for the interview and problems\xa0with electronic devices. Conclusions Online technologies can be used to achieve a diverse sample of research participants, contributing to research findings that better respond to young people’s unique identities and situations.', 'corpus_id': 219691177, 'score': 0}, {'doc_id': '168761332', 'title': 'Perceived Benefits and Risks of Smart Home Technologies', 'abstract': 'This chapter characterises the perceived benefits and risks of smart home technologies (SHTs) from multiple perspectives. A representative national survey of over a thousand UK homeowners finds prospective users have positive perceptions of the multiple functionality of SHTs including energy management. Ceding autonomy and independence in the home for increased technological control are the main perceived risks. An additional survey of actual SHT users participating in a SHT field trial (see Chap. 1) identifies the key role of early adopters in lowering perceived SHT risks for the mass market. Content analysis of SHT marketing material finds that the SHT industry is insufficiently emphasising measures to build consumer confidence on data security and privacy. These multiple perspectives draw on insights from across the functional, instrumental and socio-technical views identified in the analytical framework for research on smart homes and their users (Chap. 2 and Table 2.1).', 'corpus_id': 168761332, 'score': 1}, {'doc_id': '219259981', 'title': 'A review of smartphones based indoor positioning: challenges and applications', 'abstract': 'The continual proliferation of mobile devices has encouraged much effort in using the smartphones for indoor positioning. This article is dedicated to review the most recent and interesting smartphones based indoor navigation systems, ranging from electromagnetic to inertia to visible light ones, with an emphasis on their unique challenges and potential real-world applications. A taxonomy of smartphones sensors will be introduced, which serves as the basis to categorise different positioning systems for reviewing. A set of criteria to be used for the evaluation purpose will be devised. For each sensor category, the most recent, interesting and practical systems will be examined, with detailed discussion on the open research questions for the academics, and the practicality for the potential clients.', 'corpus_id': 219259981, 'score': 0}]"
52	{'doc_id': '233284509', 'title': 'Applications of Computational Intelligence in Computer Music Composition', 'abstract': 'Engaging computers in composing musical pieces is a challenging and trending field of research. The musical tasks that can be performed or aided by computers’ computational powers, are numerous. This paper is concerned with applications of computational intelligence in music composition. Its main objective is to survey various computational intelligence techniques for performing miscellaneous music composition tasks. To achieve this objective, we first define each music composition task, then we discuss the recent applications of each, and the techniques adopted in them. We also highlight the most suitable techniques for performing each task. Our study shows that the most suitable techniques for human composers imitative systems are case-based reasoning and artificial neural networks. It is also shown that Markov models are more suitable for predicting musical notes based on the given previous notes. Genetic algorithms excel in chord progressions generation. Deep neural networks are clever at capturing temporal information of a musical piece. The state-of-the-art generative adversarial networks produce music as close as possible to real compositions. At the end of this study, we shed the light on many future research directions in the field of computer music composition.', 'corpus_id': 233284509}	16008	[{'doc_id': '232372487', 'title': 'Enhancing Local Dependencies for Transformer-Based Text-to-Speech via Hybrid Lightweight Convolution', 'abstract': 'Owing to the powerful self-attention mechanism, the Transformer network has achieved considerable successes across many sequence modeling tasks and has become one of the most popular methods in text-to-speech (TTS). The vanilla self-attention excels in capturing long-range dependencies but suffers in modeling stable short-range dependencies that are quite important for speech synthesis where the local audio signals are highly correlated. To address this problem, we propose the hybrid lightweight convolution (HLC), which is responsible for fully exploiting local structures of a sequence, and combine it with the self-attention to improve the Transformer-based TTS. The experimental results show that our modified model obtains better performance in both objective and subjective evaluations. At the same time, we also demonstrate that a more compact TTS model may be built through the combination of self-attention and proposed hybrid lightweight convolution. Besides, this method is also potentially adaptable for other sequence modeling tasks.', 'corpus_id': 232372487, 'score': 0}, {'doc_id': '233147773', 'title': 'MELODY GENERATION WITH MARKOV MODELS, A RULE BASED APPROACH', 'abstract': 'Algorithmic composition is an important area of musical research in machine learning and artificial intelligence. We propose a low-complexity solution to melody generation that develops melodies based on expert rule-based models and learned melody generation from Markov Models. Recent advances in machine learning based techniques for algorithmic composition have created an excellent set of methods for generating artistic musical works. These methods, while intrinsically very different from rule based models for the same purpose, can produce similar outputs if both are executed well. This project compares and contrasts a rule-based model and a markov model for melody generation. Both models strive to compare to a dataset of folk songs parsed into MIDI files, outputting MIDI data for playback and comparison. We compare the model outputs both quantitatively and qualitatively, both in terms of loyalty to source material, and in terms of adherence to music theory rules and practices. Our comparative results confirm the comparability of our melody generation with two additional advantages being flexibility of melody generation based on user preference and user dataset selection to program in inherent biases.', 'corpus_id': 233147773, 'score': 1}, {'doc_id': '233033844', 'title': 'Weakly-supervised Audio-visual Sound Source Detection and Separation', 'abstract': 'Learning how to localize and separate individual object sounds in the audio channel of the video is a difficult task. Current state-of-the-art methods predict audio masks from artificially mixed spectrograms, known as Mix-and-Separate framework. We propose an audio-visual co-segmentation, where the network learns both what individual objects look and sound like, from videos labeled with only object labels. Unlike other recent visually-guided audio source separation frameworks, our architecture can be learned in an end-to-end manner and requires no additional supervision or bounding box proposals. Specifically, we introduce weakly-supervised object segmentation in the context of sound separation. We also formulate spectrogram mask prediction using a set of learned mask bases, which combine using coefficients conditioned on the output of object segmentation , a design that facilitates separation. Extensive experiments on the MUSIC dataset show that our proposed approach outperforms state-of-the-art methods on visually guided sound source separation and sound denoising.', 'corpus_id': 233033844, 'score': 0}, {'doc_id': '233366278', 'title': 'Top 10 Artificial Intelligence Algorithms in Computer Music Composition', 'abstract': 'Music composition is now appealing to both musicians and non-musicians equally. It branches into various musical tasks such as the generation of melody, accompaniment, or rhythm. This paper discusses the top ten artificial intelligence algorithms with applications in computer music composition from 2010 to 2020. We give an analysis of each algorithm and highlight its recent applications in music composition tasks, shedding the light on its strengths and weaknesses. Our study gives an insight on the most suitable algorithm for each musical task, such as rule-based systems for music theory representation, case-based reasoning for capturing previous musical experiences, Markov chains for melody generation, generative grammars for fast composition of musical pieces that comply to music rules, and linear programming for timbre synthesis. Additionally, there are biologically inspired algorithms such as: genetic algorithms, and algorithms used by artificial immune systems and artificial neural networks, including shallow neural networks, deep neural networks, and generative adversarial networks. These relatively new algorithms are currently heavily used in performing numerous music composition tasks.', 'corpus_id': 233366278, 'score': 1}, {'doc_id': '232401622', 'title': 'Artificial intelligence and music: History and the future perceptive', 'abstract': 'Several music software programs have been developed that use AI to produce music. Like its applications in other fields, the A.I. in this case also simulates mental tasks. A prominent feature is the capability of the A.I. algorithm to learn based on information obtained such as the computer accompaniment technology, which is capable of listening to and following a human performer so it can perform in synchrony. Artificial intelligence also drives the so-called interactive composition technology, wherein a computer composes music in response to the performance of a live musician. There are several other A.I. applications to music that covers not only music composition, production, and performance but also the way it is marketed and consumed. Apart from programs that use AI to produce music, several music player programs have been developed to use voice recognition and natural language processing technology for music voice control.', 'corpus_id': 232401622, 'score': 1}, {'doc_id': '232185495', 'title': 'Multi-Format Contrastive Learning of Audio Representations', 'abstract': 'Recent advances suggest the advantage of multi-modal training in comparison with single-modal methods. In contrast to this view, in our work we find that similar gain can be obtained from training with different formats of a single modality. In particular, we investigate the use of the contrastive learning framework to learn audio representations by maximizing the agreement between the raw audio and its spectral representation. We find a significant gain using this multi-format strategy against the single-format counterparts. Moreover, on the downstream AudioSet and ESC-50 classification task, our audio-only approach achieves new state-ofthe-art results with a mean average precision of 0.376 and an accuracy of 90.5%, respectively.', 'corpus_id': 232185495, 'score': 0}, {'doc_id': '233255223', 'title': 'AUTOMATIC MUSIC PRODUCTION USING GENERA-', 'abstract': 'When talking about computer-based music generation, two are the main threads of research: the construction of autonomous music-making systems, and the design of computer-based environments to assist musicians. However, even though creating accompaniments for melodies is an essential part of every producer’s and songwriter’s work, little effort has been done in the field of automatic music arrangement in the audio domain. In this contribution, we propose a novel framework for automatic music accompaniment in the Mel-frequency domain. Using several songs converted into Mel-spectrograms – a two-dimensional time-frequency representation of audio signals – we were able to automatically generate original arrangements for both bass and voice lines. Treating music pieces as images (Mel-spectrograms) allowed us to reformulate our problem as an unpaired imageto-image translation problem, and to tackle it with CycleGAN, a well-established framework. Moreover, the choice to deploy raw audio and Mel-spectrograms enabled us to more effectively model long-range dependencies, to better represent how humans perceive music, and to potentially draw sounds for new arrangements from the vast collection of music recordings accumulated in the last century. Our approach was tested on two different downstream tasks: given a bass line creating credible and on-time drums, and given an acapella song arranging it to a full song. In absence of an objective way of evaluating the output of music generative systems, we also defined a possible metric for the proposed task, partially based on human (and expert) judgement.', 'corpus_id': 233255223, 'score': 1}, {'doc_id': '232372180', 'title': 'Chord Conditioned Melody Generation With Transformer Based Decoders', 'abstract': 'For successful artificial music composition, chords and melody must be aligned well. Yet, chord conditioned melody generation remains a challenging task mainly due to its multimodality. While few studies have focused on this task, they face difficulties in generating dynamic rhythm patterns aligned appropriately with a given chord progression. In this paper, we propose a chord conditioned melody Transformer, a K-POP melody generation model, which separately produces rhythm and pitch conditioned on a chord progression. The model is trained in two phases. A rhythm decoder (RD) is trained first, and subsequently a pitch decoder is trained by utilizing the pre-trained RD. Experimental results show that reusing RD at the pitch decoding stage and training with pitch varied rhythm data improve the performance. It was also observed that the samples produced by the model well reflected the key characteristics of dataset in terms of both pitch and rhythm related features, including chord tone ratio and rhythm distribution. Qualitative analysis reveals the model’s capability of generating various melodies in accordance with a given chord progression, as well as the presence of repetitions and variations within the generated melodies. With subjective human listening test, we come to a conclusion that the model was able to successfully produce new melodies that sound pleasant in terms of both rhythm and pitch (Source code available at https://github.com/ckycky3/CMT-pytorch).', 'corpus_id': 232372180, 'score': 1}, {'doc_id': '233296970', 'title': 'Visually Guided Sound Source Separation and Localization using Self-Supervised Motion Representations', 'abstract': 'The objective of this paper is to perform audio-visual sound source separation, i.e. to separate component audios from a mixture based on the videos of sound sources. Moreover, we aim to pinpoint the source location in the input video sequence. Recent works have shown impressive audio-visual separation results when using prior knowledge of the source type (e.g. human playing instrument) and pre-trained motion detectors (e.g. keypoints or optical flows). However, at the same time, the models are limited to a certain application domain. In this paper, we address these limitations and make the following contributions: i) we propose a two-stage architecture, called Appearance and Motion network (AMnet), where the stages specialise to appearance and motion cues, respectively. The entire system is trained in a self-supervised manner; ii) we introduce an Audio-Motion Embedding (AME) framework to explicitly represent the motions that related to sound; iii) we propose an audio-motion transformer architecture for audio and motion feature fusion; iv) we demonstrate state-of-the-art performance on two challenging datasets (MUSIC-21 and AVE) despite the fact that we do not use any pre-trained keypoint detectors or optical flow estimators. Project page: https://ly-zhu.github.io/self-supervisedmotion-representations.', 'corpus_id': 233296970, 'score': 0}, {'doc_id': '232046451', 'title': 'MixSpeech: Data Augmentation for Low-Resource Automatic Speech Recognition', 'abstract': 'In this paper, we propose MixSpeech, a simple yet effective data augmentation method based on mixup for automatic speech recognition (ASR). MixSpeech trains an ASR model by taking a weighted combination of two different speech features (e.g., mel-spectrograms or MFCC) as the input, and recognizing both text sequences, where the two recognition losses use the same combination weight. We apply MixSpeech on two popular end-to-end speech recognition models including LAS (Listen, Attend and Spell) and Transformer, and conduct experiments on several low-resource datasets including TIMIT, WSJ, and HKUST. Experimental results show that MixSpeech achieves better accuracy than the baseline models without data augmentation, and outperforms a strong data augmentation method SpecAugment on these recognition tasks. Specifically, MixSpeech outperforms SpecAugment with a relative PER improvement of 10.6% on TIMIT dataset, and achieves a strong WER of 4.7% on WSJ dataset.', 'corpus_id': 232046451, 'score': 0}]
53	{'doc_id': '324600', 'title': 'Active Learning Literature Survey', 'abstract': 'The key idea behind active learning is that a machine learning algorithm can achieve greater accuracy with fewer labeled training instances if it is allowed to choose the data from which is learns. An active learner may ask queries in the form of unlabeled instances to be labeled by an oracle (e.g., a human annotator). Active learning is well-motivated in many modern machine learning problems, where unlabeled data may be abundant but labels are difficult, time-consuming, or expensive to obtain. This report provides a general introduction to active learning and a survey of the literature. This includes a discussion of the scenarios in which queries can be formulated, and an overview of the query strategy frameworks proposed in the literature to date. An analysis of the empirical and theoretical evidence for active learning, a summary of several problem setting variants, and a discussion of related topics in machine learning research are also presented.', 'corpus_id': 324600}	3494	"[{'doc_id': '7806109', 'title': 'Support Vector Machine Active Learning with Applications to Text Classification', 'abstract': 'Support vector machines have met with significant success in numerous real-world learning tasks. However, like most machine learning algorithms, they are generally applied using a randomly selected training set classified in advance. In many settings, we also have the option of using pool-based active learning. Instead of using a randomly selected training set, the learner has access to a pool of unlabeled instances and can request the labels for some number of them. We introduce a new algorithm for performing active learning with support vector machines, i.e., an algorithm for choosing which instances to request next. We provide a theoretical motivation for the algorithm using the notion of a version space. We present experimental results showing that employing our active learning method can significantly reduce the need for labeled training instances in both the standard inductive and transductive settings.', 'corpus_id': 7806109, 'score': 1}, {'doc_id': '211133088', 'title': 'Reinforced active learning for image segmentation', 'abstract': 'Learning-based approaches for semantic segmentation have two inherent challenges. First, acquiring pixel-wise labels is expensive and time-consuming. Second, realistic segmentation datasets are highly unbalanced: some categories are much more abundant than others, biasing the performance to the most represented ones. In this paper, we are interested in focusing human labelling effort on a small subset of a larger pool of data, minimizing this effort while maximizing performance of a segmentation model on a hold-out set. We present a new active learning strategy for semantic segmentation based on deep reinforcement learning (RL). An agent learns a policy to select a subset of small informative image regions -- opposed to entire images -- to be labeled, from a pool of unlabeled data. The region selection decision is made based on predictions and uncertainties of the segmentation model being trained. Our method proposes a new modification of the deep Q-network (DQN) formulation for active learning, adapting it to the large-scale nature of semantic segmentation problems. We test the proof of concept in CamVid and provide results in the large-scale dataset Cityscapes. On Cityscapes, our deep RL region-based DQN approach requires roughly 30% less additional labeled data than our most competitive baseline to reach the same performance. Moreover, we find that our method asks for more labels of under-represented categories compared to the baselines, improving their performance and helping to mitigate class imbalance.', 'corpus_id': 211133088, 'score': 0}, {'doc_id': '214667381', 'title': 'Instance Credibility Inference for Few-Shot Learning', 'abstract': 'Few-shot learning (FSL) aims to recognize new objects with extremely limited training data for each category. Previous efforts are made by either leveraging meta-learning paradigm or novel principles in data augmentation to alleviate this extremely data-scarce problem. In contrast, this paper presents a simple statistical approach, dubbed Instance Credibility Inference (ICI) to exploit the distribution support of unlabeled instances for few-shot learning. Specifically, we first train a linear classifier with the labeled few-shot examples and use it to infer the pseudo-labels for the unlabeled data. To measure the credibility of each pseudo-labeled instance, we then propose to solve another linear regression hypothesis by increasing the sparsity of the incidental parameters and rank the pseudo-labeled instances with their sparsity degree. We select the most trustworthy pseudo-labeled instances alongside the labeled examples to re-train the linear classifier. This process is iterated until all the unlabeled samples are included in the expanded training set, i.e. the pseudo-label is converged for unlabeled data pool. Extensive experiments under two few-shot settings show that our simple approach can establish new state-of-the-arts on four widely used few-shot learning benchmark datasets including miniImageNet, tieredImageNet, CIFAR-FS, and CUB. Our code is available at: https://github.com/Yikai-Wang/ICI-FSL', 'corpus_id': 214667381, 'score': 0}, {'doc_id': '49741470', 'title': 'How transferable are the datasets collected by active learners?', 'abstract': 'Active learning is a widely-used training strategy for maximizing predictive performance subject to a fixed annotation budget. Between rounds of training, an active learner iteratively selects examples for annotation, typically based on some measure of the model\'s uncertainty, coupling the acquired dataset with the underlying model. However, owing to the high cost of annotation and the rapid pace of model development, labeled datasets may remain valuable long after a particular model is surpassed by new technology. In this paper, we investigate the transferability of datasets collected with an acquisition model A to a distinct successor model S. We seek to characterize whether the benefits of active learning persist when A and S are different models. To this end, we consider two standard NLP tasks and associated datasets: text classification and sequence tagging. We find that training S on a dataset actively acquired with a (different) model A typically yields worse performance than when S is trained with ""native"" data (i.e., acquired actively using S), and often performs worse than training on i.i.d. sampled data. These findings have implications for the use of active learning in practice,suggesting that it is better suited to cases where models are updated no more frequently than labeled data.', 'corpus_id': 49741470, 'score': 1}, {'doc_id': '211020634', 'title': 'Iterative Data Programming for Expanding Text Classification Corpora', 'abstract': 'Real-world text classification tasks often require many labeled training examples that are expensive to obtain. Recent advancements in machine teaching, specifically the data programming paradigm, facilitate the creation of training data sets quickly via a general framework for building weak models, also known as labeling functions, and denoising them through ensemble learning techniques. We present a fast, simple data programming method for augmenting text data sets by generating neighborhood-based weak models with minimal supervision. Furthermore, our method employs an iterative procedure to identify sparsely distributed examples from large volumes of unlabeled data. The iterative data programming techniques improve newer weak models as more labeled data is confirmed with human-in-loop. We show empirical results on sentence classification tasks, including those from a task of improving intent recognition in conversational agents.', 'corpus_id': 211020634, 'score': 0}, {'doc_id': '2754649', 'title': 'Active Discriminative Text Representation Learning', 'abstract': ""We propose a new active learning (AL) method for text classification with convolutional neural networks (CNNs). In AL, one selects the instances to be manually labeled with the aim of maximizing model performance with minimal effort. Neural models capitalize on word embeddings as representations (features), tuning these to the task at hand. We argue that AL strategies for multi-layered neural models should focus on selecting instances that most affect the embedding space (i.e., induce discriminative word representations). This is in contrast to traditional AL approaches (e.g., entropy-based uncertainty sampling), which specify higher level objectives. We propose a simple approach for sentence classification that selects instances containing words whose embeddings are likely to be updated with the greatest magnitude, thereby rapidly learning discriminative, task-specific embeddings. We extend this approach to document classification by jointly considering: (1) the expected changes to the constituent word representations; and (2) the model's current overall uncertainty regarding the instance. The relative emphasis placed on these criteria is governed by a stochastic process that favors selecting instances likely to improve representations at the outset of learning, and then shifts toward general uncertainty sampling as AL progresses. Empirical results show that our method outperforms baseline AL approaches on both sentence and document classification tasks. We also show that, as expected, the method quickly learns discriminative word embeddings. To the best of our knowledge, this is the first work on AL addressing neural models for text classification."", 'corpus_id': 2754649, 'score': 1}, {'doc_id': '215548017', 'title': 'Scalable Active Learning for Object Detection', 'abstract': 'Deep Neural Networks trained in a fully supervised fashion are the dominant technology in perception-based autonomous driving systems. While collecting large amounts of unlabeled data is already a major undertaking, only a subset of it can be labeled by humans due to the effort needed for high-quality annotation. Therefore, finding the right data to label has become a key challenge. Active learning is a powerful technique to improve data efficiency for supervised learning methods, as it aims at selecting the smallest possible training set to reach a required performance. We have built a scalable production system for active learning in the domain of autonomous driving. In this paper, we describe the resulting high-level design, sketch some of the challenges and their solutions, present our current results at scale, and briefly describe the open problems and future directions.', 'corpus_id': 215548017, 'score': 0}, {'doc_id': '157059801', 'title': 'Teaching a black-box learner', 'abstract': 'One widely-studied model of teaching (Goldman & Kearns, 1995; Shinohara & Miyano, 1991; Anthony et al., 1992) calls for a teacher to provide the minimal set of labeled examples that uniquely specifies a target concept. The assumption is that the teacher knows the learner’s hypothesis class, which is often not true of real-life teaching scenarios. We consider the problem of teaching a learner whose representation and hypothesis class are unknown: that is, the learner is a black box. We find that a teacher who does not interact with the learner can do no better than providing random examples. However, by interacting with the black-box learner, a teacher can efficiently find a set of teaching examples that is a provably good approximation to the optimal set. As an illustration, we show how this scheme can be used to shrink training sets for any family of classifiers: that is, to find an approximatelyminimal subset of training instances that yields the same classifier as the entire set.', 'corpus_id': 157059801, 'score': 1}, {'doc_id': '210965980', 'title': 'A Graph-Based Approach for Active Learning in Regression', 'abstract': 'Active learning aims to reduce labeling efforts by selectively asking humans to annotate the most important data points from an unlabeled pool and is an example of human-machine interaction. Though active learning has been extensively researched for classification and ranking problems, it is relatively understudied for regression problems. Most existing active learning for regression methods use the regression function learned at each active learning iteration to select the next informative point to query. This introduces several challenges such as handling noisy labels, parameter uncertainty and overcoming initially biased training data. Instead, we propose a feature-focused approach that formulates both sequential and batch-mode active regression as a novel bipartite graph optimization problem. We conduct experiments on both noise-free and noisy settings. Our experimental results on benchmark data sets demonstrate the effectiveness of our proposed approach.', 'corpus_id': 210965980, 'score': 1}, {'doc_id': '214727875', 'title': 'Neural Networks Are More Productive Teachers Than Human Raters: Active Mixup for Data-Efficient Knowledge Distillation From a Blackbox Model', 'abstract': 'We study how to train a student deep neural network for visual recognition by distilling knowledge from a blackbox teacher model in a data-efficient manner. Progress on this problem can significantly reduce the dependence on large-scale datasets for learning high-performing visual recognition models. There are two major challenges. One is that the number of queries into the teacher model should be minimized to save computational and/or financial costs. The other is that the number of images used for the knowledge distillation should be small; otherwise, it violates our expectation of reducing the dependence on large-scale datasets. To tackle these challenges, we propose an approach that blends mixup and active learning. The former effectively augments the few unlabeled images by a big pool of synthetic images sampled from the convex hull of the original images, and the latter actively chooses from the pool hard examples for the student neural network and query their labels from the teacher model. We validate our approach with extensive experiments.', 'corpus_id': 214727875, 'score': 0}]"
54	{'doc_id': '214769387', 'title': 'Bigtable: A Distributed Storage System for Structured Data', 'abstract': 'Bigtable is a distributed storage system for managing structured data that is designed to scale to a very large size: petabytes of data across thousands of commodity servers. Many projects at Google store data in Bigtable, including web indexing, Google Earth, and Google Finance. These applications place very different demands on Bigtable, both in terms of data size (from URLs to web pages to satellite imagery) and latency requirements (from backend bulk processing to real-time data serving). Despite these varied demands, Bigtable has successfully provided a flexible, high-performance solution for all of these Google products. In this article, we describe the simple data model provided by Bigtable, which gives clients dynamic control over data layout and format, and we describe the design and implementation of Bigtable.', 'corpus_id': 214769387}	9088	"[{'doc_id': '222067118', 'title': 'Persistent Data Retention Models', 'abstract': 'Non-Volatile Memory devices may soon be a part of main memory, and programming models that give programmers direct access to persistent memory through loads and stores are sought to maximize the performance benefits of these new devices. Direct access introduces new challenges. In this work, we identify an important aspect of programming for persistent memory: the persistent data retention model. \nA Persistent Data Retention Model describes what happens to persistent data when code that uses it is modified. We identify two models present in prior work but not described as such, the Reset and Manual Model, and we propose a new one called the Automatic Model. The Reset model discards all persistent data when a program changes leading to performance overheads and write amplification. In contrast, if data is to be retained, the Manual Model relies on the programmer to implement code that upgrades data from one version of the program to the next. This reduces overheads but places a larger burden on the programmer. \nWe propose the Automatic Model to assist a programmer by automating some or all of the conversion. We describe one such automatic approach, Lazily Extendible Data Structures, that uses language extensions and compiler support to reduce the effort and complexity associated with updating persistent data. We evaluate our PDRMs in the context of the Persistent Memory Development Kit (PMDK) using kernels and the TPC-C application. Manual Model shows an overhead of 2.90% to 4.10% on average, and LEDS shows overhead of 0.45% to 10.27% on average, depending on the workload. LEDS reduces the number of writes by 26.36% compared to Manual Model. Furthermore, LEDS significantly reduces the programming complexity by relying on the compiler to migrate persistent data.', 'corpus_id': 222067118, 'score': 1}, {'doc_id': '220686636', 'title': 'DBOS: A Proposal for a Data-Centric Operating System', 'abstract': ""Current operating systems are complex systems that were designed before today's computing environments. This makes it difficult for them to meet the scalability, heterogeneity, availability, and security challenges in current cloud and parallel computing environments. To address these problems, we propose a radically new OS design based on data-centric architecture: all operating system state should be represented uniformly as database tables, and operations on this state should be made via queries from otherwise stateless tasks. This design makes it easy to scale and evolve the OS without whole-system refactoring, inspect and debug system state, upgrade components without downtime, manage decisions using machine learning, and implement sophisticated security features. We discuss how a database OS (DBOS) can improve the programmability and performance of many of today's most important applications and propose a plan for the development of a DBOS proof of concept."", 'corpus_id': 220686636, 'score': 1}, {'doc_id': '221516566', 'title': 'Design and Evaluation of a Simple Data Interface for Efficient Data Transfer across Diverse Storage', 'abstract': 'Modern science and engineering computing environments often feature storage systems of different types, from parallel file systems in high-performance computing centers to object stores operated by cloud providers. To enable easy, reliable, secure, and performant data exchange among these different systems, we propose Connector, a plug-able data access architecture for diverse, distributed storage. By abstracting low-level storage system details, this abstraction permits a managed data transfer service (Globus, in our case) to interact with a large and easily extended set of storage systems. Equally important, it supports third-party transfers: that is, direct data transfers from source to destination that are initiated by a third-party client but do not engage that third party in the data path. The abstraction also enables management of transfers for performance optimization, error handling, and end-to-end integrity. We present the Connector design, describe implementations for different storage services, evaluate tradeoffs inherent in managed vs. direct transfers, motivate recommended deployment options, and propose a model-based method that allows for easy characterization of performance in different contexts without exhaustive benchmarking.', 'corpus_id': 221516566, 'score': 1}, {'doc_id': '220936226', 'title': 'A Survey on the Evolution of Stream Processing Systems', 'abstract': ""Stream processing has been an active research field for more than 20 years, but it is now witnessing its prime time due to recent successful efforts by the research community and numerous worldwide open-source communities. This survey provides a comprehensive overview of fundamental aspects of stream processing systems and their evolution in the functional areas of out-of-order data management, state management, fault tolerance, high availability, load management, elasticity, and reconfiguration. We review noteworthy past research findings, outline the similarities and differences between early ('00-'10) and modern ('11-'18) streaming systems, and discuss recent trends and open problems."", 'corpus_id': 220936226, 'score': 0}, {'doc_id': '221970585', 'title': 'A Big Data Lake for Multilevel Streaming Analytics', 'abstract': 'Large organizations are seeking to create new architectures and scalable platforms to effectively handle data management challenges due to the explosive nature of data rarely seen in the past. These data management challenges are largely posed by the availability of streaming data at high velocity from various sources in multiple formats. The changes in data paradigm have led to the emergence of new data analytics and management architecture. This paper focuses on storing high volume, velocity and variety data in the raw formats in a data storage architecture called a data lake. First, we present our study on the limitations of traditional data warehouses in handling recent changes in data paradigms. We discuss and compare different open source and commercial platforms that can be used to develop a data lake. We then describe our end-to-end data lake design and implementation approach using the Hadoop Distributed File System (HDFS) on the Hadoop Data Platform (HDP). Finally, we present a real-world data lake development use case for data stream ingestion, staging, and multilevel streaming analytics which combines structured and unstructured data. This study can serve as a guide for individuals or organizations planning to implement a data lake solution for their use cases.', 'corpus_id': 221970585, 'score': 0}, {'doc_id': '221818607', 'title': 'A FaaS File System for Serverless Computing', 'abstract': 'Serverless computing with cloud functions is quickly gaining adoption, but constrains programmers with its limited support for state management. We introduce a shared file system for cloud functions. It offers familiar POSIX semantics while taking advantage of distinctive aspects of cloud functions to achieve scalability and performance beyond what traditional shared file systems can offer. We take advantage of the function-grained fault tolerance model of cloud functions to proceed optimistically using local state, safe in the knowledge that we can restart if cache reads or lock activity cannot be reconciled upon commit. The boundaries of cloud functions provide implicit commit and rollback points, giving us the flexibility to use transaction processing techniques without changing the programming model or API. This allows a variety of stateful sever-based applications to benefit from the simplicity and scalability of serverless computing, often with little or no modification.', 'corpus_id': 221818607, 'score': 1}, {'doc_id': '220919727', 'title': 'The Impact of Distance on Performance and Scalability of Distributed Database Systems in Hybrid Clouds', 'abstract': 'The increasing need for managing big data has led the emergence of advanced database management systems. There has been increased efforts aimed at evaluating the performance and scalability of NoSQL and Relational databases hosted by either private or public cloud datacenters. However, there has been little work on evaluating the performance and scalability of these databases in hybrid clouds, where the distance between private and public cloud datacenters can be one of the key factors that can affect their performance. Hence, in this paper, we present a detailed evaluation of throughput, scalability, and VMs size vs. VMs number for six modern databases in a hybrid cloud, consisting of a private cloud in Adelaide and Azure based datacenter in Sydney, Mumbai, and Virginia regions. Based on results, as the distance between private and public clouds increases, the throughput performance of most databases reduces. Second, MongoDB obtains the best throughput performance, followed by MySQL C luster, whilst Cassandra exposes the most fluctuation in through performance. Third, vertical scalability improves the throughput of databases more than the horizontal scalability. Forth, exploiting bigger VMs rather than more VMs with less cores can increase throughput performance for Cassandra, Riak, and Redis.', 'corpus_id': 220919727, 'score': 0}, {'doc_id': '221376736', 'title': 'Power and Performance Analysis of Persistent Key-Value Stores', 'abstract': 'With the current rate of data growth, processing needs are becoming difficult to fulfill due to CPU power and energy limitations. Data serving systems and especially persistent key-value stores have become a substantial part of data processing stacks in the data center, providing access to massive amounts of data for applications and services. Key-value stores exhibit high CPU and I/O overheads because of their constant need to reorganize data on the devices. In this paper, we examine the efficiency of two key-value stores on four servers of different generations and with different CPU architectures. We use RocksDB, a key-value that is deployed widely, e.g. in Facebook, and Kreon, a research key-value store that has been designed to reduce CPU overhead. We evaluate their behavior and overheads on an ARM-based microserver and three different generations of x86 servers. Our findings show that microservers have better power efficiency in the range of 0.68-3.6x with a comparable tail latency.', 'corpus_id': 221376736, 'score': 1}, {'doc_id': '235355885', 'title': 'Viper: An Efficient Hybrid PMem-DRAM Key-Value Store', 'abstract': 'Key-value stores (KVSs) have found wide application in modern software systems. For persistence, their data resides in slow secondary storage, which requires KVSs to employ various techniques to increase their read and write performance from and to the underlying medium. Emerging persistent memory (PMem) technologies offer data persistence at close-to-DRAM speed, making them a promising alternative to classical disk-based storage. However, simply drop-in replacing existing storage with PMem does not yield good results, as block-based access behaves differently in PMem than on disk and ignores PMem’s byte addressability, layout, and unique performance characteristics. In this paper, we propose three PMem-specific access patterns and implement them in a hybrid PMem-DRAM KVS called Viper. We employ a DRAM-based hash index and a PMem-aware storage layout to utilize the randomwrite speed of DRAM and efficient sequential-write performance PMem. Our evaluation shows that Viper significantly outperforms existing KVSs for core KVS operations while providing full data persistence. Moreover, Viper outperforms existing PMem-only, hybrid, and disk-based KVSs by 4–18x for write workloads, while matching or surpassing their get performance. PVLDB Reference Format: Lawrence Benson, Hendrik Makait, Tilmann Rabl. Viper: An Efficient Hybrid PMem-DRAM Key-Value Store. PVLDB, 14(9): 1544 1556, 2021. doi:10.14778/3461535.3461543 PVLDB Artifact Availability: The source code, data, and/or other artifacts have been made available at https://github.com/hpides/viper.', 'corpus_id': 235355885, 'score': 1}, {'doc_id': '221150551', 'title': 'Dependability Evaluation of Middleware Technology for Large-scale Distributed Caching', 'abstract': 'Distributed caching systems (e.g., Memcached) are widely used by service providers to satisfy accesses by millions of concurrent clients. Given their large-scale, modern distributed systems rely on a middleware layer to manage caching nodes, to make applications easier to develop, and to apply load balancing and replication strategies. In this work, we performed a dependability evaluation of three popular middleware platforms, namely Twemproxy by Twitter, Mcrouter by Facebook, and Dynomite by Netflix, to assess availability and performance under faults, including failures of Memcached nodes and congestion due to unbalanced workloads and network link bandwidth bottlenecks. We point out the different availability and performance trade-offs achieved by the three platforms, and scenarios in which few faulty components cause cascading failures of the whole distributed system.', 'corpus_id': 221150551, 'score': 0}]"
55	"{'doc_id': '221095574', 'title': 'GANBERT: Generative Adversarial Networks with Bidirectional Encoder Representations from Transformers for MRI to PET synthesis', 'abstract': 'Synthesizing medical images, such as PET, is a challenging task due to the fact that the intensity range is much wider and denser than those in photographs and digital renderings and are often heavily biased toward zero. Above all, intensity values in PET have absolute significance, and are used to compute parameters that are reproducible across the population. Yet, usually much manual adjustment has to be made in pre-/post- processing when synthesizing PET images, because its intensity ranges can vary a lot, e.g., between -100 to 1000 in floating point values. To overcome these challenges, we adopt the Bidirectional Encoder Representations from Transformers (BERT) algorithm that has had great success in natural language processing (NLP), where wide-range floating point intensity values are represented as integers ranging between 0 to 10000 that resemble a dictionary of natural language vocabularies. BERT is then trained to predict a proportion of masked values images, where its ""next sentence prediction (NSP)"" acts as GAN discriminator. Our proposed approach, is able to generate PET images from MRI images in wide intensity range, with no manual adjustments in pre-/post- processing. It is a method that can scale and ready to deploy.', 'corpus_id': 221095574}"	12457	"[{'doc_id': '229924266', 'title': 'FREA-Unet: Frequency-aware U-net for Modality Transfer', 'abstract': ""While Positron emission tomography (PET) imaging has been widely used in diagnosis of number of diseases, it has costly acquisition process which involves radiation exposure to patients. However, magnetic resonance imaging (MRI) is a safer imaging modality that does not involve patient's exposure to radiation. Therefore, a need exists for an efficient and automated PET image generation from MRI data. In this paper, we propose a new frequency-aware attention U-net for generating synthetic PET images. Specifically, we incorporate attention mechanism into different U-net layers responsible for estimating low/high frequency scales of the image. Our frequency-aware attention Unet computes the attention scores for feature maps in low/high frequency layers and use it to help the model focus more on the most important regions, leading to more realistic output images. Experimental results on 30 subjects from Alzheimers Disease Neuroimaging Initiative (ADNI) dataset demonstrate good performance of the proposed model in PET image synthesis that achieved superior performance, both qualitative and quantitative, over current state-of-the-arts."", 'corpus_id': 229924266, 'score': 0}, {'doc_id': '229923960', 'title': 'Generative Adversarial Network for Image Synthesis', 'abstract': 'This chapter reviews recent developments of generative adversarial networks (GAN)-based methods for medical and biomedical image synthesis tasks. These methods are classified into conditional GAN and Cycle-GAN according to the network architecture designs. For each category, a literature survey is given, which covers discussions of the network architecture designs, highlights important contributions and identifies specific challenges. keywords: Image synthesis, deep learning, Generative Adversarial Network, GAN.', 'corpus_id': 229923960, 'score': 0}, {'doc_id': '223957158', 'title': 'Coarse-to-Fine Pre-training for Named Entity Recognition', 'abstract': 'More recently, Named Entity Recognition hasachieved great advances aided by pre-trainingapproaches such as BERT. However, currentpre-training techniques focus on building lan-guage modeling objectives to learn a gen-eral representation, ignoring the named entity-related knowledge. To this end, we proposea NER-specific pre-training framework to in-ject coarse-to-fine automatically mined entityknowledge into pre-trained models. Specifi-cally, we first warm-up the model via an en-tity span identification task by training it withWikipedia anchors, which can be deemed asgeneral-typed entities. Then we leverage thegazetteer-based distant supervision strategy totrain the model extract coarse-grained typedentities. Finally, we devise a self-supervisedauxiliary task to mine the fine-grained namedentity knowledge via clustering.Empiricalstudies on three public NER datasets demon-strate that our framework achieves significantimprovements against several pre-trained base-lines, establishing the new state-of-the-art per-formance on three benchmarks. Besides, weshow that our framework gains promising re-sults without using human-labeled trainingdata, demonstrating its effectiveness in label-few and low-resource scenarios', 'corpus_id': 223957158, 'score': 1}, {'doc_id': '227151785', 'title': 'Domain-Transferable Method for Named Entity Recognition Task', 'abstract': 'Named Entity Recognition (NER) is a fundamental task in the fields of natural language processing and information extraction. NER has been widely used as a standalone tool or an essential component in a variety of applications such as question answering, dialogue assistants and knowledge graphs development. However, training reliable NER models requires a large amount of labelled data which is expensive to obtain, particularly in specialized domains. This paper describes a method to learn a domain-specific NER model for an arbitrary set of named entities when domain-specific supervision is not available. We assume that the supervision can be obtained with no human effort, and neural models can learn from each other. The code, data and models are publicly available.', 'corpus_id': 227151785, 'score': 1}, {'doc_id': '226476611', 'title': 'DUG-RECON: A Framework for Direct Image Reconstruction Using Convolutional Generative Networks', 'abstract': 'This article explores convolutional generative networks as an alternative to iterative reconstruction algorithms in medical image reconstruction. The task of medical image reconstruction involves mapping of projection domain data collected from the detector to the image domain. This mapping is done typically through iterative reconstruction algorithms which are time consuming and computationally expensive. Trained deep learning networks provide faster outputs as proven in various tasks across computer vision. In this work, we propose a direct reconstruction framework exclusively with deep learning architectures. The proposed framework consists of three segments, namely, denoising, reconstruction, and super resolution (SR). The denoising and the SR segments act as processing steps. The reconstruction segment consists of a novel double U-Net generator (DUG) which learns the sinogram-to-image transformation. This entire network was trained on positron emission tomography (PET) and computed tomography (CT) images. The reconstruction framework approximates 2-D mapping from the projection domain to the image domain. The architecture proposed in this proof-of-concept work is a novel approach to direct image reconstruction; further improvement is required to implement it in a clinical setting.', 'corpus_id': 226476611, 'score': 0}, {'doc_id': '226262399', 'title': 'Entity Enhanced BERT Pre-training for Chinese NER', 'abstract': 'Character-level BERT pre-trained in Chinese suffers a limitation of lacking lexicon information, which shows effectiveness for Chinese NER. To integrate the lexicon into pre-trained LMs for Chinese NER, we investigate a semi-supervised entity enhanced BERT pre-training method. In particular, we first extract an entity lexicon from the relevant raw text using a new-word discovery method. We then integrate the entity information into BERT using Char-Entity-Transformer, which augments the self-attention using a combination of character and entity representations. In addition, an entity classification task helps inject the entity information into model parameters in pre-training. The pre-trained models are used for NER fine-tuning. Experiments on a news dataset and two datasets annotated by ourselves for NER in long-text show that our method is highly effective and achieves the best results.', 'corpus_id': 226262399, 'score': 1}, {'doc_id': '229923303', 'title': 'Few-Shot Named Entity Recognition: A Comprehensive Study', 'abstract': 'This paper presents a comprehensive study to efficiently build named entity recognition (NER) systems when a small number of indomain labeled data is available. Based upon recent Transformer-based self-supervised pretrained language models (PLMs), we investigate three orthogonal schemes to improve the model generalization ability for few-shot settings: (1) meta-learning to construct prototypes for different entity types, (2) supervised pre-training on noisy web data to extract entity-related generic representations and (3) self-training to leverage unlabeled in-domain data. Different combinations of these schemes are also considered. We perform extensive empirical comparisons on 10 public NER datasets with various proportions of labeled data, suggesting useful insights for future research. Our experiments show that (i) in the few-shot learning setting, the proposed NER schemes significantly improve or outperform the commonly used baseline, a PLM-based linear classifier fine-tuned on domain labels. (ii) We create new state-of-the-art results on both few-shot and training-free settings compared with existing methods. We will release our code and pretrained models for reproducible research.', 'corpus_id': 229923303, 'score': 1}, {'doc_id': '227231779', 'title': 'A Semi-Supervised BERT Approach for Arabic Named Entity Recognition', 'abstract': 'Named entity recognition (NER) plays a significant role in many applications such as information extraction, information retrieval, question answering, and even machine translation. Most of the work on NER using deep learning was done for non-Arabic languages like English and French, and only few studies focused on Arabic. This paper proposes a semi-supervised learning approach to train a BERT-based NER model using labeled and semi-labeled datasets. We compared our approach against various baselines, and state-of-the-art Arabic NER tools on three datasets: AQMAR, NEWS, and TWEETS. We report a significant improvement in F-measure for the AQMAR and the NEWS datasets, which are written in Modern Standard Arabic (MSA), and competitive results for the TWEETS dataset, which contains tweets that are mostly in the Egyptian dialect and contain many mistakes or misspellings.', 'corpus_id': 227231779, 'score': 1}, {'doc_id': '228063930', 'title': 'You Only Need Adversarial Supervision for Semantic Image Synthesis', 'abstract': 'Despite their recent successes, GAN models for semantic image synthesis still suffer from poor image quality when trained with only adversarial supervision. Historically, additionally employing the VGG-based perceptual loss has helped to overcome this issue, significantly improving the synthesis quality, but at the same time limiting the progress of GAN models for semantic image synthesis. In this work, we propose a novel, simplified GAN model, which needs only adversarial supervision to achieve high quality results. We re-design the discriminator as a semantic segmentation network, directly using the given semantic label maps as the ground truth for training. By providing stronger supervision to the discriminator as well as to the generator through spatially- and semantically-aware discriminator feedback, we are able to synthesize images of higher fidelity with better alignment to their input label maps, making the use of the perceptual loss superfluous. Moreover, we enable high-quality multi-modal image synthesis through global and local sampling of a 3D noise tensor injected into the generator, which allows complete or partial image change. We show that images synthesized by our model are more diverse and follow the color and texture distributions of real images more closely. We achieve an average improvement of $6$ FID and $5$ mIoU points over the state of the art across different datasets using only adversarial supervision.', 'corpus_id': 228063930, 'score': 0}, {'doc_id': '226965681', 'title': 'SAG-GAN: Semi-Supervised Attention-Guided GANs for Data Augmentation on Medical Images', 'abstract': 'Recently deep learning methods, in particular, convolutional neural networks (CNNs), have led to a massive breakthrough in the range of computer vision. Also, the large-scale annotated dataset is the essential key to a successful training procedure. However, it is a huge challenge to get such datasets in the medical domain. Towards this, we present a data augmentation method for generating synthetic medical images using cycle-consistency Generative Adversarial Networks (GANs). We add semi-supervised attention modules to generate images with convincing details. We treat tumor images and normal images as two domains. The proposed GANs-based model can generate a tumor image from a normal image, and in turn, it can also generate a normal image from a tumor image. Furthermore, we show that generated medical images can be used for improving the performance of ResNet18 for medical image classification. Our model is applied to three limited datasets of tumor MRI images. We first generate MRI images on limited datasets, then we trained three popular classification models to get the best model for tumor classification. Finally, we train the classification model using real images with classic data augmentation methods and classification models using synthetic images. The classification results between those trained models showed that the proposed SAG-GAN data augmentation method can boost Accuracy and AUC compare with classic data augmentation methods. We believe the proposed data augmentation method can apply to other medical image domains, and improve the accuracy of computer-assisted diagnosis.', 'corpus_id': 226965681, 'score': 0}]"
56	{'doc_id': '29454141', 'title': 'A novel 3D shape descriptor for automatic retrieval of anatomical structures from medical images', 'abstract': 'Content-based image retrieval (CBIR) aims at retrieving from a database objects that are similar to an object provided by a query, by taking into consideration a set of extracted features. While CBIR has been widely applied in the two-dimensional image domain, the retrieval of3D objects from medical image datasets using CBIR remains to be explored. In this context, the development of descriptors that can capture information specific to organs or structures is desirable. In this work, we focus on the retrieval of two anatomical structures commonly imaged by Magnetic Resonance Imaging (MRI) and Computed Tomography (CT) techniques, the left ventricle of the heart and blood vessels. Towards this aim, we developed the Area-Distance Local Descriptor (ADLD), a novel 3D local shape descriptor that employs mesh geometry information, namely facet area and distance from centroid to surface, to identify shape changes. Because ADLD only considers surface meshes extracted from volumetric medical images, it substantially diminishes the amount of data to be analyzed. A 90% precision rate was obtained when retrieving both convex (left ventricle) and non-convex structures (blood vessels), allowing for detection of abnormalities associated with changes in shape. Thus, ADLD has the potential to aid in the diagnosis of a wide range of vascular and cardiac diseases.', 'corpus_id': 29454141}	12683	[{'doc_id': '230799369', 'title': 'Efficient 3D Point Cloud Feature Learning for Large-Scale Place Recognition', 'abstract': 'Point cloud based retrieval for place recognition is still a challenging problem due to drastic appearance and illumination changes of scenes in changing environments. Existing deep learning based global descriptors for the retrieval task usually consume a large amount of computation resources (e.g., memory), which may not be suitable for the cases of limited hardware resources. In this paper, we develop an efficient point cloud learning network (EPC-Net) to form a global descriptor for visual place recognition, which can obtain good performance and reduce computation memory and inference time. First, we propose a lightweight but effective neural network module, called ProxyConv, to aggregate the local geometric features of point clouds. We leverage the spatial adjacent matrix and proxy points to simplify the original edge convolution for lower memory consumption. Then, we design a lightweight grouped VLAD network (G-VLAD) to form global descriptors for retrieval. Compared with the original VLAD network, we propose a grouped fully connected (GFC) layer to decompose the high-dimensional vectors into a group of low-dimensional vectors, which can reduce the number of parameters of the network and maintain the discrimination of the feature vector. Finally, to further reduce the inference time, we develop a simple version of EPC-Net, called EPC-Net-L, which consists of two ProxyConv modules and one max pooling layer to aggregate global descriptors. By distilling the knowledge from EPC-Net, EPC-Net-L can obtain discriminative global descriptors for retrieval. Extensive experiments on the Oxford dataset and three in-house datasets demonstrate that our proposed method can achieve state-of-the-art performance with lower parameters, FLOPs, and runtime per frame. Our code is available at https://github.com/fpthink/EPC-Net.', 'corpus_id': 230799369, 'score': 0}, {'doc_id': '231847235', 'title': 'An Efficient Framework for Zero-Shot Sketch-Based Image Retrieval', 'abstract': 'Recently, Zero-shot Sketch-based Image Retrieval (ZS-SBIR) has attracted the attention of the computer vision community due to it’s real-world applications, and the more realistic and challenging setting than found in SBIR. ZS-SBIR inherits the main challenges of multiple computer vision problems including contentbased Image Retrieval (CBIR), zero-shot learning and domain adaptation. The majority of previous studies using deep neural networks have achieved improved results through either projecting sketch and images into a common low-dimensional space or transferring knowledge from seen to unseen classes. However, those approaches are trained with complex frameworks composed of multiple deep convolutional neural networks (CNNs) and are dependent on category-level word labels. This increases the requirements on training resources and datasets. In comparison, we propose a simple and efficient framework that does not require high computational training resources, and can be trained on datasets without semantic categorical labels. Furthermore, at training and inference stages our method only uses a single CNN. In this work, a pre-trained ImageNet CNN (i.e.ResNet50) is fine-tuned with three proposed learning objects: domain-aware quadruplet loss, semantic classification loss, and semantic knowledge preservation loss. The domain-aware quadruplet and semantic classification losses are introduced to learn discriminative, semantic and domain invariant features through considering ZS-SBIR as a object detection and verification problem. To preserve semantic knowledge learned with ImageNet and utilise it on unseen categories, the semantic knowledge preservation loss is proposed. To reduce computational cost and increase the accuracy of the semantic knowledge distillation process, ground-truth semantic knowledge is prepared in a class-oriented fashion prior to training. Extensive experiments are conducted on three challenging ZS-SBIR datasets, Sketchy Extended, TU-Berlin Extended and QuickDraw Extended. The proposed method achieves state-of-the-art results, and outperforms the majority of related works by a large margin.', 'corpus_id': 231847235, 'score': 0}, {'doc_id': '232320804', 'title': 'Decomposing Normal and Abnormal Features of Medical Images into Discrete Latent Codes for Content-Based Image Retrieval', 'abstract': 'In medical imaging, the characteristics purely derived from a disease should reflect the extent to which abnormal findings deviate from the normal features. Indeed, physicians often need corresponding images without abnormal findings of interest or, conversely, images that contain similar abnormal findings regardless of normal anatomical context. This is called comparative diagnostic reading of medical images, which is essential for a correct diagnosis. To support comparative diagnostic reading, content-based image retrieval (CBIR), which can selectively utilize normal and abnormal features in medical images as two separable semantic components, will be useful. Therefore, we propose a neural network architecture to decompose the semantic components of medical images into two latent codes: normal anatomy code and abnormal anatomy code. The normal anatomy code represents normal anatomies that should have existed if the sample is healthy, whereas the abnormal anatomy code attributes to abnormal changes that reflect deviation from the normal baseline. These latent codes are discretized through vector quantization to enable binary hashing, which can reduce the computational burden at the time of similarity search. By calculating the similarity based on either normal or abnormal anatomy codes or the combination of the two codes, our algorithm can retrieve images according to the selected semantic component from a dataset consisting of brain magnetic resonance images of gliomas. Our CBIR system qualitatively and quantitatively achieves remarkable results.', 'corpus_id': 232320804, 'score': 1}, {'doc_id': '232320725', 'title': 'IAIA-BL: A Case-based Interpretable Deep Learning Model for Classification of Mass Lesions in Digital Mammography', 'abstract': 'Interpretability in machine learning models is important in high-stakes decisions, such as whether to order a biopsy based on a mammographic exam. Mammography poses important challenges that are not present in other computer vision tasks: datasets are small, confounding information is present, and it can be difficult even for a radiologist to decide between watchful waiting and biopsy based on a mammogram alone. In this work, we present a framework for interpretable machine learning-based mammography. In addition to predicting whether a lesion is malignant or benign, our work aims to follow the reasoning processes of radiologists in detecting clinically relevant semantic features of each image, such as the characteristics of the mass margins. The framework includes a novel interpretable neural network algorithm that uses casebased reasoning for mammography. Our algorithm can incorporate a combination of data with whole image labelling and data with pixel-wise annotations, leading to better accuracy and interpretability even with a small number of images. Our interpretable models are able to highlight the classification-relevant parts of the image, whereas other methods highlight healthy tissue and confounding information. Our models are decision aids, rather than decision makers, aimed at better overall human-machine collaboration. We do not observe a loss in mass margin classification accuracy over a black box neural network trained on the same data.', 'corpus_id': 232320725, 'score': 0}, {'doc_id': '713532', 'title': 'A fully automatic end-to-end method for content-based image retrieval of CT scans with similar liver lesion annotations', 'abstract': 'PurposeThe goal of medical content-based image retrieval (M-CBIR) is to assist radiologists in the decision-making process by retrieving medical cases similar to a given image. One of the key interests of radiologists is lesions and their annotations, since the patient treatment depends on the lesion diagnosis. Therefore, a key feature of M-CBIR systems is the retrieval of scans with the most similar lesion annotations. To be of value, M-CBIR systems should be fully automatic to handle large case databases.MethodsWe present a fully automatic end-to-end method for the retrieval of CT scans with similar liver lesion annotations. The input is a database of abdominal CT scans labeled with liver lesions, a query CT scan, and optionally one radiologist-specified lesion annotation of interest. The output is an ordered list of the database CT scans with the most similar liver lesion annotations. The method starts by automatically segmenting the liver in the scan. It then extracts a histogram-based features vector from the segmented region, learns the features’ relative importance, and ranks the database scans according to the relative importance measure. The main advantages of our method are that it fully automates the end-to-end querying process, that it uses simple and efficient techniques that are scalable to large datasets, and that it produces quality retrieval results using an unannotated CT scan.ResultsOur experimental results on 9 CT queries on a dataset of 41 volumetric CT scans from the 2014 Image CLEF Liver Annotation Task yield an average retrieval accuracy (Normalized Discounted Cumulative Gain index) of 0.77 and 0.84 without/with annotation, respectively.ConclusionsFully automatic end-to-end retrieval of similar cases based on image information alone, rather that on disease diagnosis, may help radiologists to better diagnose liver lesions.', 'corpus_id': 713532, 'score': 1}, {'doc_id': '232167433', 'title': 'MEDICAL IMAGE RETRIEVAL BY CONTENT AND KEYWORD IN A ON-LINE HEALTH-CATALOGUE CONTEXT', 'abstract': 'In this paper we present keyword and content-based medical image retrieval approaches. Our primary goal is to measure the relevance of our automatic medical image indexing process, which provides us with two signatures: numerical and symbolical. For indexing and retrieval purposes a medical image database, containing six medical modalities (i.e. angiography, ultrasonography, magnetic resonance imaging, standard radiography, computer tomography, and scintigraphy), was created by a medical specialist, from a real healthcare environment. This database was used as a test platform for image feature extraction and modality classification during the indexing stage, and then for performance evaluation during the image retrieval stage. The content-based retrieval showed decent performance, with an average precision of 51% within the 5, 10 or 20 best matches (i.e. most similar to the query image, by a given metric). The modality keyword-based retrieval process yields 88% for both average precision and recall, when an SVM classifier was used to automatically determine the symbolic signatures (i.e. modality class) of medical images. Using the modality information represented by the symbolical signature, allows better adapted image retrieval approaches in a large online medical context.', 'corpus_id': 232167433, 'score': 0}, {'doc_id': '232372659', 'title': 'These do not Look Like Those: An Interpretable Deep Learning Model for Image Recognition', 'abstract': 'Interpretation of the reasoning process of a prediction made by a deep learning model is always desired. However, when it comes to the predictions of a deep learning model that directly impacts on the lives of people then the interpretation becomes a necessity. In this paper, we introduce a deep learning model: negative-positive prototypical part network (NP-ProtoPNet). This model attempts to imitate human reasoning for image recognition while comparing the parts of a test image with the corresponding parts of the images from known classes. We demonstrate our model on the dataset of chest $X$ -ray images of Covid-19 patients, pneumonia patients and normal people. The accuracy and precision that our model receives is on par with the best performing non-interpretable deep learning models.', 'corpus_id': 232372659, 'score': 1}, {'doc_id': '208617390', 'title': 'Efficient feature embedding of 3D brain MRI images for content-based image retrieval with deep metric learning', 'abstract': 'Increasing numbers of MRI brain scans, improvements in image resolution, and advancements in MRI acquisition technology are causing significant increases in the demand for and burden on radiologists’ efforts in terms of reading and interpreting brain MRIs. Content-based image retrieval (CBIR) is an emerging technology for reducing this burden by supporting the reading of medical images. High dimensionality is a major challenge in developing a CBIR system that is applicable for 3D brain MRIs. In this study, we propose a system called disease-oriented data concentration with metric learning (DDCML). In DDCML, we introduce deep metric learning to a 3D convolutional autoencoder (CAE). Our proposed DDCML scheme achieves a high dimensional compression rate (4096:1) while preserving the disease-related anatomical features that are important for medical image classification. The low-dimensional representation obtained by DDCML improved the clustering performance by 29.1% compared to plain 3D-CAE in terms of discriminating Alzheimer’s disease patients from healthy subjects, and successfully reproduced the relationships of the severity of disease categories that were not included in the training.', 'corpus_id': 208617390, 'score': 1}, {'doc_id': '232304125', 'title': 'Integration of CNN, CBMIR, and Visualization Techniques for Diagnosis and Quantification of Covid-19 Disease', 'abstract': 'Diagnosis techniques based on medical image modalities have higher sensitivities compared to conventional RT-PCT tests. We propose two methods for diagnosing COVID-19 disease using X-ray images and differentiating it from viral pneumonia. The diagnosis section is based on deep neural networks, and the discriminating uses an image retrieval approach. Both units were trained by healthy, pneumonia, and COVID-19 images. In COVID-19 patients, the maximum intensity projection of the lung CT is visualized to a physician, and the CT Involvement Score is calculated. The performance of the CNN and image retrieval algorithms were improved by transfer learning and hashing functions. We achieved an accuracy of 97% and an overall prec@10 of 87%, respectively, concerning the CNN and the retrieval methods.', 'corpus_id': 232304125, 'score': 0}, {'doc_id': '3911272', 'title': 'A new method of content based medical image retrieval and its applications to CT imaging sign retrieval', 'abstract': 'This paper proposes a new method of content based medical image retrieval through considering fused, context-sensitive similarity. Firstly, we fuse the semantic and visual similarities between the query image and each image in the database as their pairwise similarities. Then, we construct a weighted graph whose nodes represent the images and edges measure their pairwise similarities. By using the shortest path algorithm over the weighted graph, we obtain a new similarity measure, context-sensitive similarity measure, between the query image and each database image to complete the retrieval process. Actually, we use the fused pairwise similarity to narrow down the semantic gap for obtaining a more accurate pairwise similarity measure, and spread it on the intrinsic data manifold to achieve the context-sensitive similarity for a better retrieval performance. The proposed method has been evaluated on the retrieval of the Common CT Imaging Signs of Lung Diseases (CISLs) and achieved not only better retrieval results but also the satisfactory computation efficiency.', 'corpus_id': 3911272, 'score': 1}]
57	{'doc_id': '221201647', 'title': 'Differential signaling signatures evoked by DOI versus lisuride stimulation of the 5-HT2A receptor.', 'abstract': 'The 5-HT2A receptor is a target for hallucinogenic and non-hallucinogenic ligands that evoke unique behavioral, electrophysiological and molecular consequences. Here, we explored the differential effects of distinct 5-HT2A receptor ligands on signaling pathways downstream to the 5-HT2A receptor. The hallucinogenic 5-HT2A receptor agonist DOI evoked an enhanced signaling response compared to the non-hallucinogenic 5-HT2A receptor agonist lisuride in human/rat 5-HT2AR-EGFP receptor expressing HEK293\xa0cell lines and cortical neuronal cultures. We noted higher levels of phospho-PLC, pPKC, pERK, pCaMKII, pCREB, as well as higher levels of IP3 and DAG production following 5-HT2A receptor stimulation with DOI. Our study reveals distinct signaling signatures, differing in magnitude and kinetics at the 5-HT2A receptor in response to DOI versus lisuride.', 'corpus_id': 221201647}	9333	"[{'doc_id': '221140232', 'title': 'Angiotensin AT1 and AT2 receptor heteromer expression in the hemilesioned rat model of Parkinson’s disease that increases with levodopa-induced dyskinesia', 'abstract': 'The renin-angiotensin system (RAS) is altered in Parkinson’s disease (PD), a disease due to substantia nigra neurodegeneration and whose dopamine-replacement therapy, using the precursor levodopa, leads to dyskinesias as the main side effect. Angiotensin AT1 and AT2 receptors, mainly known for their role in regulating water homeostasis and blood pressure and able to form heterodimers (AT1/2Hets), are present in the central nervous system. We assessed the functionality and expression of AT1/2Hets in Parkinson disease (PD). Immunocytochemistry was used to analyze the colocalization between angiotensin receptors; bioluminescence resonance energy transfer was used to detect AT1/2Hets. Calcium and cAMP determination, MAPK activation, and label-free assays were performed to characterize signaling in homologous and heterologous systems. Proximity ligation assays were used to quantify receptor expression in mouse primary cultures and in rat striatal sections. We confirmed that AT1 and AT2 receptors form AT1/2Hets that are expressed in cells of the central nervous system. AT1/2Hets are novel functional units with particular signaling properties. Importantly, the coactivation of the two receptors in the heteromer reduces the signaling output of angiotensin. Remarkably, AT1/2Hets that are expressed in both striatal neurons and microglia make possible that candesartan, the antagonist of AT1, increases the effect of AT2 receptor agonists. In addition, the level of striatal expression increased in the unilateral 6-OH-dopamine lesioned rat PD model and was markedly higher in parkinsonian-like animals that did not become dyskinetic upon levodopa chronic administration if compared with expression in those that became dyskinetic. The results indicate that boosting the action of neuroprotective AT2 receptors using an AT1 receptor antagonist constitutes a promising therapeutic strategy in PD.', 'corpus_id': 221140232, 'score': 0}, {'doc_id': '221589141', 'title': 'Alfaxalone activates Human Pregnane-X Receptors with greater efficacy than Allopregnanolone: an in-vitro study with implications for neuroprotection during anesthesia', 'abstract': 'Background Alfaxalone is a fast acting intravenous anesthetic with high therapeutic index. It is an analogue of the naturally-occurring neurosteroid, allopregnanolone which has been implicated in causing neuroprotection, neurogenesis and preservation of cognition, through activation of pregnane X receptors in the central nervous system. This study investigated whether alfaxalone can activate human pregnane X receptors (h-PXR) as effectively as allopregnanolone. Methods Allopregnanolone and alfaxalone were dissolved in dimethyl sulfoxide to make allopregnanolone and alfaxalone treatment solutions (serial 3-fold dilution concentration range, 50,000 – 206 nM). Activation of h-PXR by these ligand solutions compared with vehicle control was measured by an in-vitro method using human embryonic kidney cells (HEK293) expressing h-PXR hybridised and linked to the firefly luciferase gene. Ligand binding with and activation of h-PXR in those cells caused downstream changes in luciferase activity and light emission. That activity was measured as relative light units using a plate-reading luminometer, thus quantifying the changes in h-PXR activity caused by the ligand applied to the HEK293 cells. Ligand log concentration response curves were constructed to compare efficacy and potency of allopregnanolone and alfaxalone. Results Allopregnanolone and alfaxalone both activated the h-PXR to cause dose-related light emission by the linked firefly luciferase. Control solutions (0.1% dimethyl sulfoxide) produced low level light emissions. Equimolar concentrations of alfaxalone were more efficacious in activation of h-PXR: 50,000 nM, p = 0.0019; 16,700 nM, p = 0.0472; 5,600 nM, p = 0.0031 [Brown-Forsythe and Welch ANOVA]. Conclusions Alfaxalone activates human-pregnane X receptors with greater efficacy compared with the endogenous ligand allopregnanolone. These results suggest that alfaxalone sedation and anesthesia may be accompanied by beneficial effects normally caused by the physiological effects of allopregnanolone, namely neuroprotection, neurogenesis, and preservation of cognition.', 'corpus_id': 221589141, 'score': 0}, {'doc_id': '221140553', 'title': 'Subcellular hot spots of GPCR signaling promote vascular inflammation', 'abstract': '\n Abstract\n \n G-coupled protein receptors (GPCRs) comprise the largest class of druggable targets. Signaling by GPCRs is initiated from subcellular hot spots including the plasma membrane, signalosomes and endosomes to contribute to vascular inflammation. GPCR-G protein signaling at the plasma membrane causes endothelial barrier disruption and also cross-talks with growth factor receptors to promote proinflammatory signaling. A second surge of GPCR signaling is initiated by cytoplasmic NFκB activation mediated by β-arrestins and CARMA-Bcl10-MALT1 signalosomes. Once internalized, ubiquitinated GPCRs initiate signaling from endosomes via assembly of the transforming growth factor-β-activated kinase binding protein-1 (TAB1)-TAB2-p38 MAPK complex to promote vascular inflammation. Understanding the complexities of GPCR signaling is critical for development of new strategies to treat vascular inflammation such as that associated with COVID-19.\n \n', 'corpus_id': 221140553, 'score': 0}, {'doc_id': '54276126', 'title': 'P.1.g.022 Hallucinogenic and non-hallucinogenic 5-HT2A receptor agonists induce distinct patterns of G protein coupling in postmortem human brain', 'abstract': None, 'corpus_id': 54276126, 'score': 1}, {'doc_id': '220366007', 'title': 'Receptor-independent membrane mediated pathways of serotonin action', 'abstract': 'Serotonin is a neurotransmitter as well as a somatic signaling molecule, and the serotonergic system is a major target for psychotropic drugs. Serotonin, together with a few related neurotransmitters, has recently been found to exhibit an unexpectedly high lipid membrane affinity1–3. It has been conjectured that extrasynaptic serotonin can diffuse in the lipid membrane to efficiently reach remote receptors (and receptors with buried ligand-binding sites)4, providing a mechanism for the diffuse ‘volume’ neurotransmission that serotonin is capable of5–10. Here we show that membrane binding by serotonin can directly modulate membrane properties and cellular function, independent of its receptor-mediated actions. Atomic force microscopy shows that serotonin binding makes artificial lipid bilayers softer. It induces nucleation of liquid disordered domains inside the raft-like liquid-ordered domains in a ternary bilayer displaying phase separation. Solid-state NMR spectroscopy corroborates this data, revealing a rather homogeneous decrease in the order parameter of the lipid chains in the presence of serotonin. In the RN46A immortalized serotonergic neuronal cell line, extracellular serotonin enhances transferrin receptor endocytosis, an action exerted even in the presence of both broad-spectrum serotonin receptor and transporter inhibitors. Similarly, it increases the binding and internalization of Islet Amyloid Polypeptide (IAPP) oligomers, suggesting a connection between serotonin, which is co-secreted with IAPP by pancreatic beta cells, and the cellular effects of IAPP. Our results uncover a hitherto unknown serotonin-bilayer interaction that can potentiate key cellular processes in a receptor-independent fashion. Therefore, some pathways of serotonergic action may escape potent pharmaceutical agents designed for serotonin transporters or receptors. Conversely, bio-orthogonal serotonin-mimetics may provide a new class of cell-membrane modulators.', 'corpus_id': 220366007, 'score': 1}, {'doc_id': '206649856', 'title': 'Hallucinogens and Serotonin 5-HT2A Receptor-Mediated Signaling Pathways.', 'abstract': 'The neuropsychological effects of naturally occurring psychoactive chemicals have been recognized for millennia. Hallucinogens, which include naturally occurring chemicals such as mescaline and psilocybin, as well as synthetic compounds, such as lysergic acid diethylamide (LSD), induce profound alterations of human consciousness, emotion, and cognition. The discovery of the hallucinogenic effects of LSD and the observations that LSD and the endogenous ligand serotonin share chemical and pharmacological profiles led to the suggestion that biogenic amines like serotonin were involved in the psychosis of mental disorders such as schizophrenia. Although they bind other G protein-coupled receptor (GPCR) subtypes, studies indicate that several effects of hallucinogens involve agonist activity at the serotonin 5-HT2A receptor. In this chapter, we review recent advances in understanding hallucinogen drug action through characterization of structure, neuroanatomical location, and function of the 5-HT2A receptor.', 'corpus_id': 206649856, 'score': 1}, {'doc_id': '222167821', 'title': 'SARS-CoV-2 spike protein co-opts VEGF-A/neuropilin-1 receptor signaling to induce analgesia', 'abstract': ""Supplemental Digital Content is Available in the Text. Severe acute respiratory syndrome coronavirus 2's spike protein promotes analgesia by interfering with vascular endothelial growth factor-A/NRP1 pathway, which may affect disease transmission dynamics."", 'corpus_id': 222167821, 'score': 0}, {'doc_id': '207136850', 'title': 'Chronic treatment with LY341495 decreases 5-HT2A receptor binding and hallucinogenic effects of LSD in mice', 'abstract': 'Hallucinogenic drugs, such as lysergic acid diethylamide (LSD), mescaline and psilocybin, alter perception and cognitive processes. All hallucinogenic drugs have in common a high affinity for the serotonin 5-HT(2A) receptor. Metabotropic glutamate 2/3 (mGlu2/3) receptor ligands show efficacy in modulating the cellular and behavioral responses induced by hallucinogenic drugs. Here, we explored the effect of chronic treatment with the mGlu2/3 receptor antagonist 2S-2-amino-2-(1S,2S-2-carboxycyclopropan-1-yl)-3-(xanth-9-yl)-propionic acid (LY341495) on the hallucinogenic-like effects induced by LSD (0.24mg/kg). Mice were chronically (21 days) treated with LY341495 (1.5mg/kg), or vehicle, and experiments were carried out one day after the last injection. Chronic treatment with LY341495 down-regulated [(3)H]ketanserin binding in somatosensory cortex of wild-type, but not mGlu2 knockout (KO), mice. Head-twitch behavior, and expression of c-fos, egr-1 and egr-2, which are responses induced by hallucinogenic 5-HT(2A) agonists, were found to be significantly decreased by chronic treatment with LY341495. These findings suggest that repeated blockade of the mGlu2 receptor by LY341495 results in reduced 5-HT(2A) receptor-dependent hallucinogenic effects of LSD.', 'corpus_id': 207136850, 'score': 1}, {'doc_id': '13057908', 'title': 'Tolerance and Cross-Tolerance to Head Twitch Behavior Elicited by Phenethylamine- and Tryptamine-Derived Hallucinogens in Mice', 'abstract': 'The serotonin 5-hydroxytryptamine 2A (5-HT2A) receptor is a potential therapeutic target to a host of neuropsychiatric conditions, but agonist actions at this site are linked to abuse-related hallucinogenic effects that may limit therapeutic efficacy of chronic drug administration. Tolerance to some effects of hallucinogens has been observed in humans and laboratory animals, but the understanding of tolerance and cross-tolerance between distinct structural classes of hallucinogens is limited. Here, we used the drug-elicited head twitch response (HTR) in mice to assess the development of tolerance and cross-tolerance with two phenethylamine-derived [DOI (2,5-dimethoxy-4-iodoamphetamine) and 2C-T-7 (2,5-dimethoxy-4-propylthiophenethylamine)] and two tryptamine-derived [DPT (N,N-dipropyltryptamine) and DIPT (N,N-diisopropyltryptamine)] drugs with agonist affinity for 5-HT2A receptors. Tolerance developed to HTR elicited by daily DOI or 2C-T-7, but not to HTR elicited by DPT or DIPT. DOI-elicited tolerance was not surmountable with dose, and a similar insurmountable cross-tolerance was evident when DOI-tolerant mice were tested with various doses of 2C-T-7 or DPT. These studies suggest that the use of phenethylamine-derived hallucinogens as therapeutic agents may be limited not only by their abuse potential, but also by the rapid development of tolerance that would likely be maintained even if a patient were switched to a different 5-HT2A agonist medication from a distinct structural class. However, these experiments also imply that tryptamine-derived hallucinogens might have a reduced potential for tolerance development, compared with phenethylamine-derived 5-HT2A agonists, and might therefore be more suitable for chronic administration in a therapeutic context.', 'corpus_id': 13057908, 'score': 1}, {'doc_id': '221298949', 'title': 'Growth factors and SARS-CoV-2', 'abstract': 'Phosphoproteomics analysis reveals that blocking growth factor receptor signaling inhibits SARS-CoV-replication. Phosphoproteomics analysis reveals that blocking growth factor receptor signaling inhibits SARS-CoV-2 replication.', 'corpus_id': 221298949, 'score': 0}]"
58	{'doc_id': '202230751', 'title': 'SystemDS: A Declarative Machine Learning System for the End-to-End Data Science Lifecycle', 'abstract': 'Machine learning (ML) applications become increasingly common in many domains. ML systems to execute these workloads include numerical computing frameworks and libraries, ML algorithm libraries, and specialized systems for deep neural networks and distributed ML. These systems focus primarily on efficient model training and scoring. However, the data science process is exploratory, and deals with underspecified objectives and a wide variety of heterogeneous data sources. Therefore, additional tools are employed for data engineering and debugging, which requires boundary crossing, unnecessary manual effort, and lacks optimization across the lifecycle. In this paper, we introduce SystemDS, an open source ML system for the end-to-end data science lifecycle from data integration, cleaning, and preparation, over local, distributed, and federated ML model training, to debugging and serving. To this end, we aim to provide a stack of declarative languages with R-like syntax for the different lifecycle tasks, and users with different expertise. We describe the overall system architecture, explain major design decisions (motivated by lessons learned from Apache SystemML), and discuss key features and research directions. Finally, we provide preliminary results that show the potential of end-to-end lifecycle optimization.', 'corpus_id': 202230751}	763	"[{'doc_id': '211171885', 'title': 'MLModelScope: A Distributed Platform for Model Evaluation and Benchmarking at Scale', 'abstract': 'Machine Learning (ML) and Deep Learning (DL) innovations are being introduced at such a rapid pace that researchers are hard-pressed to analyze and study them. The complicated procedures for evaluating innovations, along with the lack of standard and efficient ways of specifying and provisioning ML/DL evaluation, is a major ""pain point"" for the community. This paper proposes MLModelScope, an open-source, framework/hardware agnostic, extensible and customizable design that enables repeatable, fair, and scalable model evaluation and benchmarking. We implement the distributed design with support for all major frameworks and hardware, and equip it with web, command-line, and library interfaces. To demonstrate MLModelScope\'s capabilities we perform parallel evaluation and show how subtle changes to model evaluation pipeline affects the accuracy and HW/SW stack choices affect performance.', 'corpus_id': 211171885, 'score': 0}, {'doc_id': '209439707', 'title': 'Data Science through the looking glass and what we found there', 'abstract': 'The recent success of machine learning (ML) has led to an explosive growth both in terms of new systems and algorithms built in industry and academia, and new applications built by an ever-growing community of data science (DS) practitioners. This quickly shifting panorama of technologies and applications is challenging for builders and practitioners alike to follow. In this paper, we set out to capture this panorama through a wide-angle lens, by performing the largest analysis of DS projects to date, focusing on questions that can help determine investments on either side. Specifically, we download and analyze: (a) over 6M Python notebooks publicly available on GITHUB, (b) over 2M enterprise DS pipelines developed within COMPANYX, and (c) the source code and metadata of over 900 releases from 12 important DS libraries. The analysis we perform ranges from coarse-grained statistical characterizations to analysis of library imports, pipelines, and comparative studies across datasets and time. We report a large number of measurements for our readers to interpret, and dare to draw a few (actionable, yet subjective) conclusions on (a) what systems builders should focus on to better serve practitioners, and (b) what technologies should practitioners bet on given current trends. We plan to automate this analysis and release associated tools and results periodically.', 'corpus_id': 209439707, 'score': 1}, {'doc_id': '210157176', 'title': 'Multi-layer optimizations for end-to-end data analytics', 'abstract': ""We consider the problem of training machine learning models over multi-relational data. The mainstream approach is to first construct the training dataset using a feature extraction query over input database and then use a statistical software package of choice to train the model. In this paper we introduce Iterative Functional Aggregate Queries (IFAQ), a framework that realizes an alternative approach. IFAQ treats the feature extraction query and the learning task as one program given in the IFAQ's domain-specific language, which captures a subset of Python commonly used in Jupyter notebooks for rapid prototyping of machine learning applications. The program is subject to several layers of IFAQ optimizations, such as algebraic transformations, loop transformations, schema specialization, data layout optimizations, and finally compilation into efficient low-level C++ code specialized for the given workload and data. We show that a Scala implementation of IFAQ can outperform mlpack, Scikit, and TensorFlow by several orders of magnitude for linear regression and regression tree models over several relational datasets."", 'corpus_id': 210157176, 'score': 1}, {'doc_id': '211082718', 'title': 'Machine Learning in Python: Main developments and technology trends in data science, machine learning, and artificial intelligence', 'abstract': 'Smarter applications are making better use of the insights gleaned from data, having an impact on every industry and research discipline. At the core of this revolution lies the tools and the methods that are driving it, from processing the massive piles of data generated each day to learning from and taking useful action. Deep neural networks, along with advancements in classical ML and scalable general-purpose GPU computing, have become critical components of artificial intelligence, enabling many of these astounding breakthroughs and lowering the barrier to adoption. Python continues to be the most preferred language for scientific computing, data science, and machine learning, boosting both performance and productivity by enabling the use of low-level libraries and clean high-level APIs. This survey offers insight into the field of machine learning with Python, taking a tour through important topics to identify some of the core hardware and software paradigms that have enabled it. We cover widely-used libraries and concepts, collected together for holistic comparison, with the goal of educating the reader and driving the field of Python machine learning forward.', 'corpus_id': 211082718, 'score': 1}, {'doc_id': '211296505', 'title': ""MLIR: A Compiler Infrastructure for the End of Moore's Law"", 'abstract': 'This work presents MLIR, a novel approach to building reusable and extensible compiler infrastructure. MLIR aims to address software fragmentation, improve compilation for heterogeneous hardware, significantly reduce the cost of building domain specific compilers, and aid in connecting existing compilers together. MLIR facilitates the design and implementation of code generators, translators and optimizers at different levels of abstraction and also across application domains, hardware targets and execution environments. The contribution of this work includes (1) discussion of MLIR as a research artifact, built for extension and evolution, and identifying the challenges and opportunities posed by this novel design point in design, semantics, optimization specification, system, and engineering. (2) evaluation of MLIR as a generalized infrastructure that reduces the cost of building compilers-describing diverse use-cases to show research and educational opportunities for future programming languages, compilers, execution environments, and computer architecture. The paper also presents the rationale for MLIR, its original design principles, structures and semantics.', 'corpus_id': 211296505, 'score': 1}, {'doc_id': '211043646', 'title': 'A Language for Describing Optimization Strategies', 'abstract': 'Optimizing programs to run efficiently on modern parallel hardware is hard but crucial for many applications. The predominantly used imperative languages - like C or OpenCL - force the programmer to intertwine the code describing functionality and optimizations. This results in a nightmare for portability which is particularly problematic given the accelerating trend towards specialized hardware devices to further increase efficiency. \nMany emerging DSLs used in performance demanding domains such as deep learning, automatic differentiation, or image processing attempt to simplify or even fully automate the optimization process. Using a high-level - often functional - language, programmers focus on describing functionality in a declarative way. In some systems such as Halide or TVM, a separate schedule specifies how the program should be optimized. Unfortunately, these schedules are not written in well-defined programming languages. Instead, they are implemented as a set of ad-hoc predefined APIs that the compiler writers have exposed. \nIn this paper, we present Elevate: a functional language for describing optimization strategies. Elevate follows a tradition of prior systems used in different contexts that express optimization strategies as composition of rewrites. In contrast to systems with scheduling APIs, in Elevate programmers are not restricted to a set of built-in optimizations but define their own optimization strategies freely in a composable way. We show how user-defined optimization strategies in Elevate enable the effective optimization of programs expressed in a functional data-parallel language demonstrating competitive performance with Halide and TVM.', 'corpus_id': 211043646, 'score': 1}, {'doc_id': '210023441', 'title': 'Vamsa: Tracking Provenance in Data Science Scripts', 'abstract': ""Machine learning (ML) which was initially adopted for search ranking and recommendation systems has firmly moved into the realm of core enterprise operations like sales optimization and preventative healthcare. For such ML applications, often deployed in regulated environments, the standards for user privacy, security, and data governance are substantially higher. This imposes the need for tracking provenance end-to-end, from the data sources used for training ML models to the predictions of the deployed models. \nIn this work, we take a first step towards this direction by introducing the ML provenance tracking problem in the context of data science scripts. The fundamental idea is to automatically identify the relationships between data and ML models and in particular, to track which columns in a dataset have been used to derive the features of a ML model. We discuss the challenges in capturing such provenance information in the context of Python, the most common language used by data scientists. We then, present Vamsa, a modular system that extracts provenance from Python scripts without requiring any changes to the user's code. Using up to 450K real-world data science scripts from Kaggle and publicly available Python notebooks, we verify the effectiveness of Vamsa in terms of coverage, and performance. We also evaluate Vamsa's accuracy on a smaller subset of manually labeled data. Our analysis shows that Vamsa's precision and recall range from 87.5% to 98.3% and its latency is typically in the order of milliseconds for scripts of average size."", 'corpus_id': 210023441, 'score': 0}, {'doc_id': '214802265', 'title': 'The collection Virtual Machine: an abstraction for multi-frontend multi-backend data analysis', 'abstract': 'Getting the best performance from the ever-increasing number of hardware platforms has been a recurring challenge for data processing systems. In recent years, the advent of data science with its increasingly numerous and complex types of analytics has made this challenge even more difficult. In practice, system designers are overwhelmed by the number of combinations and typically implement a single analytics type on one platform, leading to repeated implementation effort---and a plethora of semi-compatible tools for data scientists. In this paper, we propose the ""Collection Virtual Machine"" (or CVM)---an extensible compiler framework designed to keep the specialization process of data analytics systems tractable. It can capture at the same time the essence of a large span of low-level, hardware-specific implementation techniques as well as high-level operations of different types of analyses. At its core lies a language for defining nested, collection-oriented intermediate representations (IRs). Frontends produce programs in their IR flavors defined in that language, which get optimized through a series of rewritings (possibly changing the IR flavor multiple times) until the program is finally expressed in an IR of platform-specific operators. While reducing the overall implementation effort, this also improves the interoperability of both analyses and hardware platforms. We have used CVM successfully to build specialized backends for platforms as diverse as multi-core CPUs, RDMA clusters, and serverless computing infrastructure in the cloud and expect similar results for many more frontends and hardware platforms in the near future.', 'corpus_id': 214802265, 'score': 0}, {'doc_id': '210164864', 'title': 'Towards High Performance Java-based Deep Learning Frameworks', 'abstract': ""The advent of modern cloud services along with the huge volume of data produced on a daily basis, have set the demand for fast and efficient data processing. This demand is common among numerous application domains, such as deep learning, data mining, and computer vision. Prior research has focused on employing hardware accelerators as a means to overcome this inefficiency. This trend has driven software development to target heterogeneous execution, and several modern computing systems have incorporated a mixture of diverse computing components, including GPUs and FPGAs. However, the specialization of the applications' code for heterogeneous execution is not a trivial task, as it requires developers to have hardware expertise in order to obtain high performance. The vast majority of the existing deep learning frameworks that support heterogeneous acceleration, rely on the implementation of wrapper calls from a high-level programming language to a low-level accelerator backend, such as OpenCL, CUDA or HLS. \nIn this paper we have employed TornadoVM, a state-of-the-art heterogeneous programming framework to transparently accelerate Deep Netts; a Java-based deep learning framework. Our initial results demonstrate up to 8x performance speedup when executing the back propagation process of the network's training on AMD GPUs against the sequential execution of the original Deep Netts framework."", 'corpus_id': 210164864, 'score': 0}, {'doc_id': '211004086', 'title': 'DIVA: A Declarative and Reactive Language for in situ Visualization', 'abstract': 'The use of adaptive workflow management for in situ visualization and analysis has been a growing trend in large-scale scientific simulations. However, coordinating adaptive workflows with traditional procedural programming languages can be difficult because system flow is determined by unpredictable scientific phenomena, which often appear in an unknown order and can evade event handling. This makes the implementation of adaptive workflows tedious and error-prone. Recently, reactive and declarative programming paradigms have been recognized as well-suited solutions to similar problems in other domains. However, there is a dearth of research on adapting these approaches to in situ visualization and analysis. With this paper, we present a language design and runtime system for developing adaptive systems through a declarative and reactive programming paradigm. We illustrate how an adaptive workflow programming system is implemented using our approach and demonstrate it with a use case from a combustion simulation.', 'corpus_id': 211004086, 'score': 0}]"
59	{'doc_id': '231913300', 'title': 'Grocery food taxes and U.S. county obesity and diabetes rates', 'abstract': 'Background Grocery food taxes represent a stable tax revenue stream for state and municipal government during times of adverse economic shocks such as that observed under the coronavirus disease 2019 (COVID-19) pandemic. Previous research, however, suggests a possible mechanism through which grocery taxes may adversely affect health. Our objectives are to document the spatial and temporal variation in grocery taxes and to empirically examine the statistical relationship between county-level grocery taxes and obesity and diabetes. Methods We collect and assemble a novel national dataset of annual county and state-level grocery taxes from 2009 through 2016. We link this data to three-year, county-level estimates based on data from the Centers for Disease Control and Prevention on rates of obesity and diabetes and provide a nation-wide spatial characterization of grocery taxes and these two health outcomes. Using a county-level fixed effects estimator, we estimate the effect of grocery taxes on obesity and diabetes rates, also controlling for a subset of potential confounders that vary over time. Results We find a 1 percentage point increase in grocery taxes is associated with 0.588 and 0.215 percentage point increases in the county-level obesity and diabetes rates. Conclusion Counties with grocery taxes have increased prevalence of obesity and diabetes. We estimate the economic burden of increased obesity and diabetes rates resulting from grocery taxes to be $5.9 billion. Based on this estimate, the benefit-cost ratio of removing grocery taxes across the United States only considering the effects on obesity and diabetes rates is 1.90.', 'corpus_id': 231913300}	14652	"[{'doc_id': '232203161', 'title': 'Dynamics of food price volatility and households’ welfare in Nigeria: implications for post-COVID-19 recovery', 'abstract': ""One of the most important economic factors in food choice is the price. Its value in food dynamics is a subject of controversies and opinions, especially price issues, and sensitivity is often peculiar to seasons, crisis and market forces. Price dynamics have the potential to introduce and change consumptions, thus affecting household welfare. In this study, we examined the dynamics of food price volatility and households' welfare in Nigeria from 1990: Q1 to 2019: Q4n. . We sourced the data for the study from FAO and the World Bank. We estimated the Quadratic trend equation, Generalized Autoregressive Conditional Heteroscedasticity (GARCH) and Auto-Regressive Distributed Lag (ARDL) models and forecast the changes in food price past COVID-19 pandemic period. Food prices, depth of food deficiency, food import, and food production index had a significant short-run impact on the households' welfare. Policymakers should focus on the short-term benefits while formulating policies aimed at households' welfare. The post-COVID -19 recovery policies aimed at the household level will be impactful in the short-run compared to the long-run."", 'corpus_id': 232203161, 'score': 0}, {'doc_id': '230800312', 'title': 'An Exploration of the Role of Sugar-Sweetened Beverage in Promoting Obesity and Health Disparities', 'abstract': 'The mechanistic role of sugar-sweetened beverage (SSB) in the etiology of obesity is undetermined. We address whether, compared to other foods, does consumption of SSB (1) automatically lead to failure to compensate for the energy it contains? (2) fail to elicit homeostatic hormone responses? (3) promote hedonic eating through activation of the brain’s reward pathways? We followed the evidence to address: (4) Would restriction of targeted marketing of SSB and other unhealthy foods to vulnerable populations decrease their prevalence of obesity? The data are lacking to demonstrate that SSB consumption promotes body weight gain compared with isocaloric consumption of other beverages or foods and that this is linked to its failure to elicit adequate homeostatic hormone responses. However, more recent data have linked body weight gain to reward activation in the brain to palatable food cues and suggest that sweet tastes and SSB consumption heightens the reward response to food cues. Studies investigating the specificity of these responses have not been conducted. Nevertheless, the current data provide a biological basis to the body of evidence demonstrating that the targeted marketing (real life palatable food cues) of SSB and other unhealthy foods to vulnerable populations, including children and people of color and low socioeconomic status, is increasing their risk for obesity. While the mechanisms for the association between SSB consumption and body weight gain cannot be identified, current scientific evidence strongly suggests that proactive environmental measures to reduce exposure to palatable food cues in the form of targeting marketing will decrease the risk of obesity in vulnerable populations.', 'corpus_id': 230800312, 'score': 1}, {'doc_id': '231752088', 'title': 'Perceived Precautionary Savings Motives: Evidence from Digital Banking*', 'abstract': 'In a representative sample of new borrowers, access to digital lines of credit increases the spending of consumers with ex-ante higher savings rates (liquid consumers) permanently. After access to the line of credit, these consumers reduce their existing savings rate but do not tap into negative deposits and hence do not raise debt. Through our FinTech bank setting, we can elicit consumers’ risk preferences, beliefs, perceptions, and other characteristics directly. Common theoretical determinants of precautionary savings motives do not differ systematically across liquid and illiquid consumers but liquid consumers have higher subjective beliefs about the need for precautionary savings nonetheless. JEL Codes: D14, E21, E51, G21', 'corpus_id': 231752088, 'score': 0}, {'doc_id': '158948768', 'title': 'Do Taxes for Soda and Sugary Drinks Work? Scanner Data Evidence from Berkeley and Washington', 'abstract': 'Curbing obesity through taxation of certain beverage products has been a priority in the policy agenda across many U.S. jurisdictions. We assess the effectiveness of this highly debated policy instrument through two measures of its impact: the pass-through rate (the extent to which the tax actually translates into a retail price increase) and the impact on consumption (volume sales). We evaluate the actual effect of two excise taxes on the beverages market: the sugar-sweetened-beverages (SSB) tax of 1¢ per ounce in the city of Berkeley that has been effect since 2015 and the tax of 1/6¢ per ounce on carbonated drinks (soda) that the state of Washington imposed from July through December of 2010. We carry out the analysis with a barcode-level dataset containing price and volume sales information from a large number of retail outlets. Our identification relies on sales data from stores located in taxed areas as well as from stores in nearby localities. We find differences across the two tax events on pass-through: retail prices in Washington reacted sharply (by a larger magnitude than the tax) and promptly whereas in Berkeley retail prices reacted only marginally (by less than 30% the magnitude of the tax). In terms of volume sales, we find a 5% volume reduction in Washington but fail to find any evidence of an effect in Berkeley.', 'corpus_id': 158948768, 'score': 1}, {'doc_id': '231839562', 'title': 'Does Collateral Value Affect Asset Prices? Evidence from a Natural Experiment in Texas', 'abstract': 'Does the ability to pledge an asset as collateral, after purchase, affect its price? This paper identifies the impact of collateral service flows on house prices, exploiting a plausibly exogenous constitutional amendment in Texas that legalized home equity loans in 1998. The law change increased Texas house prices 4%; this is price-based evidence that households are credit-constrained and value home equity loans to facilitate consumption smoothing. Prices rose more in locations with inelastic supply, higher prelaw house prices, higher income, and lower unemployment. These estimates reveal that richer households value the option to pledge their home as collateral more strongly. (JEL R0, R3, G0, E21, E44, G2) ∗I thank my discussants Marc Francke, Sanket Korgaonkar, Adam Nowak, and Johannes Stroebel for detailed comments. I thank Sumit Agarwal, Brent Ambrose, Zahi Ben-David, Sean Chu, Anthony DeFusco, Gilles Duranton, Vadim Elenev, Alex Gelber, Todd Gormley, Ben Keys, Olivia Mitchell, Jonathan Parker, Tomasz Piskorski, Nikolai Roussanov, Todd Sinai, Boris Vabson, Stijn Van Nieuwerburgh, Jessica Wachter, Susan Wachter, Vincent Yao, and Yildiray Yildirim, as well as seminar participants at AMES, AREUEA, ASSA, Baruch, Brandeis, Dallas Fed, EFA, EMES, Federal Reserve Board, Houston Finance, IAAE, Johns Hopkins, Maryland, MMM, NASMES, NYCREC, Penn State, PFMC, Philadelphia Fed, Rice, ReCapNet, SEM, UT Dallas, Wharton, and XXVI Finance Forum for helpful comments. I thank Albert Saiz for sharing his data. I thank Guido Imbens for sharing his synthetic control code. I thank Cheng Chen and Sung Son for excellent research assistance. I gratefully acknowledge funding from the PSC CUNY Research Foundation under grant 60202-00-48. All errors are my own. Send correspondence to Albert Alex Zevelev, 1 Bernard Baruch Way, New York, NY 10010. Email: Albert.Zevelev@baruch.cuny.edu. 1 ar X iv :2 10 2. 02 93 5v 1 [ ec on .G N ] 5 F eb 2 02 1', 'corpus_id': 231839562, 'score': 0}, {'doc_id': '229378642', 'title': 'ECONOMETRIC ANALYSIS ON THE IMPACT OF WATER RATES ON RESIDENT USAGE AND CITY REVENUE: A STUDY IN SALT LAKE COUNTY, UTAH', 'abstract': 'This project aims to examine what effect a change in municipal water rates has on water consumption. Data was provided by two cities in Salt Lake County, Utah and consists of monthly water usage for 26,510 households from July 2016 to July 2020, yielding a total of 1,192,463 observations. Beginning the month of July 2018, one city increased the rates charged per thousand gallons used, creating a natural experiment, while the other did not, serving as a control effect. City officials estimated a 5% increase in water rates would yield a 5% increase in city revenue. The year after the rate change however, water revenue decreased by 1%. The question of city officials was whether this was due to the rate change or other factors. The goal of this study is to determine if the rate increase had a measurable effect on water consumption and if so, whether or not it was enough to cause revenue city-wide to decrease. Using linear regression techniques and controlling for several variables that impact water consumption, I find average usage should have decreased between 119-238 gallons per month for the average household after the rate change. Even with the decrease in water usage, I estimate the average monthly bill should have risen by approximately $3.43. This information is important, as it shows that without other factors that affect usage, the city should have expected to see a small impact in conservation and an increase in city revenue after the rate change. In normal years, this small 5% rate increase would grow city revenue by an estimation of $889,718 per year, increasing the ability of the city to provide for essential public goods and services to its residents.', 'corpus_id': 229378642, 'score': 0}, {'doc_id': '233274067', 'title': 'Did state‐mandated restrictions on sugar‐sweetened drinks in California high schools increase soda purchases in school neighborhoods?', 'abstract': 'Correspondence Kristin Kiesel, Department of Agricultural and Resource Economics, University of California, Davis, 2147 Social Science and Humanities Building, One Shields Avenue, Davis, CA 95616, USA. Email: kiesel@ucdavis.edu Abstract This paper evaluates the effectiveness of restrictions on sugar-sweetened beverages (SSBs) in schools as a policy approach aimed at reversing the upward trend in obesity among adolescents. Specifically, we test if the implementation of SB 965 in California high schools led to detectable compensation effects outside of school by estimating changes in soda purchases observed in store-level scanner data. Our unique data and identification strategy address data limitations of previously published studies, and our reported results strengthen the notion that preferences for unhealthy foods will persist even after their availability is restricted in select environments.', 'corpus_id': 233274067, 'score': 1}, {'doc_id': '230532930', 'title': 'Assessment of global food demand in unexpected situations', 'abstract': 'The methodological approach for assessing the formation of food demand in unforeseen situations using digital Internet-technologies and the assessment itself, is substantiated in the paper (in the context of the COVID-19 pandemic of 2020) Comparison and theoretical generalization, as well as statistical test-assessment of hypotheses and structural regularities based on the data of Google Trends Internet platform, is used to analyze consumer preferences and intensity of demand changes for meat, milk, sugar, bread, and flour during the pandemic and quarantine, both in developed and developing countries It is discovered that the biggest changes can be observed in the developed countries: consumer preferences shifted from rather expensive food products (milk and meat) to much cheaper ones (flour and bread) It is asserted that a decrease in consumer demand for basic food products will have a negative impact on the global economy In 2020, a considerable decrease in GDP is expected for the developed countries;in the developing countries, GDP decline will not be as large, but prices are expected to rise much more noticeably The following anti-crisis measures are proposed: support of the most vulnerable population and increase of food accessibility;temporary reduction of the VAT and other taxes influencing the price of food;reduction of central banks’ lending rates, etc With the correct measures applied, the stabilization of consumer demand for food and gradual growth of the global economy is expected by the end of 2021 © Iaroslav Gadzalo, Mykola Sychevskiy, Olha Kovalenko, Liudmyla Deineko, Lyudmila Yashchenko, 2020', 'corpus_id': 230532930, 'score': 0}, {'doc_id': '232161215', 'title': 'How do consumers respond to ""sin taxes""? New evidence from a tax on sugary drinks.', 'abstract': 'It is unclear what the effects of taxes on sugar sweetened beverages (SSBs) are on consumer behaviour and which consumers may be affected the most. We evaluate the effect of the SSB tax introduced in Catalonia (but not in the rest of Spain) in May 2017 using loyalty card data of monthly purchases by 884,843 households from May 2016 to April 2018. Using a Difference-in-Differences approach, we study the SSB tax effect on the purchased quantity of beverages and sugar. Our results suggest a reduction in purchases of taxed beverages and a small increase in purchases of untaxed beverages. Households have substituted taxed beverages with their lower sugar (untaxed) counterparts. This has led to a 2.2% overall reduction in sugar purchases from beverages. Our study implies that although sin taxes moderately change consumer behaviour, a combination of different policies would be required to tackle obesity.', 'corpus_id': 232161215, 'score': 1}, {'doc_id': '515326', 'title': 'Changes in Prices After an Excise Tax to Sweetened Sugar Beverages Was Implemented in Mexico: Evidence from Urban Areas', 'abstract': 'In 2014 an excise tax to non-alcoholic sweetened beverages (SSB) was implemented in Mexico. The objective of this paper is to study whether and to what degree these taxes passed-through onto SSB prices in urban areas overall and by region, type of beverage and package size. Prices were obtained from the National Institute of Statistics and Geography from 2011 to 2014. We applied a pre-post quasi-experimental approach using fixed effects models. In sensitivity analysis we applied other model specifications to test the robustness of the findings and we also present weighted estimations based on household purchases. The dependent variables are real prices of a specific beverage category; the main independent variables are dummies for each month of 2014, and the models adjust for time trends and seasonality. Results suggest that the SSB tax passed along to consumers for all SSBs and we found overshifting for the carbonated SSBs. A greater effect is seen among the small package sizes, and we see heterogeneous effects by region. Estimating the effect of the tax on prices is important to understand the potential effect on consumption.', 'corpus_id': 515326, 'score': 1}]"
60	{'doc_id': '217165681', 'title': 'Using the Least Squares Support Vector Regression to Forecast Movie Sales with Data from Twitter and Movie Databases', 'abstract': 'Due to the rapid prominence and popularity of social media, social broadcasting networks with voluntary information sharing have become one of the most powerful ways to spread word-of-mouth opinions, and thus, have influence on consumers’ preferences toward products. Therefore, sentiment analysis data from social media have become more important in forecasting product sales. For the movie industry, the opinions expressed on social media have increasing impacts on movie sales. In addition, some databases, such as the Box Office Mojo and Internet Movie Database (IMDb), contain structured data for predicting movie sales. Thus, three categories of data—data of movie databases, data of tweets, and hybrid data including movies databases and tweets—are employed symmetrically in this study. The aim of this study is to employ the least squares support vector regression (LSSVR) to forecast movie sales worldwide according to these three forms of data. In addition, three other forecasting techniques—namely, the back propagation neural network (BPNN), the generalized regression neural network (GRNN), and the multivariate linear regression (MLR) model—were used to forecast movie sales with the three types of data. The empirical results show that the LSSVR model with hybrid data can obtain more accurate results than the other forecasting models with all data types. Thus, forecasting movie sales using the LSSSVR model with data containing movie databases and tweets is a feasible and prospective method to forecast movie sales.', 'corpus_id': 217165681}	19767	"[{'doc_id': '63111818', 'title': 'Study on prediction for a film success using text mining', 'abstract': 'Recently, big data is positioning as a keyword in the academic circles. And usefulness of big data is carried into government, a local public body and enterprise as well as academic circles. Also they are endeavoring to obtain useful information in big data. This research mainly deals with analyses of box office success or failure of films using text mining. For data, it used a portal site `D` and film review data, grade point average and the number of screens gained from the Korean Film Commission. The purpose of this paper is to propose a model to predict whether a film is success or not using these data. As a result of analysis, the correct classification rate by the prediction model method proposed in this paper is obtained 95.74%.', 'corpus_id': 63111818, 'score': 1}, {'doc_id': '69575945', 'title': 'A Survey on Prediction of Movie’s Box Office Collection Using Social Media', 'abstract': 'Predicting the box office profits of a movie prior to its world wide release are a significant but also an exigent problem that needs a advanced of Intelligence. Currently, social media has given away its diagnostic strength in a variety of fields, which encourages us to develop social media substance to predict box office profits. The collection of movies in provisions of profit relies on so many features for instance its making studio, type, screenplay superiority, pre release endorsement etc, each of which are usually utilized to approximation their probable achievement at the box office. Nevertheless, the “Wisdom of Crowd” and social media have been accredited as a powerful indication in appreciative customer activities to media. In this survey, we converse the influence of socially created Meta data derived from the social multimedia websites and review the effect of social media on box office collection and success of movies. This survey paper is written for (social networking) investigators who looking for to evaluate prediction of movies using social media. It gives a complete study of social media analytics for social networking, wikis, actually easy syndication feeds, blogs, newsgroups, chat and news feeds etc. Keywords: Social Networking, Social media. Movie’s Box Office, Prediction, Profitability, Sentiment analysis', 'corpus_id': 69575945, 'score': 1}, {'doc_id': '237012351', 'title': 'Sentiment analysis of twitter data using Machine learning algorithms', 'abstract': 'The rapid increase in usage of Technology has changed the way of expressing people’s opinions, views and Sentiments about specific product, services, people and more, by using social media services such as Facebook, Instagram and Twitter. Due to this is massive amount of data gets generated. To find insights from this Data generated and make certain decision we implement web application that collects twitter data and shows it indifferent statistical forms. The main objective of the work presented with in this paper was to design and implement twitter data analysis and visualization in Python platform. Our primary approach was to focus on real-time analysis rather than historic datasets. Twitter API allow for collecting the sentiments information in the form of either positive score, negative score or neutral. We show the application of sentimental analysis and how to connect to Twitter and run sentimental analysis queries. We run experiments on different queries from politics to humanity and show the interesting results. We realized that the neutral sentiment for tweets are significantly high which clearly shows the limitations of the current works. this study focuses mainly on sentiment analysis of twitter data which is helpful to analyze the information in the tweets where opinions are highly unstructured, heterogeneous and are either positive or negative, or neutral in some cases. In this paper, we provide a survey and a comparative analyses of existing techniques for opinion mining like machine learning and lexicon-based approaches, together with evaluation metrics. Using various machine learning algorithms like Naive Bayes, XGBoost Classifier and Support Vector Machine, we provide research on twitter data streams. We have also discussed general challenges and applications of Sentiment Analysis on Twitter.', 'corpus_id': 237012351, 'score': 0}, {'doc_id': '236836774', 'title': 'SENTIMENT ANALYSIS OF PRODUCT REVIEWS USING SUPERVISED LEARNING', 'abstract': ""Today, Online Reviews are global communications among consumers and E-commerce businesses. When Somebody wants to make a purchase online, they read the reviews and comments that many people have written about the product. Only after customers decide whether to buy the product or not. Based on that, the Success of any Products directly depends on its Customer. Customer Likes Products It’s Success. if not, then Company needs to improve it by making some changes in it. For that, the need is to analyze the customers' written reviews and find the sentiment from that. the task of Classifying the comments and the reviews in positive or negative is known as sentiment analysis.in this paper, A Standard dataset reviews have been classified into positive and negative sentiments using Sentiment Analysis. For that different Machine Learning and Deep Learning Technique is used and also Compared the performance of word2vec-CNN Model with FastTextCNN Model on amazon unlocked mobile phone Dataset."", 'corpus_id': 236836774, 'score': 0}, {'doc_id': '233840311', 'title': 'Analysing Movie Success Based on Machine Learning Algorithm', 'abstract': 'The Film Industry is producing several hundreds of movies paving the way for the United States for third position in the list of giants of film Industry of the whole world [1]. The expenditure on these movies are of the ten to eleven figures or we can say thousands of millions of dollars, which ensure their box office success, is absolutely essential for the survival of the industry. Predicting what sort of movies are going to earn more and what type is going to come up short before the release, may profit the houses extraordinarily because it will also empower them to work on their promoting efforts which itself need numerous dollars, appropriately[2]. Furthermore, it would likewise assist them with knowing when it is generally suitable to deliver a film by taking a look at the general market. Along these lines, the examination of film achievement is critical to the business. ML (Machine Learning) algorithms [3] calculations broadly need to make expectations like development inside the stock trade, interest for items, nature of tumors, and so on.', 'corpus_id': 233840311, 'score': 1}, {'doc_id': '236909721', 'title': 'A STUDY: SENTIMENTAL ANALYSIS FOR ELECTION RESULTS BY USING TWITTER DATA', 'abstract': 'The entire world is changing at a breakneck pace, and technology is no exception. User-generated data is abundant on social networking platforms like Twitter. Users from all around the world offer their thoughts, opinions, ideas, and feelings about a variety of topics, including products, movies, and politics. Manual sentiment analysis is a time-consuming task. Opinion mining has recently gained popularity as a result of the large volume of opinionated data available on social networking sites such as Twitter. In this paper, we used a chronological approach to data collection, data pre-processing, emotional analysis, and machine learning analysis to forecast the outcome of the US 2020 presidential election using Twitter emotional analysis. We used a Random Forest classifier after completing a literature review and comparing all supervised ensemble machine learning algorithms to determine which one was the best. The proposed technique was tested on Twitter data, and it outperformed existing approaches.', 'corpus_id': 236909721, 'score': 0}, {'doc_id': '218613673', 'title': 'Screenplay Quality Assessment: Can We Predict Who Gets Nominated?', 'abstract': 'Deciding which scripts to turn into movies is a costly and time-consuming process for filmmakers. Thus, building a tool to aid script selection, an initial phase in movie production, can be very beneficial. Toward that goal, in this work, we present a method to evaluate the quality of a screenplay based on linguistic cues. We address this in a two-fold approach: (1) we define the task as predicting nominations of scripts at major film awards with the hypothesis that the peer-recognized scripts should have a greater chance to succeed. (2) based on industry opinions and narratology, we extract and integrate domain-specific features into common classification techniques. We face two challenges (1) scripts are much longer than other document datasets (2) nominated scripts are limited and thus difficult to collect. However, with narratology-inspired modeling and domain features, our approach offers clear improvements over strong baselines. Our work provides a new approach for future work in screenplay analysis.', 'corpus_id': 218613673, 'score': 1}, {'doc_id': '237257749', 'title': 'Predicting the sentiment analysis for the customer reviews to analyze the text analytics', 'abstract': 'The social web has generated huge amounts of data for the users across the globe with just the click of a button. Even in the age of digitalization other’s opinions are considered while making a decision. This reliability is found in the form of opinions and experiences regarding a particular product or service. Sentiment analysis discusses these opinions. The information gathered through the World Wide Web via forums, blogs, social networks and content-sharing services is not structured which leads to the rise of fields like opinion mining, text analysis and sentiment analysis. This paper discusses the different methods of sentiment analysis and highlights its importance in understanding customer reviews to assess text analytics. Since reviews based on sentiment analysis have been included so this paper will focus on reviewing some previous review works of sentiment analysis for customer reviews.', 'corpus_id': 237257749, 'score': 0}, {'doc_id': '230524019', 'title': 'Analyzing movies to predict their commercial viability for producers', 'abstract': 'Upon film premiere, a major form of speculation concerns the relative success of the film. This relativity is in particular regards to the film’s original budget – as many a time have “big-budget blockbusters” been met with exceptional success as met with abject failure. So how does one predict the success of an upcoming film? In this paper, we explored a vast array of film data in an attempt to develop a model that could predict the expected return of an upcoming film. The approach to this development is as follows: First, we began with the MovieLens dataset [2] having common movie attributes alongwith genome tags per each film. Genome tags give insight into what particular characteristics of the film are most salient. We then included additional features regarding film content, cast/crew, audience perception, budget, and earnings from TMDB, IMDB, and Metacritic websites. Next, we performed exploratory data analysis and engineered awide range of new features capturing historical information for the available features. Thereafter, we used singular value decomposition (SVD) for dimensionality reduction of the high dimensional features (ex. genome tags). Finally, we built a Random Forest Classifier and performed hyper-parameter tuning to optimize for model accuracy. A future application of our model could be seen in the film industry, allowing production companies to better predict the expected return of their projects based on their envisioned outline for their production procedure, thereby allowing them to revise their plan in an attempt to achieve optimal returns.', 'corpus_id': 230524019, 'score': 1}, {'doc_id': '236205880', 'title': 'Machine learning based recommendation system on movie reviews using KNN classifiers', 'abstract': 'Recommender systems are the systems that are designed to recommend items to the consumer depending on several different criteria. These systems estimate the most possible product that the consumers are most likely to buy and are of interest to. Companies like Netflix, Amazon, etc. use recommender services to allow their customers to find the right items or movies for them.In the current system recommendations, the content of ltering and collective ltering typically fall into two groups. The method is formerly Periment in our paper in all methods. We take film features such as stars, directors, for content-based ltering. Movie definition and keywords as inputs use TF-IDF and doc2vec for measuring the film resemblance. For the first time, Input to our algorithm is the film ranking encountered by users, and we use neighbours nearest K, as Factorization of matrix to estimate film scores for consumers. We find that teamwork functions better than content. Predictive error and estimation time ltering.', 'corpus_id': 236205880, 'score': 0}]"
61	{'doc_id': '1452971', 'title': 'Contextual Bandits with Linear Payoff Functions', 'abstract': 'In this paper we study the contextual bandit problem (also known as the multi-armed bandit problem with expert advice) for linear payoff functions. For T rounds, K actions, and d dimensional feature vectors, we prove an O (√ Td ln(KT ln(T )/δ) ) regret bound that holds with probability 1− δ for the simplest known (both conceptually and computationally) efficient upper confidence bound algorithm for this problem. We also prove a lower bound of Ω( √ Td) for this setting, matching the upper bound up to logarithmic factors.', 'corpus_id': 1452971}	86	"[{'doc_id': '208006409', 'title': 'Contextual Bandits Evolving Over Finite Time', 'abstract': 'Contextual bandits have the same exploration-exploitation trade-off as standard multi-armed bandits. On adding positive externalities that decay with time, this problem becomes much more difficult as wrong decisions at the start are hard to recover from. We explore existing policies in this setting and highlight their biases towards the inherent reward matrix. We propose a rejection based policy that achieves a low regret irrespective of the structure of the reward probability matrix.', 'corpus_id': 208006409, 'score': 1}, {'doc_id': '208202171', 'title': 'Safe Linear Stochastic Bandits', 'abstract': 'We introduce the safe linear stochastic bandit framework---a generalization of linear stochastic bandits---where, in each stage, the learner is required to select an arm with an expected reward that is no less than a predetermined (safe) threshold with high probability. We assume that the learner initially has knowledge of an arm that is known to be safe, but not necessarily optimal. Leveraging on this assumption, we introduce a learning algorithm that systematically combines known safe arms with exploratory arms to safely expand the set of safe arms over time, while facilitating safe greedy exploitation in subsequent stages. In addition to ensuring the satisfaction of the safety constraint at every stage of play, the proposed algorithm is shown to exhibit an expected regret that is no more than $O(\\sqrt{T}\\log (T))$ after $T$ stages of play.', 'corpus_id': 208202171, 'score': 1}, {'doc_id': '202539449', 'title': 'AutoML for Contextual Bandits', 'abstract': 'Contextual Bandits is one of the widely popular techniques used in applications such as personalization, recommendation systems, mobile health, causal marketing etc . As a dynamic approach, it can be more efficient than standard A/B testing in minimizing regret. We propose an end to end automated meta-learning pipeline to approximate the optimal Q function for contextual bandits problems. We see that our model is able to perform much better than random exploration, being more regret efficient and able to converge with a limited number of samples, while remaining very general and easy to use due to the meta-learning approach. We used a linearly annealed e-greedy exploration policy to define the exploration vs exploitation schedule. We tested the system on a synthetic environment to characterize it fully and we evaluated it on some open source datasets to benchmark against prior work. We see that our model outperforms or performs comparatively to other models while requiring no tuning nor feature engineering.', 'corpus_id': 202539449, 'score': 1}, {'doc_id': '204852240', 'title': 'Fixed-Confidence Guarantees for Bayesian Best-Arm Identification', 'abstract': 'We investigate and provide new insights on the sampling rule called Top-Two Thompson Sampling (TTTS). In particular, we justify its use for fixed-confidence best-arm identification. We further propose a variant of TTTS called Top-Two Transportation Cost (T3C), which disposes of the computational burden of TTTS. As our main contribution, we provide the first sample complexity analysis of TTTS and T3C when coupled with a very natural Bayesian stopping rule, for bandits with Gaussian rewards, solving one of the open questions raised by Russo (2016). We also provide new posterior convergence results for TTTS under two models that are commonly used in practice: bandits with Gaussian and Bernoulli rewards and conjugate priors.', 'corpus_id': 204852240, 'score': 0}, {'doc_id': '204512315', 'title': 'Actor Critic with Differentially Private Critic', 'abstract': 'Reinforcement learning algorithms are known to be sample inefficient, and often performance on one task can be substantially improved by leveraging information (e.g., via pre-training) on other related tasks. In this work, we propose a technique to achieve such knowledge transfer in cases where agent trajectories contain sensitive or private information, such as in the healthcare domain. Our approach leverages a differentially private policy evaluation algorithm to initialize an actor-critic model and improve the effectiveness of learning in downstream tasks. We empirically show this technique increases sample efficiency in resource-constrained control problems while preserving the privacy of trajectories collected in an upstream task.', 'corpus_id': 204512315, 'score': 0}, {'doc_id': '204734398', 'title': 'Adaptive Exploration in Linear Contextual Bandit', 'abstract': 'Contextual bandits serve as a fundamental model for many sequential decision making tasks. The most popular theoretically justified approaches are based on the optimism principle. While these algorithms can be practical, they are known to be suboptimal asymptotically (Lattimore and Szepesvari, 2017). On the other hand, existing asymptotically optimal algorithms for this problem do not exploit the linear structure in an optimal way and suffer from lower-order terms that dominate the regret in all practically interesting regimes. We start to bridge the gap by designing an algorithm that is asymptotically optimal and has good finite-time empirical performance. At the same time, we make connections to the recent literature on when exploration-free methods are effective. Indeed, if the distribution of contexts is well behaved, then our algorithm acts mostly greedily and enjoys sub-logarithmic regret. Furthermore, our approach is adaptive in the sense that it automatically detects the nice case. Numerical results demonstrate significant regret reductions by our method relative to several baselines.', 'corpus_id': 204734398, 'score': 1}, {'doc_id': '207852695', 'title': 'Neural Contextual Bandits with Upper Confidence Bound-Based Exploration', 'abstract': 'We study the stochastic contextual bandit problem, where the reward is generated from an unknown bounded function with additive noise. We propose the NeuralUCB algorithm, which leverages the representation power of deep neural networks and uses a neural network-based random feature mapping to construct an upper confidence bound (UCB) of reward for efficient exploration. We prove that, under mild assumptions, NeuralUCB achieves $\\tilde O(\\sqrt{T})$ regret, where $T$ is the number of rounds. To the best of our knowledge, our algorithm is the first neural network-based contextual bandit algorithm with near-optimal regret guarantee. Preliminary experiment results on synthetic data corroborate our theory, and shed light on potential applications of our algorithm to real-world problems.', 'corpus_id': 207852695, 'score': 1}, {'doc_id': '199490452', 'title': 'Minimax Optimal Algorithms for Adversarial Bandit Problem With Multiple Plays', 'abstract': 'We investigate the adversarial bandit problem with multiple plays under semi-bandit feedback. We introduce a highly efficient algorithm that asymptotically achieves the performance of the best switching <inline-formula><tex-math notation=""LaTeX"">$m$</tex-math></inline-formula>-arm strategy with minimax optimal regret bounds. To construct our algorithm, we introduce a new expert advice algorithm for the multiple-play setting. By using our expert advice algorithm, we additionally improve the best-known high-probability bound for the multi-play setting by <inline-formula><tex-math notation=""LaTeX"">$O(\\sqrt{m})$</tex-math></inline-formula>. Our results are guaranteed to hold in an individual sequence manner since we have no statistical assumption on the bandit arm gains. Through an extensive set of experiments involving synthetic and real data, we demonstrate significant performance gains achieved by the proposed algorithm with respect to the state-of-the-art algorithms.', 'corpus_id': 199490452, 'score': 0}, {'doc_id': '51882545', 'title': 'Problem Dependent Reinforcement Learning Bounds Which Can Identify Bandit Structure in MDPs', 'abstract': 'In order to make good decision under uncertainty an agent must learn from observations. To do so, two of the most common frameworks are Contextual Bandits and Markov Decision Processes (MDPs). In this paper, we study whether there exist algorithms for the more general framework (MDP) which automatically provide the best performance bounds for the specific problem at hand without user intervention and without modifying the algorithm. In particular, it is found that a very minor variant of a recently proposed reinforcement learning algorithm for MDPs already matches the best possible regret bound $\\tilde O (\\sqrt{SAT})$ in the dominant term if deployed on a tabular Contextual Bandit problem despite the agent being agnostic to such setting.', 'corpus_id': 51882545, 'score': 0}, {'doc_id': '208202024', 'title': 'Parameter-Free Locally Differentially Private Stochastic Subgradient Descent', 'abstract': 'We consider the problem of minimizing a convex risk with stochastic subgradients guaranteeing $\\epsilon$-locally differentially private ($\\epsilon$-LDP). While it has been shown that stochastic optimization is possible with $\\epsilon$-LDP via the standard SGD (Song et al., 2013), its convergence rate largely depends on the learning rate, which must be tuned via repeated runs. Further, tuning is detrimental to privacy loss since it significantly increases the number of gradient requests. In this work, we propose BANCO (Betting Algorithm for Noisy COins), the first $\\epsilon$-LDP SGD algorithm that essentially matches the convergence rate of the tuned SGD without any learning rate parameter, reducing privacy loss and saving privacy budget.', 'corpus_id': 208202024, 'score': 0}]"
62	{'doc_id': '16220287', 'title': 'Breathing as a Fundamental Rhythm of Brain Function', 'abstract': 'Ongoing fluctuations of neuronal activity have long been considered intrinsic noise that introduces unavoidable and unwanted variability into neuronal processing, which the brain eliminates by averaging across population activity (Georgopoulos et al., 1986; Lee et al., 1988; Shadlen and Newsome, 1994; Maynard et al., 1999). It is now understood, that the seemingly random fluctuations of cortical activity form highly structured patterns, including oscillations at various frequencies, that modulate evoked neuronal responses (Arieli et al., 1996; Poulet and Petersen, 2008; He, 2013) and affect sensory perception (Linkenkaer-Hansen et al., 2004; Boly et al., 2007; Sadaghiani et al., 2009; Vinnik et al., 2012; Palva et al., 2013). Ongoing cortical activity is driven by proprioceptive and interoceptive inputs. In addition, it is partially intrinsically generated in which case it may be related to mental processes (Fox and Raichle, 2007; Deco et al., 2011). Here we argue that respiration, via multiple sensory pathways, contributes a rhythmic component to the ongoing cortical activity. We suggest that this rhythmic activity modulates the temporal organization of cortical neurodynamics, thereby linking higher cortical functions to the process of breathing.', 'corpus_id': 16220287}	14775	[{'doc_id': '234348738', 'title': 'Theta phase mediates deliberate action switching in human Supplementary Motor Areas', 'abstract': 'The ability to deliberately overwrite ongoing automatic actions is a necessary feature of adaptive behavior. It has been proposed that the supplementary motor areas (SMAs) operate as a controller that orchestrates the switching between automatic and deliberate processes by inhibiting ongoing behaviors and so facilitating the execution of alternative ones. In addition, previous studies support the involvement of SMAs theta waves (4-9 Hz) in cognitive control. However, the exact role of such oscillatory dynamics and their contribution to the control of action are not fully understood. To investigate the mechanisms by which the SMAs support direct control of deliberate behavior, we recorded intracranial electroencephalography (iEEG) activity in humans performing a motor sequence task. Subjects had to perform a “change of plans” motor task requiring habitual movements to be overwritten at unpredictable moments. We found that SMAs were exclusively active during trials that demand action reprogramming in response to the unexpected cue but were silent during automatic action execution. Importantly, SMAs activity was characterized by a distinct temporal pattern, expressed in a stereotypical phase alignment of theta oscillations. More specifically, single trial motor performance was correlated with the trial contribution to the global inter-trial phase coherence, with higher coherence associated with faster trials. In addition, theta phase modulated the amplitude of gamma oscillations, with higher cross-frequency coupling in faster trials. Our results suggest that within frontal cortical networks, theta oscillations could encode a control signal that promotes the execution of deliberate actions.', 'corpus_id': 234348738, 'score': 0}, {'doc_id': '27777215', 'title': 'Nasal Respiration Entrains Human Limbic Oscillations and Modulates Cognitive Function', 'abstract': 'The need to breathe links the mammalian olfactory system inextricably to the respiratory rhythms that draw air through the nose. In rodents and other small animals, slow oscillations of local field potential activity are driven at the rate of breathing (∼2–12 Hz) in olfactory bulb and cortex, and faster oscillatory bursts are coupled to specific phases of the respiratory cycle. These dynamic rhythms are thought to regulate cortical excitability and coordinate network interactions, helping to shape olfactory coding, memory, and behavior. However, while respiratory oscillations are a ubiquitous hallmark of olfactory system function in animals, direct evidence for such patterns is lacking in humans. In this study, we acquired intracranial EEG data from rare patients (Ps) with medically refractory epilepsy, enabling us to test the hypothesis that cortical oscillatory activity would be entrained to the human respiratory cycle, albeit at the much slower rhythm of ∼0.16–0.33 Hz. Our results reveal that natural breathing synchronizes electrical activity in human piriform (olfactory) cortex, as well as in limbic-related brain areas, including amygdala and hippocampus. Notably, oscillatory power peaked during inspiration and dissipated when breathing was diverted from nose to mouth. Parallel behavioral experiments showed that breathing phase enhances fear discrimination and memory retrieval. Our findings provide a unique framework for understanding the pivotal role of nasal breathing in coordinating neuronal oscillations to support stimulus processing and behavior. SIGNIFICANCE STATEMENT Animal studies have long shown that olfactory oscillatory activity emerges in line with the natural rhythm of breathing, even in the absence of an odor stimulus. Whether the breathing cycle induces cortical oscillations in the human brain is poorly understood. In this study, we collected intracranial EEG data from rare patients with medically intractable epilepsy, and found evidence for respiratory entrainment of local field potential activity in human piriform cortex, amygdala, and hippocampus. These effects diminished when breathing was diverted to the mouth, highlighting the importance of nasal airflow for generating respiratory oscillations. Finally, behavioral data in healthy subjects suggest that breathing phase systematically influences cognitive tasks related to amygdala and hippocampal functions.', 'corpus_id': 27777215, 'score': 1}, {'doc_id': '4485658', 'title': 'Respiration-Entrained Brain Rhythms Are Global but Often Overlooked', 'abstract': 'We revisit recent evidence showing that nasal respiration entrains oscillations at the same frequency as breathing in several regions of the rodent brain. Moreover, respiration modulates the amplitude of a specific gamma sub-band (70-120Hz), most prominently in frontal regions. Since rodents often breathe at delta and theta frequencies, we caution that previous studies on delta and theta power and their cross-regional synchrony, as well as on delta-gamma and theta-gamma coupling, may have detected the respiration-entrained rhythm and respiration-gamma coupling. We argue that the simultaneous tracking of respiration along with electrophysiological recordings is necessary to properly identify brain oscillations. We hypothesize that respiration-entrained oscillations aid long-range communication in the brain.', 'corpus_id': 4485658, 'score': 1}, {'doc_id': '3008175', 'title': 'Selective entrainment of gamma subbands by different slow network oscillations', 'abstract': 'Significance Theta-gamma coupling has been largely documented in hippocampal and neocortical areas and hypothesized to constitute a network mechanism for information processing. However, we identify here another global slow rhythm at near-theta frequency that also couples to gamma. By simultaneously recording respiration, we could distinguish actual theta oscillations from a respiration-entrained rhythm (RR) in the local field potential whose peak frequency may overlap with theta. We demonstrate a robust specificity for the coupling of different gamma subbands to either theta or RR depending on brain state and region. The results suggest that the brain uses different frequency channels for transferring different types of information. Theta oscillations (4–12 Hz) are thought to provide a common temporal reference for the exchange of information among distant brain networks. On the other hand, faster gamma-frequency oscillations (30–160 Hz) nested within theta cycles are believed to underlie local information processing. Whether oscillatory coupling between global and local oscillations, as showcased by theta-gamma coupling, is a general coding mechanism remains unknown. Here, we investigated two different patterns of oscillatory network activity, theta and respiration-induced network rhythms, in four brain regions of freely moving mice: olfactory bulb (OB), prelimbic cortex (PLC), parietal cortex (PAC), and dorsal hippocampus [cornu ammonis 1 (CA1)]. We report differential state- and region-specific coupling between the slow large-scale rhythms and superimposed fast oscillations. During awake immobility, all four regions displayed a respiration-entrained rhythm (RR) with decreasing power from OB to CA1, which coupled exclusively to the 80- to 120-Hz gamma subband (γ2). During exploration, when theta activity was prevailing, OB and PLC still showed exclusive coupling of RR with γ2 and no theta-gamma coupling, whereas PAC and CA1 switched to selective coupling of theta with 40- to 80-Hz (γ1) and 120- to 160-Hz (γ3) gamma subbands. Our data illustrate a strong, specific interaction between neuronal activity patterns and respiration. Moreover, our results suggest that the coupling between slow and fast oscillations is a general brain mechanism not limited to the theta rhythm.', 'corpus_id': 3008175, 'score': 1}, {'doc_id': '233175685', 'title': 'Neocortical rhythm entrainment by parvalbumin-positive interneurons across cortical layers', 'abstract': 'Neocortical interneurons provide local inhibition responsible for organizing neuronal activity into brain oscillations that subserve several functions such as memory, attention and neuronal communication. However, little is known about the contribution of interneurons to the entrainment of neocortical oscillations across cortical layers. Here, using layer-specific optogenetic stimulations with micro-Light-Emitting-Diode (μLED) arrays, directed toward parvalbumin-expressing (PV) interneurons in non-anesthetized awake mice, we find that supragranular layer stimulations of PV neurons were most efficient at entraining supragranular layer neurons and local field potential (LFP) oscillations at gamma frequencies (γ: 25 - 80 Hz), whereas infragranular layer stimulation of PV neurons better entrained delta (δ: 2 - 5 Hz) and theta (θ: 6 - 10 Hz) frequency LFP oscillations. We found that PV neurons tightly control the transmission of multiple rhythms to the network across cortical layers in an orientation-selective manner. Intrinsic resonant properties of neurons could underlie such layer-specific properties of rhythm entrainment.', 'corpus_id': 233175685, 'score': 0}, {'doc_id': '6650980', 'title': 'Organization of prefrontal network activity by respiration-related oscillations', 'abstract': 'The medial prefrontal cortex (mPFC) integrates information from cortical and sub-cortical areas and contributes to the planning and initiation of behaviour. A potential mechanism for signal integration in the mPFC lies in the synchronization of neuronal discharges by theta (6–12\u2009Hz) activity patterns. Here we show, using in vivo local field potential (LFP) and single-unit recordings from awake mice, that prominent oscillations in the sub-theta frequency band (1–5\u2009Hz) emerge during awake immobility in the mPFC. These oscillation patterns are distinct from but phase-locked to hippocampal theta activity and occur synchronized with nasal respiration (hence termed prefrontal respiration rhythm [PRR]). PRR activity modulates the amplitude of prefrontal gamma rhythms with greater efficacy than theta oscillations. Furthermore, single-unit discharges of putative pyramidal cells and GABAergic interneurons are entrained by prefrontal PRR and nasal respiration. Our data thus suggest that PRR activity contributes to information processing in the prefrontal neuronal network.', 'corpus_id': 6650980, 'score': 1}, {'doc_id': '232272044', 'title': 'Laminar Profile of Auditory Steady-State Response in the Auditory Cortex of Awake Mice', 'abstract': 'Objective Auditory steady-state response (ASSR) is a gamma oscillation evoked by periodic auditory stimuli, which is commonly used in clinical electroencephalographic examination to evaluate the neurological functions. Though it has been suggested that auditory cortex is the origin of ASSR, how the laminar architecture of the neocortex contributes to the ASSR recorded from the brain surface remains unclear. Methods We used a 16-channel silicon probe to record the local field potential and the single-unit spike activity in the different layers of the auditory cortex of unanesthetized mice. Click-trains with a repetition rate at 40-Hz were present as sound stimuli to evoke ASSR. Results We found that the LFPs of all cortical layers showed a stable ASSR synchronizing to the 40-Hz click stimuli, while the ASSR was strongest in the granular (thalamorecipient) layer. Furthermore, time-frequency analyses also revealed the strongest coherence between the signals recorded from the granular layer and pial surface. Conclusion Our results reveal that the 40-Hz ASSR primarily shows the evoked gamma oscillation of thalamorecipient layers in the neocortex, and that the ASSR may be a biomarker to detect the cognitive deficits associated with impaired thalamo-cortical connection.', 'corpus_id': 232272044, 'score': 0}, {'doc_id': '207603646', 'title': 'Breathing above the brain stem: volitional control and attentional modulation in humans.', 'abstract': 'Whereas the neurophysiology of respiration has traditionally focused on automatic brain stem processes, higher brain mechanisms underlying the cognitive aspects of breathing are gaining increasing interest. Therapeutic techniques have used conscious control and awareness of breathing for millennia with little understanding of the mechanisms underlying their efficacy. Using direct intracranial recordings in humans, we correlated cortical and limbic neuronal activity as measured by the intracranial electroencephalogram (iEEG) with the breathing cycle. We show this to be the direct result of neuronal activity, as demonstrated by both the specificity of the finding to the cortical gray matter and the tracking of breath by the gamma-band (40-150 Hz) envelope in these structures. We extend prior observations by showing the iEEG signal to track the breathing cycle across a widespread network of cortical and limbic structures. We further demonstrate a sensitivity of this tracking to cognitive factors by using tasks adapted from cognitive behavioral therapy and meditative practice. Specifically, volitional control and awareness of breathing engage distinct but overlapping brain circuits. During volitionally paced breathing, iEEG-breath coherence increases in a frontotemporal-insular network, and during attention to breathing, we demonstrate increased coherence in the anterior cingulate, premotor, insular, and hippocampal cortices. Our findings suggest that breathing can act as an organizing hierarchical principle for neuronal oscillations throughout the brain and detail mechanisms of how cognitive factors impact otherwise automatic neuronal processes during interoceptive attention. NEW & NOTEWORTHY Whereas the link between breathing and brain activity has a long history of application to therapy, its neurophysiology remains unexplored. Using intracranial recordings in humans, we show neuronal activity to track the breathing cycle throughout widespread cortical/limbic sites. Volitional pacing of the breath engages frontotemporal-insular cortices, whereas attention to automatic breathing modulates the cingulate cortex. Our findings imply a fundamental role of breathing-related oscillations in driving neuronal activity and provide insight into the neuronal mechanisms of interoceptive attention.', 'corpus_id': 207603646, 'score': 1}, {'doc_id': '233383068', 'title': 'Thalamocortical excitability modulation guides human perception under uncertainty', 'abstract': 'Knowledge about the relevance of environmental features can guide stimulus processing. However, it remains unclear how processing is adjusted when feature relevance is uncertain. We hypothesized that (a) heightened uncertainty would shift cortical networks from a rhythmic, selective processing-oriented state toward an asynchronous (“excited”) state that boosts sensitivity to all stimulus features, and that (b) the thalamus provides a subcortical nexus for such uncertainty-related shifts. Here, we had young adults attend to varying numbers of task-relevant features during EEG and fMRI acquisition to test these hypotheses. Behavioral modeling and electrophysiological signatures revealed that greater uncertainty lowered the rate of evidence accumulation for individual stimulus features, shifted the cortex from a rhythmic to an asynchronous/excited regime, and heightened neuromodulatory arousal. Crucially, this unified constellation of within-person effects was dominantly reflected in the uncertainty-driven upregulation of thalamic activity. We argue that neuromodulatory processes involving the thalamus play a central role in how the brain modulates neural excitability in the face of momentary uncertainty.', 'corpus_id': 233383068, 'score': 0}, {'doc_id': '233449793', 'title': 'Beta bursting in the retrosplenial cortex is a neurophysiological correlate of environmental novelty which is disrupted in a mouse model of Alzheimer’s disease', 'abstract': 'The retrosplenial cortex (RSC) plays a significant role in spatial learning and memory, and is functionally disrupted in the early stages of Alzheimer’s disease. In order to investigate neurophysiological correlates of spatial learning and memory in this region we employed in vivo electrophysiology in awake, behaving mice, comparing neural activity between wild-type and J20 mice, a mouse model of Alzheimer’s disease-associated amyloidopathy. To determine the response of the RSC to environmental novelty local field potentials were recorded while mice explored novel and familiar recording arenas. In familiar environments we detected short, phasic bursts of beta (20-30 Hz) oscillations (beta bursts) which arose at a low but steady rate. Exposure to a novel environment rapidly initiated a dramatic increase in the rate, size and duration of beta bursts. Additionally, theta-beta cross-frequency coupling was significantly higher during novelty, and spiking of neurons in the RSC was significantly enhanced during beta bursts. Finally, aberrant beta bursting was seen in J20 mice, including increased beta bursting during novelty and familiarity, yet a loss of coupling between beta bursts and spiking activity. These findings, support the concept that beta bursting may be responsible for the activation and reactivation of neuronal ensembles underpinning the formation and maintenance of cortical representations, and that disruptions to this activity in J20 mice may underlie cognitive impairments seen in these animals.', 'corpus_id': 233449793, 'score': 0}]
63	{'doc_id': '214611621', 'title': 'Learning Better Lossless Compression Using Lossy Compression', 'abstract': 'We leverage the powerful lossy image compression algorithm BPG to build a lossless image compression system. Specifically, the original image is first decomposed into the lossy reconstruction obtained after compressing it with BPG and the corresponding residual. We then model the distribution of the residual with a convolutional neural network-based probabilistic model that is conditioned on the BPG reconstruction, and combine it with entropy coding to losslessly encode the residual. Finally, the image is stored using the concatenation of the bitstreams produced by BPG and the learned residual coder. The resulting compression system achieves state-of-the-art performance in learned lossless full-resolution image compression, outperforming previous learned approaches as well as PNG, WebP, and JPEG2000.', 'corpus_id': 214611621}	4391	"[{'doc_id': '218568989', 'title': 'A Simple Clustering Strategy for Wireless Sensor Networks', 'abstract': 'Organizing nodes into efficient clusters in wireless sensor networks facilitates data aggregation and command dissemination, but clustering is a complex and costly process, since it has to be carried out in a distributed and periodic manner. In this letter, we propose a simple clustering strategy employing an adjacency matrix, which encodes nodes’ neighborhood and connectivity in a network. Our approach enables the assignment of cluster heads for multiple rounds in a single step, thereby limiting the cost of cluster head election and child node association.', 'corpus_id': 218568989, 'score': 0}, {'doc_id': '215745407', 'title': 'Variable Rate Image Compression Method with Dead-zone Quantizer', 'abstract': 'Deep learning based image compression methods have achieved superior performance compared with transform based conventional codec. With end-to-end Rate-Distortion Optimization (RDO) in the codec, compression model is optimized with Lagrange multiplier λ. For conventional codec, signal is decorrelated with orthonormal transformation, and uniform quantizer is introduced. We propose a variable rate image compression method with dead-zone quantizer. Firstly, the autoencoder network is trained with RaDOGAGA [6] framework, which can make the latents isometric to the metric space, such as SSIM and MSE. Then the conventional dead-zone quantization method with arbitrary step size is used in the common trained network to provide the flexible rate control. With dead-zone quantizer, the experimental results show that our method performs comparably with independently optimized models within a wide range of bitrate.', 'corpus_id': 215745407, 'score': 1}, {'doc_id': '218551728', 'title': 'IKONOS: An intelligent tool to support diagnosis of Covid-19 by texture analysis of x-ray images', 'abstract': 'In late 2019, the SARS-Cov-2 spread worldwide. The virus has high rates of proliferation and causes severe respiratory symptoms, such as pneumonia. There is still no specific treatment and diagnosis for the disease. The standard diagnostic method for pneumonia is chest X-ray image. There are many advantages to using Covid-19 diagnostic X-rays: low cost, fast and widely available. We propose an intelligent system to support diagnosis by X-ray images.We tested Haralick and Zernike moments for feature extraction. Experiments with classic classifiers were done. Support vector machines stood out, reaching an average accuracy of 89:78%, average recall and sensitivity of 0:8979, and average precision and specificity of 0:8985 and 0:9963 respectively. The system is able to differentiate Covid-19 from viral and bacterial pneumonia, with low computational cost.', 'corpus_id': 218551728, 'score': 0}, {'doc_id': '211171825', 'title': 'Variable-Bitrate Neural Compression via Bayesian Arithmetic Coding', 'abstract': 'Deep Bayesian latent variable models have enabled new approaches to both model and data compression. Here, we propose a new algorithm for compressing latent representations in deep probabilistic models, such as variational autoencoders, in post-processing. The approach thus separates model design and training from the compression task. Our algorithm generalizes arithmetic coding to the continuous domain, using adaptive discretization accuracy that exploits estimates of posterior uncertainty. A consequence of the ""plug and play"" nature of our approach is that various rate-distortion trade-offs can be achieved with a single trained model, eliminating the need to train multiple models for different bit rates. Our experimental results demonstrate the importance of taking into account posterior uncertainties, and show that image compression with the proposed algorithm outperforms JPEG over a wide range of bit rates using only a single machine learning model. Further experiments on Bayesian neural word embeddings demonstrate the versatility of the proposed method.', 'corpus_id': 211171825, 'score': 1}, {'doc_id': '211082524', 'title': 'Hierarchical Auto-Regressive Model for Image Compression Incorporating Object Saliency and a Deep Perceptual Loss', 'abstract': 'We propose a new end-to-end trainable model for lossy image compression which includes a number of novel components. This approach incorporates 1) a hierarchical auto-regressive model; 2)it also incorporates saliency in the images and focuses on reconstructing the salient regions better; 3) in addition, we empirically demonstrate that the popularly used evaluations metrics such as MS-SSIM and PSNR are inadequate for judging the performance of deep learned image compression techniques as they do not align well with human perceptual similarity. We, therefore propose an alternative metric, which is learned on perceptual similarity data specific to image compression. \nOur experiments show that this new metric aligns significantly better with human judgments when compared to other hand-crafted or learned metrics. The proposed compression model not only generates images that are visually better but also gives superior performance for subsequent computer vision tasks such as object detection and segmentation when compared to other engineered or learned codecs.', 'corpus_id': 211082524, 'score': 1}, {'doc_id': '211988449', 'title': 'G-VAE: A Continuously Variable Rate Deep Image Compression Framework', 'abstract': 'Rate adaption of deep image compression in a single model will become one of the decisive factors competing with the classical image compression codecs. However, until now, there is no perfect solution that neither increases the computation nor affects the compression performance. In this paper, we propose a novel image compression framework G-VAE (Gained Variational Autoencoder), which could achieve continuously variable rate in a single model. Unlike the previous solutions that encode progressively or change the internal unit of the network, G-VAE only adds a pair of gain units at the output of encoder and the input of decoder. It is so concise that G-VAE could be applied to almost all the image compression methods and achieve continuously variable rate with negligible additional parameters and computation. We also propose a new deep image compression framework, which outperforms all the published results on Kodak datasets in PSNR and MS-SSIM metrics. Experimental results show that adding a pair of gain units will not affect the performance of the basic models while endowing them with continuously variable rate.', 'corpus_id': 211988449, 'score': 1}, {'doc_id': '215548235', 'title': 'Feedback Recurrent Autoencoder for Video Compression', 'abstract': 'Recent advances in deep generative modeling have enabled efficient modeling of high dimensional data distributions and opened up a new horizon for solving data compression problems. Specifically, autoencoder based learned image or video compression solutions are emerging as strong competitors to traditional approaches. In this work, We propose a new network architecture, based on common and well studied components, for learned video compression operating in low latency mode. Our method yields state of the art MS-SSIM/rate performance on the high-resolution UVG dataset, among both learned video compression approaches and classical video compression methods (H.265 and H.264) in the rate range of interest for streaming applications. Additionally, we provide an analysis of existing approaches through the lens of their underlying probabilistic graphical models. Finally, we point out issues with temporal consistency and color shift observed in empirical evaluation, and suggest directions forward to alleviate those.', 'corpus_id': 215548235, 'score': 1}, {'doc_id': '218571711', 'title': 'Décision kinésithérapique : Alexandre L. 57 ans : Kinésithérapie et Covid-19 en réanimation, de la phase aiguë à la réhabilitation', 'abstract': '\n Résumé\n \n Un homme de 57 ans est hospitalisé en réanimation pour un syndrome de détresse respiratoire aigüe (SDRA) lié à une infection à Covid-19. Après une première phase au cours de laquelle le patient est sédaté et curarisé, la kinésithérapie consiste à mobiliser passivement le patient, à participer au décubitus ventral; la kinésithérapie respiratoire n’étant pas forcément nécessaire. Dans un second temps, l’extubation est possible et plusieurs aspects sont développés\xa0: la kinésithérapie respiratoire, l’oxygénation, la déglutition et surtout la réhabilitation. Cependant, des atteintes du parenchyme pulmonaire abaissent de façon importante la saturation en oxygène au cours des exercices. L’oxygénation à haut débit et/ou la ventilation non-invasive (VNI) permettent d’optimiser la réhabilitation chez ce patient avec une réserve respiratoire encore précaire.\n Indice de factualité (i-FACT): 3.2\n \n \n Abstract\n \n A 57-year-old man is hospitalized in intensive care for an acute respiratory distress syndrome related to a Covid-19 infection. After a first phase during which the patient is sedated and nerve-blocked, physiotherapy consists in passively mobilizing the patient, participating in the prone position, respiratory physiotherapy is not necessary. In a second step, extubation is possible and several aspects are developed: respiratory physiotherapy, oxygenation, swallowing and rehabilitation. However, damage to the lung significantly decreases oxygen saturation during exercise. High-flow nasal oxygenation and / or non-invasive ventilation (NIV) can optimize rehabilitation in this patient with a still precarious respiratory function.\n Evidence index (EVID-i): 3.2\n \n', 'corpus_id': 218571711, 'score': 0}, {'doc_id': '218560305', 'title': 'Simulating Combat to Explore Motivations Behind Why Military Members Make Costly Sacrifices', 'abstract': 'Why are soldiers, sailors, airmen, and marines willing to make costly sacrifices? Previous research suggests loyalty (e.g., duty) to teammates is important among other reasons. More recently, studies conducted overseas have identified sacred values (i.e., values held so deeply they are immune to material tradeoffs) and group identity fusion as primary factors. Importantly, however, these studies have been conducted using survey-based and other social science methods which assess attitudes and beliefs, but not behavior. For example, it is one thing for a respondent to say they would jump on a grenade to sacrifice for their group but another to actually jump on a grenade in real life. Thus, we have developed a simulation to help bridge the gap between what people say and do in life-or-death scenarios. This high-fidelity simulation was developed to provide a more immersive means of testing realistic, “shoot or no shoot” hostage scenarios. Using feedback from individuals with military experience, the scenarios were designed to elicit more real-life stress than attitude-based surveys. This paper describes the systems engineering process we used to design the simulation as well as the proof-of-concept study developed to explore reasons behind why people are willing to make costly sacrifices. Early pilot data have revealed that values and identities related to religion, risk to self, and the Air Force predicted engagement decisions of Air Force cadets, in a series of simulated hostage scenarios. Possibilities for future use of this simulation will also be discussed. For example, while this experimental setup lacks high stakes consequences, this simulation could be useful for selection and training in addition to a research tool for studying motivations in different simulated combat environments.', 'corpus_id': 218560305, 'score': 0}, {'doc_id': '218569100', 'title': 'The Digital Transformation Of Education In India During The Period Of Lockdown Due To Covid-19', 'abstract': 'This paper sought to an act of measuring the impact of COVID-19 pandemic in unleashing digital transformation in the education sector in India. In order to measure the impact, the study tracked the rate at which the virtual tools were used by various schools and institutions during the COVID-19 lockdown. Data were obtained from secondary sources, mainly newspaper articles, magazines and peer-reviewed journals. The findings are that, in India, during the lockdown, a variety of virtual tools were unleashed from primary education to higher and tertiary education where educational activities switched to online learning. These observations point to the fact that India, generally, has some pockets of excellence to drive the education sector to the next level, which has the potential to increase access. Access to education has always been a challenge due to a limited number of spaces available. Much as this pandemic has brought with it massive human suffering across the globe, there is an opportunity to assess successes and failures of deployed technologies, costs associated with them, and scaling these technologies to improve access. Index Terms – COVID-19, Digital Transformation, Education.', 'corpus_id': 218569100, 'score': 0}]"
64	"{'doc_id': '206032868', 'title': 'Analysis of bacterial function by multi-colour fluorescence flow cytometry and single cell sorting.', 'abstract': ""With the increased awareness of the problems associated with the growth dependent analysis of bacterial populations, direct optical detection methods such as flow cytometry have enjoyed increased popularity over the last few years. Among the analyses discussed here are: (1) Bacterial discrimination from other particles on the basis of nucleic acid staining, using sample disaggregation to provide fast reliable enumeration while minimizing data artefacts due to post sampling growth; (2) Determination of basic cell functions such as reproductive ability, metabolic activity and membrane integrity, to characterise the physiological state or degree of viability of bacteria; and (3) The use of single cell sorting onto agar plates, microscope slides or into multi-well plates to correlate viability as determined by cell growth with fluorescent labelling techniques. Simultaneous staining with different fluorochromes provides an extremely powerful way to demonstrate culture heterogeneity, and also to understand the functional differences revealed by each stain in practical applications. Analysis of bacterial fermentations showed a considerable drop (20%) in membrane potential and integrity during the latter stages of small scale (5L), well mixed fed-batch fermentations. These changes, not found in either batch or continuous culture fermentations, are probably due to the severe, steadily increasing stress associated with glucose limitation during the fed-batch process, suggesting 'on-line' flow cytometry could improve process control. Heat injured cells can already show up to 4 log of differences in recovery in different pre-enrichment media, thus contributing to the problem of viable but non-culturable cells (VBNC's). Cytometric cell sorting demonstrated decreasing recovery with increasing loss of membrane function. However, a new medium protecting the cells from intracellular and extracellular causes of oxidative stress improved recovery considerably. Actively respiring cells showed much higher recovery improvement than the other populations, demonstrating for the first time the contribution of oxidative respiration to intracellular causes of damage as a key part of the VBNC problem. Finally, absolute and relative frequencies of one species in a complex population were determined using immunofluorescent labelling in combination with the analysis of cell function. The detail and precision of multiparameter flow cytometric measurements of cell function at the single cell level now raise questions regarding the validity of classical, growth dependent viability assessment methods."", 'corpus_id': 206032868}"	1520	"[{'doc_id': '212421062', 'title': 'Metabolic host response and therapeutic approaches to influenza infection', 'abstract': 'Based on available metabolomic studies, influenza infection affects a variety of cellular metabolic pathways to ensure an optimal environment for its replication and production of viral particles. Following infection, glucose uptake and aerobic glycolysis increase in infected cells continually, which results in higher glucose consumption. The pentose phosphate shunt, as another glucose-consuming pathway, is enhanced by influenza infection to help produce more nucleotides, especially ATP. Regarding lipid species, following infection, levels of triglycerides, phospholipids, and several lipid derivatives undergo perturbations, some of which are associated with inflammatory responses. Also, mitochondrial fatty acid β-oxidation decreases significantly simultaneously with an increase in biosynthesis of fatty acids and membrane lipids. Moreover, essential amino acids are demonstrated to decline in infected tissues due to the production of large amounts of viral and cellular proteins. Immune responses against influenza infection, on the other hand, could significantly affect metabolic pathways. Mainly, interferon (IFN) production following viral infection affects cell function via alteration in amino acid synthesis, membrane composition, and lipid metabolism. Understanding metabolic alterations required for influenza virus replication has revealed novel therapeutic methods based on targeted inhibition of these cellular metabolic pathways.', 'corpus_id': 212421062, 'score': 0}, {'doc_id': '212628375', 'title': 'Olive oil by-product as functional ingredient in bakery products. Influence of processing and evaluation of biological effects.', 'abstract': 'Nowadays, the strong demand for adequate nutrition is accompanied by concern about environmental pollution and there is a considerable emphasis on the recovery and recycling of food by-products and wastes. In this study, we focused on the exploitation of olive pomace as functional ingredient in biscuits and bread. Standard and enriched bakery products were made using different flours and fermentation protocols. After characterization, they were in vitro digested and used for supplementation of intestinal cells (Caco-2), which underwent exogenous inflammation. The enrichment caused a significant increase in the phenolic content in all products, particularly in the sourdough fermented ones. Sourdough fermentation also increased tocol concentration. The increased concentration of bioactive molecules did not reflect the anti-inflammatory effect, which was modulated by the baking procedure. Conventionally fermented bread enriched with 4% pomace and sourdough fermented, not-enriched bread had the greatest anti-inflammatory effect, significantly reducing IL-8 secretion in Caco-2 cells. The cell metabolome was modified only after supplementation with sourdough fermented bread enriched with 4% pomace, probably due to the high concentration of tocopherol that acted synergistically with polyphenols. Our data highlight that changes in chemical composition cannot predict changes in functionality. It is conceivable that matrices (including enrichment) and processing differently modulated bioactive bioaccessibility, and consequently functionality.', 'corpus_id': 212628375, 'score': 0}, {'doc_id': '6769218', 'title': 'An industrial application of multiparameter flow cytometry: assessment of cell physiological state and its application to the study of microbial fermentations.', 'abstract': 'BACKGROUND\nWhen using traditional microbiological techniques to monitor cell proliferation and viability, stressed, sublethally injured, or otherwise ""viable but nonculturable"" cells often go undetected. Because of this, such cells often are not considered by mathematical models used to predict bioprocess performance on scale-up and inaccuracies result. Therefore, analytical techniques, decoupled from postsampling growth, are desirable to rapidly monitor individual cell physiologic states during microbial fermentations.\n\n\nMETHODS\nMicrobial cells, including Escherichia coli, Rhodococus sp., and Sacharomyces cerevisiae, were taken at various stages from a range of fermentation processes and stained with one of three mixtures of fluorescent stains: rhodamine 123/propidium iodide, bis-oxonol/propidium iodide, or bis-oxonol/ethidium bromide/propidium iodide. An individual cell\'s physiologic state was assessed with a Coulter Epics Elite analyzer based on the differential uptakes of these fluorescent stains.\n\n\nRESULTS\nIt was possible to resolve an individual cell\'s physiologic state beyond culturability based on the functionality of dye extrusion pumps and the presence or absence of an intact polarized cytoplasmic membrane, enabling assessment of population heterogeneity. This approach allows the simultaneous differentiation of at least four functional subpopulations in microbial populations.\n\n\nCONCLUSIONS\nFluorescent staining methods used in our laboratories have led to a functional classification of the physiological state of individual microbial cells based on reproductive activity, metabolic activity, and membrane integrity. We have used these techniques extensively for monitoring the stress responses of microorganisms in such diverse areas as bioremediation, biotransformation, food processing, and microbial fermentation; microbial fermentation is discussed in this article.', 'corpus_id': 6769218, 'score': 1}, {'doc_id': '210169449', 'title': 'An Integrated In Vitro–In Silico Approach for Silver Nanoparticle Dosimetry in Cell Cultures', 'abstract': 'Potential human and environmental hazards resulting from the exposure of living organisms to silver nanoparticles (Ag NPs) have been the subject of intensive discussion in the last decade. Despite the growing use of Ag NPs in biomedical applications, a quantification of the toxic effects as a function of the total silver mass reaching cells (namely, target cell dose) is still needed. To provide a more accurate dose-response analysis, we propose a novel integrated approach combining well-established computational and experimental methodologies. We first used a particokinetic model (ISD3) for providing experimental validation of computed Ag NP sedimentation in static-cuvette experiments. After validation, ISD3 was employed to predict the total mass of silver reaching human endothelial cells and hepatocytes cultured in 96 well plates. Cell viability measured after 24 h of culture was then related to this target cell dose. Our results show that the dose perceived by the cell monolayer after 24 h of exposure is around 85% lower than the administered nominal media concentration. Therefore, accurate dosimetry considering particle characteristics and experimental conditions (e.g., time, size and shape of wells) should be employed for better interpreting effects induced by the amount of silver reaching cells.', 'corpus_id': 210169449, 'score': 0}, {'doc_id': '54995438', 'title': 'A modified probing feeding strategy: control aspects', 'abstract': 'The paper presents a fermentation technique, which combines the advantages of the probing feeding strategy and the temperature limited fed-batch technique. The early phase of the cultivation is run under glucose limited conditions by using the original probing technique. When the maximum oxygen transfer capacity of the reactor is reached, the temperature is decreased to lower the oxygen demand. A slight glucose excess, achieved by downwards probing pulses, guarantees that thetemperature is the only limiting factor. To achieve a good control of the dissolved oxygena mid-ranging controller manipulating the stirrer speed and the temperature is used. A model describing the temperature influence on the cellmetabolism is derived to facilitate the controller design. The feeding strategy is illustrated by an experiment. (Less)', 'corpus_id': 54995438, 'score': 1}, {'doc_id': '211677756', 'title': 'In search of biomarkers and the ideotype of barley tolerant to water scarcity', 'abstract': 'In barley plants, water shortage causes many changes on the morphological, physiological and biochemical levels resulting in the reduction of grain yield. In the present study the results of various experiments on the response of the same barley recombinant inbred lines to water shortage, including phenotypic, proteomic and metabolomic traits were integrated. Obtained results suggest that by a multi-omic approach it is possible to indicate proteomic and metabolomic traits important for reaction of barley plants to reduced water availability. Analysis of regression of drought effect (DE) for grain weight per plant on DE of proteomic and metabolomic traits allowed us to suggest ideotype of barley plants tolerant to water shortage. It was shown that grain weight under drought was determined significantly by six proteins in leaves and five in roots, the function of which were connected with defence mechanisms, ion/electron transport, carbon (in leaves) and nitrogen (in roots) metabolism, and in leaves additionally by two proteins of unknown function. Out of numerous metabolites detected in roots only Aspartic and Glutamic acids and one metabolite of unknown function, were found to have significant influence on grain weight per plant. The role of these traits as biomarkers, and especially as suggested targets of ideotype breeding, has to be further studied. One of the direction to be followed is genetic co-localization of proteomic, metabolomic and phenotypic traits in the genetic and physical maps of barley genome that can describe putative functional associations between traits; this is the next step of our analysis that is in progress.', 'corpus_id': 211677756, 'score': 0}, {'doc_id': '110365836', 'title': 'Bioreactor control using a probing feeding strategy and mid-ranging control', 'abstract': 'The paper presents a fed-batch fermentation technique for bioreactors operating close to their maximum oxygen transfer capacity. The method combines the advantages of the probing feeding strategy and the temperature limited fed-batch technique. When the maximum oxygen transfer capacity of the reactor is reached, the temperature is decreased to lower the oxygen demand. To achieve a good control of the dissolved oxygen a mid-ranging controller manipulating the stirrer speed and the temperature is used. The feeding strategy is analysed and it is also illustrated by simulations and an experiment. (c) 2006 Elsevier Ltd. All rights reserved.', 'corpus_id': 110365836, 'score': 1}, {'doc_id': '211076332', 'title': 'Defining mass transfer in a capillary wave micro-bioreactor for dose-response and other cell-based assays', 'abstract': 'For high-throughput cell culture and associated analytics, droplet-based cultivation systems open up the opportunities for parallelization and rapid data generation. In contrast to microfluidics with continuous flow, sessile droplet approaches enhance the flexibility for fluid manipulation with usually less operational effort. Generating biologically favorable conditions and promoting cell growth in a droplet, however, is particularly challenging due to mass transfer limitations, which has to be solved by implementing an effective mixing technique. Here, capillary waves induced by vertical oscillation are used to mix inside a sessile droplet micro-bioreactor (MBR) system avoiding additional moving parts inside the fluid. Depending on the excitation frequency, different patterns are formed on the oscillating liquid surface, which are described by a model of a vibrated sessile droplet. Analyzing mixing times and oxygen transport into the liquid, a strong dependency of mass transfer on the oscillation parameters, especially the excitation frequency, is demonstrated. Oscillations at distinct capillary wave resonant frequencies lead to rapid homogenization with mixing times of 2 s and volumetric liquid-phase mass transfer coefficients of more than 340 h-1. This shows that the mass transfer in a droplet MBR can be specifically controlled via capillary waves, what is subsequently demonstrated for cultivations of Escherichia coli BL21 cells. Therefore, the presented MBR in combination with vertical oscillation mixing for intensified mass transfer is a promising tool for highly parallel cultivation and data generation.', 'corpus_id': 211076332, 'score': 1}, {'doc_id': '38997703', 'title': 'Studies related to the scale-up of high-cell-density E. coli fed-batch fermentations using multiparameter flow cytometry: effect of a changing microenvironment with respect to glucose and dissolved oxygen concentration.', 'abstract': 'Multiparameter flow cytometric techniques developed in our laboratories have been used for the ""at-line"" study of fed-batch bacterial fermentations. These fermentations were done at two scales, production (20 m(3)) and bench (5 x 10(-3) m(3)). In addition, at the bench scale, experiments were undertaken where the difficulty of achieving good mixing (broth homogeneity), similar to that found at the production scale, was simulated by using a two-compartment model. Flow cytometric analysis of cells in broth samples, based on a dual-staining protocol, has revealed, for the first time, that a progressive change in cell physiological state generally occurs throughout the course of such fermentations. The technique has demonstrated that a changing microenvironment with respect to substrate concentration (glucose and dissolved oxygen tension [DOT]) has a profound effect on cell physiology and hence on viable biomass yield. The relatively poorly mixed conditions in the large-scale fermentor were found to lead to a low biomass yield, but, surprisingly, were associated with a high cell viability (with respect to cytoplasmic membrane permeability) throughout the fermentation. The small-scale fermentation that most clearly mimicked the large-scale heterogeneity (i.e., a region of high glucose concentration and low DOT analogous to a feed zone) gave similar results. On the other hand, the small-scale well-mixed fermentation gave the highest biomass yield, but again, surprisingly, the lowest cell viability. The scaled-down simulations with high DOT throughout and locally low or high glucose gave biomass and viabilities between. Reasons for these results are examined in terms of environmental stress associated with an ever-increasing glucose limitation in the well-mixed case. On the other hand, at the large scale, and to differing degrees in scale-down simulations, cells periodically encounter regions of relatively higher glucose concentration.', 'corpus_id': 38997703, 'score': 1}, {'doc_id': '210192446', 'title': 'Emerging organic contaminants in wastewater: Understanding electrochemical reactors for triclosan and its by-products degradation.', 'abstract': ""Degradation technologies applied to emerging organic contaminants from human activities are one of the major water challenges in the contamination legacy. Triclosan is an emerging contaminant, commonly used as antibacterial agent in personal care products. Triclosan is stable, lipophilic and it is proved to have ecotoxicologic effects in organics. This induces great concern since its elimination in wastewater treatment plants is not efficient and its by-products (e.g. methyl-triclosan, 2,4-dichlorophenol or 2,4,6-trichlorophenol) are even more hazardous to several environmental compartments. This work provides understanding of two different electrochemical reactors for the degradation of triclosan and its derivative by-products in effluent. A batch reactor and a flow reactor (mimicking a secondary settling tank in a wastewater treatment plant) were tested with two different working anodes: Ti/MMO and Nb/BDD. The degradation efficiency and kinetics were evaluated to find the best combination of current density, electrodes and set-up design. For both reactors the best electrode combination was achieved with Ti/MMO as anode. The batch reactor at 7\xa0mA/cm2 during 4\xa0h attained degradation rates below the detection limit for triclosan and 2,4,6-trichlorophenol and, 94% and 43% for 2,4-dichlorophenol and methyl triclosan, respectively. The flow reactor obtained, in approximately 1\xa0h, degradation efficiencies between 41% and 87% for the four contaminants. This study suggests an alternative technology for emerging organic contaminants degradation, since the combination of a low current density with the flow and matrix induced disturbance increases and speeds up the compounds' elimination in a real environmental matrix."", 'corpus_id': 210192446, 'score': 0}]"
65	{'doc_id': '220831004', 'title': 'Big Bird: Transformers for Longer Sequences', 'abstract': 'Transformers-based models, such as BERT, have been one of the most successful deep learning models for NLP. Unfortunately, one of their core limitations is the quadratic dependency (mainly in terms of memory) on the sequence length due to their full attention mechanism. To remedy this, we propose, BigBird, a sparse attention mechanism that reduces this quadratic dependency to linear. We show that BigBird is a universal approximator of sequence functions and is Turing complete, thereby preserving these properties of the quadratic, full attention model. Along the way, our theoretical analysis reveals some of the benefits of having $O(1)$ global tokens (such as CLS), that attend to the entire sequence as part of the sparse attention mechanism. The proposed sparse attention can handle sequences of length up to 8x of what was previously possible using similar hardware. As a consequence of the capability to handle longer context, BigBird drastically improves performance on various NLP tasks such as question answering and summarization. We also propose novel applications to genomics data.', 'corpus_id': 220831004}	7629	"[{'doc_id': '221139844', 'title': 'Compositional Generalization via Neural-Symbolic Stack Machines', 'abstract': 'Despite achieving tremendous success, existing deep learning models have exposed limitations in compositional generalization, the capability to learn compositional rules and apply them to unseen cases in a systematic manner. To tackle this issue, we propose the Neural-Symbolic Stack Machine (NeSS). It contains a neural network to generate traces, which are then executed by a symbolic stack machine enhanced with sequence manipulation operations. NeSS combines the expressive power of neural sequence models with the recursion supported by the symbolic stack machine. Without training supervision on execution traces, NeSS achieves 100% generalization performance in four domains: the SCAN benchmark of language-driven navigation tasks, the task of few-shot learning of compositional instructions, the compositional machine translation benchmark, and context-free grammar parsing tasks.', 'corpus_id': 221139844, 'score': 1}, {'doc_id': '9556453', 'title': 'Learning to superoptimize programs', 'abstract': 'Code super-optimization is the task of transforming any given program to a more efficient version while preserving its input-output behaviour. In some sense, it is similar to the paraphrase problem from natural language processing where the intention is to change the syntax of an utterance without changing its semantics. Code-optimization has been the subject of years of research that has resulted in the development of rule-based transformation strategies that are used by compilers. More recently, however, a class of stochastic search based methods have been shown to outperform these strategies. This approach involves repeated sampling of modifications to the program from a proposal distribution, which are accepted or rejected based on whether they preserve correctness, and the improvement they achieve. These methods, however, neither learn from past behaviour nor do they try to leverage the semantics of the program under consideration. Motivated by this observation, we present a novel learning based approach for code super-optimization. Intuitively, our method works by learning the proposal distribution using unbiased estimators of the gradient of the expected improvement. Experiments on benchmarks comprising of automatically generated as well as existing (""Hacker\'s Delight"") programs show that the proposed method is able to significantly outperform state of the art approaches for code super-optimization.', 'corpus_id': 9556453, 'score': 1}, {'doc_id': '204578308', 'title': 'Stabilizing Transformers for Reinforcement Learning', 'abstract': ""Owing to their ability to both effectively integrate information over long time horizons and scale to massive amounts of data, self-attention architectures have recently shown breakthrough success in natural language processing (NLP), achieving state-of-the-art results in domains such as language modeling and machine translation. Harnessing the transformer's ability to process long time horizons of information could provide a similar performance boost in partially observable reinforcement learning (RL) domains, but the large-scale transformers used in NLP have yet to be successfully applied to the RL setting. In this work we demonstrate that the standard transformer architecture is difficult to optimize, which was previously observed in the supervised learning setting but becomes especially pronounced with RL objectives. We propose architectural modifications that substantially improve the stability and learning speed of the original Transformer and XL variant. The proposed architecture, the Gated Transformer-XL (GTrXL), surpasses LSTMs on challenging memory environments and achieves state-of-the-art results on the multi-task DMLab-30 benchmark suite, exceeding the performance of an external memory architecture. We show that the GTrXL, trained using the same losses, has stability and performance that consistently matches or exceeds a competitive LSTM baseline, including on more reactive tasks where memory is less critical. GTrXL offers an easy-to-train, simple-to-implement but substantially more expressive architectural alternative to the standard multi-layer LSTM ubiquitously used for RL agents in partially observable environments."", 'corpus_id': 204578308, 'score': 1}, {'doc_id': '85668703', 'title': 'Competition for acorns among wild boar ( Sus scrofa ) and small mammals in a Mediterranean woodland', 'abstract': 'Observations of the rooting activity of wild boar in a holly-oak grove showed that in March–April the decrease of acorns in the diet (31%) was lower than their availability (82%). Moreover the occurrence of deep rooting events remains high despite the low occurrence of grass roots in the diet. These observations suggested that wild boar may exploit hoards of acorns collected by small mammals living in the study area (mainly wood mice Apodemus sp.). In order to test this hypothesis two experimental trials were set up to: (1) investigate whether wild boar were able to locate acorns buried in the ground (range 0–30 cm) and (2) establish if mouse burrows were more likely to be excavated than locations without burrows. The results clearly show that wild boar actively search for buried acorns, mainly in March (59%vs 31% in April and nothing in May) and that burrows are excavated significantly more than locations without burrows (ratio 2:1, respectively). Moreover, locations with burrows are characterized by a decline of rooting activity as a function of the distance from their centre (P= 0.02), which is absent in the control locations (P= 0.74). Our results show that wild boar are able to partly compensate for a reduced above-ground availability of acorns by predating on hoards collected by small mammals. Since this occurs during a critical period for female wild boar when they are giving birth and lactating, this behaviour may strongly influence the population dynamics of both wild boar and small mammals.', 'corpus_id': 85668703, 'score': 0}, {'doc_id': '214623440', 'title': 'ProGraML: Graph-based Deep Learning for Program Optimization and Analysis', 'abstract': 'The increasing complexity of computing systems places a tremendous burden on optimizing compilers, requiring ever more accurate and aggressive optimizations. Machine learning offers significant benefits for constructing optimization heuristics but there remains a gap between what state-of-the-art methods achieve and the performance of an optimal heuristic. Closing this gap requires improvements in two key areas: a representation that accurately captures the semantics of programs, and a model architecture with sufficient expressiveness to reason about this representation. \nWe introduce ProGraML - Program Graphs for Machine Learning - a novel graph-based program representation using a low level, language agnostic, and portable format; and machine learning models capable of performing complex downstream tasks over these graphs. The ProGraML representation is a directed attributed multigraph that captures control, data, and call relations, and summarizes instruction and operand types and ordering. Message Passing Neural Networks propagate information through this structured representation, enabling whole-program or per-vertex classification tasks. \nProGraML provides a general-purpose program representation that equips learnable models to perform the types of program analysis that are fundamental to optimization. To this end, we evaluate the performance of our approach first on a suite of traditional compiler analysis tasks: control flow reachability, dominator trees, data dependencies, variable liveness, and common subexpression detection. On a benchmark dataset of 250k LLVM-IR files covering six source programming languages, ProGraML achieves an average 94.0 F1 score, significantly outperforming the state-of-the-art approaches. We then apply our approach to two high-level tasks - heterogeneous device mapping and program classification - setting new state-of-the-art performance in both.', 'corpus_id': 214623440, 'score': 1}, {'doc_id': '219980353', 'title': 'Exploring Software Naturalness through Neural Language Models', 'abstract': 'The Software Naturalness hypothesis argues that programming languages can be understood through the same techniques used in natural language processing. We explore this hypothesis through the use of a pre-trained transformer-based language model to perform code analysis tasks. Present approaches to code analysis depend heavily on features derived from the Abstract Syntax Tree (AST) while our transformer-based language models work on raw source code. This work is the first to investigate whether such language models can discover AST features automatically. To achieve this, we introduce a sequence labeling task that directly probes the language models understanding of AST. Our results show that transformer based language models achieve high accuracy in the AST tagging task. Furthermore, we evaluate our model on a software vulnerability identification task. Importantly, we show that our approach obtains vulnerability identification results comparable to graph based approaches that rely heavily on compilers for feature extraction.', 'corpus_id': 219980353, 'score': 0}, {'doc_id': '13662098', 'title': 'Machine Learning in Compiler Optimization', 'abstract': 'In the last decade, machine-learning-based compilation has moved from an obscure research niche to a mainstream activity. In this paper, we describe the relationship between machine learning and compiler optimization and introduce the main concepts of features, models, training, and deployment. We then provide a comprehensive survey and provide a road map for the wide variety of different research areas. We conclude with a discussion on open issues in the area and potential research directions. This paper provides both an accessible introduction to the fast moving area of machine-learning-based compilation and a detailed bibliography of its main achievements.', 'corpus_id': 13662098, 'score': 1}, {'doc_id': '212703075', 'title': 'Exploring Paraphrasing Techniques on Formal Language for Generating Semantics Preserving Source Code Transformations', 'abstract': 'Automatically identifying and generating equivalent semantic content to a word, phrase, or sentence is an important part of natural language processing (NLP). The research done so far in paraphrases in NLP has been focused exclusively on textual data, but has significant potential if it is applied to formal languages like source code. In this paper, we present a novel technique for generating source code transformations via the use of paraphrases. We explore how to extract and validate source code paraphrases. The transformations can be used for stylometry tasks and processes like refactoring. A machine learning method of identifying valid transformations has the advantage of avoiding the generation of transformations by hand and is more likely to have more valid transformations. Our dataset is comprised by 27,300 C++ source code files, consisting of 273topics each with 10 parallel files. This generates approximately152,000 paraphrases. Of these paraphrases, 11% yield valid code transformations. We then train a random forest classifier that can identify valid transformations with 83% accuracy. In this paper we also discuss some of the observed relationships betweenlinked paraphrase transformations. We depict the relationshipsthat emerge between alternative equivalent code transformationsin a graph formalism.', 'corpus_id': 212703075, 'score': 1}, {'doc_id': '219720845', 'title': 'Guiding Optimizations with Meliora: A Deep Walk down Memory Lane', 'abstract': 'Performance models can be very useful for understanding the behavior of applications and hence can help guide design and optimization decisions. Unfortunately, performance modeling of nontrivial computations typically requires significant expertise and human effort. Moreover, even when performed by experts, it is necessarily limited in scope, accuracy, or both. However, since models are not typically available, programmers, compilers or autotuners cannot use them easily to guide optimizations and are limited to heuristic-based methods that potentially take a lot of time to perform unnecessary transformations. We believe that streamlining model generation and making it scalable (both in terms of human effort and code size) would enable dramatic improvements in compilation techniques, as well as manual optimization and autotuning. To that end, we are building the Meliora code analysis infrastructure for machine learning-based performance model generation of arbitrary codes based on static analysis of intermediate language representations. We demonstrate good accuracy in matching known codes and show how Meliora can be used to optimize new codes though reusing optimization knowledge, either manually or in conjunction with an autotuner. When autotuning, Meliora eliminates or dramatically reduces the empirical search space, while generally achieving competitive performance.', 'corpus_id': 219720845, 'score': 0}, {'doc_id': '221186832', 'title': 'Static Neural Compiler Optimization via Deep Reinforcement Learning', 'abstract': ""The phase-ordering problem of modern compilers has received a lot of attention from the research community over the years, yet remains largely unsolved. Various optimization sequences exposed to the user are manually designed by compiler developers. In designing such a sequence developers have to choose the set of optimization passes, their parameters and ordering within a sequence. Resulting sequences usually fall short of achieving optimal runtime for a given source code and may sometimes even degrade the performance when compared to unoptimized version. In this paper, we employ a deep reinforcement learning approach to the phase-ordering problem. Provided with sub-sequences constituting LLVM's O3 sequence, our agent learns to outperform the O3 sequence on the set of source codes used for training and achieves competitive performance on the validation set, gaining up to 1.32x speedup on previously-unseen programs. Notably, our approach differs from autotuning methods by not depending on one or more test runs of the program for making successful optimization decisions. It has no dependence on any dynamic feature, but only on the statically-attainable intermediate representation of the source code. We believe that the models trained using our approach can be integrated into modern compilers as neural optimization agents, at first to complement, and eventually replace the handcrafted optimization sequences."", 'corpus_id': 221186832, 'score': 0}]"
66	{'doc_id': '204949631', 'title': 'BPE-Dropout: Simple and Effective Subword Regularization', 'abstract': 'Subword segmentation is widely used to address the open vocabulary problem in machine translation. The dominant approach to subword segmentation is Byte Pair Encoding (BPE), which keeps the most frequent words intact while splitting the rare ones into multiple tokens. While multiple segmentations are possible even with the same vocabulary, BPE splits words into unique sequences; this may prevent a model from better learning the compositionality of words and being robust to segmentation errors. So far, the only way to overcome this BPE imperfection, its deterministic nature, was to create another subword segmentation algorithm (Kudo, 2018). In contrast, we show that BPE itself incorporates the ability to produce multiple segmentations of the same word. We introduce BPE-dropout - simple and effective subword regularization method based on and compatible with conventional BPE. It stochastically corrupts the segmentation procedure of BPE, which leads to producing multiple segmentations within the same fixed BPE framework. Using BPE-dropout during training and the standard BPE during inference improves translation quality up to 2.3 BLEU compared to BPE and up to 0.9 BLEU compared to the previous subword regularization.', 'corpus_id': 204949631}	3840	"[{'doc_id': '202583325', 'title': 'K-BERT: Enabling Language Representation with Knowledge Graph', 'abstract': 'Pre-trained language representation models, such as BERT, capture a general language representation from large-scale corpora, but lack domain-specific knowledge. When reading a domain text, experts make inferences with relevant knowledge. For machines to achieve this capability, we propose a knowledge-enabled language representation model (K-BERT) with knowledge graphs (KGs), in which triples are injected into the sentences as domain knowledge. However, too much knowledge incorporation may divert the sentence from its correct meaning, which is called knowledge noise (KN) issue. To overcome KN, K-BERT introduces soft-position and visible matrix to limit the impact of knowledge. K-BERT can easily inject domain knowledge into the models by equipped with a KG without pre-training by-self because it is capable of loading model parameters from the pre-trained BERT. Our investigation reveals promising results in twelve NLP tasks. Especially in domain-specific tasks (including finance, law, and medicine), K-BERT significantly outperforms BERT, which demonstrates that K-BERT is an excellent choice for solving the knowledge-driven problems that require experts.', 'corpus_id': 202583325, 'score': 1}, {'doc_id': '13753208', 'title': 'Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates', 'abstract': 'Subword units are an effective way to alleviate the open vocabulary problems in neural machine translation (NMT). While sentences are usually converted into unique subword sequences, subword segmentation is potentially ambiguous and multiple segmentations are possible even with the same vocabulary. The question addressed in this paper is whether it is possible to harness the segmentation ambiguity as a noise to improve the robustness of NMT. We present a simple regularization method, subword regularization, which trains the model with multiple subword segmentations probabilistically sampled during training. In addition, for better subword sampling, we propose a new subword segmentation algorithm based on a unigram language model. We experiment with multiple corpora and report consistent improvements especially on low resource and out-of-domain settings.', 'corpus_id': 13753208, 'score': 1}, {'doc_id': '211133077', 'title': 'Incorporating BERT into Neural Machine Translation', 'abstract': 'The recently proposed BERT has shown great power on a variety of natural language understanding tasks, such as text classification, reading comprehension, etc. However, how to effectively apply BERT to neural machine translation (NMT) lacks enough exploration. While BERT is more commonly used as fine-tuning instead of contextual embedding for downstream language understanding tasks, in NMT, our preliminary exploration of using BERT as contextual embedding is better than using for fine-tuning. This motivates us to think how to better leverage BERT for NMT along this direction. We propose a new algorithm named BERT-fused model, in which we first use BERT to extract representations for an input sequence, and then the representations are fused with each layer of the encoder and decoder of the NMT model through attention mechanisms. We conduct experiments on supervised (including sentence-level and document-level translations), semi-supervised and unsupervised machine translation, and achieve state-of-the-art results on seven benchmark datasets. Our code is available at \\url{this https URL}.', 'corpus_id': 211133077, 'score': 0}, {'doc_id': '52051958', 'title': 'SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing', 'abstract': 'This paper describes SentencePiece, a language-independent subword tokenizer and detokenizer designed for Neural-based text processing, including Neural Machine Translation. It provides open-source C++ and Python implementations for subword units. While existing subword segmentation tools assume that the input is pre-tokenized into word sequences, SentencePiece can train subword models directly from raw sentences, which allows us to make a purely end-to-end and language independent system. We perform a validation experiment of NMT on English-Japanese machine translation, and find that it is possible to achieve comparable accuracy to direct subword training from raw sentences. We also compare the performance of subword training and segmentation with various configurations. SentencePiece is available under the Apache 2 license at https://github.com/google/sentencepiece.', 'corpus_id': 52051958, 'score': 1}, {'doc_id': '3643309', 'title': 'A Closer Look at Skip-gram Modelling', 'abstract': 'Data sparsity is a large problem in natural language processing that refers to the fact that language is a system of rare events, so varied and complex, that even using an extremely large corpus, we can never accurately model all possible strings of words. This paper examines the use of skip-grams (a technique where by n-grams are still stored to model language, but they allow for tokens to be skipped) to overcome the data sparsity problem. We analyze this by computing all possible skip-grams in a training corpus and measure how many adjacent (standard) n-grams these cover in test documents. We examine skip-gram modelling using one to four skips with various amount of training data and test against similar documents as well as documents generated from a machine translation system. In this paper we also determine the amount of extra training data required to achieve skip-gram coverage using standard adjacent tri-grams.', 'corpus_id': 3643309, 'score': 1}, {'doc_id': '216642136', 'title': 'Syntax-aware Data Augmentation for Neural Machine Translation', 'abstract': 'Data augmentation is an effective performance enhancement in neural machine translation (NMT) by generating additional bilingual data. In this paper, we propose a novel data augmentation enhancement strategy for neural machine translation. Different from existing data augmentation methods which simply choose words with the same probability across different sentences for modification, we set sentence-specific probability for word selection by considering their roles in sentence. We use dependency parse tree of input sentence as an effective clue to determine selecting probability for every words in each sentence. Our proposed method is evaluated on WMT14 English-to-German dataset and IWSLT14 German-to-English dataset. The result of extensive experiments show our proposed syntax-aware data augmentation method may effectively boost existing sentence-independent methods for significant translation performance improvement.', 'corpus_id': 216642136, 'score': 0}, {'doc_id': '216144648', 'title': 'Multiple Segmentations of Thai Sentences for Neural Machine Translation', 'abstract': 'Thai is a low-resource language, so it is often the case that data is not available in sufficient quantities to train an Neural Machine Translation (NMT) model which perform to a high level of quality. In addition, the Thai script does not use white spaces to delimit the boundaries between words, which adds more complexity when building sequence to sequence models. In this work, we explore how to augment a set of English–Thai parallel data by replicating sentence-pairs with different word segmentation methods on Thai, as training data for NMT model training. Using different merge operations of Byte Pair Encoding, different segmentations of Thai sentences can be obtained. The experiments show that combining these datasets, performance is improved for NMT models trained with a dataset that has been split using a supervised splitting tool.', 'corpus_id': 216144648, 'score': 0}, {'doc_id': '225067122', 'title': 'Char2Subword: Extending the Subword Embedding Space from Pre-trained Models Using Robust Character Compositionality', 'abstract': ""Byte-pair encoding (BPE) is a ubiquitous algorithm in the subword tokenization process of language models. BPE provides multiple benefits, such as handling the out-of-vocabulary problem and reducing vocabulary sparsity. However, this process is defined from the pre-training data statistics, making the tokenization on different domains susceptible to infrequent spelling sequences (e.g., misspellings as in social media or character-level adversarial attacks). On the other hand, pure character-level models, though robust to misspellings, often lead to unreasonably large sequence lengths and make it harder for the model to learn meaningful contiguous characters. To alleviate these challenges, we propose a character-based subword transformer module (char2subword) that learns the subword embedding table in pre-trained models like BERT. Our char2subword module builds representations from characters out of the subword vocabulary, and it can be used as a drop-in replacement of the subword embedding table. The module is robust to character-level alterations such as misspellings, word inflection, casing, and punctuation. We integrate it further with BERT through pre-training while keeping BERT transformer parameters fixed. We show our method's effectiveness by outperforming a vanilla multilingual BERT on the linguistic code-switching evaluation (LinCE) benchmark."", 'corpus_id': 225067122, 'score': 1}, {'doc_id': '214742998', 'title': 'Give your Text Representation Models some Love: the Case for Basque', 'abstract': 'Word embeddings and pre-trained language models allow to build rich representations of text and have enabled improvements across most NLP tasks. Unfortunately they are very expensive to train, and many small companies and research groups tend to use models that have been pre-trained and made available by third parties, rather than building their own. This is suboptimal as, for many languages, the models have been trained on smaller (or lower quality) corpora. In addition, monolingual pre-trained models for non-English languages are not always available. At best, models for those languages are included in multilingual versions, where each language shares the quota of substrings and parameters with the rest of the languages. This is particularly true for smaller languages such as Basque. In this paper we show that a number of monolingual models (FastText word embeddings, FLAIR and BERT language models) trained with larger Basque corpora produce much better results than publicly available versions in downstream NLP tasks, including topic classification, sentiment classification, PoS tagging and NER. This work sets a new state-of-the-art in those tasks for Basque. All benchmarks and models used in this work are publicly available.', 'corpus_id': 214742998, 'score': 0}, {'doc_id': '214713496', 'title': 'Learning Contextualized Sentence Representations for Document-Level Neural Machine Translation', 'abstract': 'Document-level machine translation incorporates inter-sentential dependencies into the translation of a source sentence. In this paper, we propose a new framework to model cross-sentence dependencies by training neural machine translation (NMT) to predict both the target translation and surrounding sentences of a source sentence. By enforcing the NMT model to predict source context, we want the model to learn ""contextualized"" source sentence representations that capture document-level dependencies on the source side. We further propose two different methods to learn and integrate such contextualized sentence embeddings into NMT: a joint training method that jointly trains an NMT model with the source context prediction model and a pre-training & fine-tuning method that pretrains the source context prediction model on a large-scale monolingual document corpus and then fine-tunes it with the NMT model. Experiments on Chinese-English and English-German translation show that both methods can substantially improve the translation quality over a strong document-level Transformer baseline.', 'corpus_id': 214713496, 'score': 0}]"
67	{'doc_id': '41457494', 'title': 'Occurrence and Growth of Yeasts in Yogurts', 'abstract': 'Yogurts purchased from retail outlets were examined for the presence of yeasts by being plated onto oxytetracycline malt extract agar. Of the 128 samples examined, 45% exhibited yeast counts above 103 cells per g. A total of 73 yeast strains were isolated and identified as belonging to the genera Torulopsis, Kluyveromyces, Saccharomyces, Candida, Rhodotorula, Pichia, Debaryomyces, and Sporobolomyces. Torulopsis candida and Kluyveromyces fragilis were the most frequently isolated species, followed by Saccharomyces cerevisiae, Rhodotorula rubra, Kluyveromyces lactis, and Torulopsis versatilis. The growth of yeasts in yogurts was related to the ability of the yeasts to grow at refrigeration temperatures, to ferment lactose and sucrose, and to hydrolyze milk casein. Most yeast isolates grew in the presence of 100 μg of sorbate and benzoate preservatives per ml. Higher yeast counts from yogurts were obtained when the yogurts were plated onto oxytetracycline malt extract agar than when they were plated onto acidified malt extract agar.', 'corpus_id': 41457494}	11550	[{'doc_id': '224948100', 'title': 'Antibacterial and Phytochemical Properties of Crude Leaf Extracts of Moringa oleifera Lam., Pterocarpus santalinoides L’Herit DC and Ceiba pentandra L. on Some Clinical Bacterial Isolates in Nigeria', 'abstract': 'Aims: The study was carried out to determine the phytochemical constituents and antibacterial activity of aqueous and ethanolic extracts of fresh leaves of Moringa oleifera Lam., Pterocarpus santalinoides L’Herit DC and Ceiba pentandra L. on bacterial isolates; Salmonella typhi, Staphylococcus aureus, Escherichia coli and Pseudomonas aeruginosa. Methodology: The plant leaves were dried, pulverized and phytochemical tests were done according to standard laboratory procedure. Aqueous and ethanolic extracts were obtained from 20 g of the of the ground leaves. Antibacterial assay was carried out with Disc diffusion method on seven concentrations of the extracts ;100,50,25,12.5, 6.25,3.125,1.5625 mg/ml and compared with standard antibiotics. Isolated bacterial pathogens; Salmonella typhi, Staphylococcus aureus, Escherichia coli and Pseudomonas aeruginosa (1.0 x 10 5 cfu /ml) were used as test organisms. Original Research Article Njokuocha and Ewenike; JOCAMR, 10(4): 1-15, 2020; Article no.JOCAMR.60767 2 Results: Alkaloids, steroidal aglycones, glycosides, proteins, carbohydrates, reducing sugars, tannins, saponins, vitamins A and E were present in all the plant samples. Flavonoids and cardiac glycosides were not detected in Pterocarpus santalinoides and Ceiba pentandra, respectively. Anthracene glycoside was absent in all samples. Aqueous and ethanolic extracts of M. oleifera showed antibacterial activities against all the bacterial isolates at minimum inhibitory concentration (MIC) of 3.125 mg/ml and 1.5625 mg/ml respectively. Pterocarpus santalinoides showed inhibitory activity only on Salmonella typhi at 3.125 mg/ml and Escherichia coli 1.5625 mg/ml MIC. Ceiba pentandra showed spectrum of antibacterial activity against all the bacterial isolates at 1.56 mg/ml MIC with exception of Salmonella typhi. E. coli was the most susceptible to the leaf extracts. Salmonella typhi was not sensitive to the leaf extracts of Ceiba pentandra, while Staphylococcus aureus and Pseudomonas aeruginosa were not sensitive to the leaf extracts of Pterocarpus santalinoides. Conclusion: It can be concluded that both aqueous and ethanolic leaf extracts had antibacterial activity against the test organism, thus justifying their use in folklore medicine.', 'corpus_id': 224948100, 'score': 0}, {'doc_id': '221938564', 'title': 'Potential of Cell-Free Supernatant from Lactobacillus plantarum NIBR97, Including Novel Bacteriocins, as a Natural Alternative to Chemical Disinfectants', 'abstract': 'The recent pandemic of coronavirus disease 2019 (COVID-19) has increased demand for chemical disinfectants, which can be potentially hazardous to users. Here, we suggest that the cell-free supernatant from Lactobacillus plantarum NIBR97, including novel bacteriocins, has potential as a natural alternative to chemical disinfectants. It exhibits significant antibacterial activities against a broad range of pathogens, and was observed by scanning electron microscopy (SEM) to cause cellular lysis through pore formation in bacterial membranes, implying that its antibacterial activity may be mediated by peptides or proteins and supported by proteinase K treatment. It also showed significant antiviral activities against HIV-based lentivirus and influenza A/H3N2, causing lentiviral lysis through envelope collapse. Furthermore, whole-genome sequencing revealed that NIBR97 has diverse antimicrobial peptides, and among them are five novel bacteriocins, designated as plantaricin 1 to 5. Plantaricin 3 and 5 in particular showed both antibacterial and antiviral activities. SEM revealed that plantaricin 3 causes direct damage to both bacterial membranes and viral envelopes, while plantaricin 5 damaged only bacterial membranes, implying different antiviral mechanisms. Our data suggest that the cell-free supernatant from L. plantarum NIBR97, including novel bacteriocins, is potentially useful as a natural alternative to chemical disinfectants.', 'corpus_id': 221938564, 'score': 1}, {'doc_id': '222125619', 'title': 'The potential application of probiotics and prebiotics for the prevention and treatment of COVID-19', 'abstract': 'COVID-19 is a pandemic disease caused by the novel coronavirus severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2). This new viral infection was first identified in China in December 2019, and it has subsequently spread globally. The lack of a vaccine or curative treatment for COVID-19 necessitates a focus on other strategies to prevent and treat the infection. Probiotics consist of single or mixed cultures of live microorganisms that can beneficially affect the host by maintaining the intestinal or lung microbiota that play a major role in human health. At present, good scientific evidence exists to support the ability of probiotics to boost human immunity, thereby preventing colonization by pathogens and reducing the incidence and severity of infections. Herein, we present clinical studies of the use of probiotic supplementation to prevent or treat respiratory tract infections. These data lead to promising benefits of probiotics in reducing the risk of COVID-19. Further studies should be conducted to assess the ability of probiotics to combat COVID-19.', 'corpus_id': 222125619, 'score': 0}, {'doc_id': '3852455', 'title': 'Assessment of the antifungal activity of Lactobacillus and Pediococcus spp. for use as bioprotective cultures in dairy products', 'abstract': 'Fungi are commonly involved in dairy product spoilage and the use of bioprotective cultures can be a complementary approach to reduce food waste and economic losses. In this study, the antifungal activity of 89 Lactobacillus and 23 Pediococcus spp. isolates against three spoilage species, e.g., Yarrowia lipolytica, Rhodotorula mucilaginosa and Penicillium brevicompactum, was first evaluated in milk agar. None of the tested pediococci showed antifungal activity while 3, 23 and 43 lactobacilli isolates showed strong antifungal activity or total inhibition against Y. lipolytica, R. mucilaginosa and P. brevicompactum, respectively. Then, the three most promising strains, Lactobacillus paracasei SYR90, Lactobacillus plantarum OVI9 and Lactobacillus rhamnosus BIOIII28 at initial concentrations of 105 and 107 CFU/ml were tested as bioprotective cultures against the same fungal targets in a yogurt model during a 5-week storage period at 10\xa0°C. While limited effects were observed at 105 CFU/ml inoculum level, L. paracasei SYR90 and L. rhamnosus BIOIII28 at 107 CFU/ml respectively retarded the growth of R. mucilaginosa and P. brevicompactum as compared to a control without selected cultures. In contrast, growth of Y. lipolytica was only slightly affected. In conclusion, these selected strains may be good candidates for bioprotection of fermented dairy products.', 'corpus_id': 3852455, 'score': 1}, {'doc_id': '84892752', 'title': 'The potential of phytopreservatives and nisin to control microbial spoilage of minimally processed fruit yogurts', 'abstract': 'Abstract Fruit yogurt made with minimally-processed ‘fresh’ fruit has the potential to increase consumption rates of yogurt. The efficacy of vanillin, nisin, and fresh cranberries to control microbial spoilage of a fresh fruit yogurt containing wild blueberries was tested. After introducing wild blueberries, yogurt contained a large community of yeast and bacterial cells (>103 cfu/g of yogurt). Yogurt with only wild blueberries was visibly spoiled within 1 week. However, the addition of 2000 ppm vanillin resulted in suppression of the growth of spoilage microbes. This level of vanillin did not affect survival of acid-adapted Escherichia coli. 1000 ppm vanillin was also effective in controlling growth, but lower concentrations only briefly delayed the onset of microbial spoilage. Nisin was ineffective in preventing spoilage, and in a test of yogurt containing fresh peaches, nisin hastened growth of spoilage microbes. The addition of fresh cranberries to yogurt with minimally processed wild blueberries resulted in one week extension of the shelf life, as compared to yogurt with wild blueberries alone. Phytopreservatives such as vanillin have promise as ‘natural’ antimicrobial agents in foods such as minimally processed fruit yogurt.', 'corpus_id': 84892752, 'score': 1}, {'doc_id': '222227191', 'title': 'Incidence of Bacillus spp. in ready-to- eat foods, beverages and water from different tourist destinations of north western Himalayas, Himachal Pradesh, India', 'abstract': 'Enteroaggregative Escherichia coli (EAEC) is an emerging foodborne pathogen worldwide, often responsible for persistent diarrheoa in the infants and young animals. In present study, a total of 1519 diarrhoeal fecal samples from human infants (n=890) and young domestic animals (n=629) from eight different states of India were investigated. The recovered E. coli isolates by cultural method were further confirmed as EAEC by PCR targeting chromosomal associated genes (aaiA, astA, pilS, ecp,irp2, pic, fimA) and plasmid-borne genes (cvd432, aggR, aafA, aggA, agg3A) as well as by gold standard HEp-2 cell adherence assay. A total of 197 EAEC isolates were recovered from diarrhoeal cases of human infants (n=95) and young animals (n=102) which were further subjected to Pulse Field Gel Electrophoresis (PFGE) to address their genetic diversity and to the antimicrobial susceptibility test to know their susceptibility towards antibiotics used in routine practice. Overall, a highly diverse PFGE profile was observed for most of the EAEC test isolates. However, irrespective of place of isolation, sharing or circulation of identical clones of EAEC between different species of young animals including human infants was evident. The antibiogram profile revealed an alarming multidrug resistance (MDR) profile among diarrhoeal EAEC isolates of human & animal origin. The infant origin EAEC isolates were more resistant to Beta-lactam, Third generation Cephalosporins and Fluroquinolones, while the animal origin EAEC isolates were resistant to Beta-lactam, Tetracyclines and Sulphonamides. However, all the EAEC isolates of infant and young animals were found sensitive to Imipenem drug. 6. ABST. No. PS-1-11 Isolation, Identification and Antibiotic Sensitivity profile of Salmonella Enteritidis recovered from local poultry meat shops Diksha Gourkhede, Kaushik Satyaprakash, Bhoomika Sirsant, Jay Prakash Yadav, Richa Pathak, Deepa Ujjawl, D. B. Rawool, S. V. S. Malik and S. B. Barbuddhe Division of Veterinary Public Health, ICAR-IVRI, Izatnagar, Bareilly, U.P-243122 National Research Centre on Meat, Chengicherala, Hyderabad, Telengana500092 deepak.rawool@yahoo.com Abstract Non-typhoidal Salmonellae (NTS) are leading cause of foodborne infections and the emergence of antimicrobial resistance among them is a global concern. In the present study, isolation and molecular identification of S. Enteritidis was attempted from poultry faecal droppings, caecal and meat samples. The identified S. EnteritidisNon-typhoidal Salmonellae (NTS) are leading cause of foodborne infections and the emergence of antimicrobial resistance among them is a global concern. In the present study, isolation and molecular identification of S. Enteritidis was attempted from poultry faecal droppings, caecal and meat samples. The identified S. Enteritidis isolates were then subjected to antibiotic sensitivity studies as per CLSI 2017 guidelines. A total of 72 samples comprising of faecal droppings (n=23), caecum (25) and meat (n=24) samples were collected aseptically from local poultry meat shops of Bareilly and were processed immediately for isolation of Salmonella by standard conventional method. All the presumptive Salmonella colonies on XLD medium were processed for recommended biochemical tests as well as by PCR targeting genus specific inv A gene. Of the 72 samples, 13 were found to be positive for Salmonella genus by both, PCR and biochemical tests. These 13 isolates were then subjected to identification of S. Enteritidis serovar by serotype specific conventional PCR targeting sdf gene. Of the 13 genus confirmed Salmonella isolates, 9 S. Enteritidis isolates amplified sdf gene. The antibiotic sensitivity studies of the identified S. Enteritidis isolates revealed an alarming multidrug resistance (MDR), as of the 9 S. Enteritidis isolates, 7 were resistant to three or more classes of antibiotics namely β lactam group, aminoglycosides, tetracyclines, fluoroquinolones, sulfonamides and glycopeptides. These MDR S. Enteritidis isolates were further processed to determine the MIC range. The MIC range observed were 10-30 μg/ml, 10-240 μg/ml, >240 μg/ml, >240 μg/ml and >240 μg/ml for ciprofloxacin, tetracycline, ampicillin, vancomycin and sulphomethoxazole, respectively. 7. ABST. No. PS-1-13 Does sanitizing/hygienic interventions improves the microbiological quality of street foods? Anukampa, D.K. Singh, K.N. Bhilegaonkar, Ashok Kumar, D. Bardhan, Vinodh Kumar, O.R., Shagufta Bi., Sivakumar, M., Pruthvishree, B.S., Karthikeyan, R. and Z.B. Dubal Division of Veterinary Public Health, ICARIndian Veterinary Research Institute, Izatnagar, Bareilly243 122, U.P., India. drzunjar@yahoo.co.in Abstract A total of 1020 foods of animal origin and associated environmental samples were collected aseptically before and after intervention from 18 local food vendors of Bareilly (U.P) to assess the efficacy of an intervention to improve the microbial quality of street foods. Aerobic plate count (APC), enumeration of Staphylococcus aureus, E. coli, sulphite reducing Clostridia, presence of Salmonella spp. and Listeria monocytogenes of the samples was performed. The pre and post-intervention microbiological quality of food samples were assessed and the mean log reduction of APC was observed for swabs of hand (HS), plate (PS), table (TS) and cloth (CS) were within the range of 2.56-5.61, 1.96–5.31, 2.56–6.32 and 2.66–6.77 log10 cfu/cm. The E. coli were in the range of 1.12–3.78, 1.22–3.72, 1.69-4.16 and 0-4.33 log10 cfu/cm and S. aureus in the range of 1.30-3.74, 1.47-2.30, 1.36–2.55 and 1.2–2.86 log10 cfu/cm for HS, PS, TS and CS samples, respectively. Percent positivity in samples was also reduced significantly from 48.58% to 38.12% for E. coli, and 44.66% to 34.85% for S. aureus. The cooked food samples showed low microbial count before and after intervention strategies whereas the raw foods and processed/ready-to-eat foods showed a different trend in which 88.33% & 73.33% samples were within the acceptable limits before intervention and the microbial load was reduced drastically and within acceptable limits after an intervention. BeforeA total of 1020 foods of animal origin and associated environmental samples were collected aseptically before and after intervention from 18 local food vendors of Bareilly (U.P) to assess the efficacy of an intervention to improve the microbial quality of street foods. Aerobic plate count (APC), enumeration of Staphylococcus aureus, E. coli, sulphite reducing Clostridia, presence of Salmonella spp. and Listeria monocytogenes of the samples was performed. The pre and post-intervention microbiological quality of food samples were assessed and the mean log reduction of APC was observed for swabs of hand (HS), plate (PS), table (TS) and cloth (CS) were within the range of 2.56-5.61, 1.96–5.31, 2.56–6.32 and 2.66–6.77 log10 cfu/cm. The E. coli were in the range of 1.12–3.78, 1.22–3.72, 1.69-4.16 and 0-4.33 log10 cfu/cm and S. aureus in the range of 1.30-3.74, 1.47-2.30, 1.36–2.55 and 1.2–2.86 log10 cfu/cm for HS, PS, TS and CS samples, respectively. Percent positivity in samples was also reduced significantly from 48.58% to 38.12% for E. coli, and 44.66% to 34.85% for S. aureus. The cooked food samples showed low microbial count before and after intervention strategies whereas the raw foods and processed/ready-to-eat foods showed a different trend in which 88.33% & 73.33% samples were within the acceptable limits before intervention and the microbial load was reduced drastically and within acceptable limits after an intervention. Before intervention the number of samples positive for S. aureus and number of samples within acceptable limits for raw foods, processed/ready-to-eat foods, cooked foods was 45.33%, 45.23%, 21.05% and 66.67%, 75.0%, 91.66%, respectively. After the intervention, there was a reduction in pathogenic counts for processed/ready-to-eat foods, raw foods and cooked foods. Interestingly, before intervention 82.89% of food samples were within the acceptable limits which increased to 90.78% after intervention. The cooked samples were less frequently contaminated with E. coli and S. aureus, while most of the raw food samples showed the presence of E. coli and S. aureus. The samples like chicken gravy, cooked chicken, omelet, boiled egg & boiled milk were negative for S. aureus. Few samples of raw egg, omelet, salad, chutney and meat samples were positive for SRC and Salmonella, while improved bacterial quality was noticed after an intervention. Interestingly, L. monocytogenes could not be recovered from any of the samples. It was observed that the sanitizing or hygienic interventions improve the microbiological quality of street foods. 8. ABST. No. PS-1-14 Efficacy of benzalkonium chloride, hydrogen peroxide with per acetic acid and chlorine on clean and dirty raw shelled eggs sanitation and egg safety Santosh Sajjan, Madhavaprasad C.B. and Prashant S. Bagalkote Department of Veterinary Public Health and Epidemiology Veterinary College, KVAFSU, Vinobanagar, Shivamogga, Karnataka. *Corresponding author: email: dr.santoshsajjan@gmail.com, Tel: +91-8861233410 dr.santoshsajjan@gmail.com, santoshsajjan@rediffmail.com Abstract A study was conducted to evaluate the efficacy of some commonly used sanitizers in the food processing industries viz. benzalkonium chloride, hydrogen peroxide with per acetic acid and chlorine water for surface sanitation of raw shelled eggs by artificially challenging with Salmonella Enteritidis and Listeria monocytogenes MTCC 1135under clean and dirty conditions by in-vitro trials. Minimum Inhibitory Concentration (MIC) of the selected sanitizers was determined according to the modified Rideal-Walkar test by tube dilution technique and capacity of sanitizers under clean and dirty conditions by Kelsey-Sykes test. The study revealed that, the MIC of benzalkonium chloride, hydrogen peroxide with per acetic acid and chlorine water was 25 ', 'corpus_id': 222227191, 'score': 0}, {'doc_id': '222432594', 'title': 'Role of probiotics in prevention and control of viral infection', 'abstract': 'Probiotics are living microorganisms which administered in adequate amounts confer a health benefit on a host. The risk of viral infection in humans increased exponentially. However, the efficacy of vaccines and remedies for infectious disease is limited by the high mutation rate of virus, especially RNA viruses. The most common type of microbes used as probiotics are Lactobacilli and Bifidobacteria, which are generally consumed as a part of fermented foods, such as yoghurt or dietary supplements. One of the major mechanisms of probiotic action is through the regulation of host immune response. Probiotics contain immunostimulatory substances such as lipoteichoic acid, peptidoglycan and nucleic acid, which are toll-like receptor ligands, and muramyl dipeptide, which is a nod-like receptor ligand. Different experiments provide insight on the clinical effects of probiotics against respiratory virus infections. Commonly the retro viruses interact with the respiratory epithelium, which generates an innate immune response by activating the IFN signaling and other proinflammatory cytokines. Once cytokines have been secreted, macrophages and NK cells will be recruited to phagocytize and kill both viruses and viral-infected cells. To trigger a specific immune response, the immune system needs proinflammatory cytokines, energy, and some cofactor elements. Hence, probiotics can provide some elements to boost the immune response. There is another variety of mechanism to boost immune response and therefore these are also called immuno biotics. In this communication, we highlight the effectiveness of probiotics for the prevention and treatment of virally induced infectious diseases and the unique mechanism by which viruses are eliminated. Different methods and strategies such as vaccines, antibiotics, therapies, etc. have been performed for the prevention and treatment of infectious diseases but infection control has not yet been achieved at a sufficient level for diseases like Ebola haemorrhagic fever, severe acute respiratory syndrome corona virus, avian influenza, Zika virus, etc. As the increased geographical movement of humans and export and import of goods increased, the numbers of pathogenic virus species and affected area have increased. Therefore, the risk of viral infection has now become a critical issue. Most recently, scientists identified a new corona outbreak in Wuhan, China that has now reached all over the world. The virus called Severe Acute Respiratory Syndrome Coronavirus 2 (SARS-CoV-2) can cause COVID-19. Corona virus undergoes a process of discontinuous mutation as a result development of vaccine becomes difficult and the disease becomes pandemics. Corona virus typically affects the respiratory tracts of birds and mammals including humans. Doctors associate them with common cold, bronchitis, pneumonia, and severe acute respiratory syndrome. In the current studies, special emphasis is given on the viral infection and outlining the possible application of several probiotics against viral infectious disease and to explain the immune defence mechanism against viral infection that is induced by probiotics.', 'corpus_id': 222432594, 'score': 0}, {'doc_id': '225379651', 'title': 'Control of spoilage fungi in yogurt using MicroGARD 200™, Lyofast-FPR2™ and HOLDBAC-YMC™ as bioprotectants', 'abstract': 'Abstract The aim of this study was to assess the inhibitory effect of three commercial bioprotectant agents on the growth of yogurt-spoiling fungi. Mucor circinelloides, Mucor racemosus, Penicillium spp., Saccharomyces exiguus, and Candida intermedia, commonly involved in the spoilage of dairy products, were isolated from spoiled yogurt and were fully characterized using molecular and phenotypic methods. HOLDBAC-YMC™, Lyofast-FPR2™ and MicroGARD 200™ were used as antifungal products. An optimized experimental mixture design was applied to determine the proportion of each bioprotectant in terms of growth-inhibition response against the fungal strains in standard laboratory media. The results of the challenge tests showed that the application of bioprotectants inhibited the growth of the moulds in the range of 85–100% and the growth of yeast between 1.23 and 5.40 log cycles. The optimal combination of the bioprotectants was determined, tested in standard laboratory media and found to inhibit fungal growth. The antifungal effect of the optimal combination of the bioprotectants was validated in yogurt against the most resistant fungal species of the study, M. circinelloides and C. intermedia. The bioprotectants elicited antifungal effect in yogurt by completely inhibiting all of the tested fungi compared to controls. To the best of our knowledge, this is the first time a mixture of commercial bioprotectants has been tested on yogurt as a potential alternative for the biopreservation of yogurt in order to reduce spoilage of fermented dairy products and economic losses.', 'corpus_id': 225379651, 'score': 1}, {'doc_id': '227176985', 'title': 'Lactic Acid Bacteria: Food Safety and Human Health Applications', 'abstract': 'Research on lactic acid bacteria has confirmed how specific strains possess probiotic properties and impart unique sensory characteristics to food products. The use of probiotic lactic acid bacteria (LAB) in many food products, thus confers various health benefits to humans when they are frequently consumed in adequate amounts. The advent of functional food or the concept of nutraceuticals objectively places more emphasis on seeking alternatives to limit the use of medications thus promoting the regular consumption of fermented foods. Probiotic use has thus been recommended to fulfill the role of nutraceuticals, as no side effects on human health have been reported. Probiotics and lactic acid bacteria can boost and strengthen the human immune system, thereby increasing its resistance against numerous disease conditions. Consumer safety and confidence in dairy and fermented food products and the desire of the food industry to meet the sensory and health needs of consumers, has thus increased the demand for probiotic starter cultures with exceptional performance coupled with health benefiting properties. The potential of probiotic cultures and lactic acid bacteria in many industrial applications including fermented food products generally affects product characteristics and also serves as health-promoting foods for humans. The alleviation of lactose intolerance in many populations globally has been one of the widely accepted health claims attributed to probiotics and lactic acid bacteria, although many diseases have been treated with probiotic lactic acid bacteria and have been proven with scientific and clinical studies. The aim of our review was to present information related to lactic acid bacteria, the new classification and perspectives on industrial applications with a special emphasis on food safety and human health.', 'corpus_id': 227176985, 'score': 1}, {'doc_id': '221735653', 'title': 'One Health Probiotics plus Hot Topic : Australian COVID-19 vaccine', 'abstract': 'Foods containing edible probiotic bacteria, most commonly Lactobacillus and Bifidobacterium species, form a multi-billion-dollar industry worldwide. Currentlymarketed foods containing probiotics aremostly dairy basedwith yoghurts and fermentedmilks dominating the industry. Alternative foods as carriers of probiotics are being examined to reduce or eliminate lactose intolerance issues. Food categories including fruit juices, cheese, chocolate and even beer have been shown to be suitable for probiotic delivery. In addition, technologies such as encapsulation in food-grade alginate gels have allowed for improved probiotic survival in certain foodstuffs. We have explored the use of ready-to-eat vegetables such as baby spinach as carriers for commercial probiotics and found that high dose (>8 log CFU/g) can be achieved without havingnegative effects onappearance, tasteor aroma. Leafy greens as well as other foods and beveragesmay be suitable probiotic containing new food products in the future. The most commonly used definition for probiotics, initially proposed in 2001 by the Food and Agriculture Organisation of the United Nations (FAO) and supported by the World Health Organization (WHO) is ‘live microorganisms which when administered in adequate amounts confer a health benefit on the host’. Most probiotics sold in edible products are Lactobacillus and Bifidobacterium, while products with Bacillus, Escherichia coli and Saccharomyces are less commonly available. Probiotic organisms are different to fermentation organisms and the health promoting effects may be only strain specific. More stringency around health claims of probiotics and functional foods in general in various countries has resulted in fewer unsubstantiated marketing claims, which have plagued the probiotic industry for the past three decades. In 2010, the European Food Safety Authority (EFSA) took the strict option of banning all health claims regarding probiotics and until now the only claim that is approved is regarding lactose intolerance prevention through yoghurt ingestion. Nowadays, randomised, double-blind and placebo-controlled studies with high numbers of subjects are the bench mark to demonstrate probiotic efficacy. This is reasonable as probiotics are a major business activity with global sales expected to hit $50 billion by 2022. Despite their controversial history, many scientific studies have demonstrated health promoting activities of specific strains in certain situations. In addition, with the explosion of microbiome insights, ‘next generation probiotics’, which are defined as ‘live microorganisms identified on the basis of comparative microbiota analyses that, when administered in adequate amounts, confer a health benefit on the host’, will likely be of significant commercial interest in the coming years. Probiotic foods and beverages Oral delivery of probiotics can involve a variety of different vehicles. Tablets and capsules containing high doses (e.g. 10 log CFU) of single or mixed strain probiotics are commonplace in pharmacies and supermarkets. The most common foodstuffs containing probiotics are dairy-based, including yoghurts and fermented liquid milks. Other dairy-based foods including Cheddar cheese and chocolate can also support viable probiotic bacteria but are yet to make it to market. However, these products can contain high levelsof sugar andwitharound75%of theworld’spopulationbeing lactose intolerant, alternate non-dairy-based foods which can support probiotic bacteria viability have been investigated. A leading probiotic food producer in the USA, Goodbelly, has developed snack bars containing 9 log viable Bifidobacterium BB-12 cells and fruit juice containing Lactobacillus plantarum 299v (https:// In Focus 58 10.1071/MA20017 MICROBIOLOGY AUSTRALIA * JUNE 2020 goodbelly.com/). A new juice product containing alginate microencapsulated Lactobacillus casei Lc431 cells, called PERKii, was launched in Australia during 2016 (https://perkii.com.au/). Encapsulation of probiotics improves viability during simulated gastrointestinal transit and reduces fermentation of the fruit sugars in the beverage. Other alternatives to dairy-based probiotic foods are cereal, meat and soy-based products. Cereal-based probiotic drinks containing >7.9 log CFU/mL of L. plantarum and Lactobacillus acidophilus were prepared from single and mixed flours of barley and malt. Interesting recent research has identified that beer can support high survival of probiotic Lactobacillus paracasei L26 for 3 weeks before reducing to undetectable levels by 4 weeks. Beer contains several antimicrobial compounds, including alcohol, acid and hops making it a challenging environment for bacteria to survive. A novel approach for the preparation of probiotic breads was developed by coating pan bread slices with sodium alginate film impregnated with Lactobacillus rhamnosusGGwhich could deliver up to 9 log CFU/30–40 g per bread slice. Dry fermented meat products ingested without cooking are potential vehicles to transfer probiotics into the gastrointestinal tract as probiotic cells can be embedded and protectedwithin themeatmatrix consisting of protein and fat.When added into fermented sausages, an initial population of 5 log CFU/g of L. plantarum 299v increased to 8 log CFU/g after fermentation. Soy protein is also considered as a good protector for probiotics against harsh conditions in the intestine. Lactobacillus acidophilus LA-5 showed good growth and survival of >8.7 log CFU/g in a fermented soy beverage stored at 48C for 21 days. A mix of probiotic bacteria including Lactobacillus acidophilus, L. rhamnosus, L. paracasei and Bifidobacterium lactis incorporated into a non-fermented frozen soy dessert exhibited high viable populations exceeding 7 log CFU/g during 6 months storage while maintaining desirable sensory attributes. New probiotic containing vegetable products To further expand the range of probiotic containing foods, our group has examined fresh ready-to-eat leafy green vegetables as potential carriers. Several probiotic strains that were inoculated onto baby spinach by dipping the leaves for 5 mins in a bacterial suspension resulted in attachment of 7–8 log CFU/g spinach (Figure1).Viabilityofprobiotic strainA reducedslightlyover7days, while probiotic strainB increased slightly to>8 logCFU/g. Based on a typical serving size of 60 g of baby spinach a dose of >9.8 log CFU could be achieved, making it equivalent to other high dose probiotic products on the market. We next determined if the probiotics affected the sensory properties of the spinach, such as appearance, aroma and taste. A panel of 40 volunteers, under controlled conditions in a food sensory laboratory, evaluated de-identified spinach samples stored at 48C for 4 days. Using a triangle sensory test, it was found that there were no statistically significant differences (P > 0.05) in the appearance and flavour of spinach leaves inoculated with probiotic strain A or strain B to that of the control samples. Only 12 out of 40 people could differentiate the probiotic strain A containing spinach from the control spinach and 13 out of 40 could differentiate the probiotic strain B spinach from the control spinach. Spinach leaves with and without probiotics had a similar appearance over 7 days of storage at 48C as shown in Figure 2. It may be concluded that the sensory quality of baby spinach was not adversely affected by the addition of two probiotic strains. In addition, we have found that washing the leaves in various types of salad dressings (e.g. French, Italian, Balsamic) does not detach cells or reduce their viability. Lastly survival of probiotics on spinach in simulated gastrointestinal digestion trials did not reveal any greater reduction in viability compared with', 'corpus_id': 221735653, 'score': 0}]
68	{'doc_id': '27718370', 'title': 'Dynamic mode decomposition - data-driven modeling of complex systems', 'abstract': 'Data-driven dynamical systems is a burgeoning field-it connects how measurements of nonlinear dynamical systems and/or complex systems can be used with well-established methods in dynamical systems theory. This is a critically important new direction because the governing equations of many problems under consideration by practitioners in various scientific fields are not typically known. Thus, using data alone to help derive, in an optimal sense, the best dynamical system representation of a given application allows for important new insights. The recently developed dynamic mode decomposition (DMD) is an innovative tool for integrating data with dynamical systems theory. The DMD has deep connections with traditional dynamical systems theory and many recent innovations in compressed sensing and machine learning. Dynamic Mode Decomposition: Data-Driven Modeling of Complex Systems, the first book to address the DMD algorithm, presents a pedagogical and comprehensive approach to all aspects of DMD currently developed or under development; blends theoretical development, example codes, and applications to showcase the theory and its many innovations and uses; highlights the numerous innovations around the DMD algorithm and demonstrates its efficacy using example problems from engineering and the physical and biological sciences; and provides extensive MATLAB code, data for intuitive examples of key methods, and graphical presentations. Audience: The core audience for this book is engineers and applied mathematicians working in the physical and biological sciences. It can be used in courses that integrate data analysis with dynamical systems.', 'corpus_id': 27718370}	7006	"[{'doc_id': '49216843', 'title': 'Data-driven discovery of Koopman eigenfunctions for control', 'abstract': 'Data-driven transformations that reformulate nonlinear systems in a linear framework have the potential to enable the prediction, estimation, and control of strongly nonlinear dynamics using linear systems theory. The Koopman operator has emerged as a principled linear embedding of nonlinear dynamics, and its eigenfunctions establish intrinsic coordinates along which the dynamics behave linearly. Previous studies have used finite-dimensional approximations of the Koopman operator for model-predictive control approaches. In this work, we illustrate a fundamental closure issue of this approach and argue that it is beneficial to first validate eigenfunctions and then construct reduced-order models in these validated eigenfunctions. These coordinates form a Koopman-invariant subspace by design and, thus, have improved predictive power. We show then how the control can be formulated directly in these intrinsic coordinates and discuss potential benefits and caveats of this perspective. The resulting control architecture is termed Koopman Reduced Order Nonlinear Identification and Control (KRONIC). It is further demonstrated that these eigenfunctions can be approximated with data-driven regression and power series expansions, based on the partial differential equation governing the infinitesimal generator of the Koopman operator. Validating discovered eigenfunctions is crucial and we show that lightly damped eigenfunctions may be faithfully extracted from EDMD or an implicit formulation. These lightly damped eigenfunctions are particularly relevant for control, as they correspond to nearly conserved quantities that are associated with persistent dynamics, such as the Hamiltonian. KRONIC is then demonstrated on a number of relevant examples, including (a) a nonlinear system with a known linear embedding, (b) a variety of Hamiltonian systems, and (c) a high-dimensional double-gyre model for ocean mixing.', 'corpus_id': 49216843, 'score': 1}, {'doc_id': '227209469', 'title': 'Spectral Analysis and Stability of Deep Neural Dynamics', 'abstract': 'Our modern history of deep learning follows the arc of famous emergent disciplines in engineering (e.g. aero- and fluid dynamics) when theory lagged behind successful practical applications. Viewing neural networks from a dynamical systems perspective, in this work, we propose a novel characterization of deep neural networks as pointwise affine maps, making them accessible to a broader range of analysis methods to help close the gap between theory and practice. We begin by showing the equivalence of neural networks with parameter-varying affine maps parameterized by the state (feature) vector. As the paper\'s main results, we provide necessary and sufficient conditions for the global stability of generic deep feedforward neural networks. Further, we identify links between the spectral properties of layer-wise weight parametrizations, different activation functions, and their effect on the overall network\'s eigenvalue spectra. We analyze a range of neural networks with varying weight initializations, activation functions, bias terms, and depths. Our view of neural networks as affine parameter varying maps allows us to ""crack open the black box"" of global neural network dynamical behavior through visualization of stationary points, regions of attraction, state-space partitioning, eigenvalue spectra, and stability properties. Our analysis covers neural networks both as an end-to-end function and component-wise without simplifying assumptions or approximations. The methods we develop here provide tools to establish relationships between global neural dynamical properties and their constituent components which can aid in the principled design of neural networks for dynamics modeling and optimal control.', 'corpus_id': 227209469, 'score': 0}, {'doc_id': '197464334', 'title': 'Multi-Resolution Dynamic Mode Decomposition', 'abstract': 'We demonstrate that the integration of the recently developed dynamic mode decomposition (DMD) with a multi-resolution analysis allows for a decomposition method capable of robustly separating complex systems into a hierarchy of multi-resolution time-scale components. A one-level separation allows for background (low-rank) and foreground (sparse) separation of dynamical data, or robust principal component analysis. The multi-resolution dynamic mode decomposition is capable of characterizing nonlinear dynamical systems in an equation-free manner by recursively decomposing the state of the system into low-rank terms whose temporal coefficients in time are known. DMD modes with temporal frequencies near the origin (zero-modes) are interpreted as background (low-rank) portions of the given dynamics, and the terms with temporal frequencies bounded away from the origin are their sparse counterparts. The multi-resolution dynamic mode decomposition (mrDMD) method is demonstrated on several examples involving multi-scale dynamical data, showing excellent decomposition results, including sifting the El Ni\\~no mode from ocean temperature data. It is further applied to decompose a video data set into separate objects moving at different rates against a slowly varying background. These examples show that the decomposition is an effective dynamical systems tool for data-driven discovery.', 'corpus_id': 197464334, 'score': 1}, {'doc_id': '227127165', 'title': 'Reduced Order Modeling for Parameterized Time-Dependent PDEs using Spatially and Memory Aware Deep Learning', 'abstract': 'We present a novel reduced order model (ROM) approach for parameterized time-dependent PDEs based on modern learning. The ROM is suitable for multi-query problems and is nonintrusive. It is divided into two distinct stages: A nonlinear dimensionality reduction stage that handles the spatially distributed degrees of freedom based on convolutional autoencoders, and a parameterized time-stepping stage based on memory aware neural networks (NNs), specifically causal convolutional and long short-term memory NNs. Strategies to ensure generalization and stability are discussed. The methodology is tested on the heat equation, advection equation, and the incompressible Navier-Stokes equations, to show the variety of problems the ROM can handle.', 'corpus_id': 227127165, 'score': 0}, {'doc_id': '227334662', 'title': 'Stochastic SPOD-Galerkin model of a turbulent jet.', 'abstract': 'The use of spectral proper orthogonal decomposition (SPOD) to construct low-order models for broadband turbulent flows is explored. The choice of SPOD modes as basis vectors is motivated by their optimality and space-time coherence properties for statistically stationary flows. This work follows the modeling paradigm that complex nonlinear fluid dynamics can be approximated as stochastically forced linear systems. The proposed model combines ideas from Galerkin projection and linear multi-level regression (MLR) modeling. The resulting stochastic two-level SPOD-Galerkin model governs a compound state consisting of the modal expansion coefficients and forcing coefficients. In the first level, the modal expansion coefficients are advanced by the forced linearized Navier-Stokes operator under the linear time-invariant assumption. The second level governs the forcing coefficients, which compensate for the offset between the linear approximation and the true state. At this level, least squares regression is used to achieve closure by modeling nonlinear interactions between modes. The statistics of the remaining residue are used to construct a dewhitening filter. This filter facilitates the use of white noise to drive the stochastic flow model. The model is demonstrated on large eddy simulation data of a turbulent jet at Mach number 0.9 and a Reynolds number of approximately 1M. The surrogate data obtained from the model accurately reproduces the second-order statistics and dynamics of the input data, and instantaneous instances of the surrogate flow field are visually indistinguishable from the original data. The model uncertainty, predictability, and stability are quantified analytically, when possible, and through Monte Carlo simulation.', 'corpus_id': 227334662, 'score': 0}, {'doc_id': '14103585', 'title': 'Multi-resolution Dynamic Mode Decomposition for Foreground/Background Separation and Object Tracking', 'abstract': 'We demonstrate that the integration of the recently developed dynamic mode decomposition with a multi-resolution analysis allows for a decomposition of video streams into multi-time scale features and objects. A one-level separation allows for background (low-rank) and foreground (sparse) separation of the video, or robust principal component analysis. Further iteration of the method allows a video data set to be separated into objects moving at different rates against the slowly varying background, thus allowing for multiple-target tracking and detection. The algorithm is computationally efficient and can be integrated with many further innovations including compressive sensing architectures and GPU algorithms.', 'corpus_id': 14103585, 'score': 1}, {'doc_id': '227127100', 'title': 'A non-autonomous equation discovery method for time signal classification', 'abstract': 'Certain neural network architectures, in the infinite-layer limit, lead to systems of nonlinear differential equations. Motivated by this idea, we develop a framework for analyzing time signals based on non-autonomous dynamical equations. We view the time signal as a forcing function for a dynamical system that governs a time-evolving hidden variable. As in equation discovery, the dynamical system is represented using a dictionary of functions and the coefficients are learned from data. This framework is applied to the time signal classification problem. We show how gradients can be efficiently computed using the adjoint method, and we apply methods from dynamical systems to establish stability of the classifier. Through a variety of experiments, on both synthetic and real datasets, we show that the proposed method uses orders of magnitude fewer parameters than competing methods, while achieving comparable accuracy. We created the synthetic datasets using dynamical systems of increasing complexity; though the ground truth vector fields are often polynomials, we find consistently that a Fourier dictionary yields the best results. We also demonstrate how the proposed method yields graphical interpretability in the form of phase portraits.', 'corpus_id': 227127100, 'score': 0}, {'doc_id': '33660419', 'title': 'Multiresolution Dynamic Mode Decomposition', 'abstract': 'We demonstrate that the integration of the recently developed dynamic mode decomposition (DMD) with a multiresolution analysis allows for a decomposition method capable of robustly separating complex systems into a hierarchy of multiresolution time-scale components. A one-level separation allows for background (low-rank) and foreground (sparse) separation of dynamical data, or robust principal component analysis. The multiresolution DMD (mrDMD) is capable of characterizing nonlinear dynamical systems in an equation-free manner by recursively decomposing the state of the system into low-rank terms whose temporal coefficients in time are known. DMD modes with temporal frequencies near the origin (zero-modes) are interpreted as background (low-rank) portions of the given dynamics, and the terms with temporal frequencies bounded away from the origin are their sparse counterparts. The mrDMD method is demonstrated on several examples involving multiscale dynamical data, showing excellent decomposition results, ...', 'corpus_id': 33660419, 'score': 1}, {'doc_id': '228373682', 'title': 'Rank-adaptive tensor methods for high-dimensional nonlinear PDEs', 'abstract': 'We present a new rank-adaptive tensor method to compute the numerical solution of high-dimensional nonlinear PDEs. The new method combines functional tensor train (FTT) series expansions, operator splitting time integration, and a new rank-adaptive algorithm based on a thresholding criterion that limits the component of the PDE velocity vector normal to the FTT tensor manifold. This yields a scheme that can add or remove tensor modes adaptively from the PDE solution as time integration proceeds. The new algorithm is designed to improve computational efficiency, accuracy and robustness in numerical integration of high-dimensional problems. In particular, it overcomes well-known computational challenges associated with dynamic tensor integration, including low-rank modeling errors and the need to invert the covariance matrix of the tensor cores at each time step. Numerical applications are presented and discussed for linear and nonlinear advection problems in two dimensions, and for a four-dimensional Fokker-Planck equation.', 'corpus_id': 228373682, 'score': 0}, {'doc_id': '18815881', 'title': 'Dynamic Mode Decomposition for Real-Time Background/Foreground Separation in Video', 'abstract': 'This paper introduces the method of dynamic mode decomposition (DMD) for robustly separating video frames into background (low-rank) and foreground (sparse) components in real-time. The method is a novel application of a technique used for characterizing nonlinear dynamical systems in an equation-free manner by decomposing the state of the system into low-rank terms whose Fourier components in time are known. DMD terms with Fourier frequencies near the origin (zero-modes) are interpreted as background (low-rank) portions of the given video frames, and the terms with Fourier frequencies bounded away from the origin are their sparse counterparts. An approximate low-rank/sparse separation is achieved at the computational cost of just one singular value decomposition and one linear equation solve, thus producing results orders of magnitude faster than a leading separation method, namely robust principal component analysis (RPCA). The DMD method that is developed here is demonstrated to work robustly in real-time with personal laptop-class computing power and without any parameter tuning, which is a transformative improvement in performance that is ideal for video surveillance and recognition applications.', 'corpus_id': 18815881, 'score': 1}]"
69	{'doc_id': '210832977', 'title': 'CollabAR \x96 Investigating the Mediating Role of Mobile AR Interfaces on Co-Located Group Collaboration', 'abstract': 'Mobile Augmented Reality (AR) technology is enabling new applications for different domains including architecture, education or medical work. As AR interfaces project digital data, information and models into the real world, it allows for new forms of collaborative work. However, despite the wide availability of AR applications, very little is known about how AR interfaces mediate and shape collaborative practices. This paper presents a study which examines how a mobile AR (M-AR) interface for inspecting and discovering AR models of varying complexity impacts co-located group practices. We contribute new insights into how current mobile AR interfaces impact co-located collaboration. Our results show that M-AR interfaces induce high mental load and frustration, cause a high number of context switches between devices and group discussion, and overall leads to a reduction in group interaction. We present design recommendations for future work focusing on collaborative AR interfaces.', 'corpus_id': 210832977}	13715	"[{'doc_id': '229166079', 'title': 'Collaborative Virtual Reality Usage in Educa- tional and Training Process', 'abstract': 'This paper focuses on a utilization of advanced virtual reality approaches and technologies as especially progressive tools in the context of online education, training and testing. The purpose of the utilization is to allow easier, faster and more attractive education and training in the areas containing topics and concepts that are not easy to comprehend or that are too expensive to be carried out in the real world. The paper explores collaborative virtual reality and its role in online education, primarily from the LIRKIS G-CVE utilization point of view. LIRKIS G-CVE is a web-based collaborative virtual reality system, based on the A-Frame and Networked-Aframe software solutions. LIRKIS G-CVE is described and three of its educational and training applications are presented: virtual environments for a university course dealing with virtual reality, an environment for patient rehabilitation and another one for an industrial training. Plans for LIRKIS G-CVE utilization in elementary, secondary and high school education are', 'corpus_id': 229166079, 'score': 1}, {'doc_id': '231719722', 'title': 'Art and Science Interaction Lab - A highly flexible and modular interaction science research facility', 'abstract': 'The Art and Science Interaction Lab (“ASIL”) is a unique, highly flexible and modular “interaction science” research facility to effectively bring, analyse and test experiences and interactions in mixed virtual/augmented contexts as well as to conduct research on next-gen immersive technologies. It brings together the expertise and creativity of engineers, performers, designers and scientists creating solutions and experiences shaping the lives of people. The lab is equipped with stateof-the-art visual, auditory and user-tracking equipment, fully synchronized and connected to a central backend. This synchronization allows for highly accurate multi-sensor measurements and analysis.', 'corpus_id': 231719722, 'score': 0}, {'doc_id': '231602925', 'title': 'Proxemics and Social Interactions in an Instrumented Virtual Reality Workshop', 'abstract': 'Virtual environments (VEs) can create collaborative and social spaces, which are increasingly important in the face of remote work and travel reduction. Recent advances, such as more open and widely available platforms, create new possibilities to observe and analyse interaction in VEs. Using a custom instrumented build of Mozilla Hubs to measure position and orientation, we conducted an academic workshop to facilitate a range of typical workshop activities. We analysed social interactions during a keynote, small group breakouts, and informal networking/hallway conversations. Our mixed-methods approach combined environment logging, observations, and semi-structured interviews. The results demonstrate how small and large spaces influenced group formation, shared attention, and personal space, where smaller rooms facilitated more cohesive groups while larger rooms made small group formation challenging but personal space more flexible. Beyond our findings, we show how the combination of data and insights can fuel collaborative spaces’ design and deliver more effective virtual workshops.', 'corpus_id': 231602925, 'score': 1}, {'doc_id': '231836916', 'title': 'Grand Challenges in Immersive Analytics', 'abstract': 'Immersive Analytics is a quickly evolving field that unites several areas such as visualisation, immersive environments, and human-computer interaction to support human data analysis with emerging technologies. This research has thrived over the past years with multiple workshops, seminars, and a growing body of publications, spanning several conferences. Given the rapid advancement of interaction technologies and novel application domains, this paper aims toward a broader research agenda to enable widespread adoption. We present 17 key research challenges developed over multiple sessions by a diverse group of 24 international experts, initiated from a virtual scientific workshop at ACM CHI 2020. These challenges aim to coordinate future work by providing a systematic roadmap of current directions and impending hurdles to facilitate productive and effective applications for Immersive Analytics.', 'corpus_id': 231836916, 'score': 0}, {'doc_id': '107507436', 'title': 'Blue bathroom en suite by Ypsilon', 'abstract': 'Ypsilon have launched this beautiful Blue bathroom en suite\xa0that flip the bathroom into your stylish area for refined bathing and even relaxation.\xa0The doll collection features sleek surfaces, in vibrant and feminine blue, appropriate for our area. the...', 'corpus_id': 107507436, 'score': 0}, {'doc_id': '227014601', 'title': 'Combining Gesture and Voice Control for Mid-Air Manipulation of CAD Models in VR Environments', 'abstract': 'Modeling 3D objects in domains like Computer Aided Design (CAD) is time-consuming and comes with a steep learning curve needed to master the design process as well as tool complexities. In order to simplify the modeling process, we designed and implemented a prototypical system that leverages the strengths of Virtual Reality (VR) hand gesture recognition in combination with the expressiveness of a voice-based interface for the task of 3D modeling. Furthermore, we use the Constructive Solid Geometry (CSG) tree representation for 3D models within the VR environment to let the user manipulate objects from the ground up, giving an intuitive understanding of how the underlying basic shapes connect. The system uses standard mid-air 3D object manipulation techniques and adds a set of voice commands to help mitigate the deficiencies of current hand gesture recognition techniques. A user study was conducted to evaluate the proposed prototype. The combination of our hybrid input paradigm shows to be a promising step towards easier to use CAD modeling.', 'corpus_id': 227014601, 'score': 0}, {'doc_id': '228102859', 'title': 'RealitySketch: An AR interface to create responsive sketches', 'abstract': ""Researchers at University of Calgary, Adobe Research and University of Colorado Boulder have recently created an augmented reality (AR) interface that can be used to produce responsive sketches, graphics and visualizations. Their work, initially pre-published on arXiv, won the Best Paper Honorable Mention and Best Demo Honorable Mention awards at the ACM Symposium on User Interface Software and Technology (UIST'20)."", 'corpus_id': 228102859, 'score': 1}, {'doc_id': '231171502', 'title': 'Mobile Multiuser AR/VR for Training', 'abstract': 'Augmented reality (AR) is a growing technology for building immersive and interactive applications for anyone to use. Integrated development environments such as Unity when enhanced with additional plugins like Unity’s AR Foundation and Photon Network, enable the rapid development of multiuser, mobile, augmented, and mixed reality (XR) applications that can be both entertaining and useful. This report is about a mobile AR and virtual reality (VR), or XR multiuser classroom prototype application. This project was developed on Unity with Unity’s AR Foundation Kit, Photon Network, and Android AR Core plugins to implement an Android based mobile phone application and a desktop application. The final multiuser applications include a real-time multiuser AR/VR environment for an emulated AR/VR Classroom, and an AR Spinning Top Demonstration. Keywords— augmented reality (AR). virtual reality (VR), mixed reality (XR), Unity, Photon Network, AR Foundation, AR Core.', 'corpus_id': 231171502, 'score': 1}, {'doc_id': '231879568', 'title': 'A Survey on Synchronous Augmented, Virtual and Mixed Reality Remote Collaboration Systems', 'abstract': 'Remote collaboration systems have become increasingly important in today’s society, especially during times where physical distancing is advised. Industry, research and individuals face the challenging task of collaborating and networking over long distances. While video and teleconferencing are already widespread, collaboration systems in augmented, virtual, and mixed reality are still a niche technology. We provide an overview of recent developments of synchronous remote collaboration systems and create a taxonomy by dividing them into three main components that form such systems: Environment, Avatars, and Interaction. A thorough overview of existing systems is given, categorising their main contributions in order to help researchers working in different fields by providing concise information about specific topics such as avatars, virtual environment, visualisation styles and interaction. The focus of this work is clearly on synchronised collaboration from a distance. A total of 82 unique systems for remote collaboration are discussed, including more than 100 publications and 25 commercial systems.', 'corpus_id': 231879568, 'score': 1}, {'doc_id': '96852050', 'title': 'Group III-Nitrides Nanostructures M. Pérez-Caro a , M. Ramírez-López a , J.S. Rojas-Ramírez a , I. Martínez-', 'abstract': 'We report on the growth and characterization of self-assembled InGaN columnar nanostructures grown by gas source molecular beam epitaxy (GSMBE) on Si(111) substrates. At a zero concentration of Ga, InN nanocolumns (NCs) were successfully grown. In the case of InGaN, the surface morphology is dependent on composition; however, in general, InGaN samples exhibit columnar features. At concentrations near 50%, the samples show phase separation; this result is explained in terms of solid phase immiscibility.', 'corpus_id': 96852050, 'score': 0}]"
70	{'doc_id': '231632692', 'title': 'Spatio-temporal characterization of causal electrophysiological activity stimulated by single pulse focused ultrasound: an ex vivo study on hippocampal brain slices', 'abstract': 'Objective. The brain operates via generation, transmission and integration of neuronal signals and most neurological disorders are related to perturbation of these processes. Neurostimulation by focused ultrasound (FUS) is a promising technology with potential to rival other clinically used techniques for the investigation of brain function and treatment of numerous neurological diseases. The purpose of this study was to characterize spatial and temporal aspects of causal electrophysiological signals directly stimulated by short, single pulses of FUS on ex vivo mouse hippocampal brain slices. Approach. Microelectrode arrays (MEAs) are used to study the spatio-temporal dynamics of extracellular neuronal activities both at the single neuron and neural networks scales. Hence, MEAs provide an excellent platform for characterization of electrical activity generated, modulated and transmitted in response to FUS exposure. In this study, a novel mixed FUS/MEA platform was designed for the spatio-temporal description of the causal responses generated by single 1.78 MHz FUS pulses in ex vivo mouse hippocampal brain slices. Main results. Our results show that FUS pulses can generate local field potentials (LFPs), sustained by synchronized neuronal post-synaptic potentials, and reproducing network activities. LFPs induced by FUS stimulation were found to be repeatable to consecutive FUS pulses though exhibiting a wide range of amplitudes (50–600 μV), durations (20–200 ms), and response delays (10–60 ms). Moreover, LFPs were spread across the hippocampal slice following single FUS pulses thus demonstrating that FUS may be capable of stimulating different neural structures within the hippocampus. Significance. Current knowledge on neurostimulation by ultrasound describes neuronal activity generated by trains of repetitive ultrasound pulses. This novel study details the causal neural responses produced by single-pulse FUS neurostimulation while illustrating the distribution and propagation properties of this neural activity along major neural pathways of the hippocampus.', 'corpus_id': 231632692}	15039	"[{'doc_id': '100765549', 'title': 'Evolution and effects of surface chemistry of activated coke during combined SO_2/NO_xremoval process', 'abstract': 'The evolution of surface chemistry of activated coke during combined SO2/NOx removal process can be used to elucidate the mechanism of flue gas purification process.The experiments on combined desulfurization/denitration of the flue gas were conducted in a micro reactor.The effect of trace SO2 left in the flue gas after desulfurization on removal of NO by the selective catalytic reduction(SCR),as well as the influence of denitration on followed desulfurization were investigated.The X-ray photoelectron spectroscopy(XPS) and the scanning electron microscope(SEM) were used to characterize the surface properties and morphology of activated coke before and after desulfurization/denitration.The results show that the depleting adsorbed NH3 consumed by reaction with SO2 is the main reason for decreasing NO conversion.The desulfurization efficiency of activated coke are strengthened because of the decreasing acidic surface functional groups resulting from the decrease of oxygen content,delocalization of the π-electron of carbon,and the introduction of N-containing compounds onto the surface of activated coke resulting in the enhancement of basic surface functional groups.', 'corpus_id': 100765549, 'score': 0}, {'doc_id': '232170289', 'title': 'Optogenetically Induced Spatiotemporal Gamma Oscillations in Visual Cortex', 'abstract': 'It has been hypothesized that Gamma cortical oscillations play important roles in numerous cognitive processes and may involve psychiatric conditions including anxiety, schizophrenia, and autism. Gamma rhythms are commonly observed in many brain regions during both waking and sleep states, yet their functions and mechanisms remain a matter of debate. Spatiotemporal Gamma oscillations can explain neuronal representation, computation, and the shaping of communication among cortical neurons, even neurological and neuropsychiatric disorders in neo-cortex. In this study, the neural network dynamics and spatiotemporal behavior in the cerebral cortex are examined during Gamma brain activity. We have directly observed the Gamma oscillations on visual processing as spatiotemporal waves induced by targeted optogenetics stimulation. We have experimentally demonstrated the constant optogenetics stimulation based on the ChR2 opsin under the control of the CaMKIIα promotor, which can induce sustained narrowband Gamma oscillations in the visual cortex of rats during their comatose states. The injections of the viral vector [LentiVirus – CaMKIIα – ChR2] was performed at two different depths, 200 and 500 μm. Finally, we computationally analyze our results via Wilson-Cowan model.', 'corpus_id': 232170289, 'score': 0}, {'doc_id': '53101394', 'title': 'Electrical Field Shaping Techniques in a Feline Model of Retinal Degeneration', 'abstract': 'The majority of preclinical studies investigating multi-electrode field shaping stimulation strategies for retinal prostheses, have been conducted in normally-sighted animals. This study aimed to reassess the effectiveness of two electrical field shaping techniques that have been shown to work in healthy retinae, in a more clinically relevant animal model of photoreceptor degeneration. Four cats were unilaterally blinded via intravitreal injections of adenosine triphosphate. Cortical responses to traditional monopolar (MP) stimulation, focused multipolar (FMP) stimulation and two-dimensional current steering were recorded. Contrary to our previous work, we found no significant difference between the spread of cortical activation elicited by FMP and MP stimulation, and we were not able to reproduce cortical responses to singleelectrode retinal stimulation using two-dimensional current steering. These findings suggest that while shown to be effective in normally-sighted animals, these techniques may not be readily translatable to patients with retinal degeneration and require further optimization.', 'corpus_id': 53101394, 'score': 1}, {'doc_id': '94916477', 'title': 'Nucleosides, LVIII1 Synthesis of Base - Modified Oligonucleotides Containing 6- and 7-Aryl Lumazines', 'abstract': 'Abstract 6-Phenyl-, 7-phenyl-, 6-(4-biphenyl)- 7-(4-biphenyl)lumazine N-1-2-deoxy-β-D-ribofuranosides were synthesized, then converted into the corresponding 5′-O-dimethoxytrityl-3′-O-(β-cyanoethyl, N,N-diisopropyl)phosphoramidites and incorporated into different positions of self-complementary oligonucleotides. The influence of modifications on the melting temparature of the resulting duplexes was studied. This paper is dedicated to Prof. Yoshihisa Mizuno with best wishes to his 75th birthday', 'corpus_id': 94916477, 'score': 0}, {'doc_id': '54442319', 'title': 'Focal stimulation of the sheep motor cortex with a chronically implanted minimally invasive electrode array mounted on an endovascular stent', 'abstract': 'Direct electrical stimulation of the brain can alleviate symptoms associated with Parkinson’s disease, depression, epilepsy and other neurological disorders. However, access to the brain requires invasive procedures, such as the removal of a portion of the skull or the drilling of a burr hole. Also, electrode implantation into tissue can cause inflammatory tissue responses and brain trauma, and lead to device failure. Here, we report the development and application of a chronically implanted platinum electrode array mounted on a nitinol endovascular stent for the localized stimulation of cortical tissue from within a blood vessel. Following percutaneous angiographic implantation of the device in sheep, we observed stimulation-induced responses of the facial muscles and limbs of the animals, similar to those evoked by electrodes implanted via invasive surgery. Proximity of the electrode to the motor cortex, yet not its orientation, was integral to achieving reliable responses from discrete neuronal populations. The minimally invasive endovascular surgical approach offered by the stent-mounted electrode array might enable safe and efficacious stimulation of focal regions in the brain.Focal stimulation of cortical tissue from within a blood vessel via an electrode array mounted on a minimally invasive endovascular stent elicits responses from specific facial muscles and limbs in sheep.', 'corpus_id': 54442319, 'score': 1}, {'doc_id': '232076079', 'title': 'Update on the multimodal pathophysiological dataset of gradual cerebral ischemia in a cohort of juvenile pigs: auditory, sensory and high-frequency sensory evoked potentials', 'abstract': 'We expand from a spontaneous to an evoked potentials (EP) data set of brain electrical activities as electrocorticogram (ECoG) and electrothalamogram (EThG) in juvenile pig under various sedation, ischemia and recovery states. This EP data set includes three stimulation paradigms: auditory (AEP, 40 and 2000 Hz), sensory (SEP, left and right maxillary nerve) and high-frequency oscillations (HFO) SEP. This permits derivation of electroencephalogram (EEG) biomarkers of corticothalamic communication under these conditions. The data set is presented in full band sampled at 2000 Hz. We provide technical validation of the evoked responses for the states of sedation, ischemia and recovery. This extended data set now permits mutual inferences between spontaneous and evoked activities across the recorded modalities. Future studies on the dataset may contribute to the development of new brain monitoring technologies, which will facilitate the prevention of neurological injuries. Background & Summary This extension dataset adds the dimension of evoked potentials (EPs) to the recently published1 dataset containing spontaneous ten-channel electrocorticogram (ECoG) and electrothalamogram (EThG) activities, accompanied by the data on cerebral blood flow2, cardiovascular dynamics and metabolites. Specifically, we present the recordings of the sensorimotor and auditory EPs (SEPs, AEPs) during the same conditions of several sedation states, followed by gradual and controlled cerebral ischemia and 60 minutes of recovery as presented for the spontaneous activity.1 The model remains to be that of juvenile pigs where we originally introduced the basic stereotaxic approach to chronically recording EThG and quantifying the effects of isoflurane and fentanyl sedation on the brain electrical activity from a ten-channel ECoG, EThG, and the cerebral blood flow2, followed by the characterization of the effects of gradual propofol sedation on these parameters.3 As for the study of spontaneous brain electrical and cardiovascular activities, for the study of EPs the choice of this animal model is also dictated by its excellent amenability to complex stereotactic chronic instrumentation, including the well-controlled application of SEPs via snout electrodes and AEP via in-ear headphones, prolonged stimulation required for high-frequency oscillations (HFO) SEPs4,5, studies of sedation and clinically relevant patterns of hypoxic/ischemic injury in a relatively large and gyrencephalic brain.2 We hope this dataset will contribute to the exciting area of ongoing research into the physiology of SEPs, HFO SEPs,6 and mid-latency AEPs7,8 as manifestations of thalamocortical or intracortical communication that can be harnessed as biomarkers of sedation, cerebral ischemia and neuronal recovery. For the technical validation, we present the complete raw and processed SEP, HFO SEP and AEP datasets as well as a representative analysis. We hope the present EP dataset will contribute to development of new brain monitoring technologies, which will facilitate the prevention of neurological disorders. All experiments were carried out in accordance with the European Communities Council Directive 86/609/EEC for animal care and use. The Animal Research Committee of the Thuringian State government approved laboratory animal protocols. Methods Instrumentation General instrumentation. The general surgical procedure is identical to the previously reported approach.1 Instrumentation of the head (Fig. 1). The head instrumentation procedure is identical to the previously reported approach (Table 1, Figure 1).1 Figure 1. Instrumentation of the head. In addition, the following steps were performed to enable SEP and AEP studies. SEP electrodes were installed bilaterally on the outer edges of the pig snout to stimulate maxillary nerves. For AEP studies, the animal’s ears were first freed from the stereotactic apparatus. Then, a miniature in-ear headset was inserted on each side (Sennheiser MX 300). Description of experimental stages The experimental protocol is summarized in Figure 2, as presented in 1. After post-surgical recovery, the pigs were allowed 90 minutes to stabilize (State 1) with electrocorticogram (ECoG) and electrothalamogram (EThG) recorded continuously until necropsy. Then, isoflurane in N2O and O2 was discontinued and ventilation with 100% O2 was performed for 5 minutes. Another phase (State 2a) ensued in which intravenous bolus injection of fentanyl (0,015 mg/kg b.w.) was carried out followed by continuous iv infusion of fentanyl (0.015 mg/kg b.w./h) for 90 minutes (State 2b). Next, individual doses of propofol required for the maintenance of deep anesthesia were determined under continuous control of mean ABP (MABP). Propofol was infused intravenously (0.9 mg/kg BW/min for ~ 7 min) until a burst suppression pattern (BSP) appeared in the ECoG. The depth of anesthesia was subsequently maintained for 25 minutes via propofol administration (~ 0.35 mg/kg b.w./min) (State 3). Next, 30% of the propofol dose required for BSP induction was continuously administered over the course of 90 minutes to produce moderate anesthesia. About ten minutes after the onset of the moderate anesthesia period, the first measurement was performed (State 4). State 5 represented the measurement 60 min after induction of the moderate propofol sedation. We induced gradual cerebral ischemia as follows.9 First, the cisterna magna was punctured by a lumbar puncture needle that was fixed in place by dental acrylic resin for elective artificial cerebrospinal fluid infusion/withdrawal to control ICP. Then, the mean ABP was adjusted to about 90 mmHg by the appropriate curbing of the pulmonary trunk diameter with the plastic-coated cerclage. The cerebral perfusion pressure (CPP) was then decreased at 25 mmHg, which was calculated as the difference between MABP and the intracranial pressure (ICP) by appropriate elevation of the ICP. The increase in the ICP was achieved by the infusion of artificial cerebrospinal fluid (warmed to 37°C) into the subarachnoid space via the punctured cisterna magna. The Cushing response during severe brain ischemia was prevented by the appropriate curbing of the pulmonary trunk diameter with the plastic-coated cerclage to control cardiac output. Finally, the cerclage was opened completely, and the artificial cerebrospinal fluid was withdrawn to reach an ICP < 10 mmHg. The states of gradual ischemia were maintained for 15 min twice (States 6 and 8), interceded by a recovery state (State 7) lasting 30 minutes in all animals (P746 P794) except P739 where it lasted 15 minutes. This was followed by 60 min of recovery (States 9-12). Figure 2. The experimental protocol. On the onset of each state, the evoked potentials were administered in the following order: 1. SEP left 2. SEP right 3. AEP 40 Hz 4. AEP 2000 Hz 5. HFO SEP required up to 16 minutes stimulation duration and, hence, were not performed for each state due to limitations in time allotted for each state. Table 2 reviews which recordings are available for each state. Data acquisition and analyses. The approach is identical to what we previously reported.1 In addition, the following methods were used to trigger and record SEP and AEP. As reported in 2, SEPs were induced by bipolar application of constant current rectangular impulses (70 ms, 5 mA applied at 1 pulse/s (HES-Stimulator T, Fa. Hugo Sachs Elektronik, Hugstetten, Germany), for 2 minutes at every side, respectively. The recordings were sampled at 2 kHz and averaged across 100 sweeps. AEPs were induced by applying auditory stimuli first at 40 Hz and then at 2000 Hz. The auditory stimuli were administered at 90 dB, 150 μs DC impulses, 40 ms duration followed by 5/s frequency with filter bandpass of 15-1500 Hz. The recording electrodes were placed bilaterally on the mastoid; the reference was placed on the vertex. These stimulation parameters were generated using a compiled version of the custom Matlab script provided by Dr. Akeroyd’s lab (available for download at the referenced permanent URL).10,7,11 HFO SEPs were induced by bipolar application of constant current rectangular impulses (70 ms, 5 mA applied (HES-Stimulator T, Fa. Hugo Sachs Elektronik, Hugstetten, Germany), at a frequency of 5 Hz or 9 Hz for 16 min or 10 minutes at every side, respectively. Raw data files were band-pass filtered between 400 Hz and 800 Hz with an FIR digital filter according to 4 and averaged. Raw files with constant current rectangular impulse stimulation displayed frequently short-term impulse-synchone artifacts (duration 1 ms at the stimulus time point) with detectable alteration on FIR digital filter effects. Therefore, for each ECoG and EThG channel we removed 4 ms before and after every stimulus and replaced the missing data by a moving average between the previous and subsequent values. Signal analysis methodology Evoked potentials were calculated using the EP-plugin of Watisa® software. Therefore, recorded stimulus signals were used for event triggering. Subsequently, signal segments of 100 ms duration, starting at the trigger time point, were averaged and stored. Finally, respective EPs determined under identical experimental conditions were averaged as grand mean and used for further analysis.', 'corpus_id': 232076079, 'score': 0}, {'doc_id': '3456860', 'title': 'Creating virtual electrodes with 2D current steering.', 'abstract': ""OBJECTIVE\nCurrent steering techniques have shown promise in retinal prostheses as a way to increase the number of distinct percepts elicitable without increasing the number of implanted electrodes. Previously, it has been shown that 'virtual' electrodes can be created between simultaneously stimulated electrode pairs, producing unique cortical response patterns. This study investigated whether virtual electrodes could be created using 2D current steering, and whether these virtual electrodes can produce cortical responses with predictable spatial characteristics.\n\n\nAPPROACH\nNormally-sighted eyes of seven adult anaesthetised cats were implanted with a 42-channel electrode array in the suprachoroidal space and multi-unit neural activity was recorded from the visual cortex. Stimuli were delivered to individual physical electrodes, or electrodes grouped into triangular, rectangular, and hexagonal arrangements. Varying proportions of charge were applied to each electrode in a group to 'steer' current and create virtual electrodes. The centroids of cortical responses to stimulation of virtual electrodes were compared to those evoked by stimulation of single physical electrodes.\n\n\nMAIN RESULTS\nResponses to stimulation of groups of up to six electrodes with equal ratios of charge on each electrode resulted in cortical activation patterns that were similar to those elicited by the central physical electrode (centroids: RM ANOVA on ranks, p\u2009\u2009>\u2009\u20090.05; neural spread: one-way ANOVA on Ranks, p\u2009\u2009>\u2009\u20090.05). We were also able to steer the centroid of activation towards the direction of any of the electrodes of the group by applying a greater charge to that electrode, but the movement in the centroid was not found to be significant.\n\n\nSIGNIFICANCE\nThe results suggest that current steering is possible in two dimensions between up to at least six electrodes, indicating it may be possible to increase the number of percepts in patients without increasing the number of physical electrodes. Being able to reproduce spatial characteristics of responses to individual physical electrodes suggests that this technique could also be used to compensate for faulty electrodes."", 'corpus_id': 3456860, 'score': 1}, {'doc_id': '203355359', 'title': 'Preclinical investigation of electrical field shaping strategies for retinal prostheses', 'abstract': ""Retinal prostheses are a promising technology aimed at restoring vision in people suffering severe retinal degenerative conditions such as retinitis pigmentosa (RP). Present-generation devices achieve this by electrically stimulating the residual neuronal population in the retina following degeneration in order to elicit the perception of light. At present, patients implanted with these devices are able to perceive multiple localised flashes of light, termed phosphenes, which are used to build up an artificial image of the patient's surroundings. However, present-generation retinal implants lack the spatial resolution to provide a suitable replacement for everyday visual tasks. While adequate for rudimentary tasks such as object recognition, motion detection, and pattern recognition, more complex tasks such as reading, facial recognition, and independent navigation are still not possible with modern prosthetic vision devices. Two major issues that affect retinal prostheses are: the large spread of electrical potential in the retina resulting in widespread activation of neurons, undesirable electrical field interaction and the elicitation of large phosphenes that patients find difficult to discriminate between; and that many devices are not able to elicit enough phosphenes to convey complex visual information to patients.\n\nThe studies presented in this thesis investigated the effectiveness of two multichannel electrical field shaping techniques: focused multipolar (FMP) stimulation and virtual electrode (VE) current steering. These techniques have shown considerable promise in studies conducted with one-dimensional neural prosthetic devices, such as cochlear and deep brain implants, as ways to restrict and `steer' electrical fields. In an effort to find new ways of improving spatial resolution, I have investigated whether these techniques can be adapted for use in a 2D retinal prosthesis. Using a normally-sighted cat model I have demonstrated that FMP stimulation is capable of restricting current spread in two dimensions and eliciting retinal and cortical response patterns with reduced spread compared with responses to the more conventional MP means of stimulation. I have also demonstrated that VE current steering between up to six electrodes can produce cortical activation patterns in predictable locations, with similar spread of neural activation as response patterns to physical electrode stimulation. By varying the proportions of charge applied to steering electrodes, it was also possible to shift the location of cortical activation in two dimensions in a predictable and intuitive fashion. To investigate the effectiveness of these techniques in a model more representative of patients, FMP stimulation and VE current steering were re-evaluated using a cat model of retinal degeneration. Unfortunately, many of the promising results from the normally-sighted cohort were not maintained when applied to degenerate retinae. While FMP stimulation still activated a localised population of retinal neurons, it was not found to elicit cortical response patterns with reduced spread compared to monopolar stimulation. The location of cortical response patterns elicited by VE stimulation were also found to be unpredictable. These results also show evidence of compressed retinotopy and increased spatial selectivity in the degenerate visual system, which significantly altered neural responses to electrical stimulation. These findings demonstrate that FMP stimulation and VE current steering, in their present form, may not be as effective in focusing and steering neural activation when applied to degenerate retinae. These results also provide a greater understanding of the differences between the responses of healthy and degenerate visual systems to electrical stimulation, which I hope will inform the further development and optimisation of these stimulation strategies."", 'corpus_id': 203355359, 'score': 1}, {'doc_id': '31108071', 'title': 'Spatial Restriction of Neural Activation Using Focused Multipolar Stimulation With a Retinal Prosthesis.', 'abstract': 'PURPOSE\nThe resolution provided by present state-of-the-art retinal prostheses is severely limiting for recipients, partly due to the broad spread of activation in the retina in response to monopolar (MP) electrical stimulation. Focused multipolar (FMP) stimulation has been shown to restrict neural activation in the cochlea compared to MP stimulation. We extended the FMP stimulation technique to a two-dimensional electrode array and compared its efficacy to MP and hexapolar (HP) stimulation in the retina.\n\n\nMETHODS\nNormally-sighted cats (n = 6) were implanted with a suprachoroidal electrode array containing 42 electrodes. Multichannel multiunit spiking activity was recorded from the visual cortex in response to MP, HP, and FMP retinal stimulation.\n\n\nRESULTS\nWhen inferring retinal spread using voltage recordings off the stimulating array, FMP stimulation showed significantly reduced voltages in regions surrounding the primary stimulating electrode. When measuring the retinal and cortical selectivity of neural responses, FMP and HP stimulation showed significantly higher selectivity compared to MP stimulation (separate 2-way ANOVAs, P < 0.05). However, the lowest cortical thresholds for each stimulating electrode were higher for FMP and HP compared to MP stimulation (1-way ANOVA, P < 0.001). No significant differences were observed between FMP and HP stimulation in any measures.\n\n\nCONCLUSIONS\nFocused multipolar and HP stimulation using a two-dimensional array are promising techniques to reduce the spread of activation for a retinal prosthesis. Clinical application would be expected to result in smaller phosphenes; thus, reducing phosphene overlap between electrodes and increasing the resolution at the expense of higher thresholds for activation.', 'corpus_id': 31108071, 'score': 1}, {'doc_id': '231741339', 'title': 'Open-Source Concealed EEG Data Collection for Brain-Computer-Interfaces - Real-World Neural Observation Through OpenBCI Amplifiers with Around-the-Ear cEEGrid Electrodes', 'abstract': 'Observing brain activity in real-world settings offers exciting possibilities like the support of physical health, mental well-being, and thought-controlled interaction modalities. The development of such applications is, however, strongly impeded by poor accessibility to research-grade neural data and by a lack of easy-to-use and comfortable sensors. This work presents the cost-effective adaptation of concealed around-the-ear EEG electrodes (cEEGrids) to the open-source OpenBCI EEG signal acquisition platform to provide a promising new toolkit. An integrated system design is described, that combines publicly available electronics components with newly designed 3D-printed parts to form an easily replicable, versatile, single-unit around-the-ear EEG recording system for prolonged use and easy application development. To demonstrate the system’s feasibility, observations of experimentally induced changes in visual stimulation and mental workload are presented. Lastly, as there have been no applications of the cEEGrids to HCI contexts, a novel application area for the system is investigated, namely the observation of flow experiences through observation of temporal Alpha power changes. Support for a link between temporal Alpha power and flow is found, which indicates an efficient engagement of verbal-analytic reasoning with intensified flow experiences, and specifically intensified task absorption.', 'corpus_id': 231741339, 'score': 0}]"
71	{'doc_id': '206027570', 'title': 'The sleep switch: hypothalamic control of sleep and wakefulness', 'abstract': 'More than 70 years ago, von Economo predicted a wake-promoting area in the posterior hypothalamus and a sleep-promoting region in the preoptic area. Recent studies have dramatically confirmed these predictions. The ventrolateral preoptic nucleus contains GABAergic and galaninergic neurons that are active during sleep and are necessary for normal sleep. The posterior lateral hypothalamus contains orexin/hypocretin neurons that are crucial for maintaining normal wakefulness. A model is proposed in which wake- and sleep-promoting neurons inhibit each other, which results in stable wakefulness and sleep. Disruption of wake- or sleep-promoting pathways results in behavioral state instability.', 'corpus_id': 206027570}	3414	"[{'doc_id': '14700979', 'title': 'Sleep and synaptic renormalization: a computational study.', 'abstract': 'Recent evidence indicates that net synaptic strength in cortical and other networks increases during wakefulness and returns to a baseline level during sleep. These homeostatic changes in synaptic strength are accompanied by corresponding changes in sleep slow wave activity (SWA) and in neuronal firing rates and synchrony. Other evidence indicates that sleep is associated with an initial reactivation of learned firing patterns that decreases over time. Finally, sleep can enhance performance of learned tasks, aid memory consolidation, and desaturate the ability to learn. Using a large-scale model of the corticothalamic system equipped with a spike-timing dependent learning rule, in agreement with experimental results, we demonstrate a net increase in synaptic strength in the waking mode associated with an increase in neuronal firing rates and synchrony. In the sleep mode, net synaptic strength decreases accompanied by a decline in SWA. We show that the interplay of activity and plasticity changes implements a control loop yielding an exponential, self-limiting renormalization of synaptic strength. Moreover, when the model ""learns"" a sequence of activation during waking, the learned sequence is preferentially reactivated during sleep, and reactivation declines over time. Finally, sleep-dependent synaptic renormalization leads to increased signal-to-noise ratios, increased resistance to interference, and desaturation of learning capabilities. Although the specific mechanisms implemented in the model cannot capture the variety and complexity of biological substrates, and will need modifications in line with future evidence, the present simulations provide a unified, parsimonious account for diverse experimental findings coming from molecular, electrophysiological, and behavioral approaches.', 'corpus_id': 14700979, 'score': 1}, {'doc_id': '269890', 'title': 'Toward a detailed computational model for the mammalian circadian clock', 'abstract': 'We present a computational model for the mammalian circadian clock based on the intertwined positive and negative regulatory loops involving the Per, Cry, Bmal1, Clock, and Rev-Erb α genes. In agreement with experimental observations, the model can give rise to sustained circadian oscillations in continuous darkness, characterized by an antiphase relationship between Per/Cry/Rev-Erbα and Bmal1 mRNAs. Sustained oscillations correspond to the rhythms autonomously generated by suprachiasmatic nuclei. For other parameter values, damped oscillations can also be obtained in the model. These oscillations, which transform into sustained oscillations when coupled to a periodic signal, correspond to rhythms produced by peripheral tissues. When incorporating the light-induced expression of the Per gene, the model accounts for entrainment of the oscillations by light-dark cycles. Simulations show that the phase of the oscillations can then vary by several hours with relatively minor changes in parameter values. Such a lability of the phase could account for physiological disorders related to circadian rhythms in humans, such as advanced or delayed sleep phase syndrome, whereas the lack of entrainment by light-dark cycles can be related to the non-24h sleep-wake syndrome. The model uncovers the possible existence of multiple sources of oscillatory behavior. Thus, in conditions where the indirect negative autoregulation of Per and Cry expression is inoperative, the model indicates the possibility that sustained oscillations might still arise from the negative autoregulation of Bmal1 expression.', 'corpus_id': 269890, 'score': 1}, {'doc_id': '214641382', 'title': 'Effect of Diverse Recoding of Granule Cells on Optokinetic Response in A Cerebellar Ring Network with Synaptic Plasticity', 'abstract': 'We consider a cerebellar ring network for the optokinetic response (OKR), and investigate the effect of diverse recoding of granule (GR) cells on OKR by varybing the connection probability pc from Golgi to GR cells. For an optimal value of , individual GR cells exhibit diverse spiking patterns which are in-phase, anti-phase, or complex out-of-phase with respect to their population-averaged firing activity. Then, these diversely-recoded signals via parallel fibers (PFs) from GR cells are effectively depressed by the error-teaching signals via climbing fibers from the inferior olive which are also in-phase ones. Synaptic weights at in-phase PF-Purkinje cell (PC) synapses of active GR cells are strongly depressed via strong long-term depression (LTD), while those at anti-phase and complex out-of-phase PF-PC synapses are weakly depressed through weak LTD. This kind of “effective” depression (i.e., strong/weak LTD) at the PF-PC synapses causes a big modulation in firings of PCs, which then exert effective inhibitory coordination on the vestibular nucleus (VN) neuron (which evokes OKR). For the firing of the VN neuron, the learning gain degree ℒg, corresponding to the modulation gain ratio, increases with increasing the learning cycle, and it saturates at about the 300th cycle. By varying pc from , we find that a plot of saturated learning gain degree versus pc forms a bell-shaped curve with a peak at (where the diversity degree in spiking patterns of GR cells is also maximum). Consequently, the more diverse in recoding of GR cells, the more effective in motor learning for the OKR adaptation.', 'corpus_id': 214641382, 'score': 0}, {'doc_id': '92362657', 'title': 'Observations on the effect of „Fertosan“- and „QR“- preparations upon the humification of straw', 'abstract': 'The utilization of surplus straw has formed an actual problem in Finland during the last years. In addition to the ploughing in of straw with nitrogen fertilizers or the use of straw as litter in larger quantities, fermenting of straw in composts has been recommended. The latter method contains, however, several technical weak points, the most important of which seems to be the insufficient moistening of raw straw that leads to an unequal, slow and incomplete humification of the material. Recently two humifying preparations have been obtainable even in Finland: the bacterial preparation »Fertosan» and the possibly biodynamic preparation »QR». According to the advertisements, both of them are able to accelerate the decomposition of organic material in such a degree that even straw turns into good humus within six weeks. It is stated that this occurs without any application of nitrogen or other mineral nutrients, and that it is due only to the very effective bacteria in »Fertosan» or the substances in »QR» which are claimed to activate the microbial processes to an enormous degree. In order to investigate the value of these preparations some experiments were performed both under laboratory conditions and on a larger scale. The effect of these preparations upon humification of rye straw was compared with that of an usual application of nitrogen and phosphorus fertilizers.', 'corpus_id': 92362657, 'score': 0}, {'doc_id': '46487092', 'title': 'Sleep and synaptic homeostasis: a hypothesis', 'abstract': 'During much of sleep, the cerebral cortex is rippled by slow waves, which appear in the electroencephalogram as oscillations between 0.5 and 4.5 Hz. Slow waves are regulated as a function of previous wakefulness, being maximal at the beginning of sleep and then progressively returning to a baseline level. This paper discusses a hypothesis about the significance of slow-wave activity and its homeostatic regulation. The hypothesis is as follows: 1. Wakefulness is associated with synaptic potentiation in several cortical circuits; 2. Synaptic potentiation is tied to the homeostatic regulation of slow-wave activity; 3. Slow-wave activity is associated with synaptic downscaling; 4. Synaptic downscaling is tied to the beneficial effects of sleep on performance. The hypothesized link between sleep and synaptic homeostasis is supported by several lines of evidence and leads to testable predictions.', 'corpus_id': 46487092, 'score': 1}, {'doc_id': '211126796', 'title': 'What is the function of inter-hemispheric inhibition?.', 'abstract': ""It is widely supposed that following unilateral brain injury, there arises an asymmetry in inter-hemispheric inhibition which has an adverse influence upon motor control. I argue that this 'inter-hemispheric imbalance' model arises from a fundamental misunderstanding of the roles played by inter-hemispheric (callosal) projections in mammalian brains. Drawing upon a large body of empirical data, derived largely from animal models, and associated theoretical modeling, it is demonstrated that inter-hemispheric projections perform contrast enhancing and integrative functions via mechanisms such as surround/lateral inhibition. The principal functional unit of callosal influence comprises a facilitatory centre and a depressing peripheral zone, that together shape the influence of converging inputs to pyramidal neurons. Inter-hemispheric inhibition is an instance of a more general feature of mammalian neural systems, whereby inhibitory interneurons act not simply to prevent over-excitation but to sculpt the output of specific circuits. The narrowing of the excitatory focus that occurs through crossed surround inhibition is a highly conserved motif of transcallosal interactions in mammalian sensory and motor cortices. A case is presented that the notion of 'inter-hemispheric imbalance' has been sustained, and clinical interventions derived from this model promoted, by erroneous assumptions concerning that revealed by investigative techniques such as transcranial magnetic stimulation (TMS). The alternative perspective promoted by the present analysis, also permits the basis of positive (e.g. post stroke) associations between the structural integrity of transcallosal projections and motor capability to be better understood."", 'corpus_id': 211126796, 'score': 0}, {'doc_id': '23286135', 'title': 'A two process model of sleep regulation.', 'abstract': None, 'corpus_id': 23286135, 'score': 1}, {'doc_id': '12913031', 'title': 'Mathematical models of sleep regulation.', 'abstract': 'The level of EEG slow-wave activity (SWA) is determined by the duration of prior sleep and waking. SWA is a marker of nonREM sleep intensity and may serve as an indicator of sleep homeostasis. The two-process model of sleep regulation posits the interaction of the homeostatic Process S and the circadian Process C. Also models of neurobehavioral functions (three-process model; interactive models of alertness and cognitive throughput) are based on the concept of an interaction between homeostatic and circadian factors. Whether the interaction is linear or non-linear is still unresolved. Models may serve as a guiding principle for specifying the relationship between processes occurring at the macroscopic and microscopic level of analysis.', 'corpus_id': 12913031, 'score': 1}, {'doc_id': '211818363', 'title': 'Night-to-night variability of sleep electroencephalography-based brain age measurements', 'abstract': ""OBJECTIVE\nBrain Age Index (BAI), calculated from sleep electroencephalography (EEG), has been proposed as a biomarker of brain health. This study quantifies night-to-night variability of BAI and establishes probability thresholds for inferring underlying brain pathology based on a patient's BAI.\n\n\nMETHODS\n86 patients with multiple nights of consecutive EEG recordings were selected from Epilepsy Monitoring Unit patients whose EEGs reported as within normal limits. While EEGs with epileptiform activity were excluded, the majority of patients included in the study had a diagnosis of chronic epilepsy. BAI was calculated for each 12-hour segment of patient data using a previously established algorithm, and the night-to-night variability in BAI was measured.\n\n\nRESULTS\nThe within-patient night-to-night standard deviation in BAI was 7.5\xa0years. Estimates of BAI derived by averaging over 2, 3, and 4 nights had standard deviations of 4.7, 3.7, and 3.0\xa0years, respectively.\n\n\nCONCLUSIONS\nAveraging BAI over n nights reduces night-to-night variability of BAI by a factor of n, rendering BAI a more suitable biomarker of brain health at the individual level. A brain age risk lookup table of results provides thresholds above which a patient has a high probability of excess BAI.\n\n\nSIGNIFICANCE\nWith increasing ease of EEG acquisition, including wearable technology, BAI has the potential to track brain health and detect deviations from normal physiologic function. The measure of night-to-night variability and how this is reduced by averaging across multiple nights provides a basis for using BAI in patients' homes to identify patients who should undergo further investigation or monitoring."", 'corpus_id': 211818363, 'score': 0}, {'doc_id': '215548638', 'title': 'State-dependent regulation of cortical processing speed via gain modulation', 'abstract': 'To thrive in dynamic environments, animals must be capable of rapidly and flexibly adapting behavioral responses to a changing context and internal state. Examples of behavioral flexibility include faster stimulus responses when attentive and slower responses when distracted. Contextual or state-dependent modulations may occur early in the cortical hierarchy and may be implemented via top-down projections from cortico-cortical or neuromodulatory pathways. However, the computational mechanisms mediating the effects of such projections are not known. Here, we introduce a theoretical framework to classify the effects of cell-type specific top-down perturbations on the information processing speed of cortical circuits. Our theory demonstrates that perturbation effects on stimulus processing can be predicted by intrinsic gain modulation, which controls the timescale of the circuit dynamics. Our theory leads to counter-intuitive effects such as improved performance with increased input variance. We tested the model predictions using large-scale electrophysiological recordings from the visual hierarchy in freely running mice, where we found that a decrease in single-cell intrinsic gain during locomotion led to an acceleration of visual processing. Our results establish a novel theory of cell-type specific perturbations, applicable to top-down modulation as well as optogenetic and pharmacological manipulations. Our theory links connectivity, dynamics, and information processing via gain modulation.', 'corpus_id': 215548638, 'score': 0}]"
72	"{'doc_id': '128026031', 'title': 'Mesolithic/Neolithic Interactions in the Balkans and in the Middle Danube Basin', 'abstract': '9 papers from the session on Mesolithic/Neolithic Interactions in the Balkans and in the Middle Danube Basin held at the 15th UISPP Congress in Lisbon in September 2006. Contents: 1) Mesolithic/Neolithic interactions in the Balkan Peninsula and the Carpathian Basin: an introduction (Marek Nowak); 2) The chipped stone assemblages of Mentese and the problem of the earliest occupation of Marmara region (Ivan Gatsov, Petranka Nedelcheva); 3) Late Mesolithic of Serbia and Montenegro (Duan Mihailoviae); 4) Mesolithic-Neolithic interactions in the Danube Gorges (Duan Boric); 5) Palaeogeographical background of the Mesolithic and Early Neolithic settlement in the Carpathian Basin (Pal Suemegi); 6) Mesolithic foragers and the spread of agriculture in Western Hungary by (Eszter Banffy, William J. Eichmann, Tibor Marton); 7) Early Neolithic raw material economies in the Carpathian Basin (Katalin T. Biro); 8) Neolithisation of the upper Tisza basin (Janusz K. Kozlowski, Marek Nowak); 9) Problems in reading MesolithicNeolithic relations in South-Eastern Europe (Janusz K. Kozlowski, Marek Nowak).""', 'corpus_id': 128026031}"	5026	"[{'doc_id': '54514662', 'title': 'Further notes on Mesolithic-Neolithic contacts in the Iron Gates Region and the Central Balkans', 'abstract': 'Hunter-gatherer/farmer contact in the Iron Gates region is re-examined in view of recent archaeological research, and the social dynamics, population movements and interactions of small scale societies. Full, non-hostile interaction between hunter-gatherers and farmers in the Iron Gates region is proposed for the mid- 7th millennium calBC, followed by hunter-gatherer encapsulation at the end 7th millennium calBC. The lack of archaeological records on the Central Balkan Postglacial and Early Holocene hunter-gatherers is highlighted as a major obstacle to fully understanding cultural transformations, including the Neolithic transition, in this region.', 'corpus_id': 54514662, 'score': 1}, {'doc_id': '218944570', 'title': 'Reset redux: possible evolutionary pathways towards the transformation of tourism in a COVID-19 world', 'abstract': 'Abstract With international arrivals surpassing 1.5 billion for the first time in 2019 the long-term evolution of tourism demonstrates prolific path dependence with a decade of growth since the global financial crisis. This latest period of unfettered international tourism development has come to an abrupt end as the impact of COVID-19 has brought the sector to a near standstill. As the world grapples with the realities of the global pandemic there is an opportunity to rethink exactly what tourism will look like for the decades ahead. Key concepts in evolutionary economic geography, especially path dependence/creation and institutional inertia/innovation, show variations in pathways for travel and tourism in a COVID-19 world. A path that leads to transformation in tourism can be realized if sufficient institutional innovation occurs on both the demand and supply side of tourism that can foster the emergence of new paths. COVID-19 presents a once in a generation opportunity where the institutional pump is primed for transformation. Whether that leads to a radical transformation of the tourism sector remains to be seen, but the imprint it will leave on both the demand and supply of tourism will have long-term, incremental impacts for years to come and ultimately move us closer towards the transformation of tourism.', 'corpus_id': 218944570, 'score': 0}, {'doc_id': '30379036', 'title': 'A systematic review of wild grass exploitation in relation to emerging cereal cultivation throughout the Epipalaeolithic and aceramic Neolithic of the Fertile Crescent', 'abstract': 'The present study investigates the occurrence of wild grasses at Epipalaeolithic and aceramic Neolithic sites in the Near East in order to assess their role in subsistence economies alongside the emergence of cereal cultivation. We use Chogha Golan in the foothills of the central Zagros Mountains (ca. 11.7–9.6 ka cal. BP) as a case study, where the archaeobotanical data suggest the frequent exploitation of a complex of wild grasses for almost 2,000 years. Domesticated emmer replaced these wild grasses as the major food resources towards the end of occupation at the site (ca. 9.8 ka cal. BP). We discuss possible implications of this development and conclude that the traditional concept of pre-domestication cultivation seems unsuited for explaining the patterns from Chogha Golan. These data are in good accordance with the overall picture in the Zagros Mountains, where wild grasses were routinely gathered throughout the early Holocene. In contrast, wild grasses were gradually replaced by wild cereals in the Levantine corridor since the end of the Pleistocene. However, several sites located in this region provide evidence for a continuous exploitation of wild grasses alongside emerging cereal cultivation and most of these taxa were part of the earliest segetal floras that evolved with the appearance of domestic cereals throughout the 11th millennium cal. BP. Some sites contemporary to the Pre-Pottery Neolithic B still provide evidence for the usage of wild grasses, which possibly reflects the utilization of edible arable weeds and continuous gathering of wild grasses by more mobile groups.', 'corpus_id': 30379036, 'score': 1}, {'doc_id': '214623307', 'title': 'Widening of the Andes: An interplay between subduction dynamics and crustal wedge tectonics', 'abstract': 'Shortening of the continental lithosphere is generally accommodated by the growth of crustal wedges building above megathrusts in the mantle lithosphere. We show that the locus of shortening in the western margin of South America has largely been controlled by the geometry of the slab. Numerical models confirm that horizontal subduction favors compression far from the trench, above the asthenospheric wedge and steeply dipping segment of the subducting slab. As a result, a second crustal wedge grows in the hinterland of the continent, and widens the Andes. In the Bolivian orocline, this wedge corresponds to the Eastern Cordillera, whose growth was triggered by a major episode of horizontal subduction. When the slab returned to a steeper dip angle, shortening and uplift pursued, facilitated by the structural and thermo-chemical alteration of the continental lithosphere. We review the successive episodes of horizontal subduction that have occurred beneath South America at different latitudes and show that they explain the diachronic widening of the Andes. We infer that the present-day segmented physiography of the Andes results from the latitudinally variable, transient interplay between slab dynamics and upper plate tectonics over the Cenozoic. We emphasize that slab flattening, or absence thereof, is a major driving mechanism that sets the width of the Andes, at any latitude.', 'corpus_id': 214623307, 'score': 0}, {'doc_id': '215786184', 'title': 'Refining the Uluzzian through a new lithic assemblage from Roccia San Sebastiano (Mondragone, southern Italy)', 'abstract': 'Abstract Roccia San Sebastiano is a tectonic-karstic cave located at the foot of the southern slope of Mt. Massico, in the territory of Mondragone (Caserta) in Campania (southern Italy). Systematic excavation has been carried out since 2001, leading to the partial exploration of an important Pleistocene deposit, extraordinarily rich in lithic and faunal remains. The aim of this paper is to (1) present the stratigraphic sequence of Roccia San Sebastiano, and (2) technologically describe the lithic materials of squares F14 t18, t19, t20; E16 t16, t17, t18 recently recognised as Uluzzian. The stratigraphic sequence is more than 3\xa0m thick and dates from the Middle to the Upper Palaeolithic. It contains different techno-complexes: Gravettian, Aurignacian, Uluzzian and Mousterian. In the Uluzzian lithic assemblage mostly local pebbles of chert were used in order to produce small-sized objects. The concept of debitage mainly deals with unidirectional debitage with absent or fairly accurate management of the convexities and angles; the striking platforms are usually natural or made by one stroke. It is attested the use of both direct freehand percussion and bipolar technique on anvil in the same reduction sequence. Amongst the retouched tools the presence of two lunates is of note. This study of the Roccia San Sebastiano Uluzzian lithic complexes is significant for understanding the dynamics of the transition from Middle to Upper Palaeolithic in the Tyrrhenian margin of southern Italy.', 'corpus_id': 215786184, 'score': 1}, {'doc_id': '134166255', 'title': 'Before the neolithization: Causes of mesolithic diversity in the Southern Balkans', 'abstract': 'The Balkans, particularly southern and central, were sparsely populated in the Mesolithic and the occupation networks in that period were discontinous and highly diversified, contrasting with the density and homogeneity of the Early Neolithic. The aim of this paper is to describe the environmental conditions of the Mesolithic sites in relation to Early Holocene climatic fluctuations and to discuss the causes of specificity and diversity of culture and behaviour at this period. \nSome general trends are observable in the adaptation to Early Holocene environments (trends in faunal exploitation; for ex. shift from high ranked large game to low ranked small animals) but also particular adaptations to local conditions (technological changes due to difficulties in access to better quality lithic raw materials, adaptations to coastal or to terrestrial resources reflecting the unique features of site use, etc). \nThe diversity of the Mesolithic is also reflected in cultural taxonomy: in some sequences continuity of the Balkan Epigravettian techno-morphological tradition can be seen as opposed, in other sequences, to highly isolated groups with technology and tool morphology adapted to local raw materials and specific activities. The Balkan Mesolithic was not completely cut-off from the Western Mediterranean techno-morphological influences (particularly in Southern Greece) and from the Anatolian lithic traditions (seen only in the Northern Aegean). A more intensive network of marine contacts is confirmed by obsidian circulation in the Aegean Basin.', 'corpus_id': 134166255, 'score': 1}, {'doc_id': '218727927', 'title': 'Effect of photoperiod and plant growth regulators on in vitro mass bulblet proliferation of Narcissus tazzeta L. (Amaryllidaceae), a potential source of galantamine', 'abstract': 'Narcissus tazetta L., a bulbous plant belongs to the Amaryllidaceae family, contains alkaloid galantamine (GAL) with acetylcholinesterase inhibitory activity which has been recently considered to treat Alzheimer’s disease (AD). In the current work, the effect of photoperiod (16/8 h light/dark and 24 h dark) and various concentrations of NAA, BAP, and GA3 (0, 0.5, 1 and 2 mg l‒1) on the in vitro mass bulblet regeneration of N. tazetta was studied. The GAL production ability of the regenerated bulblets was assessed by HPLC-UV-MS. Light treatments significantly affected the number of bulblet and leaf, the ratio of bulblet/leaf, and leaf length. The maximum number of bulblet (31.0\u2009±\u20091.58) and leaf (13.3\u2009±\u20091.33) was recorded from the cultures fortified with NAA and BAP (2 mg l‒1) kept in 16/8 h light/dark, while the maximum leaf length (2.1\u2009±\u20090.92 cm) was measured on the MS medium containing 0.5 mg l‒1 NAA and 2 mg l‒1 BAP incubated in the same photoperiod. The average ratio of bulblet proliferation per explant was significantly different between studied photoperiod (1.1\u2009±\u20090.86) and 24 h dark (0.62\u2009±\u20090.31). The regenerated bulblets contained 40 and 20 µg g‒1 DW GAL underexposed photoperiod and 24 h dark, respectively. This information could be useful in the commercial production of GAL as a valuable anti-AD compound through in vitro mass bulblet proliferation of N. tazetta. The regenerated mass bulblets of Narcissus tazetta (Amaryllidaceae) on MS medium containing 2 mg l‒1 NAA and BAP kept in 16/8 h light/dark are recommended to produce galanatamine and lycorine.', 'corpus_id': 218727927, 'score': 0}, {'doc_id': '218539287', 'title': 'Biodiversity Research and Innovation in Antarctica and the Southern Ocean', 'abstract': 'This article examines biodiversity research and innovation in Antarctica and the Southern Ocean based on a review of 150,401 scientific articles and 29,690 patent families for Antarctic species. The paper exploits the growing availability of open access databases, such as the Lens and Microsoft Academic Graph, along with taxonomic data from the Global Biodiversity Information Facility (GBIF) to explore the scientific and patent literature for the Antarctic at scale. The paper identifies the main contours of scientific research in Antarctica before exploring commercially oriented biodiversity research and development in the scientific literature and patent publications. The paper argues that biodiversity is not a free good and must be paid for. Ways forward in debates on commercial research and development in Antarctica can be found through increasing attention to the valuation of ecosystem services, new approaches to natural capital accounting and payment for ecosystem services that would bring the Antarctic, and the Antarctic Treaty System, into the wider fold of work on the economics of biodiversity. Economics based approaches can be criticised for reducing biodiversity to monetary exchange values at the expense of recognition of the wider values of biodiversity and its services. However, approaches grounded in the economics of biodiversity provide a transparent framework for approaching commercial activity in the Antarctic and introducing requirements for investments in the conservation of Antarctic biodiversity by those who seek to profit from it.', 'corpus_id': 218539287, 'score': 0}, {'doc_id': '56087251', 'title': 'Early Neolithic pottery dispersals and demic diffusion in southeastern Europe', 'abstract': 'The 14C gradient of pottery dispersal suggests that the sites in the southern Balkans are not significantly older than those in the northern and eastern Balkans. A gradual demic diffusion model from south to north and a millennium time span vector thus find no confirmation in the set of AMS 14C dates and associated contexts that mark pottery dispersal within Southeastern Europe. The first \'demic event\' that was hypothesised to reshape significantly European population structure and generate a uniform process of neolithisation of southestern Europe has no confirmation in fre- quency of Y-chromosome subhaplogroups J2b and E3b1 distribution within modern population in Southeastern Europe. IZVLE! EK - 14C datumi prve keramike ka! ejo, da zgodnje neolitska najdi""# a na jugu Balkana niso starej"" a od onih na severu. AMS 14C datumi ne potrjujejo modela postopne demske difuzije od juga proti severu in tiso# letni # asovni zamik pri "" iritvi keramike. Prvi \'demski dogodek\', ki naj bi domnev- no preoblikoval evropsko populacijsko sestavo, v jugovzhodni Evropi pa povzro# il proces enovite neo- litizacije, ni potrjen s pogostostjo pojavljanja Y-kromosomskih haploskupin J2b in E3b1 pri sedanjih populacijah v jugovzhodni Evropi.', 'corpus_id': 56087251, 'score': 1}, {'doc_id': '218642831', 'title': 'Correction to: Converting Home Spaces into Food Gardens at the Time of Covid-19 Quarantine: all the Benefits of Plants in this Difficult and Unprecedented Period', 'abstract': 'The number of deaths due to Covid-19 in Italy was incorrectly written as “15,35”. It should be corrected to ""15,915"". The original article has been corrected.', 'corpus_id': 218642831, 'score': 0}]"
73	{'doc_id': '233753065', 'title': 'Macroscopic Equity Markets Model: Towards Predicting Financial Crashes', 'abstract': 'This research examines the structural properties of the macroscopic model introduced in [AlShelahi and Saigal, 2018]. We present a theoretical analysis of the behavior of the macroscopic variables. In particular, we show that the model exhibits shock-like solutions, providing a new narrative for financial shocks. To solve the system of stochastic nonlinear partial differential equations adaptively, an integrative algorithm is devised and tested on abnormal and normal trading days. The results suggest that abnormalities can be identified before crashes. Our findings warrant further investigation into the macroscopic structure of equity markets.', 'corpus_id': 233753065}	20162	"[{'doc_id': '215829436', 'title': 'Stock market prediction using machine learning classifiers and social media, news', 'abstract': 'Accurate stock market prediction is of great interest to investors; however, stock markets are driven by volatile factors such as microblogs and news that make it hard to predict stock market index based on merely the historical data. The enormous stock market volatility emphasizes the need to effectively assess the role of external factors in stock prediction. Stock markets can be predicted using machine learning algorithms on information contained in social media and financial news, as this data can change investors’ behavior. In this paper, we use algorithms on social media and financial news data to discover the impact of this data on stock market prediction accuracy for ten subsequent days. For improving performance and quality of predictions, feature selection and spam tweets reduction are performed on the data sets. Moreover, we perform experiments to find such stock markets that are difficult to predict and those that are more influenced by social media and financial news. We compare results of different algorithms to find a consistent classifier. Finally, for achieving maximum prediction accuracy, deep learning is used and some classifiers are ensembled. Our experimental results show that highest prediction accuracies of 80.53% and 75.16% are achieved using social media and financial news, respectively. We also show that New York and Red Hat stock markets are hard to predict, New York and IBM stocks are more influenced by social media, while London and Microsoft stocks by financial news. Random forest classifier is found to be consistent and highest accuracy of 83.22% is achieved by its ensemble.', 'corpus_id': 215829436, 'score': 1}, {'doc_id': '236687029', 'title': 'Do the Stock Market Indices Follow a Random Walk?', 'abstract': 'This chapter aims to test the hypothesis of an efficient market, in its weak form, in the stock markets of Brazil, China, South Korea, USA, Spain, Italy, in the period from December 2, 2020 to May 12, 2020. The results show that the market efficiency hypothesis is rejected in all markets. In corroboration the DFA exponents show long memories, which put in question the market efficiency, in its weak form, suggesting that the stock markets analyzed show some predictability. In conclusion, investors should avoid investing in stock markets, at least while this pandemic lasts, and invest in less risky markets in order to mitigate risk and improve the efficiency of their portfolios.', 'corpus_id': 236687029, 'score': 0}, {'doc_id': '159414639', 'title': 'Nowcasting and forecasting GDP in emerging markets using global financial and macroeconomic diffusion indexes', 'abstract': 'Abstract This paper contributes to the nascent literature on nowcasting and forecasting GDP in emerging market economies using big data methods. This is done by analyzing the usefulness of various dimension-reduction, machine learning and shrinkage methods, including sparse principal component analysis (SPCA), the elastic net, the least absolute shrinkage operator, and least angle regression when constructing predictions using latent global macroeconomic and financial factors (diffusion indexes) in a dynamic factor model (DFM). We also utilize a judgmental dimension-reduction method called the Bloomberg Relevance Index (BRI), which is an index that assigns a measure of importance to each variable in a dataset depending on the variable’s usage by market participants. Our empirical analysis shows that, when specified using dimension-reduction methods (particularly BRI and SPCA), DFMs yield superior predictions relative to both benchmark linear econometric models and simple DFMs. Moreover, global financial and macroeconomic (business cycle) diffusion indexes constructed using targeted predictors are found to be important in four of the five emerging market economies that we study (Brazil, Mexico, South Africa, and Turkey). These findings point to the importance of spillover effects across emerging market economies, and underscore the significance of characterizing such linkages parsimoniously when utilizing high-dimensional global datasets.', 'corpus_id': 159414639, 'score': 1}, {'doc_id': '150705099', 'title': 'Why Stock Markets Crash: Critical Events in Complex Financial Systems', 'abstract': 'Why Stock Markets Crash: Critical Events in Complex Financial Systems, by Didier Sornette, 2003, Princeton, NJ: Princeton University Press Consider the following events: a pressure tank within a rocket propulsion system fails during a launch; tectonic plates shift, causing the first significant earthquake in a locale for several decades; a stock market experiences a crash after a prolonged run-up in price levels. The commonality here is that all of these events are ultimately characterized by a ""rupture"" in the underlying system, following a buildup of ""pressure"" over a period of time. The recognition of certain engineering and geologic events as analogous in this way to financial market crashes was the impetus for the interesting and enjoyable new book Why Stock Markets Crash: Critical Events in Complex Financial Systems, by Didier Sornette. The major thesis of this book is that a stock market crash is not the result of short-term exogenous events, but rather involves a long-term endogenous buildup, with exogenous events acting merely as triggers. In particular, Sornette examines financial crashes within the framework of the ""spontaneous emergence of extreme events in self-organizing systems,"" noting that ""extreme events are characteristic of many... \'complex systems.\'"" The author employs mathematical tools-specifically, log-periodic power laws-to study the prebubble or precrash buildup in a financial system to its critical point. Efforts by nonfinancial people to analyze and explain financial phenomena using quantitative techniques from the hard and engineering sciences can be of tremendous use and interest to those of us in the financial community-provided that the mathematical techniques are applied by an author with an exposure to and understanding of the financial instruments, processes, and markets that are being analyzed. The author of Why Stock Markets Crash has done an admirable job of understanding and appreciating the financial world and its nuances. Didier Sornette is a professor of geophysics at UCLA, as well as a research director at the National Center of Scientific Research in France. He specializes in the prediction of catastrophic events within a complex system framework. In this book, as well as in a portion of his hundreds of journal articles, he takes his previous work in the physical and geological sciences and exports his mathematical modeling and prediction skills to the financial markets. In the first chapter, Sornette places historical extreme financial events-in particular, market crashes-in a complex, self-organizing system framework. This is followed by two chapters devoted, respectively, to the basic concepts and characteristics of financial markets, and to some statistical analyses demonstrating that financial crashes are essentially outliers. …', 'corpus_id': 150705099, 'score': 1}, {'doc_id': '236159272', 'title': 'A Long Short-Term Memory Network Stock Price Prediction with Leading Indicators.', 'abstract': 'The accuracy of the prediction of stock price fluctuations is crucial for investors, and it helps investors manage funds better when formulating trading strategies. Using forecasting tools to get a predicted value that is closer to the actual value from a given financial data set has always been a major goal of financial researchers and a problem. In recent years, people have paid particular attention to stocks, and gradually used various tools to predict stock prices. There is more than one factor that affects financial trends, and people need to consider it from all aspects, so research on stock price fluctuations has also become extremely difficult. This paper mainly studies the impact of leading indicators on the stock market. The framework used in this article is proposed based on long short-term memory (LSTM). In this study, leading indicators that affect stock market volatility are added, and the proposed framework is thus named as a stock tending prediction framework based on LSTM with leading indicators (LSTMLI). This study uses stock markets in the United States and Taiwan, respectively, with historical data, futures, and options as data sets to predict stock prices in these two markets. We measure the predictive performance of LSTMLI relative to other neural network models, and the impact of leading indicators on stock prices is studied. Besides, when using LSTMLI to predict the rise and fall of stock prices in the article, the conventional regression method is not used, but the classification method is used, which can give a qualitative output based on the data set. The experimental results show that the LSTMLI model using the classification method can effectively reduce the prediction error. Also, the data set with leading indicators is better than the prediction results of the single historical data using the LSTMLI model.', 'corpus_id': 236159272, 'score': 0}, {'doc_id': '237447213', 'title': 'Stock Price Prediction Based on LSTM Deep Learning Model', 'abstract': 'Predicting the stock market is either the easiest or the toughest task in the field of computations. There are many factors related to prediction, physical factors vs. physiological, rational and irrational , capitalist sentiment, market , etc. All these aspects combine to make stock costs volatile and are extremely tough to predict with high accuracy. The prices of a stock market depend very much on demand and supply. High demand stocks will increase in price while heavy selling stocks will decrease. Fluctuations in stock prices affect investor perception and thus there is a need to predict future share prices and to predict stock market prices to make more acquaint and precise investment decisions. We examine data analysis in this domain as a game-changer. This paper proposes that historical value bears the impact of all other market events and can be used to predict future movement. Machine Learning techniques can detect paradigms and insights that can be used to construct surprisingly correct predictions. We propose the LSTM (Long Short Term Memory) model to examine the future price of a stock. This paper is to predict stock market prices to make more acquaint and precise investment decisions.', 'corpus_id': 237447213, 'score': 0}, {'doc_id': '237285849', 'title': 'A SURVEY ON STOCK PRICE PREDICTION USING MACHINE LEARNING', 'abstract': 'Stock returns are very fluctuating in nature. They rely upon various factors like previous stock prices, current market trends, financial news, etc. To feature their annual income, people have now started watching stock investments as a remunerative option. There are many tools available to investors using technical analysis to form decisions. With expert guidance and intelligent planning, we will almost double our annual income through stock returns. These days, social media has become a mirror. It reflects people’s thoughts and opinions on any particular event or news. Sentiments of the general public associated with an organization can have an upshot on its stock prices. This paper surveys various machine learning techniques and algorithms employed to boost the accuracy of stock price prediction. Keywords— Stock prices, Technical analysis, Stock Returns, Social Media, Sentiments', 'corpus_id': 237285849, 'score': 0}, {'doc_id': '237252600', 'title': 'On the Construction of a Positive Sentiment Index for COVID-19: Evidence from G20 Stock Markets', 'abstract': 'The present study investigates the degree of market responses through the scope of investors’ sentiment during the COVID-19 pandemic across G20 markets, by constructing a novel positive search volume index for COVID-19 (COVID19+). Our key findings, obtained using a Panel-GARCH model, indicate that an increased COVID19+ index suggests that investors decrease their COVID-19 related crisis sentiment by escalating their Google searches for positively associated COVID-19 related keywords. Specifically, we explore the predictive power of the newly constructed index on stock returns and volatility. According to our findings, investor sentiment positively (negatively) predicts the stock return (volatility) during the COVID-19. This is the first study of its kind assessing global sentiment by proposing a novel proxy and its impacts on the G20 equity market.', 'corpus_id': 237252600, 'score': 0}, {'doc_id': '221283286', 'title': 'Predicting Stock Market Trends Using Machine Learning and Deep Learning Algorithms Via Continuous and Binary Data; a Comparative Analysis', 'abstract': 'The nature of stock market movement has always been ambiguous for investors because of various influential factors. This study aims to significantly reduce the risk of trend prediction with machine learning and deep learning algorithms. Four stock market groups, namely diversified financials, petroleum, non-metallic minerals and basic metals from Tehran stock exchange, are chosen for experimental evaluations. This study compares nine machine learning models (Decision Tree, Random Forest, Adaptive Boosting (Adaboost), eXtreme Gradient Boosting (XGBoost), Support Vector Classifier (SVC), Naïve Bayes, K-Nearest Neighbors (KNN), Logistic Regression and Artificial Neural Network (ANN)) and two powerful deep learning methods (Recurrent Neural Network (RNN) and Long short-term memory (LSTM). Ten technical indicators from ten years of historical data are our input values, and two ways are supposed for employing them. Firstly, calculating the indicators by stock trading values as continuous data, and secondly converting indicators to binary data before using. Each prediction model is evaluated by three metrics based on the input ways. The evaluation results indicate that for the continuous data, RNN and LSTM outperform other prediction models with a considerable difference. Also, results show that in the binary data evaluation, those deep learning methods are the best; however, the difference becomes less because of the noticeable improvement of models’ performance in the second way.', 'corpus_id': 221283286, 'score': 1}, {'doc_id': '201107067', 'title': 'The Emergence of Critical Stocks in Market Crash', 'abstract': 'In complex systems like financial market, risk tolerance of individuals is crucial for system resilience. The single-security price limit, designed as risk tolerance to protect investors by avoiding sharp price fluctuation, is blamed for feeding market panic in times of crash. The relationship between the critical market confidence which stabilizes the whole system and the price limit is therefore an important aspect of system resilience. Using a simplified dynamic model on networks of investors and stocks, an unexpected linear association between price limit and critical market confidence is theoretically derived and empirically verified in this paper. Our results highlight the importance of relatively “small” but critical stocks that drive the system to collapse by passing the failure from periphery to core. These small stocks, largely originating from homogeneous investment strategies across the market, has unintentionally suppressed system resilience with the exclusive increment of individual risk tolerance. Imposing random investment requirements to mitigate herding behavior can thus improve the market resilience.', 'corpus_id': 201107067, 'score': 1}]"
74	"{'doc_id': '14167441', 'title': 'Insider Trading Before Accounting Scandals', 'abstract': ""We examine insider trading in a sample of more than 500 firms involved in accounting scandals revealed by earnings-decreasing restatements, and in a control sample of non-restating firms. Managers who sell stock while earnings are misstated potentially commit two crimes, earnings manipulation and insider trading, and their selling increases investor scrutiny and the likelihood of the manipulation being revealed. We examine the purchases, sales and net sales of five groups of corporate insiders during the misstated period and a pre-misstated period, using a difference-in-differences approach. Using several measures of the level of insider trading, we estimate cross-sectional regressions that control for other determinants of the level of insider trading. For the full sample of restating firms, we find weak evidence that top managers of misstating firms sell more stock during the misstated period than during the pre-misstated period, relative to the control sample. But in a number of subsamples where insiders had greater incentives to sell before the revelation of accounting problems, we find strong evidence that top managers of restating firms sell substantially more stock during the misstated period. These findings suggest that managers' desire to sell their stockholdings at inflated prices is a motive for earnings manipulation. Our finding that insiders brazenly trade on a crime for which they are potentially culpable suggests that insider trading is more widespread in the market than has been found in the prior literature."", 'corpus_id': 14167441}"	19825	[{'doc_id': '235739851', 'title': 'Firm Fundamentals in the COVID-19 Stock Market', 'abstract': 'The effects of the COVID-19 crisis allow for an opportunity to examine what types of firms are most durable to sudden and prolonged stock market shocks. Using data on the constituents of the Russell 3000, I find that firms with more capital-intensive operations, higher leverage, and larger cash balances were most prone to drops in daily return related to increases in the growth of COVID-19 infections. Larger, more profitable, and more liquid firms saw positive interactions between daily returns and COVID-19 infection growth. However, I also find a potential reversal of these trends as riskier and more leveraged firms exhibit higher cumulative abnormal returns off vaccine hopes, which may signal a renewed interest in riskier investment profiles as the world begins and continues to move past the COVID-19 pandemic. This study presents insight on daily stock return reactions to COVID-19 infection growth throughout the course of the virus.', 'corpus_id': 235739851, 'score': 0}, {'doc_id': '236505302', 'title': 'Multiple large shareholders and corporate fraud: evidence from China', 'abstract': 'This study tests the effect of multiple large shareholders on the level of corporate fraud using the data of Chinese listed companies from 2010 to 2018. We find lower probabilities and lower corporate fraud frequencies when there are multiple large shareholders in Chinese listed companies, indicating that their presence plays a supervisory role in internal governance. These results persist after we control for endogeneity. Moreover, the effect of multiple large shareholders on corporate fraud is strengthened with the separation of control right and cash flow right. Further analyses reveal that companies with multiple large shareholders experience considerably reduced information disclosure fraud but no reduction in operating or leader frauds. Additionally, information asymmetry and the capital occupation of controlling shareholders both play a mediating role in the relationship between multiple large shareholders and the level of corporate fraud. This study enriches the literature on the determinants of corporate fraud and the effects of multiple large shareholders. Our findings also provide implications for companies and regulators regarding ways to reduce fraud.', 'corpus_id': 236505302, 'score': 0}, {'doc_id': '236177869', 'title': 'How Does Reduced Timeliness of Public Enforcement Affect Corporate Disclosure Behavior in a Developing Financial Market?', 'abstract': 'We examine the consequences of a regulatory reform that reduces the timeliness of public enforcement of mandatory corporate disclosure in a developing financial market. Contrary to the intent of the regulatory reform, corporate disclosure timeliness is not increased while corporate disclosure accuracy is decreased following the reform. Our evidence suggests that independent auditors do, but many major market institutions, including independent directors, mutual fund managers and financial analysts do not, play any significant role in mitigating corporate managers’ incentive to reduce disclosure quality after the reform. Consistent with stock market efficiency, stock market investors correctly anticipate the reduced quality of corporate disclosure by discounting the stock price at the time of disclosure.', 'corpus_id': 236177869, 'score': 1}, {'doc_id': '236933027', 'title': 'Does Social Trust Mitigate Insiders’ Opportunistic Behavior? Evidence from Insider Trading', 'abstract': 'We investigate whether social trust can mitigate insider trading profitability. Our empirical evidence shows that social trust surrounding corporations’ headquarters is negatively associated with corporate insiders’ ability for trading gains. This relation holds in a range of tests including instrumental variable methods and using social trust value from CEO’s birthplace. We further find that social trust plays a more important role in curbing insiders’ trading profitability when firms have a higher level of information asymmetry, poorer corporate governance, and when firms are non-State-owned. Finally, we show that firms headquartered in high social trust regions tend to engage in more investor communication, have a lower probability to restate financial statements, and have lower stock price synchronicity.', 'corpus_id': 236933027, 'score': 1}, {'doc_id': '235719783', 'title': 'Private equity and bank capital requirements: Evidence from European firms', 'abstract': 'Using firm-level data from 16 euro-area countries over 2008-2014, we investigate how the growth and investment of bank-affiliated private equity-backed companies evolve after the European Banking Authority (EBA) increases capital requirements for their parent banks. We find that portfolio companies connected to affected banks reduce their investment, asset growth, and employment growth following the capital exercise. We further show that the effect is stronger for companies likely to face financial constraints. Finally, the findings indicate that the negative effect of the capital exercise is muted when the private equity sponsor is more experienced.', 'corpus_id': 235719783, 'score': 0}, {'doc_id': '236425831', 'title': 'Insider Trading in Family Firms: Impact of Family Ties and Management Involvement', 'abstract': 'We study corporate insider trading and information leakages within family firms. We find that the prof-itability of insider purchases of family insiders is higher compared to those of nonfamily insiders. In contrast, the profitability of insider sales of family insiders is lower compared to those of nonfamily insiders, indicating that family members time their insider trades well when buying shares, but not when selling shares. Furthermore, family insiders without management involvement time their insider sales significantly better, compared to family insiders who are involved in the firm management. Regarding information leakage, abnormal short-selling volumes prior to insider sales are higher when shares are sold by family insiders without management involvement, compared to family insiders who are in-volved in management.', 'corpus_id': 236425831, 'score': 1}, {'doc_id': '236207402', 'title': 'Earnings Forecasts of Female CEOs: Quality and Consequences', 'abstract': 'This study examines the voluntary disclosure of earnings forecasts by female CEOs. We find that in the backdrop of increased pressure to perform from investors and other stakeholders, female CEOs tend to issue more earnings forecasts than male CEOs, and those forecasts are more accurate. We also find that while financial analysts generally prefer to follow companies headed by male CEOs, female CEOs’ efforts to issue accurate earnings forecasts pay off as these efforts help them close the analyst coverage gap. We provide complementary evidence on the disclosure efforts of female CEOs with regard to updates to the forecast and the 10-K report. Lastly, we show that financial analysts rely more on the earnings forecasts of female CEOs, possibly because they realize female CEOs’ superior forecasting quality. Our results are robust to the use of alternative research designs, including difference-in-difference, propensity score matching, and entropy balancing. Overall, our study documents gender differences in voluntary disclosure by senior management.', 'corpus_id': 236207402, 'score': 0}, {'doc_id': '236451378', 'title': 'Do algorithm traders mitigate insider trading profits?: Evidence from the Thai stock market', 'abstract': 'This paper asks whether algorithm traders (AT) mitigate insider trading profits in the Thai stock market over the period of 2010–2016. We find that in general it does but not in the case of buy side, big trades nor the executive trades. Our findings suggest that, to some extent, AT can take important role to increase an efficiency in stock market by processing the public information and incorporating it into price at ultra-fast speed. Additional robustness checks based on the instrumental variable approach confirm our findings.', 'corpus_id': 236451378, 'score': 0}, {'doc_id': '237291024', 'title': 'CEO Social Media Presence and Insider Trading', 'abstract': 'Prior research finds that online social media usage may lower self-control and encourage indulgent behavior in laboratory subjects. We find that corporate CEOs show similar tendencies: CEOs with online social media presence are more likely to succumb to lower self-control and abuse their information advantage to profit from unethical insider trades. Specifically, CEOs’ social media presence strongly predicts their insider trading activity in terms of incidence, intensity (amount and frequency), and profitability. We further find that the effect is driven by insider buys (not by sells) and is more pronounced for opportunistic buys which tend to contain more material non-public information.', 'corpus_id': 237291024, 'score': 1}, {'doc_id': '235895344', 'title': 'Hiding or Helping? Determinants and Consequences of the Timing of Earnings Conference Calls', 'abstract': 'Open conference calls reveal important information because of their forward-looking discussion, interactive nature, and easy accessibility. Using Bloomberg data, we investigate why firms hold earnings conference calls at different times of the day and how the stock market interprets and reacts to firms’ timing choices. We measure retrospective and prospective news in earnings calls using earnings surprises and the tone of forward-looking statements. We find that firms with more extreme news (both good and bad) tend to hold calls outside trading hours, especially in the evening. To test whether the market understands the information embedded in call “timing,” we conduct an event study around the date when firms schedule the calls. We find higher trading volume when the market is notified of an upcoming switch from trading hours to outside them. Our results suggest that firms strategically time conference calls and that investors infer some information from their timing switches.', 'corpus_id': 235895344, 'score': 1}]
75	"{'doc_id': '25513752', 'title': 'An Evolved RNA Recognition Motif That Suppresses HIV-1 Tat/TAR-Dependent Transcription.', 'abstract': ""Potent and selective recognition and modulation of disease-relevant RNAs remain a daunting challenge. We previously examined the utility of the U1A N-terminal RNA recognition motif as a scaffold for tailoring new RNA hairpin recognition and showed that as few as one or two mutations can result in moderate affinity (low μM dissociation constant) for the human immunodeficiency virus (HIV) trans-activation response element (TAR) RNA, an RNA hairpin controlling transcription of the human immunodeficiency virus (HIV) genome. Here, we use yeast display and saturation mutagenesis of established RNA-binding regions in U1A to identify new synthetic proteins that potently and selectively bind TAR RNA. Our best candidate has truly altered, not simply broadened, RNA-binding selectivity; it binds TAR with subnanomolar affinity (apparent dissociation constant of ∼0.5 nM) but does not appreciably bind the original U1A RNA target (U1hpII). It specifically recognizes the TAR RNA hairpin in the context of the HIV-1 5'-untranslated region, inhibits the interaction between TAR RNA and an HIV trans-activator of transcription (Tat)-derived peptide, and suppresses Tat/TAR-dependent transcription. Proteins described in this work are among the tightest TAR RNA-binding reagents-small molecule, nucleic acid, or protein-reported to date and thus have potential utility as therapeutics and basic research tools. Moreover, our findings demonstrate how a naturally occurring RNA recognition motif can be dramatically resurfaced through mutation, leading to potent and selective recognition-and modulation-of disease-relevant RNA."", 'corpus_id': 25513752}"	5948	[{'doc_id': '2674461', 'title': 'A rapid, generally applicable method to engineer zinc fingers illustrated by targeting the HIV-1 promoter', 'abstract': 'DNA-binding domains with predetermined sequence specificity are engineered by selection of zinc finger modules using phage display, allowing the construction of customized transcription factors. Despite remarkable progress in this field, the available protein-engineering methods are deficient in many respects, thus hampering the applicability of the technique. Here we present a rapid and convenient method that can be used to design zinc finger proteins against a variety of DNA-binding sites. This is based on a pair of pre-made zinc finger phage-display libraries, which are used in parallel to select two DNA-binding domains each of which recognizes given 5 base pair sequences, and whose products are recombined to produce a single protein that recognizes a composite (9 base pair) site of predefined sequence. Engineering using this system can be completed in less than two weeks and yields proteins that bind sequence-specifically to DNA with Kd values in the nanomolar range. To illustrate the technique, we have selected seven different proteins to bind various regions of the human immunodeficiency virus 1 (HIV-1) promoter.', 'corpus_id': 2674461, 'score': 1}, {'doc_id': '219946674', 'title': 'A yeast BiFC-seq method for genome-wide interactome mapping', 'abstract': 'Genome-wide physical protein-protein interaction (PPI) mapping remains a major challenge for current technologies. Here, we report a high-efficiency yeast bimolecular fluorescence complementation method coupled with next-generation DNA sequencing (BiFC-seq) for interactome mapping. We applied this technology to systematically investigate an intraviral network of Ebola virus (EBOV). Two-thirds (9/13) of known interactions of EBOV were recaptured and five novel interactions were discovered. Next, we used BiFC-seq method to map the interactome of the tumor protein p53. We identified 97 interactors of p53 with more than three quarters are novel. Furthermore, in more complex background, we screened potential interactors by pooling two BiFC-libraries together, and revealing a network of 229 interactions among 205 proteins. These results show that BiFC-seq is a highly sensitive, rapid and economical method in genome-wide interactome mapping.', 'corpus_id': 219946674, 'score': 0}, {'doc_id': '216071947', 'title': 'Enhancement of trans-cleavage activity of Cas12a with engineered crRNA enables amplified nucleic acid detection', 'abstract': 'The CRISPR/Cas12a RNA-guided complexes have a tremendous potential for nucleic acid detection due to its ability to indiscriminately cleave ssDNA once bound to a target DNA. However, the current CRISPR/Cas12a systems are limited to detecting DNA in a picomolar detection limit without an amplification step. Here, we developed a platform with engineered crRNAs and optimized conditions that enabled us to detect DNA, DNA/RNA heteroduplex and methylated DNA with higher sensitivity, achieving a limit of detection of in femtomolar range without any target pre-amplification step. By extending the 3’- or 5’-ends of the crRNA with different lengths of ssDNA, ssRNA, and phosphorothioate ssDNA, we discovered a new self-catalytic behavior and an augmented rate of LbCas12a-mediated collateral cleavage activity as high as 3.5-fold compared to the wild-type crRNA. We applied this sensitive system to detect as low as 25 fM dsDNA from the PCA3 gene, an overexpressed biomarker in prostate cancer patients, in simulated urine over 6 hours. The same platform was used to detect as low as ~700 fM cDNA from HIV, 290 fM RNA from HCV, and 370 fM cDNA from SARS-CoV-2, all within 30 minutes without a need for target amplification. With isothermal amplification of SARS-CoV-2 RNA using RT-LAMP, the modified crRNAs were incorporated in a paper-based lateral flow assay that could detect the target with up to 23-fold higher sensitivity within 40-60 minutes.', 'corpus_id': 216071947, 'score': 0}, {'doc_id': '219398807', 'title': 'An Off-the-Shelf Approach for the Production of Fc Fusion Proteins by Protein Trans-Splicing towards Generating a Lectibody In Vitro', 'abstract': 'Monoclonal antibodies, engineered antibodies, and antibody fragments have become important biological therapeutic platforms. The IgG format with bivalent binding sites has a modular structure with different biological roles, i.e., effector and binding functions, in different domains. We demonstrated the reconstruction of an IgG-like domain structure in vitro by protein ligation using protein trans-splicing. We produced various binding domains to replace the binding domain of IgG from Escherichia coli and the Fc domain of human IgG from Brevibacillus choshinensis as split-intein fusions. We showed that in vitro protein ligation could produce various Fc-fusions at the N-terminus in vitro from the independently produced domains from different organisms. We thus propose an off-the-shelf approach for the combinatorial production of Fc fusions in vitro with several distinct binding domains, particularly from naturally occurring binding domains. Antiviral lectins from algae are known to inhibit virus entry of HIV and SARS coronavirus. We demonstrated that a lectin could be fused with the Fc-domain in vitro by protein ligation, producing an IgG-like molecule as a “lectibody”. Such an Fc-fusion could be produced in vitro by this approach, which could be an attractive method for developing potential therapeutic agents against rapidly emerging infectious diseases like SARS coronavirus without any genetic fusion and expression optimization.', 'corpus_id': 219398807, 'score': 0}, {'doc_id': '38524282', 'title': 'Phage display methods for selecting zinc finger proteins with novel DNA-binding specificities.', 'abstract': 'Publisher Summary DNA-binding proteins play critical roles in cell biology and the design of proteins, with novel sequence specificities or functions, may have important applications in research, biotechnology, and medicine. Recent reports have highlighted some of the potential of these design efforts. For example, proteins, with designed DNA-binding specificities, have been used to regulate the transcription of specific genes. DNA-binding domains also have been attached to other proteins, such as nucleases and general transcription factors, to create hybrid proteins, with interesting properties. These designed DNA-binding proteins offer great promise as research tools, and may eventually be used in gene therapy. However, many potential applications will require binding to novel target sites and so the utility of these proteins may depend on our ability to design or select DNA-binding domains, with desired DNA sequence specificities. This chapter describes the methods for (1) preparing zinc finger phage libraries and (2) selecting phage, with new DNA-binding specificities. Because two different phage systems have been used here, alternate examples of key protocols have been provided to illustrate the range of strategies that may be successful. Since general aspects of phage display are discussed elsewhere, this chapter discusses those aspects that are essential for and/or unique to zinc finger phage selections. The appendix at the end of this chapter describes solutions and procedures used in the protocols here.', 'corpus_id': 38524282, 'score': 1}, {'doc_id': '205165742', 'title': 'Protein and Antibody Engineering by Phage Display.', 'abstract': 'Phage display is an in vitro selection technique that allows for the rapid isolation of proteins with desired properties including increased affinity, specificity, stability, and new enzymatic activity. The power of phage display relies on the phenotype-to-genotype linkage of the protein of interest displayed on the phage surface with the encoding DNA packaged within the phage particle, which allows for selective enrichment of library pools and high-throughput screening of resulting clones. As an in vitro method, the conditions of the binding selection can be tightly controlled. Due to the high-throughput nature, rapidity, and ease of use, phage display is an excellent technological platform for engineering antibody or proteins with enhanced properties. Here, we describe methods for synthesis, selection, and screening of phage libraries with particular emphasis on designing humanizing antibody libraries and combinatorial scanning mutagenesis libraries. We conclude with a brief section on troubleshooting for all stages of the phage display process.', 'corpus_id': 205165742, 'score': 1}, {'doc_id': '215731267', 'title': 'Recombinant expression of nanobodies and nanobody-derived immunoreagents', 'abstract': '\n Abstract\n \n Antibody fragments for which the sequence is available are suitable for straightforward engineering and expression in both eukaryotic and prokaryotic systems. When produced as fusions with convenient tags, they become reagents which pair their selective binding capacity to an orthogonal function. Several kinds of immunoreagents composed by nanobodies and either large proteins or short sequences have been designed for providing inexpensive ready-to-use biological tools. The possibility to choose among alternative expression strategies is critical because the fusion moieties might require specific conditions for correct folding or post-translational modifications. In the case of nanobody production, the trend is towards simpler but reliable (bacterial) methods that can substitute for more cumbersome processes requiring the use of eukaryotic systems. The use of these will not disappear, but will be restricted to those cases in which the final immunoconstructs must have features that cannot be obtained in prokaryotic cells. At the same time, bacterial expression has evolved from the conventional procedure which considered exclusively the nanobody and nanobody-fusion accumulation in the periplasm. Several reports show the advantage of cytoplasmic expression, surface-display and secretion for at least some applications. Finally, there is an increasing interest to use as a model the short nanobody sequence for the development of in silico methodologies aimed at optimizing the yields, stability and affinity of recombinant antibodies.\n \n', 'corpus_id': 215731267, 'score': 0}, {'doc_id': '214713659', 'title': 'Structural analysis of SARS-CoV-2 genome and predictions of the human interactome', 'abstract': 'Specific elements of viral genomes regulate interactions within host cells. Here, we calculated the secondary structure content of >2000 coronaviruses and computed >100000 human protein interactions with severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2). The genomic regions display different degrees of conservation. SARS-CoV-2 domain encompassing nucleotides 22500 – 23000 is conserved both at the sequence and structural level. The regions upstream and downstream, however, vary significantly. This part codes for the Spike S protein that interacts with the human receptor angiotensin-converting enzyme 2 (ACE2). Thus, variability of Spike S may be connected to different levels of viral entry in human cells within the population. Our predictions indicate that the 5’ end of SARS-CoV-2 is highly structured and interacts with several human proteins. The binding proteins are involved in viral RNA processing such as double-stranded RNA specific editases and ATP-dependent RNA-helicases and have strong propensity to form stress granules and phase-separated complexes. We propose that these proteins, also implicated in viral infections such as HIV, are selectively recruited by SARS-CoV-2 genome to alter transcriptional and post-transcriptional regulation of host cells and to promote viral replication.', 'corpus_id': 214713659, 'score': 0}, {'doc_id': '25048621', 'title': 'Phage display of engineered binding proteins.', 'abstract': 'In current purification processes optimization of the capture step generally has a large impact on cost reduction. At present, valuable biomolecules are often produced in relatively low concentrations and, consequently, the eventual selective separation from complex mixtures can be rather inefficient. A separation technology based on a very selective high-affinity binding may overcome these problems. Proteins in their natural environment manifest functionality by interacting specifically and often with relatively high affinity with other molecules, such as substrates, inhibitors, activators, or other proteins. At present, antibodies are the most commonly used binding proteins in numerous applications. However, antibodies do have limitations, such as high production costs, low stability, and a complex patent landscape. A novel approach is therefore to use non-immunoglobulin engineered binding proteins in affinity purification. In order to obtain engineered binders with a desired specificity, a large mutant library of the new to-be-developed binding protein has to be created and screened for potential binders. A powerful technique to screen and select for proteins with desired properties from a large pool of variants is phage display. Here, we indicate several criteria for potential binding protein scaffolds and explain the principle of M13 phage display. In addition, we describe experimental protocols for the initial steps in setting up a M13 phage display system based on the pComb3X vector, including construction of the phagemid vector, production of phages displaying the protein of interest, and confirmation of display on the M13 phage.', 'corpus_id': 25048621, 'score': 1}, {'doc_id': '20635351', 'title': 'Combining structure-based design with phage display to create new Cys(2)His(2) zinc finger dimers.', 'abstract': 'BACKGROUND\nSeveral strategies have been reported for the design and selection of novel DNA-binding proteins. Most of these studies have used Cys(2)His(2) zinc finger proteins as a framework, and have focused on constructs that bind DNA in a manner similar to Zif268, with neighboring fingers connected by a canonical (Krüppel-type) linker. This linker does not seem ideal for larger constructs because only modest improvements in affinity are observed when more than three fingers are connected in this manner. Two strategies have been described that allow the productive assembly of more than three canonically linked fingers on a DNA site: connecting sets of fingers using linkers (covalent), or assembling sets of fingers using dimerization domains (non-covalent).\n\n\nRESULTS\nUsing a combination of structure-based design and phage display, we have developed a new dimerization system for Cys(2)His(2) zinc fingers that allows the assembly of more than three fingers on a desired target site. Zinc finger constructs employing this new dimerization system have high affinity and good specificity for their target sites both in vitro and in vivo. Constructs that recognize an asymmetric binding site as heterodimers can be obtained through substitutions in the zinc finger and dimerization regions.\n\n\nCONCLUSIONS\nOur modular zinc finger dimerization system allows more than three Cys(2)His(2) zinc fingers to be productively assembled on a DNA-binding site. Dimerization may offer certain advantages over covalent linkage for the recognition of large DNA sequences. Our results also illustrate the power of combining structure-based design with phage display in a strategy that assimilates the best features of each method.', 'corpus_id': 20635351, 'score': 1}]
76	{'doc_id': '3006125', 'title': 'A type theory for memory allocation and data layout', 'abstract': 'Ordered type theory is an extension of linear type theory in which variables in the context may be neither dropped nor re-ordered. This restriction gives rise to a natural notion of adjacency. We show that a language based on ordered types can use this property to give an exact account of the layout of data in memory. The fuse constructor from ordered logic describes adjacency of values in memory, and the mobility modal describes pointers into the heap. We choose a particular allocation model based on a common implementation scheme for copying garbage collection and show how this permits us to separate out the allocation and initialization of memory locations in such a way as to account for optimizations such as the coalescing of multiple calls to the allocator.', 'corpus_id': 3006125}	15429	"[{'doc_id': '201668763', 'title': 'Kindly bent to free us', 'abstract': 'Systems programming often requires the manipulation of resources like file handles, network connections, or dynamically allocated memory. Programmers need to follow certain protocols to handle these resources correctly. Violating these protocols causes bugs ranging from type mismatches over data races to use-after-free errors and memory leaks. These bugs often lead to security vulnerabilities. While statically typed programming languages guarantee type soundness and memory safety by design, most of them do not address issues arising from improper handling of resources. An important step towards handling resources is the adoption of linear and affine types that enforce single-threaded resource usage. However, the few languages supporting such types require heavy type annotations. We present Affe, an extension of ML that manages linearity and affinity properties using kinds and constrained types. In addition Affe supports the exclusive and shared borrowing of affine resources, inspired by features of Rust. Moreover, Affe retains the defining features of the ML family: it is an impure, strict, functional expression language with complete principal type inference and type abstraction. does not require any linearity annotations in expressions and supports common functional programming idioms.', 'corpus_id': 201668763, 'score': 1}, {'doc_id': '199010343', 'title': 'Efficient Deconstruction with Typed Pointer Reversal (abstract)', 'abstract': 'The resource-management model of C++ and Rust relies on compiler-generated destructors called predictably and reliably. In current implementations, the generated destructor consumes stack space proportionally to the depth of the structure it destructs. We describe a way to derive destructors for algebraic data types that consume a constant amount of stack and heap. We discuss applicability to C++ and Rust, and also some implication for anyone wishing to extend an ML-style language with first-class resources.', 'corpus_id': 199010343, 'score': 1}, {'doc_id': '3750399', 'title': 'Resource Polymorphism', 'abstract': 'We present a resource-management model for ML-style programming languages, designed to be compatible with the OCaml philosophy and runtime model. This is a proposal to extend the OCaml language with destructors, move semantics, and resource polymorphism, to improve its safety, efficiency, interoperability, and expressiveness. It builds on the ownership-and-borrowing models of systems programming languages (Cyclone, C++11, Rust) and on linear types in functional programming (Linear Lisp, Clean, Alms). It continues a synthesis of resources from systems programming and resources in linear logic initiated by Baker. It is a combination of many known and some new ideas. On the novel side, it highlights the good mathematical structure of Stroustrup\'s ""Resource acquisition is initialisation"" (RAII) idiom for resource management based on destructors, a notion sometimes confused with finalizers, and builds on it a notion of resource polymorphism, inspired by polarisation in proof theory, that mixes C++\'s RAII and a tracing garbage collector (GC). The proposal targets a new spot in the design space, with an automatic and predictable resource-management model, at the same time based on lightweight and expressive language abstractions. It is backwards-compatible: current code is expected to run with the same performance, the new abstractions fully combine with the current ones, and it supports a resource-polymorphic extension of libraries. It does so with only a few additions to the runtime, and it integrates with the current GC implementation. It is also compatible with the upcoming multicore extension, and suggests that the Rust model for eliminating data-races applies. Interesting questions arise for a safe and practical type system, many of which have already been thoroughly investigated in the languages and prototypes Cyclone, Rust, and Alms.', 'corpus_id': 3750399, 'score': 1}, {'doc_id': '232177318', 'title': 'Functional Reactive Programming with nothing but Promises Implementing Push/Pull FRP using JavaScript Promises', 'abstract': 'Functional Reactive Programming (FRP) is a model of reactive programming defined by having a well-defined semantics given by time-indexed values. Promises are one-shot communication channels which allow asynchronous programs to be written in a synchronous style. In this paper, we show how timed promise lists, a timestamped linked list structure using promises rather than pointers, can be used to implement FRP. This idea originated with Elliott’s Push/Pull FRP, and we show that it can be expressed idiomatically in a strict functional language with promises, JavaScript. We identify a potential space leak with JavaScript’s built-in promises and propose an alternative implementation that avoids the leak.', 'corpus_id': 232177318, 'score': 0}, {'doc_id': '231986346', 'title': 'Crowbar: Behavioral Symbolic Execution for Deductive Verification of Active Objects', 'abstract': 'We present the Crowbar tool, a deductive verification system for the ABS language. ABS models distributed systems with the Active Object concurrency model. Crowbar implements behavioral symbolic execution: each method is symbolically executed, but specification and prior static analyses influence the shape of the symbolic execution tree. User interaction is realized through guided counterexamples, which present failed proof branches in terms of the input program. Crowbar has a clear interface to implement new specification languages and verification calculi in the Behavioral Program Logic and has been applied for the biggest verification case study of Active Objects.', 'corpus_id': 231986346, 'score': 0}, {'doc_id': '232478760', 'title': 'Idris 2: Quantitative Type Theory in Practice', 'abstract': 'Dependent types allow us to express precisely what a function is intended to do. Recent work on Quantitative Type Theory (QTT) extends dependent type systems with linearity, also allowing precision in expressing when a function can run. This is promising, because it suggests the ability to design and reason about resource usage protocols, such as we might find in distributed and concurrent programming, where the state of a communication channel changes throughout program execution. As yet, however, there has not been a full-scale programming language with which to experiment with these ideas. Idris 2 is a new version of the dependently typed language Idris, with a new core language based on QTT, supporting linear and dependent types. In this paper, we introduce Idris 2, and describe how QTT has influenced its design. We give examples of the benefits of QTT in practice including: expressing which data is erased at run time, at the type level; and, resource tracking in the type system leading to type-safe concurrent programming with session types. 2012 ACM Subject Classification Software and its engineering → Functional languages', 'corpus_id': 232478760, 'score': 0}, {'doc_id': '53093884', 'title': 'To Memory Safety through Proofs', 'abstract': 'We present a type system capable of guaranteeing the memory safety of programs that may involve (sophisticated) pointer manipulation such as pointer arithmetic. With its root in a recently developed framework Applied Type System (ATS), the type system imposes a level of abstraction on program states through a novel notion of recursive stateful views and then relies on a form of linear logic to reason about such stateful views. We consider the design and then the formalization of the type system to constitute the primary contribution of the paper. In addition, we also mention a running implementation of the type system and then give some examples in support of the practicality of programming with recursive stateful views.', 'corpus_id': 53093884, 'score': 1}, {'doc_id': '14382250', 'title': 'Safe Programming with Pointers Through Stateful Views', 'abstract': 'The need for direct memory manipulation through pointers is essential in many applications. However, it is also commonly understood that the use (or probably misuse) of pointers is often a rich source of program errors. Therefore, approaches that can effectively enforce safe use of pointers in programming are highly sought after. ATS is a programming language with a type system rooted in a recently developed framework Applied Type System, and a novel and desirable feature in ATS lies in its support for safe programming with pointers through a novel notion of stateful views. In particular, even pointer arithmetic is allowed in ATS and guaranteed to be safe by the type system of ATS. In this paper, we give an overview of this feature in ATS, presenting some interesting examples based on a prototype implementation of ATS to demonstrate the practicality of safe programming with pointer through stateful views.', 'corpus_id': 14382250, 'score': 1}, {'doc_id': '232068782', 'title': 'AwkwardForth: accelerating Uproot with an internal DSL', 'abstract': 'File formats for generic data structures, such as ROOT, Avro, and Parquet, pose a problem for deserialization: it must be fast, but its code depends on the type of the data structure, not known at compile-time. Just-in-time compilation can satisfy both constraints, but we propose a more portable solution: specialized virtual machines. AwkwardForth is a Forth-driven virtual machine for deserializing data into Awkward Arrays. As a language, it is not intended for humans to write, but it loosens the coupling between Uproot and Awkward Array. AwkwardForth programs for deserializing record-oriented formats (ROOT and Avro) are about as fast as C++ ROOT and 10–80× faster than fastavro. Columnar formats (simple TTrees, RNTuple, and Parquet) only require specialization to interpret metadata and are therefore faster with precompiled code.', 'corpus_id': 232068782, 'score': 0}, {'doc_id': '232221083', 'title': 'Retrofitting effect handlers onto OCaml', 'abstract': 'Effect handlers have been gathering momentum as a mechanism for modular programming with user-defined effects. Effect handlers allow for non-local control flow mechanisms such as generators, async/await, lightweight threads and coroutines to be composably expressed. We present a design and evaluate a full-fledged efficient implementation of effect handlers for OCaml, an industrial-strength multi-paradigm programming language. Our implementation strives to maintain the backwards compatibility and performance profile of existing OCaml code. Retrofitting effect handlers onto OCaml is challenging since OCaml does not currently have any non-local control flow mechanisms other than exceptions. Our implementation of effect handlers for OCaml: (i) imposes a mean 1% overhead on a comprehensive macro benchmark suite that does not use effect handlers; (ii) remains compatible with program analysis tools that inspect the stack; and (iii) is efficient for new code that makes use of effect handlers.', 'corpus_id': 232221083, 'score': 0}]"
77	{'doc_id': '207908417', 'title': 'Mitochondrial Eve & Y-Chromosome Adam', 'abstract': 'Two DNA molecules in the cells of humans and other mammals have very distinct patterns of transmission. One of these is the DNA molecule present in the , mitochondrion an intra-cellular organelle responsible for transformation of energy to fuel cellular processes. Because mitochondria are provisioned by a female to the egg during its production, and not normally contributed by a fertilizing sperm, the mitochondrial DNA (or mtDNA) of each individual is inherited maternally. Your mtDNA in each of your cells was inherited from your mother, which she inherited from her mother, and so on. Inheritance of mtDNA traces the series of mothers in your single, continuous maternal line, or matriline. Modern assisted reproductive technologies are resulting in . The mtDNA of humans is a novel transmission patterns for mtDNA circular DNA molecule containing about 16,500 pairs of bases. Each difference in the sequence of bases between the mtDNA molecules of any two individuals is a consequence of a mutation in one descendant lineage since the two individuals last shared a common maternal ancestor. Mutations in the mtDNA base sequence shared among individuals provides evidence of common ancestry.', 'corpus_id': 207908417}	11059	"[{'doc_id': '195566160', 'title': 'Niche Construction Theory and Human Biocultural Evolution', 'abstract': 'Biologists and anthropologists have extensively documented how many animals—human and non-human—modify their immediate surroundings, some subtly, others extensively. This trend is carried to its extreme in Homo sapiens to the point where many of us today live in the almost entirely constructed niches of the built urban environment. Proponents of niche construction theory (NCT) argue that classical evolutionary theory does not account satisfactorily for organisms’ active niche modification that impacts selective parameters for themselves and/or also other organisms. Complementing evolutionary theory focused on genetic change alone as well as gene-culture co-evolutionary models, NCT aims at integrating ecology, anthropology and evolution by a greater awareness to ecological inheritances. The key to the niche construction approach therefore is the inclusion of organism-induced environmental modification bequeathed from the modifying generation to its offspring. Critically then, it is either selection that leads to changes over time in a given population, or individuals induce changes in their environment in order to offset or channel further selection. While there is intense debate about the merits of NCT within the biosciences, archaeology is in a situation to make an important contribution here: it allows for a greater role of organismal agency in the evolutionary process, and many objects as well as features in the archaeological record speak directly about lasting modifications of the environment. In this chapter, I aim to provide a richly but by no means exhaustively referenced review of the early emergence and development of NCT as well as the controversies surrounding it. I illustrate the specific application of NCT in anthropology and archaeology by focusing on selected behaviours of premodern and modern humans and their co-evolutionary impacts such as the use of fire, changing human-animal and human-plant relations and landscape modifications. I also highlight more subtle forms of niche construction that may have left traces in the cognitive make-up of past and indeed present populations and link NCT with the notion of the Anthropocene. NCT remains a contested body of theory. In concluding, I point to potential archaeological research avenues that can make significant contributions to this emerging field.', 'corpus_id': 195566160, 'score': 1}, {'doc_id': '226955916', 'title': 'An energy-based natural selection model.', 'abstract': ""Genetic information and environmental factors determine the path of an individuals life and therefore, the evolution of its entire species. We have succeeded in proposing and studying a model that captures this idea. In our model, a renewable resource extended throughout the environment provides the energy necessary to sustain life, including movement and reproduction. Since the resource doesn't regrow immediately, it generates competition between individuals and therefore provides a natural selection pressure from which evolution of the genetic traits is observed. As a result of this, several phenomena characteristic of living systems emerge from this model without having to introduce them explicitly. These include speciation and punctuated equilibrium, competitive exclusion, and altruistic behaviour from selfish rules."", 'corpus_id': 226955916, 'score': 1}, {'doc_id': '226237135', 'title': 'Greetings from a Triparental Planet', 'abstract': 'In this work of speculative science, scientists from a distant star system explain the emergence and consequences of triparentalism, when three individuals are required for sexual reproduction, which is the standard form of mating on their home world. The report details the evolution of their reproductive system--that is, the conditions under which triparentalism and three self-avoiding mating types emerged as advantageous strategies for sexual reproduction. It also provides an overview of the biological consequences of triparental reproduction with three mating types, including the genetic mechanisms of triparental reproduction, asymmetries between the three mating types, and infection dynamics arising from their different mode of sexual reproduction. The report finishes by discussing how central aspects of their society, such as short-lasting unions among individuals and the rise of a monoculture, might have arisen as a result of their triparental system.', 'corpus_id': 226237135, 'score': 0}, {'doc_id': '227151305', 'title': 'Open Quasispecies Systems: New Approach to Evolutionary Adaptation.', 'abstract': ""Consider a mathematical model of evolutionary adaptation of fitness landscape and mutation matrix as a reaction to population changes. As a basis, we use an open quasispecies model, which is modified to include explicit death flow. We assume that evolutionary parameters of mutation and selection processes vary in a way to maximize the mean fitness of the system. From this standpoint, Fisher's theorem of natural selection is being rethought and discussed. Another assumption is that system dynamics has two significant timescales. According to our central hypothesis, major evolutionary transitions happen in the steady-state of the corresponding dynamical system, so the evolutionary time is much slower than the one of internal dynamics. For the specific cases of quasispecies systems, we show how our premises form the fitness landscape adaptation process."", 'corpus_id': 227151305, 'score': 1}, {'doc_id': '221971043', 'title': 'The role of behavioural plasticity in finite vs infinite populations', 'abstract': ""Evolutionary game theory has proven to be an elegant framework providing many fruitful insights in population dynamics and human behaviour. Here, we focus on the aspect of behavioural plasticity and its effect on the evolution of populations. We consider games with only two strategies in both well-mixed infinite and finite populations settings. We assume that individuals might exhibit behavioural plasticity referred to as incompetence of players. We study the effect of such heterogeneity on the outcome of local interactions and, ultimately, on global competition. For instance, a strategy that was dominated before can become desirable from the selection perspective when behavioural plasticity is taken into account. Furthermore, it can ease conditions for a successful fixation in infinite populations' invasions. We demonstrate our findings on the examples of Prisoners' Dilemma and Snowdrift game, where we define conditions under which cooperation can be promoted."", 'corpus_id': 221971043, 'score': 1}, {'doc_id': '221858260', 'title': 'Bats and birds as viral reservoirs: A physiological and ecological perspective', 'abstract': '\n The birds (class Aves) and bats (order Chiroptera, class Mammalia) are well known natural reservoirs of a diverse range of viruses, including some zoonoses. The only extant volant vertebrates, bats and birds have undergone dramatic adaptive radiations that have allowed them to occupy diverse ecological niches and colonize most of the planet. However, few studies have compared the physiology and ecology of these ecologically, and medically, important taxa. Here, we review convergent traits in the physiology, immunology, flight-related ecology of birds and bats that might enable these taxa to act as viral reservoirs and asymptomatic carriers. Many species of birds and bats are well adapted to urban environments and may host more zoonotic pathogens than species that do not colonize anthropogenic habitats. These convergent traits in birds and bats and their ecological interactions with domestic animals and humans increase the potential risk of viral spillover transmission and facilitate the emergence of novel viruses that most likely sources of zoonoses with the potential to cause global pandemics.\n', 'corpus_id': 221858260, 'score': 0}, {'doc_id': '221635220', 'title': 'Unraveling molecular mechanisms of immunity and cancer-resistance using the genomes of the Neotropical bats Artibeus jamaicensis and Pteronotus mesoamericanus', 'abstract': 'Abstract Bats are exceptional among mammals for harbouring diverse pathogens and for their robust immune systems. In addition, bats are unusually long-lived and show low rates of cancer. Contiguous and complete reference genomes are needed to determine the genetic basis of these adaptations and establish bats as models for research into mammalian health. Here we sequenced and analysed the genomes of the Jamaican fruit bat (Artibeus jamaicensis) and the Mesoamerican mustached bat (Pteronotus mesoamericanus). We sequenced these two species using a mix of Illumina and Oxford Nanopore Technologies (ONT), assembling draft genomes with some of the highest contig N50s (28-29Mb) of bat genomes to date. Work is in progress to increase the base-level accuracies of these genomes. We conducted gene annotation and identified a set of 10,928 orthologs from bats and mammalian outgroups including humans, rodents, horses, pigs, and dogs. To detect positively selected genes as well as lineage-specific gene gains and losses, we carried out comprehensive branch-site likelihood ratio tests and gene family size analyses. Our analysis found signatures of rapid evolution in the innate immune response genes of bats, and evidence of past infections with diverse viral clades in Artibeus jamaicensis and Pteronotus mesoamericanus. We additionally found evidence of positive selection of tumor suppressors, which may play a role in the low cancer rates, in the most recent common ancestor of bats. These new genomic resources enable insights into the extraordinary adaptations of bats, with implications for mammalian evolutionary studies and public health.', 'corpus_id': 221635220, 'score': 0}, {'doc_id': '227056562', 'title': 'Genome recoding strategies to improve cellular properties: mechanisms and advances', 'abstract': 'The genetic code, once believed to be universal and immutable, is now known to contain many variations and is not quite universal. The basis for genome recoding strategy is genetic code variation that can be harnessed to improve cellular properties. Thus, genome recoding is a promising strategy for the enhancement of genome flexibility, allowing for novel functions that are not commonly documented in the organism in its natural environment. Here, the basic concept of genetic code and associated mechanisms for the generation of genetic codon variants, including biased codon usage, codon reassignment, and ambiguous decoding, are extensively discussed. Knowledge of the concept of natural genetic code expansion is also detailed. The generation of recoded organisms and associated mechanisms with basic targeting components, including aminoacyl-tRNA synthetase–tRNA pairs, elongation factor EF-Tu and ribosomes, are highlighted for a comprehensive understanding of this concept. The research associated with the generation of diverse recoded organisms is also discussed. The success of genome recoding in diverse multicellular organisms offers a platform for expanding protein chemistry at the biochemical level with non-canonical amino acids, genetically isolating the synthetic organisms from the natural ones, and fighting viruses, including SARS-CoV2, through the creation of attenuated viruses. In conclusion, genome recoding can offer diverse applications for improving cellular properties in the genome-recoded organisms.', 'corpus_id': 227056562, 'score': 0}, {'doc_id': '225080800', 'title': 'Closing the circle of reverse genetics in reproductive medicine', 'abstract': 'Linking genotype to phenotype is the singular charge of genetics. And some of the most foundational principles in contemporary human genetics took their origins within the annals of reproductivemedicine. But since staking claim in the 1960s and 1970s, momentum in human genetics on the broader scale it now commands has been sustained in recent decades by the exploitation of experimentally tractable animal models lending themselves to observational and functional studies of spontaneous, induced, or engineered mutations whose outcomes could be defined in rigorous terms. While intact animal and cultured cell or organoid models have retained a central role in linking genotypes to phenotypes, the field of human genetics is selfpowering forward at an incredible pace thanks in large measure to the remarkable technological advances of the past two decades. Fittingly, we have arrived at a place where a human phenotype can be mapped to a particular gene offering an opportunity to elaborate upon function for gene products perhaps never suspected of being involved in a particular process or behavior at the molecular, cellular, or tissue level. Human ARTs has contributed to transforming the scenario outlined above into a platform of discovery for reproductive biology. A case in point has materialized over the past few years regarding a specific member of the super family of tubulin genes, TUBB8. The first signs or phenotypes prompting inquiry had to do with clusters of patients who despite exhibiting normal responses to ovarian stimulation, yielded immature oocytes at first associated with meiotic arrest at the germinal vesicle stage [1]. In some ways, the ground work set by careful observation of seemingly penetrant phenotypes in the embryology lab heightened awareness of these and other unusual phenotypes in pre or post-fertilization outcomes giving at least the suggestion of a quasi-penetrant phenotype in the face of an otherwise “normal” appearing IVF cycle. But mapping of the original defect to the TUBB8 gene, upon further investigation, revealed in fact that a family of mutations were involved that phenotypically demonstrated a spectrum of disorders spanning from the failed resumption of meiosis after ovulation triggering to failure to fertilize or cleave [1, 2]. In fact, the range of disturbances observed in the oocytes of patients identified with various mutations in the TUBB8 gene is striking and will command much future research to understand basic processes at play during these initial and critical stages of human development [3]. Recently, in the pages of JARG, the work of Zhao and colleagues not only extended the analysis of the TUBB8 gene products in human oocytes but provided an elegant example of how to close the circle in reproductive genetics when such a robust and glaring phenotype shows up in the clinical embryology laboratory [4] -what some might refer to as the “bedside-to-bench” approach. And again in this issue we see the paper by Liu and collaborators extend the range of effects the TUBB8 gene seems to mediate in the human (Identification and rescue of a novel TUBB8 mutation that causes the first mitotic division defects and infertility https://doi.org/10.1007/ s10815-020-01945). In the former case, having identified a not-so-surprising phenotype of large polar bodies, they asked if the mutation they recognized in patients could be introduced into a mouse and if so, would it lead to a different or similar phenotype relative to what was originally recognized in their patient population. In this case, a comparable phenotype (phenocopy) resulted and in closing the circle, the stage was set for what we all hope will be an answer to some of the vexing basic questions being faced by the reproductive medicine community. Among these, few would argue that genetic plasticity at the dawning of human development and our preoccupation with PGT-A reflects a nexus evidencing no signs of slowing down here in this strange year of 2020. In fact, 2020 got off to an auspicious start with the publication of the paper fromMunne and associates posing a most fundamental of questions: How would the PGT-A results of in vivo produced human embryos compare to those produced by IVF/ICSI? [5]. More to the point of that puzzling plasticity noted above, since we are working in a medical subspeciality where the pressure to * David F. Albertini eicjarg@gmail.com', 'corpus_id': 225080800, 'score': 0}, {'doc_id': '222179687', 'title': 'The evolution of group differences in changing environments', 'abstract': 'The selection pressures that have shaped the evolution of complex traits in humans remain largely unknown, and in some contexts highly contentious, perhaps above all where they concern mean trait differences among groups. To date, the discussion has focused on whether such group differences have any genetic basis, and if so, whether they are without fitness consequences and arose via random genetic drift, or whether they were driven by selection for different trait optima in different environments. Here, we highlight a plausible alternative: that many complex traits evolve under stabilizing selection in the face of shifting environmental effects. Under this scenario, there will be rapid evolution at the loci that contribute to trait variation, even when the trait optimum remains the same. These considerations underscore the strong assumptions about environmental effects that are required in ascribing trait differences among groups to genetic differences.', 'corpus_id': 222179687, 'score': 1}]"
78	{'doc_id': '208437380', 'title': 'Next Generation Proteomics and Drug Sensitivity Resistance Testing Allow for the Identification of Distinct Sub-clones of Multiple Myeloma Patients', 'abstract': None, 'corpus_id': 208437380}	15043	[{'doc_id': '231791227', 'title': 'Proteomic investigation reveals dominant alterations of neutrophil degranulation and mRNA translation pathways in patients with COVID-19', 'abstract': '\n The altered molecular proteins and pathways in response to COVID-19 infection are still unclear. Here, we performed a comprehensive proteomics-based investigation of nasopharyngeal swab samples from COVID-19 patients to study the host response by employing simple extraction strategies. Few of the host proteins such as Interleukin-6, L-lactate dehydrogenase, C-reactive protein, Ferritin and Aspartate aminotransferase were found to be up-regulated only in COVID-19 positive patients using targeted Multiple Reaction Monitoring studies. The most important pathways identified by enrichment analysis were neutrophil degranulation, interleukin-12 signaling pathways and mRNA translation of proteins thus providing the detailed investigation of host response in COVID-19 infection. Thus, we conclude that mass spectrometry-detected host proteins have a potential for disease severity progression; however, suitable validation strategies should be deployed for the clinical translation. Furthermore, the in-silico docking of host proteins involved in the interleukin-12 signaling pathway might aid in COVID-19 therapeutic interventions.\n', 'corpus_id': 231791227, 'score': 0}, {'doc_id': '231789432', 'title': 'Long non‐coding RNAs: Promising new targets in pulmonary fibrosis', 'abstract': 'Pulmonary fibrosis is characterized by progressive and irreversible scarring in the lungs with poor prognosis and treatment. It is caused by various factors, including environmental and occupational exposures, and some rheumatic immune diseases. Even the rapid global spread of the COVID‐19 pandemic can also cause pulmonary fibrosis with a high probability. Functions attributed to long non‐coding RNAs (lncRNAs) make them highly attractive diagnostic and therapeutic targets in fibroproliferative diseases. Therefore, an understanding of the specific mechanisms by which lncRNAs regulate pulmonary fibrotic pathogenesis is urgently needed to identify new possibilities for therapy. In this review, we focus on the molecular mechanisms and implications of lncRNAs targeted protein‐coding and non‐coding genes during pulmonary fibrogenesis, and systematically analyze the communication of lncRNAs with various types of RNAs, including microRNA, circular RNA and mRNA. Finally, we propose the potential approach of lncRNA‐based diagnosis and therapy for pulmonary fibrosis. We hope that understanding these interactions between protein‐coding and non‐coding genes will contribute to the development of lncRNA‐based clinical applications for pulmonary fibrosis.', 'corpus_id': 231789432, 'score': 0}, {'doc_id': '213739140', 'title': 'Proteomics-inspired precision medicine for treating and understanding multiple myeloma', 'abstract': 'ABSTRACT Introduction: Remarkable progress in molecular characterization methods has led to significant improvements in how we manage multiple myeloma (MM). The introduction of novel therapies has led to significant improvements in overall survival over the past 10 years. However, MM remains incurable and treatment choice is largely based on outdated risk-adaptive strategies that do not factor in improved treatment outcomes in the context of modern therapies. Areas covered: This review discusses current risk-adaptive strategies in MM and the clinical application of proteomics in the monitoring of treatment response, disease progression, and minimal residual disease (MRD). We also discuss promising biomarkers of disease progression, treatment response, and chemoresistance. Finally, we will discuss an immunomics-based approach to monoclonal antibody (mAb), vaccine, and CAR-T cell development. Expert opinion: It is an exciting era in oncology with basic scientific knowledge translating in novel therapeutic approaches to improve patient outcomes. With the advent of effective immunotherapies and targeted therapies, it has become crucial to identify biomarkers to aid in the stratification of patients based on anticipated sensitivity to chemotherapy. As a paradigm of diseases highly dependent on protein homeostasis, multiple myeloma provides the perfect opportunity to investigate the use of proteomics to aid in precision medicine.', 'corpus_id': 213739140, 'score': 1}, {'doc_id': '231856025', 'title': 'ACE2 Is a Prognostic Biomarker and Associated with Immune Infiltration in Kidney Renal Clear Cell Carcinoma: Implication for COVID-19', 'abstract': 'Background KIRC is one of the most common cancers with a poor prognosis. ACE2 was involved in tumor angiogenesis and progression in many malignancies. The role of ACE2 in KIRC is still ambiguous. Methods Various bioinformatics analysis tools were investigated to evaluate the prognostic value of ACE2 and its association with immune infiltration in KIRC. Results ACE2 was shown to be downregulated in KIRC at the mRNA and protein level. Low expression of ACE2 protein in KIRC patients was observed in subgroup analyses based on gender, age, weight, tumor grade, and cancer stage. Upregulation of ACE2 in KIRC was associated with a favorable prognosis. ACE2 mRNA expression showed a positive correlation with the abundance of immune cells (B cells, CD8+ T cells, macrophages, neutrophils, and dendritic cells) and the level of immune markers of different immune cells in KIRC. ACE2 expression could affect, in part, the immune infiltration and the advanced cancer stage. Moreover, enrichment analysis revealed that ACE2 in KIRC were mainly involved in translation factor activity, immunoglobulin binding, metabolic pathways, transcriptional misregulation in cancerous cells, cell cycle, and ribosomal activity. Several ACE2-associated kinases, miRNA, and transcription factor targets in KIRC were also identified. Conclusion ACE2 was downregulated in KIRC and served as a prognostic biomarker. It was also shown to be associated with immune infiltration.', 'corpus_id': 231856025, 'score': 0}, {'doc_id': '232313372', 'title': 'Tumor Microenvironment Proteomics: Lessons From Multiple Myeloma', 'abstract': 'Although the “seed and soil” hypothesis was proposed by Stephen Paget at the end of the 19th century, where he postulated that tumor cells (seeds) need a propitious medium (soil) to be able to establish metastases, only recently the tumor microenvironment started to be more studied in the field of Oncology. Multiple myeloma (MM), a malignancy of plasma cells, can be considered one of the types of cancers where there is more evidence in the literature of the central role that the bone marrow (BM) microenvironment plays, contributing to proliferation, survival, migration, and drug resistance of tumor cells. Despite all advances in the therapeutic arsenal for MM treatment in the last years, the disease remains incurable. Thus, studies aiming a better understanding of the pathophysiology of the disease, as well as searching for new therapeutic targets are necessary and welcome. Therefore, the present study aimed to evaluate the protein expression profiling of mononuclear cells derived from BM of MM patients in comparison with these same cell types derived from healthy individuals, in order to fill this gap in MM treatment. Proteomic analysis was performed using the mass spectrometry technique and further analyses were done using bioinformatics tools, to identify dysregulated biological pathways and/or processes in the BM microenvironment of patients with MM as a result of the disease. Among the pathways identified in this study, we can highlight an upregulation of proteins related to protein biosynthesis, especially chaperone proteins, in patients with MM. Additionally, we also found an upregulation of several proteins involved in energy metabolism, which is one of the cancer hallmarks. Finally, with regard to the downregulated proteins, we can highlight mainly those involved in different pathways of the immune response, corroborating the data that has demonstrated that the immune system of MM is impaired and, therefore, the immunotherapies that have been studied recently for the treatment of the disease are extremely necessary in the search for a control and a cure for these patients who live with the disease.', 'corpus_id': 232313372, 'score': 1}, {'doc_id': '232342121', 'title': 'Proteomic Characterization, Biodistribution, and Functional Studies of Immune-Therapeutic Exosomes: Implications for Inflammatory Lung Diseases', 'abstract': 'Dendritic cell (DC)-derived exosomes (DC EXO), natural nanoparticles of endosomal origin, are under intense scrutiny in clinical trials for various inflammatory diseases. DC EXO are eobiotic, meaning they are well-tolerated by the host; moreover, they can be custom-tailored for immune-regulatory or -stimulatory functions, thus presenting attractive opportunities for immune therapy. Previously we documented the efficacy of immunoregulatory DCs EXO (regDCs EXO) as immunotherapy for inflammatory bone disease, in an in-vivo model. We showed a key role for encapsulated TGFβ1 in promoting a bone sparing immune response. However, the on- and off-target effects of these therapeutic regDC EXO and how target signaling in acceptor cells is activated is unclear. In the present report, therapeutic regDC EXO were analyzed by high throughput proteomics, with non-therapeutic EXO from immature DCs and mature DCs as controls, to identify shared and distinct proteins and potential off-target proteins, as corroborated by immunoblot. The predominant expression in regDC EXO of immunoregulatory proteins as well as proteins involved in trafficking from the circulation to peripheral tissues, cell surface binding, and transmigration, prompted us to investigate how these DC EXO are biodistributed to major organs after intravenous injection. Live animal imaging showed preferential accumulation of regDCs EXO in the lungs, followed by spleen and liver tissue. In addition, TGFβ1 in regDCs EXO sustained downstream signaling in acceptor DCs. Blocking experiments suggested that sustaining TGFβ1 signaling require initial interaction of regDCs EXO with TGFβ1R followed by internalization of regDCs EXO with TGFβ1-TGFβ1R complex. Finally, these regDCs EXO that contain immunoregulatory cargo and showed biodistribution to lungs could downregulate the main severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) target receptor, ACE2 on recipient lung parenchymal cells via TGFβ1 in-vitro. In conclusion, these results in mice may have important immunotherapeutic implications for lung inflammatory disorders.', 'corpus_id': 232342121, 'score': 0}, {'doc_id': '54165255', 'title': 'Current Understanding of the Potential of Proteomics and Metabolomics Approaches in Cancer Chemoresistance: A Focus on Multiple Myeloma.', 'abstract': 'Chemoresistance is one of the major hurdles in cancer treatment leading to recurrence of cancer and affects the overall survival of patients. Cancer chemoresistance can be associated with various phenomena including modulation of vital cellular pathways. Unrevealing these alterations could provide a better understanding of chemoresistance and assist in the identification of new targets to overcome it. Recent advances in the field of proteomics and metabolomics have substantially helped in the identification of potential targets for chemoresistance in various cancers. This review highlights the potential of proteomics and metabolomics research to explore the putative targets associated with cancer chemoresistance with a special focus on Multiple Myeloma (MM). MM is a type of hematological malignancy which constitutes about 13% of all blood cell cancers. The therapeutic advancements for MM have increased the median overall survival rate to over 3-fold in the last one and half decade. Although in recent times, significant improvements in the overall survival rate of MM are achieved, MM remains an incurable disease with unpredictable refractory mechanisms. In spite of therapeutic advances, chemoresistance thrives to be a major hurdle in the treatment of multiple myeloma which demands a better understanding of chemoresistance. In this review, we have attempted to highlight the potential applications of proteomics and metabolomics research in the understanding of chemoresistance in MM.', 'corpus_id': 54165255, 'score': 1}, {'doc_id': '231776957', 'title': 'Multi-omic Characterization of Human Tubular Epithelial Cell Response to Serum', 'abstract': 'Proteinuria, the spillage of serum proteins into the urine, is a feature of glomerulonephritides, podocyte disorders and diabetic nephropathy. However, the response of tubular epithelial cells to serum protein exposure has not been systematically characterized. Using transcriptomic profiling we studied serum-induced changes in primary human tubular epithelial cells cultured in 3D microphysiological devices. Serum proteins induced cellular proliferation, cytokine secretion and activated a coordinated stress response. We orthogonally confirmed our findings by comparing the transcriptomic and epigenomic landscapes of intact human kidney cortex and isolated tubular epithelial cells cultured in fetal bovine serum. Importantly, key transcriptomic programs in response to either type of serum exposure remained consistent, including comparisons to an established mouse model of kidney injury. This serum-induced transcriptional response was dominated by switching off of nuclear receptor-driven programs and activation of AP-1 and NF-κB signatures in the tubular epigenomic landscape. These features of active regulation were seen at canonical kidney injury genes (HAVCR1) and genes associated with COVID-19 (ACE2, IL6). Our data provide a reference map for dissecting the regulatory and transcriptional response of kidney tubular epithelial cells injury induced by serum.', 'corpus_id': 231776957, 'score': 0}, {'doc_id': '210122077', 'title': 'Concurrent lipidomics and proteomics on malignant plasma cells from multiple myeloma patients: Probing the lipid metabolome', 'abstract': 'Background Multiple myeloma (MM) is a hematological malignancy characterized by the clonal expansion of malignant plasma cells. Though durable remissions are possible, MM is considered incurable, with relapse occurring in almost all patients. There has been limited data reported on the lipid metabolism changes in plasma cells during MM progression. Here, we evaluated the feasibility of concurrent lipidomics and proteomics analyses from patient plasma cells, and report these data on a limited number of patient samples, demonstrating the feasibility of the method, and establishing hypotheses to be evaluated in the future. Methods Plasma cells were purified from fresh bone marrow aspirates using CD138 microbeads. Proteins and lipids were extracted using a bi-phasic solvent system with methanol, methyl tert-butyl ether, and water. Untargeted proteomics, untargeted and targeted lipidomics were performed on 7 patient samples using liquid chromatography-mass spectrometry. Two comparisons were conducted: high versus low risk; relapse versus newly diagnosed. Proteins and pathways enriched in the relapsed group was compared to a public transcriptomic dataset from Multiple Myeloma Research Consortium reference collection (n = 222) at gene and pathways level. Results From one million purified plasma cells, we were able to extract material and complete untargeted (~6000 and ~3600 features in positive and negative mode respectively) and targeted lipidomics (313 lipids), as well as untargeted proteomics analysis (~4100 reviewed proteins). Comparative analyses revealed limited differences between high and low risk groups (according to the standard clinical criteria), hence we focused on drawing comparisons between the relapsed and newly diagnosed patients. Untargeted and targeted lipidomics indicated significant down-regulation of phosphatidylcholines (PCs) in relapsed MM. Although there was limited overlap of the differential proteins/transcripts, 76 significantly enriched pathways in relapsed MM were common between proteomics and transcriptomics data. Further evaluation of transcriptomics data for lipid metabolism network revealed enriched correlation of PC, ceramide, cardiolipin, arachidonic acid and cholesterol metabolism pathways to be exclusively correlated among relapsed but not in newly-diagnosed patients. Conclusions This study establishes the feasibility and workflow to conduct integrated lipidomics and proteomics analyses on patient-derived plasma cells. Potential lipid metabolism changes associated with MM relapse warrant further investigation.', 'corpus_id': 210122077, 'score': 1}, {'doc_id': '218535291', 'title': 'Proteomics and functional study reveal marginal zone B and B1 cell specific protein as a candidate marker of multiple myeloma.', 'abstract': 'Multiple myeloma (MM) is a plasma cell‑associated cancer and accounts for 13%\xa0of all hematological malignancies, worldwide. MM still remains an incurable plasma cell malignancy with a poor prognosis due to a lack of suitable markers. Therefore, discovering novel markers and targets for diagnosis and therapeutics of MM is essential. The present study aims to identify markers associated with MM malignancy using patient‑derived MM mononuclear cells (MNCs). Label‑free quantitative proteomics analysis revealed a total of 192\xa0differentially regulated proteins, in which 79\xa0proteins were upregulated and 113\xa0proteins were found to be downregulated in MM MNCs as compared to non‑hematological malignant samples. The identified differentially expressed candidate proteins were analyzed using various bioinformatics tools, including Ingenuity Pathway Analysis (IPA), Protein Analysis THrough Evolutionary Relationships (PANTHER), Search Tool for the Retrieval of Interacting Genes/Proteins (STRING) and Database for Annotation, Visualization and Integrated Discovery (DAVID) to determine their biological context. Among the 192\xa0candidate proteins, marginal zone\xa0B\xa0and\xa0B1 cell specific protein (MZB1) was investigated in detail using the RPMI-8226 cell line model of MM. The functional studies revealed that higher expression of MZB1 is associated with promoting the progression of MM pathogenesis and could be established as a potential target for MM in the future.', 'corpus_id': 218535291, 'score': 1}]
79	{'doc_id': '234361256', 'title': 'Implementation of a deep learning model for automated classification of Aedes aegypti (Linnaeus) and Aedes albopictus (Skuse) in real time', 'abstract': 'Classification of Aedes aegypti (Linnaeus) and Aedes albopictus (Skuse) by humans remains challenging. We proposed a highly accessible method to develop a deep learning (DL) model and implement the model for mosquito image classification by using hardware that could regulate the development process. In particular, we constructed a dataset with 4120 images of Aedes mosquitoes that were older than 12 days old and had common morphological features that disappeared, and we illustrated how to set up supervised deep convolutional neural networks (DCNNs) with hyperparameter adjustment. The model application was first conducted by deploying the model externally in real time on three different generations of mosquitoes, and the accuracy was compared with human expert performance. Our results showed that both the learning rate and epochs significantly affected the accuracy, and the best-performing hyperparameters achieved an accuracy of more than 98% at classifying mosquitoes, which showed no significant difference from human-level performance. We demonstrated the feasibility of the method to construct a model with the DCNN when deployed externally on mosquitoes in real time.', 'corpus_id': 234361256}	18024	"[{'doc_id': '235355744', 'title': 'Chronic Disease Detection Using Deep Learning', 'abstract': ""Pneumonia is a life-threatening and contagious disease of the respiratory system caused mainly by bacteria, fungi, or viruses that infect a person's lungs with a load of fluid. Common technique are used to diagnose pneumonia is chest x-ray report and it needs a medical professional to evaluate the outcome of the X-ray report. Earlier troublesome methods of detecting pneumonia increase mortality due to unawareness with improper diagnosis and treatment. Computer technology is emerging these days, the development of an automatic system to identify pneumonia and treating the disease is now possible especially if the patient is in a distant area and medical services are limited. The main aim of this paper is to identify pneumonia automatically using X-rays images. This study will provide accurate detection of pneumonia and intends to apply deep learning methods to ease the problem. Convolutional Neural Network is highly optimized that perform the complex task of detecting diseases like pneumonia to assist medical experts in the diagnosis and possible treatment of the disease. This model is trained to classify between normal and pneumonia from the chest X-ray images. The first model achieves an accuracy of 95.64% and the second model achieves accuracy of 94.92%. Adam optimizer is used to optimize the model."", 'corpus_id': 235355744, 'score': 0}, {'doc_id': '234471362', 'title': 'Infrared light sensors permit rapid recording of wingbeat frequency and bioacoustic species identification of mosquitoes', 'abstract': 'Recognition and classification of mosquitoes is a critical component of vector-borne disease management. Vector surveillance, based on wingbeat frequency and other parameters, is becoming increasingly important in the development of automated identification systems, but inconsistent data quality and results frequently emerge from different techniques and data processing methods which have not been standardized on wingbeat collection of numerous species. We developed a simple method to detect and record mosquito wingbeat by multi-dimensional optical sensors and collected 21,825 wingbeat files from 29 North American mosquito species. In pairwise comparisons, wingbeat frequency of twenty six species overlapped with at least one other species. No significant differences were observed in wingbeat frequencies between and within individuals of Culex quinquefasciatus over time. This work demonstrates the potential utility of quantifying mosquito wingbeat frequency by infrared light sensors as a component of an automated mosquito identification system. Due to species overlap, wingbeat frequency will need to integrate with other parameters to accurately delineate species in support of efficient mosquito surveillance, an important component of disease intervention.', 'corpus_id': 234471362, 'score': 1}, {'doc_id': '235432850', 'title': 'CLASSIFYING FISH BY SPECIES USING CONVOLUTIONAL NEURAL NETWORKS', 'abstract': 'There is an increasing need for automated fish classification to help properly identify fish species and characteristics in a standardized, non-invasive, and cost-effective manner. Machine learning is a promising method to do this. In this paper, we present the results of a convolutional neural network (CNN) used to identify fish species across datasets. Our proposed model improves on a previously-built model by Rathi et al. (2018). The performance of our improved model is demonstrated with real-world data from a research organization called The Nature Conservancy.', 'corpus_id': 235432850, 'score': 0}, {'doc_id': '235468228', 'title': 'Identification of Plant-Leaf Diseases Using CNN and Transfer-Learning Approach', 'abstract': 'The timely identification and early prevention of crop diseases are essential for improving production. In this paper, deep convolutional-neural-network (CNN) models are implemented to identify and diagnose diseases in plants from their leaves, since CNNs have achieved impressive results in the field of machine vision. Standard CNN models require a large number of parameters and higher computation cost. In this paper, we replaced standard convolution with depth=separable convolution, which reduces the parameter number and computation cost. The implemented models were trained with an open dataset consisting of 14 different plant species, and 38 different categorical disease classes and healthy plant leaves. To evaluate the performance of the models, different parameters such as batch size, dropout, and different numbers of epochs were incorporated. The implemented models achieved a disease-classification accuracy rates of 98.42%, 99.11%, 97.02%, and 99.56% using InceptionV3, InceptionResNetV2, MobileNetV2, and EfficientNetB0, respectively, which were greater than that of traditional handcrafted-feature-based approaches. In comparison with other deep-learning models, the implemented model achieved better performance in terms of accuracy and it required less training time. Moreover, the MobileNetV2 architecture is compatible with mobile devices using the optimized parameter. The accuracy results in the identification of diseases showed that the deep CNN model is promising and can greatly impact the efficient identification of the diseases, and may have potential in the detection of diseases in real-time agricultural systems.', 'corpus_id': 235468228, 'score': 0}, {'doc_id': '235674168', 'title': 'Development and Validation of a Three-Dimensional Printed Multifunctional Trap for Surveillance of Mosquitoes.', 'abstract': 'An essential component of vector-borne disease monitoring programs is mosquito surveillance. Surveillance efforts employ various collection traps depending on mosquito species and targeted life-history stage, i.e., eggs, larvae, host-seeking, resting, or gravid adults. Surveillance activities often use commercial traps, sometimes modified to accept specific mosquito species attractants. The advent of widely available and affordable 3D printing technology allows the construction of novel trap designs and components. The study goal was to develop and assess a cost-effective, multipurpose, 6-volt mosquito trap integrating features of both host-seeking and gravid mosquito traps to collect undamaged live specimens: a multifunctional mosquito trap (MMT). We tested the MMT in comparison to commercial traps, targeting gravid Aedes albopictus, host-seeking Ae. albopictus, and total number of host-seeking mosquitos regardless of species. Field evaluations found the MMT performed as well as or better than comparable commercial traps. This project demonstrates an easy to construct, inexpensive, and versatile mosquito trap, potentially useful for surveying multiple mosquito species and other hematophagous insects by varying attractants into the MMT.', 'corpus_id': 235674168, 'score': 1}, {'doc_id': '233449745', 'title': 'Long- and short-range bimodal signals mediate mate location and recognition in yellow fever mosquitoes', 'abstract': 'As recently reported, light flashes of incident sunlight reflecting off the wings of in-flight dipterans serve as mate recognition signals. Mate location and mate selection behavior in the yellow fever mosquito, Aedes aegypti, take place in mating swarms but the mechanisms underlying swarm formation and long-range detection of females by males remain largely unexplored. Here we show that swarm formation and mate recognition are mediated, in part, by light flash signals and wingbeat sound signals that operate at long and short range, respectively. To test for range-dependent effects of these signals, we presented ‘mating swarms’ in form of two paired 8-LED assemblies that were fitted with micro-speakers and placed either well separated in a large space or side-by-side in a small space. In the large but not the small space, the LED assembly flashing light at the wingbeat frequency of females (665 Hz), and emitting their wingbeat sound (665 Hz), attracted and prompted 5.8-times more alightings by males than the LED assembly emitting constant light and wingbeat sound. In the small space, the LED assembly flashing light and emitting wingbeat sound induced 5.0-times more alightings by males than the LED assembly flashing light without wingbeat sound. Females responded to light flash signals of males, but males failed to respond to the synthetic female pheromone component ketoisophorone added to the bimodal complex of light and sound signals. The attractiveness of light flash signals to males increased with increasing numbers of signals but did not vary according to their wavelengths (UV or blue). As predicted by the sensory drive theory, light flashes had no signal function for crepuscular house mosquitoes, Culex pipiens.', 'corpus_id': 233449745, 'score': 1}, {'doc_id': '219165579', 'title': 'Real-Time Mosquito Species Identification using Deep Learning Techniques', 'abstract': 'According to the World Health Organization, diseases such as malaria and dengue account for almost one million deaths every year. Carrier mosquitoes for a particular disease remain exclusive to it. A majority of carrier mosquitoes spread the disease throughout a region by reproducing in it. With advancements in Machine Learning and Computer Vision technologies, the species of mosquitoes in a particular region can be easily and swiftly detected using recordings of their wing movements. The wingbeats of a particular mosquito species are unique, making this a reliable method to identify them. Once these solutions are deployed on mosquito traps, a particular region can be alerted if, for example, an Aedes Aegypti mosquito is found. This mosquito species is widely known to carry the Zika virus. The identification of such carrier species can also help in detecting the spread of mosquito-borne diseases in the surveyed region. In this paper, we go through various techniques that show promising results in the identification of mosquito species. The trained models can be deployed on constrained devices to make a cost-effective and efficient mosquito species identification system.', 'corpus_id': 219165579, 'score': 1}, {'doc_id': '235311444', 'title': 'Rice Leaf Disease Detection Via Deep Neural Networks With Transfer Learning For Early Identification', 'abstract': 'Rice is central to the lives of billions of people around the world. It is affected by different diseases at all growth periods of its cultivation. In plants, diseases are predominantly caused by biotic and abiotic components such as fungi, viroids, nematodes, bacteria, viruses, temperature, nutrient deficiencies and other environmental conditions. The important diseases of rice such as leaf blast, bacterial blight, and brown spot cause damage to rice can greatly reduce yield. Farmers globally deal with the problem of plant diseases diagnosis and for their appropriate treatment.Therefore, real-time and precision identification of rice leaf diseases is urgently needed. Recent developments in DL (Deep Learning) approaches have tremendously increased the capabilities of visual recognition systems through computer vision technologies. The most popular DL models leverage for computer vision problems are convolutional neural networks (CNNs) that has confirmed very successful in areas such as image categorization, object detection, image segmentation, etc. It has paved the way for automatic plant disease detection using plant images. In this research, the dataset is limited to train a deep convolutional neural network models such as VGG-16, ResNet50 and InceptionV3 in this case transfer learning came into picture. Transfer learning is a popular technique in deep learning where pre-trained models are re-purposed on a related problem. The main purpose of this study is to assess deep convolutional neural networks with transfer learning for the identification of different diseases in the rice plant leaf. The proposed CNNs were utilized with transfer learning technique to lessen the training time and enhance the functional capabilities of neural networks. The classification accuracies for the VGG-16, ResNet50 and InceptionV3 CNN models were 87%, 93%and 95% respectively. Index Terms Rice leaf diseases, Deep Learning, Convolutional Neural Network, Transfer learning, Finetuning.', 'corpus_id': 235311444, 'score': 0}, {'doc_id': '234787800', 'title': 'A Deep Convolutional Neural Network for Classification of Aedes Albopictus Mosquitoes', 'abstract': 'Monitoring the spread of disease-carrying mosquitoes is a first and necessary step to control severe diseases such as dengue, chikungunya, Zika or yellow fever. Previous citizen science projects have been able to obtain large image datasets with linked geo-tracking information. As the number of international collaborators grows, the manual annotation by expert entomologists of the large amount of data gathered by these users becomes too time demanding and unscalable, posing a strong need for automated classification of mosquito species from images. We introduce the application of two Deep Convolutional Neural Networks in a comparative study to automate this classification task. We use the transfer learning principle to train two state-of-the-art architectures on the data provided by the Mosquito Alert project, obtaining testing accuracy of 94%. In addition, we applied explainable models based on the Grad-CAM algorithm to visualise the most discriminant regions of the classified images, which coincide with the white band stripes located at the legs, abdomen, and thorax of mosquitoes of the Aedes albopictus species. The model allows us to further analyse the classification errors. Visual Grad-CAM models show that they are linked to poor acquisition conditions and strong image occlusions.', 'corpus_id': 234787800, 'score': 1}, {'doc_id': '233743819', 'title': 'Convolutional neural networks for the diagnosis and prognosis of the coronavirus disease pandemic', 'abstract': 'A neural network is one of the current trends in deep learning, which is increasingly gaining attention owing to its contribution in transforming the different facets of human life. It also paves a way to approach the current crisis caused by the coronavirus disease (COVID-19) from all scientific directions. Convolutional neural network (CNN), a type of neural network, is extensively applied in the medical field, and is particularly useful in the current COVID-19 pandemic. In this article, we present the application of CNNs for the diagnosis and prognosis of COVID-19 using X-ray and computed tomography (CT) images of COVID-19 patients. The CNN models discussed in this review were mainly developed for the detection, classification, and segmentation of COVID-19 images. The base models used for detection and classification were AlexNet, Visual Geometry Group Network with 16 layers, residual network, DensNet, GoogLeNet, MobileNet, Inception, and extreme Inception. U-Net and voxel-based broad learning network were used for segmentation. Even with limited datasets, these methods proved to be beneficial for efficiently identifying the occurrence of COVID-19. To further validate these observations, we conducted an experimental study using a simple CNN framework for the binary classification of COVID-19 CT images. We achieved an accuracy of 93% with an F1-score of 0.93. Thus, with the availability of improved medical image datasets, it is evident that CNNs are very useful for the efficient diagnosis and prognosis of COVID-19.', 'corpus_id': 233743819, 'score': 0}]"
80	{'doc_id': '221589370', 'title': 'In silico candidate variant and gene identification using inbred mouse strains', 'abstract': 'Mice are the most widely used animal model to study genotype to phenotype relationships. Inbred mice are genetically identical, which eliminates genetic heterogeneity and makes them particularly useful for genetic studies. Many different strains have been bred over decades and a vast amount of phenotypic data has been generated. In addition, recently whole genome sequencing-based genome-wide genotype data for many widely used inbred strains has been released. Here, we present an approach for in silico fine-mapping that uses genotypic data of 37 inbred mouse strains together with phenotypic data provided by the user to propose candidate variants and genes for the phenotype under study. Public genome-wide genotype data covering more than 74 million variant sites is queried efficiently in real-time to provide those variants that are compatible with the observed phenotype differences between strains. Variants can be filtered by molecular consequences and by corresponding molecular impact. Candidate gene lists can be generated from variant lists on the fly. Fine-mapping together with annotation or filtering of results is provided in a Bioconductor package called MouseFM. In order to characterize candidate variant lists under various settings, MouseFM was applied to two expression data sets across 20 inbred mouse strains, one from neutrophils and one from CD4+ T cells. Fine-mapping was assessed for about 10,000 genes, respectively, and identified candidate variants and haplotypes for many expression quantitative trait loci (eQTLs) reported previously based on these data. For albinism, MouseFM reports only one variant allele of moderate or high molecular impact that only albino mice share: a missense variant in the Tyr gene, reported previously to be causal for this phenotype. Performing in silico fine-mapping for interfrontal bone formation in mice using four strains with and five strains without interfrontal bone results in 12 genes. Of these, three are related to skull shaping abnormality. Finally performing fine-mapping for dystrophic cardiac calcification by comparing 9 strains showing the phenotype with 8 strains lacking it, we identify only one moderate impact variant in the known causal gene Abcc6. In summary, this illustrates the benefit of using MouseFM for candidate variant and gene identification.', 'corpus_id': 221589370}	15985	[{'doc_id': '208039382', 'title': 'Mouse protein coding diversity: What’s left to discover?', 'abstract': 'For over a century, mice have been used to model human disease, leading to many fundamental discoveries about mammalian biology and the development of new therapies. Mouse genetics research has been further catalysed by a plethora of genomic resources developed in the last 20 years, including the genome sequence of C57BL/6J and more recently the first draft reference genomes for 16 additional laboratory strains. Collectively, the comparison of these genomes highlights the extreme diversity that exists at loci associated with the immune system, pathogen response, and key sensory functions, which form the foundation for dissecting phenotypic traits in vivo. We review the current status of the mouse genome across the diversity of the mouse lineage and discuss the value of mice to understanding human disease.', 'corpus_id': 208039382, 'score': 1}, {'doc_id': '52898008', 'title': 'Sixteen diverse laboratory mouse reference genomes define strain specific haplotypes and novel functional loci', 'abstract': 'We report full-length draft de novo genome assemblies for 16 widely used inbred mouse strains and find extensive strain-specific haplotype variation. We identify and characterize 2,567 regions on the current mouse reference genome exhibiting the greatest sequence diversity. These regions are enriched for genes involved in pathogen defence and immunity and exhibit enrichment of transposable elements and signatures of recent retrotransposition events. Combinations of alleles and genes unique to an individual strain are commonly observed at these loci, reflecting distinct strain phenotypes. We used these genomes to improve the mouse reference genome, resulting in the completion of 10 new gene structures. Also, 62 new coding loci were added to the reference genome annotation. These genomes identified a large, previously unannotated, gene (Efcab3-like) encoding 5,874 amino acids. Mutant Efcab3-like mice display anomalies in multiple brain regions, suggesting a possible role for this gene in the regulation of brain development.Sequence assemblies for the genomes of 16 widely used inbred laboratory mouse strains highlight considerable strain-specific haplotype variation and allow for the identification of regions with the greatest sequence diversity between strains.', 'corpus_id': 52898008, 'score': 1}, {'doc_id': '233246071', 'title': 'Personalized genome assembly for accurate cancer somatic mutation discovery using cancer-normal paired reference samples', 'abstract': 'The use of personalized genome assembly as a reference for detecting the full spectrum of somatic events from cancers has long been advocated but never been systematically investigated. Here we address the critical need of assessing the accuracy of somatic mutation detection using personalized genome assembly versus the standard human reference assembly (i.e. GRCh38). We first obtained massive whole genome sequencing data using multiple sequencing technologies, and then performed de novo assembly of the first tumor-normal paired genomes, both nuclear and mitochondrial, derived from the same donor with triple negative breast cancer. Compared to standard human reference assembly, the haplotype phased chromosomal-scale personalized genome was best demonstrated with individual specific haplotypes for some complex regions and medical relevant genes. We then used this well-assembled personalized genome as a reference for read mapping and somatic variant discovery. We showed that the personalized genome assembly results in better alignments of sequencing reads and more accurate somatic mutation calls. Direct comparison of mitochondrial genomes led to discovery of unreported nonsynonymous somatic mutations. Our findings provided a unique resource and proved the necessity of personalized genome assembly as a reference in improving somatic mutation detection at personal genome level not only for breast cancer reference samples, but also potentially for other cancers.', 'corpus_id': 233246071, 'score': 0}, {'doc_id': '233330964', 'title': 'Characterization of a haplotype-reference panel for genotyping by low-pass sequencing in Swiss Large White pigs', 'abstract': 'Background The key-ancestor approach has been frequently applied to prioritize individuals for whole-genome sequencing based on their marginal genetic contribution to current populations. Using this approach, we selected 70 key ancestors from two lines of the Swiss Large White breed that have been selected divergently for fertility and fattening traits and sequenced their genomes with short paired-end reads. Results Using pedigree records, we estimated the effective population size of the dam and sire line to 72 and 44, respectively. In order to assess sequence variation in both lines, we sequenced the genomes of 70 boars at an average coverage of 16.69-fold. The boars explained 87.95 and 95.35% of the genetic diversity of the breeding populations of the dam and sire line, respectively. Reference-guided variant discovery using the GATK revealed 26,862,369 polymorphic sites. Principal component, admixture and fixation index (F ST ) analyses indicated considerable genetic differentiation between the lines. Genomic inbreeding quantified using runs of homozygosity was higher in the sire than dam line (0.28 vs 0.26). Using two complementary approaches, we detected 51 signatures of selection. However, only six signatures of selection overlapped between both lines. We used the sequenced haplotypes of the 70 key ancestors as a reference panel to call 22,618,811 genotypes in 175 pigs that had been sequenced at very low coverage (1.11-fold) using the GLIMPSE software. The genotype concordance, non-reference sensitivity and non-reference discrepancy between thus inferred and Illumina PorcineSNP60 BeadChip-called genotypes was 97.60, 98.73 and 3.24%, respectively. The low-pass sequencing-derived genomic relationship coefficients were highly correlated ( r \u2009>\u20090.99) with those obtained from microarray genotyping. Conclusions We assessed genetic diversity within and between two lines of the Swiss Large White pig breed. Our analyses revealed considerable differentiation, even though the split into two populations occurred only few generations ago. The sequenced haplotypes of the key ancestor animals enabled us to implement genotyping by low-pass sequencing which offers an intriguing cost-effective approach to increase the variant density over current array-based genotyping by more than 350-fold.', 'corpus_id': 233330964, 'score': 0}, {'doc_id': '232321157', 'title': 'In silico candidate variant and gene identification using inbred mouse strains', 'abstract': 'Mice are the most widely used animal model to study genotype to phenotype relationships. Inbred mice are genetically identical, which eliminates genetic heterogeneity and makes them particularly useful for genetic studies. Many different strains have been bred over decades and a vast amount of phenotypic data has been generated. In addition, recently whole genome sequencing-based genome-wide genotype data for many widely used inbred strains has been released. Here, we present an approach for in silico fine-mapping that uses genotypic data of 37 inbred mouse strains together with phenotypic data provided by the user to propose candidate variants and genes for the phenotype under study. Public genome-wide genotype data covering more than 74 million variant sites is queried efficiently in real-time to provide those variants that are compatible with the observed phenotype differences between strains. Variants can be filtered by molecular consequences and by corresponding molecular impact. Candidate gene lists can be generated from variant lists on the fly. Fine-mapping together with annotation or filtering of results is provided in a Bioconductor package called MouseFM. In order to characterize candidate variant lists under various settings, MouseFM was applied to two expression data sets across 20 inbred mouse strains, one from neutrophils and one from CD4+ T cells. Fine-mapping was assessed for about 10,000 genes, respectively, and identified candidate variants and haplotypes for many expression quantitative trait loci (eQTLs) reported previously based on these data. For albinism, MouseFM reports only one variant allele of moderate or high molecular impact that only albino mice share: a missense variant in the Tyr gene, reported previously to be causal for this phenotype. Performing in silico fine-mapping for interfrontal bone formation in mice using four strains with and five strains without interfrontal bone results in 12 genes. Of these, three are related to skull shaping abnormality. Finally performing fine-mapping for dystrophic cardiac calcification by comparing 9 strains showing the phenotype with eight strains lacking it, we identify only one moderate impact variant in the known causal gene Abcc6. In summary, this illustrates the benefit of using MouseFM for candidate variant and gene identification.', 'corpus_id': 232321157, 'score': 1}, {'doc_id': '1678365', 'title': 'Deep genome sequencing and variation analysis of 13 inbred mouse strains defines candidate phenotypic alleles, private variation and homozygous truncating mutations', 'abstract': 'BackgroundThe Mouse Genomes Project is an ongoing collaborative effort to sequence the genomes of the common laboratory mouse strains. In 2011, the initial analysis of sequence variation across 17 strains found 56.7\xa0M unique single nucleotide polymorphisms (SNPs) and 8.8\xa0M indels. We carry out deep sequencing of 13 additional inbred strains (BUB/BnJ, C57BL/10J, C57BR/cdJ, C58/J, DBA/1J, I/LnJ, KK/HiJ, MOLF/EiJ, NZB/B1NJ, NZW/LacJ, RF/J, SEA/GnJ and ST/bJ), cataloguing molecular variation within and across the strains. These strains include important models for immune response, leukaemia, age-related hearing loss and rheumatoid arthritis. We now have several examples of fully sequenced closely related strains that are divergent for several disease phenotypes.ResultsApproximately 27.4\xa0M unique SNPs and 5\xa0M indels are identified across these strains compared to the C57BL/6\xa0J reference genome (GRCm38). The amount of variation found in the inbred laboratory mouse genome has increased to 71\xa0M SNPs and 12\xa0M indels. We investigate the genetic basis of highly penetrant cancer susceptibility in RF/J finding private novel missense mutations in DNA damage repair and highly cancer associated genes. We use two highly related strains (DBA/1J and DBA/2J) to investigate the genetic basis of collagen-induced arthritis susceptibility.ConclusionsThis paper significantly expands the catalogue of fully sequenced laboratory mouse strains and now contains several examples of highly genetically similar strains with divergent phenotypes. We show how studying private missense mutations can lead to insights into the genetic mechanism for a highly penetrant phenotype.', 'corpus_id': 1678365, 'score': 1}, {'doc_id': '232282892', 'title': 'Genome assembly of the maize inbred line A188 provides a new reference genome for functional genomics', 'abstract': 'Heretofore, little is known about the mechanism underlying the genotype-dependence of embryonic callus (EC) induction, which has severely inhibited the development of maize genetic engineering. Here, we report the genome sequence and annotation of a maize inbred line with high EC induction ratio, A188, which is assembled from single-molecule sequencing and optical genome mapping. We assembled a 2,210 Mb genome with a scaffold N50 size of 11.61 million bases (Mb), compared to those of 9.73 Mb for B73 and 10.2 Mb for Mo17. Comparative analysis revealed that ∼30% of the predicted A188 genes had large structural variations to B73, Mo17 and W22 genomes, which caused considerable protein divergence and might lead to phenotypic variations between the four inbred lines. Combining our new A188 genome, previously reported QTLs and RNA sequencing data, we reveal 8 large structural variation genes and 4 differentially expressed genes playing potential roles in EC induction. Highlight Our manuscript presents a high-quality reference genome of the inbred line A188, and provides new insights into candidate genes underlying maize embryonic callus induction and other maize agronomic traits.', 'corpus_id': 232282892, 'score': 0}, {'doc_id': '232058192', 'title': 'Haplotype-resolved diverse human genomes and integrated analysis of structural variation', 'abstract': 'Resolving genomic structural variation Many human genomes have been reported using short-read technology, but it is difficult to resolve structural variants (SVs) using these data. These genomes thus lack comprehensive comparisons among individuals and populations. Ebert et al. used long-read structural variation calling across 64 human genomes representing diverse populations and developed new methods for variant discovery. This approach allowed the authors to increase the number of confirmed SVs and to describe the patterns of variation across populations. From this dataset, they identified quantitative trait loci affected by these SVs and determined how they may affect gene expression and potentially explain genome-wide association study hits. This information provides insights into patterns of normal human genetic variation and generates reference genomes that better represent the diversity of our species. Science, this issue p. eabf7117 Human genetic variation is elucidated from de novo assembly of 32 genomes selected as representatives of the 1000 Genomes Project. INTRODUCTION The characterization of the full spectrum of genetic variation is critical to understanding human health and disease. Recent technological advances have made it possible to survey genetic variants on the level of fully reconstructed haplotypes, leading to substantially improved sensitivity in detecting and characterizing large structural variants (SVs), including complex classes. RATIONALE We focused on comprehensive genetic variant discovery from a human diversity panel representing 25 human populations. We leveraged a recently developed computational pipeline that combines long-read technology and single-cell template strand sequencing (Strand-seq) to generate fully phased diploid genome assemblies without guidance of a reference genome or use of parent-child trio information. Variant discovery from high-quality haplotype assemblies increases sensitivity and yields variants that are not only sequence resolved but also embedded in their genomic context, substantially improving genotyping in short-read sequenced cohorts and providing an assessment of their potential functional relevance. RESULTS We generated fully phased genome assemblies for 35 individuals (32 unrelated and three children from parent-child trios). Genomes are highly contiguous [average minimum contig length needed to cover 50% of the genome: 26 million base pairs (Mbp)], accurate at the base-pair level (quality value > 40), correctly phased (average switch error rate 0.18%), and nearly complete compared with GRCh38 (median aligned contig coverage >95%). From the set of 64 unrelated haplotype assemblies, we identified 15.8 million single-nucleotide variants (SNVs), 2.3 million insertions/deletions (indels; 1 to 49 bp in length), 107,590 SVs (≥50 bp), 316 inversions, and 9453 nonreference mobile elements. The large fraction of African individuals in our study (11 of 35) enhances the discovery of previously unidentified variation (approximately twofold increase in discovery rate compared with non-Africans). Overall, ~42% of SVs are previously unidentified compared with recent long-read-based studies. Using orthogonal technologies, we validated most events and discovered ~35 structurally divergent regions per human genome (>50 kbp) not yet fully resolved with long-read genome assembly. We found that homology-mediated mechanisms of SV formation are twice as common as expected from previous reports that used short-read sequencing. We constructed a phylogeny of active L1 source elements and observed a correlation between evolutionary age and features such as the activity level, suggesting that younger elements contribute disproportionately to disease-causing variation. Transduction tracing allowed the identification of 54 active SVA retrotransposon source elements, which mobilize nonrepetitive sequences at their 5′ and 3′ ends. We genotyped up to 50,340 SVs into Illumina short-read data from the 1000 Genomes Project and identified variants associated with changes in gene expression, such as a 1069-bp SV near the gene LIPI, a locus that is associated with cardiac failure. We further identified 117 loci that show evidence for population stratification. These are candidates for local adaptation, such as a 4.0-kbp deletion of regulatory DNA LCT (lactase gene) among Europeans. CONCLUSION Fully reconstructed haplotype assemblies triple SV discovery when compared with short-read data and improve genotyping, leading to insights into SV mechanism of origin, evolutionary history, and disease association. Discovery and analysis of global human genetic diversity. Starting from a global panel of human diversity (top), we discovered structural variation from fully phased diploid genome assemblies (middle), resulting in a comprehensive catalog of sequence- and context-resolved variants. This facilitates integrative analysis and identification of new associations between variants and molecular phenotypes (bottom). SAS, South Asian; AMR, Admixed American; AFR, African; EUR, European; EAS, East Asian; INV, inversion; INS, insertion; DEL, deletion; MEI, mobile element insertion. Long-read and strand-specific sequencing technologies together facilitate the de novo assembly of high-quality haplotype-resolved human genomes without parent-child trio data. We present 64 assembled haplotypes from 32 diverse human genomes. These highly contiguous haplotype assemblies (average minimum contig length needed to cover 50% of the genome: 26 million base pairs) integrate all forms of genetic variation, even across complex loci. We identified 107,590 structural variants (SVs), of which 68% were not discovered with short-read sequencing, and 278 SV hotspots (spanning megabases of gene-rich sequence). We characterized 130 of the most active mobile element source elements and found that 63% of all SVs arise through homology-mediated mechanisms. This resource enables reliable graph-based genotyping from short reads of up to 50,340 SVs, resulting in the identification of 1526 expression quantitative trait loci as well as SV candidates for adaptive selection within the human population.', 'corpus_id': 232058192, 'score': 0}, {'doc_id': '233303483', 'title': 'Zea mays RNA-seq estimated transcript abundances are strongly affected by read mapping bias', 'abstract': 'Background Genetic variation for gene expression is a source of phenotypic variation for natural and agricultural species. The common approach to map and to quantify gene expression from genetically distinct individuals is to assign their RNA-seq reads to a single reference genome. However, RNA-seq reads from alleles dissimilar to this reference genome may fail to map correctly, causing transcript levels to be underestimated. Presently, the extent of this mapping problem is not clear, particularly in highly diverse species. We investigated if mapping bias occurred and if chromosomal features associated with mapping bias. Zea mays presents a model species to assess these questions, given it has genotypically distinct and well-studied genetic lines. Results In Zea mays , the inbred B73 genome is the standard reference genome and template for RNA-seq read assignments. In the absence of mapping bias, B73 and a second inbred line, Mo17, would each have an approximately equal number of regulatory alleles that increase gene expression. Remarkably, Mo17 had 2–4 times fewer such positively acting alleles than did B73 when RNA-seq reads were aligned to the B73 reference genome. Reciprocally, over one-half of the B73 alleles that increased gene expression were not detected when reads were aligned to the Mo17 genome template. Genes at dissimilar chromosomal ends were strongly affected by mapping bias, and genes at more similar pericentromeric regions were less affected. Biased transcript estimates were higher in untranslated regions and lower in splice junctions. Bias occurred across software and alignment parameters. Conclusions Mapping bias very strongly affects gene transcript abundance estimates in maize, and bias varies across chromosomal features. Individual genome or transcriptome templates are likely necessary for accurate transcript estimation across genetically variable individuals in maize and other species.', 'corpus_id': 233303483, 'score': 0}, {'doc_id': '211110034', 'title': 'A comprehensive and comparative phenotypic analysis of the collaborative founder strains identifies new and known phenotypes', 'abstract': 'The collaborative cross (CC) is a large panel of mouse-inbred lines derived from eight founder strains (NOD/ShiLtJ, NZO/HILtJ, A/J, C57BL/6J, 129S1/SvImJ, CAST/EiJ, PWK/PhJ, and WSB/EiJ). Here, we performed a comprehensive and comparative phenotyping screening to identify phenotypic differences and similarities between the eight founder strains. In total, more than 300 parameters including allergy, behavior, cardiovascular, clinical blood chemistry, dysmorphology, bone and cartilage, energy metabolism, eye and vision, immunology, lung function, neurology, nociception, and pathology were analyzed; in most traits from sixteen females and sixteen males. We identified over 270 parameters that were significantly different between strains. This study highlights the value of the founder and CC strains for phenotype-genotype associations of many genetic traits that are highly relevant to human diseases. All data described here are publicly available from the mouse phenome database for analyses and downloads.', 'corpus_id': 211110034, 'score': 1}]
81	{'doc_id': '7468879', 'title': 'Robots and Jobs: Evidence from US Labor Markets', 'abstract': 'As robots and other computer-assisted technologies take over tasks previously performed by labor, there is increasing concern about the future of jobs and wages. We analyze the effect of the increase in industrial robot usage between 1990 and 2007 on US local labor markets. Using a model in which robots compete against human labor in the production of different tasks, we show that robots may reduce employment and wages, and that the local labor market effects of robots can be estimated by regressing the change in employment and wages on the exposure to robots in each local labor market—defined from the national penetration of robots into each industry and the local distribution of employment across industries. Using this approach, we estimate large and robust negative effects of robots on employment and wages across commuting zones. We bolster this evidence by showing that the commuting zones most exposed to robots in the post-1990 era do not exhibit any differential trends before 1990. The impact of robots is distinct from the impact of imports from China and Mexico, the decline of routine jobs, offshoring, other types of IT capital, and the total capital stock (in fact, exposure to robots is only weakly correlated with these other variables). According to our estimates, one more robot per thousand workers reduces the employment to population ratio by about 0.18-0.34 percentage points and wages by 0.25-0.5 percent.', 'corpus_id': 7468879}	12947	"[{'doc_id': '231993412', 'title': 'www.econstor.eu Task content and job losses in the Great Lockdown', 'abstract': 'I examine the short-term labor market effects of the Great Lockdown in the United States. I analyze job losses by task content (Acemoglu & Autor 2011), and show that they follow underlying trends; jobs with a high non-routine content are especially well-protected, even if they are not teleworkable. The importance of the task content, particularly for non-routine cognitive analytical tasks, is strong even after controlling for age, gender, race, education, sector and location (and hence for differential demand and supply shocks). Jobs subject to higher structural turnover rates are much more likely to be terminated, suggesting that easier-to-replace employees were at a particular disadvantage, even within sectors; at the same time, there is evidence of labor hoarding for more valuable matches. Individuals in low-skilled jobs fared comparatively better in industries with a high share of highskilled workers. JEL Codes: D22, E32, J23, J24, M51.', 'corpus_id': 231993412, 'score': 0}, {'doc_id': '85547112', 'title': 'Artificial Intelligence and its Implications for Income Distribution and Unemployment', 'abstract': 'Inequality is one of the main challenges posed by the proliferation of artificial intelligence (AI) and other forms of worker-replacing technological progress. This paper provides a taxonomy of the associated economic issues: First, we discuss the general conditions under which new technologies such as AI may lead to a Pareto improvement. Secondly, we delineate the two main channels through which inequality is affected – the surplus arising to innovators and redistributions arising from factor price changes. Third, we provide several simple economic models to describe how policy can counter these effects, even in the case of a “singularity” where machines come to dominate human labor. Under plausible conditions, non-distortionary taxation can be levied to compensate those who otherwise might lose. Fourth, we describe the two main channels through which technological progress may lead to technological unemployment – via efficiency wage effects and as a transitional phenomenon. Lastly, we speculate on how technologies to create super-human levels of intelligence may affect inequality and on how to save humanity from the Malthusian destiny that may ensue.', 'corpus_id': 85547112, 'score': 1}, {'doc_id': '231203954', 'title': 'The productivity paradox: policy lessons from MICROPROD', 'abstract': 'MICROPROD researchers have so far delivered 20 papers on four broad issues relevant for today’s policy debates: the measurement and effects of intangible capital on productivity; the impact of globalisation, international trade and the integration of global value chains (GVCs) on productivity; factor allocation and allocative efficiency; and finally the social consequences of the two structural shocks Europe has faced in the last two decades: globalisation and technological progress.', 'corpus_id': 231203954, 'score': 0}, {'doc_id': '18383943', 'title': 'The Race between Man and Machine: Implications of Technology for Growth, Factor Shares, and Employment', 'abstract': 'We examine the concerns that new technologies will render labor redundant in a framework in which tasks previously performed by labor can be automated and new versions of existing tasks, in which labor has a comparative advantage, can be created. In a static version where capital is fixed and technology is exogenous, automation reduces employment and the labor share, and may even reduce wages, while the creation of new tasks has the opposite effects. Our full model endogenizes capital accumulation and the direction of research towards automation and the creation of new tasks. If the long-run rental rate of capital relative to the wage is sufficiently low, the long-run equilibrium involves automation of all tasks. Otherwise, there exists a stable balanced growth path in which the two types of innovations go hand-in-hand. Stability is a consequence of the fact that automation reduces the cost of producing using labor, and thus discourages further automation and encourages the creation of new tasks. In an extension with heterogeneous skills, we show that inequality increases during transitions driven both by faster automation and introduction of new tasks, and characterize the conditions under which inequality is increasing or stable in the long run.', 'corpus_id': 18383943, 'score': 1}, {'doc_id': '151194632', 'title': 'Skills, Tasks and Technologies: Implications for Employment and Earnings', 'abstract': 'A central organizing framework of the voluminous recent literature studying changes in the returns to skills and the evolution of earnings inequality is what we refer to as the canonical model, which elegantly and powerfully operationalizes the supply and demand for skills by assuming two distinct skill groups that perform two different and imperfectly substitutable tasks or produce two imperfectly substitutable goods. Technology is assumed to take a factor-augmenting form, which, by complementing either high or low skill workers, can generate skill biased demand shifts. In this paper, we argue that despite its notable successes, the canonical model is largely silent on a number of central empirical developments of the last three decades, including: (1) significant declines in real wages of low skill workers, particularly low skill males; (2) non-monotone changes in wages at different parts of the earnings distribution during different decades; (3) broad-based increases in employment in high skill and low skill occupations relative to middle skilled occupations (i.e., job ""polarization""); (4) rapid diffusion of new technologies that directly substitute capital for labor in tasks previously performed by moderately skilled workers; and (5) expanding offshoring in opportunities, enabled by technology, which allow foreign labor to substitute for domestic workers specific tasks. Motivated by these patterns, we argue that it is valuable to consider a richer framework for analyzing how recent changes in the earnings and employment distribution in the United States and other advanced economies are shaped by the interactions among worker skills, job tasks, evolving technologies, and shifting trading opportunities. We propose a tractable task-based model in which the assignment of skills to tasks is endogenous and technical change may involve the substitution of machines for certain tasks previously performed by labor. We further consider how the evolution of technology in this task-based setting may be endogenized. We show how such a framework can be used to interpret several central recent trends, and we also suggest further directions for empirical exploration.', 'corpus_id': 151194632, 'score': 1}, {'doc_id': '84834077', 'title': 'Automation and New Tasks: How Technology Displaces and Reinstates Labor', 'abstract': 'We present a framework for understanding the effects of automation and other types of technological changes on labor demand, and use it to interpret changes in US employment over the recent past. At the center of our framework is the allocation of tasks to capital and labor—the task content of production. Automation, which enables capital to replace labor in tasks it was previously engaged in, shifts the task content of production against labor because of a displacement effect. As a result, automation always reduces the labor share in value added and may reduce labor demand even as it raises productivity. The effects of automation are counterbalanced by the creation of new tasks in which labor has a comparative advantage. The introduction of new tasks changes the task content of production in favor of labor because of a reinstatement effect, and always raises the labor share and labor demand. We show how the role of changes in the task content of production—due to automation and new tasks—can be inferred from industry-level data. Our empirical decomposition suggests that the slower growth of employment over the last three decades is accounted for by an acceleration in the displacement effect, especially in manufacturing, a weaker reinstatement effect, and slower growth of productivity than in previous decades.', 'corpus_id': 84834077, 'score': 1}, {'doc_id': '232147300', 'title': ""Does automation erode governments' tax basis? An empirical assessment of tax revenues in Europe"", 'abstract': 'Decomposing taxes by source (labor, capital, sales), we analyze the impact of automation (1) on tax revenues, (2) the structure of taxation, and (3) identify channels of impact in 19 EU countries during 1995-2016. Robots and Information and Communication Technologies (ICT) are different technologies designed to automate manual (robots) or cognitive tasks (ICT). Until 2007, robot diffusion led to decreasing factor and tax income, and a shift from taxes on capital to goods. ICTs changed the structure of taxation from capital to labor. We find decreasing employment, but increasing wages and labor income. After 2008, robots have no effect but we find an ICT-induced increase in capital income, a rise of services, but no effect on taxation. Automation goes through different phases with different economic impacts which affect the amount and structure of taxes. Whether automation erodes taxation depends (a) on the technology type, (b) the stage of diffusion and (c) local conditions.', 'corpus_id': 232147300, 'score': 0}, {'doc_id': '45091696', 'title': 'Secular Stagnation? The Effect of Aging on Economic Growth in the Age of Automation', 'abstract': 'Several recent theories emphasize the negative effects of an aging population on economic growth, either because of the lower labor force participation and productivity of older workers or because aging will create an excess of savings over desired investment, leading to secular stagnation. We show that there is no such negative relationship in the data. If anything, countries experiencing more rapid aging have grown more in recent decades. We suggest that this counterintuitive finding might reflect the more rapid adoption of automation technologies in countries undergoing more pronounced demographic changes, and provide evidence and theoretical underpinnings for this argument.', 'corpus_id': 45091696, 'score': 1}, {'doc_id': '231958242', 'title': 'Financial Frictions, Allocative Efficiency, and Unemployment: A Quantitative Analysis for Argentina∗', 'abstract': 'Argentina is characterized by low levels of private credit and persistent labor market rigidities. Furthermore, financial development remained stagnant in Argentina even during episodes of fast economic growth, in stark contrast with the experience of sustained growth accelerations around the world. The goals of the paper are twofold. Firstly, it is concerned with quantifying the productivity losses associated with such low levels of private credit penetration and characterizing its implications for different subsets of firms in the economy. The latter is important in light of various policy interventions aimed at mitigating the impact of low access to credit based on firm-size thresholds. Secondly, it studies the dynamics of hypothetical reforms to credit markets in a context of rigid labor markets, which seems to be the adequate scenario in which structural reforms will have to be implemented, given the stickiness that labor market regulations have shown to reform efforts in the past. It finds sizable productivity losses from financial frictions, in the order of 13%. At the micro level it finds that it is the youngest firms, whose average marginal return to capital is far above the riskfree rate in the economy, that are more prone to become financially constrained. Turning to reform scenarios, we investigate sudden reforms that are implemented abruptly and more plausible reform paths that gradually dismantle financial frictions. In the former, productivity and the investment rate rise sharply on impact, while it also does the rate of unemployment, going from 5 to almost 12%. In the latter, the rise of unemployment is more gradual and less sharp, peaking at 7%. On the flipside, the investment rate declines on impact, although the contraction is short-lived.', 'corpus_id': 231958242, 'score': 0}, {'doc_id': '232966563', 'title': 'WORKING PAPER SERIES WAGE INEQUALITY AND FIRM GROWTH', 'abstract': 'We examine how within-firm skill premia–wage differentials associated with jobs involving different skill requirements–vary both across firms and over time. Our firm-level results mirror patterns found in aggregate wage trends, except that we find them with regard to increases in firm size. In particular, we find that wage differentials between highand either mediumor low-skill jobs increase with firm size, while those between mediumand low-skill jobs are either invariant to firm size or, if anything, slightly decreasing. We find the same pattern within firms over time, suggesting that rising wage inequality–even nuanced patterns, such as divergent trends in upperand lower-tail inequality–may be related to firm growth. We explore two possible channels: i) wages associated with “routine” job tasks are relatively lower in larger firms due to a higher degree of automation in these firms, and ii) larger firms pay relatively lower entry-level managerial wages in return for providing better career opportunities. Lastly, we document a strong and positive relation between within-country variation in firm growth and rising wage inequality for a broad set of developed countries. In fact, our results suggest that part of what may be perceived as a global trend toward more wage inequality may be driven by an increase in employment by the largest firms in the economy. Holger M. Mueller Stern School of Business New York University 44 West Fourth Street Suite 9-190 New York, NY 10012-1126 and NBER hmueller@stern.nyu.edu Paige P. Ouimet Kenan-Flagler Business School University of North Carolina at Chapel Hill Paige_Ouimet@kenan-flagler.unc.edu Elena Simintzi University of British Columbia Sauder School of Business 2053 Main Mall Vancouver V6T 1Z2 Canada elena.simintzi@sauder.ubc.ca', 'corpus_id': 232966563, 'score': 0}]"
82	{'doc_id': '4048974', 'title': 'Is Cancer Information Exchanged on Social Media Scientifically Accurate?', 'abstract': 'Cancer patients and their caregivers are increasingly using social media as a platform to share cancer experiences, connect with support, and exchange cancer-related information. Yet, little is known about the nature and scientific accuracy of cancer-related information exchanged on social media. We conducted a content analysis of 12\xa0months of data from 18 publically available Facebook Pages hosted by parents of children with acute lymphoblastic leukemia (N\xa0=\xa015,852 posts) and extracted all exchanges of medically-oriented cancer information. We systematically coded for themes in the nature of cancer-related information exchanged on personal Facebook Pages and two oncology experts independently evaluated the scientific accuracy of each post. Of the 15,852 total posts, 171 posts contained medically-oriented cancer information. The most frequent type of cancer information exchanged was information related to treatment protocols and health services use (35%) followed by information related to side effects and late effects (26%), medication (16%), medical caregiving strategies (13%), alternative and complementary therapies (8%), and other (2%). Overall, 67% of all cancer information exchanged was deemed medically/scientifically accurate, 19% was not medically/scientifically accurate, and 14% described unproven treatment modalities. These findings highlight the potential utility of social media as a cancer-related resource, but also indicate that providers should focus on recommending reliable, evidence-based sources to patients and caregivers.', 'corpus_id': 4048974}	17500	[{'doc_id': '237551478', 'title': 'The opportunities and challenges of social media in interstitial lung disease: a viewpoint', 'abstract': 'Social media is an increasingly popular source of health information, and the rarity and complexity of interstitial lung disease (ILD) may particularly draw patients with ILD to social media for information and support. The objective of this viewpoint is to provide an overview of social media, explore the benefits and limitations of ILD-related social media use, and discuss future development of healthcare information on social media. We describe the value of integrating social media into the practice of ILD health professionals, including its role in information dissemination, patient engagement, knowledge generation, and formation of health policy. We also describe major challenges to expanded social media use in ILD, including limited access for some individuals and populations, abundance of misinformation, and concerns about patient privacy. Finally, for healthcare professionals looking to join social media, we provide practical guidance and considerations to optimize the potential benefits and minimize the potential pitfalls of social media.', 'corpus_id': 237551478, 'score': 0}, {'doc_id': '54550967', 'title': 'Dissemination of Misinformative and Biased Information about Prostate Cancer on YouTube.', 'abstract': 'YouTube is a social media platform with more than 1 billion users and >600000 videos about prostate cancer. Two small studies examined the quality of prostate cancer videos on YouTube, but did not use validated instruments, examine user interactions, or characterize the spread of misinformation. We performed the largest, most comprehensive examination of prostate cancer information on YouTube to date, including the first 150 videos on screening and treatment. We used the validated DISCERN quality criteria for consumer health information and the Patient Education Materials Assessment Tool, and compared results for user engagement. The videos in our sample had up to 1.3 million views (average 45223) and the overall quality of information was moderate. More videos described benefits (75%) than harms (53%), and only 50% promoted shared decision-making as recommended in current guidelines. Only 54% of the videos defined medical terms and few provided summaries or references. There was a significant negative correlation between scientific quality and viewer engagement (views/month p=0.004; thumbs up/views p=0.015). The comments section underneath some videos contained advertising and peer-to-peer medical advice. A total of 115 videos (77%) contained potentially misinformative and/or biased content within the video or comments section, with a total reach of >6 million viewers. PATIENT SUMMARY: Many popular YouTube videos about prostate cancer contained biased or poor-quality information. A greater number of views and thumbs up on YouTube does not mean that the information is trustworthy.', 'corpus_id': 54550967, 'score': 1}, {'doc_id': '233283191', 'title': 'A Content Analysis of YouTube™ Videos Related to Bladder Cancer', 'abstract': 'Objective: We examined the content of YouTubeTM videos on urinary bladder cancer education and evaluated their usefulness in promoting early detection of the cancer. Material and Medthods: A systematic search of YouTubeTM for videos containing knowledge information on bladder cancer was conducted using the keywords ‘bladder cancer’. Details about demographics of videos, including type, length, source and viewers’ interaction were evaluated and 2 researchers independently assessed the videos for usefulness in promoting knowledge on bladder cancer. Results: A total of 100 YouTubeTM videos (100 most viewed videos were reviewed and 48 videos were excluded including surgical technic videos, videos in non-English languages, patient testimonial videos and videos about complementary and alternative medicine. A total of 52 videos were analyzed. The highest number of videos were uploaded by medical websites (18, 34.6%), the mean number of views is highest in videos that were categorized as not useful (105,447), followed by very useful (74,940.6±120,980.8), slightly useful (46,219.6±101,261.4), moderately useful (34,941.0±35,413.1). The mean number of “likes” is highest in the very useful group (339.4±373.6), so is the “dislikes” (25.3±40.9). Conclusion: YouTubeTM contains a diverse source of information on bladder cancer. Most videos on bladder cancer may not be informative for health education. Medical professionals, medical institutions, and professional organizations should improve the content of videos about bladder cancer to provide patients with reliable and useful information.', 'corpus_id': 233283191, 'score': 1}, {'doc_id': '236517386', 'title': 'The Role of Providers and Influencers in the Use of Social Media as Solace for Psoriasis: Qualitative and Quantitative Study (Preprint)', 'abstract': 'Background: Psoriasis is a multisystem chronic inflammatory skin disease and is a relatively common disorder in children and adults. The burden of psoriasis impacts both the physiological and psychological areas of one’s life. Given the robust use of the internet and social media, patients have turned to Instagram for educational and social support to discuss psoriasis. Objective: This study aimed to characterize how patients interact with Instagram to cope with the biopsychosocial aspects of psoriasis. We analyzed journals and organizations, and compared them with the public profiles of individuals diagnosed with psoriasis who provided information and refuge. Our goal was to identify how followers engaged and what type of content they were most receptive to in terms of psoriasis. Methods: All journals and organizations representing psoriasis were selected for review. The top 10 public profiles of individuals diagnosed with psoriasis were also selected for comparison. The numbers of followers, followings, and posts were noted to evaluate popularity. The numbers of likes and comments were also recorded to understand engagement. Results: On comparing journals and organizations to public profiles, we found that the former had a greater number of followers but engaged less with the audience on Instagram based on the number of profiles they followed. Profiles of individuals with psoriasis produced content that was more personal and relatable, including experiences with flares, motivational text, and emotional support. The content produced by journals and organizations was geared toward education and providing peer-reviewed resources and commentary from licensed health care professionals. Followers were more engaged via “likes” than “comments” on the Instagram profiles of journals and organizations, as well as the public profiles of individuals diagnosed with psoriasis. Conclusions: There was evident online presence of journals and organizations, and public profiles of individuals providing content regarding psoriasis on Instagram. However, there were distinguishing features for the type of content being produced. Journals and organizations took the traditional approach in providing evidence-based information, whereas the public profiles of individuals provided content related to the psychosocial needs of the psoriasis community. The 10 profiles of individuals provided posts involving creativity and real experiences, which were evidently well-received based on “likes” and “comments.” This research helps us appreciate what the audience on Instagram is looking for to further address how we can merge these needs to provide a holistic platform on Instagram for both providers and patients. Social media creates a space for collaboration, which can be advantageous for journals and organizations to work with patient volunteers from diverse backgrounds who can help build a therapeutic alliance and public presence on Instagram with their viewers in order to deliver medical peer-reviewed information. (JMIR Dermatol 2021;4(2):e29904) doi: 10.2196/29904 JMIR Dermatol 2021 | vol. 4 | iss. 2 | e29904 | p. 1 https://derma.jmir.org/2021/2/e29904 (page number not for citation purposes) Pakhdikian & Woo JMIR DERMATOLOGY', 'corpus_id': 236517386, 'score': 0}, {'doc_id': '237377432', 'title': 'TikTok as a Health Information Source: Assessment of the Quality of Information in Diabetes-Related Videos', 'abstract': 'Background Diabetes has become one of the most prevalent chronic diseases, and many people living with diabetes use social media to seek health information. Recently, an emerging social media app, TikTok, has received much interest owing to its popularity among general health consumers. We notice that there are many videos about diabetes on TikTok. However, it remains unclear whether the information in these videos is of satisfactory quality. Objective This study aimed to assess the quality of the information in diabetes-related videos on TikTok. Methods We collected a sample of 199 diabetes-related videos in Chinese. The basic information presented in the videos was coded and analyzed. First, we identified the source of each video. Next, 2 independent raters assessed each video in terms of the completeness of six types of content (the definition of the disease, symptoms, risk factors, evaluation, management, and outcomes). Then, the 2 raters independently assessed the quality of information in the videos, using the DISCERN instrument. Results In regard to the sources of the videos, we found 6 distinct types of uploaders; these included 3 kinds of individual users (ie, health professionals, general users, and science communicators) and 3 types of organizational users (ie, news agencies, nonprofit organizations, and for-profit organizations). Regarding content, our results show that the videos were primarily about diabetes management and contained limited information on the definition of the disease, symptoms, risk factors, evaluation, and outcomes. The overall quality of the videos was acceptable, on average, although the quality of the information varied, depending on the sources. The videos created by nonprofit organizations had the highest information quality, while the videos contributed by for-profit organizations had the lowest information quality. Conclusions Although the overall quality of the information in the diabetes videos on TikTok is acceptable, TikTok might not fully meet the health information needs of patients with diabetes, and they should exercise caution when using TikTok as a source of diabetes-related information.', 'corpus_id': 237377432, 'score': 0}, {'doc_id': '235252793', 'title': 'Tetrahydrocannabinol and Skin Cancer: Analysis of YouTube Videos', 'abstract': 'Background: Cannabis oil is being used topically by patients with skin cancer as a homeopathic remedy, and has been promoted and popularized on social media, including YouTube. Although topical cannabinoids, especially tetrahydrocannabinol (THC), may have antitumor effects, results from a sparse number of clinical trials and peer-reviewed studies detailing safety and efficacy are still under investigation. Objective: We sought to assess the accuracy, quality, and reliability of THC oil and skin cancer information available on YouTube. Methods: The 10 most-viewed videos on THC oil and skin cancer were analyzed with the Global Quality Scale (GQS), DISCERN score, and useful/misleading criteria based on presentation of erroneous and scientifically unproven information. The videos were also inspected for source, length, and audience likes/dislikes. Top comments were additionally examined based on whether they were favorable, unfavorable, or neutral regarding the video content. Results: All analyzed videos (10/10, 100%) received a GQS score of 1, corresponding to poor quality of content, and 9/10 (90%) videos received a DISCERN score of 0, indicating poor reliability of information presented. All 10 videos were also found to be misleading and not useful according to established criteria. Top comments were largely either favorable (13/27, 48%) or neutral (13/27, 48%) toward the content of the videos, compared to unfavorable (1/27, 4%). Conclusions: Dermatologists should be aware that the spread of inaccurate information on skin cancer treatment currently exists on popular social media platforms and may lead to detrimental consequences for patients interested in pursuing alternative or homeopathic approaches. (JMIR Dermatol 2021;4(1):e26564) doi: 10.2196/26564', 'corpus_id': 235252793, 'score': 1}, {'doc_id': '151100183', 'title': 'Cancer Treatment Using Herbals in Arabic Social Media: Content Analysis of YouTube Videos', 'abstract': 'YouTube is a very common video sharing platform in the cyberspace. It has been emerged as a popular tool for general information in health sector. The purpose of this study was to analyze the YouTube videos of Arabic content related to using herbals for cancer treatment. The result contained 75 videos that were viewed a total of 4,770,491 times. The analyzed videos presented more than 107 different herbals and natural plants as sources of treatment for cancer. The study found that the purpose of most videos (96%) was provided by individuals without any professional connection or scientific evidence. The content analysis raised huge concerns about the accuracy, safety monitoring and exploiting the patients’ needs in YouTube messages for using herbals in cancer treatment. The study recommended that YouTube should follow the downloaded videos and establish a clear governance framework such as national health authorities and control agencies due to the high risk that can be produced by the misleading and invalid information that may harm the patients.', 'corpus_id': 151100183, 'score': 1}, {'doc_id': '236972456', 'title': 'Social media use to improve communication on children and adolescent’s health: the role of the Italian Paediatric Society influencers', 'abstract': 'Background Fake news on children’s and adolescent health are spreading. Internet availability and decreasing costs of media devices are contributing to an easy access to technology by families. Public health organizations are working to contrast misinformation and promote scientific communication. In this context, a new form of communication is emerging social media influencers. Aim of this study is to evaluate the role of paediatric influencers (PI) in communicating information about children and adolescents’ health. Materials and methods A group of PI was enrolled from December 2019 to January 2020 by a scientific commission nominated by the Italian Paediatric Society (SIP). PI were asked to share Facebook messages from the official page of the SIP to their own network. Social media tools have been evaluated across 12 months, from July 28, 2019, to July 11, 2020. For the purposes of clarity, we schematically divided the study period as follows: the period of PIs activity (January 6, 2020, to July 11, 2020) and the period when PIs were not yet active (July 28, 2019, to January 4, 2020). Information on Facebook page (lifetime total likes, daily new likes, daily page engaged, daily total reach) and on published post (lifetime post total reach, lifetime post organic reach, lifetime engaged users) were evaluated. Results A significant increase in Facebook daily new likes, page engagement and total reach, as well as in lifetime post total and organic reach was evidenced. As for PI, they reported a positive experience in most cases. Discussion In the digital era, communication strategies are becoming more important, so that the scientific community has to be actively involved in social media communication. Our pilot study demonstrated that the recruitment of paediatric influencers has increased communication and interaction of the SIP Facebook page. Conclusion Our study shows the potential role of influencers: spreading health messages via PI seems to be a successful strategy to promote correct communication about children’s and adolescents’ health.', 'corpus_id': 236972456, 'score': 0}, {'doc_id': '237487248', 'title': 'Social Media and Ethical Challenges for the Dermatologist', 'abstract': 'The purpose of review is to provide guidance on the use of social media within the context of dermatology and discuss its ethical, professional, and legal implications in education, mentorship, networking, business, and clinical settings. Despite its fundamental value as a means of communication and knowledge sharing, social media carries legal, ethical, and professional challenges. Healthcare providers have run into issues such as misinformation, conflicts of interest, and overstepping patient-physician boundaries when using social media. An interesting finding is that dermatologists commonly engage with an online audience through social media marketing or being an influencer to improve business and extend their reach to clients; however, this warrants formal training and the need to monitor their own online presence to prevent legal consequences. Social media has become integral in everyday life; billions of people now receive information and stay connected with each other through social platforms. Within medicine, social media has enhanced various aspects of healthcare, such as professional networking, patient care, and patient education. In dermatology, social media allows dermatologists to promote their businesses and services through patient testimonials, posting advice on blogs, and networking with a large audience of potential patients. However, having a social media presence must be exercised with care, purpose, and transparency to maximize benefits and minimize harmful consequences. This is especially important when inappropriate social media posts by physicians can be scrutinized for breaching patient confidentiality, violating privacy, financial conflicts of interest, and possibly disseminating incorrect information.', 'corpus_id': 237487248, 'score': 0}, {'doc_id': '234782484', 'title': 'Quality analysis of testicular cancer videos on YouTube', 'abstract': 'The aim of this study was to assess the content, reliability and quality of information regarding testicular cancer in YouTube videos. The search was performed by using term ‘testicular cancer’ on YouTube, and the first 168 videos were listed according to relevancy. Video features and source of upload were recorded. The quality, reliability and accuracy of the information were evaluated by two independent urologists using the Journal of American Medical Association (JAMA) score, the 5‐point modified DISCERN tool and the Global Quality Score (GQS). A total number of 152 videos were analysed. The most common source of upload was talk show programmes/TV programmes (25.7%), and majority of the content was about (24.3%) symptoms and diagnosis options. The mean JAMA score, modified DISCERN score and GQS were 1.59, 2.13 and 2.61 respectively. These scores were significantly higher in videos that were uploaded by physicians/nonprofit physicians/professional organisations/universities (p < 0.001). There is a positive correlation between the video length, DISCERN, JAMA scores and GQS. YouTube is a widely used source of information and advice about testicular cancer, but much of the content is of poor quality.', 'corpus_id': 234782484, 'score': 1}]
83	"{'doc_id': '145505040', 'title': 'A Conceptual Framework for Teacher Professional Development: The Whole Teacher Approach', 'abstract': ""In this article, we describe a conceptual framework for in-service professional development—the Whole Teacher approach. A significant departure from the traditional approach to professional development that speaks primarily to teachers’ acquisition of knowledge and skills, the Whole Teacher framework emphasizes promoting all aspects of a teacher's development, including attitudes, knowledge, and practice. Putting the framework in operation, we describe a project proven to be effective in helping to develop teachers’ competence and increase children's performance in early mathematics. We focus on how the Whole Teacher framework guided the project's design, implementation, and program evaluation. The article concludes with a discussion regarding the significance of the Whole Teacher approach to teacher professional development."", 'corpus_id': 145505040}"	6838	"[{'doc_id': '55841846', 'title': 'Teacher professional development in Teaching and Teacher Education over ten years', 'abstract': 'Abstract A review of publications in Teaching and Teacher Education over ten years (2000–2010) on teacher professional development is the subject of the paper. The first part synthesises production referred to learning, facilitation and collaboration, factors influencing professional development, effectiveness of professional development and issues around the themes. The second part, selects from the production nine articles for closer examination. The paper concludes by noting how the production brings out the complexities of teacher professional learning and how research and development have taken cognisance of these factors and provided food for optimism about their effects, although not yet about their sustainability in time.', 'corpus_id': 55841846, 'score': 1}, {'doc_id': '220425555', 'title': 'GUIDE for a blended learning system', 'abstract': ""This guide is proposed as an operational instrument for CONFRASIE member universities (Regional Rectors' Conference of AUF member institutions in Pacific-Asia) in their projects to set up a blended learning system for bachelor's, Master's and Doctorate degrees. It is structured in sections corresponding to a complete process of operationalizing a blended learning system, from the definition of an implementation strategy to the assessment of results. This guide covers also conceptual and theoretical fundamentals of distance learning as well as methodological and procedural tips and recommendations on how to implement blended learning in an existing face-to-face curriculum. It can serve for leaders of educational ICT-based projects as a guidance document to take pedagogical, technological and methodological decisions for the development, monitoring and assessment of a blended learning curricula. This guide can be augmented by other standards, tool and software manuals offering further training materials and guidelines on educational skills ans=d services."", 'corpus_id': 220425555, 'score': 0}, {'doc_id': '145737302', 'title': 'Professional Development and Teacher Learning: Mapping the Terrain', 'abstract': 'Teacher professional development is essential to efforts to improve our schools. This article maps the terrain of research on this important topic. It first provides an overview of what we have learned as a field, about effective professional development programs and their impact on teacher learning. It then suggests some important directions and strategies for extending our knowledge into new territory of questions not yet explored.', 'corpus_id': 145737302, 'score': 1}, {'doc_id': '220117783', 'title': 'Engaging Pre-service Teachers in an Online STEM Fair during COVID-19', 'abstract': 'COVID-19 changed not only the way we live but also the way we teach and prepare pre-service teachers. The purpose of this paper is to describe the shift from a conventional STEM fair to an online teaching environment prepared by elementary pre-service teachers after the closure of the educational system in Cyprus as of March 2020. We describe a 3-steps process followed for the design, development, and delivery of the STEM fair concerning the technological equipment used, the activities’ adaptations made by the pre-service teachers’, and the level of readiness to teach online based on the initial outcomes of the STEM fair. The brief paper concludes with practical implications and suggestions for future attempts about online teacher education.', 'corpus_id': 220117783, 'score': 0}, {'doc_id': '220360431', 'title': 'A Study on the Construction of College English Structured and Semi-structured Flipped Classroom Teaching Community', 'abstract': 'This paper is to discuss how to construct a structured and semi-structured flipped classroom teaching for college English. The structured classroom is the traditional classroom and the semi-structured classroom is partly traditional teaching and partly flipped classroom teaching. Based on humanistic learning theory, mastery learning theory, team learning theory, and constructivism learning theory, we assume that the design principle of the structured and semi-structured flipped classroom teaching model community follows the teacherled, student-oriented principle, immersive principle, heuristic principle and inquiry principle; The basic elements of structured and semi-structured flipped classroom teaching community including teachers, students and teaching platforms; the implementation strategies of the structured flipped classroom is suitable for the cultivation of students’ English abilities, while the semi-structured flipped classroom teaching is suitable for the explanation of pronunciation, grammar, reading, writing and speaking skills and methods in college English teaching. And its evaluation method should be embodied in the whole process of teaching interaction. Keywords—flipped classroom; community; construction', 'corpus_id': 220360431, 'score': 0}, {'doc_id': '147080197', 'title': 'Informal online communities and networks as a source of teacher professional development: A review', 'abstract': 'Abstract Informal online communities and networks offer teachers the possibility of voluntarily engaging in shared learning, reflecting about teaching practice and receiving emotional support. Bottom-up online communities and networks are an important source of professional development, although research around these social learning structures mainly consists in describing particular cases using a wide diversity of theoretical and methodological approaches. This review analyses the existing theoretical frameworks and methodological approaches, the main characteristics and practices of online communities and networks, as well as their principal repercussions in teacher professional development. A critical analysis of the emergent themes in the revised articles sheds light on eligible perspectives for further research.', 'corpus_id': 147080197, 'score': 1}, {'doc_id': '22923798', 'title': 'Extending experiential learning in teacher professional development', 'abstract': 'This paper introduces the use of experiential learning during the early stages of teacher professional development. Teachers observe student outcomes from the very beginning of the process and experience new pedagogical approaches as learners themselves before adapting and implementing them in their own classrooms. This research explores the implementation of this approach with teachers in Irish second level schools who are being asked to make significant pedagogic changes as part of a major curriculum reform. Teachers’ self-reflections, observations and interviews demonstrate how the process and outcomes influenced their beliefs, resulting in meaningful changes in classroom practice.', 'corpus_id': 22923798, 'score': 1}, {'doc_id': '218478436', 'title': 'Research on College English Teaching Model of Online Live Classes', 'abstract': 'The rapid development of information technology has provided more method choices for the reform of college English teaching. At the same time, due to the impact of the Novel Coronavirus in the spring semester of 2020, nationwide online live teaching has greatly promoted the form of “Internet + education”. Based on the teaching practice of the author and other teachers in the same college, this paper analyses in details the common problems in the current college English live classes, proposes some suggestions for improvement, and finally discusses how to build college English “online and offline” blended teaching model based on online live courses in the future.', 'corpus_id': 218478436, 'score': 0}, {'doc_id': '200059950', 'title': 'Teacher’s Professional Development', 'abstract': 'Globalization, rapid technological developments, and transformation of social environments call for many facets of education to keep pace with changes and play a catalytic role to equip students with global competencies. Innovations in curriculum and pedagogy, changing trend and profile of learners, diversity in the classroom and new cultural phenomena make it imperative for teachers to function differently and take on new roles as designers of learning. How can teachers’ content knowledge, pedagogical knowledge, or their understanding of learners be enhanced? What are the best practices to ensure teacher development as a continuum development, to have a knowledge-rich profession, and to strengthen teacher skillfulness? How do we encourage the values of professionalism, integrity, and excellence? How do best-performing systems encourage professional learning networks and learning circles, structure and facilitate mentoring and facilitate selfdirected learning opportunities? In the light of education for sustainable development, it is imperative that teachers consistently and continuously keep up-to-date with new knowledge, skills, and teaching practices. This keynote will attempt to address these issues and how we can enhance the professional development of teachers to meet the 21-century challenges. Keywords—teacher professional development, professionalism, professional learning.', 'corpus_id': 200059950, 'score': 1}, {'doc_id': '220634463', 'title': 'Online Teaching in Medical Training: Establishing Good Online Teaching Practices from Cumulative Experience', 'abstract': 'Online teaching has the potential to transcend geographical boundaries, is flexible, learner centered and can help students develop self-directed learning skills. The recently introduced competency-based curriculum has also advocated e-learning as an indispensable tool for self-directed learning. For effective online learning, good online teaching practices should be adopted. These include alignment of online teaching and learning with delivery of curriculum and objectives, synchronous, and asynchronous interaction between teacher and student, encouraging the development of higher-order thinking skills, active learning, and self-directed learning in students. In addition, good online teaching practices should have an inbuilt component of feedback and provide for effective time management, respect for diverse talents and ways of learning with continuous monitoring and mentoring of the learners. Online assessments, both formative and summative should also aim to ensure student involvement in the process. Capacity building of faculty through faculty development programs for the development of specific competencies such as social competency, pedagogical competency, managerial competency, and technical competency in the times of COVID-19 is now recognized as the need of the hour. Although online teaching and learning in medical education is new, it has the potential to become mainstream in future.', 'corpus_id': 220634463, 'score': 0}]"
84	{'doc_id': '137319048', 'title': 'The effect of temperature on the hardness of polycrystalline cubic boron nitride cutting tool materials', 'abstract': 'Polycrystalline cubic boron nitride (PcBN) cutting tools are used for the machining of ferrous alloys at high speeds and temperatures where the use of diamond is precluded due to graphitisation. To accurately predict the life of these tool materials it is necessary to understand inter alia the deformation and relevant mechanisms that occur at the temperatures and pressures associated with cutting. This paper uses Vickers indentation as a means of assessing the role of cBN content, binder phase and cBN grain size on the mechanical properties of a number of polycrystalline cubic boron nitride materials. It will be shown that as experimental temperatures increase, a change in deformation mechanism occurs in the tool materials, confirming that the indentation method is useful in the identification of such changes.', 'corpus_id': 137319048}	14676	[{'doc_id': '231572986', 'title': 'Processing of Nanostructured Bulk Fe-Cr Alloys by Severe Plastic Deformation', 'abstract': 'The processing of binary alloys consisting of ferromagnetic Fe and antiferromagnetic Cr by severe plastic deformation (SPD) with different chemical compositions has been investigated. Although the phase diagram exhibits a large gap in the thermodynamical equilibrium at lower temperatures, it is shown that techniques based on SPD help to overcome common processing limits. Different processing routes including initial ball milling (BM) and arc melting (AM) and a concatenation with annealing treatments prior to high-pressure torsion (HPT) deformation are compared in this work. Investigation of the deformed microstructures by electron microscopy and synchrotron X-ray diffraction reveal homogeneous, nanocrystalline microstructures for HPT deformed AM alloys. HPT deformation of powder blends and BM powders leads to an exorbitant increase in hardness or an unusual fast formation of a σ-phase and therefore impede successful processing.', 'corpus_id': 231572986, 'score': 0}, {'doc_id': '137138494', 'title': 'Application of PCBN tools in the machining of iron-based hard-facings', 'abstract': 'In this work, the performance of a PCBN tool when turning a welded hard-facing material is reported in comparison with that of alternative materials, such as whisker-reinforced alumina in field trials. The chip formation process and tool wear modes were studied and suitable machining parameters were determined by cutting tests. Chipping of the cutting edge and flaking of the rake face were found to be the predominant wear modes of the cutting tools. Finally, the machinability of the hard-facing material and the requirements of the tool material are discussed.', 'corpus_id': 137138494, 'score': 1}, {'doc_id': '231979420', 'title': 'Spinodal Decomposition Stabilizes Plastic Flow in a Nanocrystalline Cu-Ti Alloy', 'abstract': 'A combination of high strength and reasonable ductility has been achieved in a copper-1.7 at.%titanium alloy deformed by high-pressure torsion. Grain refinement and a spinodal microstructure provided a hardness of 254 ± 2 H<i>v</i>, yield strength of 800 MPa and elongation of 10%. The spinodal structure persisted during isothermal ageing, further increasing the yield strength to 890MPa while retaining an elongation of 7%. This work demonstrates the potential for spinodal microstructures to overcome the difficulties in retaining ductility in ultra-fine grained or nanocrystalline alloys, especially upon post- deformation heating where strain softening normally results in brittle behavior.', 'corpus_id': 231979420, 'score': 0}, {'doc_id': '218966045', 'title': 'Influence of the cutting direction angle on the tool wear behavior in face plunge grinding of PcBN', 'abstract': 'Abstract Polycrystalline cubic boron nitride (PcBN) is a highly wear resistant material. Due to its high hardness this material is typically machined with diamond grinding tools. The high hardness and high-temperature hardness of PcBN leads to a significant grinding tool wear. The applied cutting direction angle during face plunge grinding offers the possibility to influence the geometry of the contact area between the grinding tool and the PcBN workpiece. However, the underlying principal mechanisms and influences of parameters are not fully understood today. The contact zone geometry is described by the width of cut and the geometric contact length. The paper provides a mathematical description of these two parameters for S-shapes PcBN cutting inserts depending on the workpiece geometry and the cutting direction angle. It is shown that the contact length significantly determines the wear mechanism.', 'corpus_id': 218966045, 'score': 1}, {'doc_id': '139153043', 'title': 'Investigation of the mechanical properties and cutting performance of cBN-based cutting tools with Cr3C2 binder phase.', 'abstract': 'In order to investigate new materials for metal cutting applications, cubic boron nitride, cutting tools with different amounts of binder phase were sintered in HPHT toroid type apparatus under 7.7 GPa and in temperature range of 1450-2450°C. Initial mixtures of three composition were chosen with 50, 60, 65 vol. % of cBN, 5 vol. % of Al was added to mixture to prevent oxidation. Phase composition, microstructure, elastic properties, hardness, fracture toughness and cutting performance were investigated. The highest value of the mechanical properties and tool life demonstrated samples sintered in temperature range 1850-2150 °C.', 'corpus_id': 139153043, 'score': 1}, {'doc_id': '231583230', 'title': 'Micropillar compression of single crystal tungsten carbide, Part 1: temperature and orientation dependence of deformation behaviour', 'abstract': 'Tungsten carbide cobalt hardmetals are commonly used as cutting tools subject to high operation temperature and pressures, where the mechanical performance of the tungsten carbide phase affects the wear and lifetime of the material. In this study, the mechanical behaviour of the isolated tungsten carbide (WC) phase was investigated using single crystal micropillar compression. Micropillars 1-5 μm in diameter, in two crystal orientations, were fabricated using focused ion beam (FIB) machining and subsequently compressed between room temperature and 600 °C. The activated plastic deformation mechanisms were strongly anisotropic and weakly temperature dependent. The flow stresses of basal-oriented pillars were about three times higher than the prismatic pillars, and pillars of both orientations soften slightly with increasing temperature. The basal pillars tended to deform by either unstable cracking or unstable yield, whereas the prismatic pillars deformed by slip-mediated cracking. However, the active deformation mechanisms were also sensitive to pillar size and shape. Slip trace analysis of the deformed pillars showed that {101̅0} prismatic planes were the dominant slip plane in WC. Basal slip was also identified as a secondary slip system, activated at high temperatures.', 'corpus_id': 231583230, 'score': 0}, {'doc_id': '137785354', 'title': 'Mechanical properties and cutting performance of cBN -TiN composites sintered using HPHT technique', 'abstract': 'Summary Cubic boron nitride (cBN) – based ceramic composites in different modifications are still promising materials for high performance cutting applications [1-4]. CBN-TiN composites with different cBN volume ratios were formed by high pressure – high temperature sintering (HPHT) with the application of Bridgman anvils cell. From the microscopic observation it could be concluded that the samples exhibited a poreless and dense structure. Mechanical investigations of BN - TiN composites were performed using a hardness test and an ultrasonic method based on the measurement of the velocity of ultrasonic waves transition through the samples. Young’s modulus and the hardness of the investigated composites strictly depended on the cBN contents and on the synthesis parameters. Characteristic, optimum conditions which gave good mechanical properties for composites with different cBN ratios were determined. Cutting test conducted on tools made of selected cBN-TiN composites showed that a composite with volume ratio of cBN equal 65% exhibited the best cutting performance.', 'corpus_id': 137785354, 'score': 1}, {'doc_id': '231835816', 'title': 'Influence of the Silver Content on Mechanical Properties of Ti-Cu-Ag Thin Films', 'abstract': 'In this work, the ternary titanium, copper, and silver (Ti-Cu-Ag) system is investigated as a potential candidate for the production of mechanically robust biomedical thin films. The coatings are produced by physical vapor deposition—magnetron sputtering (MS-PVD). The composite thin films are deposited on a silicon (100) substrate. The ratio between Ti and Cu was approximately kept one, with the variation of the Ag content between 10 and 35 at.%, while the power on the targets is changed during each deposition to get the desired Ag content. Thin film characterization is performed by X-ray diffraction (XRD), nanoindentation (modulus and hardness), to quantitatively evaluate the scratch adhesion, and atomic force microscopy to determine the surface topography. The residual stresses are measured by focused ion beam and digital image correlation method (FIB-DIC). The produced Ti-Cu-Ag thin films appear to be smooth, uniformly thick, and exhibit amorphous structure for the Ag contents lower than 25 at.%, with a transition to partially crystalline structure for higher Ag concentrations. The Ti-Cu control film shows higher values of 124.5 GPa and 7.85 GPa for modulus and hardness, respectively. There is a clear trend of continuous decrease in the modulus and hardness with the increase of Ag content, as lowest value of 105.5 GPa and 6 GPa for 35 at.% Ag containing thin films. In particular, a transition from the compressive (−36.5 MPa) to tensile residual stresses between 229 MPa and 288 MPa are observed with an increasing Ag content. The obtained results suggest that the Ag concentration should not exceed 25 at.%, in order to avoid an excessive reduction of the modulus and hardness with maintaining (at the same time) the potential for an increase of the antibacterial properties. In summary, Ti-Cu-Ag thin films shows characteristic mechanical properties that can be used to improve the properties of biomedical implants such as Ti-alloys and stainless steel.', 'corpus_id': 231835816, 'score': 0}, {'doc_id': '135822228', 'title': 'Wear mechanisms of several cutting tool materials in hard turning of high carbon–chromium tool steel', 'abstract': 'Abstract The present study illustrates the performance of three different cutting tool materials, namely: PCBN, TiN coated PCBN, and mixed aluminum ceramic (Al 2 O 3 +TiC) in the turning of medium hardened D2 tool steel (52 HRC). Formation of Cr–O tribofilms on the ceramic tool surface as a result of interaction with the workpiece material and environment (identified by X-ray Photoelectron Spectroscopy) leads to improvement of lubricating properties at the tool/chip interface. Obtained results revealed that the mixed alumina ceramic tool can outperform both types of PCBN under different machinability criteria.', 'corpus_id': 135822228, 'score': 1}, {'doc_id': '232046311', 'title': 'High-throughput nanoindentation mapping of cast IN718 nickel-based superalloys: influence of the Nb concentration', 'abstract': 'A high-throughput correlative study of the local mechanical properties, chemical composition and crystallographic orientation has been carried out in selected areas of cast Inconel 718 specimens subjected to three different tempers. The specimens showed a strong Nb segregation at the scale of the dendrite arms, with local Nb contents that varied between 2 wt.% in the core of the dendrite arms to 8 wt.% in the interdendritic regions and 25 wt.% within the second phase particles (MC carbides, Laves phases and δ phase needles). The nanohardness was found to correlate strongly with the local Nb content and the temper condition. On the contrary, the indentation elastic moduli was not influenced by the local chemical composition or temper condition, but directly correlated with the crystallographic grain orientation, due to the high elastic anisotropy of nickel alloys.', 'corpus_id': 232046311, 'score': 0}]
85	"{'doc_id': '233004592', 'title': 'Weekly Bayesian Modelling Strategy to Predict Deaths by COVID-19: a Model and Case Study for the State of Santa Catarina, Brazil', 'abstract': 'Background: The novel coronavirus pandemic has affected Brazil\'s Santa Catarina State (SC) severely. At the time of writing (24 March 2021), over 764,000 cases and over 9,800 deaths by COVID-19 have been confirmed, hospitals were fully occupied with local news reporting at least 397 people in the waiting list for an ICU bed. In an attempt to better inform local policy making, we applied an existing Bayesian algorithm to model the spread of the pandemic in the seven geographic macro-regions of the state. Here we propose changes to extend the model and improve its forecasting capabilities. \nMethods: Our four proposed variations of the original method allow accessing data of daily reported infections and take into account under-reporting of cases more explicitly. Two of the proposed versions also attempt to model the delay in test reporting. We simulated weekly forecasting of deaths from the period from 31/05/2020 until 31/01/2021. First week data were used as a cold-start to the algorithm, after which weekly calibrations of the model were able to converge in fewer iterations. Google Mobility data were used as covariates to the model, as well as to estimate of the susceptible population at each simulated run. \nFindings: The changes made the model significantly less reactive and more rapid in adapting to scenarios after a peak in deaths is observed. Assuming that the cases are under-reported greatly benefited the model in its stability, and modelling retroactively-added data (due to the ""hot"" nature of the data used) had a negligible impact in performance. \nInterpretation: Although not as reliable as death statistics, case statistics, when modelled in conjunction with an overestimate parameter, provide a good alternative for improving the forecasting of models, especially in long-range predictions and after the peak of an infection wave.', 'corpus_id': 233004592}"	19008	"[{'doc_id': '235355434', 'title': 'A comparison of five epidemiological models for transmission of SARS-CoV-2 in India', 'abstract': 'Background Many popular disease transmission models have helped nations respond to the COVID-19 pandemic by informing decisions about pandemic planning, resource allocation, implementation of social distancing measures, lockdowns, and other non-pharmaceutical interventions. We study how five epidemiological models forecast and assess the course of the pandemic in India: a baseline curve-fitting model, an extended SIR (eSIR) model, two extended SEIR (SAPHIRE and SEIR-fansy) models, and a semi-mechanistic Bayesian hierarchical model (ICM). Methods Using COVID-19 case-recovery-death count data reported in India from March 15 to October 15 to train the models, we generate predictions from each of the five models from October 16 to December 31. To compare prediction accuracy with respect to reported cumulative and active case counts and reported cumulative death counts, we compute the symmetric mean absolute prediction error (SMAPE) for each of the five models. For reported cumulative cases and deaths, we compute Pearson’s and Lin’s correlation coefficients to investigate how well the projected and observed reported counts agree. We also present underreporting factors when available, and comment on uncertainty of projections from each model. Results For active case counts, SMAPE values are 35.14% (SEIR-fansy) and 37.96% (eSIR). For cumulative case counts, SMAPE values are 6.89% (baseline), 6.59% (eSIR), 2.25% (SAPHIRE) and 2.29% (SEIR-fansy). For cumulative death counts, the SMAPE values are 4.74% (SEIR-fansy), 8.94% (eSIR) and 0.77% (ICM). Three models (SAPHIRE, SEIR-fansy and ICM) return total (sum of reported and unreported) cumulative case counts as well. We compute underreporting factors as of October 31 and note that for cumulative cases, the SEIR-fansy model yields an underreporting factor of 7.25 and ICM model yields 4.54 for the same quantity. For total (sum of reported and unreported) cumulative deaths the SEIR-fansy model reports an underreporting factor of 2.97. On October 31, we observe 8.18 million cumulative reported cases, while the projections (in millions) from the baseline model are 8.71 (95% credible interval: 8.63–8.80), while eSIR yields 8.35 (7.19–9.60), SAPHIRE returns 8.17 (7.90–8.52) and SEIR-fansy projects 8.51 (8.18–8.85) million cases. Cumulative case projections from the eSIR model have the highest uncertainty in terms of width of 95% credible intervals, followed by those from SAPHIRE, the baseline model and finally SEIR-fansy. Conclusions In this comparative paper, we describe five different models used to study the transmission dynamics of the SARS-Cov-2 virus in India. While simulation studies are the only gold standard way to compare the accuracy of the models, here we were uniquely poised to compare the projected case-counts against observed data on a test period. The largest variability across models is observed in predicting the “total” number of infections including reported and unreported cases (on which we have no validation data). The degree of under-reporting has been a major concern in India and is characterized in this report. Overall, the SEIR-fansy model appeared to be a good choice with publicly available R-package and desired flexibility plus accuracy.', 'corpus_id': 235355434, 'score': 0}, {'doc_id': '44123454', 'title': 'Modelling the skip-and-resurgence of Japanese encephalitis epidemics in Hong Kong', 'abstract': '\n Abstract\n \n Japanese encephalitis virus (JEV) is a zoonotic mosquito-borne virus, persisting in pigs, Ardeid birds and Culex mosquitoes. It is endemic to China and Southeastern Asia. The case-fatality ratio (CFR) or the rate of permanent psychiatric sequelae is 30% among symptomatic patients. There were no reported local JEV human cases between 2006 and 2010 in Hong Kong, but it was followed by a resurgence of cases from 2011 to 2017. The mechanism behind this “skip-and-resurgence” patterns is unclear.\n This work aims to reveal the mechanism behind the “skip-and-resurgence” patterns using mathematical modelling and likelihood-based inference techniques. We found that pig-to-pig transmission increases the size of JEV epidemics but is unlikely to maintain the same level of transmission among pigs. The disappearance of JEV human cases in 2006–2010 could be explained by a sudden reduction of the population of farm pigs as a result of the implementation of the voluntary “pig-rearing licence surrendering” policy. The resurgence could be explained by of a new strain in 2011, which increased the transmissibility of the virus or the spill-over ratio from reservoir to host or both.\n \n', 'corpus_id': 44123454, 'score': 1}, {'doc_id': '235766676', 'title': 'Cross-sectional cycle threshold values reflect epidemic dynamics of COVID-19 in Madagascar', 'abstract': 'As the national reference laboratory for febrile illness in Madagascar, we processed samples from the first epidemic wave of COVID-19, between March and September 2020. We fit generalized additive models to cycle threshold (Ct) value data from our RT-qPCR platform, demonstrating a peak in high viral load, low-Ct value infections temporally coincident with peak epidemic growth rates estimated in real time from publicly-reported incidence data and retrospectively from our own laboratory testing data across three administrative regions. We additionally demonstrate a statistically significant effect of duration of time since infection onset on Ct value, suggesting that Ct value can be used as a biomarker of the stage at which an individual is sampled in the course of an infection trajectory. As an extension, the population level Ct distribution at a given timepoint can be used to estimate population-level epidemiological dynamics. We illustrate this concept by adopting a recently-developed, nested modeling approach, embedding a within-host viral kinetics model within a population-level Susceptible-Exposed-Infectious-Recovered (SEIR) framework, to mechanistically estimate epidemic growth rates from cross-sectional Ct distributions across three regions in Madagascar. We find that Ct-derived epidemic growth estimates slightly precede those derived from incidence data across the first epidemic wave, suggesting delays in surveillance and case reporting. Our findings indicate that public reporting of Ct values could offer an important resource for epidemiological inference in low surveillance settings, enabling forecasts of impending incidence peaks in regions with limited case reporting.', 'corpus_id': 235766676, 'score': 1}, {'doc_id': '235635730', 'title': 'Characterizing two outbreak waves of COVID-19 in Spain using phenomenological epidemic modelling', 'abstract': 'Since the first case reported of SARS-CoV-2 the end of December 2019 in China, the number of cases quickly climbed following an exponential growth trend, demonstrating that a global pandemic is possible. As of December 3, 2020, the total number of cases reported are around 65,527,000 contagions worldwide, and 1,524,000 deaths affecting 218 countries and territories. In this scenario, Spain is one of the countries that has suffered in a hard way, the ongoing epidemic caused by the novel coronavirus SARS-CoV-2, namely COVID-19 disease. In this paper, we present the utilization of phenomenological epidemic models to characterize the two first outbreak waves of COVID-19 in Spain. The study is driven using a two-step phenomenological epidemic approach. First, we use a simple generalized growth model to fit the main parameters at the early epidemic phase; later, we apply our previous finding over a logistic growth model to that characterize both waves completely. The results show that even in the absence of accurate data series, it is possible to characterize the curves of case incidence, and construct a short-term forecast of 60 days in the near time horizon, in relation to the expected total duration of the pandemic.', 'corpus_id': 235635730, 'score': 1}, {'doc_id': '79256713', 'title': 'HUBUNGAN USIA, GRAVIDA, DAN RIWAYAT HIPERTENSI DENGAN KEJADIAN KEHAMILAN PREEKLAMSIA DI RSUD WONOSARI TAHUN 2015', 'abstract': 'INTISARI \nLatar Belakang: Penyebab tertinggi ke-2 angka morbiditas dan mortalitas maternal adalah kehamilan preeklamsia. Dinas Kesehatan Daerah Istimewa Yogyakarta (DIY), tahun 2013 jumlah kematian ibu sebanyak 46 kasus. Angka kematian ibu dilaporkan sebesar 101 per 100.000 kelahiran hidup. Hasil Audit Maternal (AMP) menyimpulkan bahwa penyebab kematian ibu pada tahun 2013 adalah Preeklamsia Berat (PEB) sebanyak 28%. Studi pendahuluan yang dilakukan di RSUD Wonosari menyatakan pada tahun 2015 terdapat 106 ibu hamil dengan preeklamsia dari 1.080 kehamilan. \nTujuan: Mengetahui hubungan usia, gravida dan riwayat hipertensi dengan kejadian kehamilan preeklamsia di RSUD Wonosari Gunung Kidul Tahun 2015. \nMetode Penelitian: Menggunakan metode survey analitik dengan pendekatan case control. Analisis data penelitian menggunakan Chi Square dan Regresi Logistik. \nHasil: Penelitian terdapat hubungan usia dengan kejadian preeklamsia dengan nilai p-value 0.046, sedangkan hubungan gravida dengan kejadian preeklamsia tidak terdapat hubungan karena p-value sebesar 0.213 dan terdapat hubungan antara riwayat hipertensi dengan kejadian preeklamsia dengan nilai p-value 0.000. Hasil dari uji regresi logistik OR 6,22 menunjukkan hubungan riwayat hipertensi dengan kejadian kehamilan preeklamsia paling berisiko dibandingkan dengan usia ibu dan gravida. \nSimpulan dan Saran: Penelitian menunjukkan adanya hubungan usia dan riwayat hipertensi dengan kejadian preeklamsia, sedangkan gravida tidak terdapat hubungan dengan kejadian preeklamsia. Didapatkan faktor yang paling berpengaruh menjadi risiko terjadinya preeklamsia yaitu riwayat hipertensi sebesar 6,22 kali dibandingkan usia ibu dan gravida. Mengidentifikasi faktor- faktor yang mempengaruhi kejadian preeklamsia melalui konseling yang efektif dan peningkatan layanan ANC. \nKata Kunci : Preeklamsia, Fakor Risiko, Usia, Gravida, Riwayat Hipertensi', 'corpus_id': 79256713, 'score': 0}, {'doc_id': '201125081', 'title': 'A novel sub-epidemic modeling framework for short-term forecasting epidemic waves', 'abstract': 'BackgroundSimple phenomenological growth models can be useful for estimating transmission parameters and forecasting epidemic trajectories. However, most existing phenomenological growth models only support single-peak outbreak dynamics whereas real epidemics often display more complex transmission trajectories.MethodsWe develop and apply a novel sub-epidemic modeling framework that supports a diversity of epidemic trajectories including stable incidence patterns with sustained or damped oscillations to better understand and forecast epidemic outbreaks. We describe how to forecast an epidemic based on the premise that the observed coarse-scale incidence can be decomposed into overlapping sub-epidemics at finer scales. We evaluate our modeling framework using three outbreak datasets: Severe Acute Respiratory Syndrome (SARS) in Singapore, plague in Madagascar, and the ongoing Ebola outbreak in the Democratic Republic of Congo (DRC) and four performance metrics.ResultsThe sub-epidemic wave model outperforms simpler growth models in short-term forecasts based on performance metrics that account for the uncertainty of the predictions namely the mean interval score (MIS) and the coverage of the 95% prediction interval. For example, we demonstrate how the sub-epidemic wave model successfully captures the 2-peak pattern of the SARS outbreak in Singapore. Moreover, in short-term sequential forecasts, the sub-epidemic model was able to forecast the second surge in case incidence for this outbreak, which was not possible using the simple growth models. Furthermore, our findings support the view that the national incidence curve of the Ebola epidemic in DRC follows a stable incidence pattern with periodic behavior that can be decomposed into overlapping sub-epidemics.ConclusionsOur findings highlight how overlapping sub-epidemics can capture complex epidemic dynamics, including oscillatory behavior in the trajectory of the epidemic wave. This observation has significant implications for interpreting apparent noise in incidence data where the oscillations could be dismissed as a result of overdispersion, rather than an intrinsic part of the epidemic dynamics. Unless the oscillations are appropriately modeled, they could also give a false positive, or negative, impression of the impact from public health interventions. These preliminary results using sub-epidemic models can help guide future efforts to better understand the heterogenous spatial and social factors shaping sub-epidemic patterns for other infectious diseases.', 'corpus_id': 201125081, 'score': 1}, {'doc_id': '235249145', 'title': 'Projecting the criticality of COVID-19 transmission in India using GIS and machine learning methods', 'abstract': '\n There is a new public health catastrophe forbidding the world. With the advent and spread of 2019 novel coronavirus (2019-nCoV). Learning from the experiences of various countries and the World Health Organization (WHO) guidelines, social distancing, use of sanitizers, thermal screening, quarantining, and provision of lockdown in the cities being the effective measure that can contain the spread of the pandemic. Though complete lockdown helps in containing the spread, it generates complexity by breaking the economic activity chain. Besides, laborers, farmers, and workers may lose their daily earnings. Owing to these detrimental effects, the government has to open the lockdown strategically. Prediction of the COVID-19 spread and analyzing when the cases would stop increasing helps in developing a strategy. An attempt is made in this paper to predict the time after which the number of new cases stops rising, considering the strong implementation of lockdown conditions using three different techniques such as Decision Tree, Support Vector Machine, and Gaussian Process Regression algorithm are used to project the number of cases. Thus, the projections are used in identifying inflection points, which would help in planning the easing of lockdown in a few of the areas strategically. The criticality in a region is evaluated using the criticality index (CI), which is proposed by authors in one of the past of research works. This research work is made available in a dashboard to enable the decision-makers to combat the pandemic.\n', 'corpus_id': 235249145, 'score': 0}, {'doc_id': '235440951', 'title': 'Modelling the impact of travel restrictions on COVID-19 cases in Newfoundland and Labrador', 'abstract': 'In many jurisdictions, public health authorities have implemented travel restrictions to reduce coronavirus disease 2019 (COVID-19) spread. Policies that restrict travel within countries have been implemented, but the impact of these restrictions is not well known. On 4 May 2020, Newfoundland and Labrador (NL) implemented travel restrictions such that non-residents required exemptions to enter the province. We fit a stochastic epidemic model to data describing the number of active COVID-19 cases in NL from 14 March to 26 June. We predicted possible outbreaks over nine weeks, with and without the travel restrictions, and for contact rates 40–70% of pre-pandemic levels. Our results suggest that the travel restrictions reduced the mean number of clinical COVID-19 cases in NL by 92%. Furthermore, without the travel restrictions there is a substantial risk of very large outbreaks. Using epidemic modelling, we show how the NL COVID-19 outbreak could have unfolded had the travel restrictions not been implemented. Both physical distancing and travel restrictions affect the local dynamics of the epidemic. Our modelling shows that the travel restrictions are a plausible reason for the few reported COVID-19 cases in NL after 4 May.', 'corpus_id': 235440951, 'score': 0}, {'doc_id': '12284246', 'title': 'The Robot Head ""Flobi"": A Research Platform for Cognitive Interaction Technology', 'abstract': 'Founded on a vision of a human-friendly technology that \nadapts to users’ needs and is easy und intuitive for ordinary people to use, CITEC has established an exciting new field: Cognitive Interaction Technology. It aims to elucidate the principles and mechanisms of cognition in order to find ways of replicating them in technology and \nthus enable a new deep level of service and assistance. In order to proceed in this highly interdisciplinary field, appropriate research platforms and infrastructure are needed. The anthropomorphic robot head “Flobi” combines state-of-the-art sensing functionality with an exterior that elicits a sympathetic emotion response. In order to support several lines of research and at the same time ensure the maintainability of the software and hardware components, a virtual realization of the Flobi head has \nbeen proposed that allows an efficient prototyping, systematic testing, and software development in a continuous integration framework.', 'corpus_id': 12284246, 'score': 0}, {'doc_id': '220334247', 'title': 'Sub-epidemic model forecasts for COVID-19 pandemic spread in the USA and European hotspots, February-May 2020', 'abstract': 'Mathematical models have been widely used to understand the dynamics of the ongoing coronavirus disease 2019 (COVID-19) pandemic as well as to predict future trends and assess intervention strategies. The asynchronicity of infection patterns during this pandemic illustrates the need for models that can capture dynamics beyond a single-peak trajectory to forecast the worldwide spread and for the spread within nations and within other sub-regions at various geographic scales. Here, we demonstrate a five-parameter sub-epidemic wave modeling framework that provides a simple characterization of unfolding trajectories of COVID-19 epidemics that are progressing across the world at different spatial scales. We calibrate the model to daily reported COVID-19 incidence data to generate six sequential weekly forecasts for five European countries and five hotspot states within the United States. The sub-epidemic approach captures the rise to an initial peak followed by a wide range of post-peak behavior, ranging from a typical decline to a steady incidence level to repeated small waves for sub-epidemic outbreaks. We show that the sub-epidemic model outperforms a three-parameter Richards model, in terms of calibration and forecasting performance, and yields excellent short- and intermediate-term forecasts that are not attainable with other single-peak transmission models of similar complexity. Overall, this approach predicts that a relaxation of social distancing measures would result in continuing sub-epidemics and ongoing endemic transmission. We illustrate how this view of the epidemic could help data scientists and policymakers better understand and predict the underlying transmission dynamics of COVID-19, as early detection of potential sub-epidemics can inform model-based decisions for tighter distancing controls.', 'corpus_id': 220334247, 'score': 1}]"
86	"{'doc_id': '216521128', 'title': 'Extensive reading and viewing as input for academic vocabulary: A large-scale vocabulary profile coverage study of students’ reading and writing across multiple secondary school subjects', 'abstract': ""Abstract The extent to which extensive reading (ER) and extensive viewing (EV) support academic literacy in secondary school, particularly for L2 students, through relevant vocabulary input has yet to be established. This study undertakes a lexical profile coverage study providing information on how much coverage is afforded by ER/EV vocabulary constructs for reading and writing in biology, physics, chemistry, mathematics and English (as a subject area). It operationalizes ER/EV as vocabulary input constructs through corpora representing general fiction, science fiction, juvenile fiction, television and movies. It computes coverage provided by this input at different frequency bands (e.g. ER/EV's 1st 1000 most frequent word families, 2nd 1000 etc.) in secondary school textbooks and student writing. It finds ER/EV input appears very valuable for English as a subject area, and provides within the first 11–12,000 most frequent word families substantial coverage of receptive and productive vocabulary in secondary school science. Science-fiction provides more coverage, and juvenile fiction less. EV does not appear to offer impoverished coverage to ER. The study suggests that additional vocabulary pedagogy is nevertheless going to be needed, to cover from about 1 in 10 to 1 in 20 vocabulary items in the target subject areas."", 'corpus_id': 216521128}"	13916	"[{'doc_id': '230530194', 'title': 'Improving speaking Skill through discussion Debate Strategy of fourth Semester students of English Education Study Program of Nusa Nipa University in the Academic Year of 2019/ 2020', 'abstract': 'The purposes of this research were to improve students’ speaking skill, and to develop discussion debate as a strategy to improve students’ speaking skills. In this research, the researchers used Classroom Action Research (CAR). The procedures used in this research design included planning, acting, observing, and reflecting. In the techniques of data collection, the researcher used observation, field notes, tests, and documentation. While in the techniques of data analysis, the researcher assessed each student’s achievement on the English test using the speaking assessment rubric. The development of discussion debate strategies in this study has increased from first cycle, second cycle,and third cycle. The average score of 6 student response indicators to the implementation of the discussion debate strategy in first cycle was 30,79%, insecond cycle was 71.2% and in third cycle was 88.5%. On the other hand, the average of class percentage which met the requirement of minimum completeness criteria of speaking skill in first cycle was 20,68%, in second cycle was 37,5% and in third cycle was 87,66%. Thus, the implementation of this discussion debate strategy was quite effective in increasing student responses and speaking skill. Keywords— Speaking, Ability,Discussion Debate, Strategy.', 'corpus_id': 230530194, 'score': 0}, {'doc_id': '149592468', 'title': 'Word Knowledge: Exploring the Relationships and Order of Acquisition of Vocabulary Knowledge Components', 'abstract': '\n This study explores the overall nature of the vocabulary knowledge construct by examining the relationships and order of acquisition of multiple word knowledge components. A total of 144 Spanish learners of English were tested on their recognition and recall knowledge of four word knowledge components: the form–meaning link, derivatives, multiple meanings, and collocations. All word knowledge components were strongly intercorrelated, and implicational scaling analyses showed that there is a consistent pattern of acquisition for these components, with recognition knowledge being acquired before recall knowledge across all the different components. Structural equation modelling (SEM) revealed that all components (both recognition and recall) contribute significantly to the global vocabulary construct. However, SEM also suggested that the recognition and recall masteries of any particular word knowledge component must be seen as separate constructs. Overall, findings suggest that the distinction between recognition and recall knowledge is fundamental to the conceptualization of the development of vocabulary knowledge.', 'corpus_id': 149592468, 'score': 1}, {'doc_id': '231838931', 'title': 'SPEAKING SKILL THROUGH TASK BASED LEARNING IN ENGLISH FOREIGN LANGUAGE CLASSROOM', 'abstract': 'Speaking is one way to express ourselves when communicate. Most of students fail when they perform speaking skill. They faced problems in expressing themselves using in accurate, fluent and even simple sentences. Task based learning is an approach that need the completion of meaningful tasks. Task Based Learning focuses on the use of language for genuine in communication. This study used quasi experimental research. The sample of the study is Fourth Semester of English education study program of Baturaja University with the total sample 36 students. The data is conducted of two groups: one experimental group and one control group taught conventionally. Based on the data analysis the researcher concluded that task based learning was effective to improve the students‟ speaking skill in EFL Classroom. The result data from the mean score of posttest in experimental class was 49,11and the mean score of posttest in control class was 51,88 . While, the value of sig. (2-tailed)=0.000 less than significant level (α=0.05), it meant that alternative hypothesis (Ha) was accepted and the null hypothesis (Ho) was rejected. The researcher concluded the result of posttest in experimental higher than the result of posttest in control class. It means that task based learning was significantly improve the students‟ speaking skill at fourth semester of English education study program of Baturaja University. Based on the finding on questionnaire, it could be described that almost the students gave positive response on the use of task based learning. The students strongly agree that task based learning help them enjoy in learning English and they also mostly agree that task based activities is a good way to improve English vocabulary. Article History: Received: October, 2020 Revised: November, 2020 Published: December, 2020', 'corpus_id': 231838931, 'score': 0}, {'doc_id': '204377234', 'title': 'Moving the field of vocabulary assessment forward: The need for more rigorous test development and validation', 'abstract': ""Abstract Recently, a large number of vocabulary tests have been made available to language teachers, testers, and researchers. Unfortunately, most of them have been launched with inadequate validation evidence. The field of language testing has become increasingly more rigorous in the area of test validation, but developers of vocabulary tests have generally not given validation sufficient attention in the past. This paper argues for more rigorous and systematic procedures for test development, starting from a more precise specification of the test's purpose, intended testees and educational context, the particular aspects of vocabulary knowledge which are being measured, and the way in which the test scores should be interpreted. It also calls for greater assessment literacy among vocabulary test developers, and greater support for the end users of the tests, for instance, with the provision of detailed users' manuals. Overall, the authors present what they feel are the minimum requirements for vocabulary test development and validation. They argue that the field should self-police itself more rigorously to ensure that these requirements are met or exceeded, and made explicit for those using vocabulary tests."", 'corpus_id': 204377234, 'score': 1}, {'doc_id': '201374189', 'title': 'Does one academic year make a difference? A longitudinal study into academic vocabulary in foundation-level students’ assessed writing', 'abstract': 'Academic vocabulary is indisputably one of the most important features of academic texts and several studies (e.g., Csomay and Prades 2018; Durrant 2016; Storch and Tapper 2009) have examined the coverage of academic vocabulary in university students’ writing. However, little research has considered the usage of academic vocabulary in foundation-level students’ summative assignments from a longitudinal perspective. This study, conducted at a UK university, seeks to address this gap by applying a corpus-based approach to the investigation of academic vocabulary in assessed academic writing produced by a multilingual group of 30 foundation-level students over one academic year. \nTextual analysis of academic vocabulary was conducted using AntWordProfiler (Anthony 2013) and drawing on the Academic Word List (Coxhead 2000), New Academic Vocabulary List (Gardner and Davies 2013) and New General Service List (Brezina and Gablasova 2015). To complement the textual data, individual semi-structured interviews were conducted with 5 participants to gain insights into the students’ perceptions of the main contributors to the deployment and development of their academic vocabulary. \nThe findings highlight the effect that the topic and writing genre have on the usage of academic expressions in academic writing production as well as the importance of teacher feedback and exposure to academic vocabulary in reading materials. These findings have potentially important pedagogical implications, not only for foundation-level provisions but also for broader EAP contexts catering for a diverse student population.', 'corpus_id': 201374189, 'score': 1}, {'doc_id': '229410777', 'title': ""Improving the Students' Writing Skill in Descriptive Text Through Picture Word Inductive Model (PWIM)"", 'abstract': 'The aim of this research is to improve the students’ writing skill in descriptive text through PWIM. The subject of this research is the students of SMP An-Nuriyyah Bumiayu in the academic year 2019/2020 at class VII D. There are 29 students consisting of 17 males and 12 females. The writers apply classroom action research through online learning because of Covid-19 pandemic, based on Kemmiss’ and Mc. Taggart’s concept: planning, action, observation, reflection and revised plan. There are four techniques in collecting the data, namely observation, test, questionnaire, and documentation. The research is conducted in two cycles. The first cycle consists of two meetings (pre-test) and for the second cycle consists two meetings (post-test). To find out the problem, the writers conduct pre-observation and pre-test. The result shows that the students’ English writing test can be improved in each test. The increase obtained from pre-cycle to cycle 1 is 6,7%, and the increase from cycle 1 to cycle 2 is 22,2%. In pre-cycle to cycle 1 the mean score of the students in pre-test is 63 and post-test is 77. It reaches out the point 72 as the minimum criteria of mastery learning of English lesson. Based on the result, the students’ writing skill can be improved through PWIM in online teaching learning process.', 'corpus_id': 229410777, 'score': 0}, {'doc_id': '214240922', 'title': 'HOW DO DIFFERENT FORMS OF GLOSSING CONTRIBUTE TO L2 VOCABULARY LEARNING FROM READING?', 'abstract': 'Abstract This meta-analysis investigated the overall effects of glossing on L2 vocabulary learning from reading and the influence of potential moderator variables: gloss format (type, language, mode) and text and learner characteristics. A total of 359 effect sizes from 42 studies (N = 3802) meeting the inclusion criteria were meta-analyzed. The results indicated that glossed reading led to significantly greater learning of words (45.3% and 33.4% on immediate and delayed posttests, respectively) than nonglossed reading (26.6% and 19.8%). Multiple-choice glosses were the most effective, and in-text glosses and glossaries were the least effective gloss types. L1 glosses yielded greater learning than L2 glosses. We found no interaction between language (L1, L2) and proficiency (beginner, intermediate, advanced), and no significant difference among modes of glossing (textual, pictorial, auditory). Learning gains were moderated by test formats (recall, recognition, other), comprehension of text, and proficiency.', 'corpus_id': 214240922, 'score': 1}, {'doc_id': '231813494', 'title': ""Students' Roles in Learning English through Mobile Assisted Language Learning (MALL): A Teachers' Beliefs View"", 'abstract': 'This study aims to explore teacher’s beliefs about students’ roles in learning English through MALL and the factors contributing to their beliefs. MALL which stands for Mobile Assisted Language Learning were the main instructional media where there was no such direct physical meeting in school. This media became popular nowadays, despite its emergence in the early 2000s, especially during Covid-19 pandemic which prevents both students and teachers to have direct physical meeting in the classroom. It was important to understand what role should be played by the students which in this research was viewed from teachers’ beliefs perspective. Therefore, the research questions formulated by the researcher were; (1) What are teacher’s beliefs regarding students’ roles in learning English through MALL? (2) What are the factors shaping their beliefs? In terms of methodology employed by the researcher, this research was qualitative with descriptive case study design. The subjects of this research were two teachers of Senior High School in Surakarta, Indonesia who were teaching English through MALL since 2017. The subjects were a Javanese male and Bataknese female who were 37 years old. Both of teachers had master degree from English Education Department. The instrument employed to acquire the data was interviews. The data then were categorized into the students’ roles based on the theories. The result indicated that four students’ roles became something that the teacher held as the truth. Those roles were participant, initiator, performer, and passive receptor. Meanwhile, the factors determining teacher’s beliefs were discovered five factors.', 'corpus_id': 231813494, 'score': 0}, {'doc_id': '230125480', 'title': 'iPads for Cognitive Skills in EFL Primary Classrooms: A Case Study in Saudi Arabia', 'abstract': 'This research study was designed to clarify the effectiveness of innovative technology use in order to develop cognitive skills in Saudi Arabia with particular focus on the use of iPads in English as a Foreign Language (EFL) classes. New technology approaches are continually being implemented in educational environments but there is often lagging analysis as to the effectiveness of these approaches. In the context under review the implementation of iPads represented a significant shift from using paper and pen to using a portable touchpad and digital pen. This qualitative study comprising observations, interviews and focus groups with teachers and students in four primary EFL primary classrooms in Saudi Arabia. It aimed to investigate any links between EFL teaching approaches, revised Bloom’s Taxonomy of thinking skills and the use of iPads. The findings indicated an unevenness in the application of revised Bloom’s Taxonomy in English instruction generally and most iPad teaching practices were represented at lower order thinking levels (Remember, Understand and Apply). Also, flexible use of iPads when teaching-learning EFL represented levels of revised Bloom’s Taxonomy which aligns with specific roles of; teacher (T), teacher-student shared role (TS) and student (S) and plays a part in representing cognitive skills. These findings contribute to tablet devices use in language learning literature by highlighting the ‘how’ of EFL instruction based on revised Bloom’s Taxonomy.', 'corpus_id': 230125480, 'score': 0}, {'doc_id': '210385194', 'title': 'LEARNING VOCABULARY THROUGH READING, LISTENING, AND VIEWING', 'abstract': 'Abstract This study used a pretest-posttest-delayed posttest design at one-week intervals to determine the extent to which written, audio, and audiovisual L2 input contributed to incidental vocabulary learning. Seventy-six university students learning EFL in China were randomly assigned to four groups. Each group was presented with the input from the same television documentary in different modes: reading the printed transcript, listening to the documentary, viewing the documentary, and a nontreatment control condition. Checklist and multiple-choice tests were designed to measure knowledge of target words. The results showed that L2 incidental vocabulary learning occurred through reading, listening, and viewing, and that the gain was retained in all modes of input one week after encountering the input. However, no significant differences were found between the three modes on the posttests indicating that each mode of input yielded similar amounts of vocabulary gain and retention. A significant relationship was found between prior vocabulary knowledge and vocabulary learning, but not between frequency of occurrence and vocabulary learning. The study provides further support for the use of L2 television programs for language learning.', 'corpus_id': 210385194, 'score': 1}]"
87	"{'doc_id': '216144721', 'title': 'Exploring Explainable Selection to Control Abstractive Generation', 'abstract': ""It is a big challenge to model long-range input for document summarization. In this paper, we target using a select and generate paradigm to enhance the capability of selecting explainable contents (i.e., interpret the selection given its semantics, novelty, relevance) and then guiding to control the abstract generation. Specifically, a newly designed pair-wise extractor is proposed to capture the sentence pair interactions and their centrality. Furthermore, the generator is hybrid with the selected content and is jointly integrated with a pointer distribution that is derived from a sentence deployment's attention. The abstract generation can be controlled by an explainable mask matrix that determines to what extent the content can be included in the summary. Encoders are adaptable with both Transformer-based and BERT-based configurations. Overall, both results based on ROUGE metrics and human evaluation gain outperformance over several state-of-the-art models on two benchmark CNN/DailyMail and NYT datasets."", 'corpus_id': 216144721}"	6290	"[{'doc_id': '147742597', 'title': 'The idea of hating anyone becomes a kind of totally bizarre idea: practitioner accounts of Loving Kindness Meditation', 'abstract': 'This talk will present findings from a qualitative piece of research which forms the first part of a wider PhD project. The overarching aim of the project is to understand the effects of Loving Kindness Meditation (LKM), which is a form of meditative practice with a focus on developing feelings of kindness towards the self and others including strangers and ‘enemies’. The aim of the presented research is to explore how LKM is understood and defined by experienced practitioners and the perceived effects of the practice. The rationale for the research is based on the lack of research conducted on LKM, and that which has been done not being consistent in the definition that has been given in addition to the variation in reported outcomes. Due to the specific focus of LKM on developing feelings of kindness to others, the practice could impact on practitioners’ relationships; this has not been widely explored within past research. The talk will present and contextualise the research in relation to the other stages of research and will present findings from an IPA analysis of five interviews with experienced LKM practitioners. Themes include the perceived effects on relationships with the self and other people, and the perceived daily and longer term importance of the practice.', 'corpus_id': 147742597, 'score': 0}, {'doc_id': '138872206', 'title': 'Optimization of Machining Performance during the Turning of GFRP Composites:Topsis Based Taguchi Method', 'abstract': 'GFRP (Glass Fibre Reinforced Polymer) composites are emerging material in engineering field mainly in aerospace and automobile sector because of their excellence properties such as high toughness, low weight to volume ratio, high rigidity, strength, etc., to be widely used in various engineering application. However, the machinability of such composites is difficult to form desired shape. This thesis presents a TOPSIS Taguchi approach for parametric optimization of Glass fiber reinforced polymer bars.', 'corpus_id': 138872206, 'score': 0}, {'doc_id': '201304248', 'title': 'Text Summarization with Pretrained Encoders', 'abstract': 'Bidirectional Encoder Representations from Transformers (BERT) represents the latest incarnation of pretrained language models which have recently advanced a wide range of natural language processing tasks. In this paper, we showcase how BERT can be usefully applied in text summarization and propose a general framework for both extractive and abstractive models. We introduce a novel document-level encoder based on BERT which is able to express the semantics of a document and obtain representations for its sentences. Our extractive model is built on top of this encoder by stacking several inter-sentence Transformer layers. For abstractive summarization, we propose a new fine-tuning schedule which adopts different optimizers for the encoder and the decoder as a means of alleviating the mismatch between the two (the former is pretrained while the latter is not). We also demonstrate that a two-staged fine-tuning approach can further boost the quality of the generated summaries. Experiments on three datasets show that our model achieves state-of-the-art results across the board in both extractive and abstractive settings. Our code is available at this https URL', 'corpus_id': 201304248, 'score': 1}, {'doc_id': '21850704', 'title': 'A Deep Reinforced Model for Abstractive Summarization', 'abstract': 'Attentional, RNN-based encoder-decoder models for abstractive summarization have achieved good performance on short input and output sequences. For longer documents and summaries however these models often include repetitive and incoherent phrases. We introduce a neural network model with a novel intra-attention that attends over the input and continuously generated output separately, and a new training method that combines standard supervised word prediction and reinforcement learning (RL). Models trained only with supervised learning often exhibit ""exposure bias"" - they assume ground truth is provided at each step during training. However, when standard word prediction is combined with the global sequence prediction training of RL the resulting summaries become more readable. We evaluate this model on the CNN/Daily Mail and New York Times datasets. Our model obtains a 41.16 ROUGE-1 score on the CNN/Daily Mail dataset, an improvement over previous state-of-the-art models. Human evaluation also shows that our model produces higher quality summaries.', 'corpus_id': 21850704, 'score': 1}, {'doc_id': '220425399', 'title': 'Alleviating the Burden of Labeling: Sentence Generation by Attention Branch Encoder–Decoder Network', 'abstract': 'Domestic service robots (DSRs) are a promising solution to the shortage of home care workers. However, one of the main limitations of DSRs is their inability to interact naturally through language. Recently, data-driven approaches have been shown to be effective for tackling this limitation; however, they often require large-scale datasets, which is costly. Based on this background, we aim to perform automatic sentence generation of fetching instructions: for example, “Bring me a green tea bottle on the table.” This is particularly challenging because appropriate expressions depend on the target object, as well as its surroundings. In this letter, we propose the attention branch encoder–decoder network (ABEN), to generate sentences from visual inputs. Unlike other approaches, the ABEN has multimodal attention branches that use subword-level attention and generate sentences based on subword embeddings. In experiments, we compared the ABEN with a baseline method using four standard metrics in image captioning. Results show that the ABEN outperformed the baseline in terms of these metrics.', 'corpus_id': 220425399, 'score': 0}, {'doc_id': '52144157', 'title': 'Bottom-Up Abstractive Summarization', 'abstract': 'Neural summarization produces outputs that are fluent and readable, but which can be poor at content selection, for instance often copying full sentences from the source document. This work explores the use of data-efficient content selectors to over-determine phrases in a source document that should be part of the summary. We use this selector as a bottom-up attention step to constrain the model to likely phrases. We show that this approach improves the ability to compress text, while still generating fluent summaries. This two-step process is both simpler and higher performing than other end-to-end content selection models, leading to significant improvements on ROUGE for both the CNN-DM and NYT corpus. Furthermore, the content selector can be trained with as little as 1,000 sentences making it easy to transfer a trained summarizer to a new domain.', 'corpus_id': 52144157, 'score': 1}, {'doc_id': '8314118', 'title': 'Get To The Point: Summarization with Pointer-Generator Networks', 'abstract': 'Neural sequence-to-sequence models have provided a viable new approach for abstractive text summarization (meaning they are not restricted to simply selecting and rearranging passages from the original text). However, these models have two shortcomings: they are liable to reproduce factual details inaccurately, and they tend to repeat themselves. In this work we propose a novel architecture that augments the standard sequence-to-sequence attentional model in two orthogonal ways. First, we use a hybrid pointer-generator network that can copy words from the source text via pointing, which aids accurate reproduction of information, while retaining the ability to produce novel words through the generator. Second, we use coverage to keep track of what has been summarized, which discourages repetition. We apply our model to the CNN / Daily Mail summarization task, outperforming the current abstractive state-of-the-art by at least 2 ROUGE points.', 'corpus_id': 8314118, 'score': 1}, {'doc_id': '220403590', 'title': 'Learning Neural Textual Representations for Citation Recommendation', 'abstract': 'With the rapid growth of the scientific literature, manually selecting appropriate citations for a paper is becoming increasingly challenging and time-consuming. While several approaches for automated citation recommendation have been proposed in the recent years, effective document representations for citation recommendation are still elusive to a large extent. For this reason, in this paper we propose a novel approach to citation recommendation which leverages a deep sequential representation of the documents (Sentence-BERT) cascaded with Siamese and triplet networks in a submodular scoring function. To the best of our knowledge, this is the first approach to combine deep representations and submodular selection for a task of citation recommendation. Experiments have been carried out using a popular benchmark dataset - the ACL Anthology Network corpus - and evaluated against baselines and a state-of-the-art approach using metrics such as the MRR and F1@$k$ score. The results show that the proposed approach has been able to outperform all the compared approaches in every measured metric.', 'corpus_id': 220403590, 'score': 1}, {'doc_id': '220424713', 'title': 'Multi-task Regularization Based on Infrequent Classes for Audio Captioning', 'abstract': 'Audio captioning is a multi-modal task, focusing on using natural language for describing the contents of general audio. Most audio captioning methods are based on deep neural networks, employing an encoder-decoder scheme and a dataset with audio clips and corresponding natural language descriptions (i.e. captions). A significant challenge for audio captioning is the distribution of words in the captions: some words are very frequent but acoustically non-informative, i.e. the function words (e.g. ""a"", ""the""), and other words are infrequent but informative, i.e. the content words (e.g. adjectives, nouns). In this paper we propose two methods to mitigate this class imbalance problem. First, in an autoencoder setting for audio captioning, we weigh each word\'s contribution to the training loss inversely proportional to its number of occurrences in the whole dataset. Secondly, in addition to multi-class, word-level audio captioning task, we define a multi-label side task based on clip-level content word detection by training a separate decoder. We use the loss from the second task to regularize the jointly trained encoder for the audio captioning task. We evaluate our method using Clotho, a recently published, wide-scale audio captioning dataset, and our results show an increase of 37\\% relative improvement with SPIDEr metric over the baseline method.', 'corpus_id': 220424713, 'score': 0}, {'doc_id': '85500417', 'title': 'Fine-tune BERT for Extractive Summarization', 'abstract': 'BERT, a pre-trained Transformer model, has achieved ground-breaking performance on multiple NLP tasks. In this paper, we describe BERTSUM, a simple variant of BERT, for extractive summarization. Our system is the state of the art on the CNN/Dailymail dataset, outperforming the previous best-performed system by 1.65 on ROUGE-L. The codes to reproduce our results are available at this https URL', 'corpus_id': 85500417, 'score': 1}]"
88	{'doc_id': '202843858', 'title': 'Diagnóstico florístico-estrutural do componente arbóreo da floresta da Serra de São José, Tiradentes, MG, Brasil', 'abstract': 'ABSTRACT – (Floristic and structural diagnosis of the tree component of a forest at Serra de Sao Jose, Tiradentes, Minas Gerais State,Southeast Brazil). This work describes species composition and structure of the tree component of an area of lower to upper montaneseasonal semideciduous forest located at Serra de Sao Jose, Tiradentes, SE Brazil. Floristic and structural comparisons are mad e with 23other forest areas of the region using canonical correspondence analysis differentiating environmental and spatial variables. T he floristicanalyses indicated that, despite the transitional character between lower and upper montane forests, the tree flora of Serra de Sao Joseshowed stronger link than the former. Floristic and structural variation among the 24 forests was significantly related to alti tude, durationof the dry season and soil saturation of bases. Furthermore, a significant correlation was also found with spatial proximity of the areas. Key words : altitude, multivariate analysis, climate, Geostatistics, Semideciduous forest, soils', 'corpus_id': 202843858}	5316	"[{'doc_id': '233467606', 'title': 'Genesis and phytodiversity in the enclave pico do jabre in the brazilian semiarid', 'abstract': 'Abstract: Among the elements that make up the landscaped areas of Caatinga, they are so-called ""bogs altitude"" or ""moist forest enclaves,"" which can m have explained its genesis from the hypothesis of climate change Quaternary. This work aims to portray phytodiversity and the origin of the Pico do Jabre residual massif, located in the Northeast region, Brazilian Semi-arid region. As a result, it was found that the Pico do Jabre corresponds to a residual massif, based on the geotectonic zone of Teixeira or Pluton Teixeira, and had its origin from the differential erosion between the schist rocks of the pediplane and the granitic rocks of the massif. Its phytodiversity is distributed in altitudes where the outcrop of the plateau is home to cave species, the dimension between 900 and 1100 meters corresponds to the area of semideciduous forest montana, and Caatinga below 900 meters.', 'corpus_id': 233467606, 'score': 0}, {'doc_id': '233645343', 'title': 'Fitossociologia e diversidade em fragmentos florestais com diferentes históricos de intervenção na Amazônia Ocidental', 'abstract': ""In this study, we sought to carry out the floristic and structural characterization of three forest fragments that underwent anthropic intervention, at different time intervals, located in the Environmental Protection Area ‘Lago do Amapa’ in Rio Branco, state of Acre, Brazil. Fragment I has a history of intervention for the formation of pasture and timber extraction in the 1960s. Fragment II has a history of intervention for cutting/burning and planting in the 1970s and Fragment III has a history of intervention for the formation of pastures in the 1980s. For floristic and phytosociological sampling, two plots per fragment measuring 0.5 hectares each (20 m x 250 m) were allocated, totaling an area of three ha. All arboreal individuals with DBH ≥ 10 cm were inventoried. Phytosociological parameters such as dominance, density, frequency, Importance Value (IV) and basal area were calculated. The diversity between fragments was obtained using the Shannon-Weaver index (H') and similarity using the Jaccard index. The comparisons of means were performed using the Tukey test with 5% significance. Altogether, 1,427 individuals were inventoried, distributed in 44 families and 193 species. The H' diversity indices found were: 4.21 for Fragment I, 3.74 for Fragment II and 3.50 for Fragment III. The Jaccard index showed a greater similarity between fragments II and III. Fragment I differed statistically from the others in relation to the basal area, number of genera and species. The medium strata (height class 8 ≤ h <16.93) had the largest number of individuals. Regarding the diametric classes, a greater number of first-class individuals was observed, following the inverted J pattern in all fragments. The result of the study pointed out that the fragment with the oldest history of anthropization has structure, richness and diversity of species distinct from the other fragments."", 'corpus_id': 233645343, 'score': 1}, {'doc_id': '233304528', 'title': 'Floristic Variation of Tree Communities In Island Forests of Pulau Tuba and Gunung Raya Forest Reserve, Langkawi', 'abstract': 'Island forests are among forest habitats that are vulnerable to natural and anthropogenic disturbances, whereby the disturbances would influence the survival of biological species of the ecosystems. Langkawi Archipelago contains many small island forests and rapid development of tourism industry within this archipelago might contribute impacts to the tree flora of the forest communities on the small islands. Hence, in this study the species richness and floristic variation pattern of tree communities of two selected island forests in the Langkawi Archipelago were explored, and data gathered are anticipated to be used for management of island forests in Langkawi. Tree survey was carried out in 10 study plots of 20m x 25m each, at island forests of Pulau Tuba Forest Reserve (PTB) and Gunung Raya Forest Reserve (GRFR), making the total of 20 study plots. All trees with diameter at breast height (dbh) of 5.0 cm and above were enumerated and tree species were identified. Species data were analyzed for diversity and richness using the Shannon and Margalef indices; whilst Detrended Correspondence Analysis (DCA) was used to determine floristic pattern. A total of 1062 trees were recorded from all study plots which comprised of 49 families, 134 genera and 213 tree species. The GRFR exhibited the highest species number of 135 tree species, followed by the PTB (106 tree species). Species accumulation curves showed that the curves were far from reaching the asymptote even when the whole dataset were combined. The DCA ordination diagram clearly grouped the study plots by their geological formation that indicated a gradient of species change in GRFR and PTB sites.', 'corpus_id': 233304528, 'score': 0}, {'doc_id': '234353071', 'title': 'Phytosociology in a fragment of seasonal semideciduous forest in a legal reserve in the southwest of the Goiás state', 'abstract': 'In Goiás, seasonal forests structure studies are scarce, especially in fragments located in legal reserves, subject to human disturbances. Therefore, the study aimed to perform the phytosociology of a fragment of a rural property, in a seasonal semideciduous forest. We evaluated structure, richness, diversity, dispersion mechanisms, ecological groups, and species distribution patterns found in the study and floristic links between Cerrado, Amazon and Atlantic Forest. In a one-hectare sample plot, all live trees with a diameter at breast height ≥ 10 cm were measured. The values of density and basal area were 561 ind.ha and 26.2 m2.ha, respectively. We recorded 37 species with diversity indices (H’) of 2.36 and evenness (J’) of 0.65. Toulicia reticulata, Chaetocarpus echinocarpus, Bocageopsis mattogrossensis and Nectandra cuspidata were species with the highest importance value, corresponding to 69% and 65% of all density and basal area, respectively. Fabaceae obtained the highest floristic richness, with six species, although it was little represented in terms of abundance. Our results suggest that the fragment is a mature forest in good conservation status. This is reinforced by the high size of trees, in addition to the predominance of zoochoric (83.8%) and secondary (92.1%) individuals in the survey.', 'corpus_id': 234353071, 'score': 1}, {'doc_id': '233322060', 'title': 'STRUCTURE AND DIVERSITY OF THE ARBOREAL COMPONENT IN CERRADO SENSU STRICTO IN NORTHERN MINAS GERAIS', 'abstract': 'Due to a rapid change in the land use in northern Minas Gerais State, southeastern Brazil, large amounts of cerrado have been converted into crops. There is little information about the structure and diversity of the remaining communities in the region. The present study aimed to characterize the structure of the tree vegetation in a cerrado sensu stricto area. We delimited thirty 20m x 20m plots and sampled tree individuals with diameter equal to or greater than 4.8 cm at 30 cm from the ground level (DGH 30 ). We sampled a total of 2616 individuals of 76 species and 33 botanical families. The community showed a density of 2180 ind/ha and a basal area of 17.3 m 2 /ha. Qualea grandiflora, Terminalia fagifolia, and Dimorphandra mollis showed high importance values. Standing dead individuals stood out in the community. The Shannon diversity index (3.10) and Pielou’s evenness (0.70) indicate that the community is in an intermediate stage of succession in comparison with others. Despite the signs of disturbance, the community still includes species of great social and economic importance for the local population. This community’s floristic and structural patterns can be used to reference extractive management, restoration, and conservation initiatives of other cerrado areas in the region.', 'corpus_id': 233322060, 'score': 1}, {'doc_id': '233371437', 'title': 'Vegetation patterns and the influence of rainfall after long-term fire suppression on a woody community of a Brazilian savanna.', 'abstract': 'We evaluated the structural and floristic characteristics of a Brazilian savanna fragment occupied by cerradão (CD) and cerrado sensu stricto (CS) in response to the influence of rainfall and long-term fire suppression. We carried out floristic, phytosociological and remote sensing studies in a cerrado fragment located in Corumbataí (SP, Brazil) after 43 years of complete fire suppression. We surveyed 43 plots of 200 m2 each (17 plots in CS and 26 plots in CD) and all individuals ≥ 0.32 cm diameter measured at 30 cm from the ground were included in the sample. We calculated phytosociological parameters for each species and classified them in three ecological groups, namely savanna, generalist and forest species. The remote sensing analysis used aerial photographs and satellite images from 1962 to 2019 (i.e. 59 years). The structural study of community revealed high predominance of forest and generalist species when compared to savanna species. Non-linear correlation between CD expansion rates and total rainfall within the study period indicated a positive influence of the rainfall (R2 = 0.42). Thus, our analysis indicated a tendency of a continuous and fast expansion of CD over areas of CS in the long-term absence of fire combined with periods of heavy rain.', 'corpus_id': 233371437, 'score': 1}, {'doc_id': '233682435', 'title': 'Floristic composition and structure of the Kibate Forest along environmental gradients in Wonchi, Southwestern Ethiopia', 'abstract': 'Dry evergreen montane forests in Ethiopia are severely threatened. The status of species composition and structure of forest vegetation are important indicators to understand the trends of threats on local plant communities. In the present study, we examined the floristic composition and structure of the Kibate Forest, Wonchi Highland, Ethiopia along environmental gradients. Sixty-six (30\xa0m\u2009×\u200930\xa0m) plots were established every 100\xa0m interval along altitudinal gradients (2811‒3073\xa0m a.s.l.) in five transect lines for vegetation and environmental data collection. In total, 125 vascular plant species belonging to 104 genera and 52 families were identified. Eighteen species (14%) were endemic to Ethiopia and Eritrea. The two most dominant families, Asteraceae (29 species) and Lamiaceae (eight species) accounted for 30% of the total number of species. The highest number of species (54%) was herbs. Four major community types (viz., Olinia rochetiana-Myrsine melanophloeos, Ilex mitis-Galiniera saxifraga, Erica arborea-Protea gaguedi, and Hagenia abyssinica-Juniperus procera) were identified. The highest species richness, evenness, diversity, and importance value index were in community types 2 and 4. About 82% of the species and all endemic taxa except five were recorded in these two community types. The most dominant woody species were O. rochetiana, E. arborea, Olea europaea subsp. cuspidata, Myrica salicifolia, I. mitis var. mitis, and H. abyssinica with different patterns of population structure. The results show that there was a weak correlation between species richness and altitude. Our findings confirm that environmental variables both with interactions (such as altitude) and without interactions (such as livestock grazing) significantly (p\u2009<\u20090.05) affect species richness. Anthropogenic activities and overgrazing by livestock appear to be the main threat in community types 2 and 3. Urgent management practices and conservation measures such as prohibiting forest clearing and overgrazing and planting indigenous trees through community participation should be considered in community types that are rich in endemic species but are highly threatened.', 'corpus_id': 233682435, 'score': 0}, {'doc_id': '233669840', 'title': 'Observations on the floristic, life-form, leaf-size spectra and habitat diversity of vegetation in the Bhimber hills of Kashmir Himalayas', 'abstract': 'Abstract Vegetation analysis provides the prerequisites to understand the overall community structure and function of any ecosystem and is a fundamental requirement for the precise evaluation of biodiversity. Although many studies have assessed floristic attributes of specific areas, there are still unexplored regions, as is the case of the mountain region in the Kashmir Himalayas. Current research highlighted the recent findings of the scientific characterization of floristic and ecological aspects on the forest flora found in the Bhimber hills, Pakistan. Floristically, a total of 93 species belonging to 80 genera in 41 families were recorded. The species distribution patterns across the families were disproportionate with half of the species contributed by 8 families and 25 families were monotypic. Based on the floristic analysis, Asteraceae was the largest family with 12% of species followed by Poaceae with (11%) species. PAST software, a multivariate ecological community analysis was used to classify the species similarities and differences among the different habitat types. According to the habitat wise distribution, 21% of species were growing in the natural forest habitat, while 15% of species were dispersed in highly distributed habitats along roadsides and 8% on pedestrians. In terms of functional diversity, the herbaceous growth form was dominant (58%). The biological spectrum revealed therophytes as the dominant life form as it indicates the disturbed habitat vegetation. The phytogeographical analysis revealed that the maximum (69%) species were native, while the minimum (31%) species were exotic. Thus, the study of these functional and habitat diversity patterns can significantly improve our understanding of the ecological aspects of the flora in the geographical location. This information may additionally be useful in devising management plans to ensure sustainable utilization and better management of forest landscapes in this Himalayan region.', 'corpus_id': 233669840, 'score': 0}, {'doc_id': '233697471', 'title': 'THE TRADE OF PINHÃO (Araucaria angustifolia SEED) IN MINAS GERAIS: A STIMULUS FOR CONSERVATION?', 'abstract': 'The knowledge of the production and marketing chains of pinhao is centered in the Mixed Ombrophilous Forest region, in southern Brazil. This study aimed to evaluate the pinhao production and marketing chains in a region of ecological tension (Seasonal Semideciduous Forest) and verify the pinhao trade effectiveness as a tool for araucaria conservation. This research was based on a statistical survey on pinhao extraction and trade, made available by governmental institutions. To identify establishments operating in the trade of pinhao in Minas Gerais (Brazil), structured interviews and a literature review of the legislation associated with the trade of forest products were conducted, establishing the relationship between the trade and conservation of araucaria. The results showed that Minas Gerais is the third-largest national producer of pinhao, accounting for 13.9% of production, and receiving the lowest remuneration for the extracted product. There is a predominance of a short marketing chain, in which producers or retailers negotiate with the consumer, resulting in greater profits for those involved in this process, who benefit from the lack of specific regulations and inspections in the extraction and trade of pinhao, leading to a predatory activity, which does not contribute to the preservation of araucaria. However, the use of appropriate instruments may make feasible the conservation of this species, combining its potential for use in forest restoration projects with the income from the commercialization of its seeds, thus developing an effective tool for the conservation of araucaria in rural properties in Minas Gerais.', 'corpus_id': 233697471, 'score': 0}, {'doc_id': '233668181', 'title': 'Florística e estrutura de diferentes estratos em uma Floresta Estacional Semidecidual urbana, Jataí, GO', 'abstract': ""Comparative studies between different strata of forests in ecological succession are important to understand the forest dynamics. The study evaluated floristic, phytosociological and ecological aspects between the tree and regenerating strata of an urban fragment of semideciduous forest, in Jatai, Goias state. In the last decades, the vegetation that was typically savanna has become a forest. Trees (diameter at breast height, DBH ≥ 5 cm) were sampled in 12 permanent 200m² plots. In each of them, 25m² sub-plots were sampled for the regenerant stratum, which comprised individuals with DBH <5cm and height ≥ 1m. Regenerants had greater richness (31 species), with two more species than adults. There was low floristic and structural similarity between the strata, indicating a replacement of species over time. Shannon's diversity was significantly lower for the tree stratum ( t test), since it presented low equability, with ecological dominance of few species. Both strata had a high proportion of pioneer species, when compared to other studies. Zoochory and species of forest environments were significantly more abundant in the regenerating stratum. The study revealed a fragment in a frank process of secondary succession and vulnerability in the face of anthropic pressures, with care regarding its conservation being important."", 'corpus_id': 233668181, 'score': 1}]"
89	{'doc_id': '215737187', 'title': 'Dense Passage Retrieval for Open-Domain Question Answering', 'abstract': 'Open-domain question answering relies on efficient passage retrieval to select candidate contexts, where traditional sparse vector space models, such as TF-IDF or BM25, are the de facto method. In this work, we show that retrieval can be practically implemented using dense representations alone, where embeddings are learned from a small number of questions and passages by a simple dual-encoder framework. When evaluated on a wide range of open-domain QA datasets, our dense retriever outperforms a strong Lucene-BM25 system largely by 9%-19% absolute in terms of top-20 passage retrieval accuracy, and helps our end-to-end QA system establish new state-of-the-art on multiple open-domain QA benchmarks.', 'corpus_id': 215737187}	11003	[{'doc_id': '221971009', 'title': 'SPARTA: Efficient Open-Domain Question Answering via Sparse Transformer Matching Retrieval', 'abstract': 'We introduce SPARTA, a novel neural retrieval method that shows great promise in performance, generalization, and interpretability for open-domain question answering. Unlike many neural ranking methods that use dense vector nearest neighbor search, SPARTA learns a sparse representation that can be efficiently implemented as an Inverted Index. The resulting representation enables scalable neural retrieval that does not require expensive approximate vector search and leads to better performance than its dense counterpart. We validated our approaches on 4 open-domain question answering (OpenQA) tasks and 11 retrieval question answering (ReQA) tasks. SPARTA achieves new state-of-the-art results across a variety of open-domain question answering tasks in both English and Chinese datasets, including open SQuAD, CMRC and etc. Analysis also confirms that the proposed method creates human interpretable representation and allows flexible control over the trade-off between performance and efficiency.', 'corpus_id': 221971009, 'score': 1}, {'doc_id': '227151337', 'title': 'SEA: Sentence Encoder Assembly for Video Retrieval by Textual Queries', 'abstract': 'Retrieving unlabeled videos by textual queries, known as Ad-hoc Video Search (AVS), is a core theme in multimedia data management and retrieval. The success of AVS counts on cross-modal representation learning that encodes both query sentences and videos into common spaces for semantic similarity computation. Inspired by the initial success of previously few works in combining multiple sentence encoders, this paper takes a step forward by developing a new and general method for effectively exploiting diverse sentence encoders. The novelty of the proposed method, which we term Sentence Encoder Assembly (SEA), is two-fold. First, different from prior art that use only a single common space, SEA supports text-video matching in multiple encoder-specific common spaces. Such a property prevents the matching from being dominated by a specific encoder that produces an encoding vector much longer than other encoders. Second, in order to explore complementarities among the individual common spaces, we propose multi-space multi-loss learning. As extensive experiments on four benchmarks (MSR-VTT, TRECVID AVS 2016-2019, TGIF and MSVD) show, SEA surpasses the state-of-the-art. In addition, SEA is extremely ease to implement. All this makes SEA an appealing solution for AVS and promising for continuously advancing the task by harvesting new sentence encoders.', 'corpus_id': 227151337, 'score': 0}, {'doc_id': '211204736', 'title': 'REALM: Retrieval-Augmented Language Model Pre-Training', 'abstract': 'Language model pre-training has been shown to capture a surprising amount of world knowledge, crucial for NLP tasks such as question answering. However, this knowledge is stored implicitly in the parameters of a neural network, requiring ever-larger networks to cover more facts. \nTo capture knowledge in a more modular and interpretable way, we augment language model pre-training with a latent knowledge retriever, which allows the model to retrieve and attend over documents from a large corpus such as Wikipedia, used during pre-training, fine-tuning and inference. For the first time, we show how to pre-train such a knowledge retriever in an unsupervised manner, using masked language modeling as the learning signal and backpropagating through a retrieval step that considers millions of documents. \nWe demonstrate the effectiveness of Retrieval-Augmented Language Model pre-training (REALM) by fine-tuning on the challenging task of Open-domain Question Answering (Open-QA). We compare against state-of-the-art models for both explicit and implicit knowledge storage on three popular Open-QA benchmarks, and find that we outperform all previous methods by a significant margin (4-16% absolute accuracy), while also providing qualitative benefits such as interpretability and modularity.', 'corpus_id': 211204736, 'score': 0}, {'doc_id': '226965686', 'title': 'JNLP Team: Deep Learning for Legal Processing in COLIEE 2020', 'abstract': 'We propose deep learning based methods for automatic systems of legal retrieval and legal question-answering in COLIEE 2020. These systems are all characterized by being pre-trained on large amounts of data before being finetuned for the specified tasks. This approach helps to overcome the data scarcity and achieve good performance, thus can be useful for tackling related problems in information retrieval, and decision support in the legal domain. Besides, the approach can be explored to deal with other domain specific problems.', 'corpus_id': 226965686, 'score': 0}, {'doc_id': '52967399', 'title': 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding', 'abstract': 'We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).', 'corpus_id': 52967399, 'score': 1}, {'doc_id': '226254161', 'title': 'Language Model is all You Need: Natural Language Understanding as Question Answering', 'abstract': 'Different flavors of transfer learning have shown tremendous impact in advancing research and applications of machine learning. In this work we study the use of a certain family of transfer learning, where the target domain is mapped to the source domain. Specifically we map Natural Language Understanding (NLU) problems to Question Answering (QA) problems and we show that in low data regimes this approach offers significant improvements compared to other approaches to NLU. Moreover, we show that these gains could be increased through sequential transfer learning across NLU problems from different domains. We show that our approach could reduce the amount of required data for the same performance by up to a factor of 10.', 'corpus_id': 226254161, 'score': 1}, {'doc_id': '226246229', 'title': 'Indic-Transformers: An Analysis of Transformer Language Models for Indian Languages', 'abstract': 'Language models based on the Transformer architecture have achieved state-of-the-art performance on a wide range of NLP tasks such as text classification, question-answering, and token classification. However, this performance is usually tested and reported on high-resource languages, like English, French, Spanish, and German. Indian languages, on the other hand, are underrepresented in such benchmarks. Despite some Indian languages being included in training multilingual Transformer models, they have not been the primary focus of such work. In order to evaluate the performance on Indian languages specifically, we analyze these language models through extensive experiments on multiple downstream tasks in Hindi, Bengali, and Telugu language. Here, we compare the efficacy of fine-tuning model parameters of pre-trained models against that of training a language model from scratch. Moreover, we empirically argue against the strict dependency between the dataset size and model performance, but rather encourage task-specific model and method selection. We achieve state-of-the-art performance on Hindi and Bengali languages for text classification task. Finally, we present effective strategies for handling the modeling of Indian languages and we release our model checkpoints for the community : https://huggingface.co/neuralspace-reverie.', 'corpus_id': 226246229, 'score': 0}, {'doc_id': '227053761', 'title': 'Neuro-Symbolic Representations for Video Captioning: A Case for Leveraging Inductive Biases for Vision and Language', 'abstract': 'Neuro-symbolic representations have proved effective in learning structure information in vision and language. In this paper, we propose a new model architecture for learning multi-modal neuro-symbolic representations for video captioning. Our approach uses a dictionary learning-based method of learning relations between videos and their paired text descriptions. We refer to these relations as relative roles and leverage them to make each token role-aware using attention. This results in a more structured and interpretable architecture that incorporates modality-specific inductive biases for the captioning task. Intuitively, the model is able to learn spatial, temporal, and cross-modal relations in a given pair of video and text. The disentanglement achieved by our proposal gives the model more capacity to capture multi-modal structures which result in captions with higher quality for videos. Our experiments on two established video captioning datasets verifies the effectiveness of the proposed approach based on automatic metrics. We further conduct a human evaluation to measure the grounding and relevance of the generated captions and observe consistent improvement for the proposed model. The codes and trained models can be found at this https URL', 'corpus_id': 227053761, 'score': 0}, {'doc_id': '226227533', 'title': 'COOT: Cooperative Hierarchical Transformer for Video-Text Representation Learning', 'abstract': 'Many real-world video-text tasks involve different levels of granularity, such as frames and words, clip and sentences or videos and paragraphs, each with distinct semantics. In this paper, we propose a Cooperative hierarchical Transformer (COOT) to leverage this hierarchy information and model the interactions between different levels of granularity and different modalities. The method consists of three major components: an attention-aware feature aggregation layer, which leverages the local temporal context (intra-level, e.g., within a clip), a contextual transformer to learn the interactions between low-level and high-level semantics (inter-level, e.g. clip-video, sentence-paragraph), and a cross-modal cycle-consistency loss to connect video and text. The resulting method compares favorably to the state of the art on several benchmarks while having few parameters. All code is available open-source at this https URL', 'corpus_id': 226227533, 'score': 0}, {'doc_id': '59604492', 'title': 'End-to-End Open-Domain Question Answering with BERTserini', 'abstract': 'We demonstrate an end-to-end question answering system that integrates BERT with the open-source Anserini information retrieval toolkit. In contrast to most question answering and reading comprehension models today, which operate over small amounts of input text, our system integrates best practices from IR with a BERT-based reader to identify answers from a large corpus of Wikipedia articles in an end-to-end fashion. We report large improvements over previous results on a standard benchmark test collection, showing that fine-tuning pretrained BERT with SQuAD is sufficient to achieve high accuracy in identifying answer spans.', 'corpus_id': 59604492, 'score': 1}]
90	"{'doc_id': '207229035', 'title': 'PrivacyGuard: A VPN-based Platform to Detect Information Leakage on Android Devices', 'abstract': ""More and more people rely on mobile devices to access the Internet, which also increases the amount of private information that can be gathered from people's devices. Although today's smartphone operating systems are trying to provide a secure environment, they fail to provide users with adequate control over and visibility into how third-party applications use their private data. Whereas there are a few tools that alert users when applications leak private information, these tools are often hard to use by the average user or have other problems. To address these problems, we present PrivacyGuard, an open-source VPN-based platform for intercepting the network traffic of applications. PrivacyGuard requires neither root permissions nor any knowledge about VPN technology from its users. PrivacyGuard does not significantly increase the trusted computing base since PrivacyGuard runs in its entirety on the local device and traffic is not routed through a remote VPN server. We implement PrivacyGuard on the Android platform by taking advantage of the VPNService class provided by the Android SDK. PrivacyGuard is configurable, extensible, and useful for many different purposes. We investigate its use for detecting the leakage of multiple types of sensitive data, such as a phone's IMEI number or location data. PrivacyGuard also supports modifying the leaked information and replacing it with crafted data for privacy protection. According to our experiments, PrivacyGuard can detect more leakage incidents by applications and advertisement libraries than TaintDroid. We also demonstrate that PrivacyGuard has reasonable overhead on network performance and almost no overhead on battery consumption."", 'corpus_id': 207229035}"	10159	"[{'doc_id': '15434409', 'title': 'Targeted Mimicry Attacks on Touch Input Based Implicit Authentication Schemes', 'abstract': ""Touch input implicit authentication (``touch IA'') employs behavioural biometrics like touch location and pressure to continuously and transparently authenticate smartphone users. We provide the first ever evaluation of targeted mimicry attacks on touch IA and show that it fails against shoulder surfing and offline training attacks. Based on experiments with three diverse touch IA schemes and 256 unique attacker-victim pairs, we show that shoulder surfing attacks have a bypass success rate of 84% with the majority of successful attackers observing the victim's behaviour for less than two minutes. Therefore, the accepted assumption that shoulder surfing attacks on touch IA are infeasible due to the hidden nature of some features is incorrect. For offline training attacks, we created an open-source training app for attackers to train on their victims' touch data. With this training, attackers achieved bypass success rates of 86%, even with only partial knowledge of the underlying features used by the IA scheme. Previous work failed to find these severe vulnerabilities due to its focus on random, non-targeted attacks. Our work demonstrates the importance of considering targeted mimicry attacks to evaluate the security of an implicit authentication scheme. Based on our results, we conclude that touch IA is unsuitable from a security standpoint."", 'corpus_id': 15434409, 'score': 1}, {'doc_id': '221758178', 'title': 'Security and Privacy Issues in Wireless Networks and Mitigation Methods', 'abstract': 'The rapid growth of network services, Internet of Things devices and online users on the Internet have led to an increase in the amount of data transmitted daily. As more and more information is stored and transmitted on the Internet, cybercriminals are trying to gain access to the information to achieve their goals, whether it is to sell it on the dark web or for other malicious intent. Through thorough literature study relating to the causes and issues that are brought from the security and privacy segment of wireless networks, it is observed that there are various factors that can cause the networks to be an insecure; especially factors that revolve around cybercriminals with their growing expertise and the lack of preparation and efforts to combat them by relevant bodies. The aim of this paper is to showcase major and frequent security as well as privacy issues in wireless networks along with specialized solutions that can assist the related organizations or the public to fathom how great of an impact these challenges can bring if everyone took a step in reducing them. Hence, through this paper it is discovered that there are many ways these challenges can be mitigated, however, the lack of implementation of privacy and security solutions is still largely present due to the absence of practical application of these solutions by responsible parties in real world scenarios.', 'corpus_id': 221758178, 'score': 0}, {'doc_id': '222117825', 'title': 'DoThisHere: Multimodal Interaction to Improve Cross-Application Tasks on Mobile Devices', 'abstract': 'Many computing tasks, such as comparison shopping, two-factor authentication, and checking movie reviews, require using multiple apps together. On large screens, ""windows, icons, menus, pointer"" (WIMP) graphical user interfaces (GUIs) support easy sharing of content and context between multiple apps. So, it is straightforward to see the content from one application and write something relevant in another application, such as looking at the map around a place and typing walking instructions into an email. However, although today\'s smartphones also use GUIs, they have small screens and limited windowing support, making it hard to switch contexts and exchange data between apps. We introduce DoThisHere, a multimodal interaction technique that streamlines cross-app tasks and reduces the burden these tasks impose on users. Users can use voice to refer to information or app features that are off-screen and touch to specify where the relevant information should be inserted or is displayed. With DoThisHere, users can access information from or carry information to other apps with less context switching. We conducted a survey to find out what cross-app tasks people are currently performing or wish to perform on their smartphones. Among the 125 tasks that we collected from 75 participants, we found that 59 of these tasks are not well supported currently. DoThisHere is helpful in completing 95% of these unsupported tasks. A user study, where users are shown the list of supported voice commands when performing a representative sample of such tasks, suggests that DoThisHere may reduce expert users\' cognitive load; the Query action, in particular, can help users reduce task completion time.', 'corpus_id': 222117825, 'score': 0}, {'doc_id': '17077058', 'title': 'Itus: an implicit authentication framework for android', 'abstract': 'Security and usability issues with pass-locks on mobile devices have prompted researchers to develop implicit authentication (IA) schemes, which continuously and transparently authenticate users using behavioural biometrics. Contemporary IA schemes proposed by the research community are challenging to deploy, and there is a need for a framework that supports: different behavioural classifiers, given that different apps have different requirements; app developers using IA without becoming domain experts; and real-time classification on resource-constrained mobile devices. We present Itus, an IA framework for Android that allows the research community to improve IA schemes incrementally, while allowing app developers to adopt these improvements at their own pace. We describe the Itus framework and how it provides: ease of use: Itus allows app developers to use IA by changing as few as two lines of their existing code - on the other hand, Itus provides an oracle capable of making advanced recommendations should developers wish to fine-tune the classifiers; flexibility: developers can deploy Itus in an application-specific manner, adapting to their unique needs; extensibility: researchers can contribute new behavioural features and classifiers without worrying about deployment particulars; low performance overhead: Itus operates with minimal performance overhead, allowing app developers to deploy it without compromising end-user experience. These goals are accomplished with an API allowing individual stakeholders to incrementally improve Itus without re-engineering new systems. We implement Itus in two demo apps and measure its performance impact. To our knowledge, Itus is the first open-source extensible IA framework for Android that can be deployed off-the-shelf.', 'corpus_id': 17077058, 'score': 1}, {'doc_id': '225075733', 'title': 'On the Root of Trust Identification Problem', 'abstract': ""Trusted Execution Environments (TEEs) are becoming ubiquitous and are currently used in many security applications: from personal IoT gadgets to banking and databases. Prominent examples of such architectures are Intel SGX, ARM TrustZone, and Trusted Platform Modules (TPMs). A typical TEE relies on a dynamic Root of Trust (RoT) to provide security services such as code/data confidentiality and integrity, isolated secure software execution, remote attestation, and sensor auditing. Despite their usefulness, there is currently no secure means to determine whether a given security service or task is being performed by the particular RoT within a specific physical device. We refer to this as the Root of Trust Identification (RTI) problem and discuss how it inhibits security for applications such as sensing and actuation. We formalize the RTI problem and argue that security of RTI protocols is especially challenging due to local adversaries, cuckoo adversaries, and the combination thereof. To cope with this problem we propose a simple and effective protocol based on biometrics. Unlike biometric-based user authentication, our approach is not concerned with verifying user identity, and requires neither pre-enrollment nor persistent storage for biometric templates. Instead, it takes advantage of the difficulty of cloning a biometric in realtime to securely identify the RoT of a given physical device, by using the biometric as a challenge. Security of the proposed protocol is analyzed in the combined Local and Cuckoo adversarial model. Also, a prototype implementation is used to demonstrate the protocol's feasibility and practicality. We further propose a Proxy RTI protocol, wherein a previously identified RoT assists a remote verifier in identifying new RoTs."", 'corpus_id': 225075733, 'score': 0}, {'doc_id': '221139328', 'title': 'Making Distributed Mobile Applications SAFE: Enforcing User Privacy Policies on Untrusted Applications with Secure Application Flow Enforcement', 'abstract': ""Today's mobile devices sense, collect, and store huge amounts of personal information, which users share with family and friends through a wide range of applications. Once users give applications access to their data, they must implicitly trust that the apps correctly maintain data privacy. As we know from both experience and all-too-frequent press articles, that trust is often misplaced. While users do not trust applications, they do trust their mobile devices and operating systems. Unfortunately, sharing applications are not limited to mobile clients but must also run on cloud services to share data between users. In this paper, we leverage the trust that users have in their mobile OSes to vet cloud services. To do so, we define a new Secure Application Flow Enforcement (SAFE) framework, which requires cloud services to attest to a system stack that will enforce policies provided by the mobile OS for user data. We implement a mobile OS that enforces SAFE policies on unmodified mobile apps and two systems for enforcing policies on untrusted cloud services. Using these prototypes, we demonstrate that it is possible to enforce existing user privacy policies on unmodified applications."", 'corpus_id': 221139328, 'score': 0}, {'doc_id': '211041059', 'title': 'Mimicry Attacks on Smartphone Keystroke Authentication', 'abstract': 'Keystroke behaviour-based authentication employs the unique typing behaviour of users to authenticate them. Recent such proposals for virtual keyboards on smartphones employ diverse temporal, contact, and spatial features to achieve over 95% accuracy. Consequently, they have been suggested as a second line of defense with text-based password authentication. We show that a state-of-the-art keystroke behaviour-based authentication scheme is highly vulnerable against mimicry attacks. While previous research used training interfaces to attack physical keyboards, we show that this approach has limited effectiveness against virtual keyboards. This is mainly due to the large number of diverse features that the attacker needs to mimic for virtual keyboards. We address this challenge by developing an augmented reality-based app that resides on the attacker’s smartphone and leverages computer vision and keystroke data to provide real-time guidance during password entry on the victim’s phone. In addition, we propose an audiovisual attack in which the attacker overlays transparent film printed with spatial pointers on the victim’s device and uses audio cues to match the temporal behaviour of the victim. Both attacks require neither tampering or installing software on the victim’s device nor specialized hardware. We conduct experiments with 30 users to mount over 400 mimicry attacks. We show that our methods enable an attacker to mimic keystroke behaviour on virtual keyboards with little effort. We also demonstrate the extensibility of our augmented reality-based technique by successfully mounting mimicry attacks on a swiping behaviour-based continuous authentication system.', 'corpus_id': 211041059, 'score': 1}, {'doc_id': '204764340', 'title': 'AppVeto: mobile application self-defense through resource access veto', 'abstract': ""Modern mobile operating systems such as Android and Apple iOS allow apps to access various system resources, with or without explicit user permission. Running multiple concurrent apps is also commonly supported, although the OS generally maintains strict separation between apps. However, an app can still get access to another app's private information, such as the user input, through numerous side-channels, mostly enabled by having access to permissioned or permission-less (sometimes even unrelated) resources, e.g., inferring keystroke and swipe gestures from a victim app via the accelerometer or gyroscope. Current mobile OSes do not empower an app to defend itself from such implicit interference from other apps; few exceptions exist such as blocking screenshot captures in Android. We propose a general mechanism for apps to defend themselves from any unwanted implicit or explicit interference from other concurrently running apps. Our AppVeto solution enables an app to easily configure its requirements for a safe environment; a foreground app can request the OS to disallow access---i.e., to enable veto powers---to selected side-channel-prone resources to all other running apps for a certain (short) duration, e.g., no access to the accelerometer during password input. In a sense, we enable a finer-grained access control policy than the current runtime permission model, and delegate the responsibility of the resource access decision (for vetoing) from users to app developers. We implement AppVeto on Android using the Xposed framework, without changing Android APIs. Furthermore, we show that AppVeto imposes negligible overhead, while being effective against several well-known side-channel attacks."", 'corpus_id': 204764340, 'score': 1}, {'doc_id': '221557401', 'title': 'Impersonation-as-a-Service: Characterizing the Emerging Criminal Infrastructure for User Impersonation at Scale', 'abstract': 'In this paper we provide evidence of an emerging criminal infrastructure enabling impersonation attacks at scale. Impersonation-as-a-Service (IMPaaS) allows attackers to systematically collect and enforce user profiles (consisting of user credentials, cookies, device and behavioural fingerprints, and other metadata) to circumvent risk-based authentication system and effectively bypass multi-factor authentication mechanisms. We present the IMPaaS model and evaluate its implementation by analysing the operation of a large, invite-only, Russian IMPaaS platform providing user profiles for more than 260,000 Internet users worldwide. Our findings suggest that the IMPaaS model is growing, and provides the mechanisms needed to systematically evade authentication controls across multiple platforms, while providing attackers with a reliable, up-to-date, and semi-automated environment enabling target selection and user impersonation against Internet users as scale.', 'corpus_id': 221557401, 'score': 0}, {'doc_id': '12012671', 'title': 'Usability and Security Perceptions of Implicit Authentication: Convenient, Secure, Sometimes Annoying', 'abstract': 'Implicit authentication (IA) uses behavioural biometrics to provide continuous authentication on smartphones. IA has been advocated as more usable when compared to traditional explicit authentication schemes, albeit with some security limitations. Consequently researchers have proposed that IA provides a middle-ground for people who do not use traditional authentication due to its usability limitations or as a second line of defence for users who already use authentication. However, there is a lack of empirical evidence that establishes the usability superiority of IA and its security perceptions. We report on the first extensive two-part study (n = 37) consisting of a controlled lab experiment and a field study to gain insights into usability and security perceptions of IA. Our findings indicate that 91% of participants found IA to be convenient (26% more than the explicit authentication schemes tested) and 81% perceived the provided level of protection to be satisfactory. While this is encouraging, false rejects with IA were a source of annoyance for 35% of the participants and false accepts and detection delay were prime security concerns for 27% and 22% of the participants, respectively. We point out these and other barriers to the adoption of IA and suggest directions to overcome them.', 'corpus_id': 12012671, 'score': 1}]"
91	{'doc_id': '222291214', 'title': 'Contrastive Representation Learning: A Framework and Review', 'abstract': 'Contrastive Learning has recently received interest due to its success in self-supervised representation learning in the computer vision domain. However, the origins of Contrastive Learning date as far back as the 1990s and its development has spanned across many fields and domains including Metric Learning and natural language processing. In this paper, we provide a comprehensive literature review and we propose a general Contrastive Representation Learning framework that simplifies and unifies many different contrastive learning methods. We also provide a taxonomy for each of the components of contrastive learning in order to summarise it and distinguish it from other forms of machine learning. We then discuss the inductive biases which are present in any contrastive learning system and we analyse our framework under different views from various sub-fields of Machine Learning. Examples of how contrastive learning has been applied in computer vision, natural language processing, audio processing, and others, as well as in Reinforcement Learning are also presented. Finally, we discuss the challenges and some of the most promising future research directions ahead.', 'corpus_id': 222291214}	14806	"[{'doc_id': '93619627', 'title': 'Hipólito de Eurípides: una visión política de la alteridad', 'abstract': 'Resumen es: La relacion entre pretendientes, nobles y servidores resulta muy compleja en Odisea. Dentro del expandido mundo domestico que el poema presenta no se ha...', 'corpus_id': 93619627, 'score': 0}, {'doc_id': '229678289', 'title': 'Deep Visual Domain Adaptation', 'abstract': 'Domain adaptation (DA) aims at improving the performance of a model on target domains by transferring the knowledge contained in different but related source domains. With recent advances in deep learning models which are extremely data hungry, the interest for visual DA has significantly increased in the last decade and the number of related work in the field exploded. The aim of this paper, therefore, is to give a comprehensive overview of deep domain adaptation methods for computer vision applications. First, we detail and compared different possible ways of exploiting deep architectures for domain adaptation. Then, we propose an overview of recent trends in deep visual DA. Finally, we mention a few improvement strategies, orthogonal to these methods, that can be applied to these models. While we mainly focus on image classification, we give pointers to papers that extend these ideas for other applications such as semantic segmentation, object detection, person re-identifications, and others.', 'corpus_id': 229678289, 'score': 1}, {'doc_id': '221702858', 'title': 'Efficient Transformers: A Survey', 'abstract': 'Transformer model architectures have garnered immense interest lately due to their effectiveness across a range of domains like language, vision and reinforcement learning. In the field of natural language processing for example, Transformers have become an indispensable staple in the modern deep learning stack. Recently, a dizzying number of ""X-former"" models have been proposed - Reformer, Linformer, Performer, Longformer, to name a few - which improve upon the original Transformer architecture, many of which make improvements around computational and memory efficiency. With the aim of helping the avid researcher navigate this flurry, this paper characterizes a large and thoughtful selection of recent efficiency-flavored ""X-former"" models, providing an organized and comprehensive overview of existing work and models across multiple domains.', 'corpus_id': 221702858, 'score': 1}, {'doc_id': '232240392', 'title': 'Spatial Dependency Networks: Neural Layers for Improved Generative Image Modeling', 'abstract': 'How to improve generative modeling by better exploiting spatial regularities and coherence in images? We introduce a novel neural network for building image generators (decoders) and apply it to variational autoencoders (VAEs). In our spatial dependency networks (SDNs), feature maps at each level of a deep neural net are computed in a spatially coherent way, using a sequential gating-based mechanism that distributes contextual information across 2-D space. We show that augmenting the decoder of a hierarchical VAE by spatial dependency layers considerably improves density estimation over baseline convolutional architectures and the state-of-the-art among the models within the same class. Furthermore, we demonstrate that SDN can be applied to large images by synthesizing samples of high quality and coherence. In a vanilla VAE setting, we find that a powerful SDN decoder also improves learning disentangled representations, indicating that neural architectures play an important role in this task. Our results suggest favoring spatial dependency over convolutional layers in various VAE settings. The accompanying source code is given at: https://github.com/djordjemila/sdn.', 'corpus_id': 232240392, 'score': 0}, {'doc_id': '232170565', 'title': 'Reframing Neural Networks: Deep Structure in Overcomplete Representations', 'abstract': 'In comparison to classical shallow representation learning techniques, deep neural networks have achieved superior performance in nearly every application benchmark. But despite their clear empirical advantages, it is still not well understood what makes them so effective. To approach this question, we introduce deep frame approximation, a unifying framework for representation learning with structured overcomplete frames. While exact inference requires iterative optimization, it may be approximated by the operations of a feed-forward deep neural network. We then indirectly analyze how model capacity relates to the frame structure induced by architectural hyperparameters such as depth, width, and skip connections. We quantify these structural differences with the deep frame potential, a data-independent measure of coherence linked to representation uniqueness and stability. As a criterion for model selection, we show correlation with generalization error on a variety of common deep network architectures such as ResNets and DenseNets. We also demonstrate how recurrent networks implementing iterative optimization algorithms achieve performance comparable to their feed-forward approximations. This connection to the established theory of overcomplete representations suggests promising new directions for principled deep network architecture design with less reliance on ad-hoc engineering.', 'corpus_id': 232170565, 'score': 0}, {'doc_id': '220265779', 'title': 'Involutive MCMC: a Unifying Framework', 'abstract': 'Markov Chain Monte Carlo (MCMC) is a computational approach to fundamental problems such as inference, integration, optimization, and simulation. The field has developed a broad spectrum of algorithms, varying in the way they are motivated, the way they are applied and how efficiently they sample. Despite all the differences, many of them share the same core principle, which we unify as the Involutive MCMC (iMCMC) framework. Building upon this, we describe a wide range of MCMC algorithms in terms of iMCMC, and formulate a number of ""tricks"" which one can use as design principles for developing new MCMC algorithms. Thus, iMCMC provides a unified view of many known MCMC algorithms, which facilitates the derivation of powerful extensions. We demonstrate the latter with two examples where we transform known reversible MCMC algorithms into more efficient irreversible ones.', 'corpus_id': 220265779, 'score': 1}, {'doc_id': '53081770', 'title': 'Efficient Computing of Radius-Bounded k-Cores', 'abstract': 'Driven by real-life applications in geo-social networks, in this paper, we investigate the problem of computing the radius-bounded k-cores (RB-k-cores) that aims to find cohesive subgraphs satisfying both social and spatial constraints on large geo-social networks. In particular, we use k-core to ensure the social cohesiveness and we use a radius-bounded circle to restrict the locations of users in a RB-k-core. We explore several algorithmic paradigms to compute RB-k-cores, including a triple vertex-based paradigm, a binary-vertex-based paradigm, and a paradigm utilizing the concept of rotating circles. The rotating circle-based paradigm is further enhanced with several pruning techniques to achieve better efficiency. The experimental studies conducted on both real and synthetic datasets demonstrate that our proposed rotating-circle-based algorithms can compute all RB-k-cores very efficiently. Moreover, it can also be used to compute the minimum-circle-bounded k-core and significantly outperforms the existing techniques for computing the minimum circle-bounded k-core.', 'corpus_id': 53081770, 'score': 0}, {'doc_id': '232233740', 'title': 'Deep Consensus Learning', 'abstract': 'Both generative learning and discriminative learning have recently witnessed remarkable progress using Deep Neural Networks (DNNs). For structured input synthesis and structured output prediction problems (e.g., layout-toimage synthesis and image semantic segmentation respectively), they often are studied separately. This paper proposes deep consensus learning (DCL) for joint layout-toimage synthesis and weakly-supervised image semantic segmentation. The former is realized by a recently proposed LostGAN approach [61], and the latter by introducing an inference network as the third player joining the two-player game of LostGAN. Two deep consensus mappings are exploited to facilitate training the three networks end-to-end: Given an input layout (a list of object bounding boxes), the generator generates a mask (label map) and then use it to help synthesize an image. The inference network infers the mask for the synthesized image. Then, the latent consensus is measured between the mask generated by the generator and the one inferred by the inference network. For the real image corresponding to the input layout, its mask also is computed by the inference network, and then used by the generator to reconstruct the real image. Then, the data consensus is measured between the real image and its reconstructed image. The discriminator still plays the role of an adversary by computing the realness scores for a real image, its reconstructed image and a synthesized image. In experiments, our DCL is tested in the COCO-Stuff dataset [7]. It obtains compelling layout-to-image synthesis results and weakly-supervised image semantic segmentation results.', 'corpus_id': 232233740, 'score': 0}, {'doc_id': '10510670', 'title': 'Tutorial on Variational Autoencoders', 'abstract': 'In just three years, Variational Autoencoders (VAEs) have emerged as one of the most popular approaches to unsupervised learning of complicated distributions. VAEs are appealing because they are built on top of standard function approximators (neural networks), and can be trained with stochastic gradient descent. VAEs have already shown promise in generating many kinds of complicated data, including handwritten digits, faces, house numbers, CIFAR images, physical models of scenes, segmentation, and predicting the future from static images. This tutorial introduces the intuitions behind VAEs, explains the mathematics behind them, and describes some empirical behavior. No prior knowledge of variational Bayesian methods is assumed.', 'corpus_id': 10510670, 'score': 1}, {'doc_id': '218571031', 'title': 'Machine Learning on Graphs: A Model and Comprehensive Taxonomy', 'abstract': 'There has been a surge of recent interest in learning representations for graph-structured data. Graph representation learning methods have generally fallen into three main categories, based on the availability of labeled data. The first, network embedding (such as shallow graph embedding or graph auto-encoders), focuses on learning unsupervised representations of relational structure. The second, graph regularized neural networks, leverages graphs to augment neural network losses with a regularization objective for semi-supervised learning. The third, graph neural networks, aims to learn differentiable functions over discrete topologies with arbitrary structure. However, despite the popularity of these areas there has been surprisingly little work on unifying the three paradigms. Here, we aim to bridge the gap between graph neural networks, network embedding and graph regularization models. We propose a comprehensive taxonomy of representation learning methods for graph-structured data, aiming to unify several disparate bodies of work. Specifically, we propose a Graph Encoder Decoder Model (GRAPHEDM), which generalizes popular algorithms for semi-supervised learning on graphs (e.g. GraphSage, Graph Convolutional Networks, Graph Attention Networks), and unsupervised learning of graph representations (e.g. DeepWalk, node2vec, etc) into a single consistent approach. To illustrate the generality of this approach, we fit over thirty existing methods into this framework. We believe that this unifying view both provides a solid foundation for understanding the intuition behind these methods, and enables future research in the area.', 'corpus_id': 218571031, 'score': 1}]"
92	{'doc_id': '211259472', 'title': 'Learning to Continually Learn', 'abstract': 'Continual lifelong learning requires an agent or model to learn many sequentially ordered tasks, building on previous knowledge without catastrophically forgetting it. Much work has gone towards preventing the default tendency of machine learning models to catastrophically forget, yet virtually all such work involves manually-designed solutions to the problem. We instead advocate meta-learning a solution to catastrophic forgetting, allowing AI to learn to continually learn. Inspired by neuromodulatory processes in the brain, we propose A Neuromodulated Meta-Learning Algorithm (ANML). It differentiates through a sequential learning process to meta-learn an activation-gating function that enables context-dependent selective activation within a deep neural network. Specifically, a neuromodulatory (NM) neural network gates the forward pass of another (otherwise normal) neural network called the prediction learning network (PLN). The NM network also thus indirectly controls selective plasticity (i.e. the backward pass of) the PLN. ANML enables continual learning without catastrophic forgetting at scale: it produces state-of-the-art continual learning performance, sequentially learning as many as 600 classes (over 9,000 SGD updates).', 'corpus_id': 211259472}	1026	"[{'doc_id': '55355911', 'title': ""ENHANCING SUPPLIER'S QUALITY BY PREVENTIVE QUALITY ASSESSMENTS DURING DESIGN OF AUTOMOTIVE ELECTRONICS"", 'abstract': '1. Importance of mechanical vendor parts’ quality In the following, proceeding and results of a strategic improvement project aiming at enhancing quality of mechanical vendor parts are presented. Bosch Automotive Electronics is an automobile supplier with globally distributed development and production. Many mechanical components are provided by local suppliers. For the last few years, the ability to produce with a defect rate less than 10 parts per million (ppm) advanced as criterion for market entry of automotive electronics products. Therefore much effort was spend on quality improvements. One of the most important influence factors on product quality were electronic vendor parts. Present quality statistics prove success of actions taken. For example, in the years 2003 to 2006, 0-miles-defects of electronic vendor parts were diminished from 0.18 ppm down to 0.04 ppm. However, the better electronics quality became, the more mechanics quality was spotlighted. In contrast to electronics, quality problems with mechanical vendor parts in high level series production sometimes follow no pattern. Problems can occur unexpectedly and may have massive effects. As an example, a well proven tooling may suddenly cause immense problems if a wrong tooling insert for a specific connector plug was taken. What makes quality of mechanical vendor parts even more critical is that suppliers are often mediumsized businesses with little capacity for process development. Automotive capability involves steadily increasing requirements, e.g. minimization of particle contamination. In comparison, suppliers of electronic vendor parts typically are large-scale enterprises ensuring their production processes independently. For them, automotive electronics products make up only small share of the market. Further, electronic vendor parts are released comprehensively for many projects and are standardized as far as possible. Mechanical components and toolings however have to be released for each project. Therefore, their release process always lies on critical path of a development project.', 'corpus_id': 55355911, 'score': 0}, {'doc_id': '221764733', 'title': 'Finding relationships among biological entities', 'abstract': '\n Confusion over the concepts of “relationships” and “similarities” lies at the heart of many battles over the direction and intent of research projects. Here is a short story that demonstrates the difference between the two concepts: You look up at the clouds, and you begin to see the shape of a lion. The cloud has a tail, like a lion’s tale, and a fluffy head, like a lion’s mane. With a little imagination the mouth of the lion seems to roar down from the sky. You have succeeded in finding similarities between the cloud and a lion. If you look at a cloud and you imagine a tea kettle producing a head of steam and you recognize that the physical forces that create a cloud and the physical forces that produced steam from a heated kettle are the same, then you have found a relationship. Most popular classification algorithms operate by grouping together data objects that have similar properties or values. In so doing, they may miss finding the true relationships among objects. Traditionally, relationships among data objects are discovered by an intellectual process. In this chapter, we will discuss the scientific gains that come when we classify biological entities by relationships, not by their similarities.\n', 'corpus_id': 221764733, 'score': 0}, {'doc_id': '211146649', 'title': 'Evolutionary optimization of deep learning activation functions', 'abstract': 'The choice of activation function can have a large effect on the performance of a neural network. While there have been some attempts to hand-engineer novel activation functions, the Rectified Linear Unit (ReLU) remains the most commonly-used in practice. This paper shows that evolutionary algorithms can discover novel activation functions that outperform ReLU. A tree-based search space of candidate activation functions is defined and explored with mutation, crossover, and exhaustive search. Experiments on training wide residual networks on the CIFAR-10 and CIFAR-100 image datasets show that this approach is effective. Replacing ReLU with evolved activation functions results in statistically significant increases in network accuracy. Optimal performance is achieved when evolution is allowed to customize activation functions to a particular task; however, these novel activation functions are shown to generalize, achieving high performance across tasks. Evolutionary optimization of activation functions is therefore a promising new dimension of metalearning in neural networks.', 'corpus_id': 211146649, 'score': 1}, {'doc_id': '219530794', 'title': 'Efficient Architecture Search for Continual Learning', 'abstract': 'Continual learning with neural networks is an important learning framework in AI that aims to learn a sequence of tasks well. However, it is often confronted with three challenges: (1) overcome the catastrophic forgetting problem, (2) adapt the current network to new tasks, and meanwhile (3) control its model complexity. To reach these goals, we propose a novel approach named as Continual Learning with Efficient Architecture Search, or CLEAS in short. CLEAS works closely with neural architecture search (NAS) which leverages reinforcement learning techniques to search for the best neural architecture that fits a new task. In particular, we design a neuron-level NAS controller that decides which old neurons from previous tasks should be reused (knowledge transfer), and which new neurons should be added (to learn new knowledge). Such a fine-grained controller allows one to find a very concise architecture that can fit each new task well. Meanwhile, since we do not alter the weights of the reused neurons, we perfectly memorize the knowledge learned from previous tasks. We evaluate CLEAS on numerous sequential classification tasks, and the results demonstrate that CLEAS outperforms other state-of-the-art alternative methods, achieving higher classification accuracy while using simpler neural architectures.', 'corpus_id': 219530794, 'score': 0}, {'doc_id': '142581481', 'title': 'Instructional practices conducive to the high achievement of Hispanic limited English proficient students on the Texas Assessment of Knowledge and Skills', 'abstract': 'The goal of current education reform is to increase student achievement (Odden & Clune, 1995). Discrepancies, however, continue to exist in the achievement between the White majority and the minorities of color, including Hispanics as seen in the results of the Texas Assessment of Knowledge and Skills (TAKS). Although 198 Texas elementary schools received an exemplary rating in 2005 for their TAKS performance, only a handful of those schools with a high percentage of Hispanic, economically disadvantaged, and Limited English Proficient (LEP) student enrollment achieved this coveted academic rating (TEA, 2005). This study attempts to answer the research questions: 1) Which, if any, instructional practices are present in the exemplary-rated campuses with high numbers of Hispanic LEP students compared to acceptable-rated campuses with the same type of viii student populations? and 2) Are educators aware of and modifying their instructional practices to be more aligned with proven research-based practices? The Best Practice and Benchmark Concept provides the framework for the study. The design includes the use of a survey, interviews, an observation checklist, and an analysis of documents to compare the practices of two exemplary-rated campuses and two acceptable-rated campuses, all spanning grades PreK-5 grade, enrolling at least 500 students, and serving high percentages of Hispanic, economically disadvantaged, and LEP students. Findings revealed differences in the consistent use of best practices, in the methods of instruction (structured and directive versus constructivist), in the positive attitude and commitment of teachers, in the type of research-based programs, and in the instructional settings of the bilingual/ESL students. The finding of mixing structured, directive instruction to promote student success before moving to a more constructivist method of teaching is a practice rarely encountered in literature. All other practices observed have been documented in literature. In addition, educators were indeed found to be modifying their practices to align with those proven in research. Other factors besides best practices which influence student achievement surfaced, indicating the difference in performance between the exemplary and the acceptable campuses could not be attributed solely to the use of best practices.', 'corpus_id': 142581481, 'score': 0}, {'doc_id': '59597264', 'title': 'Designing neural networks through neuroevolution', 'abstract': 'Much of recent machine learning has focused on deep learning, in which neural network weights are trained through variants of stochastic gradient descent. An alternative approach comes from the field of neuroevolution, which harnesses evolutionary algorithms to optimize neural networks, inspired by the fact that natural brains themselves are the products of an evolutionary process. Neuroevolution enables important capabilities that are typically unavailable to gradient-based approaches, including learning neural network building blocks (for example activation functions), hyperparameters, architectures and even the algorithms for learning themselves. Neuroevolution also differs from deep learning (and deep reinforcement learning) by maintaining a population of solutions during search, enabling extreme exploration and massive parallelization. Finally, because neuroevolution research has (until recently) developed largely in isolation from gradient-based neural network research, it has developed many unique and effective techniques that should be effective in other machine learning areas too. This Review looks at several key aspects of modern neuroevolution, including large-scale computing, the benefits of novelty and diversity, the power of indirect encoding, and the field’s contributions to meta-learning and architecture search. Our hope is to inspire renewed interest in the field as it meets the potential of the increasing computation available today, to highlight how many of its ideas can provide an exciting resource for inspiration and hybridization to the deep learning, deep reinforcement learning and machine learning communities, and to explain how neuroevolution could prove to be a critical tool in the long-term pursuit of artificial general intelligence.Deep neural networks have become very successful at certain machine learning tasks partly due to the widely adopted method of training called backpropagation. An alternative way to optimize neural networks is by using evolutionary algorithms, which, fuelled by the increase in computing power, offers a new range of capabilities and modes of learning.', 'corpus_id': 59597264, 'score': 1}, {'doc_id': '204791367', 'title': 'Learning to Learn with Feedback and Local Plasticity', 'abstract': ""Interest in biologically inspired alternatives to backpropagation is driven by the desire to both advance connections between deep learning and neuroscience and address backpropagation's shortcomings on tasks such as online, continual learning. However, local synaptic learning rules like those employed by the brain have so far failed to match the performance of backpropagation in deep networks. In this study, we employ meta-learning to discover networks that learn using feedback connections and local, biologically inspired learning rules. Importantly, the feedback connections are not tied to the feedforward weights, avoiding biologically implausible weight transport. Our experiments show that meta-trained networks effectively use feedback connections to perform online credit assignment in multi-layer architectures. Surprisingly, this approach matches or exceeds a state-of-the-art gradient-based online meta-learning algorithm on regression and classification tasks, excelling in particular at continual learning. Analysis of the weight updates employed by these models reveals that they differ qualitatively from gradient descent in a way that reduces interference between updates. Our results suggest the existence of a class of biologically plausible learning mechanisms that not only match gradient descent-based learning, but also overcome its limitations."", 'corpus_id': 204791367, 'score': 1}, {'doc_id': '211082582', 'title': 'Neuroevolution of Neural Network Architectures Using CoDeepNEAT and Keras', 'abstract': 'Machine learning is a huge field of study in computer science and statistics dedicated to the execution of computational tasks through algorithms that do not require explicit instructions but instead rely on learning patterns from data samples to automate inferences. A large portion of the work involved in a machine learning project is to define the best type of algorithm to solve a given problem. Neural networks - especially deep neural networks - are the predominant type of solution in the field. However, the networks themselves can produce very different results according to the architectural choices made for them. Finding the optimal network topology and configurations for a given problem is a challenge that requires domain knowledge and testing efforts due to a large number of parameters that need to be considered. The purpose of this work is to propose an adapted implementation of a well-established evolutionary technique from the neuroevolution field that manages to automate the tasks of topology and hyperparameter selection. It uses a popular and accessible machine learning framework - Keras - as the back-end, presenting results and proposed changes concerning the original algorithm. The implementation is available at GitHub (this https URL) with documentation and examples to reproduce the experiments performed for this work.', 'corpus_id': 211082582, 'score': 0}, {'doc_id': '212628602', 'title': 'Finding online neural update rules by learning to remember', 'abstract': 'We investigate learning of the online local update rules for neural activations (bodies) and weights (synapses) from scratch. We represent the states of each weight and activation by small vectors, and parameterize their updates using (meta-) neural networks. Different neuron types are represented by different embedding vectors which allows the same two functions to be used for all neurons. Instead of training directly for the objective using evolution or long term back-propagation, as is commonly done in similar systems, we motivate and study a different objective: That of remembering past snippets of experience. We explain how this objective relates to standard back-propagation training and other forms of learning. We train for this objective using short term back-propagation and analyze the performance as a function of both the different network types and the difficulty of the problem. We find that this analysis gives interesting insights onto what constitutes a learning rule. We also discuss how such system could form a natural substrate for addressing topics such as episodic memories, meta-learning and auxiliary objectives.', 'corpus_id': 212628602, 'score': 1}, {'doc_id': '221780511', 'title': 'Soft Computing Techniques in Various Areas', 'abstract': 'Soft computing is a study of the science of logic, thinking, analysis and research that combines real-world problems with biologically inspired methods. Soft computing is the main motivation behind the idea of conceptual intelligence in machines. As such, it is an extension of heuristics and the resolution of complex problems that are very difficult to model mathematically. Smooth computing tolerates printing; uncertainty and approximation that differ from manual calculation. Soft Computing enumerates techniques like ANN, Evolutionary computing, Fuzzy Logic and statistics, they are advantageous and separately applied techniques which are used together to solve problems which are complex, very easily. This article highlights the various soft computing ting techniques and emerging areas of soft computing ting where they have been successfully implemented.', 'corpus_id': 221780511, 'score': 0}]"
93	{'doc_id': '237194731', 'title': 'FACIAL: Synthesizing Dynamic Talking Face with Implicit Attribute Learning', 'abstract': 'In this paper, we propose a talking face generation method that takes an audio signal as input and a short target video clip as reference, and synthesizes a photo-realistic video of the target face with natural lip motions, head poses, and eye blinks that are in-sync with the input audio signal. We note that the synthetic face attributes include not only explicit ones such as lip motions that have high correlations with speech, but also implicit ones such as head poses and eye blinks that have only weak correlation with the input audio. To model such complicated relationships among different face attributes with input audio, we propose a FACe Implicit Attribute Learning Generative Adversarial Network (FACIAL-GAN), which integrates the phoneticsaware, context-aware, and identity-aware information to synthesize the 3D face animation with realistic motions of lips, head poses, and eye blinks. Then, our Rendering-toVideo network takes the rendered face images and the attention map of eye blinks as input to generate the photorealistic output video frames. Experimental results and user studies show our method can generate realistic talking face videos with not only synchronized lip motions, but also natural head movements and eye blinks, with better qualities than the results of state-of-the-art methods.', 'corpus_id': 237194731}	19770	[{'doc_id': '236635478', 'title': 'OpenForensics: Large-Scale Challenging Dataset For Multi-Face Forgery Detection And Segmentation In-The-Wild', 'abstract': 'The proliferation of deepfake media is raising concerns among the public and relevant authorities. It has become essential to develop countermeasures against forged faces in social media. This paper presents a comprehensive study on two new countermeasure tasks: multi-face forgery detection and segmentation in-the-wild. Localizing forged faces among multiple human faces in unrestricted natural scenes is far more challenging than the traditional deepfake recognition task. To promote these new tasks, we have created the first large-scale dataset posing a high level of challenges that is designed with face-wise rich annotations explicitly for face forgery detection and segmentation, namely OpenForensics. With its rich annotations, our OpenForensics dataset has great potentials for research in both deepfake prevention and general human face detection. We have also developed a suite of benchmarks for these tasks by conducting an extensive evaluation of state-of-the-art instance detection and segmentation methods on our newly constructed dataset in various scenarios.', 'corpus_id': 236635478, 'score': 1}, {'doc_id': '236635340', 'title': 'Temporal Feature Warping for Video Shadow Detection', 'abstract': 'While single image shadow detection has been improving rapidly in recent years, video shadow detection remains a challenging task due to data scarcity and the difficulty in modelling temporal consistency. The current video shadow detection method achieves this goal via co-attention, which mostly exploits information that is temporally coherent but is not robust in detecting moving shadows and small shadow regions. In this paper, we propose a simple but powerful method to better aggregate information temporally. We use an optical flow based warping module to align and then combine features between frames. We apply this warping module across multiple deep-network layers to retrieve information from neighboring frames including both local details and high-level semantic information. We train and test our framework on the ViSha dataset. Experimental results show that our model outperforms the state-of-the-art video shadow detection method by 28%, reducing BER from 16.7 to 12.0.', 'corpus_id': 236635340, 'score': 0}, {'doc_id': '237266437', 'title': 'A Synthesis-Based Approach for Thermal-to-Visible Face Verification', 'abstract': 'In recent years, visible-spectrum face verification systems have been shown to match expert forensic examiner recognition performance. However, such systems are ineffective in low-light and nighttime conditions. Thermal face imagery, which captures body heat emissions, effectively augments the visible spectrum, capturing discriminative facial features in scenes with limited illumination. Due to the increased cost and difficulty of obtaining diverse, paired thermal and visible spectrum datasets, algorithms and large-scale benchmarks for low-light recognition are limited. This paper presents an algorithm that achieves state-of-the-art performance on both the ARL-VTF and TUFTS multi-spectral face datasets. Importantly, we study the impact of face alignment, pixel-level correspondence, and identity classification with label smoothing for multi-spectral face synthesis and verification. We show that our proposed method is widely applicable, robust, and highly effective. In addition, we show that the proposed method significantly outperforms face frontalization methods on profileto-frontal verification. Finally, we present MILAB-VTF(B), a challenging multi-spectral face dataset that is composed of paired thermal and visible videos. To the best of our knowledge, with face data from 400 subjects, this dataset represents the most extensive collection of publicly available indoor and longrange outdoor thermal-visible face imagery. Lastly, we show that our end-to-end thermal-to-visible face verification system provides strong performance on the MILAB-VTF(B) dataset.', 'corpus_id': 237266437, 'score': 0}, {'doc_id': '236087940', 'title': 'Heterogeneous Face Frontalization via Domain Agnostic Learning', 'abstract': 'Recent advances in deep convolutional neural networks (DCNNs) have shown impressive performance improvements on thermal to visible face synthesis and matching problems. However, current DCNN-based synthesis models do not perform well on thermal faces with large pose variations. In order to deal with this problem, heterogeneous face frontalization methods are needed in which a model takes a thermal profile face image and generates a frontal visible face. This is an extremely difficult problem due to the large domain as well as large pose discrepancies between the two modalities. Despite its applications in biometrics and surveillance, this problem is relatively unexplored in the literature. We propose a domain agnostic learning-based generative adversarial network (DAL-GAN) which can synthesize frontal views in the visible domain from thermal faces with pose variations. DAL-GAN consists of a generator with an auxiliary classifier and two discriminators which capture both local and global texture discriminations for better synthesis. A contrastive constraint is enforced in the latent space of the generator with the help of a dual-path training strategy, which improves the feature vector’s discrimination. Finally, a multi-purpose loss function is utilized to guide the network in synthesizing identitypreserving cross-domain frontalization. Extensive experimental results demonstrate that DAL-GAN can generate better quality frontal views compared to the other baseline methods.', 'corpus_id': 236087940, 'score': 0}, {'doc_id': '236956926', 'title': 'Two-stream Convolutional Networks for Multi-frame Face Anti-spoofing', 'abstract': 'Face anti-spoofing is an important task to protect the security of face recognition. Most of previous work either struggle to capture discriminative and generalizable feature or rely on auxiliary information which is unavailable for most of industrial product. Inspired by the video classification work, we propose an efficient two-stream model to capture the key differences between live and spoof faces, which takes multi-frames and RGB difference as input respectively. Feature pyramid modules with two opposite fusion directions and pyramid pooling modules are applied to enhance feature representation. We evaluate the proposed method on the datasets of Siw, Oulu-NPU, CASIAMFSD and Replay-Attack. The results show that our model achieves the state-of-the-art results on most of datasets’ protocol with much less parameter size.', 'corpus_id': 236956926, 'score': 1}, {'doc_id': '235795097', 'title': 'Multi-modality Deep Restoration of Extremely Compressed Face Videos', 'abstract': 'Arguably the most common and salient object in daily video communications is the talking head, as encountered in social media, virtual classrooms, teleconferences, news broadcasting, talk shows, etc. When communication bandwidth is limited by network congestions or cost effectiveness, compression artifacts in talking head videos are inevitable. The resulting video quality degradation is highly visible and objectionable due to high acuity of human visual system to faces. To solve this problem, we develop a multi-modality deep convolutional neural network method for restoring face videos that are aggressively compressed. The main innovation is a new DCNN architecture that incorporates known priors of multiple modalities: the video-synchronized speech signal and semantic elements of the compression code stream, including motion vectors, code partition map and quantization parameters. These priors strongly correlate with the latent video and hence they are able to enhance the capability of deep learning to remove compression artifacts. Ample empirical evidences are presented to validate the superior performance of the proposed DCNN method on face videos over the existing state-of-the-art methods.', 'corpus_id': 235795097, 'score': 1}, {'doc_id': '235828780', 'title': 'Detect and Locate: A Face Anti-Manipulation Approach with Semantic and Noise-level Supervision', 'abstract': 'The technological advancements of deep learning have enabled sophisticated face manipulation schemes, raising severe trust issues and security concerns in modern society. Generally speaking, detecting manipulated faces and locating the potentially altered regions are challenging tasks. Herein, we propose a conceptually simple but effective method to efficiently detect forged faces in an image while simultaneously locating the manipulated regions. The proposed scheme relies on a segmentation map that delivers meaningful high-level semantic information clues about the image. Furthermore, a noise map is estimated, playing a complementary role in capturing low-level clues and subsequently empowering decision-making. Finally, the features from these two modules are combined to distinguish fake faces. Extensive experiments show that the proposed model achieves state-of-the-art detection accuracy and remarkable localization performance.', 'corpus_id': 235828780, 'score': 1}, {'doc_id': '235670062', 'title': 'Deep Learning for Face Anti-Spoofing: A Survey', 'abstract': 'Face anti-spoofing (FAS) has lately attracted increasing attention due to its vital role in securing face recognition systems from presentation attacks (PAs). As more and more realistic PAs with novel types spring up, traditional FAS methods based on handcrafted features become unreliable due to their limited representation capacity. With the emergence of large-scale academic datasets in the recent decade, deep learning based FAS achieves remarkable performance and dominates this area. However, existing reviews in this field mainly focus on the handcrafted features, which are outdated and uninspiring for the progress of FAS community. In this paper, to stimulate future research, we present the first comprehensive review of recent advances in deep learning based FAS. It covers several novel and insightful components: 1) besides supervision with binary label (e.g., ‘0’ for bonafide vs. ‘1’ for PAs), we also investigate recent methods with pixel-wise supervision (e.g., pseudo depth map); 2) in addition to traditional intra-dataset evaluation, we collect and analyze the latest methods specially designed for domain generalization and open-set FAS; and 3) besides commercial RGB camera, we summarize the deep learning applications under multi-modal (e.g., depth and infrared) or specialized (e.g., light field and flash) sensors. We conclude this survey by emphasizing current open issues and highlighting potential prospects.', 'corpus_id': 235670062, 'score': 1}, {'doc_id': '235716689', 'title': 'Makeup removal for face verification based upon contrastive learning', 'abstract': 'Makeup, derived from human`s pursuit of beauty, is widely accepted by the public. Despite its popularization, there are little effect been made to tackle the makeup face verification challenges. Aiming to promote existing verification system to accept or reject the claimed identity of a person with makeup in an image, a makeup robust face verification framework is proposed based upon a generative adversarial network. The proposal synthesizes non-makeup face images from makeup images for further verification. Specifically, a patchwise contrastive loss is introduced in the generative model to narrow the distance between makeup and non-makeup images. The challenge in the state-of-the-art is the employment of a pre-specified and hand-designed loss function to measure the performance, which is not the case in the proposal. Experimental results demonstrate that the proposal generates non-makeup faces with few artifacts and achieves 96.3% accuracy on Dataset1 in face verification, which is at least 0.8% better than some well discussed models.', 'corpus_id': 235716689, 'score': 0}, {'doc_id': '236428234', 'title': 'PoseFace: Pose-Invariant Features and Pose-Adaptive Loss for Face Recognition', 'abstract': 'Despite the great success achieved by deep learning methods in face recognition, severe performance drops are observed for large pose variations in unconstrained environments (e.g., in cases of surveillance and photo-tagging). To address it, current methods either deploy pose-specific models or frontalize faces by additional modules. Still, they ignore the fact that identity information should be consistent across poses and are not realizing the data imbalance between frontal and profile face images during training. In this paper, we propose an efficient PoseFace framework which utilizes the facial landmarks to disentangle the poseinvariant features and exploits a pose-adaptive loss to handle the imbalance issue adaptively. Extensive experimental results on the benchmarks of Multi-PIE, CFP, CPLFW and IJB have demonstrated the superiority of our method over the state-of-the-arts.', 'corpus_id': 236428234, 'score': 0}]
94	{'doc_id': '211259030', 'title': 'The Early Phase of Neural Network Training', 'abstract': 'Recent studies have shown that many important aspects of neural network learning take place within the very earliest iterations or epochs of training. For example, sparse, trainable sub-networks emerge (Frankle et al., 2019), gradient descent moves into a small subspace (Gur-Ari et al., 2018), and the network undergoes a critical period (Achille et al., 2019). Here we examine the changes that deep neural networks undergo during this early phase of training. We perform extensive measurements of the network state and its updates during these early iterations of training, and leverage the framework of Frankle et al. (2019) to quantitatively probe the weight distribution and its reliance on various aspects of the dataset. We find that, within this framework, deep networks are not robust to reinitializing with random weights while maintaining signs, and that weight distributions are highly non-independent even after only a few hundred iterations. Despite this, pre-training with blurred inputs or an auxiliary self-supervised task can approximate the changes in supervised networks, suggesting that these changes are label-agnostic, though labels significantly accelerate this process. Together, these results help to elucidate the network changes occurring during this pivotal initial period of learning.', 'corpus_id': 211259030}	13843	"[{'doc_id': '232119481', 'title': 'Trade-offs of Local SGD at Scale: An Empirical Study', 'abstract': 'As datasets and models become increasingly large, distributed training has become a necessary component to allow deep neural networks to train in reasonable amounts of time. However, distributed training can have substantial communication overhead that hinders its scalability. One strategy for reducing this overhead is to perform multiple unsynchronized SGD steps independently on each worker between synchronization steps, a technique known as local SGD. We conduct a comprehensive empirical study of local SGD and related methods on a large scale image classification task. We find that performing local SGD comes at a price: lower communication costs (and thereby faster training) are accompanied by lower accuracy. This finding is in contrast from the smaller-scale experiments in prior work, suggesting that local SGD encounters challenges at scale. We further show that incorporating the slow momentum framework of Wang et al. (2020) consistently improves accuracy without requiring additional communication, hinting at future directions for potentially escaping this trade-off.', 'corpus_id': 232119481, 'score': 0}, {'doc_id': '232320578', 'title': 'How to decay your learning rate', 'abstract': 'Complex learning rate schedules have become an integral part of deep learning. We find empirically that common fine-tuned schedules decay the learning rate after the weight norm bounces. This leads to the proposal of ABEL: an automatic scheduler which decays the learning rate by keeping track of the weight norm. ABEL’s performance matches that of tuned schedules and is more robust with respect to its parameters. Through extensive experiments in vision, NLP, and RL, we show that if the weight norm does not bounce, we can simplify schedules even further with no loss in performance. In such cases, a complex schedule has similar performance to a constant learning rate with a decay at the end of training.', 'corpus_id': 232320578, 'score': 1}, {'doc_id': '231933851', 'title': 'GradInit: Learning to Initialize Neural Networks for Stable and Efficient Training', 'abstract': 'Changes in neural architectures have fostered significant breakthroughs in language modeling and computer vision. Unfortunately, novel architectures often require re-thinking the choice of hyperparameters (e.g., learning rate, warmup schedule, and momentum coefficients) to maintain stability of the optimizer. This optimizer instability is often the result of poor parameter initialization, and can be avoided by architecture-specific initialization schemes. In this paper, we present GradInit, an automated and architecture agnostic method for initializing neural networks. GradInit is based on a simple heuristic; the variance of each network layer is adjusted so that a single step of SGD or Adam results in the smallest possible loss value. This adjustment is done by introducing a scalar multiplier variable in front of each parameter block, and then optimizing these variables using a simple numerical scheme. GradInit accelerates the convergence and test performance of many convolutional architectures, both with or without skip connections, and even without normalization layers. It also enables training the original Post-LN Transformer for machine translation without learning rate warmup under a wide range of learning rates and momentum coefficients. Code is available at https: //github.com/zhuchen03/gradinit.', 'corpus_id': 231933851, 'score': 1}, {'doc_id': '201623370', 'title': 'Recommendations for the Use of Social Media in Pharmacovigilance: Lessons from IMI WEB-RADR', 'abstract': 'Over a period of 3 years, the European Union’s Innovative Medicines Initiative WEB-RADR project has explored the value of social media (i.e., information exchanged through the internet, typically via online social networks) for identifying adverse events as well as for safety signal detection. Many patients and clinicians have taken to social media to discuss their positive and negative experiences of medications, creating a source of publicly available information that has the potential to provide insights into medicinal product safety concerns. The WEB-RADR project has developed a collaborative English language workspace for visualising and analysing social media data for a number of medicinal products. Further, novel text and data mining methods for social media analysis have been developed and evaluated. From this original research, several recommendations are presented with supporting rationale and consideration of the limitations. Recommendations for further research that extend beyond the scope of the current project are also presented.', 'corpus_id': 201623370, 'score': 0}, {'doc_id': '36738097', 'title': 'Benefits of the low pressure multichannel endotracheal ventilation.', 'abstract': 'Mechanical ventilation using a modified endotracheal tube, allowing bypass and washout of the endotracheal dead space (McETV), was compared with conventional controlled mechanical ventilation (CMV) in healthy and in surfactant-depleted rabbits. In healthy animals, shifting from CMV to McETV led to an increase in PaO2 (89 +/- 16 versus 104 +/- 13 mm Hg; p < 0.05) and a decrease in PaCO2 (41.5 +/- 3 versus 30 +/- 3 mm Hg; p < 0.05). As a result of reducing the peak inspiratory pressure (PIP) from 21 +/- 2 to 12 +/- 2 cm H2O (p < 0.05), it was possible in McETV mode to maintain comparable ventilation to that achieved by CMV. In surfactant-depleted animals, compared with CMV, McETV produced a rise in PaO2 without change in thoracic volume (from 100 +/- 40 to 150 +/- 60 mm Hg, p < 0.05) and a fall in PaCO2 (from 46 +/- 5 to 37 +/- 4 mm Hg, p < 0.05). After 4 h of ventilation, the surfactant-depleted animals from the CMV group developed thoracic overdistension quicker (at hour 1, p < 0.05) and, consequently, more animals died from pneumothorax compared with the McETV group (five versus two). We concluded that McETV ensured adequate gas exchanges with lower insufflation pressures and could diminish positive pressure ventilation-induced injury.', 'corpus_id': 36738097, 'score': 0}, {'doc_id': '231986276', 'title': 'A Probabilistically Motivated Learning Rate Adaptation for Stochastic Optimization', 'abstract': 'Machine learning practitioners invest significant manual and computational resources in finding suitable learning rates for optimization algorithms. We provide a probabilistic motivation, in terms of Gaussian inference, for popular stochastic firstorder methods. As an important special case, it recovers the Polyak step with a general metric. The inference allows us to relate the learning rate to a dimensionless quantity that can be automatically adapted during training by a control algorithm. The resulting meta-algorithm is shown to adapt learning rates in a robust manner across a large range of initial values when applied to deep learning benchmark problems.', 'corpus_id': 231986276, 'score': 1}, {'doc_id': '233284212', 'title': 'FACTORIZED NEURAL LAYERS', 'abstract': 'Factorized layers—operations parameterized by products of two or more matrices—occur in a variety of deep learning contexts, including compressed model training, certain types of knowledge distillation, and multi-head selfattention architectures. We study how to initialize and regularize deep nets containing such layers, examining two simple, understudied schemes, spectral initialization and Frobenius decay, for improving their performance. The guiding insight is to design optimization routines for these networks that are as close as possible to that of their well-tuned, non-decomposed counterparts; we back this intuition with an analysis of how the initialization and regularization schemes impact training with gradient descent, drawing on modern attempts to understand the interplay of weight-decay and batch-normalization. Empirically, we highlight the benefits of spectral initialization and Frobenius decay across a variety of settings. In model compression, we show that they enable low-rank methods to significantly outperform both unstructured sparsity and tensor methods on the task of training low-memory residual networks; analogs of the schemes also improve the performance of tensor decomposition techniques. For knowledge distillation, Frobenius decay enables a simple, overcomplete baseline that yields a compact model from over-parameterized training without requiring retraining with or pruning a teacher network. Finally, we show how both schemes applied to multi-head attention lead to improved performance on both translation and unsupervised pre-training.', 'corpus_id': 233284212, 'score': 0}, {'doc_id': '232073314', 'title': 'A Theoretical analysis of early learning and memorization in a linear model', 'abstract': 'In this section, we formalize and substantiate the claims of Theorem 1. Theorem 1 has three parts, which we address in the following sections. First, in Section A.2, we show that the classifier makes progress during the early-learning phase: over the first T iterations, the gradient is well correlated with v and the accuracy on mislabeled examples increases. However, as noted in the main text, this early progress halts because the gradient terms corresponding to correctly labeled examples begin to disappear. We prove this rigorously in Section A.3, which shows that the overall magnitude of the gradient terms corresponding to correctly labeled examples shrinks over the first T iterations. Finally, in Section A.4, we prove the claimed asymptotic behavior: as t ! 1, gradient descent perfectly memorizes the noisy labels.', 'corpus_id': 232073314, 'score': 0}, {'doc_id': '195874215', 'title': 'Towards Explaining the Regularization Effect of Initial Large Learning Rate in Training Neural Networks', 'abstract': ""Stochastic gradient descent with a large initial learning rate is a widely adopted method for training modern neural net architectures. Although a small initial learning rate allows for faster training and better test performance initially, the large learning rate achieves better generalization soon after the learning rate is annealed. Towards explaining this phenomenon, we devise a setting in which we can prove that a two layer network trained with large initial learning rate and annealing provably generalizes better than the same network trained with a small learning rate from the start. The key insight in our analysis is that the order of learning different types of patterns is crucial: because the small learning rate model first memorizes low noise, hard-to-fit patterns, it generalizes worse on higher noise, easier-to-fit patterns than its large learning rate counterpart. This concept translates to a larger-scale setting: we demonstrate that one can add a small patch to CIFAR-10 images that is immediately memorizable by a model with small initial learning rate, but ignored by the model with large learning rate until after annealing. Our experiments show that this causes the small learning rate model's accuracy on unmodified images to suffer, as it relies too much on the patch early on."", 'corpus_id': 195874215, 'score': 1}, {'doc_id': '204008515', 'title': 'On the adequacy of untuned warmup for adaptive optimization', 'abstract': 'Adaptive optimization algorithms such as Adam (Kingma & Ba, 2014) are widely used in deep learning. The stability of such algorithms is often improved with a warmup schedule for the learning rate. Motivated by the difficulty of choosing and tuning warmup schedules, Liu et al. (2019) propose automatic variance rectification of Adam\'s adaptive learning rate, claiming that this rectified approach (""RAdam"") surpasses the vanilla Adam algorithm and reduces the need for expensive tuning of Adam with warmup. In this work, we point out various shortcomings of this analysis. We then provide an alternative explanation for the necessity of warmup based on the magnitude of the update term, which is of greater relevance to training stability. Finally, we provide some ""rule-of-thumb"" warmup schedules, and we demonstrate that simple untuned warmup of Adam performs more-or-less identically to RAdam in typical practical settings. We conclude by suggesting that practitioners stick to linear warmup with Adam, with a sensible default being linear warmup over $2 / (1 - \\beta_2)$ training iterations.', 'corpus_id': 204008515, 'score': 1}]"
95	{'doc_id': '103196273', 'title': 'Isolation and Characterization of Polyethylene Glycol (PEG)-Modified Glycol Lignin via PEG Solvolysis of Softwood Biomass in a Large-Scale Batch Reactor', 'abstract': 'We have developed an environmentally benign large-scale (50 kg wood meal per batch) lignin production plant, operating based on acid-catalyzed polyethylene glycol (PEG) solvolysis of softwood biomass. The motivation for the proposed process was to promote technological innovation in biomass utilization systems in Japanese rural areas based on widely abundant Japanese cedar (sugi) biomass. In this study, the process was evaluated by investigating the effects of the source sugi wood meal size and the solvent PEG molecular mass on the yield, chemical structure, molecular mass, and thermal properties of the resultant PEG-modified lignin derivatives, glycol lignins (GLs). Reducing the source wood meal size and PEG solvent molecular mass not only promoted lignin PEGylation but also the subsequent acid-induced chemical rearrangements of the GLs as demonstrated by chemical analyses, 2D NMR, and size exclusion chromatography (SEC). Reducing the source wood meal size and/or increasing the solvent PEG molecular mass...', 'corpus_id': 103196273}	11006	[{'doc_id': '136316858', 'title': 'Polyethylene glycol treatments for waterlogged wood at the cell level', 'abstract': 'The natural dimensional stability of wood is a function of its total water content. The success of a wood treatment depends on the extent to which the consolidating material replaces the water and where the replacement occurs in the wood structure. Polyethylene glycol (PEG) could be expected either to bulk the cell lumina or to infiltrate the cell walls, or do both. The final location of the PEG depends on its physical properties, and these depend on molecular weight. Previously, locating PEG within the cell wall layers was not possible. A new, simple stain has been developed for the light microscopical differentiation of PEG in wood. It is based on cobalt thiocyanate. This paper concerns its use to determine the location of PEG400, 3350, and 540 Blend in transverse sections of treated wood. (See also AATA 21-410)', 'corpus_id': 136316858, 'score': 1}, {'doc_id': '221534048', 'title': 'Encapsulation of fragrances and oils by core-shell structures from silica nanoparticles, surfactant and polymer: Effect of particle size', 'abstract': 'Abstract Oils and fragrances can be encapsulated by using composite shells of silica nanoparticles, polymer and surfactant (potassium oleate). The template for the creation of the core-shell structure is a particle stabilized (Pickering) emulsion. The surfactant adsorbs on the nanoparticles and leads to their reversible hydrophobization and adsorption on the oil/water interface. The outer layer of the self-assembled shell represents a layer from crosslinked polymer. The procedure of encapsulation is simple and includes single homogenization by ultrasound of the formulation that contains all ingredients together. The produced capsules have mean radius in the range between 2 and 11 microns. By order of magnitude and trend, the capsule size follows the law of limited coalescence with respect to the dependence on nanoparticle size and concentration. The composite structure of the shells leads also to dependence on the concentrations of added polymer and surfactant. The produced microcapsules are stable when rinsed with pure water of pH in the range 3 – 10. However, if dispersed in water of pH > 11, the microcapsules are destabilized and release their cargo, i.e., they are pH-responsive. Various fragrances and oils, such as limonene, citronellol, benzyl acetate, and sunflower seed oil were encapsulated. The developed methodology could find applications in any field, in which reversible encapsulation of oily substances is needed.', 'corpus_id': 221534048, 'score': 0}, {'doc_id': '224382370', 'title': 'Effect of composition and sintering temperature on thermal properties of zeolite-alumina composite materials', 'abstract': 'This research work provides a technical description of the utilization of natural zeolites in the synthesis of ceramic composite material using mechanical milling and reactive sintering technique. Two commercially available minerals (Natural zeolite from Mád in Tokaj region and MOTIM Al2O3) were used as starting raw materials, A comprehensive analysis has been conducted for the detailed characterization of raw materials as well as produced products, the analysis combines the mineralogical examination using X-ray diffraction (XRD) together with chemical constituent determination by (XRF) and thermoanalytical studies using (TG/DTA), heating electron microscope and thermal conductivity analyzer to determine the influence of sintering temperatures on the thermal properties of the produced zeolite-alumina composite materials. Based on the results obtained from XRD, XRF and TG/DTA, the authors have found a great connection between the composition, firing temperature and thermal properties of the produced ceramic samples.', 'corpus_id': 224382370, 'score': 0}, {'doc_id': '224803467', 'title': 'Tailoring the Viscoelasticity of Polymer Gels of Gluten Proteins through Solvent Quality', 'abstract': 'We investigate the linear viscoelasticity of polymer gels produced by the dispersion of gluten proteins in water:ethanol binary mixtures with various ethanol contents, from pure water to 60% v/v ethanol. We show that the complex viscoelasticity of the gels exhibits a time/solvent composition superposition principle, demonstrating the self-similarity of the gels produced in different binary solvents. All gels can be regarded as near critical gels with characteristic rheological parameters, elastic plateau and characteristic relaxation time, which are related one to another, as a consequence of self-similarity, and span several orders of magnitude when changing the solvent composition. Thanks to calorimetry and neutron scattering experiments, we evidencea co-solvency effect with a better solvation of the complex polymer-like chains of the gluten proteins as the amount of ethanol increases. Overall the gel viscoelasticity can be accounted for by a unique characteristic length characterizing the crosslink density of the supramolecular network, which is solvent composition-dependent. On a molecular level, these findings could be interpreted as a transition of the supramolecular interactions, mainly H-bonds, from intra- to interchains, which would be facilitated by the disruption of hydrophobic interactions by ethanol molecules. This work provides new insight for tailoring the gelation process of complex polymer gels.', 'corpus_id': 224803467, 'score': 0}, {'doc_id': '225045974', 'title': 'Reprocessability of self-healing polymer coatings based on Diels-Alder thermo-reversible chemistry', 'abstract': 'We investigated reprocessability of self-healing thermosetting polymer coatings based on furan-grafted polyketones crosslinked with bis-maleimide. Mixtures of furan-grafted polyketones and bis-maleimide were prepared and spin coated onto glass plates. The coated glass plates were submerged into anthracene-solvent mixtures and the solutions underwent different thermal treatments (25, 50, 75, 100 and 120 0C) for 32h. During thermal treatment, UV-vis spectroscopy was used to follow furan-grafted polymer solubilization at different time intervals. It followed that all of the investigated solvents were able to solubilize (part of) the self-healing polymeric coating at different temperatures. At lower temperatures (25, 50 and 75 0C), the solubility/miscibility between the furan-grafted polymer and the apparent solvent appeared to be decisive in determining the order of solubilization for the examined solvents. At higher temperatures (100 and 120 oC), the reverse Diels-Alder reaction occurred to a larger extent. This diminished the thermosetting polymer network, thus increasing solubilization of furangrafted polyketones for all solvents. It appeared that, at these higher temperatures, the relative solubility (estimated based on Hansen Solubility Parameters) of the solvents and furan-grafted polyketones was no longer decisive for the observed order of polymer solubilization. To our knowledge, the reprocessability of self-healing thermosetting polymer coatings has not been studied before. The observed reprocessability of such coatings might drive the replacement of conventional non-recyclable thermosets by self-healing polymeric systems.', 'corpus_id': 225045974, 'score': 0}, {'doc_id': '222451129', 'title': 'Production of Abrasive Sandpaper using Periwinkle Shells and Crab Shells', 'abstract': 'In this study, the abrasive properties of periwinkle shell and crab shell grains with the binding effect of polyester resin at high concentration were evaluated. The abrasive properties considered were the hardness, compressive strength and wear resistance. The shells were processed into grit standards by crushing, grinding and then sieving using ASTM E11 set of sieves into grain sizes of P40 and P60. Furthermore on, the grits were developed into polymer matrix composite with particles varying from 96 wt.% to 92 wt.% and resin 3 wt.% to 7 wt.% with 1 wt.% each of cobalt naphthalene and methyl ethyl ketone peroxide hardener respectively by mixing and mold compression using a hydraulic press. It was found that, with an increase in polyester resin content, the hardness and compressive strength increased, while the wear rate decreased. The composition with most improved abrasive properties was 92 wt.% periwinkle shell grains to 7 wt.% polyester resin.', 'corpus_id': 222451129, 'score': 0}, {'doc_id': '84883267', 'title': 'Permeability to Water of the Fibre Cell Wall Material of Two Hardwoods', 'abstract': 'The permeability to water of the fibre cell wall of birch (Betula pubescens Ehrh.) and lime (Tilia x vulgaris Hayne) wood was measured in wood slices in which most of the void space was filled with paraffin wax or a polymerizing silicone elastomer. An osmotic technique was used with solutes of molecular weight great enough to prevent their molecules from penetrating the pores in the water-swollen cell wall. The solutes used were dextran and polyethylene glycol with molecular weights of 40 000 and 6000 respectively. Values for permeability k x 1021, as defined by the Darcy equation, ranged between 3-6 m2 for the tangential direction in birch, to 27 m2 for the longitudinal direction in birch. These results are in good agreement with previously measured values but are at least ten times less than theoretical values. It is calculated that total emptying or filling of a wood cell through the wall might be possible in as little as 5 min under a pressure difference of 0-1 MPa if other flow pathways were blocked.', 'corpus_id': 84883267, 'score': 1}, {'doc_id': '45083750', 'title': 'Permeability to water of the cell wall material of spruce heartwood', 'abstract': 'SummaryThe permeability to water of spruce heartwood has been measured by applying a known osmotic pressure difference across specimens in which the cell walls were water swollen and most of the void space was filled with paraffin wax.To ensure that solute molecules should not penetrate the cell wall aqueous solutions of polyethylene glycol of molecular weight 6,000 and dextran of molecular weight 40,000 were used to generate the osmotic pressures. The mean values of the permeability k×1021 defined by the Darcy equation were 67.5 m2 for longitudinal flow, 7.12 m2 for radial flow and 4.03 m2 for tangential flow. Permeability was probably overestimated by about 10% owing to water entering some of the void space not filled by wax. The measured values are 10 to 100 times smaller than those calculated assuming all water in the cell walls is free to flow and this is probably caused by much of the water being bound to cell wall material.', 'corpus_id': 45083750, 'score': 1}, {'doc_id': '181541456', 'title': 'Variable temperature solid-state NMR spectral and relaxation analyses of the impregnation of polyethylene glycol (PEG) into coniferous wood', 'abstract': 'To investigate the behaviours of polyethylene glycol (PEG) and its interaction with biomass constituents in coniferous wood (Japanese cypress), variable temperature solid-state NMR spectra and relaxation times were measured from 20–80 °C.', 'corpus_id': 181541456, 'score': 1}, {'doc_id': '212652596', 'title': 'Effect of Heat Treatment on the Chemical Structure and Thermal Properties of Softwood-Derived Glycol Lignin', 'abstract': 'A large-scale glycol lignin (GL) production process (50 kg wood meal per batch) based on acid-catalyzed polyethylene glycol (PEG) solvolysis of Japanese cedar (JC) was developed at the Forestry and Forest Products Research Institute (FFPRI), Tsukuba, Japan. JC wood meal with various particle size distributions (JC-S < JC-M < JC-L) (average meal size, JC-S (0.4 mm) < JC-M (0.8 mm) < JC-L (1.6 mm)) and liquid PEG with various molecular masses are used as starting materials to produce PEG-modified lignin derivatives, namely, GLs, with various physicochemical and thermal properties. Because GLs are considered a potential feedstock for industrial applications, the effect of heat treatment on GL properties is an important issue for GL-based material production. In this study, GLs obtained from PEG400 solvolysis of JC-S, JC-M, and JC-L were subjected to heating in a constant-temperature drying oven at temperatures ranging from 100 to 220 °C for 1 h. All heat-treated GL series were thermally stable, as determined from the Klason lignin content, TMA, and TGA analyses. SEC analysis suggests the possibility of condensation among lignin fragments during heat treatment. ATR-FTIR spectroscopy, thioacidolysis, and 2D HSQC NMR demonstrated that a structural rearrangement occurs in the heat-treated GL400 samples, in which the content of α–PEG-β–O-4 linkages decreases along with the proportional enrichments of β–5 and β–β linkages, particularly at treatment temperatures above 160 °C.', 'corpus_id': 212652596, 'score': 1}]
96	{'doc_id': '150125350', 'title': 'The Professional Development of Teachers', 'abstract': 'This chapter provides a brief synopsis of past and present teacher preparation and credentialing practices along with common professional development (PD) programs emphasized in the United States and their influence in shaping instructional activities that have dominated classroom instruction. Research studies reveal that the PD of teachers has been costly yet ineffective. Thus, a consensus view of effective PD based on the learning sciences has been formed and is being promoted by the US Department of Education, among others. The latest view of effective PD may still fall short if we do not address the underlying constructs that determine a teacher’s PPATs such as their mental model of how students learn, their implicit theories of intelligence, and the forms of knowledge they employ. Addressing these constructs has the potential to create conceptual change in teachers, adjusting their PPATs to be more learner centered, and supporting the use of formative assessment practices. In addition to addressing these constructs, a new vision should be cast based on what students need to know and be able to do in a knowledge economy and teachers need to be trained and prepared to equip them accordingly.', 'corpus_id': 150125350}	6837	"[{'doc_id': '220632211', 'title': 'EFFECTS OF COGNITIVE COACHING ON TEACHER EFFICACY', 'abstract': 'Educators in the American school system have been under pressure to increase academic achievement among all students especially within the past decade. Government initiatives and high-stakes testing have ushered in a new era of accountability. Many attempts at school reform have been made with little success. Current research has found that the variable with the greatest impact on student achievement is the quality of the instructor. Attempts at improving teacher effectiveness have included merit pay, professional development, utilizing a scripted curriculum, and coaching. Studies have shown that professional development embedded within the classroom is most beneficial in terms of improving instruction and increasing student achievement. Continuing the professional development after the initial delivery can prove to be difficult. Both Professional Learning Communities and peer coaching are techniques used to support instructors in implementing new concepts. However, Cognitive Coachingsm on a one-on-one basis has greatly influenced change in teacher behaviors, increasing teacher efficacy, and elevating student achievement scores. COGNITIVE COACHING AND TEACHER EFFICACY Table of', 'corpus_id': 220632211, 'score': 1}, {'doc_id': '220249784', 'title': 'An Analysis of Academic Performance of University Students in Namibia', 'abstract': 'Based on observations it seems that a considerable proportion of university students in Namibia need to enhance their academic performance in regard to be fully competitive in a globalised working environment. This specifically includes thorough understanding of mathematics, as it forms an integral part of modern knowledge disciplines such as information technology and the management sciences. Yet students struggle or fail to engage in relevant coursework, either because of an absence of adequate material, capable educators, their own will power, or a combination thereof. This study aims to investigate the critical factors that are related to academic performance of university students in Namibia.', 'corpus_id': 220249784, 'score': 0}, {'doc_id': '220425555', 'title': 'GUIDE for a blended learning system', 'abstract': ""This guide is proposed as an operational instrument for CONFRASIE member universities (Regional Rectors' Conference of AUF member institutions in Pacific-Asia) in their projects to set up a blended learning system for bachelor's, Master's and Doctorate degrees. It is structured in sections corresponding to a complete process of operationalizing a blended learning system, from the definition of an implementation strategy to the assessment of results. This guide covers also conceptual and theoretical fundamentals of distance learning as well as methodological and procedural tips and recommendations on how to implement blended learning in an existing face-to-face curriculum. It can serve for leaders of educational ICT-based projects as a guidance document to take pedagogical, technological and methodological decisions for the development, monitoring and assessment of a blended learning curricula. This guide can be augmented by other standards, tool and software manuals offering further training materials and guidelines on educational skills ans=d services."", 'corpus_id': 220425555, 'score': 0}, {'doc_id': '220123138', 'title': 'Identity in lockdown: supporting primary care professional identity development in the COVID-19 generation', 'abstract': 'ABSTRACT The current COVID-19 pandemic has dramatically impacted undergraduate medical studies. Whilst challenges for knowledge and clinical skills are being actively addressed, wider considerations such as the impact on professional identity development have been mostly neglected thus far. A robust professional identity is linked to professional behaviour and has been shown to reduce burnout and be an important factor for general practice career choice amongst medical students. The Communities of Practice Model is a sociocultural approach that conceptualises the formation of professional identity through student engagement within a community. We argue the current suspension of clinical placements holds the potential to negatively influence such identity acquisition. In this commentary we explore how the Communities of Practice Model may inform professional identity development of medical students within the COVID-19 environment, considering digital communities and volunteering roles within primary care. We further encourage educators and institutions to consider professional identity in future planning to address the challenges posed by the current situation, both in terms of placement loss but also changes in the way primary care is delivered. Such considerations will be essential if we are to avoid problems relating to poor medical student professional identity development in future.', 'corpus_id': 220123138, 'score': 1}, {'doc_id': '219523312', 'title': 'Whether the School Self-Developed e-Learning Platform is More Conducive to Learning during the COVID-19 Pandemic?', 'abstract': 'The school self-developed e-learning platform can provide students with real and non-real-time learning resources based on its personalized learning properties like self-owned teaching conditions and levels. Meanwhile, it is beneficial to students’ independent study and academic performance. During the COVID-19 pandemic, schools in China adopted e-learning platforms to conduct online teaching, whereas the efficacy of these online teaching platforms was not definitely known. It is necessary to review their effectiveness, especially for those developed by schools themselves. A total of 417 of 7th and 8th graders from two middle schools in Nanjing, Jiangsu Province were enrolled with 208 in the experimental group and 209 as the comparisons. The primary outcome of the study was students’ academic performance. The results demonstrated that: (i) the online platform-based self-learning was conducive to students’ grades; (ii) school self-developed e-learning platform was more effective in improving student achievement than other non-school self-developed ones.', 'corpus_id': 219523312, 'score': 1}, {'doc_id': '219443516', 'title': 'TRAINING FUNCTIONAL COMMUNICATION RESPONSES TO DECREASE CHALLENGING BEHAVIORS AMONG STUDENTS IN AN ALTERNATIVE EDUCATION SETTING', 'abstract': 'OF THESIS TRAINING FUNCTIONAL COMMUNICATION RESPONSES TO DECREASE CHALLENGING BEHAVIORS AMONG STUDENTS IN AN ALTERNATIVE EDUCATION SETTING This study answered a series of questions related to single case research design and potential use of functional communication-based intervention. It compared and contrasted multiple probe versus multiple baseline single case research designs, finding that multiple probe would be more appropriate to answer the intended research question. This study further analyzed the rigor, quality, and bias of Turton, Umbreit, & Mathur’s (2011) article in which functional communication-based intervention was used to decrease challenging behaviors among adolescents in an alternative education setting. Finally, this study described for practitioners the importance of studying the effectiveness of functional communication-based intervention and how such intervention might be implemented in classroom settings.', 'corpus_id': 219443516, 'score': 0}, {'doc_id': '220728184', 'title': 'Nature of “STEM”?', 'abstract': 'In recent years, there has been an increasing emphasis on STEM (science, technology, engineering, and mathematics) education in international curriculum and policy documents (e.g., NSTA, 2020; Office of the Chief Scientist, 2014). A key argument in the proposals for STEM education is that science, technology, engineering, and mathematics workers play a pivotal role in economic growth and STEM education produces critical thinkers, scientifically literate professionals and citizens, and enables the next generation of innovators. The infusion of “engineering practices” in the Next Generation Science Standards in the USA signals a major shift in curriculum policy for integrating related domains to science teaching and learning. Furthermore, there has been plethora of journals, research centers, and community organizations that have made STEM a central educational goal, and many funding agencies are supporting research and development efforts to advance STEM education. But what exactly does “STEM” mean? Is there a particular “nature” to STEM or are there disciplinary variations across the “natures” of science, technology, engineering, and mathematics? What are the epistemic underpinnings of STEM and what do they imply for STEM education? A question in a similar vein had been raised by Erin Peters-Burton in an editorial of School Science and Mathematics a few years ago (Peters-Burton, 2014) but has since received little attention despite the wealth of interest in research on STEM education. The primary purpose of this special issue is, then, to address some questions about the nature of STEM and STEM education. The questions raised by the papers in the special issue relate to theoretical characterization of STEM as well as a range of educational considerations including the implications for curriculum reform as well as for students’ and teachers’ learning. A fundamental issue is whether or not “STEM” is a warranted notion in the first place. Despite the plethora of work on STEMeducation, what STEMpromises to be and how it manifests itself in education can be questioned. Hence, the special issue is set against a backdrop of some critiques of STEM education, followed by a set of studies that illustrate its merits. Reynante, Selback-Allen, and Pimentel question how many STEM education efforts have not explicitly accounted for the distinct epistemologies of the disciplines. The authors critically examine the concept of integrated Science & Education https://doi.org/10.1007/s11191-020-00150-6', 'corpus_id': 220728184, 'score': 0}, {'doc_id': '220118107', 'title': 'The Potential of Research for Professional Development in Isolated Settings During the Covid-19 Crisis and Beyond', 'abstract': 'The global COVID-19 pandemic has created the urgent need for quality online instruction throughout all levels of education. However, this pandemic has found teachers physically isolated within their homes, and unprepared for the challenging tasks of teaching online. Many of the challenges faced by teachers due to this isolation, are similar to those faced by teachers in remote areas around the world. One such issue, is the lack of access to traditional professional development opportunities, which is frequently the only type of professional development offered to teachers in many countries. Thus, many teachers lack guidance on how to resolve many online teaching challenges during this period. Therefore, this study examines the potential of utilizing educational research for assisting teachers through this trying period of COVID-19.', 'corpus_id': 220118107, 'score': 1}, {'doc_id': '219310491', 'title': 'Innovations in Teaching and Learning during a Time of Crisis', 'abstract': 'During the COVID-19 pandemic higher education institutions have faced many challenges, and among these are questions about how best to offer instruction in the face of sudden and mandatory college closures as well as in light of uncertainties about when campuses might open again. Coupled with these challenges have been signs that higher education is more innovative and flexible than we might have imagined, particularly when it comes to teaching and learning. As college and university educators have navigated the shifting landscape, they have found new and innovative ways to use technology to accomplish instructional goals; and they will most likely have reason to continue in the path of innovation going forward. With the advent of mass remote instruction, college and university faculty members have had to determine how to teach online quickly, with the knowledge that many students did not sign up for online learning and may only have mobile devices coupled with limited internet access. Educators have been remarkable in putting workable short-term technological solutions for remote teaching and learning in place. As a result, the adoption of technology for teaching and learning in recent months has been unprecedented, with synchronous learning taking the fore initially. In particular, video-conferencing apps like Zoom and Skype have provided faculty and students a lifeline during this challenging time. With the close of the spring academic term, however, colleges and university teachers around the world have recognized that there has been a shift in the way that we offer instruction, at least for the foreseeable future, as students may or may not return to campus in the fall. While some institutions have announced that they will be virtual in the fall, others have announced that students will return to campus; but the question of whether that will happen or not looms large. Some institutions have not yet made decisions. At the very least, many faculty members now find themselves planning for fall in a way that allows them to be flexible about whether the teaching they do will occur onsite or online. What this means in practice is that we need to be prepared to teach fully onsite and to shift to fully online teaching at a moment’s notice if an unanticipated campus closure should require it. As a result, many faculty members are now realizing that remote learning is just a first step in a longer journey to offering high quality online education. They are taking up new tools, which allow for Innovative Higher Education https://doi.org/10.1007/s10755-020-09514-w', 'corpus_id': 219310491, 'score': 1}, {'doc_id': '219177027', 'title': 'Critical pedagogy in the implementation of educational technologies', 'abstract': 'This paper presents a critical review of the challenges to the implementation of learning technologies with particular focus on developing countries. A comprehensive literature review on learning technologies was undertaken for the purpose of understanding the challenges in developing countries. The research question is: what extent does education empower learners to be full participants in a socially democratic society? The literature review identified 25 papers relevant to this topic. Challenges are interrelated and to bring about changes in developing countries, this paper proposes two educational technology frameworks based on: 1. cultural conceptual framework, and 2. problem-based constructivist psychology simulation model. The framework and simulation model are both useful to guide practice and research.', 'corpus_id': 219177027, 'score': 0}]"
97	{'doc_id': '212740602', 'title': 'Computer simulations explain mutation-induced effects on the DNA editing by adenine base editors', 'abstract': 'We uncovered structural and functional changes induced by mutations that dictate the DNA editing activity of ABE enzymes. Adenine base editors, which were developed by engineering a transfer RNA adenosine deaminase enzyme (TadA) into a DNA editing enzyme (TadA*), enable precise modification of A:T to G⋮C base pairs. Here, we use molecular dynamics simulations to uncover the structural and functional roles played by the initial mutations in the onset of the DNA editing activity by TadA*. Atomistic insights reveal that early mutations lead to intricate conformational changes in the structure of TadA*. In particular, the first mutation, Asp108Asn, induces an enhancement in the binding affinity of TadA to DNA. In silico and in vivo reversion analyses verify the importance of this single mutation in imparting functional promiscuity to TadA* and demonstrate that TadA* performs DNA base editing as a monomer rather than a dimer.', 'corpus_id': 212740602}	5231	"[{'doc_id': '183589618', 'title': 'Informationsverarbeitung im Entscheidungsprozeß', 'abstract': 'In Abschnitt 2.1 wird zunachst der Begriff der Information fur die weitere Arbeit definiert. Ausgehend von der Bedeutung, die dem Produktionsfaktor Information zukommt, wird eine operationale Definition des Informationsbegriffs in Entscheidungssituationen vorgenommen. Die Grundlage bildet das Verstandnis der Information als handlungsrelevantem Wissen. Bei den hier interessierenden Entscheidungsprozessen liegt dieses Wissen in unterschiedlichen Auspragungen vor, die teils in einem komplementaren, teils in einem substitutiven Verhaltnis stehen. Fur die Entwicklung des objektorientierten MIS ist an dieser Stelle Faktenwissen sowie die Kenntnis von Kausalzusammenhangen und die Klassifikationsfahigkeit von besonderer Bedeutung.', 'corpus_id': 183589618, 'score': 0}, {'doc_id': '218764757', 'title': 'Two mutations P/L and Y/C in SARS-CoV-2 helicase domain exist together and influence helicase RNA binding', 'abstract': 'RNA helicases play pivotal role in RNA replication by catalysing the unwinding of complex RNA duplex structures into single strands in ATP/NTP dependent manner. SARS coronavirus 2 (SARS-CoV-2) is a single stranded positive sense RNA virus belonging to the family Coronaviridae. The viral RNA encodes non structural protein Nsp13 or the viral helicase protein that helps the viral RNA dependent RNA polymerase (RdRp) to execute RNA replication by unwinding the RNA duplexes. In this study we identified a novel mutation at position 541of the helicase where the tyrosine (Y) got substituted with cytosine (C). We found that Y541C is a destabilizing mutation increasing the molecular flexibility and leading to decreased affinity of helicase binding with RNA. Earlier we had reported a mutation P504L in the helicase protein for which had not performed RNA binding study. Here we report that P504L mutation leads to increased affinity of helicase RNA interaction. So, both these mutations have opposite effects on RNA binding. Moreover, we found a significant fraction of isolate population where both P504L and Y541C mutations were co-existing.', 'corpus_id': 218764757, 'score': 0}, {'doc_id': '15397223', 'title': 'Investigating dynamic and energetic determinants of protein nucleic acid recognition: analysis of the zinc finger zif268-DNA complexes', 'abstract': ""BackgroundProtein-DNA recognition underlies fundamental biological processes ranging from transcription to replication and modification. Herein, we present a computational study of the sequence modulation of internal dynamic properties and of intraprotein networks of aminoacid interactions that determine the stability and specificity of protein-DNA complexes.ResultsTo this aim, we apply novel theoretical approaches to analyze the dynamics and energetics of biological systems starting from MD trajectories. As model system, we chose different sequences of Zinc Fingers (ZF) of the Zif268 family bound with different sequences of DNA. The complexes differ for their experimental stability properties, but share the same overall 3 D structure and do not undergo structural modifications during the simulations. The results of our analysis suggest that the energy landscape for DNA binding may be populated by dynamically different states, even in the absence of major conformational changes. Energetic couplings between residues change in response to protein and/or DNA sequence variations thus modulating the selectivity of recognition and the relative importance of different regions for binding.ConclusionsThe results show differences in the organization of the intra-protein energy-networks responsible for the stabilization of the protein conformations recognizing and binding DNA. These, in turn, are reflected into different modulation of the ZF's internal dynamics. The results also show a correlation between energetic and dynamic properties of the different proteins and their specificity/selectivity for DNA sequences. Finally, a dynamic and energetic model for the recognition of DNA by Zinc Fingers is proposed."", 'corpus_id': 15397223, 'score': 1}, {'doc_id': '4581176', 'title': 'Molecular dynamic simulations of protein/RNA complexes: CRISPR/Csy4 endoribonuclease.', 'abstract': ""BACKGROUND\nMany prokaryotic genomes comprise Clustered Regularly Interspaced Short Palindromic Repeats (CRISPRs) offering defense against foreign nucleic acids. These immune systems are conditioned by the production of small CRISPR-derived RNAs matured from long RNA precursors. This often requires a Csy4 endoribonuclease cleaving the RNA 3'-end.\n\n\nMETHODS\nWe report extended explicit solvent molecular dynamic (MD) simulations of Csy4/RNA complex in precursor and product states, based on X-ray structures of product and inactivated precursor (55 simulations; ~3.7μs in total).\n\n\nRESULTS\nThe simulations identify double-protonated His29 and deprotonated terminal phosphate as the likely dominant protonation states consistent with the product structure. We revealed potential substates consistent with Ser148 and His29 acting as the general base and acid, respectively. The Ser148 could be straightforwardly deprotonated through solvent and could without further structural rearrangements deprotonate the nucleophile, contrasting similar studies investigating the general base role of nucleobases in ribozymes. We could not locate geometries consistent with His29 acting as general base. However, we caution that the X-ray structures do not always capture the catalytically active geometries and then the reactive structures may be unreachable by the simulation technique.\n\n\nCONCLUSIONS\nWe identified potential catalytic arrangement of the Csy4/RNA complex but we also report limitations of the simulation technique. Even for the dominant protonation state we could not achieve full agreement between the simulations and the structural data.\n\n\nGENERAL SIGNIFICANCE\nPotential catalytic arrangement of the Csy4/RNA complex is found. Further, we provide unique insights into limitations of simulations of protein/RNA complexes, namely, the influence of the starting experimental structures and force field limitations. This article is part of a Special Issue entitled Recent developments of molecular dynamics."", 'corpus_id': 4581176, 'score': 1}, {'doc_id': '216072055', 'title': 'Cryo-EM structures reveal transcription initiation steps by yeast mitochondrial RNA polymerase', 'abstract': 'Cryo-EM structures of transcription pre-initiation complex (PIC) and initiation complex (IC) of yeast mitochondrial RNA polymerase show fully resolved transcription bubbles and explain promoter melting, template alignment, DNA scrunching, transition into elongation, and abortive synthesis. Promoter melting initiates in PIC with MTF1 trapping the −4 to −2 non-template (NT) bases in its NT-groove. Transition to IC is marked by a large-scale movement that aligns the template with RNA at the active site. RNA synthesis scrunches the NT strand into an NT-loop, which interacts with centrally positioned MTF1 C-tail. Steric clashes of the C-tail with RNA:DNA and NT-loop, and dynamic scrunching-unscrunching of DNA explain abortive synthesis and transition into elongation. Capturing the catalytically active IC-state with UTPαS poised for incorporation enables modeling toxicity of antiviral nucleosides/nucleotides.', 'corpus_id': 216072055, 'score': 0}, {'doc_id': '25938690', 'title': 'Localized frustration and binding-induced conformational change in recognition of 5S RNA by TFIIIA zinc finger.', 'abstract': 'Protein TFIIIA is composed of nine tandemly arranged Cys2His2 zinc fingers. It can bind either to the 5S RNA gene as a transcription factor or to the 5S RNA transcript as a chaperone. Although structural and biochemical data provided valuable information on the recognition between the TFIIIIA and the 5S DNA/RNA, the involved conformational motions and energetic factors contributing to the binding affinity and specificity remain unclear. In this work, we conducted MD simulations and MM/GBSA calculations to investigate the binding-induced conformational changes in the recognition of the 5S RNA by the central three zinc fingers of TFIIIA and the energetic factors that influence the binding affinity and specificity at an atomistic level. Our results revealed drastic interdomain conformational changes between these three zinc fingers, involving the exposure/burial of several crucial DNA/RNA binding residues, which can be related to the competition between DNA and RNA for the binding of TFIIIA. We also showed that the specific recognition between finger 4/finger 6 and the 5S RNA introduces frustrations to the nonspecific interactions between finger 5 and the 5S RNA, which may be important to achieve optimal binding affinity and specificity.', 'corpus_id': 25938690, 'score': 1}, {'doc_id': '52096989', 'title': 'Molecular Modeling Applied to Nucleic Acid-Based Molecule Development', 'abstract': 'Molecular modeling by means of docking and molecular dynamics (MD) has become an integral part of early drug discovery projects, enabling the screening and enrichment of large libraries of small molecules. In the past decades, special emphasis was drawn to nucleic acid (NA)-based molecules in the fields of therapy, diagnosis, and drug delivery. Research has increased dramatically with the advent of the SELEX (systematic evolution of ligands by exponential enrichment) technique, which results in single-stranded DNA or RNA sequences that bind with high affinity and specificity to their targets. Herein, we discuss the role and contribution of docking and MD to the development and optimization of new nucleic acid-based molecules. This review focuses on the different approaches currently available for molecular modeling applied to NA interaction with proteins. We discuss topics ranging from structure prediction to docking and MD, highlighting their main advantages and limitations and the influence of flexibility on their calculations.', 'corpus_id': 52096989, 'score': 1}, {'doc_id': '139532360', 'title': 'Effects of sorption and desorption of CO2 on the thermomechanical experimental behavior of HNBR and FKM O-rings - Influence of nanofiller-reinforced rubber', 'abstract': 'Abstract Despite its importance in sealing, a fundamental understanding of the behavior of elastomeric O-rings in a gas environment is still incomplete. Further experimental research is needed to obtain a precise predictive model which can describe the combined effects of gases and mechanical loading. The effect of CO2 pressure (2, 4 and 6\u202fMPa) on the mechanical compression behavior of HNBR and FKM seals at two temperatures (60 and 130\u202f°C) is described in the paper. To evaluate the contribution of nanofiller reinforcement, experimental tests were carried out on two kinds of rubber which are reinforced or not with nanofiller (10 phr of expanded graphite). A ranking of materials was performed thanks to experimental database. HNBR seems to be the better candidate in the service conditions applied. Furthermore, the nanofillers which are introduced in the rubber can lead some damages in Rapid Gas Decompression conditions, notably when the zero pressure was applied (desorption).', 'corpus_id': 139532360, 'score': 0}, {'doc_id': '39425134', 'title': 'Molecular dynamics simulations of nucleic acid-protein complexes.', 'abstract': 'Molecular dynamics simulation studies of protein-nucleic acid complexes are more complicated than studies of either component alone-the force field has to be properly balanced, the systems tend to become very large, and a careful treatment of solvent and of electrostatic interactions is necessary. Recent investigations into several protein-DNA and protein-RNA systems have shown the feasibility of the simulation approach, yielding results of biological interest not readily accessible to experimental methods.', 'corpus_id': 39425134, 'score': 1}, {'doc_id': '215800977', 'title': 'Identification of potential binders of the main protease 3CLpro of the COVID-19 via structure-based ligand design and molecular modeling', 'abstract': '\n Abstract\n \n We have applied a computational strategy, using a combination of virtual screening, docking and molecular dynamics techniques, aimed at identifying possible lead compounds for the non-covalent inhibition of the main protease 3CLpro of the SARS-CoV2 Coronavirus. Based on the X-ray structure (PDB code: 6LU7), ligands were generated using a multimodal structure-based design and then docked to the monomer in the active state. Docking calculations show that ligand-binding is strikingly similar in SARS-CoV and SARS-CoV2 main proteases. The most potent docked ligands are found to share a common binding pattern with aromatic moieties connected by rotatable bonds in a pseudo-linear arrangement.\n \n', 'corpus_id': 215800977, 'score': 0}]"
98	"{'doc_id': '211012522', 'title': 'Integrating dynamic environmental predictors and species occurrences: Toward true dynamic species distribution models', 'abstract': ""Abstract While biological distributions are not static and change/evolve through space and time, nonstationarity of climatic and land‐use conditions is frequently neglected in species distribution models. Even recent techniques accounting for spatiotemporal variation of species occurrence basically consider the environmental predictors as static; specifically, in most studies using species distribution models, predictor values are averaged over a 50‐ or 30‐year time period. This could lead to a strong bias due to monthly/annual variation between the climatic conditions in which species' locations were recorded and those used to develop species distribution models or even a complete mismatch if locations have been recorded more recently. Moreover, the impact of land‐use change has only recently begun to be fully explored in species distribution models, but again without considering year‐specific values. Excluding dynamic climate and land‐use predictors could provide misleading estimation of species distribution. In recent years, however, open‐access spatially explicit databases that provide high‐resolution monthly and annual variation in climate (for the period 1901–2016) and land‐use (for the period 1992–2015) conditions at a global scale have become available. Combining species locations collected in a given month of a given year with the relative climatic and land‐use predictors derived from these datasets would thus lead to the development of true dynamic species distribution models (D‐SDMs), improving predictive accuracy and avoiding mismatch between species locations and predictor variables. Thus, we strongly encourage modelers to develop D‐SDMs using month‐ and year‐specific climatic data as well as year‐specific land‐use data that match the period in which species data were collected."", 'corpus_id': 211012522}"	19867	"[{'doc_id': '199098266', 'title': 'Tropical bird species richness is strongly associated with patterns of primary productivity captured by the Dynamic Habitat Indices', 'abstract': 'Abstract Biodiversity science and conservation alike require environmental indicators to understand species richness and predict species distribution patterns. The Dynamic Habitat Indices (DHIs) are a set of three indices that summarize annual productivity measures from satellite data for biodiversity applications, and include: a) cumulative annual productivity; b) minimum annual productivity; and c) variation in annual productivity. At global scales and in temperate regions the DHIs predict species diversity patterns well, but the DHIs have not been tested in the tropics, where higher levels of productivity lead to the saturation of many remotely sensed vegetation indices. Our goal was to explain bird species richness patterns based on the DHIs in tropical areas. We related the DHIs to species richness of resident landbirds for five guilds (forest, scrub, grassland, generalist, and all resident birds) based on a) species distribution model (SDM) maps for 217 species, and b) range map for 564 species across Thailand. We also quantified the relative importance of the DHIs in multiple regression models that included two measures of topography, and two climate metrics using multiple regression, best-subsets, and hierarchical partitioning analyses. We found that the three DHIs alone explained forest bird richness best (R2adj 0.61 for both SDM- and rangemap based richness; 0.15–0.54 for the other guilds). When combining the DHIs with topography and climate, the richness of both forest birds and all resident bird species was equally well explained (R2adj 0.85 and 0.67 versus 0.81 and 0.68). Among the three DHIs, cumulative annual productivity had the greatest explanatory power for all guilds based on SDM richness maps (R2adj 0.54–0.61). The strong relationship between the DHIs and bird species richness in Thailand suggests that the DHIs capture energy availability well and are useful in biodiversity assessments and potentially bird conservation in tropical areas.', 'corpus_id': 199098266, 'score': 1}, {'doc_id': '236239607', 'title': 'Patterns of bird species richness explained by annual variation in remotely sensed Dynamic Habitat Indices', 'abstract': 'Abstract Bird species richness is highly dependent on the amount of energy available in an ecosystem, with more available energy supporting higher species richness. A good indicator for available energy is Gross Primary Productivity (GPP), which can be estimated from satellite data. Our question was how temporal dynamics in GPP affect bird species richness. Specifically, we evaluated the potential of the Dynamic Habitat Indices (DHIs) derived from MODIS GPP data together with environmental and climatic variables to explain annual patterns in bird richness across the conterminous United States. By focusing on annual DHIs, we expand on previous applications of multi-year composite DHIs, and could evaluate lag-effects between changes in GPP and species richness. We used 8-day GPP data from 2003 to 2013 to calculate annual DHIs, which capture three aspects of vegetation productivity: (1) annual cumulative productivity, (2) annual minimum productivity, and (3) annual seasonality expressed as the coefficient of variation in productivity. For each year from 2003 to 2013, we calculated total bird species richness and richness within six functional guilds, based on North American Breeding Bird Survey data. The DHIs alone explained up to 53% of the variation in annual bird richness within the different guilds (adjusted deviance-squared D2adj\xa0=\xa00.20–0.52), and up to 75% of the variation (D2adj\xa0=\xa00.28–0.75) when combined with other environmental and climatic variables. Annual DHIs had the highest explanatory power for habitat-based guilds, such as grassland (D2adj\xa0=\xa00.67) and woodland breeding species (D2adj\xa0=\xa00.75). We found some inter-annual variability in the explanatory power of annual DHIs, with a difference of 5–7 percentage points in explained variation among years in DHI-only models, and 3–7 points for models combining DHI, environmental and climatic variables. Our results using lagged year models did not deviate substantially from same-year annual models. We demonstrate the relevance of annual DHIs for biodiversity science, as effective predictors of temporal variation in species richness patterns. We suggest that the use of annual DHIs can improve conservation planning, by conveying the range of patterns of biodiversity response to global changes, over time.', 'corpus_id': 236239607, 'score': 1}, {'doc_id': '116238824', 'title': 'Sensitivity analysis of a Pelton hydropower station based on a novel approach of turbine torque', 'abstract': 'Abstract Hydraulic turbine generator units with long-running operation may cause the values of hydraulic, mechanic or electric parameters changing gradually, which brings a new challenge, namely that whether the operating stability of these units will be changed in the next thirty or forty years. This paper is an attempt to seek a relatively unified model for sensitivity analysis from three aspects: hydraulic parameters (turbine flow and turbine head), mechanic parameters (axis coordinates and axial misalignment) and electric parameters (generator speed and excitation current). First, a novel approach of the Pelton turbine torque is proposed, which can make connections between the hydraulic turbine governing system and the shafting system of the hydro-turbine generator unit. Moreover, the correctness of this approach is verified by comparing with other three models of hydropower stations. Second, this latter is analyzed to obtain the sensitivity of electric parameter (excitation current), the mechanic parameters (axial misalignment, upper guide bearing rigidity, lower guide bearing rigidity, and turbine guide bearing rigidity) on hydraulic parameters on the operating stability of the unit. In addition to this, some critical values and ranges are proposed. Finally, these results can provide some bases for the design and stable operation of Peltonhydropower stations.', 'corpus_id': 116238824, 'score': 0}, {'doc_id': '237261004', 'title': 'Not all species will migrate poleward as the climate warms: the case of the seven baobab species in Madagascar.', 'abstract': ""It is commonly accepted that species should move toward higher elevations and latitudes to track shifting isotherms as climate warms. However, temperature might not be the only limiting factor determining species distribution. Species might move to opposite directions to track changes in other climatic variables. Here, we used an extensive occurrence dataset and an ensemble modelling approach to model the climatic niche and to predict the distribution of the seven baobab species (genus Adansonia) present in Madagascar. Using climatic projections from three global circulation models, we predicted species' future distribution and extinction risk for 2055 and 2085 under two representative concentration pathways (RCPs) and two dispersal scenarios. We disentangled the role of each climatic variable in explaining species range shift looking at relative variable importance and future climatic anomalies. Four baobab species (A. rubrostipa, A. madagascariensis, A. perrieri¸ and A. suarezensis) could experience a severe range contraction in the future (> 70% for year 2085 under RCP 8.5, assuming a zero-dispersal hypothesis). For three out of the four threatened species, range contraction was mainly explained by an increase in temperature seasonality, especially in the North of Madagascar, where they are currently distributed. In tropical regions, where species are commonly adapted to low seasonality, we found that temperature seasonality will generally increase. It is thus very likely that many species in the tropics will be forced to move equatorward to avoid an increase in temperature seasonality. Yet, several ecological (e.g. equatorial limit, or unsuitable deforested habitat) or geographical barriers (absence of lands) could prevent species to move equatorward, thus increasing the extinction risk of many tropical species, like endemic baobab species in Madagascar."", 'corpus_id': 237261004, 'score': 0}, {'doc_id': '236968585', 'title': ""The evolutionary genomics of species' responses to climate change."", 'abstract': ""Climate change is a threat to biodiversity. One way that this threat manifests is through pronounced shifts in the geographical range of species over time. To predict these shifts, researchers have primarily used species distribution models. However, these models are based on assumptions of niche conservatism and do not consider evolutionary processes, potentially limiting their accuracy and value. To incorporate evolution into the prediction of species' responses to climate change, researchers have turned to landscape genomic data and examined information about local genetic adaptation using climate models. Although this is an important advancement, this approach currently does not include other evolutionary processes-such as gene flow, population dispersal and genomic load-that are critical for predicting the fate of species across the landscape. Here, we briefly review the current practices for the use of species distribution models and for incorporating local adaptation. We next discuss the rationale and theory for considering additional processes, reviewing how they can be incorporated into studies of species' responses to climate change. We summarize with a conceptual framework of how manifold layers of information can be combined to predict the potential response of specific populations to climate change. We illustrate all of the topics using an exemplar dataset and provide the source code as potential tutorials. This Perspective is intended to be a step towards a more comprehensive integration of population genomics with climate change science."", 'corpus_id': 236968585, 'score': 0}, {'doc_id': '61134020', 'title': 'Combining static and dynamic variables in species distribution models under climate change', 'abstract': ""1.Methods used to predict shifts in species' ranges because of climate change commonly involve species distribution (niche) modelling using climatic variables, future values of which are predicted for the next several decades by general circulation models. However, species' distributions also depend on factors other than climate, such as land cover, land use and soil type. Changes in some of these factors, such as soil type, occur over geologic time and are thus imperceptible over the timescale of these types of projections. Other factors, such as land use and land cover, are expected to change over shorter timescales, but reliable projections are not available. Some important predictor variables, therefore, must be treated as unchanging, or static, whether because of the properties of the variable or out of necessity. The question of how best to combine dynamic variables predicted by climate models with static variables is not trivial and has been dealt with differently in studies to date. Alternative methods include using the static variables as masks, including them as independent explanatory variables in the model, or excluding them altogether. 2.Using a set of simulated species, we tested various methods for combining static variables with future climate scenarios. Our results showed that including static variables in the model with the dynamic variables performed better or no worse than either masking or excluding the static variables. 3.The difference in predictive ability was most pronounced when there is an interaction between the static and dynamic variables. 4.For variables such as land use, our results indicate that if such variables affect species distributions, including them in the model is better than excluding them, even though this may mean making the unrealistic assumption that the variable will not change in the future. 5.These results demonstrate the importance of including static and dynamic non-climate variables in addition to climate variables in species distribution models designed to predict future change in a species' habitat or distribution as a result of climate change. © 2011 The Authors. Methods in Ecology and Evolution © 2011 British Ecological Society."", 'corpus_id': 61134020, 'score': 1}, {'doc_id': '91389846', 'title': 'Understanding species distribution in dynamic populations: a new approach using spatio‐temporal point process models', 'abstract': 'Understanding and predicting a species’ distribution across a landscape is of central importance in ecology, biogeography and conservation biology. However, it presents daunting challenges when populations are highly dynamic (i.e. increasing or decreasing their ranges), particularly for small populations where information about ecology and life history traits is lacking. Currently, many modelling approaches fail to distinguish whether a site is unoccupied because the available habitat is unsuitable or because a species expanding its range has not arrived at the site yet. As a result, habitat that is indeed suitable may appear unsuitable. To overcome some of these limitations, we use a statistical modelling approach based on spatio‐temporal log‐Gaussian Cox processes. These model the spatial distribution of the species across available habitat and how this distribution changes over time, relative to covariates. In addition, the model explicitly accounts for spatio‐temporal dynamics that are unaccounted for by covariates through a spatio‐temporal stochastic process. We illustrate the approach by predicting the distribution of a recently established population of Eurasian cranes Grus grus in England, UK, and estimate the effect of a reintroduction in the range expansion of the population. Our models show that wetland extent and perimeter‐to‐area ratio have a positive and negative effect, respectively, in crane colonisation probability. Moreover, we find that cranes are more likely to colonise areas near already occupied wetlands and that the colonisation process is progressing at a low rate. Finally, the reintroduction of cranes in SW England can be considered a human‐assisted long‐distance dispersal event that has increased the dispersal potential of the species along a longitudinal axis in S England. Spatio‐temporal log‐Gaussian Cox process models offer an excellent opportunity for the study of species where information on life history traits is lacking, since these are represented through the spatio‐temporal dynamics reflected in the model.', 'corpus_id': 91389846, 'score': 1}, {'doc_id': '82120544', 'title': 'Multi‐temporal distribution modelling with satellite tracking data: predicting responses of a long‐distance migrant to changing environmental conditions', 'abstract': 'Summary\r\n\r\n\r\n1.\u2002Despite the wealth of data available from satellite tracking (ST) studies, such data have rarely been used to model species distributions. Using a novel method, we show how to exploit satellite data to analyse whether and how a migratory species responds to fluctuating environmental conditions in its wintering area. This is particularly crucial for establishing comprehensive conservation measures for rare species in areas that are threatened by increasing land use and climate change.\r\n\r\n\r\n\r\n2.\u2002We use ST data of Eleonora’s falcon Falco eleonorae, a long-distance migratory raptor that winters in Madagascar, and assess the performance of static species distribution models (SDM) as well as multi-temporal models. ST data were derived from seven falcons tracked during three consecutive wintering periods and for a total of 2410 bearings, of which 512 locations were used in SDMs. We employed environmental predictors (climate, topography and land cover) with a spatial resolution of 30\xa0arc seconds (c. 1\xa0km2) to match rigorously filtered ST data with an accuracy of ≤1\xa0km.\r\n\r\n\r\n\r\n3.\u2002We first created a model with low temporal but high spatial resolution (half-year). To predict suitable habitat for each month of the wintering season, we took advantage of the high temporal resolution inherent in ST data and employed temporally corresponding remote sensing data [Normalized Difference Vegetation Index (NDVI) 10-day composites] together with other variables to create monthly models.\r\n\r\n\r\n\r\n4.\u2002We show that ST data are suited to build robust and transferable SDMs despite a low number of tracked individuals. Multi-temporal SMDs further revealed seasonal responses of the study species to changing environmental conditions in its wintering area.\r\n\r\n\r\n\r\n5.\u2002Synthesis and applications. We present a transferable approach to predict the potential distribution of organisms as well as their dynamic response to changing environmental conditions. Future conservation management plans could include the prediction of a species’ reaction to changing land-use practices or climate change based on the methodology proposed here. This would provide an early warning system for the decline of populations wintering in remote areas that underlie strong climatic fluctuations.', 'corpus_id': 82120544, 'score': 1}, {'doc_id': '23435195', 'title': 'Heterologous expression and characterization of a sigma glutathione S-transferase involved in carbaryl detoxification from oriental migratory locust, Locusta migratoria manilensis (Meyen).', 'abstract': 'Glutathione S-transferases (GSTs) play a major role in detoxification of xenobiotics and resistance to insecticides in insects. In the present study, a sigma-class GST gene (LmGSTs3) was identified from the locust, Locusta migratoria manilensis. Its full-length cDNA sequence is 828 bp containing an open reading frame (ORF) of 612 bp that encodes 204 amino acid residues. The predicted protein molecular mass and pI are 23.4 kDa and 7.62, respectively. Recombinant LmGSTs3 was heterologously expressed in Escherichia coli as a soluble fusion protein. Its optimal activity was observed at pH 8.0. Incubation for 30 min at temperatures below 40 °C scarcely affected activity. The LmGSTs3 at pH values between 4.0 and 11.0 retained more than 80% of its original activity. Ethacrynic acid and cibacron blue were very effective inhibitors of LmGSTs3 with I50-values 1.7 and 3.7 μM, respectively. In response to heavy metal (CuSO4, CdCl2) exposure there was a concentration-dependent and time-dependent decrease in activity. The nymph mortalities after carbaryl treatment increased 38.7% after LmGSTs3 were silenced. These results suggest that LmGSTs3 may be involved in carbaryl detoxification in L. migratoria manilensis.', 'corpus_id': 23435195, 'score': 0}, {'doc_id': '235814885', 'title': 'Rapid Ecosystem Change at the Southern Limit of the Canadian Arctic, Torngat Mountains National Park', 'abstract': 'Northern protected areas guard against habitat and species loss but are themselves highly vulnerable to environmental change due to their fixed spatial boundaries. In the low Arctic, Torngat Mountains National Park (TMNP) of Canada, widespread greening has recently occurred alongside warming temperatures and regional declines in caribou. Little is known, however, about how biophysical controls mediate plant responses to climate warming, and available observational data are limited in temporal and spatial scope. In this study, we investigated the drivers of land cover change for the 9700 km2 extent of the park using satellite remote sensing and geostatistical modelling. Random forest classification was used to hindcast and simulate land cover change for four different land cover types from 1985 to 2019 with topographic and surface reflectance imagery (Landsat archive). The resulting land cover maps, in addition to topographic and biotic variables, were then used to predict where future shrub expansion is likely to occur using a binomial regression framework. Land cover hindcasts showed a 235% increase in shrub and a 105% increase in wet vegetation cover from 1985/89 to 2015/19. Shrub cover was highly persistent and displaced wet vegetation in southern, low-elevation areas, whereas wet vegetation expanded to formerly dry, mid-elevations. The predictive model identified both biotic (initial cover class, number of surrounding shrub neighbors), and topographic variables (elevation, latitude, and distance to the coast) as strong predictors of future shrub expansion. A further 51% increase in shrub cover is expected by 2039/43 relative to 2014 reference data. Establishing long-term monitoring plots within TMNP in areas where rapid vegetation change is predicted to occur will help to validate remote sensing observations and will improve our understanding of the consequences of change for biotic and abiotic components of the tundra ecosystem, including important cultural keystone species.', 'corpus_id': 235814885, 'score': 0}]"
99	{'doc_id': '212718105', 'title': 'Revisiting wrong sign Yukawa coupling of type II two-Higgs-doublet model in light of recent LHC data', 'abstract': 'In light of the recent LHC Higgs data, we examine the parameter space of type II two-Higgs-doublet model in which the 125 GeV Higgs has the wrong sign Yukawa couplings. Combining related theoretical and experimental limits, we find that the LHC Higgs data exclude most of the parameter space of the wrong sign Yukawa coupling. For $m_H=$ 600 GeV, the allowed samples are mainly distributed in several corners and narrow bands of m_A<20 GeV, 30 GeV<m_A<120 GeV, 240 GeV<m_A<300 GeV, 380 GeV <m_A<430 GeV, and 480 GeV<m_A<550 GeV. For m_A=600 GeV, m_H is required to be less than 470 GeV. The light pseudo-scalar with a mass of 20 GeV is still allowed in case of the wrong sign Yukawa coupling of 125 GeV Higgs.', 'corpus_id': 212718105}	1866	"[{'doc_id': '212657625', 'title': 'The “96 GeV excess” at the LHC', 'abstract': 'The CMS collaboration reported an intriguing [Formula: see text] (local) excess at 96 GeV in the light Higgs-boson search in the diphoton decay mode. This mass coincides with a [Formula: see text] (local) excess in the [Formula: see text] final state at LEP. We briefly review the proposed combined interpretations for the two excesses. In more detail, we review the interpretation of this possible signal as the lightest Higgs boson in the 2 Higgs Doublet Model with an additional real Higgs singlet (N2HDM). We show which channels have the best prospects for the discovery of additional Higgs bosons at the upcoming Run 3 of the LHC.', 'corpus_id': 212657625, 'score': 1}, {'doc_id': '209532047', 'title': 'Probing Exotic Triple Higgs Couplings for Almost Inert Higgs Bosons at the LHC', 'abstract': 'In extended Higgs sectors that exhibit alignment without decoupling, the additional scalars are allowed to have large couplings to the Standard Model Higgs. We show that current nonresonant di-Higgs searches can be straightforwardly adapted to look for additional Higgses in these scenarios, where pair production of non-SM Higgses can be enhanced. For concreteness, we study pair production of exotic Higgses in the context of an almost inert two Higgs doublet model, where alignment is explained through an approximate $\\mathbb{Z}_2$ symmetry under which the additional scalars are odd. In this context, the smallness of the $\\mathbb Z_2$ violating parameter suppresses single production of exotic Higgses, but it does not prevent a sizeable trilinear coupling $hHH$ between the SM Higgs ($h$) and the additional states ($H$). We study the process $pp\\rightarrow h^* \\rightarrow HH$ in the final states of $b\\bar b b \\bar b$, $b\\bar b\\gamma\\gamma$, and multi-leptons. We find that at the HL-LHC these modes could be sensitive to masses of the additional neutral scalars in the range $130\\mbox{ GeV} \\lesssim m_H \\lesssim 290\\mbox{ GeV}$.', 'corpus_id': 209532047, 'score': 1}, {'doc_id': '214792914', 'title': 'Me(a)t the Future', 'abstract': 'When I was given the opportunity to take the podium at last year’s WTO-public forum in Geneva, I raised the issue of crisis management and the necessity of coming together, more efficiently and much faster than the usual worldwide procedure. I repeated it in a Civil Dialogue Group in Brussels this January, although I have to admit that at the time, I was actually thinking about a solution for the African Swine Fever. Today, the Coronavirus puts the world in a completely new and serious situation that hardly anyone could have predicted.', 'corpus_id': 214792914, 'score': 0}, {'doc_id': '211021035', 'title': 'Suppression of the Higgs boson dimuon decay', 'abstract': ""It is often stated that elimination of tree-level flavor-changing neutral currents in multi-Higgs models requires that all fermions of a given charge to couple to the same Higgs boson. A counterexample was provided by Abe, Sato and Yagyu in a muon-specific two-Higgs doublet model with a softly-broken $Z_4$ symmetry. In this model, all fermions except the muon couple to one Higgs and the muon couples to the other. We study the phenomenology of the model and show that there is a wide range of parameter-space in which the branching ratios of the 125 GeV Higgs are very close to their Standard Model values, with the exception of the branching ratio into muons, which can be substantially suppressed -- this is an interesting possibility, since the current value of this branching ratio is $0.5 \\pm 0.7$ times the Standard Model value. We also study the charged Higgs boson and show that, if it is lighter than $200$ GeV, it could have a large branching ratio into $\\mu\\nu$ - even substantially larger than the usual decay into $\\tau\\nu$. The decays of the heavy neutral scalars are also studied. The model does have a relationship between the branching ratios of the 125 GeV Higgs into $Z$'s, $\\tau$'s and $\\mu$'s, which can be tested in future accelerators."", 'corpus_id': 211021035, 'score': 1}, {'doc_id': '214775593', 'title': 'Aggregate and Firm-Level Stock Returns During Pandemics, in Real Time', 'abstract': ""We show that unexpected changes in the trajectory of COVID-19 infections predict US stock returns, in real time. Parameter estimates indicate that an unanticipated doubling (halving) of projected infections forecasts next-day decreases (increases) in aggregate US market value of 4 to 11 percent, indicating that equity markets may begin to rebound even as infections continue to rise, if the trajectory of the disease becomes less severe than initially anticipated. Using the same variation in unanticipated projected cases, we find that COVID-19-related losses in market value at the firm level rise with capital intensity and leverage, and are deeper in industries more conducive to disease transmission. These relationships provide important insight into current record job losses. Measuring US states' drops in market value as the employment weighted average declines of the industries they produce, we find that states with milder drops in market value exhibit larger initial jobless claims per worker. This initially counter-intuitive result suggests that investors value the relative ease with which labor versus capital costs can be shed as revenues decline."", 'corpus_id': 214775593, 'score': 0}, {'doc_id': '214641024', 'title': 'Implications of light charged Higgs boson at the LHC Run III in the 2HDM', 'abstract': 'In this study, we focus on the bosonic decays of light charged Higgs boson (i.e., with $M_{H^\\pm}<m_t$) in the 2-Higgs Doublet Model (2HDM) Type-I. To study the signal of such a charged Higgs state at the Large Hadron Collider (LHC), in a scenario where the $H^0$ boson is the Standard Model (SM)-like one already discovered, we assume that it decays mainly via $h^0W^{\\pm *}$ and/or $H^\\pm\\to A^0W^{\\pm *}$ (i.e., via an off-shell $W^{\\pm}$ boson), which can reach a sizable Branching Ratio (BR) for $\\tan\\beta\\geq4$, when the exclusion bounds from $H^\\pm\\to\\tau\\nu$ and $c{s}$ searches get weaker. By using six Benchmark Points (BPs), which are consistent with current LHC constraints, we perform a Monte Carlo (MC) study and examine the sensitivity of the LHC to light charged Higgs boson decaying via the above bosonic modes and produced in top decay following both single top and top pair production processes. Our findings demonstrate that, when the integrated luminosity can reach 100 fb$^{-1}$, the LHC has the potential to either discover or rule out most of these BPs via either of these two production and decay channels or both.', 'corpus_id': 214641024, 'score': 1}, {'doc_id': '212718125', 'title': 'Light charged Higgs boson production via $cb-$fusion at the Large Hadron Collider', 'abstract': 'We analyse the production of a light charged Higgs boson at the Large Hadron Collider (LHC) via the quark-fusion mechanism cb̄ → H considering the decay channel H → τ ν̄τ in the final state. We study this process in the framework of the 2-Higgs Doublet Model Type III (2HDM-III) with lepton-specific Yukawa couplings and assess the LHC sensitivity to such H signals against the dominant irreducible background. We show that BR(H → cb) ∼ 0.1 − 0.2 and BR(H → τν) ∼ 0.7− 0.9 so that, under these conditions, the prospects for H detection in the 2HDM-III in the aforementioned production and decay channels are excellent assuming standard collider energy and luminosity conditions.', 'corpus_id': 212718125, 'score': 1}, {'doc_id': '214779060', 'title': 'ESTIMATING THE PREVALENCE: PROPERTIES OF THE ESTIMATOR REGARDING SPECIFICITY AND SENSITIVITYOF THE UNDERLYING TEST', 'abstract': 'We provide a calculation tool to assess the properties of a maximumlikelihood (ML) estimator that extrapolates the true prevalence of an infectious disease from a random sample. The tools allow the researcher to correct for the specificity and sensitivity of the underlying medical test, calculate the standard deviation of the estimator and to plan the needed sample size. This document explains the underlying methods of the calculation tools and provides instructions for their proper use. We apply an adaption of the epidemiological SEIR-model to show that ML-estimators from random sampling tests provide a more realistic rate of infection than common approaches. ∗The authors greatly appreciate the research assistance of Falk Wendorff †Kiel Institute for the World Economy ‡Kiel Institute for the World Economy, Kiel University 1 Aim of the calculators During pandemic outbreaks of infectious diseases policy makers are forced to take actions against the spread quickly, often at the expense of economic activity. A recent study by Burns et al. (2006) estimates that 60% of economic damages incurred during a pandemic can be attributed to demand shocks, i.e. the indirect costs of an outbreak. Factoring in the interruption of supply chains and detrimental uncertainty likely increases the economic costs significantly. While human health must be protected, governments typically have limited information on the actual spread of the disease. Indeed, the true rate of infection in the population is rarely known. Random testing can be a remedy to achieve the needed information of the prevalence. However, given that specificity and sensitivity of a test can deviate from one, the prevalence has to be estimated from test results e.g. via a Maximum Likelihood estimation. We provide the ready-to-use tools for such a Maximum Likelihood estimation, which calculates the standard deviation of the estimator for given sensitivity, specificity, sample size and expected prevalence. Vice versa the needed sample size can be retrieved for a standard deviation or precision that shall be achieved. While the tools presented in this paper are applicable to any infectious disease, we provide examples from the COVID-19 pandemic. Indeed, the outbreak of Sars-CoV-2 is a suitable illustration for the need of statistical tests: At the moment, mainly patients who are at high risk of infection (e.g. because of contact with an infected individual) are tested for the presence of the pathogen by use of a rRT-PCR (reverse transcription polymerase chain reaction) test. This approach swiftly diagnoses COVID-19 and helps to trace the chain of infection. However, the virus has a high level of contagion, an incubation period of approximately five days (Lauer et al., 2020) and results only in minor symptoms for many people. This suggests that the true rate of', 'corpus_id': 214779060, 'score': 0}, {'doc_id': '214777187', 'title': 'Inhibitor Prevents Viral Infection', 'abstract': 'The human body is a constant flux of thousands of chemical/biological interactions and processes connecting molecules, cells, organs, and fluids, throughout the brain, body, and nervous system. Up until recently it was thought that all these interactions operated in a linear sequence, passing on information much like a runner passing the baton to the next runner. However, the latest findings in quantum biology and biophysics have discovered that there is in fact a tremendous degree of coherence within all living systems.', 'corpus_id': 214777187, 'score': 0}, {'doc_id': '214780318', 'title': 'Antibiotic Resistance Gene', 'abstract': 'A completely new resistance gene, which is likely to counteract the newest aminoglycoside-drug plazomycin, was recently discovered by scientists in Gothenburg, Sweden. [15] Now investigators at Massachusetts General Hospital (MGH) have modified the system to be nearly free of this requirement, making it possible to potentially target any location across the entire human genome. [14] An ancient group of microbes that contains some of the smallest life forms on Earth also has the smallest CRISPR gene-editing machinery discovered to date. [13] ETH scientists have been able to prove that a protein structure widespread in nature – the amyloid – is theoretically capable of multiplying itself. [12]', 'corpus_id': 214780318, 'score': 0}]"
100	{'doc_id': '149314182', 'title': 'A Scientific Approach to Entrepreneurial Decision-Making: Evidence from a Randomized Control Trial', 'abstract': 'A classical approach to collecting and elaborating information to make entrepreneurial decisions combines search heuristics, such as trial and error, effectuation, and confirmatory search. This paper develops a framework for exploring the implications of a more scientific approach to entrepreneurial decision making. The panel sample of our randomized control trial includes 116 Italian startups and 16 data points over a period of about one year. Both the treatment and control groups receive 10 sessions of general training on how to obtain feedback from the market and gauge the feasibility of their idea. We teach the treated startups to develop frameworks for predicting the performance of their idea and conduct rigorous tests of their hypotheses, very much as scientists do in their research. We let the firms in the control group instead follow their intuitions about how to assess their idea, which has typically produced fairly standard search heuristics. We find that entrepreneurs who behave like scientists perform better, are more likely to pivot to a different idea, and are not more likely to drop out than the control group in the early stages of the startup. These results are consistent with the main prediction of our theory: a scientific approach improves precision—it reduces the odds of pursuing projects with false positive returns and increases the odds of pursuing projects with false negative returns.', 'corpus_id': 149314182}	18785	"[{'doc_id': '235195356', 'title': 'Board Reforms and Innovation', 'abstract': ""We study the effect of board reforms on firms' research and development investments utilizing a sample of 40 countries. Using a difference-in-differences analysis, we find that firms' invest more in research and development following corporate governance reforms. Of these, two reforms - having an independent audit committee and board independence - have a greater impact on innovation. Additionally, we show that reforms have the largest impact on research and development investment in hi-tech industries and the health sector."", 'corpus_id': 235195356, 'score': 0}, {'doc_id': '182309275', 'title': 'Approccio Scientifico al Decision Making: l’impatto del leader di una startup = Scientific Approach to Decision Making: the impact of a startup leader', 'abstract': ""Approccio scientifico al decision making: un esperimento RCT L'impatto delle caratteristiche del leader di una startup sul processo decisionale"", 'corpus_id': 182309275, 'score': 1}, {'doc_id': '235079323', 'title': 'Design and Evaluation of Optimal Free Trials', 'abstract': 'Free trial promotions are a commonly used customer acquisition strategy in the Software as a Service (SaaS) industry. We use data from a large-scale field experiment conducted by a leading SaaS firm to study the effect of trial length on outcomes and the returns from personalized trial length assignment. We find that the 7-days trial is the best average treatment that maximizes customer acquisition, retention, and profitability. In terms of mechanism, we rule out the demand cannibalization theory, find support for the consumer learning hypothesis, and show that long stretches of inactivity at the end of the trial are associated with lower conversions. We then develop a framework for personalized targeting policy design and evaluation. We first learn a lasso model of outcomes as a function of users’ pre-treatment variables and treatment. Next, we use individual-level predictions of the outcome to assign the optimal treatment to each user. We then evaluate the personalized policy using the inverse propensity score reward estimator. We find that a personalization based on lasso leads to 6.8% improvement in subscription compared to a uniform 30-days for all policy. It also performs well on long-term customer retention and revenues. Segmentation analysis suggests that skilled and experienced users are more likely to benefit from longer trials. Finally, we see that personalized policies based on many outcome estimators and heterogeneous treatment effects estimators (e.g., generalized random forests) perform poorly. This suggests that adopting a simple uniform policy can be better than personalizing policies based on these methods.', 'corpus_id': 235079323, 'score': 0}, {'doc_id': '235375442', 'title': 'Experimentation and Incrementalism: The Impact of the Adoption of A/B Testing', 'abstract': 'This paper studies how the adoption of experimentation as a selection method shapes the direction of innovation. The spread of A/B testing, or digital randomized experiments, has made experimentation one of the most common methods organizations use to evaluate and select internally generated ideas. The strength of experimental evidence can be a force for radical innovation. By providing seemingly irrefutable evidence, well-designed and wellexecuted experiments can disabuse people of their false beliefs and generate breakthrough discoveries. However, I argue that the adoption of experimentation can also result in incrementalism, whereby firms focus on minuscule yet reliable improvements. These divergent outcomes can be explained by the incentives driving the people who design and implement experiments. The incentives of managers in established firms may lead them to use experiments in a way that undermines the pursuit of novelty while encouraging the search for incremental improvements. I investigate the relationship between experimentation and innovation in the context of US newspaper websites and their adoption of A/B testing. Using a historical archive of US newspaper websites and a novel computational method, I find that the adoption of A/B testing decreases the likelihood of radical change and makes websites more likely to change incrementally. ∗I am grateful to Jesper B. Sørensen, Julien Clement, William Barnett, J.P. Eggers, Hazjier Pourkhalkhali, seminar participants at Stanford GSB, Academy of Management, University of Chicago Computational Social Science Workshop, UC Berkeley Haas School of Business, Wisconsin Business School, University of Toronto Rotman School of Management, Harvard Business School, UCLA Anderson, UCL School of Management, Sabanci University, HEC Paris, Trans Atlantic Doctoral Conference, and Organizational Theory and Economic Sociology Conference for their helpful comments and feedback. The content is solely the responsibility of the author and does not represent the affiliated institutions.', 'corpus_id': 235375442, 'score': 1}, {'doc_id': '235312261', 'title': 'CESifo Working Paper no. 6642', 'abstract': 'Today, startups often obtain financing via the Internet through many small contributions of nonsophisticated investors. Yet little is known about whether these startups can ultimately build enduring businesses. In this article, we hand-collected data from 14 different equity crowdfunding (ECF) portals and 426 firms that ran at least one successful ECF campaign in Germany or the United Kingdom. We empirically analyze different factors affecting follow-up funding and firm failure. The findings show that German firms that received ECF stood a higher chance of obtaining follow-up funding through business angels or venture capitalists, but also had a higher likelihood of failure. The number of senior managers, subsequent successful ECF campaigns, and the number of venture capital investors all had a positive impact on obtaining post-campaign financing, while firm age had a negative impact. Subsequent successful ECF campaigns were significant predictors decreasing firm failure. JEL-Codes: G240, M130.', 'corpus_id': 235312261, 'score': 1}, {'doc_id': '235657886', 'title': 'Go West Young Firm: The Benefits of Startup Relocation to Silicon Valley', 'abstract': 'I study the benefits to entrepreneurial migration, focused on firms moving to Silicon Valley. Using a machine learning estimator and panel data, I find moving to Silicon Valley leads to higher startup performance on equity outcomes, financing, patenting, products, and revenue. These results are robust to a stringent coefficient stability test, and show no evidence of pre-trends. The benefits are partially driven by knowledge spillovers, and sensitive to capital market conditions during migration. Despite the positive benefits to migration, most startups do not move. A simple analysis suggests this may be due to the personal costs of moving for founders themselves.', 'corpus_id': 235657886, 'score': 0}, {'doc_id': '169673212', 'title': 'Experimental Evidence of a Scientific Approach to Decision Making of Entrepreneurial Firms', 'abstract': 'This study examines the impact of a scientific approach to decision making on early-stage entrepreneurial firms. In these early-stage projects, we argue that using a scientific approach to decision making leads to a higher chance to uncover false positives and false negatives associated with new business ideas. We hypothesize that this precision helps entrepreneurs to understand both if their ideas have negative financial returns and if tweaking their idea (pivot) can lead to more positive financial returns. We embed a field experiment in a pre-accelerator program geared towards early-stage entrepreneurial firms, and use this initiative to provide treatment to a group of entrepreneurs (130 entrepreneurial teams) by training them on how to use a scientific approach to business development. The control group (128 entrepreneurial teams) receives the same amount and type of training, but is not taught to use a scientific approach to business development. Consistently with our predictions, we find that the use...', 'corpus_id': 169673212, 'score': 1}, {'doc_id': '164710866', 'title': 'Small Changes with Big Impact: Experimental Evidence of a Scientific Approach to Decision-Making of Entrepreneurs', 'abstract': 'This study examines the impact of a scientific approach to decision-making on early-stage entrepreneurial firms. We argue that using a scientific approach to decision-making increases the probabili...', 'corpus_id': 164710866, 'score': 1}, {'doc_id': '235783834', 'title': 'Fear of the unknown - The effect of economic policy uncertainty on start-up financing and success', 'abstract': 'This paper investigates the effect of economic policy uncertainty on the financing and success probability of start-ups in the European venture capital market. Specifically, our results show that venture capital investors are less likely to engage in financing activities and start-ups are less likely to have a successful exit under high economic policy uncertainty. Even after controlling for economic uncertainty and industry trends, our results remain stable and significant, highlighting the importance of uncertainty surrounding government policies. However, we point out the special role of governmental venture capital as it bridges the financing gap for start-ups under high levels of economic policy uncertainty. Besides, governments seem to be better investors facing higher uncertainty leading to crucial implications for government policies. JEL classification: G24, G28, G34, H11', 'corpus_id': 235783834, 'score': 0}, {'doc_id': '235351378', 'title': 'Stimulating Marketing Strategy Innovation with Entrepreneurs in Uganda: Examining the Impact of Skype-aided Business Coaching on Firm Sales', 'abstract': 'This paper studies the impact of Skype-aided remote business coaching on the strategies and sales of emerging market entrepreneurs. It sheds light on three novel research questions: (1) What is the effect of remote business coaching on firm sales? (2) What is the mechanism through which this effect occurs; specifically, does remote coaching stimulate changes in marketing strategies (pivots)? (3) Do entrepreneurs benefit more from remote coaching when they are less strategic in their decision-making? We conducted a randomized controlled field experiment with 930 entrepreneurs in Uganda to examine the impact of a remote coaching intervention that connects management professionals in primarily advanced markets and entrepreneurs in emerging markets with the aim of improving business performance. The analysis finds a positive and significant main effect on firm sales – treated entrepreneurs increase monthly sales by 27.6% on average. In addition, entrepreneurs who receive remote coaching are 63.3% more likely to have “pivoted” or shifted their marketing strategy in a new direction. And consistent with this mechanism of inducing strategic business changes, the results show that entrepreneurs who receive remote coaching tend to do better when they (ex ante) lack strategic focus. These results have important implications for the development of marketing strategies by entrepreneurs and multinational managers, as well as for organizations interested in improving the performance of small firms in emerging markets and beyond.', 'corpus_id': 235351378, 'score': 0}]"
101	{'doc_id': '222178732', 'title': 'A Practical Guide to Resonance Frequency Assessment for Heart Rate Variability Biofeedback', 'abstract': 'Heart rate variability (HRV) represents fluctuations in the time intervals between successive heartbeats, which are termed interbeat intervals. HRV is an emergent property of complex cardiac-brain interactions and non-linear autonomic nervous system (ANS) processes. A healthy heart is not a metronome because it exhibits complex non-linear oscillations characterized by mathematical chaos. HRV biofeedback displays both heart rate and frequently, respiration, to individuals who can then adjust their physiology to improve affective, cognitive, and cardiovascular functioning. The central premise of the HRV biofeedback resonance frequency model is that the adult cardiorespiratory system has a fixed resonance frequency. Stimulation at rates near the resonance frequency produces large-amplitude blood pressure oscillations that can increase baroreflex sensitivity over time. The authors explain the rationale for the resonance frequency model and provide detailed instructions on how to monitor and assess the resonance frequency. They caution that patterns of physiological change must be compared across several breathing rates to evaluate candidate resonance frequencies. They describe how to fine-tune the resonance frequency following an initial assessment. Furthermore, the authors critically assess the minimum epochs required to measure key HRV indices, resonance frequency test-retest reliability, and whether rhythmic skeletal muscle tension can replace slow paced breathing in resonance frequency assessment.', 'corpus_id': 222178732}	16635	"[{'doc_id': '233225167', 'title': 'Intensification of functional neural control on heartbeat dynamics in subclinical depression', 'abstract': 'Subclinical depression (dysphoria) is a common condition that may increase the risk of major depression and leads to impaired quality of life and severe comorbid somatic diseases. Despite its prevalence, specific biological markers are unknown; consequently, the identification of dysphoria currently relies exclusively on subjective clinical scores and structured interviews. Based on recent neurocardiology studies that link brain and cardiovascular disorders, it was hypothesized that multi-system biomarkers of brain–body interplay may effectively characterize dysphoria. Thus, an ad hoc computational technique was developed to quantify the functional bidirectional brain–heart interplay. Accordingly, 32-channel electroencephalographic and heart rate variability series were obtained from 24 young dysphoric adults and 36 healthy controls. All participants were females of a similar age, and results were obtained during a 5-min resting state. The experimental results suggest that a specific feature of dysphoria is linked to an augmented functional central-autonomic control to the heart, which originates from central, frontopolar, and occipital oscillations and acts through cardiovascular sympathovagal activity. These results enable further development of a large set of novel biomarkers for mood disorders based on comprehensive brain–body measurements.', 'corpus_id': 233225167, 'score': 0}, {'doc_id': '232773720', 'title': 'Neurophysiological Approach by Self-Control of Your Stress-Related Autonomic Nervous System with Depression, Stress and Anxiety Patients', 'abstract': 'Background: Heart Rate Variability Biofeedback (HRVB) is a treatment in which patients learn self-regulation of a physiological dysregulated vagal nerve function. While the therapeutic approach of HRVB is promising for a variety of disorders, it has not yet been regularly offered in a mental health treatment setting. Aim: To provide a systematic review about the efficacy of HRV-Biofeedback in treatment of anxiety, depression, and stress related disorders. Method: Systematic review in PubMed and Web of Science in 2020 with terms HRV, biofeedback, Post-Traumatic Stress Disorder (PTSD), depression, panic disorder, and anxiety disorder. Selection, critical appraisal, and description of the Random Controlled Trials (RCT) studies. Combined with recent meta-analyses. Results: The search resulted in a total of 881 studies. After critical appraisal, nine RCTs have been selected as well as two other relevant studies. The RCTs with control groups treatment as usual, muscle relaxation training and a “placebo“-biofeedback instrument revealed significant clinical efficacy and better results compared with control conditions, mostly significant. In the depression studies average reduction at the Beck Depression Inventory (BDI) scale was 64% (HRVB plus Treatment as Usual (TAU) versus 25% (control group with TAU) and 30% reduction (HRVB) at the PSQ scale versus 7% (control group with TAU). In the PTSD studies average reduction at the BDI-scale was 53% (HRV plus TAU) versus 24% (control group with TAU) and 22% (HRVB) versus 10% (TAU) with the PTSD Checklist (PCL). In other systematic reviews significant effects have been shown for HRV-Biofeedback in treatment of asthma, coronary artery disease, sleeping disorders, postpartum depression and stress and anxiety. Conclusion: This systematic review shows significant improvement of the non-invasive HRVB training in stress related disorders like PTSD, depression, and panic disorder, in particular when combined with cognitive behavioral therapy or different TAU. Effects were visible after four weeks of training, but clinical practice in a longer daily self-treatment of eight weeks is more promising. More research to integrate HRVB in treatment of stress related disorders in psychiatry is warranted, as well as research focused on the neurophysiological mechanisms.', 'corpus_id': 232773720, 'score': 0}, {'doc_id': '11104601', 'title': 'An Overview of Heart Rate Variability Metrics and Norms', 'abstract': 'Healthy biological systems exhibit complex patterns of variability that can be described by mathematical chaos. Heart rate variability (HRV) consists of changes in the time intervals between consecutive heartbeats called interbeat intervals (IBIs). A healthy heart is not a metronome. The oscillations of a healthy heart are complex and constantly changing, which allow the cardiovascular system to rapidly adjust to sudden physical and psychological challenges to homeostasis. This article briefly reviews current perspectives on the mechanisms that generate 24\u2009h, short-term (~5\u2009min), and ultra-short-term (<5\u2009min) HRV, the importance of HRV, and its implications for health and performance. The authors provide an overview of widely-used HRV time-domain, frequency-domain, and non-linear metrics. Time-domain indices quantify the amount of HRV observed during monitoring periods that may range from ~2\u2009min to 24\u2009h. Frequency-domain values calculate the absolute or relative amount of signal energy within component bands. Non-linear measurements quantify the unpredictability and complexity of a series of IBIs. The authors survey published normative values for clinical, healthy, and optimal performance populations. They stress the importance of measurement context, including recording period length, subject age, and sex, on baseline HRV values. They caution that 24\u2009h, short-term, and ultra-short-term normative values are not interchangeable. They encourage professionals to supplement published norms with findings from their own specialized populations. Finally, the authors provide an overview of HRV assessment strategies for clinical and optimal performance interventions.', 'corpus_id': 11104601, 'score': 1}, {'doc_id': '31350772', 'title': 'The Future of Heart Rate Variability Biofeedback', 'abstract': None, 'corpus_id': 31350772, 'score': 1}, {'doc_id': '232413559', 'title': 'Altered Heart Rate Variability in Patients With Schizophrenia During an Autonomic Nervous Test', 'abstract': 'Reduced heart rate variability (HRV) and dysfunction of the autonomic nervous system (ANS) have been observed in schizophrenia patients. HRV parameters of schizophrenia patients in the resting state have been well-documented; however, these parameters of schizophrenia patients who experience continuous psychophysiological stress remain unclear. The objective of this study was to systematically explore the linear and nonlinear HRV parameters between schizophrenia patients and normal controls and to detect the adaptive capabilities of HRV of schizophrenia patients during the stimulation tests of autonomic nervous system. Forty-five schizophrenia patients and forty-five normal controls, matched for age, sex and body mass index, completed a 14 min ANS test. Thirteen linear and nonlinear HRV parameters of all subjects under the ANS test were computed and statistically analyzed between groups and between sessions. The STROBE checklist was adhered to in this study. All time-domain HRV features in the ANS test were significantly different between schizophrenia patients and normal controls (p < 0.01). The schizophrenia patients showed significantly low values in the Poincaré indices, which revealed significantly decreased heart rate fluctuation complexity compared with that of normal controls (p < 0.001). In addition, the normal controls, not schizophrenia patients, showed significant differences between the recovery and stress states in the parameters of low frequency, high frequency, and nonlinear dynamics. Schizophrenia patients showed autonomic dysfunction of the heart in a series of stimulation tests of the autonomic nervous system and could not regain normal physiological functions after stress cessation. Our findings revealed that the dynamic parameters of HRV in psychophysiological stress are sensitive and practical for a diagnosis of schizophrenia.', 'corpus_id': 232413559, 'score': 0}, {'doc_id': '205352055', 'title': 'Breathing at a rate of 5.5 breaths per minute with equal inhalation-to-exhalation ratio increases heart rate variability.', 'abstract': 'OBJECTIVES\nPrior studies have found that a breathing pattern of 6 or 5.5 breaths per minute (bpm) was associated with greater heart rate variability (HRV) than that of spontaneous breathing rate. However, the effects of combining the breathing rate with the inhalation-to-exhalation ratio (I:E ratio) on HRV indices are inconsistent. This study aimed to examine the differences in HRV indices and subjective feelings of anxiety and relaxation among four different breathing patterns.\n\n\nMETHODS\nForty-seven healthy college students were recruited for the study, and a Latin square experimental design with a counterbalance in random sequences was applied. Participants were instructed to breathe at two different breathing rates (6 and 5.5 breaths) and two different I:E ratios (5:5 and 4:6). The HRV indices as well as anxiety and relaxation levels were measured at baseline (spontaneous breathing) and for the four different breathing patterns.\n\n\nRESULTS\nThe results revealed that a pattern of 5.5 bpm with an I:E ratio of 5:5 produced a higher NN interval standard deviation and higher low frequency power than the other breathing patterns. Moreover, the four different breathing patterns were associated with significantly increased feeling of relaxation compared with baseline.\n\n\nCONCLUSION\nThe study confirmed that a breathing pattern of 5.5 bpm with an I:E ratio of 5:5 achieved greater HRV than the other breathing patterns. This finding can be applied to HRV biofeedback or breathing training in the future.', 'corpus_id': 205352055, 'score': 1}, {'doc_id': '14752473', 'title': ""A healthy heart is not a metronome: an integrative review of the heart's anatomy and heart rate variability"", 'abstract': ""Heart rate variability (HRV), the change in the time intervals between adjacent heartbeats, is an emergent property of interdependent regulatory systems that operate on different time scales to adapt to challenges and achieve optimal performance. This article briefly reviews neural regulation of the heart, and its basic anatomy, the cardiac cycle, and the sinoatrial and atrioventricular pacemakers. The cardiovascular regulation center in the medulla integrates sensory information and input from higher brain centers, and afferent cardiovascular system inputs to adjust heart rate and blood pressure via sympathetic and parasympathetic efferent pathways. This article reviews sympathetic and parasympathetic influences on the heart, and examines the interpretation of HRV and the association between reduced HRV, risk of disease and mortality, and the loss of regulatory capacity. This article also discusses the intrinsic cardiac nervous system and the heart-brain connection, through which afferent information can influence activity in the subcortical and frontocortical areas, and motor cortex. It also considers new perspectives on the putative underlying physiological mechanisms and properties of the ultra-low-frequency (ULF), very-low-frequency (VLF), low-frequency (LF), and high-frequency (HF) bands. Additionally, it reviews the most common time and frequency domain measurements as well as standardized data collection protocols. In its final section, this article integrates Porges' polyvagal theory, Thayer and colleagues' neurovisceral integration model, Lehrer et al.'s resonance frequency model, and the Institute of HeartMath's coherence model. The authors conclude that a coherent heart is not a metronome because its rhythms are characterized by both complexity and stability over longer time scales. Future research should expand understanding of how the heart and its intrinsic nervous system influence the brain."", 'corpus_id': 14752473, 'score': 1}, {'doc_id': '233385894', 'title': 'Effect of Yoga on Pulse rate and Oxygen Saturation: Analysis of Psychophysiological Parameters', 'abstract': 'Introduction Pranayamic breathing is a process of continuous, regularity of inhalation, holding of breath and exhalation. All venous blood is converted to oxygenated blood . However, does deep breathing in which oxygen is inhaled in large amounts increase oxygen saturation or does the saturation decrease due to anaerobic metabolism associated with yoga? Does the psycho-physiological parameters of stress index, power, vegetative index & regulation, neurohumoral regulation, psycho-emotional state, energy resources, complex index, harmonization, biological age and energies in the spine get affected a""er a yogic intervention? Methods 52 subjects of age range from 15-70 years performed “Yoga module for the Healthy Heart” for 45 minutes at AYUSH, AIIMS, Bhopal. Pulse rate and oxygen saturation was measured by pulse oximeter a""er initial rest of ten minutes and a""er 45 minutes of yoga. A pilot study was conducted using the DINAMIKA HRV for ten yogic practitioner who were regular in their practice for last 10-15 years. #eir psycho-physiological parameters were measured before and a""er their yogic routine of 35 to 40 minutes by Dinamika Heart rate variability (HRV) instrument. Results #e readings were analysed using paired t  test. #e pulse rate dropped from 81.98 ± 13.05 to is 74.98 ± 11.64 at p value <0.0001 indicating a shi"" towards parasympathetic dominance. Oxygen saturation dropped from 97.40 +/1.11 to 97.21 +/1.30 at p value of 0.2736, indicating a shi"" to anaerobic metabolism during yoga practice #e psychophysiological parameters of pulse rate, stress index, power, vegetative index & regulation, neurohumoral regulation, psycho-emotional state, energy resources, complex index, harmonization, biological age and energies in the spine were statistically significant post yogic intervention. By the power of will the yogic practitioner is able to draw cosmic energy in the spine which helps to renew it. #e mind is able to overcome strong physical distractions, the body is relaxed and calm. Relaxation is achieved by stilling of muscles, calming and slowing down the activity of heart, respiration and circulation.', 'corpus_id': 233385894, 'score': 0}, {'doc_id': '233997132', 'title': 'Association of heart rate variability, blood pressure variability, and baroreflex sensitivity with gastric motility at rest and during cold pressor test', 'abstract': 'Aim: To understand the mutual interaction of gastric motility and autonomic functions, the present study evaluated the association of heart rate variability (HRV), blood pressure variability (BPV), and baroreflex sensitivity (BRS) with gastric motility assessed by electrogastrography (EGG) at rest and during CPT and explored the effect of sympathetic activation by cold pressor test (CPT) on gastric motility. Background: The autonomic nervous system has a significant influence on gastrointestinal motility. HRV is commonly employed to assess the functions of the autonomic nervous system. BPV and BRS are relatively newer techniques and give a more holistic picture of autonomic functions along with the short-term regulation of blood pressure (BP). Methods: In fourteen young, healthy subjects, gastric motility was assessed by EGG. Beat-to-beat BP and lead II ECG were recorded to assess HRV, BPV, and BRS. BPV and BRS parameters were calculated for systolic, mean, and diastolic BP. Parameters of HRV and BPV were calculated for time and frequency domains. BRS was calculated by sequence and spectral methods. Results Significant increases in diastolic BP (p = <0.0001) and EGG frequency (p = 0.0229) were observed during CPT. Significant correlations were observed between EGG frequencies and many of the HRV, BPV, and BRS parameters. The correlation coefficient was found to be highest between total power of HRV and EGG frequencies during baseline (p = 0.0107, r = -0.6571) and during CPT (p = 0.0059, r = -0.6935). Conclusion: EGG frequency can be decreased by an acute increase in sympathetic activity induced by CPT. The novel findings are the significant correlations between many of the HRV, BPV, and BRS parameters and EGG frequency.', 'corpus_id': 233997132, 'score': 0}, {'doc_id': '17621603', 'title': 'Heart rate variability biofeedback: how and why does it work?', 'abstract': 'In recent years there has been substantial support for heart rate variability biofeedback (HRVB) as a treatment for a variety of disorders and for performance enhancement (Gevirtz, 2013). Since conditions as widely varied as asthma and depression seem to respond to this form of cardiorespiratory feedback training, the issue of possible mechanisms becomes more salient. The most supported possible mechanism is the strengthening of homeostasis in the baroreceptor (Vaschillo et al., 2002; Lehrer et al., 2003). Recently, the effect on the vagal afferent pathway to the frontal cortical areas has been proposed. In this article, we review these and other possible mechanisms that might explain the positive effects of HRVB.', 'corpus_id': 17621603, 'score': 1}]"
102	{'doc_id': '53081945', 'title': 'NPRF: A Neural Pseudo Relevance Feedback Framework for Ad-hoc Information Retrieval', 'abstract': 'Pseudo relevance feedback (PRF) is commonly used to boost the performance of traditional information retrieval (IR) models by using top-ranked documents to identify and weight new query terms, thereby reducing the effect of query-document vocabulary mismatches. While neural retrieval models have recently demonstrated strong results for ad-hoc retrieval, combining them with PRF is not straightforward due to incompatibilities between existing PRF approaches and neural architectures. To bridge this gap, we propose an end-to-end neural PRF framework that can be used with existing neural IR models by embedding different neural models as building blocks. Extensive experiments on two standard test collections confirm the effectiveness of the proposed NPRF framework in improving the performance of two state-of-the-art neural IR models.', 'corpus_id': 53081945}	18792	"[{'doc_id': '235271356', 'title': 'RuBQ 2.0: An Innovated Russian Question Answering Dataset', 'abstract': 'The paper describes the second version of RuBQ, a Russian dataset for knowledge base question answering (KBQA) over Wikidata. Whereas the first version builds on Q&A pairs harvested online, the extension is based on questions obtained through search engine query suggestion services. The questions underwent crowdsourced and in-house annotation in a quite different fashion compared to the first edition. The dataset doubled in size: RuBQ 2.0 contains 2,910 questions along with the answers and SPARQL queries. The dataset also incorporates answer-bearing paragraphs from Wikipedia for the majority of questions. The dataset is suitable for the evaluation of KBQA, machine reading comprehension (MRC), hybrid questions answering, as well as semantic parsing. We provide the analysis of the dataset and report several KBQA and MRC baseline results. The dataset is freely available under the', 'corpus_id': 235271356, 'score': 0}, {'doc_id': '235641228', 'title': 'Representing Long Documents with Contextualized Passage Embeddings', 'abstract': 'In this study we investigated a method for processing a large document collection with many long documents. The goal was to improve the processing runtime and memory requirements for document level tasks. We propose to represent the document collection by a database of passage embeddings instead by the document’s tokens. For our approach the passage embeddings should be task-agnostic, such they can be pretrained, precomputed and then be reused for many tasks. We propose a pretraining method for passage embeddings with (i) a passage encoder (PE) coupled with (ii) a bidirectional document encoder (BDE) over the passage embeddings. BDE can be finetuned for different downstream tasks to quickly learn a new model for a new task. In experiments, we found that PE+BDE is competitive with token-level or sentence-level models and sometimes even better. For plagiarism detection, for example, we improve over a Longformer-based model by +14 accuracy points. For passage classification on the contract understanding task PE+BDE reaches an AUPR 60.3 while a token-level information extraction approach using RoBERTalarge obtained an AUPR of 48.2 .', 'corpus_id': 235641228, 'score': 0}, {'doc_id': '235792421', 'title': 'ReadsRE: Retrieval-Augmented Distantly Supervised Relation Extraction', 'abstract': 'Distant supervision (DS) has been widely used to automatically construct (noisy) labeled data for relation extraction (RE). To address the noisy label problem, most models have adopted the multi-instance learning paradigm by representing entity pairs as a bag of sentences. However, this strategy depends on multiple assumptions (e.g., all sentences in a bag share the same relation), which may be invalid in real-world applications. Besides, it cannot work well on long-tail entity pairs which have few supporting sentences in the dataset. In this work, we propose a new paradigm named retrieval-augmented distantly supervised relation extraction (ReadsRE), which can incorporate large-scale open-domain knowledge (e.g., Wikipedia) into the retrieval step. ReadsRE seamlessly integrates a neural retriever and a relation predictor in an end-to-end framework. We demonstrate the effectiveness of ReadsRE on the well-known NYT10 dataset. The experimental results verify that ReadsRE can effectively retrieve meaningful sentences (i.e., denoise), and relieve the problem of long-tail entity pairs in the original dataset through incorporating external open-domain corpus. Through comparisons, we show ReadsRE outperforms other baselines for this task.', 'corpus_id': 235792421, 'score': 0}, {'doc_id': '235212203', 'title': 'Improve Query Focused Abstractive Summarization by Incorporating Answer Relevance', 'abstract': 'Query focused summarization (QFS) models aim to generate summaries from source documents that can answer the given query. Most previous work on QFS only considers the query relevance criterion when producing the summary. However, studying the effect of answer relevance in the summary generating process is also important. In this paper, we propose QFS-BART, a model that incorporates the explicit answer relevance of the source documents given the query via a question answering model, to generate coherent and answerrelated summaries. Furthermore, our model can take advantage of large pre-trained models which improve the summarization performance significantly. Empirical results on the Debatepedia dataset show that the proposed model achieves the new state-of-the-art performance.1', 'corpus_id': 235212203, 'score': 0}, {'doc_id': '173990818', 'title': 'Latent Retrieval for Weakly Supervised Open Domain Question Answering', 'abstract': 'Recent work on open domain question answering (QA) assumes strong supervision of the supporting evidence and/or assumes a blackbox information retrieval (IR) system to retrieve evidence candidates. We argue that both are suboptimal, since gold evidence is not always available, and QA is fundamentally different from IR. We show for the first time that it is possible to jointly learn the retriever and reader from question-answer string pairs and without any IR system. In this setting, evidence retrieval from all of Wikipedia is treated as a latent variable. Since this is impractical to learn from scratch, we pre-train the retriever with an Inverse Cloze Task. We evaluate on open versions of five QA datasets. On datasets where the questioner already knows the answer, a traditional IR system such as BM25 is sufficient. On datasets where a user is genuinely seeking an answer, we show that learned retrieval is crucial, outperforming BM25 by up to 19 points in exact match.', 'corpus_id': 173990818, 'score': 1}, {'doc_id': '125545', 'title': 'Task-Oriented Query Reformulation with Reinforcement Learning', 'abstract': 'Search engines play an important role in our everyday lives by assisting us in finding the information we need. When we input a complex query, however, results are often far from satisfactory. In this work, we introduce a query reformulation system based on a neural network that rewrites a query to maximize the number of relevant documents returned. We train this neural network with reinforcement learning. The actions correspond to selecting terms to build a reformulated query, and the reward is the document recall. We evaluate our approach on three datasets against strong baselines and show a relative improvement of 5-20% in terms of recall. Furthermore, we present a simple method to estimate a conservative upper-bound performance of a model in a particular environment and verify that there is still large room for improvements.', 'corpus_id': 125545, 'score': 1}, {'doc_id': '235097600', 'title': 'TextGraphs 2021 Shared Task on Multi-Hop Inference for Explanation Regeneration', 'abstract': 'The Shared Task on Multi-Hop Inference for Explanation Regeneration asks participants to compose large multi-hop explanations to questions by assembling large chains of facts from a supporting knowledge base. While previous editions of this shared task aimed to evaluate explanatory completeness – finding a set of facts that form a complete inference chain, without gaps, to arrive from question to correct answer, this 2021 instantiation concentrates on the subtask of determining relevance in large multi-hop explanations. To this end, this edition of the shared task makes use of a large set of approximately 250k manual explanatory relevancy ratings that augment the 2020 shared task data. In this summary paper, we describe the details of the explanation regeneration task, the evaluation data, and the participating systems. Additionally, we perform a detailed analysis of participating systems, evaluating various aspects involved in the multi-hop inference process. The best performing system achieved an NDCG of 0.82 on this challenging task, substantially increasing performance over baseline methods by 32%, while also leaving significant room for future improvement.', 'corpus_id': 235097600, 'score': 0}, {'doc_id': '218470027', 'title': 'Sparse, Dense, and Attentional Representations for Text Retrieval', 'abstract': 'Abstract Dual encoders perform retrieval by encoding documents and queries into dense low-dimensional vectors, scoring each document by its inner product with the query. We investigate the capacity of this architecture relative to sparse bag-of-words models and attentional neural networks. Using both theoretical and empirical analysis, we establish connections between the encoding dimension, the margin between gold and lower-ranked documents, and the document length, suggesting limitations in the capacity of fixed-length encodings to support precise retrieval of long documents. Building on these insights, we propose a simple neural model that combines the efficiency of dual encoders with some of the expressiveness of more costly attentional architectures, and explore sparse-dense hybrids to capitalize on the precision of sparse retrieval. These models outperform strong alternatives in large-scale retrieval.', 'corpus_id': 218470027, 'score': 1}, {'doc_id': '225062257', 'title': 'Retrieve, Rerank, Read, then Iterate: Answering Open-Domain Questions of Arbitrary Complexity from Text', 'abstract': 'Current approaches to open-domain question answering often make crucial assumptions that prevent them from generalizing to real-world settings, including the access to parameterized retrieval systems well-tuned for the task, access to structured metadata like knowledge bases and web links, or a priori knowledge of the complexity of questions to be answered (e.g., single-hop or multi-hop). To address these limitations, we propose a unified system to answer open-domain questions of arbitrary complexity directly from text that works with off-the-shelf retrieval systems on arbitrary text collections. We employ a single multi-task model to perform all the necessary subtasks---retrieving supporting facts, reranking them, and predicting the answer from all retrieved documents---in an iterative fashion. To emulate a more realistic setting, we also constructed a new unified benchmark by collecting about 200 multi-hop questions that require three Wikipedia pages to answer, and combining them with existing datasets. We show that our model not only outperforms state-of-the-art systems on several existing benchmarks that exclusively feature single-hop or multi-hop open-domain questions, but also achieves strong performance on the new benchmark.', 'corpus_id': 225062257, 'score': 1}, {'doc_id': '6740934', 'title': 'Win-win search: dual-agent stochastic game in session search', 'abstract': 'Session search is a complex search task that involves multiple search iterations triggered by query reformulations. We observe a Markov chain in session search: user\'s judgment of retrieved documents in the previous search iteration affects user\'s actions in the next iteration. We thus propose to model session search as a dual-agent stochastic game: the user agent and the search engine agent work together to jointly maximize their long term rewards. The framework, which we term ""win-win search"", is based on Partially Observable Markov Decision Process. We mathematically model dynamics in session search, including decision states, query changes, clicks, and rewards, as a cooperative game between the user and the search engine. The experiments on TREC 2012 and 2013 Session datasets show a statistically significant improvement over the state-of-the-art interactive search and session search algorithms.', 'corpus_id': 6740934, 'score': 1}]"
103	{'doc_id': '226511109', 'title': 'Chemometrics with R', 'abstract': None, 'corpus_id': 226511109}	17836	[{'doc_id': '235296704', 'title': 'FragMAXapp: crystallographic fragment-screening data-analysis and project-management system', 'abstract': 'The amount of data generated during crystallographic fragment-screening projects requires sophisticated automated methods to analyse the data and to identify binders. FragMAXapp is the MAX IV Laboratory approach to managing fragment-screening campaigns: a web application that provides scientists with access to the MAX IV computing cluster and visualization tools.', 'corpus_id': 235296704, 'score': 0}, {'doc_id': '213323653', 'title': 'mdatools – R package for chemometrics', 'abstract': 'Abstract The paper describes mdatools – R package, which implements mainly basic but also some advanced chemometric methods providing a unified interface and user experience. The package was created to give a low entry level for beginners, so they can start using the implemented methods without writing much of code. While progressing, though, users can also have direct access to all computed results thus extending the package functionality by writing own code on top.', 'corpus_id': 213323653, 'score': 1}, {'doc_id': '201221359', 'title': 'AlradSpectra: a Quantification Tool for Soil Properties Using Spectroscopic Data in R', 'abstract': 'ABSTRACT Soil reflectance spectroscopy has become an innovative method for soil property quantification supplying data for studies in soil fertility, soil classification, digital soil mapping, while reducing laboratory time and applying a clean technology. This paper describes the implementation of a Graphical User Interface (GUI) using R named AlradSpectra. It contains several tools to process spectroscopic data and generate models to predict soil properties. The GUI was developed to accomplish tasks such as perform a large range of spectral preprocessing [...]', 'corpus_id': 201221359, 'score': 1}, {'doc_id': '234949295', 'title': 'Erratum to: ESPEI for efficient thermodynamic database development, modification, and uncertainty quantification: application to Cu–Mg—CORRIGENDUM', 'abstract': 'In regards to the original publication of this article[1], the authors would like to correct the following: the opening sentence of the caption for Figure 5a should read “Corner plot (a) of the parameters in the liquid phase.”', 'corpus_id': 234949295, 'score': 0}, {'doc_id': '233988668', 'title': 'Analysing spectroscopy data using two-step group penalized partial least squares regression', 'abstract': 'A statistical challenge to analyse hyperspectral data is the multicollinearity between spectral bands. Partial least squares (PLS) has been extensively used as a dimensionality reduction technique through constructing lower dimensional latent variables from the spectral bands that correlate with the response variables. However, it does not take into account the grouping structure of the full spectrum where spectral subsets may exhibit distinct relationships with the response variables. We propose a two-step group penalized PLS regression approach by performing a PLS regression on each group of predictors identified from a clustering approach in the first step. In the second step, a group penalty is imposed on the latent components to select the group with the highest predictive power. Our proposed method demonstrated a superior prediction performance, higher R-squared value and faster computation time over other PLS variations when applied to simulations and a real-world observational data set. Interpretations of the model performance are illustrated using the real-world data example of leaf spectra to indirectly quantify leaf traits. The method is implemented in an R package called “ groupPLS ”, which is accessible from github.com/jialiwang1211/groupPLS.', 'corpus_id': 233988668, 'score': 1}, {'doc_id': '233486524', 'title': 'argoFloats: An R Package for Analyzing Argo Data', 'abstract': 'An R package named argoFloats has been developed to facilitate identifying, downloading, caching, and analyzing oceanographic data collected by Argo profiling floats. The analysis phase benefits from close connections between argoFloats and the oce package, which is likely to be familiar to those who already use R for the analysis of oceanographic data of other kinds. This paper outlines how to use argoFloats to accomplish some everyday tasks that are particular to Argo data, ranging from downloading data and finding subsets to handling quality control and producing a variety of diagnostic plots. The benefits of the R environment are sketched in the examples, and also in some notes on the future of the argoFloats package.', 'corpus_id': 233486524, 'score': 0}, {'doc_id': '234162901', 'title': 'GWpy: A Python package for gravitational-wave astrophysics', 'abstract': 'Abstract GWpy is a Python software package that provides an intuitive, object-oriented interface through which to access, process, and visualise data from gravitational-wave detectors. GWpy provides a number of new utilities for studying data, as well as an improved user interface for a number of existing tools. The ease-of-use, along with extensive online documentation and examples, has resulted in widespread adoption of GWpy as a basis for Python software development in the international gravitational-wave community.', 'corpus_id': 234162901, 'score': 0}, {'doc_id': '234794563', 'title': 'webSalvador: a Web Tool for the Luria-Delbrük Experiment', 'abstract': 'Existing Web tools for the Luria-Delbrück fluctuation experiment do not offer many desirable capabilities that are vital to mutation research. webSalvador offers these capabilities via a user interface that allows researchers to access most of the functions in the R package rSalvador without having to learn the R language. ABSTRACT Existing Web tools for the Luria-Delbrück fluctuation experiment do not offer many desirable capabilities that are vital to mutation research. webSalvador offers these capabilities via a user interface that allows researchers to access most of the functions in the R package rSalvador without having to learn the R language.', 'corpus_id': 234794563, 'score': 0}, {'doc_id': '233397921', 'title': 'M3GPSpectra: A novel approach integrating variable selection/construction and MLR modeling for quantitative spectral analysis.', 'abstract': 'Quantitative analysis of the physical or chemical properties of various materials by using spectral analysis technology combined with chemometrics has become an important method in the field of analytical chemistry. This method aims to build a model relationship (called prediction model) between feature variables acquired by spectral sensors and components to be measured. Feature selection or transformation should be conducted to reduce the interference of irrelevant information on the prediction model because original spectral feature variables contain redundant information and massive noise. Most existing feature selection and transformation methods are single linear or nonlinear operations, which easily lead to the loss of feature information and affect the accuracy of subsequent prediction models. This research proposes a novel spectroscopic technology-oriented, quantitative analysis model construction strategy named M3GPSpectra. This tool uses genetic programming algorithm to select and reconstruct the original feature variables, evaluates the performance of selected and reconstructed variables by using multivariate regression model (MLR), and obtains the best feature combination and the final parameters of MLR through iterative learning. M3GPSpectra integrates feature selection, linear/nonlinear feature transformation, and subsequent model construction into a unified framework and thus easily realizes end-to-end parameter learning to significantly improve the accuracy of the prediction model. When applied to six types of datasets, M3GPSpectra obtains 19 prediction models, which are compared with those obtained by seven linear or non-linear popular methods. Experimental results show that M3GPSpectra obtains the best performance among the eight methods tested. Further investigation verifies that the proposed method is not sensitive to the size of the training samples. Hence, M3GPSpectra is a promising spectral quantitative analytical tool.', 'corpus_id': 233397921, 'score': 1}, {'doc_id': '234825279', 'title': 'A MATLAB toolbox for multivariate regression coupled with variable selection', 'abstract': 'Abstract Multivariate regression is a fundamental supervised chemometric approach that defines the relationship between a set of independent variables and a quantitative response. It enables the subsequent prediction of the response for future samples, thus avoiding its experimental measurement. Regression approaches have been widely applied for data analysis in different scientific fields. In this paper, we describe the regression toolbox for MATLAB, which is a collection of modules for calculating some well-known regression methods: Ordinary Least Squares (OLS), Partial Least Squares (PLS), Principal Component Regression (PCR), Ridge and local regression based on sample similarities, such as Binned Nearest Neighbours (BNN) and k-Nearest Neighbours (kNN) regression methods. Moreover, the toolbox includes modules to couple regression approaches with supervised variable selection based on All Subset models, Forward Selection, Genetic Algorithms and Reshaped Sequential Replacement. The toolbox is freely available at the Milano Chemometrics and QSAR Research Group website and provides a graphical user interface (GUI), which allows the calculation in a user-friendly graphical environment.', 'corpus_id': 234825279, 'score': 1}]
104	{'doc_id': '212492748', 'title': 'Identification of Multiword Expressions: A Comparative Literature Study', 'abstract': 'A multiword expression (MWE) is a lexeme made up of a sequence of two or more lexemes that has properties which are not predictable from the properties of the individual lexemes or their normal mode of combination. MWEs play an inevitable role in the applications of Natural Language Processing and Computational Linguistics. This paper presents a study and analysis of types, structures and key problems related to the MWEs. Also this paper describes a comparative literature study of methodologies and associated measures to recognize MWEs, have been featured. MWEs constitute an enormous problem to unambiguous language processing due to their idiosyncratic nature and diversity of their semantic, lexical, syntactic, pragmatic and/or statistical properties.', 'corpus_id': 212492748}	18996	[{'doc_id': '236166928', 'title': 'Toward a semantic analyzer for Arabic language', 'abstract': 'Various research works have shown the requirements in terms of semantic processing to achieve more accurate Natural Language Processing (NLP) applications such as Information Retrieval (IR), Question Answering (QA), Reasoning, etc. This paper describes our approach of semantic analysis of the Arabic language. We adopt for this purpose a two-step approach; we firstly build an Arabic ontology, leveraging the content of the two linguistic resources Arabic WordNet and VerbNet and we secondly combine some Arabic NLP (ANLP) tools such as Stanford syntactic parser with the built ontology to automatically extract the semantics conveyed by the Arabic text. The extracted semantic is formulated in Conceptual Graph formalism, which is a promising and already proven powerful knowledge representation formalism, in reasoning and in many NLP fields.', 'corpus_id': 236166928, 'score': 0}, {'doc_id': '235266206', 'title': 'An In-depth Study on Internal Structure of Chinese Words', 'abstract': 'Unlike English letters, Chinese characters have rich and specific meanings. Usually, the meaning of a word can be derived from its constituent characters in some way. Several previous works on syntactic parsing propose to annotate shallow word-internal structures for better utilizing character-level information. This work proposes to model the deep internal structures of Chinese words as dependency trees with 11 labels for distinguishing syntactic relationships. First, based on newly compiled annotation guidelines, we manually annotate a word-internal structure treebank (WIST) consisting of over 30K multi-char words from Chinese Penn Treebank. To guarantee quality, each word is independently annotated by two annotators and inconsistencies are handled by a third senior annotator. Second, we present detailed and interesting analysis on WIST to reveal insights on Chinese word formation. Third, we propose word-internal structure parsing as a new task, and conduct benchmark experiments using a competitive dependency parser. Finally, we present two simple ways to encode word-internal structures, leading to promising gains on the sentence-level syntactic parsing task.', 'corpus_id': 235266206, 'score': 0}, {'doc_id': '235366762', 'title': 'Paths to Relation Extraction through Semantic Structures', 'abstract': 'Syntactic and semantic structure directly reflect relations expressed by the text at hand and are thus very useful for the relation extraction (RE) task. Their symbolic nature allows increased interpretability for end-users and developers, which is particularly appealing in RE. Although they have been somewhat overshadowed recently by the use of end-to-end neural network models and contextualized word embeddings, we show that they may be leveraged as input for neural networks to positive effect. We present two methods for integrating broad-coverage semantic structure (specifically, UCCA) into supervised neural RE models, demonstrating benefits over the use of exclusively syntactic integrations. The first method involves reduction of UCCA into a bilexical structure, while the second leverages a novel technique for encoding semantic DAG structures. Our approach is general and can be used for integrating a wide range of graphbased semantic structures.1', 'corpus_id': 235366762, 'score': 1}, {'doc_id': '67856404', 'title': 'Still a Pain in the Neck: Evaluating Text Representations on Lexical Composition', 'abstract': 'Building meaningful phrase representations is challenging because phrase meanings are not simply the sum of their constituent meanings. Lexical composition can shift the meanings of the constituent words and introduce implicit information. We tested a broad range of textual representations for their capacity to address these issues. We found that, as expected, contextualized word representations perform better than static word embeddings, more so on detecting meaning shift than in recovering implicit information, in which their performance is still far from that of humans. Our evaluation suite, consisting of six tasks related to lexical composition effects, can serve future research aiming to improve representations.', 'corpus_id': 67856404, 'score': 1}, {'doc_id': '235619223', 'title': 'A Study of Word Sense Disambiguation in Malayalam', 'abstract': 'The task of determining the correct sense of a word within the context by the help of computer is known as word sense disambiguation (WSD). Word Sense Disambiguation is basically solution to the ambiguity which arises due to different meaning of words in different context .It includes different approaches towards word sense disambiguation and help to detect the correct meaning of the ambiguous words. The word sense disambiguation provides multilingual feature representation.WSD has been a trending area of research in natural language processing and machine learning. This paper studies the word disambiguation of the Malayalam sentences.', 'corpus_id': 235619223, 'score': 0}, {'doc_id': '235258289', 'title': 'Chunking Historical German', 'abstract': 'Quantitative studies of historical syntax require large amounts of syntactically annotated data, which are rarely available. The application of NLP methods could reduce manual annotation effort, provided that they achieve sufficient levels of accuracy. The present study investigates the automatic identification of chunks in historical German texts. Because no training data exists for this task, chunks are extracted from modern and historical constituency treebanks and used to train a CRF-based neural sequence labeling tool. The evaluation shows that the neural chunker outperforms an unlexicalized baseline and achieves overall F-scores between 90% and 94% for different historical data sets when POS tags are used as feature. The conducted experiments demonstrate the usefulness of including historical training data while also highlighting the importance of reducing boundary errors to improve annotation precision.', 'corpus_id': 235258289, 'score': 0}, {'doc_id': '235374436', 'title': 'A Multilayered Urdu Treebank', 'abstract': 'The paper presents the design and construction of a multilayered phrase structure treebank. The treebank consists of three layers for phrases, grammatical functions and semantic roles. A small phrase tagset (consisting of 12 tags) is used as the primary label of the phrase. Phrase label is followed by grammatical function (mainly inspired by lexical functional grammar). It is followed by the semantic role label using propbank roles. 1,300 sentences from CLE Urdu Digest Corpus are annotated using the treebank guideline1.', 'corpus_id': 235374436, 'score': 0}, {'doc_id': '235765693', 'title': 'COMBO: A New Module for EUD Parsing', 'abstract': 'We introduce the COMBO-based approach for EUD parsing and its implementation, which took part in the IWPT 2021 EUD shared task. The goal of this task is to parse raw texts in 17 languages into Enhanced Universal Dependencies (EUD). The proposed approach uses COMBO to predict UD trees and EUD graphs. These structures are then merged into the final EUD graphs. Some EUD edge labels are extended with case information using a single language-independent expansion rule. In the official evaluation, the solution ranked fourth, achieving an average ELAS of 83.79%. The source code is available at https://gitlab.clarin-pl.eu/syntactic-tools/combo.', 'corpus_id': 235765693, 'score': 1}, {'doc_id': '236087324', 'title': 'Argument Linking: A Survey and Forecast', 'abstract': 'Semantic role labeling (SRL)—identifying the semantic relationships between a predicate and other constituents in the same sentence—is a well-studied task in natural language understanding (NLU). However, many of these relationships are evident only at the level of the document, as a role for a predicate in one sentence may often be filled by an argument in a different one. This more general task, known as implicit semantic role labeling or argument linking, has received increased attention in recent years, as researchers have recognized its centrality to information extraction and NLU. This paper surveys the literature on argument linking and identifies several notable shortcomings of existing approaches that indicate the paths along which future research effort could most profitably be spent.', 'corpus_id': 236087324, 'score': 1}, {'doc_id': '236088184', 'title': 'Architectures of Meaning, A Systematic Corpus Analysis of NLP Systems', 'abstract': 'This paper proposes a novel statistical corpus analysis framework targeted towards the interpretation of Natural Language Processing (NLP) architectural patterns at scale. The proposed approach combines saturation-based lexicon construction, statistical corpus analysis methods and graph collocations to induce a synthesis representation of NLP architectural patterns from corpora. The framework is validated in the full corpus of Semeval tasks and demonstrated coherent architectural patterns which can be used to answer architectural questions on a data-driven fashion, providing a systematic mechanism to interpret a largely dynamic and exponentially growing field.', 'corpus_id': 236088184, 'score': 1}]
105	{'doc_id': '232765377', 'title': 'Segmentation of the thoracic aorta using an attention-gated U-Net', 'abstract': 'Accurate segmentation of the aorta in computed tomography angiography (CTA) images is the first step for analysis of diseases such as aortic aneurysm, but manual segmentation can be prohibitively time-consuming and error prone. Convolutional neural network (CNN) based models have been utilized for automated segmentation of anatomy in CTA scans, with the ubiquitous U-Net being one of the most popular architectures. For many downstream image analysis tasks (e.g., registration, diameter measurement) very accurate segmentation accuracy may be required. In this work, we developed and tested a U-Net model with attention gating for segmentation of the thoracic aorta in clinical CTA data of patients with thoracic aortic aneurysm. Attention gating helps the model focus on difficult to segment target structures automatically and has been previously shown to increase segmentation accuracy in other applications. We trained U-Nets both with and without attention gating on 145 CTAs. Performance of the models were evaluated by calculating the DCS and Average Hausdorff Distance (AHD) on a test set of 20 CTAs. We found that the U-Net with attention gating yields more accurate segmentation than the U-Net without attention gating (DCS 0.966±0.028 vs. 0.944±0.022, AHD 0.189±0.134mm vs. 0.247±0.155mm). Furthermore, we explored the segmentation accuracy of this U-Net for multi-class labeling of various anatomic segments of the thoracic aorta, and found an average DCS of 0.86 for across 7 different labels. We conclude that the U-Net with attention gating improves segmentation performance and may aid segmentation tasks that require high levels of accuracy.', 'corpus_id': 232765377}	15213	"[{'doc_id': '233263564', 'title': 'Deep learning-based techniques for the automatic segmentation of organs in thoracic computed tomography images: A Comparative study', 'abstract': 'Medical images have become the important part in medical diagnosis and treatment. These images play a significant role in medical field because the doctors are highly interested in exploring the anatomy of the human body. The medical images captured with various modalities like (PET, CT, SPECT, MRI, etc.) have different variability based on the intensity level. The segmentation of organs in medical images is the most crucial image-related application. Organs segmentation in the medical images help the doctors in planning the treatment in lesser time and with higher efficiency. Results of manual segmentation vary from experts to experts and it is very time taking task. Automatic segmentation is the solution to the problem as it gives precise results. Several techniques had been addressed in the literature for the segmentation of thoracic organs (heart, aorta, trachea, esophagus) automatically in the medical images. Out of those deep learning-based techniques outperformed in the automatic segmentation of organs by giving precise accuracy. Using deep learning models for segmentation purposes improve the segmentation results in various clinical applications. In this paper various deep learning-based techniques for automatic segmentation had been discussed. Also, the three authors’ results are compared based on the parameters such as Dice Coefficient (DC) and Hausdorff Metric (HM).', 'corpus_id': 233263564, 'score': 0}, {'doc_id': '232765289', 'title': 'Automatic pancreas segmentation in abdominal CT image with contrast enhancement block', 'abstract': 'Automatic pancreas segmentation in abdominal CT images plays an important role in clinical applications. It can provide doctors with quantitative and qualitative information. Due to the small size, unclear edges, and the high anatomical differences between patients, it is a challenging task to accurately segment the pancreas with diseases. In this paper, we propose a new method to automatically segment the pancreas in abdominal CT images. First, we propose a contrast enhancement block. The block generates edge information and uses gating mechanism to enhance edge details of the pancreas. Second, we leverage a reverse attention block. This block utilizes the decoder feature map to guide the network to mine complementary discriminative regions. The proposed method is trained on 63 3D CT images, validated on 15 3D CT images, and tested on 28 3D CT images. Compared with manual segmentation, the mean Dice similarity coefficient can reach 86.11±8.02%. Experimental results show that our method can obtain more accurate segmentation results compared with existing segmentation methods.', 'corpus_id': 232765289, 'score': 0}, {'doc_id': '231971077', 'title': 'A computationally efficient approach to segmentation of the aorta and coronary arteries using deep learning', 'abstract': 'A fully automatic two-dimensional Unet model is proposed to segment aorta and coronary arteries in computed tomography images. Two models are trained to segment two regions of interest, (1) the aorta and the coronary arteries or (2) the coronary arteries alone. Our method achieves 91.20% and 88.80% dice similarity coefficient accuracy on regions of interest 1 and 2 respectively. Compared with a semi-automatic segmentation method, our model performs better when segmenting the coronary arteries alone. The performance of the proposed method is comparable to existing published two-dimensional or three-dimensional deep learning models. Furthermore, the algorithmic and graphical processing unit memory efficiencies are maintained such that the model can be deployed within hospital computer networks where graphical processing units are typically not available.', 'corpus_id': 231971077, 'score': 0}, {'doc_id': '232765250', 'title': 'Modifying U-Net for small dataset: a simplified U-Net version for liver parenchyma segmentation', 'abstract': 'Automation in the field of medical image segmentation is critical in helping the oncologists and surgeons for the accurate analysis of several pathological conditions by saving time. The ability to automatically segment the liver fast and accurately enables clinicians to understand the anatomical structure of the organ, and helps in the decision making process of diagnosis, surgery planning and as an anatomical map during surgical navigation especially important when using intraoperative image modalities . This work aims to develop an automatic liver parenchyma segmentation network which is based on U-Net architecture, a widely used architecture for medical image segmentation. This modified U-Net architecture includes reduced convolutional layers and using dropout layers as well as pre-processing the dataset to overcome the constraints of a small sample set. Reduced architecture complexity and introducing dropout regularization, addresses the problem of overfitting. We experimented with a callback for observing the training failure where it follows the early stopping policy and selecting the best model. Adding Gaussian noise to data can help the model to generalise well. For choosing the appropriate loss function we tested four different loss functions; Dice, binary cross entropy, Tversky and focal Tversky and concluded that Dice performs better. The network has been trained and validated using publicly available 3D-IRCADb dataset with images from 20 patients and achieved an overall Dice score of 94.5%. The overall objective of this work is to construct a network from a small sample set without the problem of overfitting or under-fitting, but delivering an acceptable Dice score.', 'corpus_id': 232765250, 'score': 0}, {'doc_id': '212799954', 'title': 'Machine deep learning accurately detects endoleak after endovascular abdominal aortic aneurysm repair', 'abstract': ""Objective The objective of this study was to develop a machine deep learning algorithm for endoleak detection and measurement of aneurysm diameter, area, and volume from computed tomography angiography (CTA). Methods Digital Imaging and Communications in Medicine files representing three-phase postoperative CTA images (N = 334) of 191 unique patients undergoing endovascular aneurysm repair for infrarenal abdominal aortic aneurysm (AAA) with a variety of commercial devices were used to train a deep learning pipeline across four tasks. The RetinaNet object-detection convolutional neural network (CNN) architecture was trained to predict bounding boxes around the axial CTA slices that were then stitched together in two dimensions into a smaller region containing the aneurysm. Multiclass endoleak detection and segmentation of the AAA, endograft, and endoleak were performed on this smaller region. Segmentations on a single randomly selected contrast from each scan included 33 full and 68 partial segmentations for endograft and AAA and 99 full segmentations for endoleak. A modified version of ResNet-50 CNN was used to detect endoleak on individual axial slices. A three-dimensional U-Net CNN model was trained on the task of dense three-dimensional segmentation and used to measure diameter and volume with a specially designed loss function. We made use of fivefold cross-validation to evaluate model performance for each step, splitting training and testing data at each fold, such that multiple scans from the same patient were preserved with the same fold. Algorithm predictions for endoleak were compared with the radiology report and with a subset of CTA images independently read by two vascular specialists. Results The localization portion of the network accurately predicted a region of interest containing the AAA in 99% of cases. The best model of binary endoleak detection obtained an area under the receiver operating characteristic curve of 0.94 ± 0.03 with an optimized accuracy of 0.89 ± 0.03 on a balanced data set. An introduced postprocessing algorithm for determining maximum diameter was used on both the predicted AAA segmentation and ground truth segmentation, predicting on average an absolute diameter error of 2.3 ± 2.0 mm by 1.4 ± 1.7 mm for each measurement, respectively. The algorithm measured AAA and endograft volume accurately (Dice coefficient, 0.95 ± 0.2) with an absolute volume error of 10.1 ± 9.1 mL. The algorithm measured endoleak volume less accurately, with a Dice score of 0.53 ± 0.21 and an average absolute volume error of 1.2 ± 1.9 mL. Conclusions This machine learning algorithm shows promise in augmenting a human's ability to interpret postoperative CTA images and may help improve surveillance after endovascular aneurysm repair. External validation on larger data sets and prospective study are required before the algorithm can be clinically applicable."", 'corpus_id': 212799954, 'score': 1}, {'doc_id': '232320782', 'title': 'Multiview and multiclass image segmentation using deep learning in fetal echocardiography', 'abstract': 'Congenital heart disease (CHD) is the most common congenital abnormality associated with birth defects in the United States. Despite training efforts and substantial advancement in ultrasound technology over the past years, CHD remains an abnormality that is frequently missed during prenatal ultrasonography. Therefore, computeraided detection of CHD can play a critical role in prenatal care by improving screening and diagnosis. Since many CHDs involve structural abnormalities, automatic segmentation of anatomical structures is an important step in the analysis of fetal echocardiograms. While existing methods mainly focus on the four-chamber view with a small number of structures, here we present a more comprehensive deep learning segmentation framework covering 14 anatomical structures in both three-vessel trachea and four-chamber views. Specifically, our framework enhances the V-Net with spatial dropout, group normalization, and deep supervision to train a segmentation model that can be applied on both views regardless of abnormalities. By identifying the pitfall of using the Dice loss when some labels are unavailable in some images, this framework integrates information from multiple views and is robust to missing structures due to anatomical anomalies, achieving an average Dice score of 79%.', 'corpus_id': 232320782, 'score': 0}, {'doc_id': '232104682', 'title': 'Deep Learning-based Automated Aortic Area and Distensibility Assessment: The Multi-Ethnic Study of Atherosclerosis (MESA)', 'abstract': 'Background: This study applies convolutional neural network (CNN)-based automatic segmentation and distensibility measurement of the ascending and descending aorta from 2D phase-contrast cine magnetic resonance imaging (PC-cine MRI) within the large MESA cohort with subsequent assessment on an external cohort of thoracic aortic aneurysm (TAA) patients. Methods: 2D PC-cine MRI images and corresponding analysis of the ascending and descending aorta at the pulmonary artery bifurcation from the MESA study were included. Train, validation, and internal test sets consisted of 1123 studies (24282 images), 374 studies (8067 images), and 375 studies (8069 images), respectively. An external test set of TAAs was included, which consisted of 37 studies (3224 images). A U-Net based CNN was constructed, and performance was evaluated utilizing dice coefficient (for segmentation) and concordance correlation coefficients (CCC) of aortic geometric parameters by comparing to manual segmentation and parameter estimation. Results: Dice coefficients for aorta segmentation were 97.6% (CI: 97.5%-97.6%) and 93.6% (84.6%-96.7%) on the internal and external test of TAAs, respectively. CCC for comparison of manual and CNN based maximum and minimum and ascending aortic areas were 0.97 and 0.95, respectively, on the internal test set and 0.997 and 0.995, respectively, for the external test. CCCs for maximum and minimum descending aortic areas were 0.96 and 0. 98, respectively, on the internal test set and 0.93 and 0.93, respectively, on the external test set. The absolute differences between manual and CNN-based ascending and descending aortic distensibility measures were 0.02±9.7 and 20±10 10-4mmHg-1, respectively, on the internal test set, and 0.44±20 and 20±10 10-4mmHg-1, respectively, on the external test set. Conclusion: We successfully developed and validated a U-Net based ascending and descending aortic segmentation and distensibility quantification model in a large multi-ethnic database and in an external cohort of TAA patients.', 'corpus_id': 232104682, 'score': 1}, {'doc_id': '234476200', 'title': 'Aortic wall segmentation in 18F-sodium fluoride PET/CT scans: Head-to-head comparison of artificial intelligence-based versus manual segmentation', 'abstract': 'We aimed to establish and test an automated AI-based method for rapid segmentation of the aortic wall in positron emission tomography/computed tomography (PET/CT) scans. For segmentation of the wall in three sections: the arch, thoracic, and abdominal aorta, we developed a tool based on a convolutional neural network (CNN), available on the Research Consortium for Medical Image Analysis (RECOMIA) platform, capable of segmenting 100 different labels in CT images. It was tested on 18F-sodium fluoride PET/CT scans of 49 subjects (29 healthy controls and 20 angina pectoris patients) and compared to data obtained by manual segmentation. The following derived parameters were compared using Bland–Altman Limits of Agreement: segmented volume, and maximal, mean, and total standardized uptake values (SUVmax, SUVmean, SUVtotal). The repeatability of the manual method was examined in 25 randomly selected scans. CNN-derived values for volume, SUVmax, and SUVtotal were all slightly, i.e., 13-17%, lower than the corresponding manually obtained ones, whereas SUVmean values for the three aortic sections were virtually identical for the two methods. Manual segmentation lasted typically 1-2 hours per scan compared to about one minute with the CNN-based approach. The maximal deviation at repeat manual segmentation was 6%. The automated CNN-based approach was much faster and provided parameters that were about 15% lower than the manually obtained values, except for SUVmean values, which were comparable. AI-based segmentation of the aorta already now appears as a trustworthy and fast alternative to slow and cumbersome manual segmentation.', 'corpus_id': 234476200, 'score': 1}, {'doc_id': '232765374', 'title': 'Improved segmentation by adversarial U-Net', 'abstract': 'Medical image segmentation has a fundamental role in many computer-aided diagnosis (CAD) applications. Accurate segmentation of medical images is a key step in tracking changes over time, contouring during radiotherapy planning, and more. One of the state-of-the-art models for medical image segmentation is the U–Net that consists of an encoder-decoder based architecture. Many variations exist to the U–Net architecture. In this work, we present a new training procedure that combines U–Net with an adversarial training we refer to as Adversarial U–Net. We show that Adversarial U–Net outperformes the conventional U–Net in three versatile domains that differ in the acquisition method as well as the physical characteristics and yields smooth and improved segmentation maps.', 'corpus_id': 232765374, 'score': 0}, {'doc_id': '232765420', 'title': 'Automated localization and segmentation of vertebrae in the micro-CT images of rabbit fetuses using 3D Convolutional Neural Networks', 'abstract': 'In the pharmaceutical industry, micro-CT images of Dutch-Belted rabbit fetuses have been used for the assessment of compound-induced skeletal abnormalities in developmental and reproductive toxicology (DART) studies. In the automated approach proposed to assess the morphology of each bone, localization and segmentation of each vertebral bone is a critical task. In this work, we are extending our previous work for the localization of cervical vertebrae to the entire spine following a multivariate regression framework based on a 3D convolutional neural network (CNN). We also introduce a multitasking 3D CNN for the segmentation of each vertebral bone, in which features at the most compact level are processed with two additional convolution layers with max pooling to generate features leading to a classification of whether the patch contains a complete vertebra or not. This multi-tasking mechanism allows us to ensure only complete pieces of vertebrae are segmented. Experimenting on 345 rabbit fetuses with 80/10/10 ratio for training/validation/testing, we were able to achieve successful localization on 94.3% of the cases (i.e. median bone-by-bone localization error under 5 voxels over the entire spine) and an average Dice similarity coefficient (DSC) of 0.80 between automated and ground truth segmentations on the testing set.', 'corpus_id': 232765420, 'score': 0}]"
106	"{'doc_id': '4689304', 'title': 'Tacotron: Towards End-to-End Speech Synthesis', 'abstract': ""A text-to-speech synthesis system typically consists of multiple stages, such as a text analysis frontend, an acoustic model and an audio synthesis module. Building these components often requires extensive domain expertise and may contain brittle design choices. In this paper, we present Tacotron, an end-to-end generative text-to-speech model that synthesizes speech directly from characters. Given pairs, the model can be trained completely from scratch with random initialization. We present several key techniques to make the sequence-to-sequence framework perform well for this challenging task. Tacotron achieves a 3.82 subjective 5-scale mean opinion score on US English, outperforming a production parametric system in terms of naturalness. In addition, since Tacotron generates speech at the frame level, it's substantially faster than sample-level autoregressive methods."", 'corpus_id': 4689304}"	3624	"[{'doc_id': '219401642', 'title': 'End-to-End Adversarial Text-to-Speech', 'abstract': 'Modern text-to-speech synthesis pipelines typically involve multiple processing stages, each of which is designed or learnt independently from the rest. In this work, we take on the challenging task of learning to synthesise speech from normalised text or phonemes in an end-to-end manner, resulting in models which operate directly on character or phoneme input sequences and produce raw speech audio outputs. Our proposed generator is feed-forward and thus efficient for both training and inference, using a differentiable alignment scheme based on token length prediction. It learns to produce high fidelity audio through a combination of adversarial feedback and prediction losses constraining the generated audio to roughly match the ground truth in terms of its total duration and mel-spectrogram. To allow the model to capture temporal variation in the generated audio, we employ soft dynamic time warping in the spectrogram-based prediction loss. The resulting model achieves a mean opinion score exceeding 4 on a 5 point scale, which is comparable to the state-of-the-art models relying on multi-stage training and additional supervision.', 'corpus_id': 219401642, 'score': 1}, {'doc_id': '6254678', 'title': 'WaveNet: A Generative Model for Raw Audio', 'abstract': 'This paper introduces WaveNet, a deep neural network for generating raw audio waveforms. The model is fully probabilistic and autoregressive, with the predictive distribution for each audio sample conditioned on all previous ones; nonetheless we show that it can be efficiently trained on data with tens of thousands of samples per second of audio. When applied to text-to-speech, it yields state-of-the-art performance, with human listeners rating it as significantly more natural sounding than the best parametric and concatenative systems for both English and Mandarin. A single WaveNet can capture the characteristics of many different speakers with equal fidelity, and can switch between them by conditioning on the speaker identity. When trained to model music, we find that it generates novel and often highly realistic musical fragments. We also show that it can be employed as a discriminative model, returning promising results for phoneme recognition.', 'corpus_id': 6254678, 'score': 1}, {'doc_id': '235208684', 'title': 'Speech Synthesis Method Based on Tacotron2', 'abstract': 'Compared with traditional speech synthesis systems, end-to-end speech synthesis systems based on deep learning (such as DeepVoice3, Tacotron2) not only reduce the requirements for linguistic knowledge, but the synthesis effect is almost close to the level of human pronunciation. However, the end-to-end speech synthesis system based on deep learning has disadvantages such as missing words, repeated pronunciation, and slow synthesis speed. In view of the local information preference of the Tacotron2 model in the decoder, this paper proposes to maximize the interactive information between the text and the predicted acoustic features and use the WaveGlow synthesizer to reduce the local information preference and the problem of slow synthesis speed, pronunciation in the Tacotron2 model. Experimental results show that the improved model subjective evaluation MOS (Mean Opinion Score) score is 3.94, and the synthesis speed is significantly improved.', 'corpus_id': 235208684, 'score': 0}, {'doc_id': '215416282', 'title': 'Noise Tokens: Learning Neural Noise Templates for Environment-Aware Speech Enhancement', 'abstract': 'In recent years, speech enhancement (SE) has achieved impressive progress with the success of deep neural networks (DNNs). However, the DNN approach usually fails to generalize well to unseen environmental noise that is not included in the training. To address this problem, we propose ""noise tokens"" (NTs), which are a set of neural noise templates that are jointly trained with the SE system. NTs dynamically capture the environment variability and thus enable the DNN model to handle various environments to produce STFT magnitude with higher quality. Experimental results show that using NTs is an effective strategy that consistently improves the generalization ability of SE systems across different DNN architectures. Furthermore, we investigate applying a state-of-the-art neural vocoder to generate waveform instead of traditional inverse STFT (ISTFT). Subjective listening tests show the residual noise can be significantly suppressed through mel-spectrogram correction and vocoder-based waveform synthesis.', 'corpus_id': 215416282, 'score': 0}, {'doc_id': '210702343', 'title': 'SqueezeWave: Extremely Lightweight Vocoders for On-device Speech Synthesis', 'abstract': 'Automatic speech synthesis is a challenging task that is becoming increasingly important as edge devices begin to interact with users through speech. Typical text-to-speech pipelines include a vocoder, which translates intermediate audio representations into an audio waveform. Most existing vocoders are difficult to parallelize since each generated sample is conditioned on previous samples. WaveGlow is a flow-based feed-forward alternative to these auto-regressive models (Prenger et al., 2019). However, while WaveGlow can be easily parallelized, the model is too expensive for real-time speech synthesis on the edge. This paper presents SqueezeWave, a family of lightweight vocoders based on WaveGlow that can generate audio of similar quality to WaveGlow with 61x - 214x fewer MACs. Code, trained models, and generated audio are publicly available at this https URL.', 'corpus_id': 210702343, 'score': 1}, {'doc_id': '211572600', 'title': 'Comparison of Speech Representations for Automatic Quality Estimation in Multi-Speaker Text-to-Speech Synthesis', 'abstract': 'We aim to characterize how different speakers contribute to the perceived output quality of multi-speaker Text-to-Speech (TTS) synthesis. We automatically rate the quality of TTS using a neural network (NN) trained on human mean opinion score (MOS) ratings. First, we train and evaluate our NN model on 13 different TTS and voice conversion (VC) systems from the ASVSpoof 2019 Logical Access (LA) Dataset. Since it is not known how best to represent speech for this task, we compare 8 different representations alongside MOSNet frame-based features. Our representations include image-based spectrogram features and x-vector embeddings that explicitly model different types of noise such as T60 reverberation time. Our NN predicts MOS with a high correlation to human judgments. We report prediction correlation and error. A key finding is the quality achieved for certain speakers seems consistent, regardless of the TTS or VC system. It is widely accepted that some speakers give higher quality than others for building a TTS system: our method provides an automatic way to identify such speakers. Finally, to see if our quality prediction models generalize, we predict quality scores for synthetic speech using a separate multi-speaker TTS system that was trained on LibriTTS data, and conduct our own MOS listening test to compare human ratings with our NN predictions.', 'corpus_id': 211572600, 'score': 1}, {'doc_id': '17347401', 'title': 'RDF-based schema mediation for database grid', 'abstract': 'In presence of grid where a huge amount of databases can be involved in sharing cycle, database tools and middlewares should be well suited for schema mediation and query processing in a semantically meaningful way. Dart is an implemented prototype system whose goal is to provide a semantic solution for database resource sharing capable of deployment in grid settings. This paper particularly concerns the problems of schema mediation in DartGrid. Our approach mainly involves the following notions: a) we use RDF/OWL to define the mediated ontologies for integration, b) we devise a set of rules for automatically converting the relational schema to RDF/OWL description called source data semantic, c) we define the source data semantic (source schema) as the view of shared ontologies (mediated schema), d) query is formulated and posed on the shared ontologies. A set of grid services is developed for the implementation of above functionalities.', 'corpus_id': 17347401, 'score': 0}, {'doc_id': '125537082', 'title': 'Limiting behavior of weighted central paths in linear programming', 'abstract': ""We study the limiting behavior of the weighted central paths { (x(t-t), s(t-t))} ~-'>o in linear pro\xad gramming at both t-t = 0 and t-t = oo. We establish the existence of a partition (Boo, N 00 ) of the index set { 1, ... , n} such thatx;(t-t) ~ oo andsj(t-t) ~ oo as t-t~oo for iEBoo andjENoo, andxNoo(t.t), s8 oo(tL) converge to weighted analytic centers of certain polytopes. For all k~ 1, we show that the kth order derivatives x<kl (t-t) and s<kl (t-t) converge when t-t~ 0 and t-t~ oo. Consequently, the derivatives of each order are bounded in the interval (0, co). We calculate the limiting derivatives explicitly, and establish the surprising result that all higher order derivatives (k~ 2) converge to zero when t-t~oo."", 'corpus_id': 125537082, 'score': 0}, {'doc_id': '218595994', 'title': 'FeatherWave: An efficient high-fidelity neural vocoder with multi-band linear prediction', 'abstract': 'In this paper, we propose the FeatherWave, yet another variant of WaveRNN vocoder combining the multi-band signal processing and the linear predictive coding. The LPCNet, a recently proposed neural vocoder which utilized the linear predictive characteristic of speech signal in the WaveRNN architecture, can generate high quality speech with a speed faster than real-time on a single CPU core. However, LPCNet is still not efficient enough for online speech generation tasks. To address this issue, we adopt the multi-band linear predictive coding for WaveRNN vocoder. The multi-band method enables the model to generate several speech samples in parallel at one step. Therefore, it can significantly improve the efficiency of speech synthesis. The proposed model with 4 sub-bands needs less than 1.6 GFLOPS for speech generation. In our experiments, it can generate 24 kHz high-fidelity audio 9x faster than real-time on a single CPU, which is much faster than the LPCNet vocoder. Furthermore, our subjective listening test shows that the FeatherWave can generate speech with better quality than LPCNet.', 'corpus_id': 218595994, 'score': 1}, {'doc_id': '215785864', 'title': 'F0-Consistent Many-To-Many Non-Parallel Voice Conversion Via Conditional Autoencoder', 'abstract': 'Non-parallel many-to-many voice conversion remains an interesting but challenging speech processing task. Many style-transfer-inspired methods such as generative adversarial networks (GANs) and variational autoencoders (VAEs) have been proposed. Recently, AU-TOVC, a conditional autoencoders (CAEs) based method achieved state-of-the-art results by disentangling the speaker identity and speech content using information-constraining bottlenecks, and it achieves zero-shot conversion by swapping in a different speaker’s identity embedding to synthesize a new voice. However, we found that while speaker identity is disentangled from speech content, a significant amount of prosodic information, such as source F0, leaks through the bottleneck, causing target F0 to fluctuate unnaturally. Furthermore, AutoVC has no control of the converted F0 and thus unsuitable for many applications. In the paper, we modified and improved autoencoder-based voice conversion to disentangle content, F0, and speaker identity at the same time. Therefore, we can control the F0 contour, generate speech with F0 consistent with the target speaker, and significantly improve quality and similarity. We support our improvement through quantitative and qualitative analysis.', 'corpus_id': 215785864, 'score': 0}]"
107	{'doc_id': '182225505', 'title': 'Multi-Round Transfer Learning for Low-Resource NMT Using Multiple High-Resource Languages', 'abstract': 'Neural machine translation (NMT) has made remarkable progress in recent years, but the performance of NMT suffers from a data sparsity problem since large-scale parallel corpora are only readily available for high-resource languages (HRLs). In recent days, transfer learning (TL) has been used widely in low-resource languages (LRLs) machine translation, while TL is becoming one of the vital directions for addressing the data sparsity problem in low-resource NMT. As a solution, a transfer learning method in NMT is generally obtained via initializing the low-resource model (child) with the high-resource model (parent). However, leveraging the original TL to low-resource models is neither able to make full use of highly related multiple HRLs nor to receive different parameters from the same parents. In order to exploit multiple HRLs effectively, we present a language-independent and straightforward multi-round transfer learning (MRTL) approach to low-resource NMT. Besides, with the intention of reducing the differences between high-resource and low-resource languages at the character level, we introduce a unified transliteration method for various language families, which are both semantically and syntactically highly analogous with each other. Experiments on low-resource datasets show that our approaches are effective, significantly outperform the state-of-the-art methods, and yield improvements of up to 5.63 BLEU points.', 'corpus_id': 182225505}	15237	[{'doc_id': '216562574', 'title': 'Extending Multilingual BERT to Low-Resource Languages', 'abstract': 'Multilingual BERT (M-BERT) has been a huge success in both supervised and zero-shot cross-lingual transfer learning. However, this success is focused only on the top 104 languages in Wikipedia it was trained on. In this paper, we propose a simple but effective approach to extend M-BERT E-MBERT so it can benefit any new language, and show that our approach aids languages that are already in M-BERT as well. We perform an extensive set of experiments with Named Entity Recognition (NER) on 27 languages, only 16 of which are in M-BERT, and show an average increase of about 6% F1 on M-BERT languages and 23% F1 increase on new languages. We release models and code at http://cogcomp.org/page/publication_view/912.', 'corpus_id': 216562574, 'score': 1}, {'doc_id': '233004302', 'title': 'Canonical and Surface Morphological Segmentation for Nguni Languages', 'abstract': 'Morphological Segmentation involves decomposing words into morphemes, the smallest meaning-bearing units of language. This is an important NLP task for morphologically-rich agglutinative languages such as the Southern African Nguni language group. In this paper, we investigate supervised and unsupervised models for two variants of morphological segmentation: canonical and surface segmentation. We train sequence-to-sequence models for canonical segmentation, where the underlying morphemes may not be equal to the surface form of the word, and Conditional Random Fields (CRF) for surface segmentation. Transformers outperform LSTMs with attention on canonical segmentation, obtaining an average F1 score of 72.5% across 4 languages. Feature-based CRFs outperform bidirectional LSTM-CRFs to obtain an average of 97.1% F1 on surface segmentation. In the unsupervised setting, an entropy-based approach using a character-level LSTM language model fails to outperforms a Morfessor baseline, while on some of the languages neither approach perform much better than a random baseline. We hope that the high performance of the supervised segmentation models will help to facilitate the development of better NLP tools for Nguni languages.', 'corpus_id': 233004302, 'score': 0}, {'doc_id': '232404736', 'title': 'PENELOPIE: Enabling Open Information Extraction for the Greek Language through Machine Translation', 'abstract': 'In this work, we present a methodology that aims at bridging the gap between high and low-resource languages in the context of Open Information Extraction, showcasing it on the Greek language. The goals of this paper are twofold: First, we build Neural Machine Translation (NMT) models for English-to-Greek and Greek-to-English based on the Transformer architecture. Second, we leverage these NMT models to produce English translations of Greek text as input for our NLP pipeline, to which we apply a series of pre-processing and triple extraction tasks. Finally, we back-translate the extracted triples to Greek. We conduct an evaluation of both our NMT and OIE methods on benchmark datasets and demonstrate that our approach outperforms the current state-of-the-art for the Greek natural language.', 'corpus_id': 232404736, 'score': 0}, {'doc_id': '41480412', 'title': 'Context Models for OOV Word Translation in Low-Resource Languages', 'abstract': 'Out-of-vocabulary word translation is a major problem for the translation of low-resource languages that suffer from a lack of parallel training data. This paper evaluates the contributions of target-language context models towards the translation of OOV words, specifically in those cases where OOV translations are derived from external knowledge sources, such as dictionaries. We develop both neural and non-neural context models and evaluate them within both phrase-based and self-attention based neural machine translation systems. Our results show that neural language models that integrate additional context beyond the current sentence are the most effective in disambiguating possible OOV word translations. We present an efficient second-pass lattice-rescoring method for wide-context neural language models and demonstrate performance improvements over state-of-the-art self-attention based neural MT systems in five out of six low-resource language pairs.', 'corpus_id': 41480412, 'score': 1}, {'doc_id': '53145837', 'title': 'Addressing word-order Divergence in Multilingual Neural Machine Translation for extremely Low Resource Languages', 'abstract': 'Transfer learning approaches for Neural Machine Translation (NMT) train a NMT model on an assisting language-target language pair (parent model) which is later fine-tuned for the source language-target language pair of interest (child model), with the target language being the same. In many cases, the assisting language has a different word order from the source language. We show that divergent word order adversely limits the benefits from transfer learning when little to no parallel corpus between the source and target language is available. To bridge this divergence, we propose to pre-order the assisting language sentences to match the word order of the source language and train the parent model. Our experiments on many language pairs show that bridging the word order gap leads to significant improvement in the translation quality in extremely low-resource scenarios.', 'corpus_id': 53145837, 'score': 1}, {'doc_id': '195767029', 'title': 'Evaluating Language Model Finetuning Techniques for Low-resource Languages', 'abstract': 'Unlike mainstream languages (such as English and French), low-resource languages often suffer from a lack of expert-annotated corpora and benchmark resources that make it hard to apply state-of-the-art techniques directly. In this paper, we alleviate this scarcity problem for the low-resourced Filipino language in two ways. First, we introduce a new benchmark language modeling dataset in Filipino which we call WikiText-TL-39. Second, we show that language model finetuning techniques such as BERT and ULMFiT can be used to consistently train robust classifiers in low-resource settings, experiencing at most a 0.0782 increase in validation error when the number of training examples is decreased from 10K to 1K while finetuning using a privately-held sentiment dataset.', 'corpus_id': 195767029, 'score': 1}, {'doc_id': '232290458', 'title': 'Acoustic Word Embeddings for Zero-Resource Languages Using Self-Supervised Contrastive Learning and Multilingual Adaptation', 'abstract': 'Acoustic word embeddings (AWEs) are fixed-dimensional representations of variable-length speech segments. For zero-resource languages where labelled data is not available, one AWE approach is to use unsupervised autoencoder-based re-current models. Another recent approach is to use multilingual transfer: a supervised AWE model is trained on several well-resourced languages and then applied to an unseen zero-resource language. We consider how a recent contrastive learning loss can be used in both the purely unsupervised and multilingual transfer settings. Firstly, we show that terms from an unsupervised term discovery system can be used for contrastive self-supervision, resulting in improvements over previous unsupervised monolingual AWE models. Secondly, we consider how multilingual AWE models can be adapted to a specific zero-resource language using discovered terms. We find that self-supervised contrastive adaptation outperforms adapted multilingual correspondence autoencoder and Siamese AWE models, giving the best overall results in a word discrimination task on six zero-resource languages.', 'corpus_id': 232290458, 'score': 0}, {'doc_id': '219636137', 'title': 'Low-resource Languages: A Review of Past Work and Future Challenges', 'abstract': 'A current problem in NLP is massaging and processing low-resource languages which lack useful training attributes such as supervised data, number of native speakers or experts, etc. This review paper concisely summarizes previous groundbreaking achievements made towards resolving this problem, and analyzes potential improvements in the context of the overall future research direction.', 'corpus_id': 219636137, 'score': 1}, {'doc_id': '232233519', 'title': 'Multilingual Code-Switching for Zero-Shot Cross-Lingual Intent Prediction and Slot Filling', 'abstract': 'Predicting user intent and detecting the corresponding slots from text are two key problems in Natural Language Understanding (NLU). In the context of zero-shot learning, this task is typically approached by either using representations from pre-trained multilingual transformers such as mBERT, or by machine translating the source data into the known target language and then fine-tuning. Our work focuses on a particular scenario where the target language is unknown during training. To this goal, we propose a novel method to augment the monolingual source data using multilingual code-switching via random translations to enhance a transformer’s language neutrality when fine-tuning it for a downstream task. This method also helps discover novel insights on how code-switching with different language families around the world impact the performance on the target language. Experiments on the benchmark dataset of MultiATIS++ yielded an average improvement of +4.2% in accuracy for intent task and +1.8% in F1 for slot task using our method over the state-of-the-art across 8 different languages1. Furthermore, we present an application of our method for crisis informatics using a new human-annotated tweet dataset of slot filling in English and Haitian Creole, collected during Haiti earthquake disaster.', 'corpus_id': 232233519, 'score': 0}, {'doc_id': '232257615', 'title': 'Investigating Monolingual and Multilingual BERTModels for Vietnamese Aspect Category Detection', 'abstract': 'Aspect category detection (ACD) is one of the challenging tasks in the Aspect-based sentiment Analysis problem. The purpose of this task is to identify the aspect categories mentioned in user-generated reviews from a set of pre-defined categories. In this paper, we investigate the performance of various monolingual pre-trained language models compared with multilingual models on the Vietnamese aspect category detection problem. We conduct the experiments on two benchmark datasets for the restaurant and hotel domain. The experimental results demonstrated the effectiveness of the monolingual PhoBERT model than others on two datasets. We also evaluate the performance of the multilingual model based on the combination of whole SemEval-2016 datasets in other languages with the Vietnamese dataset. To the best of our knowledge, our research study is the first attempt at performing various available pretrained language models on aspect category detection task and utilize the datasets from other languages based on multilingual models.', 'corpus_id': 232257615, 'score': 0}]
108	{'doc_id': '226192896', 'title': 'Expansion of US wood pellet industry points to positive trends but the need for continued monitoring', 'abstract': 'Implementation of the European Union Renewable Energy Directive has triggered exponential growth in trading of pelletized wood fibers. Over 18 million tons of wood pellets were traded by EU member countries in 2018 of which a third were imported from the US. Concerns exist about negative impacts on US forests but systematic assessments are currently lacking. We assessed variability in fundamental attributes for timberland structure and carbon stocks within 123 procurement landscapes of wood pellet mills derived from over 38 thousand forest inventory plots in the eastern US from 2005 to 2017. We found more carbon stocks in live trees, but a fewer number of standing-dead trees, associated with the annual operation of large-scale wood pellet mills. In the US coastal southeast—where US pellet exports to the EU originate—there were fewer live and growing-stock trees and less carbon in soils with every year of milling operation than in the rest of the eastern US—which supplies the domestic market. Greater overlap of mills’ procurement areas exhibited discernible increments across selected carbon stocks. These trends likely reflect more intensive land management practices. Localized forest impacts associated with the wood pellet industry should continue to be monitored.', 'corpus_id': 226192896}	18443	"[{'doc_id': '234021726', 'title': 'Carbon Life Cycle Assessment on California-Specific Wood Products Industries: Do Data Backup General Default Values for Wood Harvest and Processing?', 'abstract': 'Carbon life cycle assessments (C LCA) play a major role in greenhouse gas (GHG)-related forest management analytics for wood products and consist of several steps along a forest to disposal path. Yet, input values for wood product C LCAs frequently rely on potentially outdated generic datasets for wood product outputs and mill efficiencies. Assumptions regarding sawmill efficiencies and sawmill-specific wood product outputs have a direct and significant impact on wood product C LCAs because these variables affect the net carbon footprint of the finished product. The goal of this analysis was to evaluate how well standard wood product C LCA inputs and assumptions for the two initial wood products LCA steps (i) forest operations and (ii) wood processing represent the current state of the wood processing industry in California. We found that sawmill efficiencies and wood product outputs both support and deviate from lookup tables currently used in publications supporting the climate-forest policy dialogue. We recommend further analysis to resolve the major discrepancies in the carbon fraction stored in durable wood products and production-related emissions to improve C LCA metrics and advance forest-related climate policy discussions in California and elsewhere.', 'corpus_id': 234021726, 'score': 1}, {'doc_id': '234310540', 'title': 'Effects of Production of Woody Pellets in the Southeastern United States on the Sustainable Development Goals', 'abstract': 'Wood-based pellets are produced in the southeastern United States (SE US) and shipped to Europe for the generation of heat and power. Effects of pellet production on selected Sustainability Development Goals (SDGs) are evaluated using industry information, available energy consumption data, and published research findings. Challenges associated with identifying relevant SDG goals and targets for this particular bioenergy supply chain and potential deleterious impacts are also discussed. We find that production of woody pellets in the SE US and shipments to displace coal for energy in Europe generate positive effects on affordable and clean energy (SDG 7), decent work and economic growth (SDG 8), industry innovation and infrastructure (SDG 9), responsible consumption and production (SDG 12), and life on land (SDG 15). Primary strengths of the pellet supply chain in the SE US are the provisioning of employment in depressed rural areas and the displacement of fossil fuels. Weaknesses are associated with potential impacts on air, water, and biodiversity that arise if the resource base and harvest activities are improperly managed. The SE US pellet supply chain provides an opportunity for transition to low-carbon industries and innovations while incentivizing better resource management.', 'corpus_id': 234310540, 'score': 1}, {'doc_id': '234949895', 'title': 'Oregon’s biomass producer tax credit—impacts of land ownership on woody bioenergy use', 'abstract': 'From 2007 through 2017, Oregon implemented an incentive program for biomass collection and production. This research evaluates renewable biomass production and deliveries during a 3-year period (2012 to 2014) in which this tax credit was in place. We evaluated total delivered tons, average payments per load, delivered location, and average transportation distance of woody biomass. We found that total delivered tons of biomass decreased each year between 2012 and 2014, as did the number of users participating in the tax credit program. The average delivered tons, by participant, was more than double in 2014 its level in earlier years, suggesting that fewer, larger entities were participating. We also evaluated differences in biomass delivery, based on receipts, transportation distances, and tons delivered, for each land ownership class. There were statistically significant differences between private and public land ownership for 2012 and 2013 but not for 2014, which included fewer applicants. Our study showed that effective biomass utilization policies need to provide sufficient economic incentives to encourage adoption by both participants and biomass energy producers, and, to be effective, to consider the complete supply chain and type of energy produced. Future economic conditions in Oregon will most likely include rapid changes in renewable energy technologies and fluctuations in fossil fuel prices, and any truly effective renewable energy policies must be sufficiently nimble to account for these and other uncertainties.', 'corpus_id': 234949895, 'score': 1}, {'doc_id': '235249414', 'title': 'Leaching of Phosphomonoesterase Activities in Beech Forest Soils: Consequences for Phosphorus Forms and Mobility', 'abstract': 'Phosphomonoesterases play an important role in the soil phosphorus (P) cycle since they hydrolyze P monoester to phosphate. Their activity is generally measured in soil extracts, and thus, it remains uncertain how mobile these enzymes are and to which extent they can be translocated within the soil profile. The presence of phosphomonoesterases in soil solutions potentially affects the share of labile dissolved organic P (DOP), which in turn would affect P leaching. Our study aimed at assessing the production and leaching of phosphomonoesterases from organic layers and topsoil horizons in forest soils and its potential effect on dissolved P forms in leachates obtained from zero-tension lysimeters. We measured phosphomonoesterase activities in leached soil solutions and compared it with those in water extracts from litter, Oe/Oa, and A horizons of two beech forests with a contrasting nitrogen (N) and P availability, subjected to experimental N × P fertilization. In addition, we determined phosphate and DOP. In soil solutions leached from litter, Oe/Oa, and A horizons, phosphomonoesterase activities ranged from 2 to 8 μmol L–1 h–1 during summer, but remained below detection limits in winter. The summer values represent 0.1–1% of the phosphomonoesterase activity in soil extracts, indicating that enzymes can be translocated from organic layers and topsoils to greater soil depths. Activities of phosphomonoesterases obtained by water extracts were greater in the organic layer of the P-poor site, while activities of those in soil solutions were similar at the two sites. Nitrogen addition increased phosphomonoesterase activities in leached soil solutions of the organic layer of the N- and P-poor soil. Using a modeling approach, we estimated that approx. 76% of the initial labile DOP was hydrolyzed to dissolved inorganic P within the first 24 h. Back calculations from measured labile DOP revealed an underestimation of approx. 15% of total dissolved P, or 0.03 mg L–1. The observed leaching of phosphomonoesterases implies that labile organic P could be hydrolyzed in deeper soil horizons and that extended sample storage leads to an underestimation of the contribution of DOP to total dissolved P leaching. This has been neglected in the few field studies measuring DOP leaching.', 'corpus_id': 235249414, 'score': 0}, {'doc_id': '235414004', 'title': 'Continued obstacles to wood‐based biomass production in the southeastern United States', 'abstract': ""International demand for wood‐based biomass for bioenergy production is growing, and private forestlands in the southeastern United States have the potential to supply that demand. The southeastern United States (Southeast) is the world's largest exporter of wood pellets for bioenergy, primarily to the United Kingdom (UK) and the European Union (EU). However, wood‐based biomass production accounts for only a small share of total wood removals from private forestlands in the Southeast. There is sufficient wood‐based biomass in the Southeast to support greater production of wood pellets for domestic and international markets without redirecting timber from sawtimber and pulpwood production. In 2018–19, we conducted 39 semi‐structured interviews with private forest landowners, foresters, loggers, and biomass production facility managers in Alabama, Florida, and Georgia to obtain their views on wood‐based biomass production in the Southeast. Although landowners were interested in supplying wood for biomass as a byproduct of timber harvesting, they seldom participated in wood‐based biomass production because of limited and unreliable access to biomass markets. Loggers and production facility managers had not invested in biomass production because they remain skeptical about the financial viability of wood‐based biomass. Continued obstacles to biomass production include: price competition with fossil fuels and conventional wood products; inconsistent domestic government support for biomass production; concerns about meeting the sustainability requirements to export wood‐based biomass to the UK and EU; and the high costs associated with harvesting low‐grade wood for biomass. The barriers to biomass expansion in the southeastern United States remain primarily economic and political rather than biophysical."", 'corpus_id': 235414004, 'score': 1}, {'doc_id': '234684196', 'title': 'Subsoils—a sink for excess fertilizer P but a minor contribution to P plant nutrition: evidence from long-term fertilization trials', 'abstract': 'Background The phosphorus (P) stocks of arable subsoils not only influence crop production but also fertilizer P sequestration. However, the extent of this influence is largely unknown. This study aimed to (i) determine the extent of P sequestration with soil depth, (ii) analyze P speciation after long-term P fertilization, and (iii) compare soil P tests in predicting crop yields. We analyzed four long-term fertilizer trials in Germany to a depth of 90\xa0cm. Treatments received either mineral or organic P, or a combination of both, for 16 to 113\xa0years. We determined inorganic and organic P pools using sequential extraction, and P speciation using 31 P nuclear magnetic resonance (NMR) and X-ray absorption near edge structure (XANES) spectroscopy. In addition, we applied three P soil tests, double-lactate (DL), calcium acetate lactate (CAL), and diffusive gradients in thin films (DGT). Results The results suggested that plants are capable of mobilizing P from deeper soil layers when there is a negative P budget of the topsoil. However, fertilization mostly only showed insignificant effects on P pools, which were most pronounced in the topsoil, with a 1.6- to 4.4-fold increase in labile inorganic P (P i ; resin-P, NaHCO 3 –P i ) after mineral fertilization and a 0- to 1.9-fold increase of organic P (P o ; NaHCO 3 –P o , NaOH–P o ) after organic P fertilization. The differences in P o and P i speciation were mainly controlled by site-specific factors, e.g., soil properties or soil management practice rather than by fertilization. When modeling crop yield response using the Mitscherlich equation, we obtained the highest R 2 ( R 2 \u2009=\u20090.61, P \u2009<\u20090.001) among the soil P tests when using topsoil P DGT . However, the fit became less pronounced when incorporating the subsoil. Conclusion We conclude that if the soil has a good P supply, the majority of P taken up by plants originates from the topsoil and that the DGT method is a mechanistic surrogate of P plant uptake. Thus, DGT is a basis for optimization of P fertilizer recommendation to add as much P fertilizer as required to sustain crop yields but as low as necessary to prevent harmful P leaching of excess fertilizer P.', 'corpus_id': 234684196, 'score': 0}, {'doc_id': '234349863', 'title': 'When Biomass Electricity Demand Prompts Thinnings in Southern US Pine Plantations: A Forest Sector Greenhouse Gas Emissions Case Study', 'abstract': 'Increasing demand for woody biomass-derived electricity in the UK and elsewhere has resulted in a rapidly expanding wood pellet manufacturing industry in the southern US. Since this demand is driven by climate concerns and an objective to lower greenhouse gas (GHG) emissions from the electricity sector, it is crucial to understand the full carbon consequences of wood pellet sourcing, processing, and utilization. We performed a comparative carbon life cycle assessment (LCA) for pellets sourced from three mills in the southern US destined for electricity generation in the UK. The baseline assumptions included GHG emissions of the UK’s 2018 and 2025 target electricity grid mix and feedstock supplied primarily from non-industrial private forest (NIPF) pine plantations augmented with a fraction of sawmill residues. Based on regional expert input, we concluded that forest management practices on the NIPF pine plantations would include timely thinning harvest treatments in the presence of pellet demand. The LCA analysis included landscape carbon stock changes based on USDA Forest Service Forest Vegetation Simulator using current USDA Forest Service Forest Inventory and Analysis data as the starting condition of supply areas in Arkansas, Louisiana and Mississippi. We found that GHG emission parity (i.e., the time when accumulated carbon GHG emissions for the bioenergy scenario equal the baseline scenario) is more than 40 years for pellets produced at each individual pellet mill and for all three pellet mills combined when compared to either the UK’s 2018 electricity grid mix or the UK’s targeted electricity grid mix in 2025. The urgency to mitigate climate change with near-term actions as well as increasing uncertainty with longer-term simulations dictated a focus on the next four decades in the analysis. Even at 50% sawmill residues, GHG emission parity was not reached during the 40 years modeled. Results are most likely conservative since we assume a high share of sawmill residues (ranging from 20 to 50%) and did not include limited hardwood feedstocks as reported in the supply chain which are generally associated with delayed GHG emission parity because of lower growth rates.', 'corpus_id': 234349863, 'score': 1}, {'doc_id': '42928058', 'title': 'Assessment of bioavailable organic phosphorus in tropical forest soils by organic acid extraction and phosphatase hydrolysis', 'abstract': 'Soil organic phosphorus contributes to the nutrition of tropical trees, but is not accounted for in standard soil phosphorus tests. Plants and microbes can release organic anions to solubilize organic phosphorus from soil surfaces, and synthesize phosphatases to release inorganic phosphate from the solubilized compounds. We developed a procedure to estimate bioavailable organic phosphorus in tropical forest soils by simulating the secretion processes of organic acids and phosphatases. Five lowland tropical forest soils with contrasting properties (pH 4.4–6.1, total P 86–429 mg P kg− 1) were extracted with 2 mM citric acid (i.e., 10 μmol g− 1, approximating rhizosphere concentrations) adjusted to soil pH in a 4:1 solution to soil ratio for 1 h. Three phosphatase enzymes were then added to the soil extract to determine the forms of hydrolysable organic phosphorus. Total phosphorus extracted by the procedure ranged between 3.22 and 8.06 mg P kg− 1 (mean 5.55 ± 0.42 mg P kg− 1), of which on average three quarters was unreactive phosphorus (i.e., organic phosphorus plus inorganic polyphosphate). Of the enzyme-hydrolysable unreactive phosphorus, 28% was simple phosphomonoesters hydrolyzed by phosphomonoesterase from bovine intestinal mucosa, a further 18% was phosphodiesters hydrolyzed by a combination of nuclease from Penicillium citrinum and phosphomonoesterase, and the remaining 51% was hydrolyzed by a broad-spectrum phytase from wheat. We conclude that soil organic phosphorus can be solubilized and hydrolyzed by a combination of organic acids and phosphatase enzymes in lowland tropical forest soils, indicating that this pathway could make a significant contribution to biological phosphorus acquisition in tropical forests. Furthermore, we have developed a method that can be used to assess the bioavailability of this soil organic phosphorus.', 'corpus_id': 42928058, 'score': 0}, {'doc_id': '234840253', 'title': 'Low molecular weight organic acids regulate soil phosphorus availability in the soils of subalpine forests, eastern Tibetan Plateau', 'abstract': 'Abstract The availability of phosphorus (P) in soils is an essential limiting factor for the development of terrestrial ecosystems; however, how root exudates affect the availability of soil P in subalpine forests remains unclear. The low molecular weight organic acids (LMWOAs) were analyzed in bulk and rhizosphere soils of Abie fabri dominated forests across four altitudes (2800–3500\xa0m) on Gongga Mountain, eastern Tibetan Plateau, to delineate the effects of LMWOAs on the availability of soil P in response to the rhizosphere and altitude differentiation. The results showed that the concentrations of available P were markedly higher in rhizosphere soils compared to bulk soils, and the concentrations did not present a clear altitudinal trend. The increase in the concentrations of available P in rhizosphere soils was significantly related to the LMWOAs that dominated by citric acid, likely through the desorption or ligand exchange rather than acidification effect because of limited range of soil pH. Climate did not alter the distribution of soil available P along the altitude; however, citric acid and soil variables including soil moisture, ammonium nitrogen and microbial biomass P significantly contributed to soil available P. The results of this study indicate that LMWOAs can significantly promote P availability in rhizosphere soils of subalpine forests, and the compensation effect of soil moisture on soil available P for cold stress may induce the higher P availability at higher altitudes.', 'corpus_id': 234840253, 'score': 0}, {'doc_id': '234209037', 'title': 'Hotspots of Legacy Phosphorus in Agricultural Landscapes: Revisiting Water-Extractable Phosphorus Pools in Soils', 'abstract': 'Controlling phosphorus (P) losses from intensive agricultural areas to water bodies is an ongoing challenge. A critical component of mitigating P losses lies in accurately predicting dissolved P loss from soils, which often includes estimating the amount of soluble P extracted with a laboratory-based extraction, i.e., water-extractable P (WEP). A standard extraction method to determine the WEP pool in soils is critical to accurately quantify and assess the risk of P loss from soils to receiving waters. We hypothesized that narrower soil-to-water ratios (1:10 or 1:20) used in current methods underestimate the pool of WEP in high or legacy P soils due to the equilibrium constraints that limit the further release of P from the solid-to-solution phase. To investigate P release and develop a more exhaustive and robust method for measuring WEP, soils from eight legacy P fields (Mehlich 3–P of 502 to 1127 mg kg−1; total P of 692 to 2235 mg kg−1) were used for WEP extractions by varying soil-to-water ratios from 1:10 to 1:100 (weight:volume) and in eight sequential extractions (equivalent to 1:800 soil-to-water ratio). Extracts were analyzed for total (WEPt) and inorganic (WEPi) pools, and organic (WEPo) pool was calculated. As the ratios widened, mean WEPi increased from 23.7 mg kg−1 (at 1:10) to 58.5 mg kg−1 (at 1:100). Further, WEPi became the dominant form, encompassing 92.9% of WEPt at 1:100 in comparison to 79.0% of WEPt at 1:10. Four of the eight selected soils were extracted using a 1:100 ratio in eight sequential extractions to fully exhaust WEP, which removed a cumulative WEPt of 125 to 549 mg kg−1, equivalent to 276–416% increase from the first 1:100 extraction. Although WEP concentrations significantly declined after the first sequential extraction, WEP was not exhausted during the subsequent extractions, indicating a sizeable pool of soluble P in legacy P soils. We conclude that (i) legacy P soils are long-term sources of soluble P in agricultural landscapes and (ii) the use of a 1:100 soil-to-water ratio can improve quantification and risk assessment of WEP loss in legacy P soils.', 'corpus_id': 234209037, 'score': 0}]"
109	{'doc_id': '212717830', 'title': 'Meta-Learning Initializations for Low-Resource Drug Discovery', 'abstract': 'Building in silico models to predict chemical properties and activities is a crucial step in drug discovery. However, drug discovery projects are often characterized by limited labeled data, hindering the applications of deep learning in this setting. Meanwhile advances in meta-learning have enabled state-of-the-art performances in few-shot learning benchmarks, naturally prompting the question: Can meta-learning improve deep learning performance in low-resource drug discovery projects? In this work, we assess the efficiency of the Model-Agnostic Meta-Learning (MAML) algorithm - along with its variants FO-MAML and ANIL - at learning to predict chemical properties and activities. Using the ChEMBL20 dataset to emulate low-resource settings, our benchmark shows that meta-initializations perform comparably to or outperform multi-task pre-training baselines on 16 out of 20 in-distribution tasks and on all out-of-distribution tasks, providing an average improvement in AUPRC of 7.2% and 14.9% respectively. Finally, we observe that meta-initializations consistently result in the best performing models across fine-tuning sets with $k \\in \\{16, 32, 64, 128, 256\\}$ instances.', 'corpus_id': 212717830}	3583	"[{'doc_id': '214756833', 'title': 'A medicinal chemistry perspective of drug repositioning: Recent advances and challenges in drug discovery', 'abstract': '\n Abstract\n \n Drug repurposing is a strategy consisting of finding new indications for already known marketed drugs used in various clinical settings or highly characterized compounds despite they can be failed drugs. Recently, it emerges as an alternative approach for the rapid identification and development of new pharmaceuticals for various rare and complex diseases for which lack the effective drug treatments. The success rate of drugs repurposing approach accounts for approximately 30% of new FDA approved drugs and vaccines in recent years. This review focuses on the status of drugs repurposing approach for various diseases including skin diseases, infective, inflammatory, cancer, and neurodegenerative diseases. Efforts have been made to provide structural features and mode of actions of drugs.\n \n', 'corpus_id': 214756833, 'score': 0}, {'doc_id': '216045142', 'title': 'Resveratrol plus carboxymethyl-β-glucan in infants with common cold: A randomized double-blind trial', 'abstract': '\n               Abstract\n               \n                  Objectives\n                  To evaluate effectiveness of a nasal resveratrol/carboxymethyl-β-glucan solution compared to nasal saline solution: a) on common cold symptoms by means of a validated measure scale (CARIFS score), b) on Rhinovirus infection and CCL2, CCL5, IL8, IL6, CXCL10 and TLR2 expression in nasal swabs, c) on frequency of relapses after 30 days of follow-up.\n               \n               \n                  Methods\n                  89 infants with respiratory infection symptoms were randomly assigned to receive either a nasal resveratrol/carboxymethyl-β-glucan solution or nasal saline solution.\n                  All patients were evaluated with CARIFS score at enrollment, after 48 h, 7 and 30 days by physicians and parents. Nasal swabs were obtained at enrollment, after 48 h and after one week.\n               \n               \n                  Results\n                  CARIFS score improved in both groups. Episodes of sneezing and cough were fewer in study group after 7 days of follow-up (p < 0.05). No significant differences were found on nasopharyngeal swabs in Rhinovirus detection and cytokines expression after 48 h, nor in 30 days relapses. TLR2 expression was significantly higher in Rhinovirus infected children of the study group. No adverse effects occurred.\n               \n               \n                  Conclusions\n                  These data suggest that a solution containing resveratrol plus carboxymethyl-β-glucan might have a positive impact on both clinical and socio-economic burden due to infant common cold.\n               \n            ', 'corpus_id': 216045142, 'score': 0}, {'doc_id': '216055359', 'title': 'An exploratory randomized controlled study on the efficacy and safety of lopinavir/ritonavir or arbidol treating adult patients hospitalized with mild/moderate COVID-19 (ELACOI)', 'abstract': 'Background: Antiviral therapies against the novel coronavirus SARS-CoV-2, which has caused a global pandemic of respiratory illness called COVID-19, are still lacking. Methods: Our study (NCT04252885, named ELACOI), was an exploratory randomized (2:2:1) controlled trial assessing the efficacy and safety of lopinavir/ritonavir (LPV/r) or arbidol monotherapy for treating patients with mild/moderate COVID-19. Findings: This study successfully enrolled 86 patients with mild/moderate COVID-19 with 34 randomly assigned to receive LPV/r, 35 to arbidol and 17 with no antiviral medication as control. Baseline characteristics of the three groups were comparable. The primary endpoints, the average time of positive-to-negative conversion of SARS-CoV-2 nucleic acid and conversion rates at days 7 and 14, were similar between groups (all P>0.05). There were no differences between groups in the secondary endpoints, the rates of antipyresis, cough alleviation, or improvement of chest CT at days 7 or 14 (all P>0.05). At day 7, eight (23.5%) patients in the LPV/r group, 3 (8.6%) in the arbidol group and 2(11.8%) in the control group showed a deterioration in clinical status from moderate to severe/critical(P =0.206). Overall, 12 (35.3%) patients in the LPV/r group and 5 (14.3%) in the arbidol group experienced adverse events during the follow-up period. No apparent adverse event occurred in the control group. Conclusions: LPV/r or arbidol monotherapy present little benefit for improving the clinical outcome of patients hospitalized with mild/moderate COVID-19 over supportive care.', 'corpus_id': 216055359, 'score': 0}, {'doc_id': '211066459', 'title': 'A deep-learning view of chemical space designed to facilitate drug discovery', 'abstract': 'Drug discovery projects entail cycles of design, synthesis, and testing that yield a series of chemically related small molecules whose properties, such as binding affinity to a given target protein, are progressively tailored to a particular drug discovery goal. The use of deep learning technologies could augment the typical practice of using human intuition in the design cycle, and thereby expedite drug discovery projects. Here we present DESMILES, a deep neural network model that advances the state of the art in machine learning approaches to molecular design. We applied DESMILES to a previously published benchmark that assesses the ability of a method to modify input molecules to inhibit the dopamine receptor D2, and DESMILES yielded a 77% lower failure rate compared to state-of-the-art models. To explain the ability of DESMILES to hone molecular properties, we visualize a layer of the DESMILES network, and further demonstrate this ability by using DESMILES to tailor the same molecules used in the D2 benchmark test to dock more potently against seven different receptors.', 'corpus_id': 211066459, 'score': 1}, {'doc_id': '215782277', 'title': 'Hydroxychloroquine in patients with COVID-19: an open-label, randomized, controlled trial', 'abstract': 'Abstract Objectives To assess the efficacy and safety of hydroxychloroquine (HCQ) plus standard-of-care (SOC) compared with SOC alone in adult patients with COVID-19. Design Multicenter, open-label, randomized controlled trial. Setting 16 government-designated COVID-19 treatment centers in China through 11 to 29 in February 2020. Participants 150 patients hospitalized with COVID-19. 75 patients were assigned to HCQ plus SOC and 75 were assigned to SOC alone (control group). Interventions HCQ was administrated with a loading dose of 1, 200 mg daily for three days followed by a maintained dose of 800 mg daily for the remaining days (total treatment duration: 2 or 3 weeks for mild/moderate or severe patients, respectively). Main outcome measures The primary endpoint was the 28-day negative conversion rate of SARS-CoV-2. The assessed secondary endpoints were negative conversion rate at day 4, 7, 10, 14 or 21, the improvement rate of clinical symptoms within 28-day, normalization of C-reactive protein and blood lymphocyte count within 28-day. Primary and secondary analysis was by intention to treat. Adverse events were assessed in the safety population. Results The overall 28-day negative conversion rate was not different between SOC plus HCQ and SOC group (Kaplan-Meier estimates 85.4% versus 81.3%, P=0.341). Negative conversion rate at day 4, 7, 10, 14 or 21 was also similar between the two groups. No different 28-day symptoms alleviation rate was observed between the two groups. A significant efficacy of HCQ on alleviating symptoms was observed when the confounding effects of anti-viral agents were removed in the post-hoc analysis (Hazard ratio, 8.83, 95%CI, 1.09 to 71.3). This was further supported by a significantly greater reduction of CRP (6.986 in SOC plus HCQ versus 2.723 in SOC, milligram/liter, P=0.045) conferred by the addition of HCQ, which also led to more rapid recovery of lymphopenia, albeit no statistical significance. Adverse events were found in 8.8% of SOC and 30% of HCQ recipients with two serious adverse events. The most common adverse event in the HCQ recipients was diarrhea (10%). Conclusions The administration of HCQ did not result in a higher negative conversion rate but more alleviation of clinical symptoms than SOC alone in patients hospitalized with COVID-19 without receiving antiviral treatment, possibly through anti-inflammatory effects. Adverse events were significantly increased in HCQ recipients but no apparently increase of serious adverse events. Trial registration ChiCTR2000029868.', 'corpus_id': 215782277, 'score': 0}, {'doc_id': '53307328', 'title': 'Efficacy, Safety, and Quality of Life in a Multicenter, Randomized, Placebo-Controlled Trial of Low-Dose Peanut Oral Immunotherapy in Children with Peanut Allergy.', 'abstract': 'BACKGROUND\nOnly 2 small placebo-controlled trials on peanut oral immunotherapy (OIT) have been published.\n\n\nOBJECTIVE\nWe examined the efficacy, safety, immunologic parameters, quality of life (QOL), and burden of treatment (BOT) of low-dose peanut OIT in a multicenter, double-blind, randomized placebo-controlled trial.\n\n\nMETHODS\nA total of 62 children aged 3 to 17 years with IgE-mediated, challenge-proven peanut allergy were randomized (1:1) to receive peanut OIT with a maintenance dose of 125 to 250 mg peanut protein or placebo. The primary outcome was the proportion of children tolerating 300 mg or more peanut protein at oral food challenge (OFC) after 16 months of OIT. We measured the occurrence of adverse events (AEs), immunologic changes, and QOL before and after OIT and BOT during OIT.\n\n\nRESULTS\nTwenty-three of 31 (74.2%) children of the active group tolerated at least 300 mg peanut protein at final OFC compared with 5 of 31 (16.1%) in the placebo group (P < .001). Thirteen of 31 (41.9%) children of the active versus 1 of 31 (3.2%) of the placebo group tolerated the highest dose of 4.5 g peanut protein at final OFC (P < .001). There was no significant difference between the groups in the occurrence of AE-related dropouts or in the number, severity, and treatment of objective AEs. In the peanut-OIT group, we noted a significant reduction in peanut-specific IL-4, IL-5, IL-10, and IL-2 production by PBMCs compared with the placebo group, as well as a significant increase in peanut-specific IgG4 levels and a significant improvement in QOL; 86% of children evaluated the BOT positively.\n\n\nDISCUSSION\nLow-dose OIT is a promising, effective, and safe treatment option for peanut-allergic children, leading to improvement in QOL, a low BOT, and immunologic changes showing tolerance development.', 'corpus_id': 53307328, 'score': 1}, {'doc_id': '18975694', 'title': 'Safety and feasibility of oral immunotherapy to multiple allergens for food allergy', 'abstract': 'BackgroundThirty percent of children with food allergy are allergic to more than one food. Previous studies on oral immunotherapy (OIT) for food allergy have focused on the administration of a single allergen at the time. This study aimed at evaluating the safety of a modified OIT protocol using multiple foods at one time.MethodsParticipants underwent double-blind placebo-controlled food challenges (DBPCFC) up to a cumulative dose of 182\xa0mg of food protein to peanut followed by other nuts, sesame, dairy or egg. Those meeting inclusion criteria for peanut only were started on single-allergen OIT while those with additional allergies had up to 5 foods included in their OIT mix. Reactions during dose escalations and home dosing were recorded in a symptom diary.ResultsForty participants met inclusion criteria on peanut DBPCFC. Of these, 15 were mono-allergic to peanut and 25 had additional food allergies. Rates of reaction per dose did not differ significantly between the two groups (median of 3.3% and 3.7% in multi and single OIT group, respectively; p\u2009=\u2009.31). In both groups, most reactions were mild but two severe reactions requiring epinephrine occurred in each group. Dose escalations progressed similarly in both groups although, per protocol design, those on multiple food took longer to reach equivalent doses per food (median +4 mo.; p\u2009<\u2009.0001).ConclusionsPreliminary data show oral immunotherapy using multiple food allergens simultaneously to be feasible and relatively safe when performed in a hospital setting with trained personnel. Additional, larger, randomized studies are required to continue to test safety and efficacy of multi-OIT.Trial registrationClinicaltrial.gov NCT01490177', 'corpus_id': 18975694, 'score': 1}, {'doc_id': '216081629', 'title': 'Translating IL-6 biology into effective treatments', 'abstract': 'In 1973, IL-6 was identified as a soluble factor that is secreted by T cells and is important for antibody production by B cells. Since its discovery more than 40 years ago, the IL-6 pathway has emerged as a pivotal pathway involved in immune regulation in health and dysregulation in many diseases. Targeting of the IL-6 pathway has led to innovative therapeutic approaches for various rheumatic diseases, such as rheumatoid arthritis, juvenile idiopathic arthritis, adult-onset Still’s disease, giant cell arteritis and Takayasu arteritis, as well as other conditions such as Castleman disease and cytokine release syndrome. Targeting this pathway has also identified avenues for potential expansion into several other indications, such as uveitis, neuromyelitis optica and, most recently, COVID-19 pneumonia. To mark the tenth anniversary of anti-IL-6 receptor therapy worldwide, we discuss the history of research into IL-6 biology and the development of therapies that target IL-6 signalling, including the successes and challenges and with an emphasis on rheumatic diseases. In this Perspective article, the authors recount the earliest stages of translational research into IL-6 biology and the subsequent development of therapeutic IL-6 pathway inhibitors for the treatment of autoimmune rheumatic diseases and potentially numerous other indications.', 'corpus_id': 216081629, 'score': 0}, {'doc_id': '186596484', 'title': 'ChemNet: A Transferable and Generalizable Deep Neural Network for Small-Molecule Property Prediction', 'abstract': 'With access to large datasets, deep neural networks (DNN) have achieved human-level accuracy in image and speech recognition tasks. However, in chemistry, availability of large standardized and labelled datasets is scarce, and many chemical properties of research interest, chemical data is inherently small and fragmented. In this work, we explore transfer learning techniques in conjunction with the existing Chemception CNN model, to create a transferable and generalizable deep neural network for small-molecule property prediction. Our latest model, ChemNet learns in a semi-supervised manner from inexpensive labels computed from the ChEMBL database. When fine-tuned to the Tox21, HIV and FreeSolv dataset, which are 3 separate chemical properties that ChemNet was not originally trained on, we demonstrate that ChemNet exceeds the performance of existing Chemception models and other contemporary DNN models. Furthermore, as ChemNet has been pre-trained on a large diverse chemical database, it can be used as a general-purpose plug-and-play deep neural network for the prediction of novel small-molecule chemical properties.', 'corpus_id': 186596484, 'score': 1}, {'doc_id': '14664656', 'title': 'Assessing the efficacy of oral immunotherapy for the desensitisation of peanut allergy in children (STOP II): a phase 2 randomised controlled trial', 'abstract': ""Summary Background Small studies suggest peanut oral immunotherapy (OIT) might be effective in the treatment of peanut allergy. We aimed to establish the efficacy of OIT for the desensitisation of children with allergy to peanuts. Methods We did a randomised controlled crossover trial to compare the efficacy of active OIT (using characterised peanut flour; protein doses of 2–800 mg/day) with control (peanut avoidance, the present standard of care) at the NIHR/Wellcome Trust Cambridge Clinical Research Facility (Cambridge, UK). Randomisation (1:1) was by use of an audited online system; group allocation was not masked. Eligible participants were aged 7–16 years with an immediate hypersensitivity reaction after peanut ingestion, positive skin prick test to peanuts, and positive by double-blind placebo-controlled food challenge (DBPCFC). We excluded participants if they had a major chronic illness, if the care provider or a present household member had suspected or diagnosed allergy to peanuts, or if there was an unwillingness or inability to comply with study procedures. Our primary outcome was desensitisation, defined as negative peanut challenge (1400 mg protein in DBPCFC) at 6 months (first phase). Control participants underwent OIT during the second phase, with subsequent DBPCFC. Immunological parameters and disease-specific quality-of-life scores were measured. Analysis was by intention to treat. Fisher's exact test was used to compare the proportion of those with desensitisation to peanut after 6 months between the active and control group at the end of the first phase. This trial is registered with Current Controlled Trials, number ISRCTN62416244. Findings The primary outcome, desensitisation, was recorded for 62% (24 of 39 participants; 95% CI 45–78) in the active group and none of the control group after the first phase (0 of 46; 95% CI 0–9; p<0·001). 84% (95% CI 70–93) of the active group tolerated daily ingestion of 800 mg protein (equivalent to roughly five peanuts). Median increase in peanut threshold after OIT was 1345 mg (range 45–1400; p<0·001) or 25·5 times (range 1·82–280; p<0·001). After the second phase, 54% (95% CI 35–72) tolerated 1400 mg challenge (equivalent to roughly ten peanuts) and 91% (79–98) tolerated daily ingestion of 800 mg protein. Quality-of-life scores improved (decreased) after OIT (median change −1·61; p<0·001). Side-effects were mild in most participants. Gastrointestinal symptoms were, collectively, most common (31 participants with nausea, 31 with vomiting, and one with diarrhoea), then oral pruritus after 6·3% of doses (76 participants) and wheeze after 0·41% of doses (21 participants). Intramuscular adrenaline was used after 0·01% of doses (one participant). Interpretation OIT successfully induced desensitisation in most children within the study population with peanut allergy of any severity, with a clinically meaningful increase in peanut threshold. Quality of life improved after intervention and there was a good safety profile. Immunological changes corresponded with clinical desensitisation. Further studies in wider populations are recommended; peanut OIT should not be done in non-specialist settings, but it is effective and well tolerated in the studied age group. Funding MRC-NIHR partnership."", 'corpus_id': 14664656, 'score': 1}]"
110	{'doc_id': '117162512', 'title': 'Multi-GNSS satellite clock estimation constrained with oscillator noise model in the existence of data discontinuity', 'abstract': 'During the past years, real-time precise point positioning has been proven to be an efficient tool in the applications of navigation, precise orbit determination of LEO as well as earthquake and tsunami early warning, etc. One of the most crucial issues of these applications is the high-precision real-time GNSS satellite clock. Though the performance and character of the GNSS onboard atomic frequency standard have been widely studied, the white noise model is still the most popular hypothesis that employed in the real-time GNSS satellite clock estimation. However, concerning the real-time applications, significant data discontinuity may arise either due to the fact that only regional stations involved, or the failure in the stations, satellites and network connections. These data discontinuity would result in an arbitrary clock jump between adjacent arcs when the clock offsets are modeled as white noise. In addition, it is also expected that the detection and identification of outliers would be benefited from the constrains of the satellite oscillator noise model. Thus in this contribution, based on the statistic analysis of almost 2-year multi-GNSS precise clock products, we developed the oscillator noise model for the satellites of GPS, GLONASS, BDS and Galileo according to the oscillator type as well as the block type. Then, the efficiency of this oscillator noise model in multi-GNSS satellite clock estimation is demonstrated with 2-months data for both regional and global networks in simultaneous real-time mode. For the regional network, the results suggest that compared with the traditional solution based on white noise model, the improvement is 44.4 and 12.1% on average for STD and RMS, respectively, and the improvement is mainly attributed to the efficiency of the oscillator noise model during the convergence period and the gross error resistance. Concerning the global experiment, since the stations guarantee the continuous tracking of the satellites with redundant observable, the improvement is not as evident as that of regional experiment for GPS, GLONASS and BDS. The STD of Galileo clock improves from 0.28 to 0.19\xa0ns due to that, the satellites E14 and E18 still suffer significant data discontinuity during our experimental period.', 'corpus_id': 117162512}	17537	[{'doc_id': '235212649', 'title': 'Performance Evaluation of Triple-Frequency GPS/Galileo Techniques for Precise Static and Kinematic Applications', 'abstract': 'The objective of this research was to develop new precise point positioning (PPP) processing models using triple-frequency GPS/Galileo observations. Different triple-frequency PPP models were developed including undifferenced, between-satellite single-difference (BSSD) and semi-decoupled PPP models. Additionally, a dual-frequency ionosphere-free undifferenced PPP model was developed. The performance of our developed PPP models was evaluated for both static and kinematic applications. To validate the proposed PPP models for static applications, triple-frequency GPS/Galileo observations spanning three successive days from eight globally distributed reference stations were acquired. Then, the observations were processed using the four static PPP solutions. It is found that the 3D positioning accuracy of the triple-frequency semi-decoupled, BSSD and undifferenced PPP models is enhanced after 10 min by about 50, 41 and 29%, respectively, compared with the dual-frequency undifferenced PPP model. After 20 min of processing, improvements in the 3D positioning accuracy by 40, 31 and 21% are obtained for the triple-frequency semi-decoupled, BSSD and undifferenced PPP models, respectively, with respect to the dual-frequency PPP model. The 3D positioning accuracy is also improved after 60 min, compared with the dual-frequency solution, by 40, 40 and 35% for the triple-frequency semi-decoupled, BSSD and undifferenced PPP solutions, respectively. For kinematic application validation, a vehicle trajectory was carried out. The collected triple-frequency GPS/Galileo observations were processed using the four kinematic PPP solutions. It is shown that the triple-frequency semi-decupled, BSSD and undifferenced PPP solutions enhance the 3D positioning accuracy by 31, 23 and 10%, respectively, in comparison with the dual-frequency undifferenced PPP solutions.', 'corpus_id': 235212649, 'score': 0}, {'doc_id': '235128448', 'title': 'Assessing IGS GPS/Galileo/BDS-2/BDS-3 phase bias products with PRIDE PPP-AR', 'abstract': 'Ambiguity Resolution in Precise Point Positioning (PPP-AR) is important to achieving high-precision positioning in wide areas. The International GNSS (Global Navigation Satellite System) Service (IGS) and some other academic organizations have begun to provide phase bias products to enable PPP-AR, such as the integer-clock like products by Centre National d’Etudes Spatials (CNES), Wuhan University (WUM) and the Center for Orbit Determination in Europe (CODE), as well as the Uncalibrated Phase Delay (UPD) products by School of Geodesy and Geomatics (SGG). To evaluate these disparate products, we carry out Global Positioning System (GPS)/Galileo Navigation Satellite System (Galileo) and BeiDou Navigation Satellite System (BDS-only) PPP-AR using 30\xa0days of data in 2019. In general, over 70% and 80% of GPS and Galileo ambiguity residuals after wide-lane phase bias corrections fall in\u2009±\u20090.1 cycles, in contrast to less than 50% for BeiDou Navigation Satellite (Regional) System (BDS-2); moreover, around 90% of GPS/Galileo narrow-lane ambiguity residuals are within\u2009±\u20090.1 cycles, while the percentage drops to about 55% in the case of BDS products. GPS/Galileo daily PPP-AR can usually achieve a positioning precision of 2, 2 and 6\xa0mm for the east, north and up components, respectively, for all phase bias products except those based on German Research Centre for Geosciences (GBM) rapid satellite orbits and clocks. Due to the insufficient number of BDS satellites during 2019, the BDS phase bias products perform worse than the GPS/Galileo products in terms of ambiguity fixing rates and daily positioning precisions. BDS-2 daily positions can only reach a precision of about 10\xa0mm in the horizontal and 20\xa0mm in the vertical components, which can be slightly improved after PPP-AR. However, for the year of 2020, BDS-2/BDS-3 (BDS-3 Navigation Satellite System) PPP-AR achieves about 50% better precisions for all three coordinate components.', 'corpus_id': 235128448, 'score': 0}, {'doc_id': '235310144', 'title': 'Estimation of fractional cycle bias for GPS/BDS-2/Galileo based on international GNSS monitoring and assessment system observations using the uncombined PPP model', 'abstract': 'The Fractional Cycle Bias (FCB) product is crucial for the Ambiguity Resolution (AR) in Precise Point Positioning (PPP). Different from the traditional method using the ionospheric-free ambiguity which is formed by the Wide Lane (WL) and Narrow Lane (NL) combinations, the uncombined PPP model is flexible and effective to generate the FCB products. This study presents the FCB estimation method based on the multi-Global Navigation Satellite System (GNSS) precise satellite orbit and clock corrections from the international GNSS Monitoring and Assessment System (iGMAS) observations using the uncombined PPP model. The dual-frequency raw ambiguities are combined by the integer coefficients (4,−\u20093) and (1,−\u20091) to directly estimate the FCBs. The details of FCB estimation are described with the Global Positioning System (GPS), BeiDou-2 Navigation Satellite System (BDS-2) and Galileo Navigation Satellite System (Galileo). For the estimated FCBs, the Root Mean Squares (RMSs) of the posterior residuals are smaller than 0.1 cycles, which indicates a high consistency for the float ambiguities. The stability of the WL FCBs series is better than 0.02 cycles for the three GNSS systems, while the STandard Deviation (STD) of the NL FCBs for BDS-2 is larger than 0.139 cycles. The combined FCBs have better stability than the raw series. With the multi-GNSS FCB products, the PPP AR for GPS/BDS-2/Galileo is demonstrated using the raw observations. For hourly static positioning results, the performance of the PPP AR with the three-system observations is improved by 42.6%, but only 13.1% for kinematic positioning results. The results indicate that precise and reliable positioning can be achieved with the PPP AR of GPS/BDS-2/Galileo, supported by multi-GNSS satellite orbit, clock, and FCB products based on iGMAS.', 'corpus_id': 235310144, 'score': 0}, {'doc_id': '214092137', 'title': 'Multi-GNSS processing, positioning and applications', 'abstract': 'In the past few decades the Global Positioning System (GPS) has been the number one positioning tool in a range of Geodesy and Geophysics applications. The emerging Regional and Global Navigation Satellite Systems (RNSSs/GNSSs) can enhance positioning and non-positioning applications. GPS modernisation from dual-frequency to triplefrequency signals has lately also been complemented by global GLObal’ naya NAvigatsionnaya Sputnikovaya Sistema (GLONASS), Galileo and BeiDou Navigation Satellite System (BDS), as well as the regional Quasi-Zenith Satellite system (QZSS) and Navigation with Indian Constellation (NavIC). By 2024 it is expected that we will have access to more than 110 satellites in a multi-GNSS model transmitting their signals on a range of different frequencies. This will significantly improve current dual-frequency GPS positioning as well as non-positioning applications, such as atmospheric modelling and timing applications. Rigorous models and algorithms are however needed so as to link and integrate such multi-frequency, multi-GNSS signals to the estimable parameters of interest. Important topics in this field include singleand multi-frequency, multi-GNSS modelling, while making use of survey-grade and low-cost receiver and antenna equipment, including smartphones. With low-cost we refer to GNSS receivers and antennas having a cost of at most a few hundreds of dollars, whereas survey-grade receivers and antennas typically have a cost of several thousands of dollars. Other important topics include ionospheric and tropospheric delay estimation, and multi-GNSS Precise Point Positioning Real Time Kinematic (PPP-RTK). This Special Feature showcases the latest trends in multi-GNSS modelling. Five papers were accepted, based on their quality and significant contributions to the field. The topics covered are: multi-frequency and multiGNSS for stochastic modelling, single-baseline RTK and PPP-RTK, respectively, ionospheric modelling, and a PPP performance analysis of dual-frequency, multi-GNSS data collected in smartphones.', 'corpus_id': 214092137, 'score': 1}, {'doc_id': '234259867', 'title': 'Application of GPS and GNSS technology in geosciences', 'abstract': 'Abstract Global Positioning System (GPS) is a global navigational satellite system developed by the United States Department of Defense. This technology is available only with America, Russia (GLONASS), China (BeiDou), and Japan (Quasi-Zenith Satellite System). In this, the navigation systems of America and Russia are global, while countries like China and Japan are using it regionally. The European Union has also completed preparations to start its navigation system. In terms of surveying, mapping technology, and engineering construction, it is used not only in the establishment of Earth control networks but also in the establishment of land and ocean geodetic survey benchmarks. Global Navigation Satellite System (GNSS) framework is one of the four major positioning systems, mainly GPS, GLONASS, GNS, and BeiDou, in the world. This chapter describes the application of GPS and GNSS Technology in Geosciences like rescue and relief projects, agriculture, dynamic observation, time transmission, speed measurement, vehicle guidance, and other fields.', 'corpus_id': 234259867, 'score': 1}, {'doc_id': '233437128', 'title': 'Single-epoch RTK performance assessment of tightly combined BDS-2 and newly complete BDS-3', 'abstract': 'The BeiDou global navigation satellite system (BDS-3) constellation deployment has been completed on June 23, 2020, with a full constellation comprising 30 satellites. In this study, we present the performance assessment of single-epoch Real-Time Kinematic (RTK) positioning with tightly combined BeiDou regional navigation satellite system (BDS-2) and BDS-3. We first investigate whether code and phase Differential Inter-System Biases (DISBs) exist between the legacy B1I/B3I signals of BDS-3/BDS-2. It is discovered that the DISBs are in fact about zero for the baselines with the same or different receiver types at their endpoints. These results imply that BDS-3 and BDS-2 are fully interoperable and can be regarded as one constellation without additional DISBs when the legacy B1I/B3I signals are used for precise relative positioning. Then we preliminarily evaluate the single-epoch short baseline RTK performance of tightly combined BDS-2 and the newly completed BDS-3. The performance is evaluated through ambiguity resolution success rate, ambiguity dilution of precision, as well as positioning accuracy in kinematic and static modes using the datasets collected in Wuhan. Experimental results demonstrate that the current BDS-3 only solutions can deliver comparable ambiguity resolution performance and much better positioning accuracy with respect to BDS-2 only solutions. Moreover, the RTK performance is much improved with tightly combined BDS-3/BDS-2, particularly in challenging or harsh conditions. The single-frequency single-epoch tightly combined BDS-3/BDS-2 solution could deliver an ambiguity resolution success rate of 96.9% even with an elevation cut-off angle of 40°, indicating that the tightly combined BDS-3/BDS-2 could achieve superior RTK positioning performance in the Asia–Pacific region. Meanwhile, the three-dimensional (East/North/Up) positioning accuracy of BDS-3 only solution (0.52\xa0cm/0.39\xa0cm/2.14\xa0cm) in the kinematic test is significantly better than that of the BDS-2 only solution (0.85\xa0cm/1.02\xa0cm/3.01\xa0cm) due to the better geometry of the current BDS-3 constellation. The tightly combined BDS-3/BDS-2 solution can provide the positioning accuracy of 0.52\xa0cm, 0.22\xa0cm, and 1.80\xa0cm, respectively.', 'corpus_id': 233437128, 'score': 0}, {'doc_id': '123981207', 'title': 'Multi-technique combination of space geodesy observations: Impact of the Jason-2 satellite on the GPS satellite orbits estimation', 'abstract': 'Abstract In order to improve the Precise Orbit Determination (POD) of the GPS constellation and the Jason-2 Low Earth Orbiter (LEO), we carry out a simultaneous estimation of GPS satellite orbits along with Jason-2 orbits, using GINS software. Along with GPS station observations, we use Jason-2 GPS, SLR and DORIS observations, over a data span of 6\xa0months (28/05/2011–03/12/2011). We use the Geophysical Data Records-D (GDR-D) orbit estimation standards for the Jason-2 satellite. A GPS-only solution is computed as well, where only the GPS station observations are used. It appears that adding the LEO GPS observations results in an increase of about 0.7% of ambiguities fixed, with respect to the GPS-only solution. The resulting GPS orbits from both solutions are of equivalent quality, agreeing with each other at about 7\xa0mm on Root Mean Square (RMS). Comparisons of the resulting GPS orbits to the International GNSS Service (IGS) final orbits show the same level of agreement for both the GPS-only orbits, at 1.38\xa0cm in RMS, and the GPS\xa0+\xa0Jason2 orbits at 1.33\xa0cm in RMS. We also compare the resulting Jason-2 orbits with the 3-technique Segment Sol multi-missions d’ALTimetrie, d’orbitographie et de localisation precise (SSALTO) POD products. The orbits show good agreement, with 2.02\xa0cm of orbit differences global RMS, and 0.98\xa0cm of orbit differences RMS on the radial component.', 'corpus_id': 123981207, 'score': 1}, {'doc_id': '126259910', 'title': 'An analytical study on the carrier-phase linear combinations for triple-frequency GNSS', 'abstract': 'The linear combinations of multi-frequency carrier-phase measurements for Global Navigation Satellite System (GNSS) are greatly beneficial to improving the performance of ambiguity resolution (AR), cycle slip correction as well as precise positioning. In this contribution, the existing definitions of the carrier-phase linear combination are reviewed and the integer property of the resulting ambiguity of the phase linear combinations is examined. The general analytical method for solving the optimal integer linear combinations for all triple-frequency GNSS is presented. Three refined triple-frequency integer combinations solely determined by the frequency values are introduced, which are the ionosphere-free (IF) combination that the Sum of its integer coefficients equal to 0 (IFS0), the geometry-free (GF) combination that the Sum of its integer coefficients equal to 0 (GFS0) and the geometry-free and ionosphere-free (GFIF) combination. Besides, the optimal GF, IF, extra-wide lane and ionosphere-reduced integer combinations for GPS and BDS are solved exhaustively by the presented method. Their potential applications in cycle slip detection, AR as well as precise positioning are discussed. At last, a more straightforward GF and IF AR scheme than the existing method is presented based on the GFIF integer combination.', 'corpus_id': 126259910, 'score': 1}, {'doc_id': '235274298', 'title': 'Functional model modification of precise point positioning considering the time-varying code biases of a receiver', 'abstract': 'Precise Point Positioning (PPP), initially developed for the analysis of the Global Positing System (GPS) data from a large geodetic network, gradually becomes an effective tool for positioning, timing, remote sensing of atmospheric water vapor, and monitoring of Earth’s ionospheric Total Electron Content (TEC). The previous studies implicitly assumed that the receiver code biases stay constant over time in formulating the functional model of PPP. In this contribution, it is shown this assumption is not always valid and can lead to the degradation of PPP performance, especially for Slant TEC (STEC) retrieval and timing. For this reason, the PPP functional model is modified by taking into account the time-varying receiver code biases of the two frequencies. It is different from the Modified Carrier-to-Code Leveling (MCCL) method which can only obtain the variations of Receiver Differential Code Biases (RDCBs), i.e., the difference between the two frequencies’ code biases. In the Modified PPP (MPPP) model, the temporal variations of the receiver code biases become estimable and their adverse impacts on PPP parameters, such as ambiguity parameters, receiver clock offsets, and ionospheric delays, are mitigated. This is confirmed by undertaking numerical tests based on the real dual-frequency GPS data from a set of global continuously operating reference stations. The results imply that the variations of receiver code biases exhibit a correlation with the ambient temperature. With the modified functional model, an improvement by 42% to 96% is achieved in the Differences of STEC (DSTEC) compared to the original PPP model with regard to the reference values of those derived from the Geometry-Free (GF) carrier phase observations. The medium and long term (1\u2009×\u2009104 to 1.5\u2009×\u2009104\xa0s) frequency stability of receiver clocks are also significantly improved.', 'corpus_id': 235274298, 'score': 0}, {'doc_id': '128903173', 'title': 'Assessment of Several Interpolation Methods for Precise GPS Orbit', 'abstract': 'GPS applications such as Precise Point Positioning (PPP) require the availability of precise ephemeris at high rate. To support these applications, several institutions such as the International GNSS Service (IGS) have developed precise orbital service. Unfortunately, however, the data rate of such precise orbits is usually limited to 15 minutes. To overcome this problem, a number of orbital interpolation methods are proposed. This paper examines the performance of four interpolation methods for IGS precise GPS orbits, namely Lagrange, Newton Divided Difference, Cubic Spline and Trigonometric interpolation. In addition, the paper discusses a new approach, which utilizes the residuals between the broadcast and precise ephemeris to generate a high density precise ephemeris. It is shown that the new approach produces better results than previously reported orbital interpolation accuracy.', 'corpus_id': 128903173, 'score': 1}]
111	{'doc_id': '204702729', 'title': 'An improved platform for functional assessment of large protein libraries in mammalian cells', 'abstract': 'Abstract Multiplex genetic assays can simultaneously test thousands of genetic variants for a property of interest. However, limitations of existing multiplex assay methods in cultured mammalian cells hinder the breadth, speed and scale of these experiments. Here, we describe a series of improvements that greatly enhance the capabilities of a Bxb1 recombinase-based landing pad system for conducting different types of multiplex genetic assays in various mammalian cell lines. We incorporate the landing pad into a lentiviral vector, easing the process of generating new landing pad cell lines. We also develop several new landing pad versions, including one where the Bxb1 recombinase is expressed from the landing pad itself, improving recombination efficiency more than 2-fold and permitting rapid prototyping of transgenic constructs. Other versions incorporate positive and negative selection markers that enable drug-based enrichment of recombinant cells, enabling the use of larger libraries and reducing costs. A version with dual convergent promoters allows enrichment of recombinant cells independent of transgene expression, permitting the assessment of libraries of transgenes that perturb cell growth and survival. Lastly, we demonstrate these improvements by assessing the effects of a combinatorial library of oncogenes and tumor suppressors on cell growth. Collectively, these advancements make multiplex genetic assays in diverse cultured cell lines easier, cheaper and more effective, facilitating future studies probing how proteins impact cell function, using transgenic variant libraries tested individually or in combination.', 'corpus_id': 204702729}	16591	[{'doc_id': '205506557', 'title': 'A novel Bxb1 integrase RMCE system for high fidelity site‐specific integration of mAb expression cassette in CHO Cells', 'abstract': 'As CHO cell line development for biotherapeutic production becomes more sophisticated through the availability of the CHO genome sequence, the ability to accurately and reproducibly engineer the host cell genome has become increasingly important. Multiple well characterized systems for site‐specific integration will enable more complex cell line engineering to generate cell lines with desirable attributes. We built and characterized a novel recombinase mediated cassette exchange (RMCE) system using Bxb1 integrase and compared it to the commonly used Flp/FRT RMCE system. We first integrated a DNA construct flanked by either Bxb1 attachment sites or FRT sequences (referred to as a landing pad) into the Fer1L4 genomic locus of CHO‐S cells using CRISPR/Cas9 mediated homologous recombination. We characterized the resulting clones harboring either the Bxb1 or Flp/FRT landing pad using whole genome resequencing to compare their genomes with the parental host cell line. We determined that each landing pad was specifically integrated into the Fer1L4 locus in the selected clones and observed no major structural changes in the genome or variations in copy number as a result of CRISPR/Cas9 modification. We subsequently tested the ability of the Bxb1 and Flp/FRT landing pad clones to perform proper RMCE with donor vectors containing identical mAb expression cassettes flanked by either Bxb1 attachment sites or FRT sites. We demonstrated that both RMCE systems were able to generate stable pools in a similar time frame with comparable mAb expression. Through genetic characterization of up to 24 clones derived from either system, we determined that the BxB1 RMCE system yielded higher fidelity RMCE events than the Flp/FRT system as evidenced by a higher percentage of clones with expected integration of the mAb cassette into the landing pad in the respective cell lines. We conclude that Bxb1 RMCE is an excellent alternative to Flp/FRT RMCE and valuable addition to our toolbox enabling the engineering of more sophisticated cell lines for biotherapeutic production. Biotechnol. Bioeng. 2017;114: 1837–1846. © 2017 Wiley Periodicals, Inc.', 'corpus_id': 205506557, 'score': 1}, {'doc_id': '233390743', 'title': 'Strategies for Bacteriophage T5 Mutagenesis: Expanding the Toolbox for Phage Genome Engineering', 'abstract': 'Phage genome editing is crucial to uncover the molecular mechanisms of virus infection and to engineer bacteriophages with enhanced antibacterial properties. Phage genetic engineering relies mostly on homologous recombination (HR) assisted by the targeted elimination of wild-type phages by CRISPR-Cas nucleases. These strategies are often less effective in virulent bacteriophages with large genomes. T5 is a virulent phage that infects Escherichia coli. We found that CRISPR-Cas9 system (type II-A) had ununiform efficacies against T5, which impairs a reliable use of CRISPR-Cas-assisted counterselection in the gene editing of T5. Here, we present alternative strategies for the construction of mutants in T5. Bacterial retroelements (retrons) proved to be efficient for T5 gene editing by introducing point mutations in the essential gene A1. We set up a protocol based on dilution-amplification-screening (DAS) of phage pools for mutant enrichment that was used to introduce a conditional mutation in another essential gene (A2), insert a new gene (lacZα), and construct a translational fusion of a late phage gene with a fluorescent protein coding gene (pb10-mCherry). The method should be applicable to other virulent phages that are naturally resistant to CRISPR/Cas nucleases.', 'corpus_id': 233390743, 'score': 0}, {'doc_id': '232412238', 'title': 'Improved architectures for flexible DNA production using retrons across kingdoms of life', 'abstract': 'Exogenous DNA is a critical molecular tool for biology. This is particularly true for gene editing, where exogenous DNA can be used as a template to introduce precise changes to the sequence of a cell’s genome. This DNA is typically synthesized or assembled in vitro and then delivered to target cells. However, delivery can be inefficient, and low abundance of template DNA may be one reason that precise editing typically occurs at a low rate. It has recently been shown that producing DNA inside cells can using reverse transcriptases can increase the efficiency of genome editing. One tool to produce that DNA is a retron, a bacterial retroelement that has an endogenous role in phage defense. However, little effort has been directed at optimizing the retron for production of designed sequences when used as a component of biotechnology. Here, we identify modifications to the retron non-coding RNA that result in more abundant reverse transcribed DNA. We also test architectures of the retron operon that enable efficient reverse transcription across kingdoms of life from bacteria, to yeast, to cultured human cells. We find that gains in DNA production using modified retrons are portable from prokaryotic to eukaryotic cells. Finally, we demonstrate that increased production of RT-DNA results in more efficient genome editing in both prokaryotic and eukaryotic cells. These experiments provide a general framework for production of DNA using a retron for biotechnological applications.', 'corpus_id': 232412238, 'score': 1}, {'doc_id': '4664629', 'title': 'A multi-landing pad DNA integration platform for mammalian cell engineering', 'abstract': 'Abstract Engineering mammalian cell lines that stably express many transgenes requires the precise insertion of large amounts of heterologous DNA into well-characterized genomic loci, but current methods are limited. To facilitate reliable large-scale engineering of CHO cells, we identified 21 novel genomic sites that supported stable long-term expression of transgenes, and then constructed cell lines containing one, two or three ‘landing pad’ recombination sites at selected loci. By using a highly efficient BxB1 recombinase along with different selection markers at each site, we directed recombinase-mediated insertion of heterologous DNA to selected sites, including targeting all three with a single transfection. We used this method to controllably integrate up to nine copies of a monoclonal antibody, representing about 100 kb of heterologous DNA in 21 transcriptional units. Because the integration was targeted to pre-validated loci, recombinant protein expression remained stable for weeks and additional copies of the antibody cassette in the integrated payload resulted in a linear increase in antibody expression. Overall, this multi-copy site-specific integration platform allows for controllable and reproducible insertion of large amounts of DNA into stable genomic sites, which has broad applications for mammalian synthetic biology, recombinant protein production and biomanufacturing.', 'corpus_id': 4664629, 'score': 1}, {'doc_id': '233745187', 'title': 'Targetron-assisted delivery of exogenous DNA sequences into Pseudomonas putida through CRISPR-aided counterselection', 'abstract': 'Genome editing methods based on Group II introns (known as Targetron technology) have been long used as a gene knock-out strategy in a wide range of organisms in a fashion independent of homologous recombination. Yet, their utility as delivery systems has been typically suboptimal because of their reduced efficiency of insertion when they carry exogenous sequences. We show that this limitation can be tackled and Targetron adapted as a general tool in Gram-negative bacteria. To this end, a set of broad host range standardized vectors were designed for conditional expression of the Ll.LtrB intron. After testing the correct functionality of these plasmids in Escherichia coli and Pseudomonas putida, we created a library of Ll.LtrB variants carrying cargo DNA sequences of different lengths to benchmark the capacity of intron-mediated delivery in these bacteria. Next, we combined CRISPR/Cas9-facilitated counterselection to increase the chances of finding genomic sites inserted with the thereby engineered introns. By following this pipeline, we were able to insert exogenous sequences of up to 600 bp at specific genomic locations in wild-type P. putida KT2440 and its ΔrecA derivative. Finally, we were able to apply this technology to successfully tag this strain with an orthogonal short sequence (barcode) that acts as a unique identifier for tracking this microorganism in biotechnological settings. The results with P. putida exemplified the value of the Targetron approach for unrestricted delivery of small DNA fragments to the genomes of Gram-negative bacteria for a suite of genome editing endeavours.', 'corpus_id': 233745187, 'score': 1}, {'doc_id': '233449677', 'title': 'SIBR-Cas enables host-independent and universal CRISPR genome engineering in bacteria', 'abstract': 'CRISPR-Cas is a powerful tool for genome editing in bacteria. However, its efficacy is dependent on host factors (such as DNA repair pathways) and/or exogenous expression of recombinases. In this study, we mitigated these constraints by developing a simple and universal genome engineering tool for bacteria which we termed SIBR-Cas (Self-splicing Intron-Based Riboswitch-Cas). SIBR-Cas was generated from a mutant library of the theophylline-dependent self-splicing T4 td intron that allows for universal and inducible control over CRISPR-Cas counterselection. This control delays CRISPR-Cas counterselection, granting more time for the editing event (e.g., by homologous recombination) to occur. Without the use of exogenous recombinases, SIBR-Cas was successfully applied to knock-out several genes in three bacteria with poor homologous recombination systems. Compared to other genome engineering tools, SIBR-Cas is simple, tightly regulated and widely applicable for most (non-model) bacteria. Furthermore, we propose that SIBR can have a wider application as a universal gene expression and gene regulation control mechanism for any gene or RNA of interest in bacteria.', 'corpus_id': 233449677, 'score': 0}, {'doc_id': '233395816', 'title': 'Genome Editing in Bacteria: CRISPR-Cas and Beyond', 'abstract': 'Genome editing in bacteria encompasses a wide array of laborious and multi-step methods such as suicide plasmids. The discovery and applications of clustered regularly interspaced short palindromic repeats (CRISPR)-Cas based technologies have revolutionized genome editing in eukaryotic organisms due to its simplicity and programmability. Nevertheless, this system has not been as widely favored for bacterial genome editing. In this review, we summarize the main approaches and difficulties associated with CRISPR-Cas-mediated genome editing in bacteria and present some alternatives to circumvent these issues, including CRISPR nickases, Cas12a, base editors, CRISPR-associated transposases, prime-editing, endogenous CRISPR systems, and the use of pre-made ribonucleoprotein complexes of Cas proteins and guide RNAs. Finally, we also address fluorescent-protein-based methods to evaluate the efficacy of CRISPR-based systems for genome editing in bacteria. CRISPR-Cas still holds promise as a generalized genome-editing tool in bacteria and is developing further optimization for an expanded application in these organisms. This review provides a rarely offered comprehensive view of genome editing. It also aims to familiarize the microbiology community with an ever-growing genome-editing toolbox for bacteria.', 'corpus_id': 233395816, 'score': 0}, {'doc_id': '58536611', 'title': 'Comparison of Integrases Identifies Bxb1-GA Mutant as the Most Efficient Site-Specific Integrase System in Mammalian Cells.', 'abstract': 'Phage-derived integrases can catalyze irreversible, site-specific integration of transgenic payloads into a chromosomal locus, resulting in mammalian cells that stably express transgenes or circuits of interest. Previous studies have demonstrated high-efficiency integration by the Bxb1 integrase in mammalian cells. Here, we show that a point mutation (Bxb1-GA) in Bxb1 target sites significantly increases Bxb1-mediated integration efficiency at the Rosa26 locus in Chinese hamster ovary cells, resulting in the highest integration efficiency reported with a site-specific integrase in mammalian cells. Bxb1-GA point mutant sites do not cross-react with Bxb1 wild-type sites, enabling their use in applications that require orthogonal pairs of target sites. In comparison, we test the efficiency and orthogonality of ϕC31 and Wβ integrases, and show that Wβ has an integration efficiency between those of Bxb1-GA and wild-type Bxb1. Our data present a toolbox of integrases for inserting payloads such as gene circuits or therapeutic transgenes into mammalian cell lines.', 'corpus_id': 58536611, 'score': 1}, {'doc_id': '232327578', 'title': 'Generation of Drosophila attP containing cell lines using CRISPR-Cas9', 'abstract': 'The generation of Drosophila stable cell lines have become invaluable for complementing in vivo experiments and as tools for genetic screens. Recent advances utilizing attP/PhiC31 integrase system has permitted the creation of Drosophila cells in which recombination mediated cassette exchange (RMCE) can be utilized to generate stably integrated transgenic cell lines that contain a single copy of the transgene at the desired locus. Current techniques, besides being laborious and introducing extraneous elements, are limited to a handful of cell lines of embryonic origin. Nonetheless, with well over 100 Drosophila cell lines available, including an ever-increasing number CRISPR/Cas9 modified cell lines, a more universal methodology is needed to generate a stably integrated transgenic line from any one of the available Drosophila melanogaster cell lines. Here we describe a toolkit and procedure that combines CRISPR/Cas9 and the PhiC31 integrase system. We have generated and isolated single cell clones containing an Actin5C::dsRed cassette flanked by attP sites into the genome of Kc167 and S2R+ cell lines that mimic the in vivo attP sites located at 25C6 and 99F8 of the Drosophila genome. Furthermore, we tested the functionality of the attP docking sites utilizing two independent GFP expressing constructs flanked by attB sites that permit RMCE and therefore the insertion of any DNA of interest. Lastly, to demonstrate the universality of our methodology and existing constructs, we have successfully integrated the Actin5C::dsRed cassette flanked by attP sites into two different CNS cell lines, ML-DmBG2-c2 and ML-DmBG3-c2. Overall, the reagents and methodology reported here permit the efficient generation of stable transgenic cassettes with minimal change in the cellular genomes in existing D. melanogaster cell lines.', 'corpus_id': 232327578, 'score': 0}, {'doc_id': '233220477', 'title': 'An efficient vector-based CRISPR/Cas9 system in an Oreochromis mossambicus cell line using endogenous promoters', 'abstract': 'CRISPR/Cas9 gene editing is effective in manipulating genetic loci in mammalian cell cultures and whole fish but efficient platforms applicable to fish cell lines are currently limited. Our initial attempts to employ this technology in fish cell lines using heterologous promoters or a ribonucleoprotein approach failed to indicate genomic alteration at targeted sites in a tilapia brain cell line (OmB). For potential use in a DNA vector approach, endogenous tilapia beta Actin (OmBAct), EF1 alpha (OmEF1a), and U6 (TU6) promoters were isolated. The strongest candidate promoter determined by EGFP reporter assay, OmEF1a, was used to drive constitutive Cas9 expression in a modified OmB cell line (Cas9-OmB1). Cas9-OmB1 cell transfection with vectors expressing gRNAs driven by the TU6 promoter achieved mutational efficiencies as high as 81% following hygromycin selection. Mutations were not detected using human and zebrafish U6 promoters demonstrating the phylogenetic proximity of U6 promoters as critical when used for gRNA expression. Sequence alteration to TU6 improved mutation rate and cloning efficiency. In conclusion, we report new tools for ectopic expression and a highly efficient, economical system for manipulation of genomic loci and evaluation of their causal relationship with adaptive cellular phenotypes by CRISPR/Cas9 gene editing in fish cells.', 'corpus_id': 233220477, 'score': 0}]
112	{'doc_id': '208623434', 'title': 'Highly structured slow solar wind emerging from an equatorial coronal hole', 'abstract': 'During the solar minimum, when the Sun is at its least active, the solar wind1,2 is observed at high latitudes as a predominantly fast (more than 500\xa0kilometres per second), highly Alfvénic rarefied stream of plasma originating from deep within coronal holes. Closer to the ecliptic plane, the solar wind is interspersed with a more variable slow wind3 of less than 500 kilometres per second. The precise origins of the slow wind streams are less certain4; theories and observations suggest that they may originate at the tips of helmet streamers5,6, from interchange reconnection near coronal hole boundaries7,8, or within coronal holes with highly diverging magnetic fields9,10. The heating mechanism required to drive the solar wind is also unresolved, although candidate mechanisms include Alfvén-wave turbulence11,12, heating by reconnection in nanoflares13, ion cyclotron wave heating14 and acceleration by thermal gradients1. At a distance of one astronomical unit, the wind is mixed and evolved, and therefore much of the diagnostic structure of these sources and processes has been lost. Here we present observations from the Parker Solar Probe15 at 36 to 54 solar radii that show evidence of slow Alfvénic solar wind emerging from a small equatorial coronal hole. The measured magnetic field exhibits patches of large, intermittent reversals that are associated with jets of plasma and enhanced Poynting flux and that are interspersed in a smoother and less turbulent flow with a near-radial magnetic field. Furthermore, plasma-wave measurements suggest the existence of electron and ion velocity-space micro-instabilities10,16 that are associated with plasma heating and thermalization processes. Our measurements suggest that there is an impulsive mechanism associated with solar-wind energization and that micro-instabilities play a part in heating, and we provide evidence that low-latitude coronal holes are a key source of the slow solar wind.Measurements from the Parker Solar Probe show that slow solar wind near the Sun’s equator originates in coronal holes.', 'corpus_id': 208623434}	19196	"[{'doc_id': '236976383', 'title': 'Localised acceleration of energetic particles by a weak shock in the solar corona', 'abstract': 'Globally-propagating shocks in the solar corona have long been studied to quantify their involvement in the acceleration of energetic particles. However, this work has tended to focus on large events associated with strong solar flares and fast coronal mass ejections (CMEs), where the waves are sufficiently fast to easily accelerate particles to high energies. Here we present observations of particle acceleration associated with a global wave event which occurred on 1 October 2011. Using differential emission measure analysis, the global shock wave was found to be incredibly weak, with an Alfvén Mach number of ∼1.008–1.013. Despite this, spatiallyresolved type III radio emission was observed by the Nançay RadioHeliograph at distinct locations near the shock front, suggesting localised acceleration of energetic electrons. Further investigation using a magnetic field extrapolation identified a fan structure beneath a magnetic null located above the source active region, with the erupting CME contained within this topological feature. We propose that a reconfiguration of the coronal magnetic field driven by the erupting CME enabled the weak shock to accelerate particles along field lines initially contained within the fan and subsequently opened into the heliosphere, producing the observed type III emission. These results suggest that even weak global shocks in the solar corona can accelerate energetic particles via reconfiguration of the surrounding magnetic field.', 'corpus_id': 236976383, 'score': 0}, {'doc_id': '237283732', 'title': 'Mesoscale Structure in the Solar Wind', 'abstract': 'Structures in the solar wind result from two basic mechanisms: structures injected or imposed directly by the Sun, and structures formed through processing en route as the solar wind advects outward and fills the heliosphere. On the largest scales, solar structures directly impose heliospheric structures, such as coronal holes imposing high speed streams of solar wind. Transient solar processes can inject large-scale structure directly into the heliosphere as well, such as coronal mass ejections. At the smallest, kinetic scales, the solar wind plasma continually evolves, converting energy into heat, and all structure at these scales is formed en route. “Mesoscale” structures, with scales at 1 AU in the approximate spatial range of 5–10,000 Mm and temporal range of 10 s–7 h, lie in the orders of magnitude gap between the two size-scale extremes. Structures of this size regime are created through both mechanisms. Competition between the imposed and injected structures with turbulent and other evolution leads to complex structuring and dynamics. The goal is to understand this interplay and to determine which type of mesoscale structures dominate the solar wind under which conditions. However, the mesoscale regime is also the region of observation space that is grossly under-sampled. The sparse in situ measurements that currently exist are only able to measure individual instances of discrete structures, and are not capable of following their evolution or spatial extent. Remote imaging has captured global and large scale features and their evolution, but does not yet have the sensitivity to measure most mesoscale structures and their evolution. Similarly, simulations cannot model the global system while simultaneously resolving kinetic effects. It is important to understand the source and evolution of solar wind mesoscale structures because they contain information on how the Sun forms the solar wind, and constrains the physics of turbulent processes. Mesoscale structures also comprise the ground state of space weather, continually buffeting planetary magnetospheres. In this paper we describe the current understanding of the formation and evolution mechanisms of mesoscale structures in the solar wind, their characteristics, implications, and future steps for research progress on this topic.', 'corpus_id': 237283732, 'score': 1}, {'doc_id': '236036788', 'title': 'First Solar Orbiter observation of the Alfvénic slow wind and identification of its solar source', 'abstract': 'Context. Turbulence dominated by large amplitude nonlinear Alfvén-like fluctuations mainly propagating away from the Sun is ubiquitous in high speed solar wind streams. Recent studies have shown that also slow wind streams may show strong Alfvénic signatures, especially in the inner heliosphere. Aims. The present study focuses on the characterisation of an Alfvénic slow solar wind interval observed by Solar Orbiter on July 14-18, 2020 at a heliocentric distance of 0.64 AU. Methods. Our analysis is based on plasma moments and magnetic field measurements from the Solar Wind Analyser (SWA) and Magnetometer (MAG) instruments, respectively. We compare the behaviour of different parameters to characterise the stream in terms of the Alfvénic content and magnetic properties. We perform also a spectral analysis to highlight spectral features and waves signature using power spectral density and magnetic helicity spectrograms, respectively. Moreover, we reconstruct the Solar Orbiter magnetic connectivity to the solar sources via both a ballistic and a Potential Field Source Surface (PFSS) model. Results. The Alfvénic slow wind stream described in this paper resembles in many respects a fast wind stream. Indeed, at large scales, the time series of the speed profile shows a compression region, a main portion of the stream and a rarefaction region, characterised by different features. Moreover, before the rarefaction region, we pinpoint several structures at different scales recalling the spaghetti-like flux-tube texture of the interplanetary magnetic field. Finally, we identify the connections between Solar Orbiter in situ measurements, tracing them down to coronal streamer and pseudostreamer configurations. Conclusions. The characterisation of the Alfvénic slow wind stream observed by Solar Orbiter and the identification of its solar source are extremely important aspects to understand possible future observations of the same solar wind regime, especially as solar activity is increasing toward a maximum, where a higher incidence of this solar wind regime is expected.', 'corpus_id': 236036788, 'score': 1}, {'doc_id': '237259777', 'title': 'Magnetic Reconnection within the Boundary Layer of a Magnetic Cloud in the Solar Wind', 'abstract': 'The twisted local magnetic field at the front or rear regions of the magnetic clouds (MCs) associated with interplanetary coronal mass ejections (ICMEs) is often nearly opposite to the direction of the ambient interplanetary magnetic field (IMF). There is also observational evidence for magnetic reconnection (MR) outflows occurring within the boundary layers of MCs. In this paper a MR event located at the western flank of the MC occurring on 2000-10-03 is studied in detail. Both the large-scale geometry of the helical MC and the MR outflow structure are scrutinized in a detailed multi-point study. The ICME sheath is of hybrid propagation-expansion type. Here the freshly reconnected open field lines are expected to slip slowly over the MC resulting in plasma mixing at the same time. As for MR, the current sheet geometry and the vertical motion of the outflow channel between ACE-Geotail-WIND spacecraft was carefully studied and tested. The main findings on MR include: (1) First-time observation of non-Petschek-type slowshock-like discontinuities in the inflow regions; (2) Observation of turbulent Hall magnetic field associated with a Lorentz force deflected electron jet; (3) Acceleration of protons by reconnection electric field and their back-scatter from the slow shock-like discontinuity; (4) Observation of relativistic electron near the MC inflow boundary/separatrix; these electron populations can presumably appear as a result of non-adiabatic acceleration, gradient B drift and via acceleration in the electrostatic potential well associated with the Hall current system; (5) Observation of Doppler shifted ion-acoustic and Langmuir waves in the MC inflow region.', 'corpus_id': 237259777, 'score': 0}, {'doc_id': '236782169', 'title': 'Near-Sun Switchback Boundaries: Dissipation with Solar Distance', 'abstract': 'The most surprising result from the first solar encounters by the Parker Solar Probe (PSP) is the large amount of brief magnetic field reversals often referred to as switchbacks. Switchbacks have previously been observed further downstream in the solar wind by spacecraft such as Helios 2 at 62 R s from the Sun. However, these observations lack a distinct proton temperature increase detected inside switchbacks by PSP, implying that they are evolving over time to eventually reach a pressure balance at the switchback boundaries. We look at the evolution of switchback boundaries as a function of radial distance from the Sun, from closest approach at 35.7 R s during PSP’s first two encounters to beyond 80 R s . Using magnetic field and proton data from PSP’s FIELDS and SWEAP instruments, we perform a day-by-day superposed epoch analysis of the 25 switchbacks with the sharpest step-like boundaries. During both encounters we found the proton temperature spikes to gradually decline before vanishing completely around 55 R s . Magnetic reversals and velocity spikes also steadily drop in magnitude, but eventually flatten out instead of disappearing. Most interestingly, proton temperature change ΔT p across switchback boundaries is found to reach 2 × 105 in magnitude below 40 R s during PSP’s outbound trajectory, but is an order of magnitude less on the inbound trajectory, suggesting a possible common change in switchback characteristics near closest approach during both encounters.', 'corpus_id': 236782169, 'score': 1}, {'doc_id': '236170928', 'title': 'Whistler instability driven by the sunward electron deficit in the solar wind. High-cadence Solar Orbiter observations', 'abstract': 'Context. Solar wind electrons play an important role in the energy balance of the solar wind acceleration by carrying energy into interplanetary space in the form of electron heat flux. The heat flux is stored in the complex electron velocity distribution functions (VDFs) shaped by expansion, Coulomb collisions, and field-particle interactions. Aims. We investigate how the suprathermal electron deficit in the anti-strahl direction, which was recently discovered in the near-Sun solar wind, drives a kinetic instability and creates whistler waves with wave vectors that are quasi-parallel to the direction of the background magnetic field. Methods. We combine high-cadence measurements of electron pitch-angle distribution functions and electromagnetic waves provided by Solar Orbiter during its first orbit. Our case study is based on a burst-mode data interval from the Electrostatic Analyser System (SWA-EAS) at a distance of 112 RS (0.52 au) from the Sun, during which several whistler wave packets were detected by Solar Orbiter’s Radio and Plasma Waves (RPW) instrument. Results. The sunward deficit creates kinetic conditions under which the quasi-parallel whistler wave is driven unstable. We directly test our predictions for the existence of these waves through solar wind observations. We find whistler waves that are quasi-parallel and almost circularly polarised, propagating away from the Sun, coinciding with a pronounced sunward deficit in the electron VDF. The cyclotron-resonance condition is fulfilled for electrons moving in the direction opposite to the direction of wave propagation, with energies corresponding to those associated with the sunward deficit. Conclusions. We conclude that the sunward deficit acts as a source of quasi-parallel whistler waves in the solar wind. The quasilinear diffusion of the resonant electrons tends to fill the deficit, leading to a reduction in the total electron heat flux.', 'corpus_id': 236170928, 'score': 0}, {'doc_id': '237158230', 'title': 'Magnetic Field Draping of the Heliopause and Its Consequences for Radio Emission in the Very Local Interstellar Medium', 'abstract': 'We discuss the observations and simulations related to the interaction of the solar wind (SW) and local interstellar medium (LISM), and the interstellar magnetic field draping around the heliopause (HP). This Letter sheds light on some processes that are not directly seen in the Voyager data. Special attention is paid to the magnetic field behavior at the HP crossing, penetration of shocks, and compression waves across the HP, and their merging in the LISM surrounding it. Modeling identifies forward and reverse shocks propagating through the heliosheath. Voyager data shows that the magnetic field strength experiences a jump at the HP, while the elevation and azimuthal angles are continuous across it. We show that our prior numerical results are in agreement with the Voyager data, if the heliospheric magnetic field is not assumed unipolar. The simulations confirm the importance of taking into account time dependencies of the SW flow, including the presence of transient structures and magnetohydrodynamic instabilities. For the first time, we provide the heliospheric community with the Alfvén speed distribution observed by Voyagers, which shows that it is unexpectedly small and decreases with distance from the HP. This is of critical importance for the identification of physical mechanisms responsible for the Langmuir wave and radio emission generation behind the HP. The data shows that outward-propagating, subcritical shocks traversing the LISM have a rather wide dissipation structure, which raises questions about their ability to reflect electrons as collisionless shocks can do.', 'corpus_id': 237158230, 'score': 0}, {'doc_id': '236640121', 'title': 'Solar-Wind Structures That Are Not Destroyed by the Action of Solar-Wind Turbulence', 'abstract': 'If MHD turbulence is a dominant process acting in the solar wind between the Sun and 1 AU, then the destruction and regeneration of structure in the solar-wind plasma is expected. Six types of solar-wind structure at 1 AU that are not destroyed by turbulence are examined: 1) corotating-interaction-region stream interfaces, 2) periodic density structures, 3) magnetic structure anisotropy, 4) ion-composition boundaries and their co-located current sheets, 5) strahl-intensity boundaries and their co-located current sheets, and 6) non-evolving Alfvénic magnetic structure. Implications for the solar wind and for turbulence in the solar wind are highlighted and a call for critical future solar-wind measurements is given.', 'corpus_id': 236640121, 'score': 1}, {'doc_id': '235795106', 'title': 'Is Solar Minimum 24/25 Another Unusual One?', 'abstract': 'Solar minimum 23/24 is considered to be unusual because it exhibits features that differ notably from those commonly seen in previous minima. In this Letter, we analyze the solar polar magnetic field, the potential-field solution of the solar corona, and the in situ solar wind measurements to see if the recent solar minimum 24/25 is another unusual one. While the dipolar configuration that is commonly seen during minimum 22/23 and earlier minima persist for about half a year after the absolute minimum of solar cycle 24, the corona has a morphology that is more complex than a simple dipole before the absolute minimum. The fast solar wind streams are less dominant than minimum 23/24. The interplanetary magnetic field strength, density, and mass flux that are historically low in the minimum 23/24 are regained during minimum 24/25, but still do not reach the minimum 22/23 level. From the analysis of this Letter, it seems that the minimum 24/25 is only partly unusual, and the recovery of the commonly minimum features may result from the enhancement of the polar field.', 'corpus_id': 235795106, 'score': 0}, {'doc_id': '236428214', 'title': ""Large-scale Structure and Turbulence Transport in the Inner Solar Wind -- Comparison of Parker Solar Probe's First Five Orbits with a Global 3D Reynolds-averaged MHD Model"", 'abstract': 'Simulation results from a global magnetohydrodynamic model of the solar corona and solar wind are compared with Parker Solar Probe (PSP) observations during its first five orbits. The fully threedimensional model is based on Reynolds-averaged mean-flow equations coupled with turbulence transport equations. The model includes the effects of electron heat conduction, Coulomb collisions, turbulent Reynolds stresses, and heating of protons and electrons via a turbulent cascade. Turbulence transport equations for average turbulence energy, cross helicity, and correlation length are solved concurrently with the mean-flow equations. Boundary conditions at the coronal base are specified using solar synoptic magnetograms. Plasma, magnetic field, and turbulence parameters are calculated along the PSP trajectory. Data from the first five orbits are aggregated to obtain trends as a function of heliocentric distance. Comparison of simulation results with PSP data shows good agreement, especially for mean-flow parameters. Synthetic distributions of magnetic fluctuations are generated, constrained by the local rms turbulence amplitude given by the model. Properties of this computed turbulence are compared with PSP observations.', 'corpus_id': 236428214, 'score': 1}]"
113	{'doc_id': '802243', 'title': 'Text to 3D Scene Generation with Rich Lexical Grounding', 'abstract': 'The ability to map descriptions of scenes to 3D geometric representations has many applications in areas such as art, education, and robotics. However, prior work on the text to 3D scene generation task has used manually specified object categories and language that identifies them. We introduce a dataset of 3D scenes annotated with natural language descriptions and learn from this data how to ground textual descriptions to physical objects. Our method successfully grounds a variety of lexical terms to concrete referents, and we show quantitatively that our method improves 3D scene generation over previous work using purely rule-based methods. We evaluate the fidelity and plausibility of 3D scenes generated with our grounding approach through human judgments. To ease evaluation on this task, we also introduce an automated metric that strongly correlates with human judgments.', 'corpus_id': 802243}	10043	"[{'doc_id': '225070343', 'title': 'Text Editing by Command', 'abstract': 'A prevailing paradigm in neural text generation is one-shot generation, where text is produced in a single step. The one-shot setting is inadequate, however, when the constraints the user wishes to impose on the generated text are dynamic, especially when authoring longer documents. We address this limitation with an interactive text generation setting in which the user interacts with the system by issuing commands to edit existing text. To this end, we propose a novel text editing task, and introduce WikiDocEdits, a dataset of single-sentence edits crawled from Wikipedia. We show that our Interactive Editor, a transformer-based model trained on this dataset, outperforms baselines and obtains positive results in both automatic and human evaluations. We present empirical and qualitative analyses of this model’s performance.', 'corpus_id': 225070343, 'score': 0}, {'doc_id': '2554264', 'title': 'ShapeNet: An Information-Rich 3D Model Repository', 'abstract': 'We present ShapeNet: a richly-annotated, large-scale repository of shapes represented by 3D CAD models of objects. ShapeNet contains 3D models from a multitude of semantic categories and organizes them under the WordNet taxonomy. It is a collection of datasets providing many semantic annotations for each 3D model such as consistent rigid alignments, parts and bilateral symmetry planes, physical sizes, keywords, as well as other planned annotations. Annotations are made available through a public web-based interface to enable data visualization of object attributes, promote data-driven geometric analysis, and provide a large-scale quantitative benchmark for research in computer graphics and vision. At the time of this technical report, ShapeNet has indexed more than 3,000,000 models, 220,000 models out of which are classified into 3,135 categories (WordNet synsets). In this report we describe the ShapeNet effort as a whole, provide details for all currently available datasets, and summarize future plans.', 'corpus_id': 2554264, 'score': 1}, {'doc_id': '224814065', 'title': 'Image‐Driven Furniture Style for Interactive 3D Scene Modeling', 'abstract': ""Creating realistic styled spaces is a complex task, which involves design know‐how for what furniture pieces go well together. Interior style follows abstract rules involving color, geometry and other visual elements. Following such rules, users manually select similar‐style items from large repositories of 3D furniture models, a process which is both laborious and time‐consuming. We propose a method for fast‐tracking style‐similarity tasks, by learning a furniture's style‐compatibility from interior scene images. Such images contain more style information than images depicting single furniture. To understand style, we train a deep learning network on a classification task. Based on image embeddings extracted from our network, we measure stylistic compatibility of furniture. We demonstrate our method with several 3D model style‐compatibility results, and with an interactive system for modeling style‐consistent scenes."", 'corpus_id': 224814065, 'score': 0}, {'doc_id': '4707877', 'title': 'Text2Shape: Generating Shapes from Natural Language by Learning Joint Embeddings', 'abstract': 'We present a method for generating colored 3D shapes from natural language. To this end, we first learn joint embeddings of freeform text descriptions and colored 3D shapes. Our model combines and extends learning by association and metric learning approaches to learn implicit cross-modal connections, and produces a joint representation that captures the many-to-many relations between language and physical properties of 3D shapes such as color and shape. To evaluate our approach, we collect a large dataset of natural language descriptions for physical 3D objects in the ShapeNet dataset. With this learned joint embedding we demonstrate text-to-shape retrieval that outperforms baseline approaches. Using our embeddings with a novel conditional Wasserstein GAN framework, we generate colored 3D shapes from text. Our method is the first to connect natural language text with realistic 3D objects exhibiting rich variations in color, texture, and shape detail. See video at this https URL', 'corpus_id': 4707877, 'score': 1}, {'doc_id': '143705532', 'title': 'Sojourners to Settlers: Early Constructions of Chinese Identity in South Africa, 1879–1949', 'abstract': 'Chinese presence throughout the African continent has increased tremendously in the last decade, resulting in both high hopes and many serious concerns. The Chinese state news agency Xinhua reported that in 2005 trade volume with Africa totalled nearly $40billion – a forty-fold increase from 1990 – and direct investment in Africa reached $1.18billion (Mail & Guardian June 16–22 2006; AFP June 18 2006). In South Africa, trade between South Africa and China totalled $5.7billion (Mail & Guardian 20 January 2005). For the most part, these large-scale investments have resulted only in temporary populations of Chinese in various African cities. However, there has been small-scale immigration of Chinese, particularly into South Africa, since the earliest days of white settlement in the mid-seventeenth century. Since the mid-1990s, the Chinese population of South Africa has increased from approximately 20,000 to between 200,000 and 300,000 today.', 'corpus_id': 143705532, 'score': 0}, {'doc_id': '222134031', 'title': 'Holistic static and animated 3D scene generation from diverse text descriptions', 'abstract': 'We propose a framework for holistic static and animated 3D scene generation from diverse text descriptions. Prior works of scene generation rely on static rule-based entity extraction from natural language description. However, this limits the usability of a practical solution. To overcome this limitation, we use one of state-of-the-art architecture - TransformerXL. Instead of rule-based extraction, our framework leverages the rich contextual encoding which allows us to process a larger range (diverse) of possible natural language descriptions. We empirically show how our proposed mechanism generalizes even on novel combinations of object-features during inference. We also show how our framework can jointly generate static and animated 3D scene efficiently. We modify CLEVR to generate a large, scalable dataset - Integrated static and animated 3D scene (Iscene). Data preparation code and pre-trained model available at - this https URL.', 'corpus_id': 222134031, 'score': 1}, {'doc_id': '153390455', 'title': 'HUMAN CAPITAL IN AFRICA: TECHNICAL CHANGE, EFFICIENCY AND PRODUCTIVITY', 'abstract': 'The aim of this paper is to analyze the importance of human capital on productivity growth for 26 African countries over the period 1976-1996. To estimate productivity change, the Malmquist total factor productivity (TFP) growth index is computed. This index is then decomposed into technical change and efficiency change. In general, human capital is found to exert a positive effect on technical change and productivity change while efficiency change is left unaffected. Additionally, higher technical inefficiency tends to lead to lower the rate of innovation.', 'corpus_id': 153390455, 'score': 0}, {'doc_id': '195776094', 'title': 'Language2Pose: Natural Language Grounded Pose Forecasting', 'abstract': 'Generating animations from natural language sentences finds its applications in a a number of domains such as movie script visualization, virtual human animation and, robot motion planning. These sentences can describe different kinds of actions, speeds and direction of these actions, and possibly a target destination. The core modeling challenge in this language-to-pose application is how to map linguistic concepts to motion animations. In this paper, we address this multimodal problem by introducing a neural architecture called Joint Language-to-Pose (or JL2P), which learns a joint embedding of language and pose. This joint embedding space is learned end-to-end using a curriculum learning approach which emphasizes shorter and easier sequences first before moving to longer and harder ones. We evaluate our proposed model on a publicly available corpus of 3D pose data and human-annotated sentences. Both objective metrics and human judgment evaluation confirm that our proposed approach is able to generate more accurate animations and are deemed visually more representative by humans than other data driven approaches.', 'corpus_id': 195776094, 'score': 1}, {'doc_id': '52838251', 'title': 'Language-driven synthesis of 3D scenes from scene databases', 'abstract': ""We introduce a novel framework for using natural language to generate and edit 3D indoor scenes, harnessing scene semantics and text-scene grounding knowledge learned from large annotated 3D scene databases. The advantage of natural language editing interfaces is strongest when performing semantic operations at the sub-scene level, acting on groups of objects. We learn how to manipulate these sub-scenes by analyzing existing 3D scenes. We perform edits by first parsing a natural language command from the user and transforming it into a semantic scene graph that is used to retrieve corresponding sub-scenes from the databases that match the command. We then augment this retrieved sub-scene by incorporating other objects that may be implied by the scene context. Finally, a new 3D scene is synthesized by aligning the augmented sub-scene with the user's current scene, where new objects are spliced into the environment, possibly triggering appropriate adjustments to the existing scene arrangement. A suggestive modeling interface with multiple interpretations of user commands is used to alleviate ambiguities in natural language. We conduct studies comparing our approach against both prior text-to-scene work and artist-made scenes and find that our method significantly outperforms prior work and is comparable to handmade scenes even when complex and varied natural sentences are used."", 'corpus_id': 52838251, 'score': 1}, {'doc_id': '224704637', 'title': 'Learning to Reconstruct and Segment 3D Objects', 'abstract': 'To endow machines with the ability to perceive the real-world in a three dimensional representation as we do as humans is a fundamental and long-standing topic in Artificial Intelligence. Given different types of visual inputs such as images or point clouds acquired by 2D/3D sensors, one important goal is to understand the geometric structure and semantics of the 3D environment. Traditional approaches usually leverage hand-crafted features to estimate the shape and semantics of objects or scenes. However, they are difficult to generalize to novel objects and scenarios, and struggle to overcome critical issues caused by visual occlusions. By contrast, we aim to understand scenes and the objects within them by learning general and robust representations using deep neural networks, trained on large-scale real-world 3D data. To achieve these aims, this thesis makes three core contributions from object-level 3D shape estimation from single or multiple views to scene-level semantic understanding.', 'corpus_id': 224704637, 'score': 0}]"
114	{'doc_id': '28605369', 'title': 'Origin of physical degradation in AlGaN/GaN on Si high electron mobility transistors under reverse bias stressing', 'abstract': 'We have investigated the role of threading dislocations in pit formation during stressing of AlGaN/GaN on Si high electron mobility transistors under high reverse bias. Upon stressing, the drain current saturation (ID-saturation) decreases over time. The amount of ID-saturation degradation correlates well with pit formation at the gate-edge, where the electric field is the highest. Using a transmission electron microscope weak-beam technique, it is found that pits tend to nucleate at threading dislocations that have a screw component, even when these dislocations are at locations away from the gate-edge. An explanation based on an electrochemical oxidation model is proposed.', 'corpus_id': 28605369}	10405	[{'doc_id': '174820600', 'title': 'Electrochemical Oxidation in AlGaN/GaN-on-Si High Electron Mobility Transistors', 'abstract': 'Pit formation via electrochemical oxidation has been identified as one of the critical degradation mechanisms in AlGaN/GaN high electron mobility transistors. It is known to be strongly influenced by the electric field, temperature and stressing environment. In this paper, we will extend this understanding to include the role of threading dislocations, current density and the density of the passivation layer.', 'corpus_id': 174820600, 'score': 1}, {'doc_id': '221266265', 'title': 'Gate-controlled field emission current from MoS$_2$ nanosheets', 'abstract': 'Monolayer molybdenum disulfide (MoS$_2$) nanosheets, obtained via chemical vapor deposition onto SiO$_2$/Si substrates, are exploited to fabricate field-effect transistors with n-type conduction, high on/off ratio, steep subthreshold slope and good mobility. The transistor channel conductance increases with the reducing air pressure due to oxygen and water desorption. Local field emission measurements from the edges of the MoS$_2$ nanosheets are performed in high vacuum using a tip-shaped anode. It is demonstrated that the voltage applied to the Si substrate back-gate modulates the field emission current. Such a finding, that we attribute to gate-bias lowering of the MoS$_2$ electron affinity, enables a new field-effect transistor based on field emission.', 'corpus_id': 221266265, 'score': 0}, {'doc_id': '104562707', 'title': 'Influence of substrate nitridation on the threading dislocation density of GaN grown on 200 mm Si (111) substrate', 'abstract': 'Abstract High quality GaN was grown on 200\u202fmm Si (111) substrates by using AlN and 3 step-graded AlxGa1-xN as the buffer layer in a metalorganic chemical vapor deposition system. We have investigated the influence of NH3 pre-flow time on the threading dislocation density (TDD) of AlN, AlGaN buffer layers and GaN layers. It was observed that the compressive stress introduced into the buffer layer and GaN is dependent on the nitridation time. The lowest TDD for GaN obtained in our samples was ~1\u202f×\u202f109\u202fcm−2 for screw type and 3.2\u202f×\u202f109 cm−2 for edge type dislocations, as obtained from atomic force microscopy and further confirmed by high resolution X-ray diffraction analysis. The threading dislocations generated in the first buffer layer (AlN) during its nucleation are found to influence the TDD in the subsequent layers. Samples without an intentional nitridation step exhibit higher TDD compared to the samples with optimal nitridation time. Longer nitridation time also leads to poor crystalline quality likely because of amorphous SiNx formation at the interface.', 'corpus_id': 104562707, 'score': 1}, {'doc_id': '222173489', 'title': 'N-polar GaN/AlN resonant tunneling diodes', 'abstract': 'N-polar GaN/AlN resonant tunneling diodes are realized on single-crystal N-polar GaN bulk substrate by plasma-assisted molecular beam epitaxy growth. The room-temperature current-voltage characteristics reveal a negative differential conductance (NDC) region with a peak tunneling current of 6.8$\\pm$ 0.8 kA/cm$^2$ at a forward bias of ~8 V. Under reverse bias, the polarization-induced threshold voltage is measured at ~$-$4 V. These resonant and threshold voltages are well explained with the polarization field which is opposite to that of the metal-polar counterpart, confirming the N-polarity of the RTDs. When the device is biased in the NDC-region, electronic oscillations are generated in the external circuit, attesting to the robustness of the resonant tunneling phenomenon. In contrast to metal-polar RTDs, N-polar structures have the emitter on the top of the resonant tunneling cavity. As a consequence, this device architecture opens up the possibility of seamlessly interfacing$-$via resonant tunneling injection$-$a wide range of exotic materials with III-nitride semiconductors, providing a route to explore new device physics.', 'corpus_id': 222173489, 'score': 0}, {'doc_id': '221172915', 'title': 'Performance Enhancement of Thin-Film Tunneling FETs by Drain Material Engineering Using an Ultra-Thin SiGe', 'abstract': 'In this work, the drain structure of a p-type thin-film tunneling FET is engineered to get better performance. An ultra-thin SiGe along with Si is used in the drain of silicon-based TFET. Two structures are compared with conventional TFET, one, SiGe is located on the top of Si in the drain and another one in reverse. Simulations approve these structures can reduce sub-threshold swing, OFF-current several times, and increase the ON-OFF ratio. Band diagram for conduction and valance bands are investigated and band to band tunneling (BTBT) generation rate is used to find better performance. We find current flows at Si in the drain with the wider bandgap. Ge mole fraction of SiGe is varied and its effects on the performance of TFET are studied. The SiGe thickness for both structures is explored to obtain the best thickness for SiGe.', 'corpus_id': 221172915, 'score': 0}, {'doc_id': '224818443', 'title': 'Role of ALD Al2O3 Surface Passivation on the Performance of p-Type Cu2O Thin Film Transistors.', 'abstract': 'High-performance p-type oxide thin film transistors (TFTs) have great potential for many semiconductor applications. However, these devices typically suffer from low hole mobility and high off-state currents. We fabricated p-type TFTs with a phase-pure polycrystalline Cu2O semiconductor channel grown by atomic layer deposition (ALD). The TFT switching characteristics were improved by applying a thin ALD Al2O3 passivation layer on the Cu2O channel, followed by vacuum annealing at 300 °C. Detailed characterization by transmission electron microscopy-energy dispersive X-ray analysis and X-ray photoelectron spectroscopy shows that the surface of Cu2O is reduced following Al2O3 deposition and indicates the formation of a 1-2 nm thick CuAlO2 interfacial layer. This, together with field-effect passivation caused by the high negative fixed charge of the ALD Al2O3, leads to an improvement in the TFT performance by reducing the density of deep trap states as well as by reducing the accumulation of electrons in the semiconducting layer in the device off-state.', 'corpus_id': 224818443, 'score': 0}, {'doc_id': '35477667', 'title': 'Role of two-dimensional electron gas (2DEG) in AlGaN/GaN high electron mobility transistor (HEMT) ON-state degradation', 'abstract': 'Abstract We have investigated the influence of the two-dimensional electron gas (2DEG) in AlGaN/GaN high electron mobility transistors (HEMTs) on their reliability under ON-state conditions. Devices stressed in the ON-state showed a faster decrease in the maximum drain current (I Dmax ) compared to identical devices stressed in the OFF-state with a comparable electric field and temperature. Scanning electron microscope (SEM) images of ON-state stressed devices showed pit formation at locations away from the gate-edge in the drain-gate access region. Cross-sectional transmission electron microscope (TEM) images also showed dark features at the AlGaN/SiN interface away from the gate edge. Electron energy loss spectroscopy (EELS) analysis of the dark features indicated the presence of gallium, aluminum and oxygen. These dark features correlate with pits observed in the SEM micrographs. It is proposed that in addition to causing joule heating, energetic electrons in the 2D electron gas contribute to device degradation by promoting electrochemical oxidation of the AlGaN.', 'corpus_id': 35477667, 'score': 1}, {'doc_id': '99351612', 'title': 'Threading dislocation movement in AlGaN/GaN-on-Si high electron mobility transistors under high temperature reverse bias stressing', 'abstract': 'Dislocations are known to be associated with both physical and electrical degradation mechanisms of AlGaN/GaN-on-Si high electron mobility transistors (HEMTs). We have observed threading dislocation movement toward the gate-edges in AlGaN/GaN-on-Si HEMT under high reverse bias stressing. Stressed devices have higher threading dislocation densities (i.e. ∼5 × 109/cm2) at the gate-edges, as compared to unstressed devices (i.e. ∼2.5 × 109/cm2). Dislocation movement correlates well with high tensile stress (∼1.6 GPa) at the gate-edges, as seen from inverse piezoelectric calculations and x-ray synchrotron diffraction residual stress measurements. Based on Peierls stress calculation, we believe that threading dislocations move via glide in 〈112¯0〉/{11¯00} and 〈112¯0〉/{11¯01} slip systems. This result illustrates the importance of threading dislocation mobility in controlling the reliability of AlGaN/GaN-on-Si HEMTs.', 'corpus_id': 99351612, 'score': 1}, {'doc_id': '13617792', 'title': 'Improved reliability of AlGaN/GaN-on-Si high electron mobility transistors (HEMTs) with high density silicon nitride passivation', 'abstract': 'Abstract We have systematically studied the effects of SixN1\xa0−\xa0x passivation density on the reliability of AlGaN/GaN high electron mobility transistors. Upon stressing, devices degrade in two stages, fast-mode degradation and followed by slow-mode degradation. Both degradations can be explained as different stages of pit formation at the gate-edge. Fast-mode degradation is caused by pre-existing oxygen at the SixN1\xa0−\xa0x/AlGaN interface. It is not significantly affected by the SixN1\xa0−\xa0x density. On the other hand, slow-mode degradation is associated with SixN1\xa0−\xa0x degradation. SixN1\xa0−\xa0x degrades through electric-field induced oxidation in discrete locations along the gate-edges. The size of these degraded locations ranged from 100 to 300\xa0nm from the gate edge. There are about 16 degraded locations per 100\xa0μm gate-width. In each degraded location, low density nano-globes are formed within the SixN1\xa0−\xa0x. Because of the low density of the degraded locations, oxygen can diffuse through these areas and oxidize the AlGaN/GaN to form pits. This slow-mode degradation can be minimized by using high density (ρ\xa0=\xa02.48\xa0g/cm3) Si36N64 as the passivation layer. For slow-mode degradation, the median time to failure of devices with high density passivation is found to increase up to 2× as compared to the low density (ρ\xa0=\xa02.25\xa0g/cm3) Si43N57 passivation. A model based on Johnson-Mehl-Avrami theory is proposed to explain the kinetics of pit formation.', 'corpus_id': 13617792, 'score': 1}, {'doc_id': '221265989', 'title': 'Indium-Tin-Oxide Transistors with One Nanometer Thick Channel and Ferroelectric Gating.', 'abstract': 'In this work, we demonstrate high performance indium-tin-oxide (ITO) transistors with the channel thickness down to 1 nm and ferroelectric Hf0.5Zr0.5O2 as gate dielectric. On-current of 0.243 A/mm is achieved on sub-micron gate-length ITO transistors with a channel thickness of 1 nm, while it increases to as high as 1.06 A/mm when the channel thickness increases to 2 nm. A raised source/drain structure with a thickness of 10 nm is employed, contributing to a low contact resistance of 0.15 Ω⋅mm and a low contact resistivity of 1.1×10-7 Ω⋅cm2. The ITO transistor with a recessed channel and ferroelectric gating demonstrates several unique advantages over 2D semiconductor transistors and other thin film transistors, including large-area wafer-size nanometer thin film formation, low contact resistance and contact resistivity, atomic thin channel being immunity to short channel effects, large gate modulation of high carrier density by ferroelectric gating, high-quality gate dielectric and passivation formation, and a large bandgap for the low-power back-end-of-line (BEOL) CMOS application.', 'corpus_id': 221265989, 'score': 0}]
115	{'doc_id': '232223576', 'title': 'ASPECTS OF STORAGE AT ROME', 'abstract': 'imperial freedmen and the increase in their wealth, they became a new target for the senatorial elite during the Principate. Chapter 7, by C.J. Berry, inspects the integration of the ideal of frugalitas into Christian thought and its repercussions in post-Classical times, in the cases of David Hume and Adam Smith’s recalibrations of frugality, which focus on the economic benefits of frugal resource management. This book makes a distinctive contribution and is an original and engaging volume employing multidisciplinary evidence related to Roman frugality and modes of moderation. Thus, it is greatly welcomed and much recommended. It is not the aim and scope of this book to provide an account of frugal ideology from late antiquity onwards (p. 107). However, there is a future wish. The last decade has yielded immense material evidence (pottery, glass, coin finds) regarding the late antique economy. This forces us to re-think established concepts such as resilience, transformation of the empire and so on. One decline, however, was recorded in the epigraphic habit from the fourth century AD onwards, especially in funerary inscriptions. Could we simply relate this to the general economic change affecting the use of marble? What was the impact of Christian thought and ideology on funerary customs? Modesty, simplicity? More concern about the afterlife than about the mundane world? Thus, further study on Christianity and its impact (if there is one) on consumption culture and economic behaviour in general are suggested (for some preliminary indications, see pp. 101–7).', 'corpus_id': 232223576}	15857	[{'doc_id': '165994848', 'title': 'Images of Vestal Virgins, and questions about interpreting the evidence', 'abstract': 'The virgin priestesses of Vesta (virgines Vestales) have long fascinated classicists and the general public alike. That an empire as mighty and long-lasting as Rome’s symbolically based its survival on a group of women and their bodies’ integrity required an explanation. Given the utmost religious and political importance of both cult and priestesses over a span of more than a thousand years, it is striking how fragmentary and scattered a historical record they have left behind. It is equally remarkable that attempts at a comprehensive study that includes archaeological and visual material have been published only in the present millennium, M. Lindner’s book being the most recent of them. Since it is based on her University of Michigan dissertation of 1996, the author should be credited for being among the first to have drawn attention to a long-neglected body of material; besides the architecture of the atrium Vestae, it includes various sculptural remains found therein (most importantly, portraits of the Vestals), additional archival sources (she obtained access to unpublished reports from R. Lanciani’s excavations in the late 19th c.), and other depictions on reliefs and coins. In the 19 years since the dissertation’s completion scholarship on the Vestals has flourished. Sophisticated and theoretically-informed historical and philological studies on the Vestals’ sexual and political status have appeared.1 The epigraphic and archaeological evidence has also been systematically studied, preparing material that Lindner did not consider at the time but which she could draw on here.2 However, Lindner regrettably misses the opportunity to reconsider some of her arguments, despite serious methodological objections that had already been voiced.3 In the following, I will give a short overview of the book’s content before addressing some of the major concerns.', 'corpus_id': 165994848, 'score': 1}, {'doc_id': '233271202', 'title': 'Editorial: Understanding the Conundrum Secular States, Fundamentalist Politics', 'abstract': 'The theme of this special issue of Feminist Dissent focuses on the ways in which religious fundamentalist movements have become hegemonic in many secular states around the world. This purported paradox of fundamentalist politics gaining power in secular states is all the more challenging to analyse in the context of both the consolidation and re-articulation of neoliberalism as an ideology and framework for organising economy and society in the era of late capitalism and its successive crises. Specifically, we are interested in exploring the ways in which these transformations within state, society and the economy have affected women’s positions and gender relations. The illustrative case studies we examine in this issue are India, Israel and Turkey.', 'corpus_id': 233271202, 'score': 0}, {'doc_id': '232281755', 'title': 'Book Review: Kees Boterbloem: Russia as Empire: Past and Present and Robin Milner-Gulland: Patterns of Russia: History, Culture, Spaces', 'abstract': 'transformations of the notion of historical progress alongside conceptions of the origins and processes of modernity. The core of her book concerns five fundamental presuppositions on which the world view of the ‘Modern Time Regime’ rest, namely ‘rupture’ (p. 93), ‘the fiction of beginning’ (p. 105), ‘creative destruction’ (p. 116), ‘destroying and preserving’ (p. 126) and ‘acceleration’ (p. 135). Having critically examined their complex interrelation, Assmann challenges herself and her readers not only to recognize that ‘Hamlet’s old claim that “the time is out of joint” today means that something indeed seems to be wrong in the relation between the temporal registers of past, present, and future’ (p. 190), but also to consider how we might overcome this, at least in an historiographical sense, without overreliance on an alarmist and ambiguous discourse of finger-pointing. For Assmann, repairing the ‘Modern Time Regime’ must concern the recentring of three elements in society: culture, identity and memory. After all, she affirms, ‘the temporal structure of the past, present, and future has collapsed and cannot simply be reproduced, but rather must be newly joined together’ (p. 229). If, as Bruno Latour once suggested, modernism should be recalled like a defective industrial product, then it is important to remember that when any product is recalled, this is not done with a view to disposing of it, but rather improving it. In a world still reeling from the ongoing Covid-19 pandemic, Assmann’s advocation that such a reconfiguration become ‘not the task of individual theorists of time but of society as a whole’ (p. 229) has never sounded more urgent.', 'corpus_id': 232281755, 'score': 0}, {'doc_id': '189594602', 'title': 'The Vestal Virgins', 'abstract': None, 'corpus_id': 189594602, 'score': 1}, {'doc_id': '232115721', 'title': 'Karl Marx and the Birth of Modern Society: The Life of Marx and the Development of His Work', 'abstract': 'Michael Heinrich’s five-volume opus may be the most definitive and comprehensive biography of Marx ever written, and it may turn out to be the best ever written. It could have been subtitled ‘‘everything you wanted to know about Marx and didn’t have years to research.’’ Heinrich’s biography reveals things about the life of Marx that others don’t. It is a bit difficult for a sociologist to summarize the text, given the differences between a historical biography, rich in minute details, and a sociological perspective, more focused on larger-scale events or processes. Such a biography is based on extensive research of various archives, letters, and events, as well as critiques of other, earlier biographies. Moreover, the intellectual context of Marx’s embrace of philosophy, from the pre-Socratics of his dissertation to German Idealism, especially Hegel, Bauer, and the like, and the many writings and fragments in the Marx-Engels-Gesamtausgabe (MEGA) may not be too familiar to many. But given current conditions and the growing interest in Marx, Heinrich’s Karl Marx and the Birth of Modern Society: The Life of Marx and the Development of His Work— showing the importance of a radical critique of everything and informed by a fervent embrace of freedom—is essential reading. ‘‘This book is not concerned with a cult of personality . . . but the historical process in which Karl Marx developed as a person, as a theorist, as a political activist, and as a revolutionary’’ and will consider his ‘‘school days, his attempts at poetry, his engagement with religion and the philosophy of religion’’ (pp. 9–10). Heinrich informs the reader of the historical and political-economic contexts of Trier, where young Marx grew up; Prussia, both from within and without; the impact of Lutheranism; and the tensions between the legacies of dynastic tradition and forces of modernity, from Rousseau’s Discourse on Inequality to Kant on ‘‘Enlightenment.’’ And then came the rise of Napoleon, trade with France, wars with Prussia, and the spread of rational, bourgeois law, typically opposed by monarchs. Along with the events of 1776, 1789, and 1848 and the beginnings of capitalist industrialization, Marx was the product of emerging capitalist modernity as well as its witness and, eventually, its most trenchant critic. For the sake of clarity, if not brevity, I would suggest that we look at five intertwined themes of Marx’s life.', 'corpus_id': 232115721, 'score': 0}, {'doc_id': '133065485', 'title': 'Some Observations on the Worship of Vesta, and the Holy Fire, in Ancient Rome: with an Account of the Vestal Virgins.', 'abstract': None, 'corpus_id': 133065485, 'score': 1}, {'doc_id': '233282077', 'title': 'Discovering Neverland: São Tomé and Príncipe and the development of the agricultural heritage of a multi-ethnic population', 'abstract': 'The history of São Tomé and Príncipe (STP) shows that the development of the roças, plantations established in colonial times, form a heritage linked to the human development of STP. Various agricultural products have characterized the historical periods of migration, slavery, creolization, and gender emancipation up to the present day; agricultural products and the history of creolization make STP unique, while the relationship between culture and nature provides a useful tool for a better understanding of its historical roots. The essay argues that STP’s sustainable development could be fostered by valorizing its historical agricultural heritage. Agrifood geographical indications (GIs), which directly link territories, peoples, and traditions could also serve this purpose. GIs could lead to raising the export price of STP’s cocoa, coffee, and pepper, at the same time increasing cultivation of a number of other crops, especially indigenous fruits, which are usually planted in combination. These systems have proven to lead to better prices for products and increase the specialist labour market; they could also foster a multi-faceted approach to territorial development, including eco-tourism. However, challenges remain, as the country is still lacking in proper infrastructures, skilled labour, management, and institutional support. The question will be whether STP’s fragile agri-food setting is able to support these new mechanisms, which require strong value chains, respect for territorial biodiversity, and a fresh look at the role played by small farmers running AgriSMEs.', 'corpus_id': 233282077, 'score': 0}, {'doc_id': '232264267', 'title': 'Follow the bodies: Global capitalism, global war, global crisis and feminist IPE', 'abstract': 'Global Capitalism, Global War, Global Crisis by Andreas Bieler and Adam Morton makes a persuasive case for the enduring relevance of historical materialism as the hermeneutical tool for analysis and understanding of contemporary political economy.1 The most interesting aspect of the book – and the one which sets it apart from other recent critical works in International Political Economy (IPE) in the same tradition – is the concept of ‘internal relations’, which allows Bieler and Morton to endogenise wars and crises within the capitalism itself.2 Arguing against the habitual dualisms in the discipline of International Relations – between states and markets, agents and structures, material conditions and ideologies – Bieler and Morton suggest instead that global capitalism, war and crisis should be analysed ‘in terms of their internality’.3 Their ‘relational method’ thus ‘captures capital’s internalisation through the states system of uneven and combined development, geopolitics and the global crisis conditions facing humanity that are themselves embedded within world ecology’.4 In this intervention, I would like to elaborate upon this ‘radical ontology’ of ‘internal relations’ from the perspective of feminist IPE. There are significant overlaps between feminist IPE and Bieler and Morton’s analysis in this book, none the least their effort to integrate what Nancy Fraser aptly calls ‘Marx’s hidden abode’ – the background conditions of expropriation and social reproduction – with Marx’s ‘front story’ of exploitation and capitalist production.5 However, while Bieler and Morton noticeably expand Marx’s framework to incorporate and relate aspects of the Gramscian ‘social factory’ within the world of class struggle and geopolitical contestations, feminists begin their analyses by looking at the world through the lens of gendered hierarchies and embodied (rather than', 'corpus_id': 232264267, 'score': 0}, {'doc_id': '217063024', 'title': 'The Veiled Exploitation of the Vestal Virgins', 'abstract': None, 'corpus_id': 217063024, 'score': 1}, {'doc_id': '229264989', 'title': 'Time and Eternity: The Vestal Virgins and the Crisis of the Third Century', 'abstract': None, 'corpus_id': 229264989, 'score': 1}]
116	{'doc_id': '221498179', 'title': 'Metabolic response of the Siberian wood frog Rana amurensis to extreme hypoxia', 'abstract': 'The Siberian wood frog Rana amurensis is a recently discovered example of extreme hypoxia tolerance that is able to survive several months without oxygen. We studied metabolomic profiles of heart and liver of R. amurensis exposed to 17 days of extreme hypoxia. Without oxygen, the studied tissues experience considerable stress with a drastic decrease of ATP, phosphocreatine, and NAD+\u2009concentrations, and concomitant increase of AMP, creatine, and NADH. Heart and liver switch to different pathways of glycolysis with differential accumulation of lactate, alanine, succinate, as well as 2,3-butanediol (previously not reported for vertebrates as an end product of glycolysis) and depletion of aspartate. We also observed statistically significant changes in concentrations of certain osmolytes and choline-related compounds. Low succinate/fumarate ratio and high glutathione levels indicate adaptations to reoxygenation stress. Our data suggest that maintenance of the ATP/ADP pool is not required for survival of R. amurensis, in contrast to anoxia-tolerant turtles.', 'corpus_id': 221498179}	20234	"[{'doc_id': '236998994', 'title': 'Rapid and reversible modulation of blood haemoglobin content during diel cycles of hypoxia in killifish (Fundulus heteroclitus).', 'abstract': 'We investigated whether fish can make dynamic haematological adjustments to support aerobic metabolism during repeated cycles of hypoxia-reoxygenation. Killifish were acclimated to normoxia, constant hypoxia (2\u202fkPa O2), or intermittent cycles of nocturnal hypoxia (12\u202fh of normoxia: 12\u202fh of 2\u202fkPa O2 hypoxia) for 28 days. Normoxia-acclimated fish were sampled in the daytime in normoxia and after exposure to a single bout of nocturnal hypoxia. Each hypoxia acclimation group were sampled at the PO2 experienced during acclimation during both the day and night. All acclimation groups had increased blood haemoglobin content and haematocrit and reduced spleen mass during nocturnal hypoxia compared to normoxic controls. Blood haemoglobin content was negatively correlated with spleen mass at both the individual and group level. Fish acclimated to intermittent hypoxia rapidly reversed these changes during diurnal reoxygenation. The concentrations of haemoglobin, ATP, and GTP within erythrocytes did not vary substantially between groups. We also measured resting O2 consumption rate (MO2) and maximum MO2 (induced by an exhaustive chase) in hypoxia in each acclimation group. Fish acclimated to intermittent hypoxia maintained higher resting MO2 than other groups in hypoxia, comparable to the resting MO2 of normoxia-acclimated controls measured in normoxia. Differences in resting MO2 in hypoxia did not result from variation in O2 transport capacity, because maximal MO2 in hypoxia always exceeded resting MO2. Therefore, reversible modulation of blood haemoglobin content along with metabolic adjustments help killifish cope with intermittent cycles of hypoxia in the estuarine environment.', 'corpus_id': 236998994, 'score': 1}, {'doc_id': '237344158', 'title': 'Metabolic responses of plasma to extreme environments in overwintering Tibetan frogs Nanorana parkeri: a metabolome integrated analysis', 'abstract': 'Many animals lower their metabolic rate in response to low temperatures and scarcity of food in the winter in phenomena called hibernation or overwintering. Living at high altitude on the Tibetan Plateau where winters are very cold, the frog Nanorana parkeri , survives in one of the most hostile environments on Earth but, to date, relatively little is known about the biochemical and physiological adjustments for overwintering by this species. The present study profiled changes in plasma metabolites of N. parkeri between winter and summer using UHPLC-QE-MS non-target metabolomics in order to explore metabolic adaptations that support winter survival. The analysis showed that, in total, 11 metabolites accumulated and 95 were reduced in overwintering frogs compared with summer-active animals. Metabolites that increased included some that may have antioxidant functions (canthaxanthin, galactinol), act as a metabolic inhibitor (mono-ethylhexylphthalate), or accumulate as a product of anaerobic metabolism (lactate). Most other metabolites in plasma showed reduced levels in winter and were generally involved in energy metabolism including 11 amino acids (proline, isoleucine, leucine, valine, phenylalanine, tyrosine, arginine, tryptophan, methionine, threonine and histidine) and 4 carbohydrates (glucose, citrate, succinate, and malate). Pathway analysis indicated that aminoacyl-tRNA biosynthesis, phenylalanine, tyrosine and tryptophan biosynthesis, and nitrogen metabolism were potentially the most prominently altered pathways in overwintering frogs. Changes to these pathways are likely due to fasting and global metabolic depression in overwintering frogs. Concentrations of glucose and urea, commonly used as cryoprotectants by amphibians that winter on land, were significantly reduced during underwater hibernation in N. parkeri . In conclusion, winter survival of the high-altitude frog, N. parkeri was accompanied by substantial changes in metabolomic profiles and this study provides valuable information towards understanding the special adaptive mechanisms of N. parkeri to winter stresses.', 'corpus_id': 237344158, 'score': 1}, {'doc_id': '237268060', 'title': 'African Naked Mole-Rats Demonstrate Extreme Tolerance to Hypoxia and Hypercapnia.', 'abstract': ""Naked mole-rats are extremely tolerant to low concentrations of oxygen (hypoxia) and high concentrations of carbon dioxide (hypercapnia), which is consistent with the environment that they inhabit. Naked mole-rats combine subterranean living with living in very densely populated colonies where oxygen becomes depleted and carbon dioxide accumulates. In the laboratory, naked mole-rats fully recover from 5\xa0h exposure to 5% O2 and 5\xa0h exposure to 80% CO2, whereas both conditions are rapidly lethal to similarly sized laboratory mice. During anoxia (0% O2) naked mole-rats enter a suspended animation-like state and switch from aerobic metabolism of glucose to anaerobic metabolism of fructose. Additional fascinating characteristics include that naked mole-rats show intrinsic brain tolerance to anoxia; a complete lack of hypoxia-induced and CO2-induced pulmonary edema; and reduced aversion to high concentrations of CO2 and acidic fumes. Here we outline a constellation of physiological and molecular adaptations that correlate with the naked mole-rat's hypoxic/hypercapnic tolerance and which offer potential targets for ameliorating pathological conditions in humans, such as the damage caused during cerebral ischemia."", 'corpus_id': 237268060, 'score': 1}, {'doc_id': '236031849', 'title': 'Correlation of tissue respiration and some mitochondria stereometric characteristics of the lung tissue in different modifications of hypoxic hypoxia', 'abstract': 'In experiments on the adult white laboratory rats the correlation of tissue respiration and some morpho- and stereometric characteristics of mitochondria in lung tissue under breathing by air and gas mixture with 7% O2 in N2 was investigated. The following agents were used as modulators: indomethacin, a blockator of cyclooxygenase way of arachidonic acid metabolism; quercetin and linoleil of hydroxamic acid, blockators of lipooxygenase way ofarachidonic acid metabolism; taurine, an antihypoxant and energy source under hypoxic conditions; lipin-antihypoxant with significant membrane protective effect. It was shown, that the respiration intensity of tissue homogenate, not only of its mitochondrial fraction, closely connected with structural organization of mitochondria. It was demonstrated that changes of O2 concentration in gas mixture lead to the alteration of interrelation between O2 consumption and stereometric characteristics of mitochondria: in normoxia the intimate correlation was established with number of mitochondria and its total surface; in hypoxia such correlation was established with mitochondria diameter and number of structurally damage organelles. Pharmacological modulation factors play in this process not so significant role.', 'corpus_id': 236031849, 'score': 0}, {'doc_id': '237411720', 'title': 'Hypoxia tolerance in two amazon cichlids: mitochondrial respiration and cellular metabolism adjustments are result of species environmental preferences and distribution.', 'abstract': ""The amazon fishes' responses to hypoxia seem to be related to the Amazon basin diversity of aquatic environments, which present drastic daily and seasonal variations in the dissolved oxygen concentration. Among these fishes' adaptation to hypoxia, behavioral, metabolic, physiological, and biochemical responses are well known for some species. In this work, we aimed to identify how two different aquatic environments, normoxic forest streams and hypoxic lakes, dictate the responses to hypoxia for two cichlid species, Mesonauta festivus and Aequidens pallidus. In our results, we found that A. pallidus is less tolerant to hypoxia, which seems to be related to this animal's natural normoxic environment. Even though this species modulated the mitochondrial respiration in order to improve the oxygen use, it also showed a lower decrease in metabolic rate when exposed to hypoxia and no activation of the anaerobic metabolism. Instead, M. festivus showed a higher decrease in metabolic rate and an activation of the anaerobic metabolism. Our data reveal that the natural dissolved oxygen influences the hypoxia tolerance and the species' tolerance is related to its ability to perform metabolic depression. The interest results are the absence of mitochondrial respiration influences in these processes. The results observed with A. pallidus bring to light also the importance of preserving the forests, in which streams hold very specialized species acclimated to normoxia and lower temperature. The importance of hypoxia tolerance is, thus, important to keep fish assemblage and is thought to be a strong driver of fish biodiversity."", 'corpus_id': 237411720, 'score': 1}, {'doc_id': '236637582', 'title': 'Indirect evidence that anoxia exposure and cold acclimation alters transarcolemmal Ca2+ flux in the cardiac pacemaker, right atrium and ventricle of the red-eared slider turtle (Trachemys scripta).', 'abstract': 'We indirectly assessed if altered transarcolemmal Ca2+ flux accompanies the decreased cardiac activity displayed by Trachemys scripta with anoxia exposure and cold acclimation. Turtles were first acclimated to 21\u202f°C or 5\u202f°C and held under normoxic (21\u202fN; 5\u202fN) or anoxic conditions (21A; 5A). We then compared the response of intrinsic heart rate (fH) and maximal developed force of spontaneously contracting right atria (Fmax,RA), and maximal developed force of isometrically-contracting ventricular strips (Fmax,V), to Ni2+ (0.1-10\u202fmM), which respectively blocks T-type Ca2+ channels, L-type Ca2+ channels and the Na+-Ca2+-exchanger at the low, intermediate and high concentrations employed. Dose-response curves were established in simulated in vivo normoxic (Sim Norm) or simulated in vivo anoxic extracellular conditions (21A and 5A preparations). Ni2+ decreased intrinsic fH, Fmax,RA and Fmax,V of 21\u202fN tissues in a concentration-dependent manner, but the responses were blunted in 21A tissues in Sim Norm. Similarly, dose-response curves for Fmax,RA and Fmax,V of 5\u202fN tissues were right-shifted, whereas anoxia exposure at 5\u202f°C did not further alter the responses. The influence of Sim Anx was acclimation temperature-, cardiac chamber- and contractile parameter-dependent. Combined, the findings suggest that: (1) reduced transarcolemmal Ca2+ flux in the cardiac pacemaker is a potential mechanism underlying the slowed intrinsic fH of anoxic turtles at 21\u202f°C, but not 5\u202f°C, (2) a downregulation of transarcolemmal Ca2+ flux may aid cardiac anoxia survival at 21\u202f°C and prime the turtle myocardium for winter anoxia and (3) confirm that altered extracellular conditions with anoxia exposure can modify turtle cardiac transarcolemmal Ca2+ flux.', 'corpus_id': 236637582, 'score': 1}, {'doc_id': '237440021', 'title': 'Skeletal muscle Histidine containing dipeptide contents are increased in freshwater turtles (C. picta bellii) with cold-acclimation.', 'abstract': ""Freshwater turtles found in higher latitudes can experience extreme challenges to acid-base homeostasis while overwintering, due to a combination of cold temperatures along with the potential for environmental hypoxia. Histidine containing dipeptides (HCDs; carnosine, anserine and balenine) may facilitate pH regulation in response to these challenges, through their role as pH buffers. We measured the HCD content of three tissues (liver, cardiac and skeletal muscle) from the anoxia-tolerant painted turtle (C. picta bellii) acclimated to either 3 or 20\u202f°C. HCDs were detected in all tissues, with the highest content shown in the skeletal muscle. Turtles acclimated to 3\u202f°C had more HCD in their skeletal muscle than those acclimated to 20\u202f°C (carnosine\u202f=\u202f20.8\u202f±\u202f4.5 vs 12.5\u202f±\u202f5.9\u202fmmol·kg DM-1; ES\u202f=\u202f1.59 (95%CI: 0.16-3.00), P\u202f=\u202f0.013). The higher HCD content shown in the skeletal muscle of the cold-acclimated turtles suggests a role in acid-base regulation in response to physiological challenges associated with living in the cold, with the increase possibly related to the temperature sensitivity of carnosine's dissociation constant."", 'corpus_id': 237440021, 'score': 0}, {'doc_id': '237455657', 'title': 'Exposure to low temperature prepares the turtle brain to withstand anoxic environments during overwintering.', 'abstract': 'In most vertebrates, anoxia drastically reduces the production of the essential adenosine triphosphate (ATP) to power its many necessary functions, and consequently, cell death occurs within minutes. However, some vertebrates, such as the painted turtle (Chrysemys picta bellii), have evolved the ability to survive months without oxygen by simultaneously decreasing ATP supply and demand, surviving the anoxic period without any apparent cellular damage. The impact of anoxia on the metabolic function of painted turtles has received a lot of attention. Still, the impact of low temperature has received less attention and the interactive effect of anoxia and temperature even less. In the present study, we investigated the interactive impacts of reduced temperature and severe hypoxia on the electrophysiological properties of pyramidal neurons in painted turtle cerebral cortex. Our results show that an acute reduction in temperature from 20 to 5°C decreases membrane potential, action potential width and amplitude, and whole-cell conductance. Importantly, acute exposure to 5°C considerably slows membrane repolarization by voltage-gated K+ channels. Exposing pyramidal cells to severe hypoxia in addition to an acute temperature change slightly depolarized membrane potential but did not alter action potential amplitude or width and whole-cell conductance. These results suggest that acclimation to low temperatures, preceding severe environmental hypoxia, induces cellular responses in pyramidal neurons that facilitate survival under low oxygen concentration. In particular, our results show that temperature acclimation invokes a change in voltage-gated K+ channel kinetics that overcomes the acute inhibition of the channel.', 'corpus_id': 237455657, 'score': 1}, {'doc_id': '237279867', 'title': 'Hypoxic acclimation improves mitochondrial bioenergetic function in large yellow croaker Larimichthys crocea under Cu stress.', 'abstract': 'The purpose of this study was to investigate how pre-hypoxia exposure affected the mitochondrial structure and bioenergetic function of large yellow croaker in responding to Cu stress. Fish were acclimated to normoxia and 3.0\xa0mg DO L-1 for 48\xa0h, then subjected to 0 and 120\xa0μg Cu L-1 for another 48\xa0h. Hypoxic acclimation did not affect mitochondrial ultrastructure and reactive oxygen species (ROS), but reduced oxidative phosphorylation (OXPHOS) efficiency. Cu exposure impaired mitochondrial ultrastructure, increased ROS generation and inhibited OXPHOS efficiency. Compared with Cu exposure alone, hypoxic acclimation plus Cu exposure reduced ROS production and improved OXPHOS efficiency by enhancing mitochondrial respiratory control ratio, mitochondrial membrane potential, and activities and gene expressions of electron transport chain enzymes. In conclusion, hypoxic acclimation improved the mitochondrial energy metabolism of large yellow croaker under Cu stress, facilitating our understanding of the molecular mechanisms regarding adaptive responses of hypoxia-acclimated fish under Cu stress.', 'corpus_id': 237279867, 'score': 0}, {'doc_id': '236092820', 'title': 'Aerobic and anaerobic movement energetics of hybrid and pure parental abalone.', 'abstract': 'The underlying mechanisms controlling growth heterosis in marine invertebrates remain poorly understood. We used pure blacklip (Haliotis rubra) and greenlip (Haliotis laevigata) abalone, as well as their hybrid, to test whether differences in movement and/or aerobic versus anaerobic energy use are linked to a purported increased growth rate in hybrids. Abalone were acclimated to control (16\xa0°C) and typical summer temperatures (23\xa0°C), each with oxygen treatments of 100% air saturation (O2sat) or 70% O2sat. The experiment then consisted of two phases. During the first phase (chronic exposure), movement and oxygen consumption rates (ṀO2) of abalone were measured during a 2\xa0day observation period at stable acclimation conditions. Additionaly, lactate dehydrogenase (LDH) and tauropine dehydrogenase (TDH) activities were measured. During phase two (acute exposure), O2sat was raised to 100% for abalone acclimated to 70% O2sat followed by an acute decrease in oxygen to anoxia for all acclimation groups during which movement and ṀO2 were determined again. During the chronic exposure, hybrids and H. laevigata moved shorter distances than H. rubra. Resting ṀO2, LDH and TDH activities, however, were similar between abalone types but were increased at 23\xa0°C compared to 16\xa0°C. During the acute exposure, the initial increase to 100% O2sat for individuals acclimated to 70% O2sat resulted in increased movement compared to individuals acclimated to 100% O2sat for hybrids and H. rubra when compared within type of abalone. Similarly, ṀO2 during spontaneous activity of all three types of abalone previously subjected to 70% O2sat increased above those at 100% O2sat. When oxygen levels had dropped below the critical oxygen level (Pcrit), movement in hybrids and H. laevigata increased up to 6.5-fold compared to movement above Pcrit. Differences in movement and energy use between hybrids and pure species were not marked enough to support the hypothesis that the purportedly higher growth in hybrids is due to an energetic advantage over pure species.', 'corpus_id': 236092820, 'score': 0}]"
117	{'doc_id': '4558131', 'title': 'Fully automatic detection and segmentation of abdominal aortic thrombus in post‐operative CTA images using Deep Convolutional Neural Networks', 'abstract': 'HIGHLIGHTSA DCNN‐based fully automatic thrombus detection and segmentation pipeline that is easily translatable to clinical practice is proposed.A new DCNN architecture adapted to post‐operative thrombus segmentation from CTA images is presented, which combines low level features with coarser representations.The well‐known Detectnet computer vision network is translated into the clinical domain, specifically for thrombus region of interest detection in CTA images.Automatic segmentation exceeds previous state of the art results, with a mean Dice similarity coefficient of 82%.In terms of clinical applicability, the obtained segmentation results lay within the experienced human observer variance without the need of human intervention in most common cases. ABSTRACT Computerized Tomography Angiography (CTA) based follow‐up of Abdominal Aortic Aneurysms (AAA) treated with Endovascular Aneurysm Repair (EVAR) is essential to evaluate the progress of the patient and detect complications. In this context, accurate quantification of post‐operative thrombus volume is required. However, a proper evaluation is hindered by the lack of automatic, robust and reproducible thrombus segmentation algorithms. We propose a new fully automatic approach based on Deep Convolutional Neural Networks (DCNN) for robust and reproducible thrombus region of interest detection and subsequent fine thrombus segmentation. The DetecNet detection network is adapted to perform region of interest extraction from a complete CTA and a new segmentation network architecture, based on Fully Convolutional Networks and a Holistically‐Nested Edge Detection Network, is presented. These networks are trained, validated and tested in 13 post‐operative CTA volumes of different patients using a 4‐fold cross‐validation approach to provide more robustness to the results. Our pipeline achieves a Dice score of more than 82% for post‐operative thrombus segmentation and provides a mean relative volume difference between ground truth and automatic segmentation that lays within the experienced human observer variance without the need of human intervention in most common cases.', 'corpus_id': 4558131}	18504	[{'doc_id': '235700910', 'title': 'Automated Left Ventricle Ischemic Scar Detection in CT Using Deep Neural Networks', 'abstract': 'Objectives: The aim of this study is to develop a scar detection method for routine computed tomography angiography (CTA) imaging using deep convolutional neural networks (CNN), which relies solely on anatomical information as input and is compatible with existing clinical workflows. Background: Identifying cardiac patients with scar tissue is important for assisting diagnosis and guiding interventions. Late gadolinium enhancement (LGE) magnetic resonance imaging (MRI) is the gold standard for scar imaging; however, there are common instances where it is contraindicated. CTA is an alternative imaging modality that has fewer contraindications and is faster than Cardiovascular magnetic resonance imaging but is unable to reliably image scar. Methods: A dataset of LGE MRI (200 patients, 83 with scar) was used to train and validate a CNN to detect ischemic scar slices using segmentation masks as input to the network. MRIs were segmented to produce 3D left ventricle meshes, which were sampled at points along the short axis to extract anatomical masks, with scar labels from LGE as ground truth. The trained CNN was tested with an independent CTA dataset (25 patients, with ground truth established with paired LGE MRI). Automated segmentation was performed to provide the same input format of anatomical masks for the network. The CNN was compared against manual reading of the CTA dataset by 3 experts. Results: Note that 84.7% cross-validated accuracy (AUC: 0.896) for detecting scar slices in the left ventricle on the MRI data was achieved. The trained network was tested against the CTA-derived data, with no further training, where it achieved an 88.3% accuracy (AUC: 0.901). The automated pipeline outperformed the manual reading by clinicians. Conclusion: Automatic ischemic scar detection can be performed from a routine cardiac CTA, without any scar-specific imaging or contrast agents. This requires only a single acquisition in the cardiac cycle. In a clinical setting, with near zero additional cost, scar presence could be detected to triage images, reduce reading times, and guide clinical decision-making.', 'corpus_id': 235700910, 'score': 0}, {'doc_id': '195820361', 'title': 'DeepAAA: clinically applicable and generalizable detection of abdominal aortic aneurysm using deep learning', 'abstract': 'We propose a deep learning-based technique for detection and quantification of abdominal aortic aneurysms (AAAs). The condition, which leads to more than 10,000 deaths per year in the United States, is asymptomatic, often detected incidentally, and often missed by radiologists. Our model architecture is a modified 3D U-Net combined with ellipse fitting that performs aorta segmentation and AAA detection. The study uses 321 abdominal-pelvic CT examinations performed by Massachusetts General Hospital Department of Radiology for training and validation. The model is then further tested for generalizability on a separate set of 57 examinations with differing patient demographics and acquisition characteristics than the original dataset. DeepAAA achieves high performance on both sets of data (sensitivity/specificity 0.91/0.95 and 0.85 / 1.0 respectively), on contrast and non-contrast CT scans and works with image volumes with varying numbers of images. We find that DeepAAA exceeds literature-reported performance of radiologists on incidental AAA detection. It is expected that the model can serve as an effective background detector in routine CT examinations to prevent incidental AAAs from being missed.', 'corpus_id': 195820361, 'score': 1}, {'doc_id': '221178714', 'title': 'Abdominal Aortic Aneurysm Segmentation from Contrast-Enhanced Computed Tomography Angiography Using Deep Convolutional Networks', 'abstract': 'One of the most common imaging methods for diagnosing an abdominal aortic aneurysm, and an endoleak detection is computed tomography angiography. In this paper, we address the problem of aorta and thrombus semantic segmentation, what is a mandatory step to estimate aortic aneurysm diameter. Three end-to-end convolutional neural networks were trained and evaluated. Finally, we proposed an ensemble of deep neural networks with underlying U-Net, ResNet, and VBNet frameworks. Our results show that we are able to outperform state-of-the-art methods by 3% on the Dice metric without any additional post-processing steps.', 'corpus_id': 221178714, 'score': 1}, {'doc_id': '235226706', 'title': 'Comparing methods of detecting and segmenting unruptured intracranial aneurysms on TOF-MRAS: The ADAM challenge', 'abstract': 'Accurate detection and quantification of unruptured intracranial aneurysms (UIAs) is important for rupture risk assessment and to allow an informed treatment decision to be made. Currently, 2D manual measures used to assess UIAs on Time-of-Flight magnetic resonance angiographies (TOF-MRAs) lack 3D information and there is substantial inter-observer variability for both aneurysm detection and assessment of aneurysm size and growth (Forbes et al., 1996; Kim et al., 2017; White et al., 2000). 3D measures could be helpful to improve aneurysm detection and quantification but are time-consuming and would therefore benefit from a reliable automatic UIA detection and segmentation method. The Aneurysm Detection and segMentation (ADAM) challenge was organised in which methods for automatic UIA detection and segmentation were developed and submitted to be evaluated on a diverse clinical TOF-MRA dataset. A training set (113 cases with a total of 129 UIAs) was released, each case including a TOF-MRA, a structural MR image (T1, T2 or FLAIR), annotation of any present UIA(s) and the centre voxel of the UIA(s). A test set of 141 cases (with 153 UIAs) was used for evaluation. Two tasks were proposed: (1) detection and (2) segmentation of UIAs on TOF-MRAs. Teams developed and submitted containerised methods to be evaluated on the test set. Task 1 was evaluated using metrics of sensitivity and false positive count. Task 2 was evaluated using dice similarity coefficient, modified hausdorff distance (95th percentile) and volumetric similarity. For each task, a ranking was made based on the average of the metrics. In total, eleven teams participated in task 1 and nine of those teams participated in task 2. Task 1 was won by a method specifically designed for the detection task (i.e. not participating in task 2). Based on segmentation metrics, the top two methods for task 2 performed statistically significantly better than all other methods. The detection performance of the top-ranking methods was comparable to visual inspection for larger aneurysms. Segmentation performance of the top ranking method, after selection of true UIAs, was similar to interobserver performance. The ADAM challenge remains open for future submissions and improved submissions, with a live leaderboard to provide benchmarking for method developments at https://adam.isi.uu.nl/.', 'corpus_id': 235226706, 'score': 0}, {'doc_id': '235639340', 'title': 'Fast Lung Localization in Computed Tomography by a 1D Detection Network', 'abstract': 'Deep learning models performed very well in many medical image analysis tasks. However, the majority of these results had been obtained on carefully selected datasets. At the same time, the real clinical flow of Computed Tomography studies often contains series with different properties. We address a particular discrepancy related to a much larger scanning interval, e.g., a single series for thorax, abdomen, and pelvis. We propose to use 1D body organ detection for coarse organ localization on thorax-abdomen CT scans. Localized segments, containing volumes of interests, could be further processed by a heavier task-specific network. We convert 3D CT images into multi-channel 2D coronal images, thus drastically decreasing the dimensionality of the data. We next train a conventional U-net like architecture to solve the task of body part regression and build simple threshold rules to localize lungs along the coronal plane. Additionally, this approach allows for the detection of organs only partially presented in the image. Our network was trained on 20 thousand thorax-abdomen volume segments and validated on three separate datasets. It shows high localization accuracy, stability across datasets and processes a high-resolution CT volume in no more than 200 ms.', 'corpus_id': 235639340, 'score': 0}, {'doc_id': '202733345', 'title': 'A fully automated pipeline for mining abdominal aortic aneurysm using image segmentation', 'abstract': 'Imaging software have become critical tools in the diagnosis and the treatment of abdominal aortic aneurysms (AAA). The aim of this study was to develop a fully automated software system to enable a fast and robust detection of the vascular system and the AAA. The software was designed from a dataset of injected CT-scans images obtained from 40 patients with AAA. Pre-processing steps were performed to reduce the noise of the images using image filters. The border propagation based method was used to localize the aortic lumen. An online error detection was implemented to correct errors due to the propagation in anatomic structures with similar pixel value located close to the aorta. A morphological snake was used to segment 2D or 3D regions. The software allowed an automatic detection of the aortic lumen and the AAA characteristics including the presence of thrombus and calcifications. 2D and 3D reconstructions visualization were available to ease evaluation of both algorithm precision and AAA properties. By enabling a fast and automated detailed analysis of the anatomic characteristics of the AAA, this software could be useful in clinical practice and research and be applied in a large dataset of patients.', 'corpus_id': 202733345, 'score': 1}, {'doc_id': '235168325', 'title': 'Automatic cervical lymphadenopathy segmentation from CT data using deep learning.', 'abstract': 'PURPOSE\nThe purpose of this study was to develop a fast and automatic algorithm to detect and segment lymphadenopathy from head and neck computed tomography (CT) examination.\n\n\nMATERIALS AND METHODS\nAn ensemble of three convolutional neural networks (CNNs) based on a U-Net architecture were trained to segment the lymphadenopathies in a fully supervised framework. The resulting predictions were assessed using the Dice similarity coefficient (DSC) on examinations presenting one or more adenopathies. On examinations without adenopathies, the score was given by the formula M/(M+A) where M was the mean adenopathy volume per patient and A the volume segmented by the algorithm. The networks were trained on 117 annotated CT acquisitions.\n\n\nRESULTS\nThe test set included 150 additional CT acquisitions unseen during the training. The performance on the test set yielded a mean score of 0.63.\n\n\nCONCLUSION\nDespite limited available data and partial annotations, our CNN based approach achieved promising results in the task of cervical lymphadenopathy segmentation. It has the potential to bring precise quantification to the clinical workflow and to assist the clinician in the detection task.', 'corpus_id': 235168325, 'score': 0}, {'doc_id': '67856657', 'title': '3D convolutional neural network for abdominal aortic aneurysm segmentation', 'abstract': 'An abdominal aortic aneurysm (AAA) is a focal dilation of the aorta that, if not treated, tends to grow and may rupture. A significant unmet need in the assessment of AAA disease, for the diagnosis, prognosis and follow-up, is the determination of rupture risk, which is currently based on the manual measurement of the aneurysm diameter in a selected Computed Tomography Angiography (CTA) scan. However, there is a lack of standardization determining the degree and rate of disease progression, due to the lack of robust, automated aneurysm segmentation tools that allow quantitatively analyzing the AAA. In this work, we aim at proposing the first 3D convolutional neural network for the segmentation of aneurysms both from preoperative and postoperative CTA scans. We extensively validate its performance in terms of diameter measurements, to test its applicability in the clinical practice, as well as regarding the relative volume difference, and Dice and Jaccard scores. The proposed method yields a mean diameter measurement error of 3.3 mm, a relative volume difference of 8.58 %, and Dice and Jaccard scores of 87 % and 77 %, respectively. At a clinical level, an aneurysm enlargement of 10 mm is considered relevant, thus, our method is suitable to automatically determine the AAA diameter and opens up the opportunity for more complex aneurysm analysis.', 'corpus_id': 67856657, 'score': 1}, {'doc_id': '235627451', 'title': 'Visceral segment aortic thrombus is associated with proximal aortic degeneration after infrarenal abdominal aortic aneurysm repair.', 'abstract': 'OBJECTIVE\nTo identify predictors of aortic aneurysm formation at or above an infrarenal abdominal aortic aneurysm repair.\n\n\nMETHODS\nA total of 881 infrarenal abdominal aortic aneurysm repairs were identified at a single institution from 2004 to 2008; 187 of the repairs were identified that had pre-operative and post-operative computed tomography imaging at least one\u2009year or greater to evaluate for aortic degeneration following repair. Aortic diameters at the celiac, superior mesenteric, and renal arteries were measured on all available computed tomographic scans. Aortic thrombus and calcification volumes in the visceral and infrarenal abdominal aortic segments were calculated. Multivariable modeling was used with log transformed variables to determine potential predictors of future aortic aneurysm development after infrarenal abdominal aortic aneurysm repair.\n\n\nRESULTS\nOf the 187 patients in the cohort, 100 had an open abdominal aortic aneurysm repair while 87 were treated with endovascular repair. Proximal aortic aneurysms developed in 26% (n\u2009=\u200949) of the cohort during an average of 72\u2009±\u200934.2\u2009months of follow-up. After multivariable modeling, visceral segment aortic thrombus on pre-operative computed tomography imaging increased the risk of aortic aneurysm development above the infrarenal abdominal aortic aneurysm repair within both the open abdominal aortic aneurysm (hazard ratio 2.04, p\u2009=\u20090.033) and endovascular repair (hazard ratio 3.31, p\u2009=\u20090.004) cohorts. Endovascular repair was independently associated with a higher risk of future aortic aneurysm development after infrarenal abdominal aortic aneurysm repair when compared to open abdominal aortic aneurysm (hazard ratio 2.19, p\u2009=\u20090.025).\n\n\nCONCLUSIONS\nVisceral aortic thrombus present prior to abdominal aortic aneurysm repair and endovascular repair are both associated with an increased risk of future proximal aortic degeneration after infrarenal abdominal aortic aneurysm repair. These factors may predict patients at higher risk of developing proximal aortic aneurysms that may require complex aortic repairs.', 'corpus_id': 235627451, 'score': 0}, {'doc_id': '204832114', 'title': 'Abdominal Aortic Aneurysm Segmentation Using Convolutional Neural Networks Trained with Images Generated with a Synthetic Shape Model', 'abstract': 'An abdominal aortic aneurysm (AAA) is a ballooning of the abdominal aorta, that if not treated tends to grow and rupture. Computed Tomography Angiography (CTA) is the main imaging modality for the management of AAAs, and segmenting them is essential for AAA rupture risk and disease progression assessment. Previous works have shown that Convolutional Neural Networks (CNNs) can accurately segment AAAs, but have the limitation of requiring large amounts of annotated data to train the networks. Thus, in this work we propose a methodology to train a CNN only with images generated with a synthetic shape model, and test its generalization and ability to segment AAAs from new original CTA scans. The synthetic images are created from realistic deformations generated by applying principal component analysis to the deformation fields obtained from the registration of few datasets. The results show that the performance of a CNN trained with synthetic data to segment AAAs from new scans is comparable to the one of a network trained with real images. This suggests that the proposed methodology may be applied to generate images and train a CNN to segment other types of aneurysms, reducing the burden of obtaining large annotated image databases.', 'corpus_id': 204832114, 'score': 1}]
118	{'doc_id': '115153204', 'title': 'The perceptual neural trace of memorable unseen scenes', 'abstract': 'Some scenes are more memorable than others: they cement in minds with consistencies across observers and time scales. While memory mechanisms are traditionally associated with the end stages of perception, recent behavioral studies suggest that the features driving these memorability effects are extracted early on, and in an automatic fashion. This raises the question: is the neural signal of memorability detectable during early perceptual encoding phases of visual processing? Using the high temporal resolution of magnetoencephalography (MEG), during a rapid serial visual presentation (RSVP) task, we traced the neural temporal signature of memorability across the brain. We found an early and prolonged memorability related signal under a challenging ultra-rapid viewing condition, across a network of regions in both dorsal and ventral streams. This enhanced encoding could be the key to successful storage and recognition.', 'corpus_id': 115153204}	8172	"[{'doc_id': '2007238', 'title': 'Recognition of natural scenes from global properties: Seeing the forest without representing the trees', 'abstract': 'Human observers are able to rapidly and accurately categorize natural scenes, but the representation mediating this feat is still unknown. Here we propose a framework of rapid scene categorization that does not segment a scene into objects and instead uses a vocabulary of global, ecological properties that describe spatial and functional aspects of scene space (such as navigability or mean depth). In Experiment 1, we obtained ground truth rankings on global properties for use in Experiments 2-4. To what extent do human observers use global property information when rapidly categorizing natural scenes? In Experiment 2, we found that global property resemblance was a strong predictor of both false alarm rates and reaction times in a rapid scene categorization experiment. To what extent is global property information alone a sufficient predictor of rapid natural scene categorization? In Experiment 3, we found that the performance of a classifier representing only these properties is indistinguishable from human performance in a rapid scene categorization task in terms of both accuracy and false alarms. To what extent is this high predictability unique to a global property representation? In Experiment 4, we compared two models that represent scene object information to human categorization performance and found that these models had lower fidelity at representing the patterns of performance than the global property model. These results provide support for the hypothesis that rapid categorization of natural scenes may not be mediated primarily though objects and parts, but also through global properties of structure and affordance.', 'corpus_id': 2007238, 'score': 1}, {'doc_id': '219966723', 'title': 'Image Sentiment Transfer', 'abstract': 'In this work, we introduce an important but still unexplored research task -- image sentiment transfer. Compared with other related tasks that have been well-studied, such as image-to-image translation and image style transfer, transferring the sentiment of an image is more challenging. Given an input image, the rule to transfer the sentiment of each contained object can be completely different, making existing approaches that perform global image transfer by a single reference image inadequate to achieve satisfactory performance. In this paper, we propose an effective and flexible framework that performs image sentiment transfer at the object level. It first detects the objects and extracts their pixel-level masks, and then performs object-level sentiment transfer guided by multiple reference images for the corresponding objects. For the core object-level sentiment transfer, we propose a novel Sentiment-aware GAN (SentiGAN). Both global image-level and local object-level supervisions are imposed to train SentiGAN. More importantly, an effective content disentanglement loss cooperating with a content alignment step is applied to better disentangle the residual sentiment-related information of the input image. Extensive quantitative and qualitative experiments are performed on the object-oriented VSO dataset we create, demonstrating the effectiveness of the proposed framework.', 'corpus_id': 219966723, 'score': 0}, {'doc_id': '51963310', 'title': 'Memorable words are monogamous: The role of synonymy and homonymy in word recognition memory', 'abstract': 'What makes a word memorable? Prior research has identified numerous factors: word frequency, concreteness, imageability, and valence have all been shown to affect recognition performance. One important dimension that has not received much attention is the nature of the relationship between words and meanings. Under the hypothesis that words are encoded primarily by their meanings, and not by their surface forms, this relationship should be central to determining word memorability. In particular, rational analysis suggests that people will more easily remember words that convey a large amount of information about their intended meaning and that have few alternatives – that is, memorable words will be those with few possible meanings and synonyms. To test this hypothesis, we ran two large-scale recognition memory experiments (each with 2,222 words, 600+ participants). Memory performance was overall high, on par with memory for pictures in a similar paradigm. Critically, however, not all words were remembered equally well. Consistent with our proposal, the best recognized words had few meanings and few synonyms. Indeed, the most memorable words had a one-to-one relationship with their meanings. Estimates of memorability derived from this rational account explain a large amount of the variance in word memorability.', 'corpus_id': 51963310, 'score': 1}, {'doc_id': '18742872', 'title': 'Patients with schizophrenia are biased toward low spatial frequency to decode facial expression at a glance', 'abstract': 'Whereas patients with schizophrenia exhibit early visual processing impairments, their capacity at integrating visual information at various spatial scales, from low to high spatial frequencies, remains untested. This question is particularly acute given that, in ecological conditions of viewing, spatial frequency bands are naturally integrated to form a coherent percept. Here, 19 patients with schizophrenia and 16 healthy controls performed a rapid emotion recognition task with hybrid faces. Because these stimuli displayed in a single image two different facial expressions, in low (LSF) and high (HSF) spatial frequencies, the selected emotion probes which spatial scale is preferentially perceived. In a control experiment participants performed the same task with either low or high spatial frequency filtered faces. Results show that patients have a strong bias towards LSF with hybrid faces compared to healthy controls. However, both patients and healthy controls performed better with HSF filtered faces than with LSF filtered faces in the control experiment, demonstrating that the bias found with hybrid stimuli in patients was not due to an inability to process HSF. Whereas previous works found a LSF contrast deficit in schizophrenia, our results suggest a deficit in the normal time course of concurrently perceiving LSF and HSF. This early visual processing impairment is likely to contribute to the difficulties of patients with schizophrenia with facial processing and therefore social interaction.', 'corpus_id': 18742872, 'score': 1}, {'doc_id': '15911191', 'title': 'Beyond Memorability: Visualization Recognition and Recall', 'abstract': ""In this paper we move beyond memorability and investigate how visualizations are recognized and recalled. For this study we labeled a dataset of 393 visualizations and analyzed the eye movements of 33 participants as well as thousands of participant-generated text descriptions of the visualizations. This allowed us to determine what components of a visualization attract people's attention, and what information is encoded into memory. Our findings quantitatively support many conventional qualitative design guidelines, including that (1) titles and supporting text should convey the message of a visualization, (2) if used appropriately, pictograms do not interfere with understanding and can improve recognition, and (3) redundancy helps effectively communicate the message. Importantly, we show that visualizations memorable “at-a-glance” are also capable of effectively conveying the message of the visualization. Thus, a memorable visualization is often also an effective one."", 'corpus_id': 15911191, 'score': 1}, {'doc_id': '92763424', 'title': 'Etude de la microstructure de liants pouzzolaniques de synthèse', 'abstract': ""La diminution de la disponibilite en sous-produits industriels habituellement utilises dans l'elaboration de liants pouzzolaniques nous a amene a nous interesser au developpement du metakaolin obtenu par activation thermique du kaolin a moyenne temperature (60-80°C). En melangeant du metakaolin a du ciment portland artificiel, il est possible d'obtenir des liants hydrauliques performants. La valorisation de tels composes passe par l'etude de leur microstructure et plus particulierement de leur porosite. En effet, la porosite et la taille des pores sont des parametres qui affectent certaines des proprietes de base des betons comme la resistance, la permeabilite et la durabilite. Les transformations des structures poreuses des pâtes composees ont ete mises en evidence. L'importance de la reaction pouzzolanique combinant la chaux libre du liant en gehlenite hydratee a ete soulignee et l'etude insiste sur l'evolution des CSH des pâtes composees qui apparaissent moins bien cristallises et plus pauvres en calcium que ceux des pâtes pures."", 'corpus_id': 92763424, 'score': 0}, {'doc_id': '130287577', 'title': 'The Failure Analysis of High-sulfur Gas Pipeline', 'abstract': 'According to the analysis of failure factors about high-sulfur gas pipeline,established a failure accident tree.On the basis of the analysis results,had done the qualitative and quantitative analysis.At last,offered several suggestions to ensure the safe operation of high-sulfur gas pipeline.', 'corpus_id': 130287577, 'score': 0}, {'doc_id': '8612834', 'title': 'High-level aftereffects to global scene properties.', 'abstract': ""Adaptation is ubiquitous in the human visual system, allowing recalibration to the statistical regularities of its input. Previous work has shown that global scene properties such as openness and mean depth are informative dimensions of natural scene variation useful for human and machine scene categorization (Greene & Oliva, 2009b; Oliva & Torralba, 2001). A visual system that rapidly categorizes scenes using such statistical regularities should be continuously updated, and therefore is prone to adaptation along these dimensions. Using a rapid serial visual presentation paradigm, we show aftereffects to several global scene properties (magnitude 8-21%). In addition, aftereffects were preserved when the test image was presented 10 degrees away from the adapted location, suggesting that the origin of these aftereffects is not solely due to low-level adaptation. We show systematic modulation of observers' basic-level scene categorization performances after adapting to a global property, suggesting a strong representational role of global properties in rapid scene categorization."", 'corpus_id': 8612834, 'score': 1}, {'doc_id': '220496304', 'title': 'Applying recent advances in Visual Question Answering to Record Linkage', 'abstract': 'Multi-modal Record Linkage is the process of matching multi-modal records from multiple sources that represent the same entity. This field has not been explored in research and we propose two solutions based on Deep Learning architectures that are inspired by recent work in Visual Question Answering. The neural networks we propose use two different fusion modules, the Recurrent Neural Network + Convolutional Neural Network fusion module and the Stacked Attention Network fusion module, that jointly combine the visual and the textual data of the records. The output of these fusion models is the input of a Siamese Neural Network that computes the similarity of the records. Using data from the Avito Duplicate Advertisements Detection dataset, we train these solutions and from the experiments, we concluded that the Recurrent Neural Network + Convolutional Neural Network fusion module outperforms a simple model that uses hand-crafted features. We also find that the Recurrent Neural Network + Convolutional Neural Network fusion module classifies dissimilar advertisements as similar more frequently if their average description is bigger than 40 words. We conclude that the reason for this is that the longer advertisements have a different distribution then the shorter advertisements who are more prevalent in the dataset. In the end, we also conclude that further research needs to be done with the Stacked Attention Network, to further explore the effects of the visual data on the performance of the fusion modules.', 'corpus_id': 220496304, 'score': 0}, {'doc_id': '220363729', 'title': 'BézierSketch: A generative model for scalable vector sketches', 'abstract': 'The study of neural generative models of human sketches is a fascinating contemporary modeling problem due to the links between sketch image generation and the human drawing process. The landmark SketchRNN provided breakthrough by sequentially generating sketches as a sequence of waypoints. However this leads to low-resolution image generation, and failure to model long sketches. In this paper we present BezierSketch, a novel generative model for fully vector sketches that are automatically scalable and high-resolution. To this end, we first introduce a novel inverse graphics approach to stroke embedding that trains an encoder to embed each stroke to its best fit Bezier curve. This enables us to treat sketches as short sequences of paramaterized strokes and thus train a recurrent sketch generator with greater capacity for longer sketches, while producing scalable high-resolution results. We report qualitative and quantitative results on the Quick, Draw! benchmark.', 'corpus_id': 220363729, 'score': 0}]"
119	{'doc_id': '127079644', 'title': 'Radiometric Tracking Techniques for Deep-Space Navigation', 'abstract': 'Foreword. Preface. Acknowledgments. Chapter 1. Introduction. Chapter 2. Earth-Based Tracking and Navigation Overview. Chapter 3. Range and Doppler Tracking Observables. Chapter 4. VLBI Tracking Observables. Chapter 5. Future Directions in Radiometric Tracking. Glossary. Acronyms.', 'corpus_id': 127079644}	1115	"[{'doc_id': '116270575', 'title': 'Fermi question, Fermi paradox: one hit, one out.', 'abstract': 'Our understanding of the universe and the history of life on Earth suggests we may not be alone in the Galaxy. Today we are ignorant of where “they” are, if they are, as was Fermi when he asked, “Where are they?” Many scientists believe the way to seek for answers is to search for “their” radiated signature. (Hence SETI, now.) But some have argued persistently that since they are not here, they are not out there, and a few insist the situation is paradoxical. This note samples speculations supporting this view and concludes one cannot make a worthy paradox from absent evidence, a belief that interstellar travel will soon be a cinch, and a willingness to predict the distant future of intelligent species.', 'corpus_id': 116270575, 'score': 1}, {'doc_id': '119446418', 'title': 'There is no Fermi Paradox', 'abstract': 'Abstract The “Fermi Paradox,” an argument that extraterrestrial intelligence cannot exist because it has not yet been observed, is a logical fallacy. This “paradox” is a formally invalid inference, both because it requires modal operators lying outside the first-order propositional calculus and because it is unsupported by the observational record.', 'corpus_id': 119446418, 'score': 1}, {'doc_id': '32314921', 'title': 'Fermi paradox', 'abstract': '• The Sun is a young star. There are billions of stars in the galaxy that are billions of years older; • Some of these stars likely have Earth-like planets which, if the Earth is typical, may develop intelligent life; • Presumably some of these civilizations will develop interstellar travel, as Earth seems likely to do; • At any practical pace of interstellar travel, the galaxy can be completely colonized in just a few tens of millions of years.', 'corpus_id': 32314921, 'score': 1}, {'doc_id': '210164436', 'title': 'Reduction of Saturn Orbit Insertion Impulse Using Deep-Space Low Thrust', 'abstract': 'Orbit insertion at Saturn requires a large impulsive maneuver due to the velocity difference between the spacecraft and the planet. This paper presents a strategy to reduce dramatically the hyperbo...', 'corpus_id': 210164436, 'score': 0}, {'doc_id': '119331771', 'title': 'Dissolving the Fermi Paradox', 'abstract': 'The Fermi paradox is the conflict between an expectation of a high {\\em ex ante} probability of intelligent life elsewhere in the universe and the apparently lifeless universe we in fact observe. The expectation that the universe should be teeming with intelligent life is linked to models like the Drake equation, which suggest that even if the probability of intelligent life developing at a given site is small, the sheer multitude of possible sites should nonetheless yield a large number of potentially observable civilizations. We show that this conflict arises from the use of Drake-like equations, which implicitly assume certainty regarding highly uncertain parameters. We examine these parameters, incorporating models of chemical and genetic transitions on paths to the origin of life, and show that extant scientific knowledge corresponds to uncertainties that span multiple orders of magnitude. This makes a stark difference. When the model is recast to represent realistic distributions of uncertainty, we find a substantial {\\em ex ante} probability of there being no other intelligent life in our observable universe, and thus that there should be little surprise when we fail to detect any signs of it. This result dissolves the Fermi paradox, and in doing so removes any need to invoke speculative mechanisms by which civilizations would inevitably fail to have observable effects upon the universe.', 'corpus_id': 119331771, 'score': 1}, {'doc_id': '212633796', 'title': 'A LOFAR observation of ionospheric scintillation from two simultaneous travelling ionospheric disturbances', 'abstract': 'This paper presents the results from one of the first observations of ionospheric scintillation taken using the Low-Frequency Array (LOFAR). The observation was of the strong natural radio source Cassiopeia A, taken overnight on 18–19 August 2013, and exhibited moderately strong scattering effects in dynamic spectra of intensity received across an observing bandwidth of 10–80\xa0MHz. Delay-Doppler spectra (the 2-D FFT of the dynamic spectrum) from the first hour of observation showed two discrete parabolic arcs, one with a steep curvature and the other shallow, which can be used to provide estimates of the distance to, and velocity of, the scattering plasma. A cross-correlation analysis of data received by the dense array of stations in the LOFAR “core” reveals two different velocities in the scintillation pattern: a primary velocity of ~20–40\xa0ms−1 with a north-west to south-east direction, associated with the steep parabolic arc and a scattering altitude in the F-region or higher, and a secondary velocity of ~110\xa0ms−1 with a north-east to south-west direction, associated with the shallow arc and a scattering altitude in the D-region. Geomagnetic activity was low in the mid-latitudes at the time, but a weak sub-storm at high latitudes reached its peak at the start of the observation. An analysis of Global Navigation Satellite Systems (GNSS) and ionosonde data from the time reveals a larger-scale travelling ionospheric disturbance (TID), possibly the result of the high-latitude activity, travelling in the north-west to south-east direction, and, simultaneously, a smaller-scale TID travelling in a north-east to south-west direction, which could be associated with atmospheric gravity wave activity. The LOFAR observation shows scattering from both TIDs, at different altitudes and propagating in different directions. To the best of our knowledge this is the first time that such a phenomenon has been reported.', 'corpus_id': 212633796, 'score': 0}, {'doc_id': '211258679', 'title': 'Spacecraft Tracking Applications of the Square Kilometre Array', 'abstract': ""The Square Kilometre Array (SKA) is the next generation radio telescope distinguished by a superb sensitivity due to its large aperture (about one square kilometre) and advanced instrumentation. It will cover a broad range of observing bands including those used for tracking of and communications to deep space missions. While spacecraft tracking is not a main application defining the technical specifications of the SKA, this facility might play a role in tracking deep space probes as a backup to the ``dedicated'' deep space tracking networks. This paper presents possible applications of the SKA as a deep space tracking facility and major related technical specifications of various concepts of the SKA. It was presented at the 3rd International Workshop on Tracking, Telemetry and Command Systems for Space Applications, ESA-ESOC, Darmstadt, Germany, 7-9 September 2004. Over the past years, the SKA concept has developed to a much higher level of detalisation and is currently at the implementation phase. A number of specific considerations in this presentation no longer correspond to the actual status of the SKA project. However, the overall concept of the SKA applications for communication and tracking of interplanetary spacecraft remain topical, and some approaches presented here remain of interest for prospective deep space missions."", 'corpus_id': 211258679, 'score': 0}, {'doc_id': '13829104', 'title': 'Bioterrorism and the Fermi Paradox', 'abstract': 'Abstract We proffer a contemporary solution to the so-called Fermi Paradox, which is concerned with conflict between Copernicanism and the apparent paucity of evidence for intelligent alien civilizations. In particular, we argue that every community of organisms that reaches its space-faring age will (1) almost immediately use its rocket-building computers to reverse-engineer its genetic chemistry and (2) self-destruct when some individual uses said technology to design an omnicidal pathogen. We discuss some of the possible approaches to prevention with regard to Homo sapiens’ vulnerability to bioterrorism, particularly on a short-term basis.', 'corpus_id': 13829104, 'score': 1}, {'doc_id': '214802472', 'title': 'Theoretical Physics and Indian Philosophy: Conceptual Coherence', 'abstract': ""The paper addresses the phenomenon of cross-cultural resonance, which arises when ideas coming from different cultures and view systems show mutual correlation, or coherence. We particularly dwell on the parallels between modern physics and Indian classical philosophy. The coherence in ideological, methodological, and ethical spheres is noted and exemplified. Interpretation of correlations in terms of the Jaspers 'ciphers of transcendence' is proposed. A brief survey of studies also dealing with coherence between modern science and ancient teachings is given. In conclusion, a broader perspective of interrelations between rational science and spiritual tradition is discussed."", 'corpus_id': 214802472, 'score': 0}, {'doc_id': '210847812', 'title': ""Time's Arrow and De Se Probabilities"", 'abstract': ""One of the most difficult problems in the foundations of physics is what gives rise to the arrow of time. Since the fundamental dynamical laws of physics are (essentially) symmetric in time, the explanation for time's arrow must come from elsewhere. A promising explanation introduces a special cosmological initial condition, now called the Past Hypothesis: the universe started in a low-entropy state. In this paper, I argue that, in a universe where there are many copies of us (in the distant past or the distant future), the Past Hypothesis needs to be supplemented with de se (self-locating) probabilities. However, letting in de se probabilities also helps its rival---the Fluctuation Hypothesis, leading to a kind of empirical underdetermination and radical epistemological skepticism. The skeptical problem is exacerbated by the possibility of ``Boltzmann bubbles.'' Hence, it seems that explaining time's arrow is more complicated than we have realized, and that its explanation may depend on how we resolve philosophical issues about de se probabilities. Thus, we need to carefully examine the epistemological and probabilistic principles underlying our explanation of time's arrow. The task is especially urgent for theories that invoke the Past Hypothesis. The philosophical analysis offered in the paper aims at preparing the conceptual foundation for such a task."", 'corpus_id': 210847812, 'score': 0}]"
120	"{'doc_id': '33038671', 'title': 'Life cycle complexity, environmental change and the emerging status of salmonid proliferative kidney disease', 'abstract': ""P>1. Proliferative kidney disease (PKD) is a disease of salmonid fish caused by the endoparasitic myxozoan, Tetracapsuloides bryosalmonae, which uses freshwater bryozoans as primary hosts. Clinical PKD is characterised by a temperature-dependent proliferative and inflammatory response to parasite stages in the kidney.;2. Evidence that PKD is an emerging disease includes outbreaks in new regions, declines in Swiss brown trout populations and the adoption of expensive practices by fish farms to reduce heavy losses. Disease-related mortality in wild fish populations is almost certainly underestimated because of e.g. oversight, scavenging by wild animals, misdiagnosis and fish stocking.;3. PKD prevalences are spatially and temporally variable, range from 0 to 90-100% and are typically highest in juvenile fish.;4. Laboratory and field studies demonstrate that (i) increasing temperatures enhance disease prevalence, severity and distribution and PKD-related mortality; (ii) eutrophication may promote outbreaks. Both bryozoans and T. bryosalmonae stages in bryozoans undergo temperature- and nutrient-driven proliferation.;5. Tetracapsuloides bryosalmonae is likely to achieve persistent infection of highly clonal bryozoan hosts through vertical transmission, low virulence and host condition-dependent cycling between covert and overt infections. Exploitation of fish hosts entails massive proliferation and spore production by stages that escape the immune response. Many aspects of the parasite's life cycle remain obscure. If infectious stages are produced in all hosts then the complex life cycle includes multiple transmission routes.;6. Patterns of disease outbreaks suggest that background, subclinical infections exist under normal environmental conditions. When conditions change, outbreaks may then occur in regions where infection was hitherto unsuspected.;7. Environmental change is likely to cause PKD outbreaks in more northerly regions as warmer temperatures promote disease development, enhance bryozoan biomass and increase spore production, but may also reduce the geographical range of this unique multihost-parasite system. Coevolutionary dynamics resulting from host-parasite interactions that maximise fitness in previous environments may pose problems for sustainability, particularly in view of extensive declines in salmonid populations and degradation of many freshwater habitats."", 'corpus_id': 33038671}"	12748	[{'doc_id': '227157696', 'title': 'Fatal Interstitial Pneumonia Associated with Bovine Coronavirus in Cows from Southern Italy', 'abstract': 'An outbreak of winter dysentery, complicated by severe respiratory syndrome, occurred in January 2020 in a high production dairy cow herd located in a hilly area of the Calabria region. Of the 52 animals belonging to the farm, 5 (9.6%) died with severe respiratory distress, death occurring 3–4 days after the appearance of the respiratory signs (caught and gasping breath). Microbiological analysis revealed absence of pathogenic bacteria whilst Real-time PCR identified the presence of RNA from Bovine Coronavirus (BCoV) in several organs: lungs, small intestine (jejunum), mediastinal lymph nodes, liver and placenta. BCoV was therefore hypothesized to play a role in the lethal pulmonary infection. Like the other CoVs, BCoV is able to cause different syndromes. Its role in calf diarrhea and in mild respiratory disease is well known: we report instead the involvement of this virus in a severe and fatal respiratory disorder, with symptoms and disease evolution resembling those of Severe Acute Respiratory Syndromes (SARS).', 'corpus_id': 227157696, 'score': 0}, {'doc_id': '226678154', 'title': 'Pathological effect of infectious bronchitis disease virus on broiler chicken trachea and kidney tissues', 'abstract': 'Aim: This study aimed to investigate the pathological effects of the infectious bronchitis virus (IBV) on chicken trachea and kidney tissues and also desired to diagnose the virus genome using a molecular tool. Materials and Methods: Twenty trachea and kidney samples collected from one broiler farm contain 10,000 chickens at Tikrit city. The chickens showed signs of gasping and mortality (20%) at early ages (20 days old), the presence of IBV investigated using conventional reverse transcriptase-polymerase chain reaction technique with routine histopathological study to tracheal and renal tissue. Results: Postmortem lesion showed severe respiratory inflammation with abscesses at tracheal bifurcation lead to airway blog. Molecular results showed two genotypes of IBV, one of them not included in primer designer research. The histological study showed different stages of inflammation, degeneration, and necrosis to the renal and tracheal tissues. Conclusion: The respiratory and renal pathological effect of the virus responsible for the symptoms appeared on the affected chicks that caused mortality, with a high probability of presence of a new viral genotype added to the untranslated region.', 'corpus_id': 226678154, 'score': 0}, {'doc_id': '84279991', 'title': 'Ultrastructure of a haplosporean-like organism: the possible causative agent of proliferative kidney disease in rainbow trout.', 'abstract': 'The protozoan parasite which has been suggested as the causative agent of proliferative kidney disease in rainbow trout has previously been described as an‘amoeba’. However, this ultra-structural study of the organism (named‘PKX’) has demonstrated many similarities to a group of invertebrate parasites, the genus Marleilia (currently classified in the Haplosporea). The suggested affinity of PKX‘ to Marteilia is based on the presence of internal cleavage,‘haplosporosomes’, and an amorphous cell wall. Other cytoplasmic inclusions also show characteristics of the genus. Since a spore stage has not been recognised in ‘PKX’, a definitive taxonomic statement cannot yet be made.', 'corpus_id': 84279991, 'score': 1}, {'doc_id': '226258233', 'title': 'Application of Humanized Zebrafish Model in the Suppression of SARS-CoV-2 Spike Protein Induced Pathology by Tri-Herbal Medicine Coronil via Cytokine Modulation', 'abstract': 'Zebrafish has been a reliable model system for studying human viral pathologies. SARS-CoV-2 viral infection has become a global chaos, affecting millions of people. There is an urgent need to contain the pandemic and develop reliable therapies. We report the use of a humanized zebrafish model, xeno-transplanted with human lung epithelial cells, A549, for studying the protective effects of a tri-herbal medicine Coronil. At human relevant doses of 12 and 58 µg/kg, Coronil inhibited SARS-CoV-2 spike protein, induced humanized zebrafish mortality, and rescued from behavioral fever. Morphological and cellular abnormalities along with granulocyte and macrophage accumulation in the swim bladder were restored to normal. Skin hemorrhage, renal cell degeneration, and necrosis were also significantly attenuated by Coronil treatment. Ultra-high-performance liquid chromatography (UHPLC) analysis identified ursolic acid, betulinic acid, withanone, withaferine A, withanoside IV–V, cordifolioside A, magnoflorine, rosmarinic acid, and palmatine as phyto-metabolites present in Coronil. In A549 cells, Coronil attenuated the IL-1β induced IL-6 and TNF-α cytokine secretions, and decreased TNF-α induced NF-κB/AP-1 transcriptional activity. Taken together, we show the disease modifying immunomodulatory properties of Coronil, at human equivalent doses, in rescuing the pathological features induced by the SARS-CoV-2 spike protein, suggesting its potential use in SARS-CoV-2 infectivity.', 'corpus_id': 226258233, 'score': 0}, {'doc_id': '87458907', 'title': 'First record of proliferative kidney disease in Iceland.', 'abstract': 'Proliferative kidney disease caused by the myxozoan parasite Tetracapsuloides bryosalmonae is reported for the first time in Iceland. Infections were confirmed in both arctic charr and brown trout but only arctic charr showed clinical signs. The last two decades, populations of arctic charr in several lakes in Iceland have greatly declined. Possible relation of this decline with increasing water temperature has been speculated. It is hypothesized that PKD may play a significant role in this decline. Studies on the distribution of PKD and its effect on wild populations of arctic charr and brown trout in Iceland are presently in progress.', 'corpus_id': 87458907, 'score': 1}, {'doc_id': '24691408', 'title': 'Comparative study of proliferative kidney disease in grayling Thymallus thymallus and brown trout Salmo trutta fario: an exposure experiment.', 'abstract': 'Proliferative kidney disease (PKD) is an emerging disease threatening wild salmonid populations, with the myxozoan parasite Tetracapsuloides bryosalmonae as the causative agent. Species differences in parasite susceptibility and disease-induced mortality seem to exist. The aim of the present study was to compare incidence, pathology and mortality of PKD in grayling Thymallus thymallus and brown trout Salmo trutta under identical semi-natural conditions. Young-of-the-year grayling and brown trout, free of T. bryosalmonae, were jointly exposed in cage compartments in a river in the northeast of Switzerland during 3 summer months. Wild brown trout were caught by electrofishing near the cage, and PKD status was compared with that of caged animals. Cage-exposed grayling showed a PKD incidence of 1%, regardless of whether parasite infection was determined by means of real-time PCR or histopathology/immunohistochemistry. In contrast, PKD incidence of caged brown trout was 77%. This value was not significantly different to PKD prevalence of wild brown trout caught above and below the cage (60 and 91%, respectively). Mortality in grayling was significantly higher compared with that of brown trout (40 versus 23%); however, grayling mortality was not considered to be associated with PKD. Mortality of caged and infected brown trout was significantly higher than mortality of non-infected caged trout. Histopathology indicated an ongoing mostly acute or chronic active infection in brown trout, which survived until the end of exposure. The results suggest that grayling are less susceptible to infection with T. bryosalmonae compared with brown trout under the tested field conditions.', 'corpus_id': 24691408, 'score': 1}, {'doc_id': '25952744', 'title': 'Proliferative kidney disease in rainbow trout: time- and temperature-related renal pathology and parasite distribution.', 'abstract': 'Proliferative kidney disease is a parasitic infection of salmonid fishes caused by Tetracapsuloides bryosalmonae. The main target organ of the parasite in the fish is the kidney. To investigate the influence of water temperature on the disease in fish, rainbow trout Oncorhynchus mykiss infected with T bryosalmonae were kept at 12 degrees C and 18 degrees C. The number of parasites, the type and degree of lesions in the kidney and the mortality rate was evaluated from infection until full development of disease. While mortality stayed low at 12 degrees C, it reached 77% at 18 degrees C. At 12 degrees C, pathological lesions were dominated by a multifocal proliferative and granulomatous interstitial nephritis. This was accompanied by low numbers of T. bryosalmonae, mainly located in the interstitial lesions. With progression of the disease, small numbers of parasites appeared in the excretory tubuli, and parasite DNA was detected in the urine. Parasite degeneration in the interstitium was observed at late stages of the disease. At 18 degrees C, pathological lesions in kidneys were more severe and more widely distributed, and accompanied by significantly higher parasite numbers. Distribution of parasites in the renal compartments, onset of parasite degeneration and time course of appearance of parasite DNA in urine were not clearly different from the 12 degrees C group. These findings indicate that higher mortality at 18 degrees C compared to 12 degrees C is associated with an enhanced severity of renal pathology and increased parasite numbers.', 'corpus_id': 25952744, 'score': 1}, {'doc_id': '225057174', 'title': 'Identification of silkworm hemocyte subsets and analysis of their response to BmNPV infection based on single-cell RNA sequencing', 'abstract': 'A wide range of hemocyte types exist in insects but a full definition of the different subclasses is not yet established. The current knowledge of the classification of silkworm hemocytes mainly comes from morphology rather than specific markers, so our understanding of the detailed classification, hemocyte lineage and functions of silkworm hemocytes is very incomplete. Bombyx mori nucleopolyhedrovirus (BmNPV) is a representative member of the baculoviruses, which are a major pathogens that specifically infects silkworms and cause serious loss in sericulture industry. Here, we performed single-cell RNA sequencing (scRNA-seq) of silkworm hemocytes in BmNPV and mock-infected larvae to comprehensively identify silkworm hemocyte subsets and determined specific molecular and cellular characteristics in each hemocyte subset before and after viral infection. A total of 19 cell clusters and their potential marker genes were identified in silkworm hemocytes. Among these hemocyte clusters, clusters 0, 1, 2, 5 and 9 might be granulocytes (GR); clusters 14 and 17 were predicted as plasmatocytes (PL); cluster 18 was tentatively identified as spherulocytes (SP); and clusters 7 and 11 could possibly correspond to oenocytoids (OE). In addition, all of the hemocyte clusters were infected by BmNPV and some infected cells carried high viral-load in silkworm larvae at 3 day post infection (dpi). Interestingly, BmNPV infection can cause severe and diverse changes in gene expression in hemocytes. Cells belonging to the infection group mainly located at the early stage of the pseudotime trajectories. Furthermore, we found that BmNPV infection suppresses the immune response in the major hemocyte types. In summary, our scRNA-seq analysis revealed the diversity of silkworm hemocytes and provided a rich resource of gene expression profiles for a systems-level understanding of their functions in the uninfected condition and as a response to BmNPV.', 'corpus_id': 225057174, 'score': 0}, {'doc_id': '46782103', 'title': 'Temperature-related parasite infection dynamics: the case of proliferative kidney disease of brown trout', 'abstract': 'SUMMARY Climate change, in particular rising temperature, is suspected to be a major driver for the emergence of many wildlife diseases. Proliferative kidney disease of salmonids, caused by the myxozoan Tetracapsuloides bryosalmonae, was used to evaluate how temperature dependence of host–parasite interactions modulates disease emergence. Brown trout (Salmo trutta fario) kept at 12 and 15 °C, were experimentally infected with T. bryosalmonae. Parasite development in the fish host and release of spores were quantified simultaneously to unravel parasite transmission potential from the vertebrate to the invertebrate host. A change to a stable plateau in infection intensity of the kidney coincided with a threshold at which spore shedding commenced. This onset of parasite release was delayed at the low temperature in accordance with reaching this infection intensity threshold, but the amount of spores released was irrespective of temperature. The production of parasite transmission stages declined with time. In conclusion, elevated temperature modifies the parasite transmission opportunities by increasing the duration of transmission stage production, which may affect the spread and establishment of the parasite in a wider range of rivers.', 'corpus_id': 46782103, 'score': 1}, {'doc_id': '226958901', 'title': 'Leprosy in wild chimpanzees', 'abstract': 'Humans are considered the main host for Mycobacterium leprae, the aetiologic agent of leprosy, but spill-over to other mammals such as nine-banded armadillos and red squirrels occurs. Although naturally acquired leprosy has also been described in captive nonhuman primates, the exact origins of infection remain unclear. Here, we report on leprosy-like lesions in two wild populations of western chimpanzees (Pan troglodytes verus) in the Cantanhez National Park, Guinea-Bissau, and the Taï National Park, Côte d’Ivoire, West Africa. Longitudinal monitoring of both populations revealed the progression of disease symptoms compatible with advanced leprosy. Screening of faecal and necropsy samples confirmed the presence of M. leprae as the causative agent at each site and phylogenomic comparisons with other strains from humans and other animals show that the chimpanzee strains belong to different and rare genotypes (4N/O and 2F). The independent evolutionary origin of M. leprae in two geographically distant populations of wild chimpanzees, with no prolonged direct contact with humans, suggests multiple introductions of M. leprae from an unknown animal or environmental source.', 'corpus_id': 226958901, 'score': 0}]
121	{'doc_id': '56719608', 'title': 'Improving free-viewing fixation-related EEG potentials with continuous-time regression', 'abstract': 'BACKGROUND\nIn the analysis of combined ET-EEG data, there are several issues with estimating FRPs by averaging. Neural responses associated with fixations will likely overlap with one another in the EEG recording and neural responses change as a function of eye movement characteristics. Especially in tasks that do not constrain eye movements in any way, these issues can become confounds.\n\n\nNEW METHOD\nHere, we propose the use of regression based estimates as an alternative to averaging. Multiple regression can disentangle different influences on the EEG and correct for overlap. It thereby accounts for potential confounds in a way that averaging cannot. Specifically, we test the applicability of the rERP framework, as proposed by Smith and Kutas (2015b), (2017), or Sassenhagen (2018) to combined eye tracking and EEG data from a visual search and a scene memorization task.\n\n\nRESULTS\nResults show that the method successfully estimates eye movement related confounds in real experimental data, so that these potential confounds can be accounted for when estimating experimental effects.\n\n\nCOMPARISON WITH EXISTING METHODS\nThe rERP method successfully corrects for overlapping neural responses in instances where averaging does not. As a consequence, baselining can be applied without risking distortions. By estimating a known experimental effect, we show that rERPs provide an estimate with less variance and more accuracy than averaged FRPs. The method therefore provides a practically feasible and favorable alternative to averaging.\n\n\nCONCLUSIONS\nWe conclude that regression based ERPs provide novel opportunities for estimating fixation related EEG in free-viewing experiments.', 'corpus_id': 56719608}	13575	"[{'doc_id': '13798828', 'title': 'Recursive Partitioning with Nonlinear Models of Change', 'abstract': ""ABSTRACT In this article, we introduce nonlinear longitudinal recursive partitioning (nLRP) and the R package longRpart2 to carry out the analysis. This method implements recursive partitioning (also known as decision trees) in order to split data based on individual- (i.e., cluster) level covariates with the goal of predicting differences in nonlinear longitudinal trajectories. At each node, a user-specified linear or nonlinear mixed-effects model is estimated. This method is an extension of Abdolell et al.'s (2002) longitudinal recursive partitioning while permitting a nonlinear mixed-effects model in addition to a linear mixed-effects model in each node. We give an overview of recursive partitioning, nonlinear mixed-effects models for longitudinal data, describe nLRP, and illustrate its use with empirical data from the Early Childhood Longitudinal Study—Kindergarten Cohort."", 'corpus_id': 13798828, 'score': 1}, {'doc_id': '232832364', 'title': 'Type I error control for cluster randomized trials under varying small sample structures', 'abstract': 'Background Linear mixed models (LMM) are a common approach to analyzing data from cluster randomized trials (CRTs). Inference on parameters can be performed via Wald tests or likelihood ratio tests (LRT), but both approaches may give incorrect Type I error rates in common finite sample settings. The impact of different combinations of cluster size, number of clusters, intraclass correlation coefficient (ICC), and analysis approach on Type I error rates has not been well studied. Reviews of published CRTs find that small sample sizes are not uncommon, so the performance of different inferential approaches in these settings can guide data analysts to the best choices. Methods Using a random-intercept LMM stucture, we use simulations to study Type I error rates with the LRT and Wald test with different degrees of freedom (DF) choices across different combinations of cluster size, number of clusters, and ICC. Results Our simulations show that the LRT can be anti-conservative when the ICC is large and the number of clusters is small, with the effect most pronouced when the cluster size is relatively large. Wald tests with the between-within DF method or the Satterthwaite DF approximation maintain Type I error control at the stated level, though they are conservative when the number of clusters, the cluster size, and the ICC are small. Conclusions Depending on the structure of the CRT, analysts should choose a hypothesis testing approach that will maintain the appropriate Type I error rate for their data. Wald tests with the Satterthwaite DF approximation work well in many circumstances, but in other cases the LRT may have Type I error rates closer to the nominal level.', 'corpus_id': 232832364, 'score': 0}, {'doc_id': '233745294', 'title': 'Movement-Preceding Neural Activity under Parametrically Varying Levels of Time Pressure', 'abstract': 'Self-initiated movements are known to be preceded by the readiness potential or RP, a gradual increase in surface-negativity of cortical potentials that can begin up to 1 second or more before movement onset. The RP has been extensively studied for decades, and yet we still lack a clear understanding of its functional role. Attempts to model the RP as an accumulation-to-bound process suggest that this signal is a by-product of time-locking to crests in neural noise rather than the outcome of a pre-conscious decision to initiate a movement. One parameter of the model accounts for the imperative to move now, with cued movements having a strong imperative and purely spontaneous movements having no imperative. Two different variants of the model have been proposed, and both predict a decrease in the (negative) amplitude of the early RP as the imperative grows stronger. In order to test this empirically, we conducted an experiment where subjects produced self-initiated movements under varying levels of time pressure, and we investigated the amplitude, shape, and latency of the RP as a function of the imperative to move, operationalised as a time limit. We identified distinct changes in the amplitude of the early RP that grew non-linearly as the time limit grew shorter. Thus these data did not support the prediction made by the model. In addition, our results confirm that the shape of the RP is not stereotypically negative, being either positive or absent in about half of the subjects.', 'corpus_id': 233745294, 'score': 0}, {'doc_id': '58625049', 'title': 'Cluster-based permutation tests of MEG/EEG data do not establish significance of effect latency or location.', 'abstract': 'Cluster-based permutation tests are gaining an almost universal acceptance as inferential procedures in cognitive neuroscience. They elegantly handle the multiple comparisons problem in high-dimensional magnetoencephalographic and EEG data. Unfortunately, the power of this procedure comes hand in hand with the allure for unwarranted interpretations of the inferential output, the most prominent of which is the overestimation of the temporal, spatial, and frequency precision of statistical claims. This leads researchers to statements about the onset or offset of a certain effect that is not supported by the permutation test. In this article, we outline problems and common pitfalls of using and interpreting cluster-based permutation tests. We illustrate these with simulated data in order to promote a more intuitive understanding of the method. We hope that raising awareness about these issues will be beneficial to common scientific practices, while at the same time increasing the popularity of cluster-based permutation procedures.', 'corpus_id': 58625049, 'score': 1}, {'doc_id': '187756919', 'title': 'بررسی الگوی مصرف آنتی بیوتیک ها در مرکز آموزشی درمانی دزیانی گرگان', 'abstract': 'بررسی الگوی مصرف آنتی بیوتیک ها در مرکز آموزشی درمانی دزیانی گرگان , بررسی الگوی مصرف آنتی بیوتیک ها در مرکز آموزشی درمانی دزیانی گرگان , کتابخانه الکترونیک و دیجیتال - آذرسا', 'corpus_id': 187756919, 'score': 0}, {'doc_id': '10662074', 'title': 'Regression-based estimation of ERP waveforms: I. The rERP framework.', 'abstract': 'ERP averaging is an extraordinarily successful method, but can only be applied to a limited range of experimental designs. We introduce the regression-based rERP framework, which extends ERP averaging to handle arbitrary combinations of categorical and continuous covariates, partial confounding, nonlinear effects, and overlapping responses to distinct events, all within a single unified system. rERPs enable a richer variety of paradigms (including high-N naturalistic designs) while preserving the advantages of traditional ERPs. This article provides an accessible introduction to what rERPs are, why they are useful, how they are computed, and when we should expect them to be effective, particularly in cases of partial confounding. A companion article discusses how nonlinear effects and overlap correction can be handled within this framework, as well as practical considerations around baselining, filtering, statistical testing, and artifact rejection. Free software implementing these techniques is available.', 'corpus_id': 10662074, 'score': 1}, {'doc_id': '1302826', 'title': 'Statistically comparing EEG/MEG waveforms through successive significant univariate tests: how bad can it be?', 'abstract': 'When making statistical comparisons, the temporal dimension of the EEG signal introduces problems. Guthrie and Buchwald (1991) proposed a formally correct statistical approach that deals with these problems: comparing waveforms by counting the number of successive significant univariate tests and then contrasting this number to a well-chosen critical value. However, in the literature, this method is often used inappropriately. Using real EEG data and Monte Carlo simulations, we examined the problems associated with the incorrect use of this approach under circumstances often encountered in the literature. Our results show inflated false-positive or false-negative rates depending on parameters of the data, including filtering. Our findings suggest that most applications of this method result in an inappropriate familywise error rate control. Solutions and alternative methods are discussed.', 'corpus_id': 1302826, 'score': 1}, {'doc_id': '53008057', 'title': 'Autocorrelated Errors in Experimental Data in the Language Sciences: Some Solutions Offered by Generalized Additive Mixed Models', 'abstract': 'A problem that tends to be ignored in the statistical analysis of experimental data in the language sciences is that responses often constitute time series, which raises the problem of autocorrelated errors. If the errors indeed show autocorrelational structure, evaluation of the significance of predictors in the model becomes problematic due to potential anti-conservatism of p-values.', 'corpus_id': 53008057, 'score': 1}, {'doc_id': '97214210', 'title': 'Selective segregation at grain boundaries', 'abstract': ""Abstract Experimental observations provide support for two different types of solute enhancement at interfaces, namely equilibrium and non-equilibrium segregation. It is shown that these segregation processes are dependent upon the structures of different large-angle grain boundaries. Experimental data clearly indicate that this selective grain boundary segregation influences the mobility and energy of grain boundaries.A fundamental relationship is found to exist between the energy of coincidence boundaries and the density of shared atom sites (or the size of the periodic unit) in the boundary. The energy and mobility results provide strong support for the boundary coincidence and “relaxed” coincidence models of grain boundary structure.A selective solute segregation to grain boundaries can lead to the formation of preferred orientations in annealed materials and enhanced corrosion at grain boundaries. Resume Les etudes experimentales apportent des arguments aux deux types differents d'enrichissement en s..."", 'corpus_id': 97214210, 'score': 0}, {'doc_id': '233446650', 'title': 'A Revised Hilbert-Huang Transformation to Track Non-Stationary Association of Electroencephalography Signals', 'abstract': 'The time-varying cross-spectrum method has been used to effectively study transient and dynamic brain functional connectivity between non-stationary electroencephalography (EEG) signals. Wavelet-based cross-spectrum is one of the most widely implemented methods, but it is limited by the spectral leakage caused by the finite length of the basic function that impacts the time and frequency resolutions. This paper proposes a new time-frequency brain functional connectivity analysis framework to track the non-stationary association of two EEG signals based on a Revised Hilbert-Huang Transform (RHHT). The framework can estimate the cross-spectrum of decomposed components of EEG, followed by a surrogate significance test. The results of two simulation examples demonstrate that, within a certain statistical confidence level, the proposed framework outperforms the wavelet-based method in terms of accuracy and time-frequency resolution. A case study on classifying epileptic patients and healthy controls using interictal seizure-free EEG data is also presented. The result suggests that the proposed method has the potential to better differentiate these two groups benefiting from the enhanced measure of dynamic time-frequency association.', 'corpus_id': 233446650, 'score': 0}]"
122	{'doc_id': '115136275', 'title': 'Effect handlers via generalised continuations', 'abstract': 'Abstract Plotkin and Pretnar’s effect handlers offer a versatile abstraction for modular programming with user-defined effects. This paper focuses on foundations for implementing effect handlers, for the three different kinds of effect handlers that have been proposed in the literature: deep, shallow, and parameterised. Traditional deep handlers are defined by folds over computation trees and are the original construct proposed by Plotkin and Pretnar. Shallow handlers are defined by case splits (rather than folds) over computation trees. Parameterised handlers are deep handlers extended with a state value that is threaded through the folds over computation trees. We formulate the extensions both directly and via encodings in terms of deep handlers and illustrate how the direct implementations avoid the generation of unnecessary closures. We give two distinct foundational implementations of all the kinds of handlers we consider: a continuation-passing style (CPS) transformation and a CEK-style abstract machine. In both cases, the key ingredient is a generalisation of the notion of continuation to accommodate stacks of effect handlers. We obtain our CPS translation through a series of refinements as follows. We begin with a first-order CPS translation into untyped lambda calculus which manages a stack of continuations and handlers as a curried sequence of arguments. We then refine the initial CPS translation by uncurrying it to yield a properly tail-recursive translation and then moving towards more and more intensional representations of continuations in order to support different kinds of effect handlers. Finally, we make the translation higher order in order to contract administrative redexes at translation time. Our abstract machine design then uses the same generalised continuation representation as the CPS translation. We have implemented both the abstract machine and the CPS transformation (plus extensions) as backends for the Links web programming language.', 'corpus_id': 115136275}	18849	"[{'doc_id': '220281107', 'title': 'Effects for efficiency: asymptotic speedup with first-class control', 'abstract': 'We study the fundamental efficiency of delimited control. Specifically, we show that effect handlers enable an asymptotic improvement in runtime complexity for a certain class of functions. We consider the generic count problem using a pure PCF-like base language λb and its extension with effect handlers λh. We show that λh admits an asymptotically more efficient implementation of generic count than any λb implementation. We also show that this efficiency gap remains when λb is extended with mutable state. To our knowledge this result is the first of its kind for control operators.', 'corpus_id': 220281107, 'score': 1}, {'doc_id': '235422592', 'title': 'Polymorphic Context-free Session Types', 'abstract': 'Context-free session types provide a typing discipline for recursive structured communication protocols on bidirectional channels. They overcome the restriction of regular session type systems to tail recursive protocols. This extension enables us to implement serialisation and deserialisation of tree structures in a fully type-safe manner. We present the theory underlying the language FreeST 2, which features context-free session types in an extension of System F with linear types and a kind system to distinguish message types and channel types. The system presents some metatheoretical challenges, which we address, contractivity in the presence of polymorphism, a non-trivial equational theory on types, and decidability of type equivalence. We also establish standard results on type preservation, progress, and a characterisation of erroneous processes.', 'corpus_id': 235422592, 'score': 0}, {'doc_id': '49392755', 'title': 'Shallow Effect Handlers', 'abstract': 'Plotkin and Pretnar’s effect handlers offer a versatile abstraction for modular programming with user-defined effects. Traditional deep handlers are defined by folds over computation trees. In this paper we study shallow handlers, defined instead by case splits over computation trees. We show that deep and shallow handlers can simulate one another up to specific notions of administrative reduction. We present the first formal accounts of an abstract machine for shallow handlers and a Continuation Passing Style (CPS) translation for shallow handlers taking special care to avoid memory leaks. We provide implementations in the Links web programming language and empirically verify that neither implementation introduces unwarranted memory leaks.', 'corpus_id': 49392755, 'score': 1}, {'doc_id': '212745479', 'title': 'Effects for Efficiency', 'abstract': 'ANONYMOUS AUTHOR(S) As Filinski showed in the 1990s, delimited control operators can express all monadic effects. Plotkin and Pretnar’s effect handlers offer a modular form of delimited control providing a uniform mechanism for concisely implementing features ranging from async/await to probabilistic programming. We study the fundamental efficiency of delimited control. Specifically, we show that effect handlers enable an asymptotic improvement in runtime complexity for a certain class of programs. We consider the generic search problem and define a pure PCF-like base language λb and its extension with effect handlers λh. We show that λh admits an asymptotically more efficient implementation of generic search than any λb implementation of generic search. We also show that this efficiency gap remains when λb is extended with mutable state. To our knowledge this result is the first of its kind for control operators.', 'corpus_id': 212745479, 'score': 1}, {'doc_id': '235129395', 'title': 'Practical normalization by evaluation for EDSLs', 'abstract': 'Embedded domain-specific languages (eDSLs) are typically implemented in a rich host language, such as Haskell, using a combination of deep and shallow embedding techniques. While such a combination enables programmers to exploit the execution mechanism of Haskell to build and specialize eDSL programs, it blurs the distinction between the host language and the eDSL. As a consequence, extension with features such as sums and effects requires a significant amount of ingenuity from the eDSL designer. In this paper, we demonstrate that Normalization by Evaluation (NbE) provides a principled framework for building, extending, and customizing eDSLs. We present a comprehensive treatment of NbE for deeply embedded eDSLs in Haskell that involves a rich set of features such as sums, arrays, exceptions and state, while addressing practical concerns about normalization such as code expansion and the addition of domain-specific features.', 'corpus_id': 235129395, 'score': 0}, {'doc_id': '233874213', 'title': 'Introducing ⦇\u202fλ\u202f⦈, a λ-calculus for effectful computation', 'abstract': 'Abstract We present ⦇ λ ⦈ , a calculus with special constructions for dealing with effects and handlers. This is an extension of the simply-typed λ-calculus (STLC). We enrich STLC with a type for representing effectful computations alongside with operations to create and process values of this type. The calculus is motivated by natural language modelling, and especially semantic representation. Traditionally, the meaning of a sentence is calculated using λ-terms, but some semantic phenomena need more flexibility. In this article we introduce the calculus and show that the calculus respects the laws of algebraic structures and it enjoys strong normalisation. To do so, confluence is proven using the Combinatory Reduction Systems (CRSs) of Klop and termination using the Inductive Data Type Systems (IDTSs) of Blanqui.', 'corpus_id': 233874213, 'score': 1}, {'doc_id': '235446313', 'title': 'Introducing Type Properties', 'abstract': 'In type theory, we can express many practical ideas by attributing some additional data to expressions we operate on during compilation. For instance, some substructural type theories augment variables’ typing judgments with the information of their usage. That is, they allow one to explicitly state how many times — 0, 1, or many — a variable can be used. This solves the problem of resource usage control and allows us to treat variables as resources. What’s more, it often happens that this attributed information is interpreted (used) during the same compilation and erased before we run a program. A case in the point is that in the same substructural type theories, their type checkers use these 0, 1, or many, to ensure that all variables are used as many times as these attributions say them to be. Yet, there wasn’t any programming language concept whose concern would be to allow a programmer to express these attributions in the language itself. That is, to let the programmer express which data the one wants to attribute to what expressions and, most importantly, the meaning of the attributed data in their program. As it turned out, the presence of such a concept allows us to express many practical ideas in the language itself. For instance, with appropriate means for assigning the meaning of these attributions, this concept would allow one to express linear types as functionality in a separate program module, without the need to refine the whole type system to add them. In this paper, we present such a concept — we propose type properties. It allows a programmer to express these attributions while fulfilling the requirement of being fully on the static level. That is, it allows one to express how to interpret these attributions during compilation and erases them before a program is passed to the runtime.', 'corpus_id': 235446313, 'score': 0}, {'doc_id': '235749610', 'title': 'Derivation of a Virtual Machine For Four Variants of Delimited-Control Operators', 'abstract': 'This paper derives an abstract machine and a virtual machine for the λ-calculus with four variants of delimited-control operators: shift/reset, control/prompt, shift0/reset0, and control0/prompt0. Starting from Shan’s definitional interpreter for the four operators, we successively apply various meaning-preserving transformations. Both trails of invocation contexts (needed for control and control0) and metacontinuations (needed for shift0 and control0) are defunctionalized and eventually represented as a list of stack frames. The resulting virtual machine clearly models not only how the control operators and captured continuations behave but also when and which portion of stack frames is copied to the heap. 2012 ACM Subject Classification Theory of computation → Control primitives; Theory of computation → Lambda calculus; Theory of computation → Operational semantics; Theory of computation → Abstract machines; Software and its engineering → Virtual machines', 'corpus_id': 235749610, 'score': 0}, {'doc_id': '235795483', 'title': 'Approximate Normalization and Eager Equality Checking for Gradual Inductive Families', 'abstract': 'ing Out Propositional Consistency. While it is easy to compare and compose members of N, not all compositions are so simple. We need a representation of evidence that captures plausible equalities between any two gradual values of the same type. Since inductive families may be indexed by any type, we need a general way of tracking indices and composing evidence. This is precisely the purpose of propositional consistency: just as the static refl : t ≡T t witnesses propositional equality of two terms, in GrInd reflt⊢t1 t2 : t1 Tt2 witnesses that t1 and t2 could plausibly be equal, given their imprecision. t is a term which is as precise as both t1 and t2. If t is not℧, then we cannot immediately conclude that t1 and t2 are inconsistent. GrEq has a propositional equality type with the usual constructors, but this elaborates to propositional consistency, so that the equalities can be tracked during normalization. We elaborate refl with the initial evidence t1 ⊓T t2. As the notation suggests, this is the precision greatest lowerbound of t1 and t2, so it is the least precise evidence for the consistency of t1 and t2. Once we have propositional consistency, we can handle inductive families. Elaboration uses a known technique where all inductive families are turned into parameterized inductive types, so that each constructor has the same return type, but takes equality proofs as arguments to constrain the indices [Chapman et al. 2010; McBride 2000]. For example, Vec is represented as data Vec : (X : Type) → (n : N) → Type where Nil : n N0 → Vec X n | Cons : (m : Nat) → X → Vec X m > n N1 +m → Vec X n Proc. ACM Program. Lang., Vol. 37, No. 4, Article 111. Publication date: July 2021. 111:22 Eremondi, Garcia, and Tanter Γ ⊢ t : T (CastEq typing, new rules) CEq Γ ⊢ T ⇒ Typel Γ ⊢ t1 ⇐ T Γ ⊢ t2 ⇐ T Γ ⊢ t1 Tt2 ⇒ Typel CRefl Γ ⊢ t1 ⇒ T Γ ⊢ t2 ⇐ T Γ ⊢ t3 ⇐ T Γ ⊢ t1 ⊑ t2 Γ ⊢ t1 ⊑ t3 Γ ⊢ reflt1⊢t2 t3 ⇒ t2 T t3 CComp Γ ⊢ t1 ⇒ T Γ ⊢ t2 ⇐ T', 'corpus_id': 235795483, 'score': 0}, {'doc_id': '1902852', 'title': 'Handlers in action', 'abstract': ""Plotkin and Pretnar's handlers for algebraic effects occupy a sweet spot in the design space of abstractions for effectful computation. By separating effect signatures from their implementation, algebraic effects provide a high degree of modularity, allowing programmers to express effectful programs independently of the concrete interpretation of their effects. A handler is an interpretation of the effects of an algebraic computation. The handler abstraction adapts well to multiple settings: pure or impure, strict or lazy, static types or dynamic types. This is a position paper whose main aim is to popularise the handler abstraction. We give a gentle introduction to its use, a collection of illustrative examples, and a straightforward operational semantics. We describe our Haskell implementation of handlers in detail, outline the ideas behind our OCaml, SML, and Racket implementations, and present experimental results comparing handlers with existing code."", 'corpus_id': 1902852, 'score': 1}]"
123	{'doc_id': '86380746', 'title': 'Toward Contextual Information Retrieval: A Review And Trends', 'abstract': 'Abstract With the growth of electronic data and the expansion of the World Wide Web (WWW), many classic existing retrieval models and systems ignore information about the actual user and search context. Due to the constraints imposed by this fact, context has received more attention in the information retrieval (IR) literature and its interactions over the past decade. In this paper, we emphasize on the importance and implications of context in information retrieval and how can it affect the retrieval systems to operate and behave more intelligently; we highlight some emerging trends of context; we present variety of practical uses of context along with its taxonomies and levels; we discuss how can we model context by these systems along with proposing some practical recommendations to enhance this research area for future research.', 'corpus_id': 86380746}	5160	"[{'doc_id': '215737055', 'title': 'CSRN: Collaborative Sequential Recommendation Networks for News Retrieval', 'abstract': 'Nowadays, news apps have taken over the popularity of paper-based media, providing a great opportunity for personalization. Recurrent Neural Network (RNN)-based sequential recommendation is a popular approach that utilizes users\' recent browsing history to predict future items. This approach is limited that it does not consider the societal influences of news consumption, i.e., users may follow popular topics that are constantly changing, while certain hot topics might be spreading only among specific groups of people. Such societal impact is difficult to predict given only users\' own reading histories. On the other hand, the traditional User-based Collaborative Filtering (UserCF) makes recommendations based on the interests of the ""neighbors"", which provides the possibility to supplement the weaknesses of RNN-based methods. However, conventional UserCF only uses a single similarity metric to model the relationships between users, which is too coarse-grained and thus limits the performance. In this paper, we propose a framework of deep neural networks to integrate the RNN-based sequential recommendations and the key ideas from UserCF, to develop Collaborative Sequential Recommendation Networks (CSRNs). Firstly, we build a directed co-reading network of users, to capture the fine-grained topic-specific similarities between users in a vector space. Then, the CSRN model encodes users with RNNs, and learns to attend to neighbors and summarize what news they are reading at the moment. Finally, news articles are recommended according to both the user\'s own state and the summarized state of the neighbors. Experiments on two public datasets show that the proposed model outperforms the state-of-the-art approaches significantly.', 'corpus_id': 215737055, 'score': 0}, {'doc_id': '216022609', 'title': 'Evaluation in Contextual Information Retrieval', 'abstract': 'Context such as the user’s search history, demographics, devices, and surroundings, has become prevalent in various domains of information seeking and retrieval such as mobile search, task-based se...', 'corpus_id': 216022609, 'score': 1}, {'doc_id': '52179704', 'title': 'Evaluation in Contextual Information Retrieval', 'abstract': 'Context such as the user’s search history, demographics, devices, and surroundings, has become prevalent in various domains of information seeking and retrieval such as mobile search, task-based search, and social search. While evaluation is central and has a long history in information retrieval, it faces the big challenge of designing an appropriate methodology that embeds the context into evaluation settings. In this article, we present a unified summary of a wide range of main and recent progress in contextual information retrieval evaluation that leverages diverse context dimensions and uses different principles, methodologies, and levels of measurements. More specifically, this survey article aims to fill two main gaps in the literature: First, it provides a critical summary and comparison of existing contextual information retrieval evaluation methodologies and metrics according to a simple stratification model; second, it points out the impact of context dynamicity and data privacy on the evaluation design. Finally, we recommend promising research directions for future investigations.', 'corpus_id': 52179704, 'score': 1}, {'doc_id': '218900643', 'title': 'How to Retrain Recommender System?: A Sequential Meta-Learning Method', 'abstract': 'Practical recommender systems need be periodically retrained to refresh the model with new interaction data. To pursue high model fidelity, it is usually desirable to retrain the model on both historical and new data, since it can account for both long-term and short-term user preference. However, a full model retraining could be very time-consuming and memory-costly, especially when the scale of historical data is large. In this work, we study the model retraining mechanism for recommender systems, a topic of high practical values but has been relatively little explored in the research community. Our first belief is that retraining the model on historical data is unnecessary, since the model has been trained on it before. Nevertheless, normal training on new data only may easily cause overfitting and forgetting issues, since the new data is of a smaller scale and contains fewer information on long-term user preference. To address this dilemma, we propose a new training method, aiming to abandon the historical data during retraining through learning to transfer the past training experience.Specifically, we design a neural network-based transfer component, which transforms the old model to a new model that is tailored for future recommendations. To learn the transfer component well, we optimize the ""future performance\'\' -- i.e., the recommendation accuracy evaluated in the next time period. Our Sequential Meta-Learning(SML) method offers a general training paradigm that is applicable to any differentiable model. We demonstrate SML on matrix factorization and conduct experiments on two real-world datasets. Empirical results show that SML not only achieves significant speed-up, but also outperforms the full model retraining in recommendation accuracy, validating the effectiveness of our proposals. We release our codes at: https://github.com/zyang1580/SML.', 'corpus_id': 218900643, 'score': 0}, {'doc_id': '218889328', 'title': 'How to Grow a (Product) Tree: Personalized Category Suggestions for eCommerce Type-Ahead', 'abstract': 'In an attempt to balance precision and recall in the search page, leading digital shops have been effectively nudging users into select category facets as early as in the type-ahead suggestions. In this work, we present SessionPath, a novel neural network model that improves facet suggestions on two counts: first, the model is able to leverage session embeddings to provide scalable personalization; second, SessionPath predicts facets by explicitly producing a probability distribution at each node in the taxonomy path. We benchmark SessionPath on two partnering shops against count-based and neural models, and show how business requirements and model behavior can be combined in a principled way.', 'corpus_id': 218889328, 'score': 0}, {'doc_id': '215989695', 'title': 'Modeling Queries with Contextual Snippets for Information Retrieval', 'abstract': 'Query expansion under the pseudo-relevance feedback (PRF) framework has been extensively studied in information retrieval. However, most expansion methods are mainly based on the statistics of sing...', 'corpus_id': 215989695, 'score': 1}, {'doc_id': '8312085', 'title': 'Exploring Query Auto-Completion and Click Logs for Contextual-Aware Web Search and Query Suggestion', 'abstract': ""Contextual data plays an important role in modeling search engine users' behaviors on both query auto-completion (QAC) log and normal query (click) log. User's recent search history on each log has been widely studied individually as the context to benefit the modeling of users' behaviors on that log. However, there is no existing work that explores or incorporates both logs together for contextual data. As QAC and click logs actually record users' sequential behaviors while interacting with a search engine, the available context of a user's current behavior based on the same type of log can be strengthened from the user's recent search history shown on the other type of log. Our paper proposes to model users' behaviors on both QAC and click logs simultaneously by utilizing both logs as the contextual data of each other. The key idea is to capture the correlation between users' behavior patterns on both logs. We model such correlation through a novel probabilistic model based on the Latent Dirichlet allocation (LDA) model. The learned users' behavior patterns on both logs are utilized to address not only the application of query auto-completion on QAC logs, but also the click prediction and relevance ranking of web documents on click logs. Experiments on real-world logs demonstrate the effectiveness of the proposed model on both applications."", 'corpus_id': 8312085, 'score': 1}, {'doc_id': '218684696', 'title': 'Table Search Using a Deep Contextualized Language Model', 'abstract': 'Pretrained contextualized language models such as BERT have achieved impressive results on various natural language processing benchmarks. Benefiting from multiple pretraining tasks and large scale training corpora, pretrained models can capture complex syntactic word relations. In this paper, we use the deep contextualized language model BERT for the task of ad hoc table retrieval. We investigate how to encode table content considering the table structure and input length limit of BERT. We also propose an approach that incorporates features from prior literature on table retrieval and jointly trains them with BERT. In experiments on public datasets, we show that our best approach can outperform the previous state-of-the-art method and BERT baselines with a large margin under different evaluation metrics.', 'corpus_id': 218684696, 'score': 0}, {'doc_id': '218685017', 'title': 'Controllable Multi-Interest Framework for Recommendation', 'abstract': ""Recently, neural networks have been widely used in e-commerce recommender systems, owing to the rapid development of deep learning. We formalize the recommender system as a sequential recommendation problem, intending to predict the next items that the user might be interacted with. Recent works usually give an overall embedding from a user's behavior sequence. However, a unified user embedding cannot reflect the user's multiple interests during a period. In this paper, we propose a novel controllable multi-interest framework for the sequential recommendation, called ComiRec. Our multi-interest module captures multiple interests from user behavior sequences, which can be exploited for retrieving candidate items from the large-scale item pool. These items are then fed into an aggregation module to obtain the overall recommendation. The aggregation module leverages a controllable factor to balance the recommendation accuracy and diversity. We conduct experiments for the sequential recommendation on two real-world datasets, Amazon and Taobao. Experimental results demonstrate that our framework achieves significant improvements over state-of-the-art models. Our framework has also been successfully deployed on the offline Alibaba distributed cloud platform."", 'corpus_id': 218685017, 'score': 0}, {'doc_id': '210882724', 'title': 'Beyond Sessions: Exploiting Hybrid Contextual Information for Web Search', 'abstract': 'It is essential to fully understand user intents for the optimization of downstream tasks such as document ranking and query suggestion in web search. As users tend to submit ambiguous queries, numer- ous studies utilize contextual information such as query sequence and user clicks for the auxiliary of user intent modeling. Most of these work adopted Recurrent Neural Network (RNN) based frame- works to encode sequential information within a session, which is hard to realize parallel computation. To this end, we plan to adopt attention-based units to generate context-aware representations for elements in sessions. As intra-session contexts are deficient for handling the data sparsity and cold-start problems in session search, we would also attempt to integrate cross-session dependen- cies by constructing session graphs on the whole corpus to enrich the representation of queries and documents.', 'corpus_id': 210882724, 'score': 1}]"
124	"{'doc_id': '135052648', 'title': 'Using rule-based regression models to predict and interpret soil properties from X-ray powder diffraction data', 'abstract': ""Abstract Data mining is often used to derive calibrations for soil property prediction from diffuse reflectance spectroscopy, facilitating inference of organic and mineral contributions to given properties. In contrast to spectroscopy, X-ray powder diffraction (XRPD) offers a more direct probe into the complexities of soil mineralogy. Here a national scale XRPD dataset of Scottish soils is used in combination with the rule-based regression algorithm ‘Cubist’ for prediction of eight soil properties (total carbon and nitrogen, cation exchange capacity, pH, aqua regia extractable potassium, and the sand, silt and clay size fractions), and interpretation of soil property–mineralogy relationships. Precision sample preparation methods prior to XRPD analysis eliminated effects of preferred orientation, creating reproducible data appropriate for data mining. For direct comparison, Cubist was also applied to an equivalent dataset of near infrared spectroscopy (NIRS) measurements. In terms of predictive performance, XRPD surpassed NIRS for prediction of six of the eight soil properties investigated. Notably, diffuse scattering from X-ray amorphous organic matter facilitated relatively accurate predictions of total carbon and nitrogen from XRPD. Aqua regia extractable potassium was predicted with substantial accuracy and confirmed to reflect the phyllosilicate potassium. The particle size fractions were predicted with moderate-substantial agreement using combinations of quartz, phyllosilicate and feldspar variables. This approach introduces the value of XRPD datasets in enhancing the understanding of soil mineralogy–property relationships whilst contributing to soil mineralogy's advance into the digital soil typing paradigm."", 'corpus_id': 135052648}"	10895	[{'doc_id': '73458884', 'title': 'Soil granular dynamics on-a-chip: fluidization inception under scrutiny.', 'abstract': 'Predicting soil evolution remains a scientific challenge. This process involves poorly understood aspects of disordered granular matter and dense suspension dynamics. This study presents a novel two-dimensional experiment on a small-scale chip structure; this allows the observation of the deformation at the particle scale of a large-grained sediment bed, under conditions where friction dominates over cohesive and thermal forces, and with an imposed fluid flow. Experiments are performed under conditions which span the particle resuspension criterion, and particle motion is detected and analyzed. The void size population and statistics of particle trajectories bring insight into the sediment dynamics near fluidization conditions. Specifically, particle rearrangement and net bed compaction are observed at flow rates significantly below the criterion for instability growth. Above a threshold flowrate, a channel forms and grows in the vertical direction; and eventually it crosses the entire bed. In the range of flow rates where channelization can occur, the coexistence of compacting and dilating bed scenarios is observed. The results of the study enhance our capacity for modeling of both slow dynamics and eventual rapid destabilization of sediment beds. Microfluidic channel soil-on-a-chip studies open avenues to new investigations including dissolution-precipitation, fine particle transport, or micro-organism swimming and population growth, which may depend on the mechanics of the porous medium itself.', 'corpus_id': 73458884, 'score': 0}, {'doc_id': '137172688', 'title': 'Construction and calibration of a low cost X-ray Fluorescence apparatus for compositional analysis of materials', 'abstract': None, 'corpus_id': 137172688, 'score': 1}, {'doc_id': '139172594', 'title': 'Portable grazing exit X-ray fluorescence system using a low-power X-ray tube', 'abstract': 'In this work was developed a portable system of grazing exit X-ray fluorescence (geometric 90° - 0°) that can be applied in several areas science and technology. GE-XRF portable system is formed by a mini X-ray tube of low power (anode of Au) and a SiPIN detector. The reflectors used as sample support (sampler carrier) were quartz discs. The grazing exit angle was experimentally determined by measuring a cooper solution (10 μg.g-1). The accuracy of the system was checked using multielement reference solution as standard reference material. The relative errors between measured and certified values are in the range of 4 to 19%. The first results showed a background was drastically reduced at grazing exit angles, enabling trace elemental analysis. The system of GE-XRF proved to be quite stable and reproducible. This paper shows that it is possible to produce a portable system of grazing exit X-ray fluorescence compact, efficient, low-cost and easy-to-handle instrumentation using a low power X-ray tube and a SiPIN compact detector.', 'corpus_id': 139172594, 'score': 1}, {'doc_id': '222141641', 'title': 'Global soil moisture from in-situ measurements using machine learning - SoMo.ml', 'abstract': 'While soil moisture information is essential for a wide range of hydrologic and climate applications, spatially-continuous soil moisture data is only available from satellite observations or model simulations. Here we present a global, long-term dataset of soil moisture generated from in-situ measurements using machine learning, this http URL. We train a Long Short-Term Memory (LSTM) model to extrapolate daily soil moisture dynamics in space and in time, based on in-situ data collected from more than 1,000 stations across the globe. this http URL provides multi-layer soil moisture data (0-10 cm, 10-30 cm, and 30-50 cm) at 0.25° spatial and daily temporal resolution over the period 2000-2019. The performance of the resulting dataset is evaluated through cross validation and inter-comparison with existing soil moisture datasets. this http URL performs especially well in terms of temporal dynamics, making it particularly useful for applications requiring time-varying soil moisture, such as anomaly detection and memory analyses. this http URL complements the existing suite of modelled and satellite-based datasets given its independent and novel derivation, to support large-scale hydrological, meteorological, and ecological analyses.', 'corpus_id': 222141641, 'score': 1}, {'doc_id': '191151013', 'title': 'Microsegmented flow-assisted miniaturized culturing for isolation and characterization of heavy metal-tolerant bacteria', 'abstract': 'Soils are complex ecosystem, and their function in the environment is mainly determined by the microbial communities. Metal-tolerant micro-organisms have an important function in the formation of soil and the development of microbial communities in all areas where heavy metals are released by natural erosion processes or by human activities. The investigation of dose-dependent growth and behaviour is an essential part of the search for heavy metal-tolerant microorganism communities and their characterization. In this study, next-generation sequencing was used for the analysis of soil sample and reduced communities and droplet-based microfluidics was used to assess the growth behaviour of unknown bacterial communities and single strains in response to different heavy metal ions. Highly resolved dose–response functions of the bacterial communities reflect the specific character in their concentration-dependent response to different culture media and heavy metals of copper, nickel and cobalt. Besides the characterization of community responses, they allowed to characterize newly isolated strains. Concentration-dependent growth patterns of the micro-organisms in the droplets could be observed. The investigation demonstrates the potential of droplet-based microfluidics for miniaturized eco-toxicological studies and their suitability for the discovery of novel strains with special tolerance features.', 'corpus_id': 191151013, 'score': 0}, {'doc_id': '212830043', 'title': 'Custom-engineered micro-habitats for characterizing rhizosphere interactions', 'abstract': 'The interactions amongst plants and microorganisms within the rhizosphere have a profound influence on global biogeochemical cycles, and a better understanding of these interactions will benefit society through improved climate change prediction, increased food security, and enhanced bioenergy production. However, the rhizosphere is one of the most complex and bio-diverse ecosystems on earth, making it difficult to parse apart specific interactions between species. This difficulty is compounded by the inability to directly visualize rhizosphere interactions through the soil. Additionally, conventional laboratory techniques do not offer real-time, high-resolution visualization or the proper environmental control to isolate and probe these interactions. A knowledge gap persists in how to design appropriate culturing platforms that allow researchers to collect spatially and temporally sensitive information about physical and chemical interactions in the rhizosphere. This dissertation addresses that gap by demonstrating the design and use of several customengineered micro-habitats in characterizing plant-microbe interactions. Specifically this thesis introduces novel protocols for culturing plants and microorganisms together in microfluidic platforms, pairing platforms to multi-modal imaging techniques with organelle scale resolution, and recreating the structural complexity of the rhizosphere in a microfluidic habitat. Not only does this thesis introduce novel engineered systems, but the work contained herein also goes beyond proof-of-concept experiments and demonstrates the ability of these platforms to generate hypotheses and answer outstanding biological questions.', 'corpus_id': 212830043, 'score': 0}, {'doc_id': '226290197', 'title': 'X-ray imaging detector for radiological applications in the harsh environments of low-income countries', 'abstract': 'This paper describes the development of a novel medical Xray imaging system adapted to the needs and constraints of low and middle income countries. The developed system is based on an indirect conversion chain: a scintillator plate produces visible light when excited by the Xrays, then a calibrated multi camera architecture converts the visible light from the scintillator into a set of digital images. The partial images are then unwarped, enhanced and stitched through parallel processing units and a specialized software. All the detector components were carefully selected focusing on optimizing the system s image quality, robustness, cost, effectiveness and capability to work in harsh tropical environments. With this aim, different customized and commercial components were characterized. The resulting detector can generate high quality medical diagnostic images with DQE levels up to 60 percent, at 2.34 micro Gray, even under harsh environments i.e. 60 degrees Celsius and 98 percent humidity.', 'corpus_id': 226290197, 'score': 0}, {'doc_id': '52092152', 'title': 'Emergent Properties of Microbial Activity in Heterogeneous Soil Microenvironments: Different Research Approaches Are Slowly Converging, Yet Major Challenges Remain', 'abstract': 'Over the last 60 years, soil microbiologists have accumulated a wealth of experimental data showing that the bulk, macroscopic parameters (e.g., granulometry, pH, soil organic matter, and biomass contents) commonly used to characterize soils provide insufficient information to describe quantitatively the activity of soil microorganisms and some of its outcomes, like the emission of greenhouse gasses. Clearly, new, more appropriate macroscopic parameters are needed, which reflect better the spatial heterogeneity of soils at the microscale (i.e., the pore scale) that is commensurate with the habitat of many microorganisms. For a long time, spectroscopic and microscopic tools were lacking to quantify processes at that scale, but major technological advances over the last 15 years have made suitable equipment available to researchers. In this context, the objective of the present article is to review progress achieved to date in the significant research program that has ensued. This program can be rationalized as a sequence of steps, namely the quantification and modeling of the physical-, (bio)chemical-, and microbiological properties of soils, the integration of these different perspectives into a unified theory, its upscaling to the macroscopic scale, and, eventually, the development of new approaches to measure macroscopic soil characteristics. At this stage, significant progress has been achieved on the physical front, and to a lesser extent on the (bio)chemical one as well, both in terms of experiments and modeling. With regard to the microbial aspects, although a lot of work has been devoted to the modeling of bacterial and fungal activity in soils at the pore scale, the appropriateness of model assumptions cannot be readily assessed because of the scarcity of relevant experimental data. For significant progress to be made, it is crucial to make sure that research on the microbial components of soil systems does not keep lagging behind the work on the physical and (bio)chemical characteristics. Concerning the subsequent steps in the program, very little integration of the various disciplinary perspectives has occurred so far, and, as a result, researchers have not yet been able to tackle the scaling up to the macroscopic level. Many challenges, some of them daunting, remain on the path ahead. Fortunately, a number of these challenges may be resolved by brand new measuring equipment that will become commercially available in the very near future.', 'corpus_id': 52092152, 'score': 0}, {'doc_id': '104363410', 'title': 'DIY XES - development of an inexpensive, versatile, and easy to fabricate XES analyzer and sample delivery system.', 'abstract': 'The application of X-ray emission spectroscopy (XES) has grown substantially with the development of X-ray free electron lasers, third and fourth generation synchrotron sources and high-power benchtop sources. By providing the high X-ray flux required for XES, these sources broaden the availability and application of this method of probing electronic structure. As the number of sources increase, so does the demand for X-ray emission detection and sample delivery systems that are cost effective and customizable. Here, we present a detailed fabrication protocol for von Hamos X-ray optics and give details for a 3D-printed spectrometer design. Additionally, we outline an automated, externally triggered liquid sample delivery system that can be used to repeatedly deliver nanoliter droplets onto a plastic substrate for measurement. These systems are both low cost, efficient and easy to recreate or modify depending on the application. A low cost multiple X-ray analyzer system enables measurement of dilute samples, whereas the sample delivery limits sample loss and replaces spent sample with fresh sample in the same position. While both systems can be used in a wide range of applications, the design addresses several challenges associated specifically with time-resolved XES (TRXES). As an example application, we show results from TRXES measurements of photosystem II, a dilute, photoactive protein.', 'corpus_id': 104363410, 'score': 1}, {'doc_id': '214016195', 'title': 'Tropical soil pH and sorption complex prediction via portable X-ray fluorescence spectrometry', 'abstract': 'Abstract Portable X-ray fluorescence (pXRF) spectrometry delivers results rapidly, at low-cost, and without generating chemical residues. This study aimed to predict soil pH, sum of bases (SB), base saturation percentage (BSP), cation exchange capacity (CEC), and Al saturation (Alsat) of 2017 contrasting Brazilian soil samples through the association of pXRF and three different algorithms [Cubist, Random forest (RF), and stepwise multiple linear regression (SMLR)]. Soil samples were collected from the surface (SURF) and subsurface (SUB) horizons in seven Brazilian states. The prediction models were generated for the SURF and SUB horizons separately and combined (SURF\xa0+\xa0SUB dataset). Overall, the best predictions were achieved via Cubist followed by RF. For the pH predictions, the model combining SURF and SUB horizons data presented better results. Satisfactory results were achieved for the predictions of SB (validation R2\xa0=\xa00.86), BSP (validation R2\xa0=\xa00.81) and Alsat (R2\xa0=\xa00.76). Moreover, promising results were obtained for predicting pH (R2\xa0=\xa00.63). Notably, CaO appeared as the most influential variable for soil property prediction models. Overall, pXRF showed great potential for predicting soil fertility properties for diversified tropical soils with low cost, rapidity, and without chemical waste generation.', 'corpus_id': 214016195, 'score': 1}]
125	{'doc_id': '201058594', 'title': 'Few-shot Text Classification with Distributional Signatures', 'abstract': 'In this paper, we explore meta-learning for few-shot text classification. Meta-learning has shown strong performance in computer vision, where low-level patterns are transferable across learning tasks. However, directly applying this approach to text is challenging--lexical features highly informative for one task may be insignificant for another. Thus, rather than learning solely from words, our model also leverages their distributional signatures, which encode pertinent word occurrence patterns. Our model is trained within a meta-learning framework to map these signatures into attention scores, which are then used to weight the lexical representations of words. We demonstrate that our model consistently outperforms prototypical networks learned on lexical knowledge (Snell et al., 2017) in both few-shot text classification and relation classification by a significant margin across six benchmark datasets (20.0% on average in 1-shot classification).', 'corpus_id': 201058594}	285	[{'doc_id': '3507990', 'title': 'Meta-Learning for Semi-Supervised Few-Shot Classification', 'abstract': 'In few-shot classification, we are interested in learning algorithms that train a classifier from only a handful of labeled examples. Recent progress in few-shot classification has featured meta-learning, in which a parameterized model for a learning algorithm is defined and trained on episodes representing different classification problems, each with a small labeled training set and its corresponding test set. In this work, we advance this few-shot classification paradigm towards a scenario where unlabeled examples are also available within each episode. We consider two situations: one where all unlabeled examples are assumed to belong to the same set of classes as the labeled examples of the episode, as well as the more challenging situation where examples from other distractor classes are also provided. To address this paradigm, we propose novel extensions of Prototypical Networks (Snell et al., 2017) that are augmented with the ability to use unlabeled examples when producing prototypes. These models are trained in an end-to-end way on episodes, to learn to leverage the unlabeled examples successfully. We evaluate these methods on versions of the Omniglot and miniImageNet benchmarks, adapted to this new framework augmented with unlabeled examples. We also propose a new split of ImageNet, consisting of a large set of classes, with a hierarchical structure. Our experiments confirm that our Prototypical Networks can learn to improve their predictions due to unlabeled examples, much like a semi-supervised algorithm would.', 'corpus_id': 3507990, 'score': 1}, {'doc_id': '220793786', 'title': 'Few-Shot Bearing Anomaly Detection Based on Model-Agnostic Meta-Learning', 'abstract': 'The rapid development of artificial intelligence and deep learning technology has provided many opportunities to further enhance the safety, stability, and accuracy of industrial Cyber-Physical Systems (CPS). As indispensable components to many mission-critical CPS assets and equipment, mechanical bearings need to be monitored to identify any trace of abnormal conditions. Most of the data-driven approaches applied to bearing anomaly detection up-to-date are trained using a large amount of fault data collected a priori. In many practical applications, however, it can be unsafe and time-consuming to collect sufficient data samples for each fault category, making it challenging to train a robust classifier. In this paper, we propose a few-shot learning approach for bearing anomaly detection based on model-agnostic meta-learning (MAML), which targets for training an effective fault classifier using limited data. In addition, it can leverage the training data and learn to identify new fault scenarios more efficiently. Case studies on the generalization to new artificial faults show that the proposed method achieves an overall accuracy up to 25% higher than a Siamese-network-based benchmark study. Finally, the robustness of the generalization capability of MAML is further validated by case studies of applying the algorithm to identify real bearing damages using data from artificial damages.', 'corpus_id': 220793786, 'score': 0}, {'doc_id': '207852358', 'title': 'Learning to Few-Shot Learn Across Diverse Natural Language Classification Tasks', 'abstract': 'Pre-trained transformer models have shown enormous success in improving performance on several downstream tasks. However, fine-tuning on a new task still requires large amounts of task-specific labeled data to achieve good performance. We consider this problem of learning to generalize to new tasks, with a few examples, as a meta-learning problem. While meta-learning has shown tremendous progress in recent years, its application is still limited to simulated problems or problems with limited diversity across tasks. We develop a novel method, LEOPARD, which enables optimization-based meta-learning across tasks with a different number of classes, and evaluate different methods on generalization to diverse NLP classification tasks. LEOPARD is trained with the state-of-the-art transformer architecture and shows better generalization to tasks not seen at all during training, with as few as 4 examples per label. Across 17 NLP tasks, including diverse domains of entity typing, natural language inference, sentiment analysis, and several other text classification tasks, we show that LEOPARD learns better initial parameters for few-shot learning than self-supervised pre-training or multi-task training, outperforming many strong baselines, for example, yielding 14.6% average relative gain in accuracy on unseen tasks with only 4 examples per label.', 'corpus_id': 207852358, 'score': 1}, {'doc_id': '220793646', 'title': 'Receptive-Field Regularized CNNs for Music Classification and Tagging', 'abstract': 'Convolutional Neural Networks (CNNs) have been successfully used in various Music Information Retrieval (MIR) tasks, both as end-to-end models and as feature extractors for more complex systems. However, the MIR field is still dominated by the classical VGG-based CNN architecture variants, often in combination with more complex modules such as attention, and/or techniques such as pre-training on large datasets. Deeper models such as ResNet -- which surpassed VGG by a large margin in other domains -- are rarely used in MIR. One of the main reasons for this, as we will show, is the lack of generalization of deeper CNNs in the music domain. In this paper, we present a principled way to make deep architectures like ResNet competitive for music-related tasks, based on well-designed regularization strategies. In particular, we analyze the recently introduced Receptive-Field Regularization and Shake-Shake, and show that they significantly improve the generalization of deep CNNs on music-related tasks, and that the resulting deep CNNs can outperform current more complex models such as CNNs augmented with pre-training and attention. We demonstrate this on two different MIR tasks and two corresponding datasets, thus offering our deep regularized CNNs as a new baseline for these datasets, which can also be used as a feature-extracting module in future, more complex approaches.', 'corpus_id': 220793646, 'score': 0}, {'doc_id': '208909759', 'title': 'Meta-Learning without Memorization', 'abstract': 'The ability to learn new concepts with small amounts of data is a critical aspect of intelligence that has proven challenging for deep learning methods. Meta-learning has emerged as a promising technique for leveraging data from previous tasks to enable efficient learning of new tasks. However, most meta-learning algorithms implicitly require that the meta-training tasks be mutually-exclusive, such that no single model can solve all of the tasks at once. For example, when creating tasks for few-shot image classification, prior work uses a per-task random assignment of image classes to N-way classification labels. If this is not done, the meta-learner can ignore the task training data and learn a single model that performs all of the meta-training tasks zero-shot, but does not adapt effectively to new image classes. This requirement means that the user must take great care in designing the tasks, for example by shuffling labels or removing task identifying information from the inputs. In some domains, this makes meta-learning entirely inapplicable. In this paper, we address this challenge by designing a meta-regularization objective using information theory that places precedence on data-driven adaptation. This causes the meta-learner to decide what must be learned from the task training data and what should be inferred from the task testing input. By doing so, our algorithm can successfully use data from non-mutually-exclusive tasks to efficiently adapt to novel tasks. We demonstrate its applicability to both contextual and gradient-based meta-learning algorithms, and apply it in practical settings where applying standard meta-learning has been difficult. Our approach substantially outperforms standard meta-learning algorithms in these settings.', 'corpus_id': 208909759, 'score': 1}, {'doc_id': '186786502', 'title': 'Geoquímica das rochas metassedimentares clásticas do grupo Igarapé Bahia, bacia Carajás', 'abstract': 'A sequência superior do Grupo Igarapé Bahia consiste de rochas metassedimentares, caracterizadas por metarritimitos metagrauvacas. Estratigraficamente acima, ocorrem metagrauvacas associadas a Formação Águas Claras. A área fonte dessas unidades sofreram intenso intemperismo químico, uma possível contribuição de rochas básicas e ambientes tectônico distintos.', 'corpus_id': 186786502, 'score': 0}, {'doc_id': '202768423', 'title': 'Meta-Reinforced Synthetic Data for One-Shot Fine-Grained Visual Recognition', 'abstract': 'This paper studies the task of one-shot fine-grained recognition, which suffers from the problem of data scarcity of novel fine-grained classes. To alleviate this problem, a off-the-shelf image generator can be applied to synthesize additional images to help one-shot learning. However, such synthesized images may not be helpful in one-shot fine-grained recognition, due to a large domain discrepancy between synthesized and original images. To this end, this paper proposes a meta-learning framework to reinforce the generated images by original images so that these images can facilitate one-shot learning. Specifically, the generic image generator is updated by few training instances of novel classes; and a Meta Image Reinforcing Network (MetaIRNet) is proposed to conduct one-shot fine-grained recognition as well as image reinforcement. The model is trained in an end-to-end manner, and our experiments demonstrate consistent improvement over baseline on one-shot fine-grained image classification benchmarks.', 'corpus_id': 202768423, 'score': 1}, {'doc_id': '52922125', 'title': 'Unsupervised Learning via Meta-Learning', 'abstract': 'A central goal of unsupervised learning is to acquire representations from unlabeled data or experience that can be used for more effective learning of downstream tasks from modest amounts of labeled data. Many prior unsupervised learning works aim to do so by developing proxy objectives based on reconstruction, disentanglement, prediction, and other metrics. Instead, we develop an unsupervised meta-learning method that explicitly optimizes for the ability to learn a variety of tasks from small amounts of data. To do so, we construct tasks from unlabeled data in an automatic way and run meta-learning over the constructed tasks. Surprisingly, we find that, when integrated with meta-learning, relatively simple task construction mechanisms, such as clustering embeddings, lead to good performance on a variety of downstream, human-specified tasks. Our experiments across four image datasets indicate that our unsupervised meta-learning approach acquires a learning algorithm without any labeled data that is applicable to a wide range of downstream classification tasks, improving upon the embedding learned by four prior unsupervised learning methods.', 'corpus_id': 52922125, 'score': 1}, {'doc_id': '101315499', 'title': 'Application and comparison of two purification technology of gel permeation chromatography and solid phase extraction in the detection of six kinds of prohibited dyes', 'abstract': 'A method to detect six kinds of banned dyes,tonyred I,II,III and IV,para red,rhodamine B,withpretreatment of solid phase extraction purification and gelchromatography respectively,and analysis with high performance liquid chromatography tandem mass spectrometry,was established. In this study,for the different Purification methods,methanol,ethyl acetate and cyclohexane( 1: 1) were used as the extraction solvents,column of C8( 1. 8 μ m,3 × 100 mm) was chosen for the separation,and the six banned dyestuffs were detected by electrospray tandem quadrupole mass spectrometry with the ESI source,in the mode of multiple reaction monitoring( MRM). The detection limit sudan red Ⅰ,Ⅱ,Ⅲ,Ⅳ and para red( LOQ) are0. 005 mg / kg by SPE method,the detection limit of rhodamine B is 0. 002 mg / kg; detction limit for Sudan redⅠ,Ⅱ,Ⅲ,Ⅳ and para red is 0. 010 mg/kg by GPC method,the detection limit of rhodamine B is 0. 005 mg/kg. Comparisons of two kinds of purification methods in the practical application of detection of complex matrix,show that the solid phase extraction purification presents a better effect than gel permeation chromatography on the removal of macromolecules and natural pigment.', 'corpus_id': 101315499, 'score': 0}, {'doc_id': '235097672', 'title': 'Learning to Learn Semantic Factors in Heterogeneous Image Classification', 'abstract': 'Few-shot learning is to recognize novel classes with a few labeled samples per class. Although numerous meta-learning methods have made significant progress, they struggle to directly address the heterogeneity of training and evaluating task distributions, resulting in the domain shift problem when transitioning to new tasks with disjoint spaces. In this paper, we propose a novel method to deal with the heterogeneity. Specifically, by simulating class-difference domain shift during the meta-train phase, a bilevel optimization procedure is applied to learn a transferable representation space that can rapidly adapt to heterogeneous tasks. Experiments demonstrate the effectiveness of our proposed method.', 'corpus_id': 235097672, 'score': 0}]
126	"{'doc_id': '221369400', 'title': 'Accurate Assessment via Process Data', 'abstract': ""Accurate assessment of students' ability is the key task of a test. Assessments based on final responses are the standard. As the infrastructure advances, substantially more information is observed. One of such instances is the process data that is collected by computer-based interactive items, which contain a student's detailed interactive processes. In this paper, we show both theoretically and empirically that appropriately including such information in the assessment will substantially improve relevant assessment precision. The precision is measured empirically by out-of-sample test reliability."", 'corpus_id': 221369400}"	16367	"[{'doc_id': '233255471', 'title': 'Hands, Hearts and Hybrids: Economic Organization, Individual Motivation and Public Benefit', 'abstract': 'Should policy makers rely upon firms motivated by profit and guided by the invisible hand of the market to promote public benefit or upon government agencies or nonprofits motivated by the compassionate heart? In this article we examine how the motivational assumptions implicit in these forms of economic organisation impact on their ability to promote the public good in key areas of the economy. We argue that in many circumstances it would be better to for policy makers to work with hybrid organisations: organisations that aim for both social and economic returns and whose stakeholders display the true complexity of individual motivation.', 'corpus_id': 233255471, 'score': 0}, {'doc_id': '14060788', 'title': 'Self-Enhancement and Learning from Performance Feedback', 'abstract': ""The theory of performance feedback views decision makers as problem solvers seeking to improve performance. By specifying how and when decision makers may instead seek to enhance their self-image by assessing performance as satisfactory, the model presented in this article specifies boundaries on performance feedback theory's critical prediction that low performance induces increased search, change, and risk taking, and it suggests one reason why decision makers sometimes fail to learn from their mistakes."", 'corpus_id': 14060788, 'score': 1}, {'doc_id': '234346172', 'title': 'Peer Effects on Individual Performance in a Team Sport', 'abstract': 'The paper investigates the variety of peer effects on individual performance in a team sport. The individual performance of more than 5,000 soccer players, from 234 teams, between 2010 and 2015, is measured with the help of the FIFA video game simulator developed by EA Sports. The study reveals positive peer effects on individual performance although the marginal benefit decreases. Additionally, team cohesion contributes to an improvement of players’ ranking.', 'corpus_id': 234346172, 'score': 1}, {'doc_id': '233393288', 'title': 'Player Benchmarking and Outcomes: A Behavioral Science Approach', 'abstract': 'Benchmarking is important to monitor and provide feedback to players on their performance. (i.e. are they under or overperforming). Behavioral science research, especially that grounded in social psychology, indicates that performance feedback has a strong impact on human behavior including: individual motivation (Deci, 1972; DeNisi, Randolph, & Blencoe, 1982; Pavett, 1983), risk-taking (Kacperczyk et al., 2015; Krueger Jr & Dickson, 1994; March & Shapira, 1992), and performance (Sehunk, 1984; Smither, London, & Reilly, 2005). As a result, performance feedback that is provided to players has the potential to induce changes in player performance.', 'corpus_id': 233393288, 'score': 1}, {'doc_id': '232484600', 'title': 'An Organizational Change With Quarantined Members', 'abstract': 'This study tackles the extent to which employees’ attributions and acknowledgments of the innovation implementation’s urgency play a role in their acceptance and readiness behavior during a crisis. Moreover, it highlights the importance of support and knowledge sharing among organization members on social media, given that an organizational change is taking place during a crisis while everyone is being quarantined. Qualitative data are collected from semi-structured interviews as well as from the chats on the WhatsApp group created for this quick innovation implementation decision. Findings reveal that during a crisis, employees’ sensemaking of the organization’s innovation adoption is triggered by attribution to constructive intentionality. The urgency imposed boosts the contextual dimension of the readiness for change, which enhances organization members’ commitment to implement the change. Moreover, when everyone is quarantined, social media is found to be the only means for maintaining social relations, ensuring colleagues’ support and sharing knowledge; and consequently boosting members’ readiness. The value of this research lies in the topic addressed, and in the unusual context in which the innovation implementation took place.', 'corpus_id': 232484600, 'score': 0}, {'doc_id': '231968541', 'title': 'Covid-19 : Impact and virtual approaches', 'abstract': '\u200bMeditation is similar to exercise – people know it would be good for them, but it’s difficult to be motivated to do it regularly. Also similarly, many technological supports exist to help people have good experiences. Over 400 college students were randomly assigned to one of four', 'corpus_id': 231968541, 'score': 0}, {'doc_id': '233637553', 'title': 'Learning heuristics, issue salience and polarization in the policy process', 'abstract': 'This paper proposes a theory that links issue salience and actor polarization to the uses of learning heuristics in the policy process. In harkening back to research on public policy, behavioral economics, and crisis management, we argue that policymakers use two learning heuristics when they update their policy ideas: policy-oriented learning and power-oriented learning. The paper develops theoretical expectations that link issue salience and polarization of actor constellations to policy-oriented and power-oriented learning. To illustrate the theoretical expectations, the article discusses EU anti-crisis policies dealing with the 2007-2009 global financial and economic crisis and its aftermath, as well as the responses to the COVID-19 pandemic in Germany and the U.S. as pathway cases. Overall, this paper contributes to the understanding of learning and the political uses of ideas in the policy process.', 'corpus_id': 233637553, 'score': 0}, {'doc_id': '233390551', 'title': 'Searching for Success—Entrepreneurs’ Responses to Crowdfunding Failure', 'abstract': 'In this study, we seek to provide new insights into the process of problemistic search by examining entrepreneurs’ behavioral responses to failures. Using a comprehensive dataset of over 65,000 crowdfunding projects, we specifically explore how negative performance feedback influences entrepreneurs’ search distance. Our results demonstrate that the severity and persistence of failure have a U-shaped and inverted U-shaped relationship with search distance, respectively. Moreover, greater temporal distance between an entrepreneur’s failure experience and a subsequent crowdfunding project is not only associated with increases in search distance, but also attenuates the curvilinear main effects.', 'corpus_id': 233390551, 'score': 1}, {'doc_id': '233549661', 'title': 'Too bad to fear, too good to dare? Performance feedback and corporate misconduct', 'abstract': 'Abstract This study explores how a firm’s performance feedback (relative to its aspirations) may influence its likelihood of involvement in misconduct. We propose and test curvilinear relationships between aspired performance and corporate misconduct by drawing upon the behavioral theory of the firm. We argue that a U-shaped relationship may exist between aspired performance and corporate misconduct when performance feedback is positive (i.e., performance is above aspirations), in that high performance firstly decreases and then increases the likelihood of misconduct. On the contrary, we propose an inverse U-shaped relationship between aspired performance and corporate misconduct when performance feedback is negative (i.e., performance is below aspirations), in that low performance firstly increases and then decreases the likelihood of misconduct. Evidence from Chinese publicly-listed manufacturers largely supports our predictions.', 'corpus_id': 233549661, 'score': 1}, {'doc_id': '233307832', 'title': 'Online Portfolio: An Alternative to a Research Paper as a Final Assessment‡', 'abstract': 'Research papers are a common end-of-semester assessment for senior-level seminars. The mid-semester switch to remote learning due to COVID-19 allowed for the opportunity to rethink this model and to provide for an alternative final assessment using a multi-part online portfolio that addresses the learning goals of the course and the objectives of research paper-based final assessments. ABSTRACT Research papers are a common end-of-semester assessment for senior-level seminars. The mid-semester switch to remote learning due to COVID-19 allowed for the opportunity to rethink this model and to provide for an alternative final assessment using a multi-part online portfolio that addresses the learning goals of the course and the objectives of research paper-based final assessments. The smaller weekly tasks coupled with ongoing peer feedback reduced student stress and allowed for creativity and community building. Students responded positively to the novel assignment.', 'corpus_id': 233307832, 'score': 0}]"
127	{'doc_id': '216144451', 'title': 'G-DAug: Generative Data Augmentation for Commonsense Reasoning', 'abstract': 'Recent advances in commonsense reasoning depend on large-scale human-annotated training sets to achieve peak performance. However, manual curation of training sets is expensive and has been shown to introduce annotation artifacts that neural models can readily exploit and overfit to. We propose a novel generative data augmentation technique, G-DAUGˆC, that aims to achieve more accurate and robust learning in a low-resource setting. Our approach generates synthetic examples using pretrained language models and selects the most informative and diverse set of examples for data augmentation. On experiments with multiple commonsense reasoning benchmarks, G-DAUGˆC consistently outperforms existing data augmentation methods based on back-translation, establishing a new state-of-the-art on WinoGrande, CODAH, and CommonsenseQA, as well as enhances out-of-distribution generalization, proving to be robust against adversaries or perturbations. Our analysis demonstrates that G-DAUGˆC produces a diverse set of fluent training examples, and that its selection and training approaches are important for performance.', 'corpus_id': 216144451}	13343	"[{'doc_id': '33124667', 'title': 'Hazardous drinking among patients attending a minor injuries unit: a pilot study', 'abstract': 'Excessive alcohol consumption increases the likelihood of accidental injury. This pilot study reports on the prevalence of hazardous drinkers presenting to a minor injuries unit. The proportion of hazardous drinkers is broadly similar to that found in emergency departments, suggesting that such units could also host alcohol intervention and brief advice activities.', 'corpus_id': 33124667, 'score': 0}, {'doc_id': '232379973', 'title': 'NL-EDIT: Correcting Semantic Parse Errors through Natural Language Interaction', 'abstract': 'We study semantic parsing in an interactive setting in which users correct errors with natural language feedback. We present NL-EDIT, a model for interpreting natural language feedback in the interaction context to generate a sequence of edits that can be applied to the initial parse to correct its errors. We show that NL-EDIT can boost the accuracy of existing text-to-SQL parsers by up to 20% with only one turn of correction. We analyze the limitations of the model and discuss directions for improvement and evaluation. The code and datasets used in this paper are publicly available at http://aka.ms/NLEdit.', 'corpus_id': 232379973, 'score': 1}, {'doc_id': '231602921', 'title': 'Structured Prediction as Translation between Augmented Natural Languages', 'abstract': 'We propose a new framework, Translation between Augmented Natural Languages (TANL), to solve many structured prediction language tasks including joint entity and relation extraction, nested named entity recognition, relation classification, semantic role labeling, event extraction, coreference resolution, and dialogue state tracking. Instead of tackling the problem by training task-specific discriminative classifiers, we frame it as a translation task between augmented natural languages, from which the task-relevant information can be easily extracted. Our approach can match or outperform task-specific models on all tasks, and in particular, achieves new state-of-the-art results on joint entity and relation extraction (CoNLL04, ADE, NYT, and ACE2005 datasets), relation classification (FewRel and TACRED), and semantic role labeling (CoNLL-2005 and CoNLL2012). We accomplish this while using the same architecture and hyperparameters for all tasks and even when training a single model to solve all tasks at the same time (multi-task learning). Finally, we show that our framework can also significantly improve the performance in a low-resource regime, thanks to better use of label semantics.', 'corpus_id': 231602921, 'score': 0}, {'doc_id': '170253777', 'title': '“I Was Not Built Up in the Womb, I Was Not Knit Together in the Egg, I Was Not Conceived”: The Christian Doctrine of the Eternal Generation of the Son in Its Egyptian Context', 'abstract': ""This article outlines the doctrine of the eternal generation of the Son, which can be found in Origen and was expanded on by Athanasius of Alexandria in his writings against Arius. The idea of the eternal generation of the Son will be seen in the context of the relationship between the Egyptian primordial creator god Atum and his offspring, the twin gods Shu and Tefnut. The various ways in which Shu is born are examined, as well as his relationship to the sun. The Egyptian understanding of duality and complementarity will also be examined in order to shed light on Athanasius' arguments."", 'corpus_id': 170253777, 'score': 0}, {'doc_id': '231749482', 'title': 'On Robustness of Neural Semantic Parsers', 'abstract': 'Semantic parsing maps natural language (NL) utterances into logical forms (LFs), which underpins many advanced NLP problems. Semantic parsers gain performance boosts with deep neural networks, but inherit vulnerabilities against adversarial examples. In this paper, we provide the first empirical study on the robustness of semantic parsers in the presence of adversarial attacks. Formally, adversaries of semantic parsing are considered to be the perturbed utterance-LF pairs, whose utterances have exactly the same meanings as the original ones. A scalable methodology is proposed to construct robustness test sets based on existing benchmark corpora. Our results answered five research questions in measuring the sate-of-the-art parsers’ performance on robustness test sets, and evaluating the effect of data augmentation.', 'corpus_id': 231749482, 'score': 1}, {'doc_id': '231749880', 'title': 'Neural Data Augmentation via Example Extrapolation', 'abstract': 'In many applications of machine learning, certain categories of examples may be underrepresented in the training data, causing systems to underperform on such “few-shot” cases at test time. A common remedy is to perform data augmentation, such as by duplicating underrepresented examples, or heuristically synthesizing new examples. But these remedies often fail to cover the full diversity and complexity of real examples. We propose a data augmentation approach that performs neural Example Extrapolation (Ex2). Given a handful of exemplars sampled from some distribution, Ex2 synthesizes new examples that also belong to the same distribution. The Ex2 model is learned by simulating the example generation procedure on data-rich slices of the data, and it is applied to underrepresented, few-shot slices. We apply Ex2 to a range of language understanding tasks and significantly improve over state-of-the-art methods on multiple few-shot learning benchmarks, including for relation extraction (FewRel) and intent classification + slot filling (SNIPS).', 'corpus_id': 231749880, 'score': 1}, {'doc_id': '231709501', 'title': 'El Volumen Louder Por Favor: Code-switching in Task-oriented Semantic Parsing', 'abstract': 'Being able to parse code-switched (CS) utterances, such as Spanish+English or Hindi+English, is essential to democratize task-oriented semantic parsing systems for certain locales. In this work, we focus on Spanglish (Spanish+English) and release a dataset, CSTOP, containing 5800 CS utterances alongside their semantic parses. We examine the CS generalizability of various Cross-lingual (XL) models and exhibit the advantage of pre-trained XL language models when data for only one language is present. As such, we focus on improving the pre-trained models for the case when only English corpus alongside either zero or a few CS training instances are available. We propose two data augmentation methods for the zero-shot and the few-shot settings: fine-tune using translate-and-align and augment using a generation model followed by match-and-filter. Combining the few-shot setting with the above improvements decreases the initial 30-point accuracy gap between the zero-shot and the full-data settings by two thirds.', 'corpus_id': 231709501, 'score': 0}, {'doc_id': '231692915', 'title': 'Distilling Large Language Models into Tiny and Effective Students using pQRNN', 'abstract': 'Large pre-trained multilingual models like mBERT, XLM-R achieve state of the art results on language understanding tasks. However, they are not well suited for latency critical applications on both servers and edge devices. It’s important to reduce the memory and compute resources required by these models. To this end, we propose pQRNN, a projectionbased embedding-free neural encoder that is tiny and effective for natural language processing tasks. Without pre-training, pQRNNs significantly outperform LSTM models with pre-trained embeddings despite being 140x smaller. With the same number of parameters, they outperform transformer baselines thereby showcasing their parameter efficiency. Additionally, we show that pQRNNs are effective student architectures for distilling large pretrained language models. We perform careful ablations which study the effect of pQRNN parameters, data augmentation, and distillation settings. On MTOP, a challenging multilingual semantic parsing dataset, pQRNN students achieve 95.9% of the performance of an mBERT teacher while being 350x smaller. On mATIS, a popular parsing task, pQRNN students on average are able to get to 97.1% of the teacher while again being 350x smaller. Our strong results suggest that our approach is great for latency-sensitive applications while being able to leverage large mBERT-like models.', 'corpus_id': 231692915, 'score': 0}, {'doc_id': '229331741', 'title': 'Learning Contextual Representations for Semantic Parsing with Generation-Augmented Pre-Training', 'abstract': 'Most recently, there has been significant interest in learning contextual representations for various NLP tasks, by leveraging large scale text corpora to train large neural language models with self-supervised learning objectives, such as Masked Language Model (MLM). However, based on a pilot study, we observe three issues of existing general-purpose language models when they are applied to text-to-SQL semantic parsers: fail to detect column mentions in the utterances, fail to infer column mentions from cell values, and fail to compose complex SQL queries. To mitigate these issues, we present a model pre-training framework, GenerationAugmented Pre-training (GAP), that jointly learns representations of natural language utterances and table schemas by leveraging generation models to generate pre-train data. GAP MODEL is trained on 2M utterance-schema pairs and 30K utterance-schema-SQL triples, whose utterances are produced by generative models. Based on experimental results, neural semantic parsers that leverage GAP MODEL as a representation encoder obtain new state-of-the-art results on both SPIDER and CRITERIA-TO-SQL benchmarks.', 'corpus_id': 229331741, 'score': 1}, {'doc_id': '229923938', 'title': 'Optimizing Deeper Transformers on Small Datasets: An Application on Text-to-SQL Semantic Parsing', 'abstract': 'Due to the common belief that training deep transformers from scratch requires large datasets, people usually only use shallow and simple additional layers on top of pre-trained models during fine-tuning on small datasets. We provide evidence that this does not always need to be the case: with proper initialization and training techniques, the benefits of very deep transformers are shown to carry over to hard structural prediction tasks, even using small datasets. In particular, we successfully train 48 layers of transformers for a semantic parsing task. These comprise 24 fine-tuned transformer layers from pre-trained RoBERTa and 24 relation-aware transformer layers trained from scratch. With fewer training steps and no task-specific pretraining, we obtain the state of the art performance on the challenging cross-domain Textto-SQL semantic parsing benchmark Spider. We achieve this by deriving a novel Data dependent Transformer Fixed-update initialization scheme (DT-Fixup), inspired by the prior T-Fixup work (Huang et al., 2020). Further error analysis demonstrates that increasing the depth of the transformer model can help improve generalization on the cases requiring reasoning and structural understanding1.', 'corpus_id': 229923938, 'score': 1}]"
128	{'doc_id': '198498191', 'title': 'Results of the European Group for the Study of Resistant Depression (GSRD) — basis for further research and clinical practice', 'abstract': 'Abstract Objectives: The overview outlines two decades of research from the European Group for the Study of Resistant Depression (GSRD) that fundamentally impacted evidence-based algorithms for diagnostics and psychopharmacotherapy of treatment-resistant depression (TRD). Methods: The GSRD staging model characterising response, non-response and resistance to antidepressant (AD) treatment was applied to 2762 patients in eight European countries. Results: In case of non-response, dose escalation and switching between different AD classes did not show superiority over continuation of original AD treatment. Predictors for TRD were symptom severity, duration of the current major depressive episode (MDE), suicidality, psychotic and melancholic features, comorbid anxiety and personality disorders, add-on treatment, non-response to the first AD, adverse effects, high occupational level, recurrent disease course, previous hospitalisations, positive family history of MDD, early age of onset and novel associations of single nucleoid polymorphisms (SNPs) within the PPP3CC, ST8SIA2, CHL1, GAP43 and ITGB3 genes and gene pathways associated with neuroplasticity, intracellular signalling and chromatin silencing. A prediction model reaching accuracy of above 0.7 highlighted symptom severity, suicidality, comorbid anxiety and lifetime MDEs as the most informative predictors for TRD. Applying machine-learning algorithms, a signature of three SNPs of the BDNF, PPP3CC and HTR2A genes and lacking melancholia predicted treatment response. Conclusions: The GSRD findings offer a unique and balanced perspective on TRD representing foundation for further research elaborating on specific clinical and genetic hypotheses and treatment strategies within appropriate study-designs, especially interaction-based models and randomized controlled trials.', 'corpus_id': 198498191}	2118	"[{'doc_id': '123674268', 'title': ""Teacher's Corner: A Review of Syllabi for a Sample of Structural Equation Modeling Courses"", 'abstract': 'With increases in the use of structural equation modeling (SEM) in the social sciences, graduate course offerings in this statistical technique can be expected to increase. Knowledge of the content of current SEM course offerings may provide ideas to instructors developing new courses or enhancing current courses. This article discusses results from a review of 55 SEM course syllabi, mostly from the disciplines of education and psychology. Areas that were reviewed include course content, software use, required and recommended readings, and techniques used to assess student learning. Summary descriptive statistics, examples of specific strategies, and a list of selected readings used in these courses are provided. This review is intended to provide information to instructors who are creating a new course and instructors seeking ideas to change or augment their courses.', 'corpus_id': 123674268, 'score': 0}, {'doc_id': '13824551', 'title': 'The Genetics of Treatment-Resistant Depression: A Critical Review and Future Perspectives', 'abstract': 'Abstract Background One-third of depressed patients develop treatment-resistant depression with the related sequelae in terms of poor functionality and worse prognosis. Solid evidence suggests that genetic variants are potentially valid predictors of antidepressant efficacy and could be used to provide personalized treatments. Methods The present review summarizes genetic findings of treatment-resistant depression including results from candidate gene studies and genome-wide association studies. The limitations of these approaches are discussed, and suggestions to improve the design of future studies are provided. Results Most studies used the candidate gene approach, and few genes showed replicated associations with treatment-resistant depression and/or evidence obtained through complementary approaches (e.g., gene expression studies). These genes included GRIK4, BDNF, SLC6A4, and KCNK2, but confirmatory evidence in large cohorts was often lacking. Genome-wide association studies did not identify any genome-wide significant association at variant level, but pathways including genes modulating actin cytoskeleton, neural plasticity, and neurogenesis may be associated with treatment-resistant depression, in line with results obtained by genome-wide association studies of antidepressant response. The improvement of aggregated tests (e.g., polygenic risk scores), possibly using variant/gene prioritization criteria, the increase in the covering of genetic variants, and the incorporation of clinical-demographic predictors of treatment-resistant depression are proposed as possible strategies to improve future pharmacogenomic studies. Conclusions Genetic biomarkers to identify patients with higher risk of treatment-resistant depression or to guide treatment in these patients are not available yet. Methodological improvements of future studies could lead to the identification of genetic biomarkers with clinical validity.', 'corpus_id': 13824551, 'score': 1}, {'doc_id': '214697436', 'title': 'Associations between immune-suppressive and stimulating drugs and novel COVID-19—a systematic review of current evidence', 'abstract': 'Background Cancer and transplant patients with COVID-19 have a higher risk of developing severe and even fatal respiratory diseases, especially as they may be treated with immune-suppressive or immune-stimulating drugs. This review focuses on the effects of these drugs on host immunity against COVID-19. Methods Using Ovid MEDLINE, we reviewed current evidence for immune-suppressing or -stimulating drugs: cytotoxic chemotherapy, low-dose steroids, tumour necrosis factorα (TNFα) blockers, interlukin-6 (IL-6) blockade, Janus kinase (JAK) inhibitors, IL-1 blockade, mycophenolate, tacrolimus, anti-CD20 and CTLA4-Ig. Results 89 studies were included. Cytotoxic chemotherapy has been shown to be a specific inhibitor for severe acute respiratory syndrome coronavirus in in vitro studies, but no specific studies exist as of yet for COVID-19. No conclusive evidence for or against the use of non-steroidal anti-inflammatory drugs (NSAIDs) in the treatment of COVID-19 patients is available, nor is there evidence indicating that TNFα blockade is harmful to patients in the context of COVID-19. COVID-19 has been observed to induce a pro-inflammatory cytokine generation and secretion of cytokines, such as IL-6, but there is no evidence of the beneficial impact of IL-6 inhibitors on the modulation of COVID-19. Although there are potential targets in the JAK-STAT pathway that can be manipulated in treatment for coronaviruses and it is evident that IL-1 is elevated in patients with a coronavirus, there is currently no evidence for a role of these drugs in treatment of COVID-19. Conclusion The COVID-19 pandemic has led to challenging decision-making about treatment of critically unwell patients. Low-dose prednisolone and tacrolimus may have beneficial impacts on COVID-19. The mycophenolate mofetil picture is less clear, with conflicting data from pre-clinical studies. There is no definitive evidence that specific cytotoxic drugs, low-dose methotrexate for auto-immune disease, NSAIDs, JAK kinase inhibitors or anti-TNFα agents are contraindicated. There is clear evidence that IL-6 peak levels are associated with severity of pulmonary complications.', 'corpus_id': 214697436, 'score': 0}, {'doc_id': '22545349', 'title': 'Glucocorticoid receptors in major depression: relevance to pathophysiology and treatment', 'abstract': 'Hyperactivity of the hypothalamic--pituitary--adrenal (HPA) axis has been reliably observed in patients with major depression. One of the primary features of this HPA axis hyperactivity is reduced sensitivity to the inhibitory effects of the glucocorticoid dexamethasone on the production of adrenocorticotropic hormone and cortisol during the dexamethasone suppression test and, more recently, the dexamethasone--corticotropin-releasing hormone test. Because the effects of glucocorticoids are mediated by intracellular receptors including, most notably, the glucocorticoid receptor (GR), a number of studies have considered the possibility that the number and/or function of GRs are reduced in depressed patients. Moreover, whether antidepressants act by reversing these putative GR changes has been examined. The extant literature on GR receptors in major depression was reviewed along with studies examining the impact of antidepressants on the GR. The data support the hypothesis that the function of the GR is reduced in major depression in the absence of clear evidence of decreased GR expression. The data also indicate that some antidepressants have direct effects on the GR, leading to enhanced GR function and increased GR expression. Hypotheses regarding the mechanism of these receptor changes involve relevant second messenger pathways that regulate GR function. The findings indicate that the GR is an important molecular target in major depression. Further elucidation of the biochemical and molecular mechanisms involved in GR changes in major depression is an exciting frontier that will no doubt lead to new insights into the pathophysiology and treatment of affective disorders.', 'corpus_id': 22545349, 'score': 1}, {'doc_id': '21691730', 'title': 'Phylogeny of babblers (Aves, Passeriformes): major lineages, family limits and classification', 'abstract': 'Babblers, family Timaliidae, have long been subject to debate on systematic position, family limits and internal taxonomy. In this study, we use five molecular regions to estimate the relationships among a large proportion of genera traditionally placed in Timaliidae. We find good support for five main clades within this radiation, and propose a new classification, dividing the babblers into the families Sylviidae and Timaliidae. Within the latter family, four subfamilies are recognized: Zosteropinae, Timaliinae, Pellorneinae and Leiothrichinae. Several taxa, previously not studied with molecular data, are phylogenetically placed within Sylviidae or Timaliidae. This is, however, not the case for the genus Pnoepyga, for which we propose the family name Pnoepygidae fam. n.', 'corpus_id': 21691730, 'score': 0}, {'doc_id': '53719667', 'title': 'Genome-wide association study of treatment-resistance in depression and meta-analysis of three independent samples', 'abstract': ""Background Treatment-resistant depression (TRD) is the most problematic outcome of depression in terms of functional impairment, suicidal thoughts and decline in physical health. Aims To investigate the genetic predictors of TRD using a genome-wide approach to contribute to the development of precision medicine. Method A sample recruited by the European Group for the Study of Resistant Depression (GSRD) including 1148 patients with major depressive disorder (MDD) was characterised for the occurrence of TRD (lack of response to at least two adequate antidepressant treatments) and genotyped using the Infinium PsychArray. Three clinically relevant patient groups were considered: TRD, responders and non-responders to the first antidepressant trial, thus outcomes were based on comparisons of these groups. Genetic analyses were performed at the variant, gene and gene-set (i.e. functionally related genes) level. Additive regression models of the outcomes and relevant covariates were used in the GSRD participants and in a fixed-effect meta-analysis performed between GSRD, STAR*D (n = 1316) and GENDEP (n = 761) participants. Results No individual polymorphism or gene was associated with TRD, although some suggestive signals showed enrichment in cytoskeleton regulation, transcription modulation and calcium signalling. Two gene sets (GO:0043949 and GO:0000183) were associated with TRD versus response and TRD versus response and non-response to the first treatment in the GSRD participants and in the meta-analysis, respectively (corrected P = 0.030 and P = 0.027). Conclusions The identified gene sets are involved in cyclic adenosine monophosphate mediated signal and chromatin silencing, two processes previously implicated in antidepressant action. They represent possible biomarkers to implement personalised antidepressant treatments and targets for new antidepressants. Declaration of interest D.S. has received grant/research support from GlaxoSmithKline and Lundbeck; has served as a consultant or on advisory boards for AstraZeneca, Bristol-Myers Squibb, Eli Lilly, Janssen and Lundbeck. S.M. has been a consultant or served on advisory boards for: AstraZeneca, Bristol-Myers Squibb, Forest, Johnson & Johnson, Leo, Lundbeck, Medelink, Neurim, Pierre Fabre, Richter. S.K. has received grant/research support from Eli Lilly, Lundbeck, Bristol-Myers Squibb, GlaxoSmithKline, Organon, Sepracor and Servier; has served as a consultant or on advisory boards for AstraZeneca, Bristol-Myers Squibb, GlaxoSmithKline, Eli Lilly, Lundbeck, Pfizer, Organon, Schwabe, Sepracor, Servier, Janssen and Novartis; and has served on speakers' bureaus for AstraZeneca, Eli Lily, Lundbeck, Schwabe, Sepracor, Servier, Pierre Fabre, Janssen and Neuraxpharm. J.Z. has received grant/research support from Lundbeck, Servier, Brainsway and Pfizer, has served as a consultant or on advisory boards for Servier, Pfizer, Abbott, Lilly, Actelion, AstraZeneca and Roche and has served on speakers' bureaus for Lundbeck, Roch, Lilly, Servier, Pfizer and Abbott. J.M. is a member of the Board of the Lundbeck International Neuroscience Foundation and of Advisory Board of Servier. A.S. is or has been consultant/speaker for: Abbott, AbbVie, Angelini, Astra Zeneca, Clinical Data, Boehringer, Bristol Myers Squibb, Eli Lilly, GlaxoSmithKline, Innovapharma, Italfarmaco, Janssen, Lundbeck, Naurex, Pfizer, Polifarma, Sanofi and Servier. C.M.L. receives research support from RGA UK Services Limited."", 'corpus_id': 53719667, 'score': 1}, {'doc_id': '211218317', 'title': 'A Precision Medicine Approach to Rhinitis Evaluation and Management', 'abstract': 'Purpose of Review Precision medicine (PM) represents a new paradigm in disease diagnosis, prevention, and treatment. The PM approach focuses on the characterization of different phenotypes and pathogenic pathways in order to allow the selection of specific biomarkers that will be useful in disease management. Rhinitis is a highly prevalent and heterogeneous disease, both in terms of underlying endotypes and clinical presentations. Therefore, to apply the PM principles to the various rhinitis subtypes rise as a meaningful strategy to improve evaluation and treatment. Recent Findings The technology of recombinant allergens has allowed molecular characterization of IgE reactivity of specific individual components of allergenic extracts. Recently published and ongoing clinical trials based on component resolved diagnosis (CRD) bring more precision to allergen immunotherapy for allergic rhinitis. Monoclonal antibodies against various cytokines involved in inflammatory allergic and nonallergic rhinitis endotypes show promissory results. Summary Better understanding of pathogenic pathways together with an accurate phenotype classification of patients presented with rhinitis symptoms contributes to point out clinical usefulness of biomarkers and other diagnostic tools, which leads to more accurate environmental control measures, personalized pharmacologic options, and new biological therapy developments.', 'corpus_id': 211218317, 'score': 0}, {'doc_id': '3818495', 'title': 'Refining Prediction in Treatment-Resistant Depression: Results of Machine Learning Analyses in the TRD III Sample.', 'abstract': 'OBJECTIVE\nThe study objective was to generate a prediction model for treatment-resistant depression (TRD) using machine learning featuring a large set of 47 clinical and sociodemographic predictors of treatment outcome.\n\n\nMETHOD\n552 Patients diagnosed with major depressive disorder (MDD) according to DSM-IV criteria were enrolled between 2011 and 2016. TRD was defined as failure to reach response to antidepressant treatment, characterized by a Montgomery-Asberg Depression Rating Scale (MADRS) score below 22 after at least 2 antidepressant trials of adequate length and dosage were administered. RandomForest (RF) was used for predicting treatment outcome phenotypes in a 10-fold cross-validation.\n\n\nRESULTS\nThe full model with 47 predictors yielded an accuracy of 75.0%. When the number of predictors was reduced to 15, accuracies between 67.6% and 71.0% were attained for different test sets. The most informative predictors of treatment outcome were baseline MADRS score for the current episode; impairment of family, social, and work life; the timespan between first and last depressive episode; severity; suicidal risk; age; body mass index; and the number of lifetime depressive episodes as well as lifetime duration of hospitalization.\n\n\nCONCLUSIONS\nWith the application of the machine learning algorithm RF, an efficient prediction model with an accuracy of 75.0% for forecasting treatment outcome could be generated, thus surpassing the predictive capabilities of clinical evaluation. We also supply a simplified algorithm of 15 easily collected clinical and sociodemographic predictors that can be obtained within approximately 10 minutes, which reached an accuracy of 70.6%. Thus, we are confident that our model will be validated within other samples to advance an accurate prediction model fit for clinical usage in TRD.', 'corpus_id': 3818495, 'score': 1}, {'doc_id': '212403702', 'title': 'Collaborative Cross Mice Yield Genetic Modifiers for Pseudomonas aeruginosa Infection in Human Lung Disease', 'abstract': 'Respiratory infection caused by P. aeruginosa is one of the most critical health burdens worldwide. People affected by P. aeruginosa infection include patients with a weakened immune system, such as those with cystic fibrosis (CF) genetic disease or non-CF bronchiectasis. Disease outcomes range from fatal pneumonia to chronic life-threatening infection and inflammation leading to the progressive deterioration of pulmonary function. The development of these respiratory infections is mediated by multiple causes. However, the genetic factors underlying infection susceptibility are poorly known and difficult to predict. Our study employed novel approaches and improved mouse disease models to identify genetic modifiers that affect the severity of P. aeruginosa lung infection. We identified candidate genes to enhance our understanding of P. aeruginosa infection in humans and provide a proof of concept that could be exploited for other human pathologies mediated by bacterial infection. ABSTRACT Human genetics influence a range of pathological and clinical phenotypes in respiratory infections; however, the contributions of disease modifiers remain underappreciated. We exploited the Collaborative Cross (CC) mouse genetic-reference population to map genetic modifiers that affect the severity of Pseudomonas aeruginosa lung infection. Screening for P. aeruginosa respiratory infection in a cohort of 39 CC lines exhibits distinct disease phenotypes ranging from complete resistance to lethal disease. Based on major changes in the survival times, a quantitative-trait locus (QTL) was mapped on murine chromosome 3 to the genomic interval of Mb 110.4 to 120.5. Within this locus, composed of 31 protein-coding genes, two candidate genes, namely, dihydropyrimidine dehydrogenase (Dpyd) and sphingosine-1-phosphate receptor 1 (S1pr1), were identified according to the level of genome-wide significance and disease gene prioritization. Functional validation of the S1pr1 gene by pharmacological targeting in C57BL/6NCrl mice confirmed its relevance in P. aeruginosa pathophysiology. However, in a cohort of Canadian patients with cystic fibrosis (CF) disease, regional genetic-association analysis of the syntenic human locus on chromosome 1 (Mb 97.0 to 105.0) identified two single-nucleotide polymorphisms (rs10875080 and rs11582736) annotated to the Dpyd gene that were significantly associated with age at first P. aeruginosa infection. Thus, there is evidence that both genes might be implicated in this disease. Our results demonstrate that the discovery of murine modifier loci may generate information that is relevant to human disease progression. IMPORTANCE Respiratory infection caused by P. aeruginosa is one of the most critical health burdens worldwide. People affected by P. aeruginosa infection include patients with a weakened immune system, such as those with cystic fibrosis (CF) genetic disease or non-CF bronchiectasis. Disease outcomes range from fatal pneumonia to chronic life-threatening infection and inflammation leading to the progressive deterioration of pulmonary function. The development of these respiratory infections is mediated by multiple causes. However, the genetic factors underlying infection susceptibility are poorly known and difficult to predict. Our study employed novel approaches and improved mouse disease models to identify genetic modifiers that affect the severity of P. aeruginosa lung infection. We identified candidate genes to enhance our understanding of P. aeruginosa infection in humans and provide a proof of concept that could be exploited for other human pathologies mediated by bacterial infection.', 'corpus_id': 212403702, 'score': 0}, {'doc_id': '23728275', 'title': 'Neuronal cell adhesion genes and antidepressant response in three independent samples', 'abstract': 'Drug-effect phenotypes in human lymphoblastoid cell lines recently allowed to identify CHL1 (cell adhesion molecule with homology to L1CAM), GAP43 (growth-associated protein 43) and ITGB3 (integrin beta 3) as new candidates for involvement in the antidepressant effect. CHL1 and ITGB3 code for adhesion molecules, while GAP43 codes for a neuron-specific cytosolic protein expressed in neuronal growth cones; all the three gene products are involved in synaptic plasticity. Sixteen polymorphisms in these genes were genotyped in two samples (n=369 and 90) with diagnosis of major depressive episode who were treated with antidepressants in a naturalistic setting. Phenotypes were response, remission and treatment-resistant depression. Logistic regression including appropriate covariates was performed. Genes associated with outcomes were investigated in the Sequenced Treatment Alternatives to Relieve Depression (STAR*D) genome-wide study (n=1861) as both individual genes and through a pathway analysis (Reactome and String databases). Gene-based analysis suggested CHL1 rs4003413, GAP43 rs283393 and rs9860828, ITGB3 rs3809865 as the top candidates due to their replication across the largest original sample and the STAR*D cohort. GAP43 molecular pathway was associated with both response and remission in the STAR*D, with ELAVL4 representing the gene with the highest percentage of single nucleotide polymorphisms (SNPs) associated with outcomes. Other promising genes emerging from the pathway analysis were ITGB1 and NRP1. The present study was the first to analyze cell adhesion genes and their molecular pathways in antidepressant response. Genes and biomarkers involved in neuronal adhesion should be considered by further studies aimed to identify predictors of antidepressant response.', 'corpus_id': 23728275, 'score': 1}]"
129	{'doc_id': '154308446', 'title': 'The colonization of space', 'abstract': 'New ideas are controversial when they challenge orthodoxy, but orthodoxy changes with time, often surprisingly fast. It is orthodox, for example, to believe that Earth is the only practical habitat for Man, and that the human race is close to its ultimate size limits. But I believe we have now reached the point where we can, if we so choose, build new habitats far more comfortable, productive and attractive than is most of Earth.', 'corpus_id': 154308446}	14346	"[{'doc_id': '231659444', 'title': 'DESIGN SOLUTIONS FOR INTERIOR ARCHITECTURE POST', 'abstract': ""The beginning of 2020 the Coronavirus spread threatened the whole world. The world's population forced to stay at homes quarantine to limit the spread of the epidemic. All our daily life practice became from home, this affects the interior space in shape, function, and its impact on human physiological health. The research discusses the design solutions according to the instructions of World Health Organization from the view of interior designers and supports the State’s plan innovation and development in the fourth-generation cities. The priorities of the post-Coronavirus phase are to redesign environmentally sustainable interior spaces for existing or new ones. The research focuses on the design solutions for the shared and public spaces, to get ready for the second wave of the pandemic, thus, limiting its spread by the usage of digital technologies that exploit our environmental resources and face the architectural design challenges in future developments."", 'corpus_id': 231659444, 'score': 0}, {'doc_id': '231799935', 'title': 'People, Timber, Forests, Fish, and Wildlife: An Assessment of the Conflict and Collaboration Dynamics in Washington 2019- 2020', 'abstract': 'As the third party neutral (TPN), I wish to express my deep gratitude to the 139 individuals who generously shared their time, wisdom, needs, concerns, insights and experience through the course of this assessment, with a special note of appreciation for the tribal leaders, members and citizens who welcomed me onto their land in the course of this assessment. My attempt to understand the long and rich history of collaboration and conflict, the complex current dynamics, and the many hopes, needs, and concerns for the future across all caucuses would not have been possible without the generosity, candor, and thoughtfulness of those whose voices and perspectives are reflected in this report.', 'corpus_id': 231799935, 'score': 0}, {'doc_id': '150758602', 'title': 'Self-preservation should be humankind’s first ethical priority and therefore rapid space settlement is necessary', 'abstract': 'Abstract Human survival is currently threatened by many existential risks. Because humankind is the only known species capable of complex morality, if humankind went extinct, morality would die with us. Given that the existence of humankind is a precondition for morality, therefore the first principle of morality should be, as Hans Jonas noted, that humankind must exist. Compared to ensuring human survival, all other moral values and actions are secondary. While protecting human life on Earth is the obvious choice for human survival, as long as humanity is in only one place, it will never be completely safe. Because space settlement gives humankind the opportunity to significantly raise the chances of survival for our species, it is therefore a moral imperative to settle space as quickly as possible.', 'corpus_id': 150758602, 'score': 1}, {'doc_id': '231831816', 'title': 'Proceeding International Conference 2020: Reposition of The Art and Cultural Heritage After Pandemic Era', 'abstract': 'Indirectly and unconsciously, a change in circumstances can affect the experience of residents because of changes in activities in the dwelling based on the repositioning of cultural-traditions. This event can act as a connectivity “machine” that encourages the creation of additional or new spaces that are tangible or intangible. The resulting spatial experience varies according to the state of the space, the activities that occur, and the memory of the occupants. This paper aims to contribute to an understanding of the repositioning of cultural-traditions occurring in the Peranakan house, Lasem, and to encourage readers to see the space from a different perspective. To answer these objectives, the method begins with surveys, interviews, and observations by documenting activities and spaces in the Kidang Mas Peranakan house complex in Lasem, Central Java as a case study. The research discussion is supported by the space trialectic concept, namely Lived Space and a phenomenological approach. This research finds that the repositioning of culture-traditions and space is interconnected and it appears that space is not just a place or a place to take shelter but a place to experience and gathering places of memories.', 'corpus_id': 231831816, 'score': 0}, {'doc_id': '134396541', 'title': 'Space Settlement Population Rotation Tolerance', 'abstract': 'To avoid a number of very negative health effects due to micro\xadg, free\xadspace settlements may be rotated to provide 1g of artificial gravity. Since the NASA/Stanford space settlement studies of the 1970s the settlement design community has assumed that rotation rates must be no more than 1\xad2 rpm to avoid motion sickness.  To achieve 1g, this rotation rate implies a settlement radius of approximately 225\xad895 m, which is much larger than any existing satellite.  In this paper we examine the literature and find good reason to believe that much higher rotation rates may be acceptable to residents and visitors alike, significantly reducing the minimum size of settlements and thus the difficulty of building them.  We find that rotation rates of up to 4 rpm, corresponding to a 56 m radius, should be acceptable, although visitors may require some training and perhaps a day or so of adaptation for those particularly susceptible to motion sickness. A rotation rate of up to 6 rpm (25 m radius) should be acceptable for residents but visitors will almost certainly need training and/or a few days to adapt.  While higher rotation rates (up to 10 rpm) may be acceptable with training, such small structures are not suitable for permanent residence (9 m radius at 10 rpm). With some caveats due to the quality of the available data, it appears that the lower limit of space settlement size is not determined by human response to rotation rate but rather by other factors.  This means that the effort necessary to build the first space settlements may be significantly less than previously believed, simply because they can be much smaller than heretofore expected.', 'corpus_id': 134396541, 'score': 1}, {'doc_id': '231923111', 'title': 'Kakadu National Park 2020 Conservation Outlook Assessment SITE INFORMATION Country : Australia Inscribed in : 1992 Criteria :', 'abstract': 'This unique archaeological and ethnological reserve, located in the Northern Territory, has been inhabited continuously for more than 40,000 years. The cave paintings, rock carvings and archaeological sites record the skills and way of life of the region’s inhabitants, from the hunter-gatherers of prehistoric times to the Aboriginal people still living there. It is a unique example of a complex of ecosystems, including tidal flats, floodplains, lowlands and plateaux, and provides a habitat for a wide range of rare or endemic species of plants and animals. © UNESCO', 'corpus_id': 231923111, 'score': 0}, {'doc_id': '150951975', 'title': ""Space settlement: What's the Rush?"", 'abstract': 'Abstract In this paper I argue that obligations associated with the scientific exploration and study of the solar system are, over sub-millennial timescales, stronger than obligations associated with the preservation of humanity via space settlement. Thus my objection is not to space settlement as such, but rather, to those claiming an urgent need to instigate space settlement. I also argue that the “disposable planet mentality” objection to space settlement is ill-founded.', 'corpus_id': 150951975, 'score': 1}, {'doc_id': '119435160', 'title': 'Can Deep Altruism Sustain Space Settlement', 'abstract': 'Space settlement represents a long-term human effort that requires unprecedented coordination across successive generations. In this chapter, I develop a comparative hierarchy for the value of long-term projects based upon their benefits to culture, their development of infrastructure, and their contributions to lasting information. I next draw upon the concept of the time capsule as an analogy, which enables a comparison of historical examples of projects across generational, intergenerational, and deep time. The concept of deep altruism can then be defined as the selfless pursuit of informational value for the well-being of others in the distant future. The first steps toward supporting an effort like space settlement through deep altruism would establish governance and funding models that begin to support ambitions with intergenerational succession.', 'corpus_id': 119435160, 'score': 1}, {'doc_id': '227317862', 'title': 'Rethinking Approach to Environmental Protection in View of Ancient Indian Wisdom', 'abstract': 'Perceptible improvement in the quality of air and water during COVID-19 lockdown has taught humanity several lessons and brought realization which might otherwise have been missed or taken several experiments to come to it. This study is aimed at understanding whether our approach towards holistic environmental protection needs rethinking by appropriate positioning of man as a species in the context of the biosphere and commensurate responsibility he has towards it. The technique used for the study is doctrinal research which shall take into account available primary resources, such as report as well as secondary resources, such as articles and books. The study focuses on the ancient wisdom available in the Hindu philosophies and literature in order to understand the relation between man, other living organisms, plants as well as the abiotic components of the earth. The study has shown that humans, by being at the top of the evolutionary ladder, has seeming dominance over the environ, and accordingly has formulated norms which are anthropocentric. However, new approaches to environmental protection and conservation are required to be adopted which recognize the interconnectedness, interrelatedness and interdependency of the biotic and abiotic components of this earth.', 'corpus_id': 227317862, 'score': 0}, {'doc_id': '150500633', 'title': 'A Hobbesian qualm with space settlement', 'abstract': 'Abstract Imagining a new life in space is exciting. However, we should not overlook that psychological and interpersonal stressors due to isolation and confinement can cause crew members to experience behavioral health deterioration and sociomoral performance degradation. In the era of space settlement, such conditions will last throughout settlers’ lives as facilities must be highly airtight to protect them against the hostile environment of space. Settlers’ behavioral health and sociomoral performance will gradually decline, and the situation will only become progressively worse because no one on Earth will be able to intervene. The settlers might ultimately find themselves in a disastrous Hobbesian state of “war of all against all”. Humans have yet to experience a comparatively long-duration sojourn in space and its consequences, so when considering possible scenarios in future space settlement, we must examine behavioral health problems due to the lifelong residence in isolated and confined facilities.', 'corpus_id': 150500633, 'score': 1}]"
130	{'doc_id': '8685319', 'title': 'Fluid–body interactions: clashing, skimming, bouncing', 'abstract': 'Solid–solid and solid–fluid impacts and bouncing are the concern here. A theoretical study is presented on fluid–body interaction in which the motion of the body and the fluid influence each other nonlinearly. There could also be many bodies involved. The clashing refers to solid–solid impacts arising from fluid–body interaction in a channel, while the skimming refers to another area where a thin body impacts obliquely upon a fluid surface. Bouncing usually then follows in both areas. The main new contribution concerns the influences of thickness and camber which lead to a different and more general form of clashing and hence bouncing.', 'corpus_id': 8685319}	7987	"[{'doc_id': '221376507', 'title': 'Quantum delocalization, gauge, and quantum optics: Light-matter interaction in relativistic quantum information', 'abstract': 'We revisit the interaction of a first-quantized atomic system (consisting of two charged quantum particles) with the quantum electromagnetic field, pointing out the subtleties related to the gauge nature of electromagnetism and the effect of multipole approximations. We connect the full minimal-coupling model with the typical effective models used in quantum optics and relativistic quantum information such as the Unruh-DeWitt (UDW) model and the dipole coupling approximation. We point out in what regimes different degrees of approximation are reasonable and in what cases effective models need to be refined to capture the features of the light-matter interaction. This is particularly important when considering the center of mass (COM) of the atom as a quantum system that can be delocalized over multiple trajectories. For example, we show that the simplest UDW approximation with a quantum COM fails to capture crucial R\\""ontgen terms coupling COM and internal atomic degrees of freedom with each other and the field. Finally we show how effective dipole interaction models can be covariantly prescribed for relativistically moving atoms.', 'corpus_id': 221376507, 'score': 0}, {'doc_id': '221103690', 'title': 'A Coordinate Free Formulation of Effective Diffusion on Channels', 'abstract': 'We study diffusion processes in regions generated by sliding a cross section by the phase flow of vector filed on curved spaces of arbitrary dimension. We do this by studying the effective diffusion coefficient D that arises when trying to reduce the n-dimensional diffusion equation to a 1-dimensional diffusion equation by means of a projection method. We use the mathematical language of exterior calculus to derive a coordinate free formula for this coefficient in both infinite and finite transversal diffusion rate cases. The use of these techniques leads to a formula for D which provides a deeper understanding of effective diffusion than when using a coordinate dependent approach.', 'corpus_id': 221103690, 'score': 1}, {'doc_id': '221266165', 'title': 'Rupture of Liquid Bridges on Porous Tips: Competing Mechanisms of Spontaneous Imbibition and Stretching.', 'abstract': 'Liquid bridges are commonly encountered in nature and the liquid transfer induced by their rupture is widely used in various industrial applications. In this work, with the focus on the porous tip, we studied the impacts of capillary effects on the liquid transfer induced by the rupture through numerical simulations. To depict the capillary effects of a porous tip, a time scale ratio, RT, is proposed to compare the competing mechanisms of spontaneous imbibition and external drag. In terms of RT, we then develop a theoretical model for estimating the liquid retention ratio considering the geometry, porosity, and wettability of tips. The mechanism presented in this work provides a possible approach to control the liquid transfer with better accuracy in microfluidics or microfabrications.', 'corpus_id': 221266165, 'score': 0}, {'doc_id': '9158644', 'title': 'Mathematical and computational models of drug transport in tumours', 'abstract': 'The ability to predict how far a drug will penetrate into the tumour microenvironment within its pharmacokinetic (PK) lifespan would provide valuable information about therapeutic response. As the PK profile is directly related to the route and schedule of drug administration, an in silico tool that can predict the drug administration schedule that results in optimal drug delivery to tumours would streamline clinical trial design. This paper investigates the application of mathematical and computational modelling techniques to help improve our understanding of the fundamental mechanisms underlying drug delivery, and compares the performance of a simple model with more complex approaches. Three models of drug transport are developed, all based on the same drug binding model and parametrized by bespoke in vitro experiments. Their predictions, compared for a ‘tumour cord’ geometry, are qualitatively and quantitatively similar. We assess the effect of varying the PK profile of the supplied drug, and the binding affinity of the drug to tumour cells, on the concentration of drug reaching cells and the accumulated exposure of cells to drug at arbitrary distances from a supplying blood vessel. This is a contribution towards developing a useful drug transport modelling tool for informing strategies for the treatment of tumour cells which are ‘pharmacokinetically resistant’ to chemotherapeutic strategies.', 'corpus_id': 9158644, 'score': 1}, {'doc_id': '202154754', 'title': 'Channel Flow Past A Near-Wall Body', 'abstract': '\n Near-wall behaviour arising when a finite sized body moves in a channel flow is investigated for high flow rates. This is over the interactive-flow length scale that admits considerable upstream influence. The focus is first on quasi-steady two-dimensional flow past a thin body in the outer reaches of one of the viscous wall layers. The jump conditions near the front of the body play an important part in the whole solution which involves an unusual multi-structured flow due to the presence of the body: flows above, below, ahead of and behind the body interact fully. Analytical solutions are presented and the repercussions for shorter and longer bodies are then examined. Second, implications are followed through for the movement of a free body in a dynamic fluid–body interaction. Particular key findings are that instability persists for all body lengths, the growth rate decreases like the $1/4$ power of distance as the body approaches the wall, and lift production on the body is dominated by high pressures from an unexpected flow region emerging on the front of the body.', 'corpus_id': 202154754, 'score': 1}, {'doc_id': '125931904', 'title': 'Free motion of a body in a boundary layer or channel flow', 'abstract': 'Coupling is considered between fluid flow and a freely moving body shorter than the development length in an oncoming boundary layer or channel flow but longer than the flow thickness. The body lies within the core of the flow. The coupling occurs between the inviscid-dominated displacement and the viscous–inviscid pressure, the latter acting to move the body. This interaction can be unstable. It is found however that three factors serve to stabilise the interaction as each one alters the decisive balance of angular momentum. One is a 10 % shift forward in the position of the centre of mass. The second is a degree of flexibility in the body shape by means of its response to the induced pressure force. Third is a slight streamwise movement of the body which is sufficient to modify the viscous–inviscid pressure response and again produce stabilisation. The effects are largely independent of the lateral position of the body.', 'corpus_id': 125931904, 'score': 1}, {'doc_id': '221103825', 'title': 'Motion Optimization for Musculoskeletal Dynamics: A Flatness-Based Polynomial Approach', 'abstract': 'A new approach for trajectory optimization of musculoskeletal dynamic models is introduced. The model combines rigid-body and muscle dynamics described with a Hill-type model driven by neural control inputs. The objective is to find input and state trajectories that are optimal with respect to a minimum-effort objective and meet constraints associated with musculoskeletal models. The measure of effort is given by the integral of pairwise average forces of the agonist-antagonist muscles. The concepts of flat parameterization of nonlinear systems and sum-of-squares optimization are combined to yield a method that eliminates the numerous set of dynamic constraints present in collocation methods. With terminal equilibrium, optimization reduces to a feasible linear program, and a recursive feasibility proof is given for more general polynomial optimization cases. The methods of the article can be used as a basis for fast, and efficient solvers for hierarchical, and receding-horizon control schemes. Two simulation examples are included to illustrate the proposed methods.', 'corpus_id': 221103825, 'score': 0}, {'doc_id': '220041992', 'title': 'The Effect of Misalignment between the Rotation Axis and Magnetic Field on the Circumstellar Disk', 'abstract': 'The formation of circumstellar disks is investigated using three-dimensional resistive magnetohydrodynamic simulations, in which the initial prestellar cloud has a misaligned rotation axis with respect to the magnetic field. We examine the effects of (i) the initial angle difference between the global magnetic field and the cloud rotation axis ($\\theta_0$) and (ii) the ratio of the thermal to gravitational energy ($\\alpha_0$). We study $16$ models in total and calculate the cloud evolution until $\\sim \\! 5000$ yr after protostar formation. Our simulation results indicate that an initial non-zero $\\theta_0$ ($> 0$) promotes the disk formation but tends to suppress the outflow driving, for models that are moderately gravitationally unstable, $\\alpha_0 \\lesssim 1$. In these models, a large-sized rotationally-supported disk forms and a weak outflow appears, in contrast to a smaller disk and strong outflow in the aligned case ($\\theta_0 = 0$). Furthermore, we find that when the initial cloud is highly unstable with small $\\alpha_0$, the initial angle difference $\\theta_0$ does not significantly affect the disk formation and outflow driving.', 'corpus_id': 220041992, 'score': 0}, {'doc_id': '213790956', 'title': 'When a small thin two-dimensional body enters a viscous wall layer', 'abstract': 'If a body enters a viscous-inviscid fluid layer near a wall, then significant effects can be felt from the presence of incident vorticity, viscous forces and nonlinear forces. The focus here is on the response in the outer edge of such a wall layer. Nonlinear two-dimensional unsteady behaviour is examined through modelling, computation and analysis applied for a thin body travelling streamwise upstream or downstream or staying still relative to the wall. The wall layer with its balance between inviscid and viscous effects interacts freely with the body movement, causing relatively high magnitudes of pressure on top of the body and nonlinear responses in the gap between the body and the wall. The study finds explicit solutions for the motion of the body, separation of the flow arising near the wall and possible instabilities occurring over the length scale of any short body.', 'corpus_id': 213790956, 'score': 1}, {'doc_id': '220459142', 'title': 'SARS-CoV-2 on the neural battleground', 'abstract': 'The haematopoietic cells of the immune system carry out their functions within tissue frameworks created by structural cells of the epithelium, endothelium and stroma (mainly fibroblasts). Interactions of these structural cells with immune cells have been difficult to study owing to their essential roles in organ function. Krausgruber et al. used multi-omics profiling of structural cells from 12 mouse organs to create a high-resolution atlas of immune gene activity. They hope that this will aid further study of immune functions in non-haematopoietic cells — a field they refer to as ‘structural immunity’. Structural cells purified from mouse organs were sorted on the basis of expression of CD31 (endothelial cells), EPCAM (epithelial cells) and GP38 (fibroblasts). Gene expression profiling by RNA sequencing (RNA-seq) showed that different structural cells within the same organ were more similar to each other than the same structural cells across organs, which indicates that there is a major effect of the tissue environment. As well as predicting crosstalk between structural cells and haematopoietic cells on the basis of known receptor–ligand pairs, the RNA-seq dataset showed high levels of activity of ‘immune gene’ modules in structural cells in cell type-specific and organ-specific patterns. Next, the authors looked at gene regulation by profiling chromatin accessibility and active H3K4me2 marks. Similar to the RNA-seq data, chromatin and histone profiles were generally more similar within an organ than within a structural cell type, although a subset of immune genes I M M U N O G e N e T I c S', 'corpus_id': 220459142, 'score': 0}]"
131	"{'doc_id': '195892791', 'title': 'Superhuman AI for multiplayer poker', 'abstract': ""AI now masters six-player poker Computer programs have shown superiority over humans in two-player games such as chess, Go, and heads-up, no-limit Texas hold'em poker. However, poker games usually include six players—a much trickier challenge for artificial intelligence than the two-player variant. Brown and Sandholm developed a program, dubbed Pluribus, that learned how to play six-player no-limit Texas hold'em by playing against five copies of itself (see the Perspective by Blair and Saffidine). When pitted against five elite professional poker players, or with five copies of Pluribus playing against one professional, the computer performed significantly better than humans over the course of 10,000 hands of poker. Science, this issue p. 885; see also p. 864 An AI dubbed Pluribus performs significantly better than human professionals in six-player no-limit Texas hold’em poker. In recent years there have been great strides in artificial intelligence (AI), with games often serving as challenge problems, benchmarks, and milestones for progress. Poker has served for decades as such a challenge problem. Past successes in such benchmarks, including poker, have been limited to two-player games. However, poker in particular is traditionally played with more than two players. Multiplayer games present fundamental additional issues beyond those in two-player games, and multiplayer poker is a recognized AI milestone. In this paper we present Pluribus, an AI that we show is stronger than top human professionals in six-player no-limit Texas hold’em poker, the most popular form of poker played by humans."", 'corpus_id': 195892791}"	5115	"[{'doc_id': '212634032', 'title': 'Min-Max Q-Learning for Multi-Player Pursuit-Evasion Games', 'abstract': ""In this paper, we address a pursuit-evasion game involving multiple players by utilizing tools and techniques from reinforcement learning and matrix game theory. In particular, we consider the problem of steering an evader to a goal destination while avoiding capture by multiple pursuers, which is a high-dimensional and computationally intractable problem in general. In our proposed approach, we first formulate the multi-agent pursuit-evasion game as a sequence of discrete matrix games. Next, in order to simplify the solution process, we transform the high-dimensional state space into a low-dimensional manifold and the continuous action space into a feature-based space, which is a discrete abstraction of the original space. Based on these transformed state and action spaces, we subsequently employ min-max Q-learning, to generate the entries of the payoff matrix of the game, and subsequently obtain the optimal action for the evader at each stage. Finally, we present extensive numerical simulations to evaluate the performance of the proposed learning-based evading strategy in terms of the evader's ability to reach the desired target location without being captured, as well as computational efficiency."", 'corpus_id': 212634032, 'score': 0}, {'doc_id': '189928084', 'title': 'Solution of Two-Player Zero-Sum Game by Successive Relaxation', 'abstract': 'We consider the problem of two-player zero-sum game. In this setting, there are two agents working against each other. Both the agents observe the same state and the objective of the agents is to compute a strategy profile that maximizes their rewards. However, the reward of the second agent is negative of reward obtained by the first agent. Therefore, the objective of the second agent is to minimize the total reward obtained by the first agent. This problem is formulated as a min-max Markov game in the literature. The solution of this game, which is the max-min reward (of first player), starting from a given state is called the equilibrium value of the state. In this work, we compute the solution of the two-player zero-sum game utilizing the technique of successive relaxation. Successive relaxation has been successfully applied in the literature to compute a faster value iteration algorithm in the context of Markov Decision Processes. We extend the concept of successive relaxation to the two-player zero-sum games. We prove that, under a special structure, this technique computes the optimal solution faster than the techniques in the literature. We then derive a generalized minimax Q-learning algorithm that computes the optimal policy when the model information is not known. Finally, we prove the convergence of the proposed generalized minimax Q-learning algorithm.', 'corpus_id': 189928084, 'score': 1}, {'doc_id': '218628845', 'title': 'Competing in a Complex Hidden Role Game with Information Set Monte Carlo Tree Search', 'abstract': 'Advances in intelligent game playing agents have led to successes in perfect information games like Go and imperfect information games like Poker. The Information Set Monte Carlo Tree Search (ISMCTS) family of algorithms outperforms previous algorithms using Monte Carlo methods in imperfect information games. In this paper, Single Observer Information Set Monte Carlo Tree Search (SO-ISMCTS) is applied to Secret Hitler, a popular social deduction board game that combines traditional hidden role mechanics with the randomness of a card deck. This combination leads to a more complex information model than the hidden role and card deck mechanics alone. It is shown in 10108 simulated games that SO-ISMCTS plays as well as simpler rule based agents, and demonstrates the potential of ISMCTS algorithms in complicated information set domains.', 'corpus_id': 218628845, 'score': 0}, {'doc_id': '152282692', 'title': 'Fast and Furious Learning in Zero-Sum Games: Vanishing Regret with Non-Vanishing Step Sizes', 'abstract': 'We show for the first time, to our knowledge, that it is possible to reconcile in online learning in zero-sum games two seemingly contradictory objectives: vanishing time-average regret and non-vanishing step sizes. This phenomenon, that we coin ``fast and furious"" learning in games, sets a new benchmark about what is possible both in max-min optimization as well as in multi-agent systems. Our analysis does not depend on introducing a carefully tailored dynamic. Instead we focus on the most well studied online dynamic, gradient descent. Similarly, we focus on the simplest textbook class of games, two-agent two-strategy zero-sum games, such as Matching Pennies. Even for this simplest of benchmarks the best known bound for total regret, prior to our work, was the trivial one of $O(T)$, which is immediately applicable even to a non-learning agent. Based on a tight understanding of the geometry of the non-equilibrating trajectories in the dual space we prove a regret bound of $\\Theta(\\sqrt{T})$ matching the well known optimal bound for adaptive step sizes in the online setting. This guarantee holds for all fixed step-sizes without having to know the time horizon in advance and adapt the fixed step-size accordingly. As a corollary, we establish that even with fixed learning rates the time-average of mixed strategies, utilities converge to their exact Nash equilibrium values.', 'corpus_id': 152282692, 'score': 1}, {'doc_id': '195658101', 'title': 'Rethinking Formal Models of Partially Observable Multiagent Decision Making', 'abstract': 'Multiagent decision-making problems in partially observable environments are usually modeled as either extensive-form games (EFGs) within the game theory community or partially observable stochastic games (POSGs) within the reinforcement learning community. While most practical problems can be modeled in both formalisms, the communities using these models are mostly distinct with little sharing of ideas or advances. The last decade has seen dramatic progress in algorithms for EFGs, mainly driven by the challenge problem of poker. We have seen computational techniques achieving super-human performance, some variants of poker are essentially solved, and there are now sound local search algorithms which were previously thought impossible. While the advances have garnered attention, the fundamental advances are not yet understood outside the EFG community. This can be largely explained by the starkly different formalisms between the game theory and reinforcement learning communities and, further, by the unsuitability of the original EFG formalism to make the ideas simple and clear. This paper aims to address these hindrances, by advocating a new unifying formalism, a variant of POSGs, which we call Factored-Observation Games (FOGs). We prove that any timeable perfect-recall EFG can be efficiently modeled as a FOG as well as relating FOGs to other existing formalisms. Additionally, a FOG explicitly identifies the public and private components of observations, which is fundamental to the recent EFG breakthroughs. We conclude by presenting the two building-blocks of these breakthroughs --- counterfactual regret minimization and public state decomposition --- in the new formalism, illustrating our goal of a simpler path for sharing recent advances between game theory and reinforcement learning community.', 'corpus_id': 195658101, 'score': 1}, {'doc_id': '218900609', 'title': 'Stochastic Potential Games', 'abstract': 'Computing the Nash equilibrium (NE) for N-player non-zerosum stochastic games is a formidable challenge. Currently, algorithmic methods in stochastic game theory are unable to compute NE for stochastic games (SGs) for settings in all but extreme cases in which the players either play as a team or have diametrically opposed objectives in a two-player setting. This greatly impedes the application of the SG framework to numerous problems within economics and practical systems of interest. In this paper, we provide a method of computing Nash equilibria in nonzero-sum settings and for populations of players more than two. In particular, we identify a subset of SGs known as stochastic potential games (SPGs) for which the (Markov perfect) Nash equilibrium can be computed tractably and in polynomial time. Unlike SGs for which, in general, computing the NE is PSPACE-hard, we show that SGs with the potential property are P-Complete. We further demonstrate that for each SPG there is a dual Markov decision process whose solution coincides with the MP-NE of the SPG. We lastly provide algorithms that tractably compute the MP-NE for SGs with more than two players.', 'corpus_id': 218900609, 'score': 0}, {'doc_id': '218870412', 'title': 'Single-Agent Optimization Through Policy Iteration Using Monte-Carlo Tree Search', 'abstract': 'The combination of Monte-Carlo Tree Search (MCTS) and deep reinforcement learning is state-of-the-art in two-player perfect-information games. In this paper, we describe a search algorithm that uses a variant of MCTS which we enhanced by 1) a novel action value normalization mechanism for games with potentially unbounded rewards (which is the case in many optimization problems), 2) defining a virtual loss function that enables effective search parallelization, and 3) a policy network, trained by generations of self-play, to guide the search. We gauge the effectiveness of our method in ""SameGame""---a popular single-player test domain. Our experimental results indicate that our method outperforms baseline algorithms on several board sizes. Additionally, it is competitive with state-of-the-art search algorithms on a public set of positions.', 'corpus_id': 218870412, 'score': 0}, {'doc_id': '76666594', 'title': 'Compact Representation of Value Function in Partially Observable Stochastic Games', 'abstract': 'Value methods for solving stochastic games with partial observability model the uncertainty of the players as a probability distribution over possible states, where the dimension of the belief space is the number of states. For many practical problems, there are exponentially many states which causes scalability problems. We propose an abstraction technique that addresses this curse of dimensionality by projecting the high-dimensional beliefs onto characteristic vectors of significantly lower dimension (e.g., marginal probabilities). Our main contributions are (1) a novel compact representation of the uncertainty in partially observable stochastic games and (2) a novel algorithm using this representation that is based on existing state-of-the-art algorithms for solving stochastic games with partial observability. Experimental evaluation confirms that the new algorithm using the compact representation dramatically increases scalability compared to the state of the art.', 'corpus_id': 76666594, 'score': 1}, {'doc_id': '214803086', 'title': 'Using Multi-Agent Reinforcement Learning in Auction Simulations', 'abstract': 'Game theory has been developed by scientists as a theory of strategic interaction among players who are supposed to be perfectly rational. These strategic interactions might have been presented in an auction, a business negotiation, a chess game, or even in a political conflict aroused between different agents. In this study, the strategic (rational) agents created by reinforcement learning algorithms are supposed to be bidder agents in various types of auction mechanisms such as British Auction, Sealed Bid Auction, and Vickrey Auction designs. Next, the equilibrium points determined by the agents are compared with the outcomes of the Nash equilibrium points for these environments. The bidding strategy of the agents is analyzed in terms of individual rationality, truthfulness (strategy-proof), and computational efficiency. The results show that using a multi-agent reinforcement learning strategy improves the outcomes of the auction simulations.', 'corpus_id': 214803086, 'score': 0}, {'doc_id': '211066535', 'title': 'Multi Type Mean Field Reinforcement Learning', 'abstract': 'Mean field theory provides an effective way of scaling multiagent reinforcement learning algorithms to environments with many agents that can be abstracted by a virtual mean agent. In this paper, we extend mean field multiagent algorithms to multiple types. The types enable the relaxation of a core assumption in mean field games, which is that all agents in the environment are playing almost similar strategies and have the same goal. We conduct experiments on three different testbeds for the field of many agent reinforcement learning, based on the standard MAgents framework. We consider two different kinds of mean field games: a) Games where agents belong to predefined types that are known a priori and b) Games where the type of each agent is unknown and therefore must be learned based on observations. We introduce new algorithms for each type of game and demonstrate their superior performance over state of the art algorithms that assume that all agents belong to the same type and other baseline algorithms in the MAgent framework.', 'corpus_id': 211066535, 'score': 1}]"
132	"{'doc_id': '53047143', 'title': 'SL2MF: Predicting Synthetic Lethality in Human Cancers via Logistic Matrix Factorization', 'abstract': 'Synthetic lethality (SL) is a promising concept for novel discovery of anti-cancer drug targets. However, wet-lab experiments for detecting SLs are faced with various challenges, such as high cost, low consistency across platforms, or cell lines. Therefore, computational prediction methods are needed to address these issues. This paper proposes a novel SL prediction method, named <inline-formula><tex-math notation=""LaTeX"">$\\mathsf{SL}^2 \\mathsf{MF}$</tex-math><alternatives><mml:math><mml:mrow><mml:msup><mml:mi mathvariant=""sans-serif"">SL</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mi mathvariant=""sans-serif"">MF</mml:mi></mml:mrow></mml:math><inline-graphic xlink:href=""liu-ieq1-2909908.gif""/></alternatives></inline-formula>, which employs logistic matrix factorization to learn latent representations of genes from the observed SL data. The probability that two genes are likely to form SL is modeled by the linear combination of gene latent vectors. As known SL pairs are more trustworthy than unknown pairs, we design importance weighting schemes to assign higher importance weights for known SL pairs and lower importance weights for unknown pairs in <inline-formula><tex-math notation=""LaTeX"">$\\mathsf{SL}^2 \\mathsf{MF}$</tex-math><alternatives><mml:math><mml:mrow><mml:msup><mml:mi mathvariant=""sans-serif"">SL</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mi mathvariant=""sans-serif"">MF</mml:mi></mml:mrow></mml:math><inline-graphic xlink:href=""liu-ieq2-2909908.gif""/></alternatives></inline-formula>. Moreover, we also incorporate biological knowledge about genes from protein-protein interaction (PPI) data and Gene Ontology (GO). In particular, we calculate the similarity between genes based on their GO annotations and topological properties in the PPI network. Extensive experiments on the SL interaction data from SynLethDB database have been conducted to demonstrate the effectiveness of <inline-formula><tex-math notation=""LaTeX"">$\\mathsf{SL}^2 \\mathsf{MF}$</tex-math><alternatives><mml:math><mml:mrow><mml:msup><mml:mi mathvariant=""sans-serif"">SL</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mi mathvariant=""sans-serif"">MF</mml:mi></mml:mrow></mml:math><inline-graphic xlink:href=""liu-ieq3-2909908.gif""/></alternatives></inline-formula>.', 'corpus_id': 53047143}"	5393	[{'doc_id': '44102394', 'title': 'Synthetic Lethality-based Identification of Targets for Anticancer Drugs in the Human Signaling Network', 'abstract': 'Chemotherapy agents can cause serious adverse effects by attacking both cancer tissues and normal tissues. Therefore, we proposed a synthetic lethality (SL) concept-based computational method to identify specific anticancer drug targets. First, a 3-step screening strategy (network-based, frequency-based and function-based screening) was proposed to identify the SL gene pairs by mining 697 cancer genes and the human signaling network, which had 6306 proteins and 62937 protein-protein interactions. The network-based screening was composed of a stability score constructed using a network information centrality measure (the average shortest path length) and the distance-based screening between the cancer gene and the non-cancer gene. Then, the non-cancer genes were extracted and annotated using drug-target interaction and drug description information to obtain potential anticancer drug targets. Finally, the human SL data in SynLethDB, the existing drug sensitivity data and text-mining were utilized for target validation. We successfully identified 2555 SL gene pairs and 57 potential anticancer drug targets. Among them, CDK1, CDK2, PLK1 and WEE1 were verified by all three aspects and could be preferentially used in specific targeted therapy in the future.', 'corpus_id': 44102394, 'score': 1}, {'doc_id': '51868618', 'title': 'DiscoverSL: an R package for multi‐omic data driven prediction of synthetic lethality in cancers', 'abstract': 'Summary: Synthetic lethality is a state when simultaneous loss of two genes is lethal to a cancer cell, while the loss of the individual genes is not. We developed an R package DiscoverSL to predict and visualize synthetic lethality in cancers using multi‐omic cancer data. Mutation, copy number alteration and gene expression data from The Cancer Genome Atlas project were combined to develop a multi‐parametric Random Forest classifier. The effects of selectively targeting the predicted synthetic lethal genes is tested in silico using shRNA and drug screening data from cancer cell line databases. The clinical outcome in patients with mutation in primary gene and over/under‐expression in the synthetic lethal gene is evaluated using Kaplan‐Meier analysis. The method helps to identify new therapeutic approaches by exploiting the concept of synthetic lethality. Availability and implementation: DiscoverSL package with user manual and sample workflow is available for download from github url: https://github.com/shaoli86/DiscoverSL/releases/tag/V1.0 under GNU GPL‐3. Supplementary information: Supplementary data are available at Bioinformatics online.', 'corpus_id': 51868618, 'score': 1}, {'doc_id': '211534922', 'title': 'EXP2SL: A Machine Learning Framework for Cell-Line-Specific Synthetic Lethality Prediction', 'abstract': 'Synthetic lethality (SL), an important type of genetic interaction, can provide useful insight into the target identification process for the development of anticancer therapeutics. Although several well-established SL gene pairs have been verified to be conserved in humans, most SL interactions remain cell-line specific. Here, we demonstrated that the cell-line-specific gene expression profiles derived from the shRNA perturbation experiments performed in the LINCS L1000 project can provide useful features for predicting SL interactions in human. In this paper, we developed a semi-supervised neural network-based method called EXP2SL to accurately identify SL interactions from the L1000 gene expression profiles. Through a systematic evaluation on the SL datasets of three different cell lines, we demonstrated that our model achieved better performance than the baseline methods and verified the effectiveness of using the L1000 gene expression features and the semi-supervise training technique in SL prediction.', 'corpus_id': 211534922, 'score': 1}, {'doc_id': '49558949', 'title': 'Harnessing synthetic lethality to predict the response to cancer treatment', 'abstract': 'While synthetic lethality (SL) holds promise in developing effective cancer therapies, SL candidates found via experimental screens often have limited translational value. Here we present a data-driven approach, ISLE (identification of clinically relevant synthetic lethality), that mines TCGA cohort to identify the most likely clinically relevant SL interactions (cSLi) from a given candidate set of lab-screened SLi. We first validate ISLE via a benchmark of large-scale drug response screens and by predicting drug efficacy in mouse xenograft models. We then experimentally test a select set of predicted cSLi via new screening experiments, validating their predicted context-specific sensitivity in hypoxic vs normoxic conditions and demonstrating cSLi’s utility in predicting synergistic drug combinations. We show that cSLi can successfully predict patients’ drug treatment response and provide patient stratification signatures. ISLE thus complements existing actionable mutation-based methods for precision cancer therapy, offering an opportunity to expand its scope to the whole genome.Synthetic lethality (SL) offers a new precision\xa0oncology approach, which is based on targeting cancer-specific vulnerabilities across the\xa0whole genome, going beyond cancer drivers. The authors develop an approach termed ISLE to identify clinically relevant SL interactions and use them for patient stratification and novel target identification.', 'corpus_id': 49558949, 'score': 1}, {'doc_id': '218486798', 'title': 'Computational modelling in single-cell cancer genomics: methods and future directions.', 'abstract': 'Single-cell technologies have revolutionized biomedical research by enabling scalable measurement of the genome, transcriptome, proteome, \u200band epigenome of multiple systems at single-cell resolution. Now widely applied to cancer models, these assays offer new insights into tumour heterogeneity, which underlies cancer initiation, progression, and relapse. However, the large quantities of high-dimensional, noisy data produced by single-cell assays can complicate data analysis, obscuring biological signals with technical artefacts. In this review article, we outline the major challenges in analyzing single-cell cancer genomics data and survey the current computational tools available to tackle these. We further outline unsolved problems that we consider major opportunities for future methods development to help interpret the vast quantities of data being generated.', 'corpus_id': 218486798, 'score': 0}, {'doc_id': '218516949', 'title': 'A Pipeline for Integrated Theory and Data-Driven Modeling of Genomic and Clinical Data', 'abstract': 'High throughput genome sequencing technologies such as RNA-Seq and Microarray have the potential to transform clinical decision making and biomedical research by enabling high-throughput measurements of the genome at a granular level. However, to truly understand causes of disease and the effects of medical interventions, this data must be integrated with phenotypic, environmental, and behavioral data from individuals. Further, effective knowledge discovery methods that can infer relationships between these data types are required. In this work, we propose a pipeline for knowledge discovery from integrated genomic and clinical data. The pipeline begins with a novel variable selection method, and uses a probabilistic graphical model to understand the relationships between features in the data. We demonstrate how this pipeline can improve breast cancer outcome prediction models, and can provide a biologically interpretable view of sequencing data.', 'corpus_id': 218516949, 'score': 0}, {'doc_id': '215613124', 'title': 'In Silico Discovery of Candidate Drugs against Covid-19', 'abstract': 'Previous studies reported that Angiotensin converting enzyme 2 (ACE2) is the main cell receptor of SARS-CoV and SARS-CoV-2. It plays a key role in the access of the virus into the cell to produce the final infection. In the present study we investigated in silico the basic mechanism of ACE2 in the lung and provided evidences for new potentially effective drugs for Covid-19. Specifically, we used the gene expression profiles from public datasets including The Cancer Genome Atlas, Gene Expression Omnibus and Genotype-Tissue Expression, Gene Ontology and pathway enrichment analysis to investigate the main functions of ACE2-correlated genes. We constructed a protein-protein interaction network containing the genes co-expressed with ACE2. Finally, we focused on the genes in the network that are already associated with known drugs and evaluated their role for a potential treatment of Covid-19. Our results demonstrate that the genes correlated with ACE2 are mainly enriched in the sterol biosynthetic process, Aryldialkylphosphatase activity, adenosylhomocysteinase activity, trialkylsulfonium hydrolase activity, acetate-CoA and CoA ligase activity. We identified a network of 193 genes, 222 interactions and 36 potential drugs that could have a crucial role. Among possible interesting drugs for Covid-19 treatment, we found Nimesulide, Fluticasone Propionate, Thiabendazole, Photofrin, Didanosine and Flutamide.', 'corpus_id': 215613124, 'score': 0}, {'doc_id': '693484', 'title': 'An in-silico approach to predict and exploit synthetic lethality in cancer metabolism', 'abstract': 'Synthetic lethality is a promising concept in cancer research, potentially opening new possibilities for the development of more effective and selective treatments. Here, we present a computational method to predict and exploit synthetic lethality in cancer metabolism. Our approach relies on the concept of genetic minimal cut sets and gene expression data, demonstrating a superior performance to previous approaches predicting metabolic vulnerabilities in cancer. Our genetic minimal cut set computational framework is applied to evaluate the lethality of ribonucleotide reductase catalytic subunit M1 (RRM1) inhibition in multiple myeloma. We present a computational and experimental study of the effect of RRM1 inhibition in four multiple myeloma cell lines. In addition, using publicly available genome-scale loss-of-function screens, a possible mechanism by which the inhibition of RRM1 is effective in cancer is established. Overall, our approach shows promising results and lays the foundation to build a novel family of algorithms to target metabolism in cancer.Exploiting synthetic lethality is a promising approach for cancer therapy. Here, the authors present an approach to identifying such interactions by finding genetic minimal cut sets (gMCSs) that block cancer proliferation, and apply it to study the lethality of RRM1 inhibition in multiple myeloma.', 'corpus_id': 693484, 'score': 1}, {'doc_id': '219153918', 'title': 'Finding disease modules for cancer and COVID-19 in gene co-expression networks with the Core&Peel method', 'abstract': 'Diseases imply dysregulation of cell’s functions at several levels. The study of differentially expressed genes in case-control cohorts of patients is often the first step in understanding the details of the cell’s dysregulation. A further level of analysis is introduced by noticing that genes are organized in functional modules (often called pathways), thus their action and their dysregulation may be better understood by the identification of the modules most affected by the disease (aka disease modules, or active subnetworks). We describe how an algorithm based on the Core&Peel method developed originally for detecting protein complexes in PPI networks, can be adapted to detect disease modules in co-expression networks of genes. We first validate Core&Peel for the easier general task of functional module detection by comparison with 42 methods participating in the Disease Module Identification DREAM challenge of 2019. Next, we use four specific disease test cases (colorectal cancer, prostate cancer, asthma and rheumatoid arthritis), four state-of-the-art algorithms (ModuleDiscoverer, Degas, KeyPathwayMiner and ClustEx), and several pathway databases to validate the proposed algorithm. Core&Peel is the only method able to find significant associations of the predicted disease module with known validated relevant pathways for all four diseases. Moreover for the two cancer data sets, Core&Peel detects further nine relevant pathways enriched in the predicted disease module, not discovered by the other methods used in the comparative analysis. Finally we apply Core&Peel, along with other methods, to explore the transcriptional response of human cells to SARS-CoV-2 infection, at a modular level, aiming at finding supporting evidence for drug repositioning efforts.', 'corpus_id': 219153918, 'score': 0}, {'doc_id': '216071787', 'title': 'Impacts of genomic networks governed by human-specific regulatory sequences and genetic loci harboring fixed human-specific neuro-regulatory single nucleotide mutations on phenotypic traits of Modern Humans', 'abstract': 'Recent advances in identification and characterization of human-specific regulatory DNA sequences set the stage for the assessment of their global impact on physiology and pathology of Modern Humans. Gene set enrichment analyses (GSEA) of 8,405 genes linked with 35,074 human-specific neuro-regulatory single-nucleotide changes (hsSNCs) revealed a staggering breadth of significant associations with morphological structures, physiological processes, and pathological conditions of Modern Humans. Significantly enriched traits include more than 1,000 anatomically-distinct regions of the adult human brain, many different types of cells and tissues, more than 200 common human disorders and more than 1,000 records of rare diseases. Thousands of genes connected with neuro-regulatory hsSNCs have been identified, which represent essential genetic elements of the autosomal inheritance and offspring survival phenotypes. A total of 1,494 hsSNC- linked genes are associated with either autosomal dominant or recessive inheritance and 2,273 hsSNC-linked genes have been associated with premature death, embryonic lethality, as well as pre-, peri-, neo-, and post-natal lethality phenotypes of both complete and incomplete penetrance. Differential GSEA implemented on hsSNC-linked loci and associated genes identify 7,990 genes linked to evolutionary distinct classes of human-specific regulatory sequences (HSRS), expression of a majority of which (5,389 genes; 67%) is regulated by stem cell-associated retroviral sequences (SCARS). Interrogations of the MGI database revealed readily available mouse models tailored for precise experimental definitions of functional effects of hsSNCs and SCARS on genes causally affecting thousands of mammalian phenotypes and implicated in hundreds of common and rare human disorders. These observations suggest that a preponderance of human-specific traits evolved under a combinatorial regulatory control of HSRS and neuro-regulatory loci harboring hsSNCs that are fixed in humans, distinct from other primates, and located in differentially-accessible chromatin regions during brain development.', 'corpus_id': 216071787, 'score': 0}]
133	{'doc_id': '88495495', 'title': 'Contact-less Vital Parameter Determination: An e-Health Solution for Elderly Care', 'abstract': 'Vital parameters are key figures for the basis functions of the human body. Without these basis body functions, such as the heart beat, life is impossible. Therefore, vital parameters are indicators for a person’s general medical condition. In recent years, the topic of vital parameter monitoring has been increasingly studied in the field of e-health. Especially the contact-less determination of vital parameters, such as heart rate, respiration rate, oxygen saturation and blood pressure, with consumer cameras brings a variety of advantages. In this work, we present methods to determine the mentioned vital parameters in a contact-less, optical way. Furthermore, we evaluated these methods for an utilisation in home environments with respect to elderly care. As a result, the remote determination of heart and respiration rate show reliable measurements, which makes the proposed methods ready for the application in home environments.', 'corpus_id': 88495495}	11159	"[{'doc_id': '221692431', 'title': 'Agile mobile robotic platform for contactless vital signs monitoring', 'abstract': 'The COVID-19 pandemic has accelerated methods to facilitate contactless evaluation of patients in hospital settings. By minimizing unnecessary in-person contact with individuals who may have COVID-19 disease, healthcare workers (HCW) can prevent disease transmission, and conserve personal protective equipment. Obtaining vital signs is a ubiquitous task that is commonly done in-person. To eliminate the need for in-person contact for vital signs measurement in the hospital setting, we developed Dr. Spot, an agile quadruped robotic system that comprises a set of contactless monitoring systems for measuring vital signs and a tablet computer to enable face-to-face medical interviewing. Dr. Spot is teleoperated by trained clinical staff to facilitate enhanced telemedicine. Specifically, it has the potential to simultaneously measure skin temperature, respiratory rate, heart rate, and blood oxygen saturation simultaneously while maintaining social distancing from the patients. This is important because fluctuations in vital sign parameters are commonly used in algorithmic decisions to admit or discharge individuals with COVID-19 disease. Here, we deployed Dr. Spot in a hospital setting with the ability to measure the vital signs from healthy volunteers from which the measurements of elevated skin temperature screening, respiratory rate, heart rate, and SpO2 were carefully verified with ground-truth sensors.', 'corpus_id': 221692431, 'score': 1}, {'doc_id': '221586448', 'title': 'Self-supervised Depth Denoising Using Lower- and Higher-quality RGB-D sensors', 'abstract': 'Consumer-level depth cameras and depth sensors embedded in mobile devices enable numerous applications, such as AR games and face identification. However, the quality of the captured depth is sometimes insufficient for 3D reconstruction, tracking and other computer vision tasks. In this paper, we propose a self-supervised depth denoising approach to denoise and refine depth coming from a low quality sensor. We record simultaneous RGB-D sequences with unzynchronized lower- and higher-quality cameras and solve a challenging problem of aligning sequences both temporally and spatially. We then learn a deep neural network to denoise the lower-quality depth using the matched higher-quality data as a source of supervision signal. We experimentally validate our method against state-of-the-art filtering-based and deep denoising techniques and show its application for 3D object reconstruction tasks where our approach leads to more detailed fused surfaces and better tracking.', 'corpus_id': 221586448, 'score': 0}, {'doc_id': '227054185', 'title': 'StressNet: Detecting Stress in Thermal Videos', 'abstract': 'Precise measurement of physiological signals is critical for the effective monitoring of human vital signs. Recent developments in computer vision have demonstrated that signals such as pulse rate and respiration rate can be extracted from digital video of humans, increasing the possibility of contact-less monitoring. This paper presents a novel approach to obtaining physiological signals and classifying stress states from thermal video. The proposed network–""StressNet""–features a hybrid emission representation model that models the direct emission and absorption of heat by the skin and underlying blood vessels. This results in an information-rich feature representation of the face, which is used by spatio-temporal network for reconstructing the ISTI ( Initial Systolic Time Interval : a measure of change in cardiac sympathetic activity that is considered to be a quantitative index of stress in humans). The reconstructed ISTI signal is fed into a stress-detection model to detect and classify the individual’s stress state (i.e. stress or no stress). A detailed evaluation demonstrates that Stress-Net achieves estimated the ISTI signal with 95% accuracy and detect stress with average precision of 0.842.', 'corpus_id': 227054185, 'score': 0}, {'doc_id': '222380430', 'title': 'The Benefit of Distraction: Denoising Remote Vitals Measurements using Inverse Attention', 'abstract': 'Attention is a powerful concept in computer vision. End-to-end networks that learn to focus selectively on regions of an image or video often perform strongly. However, other image regions, while not necessarily containing the signal of interest, may contain useful context. We present an approach that exploits the idea that statistics of noise may be shared between the regions that contain the signal of interest and those that do not. Our technique uses the inverse of an attention mask to generate a noise estimate that is then used to denoise temporal observations. We apply this to the task of camera-based physiological measurement. A convolutional attention network is used to learn which regions of a video contain the physiological signal and generate a preliminary estimate. A noise estimate is obtained by using the pixel intensities in the inverse regions of the learned attention mask, this in turn is used to refine the estimate of the physiological signal. We perform experiments on two large benchmark datasets and show that this approach produces state-of-the-art results, increasing the signal-to-noise ratio by up to 5.8 dB, reducing heart rate and breathing rate estimation error by as much as 30%, recovering subtle pulse waveform dynamics, and generalizing from RGB to NIR videos without retraining.', 'corpus_id': 222380430, 'score': 1}, {'doc_id': '221819388', 'title': 'DR2S: Deep Regression with Region Selection for Camera Quality Evaluation', 'abstract': 'In this work, we tackle the problem of estimating a camera capability to preserve fine texture details at a given lighting condition. Importantly, our texture preservation measurement should coincide with human perception. Consequently, we formulate our problem as a regression one and we introduce a deep convolutional network to estimate texture quality score. At training time, we use ground-truth quality scores provided by expert human annotators in order to obtain a subjective quality measure. In addition, we propose a region selection method to identify the image regions that are better suited at measuring perceptual quality. Finally, our experimental evaluation shows that our learning-based approach outperforms existing methods and that our region selection algorithm consistently improves the quality estimation.', 'corpus_id': 221819388, 'score': 0}, {'doc_id': '222134062', 'title': 'MetaPhys: Unsupervised Few-Shot Adaptation for Non-Contact Physiological Measurement', 'abstract': 'There are large individual differences in physiological processes, making designing personalized health sensing algorithms challenging. Existing machine learning systems struggle to generalize well to unseen subjects or contexts, especially in video-based physiological measurement. Although fine-tuning for a user might address this issue, it is difficult to collect large sets of training data for specific individuals because supervised algorithms require medical-grade sensors for generating the training target. Therefore, learning personalized or customized models from a small number of unlabeled samples is very attractive as it would allow fast calibrations. In this paper, we present a novel unsupervised meta-learning approach called MetaPhys for learning personalized cardiac signals from 18-seconds of unlabeled video data. We evaluate our proposed approach on two benchmark datasets and demonstrate superior performance in cross-dataset evaluation with substantial reductions (42% to 44%) in errors compared with state-of-the-art approaches. Visualization of attention maps and ablation experiments reveal how the model adapts to each subject and why our proposed approach leads to these improvements. We have also demonstrated our proposed method significantly helps reduce the bias in skin type.', 'corpus_id': 222134062, 'score': 1}, {'doc_id': '224802894', 'title': 'A Flatter Loss for Bias Mitigation in Cross-dataset Facial Age Estimation', 'abstract': 'The most existing studies in the facial age estimation assume training and test images are captured under similar shooting conditions. However, this is rarely valid in real-worlds applications, where training and test sets usually have different characteristics. In this paper, we advocate a cross-dataset protocol for age estimation benchmarking. In order to improve the cross-dataset age estimation performance, we mitigate the inherent bias caused by the learning algorithm itself. To this end, we propose a novel loss function that is more effective for neural network training. The relative smoothness of the proposed loss function is its advantage with regards to the optimisation process performed by stochastic gradient descent (SGD). Compared with existing loss functions, the lower gradient of the proposed loss function leads to the convergence of SGD to a better optimum point, and consequently a better generalisation. The cross-dataset experimental results demonstrate the superiority of the proposed method over the state-of-the-art algorithms in terms of accuracy and generalisation capability.', 'corpus_id': 224802894, 'score': 0}, {'doc_id': '226237255', 'title': 'RealHePoNet: a robust single-stage ConvNet for head pose estimation in the wild', 'abstract': 'Human head pose estimation in images has applications in many fields such as human–computer interaction or video surveillance tasks. In this work, we address this problem, defined here as the estimation of both vertical (tilt/pitch) and horizontal (pan/yaw) angles, through the use of a single Convolutional Neural Network (ConvNet) model, trying to balance precision and inference speed in order to maximize its usability in real-world applications. Our model is trained over the combination of two datasets: ‘Pointing’04’ (aiming at covering a wide range of poses) and ‘Annotated Facial Landmarks in the Wild’ (in order to improve robustness of our model for its use on real-world images). Three different partitions of the combined dataset are defined and used for training, validation and testing purposes. As a result of this work, we have obtained a trained ConvNet model, coined RealHePoNet, that given a low-resolution grayscale input image, and without the need of using facial landmarks, is able to estimate with low error both tilt and pan angles (\n$$~4.4^{\\circ }$$\n\n average error on the test partition). Also, given its low inference time (6 ms per head), we consider our model usable even when paired with medium-spec hardware (i.e. GTX 1060 GPU).\nCode available at: \nhttps://github.com/rafabs97/headpose_final\n\n Demo video at: \nhttps://www.youtube.com/watch?v=2UeuXh5DjAE\n\n.', 'corpus_id': 226237255, 'score': 0}, {'doc_id': '225068333', 'title': 'Advancing Non-Contact Vital Sign Measurement using Synthetic Avatars', 'abstract': 'Non-contact physiological measurement has the potential to provide low-cost, non-invasive health monitoring. However, machine vision approaches are often limited by the availability and diversity of annotated video datasets resulting in poor generalization to complex real-life conditions. To address these challenges, this work proposes the use of synthetic avatars that display facial blood flow changes and allow for systematic generation of samples under a wide variety of conditions. Our results show that training on both simulated and real video data can lead to performance gains under challenging conditions. We show state-of-the-art performance on three large benchmark datasets and improved robustness to skin type and motion.', 'corpus_id': 225068333, 'score': 1}, {'doc_id': '225066885', 'title': 'Diverse R-PPG: Camera-Based Heart Rate Estimation for Diverse Subject Skin-Tones and Scenes', 'abstract': 'Heart rate (HR) is an essential clinical measure for the assessment of cardiorespiratory instability. Since communities of color are disproportionately affected by both COVID-19 and cardiovascular disease, there is a pressing need to deploy contactless HR sensing solutions for high-quality telemedicine evaluations. Existing computer vision methods that estimate HR from facial videos exhibit biased performance against dark skin tones. We present a novel physics-driven algorithm that boosts performance on darker skin tones in our reported data. We assess the performance of our method through the creation of the first telemedicine-focused remote vital signs dataset, the VITAL dataset. 432 videos (~864 minutes) of 54 subjects with diverse skin tones are recorded under realistic scene conditions with corresponding vital sign data. Our method reduces errors due to lighting changes, shadows, and specular highlights and imparts unbiased performance gains across skin tones, setting the stage for making medically inclusive non-contact HR sensing technologies a viable reality for patients of all skin tones.', 'corpus_id': 225066885, 'score': 1}]"
134	{'doc_id': '226254510', 'title': 'CAFE: An Instructional Design Model to Assist K-12 Teachers to Teach Remotely during and beyond the Covid-19 Pandemic', 'abstract': 'The impact of the COVID-19 global pandemic on schools was massive and unprecedented. Many schools were forced to close, and teachers were forced to deliver their instruction online with a\xa0very short notice. To assist K-12 teachers to teach remotely, a simple instructional design model, CAFE (Content, Activities, Facilitation, & Evaluation), was created. This article describes the context in which CAFE was created and the three stages of improvement it went through from a simple instructional design table to the instructional design model. It also shares a reflection on the creation and characteristics of CAFE and finally, it ends with the introduction of the CAFE model.', 'corpus_id': 226254510}	14718	"[{'doc_id': '221613911', 'title': 'Online Experimentation during COVID-19 Secondary School Closures: Teaching Methods and Student Perceptions', 'abstract': 'The COVID-19 lockout situation affected people all over the world. Despite all of the disadvantages, this situation offered new experiences and perspectives and pushed education advances forward as never before. Something that seemed to be unreal became a worldwide reality within a few days. Instructors of all subjects at all educational levels moved to a virtual environment instantly. Higher education institutions, universities, and colleges seemed to be fairly prepared for this situation. Unfortunately, primary and secondary schools, especially in eastern and central Europe, never considered distance education as a valuable alternative before, so they did not have software, hardware, and staff prepared for such a situation. Moreover, students’ expectations and dilemmas concerning e-learning were not investigated earlier in the context of obligatory subject education. Moving to the virtual environment was particularly challenging for teachers, who wanted to transfer real class experiences into online lessons since chemistry is based on problems, observations, evidence, and experiments. Often, teachers claimed that they could be more efficient if they had knowledge, skills, and proper equipment to run classes online. This paper presents experiences of secondary chemistry teachers from Slovakia, participants in the IT Academy Project, who earlier, within the framework of the project, were equipped with the necessary skills and tools to run virtual classes, supported with data logging experiments. In this communication, the teachers’ efforts using online experimental practices are described, as well as reflections by their students about the experiences.', 'corpus_id': 221613911, 'score': 1}, {'doc_id': '219266171', 'title': 'Imagining what education can be post-COVID-19', 'abstract': 'This Viewpoint argues that the COVID-19 crisis offers a unique chance to imagine more equitable societies and education systems. It is also a call to action, to take meaningful action to bring about that desired future.', 'corpus_id': 219266171, 'score': 1}, {'doc_id': '221091331', 'title': 'The Balance of Roles: Graduate Student Perspectives during the COVID-19 Pandemic', 'abstract': 'The COVID-19 pandemic has impacted personal and professional lives. Graduate students juggle a variety of roles and had to quickly adjust. In this article, six graduate students share their reflections regarding the influence of the pandemic on respective stages in their doctoral program. They provide unique personal and professional perspectives that depict their abrupt shift to remote working and remote learning. The intention of this article is to garner an understanding of graduate students’ challenges during the pandemic, capture their strategies for success, and provide a space for further conversation and support about how the pandemic has impacted graduate students.', 'corpus_id': 221091331, 'score': 1}, {'doc_id': '229382696', 'title': 'Creating Innovative Structures in Workplace and Vocational Digital Learning to Ensure Social Distancing', 'abstract': 'The COVID-19 pandemic has influenced learning, including Vocational Education and Training (VET) and workplace learning in companies. Many people with special needs (social or disabilities) have not benefited from the elearning systems used in this period, and emphasized the fact that digital innovations are necessary in all types of education. This paper highlights the importance of disruptive digital innovations in education, such as personalized e-learning and e-mentors, and presents examples of structures and social measures, which can be developed around improving learning during the COVID-19 crisis to ensure social distancing. Keywords-COVID-19; workplace learning; vocational education and training; disruptive innovation.', 'corpus_id': 229382696, 'score': 0}, {'doc_id': '232162724', 'title': 'The Online Learning Experience of Theology Students in Turkey during the COVID-19 Pandemic: A New Disposition for RE?', 'abstract': 'Abstract Through a quantitative research model, this study aims to understand the general attitudes of theology students in Turkey regarding online learning during the COVID-19 pandemic. In examining the determinant factors on their attitudes, research findings include: previous online education experience, the frequency of participating in synchronous courses, the qualifications of the instructor, the assessment and evaluation procedures, the orientation training programs and the infrastructure opportunities of the universities. Interpreting these factors enables the authors of this article to suggest a conversational relationship between RE and Islamic sciences with some shared objectives that leads to naming guiding principles in a manner that significantly supports students’ vocational journey.', 'corpus_id': 232162724, 'score': 0}, {'doc_id': '220608516', 'title': 'Online interprofessional education during and post the COVID-19 pandemic: a commentary', 'abstract': 'ABSTRACT The full impact of the COVID-19 pandemic on higher education and interprofessional education programs in particular is yet to be determined, however, it is clear that this pandemic is changing the way we live, learn, and work. Online education is becoming the new normal in academia, but it is a development that may be posing a conundrum to some. Teachers of interprofessional education are expected to employ online education, but some may lack the knowledge and expertise to create and facilitate an engaging, positive, and supportive online environment for their students. This report discusses the application of Meaningful Discourse and the Community of Inquiry principles on developing online learning communities in interprofessional education.', 'corpus_id': 220608516, 'score': 1}, {'doc_id': '232169190', 'title': 'Shifting to digital during COVID-19: are teachers empowered to give voice to students?', 'abstract': ""While online learning resources are proliferating in all education delivery modes, from traditional classes to distance learning, institutions may have not recognized their potential for addressing diverse student populations, providing them online with learning experiences according to their individual needs. If teachers embrace online learning and customize their approaches to make online resources accessible to students, the interactive and collaborative nature of online learning may help reduce the lack of interaction in large classes and isolation in distance education. Research reports the need to examine the accessibility of online learning through the lenses of the digital divide dependence on factors related to physical access, skills and motivational factors. The circumstances of the pandemic have revealed inequality in access to education caused by access to technology and online delivery in which teaching approaches may not necessarily address the student voice with appreciation of their culture. Discussion address Kuo and Belland (Educ Technol Res Dev 64:661–680, 2016) article which reports experiences of minority students (e. g., African-American) in continuing education indicating that there has been little study of minority students' use of online learning resources. Authentic learning is highlighted by critical pedagogy as a means of engaging students in real-life problems and giving meaning to their real-life contexts as sources of learning and among which digital spaces play a prominent role in students’ meaning-making."", 'corpus_id': 232169190, 'score': 0}, {'doc_id': '232039108', 'title': '‘Homeschooling’ and the COVID-19 Crisis: The Insights of Parents on Curriculum and Remote Learning', 'abstract': 'The COVID-19 crisis forced schools to temporarily close from March 2020 to June 2020, producing unpredictable changes in instructional contexts and patterns. A new concept of ‘homeschooling’ emerged which required parents to support the implementation of the curriculum through remote learning. This article is based on a case study focusing on the perceptions of experiences of ten parents of Elementary school children during the school lockdown in Alberta, Canada. Parents argue that the schools’ demands on them were unreasonable. These added to the stress of the quarantine and professional losses, and to the burden of working full-time, fulfilling household responsibilities, and having children rely mostly on parents to deliver an often brief, ‘shallow’ weekly lesson plan that lacked clear expectations and reliable assessment pieces. Parents also strongly cast doubts on the popular reliability of online education by suggesting the unsuitability of online tools to promote independent learning among young children. The study may provide valuable contributions to further inform how to better support learning from home during this ongoing pandemic.', 'corpus_id': 232039108, 'score': 0}, {'doc_id': '232134635', 'title': 'Towards a hybrid ecosystem of blended learning within university contexts', 'abstract': 'The socio-health emergency of covid-19 has affected the use of distance learning The massive use of technologies and digital media in higher education has profoundly revolutionized the perceptions of the actors involved and creating innovations within practices that are already in use Starting from the didactic experiences in digital learning environments that have already been tested in recent years, it was possible to summarize in a single model the trajectories of future experimentation in e-learning for universities The purpose of this article is in fact to propose an overall ecosystem of blended learning for university teaching, within the new phase of resumption of activities in the post-covid world The key elements brought into play by the pandemic concern training, i e , (1) users, (2) market rules and (3) didactics, further modified in relation to time, place, technology and teaching content (for practical purposes, we will call these last four categories extrinsic characteristics of the educational process), which are affected by the contextual needs that have emerged The result is a proposal for the application of a hybrid ecosystem of higher university education © 2021 Copyright for this paper by its authors Use permitted under Creative Commons License Attribution 4 0 International (CC BY 4 0)', 'corpus_id': 232134635, 'score': 0}, {'doc_id': '228813553', 'title': 'Digital Literacy and Higher Education during COVID-19 Lockdown: Spain, Italy, and Ecuador', 'abstract': 'Digital literacy constitutes the basis for citizenship in order to be effective and efficient in the 21st Century in professional and personal lives. The set of skills and competences integrating digital literacy are expected to be guaranteed in higher education. During the lockdown globally imposed for the COVID-19 pandemic, educational systems worldwide had to face many disruptive changes. The aim of this research is to present a comparative study of three countries’ higher education institutions (Spain, Italy, and Ecuador), analyzing how they have faced the global lockdown situation, focusing on the development of digital literacy. The methodological approach followed in this study was quantitative with an exploratory-correlational scope using a questionnaire designed ad hoc and applied in a sample of 376 students. Results point the necessity of enhancing the main aspects such as the teacher’s digital skills, sources for learning that may be adapted, communication between universities and students, and teaching methodologies that should be appropriate to the current context. Conclusions may suggest rethinking higher education learning and reinforcing main issues for this transformation, mainly: communication, teaching, and digital competences. Otherwise, digital literacy is not being guaranteed, which means higher education is not accomplishing one of its main objectives.', 'corpus_id': 228813553, 'score': 1}]"
135	{'doc_id': '211296475', 'title': 'Efficient exploration of zero-sum stochastic games', 'abstract': 'We investigate the increasingly important and common game-solving setting where we do not have an explicit description of the game but only oracle access to it through gameplay, such as in financial or military simulations and computer games. During a limited-duration learning phase, the algorithm can control the actions of both players in order to try to learn the game and how to play it well. After that, the algorithm has to produce a strategy that has low exploitability. Our motivation is to quickly learn strategies that have low exploitability in situations where evaluating the payoffs of a queried strategy profile is costly. For the stochastic game setting, we propose using the distribution of state-action value functions induced by a belief distribution over possible environments. We compare the performance of various exploration strategies for this task, including generalizations of Thompson sampling and Bayes-UCB to this new setting. These two consistently outperform other strategies.', 'corpus_id': 211296475}	1173	"[{'doc_id': '222124941', 'title': 'Neural Thompson Sampling', 'abstract': 'Thompson Sampling (TS) is one of the most effective algorithms for solving contextual multi-armed bandit problems. In this paper, we propose a new algorithm, called Neural Thompson Sampling, which adapts deep neural networks for both exploration and exploitation. At the core of our algorithm is a novel posterior distribution of the reward, where its mean is the neural network approximator, and its variance is built upon the neural tangent features of the corresponding neural network. We prove that, provided the underlying reward function is bounded, the proposed algorithm is guaranteed to achieve a cumulative regret of $\\mathcal{O}(T^{1/2})$, which matches the regret of other contextual bandit algorithms in terms of total round number $T$. Experimental comparisons with other benchmark bandit algorithms on various data sets corroborate our theory.', 'corpus_id': 222124941, 'score': 0}, {'doc_id': '222208619', 'title': 'Reward-Biased Maximum Likelihood Estimation for Linear Stochastic Bandits', 'abstract': 'Modifying the reward-biased maximum likelihood method originally proposed in the adaptive control literature, we propose novel learning algorithms to handle the explore-exploit trade-off in linear bandits problems as well as generalized linear bandits problems. We develop novel index policies that we prove achieve order-optimality, and show that they achieve empirical performance competitive with the state-of-the-art benchmark methods in extensive experiments. The new policies achieve this with low computation time per pull for linear bandits, and thereby resulting in both favorable regret as well as computational efficiency.', 'corpus_id': 222208619, 'score': 0}, {'doc_id': '209484195', 'title': 'All Simulations Are Not Equal: Simulation Reweighing for Imperfect Information Games', 'abstract': 'Imperfect information games are challenging benchmarks for artificial intelligent systems. To reason and plan under uncertainty is a key towards general AI. Traditionally, large amounts of simulations are used in imperfect information games, and they sometimes perform sub-optimally due to large state and action spaces. In this work, we propose a simulation reweighing mechanism using neural networks. It performs backwards verification to public previous actions and assign proper belief weights to the simulations from the information set of the current observation, using an incomplete state solver network (ISSN). We use simulation reweighing in the playing phase of the game contract bridge, and show that it outperforms previous state-of-the-art Monte Carlo simulation based methods, and achieves better play per decision.', 'corpus_id': 209484195, 'score': 1}, {'doc_id': '203591495', 'title': 'Watch the Unobserved: A Simple Approach to Parallelizing Monte Carlo Tree Search', 'abstract': 'Monte Carlo Tree Search (MCTS) algorithms have achieved great success on many challenging benchmarks (e.g., Computer Go). However, they generally require a large number of rollouts, making their applications costly. Furthermore, it is also extremely challenging to parallelize MCTS due to its inherent sequential nature: each rollout heavily relies on the statistics (e.g., node visitation counts) estimated from previous simulations to achieve an effective exploration-exploitation tradeoff. In spite of these difficulties, we develop an algorithm, WU-UCT, to effectively parallelize MCTS, which achieves linear speedup and exhibits only limited performance loss with an increasing number of workers. The key idea in WU-UCT is a set of statistics that we introduce to track the number of on-going yet incomplete simulation queries (named as unobserved samples). These statistics are used to modify the UCT tree policy in the selection steps in a principled manner to retain effective exploration-exploitation tradeoff when we parallelize the most time-consuming expansion and simulation steps. Experiments on a proprietary benchmark and the Atari Game benchmark demonstrate the linear speedup and the superior performance of WU-UCT comparing to existing techniques.', 'corpus_id': 203591495, 'score': 1}, {'doc_id': '121214414', 'title': 'Distorted twisted nematic liquid‐crystal structures in zero field', 'abstract': 'Leslie’s differential equations describing the orientation of the optic axis through a twisted nematic layer are solved for the case where there is a nonzero tilt bias angle. Three classes of solutions are investigated: the simple solution of constant tilt angle through the layer with uniform twist, symmetric solutions where the tilt angle in the middle of the layer has an extreme value, and antisymmetric solutions where the tilt angle in the middle of the layer is identically zero. These solutions and their corresponding elastic deformation energies are compared for typical values of the splay, twist, and bend elastic constants of nematic liquid crystals. For 90° twisted nematic layers the antisymmetric solution always has a higher elastic energy than the symmetric solution. This result explains why regions of reverse twist are suppressed in twisted nematic display devices which have finite tilt bias angles at both boundaries. Reverse‐twisted domains can also be avoided by adding chiral dopants to the ne...', 'corpus_id': 121214414, 'score': 0}, {'doc_id': '221340812', 'title': 'The Advantage Regret-Matching Actor-Critic', 'abstract': ""Regret minimization has played a key role in online learning, equilibrium computation in games, and reinforcement learning (RL). In this paper, we describe a general model-free RL method for no-regret learning based on repeated reconsideration of past behavior. We propose a model-free RL algorithm, the AdvantageRegret-Matching Actor-Critic (ARMAC): rather than saving past state-action data, ARMAC saves a buffer of past policies, replaying through them to reconstruct hindsight assessments of past behavior. These retrospective value estimates are used to predict conditional advantages which, combined with regret matching, produces a new policy. In particular, ARMAC learns from sampled trajectories in a centralized training setting, without requiring the application of importance sampling commonly used in Monte Carlo counterfactual regret (CFR) minimization; hence, it does not suffer from excessive variance in large environments. In the single-agent setting, ARMAC shows an interesting form of exploration by keeping past policies intact. In the multiagent setting, ARMAC in self-play approaches Nash equilibria on some partially-observable zero-sum benchmarks. We provide exploitability estimates in the significantly larger game of betting-abstracted no-limit Texas Hold'em."", 'corpus_id': 221340812, 'score': 0}, {'doc_id': '203566414', 'title': 'Evolving Game State Evaluation Functions for a Hybrid Planning Approach', 'abstract': 'Real-time games often require a combination of long-term and short-term planning as well as interleaved planning and execution. In our previous work, we introduced a hybrid planning and execution approach, in which high-level strategical planning is performed by a Hierarchical Task Network Planner and micro-management is done through Monte Carlo Tree Search. We use evaluation functions that represent weighted sums of selected game features as an interface between the two hierarchy levels.In this work, we present a way of automatically evolving the weights of these evaluation functions in order to improve the efficiency of the execution of high-level tasks. We compare the agent using the evolved evaluation functions with the one using manually created evaluation functions against state-of-theart controllers in the Real Time Strategy game environment microRTS.', 'corpus_id': 203566414, 'score': 1}, {'doc_id': '5313705', 'title': 'Effects of grain species and cultivar, thermal processing, and enzymatic hydrolysis on gluten quantitation.', 'abstract': 'Gluten from wheat, rye, and barley can trigger IgE-mediated allergy or Celiac disease in sensitive individuals. Gluten-free labeled foods are available as a safe alternative. Immunoassays such as the enzyme-linked immunosorbent assay (ELISA) are commonly used to quantify gluten in foods. However, various non-assay related factors can affect gluten quantitation. The effect of gluten-containing grain cultivars, thermal processing, and enzymatic hydrolysis on gluten quantitation by various ELISA kits was evaluated. The ELISA kits exhibited variations in gluten quantitation depending on the gluten-containing grain and their cultivars. Acceptable gluten recoveries were obtained in 200mg/kg wheat, rye, and barley-spiked corn flour thermally processed at various conditions. However, depending on the enzyme, gluten grain source, and ELISA kit used, measured gluten content was significantly reduced in corn flour spiked with 200mg/kg hydrolyzed wheat, rye, and barley flour. Thus, the gluten grain source and processing conditions should be considered for accurate gluten analysis.', 'corpus_id': 5313705, 'score': 0}, {'doc_id': '218719525', 'title': 'The Second Type of Uncertainty in Monte Carlo Tree Search', 'abstract': 'Monte Carlo Tree Search (MCTS) efficiently balances exploration and exploitation in tree search based on count-derived uncertainty. However, these local visit counts ignore a second type of uncertainty induced by the size of the subtree below an action. We first show how, due to the lack of this second uncertainty type, MCTS may completely fail in well-known sparse exploration problems, known from the reinforcement learning community. We then introduce a new algorithm, which estimates the size of the subtree below an action, and leverages this information in the UCB formula to better direct exploration. Subsequently, we generalize these ideas by showing that loops, i.e., the repeated occurrence of (approximately) the same state in the same trace, are actually a special case of subtree depth variation. Testing on a variety of tasks shows that our algorithms increase sample efficiency, especially when the planning budget per timestep is small.', 'corpus_id': 218719525, 'score': 1}, {'doc_id': '219177434', 'title': 'Manipulating the Distributions of Experience used for Self-Play Learning in Expert Iteration', 'abstract': 'Expert Iteration (ExIt) is an effective framework for learning game-playing policies from self-play. ExIt involves training a policy to mimic the search behaviour of a tree search algorithm -— such as Monte-Carlo tree search -— and using the trained policy to guide it. The policy and the tree search can then iteratively improve each other, through experience gathered in self-play between instances of the guided tree search algorithm. This paper outlines three different approaches for manipulating the distribution of data collected from self-play, and the procedure that samples batches for learning updates from the collected data. Firstly, samples in batches are weighted based on the durations of the episodes in which they were originally experienced. Secondly, Prioritized Experience Replay is applied within the ExIt framework, to prioritise sampling experience from which we expect to obtain valuable training signals. Thirdly, a trained exploratory policy is used to diversify the trajectories experienced in self-play. This paper summarises the effects of these manipulations on training performance evaluated in fourteen different board games. We find major improvements in early training performance in some games, and minor improvements averaged over fourteen games.', 'corpus_id': 219177434, 'score': 1}]"
136	{'doc_id': '221344171', 'title': 'Plasma Angiotensin Peptide Profiling and ACE (Angiotensin-Converting Enzyme)-2 Activity in COVID-19 Patients Treated With Pharmacological Blockers of the Renin-Angiotensin System', 'abstract': 'Pharmacological blockade of the renin-angiotensin system (RAS) with ACE (angiotensin-converting enzyme) inhibitors or angiotensin type 1 receptor blockers (ARB) reduces morbidity and mortality in various cardiovascular diseases. One of the key RAS-modulating enzymes, ACE2, has recently gained increasing attention because it converts not only angiotensin (Ang) II to the alternative RAS metabolite Ang-(1–7) but also functions as the cellular entry receptor for severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2. At the beginning of the SARS-CoV-2 pandemic, some investigators suggested that because ACE inhibitor or ARB may lead to upregulation of ACE2 expression/activity, use of these agents in coronavirus disease 2019 (COVID-19) patients might be associated with worsened outcomes. Meanwhile, several observational studies have shown that neither the risk of COVID-19 nor its severity is negatively affected by ACE inhibitor or ARB. However, it remains unclear how RAS activity, particularly ACE2, is regulated in COVID-19 and how this is altered by ACE inhibitor/ARB therapy. In this study, we analyzed distinct RAS components in plasma from patients with COVID-19 ±ACE inhibitor/ARB therapy using liquid chromatography-mass spectrometry/mass spectrometry. The study was approved by the Charité-Universitätsmedizin, Berlin, Germany, Institutional Ethics Committee (EA2/204/19, Amendment 1) and registered in the German Registry for Clinical Studies (DRKS00019207). Surplus plasma samples were collected at the time of admission to the emergency room from 6 different patient groups (total, n=58 [women, 21]): SARS-CoV-2 negative control group (control, n=9 [4]), SARS-CoV-2 negative with ACE inhibitor (control-ACE inhibitor, n=10 [2]), SARS-CoV-2 negative with ARBs (control-ARB, n=8 [5]), COVID-19 without ACE inhibitor/ARB (COVID, n=12 [5]), COVID19 with ACE inhibitor (COVID-ACE inhibitor, n=10 [2]), and COVID-19 with ARBs (COVID-ARB, n=9 [3]). Equilibrium levels of Ang-peptides (Ang I, Ang II, Ang[1–7], and Ang-[1–5]) were measured using liquid chromatography-mass spectrometry/mass spectrometry technology (Attoquant Diagnostics). Ang-based markers for ACE (Ang II/Ang I) and plasma renin activity (Ang I+Ang II) were calculated from Ang-peptide levels. ACE2 activity was assayed by a classical kinetic approach applying its natural substrate (ex vivo spiked Ang II) and measuring the turnover to Ang(1–7)±ACE2 inhibitor MLN-4760. The inhibitor-sensitive ACE2-specific turnover was converted to an ACE2 concentration using a calibration curve of recombinant human ACE2. Ang-peptide concentrations/ratios, ACE2 activity, and age between groups were compared using the KruskalWallis test. In case of a significant result, the Dunn-Test for pairwise comparisons using Bonferroni correction was applied. A P of <0.05 was considered statistically significant, although results have to be considered exploratory.', 'corpus_id': 221344171}	16000	[{'doc_id': '232411831', 'title': 'Influence of renin‐angiotensin‐aldosterone system inhibitors on plasma levels of angiotensin‐converting enzyme 2', 'abstract': 'Concern has been raised that treatment with angiotensin‐converting enzyme inhibitors and angiotensin receptor blockers may increase the expression of angiotensin‐converting enzyme 2 (ACE2), which acts as the entry receptor for SARS‐CoV‐2, and lead to an increased risk of death from SARS‐CoV‐2. We aimed to address this concern by evaluating the in vivo relationship of treatment with ACE inhibitors and angiotensin receptor blockers (ARB) with circulating plasma concentrations of ACE2 in a large cohort of patients with established cardiovascular disease (n = 1864) or cardiovascular risk factors (n = 2144) but without a history of heart failure.', 'corpus_id': 232411831, 'score': 1}, {'doc_id': '233326726', 'title': 'Angiotensin-converting enzyme inhibitors and angiotensin II receptor blockers: potential allies in the COVID-19 pandemic instead of a threat?', 'abstract': 'Abstract Angiotensin-converting enzyme 2 (ACE2) is the leading player of the protective renin–angiotensin system (RAS) pathway but also the entry receptor for severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2). RAS inhibitors seemed to interfere with the ACE2 receptor, and their safety was addressed in COVID-19 patients. Pedrosa et al. (Clin. Sci. (Lond.) (2021), 135, 465–481) showed in rats that captopril and candesartan up-regulated ACE2 expression and the protective RAS pathway in lung tissue. In culture of pneumocytes, the captopril/candesartan-induced ACE2 up-regulation was associated with inhibition of ADAM17 activity, counterbalancing increased ACE2 expression, which was associated with reduced SARS-CoV-2 spike protein entry. If confirmed in humans, these results could become the pathophysiological background for justifying RAS inhibitors as cornerstone cardiovascular protectives even during COVID-19 pandemic.', 'corpus_id': 233326726, 'score': 0}, {'doc_id': '232329225', 'title': 'Risk of SARS-CoV-2 infection and COVID-19 prognosis with the use of renin–angiotensin–aldosterone system (RAAS) inhibitors: a systematic review', 'abstract': 'Angiotensin-converting-enzyme-2, being the receptor for SARS-CoV-2, is increased in the use of RAAS inhibitors. Therefore, concerns have been raised over risks of SARS-CoV-2 infection and poor prognosis of COVID-19 in persons with prior exposure to these drugs. This study aimed to systematically review available evidence for associations between exposure to RAAS inhibitors with susceptibility to SARS-CoV-2 infection and clinical outcomes in infected persons. It hopes to address the question on the effects of RAAS inhibitors on the risk of COVID-19 and its prognosis. Search was conducted in the databases of PubMed, Scopus, Cochrane, Embase and MedRxiv.org from December 2019 to May 31, 2020, using relevant keywords. Additional articles were identified through hand-searching of reference lists. Studies that reported associations between positive tests to COVID-19 and use of RAAS inhibitors, and treatment outcomes of COVID-19 patients who had exposure to RAAS inhibitors were considered eligible. The Newcastle–Ottawa scale was used to assess risk of bias in individual studies. The review was conducted in line with Preferred Regulatory Items for Systematic Reviews and Meta-Analysis (PRISMA) guidelines 2009. From the 952 studies screened and 2 studies from reference hand-searching, 18 were reviewed. Four studies evaluated the risks for SARS-CoV-2 infection among RAAS inhibitors users, and 16 (including 2 of the 4 studies) evaluated the clinical outcomes associated with previous exposure to RAAS inhibitors. Evidence does not suggest higher risks for SARS-CoV-2 infection or poor disease prognosis in the use of RAAS inhibitors. This suggests the continued use of RAAS inhibitors by patients with existing needs, which supports the position statements of American Heart Association and European societies for Cardiology.', 'corpus_id': 232329225, 'score': 0}, {'doc_id': '232119620', 'title': 'Soluble angiotensin-converting enzyme 2 is transiently elevated in COVID-19 and correlates with specific inflammatory and endothelial markers', 'abstract': 'Rationale: Angiotensin-converting enzyme 2 (ACE2) is the main entry receptor of severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2), but how SARS-CoV-2 interactions with ACE2 influences the renin-angiotensin system (RAS) in Coronavirus disease 2019 (COVID-19) is unknown. Objective: To measure circulating ACE2 and ACE levels in COVID-19 patients and investigate associations with risk factors, outcome and inflammatory markers. Methods and results: Soluble ACE2 (sACE2) and sACE concentrations were measured by ELISA in plasma samples from 114 hospital-treated COVID-19 patients and 10 healthy controls. Follow-up samples after four months were available for 58/114 patients. Von Willebrand factor (VWF), factor VIII (fVIII), D-dimer, interleukin 6 (IL-6), tumor necrosis factor and plasminogen activator inhibitor 1 (PAI-1) had previously been determined. Levels of sACE2 were higher in COVID-19 patients than in healthy controls, median 5.0 (interquartile range 2.8-11.8) ng/ml versus 1.4 (1.1-1.6) ng/ml, p < 0.0001. sACE2 was higher in men than women, but were not affected by other risk factors for severe COVID-19. sACE2 decreased to 2.3 (1.6-3.9) ng/ml at follow-up, p < 0.0001, but remained higher than in healthy controls, p=0.012. Follow-up sACE2 levels were higher with increasing age, BMI, total number of comorbidities, for patients with diabetes and patients on RAS-inhibition. sACE was marginally lower during COVID-19 compared with at follow-up, 57 (45-70) ng/ml versus 72 (52-87) ng/ml, p=0.008. Levels of sACE2 and sACE did not differ depending on survival or disease severity (care level, respiratory support). sACE2 during COVID-19 correlated with VWF, fVIII and D-dimer, while sACE correlated with IL-6, TNF and PAI-1. Conclusions: sACE2 was transiently elevated in COVID-19, likely due to increased shedding from infected cells. sACE2 and sACE during COVID-19 differed distinctly in their correlations with markers of inflammation and endothelial dysfunction, suggesting release from different cell types and/or vascular beds.', 'corpus_id': 232119620, 'score': 1}, {'doc_id': '214732444', 'title': 'Renin–Angiotensin–Aldosterone System Inhibitors in Patients with Covid-19', 'abstract': 'RAAS Inhibitors in Patients with Covid-19 The effects of renin–angiotensin–aldosterone system blockers on angiotensin-converting enzyme 2 levels and activity in humans are uncertain. The authors hy...', 'corpus_id': 214732444, 'score': 1}, {'doc_id': '232385135', 'title': 'Influence of Antihypertensive Treatment on RAAS Peptides in Newly Diagnosed Hypertensive Patients', 'abstract': '(1) Background: Recently, influences of antihypertensive treatment on the renin–angiotensin–aldosterone system (RAAS) has gained attention, regarding a possible influence on inflammatory and anti-inflammatory pathways. We aimed to study the effects of newly initiated antihypertensive drugs on angiotensin (Ang) II and Ang (1–7) as representers of two counter-regulatory axes. (2) Methods: In this randomized, open-label trial investigating RAAS peptides after the initiation of perindopril, olmesartan, amlodipine, or hydrochlorothiazide, Ang II and Ang (1–7) equilibrium concentrations were measured at 8 a.m. and 12 a.m. at baseline and after four weeks of treatment. Eighty patients were randomized (1:1:1:1 fashion). (3) Results: Between the four substances, we found significant differences regarding the concentrations of Ang II (p < 0.0005 for 8 a.m., 12 a.m.) and Ang (1–7) (p = 0.019 for 8 a.m., <0.0005 for 12 a.m.) four weeks after treatment start. Ang II was decreased by perindopril (p = 0.002), and increased by olmesartan (p < 0.0005), amlodipine (p = 0.012), and hydrochlorothiazide (p = 0.001). Ang (1–7) was increased by perindopril and olmesartan (p = 0.008/0.002), but not measurably altered by amlodipine and hydrochlorothiazide (p = 0.317/ 0.109). (4) Conclusion: The initiation of all first line antihypertensive treatments causes early and distinct alterations of equilibrium angiotensin levels. Given the additional AT1R blocking action of olmesartan, RAAS peptides shift upon initiation of perindopril and olmesartan appear to work in favor of the anti-inflammatory axis compared to amlodipine and hydrochlorothiazide.', 'corpus_id': 232385135, 'score': 1}, {'doc_id': '233245240', 'title': 'The fight against COVID-19: Striking a balance in the renin–angiotensin system', 'abstract': '\n The novel coronavirus severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) enters host cells by interacting with membrane-bound angiotensin-converting enzyme 2 (ACE2), a vital element in the renin–angiotensin system (RAS), which regulates blood pressure, fluid balance, and cardiovascular functions. We herein evaluate existing evidence for the molecular alterations within the RAS pathway (e.g., ACE2 and angiotensin II) during SARS-CoV-2 infection and subsequent Coronavirus 2019 (COVID-19). This includes reports regarding potential effect of RAS blockade (e.g., ACE inhibitors and angiotensin II receptor blockers) on ACE2 expression and clinical outcomes in patients with co-morbidities commonly treated with these agents. The collective evidence suggests a dual role for ACE2 in COVID-19, depending on the stage of infection and the coexisting diseases in individual patients. This information is further discussed with respect to potential therapeutic strategies targeting RAS for COVID-19 treatment.\n', 'corpus_id': 233245240, 'score': 0}, {'doc_id': '233235714', 'title': 'Classical and Counter-Regulatory Renin–Angiotensin System: Potential Key Roles in COVID-19 Pathophysiology', 'abstract': '\n In the current coronavirus disease 2019 (COVID-19) pandemic, severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) uses angiotensin-converting enzyme-2 (ACE-2) receptor for cell entry leading to ACE-2 dysfunction and downregulation which disturbs the balance between classical and counter-regulatory renin-angiotensin system (RAS) in favor of classical RAS. RAS dysregulation is one of the major characteristics of several cardiovascular diseases, thus adjustment of this system is the main therapeutic target. RAS inhibitors – particularly angiotensin-converting enzyme inhibitors (ACEIs) and angiotensin II type 1 receptor blockers (ARBs) – are commonly used for treatment of hypertension and cardiovascular disease. Patients with cardiovascular diseases are the group most commonly seen amongst those with COVID-19 comorbidity. At the beginning of this pandemic, a dilemma occurred regarding the use of ACEIs and ARBs potentially aggravating cardiovascular and pulmonary dysfunction in COVID-19 patients. Urgent clinical trials from different countries and hospitals reported that there is no association between RAS inhibitor treatment and COVID-19 infection or comorbidity complication. Nevertheless, the disturbance of the RAS system that is associated with COVID-19 infection and the potential treatment targeting this area has yet to be resolved. In this review, the link between the dysregulation of classical RAS and counter-regulatory RAS activities in COVID-19 patients with cardiovascular metabolic diseases is investigated. In addition, the latest findings based on ACEIs and ARBs administration and ACE-2 availability in relation to COVID-19 which may provide a better understanding of RAS contribution to COVID-19 pathology is discussed as it is of the utmost importance amid the current pandemic.\n', 'corpus_id': 233235714, 'score': 0}, {'doc_id': '232405071', 'title': 'Renin–Angiotensin–Aldosterone System: Friend or Foe—The Matter of Balance. Insight on History, Therapeutic Implications and COVID-19 Interactions', 'abstract': 'The renin-angiotensin-aldosterone system (RAAS) ranks among the most challenging puzzles in cardiovascular medicine [...].', 'corpus_id': 232405071, 'score': 0}, {'doc_id': '232168237', 'title': 'Plasma ACE2 levels predict outcome of COVID-19 in hospitalized patients', 'abstract': 'Background Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) binds to angiotensin converting enzyme 2 (ACE2) enabling entrance of the virus into cells and causing the infection termed coronavirus disease of 2019 (COVID-19). COVID-19 is a disease with a very broad spectrum of clinical manifestations, ranging from asymptomatic and subclinical infection to severe hyperinflammatory syndrome and death. Methods This study used data from a large longitudinal study of 306 COVID-19 positive patients and 78 COVID-19 negative patients (MGH Emergency Department COVID-19 Cohort with Olink Proteomics). Comprehensive clinical data were collected on this cohort, including 28-day outcomes classified according to the World Health Organization (WHO) COVID-19 outcomes scale. The samples were run on the Olink Explore 1536 platform which includes measurement of the ACE2 protein. Findings High baseline levels of ACE2 in plasma from COVID-19 patients were associated with worse WHOmax category at 28 days with OR=0.56, 95%-CI: 0.44-0.71 (P < 0.0001). This association was significant in regression models with correction for baseline characteristics, pre-existing medical conditions, and laboratory test results. High levels of ACE2 in plasma from COVID-19 patients were also significantly associated with worse WHO category at the time of blood sampling at both day 0, day 3, and day 7 (P = 0.0004, P < 0.0001, and P < 0.0001, respectively). The levels of ACE2 in plasma from COVID-19 patients with hypertension were significantly higher compared to patients without hypertension (P = 0.0045). The plasma ACE2 levels were also significantly higher in COVID-19 patients with pre-existing heart conditions and kidney disease compared with patients without these pre-existing conditions (P = 0.0363 and P = 0.0303, respectively). There was no difference in plasma ACE2 levels comparing patients with or without pre-existing lung disease, diabetes, or immunosuppressive conditions (P = 0.953, P = 0.291, and P = 0.237, respectively). The associations between high plasma levels of ACE2 and worse WHOmax category during 28 days were more pronounced in COVID-19 positive patients compared with COVID-19 negative patients but the difference was not significant in the two-way ANOVA analysis. Interpretation This study suggests that measuring ACE2 is potentially valuable in predicting COVID-19 outcomes. Further, ACE2 levels could be a link between severe COVID-19 disease and its risk factors, namely hypertension, pre-existing heart disease and pre-existing kidney disease. The design of the data analysis using the Olink platform does not allow assessment of quantitative differences. However, previous studies have described a positive correlation between plasma ACE2 and ACE1 activity. This is interesting because ACE1 (serum ACE) analysis is a standardized test in most hospital laboratories. Therefore, our study encourages quantitative investigations of both plasma ACE 1 and 2 in COVID-19.', 'corpus_id': 232168237, 'score': 1}]
137	{'doc_id': '149174557', 'title': 'A diasporic right to the city: the production of a Moroccan diaspora space in Granada, Spain', 'abstract': 'Abstract In this paper, I bring together ideas of ‘diaspora space’ and ‘the right to the city’ and empirically demonstrate how the formation of diasporas is frequently dependent on migrants attaining certain rights to the city. These rights, I argue, are conditioned and attained by the interplay of urban structural context with the place-making strategies of migrants. Drawing on 8 months of ethnographic fieldwork, I demonstrate that Moroccan migrants in Granada, Spain, have achieved a partial right to a neighbourhood of the city, producing a multi-sensory, self-orientalised diaspora space. First, I show that certain urban conditions in Granada provided a foothold for Moroccan migrants to begin to form a diaspora and transform urban space. Second, I demonstrate that through the mobilisation of a strategically self-orientalised cultural capital, the diaspora have partly appropriated the valuable history of Al-Andalus, a key component in the city’s tourist imagery. These factors and strategies have enabled Moroccan migrants to gain a right to have a visible presence in the city, a right to produce and transform urban space and a right to spatalise diverse identities – all key rights, I argue, in the formation of a diaspora.', 'corpus_id': 149174557}	7060	"[{'doc_id': '36666106', 'title': 'MOROCCAN DIASPORA IN FRANCE: COMMUNITY BUILDING ON YABILADI PORTAL', 'abstract': 'Over the last decade, social networking sites emerge as an ideal tool of communication that facilitate interaction among people online. At the same time, in a world which is characterized by massive waves of migration, globalization results in the construction of the diaspora who seek through new ways to build communities. Within this framework, while traditional media have empowered diaspora members to maintain ties and bonds with their homeland and fellow members, the emergence of social media have offered new opportunities for diasporas to get involved in diasporic identity and community construction. The creation of several diasporic groups on social media like Yabiladi.com and WAFIN.be, respectively in France and Belgium, emphasize the vital role they play in everyday lives of the diaspora. To study the importance and implications of these online communities for diaspora members and investigate their online practices, this article carries out a virtual ethnography of the Moroccan community on Yabiladi portal in France. By means of the qualitative approach of interviews, this article aims at justifying whether the online groups of diasporic Moroccans in France can be defined as communities, and whether social networking sites can be considered as an alternative landscape for the diaspora to create links with other diasporic members. This article, through users’ experience, provides deep understanding of Yabiladi members’ beliefs about the “community” and their online daily practices which enable them to “imagine” it as a community.', 'corpus_id': 36666106, 'score': 1}, {'doc_id': '219979369', 'title': 'Out of the Urban Shadows: Uneven Development and Spatial Politics in Immigrant Suburbs', 'abstract': 'It is now well established that the concentric zone model, developed by Ernest Burgess and elaborated by others in the Chicago School of Sociology to explain the distribution of social groups in metropolitan areas, was wrong. In the past several decades, immigrants have not only moved out of the centers of U.S. metropolitan areas, many have bypassed central cities altogether and settled directly in suburbs. Increasingly, they have done so in nontraditional gateway cities, such as those in the American South and Rustbelt, and in smaller metropolitan or nonmetropolitan areas (Singer et al. 2008). Suburban settlement has also not clearly been associated with immigrants’ “move up” or integration into the so-called Americanmainstream, as Chicago school authors argued. In many rapidly growing metropolitan areas, rising housing prices have pushed many immigrants out of their historic urban neighborhoods. While post-World War II visions of the American Dream may still pull immigrants to suburbia, the communities into which many have settled hardly reflect that dream.While Asian immigrants have high rates of settlement in middle-class, affluent, and white suburban neighborhoods, other immigrants more commonly settle into suburbs with relatively high rates of foreclosure, poverty, segregation, and other measures of disadvantage (Farrell 2016; Logan 2014). These are not the touted “opportunity neighborhoods” that provide pathways to economic mobility. In fact, compared to central city ethnic enclaves, many provide less of the social, cultural and institutional supports that have traditionally promoted the economic advancement of immigrants and their children. Chicago School scholars also failed to account for the politics within suburbs that challenge not only immigrants’ ability to settle within particular communities, but also to achieve their own purposes and pursuits within them. My research on immigrants in suburbia has sought to fill some of these gaps. It has investigated the struggles of educated, professional Asian immigrants to establish a place for themselves within largely white, middle-class suburbs in Silicon Valley. In the Washington, DC suburbs, I have examined how lower-income, primarily Latino and African immigrants have fought to maintain a presence within redeveloping neighborhoods with rising gentrification and displacement pressures.', 'corpus_id': 219979369, 'score': 0}, {'doc_id': '148884796', 'title': 'Love stories and diasporic identity narratives: Mediatized practices of storytelling in the Moroccan diaspora', 'abstract': None, 'corpus_id': 148884796, 'score': 1}, {'doc_id': '151421442', 'title': 'Moroccan Diaspora in France and the February 20 Movement in Morocco', 'abstract': ""ABSTRACT This article analyzes the links between the so-called 20 February 2011 Movement in Morocco and the Moroccan migrant organizations in France. The research examines how the 50-year-long history of these organizations shapes Moroccan migrants' political experiences and their ties to homeland in order to explain why they do not play a significant role in the events unfolding in Morocco in 2011. To this end, concepts such as diaspora and transnationalism are mobilized to grasp how activists are connecting to places and territories and reducing the distance between them, while preserving a certain unity in their collective action."", 'corpus_id': 151421442, 'score': 1}, {'doc_id': '219956472', 'title': 'Introduction: Changing Tourism in the Cities of Post-communist Central and Eastern Europe', 'abstract': 'ABSTRACT This special edition examines various aspects of urban tourism in the post-communist cities of Central and Eastern Europe (CEE). It begins by examining the nature of tourism restructuring in the region since the end of communism and the way that this unfolds in cities. It then examines major global changes in the nature of tourism and their impacts on urban tourism in CEE. These include the growing demand among tourists for new experiences and destinations; the impact of budget airlines on tourism in smaller cities; the impacts of the sharing economy (particularly Airbnb); and the growing emphasis on events and festivals as a means of attracting visitors to cities. The article ends by introducing the six articles that make up this special edition.', 'corpus_id': 219956472, 'score': 0}, {'doc_id': '133778308', 'title': ""The 'Orient' in the 'Occident' : the social, cultural and spatial dynamics of Moroccan diaspora formations in Granada, Spain"", 'abstract': None, 'corpus_id': 133778308, 'score': 1}, {'doc_id': '220754245', 'title': 'THE CHURCH AND ITS ROLE IN UNDERDEVELOPMENT OF NIGERIA’S ECONOMY: A CRITICAL ANALYSIS', 'abstract': 'Underdevelopment is a condition where the economy of the host nation is being expropriated by the dominating nation. It is equally a stage where the developing society must pass through before it finally gets to the developed stage. The Nigerian economy has had a truncated history from the period of its colonization, through the ‘civilizing mission’ of the colonialists to a peripheral zed and dependent status today. Historically, the Church – through its missionary activities – has played pivotal roles in the socio-cultural liberation and economic development of the people. Conversely, the Church equally plays colluding roles in sustaining the bourgeois’ domination of the proletariat in a manner characteristic of the capitalist economic order. This paper, therefore, examines the role of the Church in sustaining the underdevelopment of the state through religious exploitation of the followers and adherents, and economic condition of the state. The paper is anchored on the Marxian political economy theory as a major tool of analysis where the leadership of the church, in connivance with the political elite, has perpetuated itself through the struggle to cling to power and dominate the followers through resources exploitation and expropriation. It was found that the Nigerian political leaders have not lived up to the critical role of delivering on people’s expectation of guaranteed economic advancement of the Nigerian state; and that the political manipulation of the Church and its leadership works not only to impoverish the members but also to under-develop the Nigerian economy. The paper recommends that the church should exercise its power of spirituality to liberate itself from political grips and embrace democratic participation in decision making while vigorously and conscientiously pursuing members’ wellbeing and economic prosperity of the state.', 'corpus_id': 220754245, 'score': 0}, {'doc_id': '218654538', 'title': 'The Altar of National Prosperity: Extractivism and Sacrifice Zones in Argentine Patagonia', 'abstract': '\u200b: The advances in extractive technologies in the 21st century has led to the creation of a new powerful global actor, the Multinational. These multinationals have no allegiance to a state, as earlier forms of capitalism did, rather they are ventures in the industries of agribusiness and mining that operate in countries throughout Laitn America. These global actors are able to effectively dominate economies through the reprimarization of the countries that host them. Countries like Argentina have welcomed multinationals like Monsanto and Patagonia Gold into their territories, which has proven to be a detriment to the communities and environments in which they take place. These industries promise the creation of jobs, development of economies, and state revenue through taxes and royalties. Upon further inspection of these promises, it is revealed that these goals are misleading and these extractive operations are only able to succeed by preying on the preexisting social, political, and economic inequalities of communities in Argentina. I offer a vignette of socio-environmental conflicts that take place in rural, urban, and Indigenous communities. By analyzing these conflicts across space, identity reconfiguration and articulation such as that of the Mapuche in Río Negro is visibilized. As Mapuche and non-Mapuche community members come together to contest their positions within this extractive paradigm, the persisting logic and legacy of colonialism is revealed.', 'corpus_id': 218654538, 'score': 0}, {'doc_id': '220444451', 'title': 'Constructing the National Identity Discourse in Citizenship Education Policy The Case of Citizenship Education in England', 'abstract': 'The thesis examines the governmental construction of national identity through its citizenship education policy in England, the state with heightened tensions in diversity and identity re-construction aligning with its mandatory citizenship classes since 2002. Theoretically framing the study on the Foucauldian post-structuralism, the thesis utilises Foucauldian-influenced ‘What is the problem represented to be?’ (WPR) method by Bacchi that presents the government as a problem-producer. Conducting qualitative research methods, the study analyses the current National Curriculum in England with the explanatory and foundational state documents of Crick and Ajegbo Reports. The thesis identifies that the government primarily aims to re-construct the inclusive and integrative national identity based on the acknowledgement of multiple identities and a plurality of nations in the citizenship education curriculum in England. Our study, however, also reveals that the English citizenship education policy implicitly presents a few assimilationist elements in the national identity discourse through exclusion and unrepresentativeness of the ethnic and racial identities, hierarchical establishment between native English and minorities, and the division of ‘whites’ and ‘non-whites’. Comparatively examining the documents, the thesis, therefore, concludes that the government has a powerful position in socially and politically reconstructing the discourses, concepts, and meanings over time.', 'corpus_id': 220444451, 'score': 0}, {'doc_id': '199169980', 'title': 'Moroccan Diaspora in France: Web 2.0 and Identity Narratives (Yabiladi Portal As a Case Study)', 'abstract': 'The present article tries to offer an empirical explanation of how members of the Moroccan community in France use social networking sites as a reference for their diasporic collective feelings as well as a space to negotiate and express their diasporic identity narratives. Identity is considered in this article as an ongoing process for self-understanding as well as how the ‘Other’ defines it. Hence, identities are created at the cross-over of the ever-changing boundaries between ‘We’ and ‘They’. It is on the level of these boundaries that identities are negotiated, created and re-created.Some media scholars pin down that the Internet indisputably provides diasporic communities with the fundamental tools for information and communication essentials. Therefore, it has become a structural element for many ethnic communities. The present research endeavours to further investigate the intricate meaning as well as the function of virtual diasporic spheres in the process of circulating the diasporic narratives of identity.This ethnographic research sheds ligh on the online forum of the Moroccan Yabiladi portal in France. This forum provides a suitable research landscape chiefly because it represents an active discussion platform. The research offers an analysis of data which were collected in eight-month period in the forum archives. Keywords: Moroccan diaspora, Identity, Internet, Ethnography.', 'corpus_id': 199169980, 'score': 1}]"
138	{'doc_id': '90456959', 'title': 'A suspected case of myopathy in a free-ranging eastern grey kangaroo (Macropus giganteus)', 'abstract': 'Macropods are susceptible to capture myopathy. A post mortem examination, and haematological and blood chemistry analysis was conducted on a male eastern grey kangaroo (Macropus giganteus) believed to have capture myopathy. Changes in blood chemistry and necrosis of muscle tissue are the most prevalent sign of myopathy in eastern grey kangaroos.', 'corpus_id': 90456959}	20596	"[{'doc_id': '45409490', 'title': 'Biphasic Allometry of Cardiac Growth in the Developing Kangaroo Macropus fuliginosus', 'abstract': 'Interspecific studies of adult mammals show that heart mass (Mh, g) increases in direct proportion to body mass (Mb, kg), such that Mh ∝ Mb1.00. However, intraspecific studies on heart mass in mammals at different stages of development reveal considerable variation between species, Mh ∝ Mb0.70–1.00. Part of this variation may arise as a result of the narrow body size range of growing placental mammals, from birth to adulthood. Marsupial mammals are born relatively small and offer an opportunity to examine the ontogeny of heart mass over a much broader body size range. Data from 29 western grey kangaroos Macropus fuliginosus spanning 800-fold in body mass (0.084–67.5 kg) reveal the exponent for heart mass decreases significantly when the joey leaves the pouch (ca. 5–6 kg body mass). In the pouch, the heart mass of joeys scales with hyperallometry, Mh(in-pouch) = 6.39Mb1.10 ± 0.05, whereas in free-roaming juveniles and adults, heart mass scales with hypoallometry, Mh(postpouch) = 14.2Mb0.77 ± 0.08. Measurements of heart height, width, and depth support this finding. The relatively steep heart growth allometry during in-pouch development is consistent with the increase in relative cardiac demands as joeys develop endothermy and the capacity for hopping locomotion. Once out of the pouch, the exponent decreases sharply, possibly because the energy required for hopping is independent of speed, and the efficiency of energy storage during hopping increases as the kangaroo grows. The right∶left ventricular mass ratios (0.30–0.35) do not change over the body mass range and are similar to those of other mammals, reflecting the principle of Laplace for the heart.', 'corpus_id': 45409490, 'score': 1}, {'doc_id': '205024498', 'title': 'A Phase 2, randomized, partially blinded, active-controlled study assessing the efficacy and safety of variable anticoagulation reversal using the REG1 system in patients with acute coronary syndromes: results of the RADAR trial.', 'abstract': 'AIMS\nWe sought to determine the degree of anticoagulation reversal required to mitigate bleeding, and assess the feasibility of using pegnivacogin to prevent ischaemic events in acute coronary syndrome (ACS) patients managed with an early invasive approach. REG1 consists of pegnivacogin, an RNA aptamer selective factor IXa inhibitor, and its complementary controlling agent, anivamersen. REG1 has not been studied in invasively managed patients with ACS nor has an optimal level of reversal allowing safe sheath removal been defined.\n\n\nMETHODS AND RESULTS\nNon-ST-elevation ACS patients (n = 640) with planned early cardiac catheterization via femoral access were randomized 2:1:1:2:2 to pegnivacogin with 25, 50, 75, or 100% anivamersen reversal or heparin. The primary endpoint was total ACUITY bleeding through 30 days. Secondary endpoints included major bleeding and the composite of death, myocardial infarction, urgent target vessel revascularization, or recurrent ischaemia. Enrolment in the 25% reversal arm was suspended after 41 patients. Enrolment was stopped after three patients experienced allergic-like reactions. Bleeding occurred in 65, 34, 35, 30, and 31% of REG1 patients with 25, 50, 75, and 100% reversal and heparin. Major bleeding occurred in 20, 11, 8, 7, and 10% of patients. Ischaemic events occurred in 3.0 and 5.7% of REG1 and heparin patients, respectively.\n\n\nCONCLUSION\nAt least 50% reversal is required to allow safe sheath removal after cardiac catheterization. REG1 appears a safe strategy to anticoagulate ACS patients managed invasively and warrants further investigation in adequately powered clinical trials of patients who require short-term high-intensity anticoagulation.', 'corpus_id': 205024498, 'score': 0}, {'doc_id': '237637869', 'title': 'Haemosporidian infection does not alter aerobic performance in the Pink-sided Junco (Junco hyemalis mearnsi)', 'abstract': 'Avian haemosporidia are blood parasites that can have dramatic fitness consequences on their hosts, including largescale population declines when introduced to naïve hosts. Yet the physiological effects that accompany haemosporidian infection and underlie these fitness decrements are poorly characterized in most wild birds. Because haemosporidia destroy host red blood cells and consume host hemoglobin, they are predicted to have detrimental impacts on avian blood-oxygen transport and, as a result, reduce aerobic performance. However, the documented effects of infection on avian hematological traits vary across species and no effects have been demonstrated on avian aerobic performance to date. Here we quantified the physiological effects of haemosporidian infections on wild ‘Pink-sided’ Juncos (Junco hyemalis mearnsi) breeding in northwestern Wyoming, USA. We assayed hematological traits (hemoglobin concentration and hematocrit) and aerobic performance (resting and summit metabolic rates, thermogenic endurance, and aerobic scope), then screened individuals for haemosporidian infection post-hoc (n = 106 adult juncos). We found that infection status did not correlate with any of the physiological indices that we measured, suggesting there is little cost of haemosporidian infection on either junco aerobic performance or energy budgets. Our results highlight the need for more studies of haemosporidia infections in a broader range of species and in a wider array of environmental contexts.', 'corpus_id': 237637869, 'score': 0}, {'doc_id': '13094714', 'title': 'The seroprevalence and factors associated with Ross river virus infection in western grey kangaroos (Macropus fuliginosus) in Western Australia.', 'abstract': 'A serosurvey was undertaken in 15 locations in the midwest to southwest of Western Australia (WA) to investigate the seroprevalence of Ross River virus (RRV) neutralizing antibodies and factors associated with infection in western grey kangaroos (Macropus fuliginosus). The estimated seroprevalence in 2632 kangaroo samples, using a serum neutralization test, was 43.9% (95% CI 42.0, 45.8). Location was significantly associated with seroprevalence (p<0.001). There was a strong positive correlation between seroprevalence and the average log-transformed neutralizing antibody titer (r=0.98, p<0.001). The seroprevalence among adult kangaroos was significantly higher than in subadult kangaroos (p<0.05). No significant association was observed between seroprevalence and the sex of kangaroos (p>0.05). The results of this study indicate that kangaroos in WA are regularly infected with RRV and may be involved in the maintenance and transmission of RRV.', 'corpus_id': 13094714, 'score': 1}, {'doc_id': '82835616', 'title': 'Temporal dynamics of helminth infections in eastern grey kangaroos (Macropus giganteus) in Victoria', 'abstract': 'Abstract. Parasite infection is increasingly recognised as a factor shaping the population dynamics, life history and behaviour of hosts. However, before the impacts of parasites on wildlife hosts can be investigated, seasonal patterns in host exposure to parasitic agents must be determined. We examined infection patterns at three sites in Victoria, and combined field experiments and observations to construct a generalised life cycle of the helminth community in eastern grey kangaroos (Macropus giganteus). Kangaroo populations in Victoria had very similar helminth communities, with 20–25 species detected at each site. Despite examining relatively few hosts in this study, at least 87% of all gastrointestinal helminths were recovered according to bootstrap estimates. The prepatent period of infection in eastern grey kangaroo nematodes was at least 3 months, and faecal egg output showed a distinct seasonal pattern, with a peak in egg counts from October through to January each year. Data from one site indicated that faecal egg counts were influenced predominantly by the abundance of a single nematode species (Pharyngostrongylus kappa), despite adults accounting for only 7% of the total nematode burden. This highlights the problems associated with using faecal egg counts to estimate nematode burdens in this host. Contamination of pasture plots showed that nematode eggs take ∼14 days to larvate once deposited, and that autumn rains likely triggered emergence from faecal pellets. The abundance of infective larvae in the environment therefore appears to be closely tied to environmental conditions, with a peak in infection of hosts in the winter months.', 'corpus_id': 82835616, 'score': 1}, {'doc_id': '15509581', 'title': 'A retrospective study of Babesia macropus associated with morbidity and mortality in eastern grey kangaroos (Macropus giganteus) and agile wallabies (Macropus agilis)', 'abstract': 'Highlights • Detailed description of novel Babesia infection causing mortality in macropods.• First report of this infection in agile wallabies.• Information on the geographical incidence of this disease in the eastern states of Australia.• Comprehensive review of the clinical signs and pathology of the disease.', 'corpus_id': 15509581, 'score': 1}, {'doc_id': '236929374', 'title': ""Intestinal parasites in Przewalski's horses (Equus ferus przewalskii): a field survey at the Hortobágy National Park, Hungary"", 'abstract': ""Abstract The Pentezug Wildhorse Reserve, located in the Hortobágy National Park, Hungary, has one of the biggest ex situ populations of Przewalski's horses and aims to preserve its landscape and to study this subspecies. Between September and November 2018, 79 faecal samples were collected from Przewalski's horses. The McMaster, Willis flotation, natural sedimentation and coproculture methods were applied to all the samples. Results showed an average level of 1287 eggs per gram (EPG), which is a high faecal egg-shedding level. All the samples were positive for strongyle-type eggs (100%). There were no statistical differences regarding the EPG values between different harems of the population. The same happened when considering sexes, ages, lactating status or when bachelors are compared with harem members. Cyathostominae were dominant, when compared to Strongylinae and Tricostrongylidae, and 15 different morphological infective third-stage larvae types and/or species belonging to the order Strongylida were identified. The subfamily Cyathostominae was prevalent in 100% of the horses. Strongylus vulgaris was the most prevalent strongylin (40.5%). Additionally, 27.8% were positive for Parascaris sp. and 2.5% showed Oxyuris equi in their faeces. This study revealed that there is a higher prevalence of Triodontophorus serratus and Poteriostomum spp. in juveniles. Horses with S. vulgaris showed lower levels of EPG. This was the first study involving this population, showing 100% prevalence of intestinal parasites."", 'corpus_id': 236929374, 'score': 0}, {'doc_id': '82046341', 'title': 'Genome Sequence of an Australian Kangaroo, Macropus eugenii', 'abstract': 'The tammar wallaby (Macropus eugenii) is a model marsupial species that has been intensively used for research into various aspects of marsupial biology. Recently, it became one of only three marsupial species to have its genome sequenced. Comparisons of genome sequence from all three marsupials with those of other vertebrates have made it possible to begin identifying marsupial-specific genome features. Our knowledge of the biology of the tammar wallaby has informed the interpretation of these unique features and provided insight into their evolution. Among these unique features are genes involved in the sophisticated marsupial lactation system and genes encoding for potent antimicrobial peptides. Comparisons of genome organisation have also provided insight into marsupial genome evolution. \n \n \n \nKey Concepts: \n \n \n \nUncovering the genetic basis of unique marsupial features is now possible due to the availability of genome sequences for three marsupial species. \n \n \n \n \nThe tammar wallaby is a model species used for studies into marsupial biology, making it possible to link genomic features with the biological characteristics. \n \n \n \n \nAs the tammar wallaby genome was only lightly sequenced, other genomic resources such as transcriptome sequences and genome maps have enhanced the genome assembly. \n \n \n \n \nA number of novel genes are associated with the complex marsupial lactation system and immunity. \n \n \n \n \nFive novel genes have been identified on the marsupial Y chromosome. \n \n \n \n \nThe organisation of the tammar wallaby major histocompatibility complex is vastly different to that of any other species studied to date. \n \n \n \n \nA decades old debate over chromosome number in the ancestral marsupial has been resolved by comparing genome organisation among the sequenced marsupials species and the outgroup species human and chicken. \n \n \n \n \nThe wallaby genome assembly in combination with other genomic resources are proving valuable for the discovery of marsupial-specific features. \n \n \n \n \n \n \nKeywords: \n \nmarsupial; \ngenome evolution; \nmammals; \ncomparative genomics; \nlactation', 'corpus_id': 82046341, 'score': 1}, {'doc_id': '237279134', 'title': 'Genetic characterisation of cercarial stages of Choanocotyle Jue Sue and Platt, 1998 (Digenea: Choanocotylidae) in a native Australian freshwater snail, Isidorella hainesii (Tryon)', 'abstract': 'Isidorella hainesii (Tryon, 1866) is a native freshwater snail, belonging to the family Planorbidae, commonly found on aquatic vegetation in south eastern parts of Australia. In the present study, we report natural infection with a species of Choanocotyle Jue Sue and Platt, 1998 (Digenea: Choanocotylidae) parasite in inland Australia for the first time, followed by characterisation of the parasite using both morphological and molecular approaches. Snails (n = 150) were collected from recently drained, natural ponds at a local fish farm located in the Riverina region, New South Wales, Australia. Parasites were subjected to preliminary morphological examination followed by DNA extraction to obtain their ITS-2, 18S and 28S sequences. Based on their sequence data and phylogenetic analyses they were identified as Choanocotyle hobbsi Platt and Tkach, 2003, which has only previously been described from Chelodina oblonga Gray, 1841 (snake-necked turtle) in Western Australia. Previous researchers suggested that in Australia, C. oblonga and its parasite fauna are separated from their eastern counterparts due to formation of impenetrable waterless desert in the country during the late Cretaceous. Our study extends the distribution of Choanocotyle hobbsi from Western Australia to the Murray Darling Basin in New South Wales, however, the definitive host remains unknown in New South Wales.', 'corpus_id': 237279134, 'score': 0}, {'doc_id': '237625714', 'title': 'Toxoplasma gondii in Australian macropods (Macropodidae) and its implication to meat consumption', 'abstract': 'Toxoplasma gondii is a worldwide occurring apicomplexan parasite. Due to its high seroprevalence in livestock as well as in game animals, T. gondii is an important food-borne pathogen and can have significant health implications for humans as well as for pets. This article describes the prevalence of T. gondii in free-ranging macropods hunted for consumption. All hunted macropod species (commercial as well as non-commercial hunt) show a positive seroprevalence for T. gondii. This seroprevalence is influenced by various factors, such as sex or habitat. Furthermore, the parasite shows a high level of genetic variability in macropods. Genetically variable strains have already caused outbreaks of toxoplasmosis in the past (Canada and the US). These were attributed to undercooked game meat like venison. Despite this risk, neither Australia nor New Zealand currently have food safety checks against foodborne pathogens. These conditions scan pose a significant health risk to the population. Especially, since cases of toxoplasmosis have already been successfully traced back to insufficiently cooked kangaroo meat in the past.', 'corpus_id': 237625714, 'score': 0}]"
139	{'doc_id': '151042764', 'title': 'Is Magic a Serious Research Topic? Reflexions On Some French Students’ Remarks About Magic in Psychology', 'abstract': 'Is magic a serious research topic? This question seems a bit weird. Obviously, in psychology show magic is a very serious topic. It deals with psychological processes, and with social interactions. In the late 19th century and early 20th century, famous psychologists as Binet [1,2]; Jastrow [3]; Triplett [4] use magical performances to understand psychological functioning. In 1999 Lamont & Wiseman [5] published Magic in theory, a theoretical book analysing the art of magic. Between 1887 and 1999, only 12 empirical articles about perception of magic in adults were published; they were 55 between 2000 and 2016 [6]. Nowadays, this field is sometimes called “science of magic” [7,8] or, most specifically, “neuromagic” [9]. In psychology, magic can be studied as any other object, or can be a mean to study psychological processes [10]. To our knowledge, most studies deal with perception and cognitive psychology [11,12] and very few with social psychology [13,14].', 'corpus_id': 151042764}	10856	"[{'doc_id': '222089449', 'title': 'Why People Perceive Messages Differently: The Theory of Cognitive Mapping', 'abstract': 'Aim/Purpose The paper introduces new concepts including cognitive mapping, cognitive message processing, and message resonance. Background This paper draws upon philosophy, psychology, physiology, communications, and introspection to develop the theory of cognitive mapping. Methodology Theory development. Contribution The theory offers new ways to conceptualize the informing process. Findings Cognitive mapping has a far-reaching explanatory power on message resonance. Recommendations for Researchers The theory of cognitive mapping offers a new conceptualization for those exploring the informing process that is ripe for exploration and theory testing. Future Research This paper forms a building block toward the development of a fuller model of the informing process.', 'corpus_id': 222089449, 'score': 0}, {'doc_id': '221995493', 'title': 'How do Visualization Designers Think? Design Cognition as a Core Aspect of Visualization Psychology', 'abstract': 'There are numerous opportunities for engaging in research at the intersection of psychology and visualization. While most opportunities taken up by the VIS community will likely focus on the psychology of users, there are also opportunities for studying the psychology of designers. In this position paper, I argue the importance of studying design cognition as a necessary component of a holistic program of research on visualization psychology. I provide a brief overview of research on design cognition in other disciplines, and discuss opportunities for VIS to build an analogous research program. Doing so can lead to a stronger integration of research and design practice, can provide a better understanding of how to educate and train future designers, and will likely surface both challenges and opportunities for future research.', 'corpus_id': 221995493, 'score': 0}, {'doc_id': '6189450', 'title': ""There's more to magic than meets the eye"", 'abstract': 'Document S1. Supplemental Experimental ProceduresxDownload (.1 MB ) Document S1. Supplemental Experimental ProceduresMovie 1 Vanishing ball trick pro-illusion conditionxDownload (.75 MB ) Movie 1 Vanishing ball trick pro-illusion conditionMovie 2 Vanishing ball trick anti-illusion conditionxDownload (.85 MB ) Movie 2 Vanishing ball trick anti-illusion condition', 'corpus_id': 6189450, 'score': 1}, {'doc_id': '222007362', 'title': 'Emotions in Technology Design: From Experience to Ethics', 'abstract': 'Emotions are a hot topic in design, human–computer interaction and any area of business these days. Their significance in areas inwhich peoplemake choices, decisions and engage in action has been undeniable for at least the last 40 years of psychology and consumer scholarship.What once was an extremely contested, fuzzy and (almost) easily scientifically avoidable area, is now at the centre of everyone’s interest. In an era of cognitive computing, artificial intelligence (so-called learning and thinking machines), and optimization, all attention is placed on what makes us human, and theways inwhich human thought actually operates. This emotional logic, intentionality and consciousness itself, drive not simply theways inwhich individuals process (cognitise) information, but also ways in which society and the built world are structured. Emotions play asmuch a role in shaping technology design, as they do in the way we experience it. This introduction presents a book that takes many angles towards concretely understanding what it is in design that makes people emotionally experience it in the ways that they do. It introduces the main themes and concepts of the book that include ethics, culture, measurement and design methods. It additionally demonstrates a broader understanding of technology in chapters that investigate graffiti, urban and art experience, filmic experience, architecture and cultural movements. It is hoped that combining this broader cultural-emotional insight into one package will enable readers to connect their design practice and research to the broader system of emotions, culture, ethics, lived experience and technology. R. Rousi (B) University of Jyväskylä and Gofore, Jyväskylä, Finland e-mail: rebekah.rousi@jyu.fi J. Leikas VTT, Espoo, Finland e-mail: jaana.leikas@vtt.fi P. Saariluoma University of Jyväskylä, Jyväskylä, Finland e-mail: pertti.saariluoma@jyu.fi © Springer Nature Switzerland AG 2020 R. Rousi et al. (eds.), Emotions in Technology Design: From Experience to Ethics, Human–Computer Interaction Series, https://doi.org/10.1007/978-3-030-53483-7_1 1', 'corpus_id': 222007362, 'score': 0}, {'doc_id': '152214855', 'title': 'A Particular Kind of Wonder: The Experience of Magic past and Present', 'abstract': 'Wonder may be an important emotion, but the term wonder is remarkably ambiguous. For centuries, in psychological discourse, it has been defined as a variety of things. In an attempt to be more focused, and given the growing scientific interest in magic, this article describes a particular kind of wonder: the response to a magic trick. It first provides a historical perspective by considering continuity and change over time in this experience, and argues that, in certain respects, this particular kind of wonder has changed. It then describes in detail the experience of magic, considers the extent to which it might be considered acquired rather than innate, and how it relates to other emotions, such as surprise. In the process, it discusses the role of belief and offers some suggestions for future research. It concludes by noting the importance of context and meaning in shaping the nature of the experience, and argues for the value of both experimental and historical research in the attempt to understand such experiences.', 'corpus_id': 152214855, 'score': 1}, {'doc_id': '226602116', 'title': 'Investigating students’ affective states toward laboratory and context-based chemistry', 'abstract': 'Observations of natural phenomena are made possible with the invention of scientific apparatus and instruments. The focus in science education, however, has primarily been on theories rather than what enables the development of such theories, and chemistry curricula reflect this tradition. Introducing students to the role of instruments in science, both in experimental and theoretical aspects, can improve students’ overall understanding of, and appreciation for scientific practices. In addition, students’ increased perception of how chemical concepts are developed and how scientific observations are made can advance their awareness of the nature of science, thereby improving scientific literacy. Integrating the idea that instruments hold a central role in scientific progression can be achieved in both laboratories and lectures, providing students with opportunities to connect concepts to history, scientific practices, and applications. This dissertation is comprised of a series of studies which explores the use of technology and context-based curricular approach to provide general chemistry students with more information about instruments and applications in chemistry. Based on constructivism and the theory of meaningful learning, the affective learning domain, such as attitudes and motivation, was assessed in both chemistry laboratory and lecture courses. An augmented reality tool designed to connect students to information about commonly used instruments in a general chemistry lab course, specifically a pH meter and conductivity meter, was developed, implemented, and its effects on student learning and attitudes were investigated. In addition, for a chemistry lecture course, a context-based curricular approach was taken to introduce students to chemical concepts related to real-life applications, as well as to the role of scientific instruments, and this effort was assessed.', 'corpus_id': 226602116, 'score': 0}, {'doc_id': '141795421', 'title': 'Magic in Theory: An Introduction to the Theoretical and Psychological Elements of Conjuring', 'abstract': 'Magic, properly performed, is a complex and skilful art, and is capable of deceiving anyone. One of the reasons why so little information is available to non-magicians interested in the topic is that magicians are understandably relectant to expose conjuring methods. Magic is a secretive business. Psychologists have long recognised that they may have much to learn from the techniques used by magicians to fool their audiences. Parapsychologists are aware that many individuals claiming to be psychic use magic tricks to fabricate paranormal phenomena. Failure to detect such fraud can lead to serious consequences, including loss of funding and negative publicity. Greater theoretical understanding of conjuring and psychic fraud will raise awareness of how vulnerable observers can be. Parapsychologists, psychologists and magicians have all written about the strategems that lie behind successful conjuring. Each has approached the topic from different viewpoints. This book is an attempt to draw together these different theoretical approaches and present them in a way that is accessible to a non-technical readership. It is partly based on interviews conducted with present-day magicians, many of who are internationally recognized by the magical fraternity for their insight into conjuring psychology and theory.', 'corpus_id': 141795421, 'score': 1}, {'doc_id': '142592204', 'title': 'Magic in Theory', 'abstract': None, 'corpus_id': 142592204, 'score': 1}, {'doc_id': '224802259', 'title': ""Humanity's Magic Number as 1.5?"", 'abstract': 'Introduction Preliminary checklist of recognition of 1.5 Potentially significant approximations to 1.5 in practice? Clarification of the magic square governing human civilization? Radical possibility of 1.5 resulting from from golden ratio design by committee In quest of a language of proportion as the language of appropriateness Adaptation of the Uncertainty Principle to the social sciences? Language of proportion implied by poetic justice Deflowering of civilization versus Flowering of civilization: an aesthetic contrast? Spiral of silence and the associated ""conspiracy"" of silence Mysterious challenge of doubling, replication and multiplication References', 'corpus_id': 224802259, 'score': 0}, {'doc_id': '13949442', 'title': 'Where Science and Magic Meet: The Illusion of a “Science of Magic”', 'abstract': 'Recent articles calling for a scientific study of magic have been the subject of widespread interest. This article considers the topic from a broader perspective and argues that to engage in a science of magic, in any meaningful sense, is misguided. It argues that those who have called for a scientific theory of magic have failed to explain either how or why such a theory might be constructed, that a shift of focus to a neuroscience of magic is simply unwarranted, and that a science of magic is itself an inherently unsound idea. It seeks to provide a more informed view of the relationship between science and magic and suggests a more appropriate way forward for scientists.', 'corpus_id': 13949442, 'score': 1}]"
140	"{'doc_id': '3051004', 'title': 'Study of the effect of harmonics on measurments of the energy meters', 'abstract': ""An investigation of the effect of voltage and current harmonics on the accuracy of readings of energy meters was carried out. Harmonic distortion is one of the power quality problems, which typically arises from the consumer's electrical equipment. Such harmonics produced by discontinuous conducting devices and non-linear loads create recognized problems for most power distribution systems. These problems can lead to over heating and power losses of power system distribution components. Such conditions involved differences in voltage and current magnitudes as well as differences in the voltage and current total harmonic distortion levels. This study presents comparison between the electromechanical energy meters and the electronic digital energy meters. The study also includes the accuracy of readings when the meters are subjected to the same non-linear similar conditions. The harmonics were measured using VIP system III illustrating the THD of voltage and current when power factor compensating capacitors were used or not. It was found that the accuracy and the sensitivity as well as the precision of digital electronic meters are higher than that of the electromechanical one. When using both meters to read the energy consumed by the loads, it was noticed that there is a difference between readings of the two compared types as the digital electronic meter has a difference of (± 0.5% to 1%) which is a considerable amount of energy for large consumers. An economical assessment was made and showed that the digital meter is saving about L.E 1.8 to 3.6 million per annum for the distribution company. It is recommended to distribution companies to use digital meters instead of electromechanical ones especially for large and medium consumers."", 'corpus_id': 3051004}"	19395	"[{'doc_id': '235810502', 'title': 'Fuzzy logic , PI and ANN in improvement of power quality using unified Power quality conditioner', 'abstract': ""Introduction With the advent of power semiconductor switching devices, like thyristors, GTO's (Gate Turn off thyristors), IGBT's (Insulated Gate Bipolar Transistors) and many more devices, control of electric power has become a reality. Such power electronic controllers are widely used to feed electric power to electrical loads, such as adjustable speed drives (ASD's), furnaces, computer power supplies, HVDC systems etc. The power electronic devices due to their inherent nonlinearity draw harmonic and reactive power from the supply. In three phase systems, they could also cause unbalance and draw excessive neutral currents. The injected harmonics, reactive power burden, unbalance, and excessive neutral currents cause low system efficiency and poor power factor. In addition to this, the power system is subjected to various transients like voltage sags, swells, flickers etc. These transients would affect the voltage at distribution levels. Excessive reactive power of loads would increase the generating capacity of generating stations and increase the transmission losses in lines. Hence supply of reactive power at the load ends becomes essential. Power Quality (PQ) has become an important issue since many loads at various distribution ends like adjustable speed drives, process industries, printers, domestic utilities, computers, microprocessor based equipments etc. have become intolerant to voltage fluctuations, harmonic content and interruptions. Power Quality (PQ) mainly deals with issues like maintaining a fixed voltage at the Point of Common Coupling (PCC) for various distribution voltage levels irrespective of voltage fluctuations, maintaining near unity power factor power drawn from the supply, blocking of voltage and current unbalance from passing upwards from various distribution levels, reduction of voltage and current harmonics in the system and suppression of excessive supply neutral current. Conventionally, passive LC filters and fixed compensating devices with some degree of variation like thyristor switched capacitors, thyristor switched reactors were employed to improve the power factor of ac loads. Such devices have the demerits of fixed compensation, large size, ageing and resonance. Nowadays equipments using power semiconductor devices, generally known as active power filters (APF's), Active Power Line Conditioners (APLC's) etc. are used for the power quality issues due to their dynamic and adjustable solutions. Flexible AC Transmission Systems (FACTS) and Custom Power products like STATCOM synchronous COMpensator), DVR (Dynamic voltage Restorer), etc. deal with the issues related to power quality using similar control strategies and concepts. Basically, they are different only in the location in a power system where they are deployed and the objectives for which they are deployed. UPQC: Active Power Filters can be classified, based on converter type, topology and the number of phases. Converter types are Current Source Inverter (CSI) with inductive energy storage or Voltage Source Inverter (VSI) with capacitive energy storage. The topology can be shunt, series or combination of both. The third classification is based on the number of phases, such as single phase systems, three phase systems or three phase four wire systems. The Objective of this paper, one such APLC known as Unified Power Quality Conditioner (UPQC), which can be used at the PCC for improving power quality, is designed, simulated using proposed control strategy and the performance is evaluated for various nonlinear loads (steel plant loads). Unified Power Quality Conditioner (UPQC) using PLL with PWM Control is discussed and simulated. Case study of a typical steel plant has been given. Simulated proposed UPQC for various non-linear loads of steel plant and results with installation of STATCOM and UPQC are reported."", 'corpus_id': 235810502, 'score': 0}, {'doc_id': '235614032', 'title': 'Smarter Grid in the 5G Era: A Framework Integrating Power Internet of Things With a Cyber Physical System', 'abstract': 'As the energy infrastructure of smart cities, smart grid upgrades traditional power grid systems with state-of-the-art information and communication technologies. In particular, as the full deployment of the Internet of Things in the power grid (a.k.a. power Internet of Things or PIoT), the newly introduced information flow together with inherent energy flow makes it more efficient for power generation, transmission, distribution, and consumption. To further exploit the precious energy and the latest 5G technologies, this article boosts to add a value flow in the smart grid, mainly including the value created by innovative services and market mechanisms and the value added by the information flow. Specifically, by integrating PIoT with cyber-physical systems, this article sketches a conceptual framework of the cyber-physical power system (CPPS). The CPPS carries out holistic perception and ubiquitous connection of distributed energy sources and electrical facilities and builds up a smarter power grid with global information interaction, intelligent decision-making, and real-time agile control. Finally, for illustration purposes, we conduct a case study regarding an intelligent home management system.', 'corpus_id': 235614032, 'score': 0}, {'doc_id': '235599498', 'title': 'Smart Energy Meter Calibration: An Edge Computation Method: Poster', 'abstract': 'Smart meters are the backbone of smart grids. They provide real time electricity consumption data and and are widely used for measuring, monitoring and analyzing energy consumption. Sometimes, they enable users to perform corrective actions. But, to facilitate proper data analysis, it is imperative that data be accurate or have minimum error. This paper presents an edge deployed smart meter error correction algorithm that utilises Clustering (using K-Means algorithm) and Feed-Forward Artificial Neural Networks (ANN). An edge device, a Raspberry Pi Module, connects smart meters to the internet. The algorithm maps (possibly erroneous) readings of our in-house developed meters to readings of calibrated standard off-the-shelf (Schneider) meters. Usage of Clustering with ANN has helped substantially improve the accuracy of the readings from a previously used linear regression designed for the same purpose. An accuracy of 70-75% was achieved while using linear regression, whereas the proposed algorithm obtains accuracy in the range of 84.47-88%. The neural networks are also less complex, making them suitable for deployment in Raspberry Pi 3B based embedded hardware systems.', 'corpus_id': 235599498, 'score': 1}, {'doc_id': '30523191', 'title': 'Analysis of electricity meters under distorted load conditions', 'abstract': ""Due to the rapid growth of non-linear loads in power systems in recent decades, harmonic pollution is becoming more and more serious. Energy measurement devices are commonly designed for working at sine wave. In this paper the results of an analysis on different electricity meters used in distribution network under different load conditions are presented. Study shows that error of energy meter's reading depends on the type of the meter, harmonic distortion, reactive energy and direction of reactive power. Measured errors were higher than 6% under some conditions."", 'corpus_id': 30523191, 'score': 1}, {'doc_id': '235824045', 'title': 'Unified Meter for Electricity, Gas and Water with Automatic Billing and Payment', 'abstract': 'Every month it is seen that there are three bills generated for the consumption of the basic necessities like electricity, gas and water we use on a daily basis. The proposed system measures the consumption of all these resources and provides a unified billing and payment system for it. This has two sub-systems consisting of a sub-system for measurement and the server-controlled sub-system for data storage and analysis. Communication between these two subsystems is done through wireless network. This system monitors the consumption of electricity, water and gasoline resources. The amount of resources consumed is updated in the server, and a consolidated bill is generated. Every user will be provided a mobile application where the billing details are updated on a monthly basis with a portal for payment. This process is achieved by using the concept of embedded system and IoT.', 'corpus_id': 235824045, 'score': 0}, {'doc_id': '236940201', 'title': 'Power Quality Analyzers Calibration on Total Harmonics Distortion of Voltage and Current by Reference Square Waveform Signal', 'abstract': 'For calibrating the Power Quality Analyzer by Total Harmonic Distortion of voltage and current, a calibration approach is proposed, by using a reference square waveform signal. A mathematical model for Total Harmonic Distortion calibration is presented and an algorithm is implemented for control the calibration process, collection and processing of results. The experimental results obtained from the performed Total Harmonic Distortion calibrations, confirm the applicability of the proposed approach.', 'corpus_id': 236940201, 'score': 1}, {'doc_id': '236918930', 'title': 'Research on Harmonic Traceability Method of Reference Meter Based on Power Comparison', 'abstract': 'The harmonics generated by a large number of power electronic equipment in the power grid will seriously affect the measurement accuracy of electric energy measuring instruments. Therefore, in order to determine the anti-harmonic interference performance of electric energy meters, harmonic influence tests are required in the national standards of electric energy meters and the type evaluation outline. As the main standard device used in the harmonic impact test of the electric energy meter, the reference meter has passed the measurement verification, but its verification items do not include the detection of the harmonic measurement performance. In order to ensure the validity of the harmonic impact test of the electric energy meter, it is necessary to test the harmonic measurement performance of the reference meter. This paper designs a set of measurement and detection schemes that uses power comparison method to trace the source of harmonics of reference meters. It has important reference value for improving the accuracy of harmonic measurement and the reliability of electric energy meter type test.', 'corpus_id': 236918930, 'score': 1}, {'doc_id': '159436939', 'title': 'Energy meters evolution in smart grids: A review', 'abstract': 'Abstract Intelligent Energy Networks are comprised of devices capable of fulfilling their functions in an energy-efficient fashion and with communication and remote control capabilities. Therefore, some of these devices, such as smart energy meters, become attractive for use in the power generation and distribution industry, achieving the vision of Smart Grids. However, many are the challenges that need to be overcome in order to reach a fully-functional and security-aware smart grid. Providing measurement, control, communication, power, display, and synchronization capabilities shall be no easy task for smart meters. In this context, this paper elaborates on a detailed description of the main functionalities that smart meters must provide, along with the analysis of existing solutions that make use of smart meters for smart grids. Moreover, open challenges in the topic are identified and discussed. By the end of this research piece, the reader should be able to have a detailed view of the capabilities already offered by smart meters and the ones they will have available in order to tackle the challenges smart grids present.', 'corpus_id': 159436939, 'score': 1}, {'doc_id': '235755492', 'title': 'Cellular, Wide-Area, and Non-Terrestrial IoT: A Survey on 5G Advances and the Road Towards 6G', 'abstract': 'The next wave of wireless technologies is proliferating in connecting things among themselves as well as to humans. In the era of the Internet of things (IoT), billions of sensors, machines, vehicles, drones, and robots will be connected, making the world around us smarter. The IoT will encompass devices that must wirelessly communicate a diverse set of data gathered from the environment for myriad new applications. The ultimate goal is to extract insights from this data and develop solutions that improve quality of life and generate new revenue. Providing large-scale, long-lasting, reliable, and near real-time connectivity is the major challenge in enabling a smart connected world. This paper provides a comprehensive survey on existing and emerging communication solutions for serving IoT applications in the context of cellular, wide-area, as well as non-terrestrial networks. Specifically, wireless technology enhancements for providing IoT access in fifth-generation (5G) and beyond cellular networks, and communication networks over the unlicensed spectrum are presented. Aligned with the main key performance indicators of 5G and beyond 5G networks, we investigate solutions and standards that enable energy efficiency, reliability, low latency, and scalability (connection density) of current and future IoT networks. The solutions include grant-free access and channel coding for short-packet communications, nonorthogonal multiple access, and on-device intelligence. Further, a vision of new paradigm shifts in communication networks in the 2030s is provided, and the integration of the associated new technologies like artificial intelligence, non-terrestrial networks, and new spectra is elaborated. In particular, the potential of using emerging deep learning and federated learning techniques for enhancing the efficiency and security of IoT communication are discussed, and their promises and challenges are introduced. Finally, future research directions toward beyond 5G IoT networks are pointed out.', 'corpus_id': 235755492, 'score': 0}, {'doc_id': '236441483', 'title': 'Effect of K-Factor on Capability in Power Transformers', 'abstract': 'Harmonic currents generated by nonlinear loads can cause overheating and premature failure of power transformers. According to IEEE Std C57.110TM - 2008, Eddy Current losses are considered proportional to the harmonic current squared multiplied by the harmonic number. This paper will discuss 2 case studies, namely: a transformer without using a harmonic filter and a transformer using a harmonic filter. This research was conducted by measuring the amount of harmonics in arc furnace customers using power quality analysis equipment for 7 days. From the research results obtained indicate that the value of the k-factor is inversely proportional to the maximum transformer capability and is directly proportional to the decrease in transformer capability. So that the transformer that is installed using a harmonic filter has a k-factor value and derating capability is smaller than the transformer without using a harmonic filter, in order not to derating capability of the transformer without using a harmonic filter or a transformer that uses a harmonic filter, it is necessary to use the K-Factor Transformer K-4.', 'corpus_id': 236441483, 'score': 0}]"
141	{'doc_id': '39858893', 'title': '3D reconstruction of histological sections: Application to mammary gland tissue', 'abstract': 'In this article, we present a novel method for the automatic 3D reconstruction of thick tissue blocks from 2D histological sections. The algorithm completes a high‐content (multiscale, multifeature) imaging system for simultaneous morphological and molecular analysis of thick tissue samples. This computer‐based system integrates image acquisition, annotation, registration, and three‐dimensional reconstruction. We present an experimental validation of this tool using both synthetic and real data. In particular, we present the 3D reconstruction of an entire mouse mammary gland and demonstrate the integration of high‐resolution molecular data. Microsc. Res. Tech. 73:1019–1029, 2010. © 2010 Wiley‐Liss, Inc.', 'corpus_id': 39858893}	18060	"[{'doc_id': '235074286', 'title': 'Ex Vivo MR Histology and Cytometric Feature Mapping Connect Three-dimensional in Vivo MR Images to Two-dimensional Histopathologic Images of Murine Sarcomas.', 'abstract': 'Purpose To establish a platform for quantitative tissue-based interpretation of cytoarchitecture features from tumor MRI measurements. Materials and Methods In a pilot preclinical study, multicontrast in vivo MRI of murine soft-tissue sarcomas in 10 mice, followed by ex vivo MRI of fixed tissues (termed MR histology), was performed. Paraffin-embedded limb cross-sections were stained with hematoxylin-eosin, digitized, and registered with MRI. Registration was assessed by using binarized tumor maps and Dice similarity coefficients (DSCs). Quantitative cytometric feature maps from histologic slides were derived by using nuclear segmentation and compared with registered MRI, including apparent diffusion coefficients and transverse relaxation times as affected by magnetic field heterogeneity (T2* maps). Cytometric features were compared with each MR image individually by using simple linear regression analysis to identify the features of interest, and the goodness of fit was assessed on the basis of R2 values. Results Registration of MR images to histopathologic slide images resulted in mean DSCs of 0.912 for ex vivo MR histology and 0.881 for in vivo MRI. Triplicate repeats showed high registration repeatability (mean DSC, >0.9). Whole-slide nuclear segmentations were automated to detect nuclei on histopathologic slides (DSC = 0.8), and feature maps were generated for correlative analysis with MR images. Notable trends were observed between cell density and in vivo apparent diffusion coefficients (best line fit: R2 = 0.96, P < .001). Multiple cytoarchitectural features exhibited linear relationships with in vivo T2* maps, including nuclear circularity (best line fit: R2 = 0.99, P < .001) and variance in nuclear circularity (best line fit: R2 = 0.98, P < .001). Conclusion An infrastructure for registering and quantitatively comparing in vivo tumor MRI with traditional histologic analysis was successfully implemented in a preclinical pilot study of soft-tissue sarcomas. Keywords: MRI, Pathology, Animal Studies, Tissue Characterization Supplemental material is available for this article. ©\u2009RSNA, 2021.', 'corpus_id': 235074286, 'score': 1}, {'doc_id': '235438033', 'title': 'A convolutional neural network for common coordinate registration of high-resolution histology images.', 'abstract': 'MOTIVATION\nRegistration of histology images from multiple sources is a pressing problem in large-scale studies of spatial -omics data. Researchers often perform ""common coordinate registration,"" akin to segmentation, in which samples are partitioned based on tissue type to allow for quantitative comparison of similar regions across samples. Accuracy in such registration requires both high image resolution and global awareness, which mark a difficult balancing act for contemporary deep learning architectures.\n\n\nRESULTS\nWe present a novel convolutional neural network (CNN) architecture that combines (1) a local classification CNN that extracts features from image patches sampled sparsely across the tissue surface, and (2) a global segmentation CNN that operates on these extracted features. This hybrid network can be trained in an end-to-end manner, and we demonstrate its relative merits over competing approaches on a reference histology dataset as well as two published spatial transcriptomics datasets. We believe that this paradigm will greatly enhance our ability to process spatial -omics data, and has general purpose applications for the processing of high-resolution histology images on commercially available GPUs.\n\n\nAVAILABILITY\nAll code is publicly available at https://github.com/flatironinstitute/st_gridnet.\n\n\nSUPPLEMENTARY INFORMATION\nSupplementary data are available at Bioinformatics online.', 'corpus_id': 235438033, 'score': 1}, {'doc_id': '235244767', 'title': 'Whole-Body Soft-Tissue Lesion Tracking and Segmentation in Longitudinal CT Imaging Studies', 'abstract': 'In follow-up CT examinations of cancer patients, therapy success is evaluated by estimating the change in tumor size. This process is time-consuming and error-prone. We present a pipeline that automates the segmentation and measurement of matching lesions, given a point annotation in the baseline lesion. First, a region around the point annotation is extracted, in which a deep-learning-based segmentation of the lesion is performed. Afterward, a registration algorithm finds the corresponding image region in the follow-up scan and the convolutional neural network segments lesions inside this region. In the final step, the corresponding lesion is selected. We evaluate our pipeline on clinical follow-up data comprising 125 soft-tissue lesions from 43 patients with metastatic melanoma. Our pipeline succeeded for 96% of the baseline and 80% of the follow-up lesions, showing that we have laid the foundation for an efficient quantitative follow-up assessment in clinical routine.', 'corpus_id': 235244767, 'score': 0}, {'doc_id': '235243158', 'title': 'A deep learning algorithm for 3D cell detection in whole mouse brain image datasets', 'abstract': 'Understanding the function of the nervous system necessitates mapping the spatial distributions of its constituent cells defined by function, anatomy or gene expression. Recently, developments in tissue preparation and microscopy allow cellular populations to be imaged throughout the entire rodent brain. However, mapping these neurons manually is prone to bias and is often impractically time consuming. Here we present an open-source algorithm for fully automated 3D detection of neuronal somata in mouse whole-brain microscopy images using standard desktop computer hardware. We demonstrate the applicability and power of our approach by mapping the brain-wide locations of large populations of cells labeled with cytoplasmic fluorescent proteins expressed via retrograde trans-synaptic viral infection.', 'corpus_id': 235243158, 'score': 0}, {'doc_id': '235414098', 'title': 'Assessment of deep learning algorithms for 3D instance segmentation of confocal image datasets', 'abstract': 'Segmenting three dimensional microscopy images is essential for understanding phenomena like morphogenesis, cell division, cellular growth and genetic expression patterns. Recently, deep learning (DL) pipelines have been developed which claim to provide high accuracy segmentation of cellular images and are increasingly considered as the state-of-the-art for image segmentation problems. However, it remains difficult to define their relative performance as the concurrent diversity and lack of uniform evaluation strategies makes it difficult to know how their results compare. In this paper, we first made an inventory of the available DL methods for 3D segmentation. We next implemented and quantitatively compared a number of representative DL pipelines, alongside a highly efficient non-DL method named MARS. The DL methods were trained on a common dataset of 3D cellular confocal microscopy images. Their segmentation accuracies were also tested in the presence of different image artifacts. A new method for segmentation quality evaluation was adopted which isolates segmentation errors due to under/over segmentation. This is complemented with new visualization strategies that make interactive exploration of segmentation quality possible. Our analysis shows that the DL pipelines have very different levels of accuracy. Two of them show high performance, and offer clear advantages in terms of adaptability to new data.', 'corpus_id': 235414098, 'score': 0}, {'doc_id': '87383691', 'title': 'Spatial integration of radiology and pathology images to characterize breast cancer aggressiveness on pre-surgical MRI', 'abstract': 'The widespread use of screening mammography has resulted in a remarkable rise in the diagnosis of Ductal Carcinoma In Situ (DCIS). A resultant challenge is the early screening of these patients to identify those with concurrent invasive breast cancer (IBC), as one in five DCIS at biopsy, are upgraded to IBC following surgery. Both x-ray mammography and multi-parametric Magnetic Resonance Imaging (MRI) lack the ability to distinguish DCIS from IBC reliably. Our robust methodology for 3D alignment of histopathology images and MRI provides a unique opportunity to spatially map digitized histopathology slides on pre-surgical MRI which is particularly important in the tumors where DCIS and IBC co-occur as well as for the study of tumor heterogeneity. In this proof-of-concept study, we developed and evaluated a methodological framework for the 3D spatial alignment of MRI and histopathology slices, using x-ray radiographs as intermediate modality. Our methodology involves (1) the co-registration of 2D x-ray radiographs showing macrosections and corresponding 2D histology slices, (2) the 3D reconstruction of the ex vivo specimen based on the x-ray images, and aligned histology slices, and (3) the registration of the 3D reconstructed ex vivo specimen with the 3D MRI. The spatially co-registered MRI and histopathology images may enable the identification of MRI features that distinguish aggressive from indolent disease on in vivo MRI.', 'corpus_id': 87383691, 'score': 1}, {'doc_id': '235203547', 'title': 'Elastic transformation of histological slices allows precise co-registration with microCT data sets for a refined virtual histology approach', 'abstract': 'Although X-ray based 3D virtual histology is an emerging tool for the analysis of biological tissue, it falls short in terms of specificity when compared to conventional histology. Thus, the aim was to establish a novel approach that combines 3D information provided by microCT with high specificity that only (immuno-)histochemistry can offer. For this purpose, we developed a software frontend, which utilises an elastic transformation technique to accurately co-register various histological and immunohistochemical stainings with free propagation phase contrast synchrotron radiation microCT. We demonstrate that the precision of the overlay of both imaging modalities is significantly improved by performing our elastic registration workflow, as evidenced by calculation of the displacement index. To illustrate the need for an elastic co-registration approach we examined specimens from a mouse model of breast cancer with injected metal-based nanoparticles. Using the elastic transformation pipeline, we were able to co-localise the nanoparticles to specifically stained cells or tissue structures into their three-dimensional anatomical context. Additionally, we performed a semi-automated tissue structure and cell classification. This workflow provides new insights on histopathological analysis by combining CT specific three-dimensional information with cell/tissue specific information provided by classical histology.', 'corpus_id': 235203547, 'score': 1}, {'doc_id': '235072220', 'title': 'Protocol for image registration of correlative soft X-ray tomography and super-resolution structured illumination microscopy images', 'abstract': 'Summary Correlation of 3D images acquired on different microscopes can be a daunting prospect even for experienced users. This protocol describes steps for registration of images from soft X-ray absorption contrast imaging and super-resolution fluorescence imaging of hydrated biological materials at cryogenic temperatures. Although it is developed for data generated at synchrotron beamlines that offer the above combination of microscopies, it is applicable to all analogous imaging systems where the same area of a sample is examined using successive non-destructive imaging techniques. For complete details on the use and execution of this protocol, please refer to Kounatidis et al. (2020).', 'corpus_id': 235072220, 'score': 0}, {'doc_id': '233478405', 'title': 'Development of a 3D atlas of the embryonic pancreas for topological and quantitative analysis of heterologous cell interactions', 'abstract': 'Generating comprehensive image maps, while preserving spatial 3D context, is essential to quantitatively assess and locate specific cellular features and cell-cell interactions during organ development. Despite the recent advances in 3D imaging approaches, our current knowledge of the spatial organization of distinct cell types in the embryonic pancreatic tissue is still largely based on 2D histological sections. Here, we present a light-sheet fluorescence microscopy approach to image the pancreas in 3D and map tissue interactions at key development time points in the mouse embryo. We used transgenic mouse models and antibodies to visualize the three main cellular components within the developing pancreas, including epithelial, mesenchymal and endothelial cell populations. We demonstrated the utility of the approach by providing volumetric data, 3D distribution of distinct progenitor populations and quantification of relative cellular abundance within the tissue. Lastly, our image data were combined in an open source online repository (referred to as Pancreas Embryonic Cell Atlas). This image dataset will serve the scientific community by enabling further investigation on pancreas organogenesis but also for devising strategies for the in vitro generation of transplantable pancreatic tissue for regenerative therapies.', 'corpus_id': 233478405, 'score': 1}, {'doc_id': '235363019', 'title': 'High-throughput, label-free and slide-free histological imaging by computational microscopy and unsupervised learning', 'abstract': 'Rapid and high-resolution histological imaging with minimal tissue preparation has long been a challenging and yet captivating medical pursue. Here, we propose a promising and transformative histological imaging method, termed computational high-throughput autofluorescence microscopy by pattern illumination (CHAMP). With the assistance of computational microscopy, CHAMP enables high-throughput and label-free imaging of thick and unprocessed tissues with large surface irregularity at an acquisition speed of 10 mm2/10 seconds with 1.1-µm lateral resolution. Moreover, the CHAMP image can be transformed into a virtually stained histological image (Deep-CHAMP) through unsupervised learning within 15 seconds, where significant cellular features are quantitatively extracted with high accuracy. The versatility of CHAMP is experimentally demonstrated using mouse brain/kidney tissues prepared with various clinical protocols, which enables a rapid and accurate intraoperative/postoperative pathological examination without tissue processing or staining, demonstrating its great potential as an assistive imaging platform for surgeons and pathologists to provide optimal adjuvant treatment.', 'corpus_id': 235363019, 'score': 0}]"
142	{'doc_id': '210702638', 'title': 'Indoor Layout Estimation by 2D LiDAR and Camera Fusion', 'abstract': 'This paper presents an algorithm for indoor layout estimation and reconstruction through the fusion of a sequence of captured images and LiDAR data sets. In the proposed system, a movable platform collects both intensity images and 2D LiDAR information. Pose estimation and semantic segmentation is computed jointly by aligning the LiDAR points to line segments from the images. For indoor scenes with walls orthogonal to floor, the alignment problem is decoupled into top-down view projection and a 2D similarity transformation estimation and solved by the recursive random sample consensus (R-RANSAC) algorithm. Hypotheses can be generated, evaluated and optimized by integrating new scans as the platform moves throughout the environment. The proposed method avoids the need of extensive prior training or a cuboid layout assumption, which is more effective and practical compared to most previous indoor layout estimation methods. Multi-sensor fusion allows the capability of providing accurate depth estimation and high resolution visual information.', 'corpus_id': 210702638}	2533	"[{'doc_id': '131777068', 'title': 'Sensor Fusion for Joint 3D Object Detection and Semantic Segmentation', 'abstract': 'In this paper, we present an extension to LaserNet, an efficient and state-of-the-art LiDAR based 3D object detector. We propose a method for fusing image data with the LiDAR data and show that this sensor fusion method improves the detection performance of the model especially at long ranges. The addition of image data is straightforward and does not require image labels. Furthermore, we expand the capabilities of the model to perform 3D semantic segmentation in addition to 3D object detection. On a large benchmark dataset, we demonstrate our approach achieves state-of-the-art performance on both object detection and semantic segmentation while maintaining a low runtime.', 'corpus_id': 131777068, 'score': 1}, {'doc_id': '156051565', 'title': 'Multi-Task Multi-Sensor Fusion for 3D Object Detection', 'abstract': ""In this paper we propose to exploit multiple related tasks for accurate multi-sensor 3D object detection. Towards this goal we present an end-to-end learnable architecture that reasons about 2D and 3D object detection as well as ground estimation and depth completion. Our experiments show that all these tasks are complementary and help the network learn better representations by fusing information at various levels. Importantly, our approach leads the KITTI benchmark on 2D, 3D and bird's eye view object detection, while being real-time."", 'corpus_id': 156051565, 'score': 1}, {'doc_id': '210943036', 'title': 'ImVoteNet: Boosting 3D Object Detection in Point Clouds With Image Votes', 'abstract': '3D object detection has seen quick progress thanks to advances in deep learning on point clouds. A few recent works have even shown state-of-the-art performance with just point clouds input (e.g. VoteNet). However, point cloud data have inherent limitations. They are sparse, lack color information and often suffer from sensor noise. Images, on the other hand, have high resolution and rich texture. Thus they can complement the 3D geometry provided by point clouds. Yet how to effectively use image information to assist point cloud based detection is still an open question. In this work, we build on top of VoteNet and propose a 3D detection architecture called ImVoteNet specialized for RGB-D scenes. ImVoteNet is based on fusing 2D votes in images and 3D votes in point clouds. Compared to prior work on multi-modal detection, we explicitly extract both geometric and semantic features from the 2D images. We leverage camera parameters to lift these features to 3D. To improve the synergy of 2D-3D feature fusion, we also propose a multi-tower training scheme. We validate our model on the challenging SUN RGB-D dataset, advancing state-of-the-art results by 5.7 mAP. We also provide rich ablation studies to analyze the contribution of each design choice.', 'corpus_id': 210943036, 'score': 1}, {'doc_id': '211677466', 'title': 'D3VO: Deep Depth, Deep Pose and Deep Uncertainty for Monocular Visual Odometry', 'abstract': 'We propose D3VO as a novel framework for monocular visual odometry that exploits deep networks on three levels -- deep depth, pose and uncertainty estimation. We first propose a novel self-supervised monocular depth estimation network trained on stereo videos without any external supervision. In particular, it aligns the training image pairs into similar lighting condition with predictive brightness transformation parameters. Besides, we model the photometric uncertainties of pixels on the input images, which improves the depth estimation accuracy and provides a learned weighting function for the photometric residuals in direct (feature-less) visual odometry. Evaluation results show that the proposed network outperforms state-of-the-art self-supervised depth estimation networks. D3VO tightly incorporates the predicted depth, pose and uncertainty into a direct visual odometry method to boost both the front-end tracking as well as the back-end non-linear optimization. We evaluate D3VO in terms of monocular visual odometry on both the KITTI odometry benchmark and the EuRoC MAV dataset. The results show that D3VO outperforms state-of-the-art traditional monocular VO methods by a large margin. It also achieves comparable results to state-of-the-art stereo/LiDAR odometry on KITTI and to the state-of-the-art visual-inertial odometry on EuRoC MAV, while using only a single camera.', 'corpus_id': 211677466, 'score': 0}, {'doc_id': '52211898', 'title': 'Deep Continuous Fusion for Multi-sensor 3D Object Detection', 'abstract': 'In this paper, we propose a novel 3D object detector that can exploit both LIDAR as well as cameras to perform very accurate localization. Towards this goal, we design an end-to-end learnable architecture that exploits continuous convolutions to fuse image and LIDAR feature maps at different levels of resolution. Our proposed continuous fusion layer encode both discrete-state image features as well as continuous geometric information. This enables us to design a novel, reliable and efficient end-to-end learnable 3D object detector based on multiple sensors. Our experimental evaluation on both KITTI as well as a large scale 3D object detection benchmark shows significant improvements over the state of the art.', 'corpus_id': 52211898, 'score': 1}, {'doc_id': '210714109', 'title': 'Spatiotemporal Camera-LiDAR Calibration: A Targetless and Structureless Approach', 'abstract': 'The demand for multimodal sensing systems for robotics is growing due to the increase in robustness, reliability and accuracy offered by these systems. These systems also need to be spatially and temporally co-registered to be effective. In this letter, we propose a targetless and structureless spatiotemporal camera-LiDAR calibration method. Our method combines a closed-form solution with a modified structureless bundle adjustment where the coarse-to-fine approach does not require an initial guess on the spatiotemporal parameters. Also, as 3D features (structure) are calculated from triangulation only, there is no need to have a calibration target or to match 2D features with the 3D point cloud which provides flexibility in the calibration process and sensor configuration. We demonstrate the accuracy and robustness of the proposed method through both simulation and real data experiments using multiple sensor payload configurations mounted to hand-held, aerial and legged robot systems. Also, qualitative results are given in the form of a colorized point cloud visualization.', 'corpus_id': 210714109, 'score': 0}, {'doc_id': '208118518', 'title': 'RGB-D Odometry and SLAM', 'abstract': 'The emergence of modern RGB-D sensors had a significant impact in many application fields, including robotics, augmented reality (AR), and 3D scanning. They are low-cost, low-power, and low-size alternatives to traditional range sensors such as LiDAR. Moreover, unlike RGB cameras, RGB-D sensors provide the additional depth information that removes the need of frame-by-frame triangulation for 3D scene reconstruction. These merits have made them very popular in mobile robotics and AR, where it is of great interest to estimate ego-motion and 3D scene structure. Such spatial understanding can enable robots to navigate autonomously without collisions and allow users to insert virtual entities consistent with the image stream. In this chapter, we review common formulations of odometry and Simultaneous Localization and Mapping (known by its acronym SLAM) using RGB-D stream input. The two topics are closely related, as the former aims to track the incremental camera motion with respect to a local map of the scene, and the latter to jointly estimate the camera trajectory and the global map with consistency. In both cases, the standard approaches minimize a cost function using nonlinear optimization techniques. This chapter consists of three main parts: In the first part, we introduce the basic concept of odometry and SLAM and motivate the use of RGB-D sensors. We also give mathematical preliminaries relevant to most odometry and SLAM algorithms. In the second part, we detail the three main components of SLAM systems: camera pose tracking, scene mapping, and loop closing. For each component, we describe different approaches proposed in the literature. In the final part, we provide a brief discussion on advanced research topics with the references to the state of the art.', 'corpus_id': 208118518, 'score': 0}, {'doc_id': '211677612', 'title': '3D Point Cloud Processing and Learning for Autonomous Driving', 'abstract': 'We present a review of 3D point cloud processing and learning for autonomous driving. As one of the most important sensors in autonomous vehicles, light detection and ranging (LiDAR) sensors collect 3D point clouds that precisely record the external surfaces of objects and scenes. The tools for 3D point cloud processing and learning are critical to the map creation, localization, and perception modules in an autonomous vehicle. While much attention has been paid to data collected from cameras, such as images and videos, an increasing number of researchers have recognized the importance and significance of LiDAR in autonomous driving and have proposed processing and learning algorithms to exploit 3D point clouds. We review the recent progress in this research area and summarize what has been tried and what is needed for practical and safe autonomous vehicles. We also offer perspectives on open issues that are needed to be solved in the future.', 'corpus_id': 211677612, 'score': 0}, {'doc_id': '212675370', 'title': 'Confidence Guided Stereo 3D Object Detection with Split Depth Estimation', 'abstract': 'Accurate and reliable 3D object detection is vital to safe autonomous driving. Despite recent developments, the performance gap between stereo-based methods and LiDAR-based methods is still considerable. Accurate depth estimation is crucial to the performance of stereo-based 3D object detection methods, particularly for those pixels associated with objects in the foreground. Moreover, stereo-based methods suffer from high variance in the depth estimation accuracy, which is often not considered in the object detection pipeline. To tackle these two issues, we propose CG-Stereo, a confidence-guided stereo 3D object detection pipeline that uses separate decoders for foreground and background pixels during depth estimation, and leverages the confidence estimation from the depth estimation network as a soft attention mechanism in the 3D object detector. Our approach outperforms all state-of-the-art stereo-based 3D detectors on the KITTI benchmark.', 'corpus_id': 212675370, 'score': 0}, {'doc_id': '67877039', 'title': 'Selective Sensor Fusion for Neural Visual-Inertial Odometry', 'abstract': 'Deep learning approaches for Visual-Inertial Odometry (VIO) have proven successful, but they rarely focus on incorporating robust fusion strategies for dealing with imperfect input sensory data. We propose a novel end-to-end selective sensor fusion framework for monocular VIO, which fuses monocular images and inertial measurements in order to estimate the trajectory whilst improving robustness to real-life issues, such as missing and corrupted data or bad sensor synchronization. In particular, we propose two fusion modalities based on different masking strategies: deterministic soft fusion and stochastic hard fusion, and we compare with previously proposed direct fusion baselines. During testing, the network is able to selectively process the features of the available sensor modalities and produce a trajectory at scale. We present a thorough investigation on the performances on three public autonomous driving, Micro Aerial Vehicle (MAV) and hand-held VIO datasets. The results demonstrate the effectiveness of the fusion strategies, which offer better performances compared to direct fusion, particularly in presence of corrupted data. In addition, we study the interpretability of the fusion networks by visualising the masking layers in different scenarios and with varying data corruption, revealing interesting correlations between the fusion networks and imperfect sensory input data.', 'corpus_id': 67877039, 'score': 1}]"
143	"{'doc_id': '212114261', 'title': 'What is ""intelligent"" in intelligent user interfaces?: a meta-analysis of 25 years of IUI', 'abstract': 'This reflection paper takes the 25th IUI conference milestone as an opportunity to analyse in detail the understanding of intelligence in the community: Despite the focus on intelligent UIs, it has remained elusive what exactly renders an interactive system or user interface ""intelligent"", also in the fields of HCI and AI at large. We follow a bottom-up approach to analyse the emergent meaning of intelligence in the IUI community: In particular, we apply text analysis to extract all occurrences of ""intelligent"" in all IUI proceedings. We manually review these with regard to three main questions: 1) What is deemed intelligent? 2) How (else) is it characterised? and 3) What capabilities are attributed to an intelligent entity? We discuss the community\'s emerging implicit perspective on characteristics of intelligence in intelligent user interfaces and conclude with ideas for stating one\'s own understanding of intelligence more explicitly.', 'corpus_id': 212114261}"	1148	"[{'doc_id': '220713394', 'title': 'Learning User-Preferred Mappings for Intuitive Robot Control', 'abstract': 'When humans control drones, cars, and robots, we often have some preconceived notion of how our inputs should make the system behave. Existing approaches to teleoperation typically assume a one-size-fits-all approach, where the designers pre-define a mapping between human inputs and robot actions, and every user must adapt to this mapping over repeated interactions. Instead, we propose a personalized method for learning the human’s preferred or preconceived mapping from a few robot queries. Given a robot controller, we identify an alignment model that transforms the human’s inputs so that the controller’s output matches their expectations. We make this approach data-efficient by recognizing that human mappings have strong priors: we expect the input space to be proportional, reversable, and consistent. Incorporating these priors ensures that the robot learns an intuitive mapping from few examples. We test our learning approach in robot manipulation tasks inspired by assistive settings, where each user has different personal preferences and physical capabilities for teleoperating the robot arm. Our simulated and experimental results suggest that learning the mapping between inputs and robot actions improves objective and subjective performance when compared to manually defined alignments or learned alignments without intuitive priors. The supplementary video showing these user studies can be found at: https://youtu.be/rKHka0_48-Q', 'corpus_id': 220713394, 'score': 0}, {'doc_id': '8943607', 'title': 'Principles of mixed-initiative user interfaces', 'abstract': 'Recent debate has centered on the relative promise of focusinguser-interface research on developing new metaphors and tools thatenhance users abilities to directly manipulate objects versusdirecting effort toward developing interface agents that provideautomation. In this paper, we review principles that show promisefor allowing engineers to enhance human-computer interactionthrough an elegant coupling of automated services with directmanipulation. Key ideas will be highlighted in terms of the Lookoutsystem for scheduling and meeting management.', 'corpus_id': 8943607, 'score': 1}, {'doc_id': '220364234', 'title': 'Tree-Augmented Cross-Modal Encoding for Complex-Query Video Retrieval', 'abstract': 'The rapid growth of user-generated videos on the Internet has intensified the need for text-based video retrieval systems. Traditional methods mainly favor the concept-based paradigm on retrieval with simple queries, which are usually ineffective for complex queries that carry far more complex semantics. Recently, embedding-based paradigm has emerged as a popular approach. It aims to map the queries and videos into a shared embedding space where semantically-similar texts and videos are much closer to each other. Despite its simplicity, it forgoes the exploitation of the syntactic structure of text queries, making it suboptimal to model the complex queries. To facilitate video retrieval with complex queries, we propose a Tree-augmented Cross-modal Encoding method by jointly learning the linguistic structure of queries and the temporal representation of videos. Specifically, given a complex user query, we first recursively compose a latent semantic tree to structurally describe the text query. We then design a tree-augmented query encoder to derive structure-aware query representation and a temporal attentive video encoder to model the temporal characteristics of videos. Finally, both the query and videos are mapped into a joint embedding space for matching and ranking. In this approach, we have a better understanding and modeling of the complex queries, thereby achieving a better video retrieval performance. Extensive experiments on large scale video retrieval benchmark datasets demonstrate the effectiveness of our approach.', 'corpus_id': 220364234, 'score': 0}, {'doc_id': '52175668', 'title': 'Predict Responsibly: Improving Fairness and Accuracy by Learning to Defer', 'abstract': 'In many machine learning applications, there are multiple decision-makers involved, both automated and human. The interaction between these agents often goes unaddressed in algorithmic development. In this work, we explore a simple version of this interaction with a two-stage framework containing an automated model and an external decision-maker. The model can choose to say PASS, and pass the decision downstream, as explored in rejection learning. We extend this concept by proposing ""learning to defer"", which generalizes rejection learning by considering the effect of other agents in the decision-making process. We propose a learning algorithm which accounts for potential biases held by external decision-makers in a system. Experiments demonstrate that learning to defer can make systems not only more accurate but also less biased. Even when working with inconsistent or biased users, we show that deferring models still greatly improve the accuracy and/or fairness of the entire system.', 'corpus_id': 52175668, 'score': 1}, {'doc_id': '220425376', 'title': 'When Humans and Machines Make Joint Decisions: A Non-Symmetric Bandit Model', 'abstract': 'How can humans and machines learn to make joint decisions? This has become an important question in domains such as medicine, law and finance. We approach the question from a theoretical perspective and formalize our intuitions about human-machine decision making in a non-symmetric bandit model. In doing so, we follow the example of a doctor who is assisted by a computer program. We show that in our model, exploration is generally hard. In particular, unless one is willing to make assumptions about how human and machine interact, the machine cannot explore efficiently. We highlight one such assumption, policy space independence, which resolves the coordination problem and allows both players to explore independently. Our results shed light on the fundamental difficulties faced by the interaction of humans and machines. We also discuss practical implications for the design of algorithmic decision systems.', 'corpus_id': 220425376, 'score': 1}, {'doc_id': '220380881', 'title': 'Modeling and Mitigating Human Annotation Errors to Design Efficient Stream Processing Systems with Human-in-the-loop Machine Learning', 'abstract': 'High-quality human annotations are necessary for creating effective machine learning-driven stream processing systems. We study hybrid stream processing systems based on a Human-In-The-Loop Machine Learning (HITL-ML) paradigm, in which one or many human annotators and an automatic classifier (trained at least partially by the human annotators) label an incoming stream of instances. This is typical of many near-real time social media analytics and web applications, including the annotation of social media posts during emergencies by digital volunteer groups. From a practical perspective, low-quality human annotations result in wrong labels for retraining automated classifiers and indirectly contribute to the creation of inaccurate classifiers. \nConsidering human annotation as a psychological process allows us to address these limitations. We show that human annotation quality is dependent on the ordering of instances shown to annotators, and can be improved by local changes in the instance sequence/ordering provided to the annotators, yielding a more accurate annotation of the stream. We design a theoretically-motivated human error framework for the human annotation task to study the effect of ordering instances (i.e., an ""annotation schedule""). Further, we propose an error-avoidance approach to the active learning (HITL-ML) paradigm for stream processing applications that is robust to these likely human errors when deciding a human annotation schedule. We validate the human error framework using crowdsourcing experiments and evaluate the proposed algorithm against standard baselines for active learning via extensive experimentation on classification tasks of filtering relevant social media posts during natural disasters.', 'corpus_id': 220380881, 'score': 1}, {'doc_id': '220381441', 'title': 'PinnerSage: Multi-Modal User Embedding Framework for Recommendations at Pinterest', 'abstract': ""Latent user representations are widely adopted in the tech industry for powering personalized recommender systems. Most prior work infers a single high dimensional embedding to represent a user, which is a good starting point but falls short in delivering a full understanding of the user's interests. In this work, we introduce PinnerSage, an end-to-end recommender system that represents each user via multi-modal embeddings and leverages this rich representation of users to provides high quality personalized recommendations. PinnerSage achieves this by clustering users' actions into conceptually coherent clusters with the help of a hierarchical clustering method (Ward) and summarizes the clusters via representative pins (Medoids) for efficiency and interpretability. PinnerSage is deployed in production at Pinterest and we outline the several design decisions that makes it run seamlessly at a very large scale. We conduct several offline and online A/B experiments to show that our method significantly outperforms single embedding methods."", 'corpus_id': 220381441, 'score': 0}, {'doc_id': '220381402', 'title': 'From API to NLI: A New Interface for Library Reuse', 'abstract': 'Abstract Developers frequently reuse APIs from existing libraries to implement certain functionality. However, learning APIs is difficult due to their large scale and complexity. In this paper, we design an abstract framework NLI2Code to ease the reuse process. Under the framework, users can reuse library functionalities with a high-level, automatically-generated NLI (Natural Language Interface) instead of the detailed API elements. The framework consists of three components: a functional feature extractor to summarize the frequently-used library functions in natural language form, a code pattern miner to give a code template for each functional feature, and a synthesizer to complete code patterns into well-typed snippets. From the perspective of a user, a reuse task under NLI2Code starts from choosing a functional feature and our framework will guide the user to synthesize the desired solution. We instantiated the framework as a tool to reuse Java libraries. The evaluation shows our tool can generate a high-quality natural language interface and save half of the coding time for newcomers to solve real-world programming tasks.', 'corpus_id': 220381402, 'score': 0}, {'doc_id': '97965779', 'title': 'Plasma spraying of micro-composite thermal barrier coatings', 'abstract': 'The thermal barrier coatings (TBCs) by gas tunnel-type plasma spraying exhibited ceramic-composite features consisting of a host oxide matrix ceramic with an embedded second phase material.The densities of the composite TBC were found to be higher than those sprayed with 100 wt% ZrO2 or Al2O3.In the coatings produced with powder mixtures of 50 wt%, the embedded splats are found to have a relatively uniform thickness between 1 and 10mm and they exhibited clear and pore-free interfaces with the host material.The micro-composite coatings also exhibited thicknessdependent functionally gradient Vickers hardness values by the hardness measurements across the coating thickness.A one-dimensional series heat transfer model was developed to estimate upper and lower bounds of the transverse thermal resistance as a function of alumina–zirconia weight ratio.The model shows that the addition of higher thermally conducting Al2O3 can result in an increase in the transverse thermal resistivity of YSZ. r 2002 Elsevier Science Ltd. All rights reserved.', 'corpus_id': 97965779, 'score': 0}, {'doc_id': '86866942', 'title': 'Guidelines for Human-AI Interaction', 'abstract': 'Advances in artificial intelligence (AI) frame opportunities and challenges for user interface design. Principles for human-AI interaction have been discussed in the human-computer interaction community for over two decades, but more study and innovation are needed in light of advances in AI and the growing uses of AI technologies in human-facing applications. We propose 18 generally applicable design guidelines for human-AI interaction. These guidelines are validated through multiple rounds of evaluation including a user study with 49 design practitioners who tested the guidelines against 20 popular AI-infused products. The results verify the relevance of the guidelines over a spectrum of interaction scenarios and reveal gaps in our knowledge, highlighting opportunities for further research. Based on the evaluations, we believe the set of design guidelines can serve as a resource to practitioners working on the design of applications and features that harness AI technologies, and to researchers interested in the further development of human-AI interaction design principles.', 'corpus_id': 86866942, 'score': 1}]"
144	"{'doc_id': '211521586', 'title': 'Cross-Disciplinary Faculty Development in Data Science Principles for Classroom Integration', 'abstract': ""Data science in practice leverages the expertise in computer science, mathematics and statistics with applications in any field using data. The formalization of data science educational and pedagogical strategic remain in their infancy. College faculty from various disciplines are tasked with designing and delivering data science instruction without the formal knowledge of how data science principles are executed in practice. We call this the data science instruction gap. Also, these faculties are implementing their discipline's standard pedagogical strategies to their understanding of data science. In this paper, we present our cross-disciplinary instructional program model designed to narrow the data science instruction gap for faculty. It is designed to scaffold college faculties' data science learning to support their discipline-specific data science instruction. We provide individualized and group-based support structures to instill data science principles and transition them from learners to educators in data science. Lastly, we share our model's impact on and value to faculty as well as make recommendations for model adoption."", 'corpus_id': 211521586}"	20852	"[{'doc_id': '237532112', 'title': 'Student-centric graduate training in mathematics: A commentary', 'abstract': 'Career opportunities for PhDs in the mathematical sciences have never been better. Traditional faculty positions in mathematics departments in colleges and universities range from all teaching to combined teaching/research responsibilities. Beyond those, a wide array of careers has now opened up to freshly minted graduates, in academics, industry, business, and government. It is well-understood that these all require somewhat different preparations for Ph.D.s to be competitive. This commentary compares and contrasts mathematics graduate programs with Ph.D. programs in the life and biomedical sciences, which are structured in a way that allows considerable customization around students’ career goals. While these programs may not be appropriate templates for the mathematical sciences, they have some features that might be informative. This commentary is intended to add perspective to the ongoing discussion around PhD training in the mathematical sciences. It also provides some concrete proposals for changes.', 'corpus_id': 237532112, 'score': 0}, {'doc_id': '237518352', 'title': 'Equipping and Empowering Faculty through Professional Development to Create a Future-Ready Workforce in Emerging Technologies', 'abstract': 'Tech industry, especially, some areas within tech fields, such as Emerging Technology (EmTech), like cybersecurity, data science, mobile development, machine learning, AI, and cloud computing, are expected to experience immense increases in job opportunities in coming years. While a variety of solutions are necessary to address the growing workforce needs in the EmTech industry, one of the largest untapped talent pools is women and underrepresented students. Clearly, HBCU and MSI hold great potential to broaden participation in EmTech because of their more diverse student populations, access to a large number of underrepresented students, and closer faculty-to-student interaction. However, faculties at these institutions, who are at the forefront of developing required skills in students are often overlooked. Faculties at these institutions need help designing and implementing effective and evidence-based instruction materials to develop skills that are in high-demand in the EmTech industry. The goal of this panel is to offer a platform that can provide insight into the development of faculty professional development programs in EmTech in traditional institutions and within the context of HBCU and MSI.', 'corpus_id': 237518352, 'score': 0}, {'doc_id': '237376556', 'title': 'Peer teaching as bioinformatics training strategy: incentives, challenges, and benefits', 'abstract': 'As biomedical research becomes more data-intensive, bioinformatics is becoming essential to understanding biological processes, systems, and diseases. In this paper we describe the use of a series of peer teaching workshops as a strategy to respond to the bioinformatics training needs at a research-intensive institution. In addition to the data collected from the workshops, we also used personal experiences of researchers who participated as peer teachers to understand the incentives, challenges, and benefits of peer teaching. Developing communication skills such as confidence in teaching, explaining complex concepts, and better understanding of the topic emerged as primary benefits that the teachers obtained from this experience. Lack of time for teaching and the struggles of classroom management were identified as two major challenges. We suggest that peer teaching can be beneficial not only to train researchers in bioinformatics, but also as a professional development opportunity for graduate students and postdoctoral trainees.', 'corpus_id': 237376556, 'score': 0}, {'doc_id': '237329695', 'title': 'The future of data(base) education: Is the ""cow book"" dead?', 'abstract': 'This panel encourages a debate over the future of database education and its relationship to Data Science: Are Computer Science (CS) and Data Science (DS) different disciplines about to split, and how does that effect how we teach our field? Is there a ""data"" course that belongs in CS that all of our students should take? Who is the traditional database course, e.g. based on the ""cow book"", relevant to? What traditional topics should we not be teaching in our core data course(s) and which ones should be added? What do we teach the student who has one elective for data science? How does our community position itself for leadership in CS given the popularity of DS? PVLDB Reference Format: Zachary G. Ives, Rachel Pottinger, Arun Kumar, Johannes Gehrke, and Jana Giceva. The future of data(base) education: Is the ""cow book"" dead?. PVLDB, 14(12): 3239 3240, 2021. doi:10.14778/3476311.3476394', 'corpus_id': 237329695, 'score': 0}, {'doc_id': '2531277', 'title': 'A Guide to Teaching Data Science', 'abstract': 'ABSTRACT Demand for data science education is surging and traditional courses offered by statistics departments are not meeting the needs of those seeking training. This has led to a number of opinion pieces advocating for an update to the Statistics curriculum. The unifying recommendation is that computing should play a more prominent role. We strongly agree with this recommendation, but advocate the main priority is to bring applications to the forefront as proposed by Nolan and Speed in 1999. We also argue that the individuals tasked with developing data science courses should not only have statistical training, but also have experience analyzing data with the main objective of solving real-world problems. Here, we share a set of general principles and offer a detailed guide derived from our successful experience developing and teaching a graduate-level, introductory data science course centered entirely on case studies. We argue for the importance of statistical thinking, as defined by Wild and Pfannkuch in 1999 and describe how our approach teaches students three key skills needed to succeed in data science, which we refer to as creating, connecting, and computing. This guide can also be used for statisticians wanting to gain more practical knowledge about data science before embarking on teaching an introductory course. Supplementary materials for this article are available online.', 'corpus_id': 2531277, 'score': 1}, {'doc_id': '88520302', 'title': 'Data Science in Statistics Curricula: Preparing Students to “Think with Data”', 'abstract': 'A growing number of students are completing undergraduate degrees in statistics and entering the workforce as data analysts. In these positions, they are expected to understand how to use databases and other data warehouses, scrape data from Internet sources, program solutions to complex problems in multiple languages, and think algorithmically as well as statistically. These data science topics have not traditionally been a major component of undergraduate programs in statistics. Consequently, a curricular shift is needed to address additional learning outcomes. The goal of this article is to motivate the importance of data science proficiency and to provide examples and resources for instructors to implement data science in their own statistics curricula. We provide case studies from seven institutions. These varied approaches to teaching data science demonstrate curricular innovations to address new needs. Also included here are examples of assignments designed for courses that foster engagement of undergraduates with data and data science. [Received November 2014. Revised July 2015.]', 'corpus_id': 88520302, 'score': 1}, {'doc_id': '6311574', 'title': 'A Data Science Course for Undergraduates: Thinking With Data', 'abstract': 'Data science is an emerging interdisciplinary field that combines elements of mathematics, statistics, computer science, and knowledge in a particular application domain for the purpose of extracting meaningful information from the increasingly sophisticated array of data available in many settings. These data tend to be nontraditional, in the sense that they are often live, large, complex, and/or messy. A first course in statistics at the undergraduate level typically introduces students to a variety of techniques to analyze small, neat, and clean datasets. However, whether they pursue more formal training in statistics or not, many of these students will end up working with data that are considerably more complex, and will need facility with statistical computing techniques. More importantly, these students require a framework for thinking structurally about data. We describe an undergraduate course in a liberal arts environment that provides students with the tools necessary to apply data science. The course emphasizes modern, practical, and useful skills that cover the full data analysis spectrum, from asking an interesting question to acquiring, managing, manipulating, processing, querying, analyzing, and visualizing data, as well communicating findings in written, graphical, and oral forms. Supplementary materials for this article are available online. [Received June 2014. Revised July 2015.]', 'corpus_id': 6311574, 'score': 1}, {'doc_id': '60626916', 'title': 'Data Carpentry: Workshops to Increase Data Literacy for Researchers', 'abstract': 'In many domains the rapid generation of large amounts of data is fundamentally changing how research is done. The deluge of data presents great opportunities, but also many challenges in managing, analyzing and sharing data. However, good training resources for researchers looking to develop skills that will enable them to be more effective and productive researchers are scarce and there is little space in the existing curriculum for courses or additional lectures. To address this need we have developed an introductory two-day intensive workshop, Data Carpentry, designed to teach basic concepts, skills, and tools for working more effectively and reproducibly with data. These workshops are based on Software Carpentry: two-day, hands-on, bootcamp style workshops teaching best practices in software development, that have demonstrated the success of short workshops to teach foundational research skills. Data Carpentry focuses on data literacy in particular, with the objective of teaching skills to researchers to enable them to retrieve, view, manipulate, analyze and store their and other’s data in an open and reproducible way in order to extract knowledge from data.', 'corpus_id': 60626916, 'score': 1}, {'doc_id': '212702488', 'title': 'Targeted Curricular Innovations in Data Science', 'abstract': 'Many employers expect skills such as proper data generation, collection, storage, and analysis; however, these skills are often not taught in the undergraduate experience. Many STEM disciplines require computing coursework that includes coding in a modern programming language but does not explicitly address data stewardship. In this research, we present a faculty-focused data science program to address this educational gap. The faculty at two undergraduate institutions participate in a year-long slate of activities to learn about data science and design mechanisms for classroom dissemination. We describe the program details and provide sample classroom implementations as well as a summary of faculty post-participation perceptions.', 'corpus_id': 212702488, 'score': 1}, {'doc_id': '237273810', 'title': 'Faculty Development Aimed at Sustaining and Enhancing Entrepreneurial-minded Learning', 'abstract': 'Many higher education institutions have begun promoting an entrepreneurial mindset (EM) in students and integrating entrepreneurship elements in engineering education. Various approaches, including curricular, extra-curricular and co-curricular initiatives, are being used to transform the education offered at these institutions. However, in order for this transformation to be sustained and broadened, efforts must target faculty as well as students. Helping faculty to embrace entrepreneurial minded learning (EML) and equipping them with relevant tools and resources will ensure true transformation and long-term success. At the University of New Haven, our efforts started with implementing an innovative curricular model designed to develop an entrepreneurial mindset in students and establishing initiatives to provide students other forms of engagement opportunities. The curricular model involved the development and integration of e-learning modules — targeting various entrepreneurial concepts and skills — into courses spanning all four years of all engineering and computer science programs. Innovation and pitch competitions, participation in the University Innovation Fellows program, and an entrepreneurial engineering living learning community were primary extra-curricular components. Faculty development opportunities were provided as part of these initiatives including training for effectively integrating the e-learning modules into courses, participation in workshops and conferences with a focus on entrepreneurial education, and involvement in organizing and facilitating student activities. While a significant number of our engineering and computer science faculty participated in these development opportunities, in general their enthusiasm related to entrepreneurial minded learning (EML) was not strong enough to sustain and further broaden EML within the college. Therefore, we implemented a faculty development program aimed at fostering EM champions from different engineering and computer science disciplines, as well as a mini-grant program to stimulate faculty to independently integrate EML into their courses. In this paper, we present these efforts, describe the program components, and report on findings. Sample products resulting from the faculty development efforts are also provided.', 'corpus_id': 237273810, 'score': 0}]"
145	{'doc_id': '225511666', 'title': 'Fundamental Rights, Accountability and Transparency in European Governance of Migration: The Case of the European Border and Coast Guard Agency Frontex', 'abstract': 'This report analyses and interrogates\xa0the accountability and transparency regime of the European Union’s border agency Frontex.\xa0Frontex was established as a European Union agency in the field of bo ...', 'corpus_id': 225511666}	13927	"[{'doc_id': '231639821', 'title': 'The European Commission’s entrepreneurship and the social dimension of the European Semester: from the European Pillar of Social Rights to the Covid-19 pandemic', 'abstract': 'This article reconstructs the role of European Commission President’s political entrepreneurship in the further socialisation of the European Semester. Firstly, it analyses the context in which Juncker promoted the European Pillar of Social Rights. Secondly, it presents how such an initiative was mainstreamed into the Semester. Finally, by distinguishing among social retrenchment, social investment and social protection prescriptions, it analyses whether the post-2015 development of the Semester was consistent with Juncker’s political effort. We find clear evidence of entrepreneurial activities in the strategic recourse to Presidency’s Cabinet and the Secretary General and in the introduction of a new Social Scoreboard. Furthermore, we show that such socialisation process is congruent to the expected impact of European Pillar of Social Rights, which indeed contributes to strengthen an already ongoing incremental trend. Our analysis contributes to better qualify the socialisation process by highlighting its characteristics and the role of politics. These findings are particularly important to understand whether the Social Pillar left a legacy to the current Commission’s social agenda and to the EU responses to the Covid-19 pandemic, and whether these changes open a new window of opportunity to further reform the Semester.', 'corpus_id': 231639821, 'score': 0}, {'doc_id': '227249661', 'title': 'Crisis Without Borders: What Does International Law Say About Border Closure in the Context of Covid-19?', 'abstract': ""This paper is assessing the legality of border closures decided by a vast number of countries with the view of limiting the spread of Covid-19. Although this issue has raised diverging interpretations in relation to International Health Regulations and regional free movement agreements, international human rights law provides a clear-cut answer: the rule of law stops neither at the border nor in times of emergency. Against this normative framework, border control can and must be carried out with the twofold purpose of protecting public health and individual rights, whereas border closure is unable to do so because it is by essence a collective and automatic denial of admission without any other form of process. This paper argues that blanket entry bans on the ground of public health are illegal under international human rights law. They cannot be reconciled with the most basic rights of migrants and refugees, including the principle of non-refoulement and access to asylum procedures, the prohibition of collective expulsion, the best interests of the child and the principle of non-discrimination. The paper concludes on the ways to better integrate at the borders public health and human rights imperatives in due respect with the rule of law. In both law and practice, public health and migrant's rights are not mutually exclusive. They can reinforce each other within a comprehensive human rights based approach to health and migration policies."", 'corpus_id': 227249661, 'score': 0}, {'doc_id': '152913726', 'title': 'EU Human Rights Policies: A Study in Irony', 'abstract': 'Preface Acknowledgements 1. Introduction 2. Development Policy and Human Rights 3. Accession to the EU and Human Rights 4. The Scope of Internal-External Incoherence 5. Explaining Incoherence: the Orthodox Arguments 6. The Invention of Human Rights in the EU 7. European Identity and Human Rights 8. Conclusion', 'corpus_id': 152913726, 'score': 1}, {'doc_id': '231776354', 'title': 'COVID-19 Crisis - A Test for European Union’s Solidarity', 'abstract': 'Abstract The crises the European Union has gone through over time have called into question the Union’s legitimacy and efficiency. The 2008-2009 financial crisis, the European debt crisis, the migration crisis and Brexit, have all tested the solidarity between member states. The COVID-19 pandemic is without a doubt the most drastic crisis in the EU’s history, with very severe socioeconomic consequences. The EU leaders were strongly criticized for not reacting quickly and efficiently enough to mitigate the impact of the virus, reduce suffering, and ward off the economic crisis. In this context, the questions that arise are: Is the Union a modern-day Titanic? Will it sink or it will sustain its legitimacy and come out stronger and more united from this unprecedented challenge?', 'corpus_id': 231776354, 'score': 1}, {'doc_id': '229686043', 'title': 'Crisis and Intergovernmental Retrenchment in the European Union? Framing the EU’s Answer to the COVID-19 Pandemic', 'abstract': 'The outbreak of the COVID-19 pandemic has placed severe pressure on the EU’s capacity to provide a timely and coordinated response capable of curbing the pandemic’s disastrous economic and social effects on EU member states. In this situation, the supranational institutions and their models of action are evidently under pressure, seeming incapable of leading the EU out of the stormy waters of the present crisis. The article frames the first months of management of the COVID-19 crisis at EU level as characterised by the limited increase in the level of steering capacity by supranational institutions, due to the reaffirmed centrality of the intergovernmental option. To explain this situation, the article considers the absence of the institutional capacity/legitimacy to extract resources from society(ies), and the subsequent impossibility of guaranteeing an effective and autonomous process of political (re)distribution, the key factors accounting for the weakness of vertical political integration in the response to the COVID-19 challenge. This explains why during the COVID-19 crisis as well, the pattern followed by the EU is rather similar to past patterns, thus confirming that this has fed retrenchment aimed at the enforcement of the intergovernmental model and the defence of the most sensitive core state powers against inference from supranational EU institutions.', 'corpus_id': 229686043, 'score': 0}, {'doc_id': '231835488', 'title': 'Environment, Mobility, and International Law: A New Approach in the Americas', 'abstract': 'The role of international law in regulating international movement in the context of global environment change and hazards remains a topic of intense debate among both legal scholars and practitioners. Yet, as this Article shows, we have largely reached the limits of what existing international law methods and approaches can tell us about the future of the law in this area. By contrast, this Article draws on a detailed regional case study to offer a distinct perspective to that ongoing debate about the role and future of international law. Against the backdrop of emerging patterns of mobility linked to devastating environmental disasters in the Americas, this Article derives new legal insights from in-depth analysis of a developing body of comparative and international legal practice by countries from across this key region. \uf02a The author is grateful to Bruce Burson, Geoffrey Cantor, Jean-François Durieux, Walter Kälin, Ana Luquerna, and Atle Solberg for their useful comments on earlier drafts of this Article. The views expressed in the Article, and any errors that it contains, remain those of the author alone. The research on which the Article builds was generously supported by Economic and Social Research Council [grant number ES/K001051/1]. Chicago Journal of International Law 264 Vol. 21 No. 2 Table of', 'corpus_id': 231835488, 'score': 0}, {'doc_id': '155266195', 'title': 'Normative Power Europe and Human Rights: A Critical Analysis', 'abstract': 'This paper will critically assess Ian Manners’ widely discussed concept of ‘Normative Power Europe’, with specific reference to the EU’s promotion of human rights both internally and externally. It will analyse Manners’ three-fold typology of principles, actions and impact with regard to EU-Russia relations, EU-ASEAN relations and internally within the Union. More specifically, it will argue that the EU has double standards in its internal human rights policies, and prioritises strategic interests over human rights externally, to the detriment of its supposed normative identity.', 'corpus_id': 155266195, 'score': 1}, {'doc_id': '232053693', 'title': 'Revisiting the EU cybersecurity strategy: a call for EU cyber diplomacy', 'abstract': 'In December 2020, the European Union (EU) presented its new strategy on cybersecurity with the aim of strengthening Europe’s technological and digital sovereignty. The document lists reform projects that will link cybersecurity more closely with the EU’s new rules on data, algorithms, markets, and Internet services. However, it clearly falls short of the development of a European cyber diplomacy that is committed to both “strategic openness” and the protection of the digital single market. In order to achieve this, EU cyber diplomacy should be made more coherent in its supranational, democratic, and economic/technological dimensions. Germany can make an important contribution to that by providing the necessary legal, technical, and financial resources for the European External Action Service (EEAS).', 'corpus_id': 232053693, 'score': 0}, {'doc_id': '231681509', 'title': 'New actors and contested architectures in global migration governance: continuity and change', 'abstract': 'Abstract This article introduces the volume on New Actors and Contested Architectures in Global Migration Governance. It presents the aims and scope of the volume, followed by an overview of international cooperation in global migration governance, migration management and advocacy for migrants. We then discuss ‘new’ actors and how they maintain, contest or even alter established architectures and assemblages, followed by a presentation of the articles included in the volume. In conclusion, we reflect on the ways in which the COVID-19 pandemic may affect these dynamics.', 'corpus_id': 231681509, 'score': 1}, {'doc_id': '143415640', 'title': ""The European Union's Fundamental Rights Myth"", 'abstract': 'Although not in the Rome Treaty, the EEC/EU has gradually developed fundamental rights narratives which constitute a political myth. They have a common basis of foundational claims, placing fundamental rights, retrospectively, as inherent to the EU and based on a common European heritage. Like all myths, this narrative contains factual error, but is believed and acted upon by both institutional myth-makers and civil society actors. Through mythological free-riding on the Member States and the Council of Europe, the EU has been relatively successful in avoiding myth competition. Success in the longer run depends on broader myth appropriation, coherence and competition with other narratives.', 'corpus_id': 143415640, 'score': 1}]"
146	{'doc_id': '159160643', 'title': 'Buddhism and Politics in Thailand', 'abstract': None, 'corpus_id': 159160643}	17690	"[{'doc_id': '234683690', 'title': 'Peasant Studies: Subsistence, Justice, and Precarity', 'abstract': '“There are districts in which the position of the rural population is that of a man standing permanently up to the neck in water, so that even a ripple is sufficient to drown him.” With this epigraph, invoking the words of economic historian R. H. Tawney, James C. Scott launched The Moral Economy of the Peasant. His pathbreaking second book describes the social and cultural repertoires through which Southeast Asian peasantries struggled in the 1930s to dampen the ripples and torrents of political and economic change, in an effort to keep their heads above water. In the years since its publication, and despite this seemingly delimited focus, The Moral Economy of the Peasant has generated considerable ripples of its own, energizing the waters through which it has moved over the last four decades. A number of excellent reviews have delved deeply into the origins, inspiration, and impact of this work. Building on these, this short essay attempts to grapple with its intellectual energy, to understand something of how The Moral Economy of the Peasant became, and remains, a touchstone within and beyond the interdisciplinary field of Asian studies.', 'corpus_id': 234683690, 'score': 0}, {'doc_id': '235262934', 'title': 'Chapter 17 : The Japanese Transition to Democracy and Back', 'abstract': 'The first four case studies might lead readers to conclude that there was something unique about European culture that made it “ready” for parliamentary democracy in 1815. The king and council template had long been used for European governance and provided numerous opportunities for peaceful constitutional reform. Liberalism can be regarded as the political reform agenda of the enlightenment, and many of the technological innovations of the eighteenth and nineteenth centuries can be regarded as consequences the enlightenment’s emphasis on reason and nature. It can be argued that after a two or three century delay, the enlightenment produced the gains from constitutional exchange that led to parliamentary democracy. Insofar as the enlightenment can be considered European in origin, it might be argued that European ideas and institutions made Europe uniquely able to shift from autocracy to democracy without revolution. The theory developed in part I is, however, not a theory of European transitions. It suggests that similar ideas and opportunities for constitutional bargaining will exist in other societies in which broadly similar institutions are in place and trends in constitutional bargaining opportunities favor liberal reforms. The last two case studies demonstrate that the European transitions were not unique. Chapter 17 focuses on Japanese constitutional history in the nineteenth and early twentieth centuries during which parliamentary democracy emerged and then receded.289 As in the European cases, the king and council template of governance was widely used in Japan for governance at national, regional, and local levels. Constitutional negotiation and exchange were also commonplace in its medieval period, although there were no liberal trends in the constitutional bargains negotiated. During the late nineteenth and early twentieth centuries, liberal trends in economic and political reforms emerged for reasons similar to those in Europe. Coalitions that favored economic and political liberalization were in positions of sufficient authority to bargain with others in government and obtain modest reforms. Insofar as liberalism and many of the new production technologies were imported from Europe, it can be argued that the enlightenment also influenced the course of reform in Japan. Perfecting Parliament', 'corpus_id': 235262934, 'score': 0}, {'doc_id': '169228980', 'title': 'Buddhism and Postmodern Imaginings in Thailand: The Religiosity of Urban Space', 'abstract': ""Contents: Thai Buddhism in postmodernity Buddhist modernities, heresy and hybridization: Thailand's Thammakaai movement New Buddhism: copying, and the art of the imagination Buddhist cyber-worlds and changing urban space Nation, embodiment, and the charisma of a Thai saint Kammathaan monks, tradition and sites of memory Sanctification of place, power and mobility Conclusion/beginning Bibliography Index."", 'corpus_id': 169228980, 'score': 1}, {'doc_id': '234364938', 'title': 'Struggles for Recognition: The Liberal International Order and the Merger of Its Discontents', 'abstract': ""Abstract The Liberal International Order (LIO) is currently being undermined not only by states such as Russia but also by voters in the West. We argue that both veins of discontent are driven by resentment toward the LIO's status hierarchy, rather than simply by economic grievances. Approaching discontent historically and sociologically, we show that there are two strains of recognition struggles against the LIO: one in the core of the West, driven by populist politicians and their voters, and one on the semiperiphery, fueled by competitively authoritarian governments and their supporters. At this particular moment in history, these struggles are digitally, ideologically, and organizationally interconnected in their criticism of LIO institutions, amplifying each other. The LIO is thus being hollowed out from within at a time when it is also facing some of its greatest external challenges."", 'corpus_id': 234364938, 'score': 0}, {'doc_id': '151481314', 'title': 'The Irony of Democratization and the Decline of Royal Hegemony in Thailand', 'abstract': ""(ProQuest: ... denotes non-US-ASCII text omitted.)I learned about the May 22, 2014 coup d'etat by the National Council for Peace and Order (NCPO) under the leadership of the then commander-in-chief of the Royal Thai Army, General Prayut Chan-o-cha, the day it occurred. I was at a workshop comparing recent political developments and protests in Turkey and Thailand at the London School of Economics. The first thing that came to mind when I heard the news was a sentence I had come across long ago in my reading of Marx's writings on the state, in the preface to the second edition of his celebrated work on Louis Bonaparte's coup, dated 1869 (1974, 144):I show how, on the contrary, the class struggle in France created circumstances and conditions which allowed a mediocre and grotesque individual to play the hero's role.It struck me as an apt portrayal of the gist of the political crisis that had been plaguing Thailand for the past decade, namely, a mutually dissipating and destructive, protracted class conflict that had aggravatingly undermined its governing institutions and political civility, leading occasionally to partial state failures and anarchy in its administrative and business centers. With that class conflict reaching yet another impasse and stalemate in 2014, the NCPO's coup then presented itself as a statist or bureaucratic politic (a la Fred Riggs's Bureaucratic Polity in Riggs, 1966) solution to it in the Bonapartist manner.However, from the time of the preceding Thai Bonaparte-Field Marshal Sarit Thanarat, whose military absolutist rule lasted from 1958 to 1963-to the current one, much has changed in Thailand. Its population has more than doubled, from 28 million to 65 million; its GDP has increased 239-fold, from 54 billion to 12,910 billion baht; and its civil society has produced at least two successful popular uprisings, in 1973 and 1992, that managed to topple the military government of the day (Riggs 1966, 16; Pasuk and Baker 1995, 162; Baker and Pasuk 2005, xvii-xviii, 24, 201; Bank of Thailand 2015). Therefore, if the hugely corrupt and bullying womanizer of yesteryear who drank himself to death was still capable of producing some real tragedies, his latter-day sober and chaste if no less bullying aspirant seems more prone to making boastful, careless, farcical statements that have often landed his military administration in troubles both domestic and international (Thak 1979, 193-205; Grossman et al. 2009, 133; Anderson 2014, 52-53; Hookway 2015).1) He does indeed fit Marx's description of a Bonapartist hero insofar as mediocrity and grotesqueness are concerned.2)What I propose to do in this brief paper is to take a big picture and a long historical perspective of the current conflict and mass movements in Thailand, focusing on their class-related dimension, political dynamics, and royalist framing. Instead of focusing on the NCPO's coup per se, with its multifarious details and still ongoing eventuation, I would rather try to understand and assess it against the country's historical and cultural political backdrop.Power Shifts in Modern Thai Political HistoryIf one takes a long historical view of modern Thai politics since the late nineteenth century, one can't help but notice a recurrent pattern of major power shifts in modern Thai history. Its basic trajectory follows much the same logic:- It begins with the partly pressured, partly voluntary opening up of the economy to the outside world, and the resultant rapid economic growth;- That is followed by a big social change, especially the emergence and upward mobility of new social groups and classes in connection with the newly liberalized and expanding sector of the economy;- This leads to a political contest between the old elites and their privileged allies on the one hand, and the rising new groups and classes on the other;- Eventually, all this leads sooner or later to a regime change. …"", 'corpus_id': 151481314, 'score': 1}, {'doc_id': '197654356', 'title': 'Pigtail: A Pre-History of Chineseness in Siam', 'abstract': ""Abstract:Challenging the reification of ethnic categories, this paper sets out to examine the genealogy of Chineseness in Siam before the early twentieth century by focusing on the pigtail as an alleged sign of Chineseness. A critical scrutiny of G. William Skinner's arguments in his Chinese Society in Thailand and the political and cultural history of the pigtail in both the Middle Kingdom and the Kingdom of Siam reveal the variable, situational, and pluralistic meanings of the pigtail. With the pigtail as signifier being thus deconstructed, Chineseness turns out to be a recent invention in Thai racist discourse that had little to do with the pigtail as such."", 'corpus_id': 197654356, 'score': 1}, {'doc_id': '235342154', 'title': 'REFLECTIONS ON DEMOCRACY', 'abstract': 'The Bush administration would have us believe that democracy, assisted where necessary by American power, could soon sweep the debris of its competitors into the dustbin of history. Outside the administration, this belief is widely disputed, even by people prepared to accept a minimalist deanition of what democracy entails. Assessments of democracy’s prospects—and of democratic performance—have, in fact, varied widely since the “third wave” began in the 1970s. For many observers, both inside and outside the academy, the creation of democratic regimes in southern Europe, the demise of military dictatorships in South America, and, especially, the end of Soviet communism seemed to herald an era in which democracy and capitalism would hold the political aeld uncontested. Triumphalist arguments were tempered, however, both by real-world events and, within the academy, by the', 'corpus_id': 235342154, 'score': 0}, {'doc_id': '203251016', 'title': 'Commodifying Marxism: The Formation of Modern Thai Radical Culture, 1927-1958', 'abstract': None, 'corpus_id': 203251016, 'score': 1}, {'doc_id': '144135061', 'title': 'Buddhism, Politics, and Nationalism in the Twentieth and Twenty‐first Centuries', 'abstract': 'Buddhism is widely understood as a religion with a global scope. Particularly from the end of the twentieth century, the widespread growth of Buddhism internationally, and the extensive ties between Buddhists institutions, leave the impression of unity within contemporary Buddhism. Nevertheless, in this article, I argue that Buddhism cannot be understood outside of a national context. Although international ties between Buddhists are real and important, Sanghas generally remain under the governance by national governments and monks and nuns remain citizens of particular nation-states. As a result, contemporary Buddhism is marked by a tension between the transnational and the national.', 'corpus_id': 144135061, 'score': 1}, {'doc_id': '234789473', 'title': 'Japanese Imperialism and Colonialism', 'abstract': ""In the nineteenth century, Western powers saddled non-Western states with a variety of unequal arrangements, from fixed tariffs and extraterritoriality to formal colonization. The 1858 Treaty of Amity and Commerce between the United States and Japan marked the inclusion of Japan into the unfortunate side of this equation. Japanese nationalists protested the insults against their national sovereignty and led the forces which overthrew the Tokugawa regime. The prevention of further loss of sovereignty and the revision of the unequal treaties remained the new Meiji government's most pressing issues for the next twenty years. Leading their list of goals was the need to strengthen the military in order to withstand future Western impositions. They studied the organizations and techniques of Western governments and militaries, and they modeled their own institutions on them. Thus the Meiji government was born in an imperialistic milieu, and their primary models were the world's leading imperialistic states. It is not surprising, therefore, that the Japanese government would created its own empire as soon as it was able."", 'corpus_id': 234789473, 'score': 0}]"
147	{'doc_id': '90262234', 'title': 'Med3D: Transfer Learning for 3D Medical Image Analysis', 'abstract': 'The performance on deep learning is significantly affected by volume of training data. Models pre-trained from massive dataset such as ImageNet become a powerful weapon for speeding up training convergence and improving accuracy. Similarly, models based on large dataset are important for the development of deep learning in 3D medical images. However, it is extremely challenging to build a sufficiently large dataset due to difficulty of data acquisition and annotation in 3D medical imaging. We aggregate the dataset from several medical challenges to build 3DSeg-8 dataset with diverse modalities, target organs, and pathologies. To extract general medical three-dimension (3D) features, we design a heterogeneous 3D network called Med3D to co-train multi-domain 3DSeg-8 so as to make a series of pre-trained models. We transfer Med3D pre-trained models to lung segmentation in LIDC dataset, pulmonary nodule classification in LIDC dataset and liver segmentation on LiTS challenge. Experiments show that the Med3D can accelerate the training convergence speed of target 3D medical tasks 2 times compared with model pre-trained on Kinetics dataset, and 10 times compared with training from scratch as well as improve accuracy ranging from 3% to 20%. Transferring our Med3D model on state-the-of-art DenseASPP segmentation network, in case of single model, we achieve 94.6\\% Dice coefficient which approaches the result of top-ranged algorithms on the LiTS challenge.', 'corpus_id': 90262234}	17165	[{'doc_id': '235208542', 'title': 'Managing Class Imbalance in Multi-Organ CT Segmentation in Head and Neck Cancer Patients', 'abstract': 'Radiotherapy planning of head and neck cancer patients requires an accurate delineation of several organs at risk (OAR) from planning CT images in order to determine a dose plan which reduces toxicity and salvages normal tissue. However training a single deep neural network for multiple organs is highly sensitive to class imbalance and variability in size between several structures within the head and neck region. In this paper, we propose a single-class segmentation model for each OAR in order to handle class imbalance issues during training across output classes (one class per structure), where there exists a severe disparity between 12 OAR. Based on a U-net architecture, we present a transfer learning approach between similar OAR to leverage common learned features, as well as a simple weight averaging strategy to initialize a model as the average of multiple models, each trained on a separate organ. Experiments performed on an internal dataset of 200 H & N cancer patients treated with external beam radiotherapy, show the proposed model presents a significant improvement compared to the baseline multi-organ segmentation model, which attempts to simultaneously train several OAR. The proposed model yields an overall Dice score of $0.75 \\pm 0.12$, by using both transfer learning across OAR and a weight averaging strategy, indicating that a reasonable segmentation performance can be achieved by leveraging additional data from surrounding structures, limiting the uncertainty in ground-truth annotations.', 'corpus_id': 235208542, 'score': 0}, {'doc_id': '233324524', 'title': 'SSLM: Self-Supervised Learning for Medical Diagnosis from MR Video', 'abstract': 'In medical image analysis, the cost of acquiring high-quality data and their annotation by experts is a barrier in many medical applications. Most of the techniques used are based on supervised learning framework and need a large amount of annotated data to achieve satisfactory performance. As an alternative, in this paper, we propose a self-supervised learning approach to learn the spatial anatomical representations from the frames of magnetic resonance (MR) video clips for the diagnosis of knee medical conditions. The pretext model learns meaningful spatial context-invariant representations. The downstream task in our paper is a class imbalanced multi-label classification. Different experiments show that the features learnt by the pretext model provide explainable performance in the downstream task. Moreover, the efficiency and reliability of the proposed pretext model in learning representations of minority classes without applying any strategy towards imbalance in the dataset can be seen from the results. To the best of our knowledge, this work is the first work of its kind in showing the effectiveness and reliability of self-supervised learning algorithms in class imbalanced multi-label classification tasks on MR video. The code for evaluation of the proposed work is available at https://github.com/sadimanna/sslm', 'corpus_id': 233324524, 'score': 0}, {'doc_id': '231616250', 'title': 'On the Effective Transfer Learning Strategy for Medical Image Analysis in Deep Learning', 'abstract': 'In this study, we focus on exploring different strategies of transfer learning for medical applications. Firstly, we report competitive results indicating that convolutional neural networks (CNNs) that were pre-trained with different annotations could have diverse effects on the performance of medical image analysis, especially for segmentation tasks. Then, we present our further explorations of transferring different components of the CNNs, which revealed the importance of the decoder on medical segmentation. Finally, we demonstrate the advantages and disadvantages of transfer learning methods based on model integration. These observations present novel aspects of transfer learning for visual tasks in the medical field, and we expect that these discoveries will encourage the exploration of more effective transfer learning strategies for CNN-based medical image analysis.', 'corpus_id': 231616250, 'score': 1}, {'doc_id': '201070166', 'title': 'Models Genesis: Generic Autodidactic Models for 3D Medical Image Analysis', 'abstract': 'Transfer learning from natural image to medical image has established as one of the most practical paradigms in deep learning for medical image analysis. However, to fit this paradigm, 3D imaging tasks in the most prominent imaging modalities (e.g., CT and MRI) have to be reformulated and solved in 2D, losing rich 3D anatomical information and inevitably compromising the performance. To overcome this limitation, we have built a set of models, called Generic Autodidactic Models, nicknamed Models Genesis, because they are created ex nihilo (with no manual labeling), self-taught (learned by self-supervision), and generic (served as source models for generating application-specific target models). Our extensive experiments demonstrate that our Models Genesis significantly outperform learning from scratch in all five target 3D applications covering both segmentation and classification. More importantly, learning a model from scratch simply in 3D may not necessarily yield performance better than transfer learning from ImageNet in 2D, but our Models Genesis consistently top any 2D approaches including fine-tuning the models pre-trained from ImageNet as well as fine-tuning the 2D versions of our Models Genesis, confirming the importance of 3D anatomical information and significance of our Models Genesis for 3D medical imaging. This performance is attributed to our unified self-supervised learning framework, built on a simple yet powerful observation: the sophisticated yet recurrent anatomy in medical images can serve as strong supervision signals for deep models to learn common anatomical representation automatically via self-supervision. As open science, all pre-trained Models Genesis are available at https://github.com/MrGiovanni/ModelsGenesis.', 'corpus_id': 201070166, 'score': 1}, {'doc_id': '234469981', 'title': 'Swin-Unet: Unet-like Pure Transformer for Medical Image Segmentation', 'abstract': 'In the past few years, convolutional neural networks (CNNs) have achieved milestones in medical image analysis. Especially, the deep neural networks based on U-shaped architecture and skip-connections have been widely applied in a variety of medical image tasks. However, although CNN has achieved excellent performance, it cannot learn global and long-range semantic information interaction well due to the locality of convolution operation. In this paper, we propose Swin-Unet, which is a Unet-like pure Transformer for medical image segmentation. The tokenized image patches are fed into the Transformer-based Ushaped Encoder-Decoder architecture with skip-connections for localglobal semantic feature learning. Specifically, we use hierarchical Swin Transformer with shifted windows as the encoder to extract context features. And a symmetric Swin Transformer-based decoder with patch expanding layer is designed to perform the up-sampling operation to restore the spatial resolution of the feature maps. Under the direct downsampling and up-sampling of the inputs and outputs by 4×, experiments on multi-organ and cardiac segmentation tasks demonstrate that the pure Transformer-based U-shaped Encoder-Decoder network outperforms those methods with full-convolution or the combination of transformer and convolution. The codes and trained models will be publicly available at https://github.com/HuCaoFighting/Swin-Unet.', 'corpus_id': 234469981, 'score': 0}, {'doc_id': '222310113', 'title': 'Universal Model for 3D Medical Image Analysis', 'abstract': 'Deep Learning-based methods recently have achieved remarkable progress in medical image analysis, but heavily rely on massive amounts of labeled training data. Transfer learning from pre-trained models has been proposed as a standard pipeline on medical image analysis to address this bottleneck. Despite their success, the existing pre-trained models are mostly not tuned for multi-modal multi-task generalization in medical domains. Specifically, their training data are either from non-medical domain or in single modality, failing to attend to the problem of performance degradation with cross-modal transfer. Furthermore, there is no effort to explicitly extract multi-level features required by a variety of downstream tasks. To overcome these limitations, we propose Universal Model, a transferable and generalizable pre-trained model for 3D medical image analysis. A unified self-supervised learning scheme is leveraged to learn representations from multiple unlabeled source datasets with different modalities and distinctive scan regions. A modality invariant adversarial learning module is further introduced to improve the cross-modal generalization. To fit a wide range of tasks, a simple yet effective scale classifier is incorporated to capture multi-level visual representations. To validate the effectiveness of the Universal Model, we perform extensive experimental analysis on five target tasks, covering multiple imaging modalities, distinctive scan regions, and different analysis tasks. Compared with both public 3D pre-trained models and newly investigated 3D self-supervised learning methods, Universal Model demonstrates superior generalizability, manifested by its higher performance, stronger robustness and faster convergence. The pre-trained Universal Model is available at: \\href{https://github.com/xm-cmic/Universal-Model}{https://github.com/xm-cmic/Universal-Model}.', 'corpus_id': 222310113, 'score': 1}, {'doc_id': '211730343', 'title': 'AppendiXNet: Deep Learning for Diagnosis of Appendicitis from A Small Dataset of CT Exams Using Video Pretraining', 'abstract': 'The development of deep learning algorithms for complex tasks in digital medicine has relied on the availability of large labeled training datasets, usually containing hundreds of thousands of examples. The purpose of this study was to develop a 3D deep learning model, AppendiXNet, to detect appendicitis, one of the most common life-threatening abdominal emergencies, using a small training dataset of less than 500 training CT exams. We explored whether pretraining the model on a large collection of natural videos would improve the performance of the model over training the model from scratch. AppendiXNet was pretrained on a large collection of YouTube videos called Kinetics, consisting of approximately 500,000 video clips and annotated for one of 600 human action classes, and then fine-tuned on a small dataset of 438 CT scans annotated for appendicitis. We found that pretraining the 3D model on natural videos significantly improved the performance of the model from an AUC of 0.724 (95% CI 0.625, 0.823) to 0.810 (95% CI 0.725, 0.895). The application of deep learning to detect abnormalities on CT examinations using video pretraining could generalize effectively to other challenging cross-sectional medical imaging tasks when training data is limited.', 'corpus_id': 211730343, 'score': 1}, {'doc_id': '234496677', 'title': 'CMM-Net: Contextual multi-scale multi-level network for efficient biomedical image segmentation', 'abstract': 'Medical image segmentation of tissue abnormalities, key organs, or blood vascular system is of great significance for any computerized diagnostic system. However, automatic segmentation in medical image analysis is a challenging task since it requires sophisticated knowledge of the target organ anatomy. This paper develops an end-to-end deep learning segmentation method called Contextual Multi-Scale Multi-Level Network (CMM-Net). The main idea is to fuse the global contextual features of multiple spatial scales at every contracting convolutional network level in the U-Net. Also, we re-exploit the dilated convolution module that enables an expansion of the receptive field with different rates depending on the size of feature maps throughout the networks. In addition, an augmented testing scheme referred to as Inversion Recovery (IR) which uses logical “OR” and “AND” operators is developed. The proposed segmentation network is evaluated on three medical imaging datasets, namely ISIC 2017 for skin lesions segmentation from dermoscopy images, DRIVE for retinal blood vessels segmentation from fundus images, and BraTS 2018 for brain gliomas segmentation from MR scans. The experimental results showed superior state-of-the-art performance with overall dice similarity coefficients of 85.78%, 80.27%, and 88.96% on the segmentation of skin lesions, retinal blood vessels, and brain tumors, respectively. The proposed CMM-Net is inherently general and could be efficiently applied as a robust tool for various medical image segmentations.', 'corpus_id': 234496677, 'score': 0}, {'doc_id': '233346815', 'title': 'PocketNet: A Smaller Neural Network for 3D Medical Image Segmentation', 'abstract': 'Overparameterized deep learning networks have shown impressive performance in the area of automatic medical image segmentation. However, they achieve this performance at an enormous cost in memory, runtime, and energy. A large source of overparameterization in modern neural networks results from doubling the number of feature maps with each downsampling layer. This rapid growth in the number of parameters results in network architectures that require a significant amount of computing resources, making them less accessible and difficult to use. By keeping the number of feature maps constant throughout the network, we derive a new CNN architecture called PocketNet that achieves comparable segmentation results to conventional CNNs while using less than 3% of the number of parameters.', 'corpus_id': 233346815, 'score': 0}, {'doc_id': '219530962', 'title': '3D Self-Supervised Methods for Medical Imaging', 'abstract': 'Self-supervised learning methods have witnessed a recent surge of interest after proving successful in multiple application fields. In this work, we leverage these techniques, and we propose 3D versions for five different self-supervised methods, in the form of proxy tasks. Our methods facilitate neural network feature learning from unlabeled 3D images, aiming to reduce the required cost for expert annotation. The developed algorithms are 3D Contrastive Predictive Coding, 3D Rotation prediction, 3D Jigsaw puzzles, Relative 3D patch location, and 3D Exemplar networks. Our experiments show that pretraining models with our 3D tasks yields more powerful semantic representations, and enables solving downstream tasks more accurately and efficiently, compared to training the models from scratch and to pretraining them on 2D slices. We demonstrate the effectiveness of our methods on three downstream tasks from the medical imaging domain: i) Brain Tumor Segmentation from 3D MRI, ii) Pancreas Tumor Segmentation from 3D CT, and iii) Diabetic Retinopathy Detection from 2D Fundus images. In each task, we assess the gains in data-efficiency, performance, and speed of convergence. Interestingly, we also find gains when transferring the learned representations, by our methods, from a large unlabeled 3D corpus to a small downstream-specific dataset. We achieve results competitive to state-of-the-art solutions at a fraction of the computational expense. We publish our implementations for the developed algorithms (both 3D and 2D versions) as an open-source library, in an effort to allow other researchers to apply and extend our methods on their datasets.', 'corpus_id': 219530962, 'score': 1}]
148	{'doc_id': '211146667', 'title': 'An overview of distance and similarity functions for structured data', 'abstract': 'The notions of distance and similarity play a key role in many machine learning approaches, and artificial intelligence in general, since they can serve as an organizing principle by which individuals classify objects, form concepts and make generalizations. While distance functions for propositional representations have been thoroughly studied, work on distance functions for structured representations, such as graphs, frames or logical clauses, has been carried out in different communities and is much less understood. Specifically, a significant amount of work that requires the use of a distance or similarity function for structured representations of data usually employs ad-hoc functions for specific applications. Therefore, the goal of this paper is to provide an overview of this work to identify connections between the work carried out in different areas and point out directions for future work.', 'corpus_id': 211146667}	1151	[{'doc_id': '211126492', 'title': 'The Next Decade in AI: Four Steps Towards Robust Artificial Intelligence', 'abstract': 'Recent research in artificial intelligence and machine learning has largely emphasized general-purpose learning and ever-larger training sets and more and more compute. In contrast, I propose a hybrid, knowledge-driven, reasoning-based approach, centered around cognitive models, that could provide the substrate for a richer, more robust AI than is currently possible.', 'corpus_id': 211126492, 'score': 0}, {'doc_id': '5919882', 'title': 'Online Passive-Aggressive Algorithms', 'abstract': 'We present a unified view for online classification, regression, and uni-class problems. This view leads to a single algorithmic framework for the three problems. We prove worst case loss bounds for various algorithms for both the realizable case and the non-realizable case. A conversion of our main online algorithm to the setting of batch learning is also discussed. The end result is new algorithms and accompanying loss bounds for the hinge-loss.', 'corpus_id': 5919882, 'score': 1}, {'doc_id': '3854508', 'title': 'Fractal AI: A fragile theory of intelligence', 'abstract': 'Fractal AI is a theory for general artificial intelligence. It allows deriving new mathematical tools that constitute the foundations for a new kind of stochastic calculus, by modelling information using cellular automaton-like structures instead of smooth functions. \nIn the repository included we are presenting a new Agent, derived from the first principles of the theory, which is capable of solving Atari games several orders of magnitude more efficiently than other similar techniques, like Monte Carlo Tree Search. \nThe code provided shows how it is now possible to beat some of the current State of The Art benchmarks on Atari games, without previous learning and using less than 1000 samples to calculate each one of the actions when standard MCTS uses 3 Million samples. Among other things, Fractal AI makes it possible to generate a huge database of top performing examples with a very little amount of computation required, transforming Reinforcement Learning into a supervised problem. \nThe algorithm presented is capable of solving the exploration vs exploitation dilemma on both the discrete and continuous cases, while maintaining control over any aspect of the behaviour of the Agent. From a general approach, new techniques presented here have direct applications to other areas such as Non-equilibrium thermodynamics, chemistry, quantum physics, economics, information theory, and non-linear control theory.', 'corpus_id': 3854508, 'score': 1}, {'doc_id': '212415162', 'title': 'BERT as a Teacher: Contextual Embeddings for Sequence-Level Reward', 'abstract': 'Measuring the quality of a generated sequence against a set of references is a central problem in many learning frameworks, be it to compute a score, to assign a reward, or to perform discrimination. Despite great advances in model architectures, metrics that scale independently of the number of references are still based on n-gram estimates. We show that the underlying operations, counting words and comparing counts, can be lifted to embedding words and comparing embeddings. An in-depth analysis of BERT embeddings shows empirically that contextual embeddings can be employed to capture the required dependencies while maintaining the necessary scalability through appropriate pruning and smoothing techniques. We cast unconditional generation as a reinforcement learning problem and show that our reward function indeed provides a more effective learning signal than n-gram reward in this challenging setting.', 'corpus_id': 212415162, 'score': 0}, {'doc_id': '211230955', 'title': 'Incubation Period and Other Epidemiological Characteristics of 2019 Novel Coronavirus Infections with Right Truncation: A Statistical Analysis of Publicly Available Case Data', 'abstract': 'The geographic spread of 2019 novel coronavirus (COVID-19) infections from the epicenter of Wuhan, China, has provided an opportunity to study the natural history of the recently emerged virus. Using publicly available event-date data from the ongoing epidemic, the present study investigated the incubation period and other time intervals that govern the epidemiological dynamics of COVID-19 infections. Our results show that the incubation period falls within the range of 2–14 days with 95% confidence and has a mean of around 5 days when approximated using the best-fit lognormal distribution. The mean time from illness onset to hospital admission (for treatment and/or isolation) was estimated at 3–4 days without truncation and at 5–9 days when right truncated. Based on the 95th percentile estimate of the incubation period, we recommend that the length of quarantine should be at least 14 days. The median time delay of 13 days from illness onset to death (17 days with right truncation) should be considered when estimating the COVID-19 case fatality risk.', 'corpus_id': 211230955, 'score': 0}, {'doc_id': '14274323', 'title': 'All Else Being Equal Be Empowered', 'abstract': 'The classical approach to using utility functions suffers from the drawback of having to design and tweak the functions on a case by case basis. Inspired by examples from the animal kingdom, social sciences and games we propose empowerment, a rather universal function, defined as the information-theoretic capacity of an agent’s actuation channel. The concept applies to any sensorimotoric apparatus. Empowerment as a measure reflects the properties of the apparatus as long as they are observable due to the coupling of sensors and actuators via the environment.', 'corpus_id': 14274323, 'score': 1}, {'doc_id': '211476589', 'title': 'Understanding of COVID‐19 based on current evidence', 'abstract': 'Since December 2019, a series of unexplained pneumonia cases have been reported in Wuhan, China. On 12 January 2020, the World Health Organization (WHO) temporarily named this new virus as the 2019 novel coronavirus (2019‐nCoV). On 11 February 2020, the WHO officially named the disease caused by the 2019‐nCoV as coronavirus disease (COVID‐19). The COVID‐19 epidemic is spreading all over the world, especially in China. Based on the published evidence, we systematically discuss the characteristics of COVID‐19 in the hope of providing a reference for future studies and help for the prevention and control of the COVID‐19 epidemic.', 'corpus_id': 211476589, 'score': 0}, {'doc_id': '10070153', 'title': 'Large scale metric learning from equivalence constraints', 'abstract': 'In this paper, we raise important issues on scalability and the required degree of supervision of existing Mahalanobis metric learning methods. Often rather tedious optimization procedures are applied that become computationally intractable on a large scale. Further, if one considers the constantly growing amount of data it is often infeasible to specify fully supervised labels for all data points. Instead, it is easier to specify labels in form of equivalence constraints. We introduce a simple though effective strategy to learn a distance metric from equivalence constraints, based on a statistical inference perspective. In contrast to existing methods we do not rely on complex optimization problems requiring computationally expensive iterations. Hence, our method is orders of magnitudes faster than comparable methods. Results on a variety of challenging benchmarks with rather diverse nature demonstrate the power of our method. These include faces in unconstrained environments, matching before unseen object instances and person re-identification across spatially disjoint cameras. In the latter two benchmarks we clearly outperform the state-of-the-art.', 'corpus_id': 10070153, 'score': 1}, {'doc_id': '733980', 'title': 'Hyperdimensional Computing: An Introduction to Computing in Distributed Representation with High-Dimensional Random Vectors', 'abstract': 'The 1990s saw the emergence of cognitive models that depend on very high dimensionality and randomness. They include Holographic Reduced Representations, Spatter Code, Semantic Vectors, Latent Semantic Analysis, Context-Dependent Thinning, and Vector-Symbolic Architecture. They represent things in high-dimensional vectors that are manipulated by operations that produce new high-dimensional vectors in the style of traditional computing, in what is called here hyperdimensional computing on account of the very high dimensionality. The paper presents the main ideas behind these models, written as a tutorial essay in hopes of making the ideas accessible and even provocative. A sketch of how we have arrived at these models, with references and pointers to further reading, is given at the end. The thesis of the paper is that hyperdimensional representation has much to offer to students of cognitive science, theoretical neuroscience, computer science and engineering, and mathematics.', 'corpus_id': 733980, 'score': 1}, {'doc_id': '21351739', 'title': 'VoiceHD: Hyperdimensional Computing for Efficient Speech Recognition', 'abstract': 'In this paper, we propose VoiceHD, a novel speech recognition technique based on brain-inspired hyperdimensional(HD) computing. VoiceHD maps preprocessed voice signals in the frequency domain to random hypervectors and combines them to compute a hypervector (as learned patterns) representing each class. During inference, VoiceHD similarly computes a query hypervector; the classification task is done by checking the similarity of the query hypervector with all learned hypervectors and ﬁnding a class with the highest similarity. We further extend VoiceHD to VoiceHD+NN that uses a neural network with a single small hidden layer to improve the similarity measures. This neural network is a small block directly operating on the similarity outputs of VoiceHD to slightly improve the classification accuracy. We evaluate efficiency of VoiceHD and VoiceHD+NN compared to a deep neural network with three large hidden layers over Isolet spoken letter dataset. Our benchmarking results on CPU show that VoiceHD and VoiceHD+NN provide 11.9X and 8.5X higher energy efficiency, 5.3X and 4.0X faster testing time, and 4.6X and 2.9X faster training time compared to the deep neural network, while providing marginally better classification accuracy.', 'corpus_id': 21351739, 'score': 0}]
149	"{'doc_id': '51575171', 'title': 'Oral tradition and communication', 'abstract': ""Oral tradition has become a domain of great interest to scholars of different disciplines of knowledge such as literature, psychology, anthropology, and philosophy. It has a huge scope for the discipline of communication too. This article presents an appraisal of oral tradition as a means of communication from one generation to another. While doing so, it deals with following issues: Can history be narrated based on oral traditions just as it is done with ‘written documents'? Are the oral traditions only the sources of historiography or do they have other implications too? It also discusses whether oral traditions can be taken as valid historical sources, and, if not, whether there are means for testing its reliability. DOI: 10.3126/bodhi.v3i1.2813 Bodhi Vol.3(1) 2009 p.61-68"", 'corpus_id': 51575171}"	8092	"[{'doc_id': '220535649', 'title': 'A manifesto for planning after the coronavirus: Towards planning of care', 'abstract': 'The COVID-19 crisis upended the status quo of our everyday life. The rising discourse in the midst of this pandemic is ‘human guilt’ (e.g., ‘we are the virus!’), reviving the dark side of neo-Malthusian environmentalist ideology. While the pandemic should be considered a wake-up call for us to drastically rethink our relationship with nature, planning discipline cannot resign itself from its power and responsibility to make a difference in human and nonhuman lives. So, here I ask: How can we carefully reposition ‘human intervention’ in the aftermath of this ‘human guilt’, without nullifying the hopeful spirit and our belief in the power of planning? Inspired by Tronto/ Lawson’s geographies of care and Dewey-an pragmatism, this essay calls for the rise of ‘planning of care’. Planning of care not only recognises humans’ interdependency on one another, but also acknowledges cities’ on-going, dialectic relationship with their natural surroundings.', 'corpus_id': 220535649, 'score': 0}, {'doc_id': '142341155', 'title': ""Toward an Emancipatory Psychoanalysis: Brandchaft's Intersubjective Vision"", 'abstract': ""Encountering Brandchaft. Toward an Emancipatory Psychoanalysis. Reconsiderations of Psychoanalytic Listening. Theoretical Considerations. A Case of Intractable Depression. Bonds that Shackle, Ties that Free. Whose Self Is It Anyway? Codetermination and Change in Psychoanalysis. To Free the Spirit from its Cell. The Self and its Objects in Developmental Trauma. Obsessional Disorders: A Developmental Systems Perspective. Systems of Pathological Accommodation in Psychoanalysis. Reflections on the Unconscious. Brandchaft's Intersubjective Vision."", 'corpus_id': 142341155, 'score': 1}, {'doc_id': '199484922', 'title': 'Temporality as Bergsonian Critique in the Advertising and Visual Art of Bertram Brooker', 'abstract': 'This article explores time concepts derived from Henri Bergson as adapted by Canadian marketing theorist and visual artist Bertram Brooker (1888–1955) in articles and textbooks published during the 1920s and early 1930s. Inspired by Bergson’s critique of the Western metaphysical tradition, Brooker proposed innovative, participatory advertising strategies based on the French philosopher’s non-rational conception of time and the co-evolution of bodies and media. The author argues that the Toronto artist-advertiser’s descriptions of radio as offering the possibility of an interactive and synesthetic alternative to conventional print-based forms of advertising indirectly influenced Harold Innis’ redemptive gloss on the latent dialogism of radio. A critique of Brooker’s and Innis’ respective articulations of “oral” media as foreshadowing the contemporary economy of televisual “flow” is also posited. KEYWORdS Advertising; Toronto School/Transformation history; Media theory; Orality/Oral culture; Philosophy RÉSUMÉ Cet article explore les concepts temporels du philosophe français Henri Bergson ainsi qu’ils étaient adaptés par le théoricien du marketing et artiste Bertram Brooker (18881955) dans ses articles et manuels des années 1920 et début des années 1930. La critique de la tradition métaphysique occidental affirmé par Bergson en a inspiré Brooker, qui a proposé des stratégies de publicité participatives fondées sur sa conception non-rationnel de temps aussi que sa conception de la co-évolution des corps et des médias. Cet article propose que la représentation de la radio en tant qu’un objet interactif et synésthetique soutenu par l’artistedirecteur de publicité a influencé indirectement l’interprétation rédemptrice de Harold Innis au sujet du dialogisme de la radio. Une critique des déclarations de Brooker et Innis sur les médias « orale » en tant que préfigurant de l’économie contemporaine de « flux » télévisuel est aussi offert. MOTS CLÉS Publicité; École de Toronto/Histoire de la transformation; Théorie des médias; Oralité/Culture orale; Philosophie Introduction The multidisciplinary production of the Canadian artist, author, and advertising executive bertram brooker (1888–1955) is marked by a persistent obsession with time. Previous commentators have detected the influence of Henri bergson on brooker’s advertising writings and visual art (see Lauder, 2006, 2010, 2012; Luff, 1991; Zemans, Canadian Journal of Communication Vol 39 (2014) 469–496 ©2014 Canadian Journal of Communication Corporation Adam Lauder was the first W. P. Scott Chair for Research in E-Librarianship at York University, 4700 Keele Street, Toronto, ON M3J 1P3. Email: alauder@yorku.ca . 470 Canadian Journal of Communication, Vol 39 (3) 1989). The artist-advertiser’s celebration of “becoming” and “flux” in his marketing texts of the 1920s, in particular, is visualized by his abstract canvases, the first to be exhibited in a solo exhibition in Canada, and innovative graphic designs. His work of that decade is thereby aligned with the earlier bergsonian modernisms of Futurist and Vorticist artists (see Antliff, 1993). Yet the present article constitutes the first scholarly paper to frame brooker’s engagement with bergson’s non-rational temporality as a critical project. brooker’s harnessing of bergsonian temporality to articulate a sophisticated critique of modernity was all the more unusual given his status as one of the pre-eminent innovators in North American advertising circles in the 1920s. Not that his application of bergsonian concepts to problems in advertising was not, in this context at least, itself remarkably out of the ordinary. Perhaps only the futurist graphic designs executed by Fortunato depero (1892–1960) for mass-market publications such as Movie Makers, Vanity Fair, and Vogue, during his sojourn in New York from 1928 to 1930, constitute a comparable appropriation of bergson in North American popular culture. but whereas depero’s designs celebrated the flux of the machine age, brooker urged fellow admen to adopt bergson’s insights as part of a thorough re-evaluation of modernist values and beliefs embraced by the advertising profession, including progress, efficiency, and rationalization. The critical orientation of brooker’s appropriation of bergson is closer to aspects of the post–World War I cultural criticism of Canadian-born artist and author Wyndham Lewis (1882–1957), which drew on bergson’s critique of the spatial models informing classical metaphysics and modern science even while reversing the terms of the French philosopher’s arguments to articulate a renovation of spatial perception. brooker was already familiar with Lewis’ monumental Time and Western Man in the year of its publication (brooker, 1927, pp. 6–7). Gregory betts (2013) has recently proposed parallels between brooker’s “vortex of art, media, and advertising” and Lewis’ discourse on technology and mass culture (p. 217). Lauder (2012) earlier noted that Time and Western Man influenced Harold Innis’ discourse on time and modernity, with which brooker’s critical project likewise shares much in common (see also Watson, 2006). The resonance between articulations of time found in the work of brooker, Lewis, and later Innis speaks to the common currency of bergsonian rhetoric in what Paul Tiessen (1993) has termed the “pre-McLuhan body Canadian media theory” of the 1920s through the 1940s. Similarly, Janine Marchessault (2005) and darroch and Marchessault (2009) have argued that bergson’s thought played a formative—albeit largely overlooked—role in the development of Toronto School communication theory. Through his astonishingly early and critical engagement with bergsonian time concepts across a broad spectrum of discourses and media, brooker occupies a key position in this configuration. Yet this temporal dimension of the artist-advertiser’s work, and its critical orientation, have largely eluded appraisal until now. Through frequent contributions to the high-profile American trade papers Printers’ Ink and Advertising and Selling, and, from 1924 through 1927, as editor and publisher of Canada’s premier advertising journal, Marketing and Business Management (to which he also made frequent contributions as an author from 1921 until at least 1931), brooker established an international reputation as an outspoken critic of then-dominant behaviourist and quantitative approaches to marketing (see Johnston, 2001). In place of the statistical instruments and mechanistic models promoted by peers, brooker urged fellow marketers to take the literary production of Chekhov, dickens, and Shakespeare as their model. “dickens,” he wrote, “analyzed the consumer demand of his day and adjusted his production accordingly. ‘People like deep-dyed villains,’ he said to himself, and straightaway produced Quilp” (Marketing, 1921, p. 332). brooker’s unapologetically literary approach to advertising drew fierce criticism from leading American advertisers of the day: his pointed exchanges with Earnest Elmo Calkins, Charles S. Knapp, and William E. Cameron in the pages of Printers’ Ink substantiate the claim of one observer in 1951 that the Canadian artist-advertiser’s intuitive approach to copy “was strongly felt in international advertising” (betts, 2005, p. 231). While the literary and aesthetic bias of brooker’s writings on advertising topics has received broad acknowledgment, the extent to which his critique of American models also drew on the philosophy of bergson has yet to receive adequate scholarly attention. Selections of brooker’s articles from Marketing and Printers’ Ink were collected and revised in two volumes published by McGraw-Hill under the nom de plume Richard Surrey (one of several pseudonyms employed by the chameleonic artist-advertiser): Layout Technique in Advertising (Surrey, 1929b) and Copy Technique in Advertising (Surrey, 1930a). Richard Cavell (2002) has characterized these textbooks as comprising “an artistic credo” (p. 15). However, it was betts (2005) who first recognized that Layout Technique is primarily concerned with spatial concerns of the type studied by Cavell, whereas Copy Technique “organized its arguments around conceptions of time” (p. 247).This conceptual division of labour foreshadows the space/time dualism that structures the late communications writings of Harold Innis. Adam Lauder (2012) has explored the possibility that brooker may have served as an indirect influence on Innis. Yet, though Lauder briefly discussed the shared commitment to cultural continuity and the “oral tradition” disclosed by the writings of brooker and later Innis, the bulk of his analysis is devoted to an investigation of the possibility that the artist-advertiser’s visualizations (in the form of innovative charts and maps) may have contributed to the political economist’s early interest in the formative influence of geography on the development of a “staples” economy in Canada as well as his subsequent theorization of (neo-)colonial “monopolies of space.” Through original readings of articles published in Marketing and Printers’ Ink during the 1920s and early 1930s and other primary sources, this article proposes that brooker’s commercial writings and visual art alike mounted a critique of modernist space that cleared a path for Innis’ late “plea for time.” Paradoxically, the humanistic rhetoric of time deployed by both figures reveals striking parallels with the contemporary televisual paradigm critiqued, among others, by Richard dienst (1994). building on this recognition, the present article also traces some unintended consequences of the defence of time and the “oral tradition” articulated by brooker and later Innis. Brooker and Bergson: A literature review The first to observe a bergsonian inflection in brooker’s art was Joyce Zemans (1989). However, Zemans’ perception of “bergsonian flow” (p. 30) in canvases by the artistLauder Advertising and Visual Art of Bertram', 'corpus_id': 199484922, 'score': 1}, {'doc_id': '220363729', 'title': 'BézierSketch: A generative model for scalable vector sketches', 'abstract': 'The study of neural generative models of human sketches is a fascinating contemporary modeling problem due to the links between sketch image generation and the human drawing process. The landmark SketchRNN provided breakthrough by sequentially generating sketches as a sequence of waypoints. However this leads to low-resolution image generation, and failure to model long sketches. In this paper we present BezierSketch, a novel generative model for fully vector sketches that are automatically scalable and high-resolution. To this end, we first introduce a novel inverse graphics approach to stroke embedding that trains an encoder to embed each stroke to its best fit Bezier curve. This enables us to treat sketches as short sequences of paramaterized strokes and thus train a recurrent sketch generator with greater capacity for longer sketches, while producing scalable high-resolution results. We report qualitative and quantitative results on the Quick, Draw! benchmark.', 'corpus_id': 220363729, 'score': 0}, {'doc_id': '221352639', 'title': 'The politics of precarity', 'abstract': 'The adequacy of any theory of radical democracy requires that it thematize the social conditions within which an emancipatory politics might be enacted. Paul Apostolidis’s The Fight for Time offers a sustained reflection on how democratic politics is both frustrated and facilitated by widespread and increasing precarity. However, as this Critical Exchange demonstrates, the nature of precarity, the', 'corpus_id': 221352639, 'score': 0}, {'doc_id': '219632257', 'title': 'Diabolical dilemmas of COVID-19: An empirical study into Dutch society’s trade-offs between health impacts and other effects of the lockdown', 'abstract': 'We report and interpret preferences of a sample of the Dutch adult population for different strategies to end the so-called ‘intelligent lockdown’ which their government had put in place in response to the COVID-19 pandemic. Using a discrete choice experiment, we invited participants to make a series of choices between policy scenarios aimed at relaxing the lockdown, which were specified not in terms of their nature (e.g. whether or not to allow schools to re-open) but in terms of their effects along seven dimensions. These included health-related effects, but also impacts on the economy, education, and personal income. From the observed choices, we were able to infer the implicit trade-offs made by the Dutch between these policy effects. For example, we find that the average citizen, in order to avoid one fatality directly or indirectly related to COVID-19, is willing to accept a lasting lag in the educational performance of 18 children, or a lasting (>3 years) and substantial (>15%) reduction in net income of 77 households. We explore heterogeneity across individuals in terms of these trade-offs by means of latent class analysis. Our results suggest that most citizens are willing to trade-off health-related and other effects of the lockdown, implying a consequentialist ethical perspective. Somewhat surprisingly, we find that the elderly, known to be at relatively high risk of being affected by the virus, are relatively reluctant to sacrifice economic pain and educational disadvantages for the younger generation, to avoid fatalities. We also identify a so-called taboo trade-off aversion amongst a substantial share of our sample, being an aversion to accept morally problematic policies that simultaneously imply higher fatality numbers and lower taxes. We explain various ways in which our results can be of value to policy makers in the context of the COVID-19 and future pandemics.', 'corpus_id': 219632257, 'score': 0}, {'doc_id': '220602886', 'title': 'Perceptions of radiography students toward problem-based learning almost two decades after its introduction at Makerere University, Uganda', 'abstract': ""\n               Abstract\n               \n                  Introduction\n                  Problem-based learning (PBL) has been reported to be a valuable student-centred learning approach across the globe. In PBL students first encounter a problem, which triggers discussion, followed by student-centred inquiry. Makerere University College of Health Sciences has been using PBL for radiography students since 2002. Over the years, the learning landscape may have changed, including the significant disruption of learning by the coronavirus disease 2019 global pandemic. The study aimed at exploring the perceptions of undergraduate radiography students about the PBL curriculum at Makerere University almost two decades after its introduction.\n               \n               \n                  Methods\n                  This exploratory qualitative study involved 18 radiography students sampled purposively, from whom data were gathered using\xa0focus group discussions. Thematic analysis was subsequently used.\n               \n               \n                  Results\n                  Three key themes emerged from the data: (1) quality of teaching, (2) curriculum efficiency, and (3) curriculum expectations and rating. All students were generally positive about the curriculum. Most agreed that the curriculum was efficient to a greater extent and had met their expectations and desired objectives. Students, however, faced challenges; for example, with limited learning resources during the learning process.\n               \n               \n                  Conclusion\n                  This study highlights the significant role of PBL in enhancing student's problem-solving, critical thinking, literature search, and, most of all, their practical skills. Prioritization of teaching based on practical relevance and learning objectives is of great importance. The radiography students believed that their curriculum program was generally beneficial to them; however, it was affected by limited resources and limited availability of teaching personnel, which needs to be addressed.\n               \n            "", 'corpus_id': 220602886, 'score': 0}, {'doc_id': '14055899', 'title': 'The Secrets of Storytelling : Why We Love a Good Yarn', 'abstract': 'When Brad Pitt tells Eric Bana in the 2004 film Troy that “there are no pacts between lions and men,” he is not reciting a clever line from the pen of a Hollywood screenwriter. He is speaking Achillesʼ words in English as Homer wrote them in Greek more than 2,000 years ago in the Iliad. The tale of the Trojan War has captivated generations of audiences while evolving from its origins as an oral epic to written versions and, finally, to several film adaptations. The power of this story to transcend time, language and culture is clear even today, evidenced by Troyʼs robust success around the world.', 'corpus_id': 14055899, 'score': 1}, {'doc_id': '51785001', 'title': 'Gossip in Evolutionary Perspective', 'abstract': 'Conversation is a uniquely human phenomenon. Analyses of freely forming conversations indicate that approximately two thirds of conversation time is devoted to social topics, most of which can be given the generic label gossip. This article first explores the origins of gossip as a mechanism for bonding social groups, tracing these origins back to social grooming among primates. It then asks why social gossip in this sense should form so important a component of human interaction and presents evidence to suggest that, aside from servicing social networks, a key function may be related explicitly to controlling free riders. Finally, the author reviews briefly the role of social cognition in facilitating conversations of this kind.', 'corpus_id': 51785001, 'score': 1}, {'doc_id': '210440127', 'title': 'Stories That Open and Stories That Close: Theoretical and Clinical Narratives in Psychoanalysis', 'abstract': 'ABSTRACT Stories, narratives, actions over time reveal character, quirks, personal standards, human relational connections, social values, and even universal meanings! Stories do all this and then some, and good ones often cover many of these bases simultaneously. Good stories embody webs of experience that readers and listeners—and therapeutic partners—register on different levels and in different modes often at the same time. Good stories also shake our conventional assumptions and ways of thinking; they may shatter some adamantine shibboleths and open our minds and imaginations to new and future possibilities. These are the good stories, ones that are open to nuance and complexity and change, ones we try to create and cultivate in our analytic relationships. However, there are other stories, not such good ones, fixed and immovable stories, stuck in time and stultifying to growth and development. These are closed stories, and in this article I will explore varieties of therapeutic narratives that close down thinking and emotions, and I’ll speculate on their origins and remediation.', 'corpus_id': 210440127, 'score': 1}]"
150	{'doc_id': '201669150', 'title': 'Differentiable Product Quantization for End-to-End Embedding Compression', 'abstract': 'Embedding layers are commonly used to map discrete symbols into continuous embedding vectors that reflect their semantic meanings. Despite their effectiveness, the number of parameters in an embedding layer increases linearly with the number of symbols and poses a critical challenge on memory and storage constraints. In this work, we propose a generic and end-to-end learnable compression framework termed differentiable product quantization (DPQ). We present two instantiations of DPQ that leverage different approximation techniques to enable differentiability in end-to-end learning. Our method can readily serve as a drop-in alternative for any existing embedding layer. Empirically, DPQ offers significant compression ratios (14-238$\\times$) at negligible or no performance cost on 10 datasets across three different language tasks.', 'corpus_id': 201669150}	3345	"[{'doc_id': '195886317', 'title': 'Large Memory Layers with Product Keys', 'abstract': 'This paper introduces a structured memory which can be easily integrated into a neural network. The memory is very large by design and significantly increases the capacity of the architecture, by up to a billion parameters with a negligible computational overhead. Its design and access pattern is based on product keys, which enable fast and exact nearest neighbor search. The ability to increase the number of parameters while keeping the same computational budget lets the overall system strike a better trade-off between prediction accuracy and computation efficiency both at training and test time. This memory layer allows us to tackle very large scale language modeling tasks. In our experiments we consider a dataset with up to 30 billion words, and we plug our memory layer in a state-of-the-art transformer-based architecture. In particular, we found that a memory augmented model with only 12 layers outperforms a baseline transformer model with 24 layers, while being twice faster at inference time. We release our code for reproducibility purposes.', 'corpus_id': 195886317, 'score': 1}, {'doc_id': '55064675', 'title': 'TRANSLATION EQUATION AND SINCOV’S EQUATION – A HISTORICAL REMARK', 'abstract': 'Gottlob Frege (1848 – 1925), the world famous logician was also a pioneer in iteration theory. His habilitation thesis “Rechnungsmethoden, die sich auf eine Erweiterung des Grössenbegriffes gründen” (“Methods of Calculation based on an Extension of the Concept of Quantity”) published 1874 yields a foundation of iteration theory and dynamical systems in one and also in several variables. He considers there the translation equation f(s, f(t, x)) = f(s + t, x) and all the three so-called Aczél-Jabotinsky equations connected with the differentiable solutions of it. By this way Frege e.g. recognized also the importance of the infinitesimal generator of a dynamical system. A comprehensive presentation of this matter may be found in Gronau [4]. Frege treated in this connection also Sincov’s equation Ψ(z, x) = Ψ(z, y) + Ψ(y, x) and gave its general solution almost 30 years before Sincov. The history and background of Sincov’s equation is described in Gronau [5]. Here we give a detailed description of the connection between the translation equation and the Sincov equation.', 'corpus_id': 55064675, 'score': 0}, {'doc_id': '215814331', 'title': 'Highway Transformer: Self-Gating Enhanced Self-Attentive Networks', 'abstract': 'Self-attention mechanisms have made striking state-of-the-art (SOTA) progress in various sequence learning tasks, standing on the multi-headed dot product attention by attending to all the global contexts at different locations. Through a pseudo information highway, we introduce a gated component self-dependency units (SDU) that incorporates LSTM-styled gating units to replenish internal semantic importance within the multi-dimensional latent space of individual representations. The subsidiary content-based SDU gates allow for the information flow of modulated latent embeddings through skipped connections, leading to a clear margin of convergence speed with gradient descent algorithms. We may unveil the role of gating mechanism to aid in the context-based Transformer modules, with hypothesizing that SDU gates, especially on shallow layers, could push it faster to step towards suboptimal points during the optimization process.', 'corpus_id': 215814331, 'score': 0}, {'doc_id': '219530577', 'title': 'Linformer: Self-Attention with Linear Complexity', 'abstract': 'Large transformer models have shown extraordinary success in achieving state-of-the-art results in many natural language processing applications. However, training and deploying these models can be prohibitively costly for long sequences, as the standard self-attention mechanism of the Transformer uses $O(n^2)$ time and space with respect to sequence length. In this paper, we demonstrate that the self-attention mechanism can be approximated by a low-rank matrix. We further exploit this finding to propose a new self-attention mechanism, which reduces the overall self-attention complexity from $O(n^2)$ to $O(n)$ in both time and space. The resulting linear transformer, the \\textit{Linformer}, performs on par with standard Transformer models, while being much more memory- and time-efficient.', 'corpus_id': 219530577, 'score': 0}, {'doc_id': '211069202', 'title': 'Self-Assttentive Associative Memory', 'abstract': 'Heretofore, neural networks with external memory are restricted to single memory with lossy representations of memory interactions. A rich representation of relationships between memory pieces urges a high-order and segregated relational memory. In this paper, we propose to separate the storage of individual experiences (item memory) and their occurring relationships (relational memory). The idea is implemented through a novel Self-attentive Associative Memory (SAM) operator. Found upon outer product, SAM forms a set of associative memories that represent the hypothetical high-order relationships between arbitrary pairs of memory elements, through which a relational memory is constructed from an item memory. The two memories are wired into a single sequential model capable of both memorization and relational reasoning. We achieve competitive results with our proposed two-memory model in a diversity of machine learning tasks, from challenging synthetic problems to practical testbeds such as geometry, graph, reinforcement learning, and question answering.', 'corpus_id': 211069202, 'score': 1}, {'doc_id': '191029870', 'title': 'Not Art: An Action History of British Underground Cinema', 'abstract': ""NOT ART: AN ACTION IDSTORY OF BRITISH UNDERGROUND CINEMA My thesis is both an oppositional history and a (re)definition of British Underground Cinema culture (1959 2(02). The historical significance of Underground Cinema has long been ideologically entangled in a mesh of academic typologies and ultra leftist rhetoric, abducting it from those directly involved. The intention of my work is to return definition to the 'object' of study, to write from within. This process involves viewing the history of modem British culture not as a vague monolithic and hierarchic spectrum but rather as a distinct historical conflict between the repressive legitimate Art culture of the bourgeoisie and the radical illegitimate popular culture of the working class. In this context, Underground Cinema can be {re)defined as a radical hybrid culture which fused elements of popular culture, Counterculture and Anti-Art. However, the first wave of Underground Cinema was effectively suppressed by the irrational ideology of its key activists and the hegemonic power of the Art tradition. They disowned the radical popular and initiated an Avant-Garde/Independent cinema project which developed an official State administrated bourgeois alternative to popular cinema. My conclusion is that Underground Cinema still has the potential to become a radical and commercial popular culture but that this is now frustrated by an institutionalised State Art culture which has colonised the State funding agencies, higher education and the academic study of cinema. If the Underground is to flourish it must refuse and subvert this Art culture and renew its alliance with radical, experimental and commercial pop culture. My methodology is an holistic interactive praxis which combines research, writing, film/video making, digital design, performance and political activism. My final submission will be an open and heterodox mesh of polemic, history and entertainment. Its key components will be a written thesis which will locate this praxis within its intellectual context and a web site which will integrate my research and practice 1997-2003."", 'corpus_id': 191029870, 'score': 0}, {'doc_id': '219179819', 'title': 'SAN-M: Memory Equipped Self-Attention for End-to-End Speech Recognition', 'abstract': 'End-to-end speech recognition has become popular in recent years, since it can integrate the acoustic, pronunciation and language models into a single neural network. Among end-to-end approaches, attention-based methods have emerged as being superior. For example, Transformer, which adopts an encoder-decoder architecture. The key improvement introduced by Transformer is the utilization of self-attention instead of recurrent mechanisms, enabling both encoder and decoder to capture long-range dependencies with lower computational this http URL this work, we propose boosting the self-attention ability with a DFSMN memory block, forming the proposed memory equipped self-attention (SAN-M) mechanism. Theoretical and empirical comparisons have been made to demonstrate the relevancy and complementarity between self-attention and the DFSMN memory block. Furthermore, the proposed SAN-M provides an efficient mechanism to integrate these two modules. We have evaluated our approach on the public AISHELL-1 benchmark and an industrial-level 20,000-hour Mandarin speech recognition task. On both tasks, SAN-M systems achieved much better performance than the self-attention based Transformer baseline system. Specially, it can achieve a CER of 6.46% on the AISHELL-1 task even without using any external LM, comfortably outperforming other state-of-the-art systems.', 'corpus_id': 219179819, 'score': 1}, {'doc_id': '219401765', 'title': 'GMAT: Global Memory Augmentation for Transformers', 'abstract': 'Transformer-based models have become ubiquitous in natural language processing thanks to their large capacity, innate parallelism and high performance. The contextualizing component of a Transformer block is the $\\textit{pairwise dot-product}$ attention that has a large $\\Omega(L^2)$ memory requirement for length $L$ sequences, limiting its ability to process long documents. This has been the subject of substantial interest recently, where multiple approximations were proposed to reduce the quadratic memory requirement using sparse attention matrices. In this work, we propose to augment sparse Transformer blocks with a dense attention-based $\\textit{global memory}$ of length $M$ ($\\ll L$) which provides an aggregate global view of the entire input sequence to each position. Our augmentation has a manageable $O(M\\cdot(L+M))$ memory overhead, and can be seamlessly integrated with prior sparse solutions. Moreover, global memory can also be used for sequence compression, by representing a long input sequence with the memory representations only. We empirically show that our method leads to substantial improvement on a range of tasks, including (a) synthetic tasks that require global reasoning, (b) masked language modeling, and (c) reading comprehension.', 'corpus_id': 219401765, 'score': 1}, {'doc_id': '219401850', 'title': 'Funnel-Transformer: Filtering out Sequential Redundancy for Efficient Language Processing', 'abstract': 'With the success of language pretraining, it is highly desirable to develop more efficient architectures of good scalability that can exploit the abundant unlabeled data at a lower cost. To improve the efficiency, we examine the much-overlooked redundancy in maintaining a full-length token-level presentation, especially for tasks that only require a single-vector presentation of the sequence. With this intuition, we propose Funnel-Transformer which gradually compresses the sequence of hidden states to a shorter one and hence reduces the computation cost. More importantly, by re-investing the saved FLOPs from length reduction in constructing a deeper or wider model, we further improve the model capacity. In addition, to perform token-level predictions as required by common pretraining objectives, Funnel-Transformer is able to recover a deep representation for each token from the reduced hidden sequence via a decoder. Empirically, with comparable or fewer FLOPs, Funnel-Transformer outperforms the standard Transformer on a wide variety of sequence-level prediction tasks, including text classification, language understanding, and reading comprehension. The code and pretrained checkpoints are available at this https URL.', 'corpus_id': 219401850, 'score': 0}, {'doc_id': '211076318', 'title': 'Self-Attentive Associative Memory', 'abstract': 'Heretofore, neural networks with external memory are restricted to single memory with lossy representations of memory interactions. A rich representation of relationships between memory pieces urges a high-order and segregated relational memory. In this paper, we propose to separate the storage of individual experiences (item memory) and their occurring relationships (relational memory). The idea is implemented through a novel Self-attentive Associative Memory (SAM) operator. Found upon outer product, SAM forms a set of associative memories that represent the hypothetical high-order relationships between arbitrary pairs of memory elements, through which a relational memory is constructed from an item memory. The two memories are wired into a single sequential model capable of both memorization and relational reasoning. We achieve competitive results with our proposed two-memory model in a diversity of machine learning tasks, from challenging synthetic problems to practical testbeds such as geometry, graph, reinforcement learning, and question answering.', 'corpus_id': 211076318, 'score': 1}]"
151	{'doc_id': '232270003', 'title': 'Passenger-Centric Urban Air Mobility: Fairness Trade-Offs and Operational Efficiency', 'abstract': 'Urban Air Mobility (UAM) has the potential to revolutionize transportation. It will exploit the third dimension to help smooth ground traffic in densely populated areas. To be successful, it will require an integrated approach able to balance efficiency and safety while harnessing common resources and information. In this work we focus on future urban air-taxi services, and present the first methods and algorithms to efficiently operate air-taxi at scale. Our approach is twofold. First, we use a passenger-centric perspective which introduces traveling classes, and information sharing between transport modes to differentiate quality of services. This helps smooth multimodal journeys and increase passenger satisfaction. Second, we provide a flight routing and recharging solution which minimizes direct operational costs while preserving long term battery life through reduced energy-intense recharging. Our methods, which surpass the performance of a general state-of-the-art commercial solver, are also used to gain meaningful insights on the design space of the air-taxi problem, including solutions to hidden fairness issues.', 'corpus_id': 232270003}	16753	[{'doc_id': '233394096', 'title': 'Distributed Eco-Driving Algorithm of Vehicle Platoon Using Traffic Light and Road Slope Information', 'abstract': 'This paper investigates the problem of ecological driving (Eco-driving) of vehicle platoons. To reduce the probability of a platoon stopping at red lights and increase fuel efficiency, a two-layer control architecture is proposed. The first layer is in charge of optimizing the leader’s long-term speed profile using traffic light and road slope information. The second layer is short-term adaptation, in which the leader attempts to follow the planning speed profile in real time, while the follower keeps track of the nearest preceding vehicle and leader, to preserve the desired inter-vehicular distances. The longterm planning is formulated as a complex optimization problem with dynamic inequality constraints. An algorithm combining Pontryagins minimum principle and particle swarm optimization (PSO) is established to efficiently solve the long-term planning problem. The short-term adaptation is described as model predictive control (MPC) problems, of which the solutions are analytically designed. The effectiveness of the proposed algorithm is illustrated by the simulations.', 'corpus_id': 233394096, 'score': 0}, {'doc_id': '233473945', 'title': 'From Exception to Norm? Analysing Gated Housing in Sofia and Kiev since 1989', 'abstract': 'Thirty years after the fall of the Berlin wall in 1989 it is time to start a broader reflection on one of the most debated types of housing in Eastern European urban studies. Gated housing was almost unknown in the 1990s but started spreading rapidly at the end of the 2000s in different Eastern European and Post-Soviet countries. However, the 2008–2009 global financial crisis led to a sharp decrease in housing construction in general and of gated communities in particular. In recent years, housing construction as well as housing prices have increased again. Gated communities are part of this recovery. Instead of providing insights from a certain period of boom or crisis, this paper looks at three decades of housing production in general and gated communities in particular. It tries to uncover the institutional and economic background of housing development over the last 30 years. Moreover, it relates these developments to two (South)-Eastern European capital cities (Sofia and Kiev [Kyiv]) and their pathways of housing and gated community production. We focus on politico-economic and socio-spatial relationalities in these two different context conditions and scrutinise why and how gated communities emerged as well as how supply and demand changed over time. Both cases represent rather peripheral, capitalist economies concerning their national background. However, both cases are capital cities, which absorb the majority of capital investment. The polarisation and concentration of political and economic power structures lead us to discuss different actor-constellations regarding this on-going flight to privatopia, reflecting on the role of urban planning as well as glocal housing markets. Last but not least, this paper shows that gated communities are “urban assemblages” of wider processes of peripherialisation.', 'corpus_id': 233473945, 'score': 0}, {'doc_id': '232076248', 'title': 'An Intelligent Multi-Speed Advisory System using Improved Whale Optimisation Algorithm', 'abstract': 'An intelligent speed advisory system can be used to recommend speed for vehicles travelling in a given road network in cities. In this paper, we extend our previous work where a distributed speed advisory system has been devised to recommend an optimal consensus speed for a fleet of Internal Combustion Engine Vehicles (ICEVs) in a highway scenario. In particular, we propose a novel optimisation framework where the exact format of each vehicle’s cost function can be implicit, and our algorithm can be used to recommend multiple consensus speeds for vehicles travelling on different lanes in an urban highway scenario. Our studies show that the proposed scheme based on an improved whale optimisation algorithm can effectively reduce CO2 emission generated from ICEVs while providing different recommended speed options for groups of vehicles.', 'corpus_id': 232076248, 'score': 0}, {'doc_id': '228883412', 'title': 'Designing Supporting Structures of Passenger Ropeways of Minimum Cost Based on Modular Intermediate Towers of Discretely Variable Height', 'abstract': 'Passenger ropeways are a promising alternative for the development of public transport infrastructure in large cities. However, the construction of ropeways has a rather high cost and requires taking into account a significant number of restrictions associated with the features of the existing urban development and the placement of urban infrastructure. The main objective of this research is to develop optimization models that minimize the total cost of modular intermediate towers of a discretely variable height and a rope system due to the optimal placement and selection of the height of these towers, taking into account the features of the surface topography and urban development. The proposed modular principle for the construction of intermediate towers also enables the cost of construction to be further reduced. As a specific example, the design of a ropeway in the city of Bryansk, which has a complex terrain, is considered. The developed models are conveniently used at the initial stage of the design of the ropeway to compare the cost of various options for the location of the ropeway route in order to reduce the risk of error when choosing the least expensive option. The calculation results can serve as a guide for a preliminary assessment of the number and height of intermediate towers, their installation locations on the ground and the characteristics of the cable system.', 'corpus_id': 228883412, 'score': 1}, {'doc_id': '108966406', 'title': 'THE EMIRATES AIR LINE CABLE CAR AN INNOVATIVE, URBAN TRANSPORTATION SYSTEM', 'abstract': 'The Emirates Air Line is an innovative, urban and new transport link for London. It is the first urban cable car system of its kind in the UK, providing a unique river crossing linking two key destinations. The cable car is connecting with its 34 gondolas local communities, improves access to visitor destinations and speeds up river crossings. It will also encourage further regeneration at The Royal Docks and Greenwich Peninsula. Owned and operated is the cable car by Dockland Light Railway Limited (DLR) a part of the TfL network. Emirates is the Scheme Sponsor in a 10 years sponsorship deal. The operations is a unique partnership of world class companies. Each has committed to strive for the highest standards in its industry sector. The combined expertise of TfL, MaceMacro, Doppelmayr Cable Car, Continuum, CUK and Easy Clean is focused on making the Emirates Air Line a unique experience for London and not just a service.', 'corpus_id': 108966406, 'score': 1}, {'doc_id': '134412671', 'title': 'Application of a Detachable Gondola Ropeway in an Urban Transport Environment: New Orleans, La.', 'abstract': 'The first United States application of the detachable, monocable, gondola ropeway system as an urban transportation system is presented. The system is the Mississippi Aerial River Transit System (M. A. R. T. ) that crosses the Mississippi at New Orleans. Construction and operational aspects are discussed as are system costs.', 'corpus_id': 134412671, 'score': 1}, {'doc_id': '229703027', 'title': 'Urban Cableway Systems: State-of-art and analysis of the Emirates Air Line, London', 'abstract': 'Cableway systems are often associated with mountains and skiing. Nonetheless, they established in several cities for public transportation in the last years. This paper describes the state of the art in urban cable car systems and focuses especially on advances and experiences of planning processes in Germany. For this purpose, we conduct a systematic review of academic literature regarding urban ropeway systems, including analyses of the international context, the methodologies used and different perspectives on the topic in an international context. Additionally, we investigate the Emirates Air Line which operates since 2012 in London in order to identify the challenges for urban ropeway systems for Germany. The results indicate that urban ropeway systems have a potential to establish in public transportation in German cities, but still exist several obstacles that need to be settled.', 'corpus_id': 229703027, 'score': 1}, {'doc_id': '231992891', 'title': 'Urbanization in the Anthropocene: inaugural npj Urban Sustainability', 'abstract': 'This is the inaugural editorial of npj Urban Sustainability. This new journal seeks to inspire an evidence-based and globally oriented conversation on the importance of urban issues for our planet’s future. In that, npj Urban Sustainability will address some of the most challenging questions humanity has ever faced. It will investigate how urban regions around the world could radically transform to ensure global sustainability. The aim of the journal is to focus on how cities and their regions are reshaping to meet major economic, social, and environmental challenges, developing new pathways to sustainable urbanization. The journal has an intentionally very wide audience of urban scholars, policy makers, and practitioners and welcomes critical studies, comparative case studies, and impact studies, including policy solutions for existing cities and regions. At the onset, and of course leaving an open door to urban creativity and innovative city research, npj Urban Sustainability seeks to focus on three major drivers of the conversation on the global role of cities. First, while encouraging a global conversation on the place of urban processes in sustainability, we will encourage submissions from researchers studying urbanization in the Global South. It is now evident for many practitioners and scholars that much of the future of urbanization and its associated challenges, opportunities, and innovation will depend on the Global South. This is both because of the sheer importance of the urban trends emerging in these realities and also because of the innovations and perspectives that can come from Southern perspectives and investigations. Therefore, rapidly expanding knowledge for, of, and most importantly with the global south is an urgent imperative for sustainability science. Empirical and theoretical analyses of the implications of urban change in relation to inequality will be especially welcome. Second, the interplay of urbanization, climate change, and the biodiversity crisis are also highlighted themes for us. Urban systems, because of their concentration of people and infrastructure, create risks to disturbances from climate change or other challenges. Additionally, the design of cities concentrates heat, traps air pollutants, and creates a number of environmental problems that affect human health and well-being in cities. Urban systems cannot be themselves fully sustainable and are rather dependent on other systems and especially rural landscapes to provide natural resources, food, energy, and water that are the raw materials for human societies on an urban planet. Global environmental changes, such as global biodiversity decline, accelerating climate change, depleting soil productivity for food production, and more, have knock-on impacts for cities and continued urbanization. Third, we also strongly encourage submission of papers on the role of data and innovation in urban research. This is both from the perspective of integration of innovative technology and solutions that emerge from collection and analyses of urban data and case studies, including both large and fine-grained spatial and temporal data, as well as from case studies that seek to recast the boundaries of urban research for planetary sustainability. These three strands of research, and other discussions within the remit of the journal, come however at a particular political–economic juncture. npj Urban Sustainability aims to not only present the latest science to drive sustainable urbanization but also it wants to do so explicitly in dialogue with today’s most pressing international agendas. In doing so, it welcomes interventions that intersect with a key question: where are we at with global urban governance? A world of cities needs a world politics fit for those cities—a key conversation that pits the contemporary multilateral system in dialogue with, if not at times against, cities and urban processes. We also encourage submission of papers that make critical reflections on what has happened since the decisions were taken on the United Nations Sustainable Development Goals (SDGs), and their explicit acknowledgement of urban issues, and New Urban Agenda. Here we specifically prioritize research related to the SDG 11 (make cities and human settlements inclusive, safe, resilient, and sustainable), and how this goal interacts with the other SDGs in urbanization processes. However, we aim to do so by taking a cross-sectorial view of sustainability that does not disregard or overshadow other thematic global agendas in their potential to uplift the sustainable future of an urban planet. This first inaugural issue has the theme “Urbanization in the Anthropocene.” It is introduced by a Comment setting out some of the dimensions of how urbanization may not only accelerate the global processes underlying the Anthropocene but also how cities and urban regions may be a positive force in contributing to a “good” Anthropocene. The set of papers highlight both challenges and opportunities of cities and urban regions in forming a more resilient sustainable future addressing demography (Kii et al.), biodiversity (Oke et al.), climate change (Solecki et al., Norman et al.), infrastructure (Chester et al.) particularly water (Derrible et al.), and how to mobilize knowledge and science for sustainability (Zhou et al.), giving perspectives on coproduction in data-scarce circumstances in the global south (Croese et al.). In the coming issues, we will strive to encourage broad perspectives on localized implementation of a number of global policy initiatives, either from the perspective of the overall 2030 Agenda of the United Nations or within specific domain statements like the Sendai Framework on disaster risk reduction, the Addis Ababa Action Agenda on financing sustainable development, or indeed the Paris Agreement on climate change. In fact, the launch of npj Urban Sustainability comes at a time where consideration of urban issues in global agendas is again of possibly pivotal importance. It follows not only a “cities” recognition by the UN Framework Convention on Climate Change and Intergovernmental Panel on Climate Change but also similar movements in global health and migration, and it foreshadows a growing push for an equal recognition within the realm of global biodiversity governance. How has the UN system so far dealt with the urban age? What are the strategic plans for the future? What institutional reforms are needed? How have nations and cities progressed? What are shortand long-term consequences of coronavirus disease 2019 on urban development? What are the significant examples of successfully initiated implementation processes? What have been the main obstacles and barriers to www.nature.com/npjUrbanSustain', 'corpus_id': 231992891, 'score': 0}, {'doc_id': '140579453', 'title': 'Ngong Ping 360: Hong Kong’s state-of-the-art cable car', 'abstract': 'Cable cars are increasingly being seen as sustainable mass-transit solutions due to minimal emissions, short waits, low costs and rapid construction. One of the most recent examples is the 5·7 km long Ngong Ping 360 ropeway on Lantau Island in Hong Kong, the largest bi-cable circulating detachable cable car system in the world. Completed with the help of mules and helicopters in September 2006, it links Tung Chung new town next to Hong Kong International airport with the popular Ngong Ping tourist village, home of the Tian Tan Buddha. The system has the second highest transport capacity in the world, with 3500 passengers per hour each way.', 'corpus_id': 140579453, 'score': 1}, {'doc_id': '232269941', 'title': 'Enabling Opportunistic Low-cost Smart Cities By Using Tactical Edge Node Placement', 'abstract': 'Smart city projects aim to enhance the management of city infrastructure by enabling government entities to monitor, control and maintain infrastructure efficiently through the deployment of Internet-of-things (IoT) devices. However, the financial burden associated with smart city projects is a detriment to prospective smart cities. A noteworthy factor that impacts the cost and sustainability of smart city projects is providing cellular Internet connectivity to IoT devices. In response to this problem, this paper explores the use of public transportation network nodes and mules, such as bus-stops as buses, to facilitate connectivity via device-to-device communication in order to reduce cellular connectivity costs within a smart city. The data mules convey non-urgent data from IoT devices to edge computing hardware, where data can be processed or sent to the cloud. Consequently, this paper focuses on edge node placement in smart cities that opportunistically leverage public transit networks for reducing reliance on and thus costs of cellular connectivity. We introduce an algorithm that selects a set of edge nodes that provides maximal sensor coverage and explore another that selects a set of edge nodes that provide minimal delivery delay within a budget. The algorithms are evaluated for two public transit network data-sets: Chapel Hill, North Carolina and Louisville, Kentucky. Results show that our algorithms consistently outperform edge node placement strategies that rely on traditional centrality metrics (betweenness and in-degree centrality) by over 77% reduction in coverage budget and over 20 minutes reduction in latency.', 'corpus_id': 232269941, 'score': 0}]
152	{'doc_id': '221995420', 'title': 'Explainable AI without Interpretable Model', 'abstract': 'Explainability has been a challenge in AI for as long as AI has existed. With the recently increased use of AI in society, it has become more important than ever that AI systems would be able to explain the reasoning behind their results also to end-users in situations such as being eliminated from a recruitment process or having a bank loan application refused by an AI system. Especially if the AI system has been trained using Machine Learning, it tends to contain too many parameters for them to be analysed and understood, which has caused them to be called ‘black-box’ systems. Most Explainable AI (XAI) methods are based on extracting an interpretable model that can be used for producing explanations. However, the interpretable model does not necessarily map accurately to the original black-box model. Furthermore, the understandability of interpretable models for an end-user remains questionable. The notions of Contextual Importance and Utility (CIU) presented in this paper make it possible to produce human-like explanations of black-box outcomes directly, without creating an interpretable model. Therefore, CIU explanations map accurately to the black-box model itself. CIU is completely model-agnostic and can be used with any black-box system. In addition to feature importance, the utility concept that is well-known in Decision Theory provides a new dimension to explanations compared to most existing XAI methods. Finally, CIU can produce explanations at any level of abstraction and using different vocabularies and other means of interaction, which makes it possible to adjust explanations and interaction according to the context and to the target users.', 'corpus_id': 221995420}	10128	"[{'doc_id': '221471587', 'title': 'Explainable Online Validation of Machine Learning Models for Practical Applications', 'abstract': 'We present a reformulation of the regression and classification, which aims to validate the result of a machine learning algorithm. Our reformulation simplifies the original problem and validates the result of the machine learning algorithm using the training data. Since the validation of machine learning algorithms must always be explainable, we perform our experiments with the kNN algorithm as well as with an algorithm based on conditional probabilities, which is proposed in this work. For the evaluation of our approach, three publicly available data sets were used and three classification and two regression problems were evaluated. The presented algorithm based on conditional probabilities is also online capable and requires only a fraction of memory compared to the kNN algorithm.', 'corpus_id': 221471587, 'score': 0}, {'doc_id': '221970137', 'title': 'Landscape of R packages for eXplainable Artificial Intelligence', 'abstract': 'The growing availability of data and computing power fuels the development of predictive models. In order to ensure the safe and effective functioning of such models, we need methods for exploration, debugging, and validation. New methods and tools for this purpose are being developed within the eXplainable Artificial Intelligence (XAI) subdomain of machine learning. In this work (1) we present the taxonomy of methods for model explanations, (2) we identify and compare 27 packages available in R to perform XAI analysis, (3) we present an example of an application of particular packages, (4) we acknowledge recent trends in XAI. The article is primarily devoted to the tools available in R, but since it is easy to integrate the Python code, we will also show examples for the most popular libraries from Python.', 'corpus_id': 221970137, 'score': 0}, {'doc_id': '221319399', 'title': 'Making Neural Networks Interpretable with Attribution: Application to Implicit Signals Prediction', 'abstract': 'Explaining recommendations enables users to understand whether recommended items are relevant to their needs and has been shown to increase their trust in the system. More generally, if designing explainable machine learning models is key to check the sanity and robustness of a decision process and improve their efficiency, it however remains a challenge for complex architectures, especially deep neural networks that are often deemed ”black-box”. In this paper, we propose a novel formulation of interpretable deep neural networks for the attribution task. Differently to popular post-hoc methods, our approach is interpretable by design. Using masked weights, hidden features can be deeply attributed, split into several input-restricted sub-networks and trained as a boosted mixture of experts. Experimental results on synthetic data and real-world recommendation tasks demonstrate that our method enables to build models achieving close predictive performances to their non-interpretable counterparts, while providing informative attribution interpretations.', 'corpus_id': 221319399, 'score': 0}, {'doc_id': '221470230', 'title': 'Explainable Empirical Risk Minimization', 'abstract': 'The widespread use of modern machine learning methods in decision making crucially depends on their interpretability or explainability. The human users (decision makers) of machine learning methods are often not only interested in getting accurate predictions or projections. Rather, as a decision-maker, the user also needs a convincing answer (or explanation) to the question of why a particular prediction was delivered. Explainable machine learning might be a legal requirement when used for decision making with an immediate effect on the health of human beings. As an example consider the computer vision of a self-driving car whose predictions are used to decide if to stop the car. We have recently proposed an information-theoretic approach to construct personalized explanations for predictions obtained from ML. This method was model-agnostic and only required some training samples of the model to be explained along with a user feedback signal. This paper uses an information-theoretic measure for the quality of an explanation to learn predictors that are intrinsically explainable to a specific user. Our approach is not restricted to a particular hypothesis space, such as linear maps or shallow decision trees, whose predictor maps are considered as explainable by definition. Rather, we regularize an arbitrary hypothesis space using a personalized measure for the explainability of a particular predictor.', 'corpus_id': 221470230, 'score': 0}, {'doc_id': '16366369', 'title': 'Explaining instance classifications with interactions of subsets of feature values', 'abstract': ""In this paper, we present a novel method for explaining the decisions of an arbitrary classifier, independent of the type of classifier. The method works at the instance level, decomposing the model's prediction for an instance into the contributions of the attributes' values. We use several artificial data sets and several different types of models to show that the generated explanations reflect the decision-making properties of the explained model and approach the concepts behind the data set as the prediction quality of the model increases. The usefulness of the method is justified by a successful application on a real-world breast cancer recurrence prediction problem."", 'corpus_id': 16366369, 'score': 1}, {'doc_id': '224818450', 'title': 'Counterfactual Explanations for Machine Learning: A Review', 'abstract': 'Machine learning plays a role in many deployed decision systems, often in ways that are difficult or impossible to understand by human stakeholders. Explaining, in a human-understandable way, the relationship between the input and output of machine learning models is essential to the development of trustworthy machine-learning-based systems. A burgeoning body of research seeks to define the goals and methods of explainability in machine learning. In this paper, we seek to review and categorize research on counterfactual explanations, a specific class of explanation that provides a link between what could have happened had input to a model been changed in a particular way. Modern approaches to counterfactual explainability in machine learning draw connections to the established legal doctrine in many countries, making them appealing to fielded systems in high-impact areas such as finance and healthcare. Thus, we design a rubric with desirable properties of counterfactual explanation algorithms and comprehensively evaluate all currently-proposed algorithms against that rubric. Our rubric provides easy comparison and comprehension of the advantages and disadvantages of different approaches and serves as an introduction to major research themes in this field. We also identify gaps and discuss promising research directions in the space of counterfactual explainability.', 'corpus_id': 224818450, 'score': 0}, {'doc_id': '221640901', 'title': 'Accurate and Intuitive Contextual Explanations using Linear Model Trees', 'abstract': 'With the ever-increasing use of complex machine learning models in critical applications within the finance domain, explaining the decisions of the model has become a necessity. With applications spanning from credit scoring to credit marketing, the impact of these models is undeniable. Among the multiple ways in which one can explain the decisions of these complicated models, local post hoc model agnostic explanations have gained massive adoption. These methods allow one to explain each prediction independent of the modelling technique that was used while training. As explanations, they either give individual feature attributions or provide sufficient rules that represent conditions for a prediction to be made. The current state of the art methods use rudimentary methods to generate synthetic data around the point to be explained. This is followed by fitting simple linear models as surrogates to obtain a local interpretation of the prediction. In this paper, we seek to significantly improve on both, the method used to generate the explanations and the nature of explanations produced. We use a Generative Adversarial Network for synthetic data generation and train a piecewise linear model in the form of Linear Model Trees to be used as the surrogate this http URL addition to individual feature attributions, we also provide an accompanying context to our explanations by leveraging the structure and property of our surrogate model.', 'corpus_id': 221640901, 'score': 1}, {'doc_id': '17511218', 'title': 'Explanation and Reliability of Individual Predictions', 'abstract': 'Classification and regression models, either automatically generated from data by machine learning algorithms, or manually encoded with the help of domain experts, are daily used to predict the labels of new instances. Each such individual prediction, in order to be accepted/trusted by users, should be accompanied by an explanation of the prediction as well as by an estimate of its reliability. We have recently developed a general methodology for explaining individual predictions as well as for estimating their reliability. Both, explanation and reliability estimation are general techniques, independent of the underlying model and provide on-line (effective and efficient) support to the users of prediction models.', 'corpus_id': 17511218, 'score': 1}, {'doc_id': '14547367', 'title': 'Explaining Classifications For Individual Instances', 'abstract': ""We present a method for explaining predictions for individual instances. The presented approach is general and can be used with all classification models that output probabilities. It is based on the decomposition of a model's predictions on individual contributions of each attribute. Our method works for the so-called black box models such as support vector machines, neural networks, and nearest neighbor algorithms, as well as for ensemble methods such as boosting and random forests. We demonstrate that the generated explanations closely follow the learned models and present a visualization technique that shows the utility of our approach and enables the comparison of different prediction methods."", 'corpus_id': 14547367, 'score': 1}, {'doc_id': '14664111', 'title': 'How to Explain Individual Classification Decisions', 'abstract': 'After building a classifier with modern tools of machine learning we typically have a black box at hand that is able to predict well for unseen data. Thus, we get an answer to the question what is the most likely label of a given unseen data point. However, most methods will provide no answer why the model predicted a particular label for a single instance and what features were most influential for that particular instance. The only method that is currently able to provide such explanations are decision trees. This paper proposes a procedure which (based on a set of assumptions) allows to explain the decisions of any classification method.', 'corpus_id': 14664111, 'score': 1}]"
153	{'doc_id': '237578806', 'title': 'Approaches to Uncertainty Quantification in Federated Deep Learning', 'abstract': 'Trustworthy machine learning allows data privacy and a robust assessment of the uncertainty of predictions. Methods for quantifying uncertainty in deep learning have recently gained attention, while federated deep learning allows to utilize distributed data sources in a privacy-preserving manner. In this paper, we integrate several approaches for uncertainty quantification in federated deep learning. In particular, we show that prominent approaches such as MC-dropout and stochastic weight averaging Gaussian (SWAG) can be extended efficiently to federated setup. Moreover, we demonstrate that deep ensembles allow for natural integration in the federated learning framework. Our empirical evaluation confirms that a trustworthy uncertainty quantification on out-ofdistribution data is possible in federated learning with little (SWAG) to no (MC-dropout, ensembles) additional communication. While all methods perform well in our empirical analysis and should serve as baselines in future developments in this field, deep ensembles and MC-dropout allow for better uncertainty based identification of out-of-distribution data and wrong classified data.', 'corpus_id': 237578806}	20701	"[{'doc_id': '237561327', 'title': 'Training-Free Uncertainty Estimation for Dense Regression: Sensitivity as a Surrogate', 'abstract': 'Uncertainty estimation is an essential step in the evaluation of the robustness for deep learning models in computer vision, especially when applied in risk-sensitive areas. However, most stateof-the-art deep learning models either fail to obtain uncertainty estimation or need significant modification (e.g., formulating a proper Bayesian treatment) to obtain it. Most previous methods are not able to take an arbitrary model off the shelf and generate uncertainty estimation without retraining or redesigning it. To address this gap, we perform a systematic exploration into trainingfree uncertainty estimation for dense regression, an unrecognized yet important problem, and provide a theoretical construction justifying such estimations. We propose three simple and scalable methods to analyze the variance of outputs from a trained network under tolerable perturbations: infer-transformation, infer-noise, and inferdropout. They operate solely during the inference, without the need to re-train, re-design, or fine-tune the models, as typically required by state-of-theart uncertainty estimation methods. Surprisingly, even without involving such perturbations in training, our methods produce comparable or even better uncertainty estimation when compared to training-required state-of-the-art methods.', 'corpus_id': 237561327, 'score': 1}, {'doc_id': '236995859', 'title': 'PremiUm-CNN: Propagating Uncertainty Towards Robust Convolutional Neural Networks', 'abstract': ""Deep neural networks (DNNs) have surpassed human-level accuracy in various learning tasks. However, unlike humans who have a natural cognitive intuition for probabilities, DNNs cannot express their uncertainty in the output decisions. This limits the deployment of DNNs in mission-critical domains, such as warfighter decision-making or medical diagnosis. Bayesian inference provides a principled approach to reason about model's uncertainty by estimating the posterior distribution of the unknown parameters. The challenge in DNNs remains the multi-layer stages of non-linearities, which make the propagation of high-dimensional distributions mathematically intractable. This paper establishes the theoretical and algorithmic foundations of uncertainty or belief propagation by developing new deep learning models named PremiUm-CNNs (Propagating Uncertainty in Convolutional Neural Networks). We introduce a tensor normal distribution as a prior over convolutional kernels and estimate the variational posterior by maximizing the evidence lower bound (ELBO). We start by deriving the first-order mean-covariance propagation framework. Later, we develop a framework based on the unscented transformation (correct at least up to the second-order) that propagates sigma points of the variational distribution through layers of a CNN. The propagated covariance of the predictive distribution captures uncertainty in the output decision. Comprehensive experiments conducted on diverse benchmark datasets demonstrate: 1) superior robustness against noise and adversarial attacks, 2) self-assessment through predictive uncertainty that increases quickly with increasing levels of noise or attacks, and 3) an ability to detect a targeted attack from ambient noise."", 'corpus_id': 236995859, 'score': 1}, {'doc_id': '237416610', 'title': 'MACEst: The reliable and trustworthy Model Agnostic Confidence Estimator', 'abstract': 'Reliable Confidence Estimates are hugely important for any machine learning model to be truly useful. In this paper we argue that any confidence estimates based upon standard machine learning point prediction algorithms are fundamentally flawed and under situations with a large amount of epistemic uncertainty are likely to be untrustworthy. To address these issues, we present MACEst, a Model Agnostic Confidence Estimator, which provides reliable and trustworthy confidence estimates. The algorithm differs from current methods by estimating confidence independently as a local quantity which explicitly accounts for both aleatoric and epistemic uncertainty. This approach differs from standard calibration methods that use a global point prediction model as a starting point for the confidence estimate.', 'corpus_id': 237416610, 'score': 1}, {'doc_id': '237258160', 'title': 'Efficient Calculation of Adversarial Examples for Bayesian Neural Networks', 'abstract': 'Calculating adversarial examples for Bayesian neural networks is cumbersome. Due to the inherent stochasticity, the gradient of the network can only be reliable approximated by sampling multiple times from the posterior, leading to a greatly increased computational cost. In this paper we propose to efficiently attack Bayesian neural networks with adversarial examples calculated for a deterministic network with parameters given by the mean of the posterior distribution. We show in a series of experiments, that the proposed approach can be used to effectively attack Bayesian neural networks while using 4.2 times less of the resources of existing adversarial example estimation methods with comparable strength. We demonstrate that this is especially helpful during adversarial training, when multiple different model configuration need to be evaluated.', 'corpus_id': 237258160, 'score': 0}, {'doc_id': '237581126', 'title': 'Bayesian Confidence Calibration for Epistemic Uncertainty Modelling', 'abstract': 'Modern neural networks have found to be miscalibrated in terms of confidence calibration, i.e., their predicted confidence scores do not reflect the observed accuracy or precision. Recent work has introduced methods for post-hoc confidence calibration for classification as well as for object detection to address this issue. Especially in safety critical applications, it is crucial to obtain a reliable self-assessment of a model. But what if the calibration method itself is uncertain, e.g., due to an insufficient knowledge base? We introduce Bayesian confidence calibration a framework to obtain calibrated confidence estimates in conjunction with an uncertainty of the calibration method. Commonly, Bayesian neural networks (BNN) are used to indicate a network’s uncertainty about a certain prediction. BNNs are interpreted as neural networks that use distributions instead of weights for inference. We transfer this idea of using distributions to confidence calibration. For this purpose, we use stochastic variational inference to build a calibration mapping that outputs a probability distribution rather than a single calibrated estimate. Using this approach, we achieve state-of-the-art calibration performance for object detection calibration. Finally, we show that this additional type of uncertainty can be used as a sufficient criterion for covariate shift detection. All code is open source and available at https://github.com/EFS-OpenSource/calibrationframework.', 'corpus_id': 237581126, 'score': 0}, {'doc_id': '237951325', 'title': 'Ensembling as Approximate Bayesian Inference for Predictive Uncertainty Estimation in Deep Learning', 'abstract': 'We view ensembling as an approximate Bayesian inference method, justify why it should be a reasonable approximation for Deep Neural Networks and extensively compare it with other approximate methods in terms of predictive uncertainty estimation quality. We provide experimental results on illustrative toy problems and the real-world computer vision tasks of street-scene semantic segmentation and depth completion. This extended abstract describes preliminary results from ongoing work intended for NeurIPS 2019.', 'corpus_id': 237951325, 'score': 1}, {'doc_id': '236957383', 'title': 'Monte Carlo DropBlock for Modelling Uncertainty in Object Detection', 'abstract': 'With the advancements made in deep learning, computer vision problems like object detection and segmentation have seen a great improvement in performance. However, in many real-world applications such as autonomous driving vehicles, the risk associated with incorrect predictions of objects is very high. Standard deep learning models for object detection such as YOLO models are often overconfident in their predictions and do not take into account the uncertainty in predictions on out-of-distribution data. In this work, we propose an efficient and effective approach to model uncertainty in object detection and segmentation tasks using Monte-Carlo DropBlock (MC-DropBlock) based inference. The proposed approach applies drop-block during training time and test time on the convolutional layer of the deep learning models such as YOLO. We show that this leads to a Bayesian convolutional neural network capable of capturing the epistemic uncertainty in the model. Additionally, we capture the aleatoric uncertainty using a Gaussian likelihood. We demonstrate the effectiveness of the proposed approach on modeling uncertainty in object detection and segmentation tasks using out-of-distribution experiments. Experimental results show that MC-DropBlock improves the generalization, calibration, and uncertainty modeling capabilities of YOLO models in object detection and segmentation.', 'corpus_id': 236957383, 'score': 1}, {'doc_id': '237940579', 'title': 'Training on Test Data with Bayesian Adaptation for Covariate Shift', 'abstract': 'When faced with distribution shift at test time, deep neural networks often make inaccurate predictions with unreliable uncertainty estimates. While improving the robustness of neural networks is one promising approach to mitigate this issue, an appealing alternate to robustifying networks against all possible test-time shifts is to instead directly adapt them to unlabeled inputs from the particular distribution shift we encounter at test time. However, this poses a challenging question: in the standard Bayesian model for supervised learning, unlabeled inputs are conditionally independent of model parameters when the labels are unobserved, so what can unlabeled data tell us about the model parameters at test-time? In this paper, we derive a Bayesian model that provides for a well-defined relationship between unlabeled inputs under distributional shift and model parameters, and show how approximate inference in this model can be instantiated with a simple regularized entropy minimization procedure at test-time. We evaluate our method on a variety of distribution shifts for image classification, including image corruptions, natural distribution shifts, and domain adaptation settings, and show that our method improves both accuracy and uncertainty estimation.', 'corpus_id': 237940579, 'score': 0}, {'doc_id': '237102679', 'title': 'Laplace Approximation for Uncertainty Estimation of Deep Neural Networks', 'abstract': 'Deep learning is at the frontier of machine learning and automation, using deep neural networks as the main workhorse, which have revolutionized the way we extract information from large amounts of data in computer vision, natural language processing and other domains. Moving from purely academic to real world scenarios has renewed interest into the way those powerful algorithms draw their conclusions and how to quantify the quality of their predictions beyond mere accuracy. From a practitioner’s point of view, the most important information to obtain alongside the algorithm’s prediction is the uncertainty attached to it, which is the basis for an accurate assessment of confidence. Unfortunately, extracting this information from large models and datasets has proven to be difficult. A common approach so far has been to devise a method that makes as many approximations as necessary to render the problem tractable while still yielding at least somewhat useful uncertainty estimates. This work looks at the problem from a slightly different angle: By first choosing a tractable and comparatively simple method, the burden then lies on the model design to lend itself to the chosen approximation. To this end, the most popular deep neural network architectures are compared based on their compliance to uncertainty estimation by Laplace approximation, assessing empirically the methods potentials and deficiencies as well as its applicability to large models and datasets while working towards an understanding how architectural choices correlate with the quality of obtained uncertainty estimates.', 'corpus_id': 237102679, 'score': 1}, {'doc_id': '238198075', 'title': '$f$-Cal: Calibrated aleatoric uncertainty estimation from neural networks for robot perception', 'abstract': 'While modern deep neural networks are performant perception modules, performance (accuracy) alone is insufficient, particularly for safety-critical robotic applications such as self-driving vehicles. Robot autonomy stacks also require these otherwise blackbox models to produce reliable and calibrated measures of confidence on their predictions. Existing approaches estimate uncertainty from these neural network perception stacks by modifying network architectures, inference procedure, or loss functions. However, in general, these methods lack calibration, meaning that the predictive uncertainties do not faithfully represent the true underlying uncertainties (process noise). Our key insight is that calibration is only achieved by imposing constraints across multiple examples, such as those in a mini-batch; as opposed to existing approaches which only impose constraints per-sample, often leading to overconfident (thus miscalibrated) uncertainty estimates. By enforcing the distribution of outputs of a neural network to resemble a target distribution by minimizing an f -divergence, we obtain significantly better-calibrated models compared to prior approaches. Our approach, f -Cal, outperforms existing uncertainty calibration approaches on robot perception tasks such as object detection and monocular depth estimation over multiple real-world benchmarks.', 'corpus_id': 238198075, 'score': 0}]"
154	{'doc_id': '232196200', 'title': 'T-cell dysfunction in chronic lymphocytic leukemia from an epigenetic perspective.', 'abstract': 'Cellular immunotherapeutic approaches such as chimeric antigen receptor (CAR) T-cell therapy in chronic lymphocytic leukemia (CLL) thus far have not met the high expectations. Therefore it is essential to better understand the molecular mechanisms of CLLinduced T-cell dysfunction. Even though a significant number of studies are available on T-cell function and dysfunction in CLL patients, none examine dysfunction at the epigenomic level. In non-malignant T-cell research, epigenomics is widely employed to define the differentiation pathway into T-cell exhaustion. Additionally, metabolic restrictions in the tumor microenvironment that cause T-cell dysfunction are often mediated by epigenetic changes. With this review paper we argue that understanding the epigenetic (dys)regulation in T cells of CLL patients should be leveled to the knowledge we currently have of the neoplastic B cells themselves. This will permit a complete understanding of how these immune cell interactions regulate T- and B-cell function. Here we relate the cellular and phenotypic characteristics of CLL-induced T-cell dysfunction to epigenetic studies of T-cell regulation emerging from chronic viral infection and tumor models. This paper proposes a framework for future studies into the epigenetic regulation of CLL-induced Tcell dysfunction, knowledge that will help to guide improvements in the utility of autologous T-cell based therapies in CLL.', 'corpus_id': 232196200}	15149	[{'doc_id': '219589227', 'title': 'Met-Flow, a strategy for single-cell metabolic analysis highlights dynamic changes in immune subpopulations', 'abstract': 'A complex interaction of anabolic and catabolic metabolism underpins the ability of leukocytes to mount an immune response. Their capacity to respond to changing environments by metabolic reprogramming is crucial to effector function. However, current methods lack the ability to interrogate this network of metabolic pathways at single-cell level within a heterogeneous population. We present Met-Flow, a flow cytometry-based method capturing the metabolic state of immune cells by targeting key proteins and rate-limiting enzymes across multiple pathways. We demonstrate the ability to simultaneously measure divergent metabolic profiles and dynamic remodeling in human peripheral blood mononuclear cells. Using Met-Flow, we discovered that glucose restriction and metabolic remodeling drive the expansion of an inflammatory central memory T cell subset. This method captures the complex metabolic state of any cell as it relates to phenotype and function, leading to a greater understanding of the role of metabolic heterogeneity in immune responses. Patricia Ahl et al. present Met-Flow, a flow cytometry-based approach for capturing the metabolic state of immune cells across multiple pathways. The authors apply Met-Flow to a central memory T cell subset, showing the importance of glucose restriction and metabolic reprogramming to T cell polarization and expansion.', 'corpus_id': 219589227, 'score': 1}, {'doc_id': '149455657', 'title': 'Chronic lymphocytic leukemia cells impair mitochondrial fitness in CD8+ T cells and impede CAR T cell efficacy.', 'abstract': 'In chronic lymphocytic leukemia (CLL), acquired T cell dysfunction impedes development of effective immunotherapeutic strategies, through yet unresolved mechanisms. We have previously shown that CD8+ T cells in CLL exhibit impaired activation and reduced glucose uptake after stimulation. CD8+ T cells in CLL patients are chronically exposed to leukemic B cells, which potentially impacts metabolic homeostasis resulting in aberrant metabolic reprogramming upon stimulation. Here we report that resting CD8+ T cells in CLL have reduced intracellular GLUT1 reserves, and have an altered mitochondrial metabolic profile as displayed by increased mitochondrial respiration, membrane potential, and levels of reactive oxygen species. This coincided with decreased levels of PGC-1α, and in line with that, CLL-derived CD8+ T cells showed impaired mitochondrial biogenesis upon stimulation. In search of a therapeutic correlate of these findings, we analyzed mitochondrial biogenesis in CD19-directed chimeric antigen receptor (CAR) CD8+ T cells prior to infusion in CLL patients (enrolled in NCT01747486 and NCT01029366, https://clinicaltrials.gov). Interestingly, in cases with a subsequent complete response, the infused CD8+ CAR T cells had increased mitochondrial mass compared to non-responders, which positively correlated with the expansion and persistence of CAR T cells. Our findings demonstrate that GLUT1 reserves and mitochondrial fitness of CD8+ T cells are impaired in CLL. Therefore, boosting mitochondrial biogenesis in CAR T cells might improve the efficacy of CAR T cell therapy and other emerging cellular immunotherapies. Please note: the May 10, 2019 Blood First Edition publication of this paper included an incorrect version of the abstract. The correct abstract appears above.', 'corpus_id': 149455657, 'score': 1}, {'doc_id': '233463029', 'title': 'Altered Metabolism in Glioblastoma: Myeloid-Derived Suppressor Cell (MDSC) Fitness and Tumor-Infiltrating Lymphocyte (TIL) Dysfunction', 'abstract': 'The metabolism of glioblastoma (GBM), the most aggressive and lethal primary brain tumor, is flexible and adaptable to different adverse conditions, such as nutrient deprivation. Beyond glycolysis, altered lipid metabolism is implicated in GBM progression. Indeed, metabolic subtypes were recently identified based on divergent glucose and lipid metabolism. GBM is also characterized by an immunosuppressive microenvironment in which myeloid-derived suppressor cells (MDSCs) are a powerful ally of tumor cells. Increasing evidence supports the interconnection between GBM and MDSC metabolic pathways. GBM cells exert a crucial contribution to MDSC recruitment and maturation within the tumor microenvironment, where the needs of tumor-infiltrating lymphocytes (TILs) with antitumor function are completely neglected. In this review, we will discuss the unique or alternative source of energy exploited by GBM and MDSCs, exploring how deprivation of specific nutrients and accumulation of toxic byproducts can induce T-cell dysfunction. Understanding the metabolic programs of these cell components and how they impact fitness or dysfunction will be useful to improve treatment modalities, including immunotherapeutic strategies.', 'corpus_id': 233463029, 'score': 0}, {'doc_id': '231648648', 'title': 'Hematopoietic versus Solid Cancers and T Cell Dysfunction: Looking for Similarities and Distinctions', 'abstract': 'Simple Summary Dysfunction of the immune T cell compartment occurs in many hematopoietic as well as solid cancers and hampers successful application of new immunotherapeutic approaches. A complete understanding of T cell dysfunction might improve the outcome of such therapies, but an overview in the various cancers is still lacking. We aim to map areas of similarities and differences in solid versus hematopoietic malignancies, providing a high-level rather than a detailed perspective on T cell dysfunction in those tumors. Abstract Cancer cells escape, suppress and exploit the host immune system to sustain themselves, and the tumor microenvironment (TME) actively dampens T cell function by various mechanisms. Over the last years, new immunotherapeutic approaches, such as adoptive chimeric antigen receptor (CAR) T cell therapy and immune checkpoint inhibitors, have been successfully applied for refractory malignancies that could only be treated in a palliative manner previously. Engaging the anti-tumor activity of the immune system, including CAR T cell therapy to target the CD19 B cell antigen, proved to be effective in acute lymphocytic leukemia. In low-grade hematopoietic B cell malignancies, such as chronic lymphocytic leukemia, clinical outcomes have been tempered by cancer-induced T cell dysfunction characterized in part by a state of metabolic lethargy. In multiple myeloma, novel antigens such as BCMA and CD38 are being explored for CAR T cells. In solid cancers, T cell-based immunotherapies have been applied successfully to melanoma and lung cancers, whereas application in e.g., breast cancer lags behind and is modestly effective as yet. The main hurdles for CAR T cell immunotherapy in solid tumors are the lack of suitable antigens, anatomical inaccessibility, and T cell anergy due to immunosuppressive TME. Given the wide range of success and failure of immunotherapies in various cancer types, it is crucial to comprehend the underlying similarities and distinctions in T cell dysfunction. Hence, this review aims at comparing selected, distinct B cell-derived versus solid cancer types and at describing means by which malignant cells and TME might dampen T cell anti-tumor activity, with special focus on immunometabolism. Drawing a meaningful parallel between the efficacy of immunotherapy and the extent of T cell dysfunction will shed light on areas where we can improve immune function to battle cancer.', 'corpus_id': 231648648, 'score': 1}, {'doc_id': '222409176', 'title': 'Role of Mitochondria in Cancer Immune Evasion and Potential Therapeutic Approaches', 'abstract': 'The role of mitochondria in cancer formation and progression has been studied extensively, but much remains to be understood about this complex relationship. Mitochondria regulate many processes that are known to be altered in cancer cells, from metabolism to oxidative stress to apoptosis. Here, we review the evolving understanding of the role of mitochondria in cancer cells, and highlight key evidence supporting the role of mitochondria in cancer immune evasion and the effects of mitochondria-targeted antitumor therapy. Also considered is how knowledge of the role of mitochondria in cancer can be used to design and improve cancer therapies, particularly immunotherapy and radiation therapy. We further offer critical insights into the mechanisms by which mitochondria influence tumor immune responses, not only in cancer cells but also in immune cells. Given the central role of mitochondria in the complex interactions between cancer and the immune system, high priority should be placed on developing rational strategies to address mitochondria as potential targets in future preclinical and clinical studies. We believe that targeting mitochondria may provide additional opportunities in the development of novel antitumor therapeutics.', 'corpus_id': 222409176, 'score': 1}, {'doc_id': '233292259', 'title': 'Targeting SLC1A5 and SLC3A2/SLC7A5 as a Potential Strategy to Strengthen Anti-Tumor Immunity in the Tumor Microenvironment', 'abstract': 'Cancer cells are metabolically vigorous and are superior in the uptake of nutrients and in the release of the tumor microenvironment (TME)-specific metabolites. They create an acidic, hypoxic, and nutrient-depleted TME that makes it difficult for the cytotoxic immune cells to adapt to the metabolically hostile environment. Since a robust metabolism in immune cells is required for optimal anti-tumor effector functions, the challenges caused by the TME result in severe defects in the invasion and destruction of the established tumors. There have been many recent developments in NK and T cell-mediated immunotherapy, such as engineering them to express chimeric antigen receptors (CARs) to enhance tumor-recognition and infiltration. However, to defeat the tumor and overcome the limitations of the TME, it is essential to fortify these novel therapies by improving the metabolism of the immune cells. One potential strategy to enhance the metabolic fitness of immune cells is to upregulate the expression of nutrient transporters, specifically glucose and amino acid transporters. In particular, the amino acid transporters SLC1A5 and SLC7A5 as well as the ancillary subunit SLC3A2, which are required for efficient uptake of glutamine and leucine respectively, could strengthen the metabolic capabilities and effector functions of tumor-directed CAR-NK and T cells. In addition to enabling the influx and efflux of essential amino acids through the plasma membrane and within subcellular compartments such as the lysosome and the mitochondria, accumulating evidence has demonstrated that the amino acid transporters participate in sensing amino acid levels and thereby activate mTORC1, a master metabolic regulator that promotes cell metabolism, and induce the expression of c-Myc, a transcription factor essential for cell growth and proliferation. In this review, we discuss the regulatory pathways of these amino acid transporters and how we can take advantage of these processes to strengthen immunotherapy against cancer.', 'corpus_id': 233292259, 'score': 0}, {'doc_id': '23182912', 'title': 'Dietary fatty acid metabolism in prediabetes', 'abstract': 'Purpose of review Experimental evidences are strong for a role of long-chain saturated fatty acids in the development of insulin resistance and type 2 diabetes. Ectopic accretion of triglycerides in lean organs is a characteristic of prediabetes and type 2 diabetes and has been linked to end-organ complications. The contribution of disordered dietary fatty acid (DFA) metabolism to lean organ overexposure and lipotoxicity is still unclear, however. DFA metabolism is very complex and very difficult to study in vivo in humans. Recent findings We have recently developed a novel imaging method using PET with oral administration of 14-R,S-18F-fluoro-6-thia-heptadecanoic acid (18FTHA) to quantify organ-specific DFA partitioning. Our studies thus far confirmed impaired storage of DFA per volume of fat mass in abdominal adipose tissues of individuals with prediabetes. They also highlighted the increased channeling of DFA toward the heart, associated with subclinical reduction in cardiac systolic and diastolic function in individuals with prediabetes. Summary In the present review, we summarize previous work on DFA metabolism in healthy and prediabetic states and discuss these in the light of our novel findings using PET imaging of DFA metabolism. We herein provide an integrated view of abnormal organ-specific DFA partitioning in prediabetes in humans.', 'corpus_id': 23182912, 'score': 0}, {'doc_id': '22210398', 'title': 'Fuzzy Adaptive Tracking Control of Wheeled Mobile Robots With State-Dependent Kinematic and Dynamic Disturbances', 'abstract': 'Unlike most works based on pure nonholonomic constraint, this paper proposes a fuzzy adaptive tracking control method for wheeled mobile robots, where unknown slippage occurs and violates the nonholononomic constraint in the form of state-dependent kinematic and dynamic disturbances. These disturbances degrade tracking performance significantly and, therefore, should be compensated. To this end, the kinematics with state-dependent disturbances are rigorously derived based on the general form of slippage in the mobile robots, and fuzzy adaptive observers together with parameter adaptation laws are designed to estimate the state-dependent disturbances in both kinematics and dynamics. Because of the modular structure of the proposed method, it can be easily combined with the previous controllers based on the model with the pure nonholonomic constraint, such that the combination of the fuzzy adaptive observers with the previously proposed backstepping-like feedback linearization controller can guarantee the trajectory tracking errors to be globally ultimately bounded, even when the nonholonomic constraint is violated, and their ultimate bounds can be adjusted appropriately for various types of trajectories in the presence of large initial tracking errors and disturbances. Both the stability analysis and simulation results are provided to validate the proposed controller.', 'corpus_id': 22210398, 'score': 0}, {'doc_id': '233450335', 'title': 'Natural Killer Cells and Regulatory T Cells Cross Talk in Hepatocellular Carcinoma: Exploring Therapeutic Options for the Next Decade', 'abstract': 'Despite major advances in immunotherapy, hepatocellular carcinoma (HCC) remains a challenging target. Natural Killer (NK) cells are crucial components of the anti-HCC immune response, which can be manipulated for immunotherapeutic benefit as primary targets, modulators of the tumour microenvironment and in synchronising with tumour antigen specific effector CD8 cells for tumour clearance. Regulatory T cells shape the anti-tumour response from effector T cells via multiple suppressive mechanisms. Future research is needed to address the development of novel NK cell-targeted immunotherapy and on restraining Treg frequency and function in HCC. We have now entered a new era of anti-cancer treatment using checkpoint inhibitor (CPI)-based strategies. Combining GMP-NK cell immunotherapy to enhance the frequency of NK cells with CPI targeting both NK and CD8 T cells to release co-inhibitory receptors and enhance the cells anti-tumour immunity of HCC would be an attractive therapeutic option in the treatment of HCC. These therapeutic approaches should now be complemented by the application of genomic, proteomic and metabolomic approaches to understanding the microenvironment of HCC which, together with deep immune profiling of peripheral blood and HCC tissue before and during treatment, will provide the much-needed personalised medicine approach required to improve clinical outcomes for patients with HCC.', 'corpus_id': 233450335, 'score': 0}, {'doc_id': '232382784', 'title': 'Metabolic and Mitochondrial Functioning in Chimeric Antigen Receptor (CAR)—T Cells', 'abstract': 'Simple Summary We review the mechanisms of cellular metabolism and mitochondrial function that have potential to impact on the success of chimeric antigen receptor (CAR) T cell therapy. The review focuses readers on mitochondrial functions to allow a better understanding of the complexity of T cell metabolic pathways, energetics and apoptotic/antiapoptotic pathways occurring in CAR T cells. We highlight potential modifications of T cell metabolism and mitochondrial function for the benefit of improved adoptive cellular therapy. Reprogramming metabolism in CAR T cells is an attractive approach to improve antitumour functions, increase persistence and enable adaptation to the nutrient-restricted solid tumour environment. Abstract Chimeric antigen receptor (CAR) T-cell therapy has revolutionized adoptive cell therapy with impressive therapeutic outcomes of >80% complete remission (CR) rates in some haematological malignancies. Despite this, CAR T cell therapy for the treatment of solid tumours has invariably been unsuccessful in the clinic. Immunosuppressive factors and metabolic stresses in the tumour microenvironment (TME) result in the dysfunction and exhaustion of CAR T cells. A growing body of evidence demonstrates the importance of the mitochondrial and metabolic state of CAR T cells prior to infusion into patients. The different T cell subtypes utilise distinct metabolic pathways to fulfil their energy demands associated with their function. The reprogramming of CAR T cell metabolism is a viable approach to manufacture CAR T cells with superior antitumour functions and increased longevity, whilst also facilitating their adaptation to the nutrient restricted TME. This review discusses the mitochondrial and metabolic state of T cells, and describes the potential of the latest metabolic interventions to maximise CAR T cell efficacy for solid tumours.', 'corpus_id': 232382784, 'score': 1}]
155	{'doc_id': '211020980', 'title': 'Improved inter-scanner MS lesion segmentation by adversarial training on longitudinal data', 'abstract': 'The evaluation of white matter lesion progression is an important biomarker in the follow-up of MS patients and plays a crucial role when deciding the course of treatment. Current automated lesion segmentation algorithms are susceptible to variability in image characteristics related to MRI scanner or protocol differences. We propose a model that improves the consistency of MS lesion segmentations in inter-scanner studies. First, we train a CNN base model to approximate the performance of icobrain, an FDA-approved clinically available lesion segmentation software. A discriminator model is then trained to predict if two lesion segmentations are based on scans acquired using the same scanner type or not, achieving a 78% accuracy in this task. Finally, the base model and the discriminator are trained adversarially on multi-scanner longitudinal data to improve the inter-scanner consistency of the base model. The performance of the models is evaluated on an unseen dataset containing manual delineations. The inter-scanner variability is evaluated on test-retest data, where the adversarial network produces improved results over the base model and the FDA-approved solution.', 'corpus_id': 211020980}	1423	"[{'doc_id': '1510476', 'title': 'Deriving reproducible biomarkers from multi-site resting-state data: An Autism-based example', 'abstract': 'ABSTRACT Resting‐state functional Magnetic Resonance Imaging (R‐fMRI) holds the promise to reveal functional biomarkers of neuropsychiatric disorders. However, extracting such biomarkers is challenging for complex multi‐faceted neuropathologies, such as autism spectrum disorders. Large multi‐site datasets increase sample sizes to compensate for this complexity, at the cost of uncontrolled heterogeneity. This heterogeneity raises new challenges, akin to those face in realistic diagnostic applications. Here, we demonstrate the feasibility of inter‐site classification of neuropsychiatric status, with an application to the Autism Brain Imaging Data Exchange (ABIDE) database, a large (N=871) multi‐site autism dataset. For this purpose, we investigate pipelines that extract the most predictive biomarkers from the data. These R‐fMRI pipelines build participant‐specific connectomes from functionally‐defined brain areas. Connectomes are then compared across participants to learn patterns of connectivity that differentiate typical controls from individuals with autism. We predict this neuropsychiatric status for participants from the same acquisition sites or different, unseen, ones. Good choices of methods for the various steps of the pipeline lead to 67% prediction accuracy on the full ABIDE data, which is significantly better than previously reported results. We perform extensive validation on multiple subsets of the data defined by different inclusion criteria. These enables detailed analysis of the factors contributing to successful connectome‐based prediction. First, prediction accuracy improves as we include more subjects, up to the maximum amount of subjects available. Second, the definition of functional brain areas is of paramount importance for biomarker discovery: brain areas extracted from large R‐fMRI datasets outperform reference atlases in the classification tasks. HIGHLIGHTSWe propose a fully‐automatic pipeline to extract biomarkers from resting state fMRI.We demonstrate prediction in a clinical setting, on subjects coming from unseen site.On 871 subjects of the ABIDE dataset we achieve prediction accuracy better than state of the art (68%).A post‐hoc analysis of the pipeline steps sketches an ideal pipeline for prediction.Extracted autism biomarkers are stable across training sets and consistent with literature.', 'corpus_id': 1510476, 'score': 1}, {'doc_id': '212658055', 'title': 'Fine-grain atlases of functional modes for fMRI analysis', 'abstract': 'Population imaging markedly increased the size of functional-imaging datasets, shedding new light on the neural basis of inter-individual differences. Analyzing these large data entails new scalability challenges, computational and statistical. For this reason, brain images are typically summarized in a few signals, for instance reducing voxel-level measures with brain atlases or functional modes. A good choice of the corresponding brain networks is important, as most data analyses start from these reduced signals. We contribute finely-resolved atlases of functional modes, comprising from 64 to 1024 networks. These dictionaries of functional modes (DiFuMo) are trained on millions of fMRI functional brain volumes of total size 2.4TB, spanned over 27 studies and many research groups. We demonstrate the benefits of extracting reduced signals on our fine-grain atlases for many classic functional data analysis pipelines: stimuli decoding from 12,334 brain responses, standard GLM analysis of fMRI across sessions and individuals, extraction of resting-state functional-connectomes biomarkers for 2,500 individuals, data compression and meta-analysis over more than 15,000 statistical maps. In each of these analysis scenarii, we compare the performance of our functional atlases with that of other popular references, and to a simple voxel-level analysis. Results highlight the importance of using high-dimensional ""soft"" functional atlases, to represent and analyse brain activity while capturing its functional gradients. Analyses on high-dimensional modes achieve similar statistical performance as at the voxel level, but with much reduced computational cost and higher interpretability. In addition to making them available, we provide meaningful names for these modes, based on their anatomical location. It will facilitate reporting of results.', 'corpus_id': 212658055, 'score': 0}, {'doc_id': '3682932', 'title': 'Harmonization of cortical thickness measurements across scanners and sites', 'abstract': '&NA; With the proliferation of multi‐site neuroimaging studies, there is a greater need for handling non‐biological variance introduced by differences in MRI scanners and acquisition protocols. Such unwanted sources of variation, which we refer to as “scanner effects”, can hinder the detection of imaging features associated with clinical covariates of interest and cause spurious findings. In this paper, we investigate scanner effects in two large multi‐site studies on cortical thickness measurements across a total of 11 scanners. We propose a set of tools for visualizing and identifying scanner effects that are generalizable to other modalities. We then propose to use ComBat, a technique adopted from the genomics literature and recently applied to diffusion tensor imaging data, to combine and harmonize cortical thickness values across scanners. We show that ComBat removes unwanted sources of scan variability while simultaneously increasing the power and reproducibility of subsequent statistical analyses. We also show that ComBat is useful for combining imaging data with the goal of studying life‐span trajectories in the brain. HighlightsCortical thickness (CT) measurements are highly scanner specific.Identifying scanner effects is crucial for inference and biomarker development.We propose to use ComBat to harmonize cortical thickness values across scanners.', 'corpus_id': 3682932, 'score': 1}, {'doc_id': '212726155', 'title': 'Image Quality Transfer Enhances Contrast and Resolution of Low-Field Brain MRI in African Paediatric Epilepsy Patients', 'abstract': '1.5T or 3T scanners are the current standard for clinical MRI, but low-field (<1T) scanners are still common in many lower- and middle-income countries for reasons of cost and robustness to power failures. Compared to modern high-field scanners, low-field scanners provide images with lower signal-to-noise ratio at equivalent resolution, leaving practitioners to compensate by using large slice thickness and incomplete spatial coverage. Furthermore, the contrast between different types of brain tissue may be substantially reduced even at equal signal-to-noise ratio, which limits diagnostic value. Recently the paradigm of Image Quality Transfer has been applied to enhance 0.36T structural images aiming to approximate the resolution, spatial coverage, and contrast of typical 1.5T or 3T images. A variant of the neural network U-Net was trained using low-field images simulated from the publicly available 3T Human Connectome Project dataset. Here we present qualitative results from real and simulated clinical low-field brain images showing the potential value of IQT to enhance the clinical utility of readily accessible low-field MRIs in the management of epilepsy.', 'corpus_id': 212726155, 'score': 0}, {'doc_id': '8087199', 'title': 'Reliability in multi-site structural MRI studies: Effects of gradient non-linearity correction on phantom and human data', 'abstract': 'Longitudinal and multi-site clinical studies create the imperative to characterize and correct technological sources of variance that limit image reproducibility in high-resolution structural MRI studies, thus facilitating precise, quantitative, platform-independent, multi-site evaluation. In this work, we investigated the effects that imaging gradient non-linearity have on reproducibility of multi-site human MRI. We applied an image distortion correction method based on spherical harmonics description of the gradients and verified the accuracy of the method using phantom data. The correction method was then applied to the brain image data from a group of subjects scanned twice at multiple sites having different 1.5 T platforms. Within-site and across-site variability of the image data was assessed by evaluating voxel-based image intensity reproducibility. The image intensity reproducibility of the human brain data was significantly improved with distortion correction, suggesting that this method may offer improved reproducibility in morphometry studies. We provide the source code for the gradient distortion algorithm together with the phantom data.', 'corpus_id': 8087199, 'score': 1}, {'doc_id': '49642502', 'title': 'Statistical harmonization corrects site effects in functional connectivity measurements from multi‐site fMRI data', 'abstract': 'Acquiring resting‐state functional magnetic resonance imaging (fMRI) datasets at multiple MRI scanners and clinical sites can improve statistical power and generalizability of results. However, multi‐site neuroimaging studies have reported considerable nonbiological variability in fMRI measurements due to different scanner manufacturers and acquisition protocols. These undesirable sources of variability may limit power to detect effects of interest and may even result in erroneous findings. Until now, there has not been an approach that removes unwanted site effects. In this study, using a relatively large multi‐site (4 sites) fMRI dataset, we investigated the impact of site effects on functional connectivity and network measures estimated by widely used connectivity metrics and brain parcellations. The protocols and image acquisition of the dataset used in this study had been homogenized using identical MRI phantom acquisitions from each of the neuroimaging sites; however, intersite acquisition effects were not completely eliminated. Indeed, in this study, we found that the magnitude of site effects depended on the choice of connectivity metric and brain atlas. Therefore, to further remove site effects, we applied ComBat, a harmonization technique previously shown to eliminate site effects in multi‐site diffusion tensor imaging (DTI) and cortical thickness studies. In the current work, ComBat successfully removed site effects identified in connectivity and network measures and increased the power to detect age associations when using optimal combinations of connectivity metrics and brain atlases. Our proposed ComBat harmonization approach for fMRI‐derived connectivity measures facilitates reliable and efficient analysis of retrospective and prospective multi‐site fMRI neuroimaging studies.', 'corpus_id': 49642502, 'score': 1}, {'doc_id': '9155849', 'title': 'Report on a multicenter fMRI quality assurance protocol', 'abstract': 'Temporal stability during an fMRI acquisition is very important because the blood oxygen level‐dependent (BOLD) effects of interest are only a few percent in magnitude. Also, studies involving the collection of groups of subjects over time require stable scanner performance over days, weeks, months, and even years. We describe a protocol designed by one of the authors that has been tested for several years within the context of a large, multicenter collaborative fMRI research project (FIRST‐BIRN). A full description of the phantom, the quality assurance (QA) protocol, and the several calculations used to measure performance is provided. The results obtained with this protocol at multiple sites over time are presented. These data can be used as benchmarks for other centers involved in fMRI research. Some issues with the various protocol measures are highlighted and discussed, and possible protocol improvements are also suggested. Overall, we expect that other fMRI centers will find this approach to QA useful and this report may facilitate developing a similar QA protocol locally. Based on the findings reported herein, the authors are convinced that monitoring QA in this way will improve the quality of fMRI data. J. Magn. Reson. Imaging 2006. © 2006 Wiley‐Liss, Inc.', 'corpus_id': 9155849, 'score': 1}, {'doc_id': '211818317', 'title': 'Explainable and Scalable Machine-Learning Algorithms for Detection of Autism Spectrum Disorder using fMRI Data', 'abstract': 'Diagnosing Autism Spectrum Disorder (ASD) is a challenging problem, and is based purely on behavioral descriptions of symptomology (DSM-5/ICD-10), and requires informants to observe children with disorder across different settings (e.g. home, school). Numerous limitations (e.g., informant discrepancies, lack of adherence to assessment guidelines, informant biases) to current diagnostic practices have the potential to result in over-, under-, or misdiagnosis of the disorder. Advances in neuroimaging technologies are providing a critical step towards a more objective assessment of the disorder. Prior research provides strong evidence that structural and functional magnetic resonance imaging (MRI) data collected from individuals with ASD exhibit distinguishing characteristics that differ in local and global spatial, and temporal neural-patterns of the brain. Our proposed deep-learning model ASD-DiagNet exhibits consistently high accuracy for classification of ASD brain scans from neurotypical scans. We have for the first time integrated traditional machine-learning and deep-learning techniques that allows us to isolate ASD biomarkers from MRI data sets. Our method, called Auto-ASD-Network, uses a combination of deep-learning and Support Vector Machines (SVM) to classify ASD scans from neurotypical scans. Such interpretable models would help explain the decisions made by deep-learning techniques leading to knowledge discovery for neuroscientists, and transparent analysis for clinicians.', 'corpus_id': 211818317, 'score': 1}, {'doc_id': '209500678', 'title': 'Statistical agnostic mapping: a framework in neuroimaging based on concentration inequalities', 'abstract': 'In the 70s a novel branch of statistics emerged focusing its effort in selecting a function in the pattern recognition problem, which fulfils a definite relationship between the quality of the approximation and its complexity. These data-driven approaches are mainly devoted to problems of estimating dependencies with limited sample sizes and comprise all the empirical out-of sample generalization approaches, e.g. cross validation (CV) approaches. Although the latter are not designed for testing competing hypothesis or comparing different models in neuroimaging, there are a number of theoretical developments within this theory which could be employed to derive a Statistical Agnostic (non-parametric) Mapping (SAM) at voxel or multi-voxel level. Moreover, SAMs could relieve i) the problem of instability in limited sample sizes when estimating the actual risk via the CV approaches, e.g. large error bars, and provide ii) an alternative way of Family-wise-error (FWE) corrected p-value maps in inferential statistics for hypothesis testing. In this sense, we propose a novel framework in neuroimaging based on concentration inequalities, which results in (i) a rigorous development for model validation with a small sample/dimension ratio, and (ii) a less-conservative procedure than FWE p-value correction, to determine the brain significance maps from the inferences made using small upper bounds of the actual risk.', 'corpus_id': 209500678, 'score': 0}, {'doc_id': '210920478', 'title': 'PIRACY: An Optimized Pipeline for Functional Connectivity Analysis in the Rat Brain', 'abstract': 'Resting state functional MRI (rs-fMRI) is a widespread and powerful tool for investigating functional connectivity (FC) and brain disorders. However, FC analysis can be seriously affected by random and structured noise from non-neural sources, such as physiology. Thus, it is essential to first reduce thermal noise and then correctly identify and remove non-neural artifacts from rs-fMRI signals through optimized data processing methods. However, existing tools that correct for these effects have been developed for human brain and are not readily transposable to rat data. Therefore, the aim of the present study was to establish a data processing pipeline that can robustly remove random and structured noise from rat rs-fMRI data. It includes a novel denoising approach based on the Marchenko-Pastur Principal Component Analysis (MP-PCA) method, FMRIB’s ICA-based Xnoiseifier (FIX) for automatic artifact classification and cleaning, and global signal regression (GSR). Our results show that: (I) MP-PCA denoising substantially improves the temporal signal-to-noise ratio, (II) the pre-trained FIX classifier achieves a high accuracy in artifact classification, and (III) both independent component analysis (ICA) cleaning and GSR are essential steps in correcting for possible artifacts and minimizing the within-group variability in control animals while maintaining typical connectivity patterns. Reduced within-group variability also facilitates the exploration of potential between-group FC changes, as illustrated here in a rat model of sporadic Alzheimer’s disease.', 'corpus_id': 210920478, 'score': 0}]"
156	{'doc_id': '230742626', 'title': 'Testing the \nER=EPR\n conjecture', 'abstract': 'National Natural Science Foundation of ChinaNational Natural Science Foundation of China (NSFC) [11775140]; U.S. Department of EnergyUnited States Department of Energy (DOE) [DE-SC0020262]; Julian Schwinger Foundation; U.S. National Science FoundationNational Science Foundation (NSF) [PHY-1820738, PHY-2014021]', 'corpus_id': 230742626}	16491	"[{'doc_id': '232232767', 'title': 'Gross-Neveu Heisenberg criticality: Dynamical generation of quantum spin Hall masses', 'abstract': 'Yuhai Liu, Zhenjiu Wang, Toshihiro Sato, Wenan Guo, 1, ∗ and Fakher F. Assaad 4, † Beijing Computational Science Research Center, Beijing 100193, China Institut für Theoretische Physik und Astrophysik, Universität Würzburg, Am Hubland, 97074 Würzburg, Germany Department of Physics, Beijing Normal University, Beijing 100875, China Würzburg-Dresden Cluster of Excellence ct.qmat, Am Hubland, 97074 Würzburg, Germany', 'corpus_id': 232232767, 'score': 0}, {'doc_id': '119208899', 'title': 'ER=EPR, GHZ, and the Consistency of Quantum Measurements', 'abstract': 'This paper illustrates various aspects of the ER=EPR conjecture.It begins with a brief heuristic argument, using the Ryu-Takayanagi correspondence, for why entanglement between black holes implies the existence of Einstein-Rosen bridges. \nThe main part of the paper addresses a fundamental question: Is ER=EPR consistent with the standard postulates of quantum mechanics? Naively it seems to lead to an inconsistency between observations made on entangled systems by different observers. The resolution of the paradox lies in the properties of multiple black holes, entangled in the Greenberger-Horne-Zeilinger pattern. \nThe last part of the paper is about entanglement as a resource for quantum communication. ER=EPR provides a way to visualize protocols like quantum teleportation. In some sense teleportation takes place through the wormhole, but as usual, classical communication is necessary to complete the protocol.', 'corpus_id': 119208899, 'score': 1}, {'doc_id': '232233618', 'title': 'Ruling out real-number description of quantum mechanics', 'abstract': ""Standard quantum mechanics has been formulated with complex-valued Schrödinger equations, wave functions, operators, and Hilbert spaces. However, previous work has shown possible to simulate quantum systems using only real numbers by adding extra qubits and exploiting an enlarged Hilbert space. A fundamental question arises: are the complex numbers really necessary for the quantum mechanical description of nature? To answer this question, a non-local game has been developed to reveal a contradiction between a multiqubit quantum experiment and a player using only real numbers. Here, based on deterministic and high-fidelity entanglement swapping with superconducting qubits, we experimentally implement the Bell-like game and observe a quantum score of 8.09(1), which beats the real number bound of 7.66 by 43 standard deviations. Our results disprove the real-number description of nature and establish the indispensable role of complex number in the quantum mechanics. Physicists use mathematics to describe nature. In classical physics, real number appears complete to describe the physical reality in all classical phenomenon, whereas complex number is only sometimes employed as a convenient mathematical tool. In quantum mechanics, the complex number was introduced as the first principle in Schrödinger's equation and Heisenberg’s commutation relation. The complex wavefunction has been shown to represent physical reality of quantum objects. Experimentally, the real and imaginary parts of the wavefunction has been directly measured. Today, the quantum mechanics with complex wavefunction seems the most successful theory to describe nature. However, whether the complex number is necessary to represent the theory of quantum mechanics is still an open question. Starting with von Neumann in 1936, previous works have shown possible to simulate quantum systems using only real numbers by adding extra qubits and exploiting an enlarged Hilbert space. For example, by adding an extra qubit ( 0 1 ) / 2 i i \uf0b1 \uf03d \uf0b1 , a single-quantum system with a complex density matrix \uf072 and Hermitian operator H can be simulated through ( ) ( ) tr H tr H \uf072 \uf072 \uf03d , where \uf072 and H are real and of the form: * ( ) / 2 i i i i \uf072 \uf072 \uf072 \uf03d \uf0c4 \uf02b \uf02b \uf02b \uf0c4 \uf02d \uf02d , * . H H i i H i i \uf03d \uf0c4 \uf02b \uf02b \uf02b \uf0c4 \uf02d \uf02d Therefore, in this case one cannot distinguish between the realand complex-number representations. It has also been shown that real-number quantum state is universal for quantum computing, which again challenged the fundamental role of complexnumber quantum mechanics. Very recently, Renou et al. developed an elegant scheme to provide an observable effect in quantum experiments to distinguish between the two representations. The scheme is a Bell-like three-party game based on deterministic entanglement swapping. With the assumption that the quantum states produced by two independent sources is a tensor product of the states produced by each of the sources, the real-number quantum mechanics cannot obtain maximal violation of a Bell CHSH-like inequality, thus being falsified. Figure 1a shows an illustration of the non-local game for Alice, Bob, and Charlie. First, two pairs of Einstein-Podolsky-Rosen (EPR) entangled qubits are distributed between Alice and Bob, and between Bob and Charlie, respectively. Bob then performs a joint Bell-state measurement (BSM) on his two received qubits, which randomly projects them into one of the four Bell states: =( 0 0 1 1 ) / 2, =( 0 1 1 0 ) / 2, =( 0 0 1 1 ) / 2, =( 0 1 1 0 ) / 2. \uf066 \uf079 \uf066 \uf079 \uf02b"", 'corpus_id': 232233618, 'score': 1}, {'doc_id': '232404913', 'title': 'Secure quantum communication through a wormhole', 'abstract': 'An accumulation of theoretical evidence contribute to the picture of gravity as a manifestation of quantum entanglement in a certain many-body quantum system. This is in particular expresses in the ER=EPR conjecture, which relates gravitational Einstein-Rosen (ER) bridge with the EinsteinPodolsky-Rosen (EPR) quantum entangled pairs or, more generally, with the so-called Thermofield Double State. In this letter, the ER=EPR conjecture is employed to introduce unitary quantum teleportation protocol, which recycles the entanglement forming traversable generalization of the Einstein-Rosen bridge. In consequence, the wormhole remains unaffected by the quantum teleportation. Furthermore, it is shown that the protocol guarantees the unconditional security of the quantum communication. Performance of the protocol is demonstrated in a simple setting with the use of 5-qubit Santiago IBM quantum computer, giving fidelities above the 2/3 the classical limit for a representative set of teleported states. Security of the protocol has been supported by experimental studies performed with the use of the noisy quantum processor. Possible generalization of the protocol, which may have relevance in the context of macroscopic gravitational configurations, is also considered.', 'corpus_id': 232404913, 'score': 1}, {'doc_id': '233576480', 'title': 'Corrigendum to “On a generalization of a conjecture of Grosswald” [J. Number Theory 216 (2020) 216–241]', 'abstract': 'Abstract We extend the result of Lemma 4, [1] to the case that e = 0 and l = 1 which was missing in [1] but used in the proof of Theorem 1, [1] .', 'corpus_id': 233576480, 'score': 0}, {'doc_id': '233530668', 'title': 'Asymmetric bidirectional quantum state exchange between Alice and Bob through a third party', 'abstract': 'Abstract In this paper we present a quantum communication protocol which utilizes a multipartite multi-qubit entangled channel for mutual transfer of three-qubit generalized W-state and arbitrary two-qubit entangled state between two parties. The protocol is perfect with no cases of failure and is supervised by a third party. The work is in the direction of the recently initiated line of research on exploring the possibility of simultaneous quantum multitasking using multipartite entanglement.', 'corpus_id': 233530668, 'score': 0}, {'doc_id': '13896453', 'title': 'Copenhagen vs Everett, Teleportation, and ER=EPR', 'abstract': ""Quantum gravity may have as much to tell us about the foundations and interpretation of quantum mechanics as it does about gravity. The Copenhagen interpretation of quantum mechanics and Everett's Relative State Formulation are complementary descriptions which in a sense are dual to one another. My purpose here is to discuss this duality in the light of the of ER=EPR conjecture."", 'corpus_id': 13896453, 'score': 1}, {'doc_id': '233583971', 'title': 'Corrigendum to “Proof of a conjecture on distance energy change of complete multipartite graph due to edge deletion” [Linear Algebra Appl. 611 (2021) 253–259]', 'abstract': 'Abstract There is a mistake in the proof of Conjecture 1.1 in [Proof of a conjecture on distance energy change of complete multipartite graph due to edge deletion, Linear Algebra Appl. 611 (2021) 253–259]. Here we give a correct proof.', 'corpus_id': 233583971, 'score': 0}, {'doc_id': '232216965', 'title': 'M ar 2 02 1 Quantum information masking basing on quantum teleportation', 'abstract': 'The no-masking theorem says that masking quantum information is impossible in a bipartite scenario. However, there exist schemes to mask quantum states in multipartite systems. In this work, we show that, the joint measurement in the teleportation is really a masking process, when the apparatus is regarded as a quantum participant in the whole system. Based on the view, we present two four-partite maskers and a tripartite masker. One of the former provides a generalization in arbitrary dimension of the four-qubit scheme given by Li and Wang [Phys. Rev. A 98, 062306 (2018)], and the latter is precisely their tripartite scheme. The occupation probabilities and coherence of quantum states are masked in two steps of our schemes. And the information can be extracted naturally in their reverse processes.', 'corpus_id': 232216965, 'score': 0}, {'doc_id': '149679404', 'title': 'New groups to ER=EPR conjecture', 'abstract': 'ER = EPR is a conjecture in physics, stating what the entangled particles are connected by a wormhole (Einstein-Rosen bridge or ER bridge for short), and the Einstein-Podolsky-Rosen paradox (or EPR paradox) is an influential thought experiment in quantum mechanics, in which should mean that general relativity and quantum field theory can somehow unify. In pursuit of this goal, we introduce the notion of pseudoassociation group (or pa-group). We prove that some groups are pa-groups. As a matter of fact, the pa-group is determined by its commutative idempotent semigroup or ∧-semilattice . These results show some interesting properties, but most interesting is that this paper offers a new and simple but rigorous and abundant proof of the ER=EPR conjecture by using some identity of pa-groups. So, we can say the conjecture is correct.', 'corpus_id': 149679404, 'score': 1}]"
157	{'doc_id': '208617538', 'title': 'Visual Reaction: Learning to Play Catch With Your Drone', 'abstract': 'In this paper we address the problem of visual reaction: the task of interacting with dynamic environments where the changes in the environment are not necessarily caused by the agents itself. Visual reaction entails predicting the future changes in a visual environment and planning accordingly. We study the problem of visual reaction in the context of playing catch with a drone in visually rich synthetic environments. This is a challenging problem since the agent is required to learn (1) how objects with different physical properties and shapes move, (2) what sequence of actions should be taken according to the prediction, (3) how to adjust the actions based on the visual feedback from the dynamic environment (e.g., when objects bouncing off a wall), and (4) how to reason and act with an unexpected state change in a timely manner. We propose a new dataset for this task, which includes 30K throws of 20 types of objects in different directions with different forces. Our results show that our model that integrates a forecaster with a planner outperforms a set of strong baselines that are based on tracking as well as pure model-based and model-free RL baselines. The code and dataset are available at github.com/KuoHaoZeng/Visual_Reaction.', 'corpus_id': 208617538}	20512	[{'doc_id': '237592739', 'title': 'Learning to Guide Human Attention on Mobile Telepresence Robots with 360 degree Vision', 'abstract': 'Mobile telepresence robots (MTRs) allow people to navigate and interact with a remote environment that is in a place other than the person’s true location. Thanks to the recent advances in 360◦ vision, many MTRs are now equipped with an all-degree visual perception capability. However, people’s visual field horizontally spans only about 120◦ of the visual field captured by the robot. To bridge this observability gap toward human-MTR shared autonomy, we have developed a framework, called GHAL360, to enable the MTR to learn a goal-oriented policy from reinforcements for guiding human attention using visual indicators. Three telepresence environments were constructed using datasets that are extracted from Matterport3D and collected from a real robot respectively. Experimental results show that GHAL360 outperformed the baselines from the literature in the efficiency of a human-MTR team completing target search tasks. A demo video is available: https://youtu.be/aGbTxCGJSDM', 'corpus_id': 237592739, 'score': 0}, {'doc_id': '237157766', 'title': 'Stochastic Scene-Aware Motion Prediction', 'abstract': 'A long-standing goal in computer vision is to capture, model, and realistically synthesize human behavior. Specifically, by learning from data, our goal is to enable virtual humans to navigate within cluttered indoor scenes and naturally interact with objects. Such embodied behavior has applications in virtual reality, computer games, and robotics, while synthesized behavior can be used as training data. The problem is challenging because real human motion is diverse and adapts to the scene. For example, a person can sit or lie on a sofa in many places and with varying styles. We must model this diversity to synthesize virtual humans that realistically perform human-scene interactions. We present a novel data-driven, stochastic motion synthesis method that models different styles of performing a given action with a target object. Our Scene-Aware Motion Prediction method (SAMP) generalizes to target objects of various geometries while enabling the character to navigate in cluttered scenes. To train SAMP, we collected MoCap data covering various sitting, lying down, walking, and running styles. We demonstrate SAMP on complex indoor scenes and achieve superior performance than existing solutions. Code and data are available for research at https://samp.is.tue.mpg.de.', 'corpus_id': 237157766, 'score': 1}, {'doc_id': '236954782', 'title': 'NOVEL LAYOUTS USING ABSTRACT 2-D MAPS', 'abstract': 'Efficiently training agents with planning capabilities has long been one of the major challenges in decision-making. In this work, we focus on zero-shot navigation ability on a given abstract 2-D occupancy map, like human navigation by reading a paper map, by treating it as an image. To learn this ability, we need to efficiently train an agent on environments with a small proportion of training maps and share knowledge effectively across the environments. We hypothesize that model-based navigation can better adapt agent’s behaviors to a task, since it disentangles the variations in map layout and goal location and enables longer-term planning ability on novel locations compared to reactive policies. We propose to learn a hypermodel that can understand patterns from a limited number of abstract maps and goal locations, to maximize alignment between the hypermodel predictions and real trajectories to extract information from multi-task off-policy experiences, and to construct denser feedback for planners by n-step goal relabelling. We train our approach on DeepMind Lab environments with layouts from different maps, and demonstrate superior performance on zero-shot transfer to novel maps and goals.', 'corpus_id': 236954782, 'score': 0}, {'doc_id': '237454671', 'title': 'Dynamic Modeling of Hand-Object Interactions via Tactile Sensing', 'abstract': 'Tactile sensing is critical for humans to perform everyday tasks. While significant progress has been made in analyzing object grasping from vision, it remains unclear how we can utilize tactile sensing to reason about and model the dynamics of hand-object interactions. In this work, we employ a high-resolution tactile glove to perform four different interactive activities on a diversified set of objects. We build our model on a cross-modal learning framework and generate the labels using a visual processing pipeline to supervise the tactile model, which can then be used on its own during the test time. The tactile model aims to predict the 3d locations of both the hand and the object purely from the touch data by combining a predictive model and a contrastive learning module. This framework can reason about the interaction patterns from the tactile data, hallucinate the changes in the environment, estimate the uncertainty of the prediction, and generalize to unseen objects. We also provide detailed ablation studies regarding different system designs as well as visualizations of the predicted trajectories. This work takes a step on dynamics modeling in hand-object interactions from dense tactile sensing, which opens the door for future applications in activity learning, humancomputer interactions, and imitation learning for robotics.', 'corpus_id': 237454671, 'score': 1}, {'doc_id': '236942047', 'title': 'No RL, No Simulation: Learning to Navigate without Navigating', 'abstract': 'Most prior methods for learning navigation policies require access to simulation environments, as they need online policy interaction and rely on ground-truth maps for rewards. However, building simulators is expensive (requires manual effort for each and every scene) and creates challenges in transferring learned policies to robotic platforms in the real-world, due to the sim-to-real domain gap. In this paper, we pose a simple question: Do we really need active interaction, ground-truth maps or even reinforcement-learning (RL) in order to solve the image-goal navigation task? We propose a self-supervised approach to learn to navigate from only passive videos of roaming. Our approach, No RL, No Simulator (NRNS), is simple and scalable, yet highly effective. NRNS outperforms RL-based formulations by a significant margin. We present NRNS as a strong baseline for any future imagebased navigation tasks that use RL or Simulation.', 'corpus_id': 236942047, 'score': 1}, {'doc_id': '236908377', 'title': 'ICLR 2021 1 ? ? ? ? ?', 'abstract': 'We introduce environment predictive coding, a self-supervised approach to learn environment-level representations for embodied agents. In contrast to prior work on self-supervised learning for images, we aim to jointly encode a series of images gathered by an agent as it moves about in 3D environments. We learn these representations via a zone prediction task, where we intelligently mask out portions of an agent’s trajectory and predict them from the unmasked portions, conditioned on the agent’s camera poses. By learning such representations on a collection of videos, we demonstrate successful transfer to multiple downstream navigationoriented tasks. Our experiments on the photorealistic 3D environments of Gibson and Matterport3D show that our method outperforms the state-of-the-art on challenging tasks with only a limited budget of experience.', 'corpus_id': 236908377, 'score': 0}, {'doc_id': '236772213', 'title': 'An Efficient Image-to-Image Translation HourGlass-based Architecture for Object Pushing Policy Learning', 'abstract': 'Humans effortlessly solve pushing tasks in everyday life but unlocking these capabilities remains a challenge in robotics because physics models of these tasks are often inaccurate or unattainable. State-of-the-art data-driven approaches learn to compensate for these inaccuracies or replace the approximated physics models altogether. Nevertheless, approaches like Deep Q-Networks (DQNs) suffer from local optima in large state-action spaces. Furthermore, they rely on well-chosen deep learning architectures and learning paradigms. In this paper, we propose to frame the learning of pushing policies (where to push and how) by DQNs as an image-to-image translation problem and exploit an Hourglassbased architecture. We present an architecture combining a predictor of which pushes lead to changes in the environment with a state-action value predictor dedicated to the pushing task. Moreover, we investigate positional information encoding to learn position-dependent policy behaviors. We demonstrate in simulation experiments with a UR5 robot arm that our overall architecture helps the DQN learn faster and achieve higher performance in a pushing task involving objects with unknown dynamics.', 'corpus_id': 236772213, 'score': 0}, {'doc_id': '237452250', 'title': 'SORNet: Spatial Object-Centric Representations for Sequential Manipulation', 'abstract': 'Sequential manipulation tasks require a robot to perceive the state of an environment and plan a sequence of actions leading to a desired goal state, where the ability to reason about spatial relationships among object entities from raw sensor inputs is crucial. Prior works relying on explicit state estimation or endto-end learning struggle with novel objects. In this work, we propose SORNet (Spatial Object-Centric Representation Network), which extracts object-centric representations from RGB images conditioned on canonical views of the objects of interest. We show that the object embeddings learned by SORNet generalize zero-shot to unseen object entities on three spatial reasoning tasks: spatial relationship classification, skill precondition classification and relative direction regression, significantly outperforming baselines. Further, we present real-world robotic experiments demonstrating the usage of the learned object embeddings in task planning for sequential manipulation.', 'corpus_id': 237452250, 'score': 1}, {'doc_id': '237385309', 'title': 'Learning Language-Conditioned Robot Behavior from Offline Data and Crowd-Sourced Annotation', 'abstract': 'We study the problem of learning a range of vision-based manipulation tasks from a large offline dataset of robot interaction. In order to accomplish this, humans need easy and effective ways of specifying tasks to the robot. Goal images are one popular form of task specification, as they are already grounded in the robot’s observation space. However, goal images also have a number of drawbacks: they are inconvenient for humans to provide, they can over-specify the desired behavior leading to a sparse reward signal, or under-specify task information in the case of non-goal reaching tasks. Natural language provides a convenient and flexible alternative for task specification, but comes with the challenge of grounding language in the robot’s observation space. To scalably learn this grounding we propose to leverage offline robot datasets (including highly sub-optimal, autonomously collected data) with crowd-sourced natural language labels. With this data, we learn a simple classifier which predicts if a change in state completes a language instruction. This provides a language-conditioned reward function that can then be used for offline multi-task RL. In our experiments, we find that on language-conditioned manipulation tasks our approach outperforms both goalimage specifications and language conditioned imitation techniques by more than 25%, and is able to perform visuomotor tasks from natural language, such as “open the right drawer” and “move the stapler”, on a Franka Emika Panda robot.', 'corpus_id': 237385309, 'score': 1}, {'doc_id': '237361919', 'title': 'Learning Multi-Stage Tasks with One Demonstration via Self-Replay', 'abstract': 'In this work, we introduce a novel method to learn everyday-like multi1 stage tasks from a single human demonstration, without requiring any prior object 2 knowledge. Inspired by the recent Coarse-to-Fine Imitation Learning, we model 3 imitation learning as a learned object reaching phase followed by an open-loop 4 replay of the operator’s actions. We build upon this for multi-stage tasks where, 5 following the human demonstration, the robot can autonomously collect image 6 data for the entire multi-stage task, by reaching the next object in the sequence 7 and then replaying the demonstration, repeating in a loop for all stages of the task. 8 We evaluate with real-world experiments on a set of everyday multi-stage tasks, 9 which we show that our method can solve from a single demonstration. Videos 10 and supplementary material can be found at this anonymous webpage. 11', 'corpus_id': 237361919, 'score': 0}]
158	{'doc_id': '52136564', 'title': 'Modeling Empathy and Distress in Reaction to News Stories', 'abstract': 'Computational detection and understanding of empathy is an important factor in advancing human-computer interaction. Yet to date, text-based empathy prediction has the following major limitations: It underestimates the psychological complexity of the phenomenon, adheres to a weak notion of ground truth where empathic states are ascribed by third parties, and lacks a shared corpus. In contrast, this contribution presents the first publicly available gold standard for empathy prediction. It is constructed using a novel annotation methodology which reliably captures empathy assessments by the writer of a statement using multi-item scales. This is also the first computational work distinguishing between multiple forms of empathy, empathic concern, and personal distress, as recognized throughout psychology. Finally, we present experimental results for three different predictive models, of which a CNN performs the best.', 'corpus_id': 52136564}	8824	"[{'doc_id': '182953211', 'title': 'BIGPATENT: A Large-Scale Dataset for Abstractive and Coherent Summarization', 'abstract': 'Most existing text summarization datasets are compiled from the news domain, where summaries have a flattened discourse structure. In such datasets, summary-worthy content often appears in the beginning of input articles. Moreover, large segments from input articles are present verbatim in their respective summaries. These issues impede the learning and evaluation of systems that can understand an article’s global content structure as well as produce abstractive summaries with high compression ratio. In this work, we present a novel dataset, BIGPATENT, consisting of 1.3 million records of U.S. patent documents along with human written abstractive summaries. Compared to existing summarization datasets, BIGPATENT has the following properties: i) summaries contain a richer discourse structure with more recurring entities, ii) salient content is evenly distributed in the input, and iii) lesser and shorter extractive fragments are present in the summaries. Finally, we train and evaluate baselines and popular learning models on BIGPATENT to shed light on new challenges and motivate future directions for summarization research.', 'corpus_id': 182953211, 'score': 0}, {'doc_id': '59222757', 'title': 'TransferTransfo: A Transfer Learning Approach for Neural Network Based Conversational Agents', 'abstract': 'We introduce a new approach to generative data-driven dialogue systems (e.g. chatbots) called TransferTransfo which is a combination of a Transfer learning based training scheme and a high-capacity Transformer model. Fine-tuning is performed by using a multi-task objective which combines several unsupervised prediction tasks. The resulting fine-tuned model shows strong improvements over the current state-of-the-art end-to-end conversational models like memory augmented seq2seq and information-retrieval models. On the privately held PERSONA-CHAT dataset of the Conversational Intelligence Challenge 2, this approach obtains a new state-of-the-art, with respective perplexity, Hits@1 and F1 metrics of 16.28 (45 % absolute improvement), 80.7 (46 % absolute improvement) and 19.5 (20 % absolute improvement).', 'corpus_id': 59222757, 'score': 1}, {'doc_id': '33124667', 'title': 'Hazardous drinking among patients attending a minor injuries unit: a pilot study', 'abstract': 'Excessive alcohol consumption increases the likelihood of accidental injury. This pilot study reports on the prevalence of hazardous drinkers presenting to a minor injuries unit. The proportion of hazardous drinkers is broadly similar to that found in emergency departments, suggesting that such units could also host alcohol intervention and brief advice activities.', 'corpus_id': 33124667, 'score': 0}, {'doc_id': '337390', 'title': 'Reasoning about Pragmatics with Neural Listeners and Speakers', 'abstract': 'We present a model for pragmatically describing scenes, in which contrastive behavior results from a combination of inference-driven pragmatics and learned semantics. Like previous learned approaches to language generation, our model uses a simple feature-driven architecture (here a pair of neural ""listener"" and ""speaker"" models) to ground language in the world. Like inference-driven approaches to pragmatics, our model actively reasons about listener behavior when selecting utterances. For training, our approach requires only ordinary captions, annotated _without_ demonstration of the pragmatic behavior the model ultimately exhibits. In human evaluations on a referring expression game, our approach succeeds 81% of the time, compared to a 69% success rate using existing techniques.', 'corpus_id': 337390, 'score': 1}, {'doc_id': '215768182', 'title': 'Don’t Give Me the Details, Just the Summary! Topic-Aware Convolutional Neural Networks for Extreme Summarization', 'abstract': 'We introduce “extreme summarization”, a new single-document summarization task which does not favor extractive strategies and calls for an abstractive modeling approach. The idea is to create a short, one-sentence news summary answering the question “What is the article about?”. We collect a real-world, large-scale dataset for this task by harvesting online articles from the British Broadcasting Corporation (BBC). We propose a novel abstractive model which is conditioned on the article’s topics and based entirely on convolutional neural networks. We demonstrate experimentally that this architecture captures long-range dependencies in a document and recognizes pertinent content, outperforming an oracle extractive system and state-of-the-art abstractive approaches when evaluated automatically and by humans.', 'corpus_id': 215768182, 'score': 1}, {'doc_id': '40418706', 'title': '[Mechanism of oxymatrine in preventing hepatic fibrosis formation in patients with chronic hepatitis B].', 'abstract': 'OBJECTIVE\nTo explore the mechanism of oxymatrine in preventing hepatic fibrosis formation in patients with chronic hepatitis B (CHB).\n\n\nMETHODS\nA total of 80 CHB patients receiving routine therapies for liver protection and support were divided into two groups. Oxymatrine at the daily dose of 150 mg was injected intravenously in the therapeutic group (n=40), and gluthion (1.2 g daily) was injected in the control group (n=40) for 8 weeks. The liver functions, indexes of hepatic fibrosis and the levels of transforming growth factor-beta1 (TGF-beta1), tumor necrosis factor-alpha (TNF-alpha) and interleukin-10 (IL-10) were measured in these patients before and after the therapy.\n\n\nRESULTS\nLiver functions was obviously improved after therapy in both groups, showing no significant difference between them (P>0.05). The indexes of hepatic fibrosis such as HA, LN, PCIII and C-IV were significantly lower in the therapeutic group than in the control group (P<0.01). The serum levels of TGF-beta1 and TNF-alpha decreased while IL-10 increased significantly after the treatment in the therapeutic group (P<0.05).\n\n\nCONCLUSION\nThe effect of oxymatrine against hepatic fibrosis is mediated by lowering the levels of TGF-beta1 and TNF-alpha and increasing the level of IL-10 in CHB patients.', 'corpus_id': 40418706, 'score': 0}, {'doc_id': '13752552', 'title': 'Newsroom: A Dataset of 1.3 Million Summaries with Diverse Extractive Strategies', 'abstract': 'We present NEWSROOM, a summarization dataset of 1.3 million articles and summaries written by authors and editors in newsrooms of 38 major news publications. Extracted from search and social media metadata between 1998 and 2017, these high-quality summaries demonstrate high diversity of summarization styles. In particular, the summaries combine abstractive and extractive strategies, borrowing words and phrases from articles at varying rates. We analyze the extraction strategies used in NEWSROOM summaries against other datasets to quantify the diversity and difficulty of our new data, and train existing methods on the data to evaluate its utility and challenges. The dataset is available online at summari.es.', 'corpus_id': 13752552, 'score': 0}, {'doc_id': '21850704', 'title': 'A Deep Reinforced Model for Abstractive Summarization', 'abstract': 'Attentional, RNN-based encoder-decoder models for abstractive summarization have achieved good performance on short input and output sequences. For longer documents and summaries however these models often include repetitive and incoherent phrases. We introduce a neural network model with a novel intra-attention that attends over the input and continuously generated output separately, and a new training method that combines standard supervised word prediction and reinforcement learning (RL). Models trained only with supervised learning often exhibit ""exposure bias"" - they assume ground truth is provided at each step during training. However, when standard word prediction is combined with the global sequence prediction training of RL the resulting summaries become more readable. We evaluate this model on the CNN/Daily Mail and New York Times datasets. Our model obtains a 41.16 ROUGE-1 score on the CNN/Daily Mail dataset, an improvement over previous state-of-the-art models. Human evaluation also shows that our model produces higher quality summaries.', 'corpus_id': 21850704, 'score': 1}, {'doc_id': '221339146', 'title': 'Neural Generation Meets Real People: Towards Emotionally Engaging Mixed-Initiative Conversations', 'abstract': 'We present Chirpy Cardinal, an open-domain dialogue agent, as a research platform for the 2019 Alexa Prize competition. Building an open-domain socialbot that talks to real people is challenging - such a system must meet multiple user expectations such as broad world knowledge, conversational style, and emotional connection. Our socialbot engages users on their terms - prioritizing their interests, feelings and autonomy. As a result, our socialbot provides a responsive, personalized user experience, capable of talking knowledgeably about a wide variety of topics, as well as chatting empathetically about ordinary life. Neural generation plays a key role in achieving these goals, providing the backbone for our conversational and emotional tone. At the end of the competition, Chirpy Cardinal progressed to the finals with an average rating of 3.6/5.0, a median conversation duration of 2 minutes 16 seconds, and a 90th percentile duration of over 12 minutes.', 'corpus_id': 221339146, 'score': 1}, {'doc_id': '218971825', 'title': 'Language (Technology) is Power: A Critical Survey of “Bias” in NLP', 'abstract': 'We survey 146 papers analyzing “bias” in NLP systems, finding that their motivations are often vague, inconsistent, and lacking in normative reasoning, despite the fact that analyzing “bias” is an inherently normative process. We further find that these papers’ proposed quantitative techniques for measuring or mitigating “bias” are poorly matched to their motivations and do not engage with the relevant literature outside of NLP. Based on these findings, we describe the beginnings of a path forward by proposing three recommendations that should guide work analyzing “bias” in NLP systems. These recommendations rest on a greater recognition of the relationships between language and social hierarchies, encouraging researchers and practitioners to articulate their conceptualizations of “bias”---i.e., what kinds of system behaviors are harmful, in what ways, to whom, and why, as well as the normative reasoning underlying these statements—and to center work around the lived experiences of members of communities affected by NLP systems, while interrogating and reimagining the power relations between technologists and such communities.', 'corpus_id': 218971825, 'score': 1}]"
159	"{'doc_id': '122179696', 'title': 'Quantum electrodynamics in the presence of dielectrics and conductors. I. Electromagnetic-field response functions and black-body fluctuations in finite geometries', 'abstract': ""A form of quantum electrodynamics is developed which allows us to treat a number of problems involving dielectric and conducting surfaces, the presence of which leads to a number of new observable effects. A number of suitably defined response functions play a basic role in the present approach, as these in conjunction with the fluctuation-dissipation theorem lead to electromagnetic field correlation functions, which describe physical effects such as lifetimes, frequency shifts of the excited states, dispersion forces, etc. The quantization of the electromagnetic field is only implicitly used. A large part of the present paper is devoted to the calculation of the response functions involving different geometries and various types of dielectrics. Both spatially dispersive and spatially nondispersive dielectrics are considered. The response functions are calculated using Maxwell's equations and the usual boundary conditions at the interface adjoining the two mediums. As a first application of the present approach, the black-body fluctuations in finite geometries and the influence of surfaces on its temporal and spatial coherence are studied. An interesting theorem is also proved which enables us to calculate the normally ordered (antinormally ordered) correlation functions from the symmetrized correlation functions."", 'corpus_id': 122179696}"	7499	[{'doc_id': '221186592', 'title': 'Octupole correlations in light actinides from the interacting boson model based on the Gogny energy density functional', 'abstract': 'The quadrupole-octupole coupling and the related spectroscopic properties have been studied for the even-even light actinides $^{218-238}$Ra and $^{220-240}$Th. The Hartree-Fock-Bogoliubov approximation, based on the Gogny-D1M energy density functional, has been employed as a microscopic input, i.e., to obtain (axially symmetric) mean-field potential energy surfaces as functions of the quadrupole and octupole deformation parameters. The mean-field potential energy surfaces have been mapped onto the corresponding bosonic potential energy surfaces using the expectation value of the $sdf$ Interacting Boson Model (IBM) Hamiltonian in the boson condensate state. The strength parameters of the $sdf$-IBM Hamiltonian have been determined via this mapping procedure. The diagonalization of the mapped IBM Hamiltonian provides energies for positive- and negative-parity states as well as wave functions which are employed to obtain transitional strengths. The results of the calculations compare well with available data from Coulomb excitation experiments and point towards a pronounced octupole collectivity around $^{224}$Ra and $^{226}$Th.', 'corpus_id': 221186592, 'score': 0}, {'doc_id': '221186749', 'title': 'Exceptional drag enhancement of electron-phonon transport properties in 3C-SiC from fully coupled ab-initio analysis', 'abstract': 'We carry out novel ab-initio calculations of fully coupled electron and phonon transport and show that mutual drag causes the thermopower to be dominated by transport of phonons, rather than electrons, at room temperature in the case of \\textit{n}-doped 3C-SiC. The thermopower is insensitive to impurity scattering. Phonon drag also strongly boosts the intrinsic electron mobility, thermal conductivity and the Lorenz number. This work establishes the roles of microscopic scattering mechanisms in the emergence of strong drag effects in transport of the interacting electron-phonon gas.', 'corpus_id': 221186749, 'score': 0}, {'doc_id': '118837470', 'title': 'Deflection of an atomic beam by the Casimir force', 'abstract': 'The force experienced by an atom inside a parallel‐plate waveguide is one of the basic phenomena of cavity QED. The full QED expression for this force is a complicated function, which depends upon the atomic oscillator strengths, the position of the atom, and the width of the cavity. For ground‐state atoms one can distinguish two simple limits. (i) When the gap is sufficiently small, the force takes on the form of a van der Waals interaction between the instantaneous electric dipole of the atom and its multiple images in the walls of the waveguide. (ii) In a large gap, when the atom is far from the walls, the van der Waals force is suppressed and the main force predicted by theory is due to the Casimir interaction of the atom with the cavity vacuum field. Whereas the van der Waals force between atoms and conductors has previously been studied experimentally, the Casimir force has not. Here we report the first observation of atom deflection by the Casimir force.', 'corpus_id': 118837470, 'score': 1}, {'doc_id': '220968903', 'title': 'Macroscopic QED for quantum nanophotonics: emitter-centered modes as a minimal basis for multiemitter problems', 'abstract': 'Abstract We present an overview of the framework of macroscopic quantum electrodynamics from a quantum nanophotonics perspective. Particularly, we focus our attention on three aspects of the theory that are crucial for the description of quantum optical phenomena in nanophotonic structures. First, we review the light–matter interaction Hamiltonian itself, with special emphasis on its gauge independence and the minimal and multipolar coupling schemes. Second, we discuss the treatment of the external pumping of quantum optical systems by classical electromagnetic fields. Third, we introduce an exact, complete, and minimal basis for the field quantization in multiemitter configurations, which is based on the so-called emitter-centered modes. Finally, we illustrate this quantization approach in a particular hybrid metallodielectric geometry: two quantum emitters placed in the vicinity of a dimer of Ag nanospheres embedded in a SiN microdisk.', 'corpus_id': 220968903, 'score': 1}, {'doc_id': '226300096', 'title': 'Optimization of second-harmonic generation from touching plasmonic wires', 'abstract': 'We employ transformation optics to optimize the generic nonlinear wave interaction of second-harmonic generation from a pair of touching metallic wires. We demonstrate a 10 orders of magnitude increase in the second-harmonic scattering cross-section by increasing the background permittivity and a 5 orders of magnitude increase in efficiency with respect to a single wire. These results have clear implications for the design of nanostructured metallic frequency-conversion devices. Finally, we exploit our analytic solution of a non-trivial nanophotonic geometry as a platform for performing a critical comparison of the strengths, weaknesses and validity of other prevailing theoretical approaches previously employed for nonlinear wave interactions at the nanoscale.', 'corpus_id': 226300096, 'score': 0}, {'doc_id': '221095594', 'title': 'van der Waals coefficients of the multi-layered MoS$_2$ with alkali metals', 'abstract': 'The van der Waals coefficients and the separation dependent retardation functions of the interactions between the atomically thin films of the multi-layered transition metal molybdenum disulfide (MoS$_2$) dichalcogenides with the alkali atoms are investigated. First, we determine the \nfrequency-dependent dielectric permittivity and intrinsic carrier density values for different layers of MoS$_2$ by adopting various fitting models to the recently measured optical data reported by Yu and co-workers [Sci. Rep. {\\bf 5}, 16996 (2015)] using spectroscopy ellipsometry. Then, dynamic electric dipole polarizabilities of the alkali atoms are evaluated very accurately by employing the relativistic coupled-cluster theory. We also demonstrate the explicit change in the above coefficients for different number of layers. These studies are highly useful for the optoelectronics, sensing and storage applications using layered MoS$_2$.', 'corpus_id': 221095594, 'score': 1}, {'doc_id': '220525312', 'title': 'Anomaly of the dielectric function of water under confinement and its role in van der Waals interactions.', 'abstract': 'We present a theoretical calculation of the changes in the Hamaker constant due to the anomalous reduction of the static dielectric function of water. Under confinement, the dielectric function of water decreases from a bulk value of 80 down to 2. If the confining walls are made of a dielectric material, the Hamaker constant reduces by almost 90%. However, if the confinement is realized with metallic plates, there is little change in the Hamaker constant. Additionally, we show that confinement can be used to decreases the Debye screening length without changing the salt concentration. This in turn is used to change the Hamaker constant in the presence of electrolytes.', 'corpus_id': 220525312, 'score': 0}, {'doc_id': '195855704', 'title': 'Abstract Submitted for the DAMOP09 Meeting of The American Physical Society Nano-gratings and the atom-surface Van der Waals interaction1', 'abstract': 'Submitted for the DAMOP09 Meeting of The American Physical Society Nano-gratings and the atom-surface Van der Waals interaction1 VINCENT LONIJ, WILL HOLMGREN, BEN MCMORRAN, ALEX CRONIN, University of Arizona — Nano-gratings are used in several atomand electroninterferometers as coherent beamsplitters. Diffraction from these nano-gratings can be studied to observe the effect of the Van der Waals atom-surface interaction. In addition, these gratings have recently been used in atom-interferometers to detect a velocity dependent VdW induced phase shift. Determination of the VdW potential strength C3 from these studies, is limited by a lack of knowledge of the geometric parameters of the grating. Measurements of these parameters by conventional methods are plagued with several systematic errors. By studying diffraction of a beam of Na atoms at different angles of incidence, we are able to determine the geometric parameters with a precision that is competitive with conventional imaging methods. The precision is great enough to be able to notice the effect of atoms deposited on the gratings by the atom-beam. 1This work was supported by the NSF Vincent Lonij University of Arizona Date submitted: 23 Jan 2009 Electronic form version 1.4', 'corpus_id': 195855704, 'score': 1}, {'doc_id': '221186965', 'title': 'Unraveling the role of V-V dimer on the vibrational properties of VO$_2$ by first-principles simulations and Raman spectroscopic analysis', 'abstract': 'We investigate the vibrational properties of VO2, particularly the low temperature M1 phase by first-principles calculations using the density functional theory as well as Raman spectroscopy. We perform the structural optimization using SCAN meta-GGA functional and obtain the optimized crystal structures for metallic rutile and insulating M1 phases satisfying all expected features of the experimentally derived structures. Based on the harmonic approximation around the optimized structures at zero temperature, we calculate the phonon properties and compare our results with experiments. We show that our calculated phonon density of states is in excellent agreement with the previous neutron scattering experiment. Moreover, we reproduce the phonon softening in the rutile phase as well as the phonon stiffening in the M1 phase. By comparing with the Raman experiments, we find that the Raman-active vibration modes of the M1 phase is strongly correlated with the V-V dimer distance of the crystal structure. Our combined theoretical and experimental framework demonstrates that Raman spectroscopy could serve as a reliable way to detect the subtle change of V-V dimer in the strained VO$_2$.', 'corpus_id': 221186965, 'score': 0}, {'doc_id': '118343771', 'title': 'Atoms at Micrometer Distances from a Macroscopic Body', 'abstract': None, 'corpus_id': 118343771, 'score': 1}]
160	{'doc_id': '215768677', 'title': 'SPECTER: Document-level Representation Learning using Citation-informed Transformers', 'abstract': 'Representation learning is a critical ingredient for natural language processing systems. Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power. For applications on scientific documents, such as classification and recommendation, accurate embeddings of documents are a necessity. We propose SPECTER, a new method to generate document-level embedding of scientific papers based on pretraining a Transformer language model on a powerful signal of document-level relatedness: the citation graph. Unlike existing pretrained language models, Specter can be easily applied to downstream applications without task-specific fine-tuning. Additionally, to encourage further research on document-level models, we introduce SciDocs, a new evaluation benchmark consisting of seven document-level tasks ranging from citation prediction, to document classification and recommendation. We show that Specter outperforms a variety of competitive baselines on the benchmark.', 'corpus_id': 215768677}	5660	[{'doc_id': '220347127', 'title': 'On-The-Fly Information Retrieval Augmentation for Language Models', 'abstract': 'Here we experiment with the use of information retrieval as an augmentation for pre-trained language models. The text corpus used in information retrieval can be viewed as form of episodic memory which grows over time. By augmenting GPT 2.0 with information retrieval we achieve a zero shot 15% relative reduction in perplexity on Gigaword corpus without any re-training. We also validate our IR augmentation on an event co-reference task.', 'corpus_id': 220347127, 'score': 1}, {'doc_id': '231861432', 'title': 'Multi-Turn Dialogue Reading Comprehension With Pivot Turns and Knowledge', 'abstract': 'Multi-turn dialogue reading comprehension aims to teach machines to read dialogue contexts and solve tasks such as response selection and answering questions. The major challenges involve noisy history contexts and especial prerequisites of commonsense knowledge that is unseen in the given material. Existing works mainly focus on context and response matching approaches. This work thus makes the first attempt to tackle the above two challenges by extracting substantially important turns as pivot utterances and utilizing external knowledge to enhance the representation of context. We propose a pivot-oriented deep selection model (PoDS) on top of the Transformer-based language models for dialogue comprehension. In detail, our model first picks out the pivot utterances from the conversation history according to the semantic matching with the candidate response or question, if any. Besides, knowledge items related to the dialogue context are extracted from a knowledge graph as external knowledge. Then, the pivot utterances and the external knowledge are combined together with a well-designed mechanism for refining predictions. Experimental results on four dialogue comprehension benchmark tasks show that our proposed model achieves great improvements on baselines. A series of empirical comparisons are conducted to show how our selection strategies and the extra knowledge injection influence the results.', 'corpus_id': 231861432, 'score': 0}, {'doc_id': '231951580', 'title': 'Less is More: Pre-training a Strong Siamese Encoder Using a Weak Decoder', 'abstract': 'Many real-world applications use Siamese networks to efficiently match text sequences at scale, which require high-quality sequence encodings. This paper pre-trains language models dedicated to sequence matching in Siamese architectures. We first hypothesize that a representation is better for sequence matching if the entire sequence can be reconstructed from it, which, however, is unlikely to be achieved in standard autoencoders: A strong decoder can rely on its capacity and natural language patterns to reconstruct and bypass the needs of better sequence encodings. Therefore we propose a new self-learning method that pretrains the encoder with a weak decoder, which reconstructs the original sequence from the encoder’s [CLS] representations but is restricted in both capacity and attention span. In our experiments on web search and recommendation, the pre-trained SEED-Encoder, “SiamEsE oriented encoder by reconstructing from weak decoder”, shows significantly better generalization ability when fine-tuned in Siamese networks, improving overall accuracy and few-shot performances. Our code and models will be released.', 'corpus_id': 231951580, 'score': 1}, {'doc_id': '219635207', 'title': 'Performance Result for Detection of COVID-19 using Deep Learning', 'abstract': 'The 2019 novel coronavirus (COVID-19), which has sprawled fleetly among masses residing in distant nations, had a prefatory juncture in China. From both a safeness and a lucrative outlook, it has staggered the world with its hasty diffusion with conjectural vicious generic repercussions for the masses. Consequent to the escalating cases daily, there is a constricted fraction of COVID-19 inspection kits acquirable in healthcare institutions. Ergo, to obviate COVID-19 propagating betwixt masses, it is imperative to enforce an instinctive unveiling network as a prompt jack legging diagnosis appendage. The contemplated method embroils a convolutional neural networkbased model, namely ResNet50, concerted with a Fully Connected Layer (FCL), reinforced by Rectified Linear Unit (ReLU) for the unearthing of coronavirus pneumonia imparted sufferer by harnessing chest X-ray radiographs. The endorsed classification model, i.e. resnet50 affirmed by FCL and ReLU, compassed accuracy of 94% for unearthing COVID-19. When equated to diverse classification models, the purported model is preeminent. The aftereffect is premised on the attested X-ray images from the data appropriable in the arsenal of Kaggle.', 'corpus_id': 219635207, 'score': 0}, {'doc_id': '231709235', 'title': 'Evaluation of BERT and ALBERT Sentence Embedding Performance on Downstream NLP Tasks', 'abstract': 'Contextualized representations from a pre-trained language model are central to achieve a high performance on downstream NLP task. The pre-trained BERT and A Lite BERT (ALBERT) models can be fine-tuned to give state-of-the-art results in sentence-pair regressions such as semantic textual similarity (STS) and natural language inference (NLI). Although BERT-based models yield the [CLS] token vector as a reasonable sentence embedding, the search for an optimal sentence embedding scheme remains an active research area in computational linguistics. This paper explores on sentence embedding models for BERT and ALBERT. In particular, we take a modified BERT network with siamese and triplet network structures called Sentence-BERT (SBERT) and replace BERT with ALBERT to create Sentence-ALBERT (SALBERT). We also experiment with an outer CNN sentence-embedding network for SBERT and SALBERT. We evaluate performances of all sentence-embedding models considered using the STS and NLI datasets. The empirical results indicate that our CNN architecture improves ALBERT models substantially more than BERT models for STS benchmark. Despite significantly fewer model parameters, ALBERT sentence embedding is highly competitive to BERT in downstream NLP evaluations.', 'corpus_id': 231709235, 'score': 1}, {'doc_id': '231662488', 'title': 'Evaluating Multilingual Text Encoders for Unsupervised Cross-Lingual Retrieval', 'abstract': 'Pretrained multilingual text encoders based on neural Transformer architectures, such as multilingual BERT (mBERT) and XLM, have achieved strong performance on a myriad of supervised language understanding tasks. Consequently, they have been adopted as a state-of-the-art paradigm for multilingual and cross-lingual representation learning and transfer, rendering cross-lingual word embeddings (CLWEs) effectively obsolete. However, questions remain to which extent this finding generalizes 1) to unsupervised settings and 2) for ad-hoc cross-lingual IR (CLIR) tasks. Therefore, in this work we present a systematic empirical study focused on the suitability of the state-of-the-art multilingual encoders for cross-lingual document and sentence retrieval tasks across a large number of language pairs. In contrast to supervised language understanding, our results indicate that for unsupervised document-level CLIR – a setup in which there are no relevance judgments for task-specific fine-tuning – the pretrained encoders fail to significantly outperform models based on CLWEs. For sentence-level CLIR, we demonstrate that state-of-the-art performance can be achieved. However, the peak performance is not met using the general-purpose multilingual text encoders ‘off-the-shelf’, but rather relying on their variants that have been further specialized for sentence understanding tasks.', 'corpus_id': 231662488, 'score': 0}, {'doc_id': '230437704', 'title': 'Cross-Document Language Modeling', 'abstract': 'We introduce a new pretraining approach for language models that are geared to support multi-document NLP tasks. Our crossdocument language model (CD-LM) improves masked language modeling for these tasks with two key ideas. First, we pretrain with multiple related documents in a single input, via cross-document masking, which encourages the model to learn cross-document and long-range relationships. Second, extending the recent Longformer model, we pretrain with long contexts of several thousand tokens and introduce a new attention pattern that uses sequence-level global attention to predict masked tokens, while retaining the familiar local attention elsewhere. We show that our CD-LM sets new state-of-the-art results for several multi-text tasks, including crossdocument event and entity coreference resolution, paper citation recommendation, and documents plagiarism detection, while using a significantly reduced number of training parameters relative to prior works1.', 'corpus_id': 230437704, 'score': 1}, {'doc_id': '232185406', 'title': 'Does the Magic of BERT Apply to Medical Code Assignment? A Quantitative Study', 'abstract': 'Unsupervised pretraining is an integral part 001 of many natural language processing sys002 tems, and transfer learning with language mod003 els has achieved remarkable results in down004 stream tasks. In the clinical application of 005 medical code assignment, diagnosis and pro006 cedure codes are inferred from lengthy clini007 cal notes such as hospital discharge summaries. 008 However, it is not clear if pretrained models 009 are useful for medical code prediction without 010 further architecture engineering. This paper 011 conducts a comprehensive quantitative analy012 sis of various contextualized language mod013 els’ performances, pretrained in different do014 mains, for medical code assignment from clin015 ical notes. We propose a hierarchical fine016 tuning architecture to capture interactions be017 tween distant words and adopt label-wise at018 tention to exploit label information. Con019 trary to current trends, we demonstrate that a 020 carefully trained classical CNN outperforms 021 attention-based models on a MIMIC-III subset 022 with frequent codes. Our empirical findings 023 suggest directions for building robust medical 024 code assignment models. 025', 'corpus_id': 232185406, 'score': 1}, {'doc_id': '231847008', 'title': 'Does She Wink or Does She Nod? A Challenging Benchmark for Evaluating Word Understanding of Language Models', 'abstract': 'Recent progress in pretraining language models on large corpora has resulted in significant performance gains on many NLP tasks. These large models acquire linguistic knowledge during pretraining, which helps to improve performance on downstream tasks via fine-tuning. To assess what kind of knowledge is acquired, language models are commonly probed by querying them with ‘fill in the blank’ style cloze questions. Existing probing datasets mainly focus on knowledge about relations between words and entities. We introduce WDLMPro (Word Definitions Language Model Probing) to evaluate word understanding directly using dictionary definitions of words. In our experiments, three popular pretrained language models struggle to match words and their definitions. This indicates that they understand many words poorly and that our new probing task is a difficult challenge that could help guide research on LMs in the future.', 'corpus_id': 231847008, 'score': 0}, {'doc_id': '219619560', 'title': 'Improving effectiveness of different deep learning-based models for detecting COVID-19 from computed tomography (CT) images', 'abstract': 'Computerized Tomography (CT )has a prognostic role in the early diagnosis of COVID-19 due to it gives both fast and accurate results. This is very important to help decision making of clinicians for quick isolation and appropriate patient treatment. In this study, we combine methods such as segmentation, data augmentation and the generative adversarial network (GAN) to improve the effectiveness of learning models. We obtain the best performance with 99% accuracy for lung segmentation. Using the above improvements we get the highest rates in terms of accuracy (99.8%), precision (99.8%), recall (99.8%), f1-score (99.8%) and roc acu (99.9979%) with deep learning methods in this paper. Also we compare popular deep learning-based frameworks such as VGG16, VGG19, Xception, ResNet50, ResNet50V2, DenseNet121, DenseNet169, InceptionV3 and InceptionResNetV2 for automatic COVID-19 classification. The DenseNet169 amongst deep convolutional neural networks achieves the best performance with 99.8% accuracy. The second-best learner is InceptionResNetV2 with accuracy of 99.65%. The third-best learner is Xception and InceptionV3 with accuracy of 99.60%.', 'corpus_id': 219619560, 'score': 0}]
161	{'doc_id': '210115504', 'title': 'EUROPEAN ACADEMIC RESEARCH, VOL', 'abstract': 'This study is conducted to investigate about the gaps and differences that exist between the target language and learners’ culture in the teaching of English Language through Oxford Modern English. At the same time, this research has also investigated the impacts of these differences on English language teaching, learning and cultural development of the ESL learner. The design of the research is mixed as data is collected from three text books through purposive sampling then arranged in tabulated form and percentage is presented in graphs by following the quantitative method. On the other hand, interpretation is given qualitatively according to the own observation of the researcher and data is categorized by applying the model of Byram et al. (1994). Therefore, this is a content based mixed method study. This study is completed under the main stream of academic discourse analysis by applying the model of Byram et al. (1994) to describe the gaps between the contents of these ELT Textbooks and the learners’ society on sociological, religious and cultural level. Findings show that there are many cultural differences and a dire need is to review, revise or replace these foreign cultural contents in ELT/EFL text books written by foreign authors who are totally unaware about the learners’ culture but using it with the amalgamation of target culture that is creating problems for the learners in the acquisition of this language. This discourse analysis is locating the gaps to overcome and achieve the objectives of ELT. Nilofar, Munazza Shaheen, Asifa Nosheen, Marium Bashir, Shumila AnjumRepresentation of Target Culture in Oxford Modern English Textbooks: Discourse Analysis EUROPEAN ACADEMIC RESEARCH Vol. VI, Issue 9 / December 2018 5264', 'corpus_id': 210115504}	9095	"[{'doc_id': '220883655', 'title': 'Learning and Teaching Online During Covid-19: Experiences of Student Teachers in an Early Childhood Education Practicum', 'abstract': 'Online learning is an educational process which takes place over the Internet as a form of distance education. Distance education became ubiquitous as a result of the COVID-19 pandemic during 2020. Because of these circumstances, online teaching and learning had an indispensable role in early childhood education programs, even though debates continue on whether or not it is beneficial for young children to be exposed extensively to Information and Communication Technology (ICT). This descriptive study demonstrates how a preservice teacher education course in early childhood education was redesigned to provide student teachers with opportunities to learn and teach online. It reports experiences and reflections from a practicum course offered in the Spring Semester of 2020, in the USA. It describes three phases of the online student teachers’ experiences–Preparation, Implementation, and Reflection. Tasks accomplished in each phase are reported. Online teaching experiences provided these preservice teachers with opportunities to interact with children, as well as to encourage reflection on how best to promote young children’s development and learning with online communication tools.', 'corpus_id': 220883655, 'score': 0}, {'doc_id': '221728668', 'title': ""The Impact of E-Learning Strategy on Students ' Academic Achievement"", 'abstract': ""This study examines the effect of E-learning during COVID-19 pandemic on the students’ Academic Achievement at Al-Quds Open University. The study has randomly selected 382 students’ GPA from the University's official records. It is mainly based on Statistical Package for Social Sciences program (SPSS.20) to make Paired Samples T-test and to study the hypotheses. The study has revealed that there are statistically significant differences in the students’ Academic Achievements during the implementation of the E-learning strategy in COVID-19 pandemic. This study shows that in general the GPA of students has increased about 2.188 points; but in particular the GPA of male students is affected more than female’s by just a slight difference of 1,198 point. On the other hand, looking at the Program of Study at the University; the Community Service is affected most with an increase by 3,276 points, Then Business Administration, Accounting and Finance are respectively affected more with having 2.6 points more on students’ GPA. However, the greatest effect on GPA is largely noticed on the students whose GPA is low in which the increase is about 6.568 points. The study results shows the importance of the implementation of E-learning strategy in higher education institutions so as to improve the students’ Academic Achievements. In addition, it sheds the light on the necessity of taking into consideration the specific features of some learning programs such as the Arabic Language and the Social Studies."", 'corpus_id': 221728668, 'score': 1}, {'doc_id': '227125675', 'title': 'Educating Civic-Minded Engineers: A Qualitative Study of First-Year Engineering Students', 'abstract': 'This Work-in-Progress Research paper introduces a qualitative study about engineering students’ perceptions and experiences with civic engagement. This study extends the scholarship on civic engagement in higher education into a context that has received relatively little attention: the engineering profession. As professionals, engineers have a responsibility to serve public welfare and community interests. Engineers can contribute their skills to serve community needs and improve the livelihoods of community members. Postsecondary engineering education provides a valuable opportunity to help students develop an appreciation for the civic aspects of the profession. As a result, it is important to understand how engineering programs influence students’ civic-minded dispositions. Using a social cognitive approach, this study seeks to understand how civic mindedness manifests in engineering students’ pre-curricular, curricular, and co-curricular experiences and how these experiences shape their perceptions of civics within the engineering profession. This paper outlines the theoretical approach and research design of this study, including data collection methods and validity considerations. The paper concludes with a summary of future work.', 'corpus_id': 227125675, 'score': 0}, {'doc_id': '221691590', 'title': 'The Development of Financial Management Textbook Base on Problem-Based Learning With Blended Learning Approach', 'abstract': 'The now days education is hoped to be a learning process which the students as the central of learning activity. This way of teaching is aimed to increase the comprehension of the students toward the material independently. In other way, the application of the technology in teaching process is needed in order to make the teaching process more comprehensive. This point of view emerges the implementation of blended learning which combine offline and online to lead the students to the broader knowledge comprehension. The objective of this research is to find the different of the problems-based learning finance management textbooks implementation and proper when the books are used in teaching process with blended learning approach. The method of this research is RnD (Research and Development) while the subjects of this research are 12 students. The stages of this research are: 1) The previous researches, 2) problems identification, 3) Data collecting and textbooks designing, 4) textbooks validation by experts, 5) Textbooks revision, 6) Small Scale trial, 7) ongoing textbooks revision, 8) textbooks trial, 9) the textbook final revision. The data analysis technique in this research is textbooks validation data, and static comparative analysis. The analysis is helped by SPSS software. The result of the research produces a valid problem-based learning finance management textbooks to apply in lecturing. The significant increase of studying result is found before and after the finance management textbooks. The comparative statistic found the significant different in problem-based learning finance management textbooks taught by using blended leaning approach.', 'corpus_id': 221691590, 'score': 0}, {'doc_id': '221152230', 'title': 'Measuring teaching quality, designing tests, and transforming feedback targeting various education actors', 'abstract': 'Education systems across the world face various challenges which may be caused or increased by global mobility, economic and political crises and more recently, pandemics such as Covid-19. Eventually, dealing with some challenges becomes part of educators’ daily business but still requires flexibility whilst maintaining focus on core issues like educational quality. This issue of EAEA focuses on educational quality regarding (1) measuring teaching quality across countries and regions, (2) designing tests and test formats and (3) providing and transforming feedback to teachers and migrant students for learning and change.', 'corpus_id': 221152230, 'score': 1}, {'doc_id': '221459540', 'title': 'From Study-Abroad to Study-at-Home: Teaching Cross-Cultural Design Thinking During COVID-19', 'abstract': 'Undergraduate degrees at The University of Texas at Austin (UT Austin) require ‘‘Flags’’ in each of six areas: (1) Cultural Diversity in the United States; (2) Ethics; (3) Global Cultures; (4) Independent Inquiry; (5) Quantitative Reasoning; and (6) Writing. Courses that carry the Global Cultures Flag guide students in exploring the practices, beliefs, and histories of at least one non-US cultural group, and encourage reflection on one’s own cultural experiences. Many engineering students are excited to learn about global cultures in the context of their discipline through faculty-led study-abroad. Currently, there are 11 engineering courses carrying a Global Cultures Flag, 10 of which are faculty-led study-abroad. Unfortunately, the COVID-19 pandemic has prevented almost 400 of our engineering students from participating in faculty-led study-abroad. Our challenge was to quickly transition a faculty-led study-abroad carrying a Global Cultures Flag to an online format.', 'corpus_id': 221459540, 'score': 1}, {'doc_id': '221860819', 'title': 'AN ACTIVE LEARNING APPROACH FOR A DESIGN THINKING COURSE', 'abstract': 'Design research has always tried to shed light on the way designers think, opening a variety of discourses on this topic, both in the professional and in the academic fields. On the other side, in Politecnico di Milano design students are exposed to “lear', 'corpus_id': 221860819, 'score': 1}, {'doc_id': '221972246', 'title': 'Alina Schartner, Tony J. Young: Intercultural transitions in higher education: international student adjustment and adaptation', 'abstract': 'COVID-19 has shed a light on what it is like to be international students and the issues that they face in higher education (Cheung, 2020). Although there has already been a burgeoning literature on international students’ experiences, research in this area is largely fragmented and under-theorised, since different research fields (e.g. cultural studies, applied linguistics, social psychology, and educational research) have adopted different approaches to investigate international student mobility. Schartner and Young integrates a number of theoretical and methodological approaches to address this issue by presenting a conceptual model and empirical findings on the multifaceted experiences of mobile students, based on a research project in British higher education. The book comprises three parts. Part 1 addresses the theoretical and methodological approaches of the research project presented in the subject matter, Part 2 focuses on the empirical data informing the new model of international student experiences, and Part 3 explores the implications and applications of the findings. Throughout Chap. 1, the authors contextualise their study within the current political and social circumstances, providing a rationale for how ecumenical approaches to interculturality best fulfil their research purposes. The introductory chapter also clarifies the meaning of keywords, such as ‘adjustment’ and ‘adaptation’, which have been used ambiguously by earlier researchers. Chapter 2 outlines previous research on the transitional experiences of international students. It explores the multifaceted nature of the issue, which requires academic, psychological, and sociocultural adjustment/adaptation. The chapter also summarises earlier findings on the major factors including linguistic and sociocultural resources that contribute to the successful transition of mobile students. The authors turn to a methodological discussion in Chap. 3 and emphasise how a mixed-method design can (1) address the dichotomy between positivist and interpretivist approaches to intercultural research and (2) synthesise both adjustment and adaptation, the process and outcome of transition. Chapters 4–8 provide empirical data, both quantitative and qualitative, on the adjustment and adaptation experiences of international postgraduate students, which were', 'corpus_id': 221972246, 'score': 1}, {'doc_id': '227138977', 'title': 'Teaching Engineering Ethics With Drama', 'abstract': 'In this Work in Progress, we present a new, simple, and fun way of incorporating drama into engineering ethics education with students producing, performing, and watching drama. The method differs from more established drama-based pedagogies in engineering ethics education, such as role-plays. We argue that this method can contribute to learning by stimulating moral imagination, empathy and sympathy, which balances the otherwise cognitive focus in engineering ethics courses. The component has been tested in practice three times in 2019-2020 in the Engineering Ethics course at Uppsala University. Student feedback is used to illustrate how the method has been perceived by students.', 'corpus_id': 227138977, 'score': 0}, {'doc_id': '227160516', 'title': 'Going beyond traditional approaches on industrial engineering education', 'abstract': 'This Research-to-Practice full paper refers to academic perspectives on educational innovation for industrial engineering education. Two common views prevail in educational innovation that turn into different results. One view refers to the use of pedagogical approaches to improve in-classroom students’ learning. This is an operational perspective about teaching activities, instructional facilitation and the use of academic resources. The second view refers to educational value creation for students, educational partners, society and to improve the academic positioning of universities. However, both views complement each other and can articulate a holistic approach on educational innovation. To proceed in this direction, this work unfolds in three parts. First, a literature review illustrates the differences between the two complementary views. Second, a conceptual framework is provided to connect the two perspectives and guide further educational innovation efforts. Third, a descriptive and exploratory application case is offered to exemplify the framework at the MIT Supply Chain and Logistics Excellence (SCALE) Latin America Network for industrial engineering education. This work contributes to educational practice with a tool to reflect upon innovation efforts, identify instances and align initiatives with intended educational purposes.', 'corpus_id': 227160516, 'score': 0}]"
162	{'doc_id': '3842591', 'title': 'Altmetric Versus Bibliometric Perspective Regarding Publication Impact and Force', 'abstract': 'BackgroundBibliometric and Altmetric analyses highlight key publications, which have been considered to be the most influential in their field. The hypothesis was that highly cited articles would correlate positively with levels of evidence and Altmetric scores (AS) and rank.MethodsSurgery as a search term was entered into Thomson Reuter’s Web of Science database to identify all English-language full articles. The 100 most cited articles were analysed by topic, journal, author, year, institution, and AS.ResultsBy bibliometric criteria, eligible articles numbered 286,122 and the median (range) citation number was 574 (446–5746). The most cited article (Dindo et al.) classified surgical complications by severity score (5746 citations). Annals of Surgery published most articles and received most citations (26,457). The country and year with most publications were the USA (n\u2009=\u200950) and 1999 (n\u2009=\u200911). By Altmetric criteria, the article with the highest AS was by Bigelow et al. (AS\u2009=\u200953, hypothermia’s role in cardiac surgery); Annals of Surgery published most articles, and the country and year with most publications were USA (n\u2009=\u20094) and 2007 (n\u2009=\u20093). Level-1-evidence articles numbered 13, but no correlation was found between evidence level and citation number (SCC 0.094, p\u2009=\u20090.352) or AS (SCC\u2009=\u20090.149, p\u2009=\u20090.244). Median AS was 0 (0–53), and in articles published after the year 2000, AS was associated with citation number (r\u2009=\u20090.461, p\u2009=\u20090.001) and citation rate index (r\u2009=\u20090.455, p\u2009=\u20090.002). AS was not associated with journal impact factor (r\u2009=\u20090.160, p\u2009=\u20090.118).ConclusionBibliometric and Altmetric analyses provide important but different perspectives regarding article impact, which are unrelated to evidence level.', 'corpus_id': 3842591}	20706	"[{'doc_id': '237943894', 'title': 'Bibliometric analysis of the top 50 most cited publications of the Journal of Clinical Orthopaedics and Trauma.', 'abstract': 'Background\nThe Journal of Clinical Orthopaedics and Trauma (JCOT) is one of the top three orthopaedic journals from India. We set out to analyse the top 50 cited articles from JCOT since indexing in PubMed and Scopus.\n\n\nMethods\nWe looked into the bibliometrics of the top 50 cited articles and compared citations from PubMed and Scopus, and depicted outputs from VOS viewer analysis on co-authorship and keywords.\n\n\nResults\nTotal citations for top-cited articles were 1076 in numbers, with a maximum of 103.2016 and 2018 were the most productive years. The major contribution was from India with 74%, followed by the USA. New Delhi published maximally at 72%. Clinical topics and narrative reviews were the most common types of studies. Trauma and Adult reconstruction was the most common sub-specialities, and Level 4 was the most frequent level of study. The basic science and COVID-19 related articles received the maximum citations. The authors from Indraprastha Apollo Hospitals published the maximum number of top-50 cited articles in the JCOT.\n\n\nConclusions\nThere is a steady increase in the number of publications in the JCOT, with an increasing number of citation counts. Both the Indian and foreign authors have been publishing in this journal at a comparative rate. Although the citation counts in Scopus are more than those in PubMed for given articles, more than 80% of articles are listed in both databases as top 50 cited articles. The majority of top-cited articles belonged to trauma and adult reconstruction, level III studies, and narrative reviews.', 'corpus_id': 237943894, 'score': 0}, {'doc_id': '237561744', 'title': 'Bibliometric analysis of manuscript characteristics that influence citations: A comparison of four major neurology journals', 'abstract': 'Objective: To inspect 28 data characteristics among the top neurology journals with the highest impact factor and their influence on citation rate. Methods: Consecutive articles from January 2004 to June 2004 were collected from four major neurology journals with the highest impact factor: The Lancet Neurology (impact factor, 11.964), Acta Neuropathologica (7.589), Brain (5.858) and Annals of Neurology (5.706). Web of Science was used to extract the citation count for these articles, and 28 article characteristics were tabulated manually. Univariate analysis and a multiple regression model were performed to predict citation number from the collected variables. Results: A total of 288 manuscripts i.e. 24 in The Lancet Neurology, 70 in Acta Neuropathologica, 117 in Brain and 77 in Annals of Neurology. Univariate analysis revealed the following variables to have a significant positive correlation with increased citations: journal (1; p<0.0001), country of origin (15; p<0.0001), number of tables (28; p=0.0007), words per title (7; p=0.0006), design of study (17; p=0.001), open access (22; p<0.0001), total words (24; p<0.0001), total references (25; p<0.0001) and total number of pages (26; p<0.0001). In a multivariate regression model the following variables predicted increased citation count (p < 0.0001, R2 = 0.4377): number of pages, open access status, multicenter studies and journal origin. Conclusion: The results of our bibliometric study may be used by authors while compiling their manuscript to increase recognition and improve the impact of their articles over what they would normally experience.', 'corpus_id': 237561744, 'score': 1}, {'doc_id': '237292881', 'title': 'Social Media and Academic Impact: Do Early Tweets Correlate With Future Citations?', 'abstract': ""OBJECTIVE\nDetermine whether social media platforms can influence article impact as measured by citations.\n\n\nMETHODS\nCross-sectional study that analyzed articles published in the top 10 otolaryngology journals by Eigenfactor score in January 2015. Total accumulated Twitter mentions and citations were recorded in 2021. The main outcomes examined the difference in citations, tweets, article types, and author counts accumulated over a 5-year period for all articles that were either tweeted or nontweeted.\n\n\nRESULTS\nA total of 3094 articles were included for analysis. The average article was cited 11.2 ± 13.2 times and tweeted 2.10 ± 4.0 times. Sixty-four percent of the articles had at least one tweet. Over the study period, there was a statistically significant difference in mean number of citations between tweeted articles (12.1 ± 15.0) versus nontweeted articles (9.6 ± 10.5) citations, representing a 26% difference (P < .001). Review articles had the highest mean citations (19.4 ± 23.4) while editorials had the lowest mean citations (2.8 ± 6.9). Tweets peaked in the year of publication, but citations continued to rise in the subsequent years. Tweeted articles' peak citation rate change was +1.27 mean citations per year, compared to +0.99 mean citations per year in nontweeted articles. The mean author count in tweeted articles (5.40 ± 3.1) was not significantly different than the mean author count in nontweeted articles (5.19 ± 2.65, P = .0794).\n\n\nCONCLUSION\nThese data suggest a moderate correlation between tweets and article citations, but a clear difference in the number of citations in articles tweeted versus those with no tweets. Thus, dissemination of knowledge may be impacted by social medial platforms such as Twitter."", 'corpus_id': 237292881, 'score': 1}, {'doc_id': '237950728', 'title': 'Relationship Between the Journal Self-Citation and Author Self-Citation and the Impact Factor in Iranian, American, and European Institute for Scientific Information Indexed Medical Journals in 2014-2021', 'abstract': ""Background: Author and journal self-citation contributes to the overall citation count of an article and the impact factor (IF) of the journal in which it appears. However, little is known about the extent of self-citation in the general clinical medicine literature. This study aimed to determine the effect of self-citation (journal and author) on the IF of Iranian, American, and European English medical journals. Materials and Methods: IF, IF without self-citations (corrected IF), journal self-citation rate, and author self-citation rate for medical journals were investigated from 2014–2021 by reviewing the Journal Citation Report (JCR). Twenty Iranian medical journals (published in English) in the web of science indexed were selected and compared with 20 American and 20 European medical journals. The correlation between the journal self-citation and author self-citation with IF was obtained. We used Spearman’s correlation coefficient for the correlation of self-citation and IF. Results: The overall journal citations were higher in the American and European journals compared to the Iranian ones between 2014 and 2021. There was a significant relationship between journal self-citation rates and IF (P<0.001). The findings demonstrated that the rate of author self-citation was higher in the European and American journals compared to the Iranian ones. Findings also showed a significant difference between the author's self-citation and IF in Iranian, American, and European medical journals that published in English (P<0.001). Conclusions: The self-citation rate positively affects the IF in medical journals. A high concentration of self-citation of some journals could distort the ranking among medical surgery journals in general."", 'corpus_id': 237950728, 'score': 1}, {'doc_id': '236948039', 'title': 'The Top 50 Most Cited Publications in Meniscus Research.', 'abstract': 'Injuries to the menisci of the knee are common in orthopedic sports medicine. Bibliometric studies can identify the core literature on a topic and help further our collective knowledge for both clinical and educational purposes. The purpose of the current study was to (1) identify and describe the 50 most cited articles in meniscus research over an 80-year time period to capture a wide range of influential articles and (2) identify the ""citation classics"" and milestone articles related to the meniscus of the knee. The Science Citation Index Expanded subsection of the Web of Science Core Collection was systematically searched for the 50 most cited meniscus articles. Data pertaining to bibliometric and publication characteristics were extracted and reported using descriptive statistics. The top 50 articles were published between the years 1941 and 2014 and collectively cited 13,152 times. The median (interquartile [IQR]) number of total citations per article was 203.5 (167.0-261.8), while the median citation rate was 9.6 (7.4-13.9) citations per year. The most cited article was ""Knee joint changes after meniscectomy,"" published in 1948. The article with the highest citation rate of 78.4 citations per year was ""The long-term consequence of anterior cruciate ligaments and meniscus injuries - osteoarthritis,"" published in 2007. The majority of articles were clinical outcome studies (n\u2009=\u200928, 56%). The top 50 most cited meniscus articles represent a compilation of highly influential articles which may augment reading curriculums and provide a strong knowledge base for orthopaedic surgery residents and fellows. The decade with the most articles was the 2000s, representing a recent acceleration in meniscus-based research. This is a level IV, cross-sectional study.', 'corpus_id': 236948039, 'score': 0}, {'doc_id': '237468223', 'title': 'Global research trends in robotic application on spine: a systematic bibliometric analysis.', 'abstract': 'OBJECTIVE\nThe authors aimed to systematically evaluate the global research trends in robotic application on spine through bibliometric analysis and mapping knowledge domains.\n\n\nMETHODS\nA systematic literature search was performed on PubMed and Web of Science (WoS), including the Science Citation Index Expanded (SCIE) database. The number of publications, countries of publications, journals of publications, authors of publications, total citation, average publication year and institution sources were analyzed by Microsoft Excel, Online Analysis Platform of Bibliometrics and VOSviewer. Hotspots were analyzed and visualized based on VOSviewer.\n\n\nRESULTS\nA total of 2135 publications were identified in this study. The United States ranked first in the number of publications (824, 38.63%) and the frequency of citations (29075). Northwestern University had highest number of publications (67) and Harvard university had highest number of citations (4198). Journal of NeuroEngineering and Rehabilitation has published the largest number of papers (73) and the most frequently cited journal is Nature (3844 citations). Research hotspots were divided into three categories analyzed by VOSviewer: rehabilitation, basic science and surgery. According to the average publication year, the most recent hotspot was radiation exposure and the earliest hotspot was radiosurgery.\n\n\nCONCLUSIONS\nThe number of literatures of robotic application on spine has continued to increase. The United States is the most contributor to robotic applications on spine. The robot-assisted rehabilitation for neurological and orthopedic lesions is still a major research hotspot. The range of robotic applications on spine has expanded from assisted rehabilitation to assisted rehabilitation and surgery.', 'corpus_id': 237468223, 'score': 0}, {'doc_id': '237386104', 'title': 'Quotation Errors in High-Impact-Factor Orthopaedic and Sports Medicine Journals', 'abstract': 'Background: Inappropriate referencing of the existing literature has the potential to propagate false information. Quotation errors are defined as citations in which the referenced article fails to substantiate the authors’ claims. The aim of this study was to determine the rate of quotation errors in high-impact general orthopaedic and sports medicine journals and to determine whether there are article or journal-related factors that are related to the rate of inaccuracies. Methods: A total of 250 citations from the 5 orthopaedic and sports medicine journals with the highest impact factors in 2019 (per Journal Citation Reports) were chosen using a random sequence generator. Reviewers rated the chosen citations by comparing the claims made by the authors with the data and conclusions of the referenced source to determine whether quotation errors were present. Logistic regression was utilized to assess for article- and journal-related factors related to the rate of quotation errors. Results: The overall quotation error rate was 13.6%. A total of 2.8% of the claims were completely unsubstantiated. The number of quotation errors did not significantly differ between the included journals. Single citations were significantly more likely than string citations to result in citations that could not be fully substantiated (χ2 = 4.57; odds ratio = 2.22; 95% confidence interval = 1.06 to 4.66; p = 0.03). No relationship was found between the rate of quotation errors and the total number of citations in the article, study type, or the graded level of evidence of the article. Conclusions: Quotation errors in high-impact factor orthopaedic and sports medicine journals are common. This is particularly important given the higher likelihood that studies in these journals are cited elsewhere, thus propagating the inaccuracies. Efforts from both authors and journals are needed to reduce quotation errors in the orthopaedic literature.', 'corpus_id': 237386104, 'score': 0}, {'doc_id': '237241892', 'title': 'The Relationship Between Open Access Article Publishing and Short-Term Citations in Otolaryngology.', 'abstract': 'OBJECTIVES\nThe purpose of this study is to compare the number of citations received by open access articles versus subscription access articles in subscription journals in the Otolaryngology literature.\n\n\nMETHODS\nUsing the Dimensions research database, we examined articles indexed to PubMed with at least 5 citations published in 2018. Articles were included from Otolaryngology-Head and Neck Surgery, The Laryngoscope, JAMA Otolaryngology-Head and Neck Surgery, Annals of Otology, Rhinology, and Laryngology, and American Journal of Otolaryngology. Multivariate Poisson regression modeling was used to adjust for journal, article type, and topic. Practice guidelines, position statements, or retractions were excluded as potential outliers.\n\n\nRESULTS\n137 open access articles and 337 subscription access articles meeting inclusion criteria were identified, with a median citation number of 8 (IQR 6-11). The most common article type was original investigation (82.5%), and the most common study topic was head and neck (28.9%). Open access articles had a higher median number of citations at 9 (IQR 6-13) when compared to subscription access articles at 7 (IQR 6-10) (P\u2009=\u2009.032). Open access status was significantly associated with a higher number of citations than subscription access articles when adjusting for journal, article type, and topic (β\u2009=\u2009.272, CI 0.194-0.500, P\u2009<\u2009.001).\n\n\nCONCLUSIONS\nAlthough comprising a minority of articles examined in this study of subscription journals, open access articles were associated with a higher number of citations than subscription access articles. Open access publishing may facilitate the spread of novel findings in Otolaryngology.', 'corpus_id': 237241892, 'score': 1}, {'doc_id': '236942244', 'title': 'Examining the Relationship between Altmetric Score and Traditional Bibliometrics in the Ophthalmology Literature for 2013 and 2016 Cohorts', 'abstract': ""\n Background\u2003In this study, we reviewed a select sample of ophthalmology literature to determine if there was a correlation between Altimetric and traditional citation-based and impact factor metrics. We hypothesized that Altmetric score would more closely correlate with impact factor and citations in 2016.\n Methods\u2003Journal Citation Reports for the year 2013 was used to find the 15 highest impact factor ophthalmology journals in 2013. Then Elsevier's Scopus was used to identify the 10 most cited articles from each journal for the years 2013 and 2016. Metrics for all identified articles were collected using the Altmetric Bookmarklet, and date of Twitter account creation was noted for journals with such an account. Altmetric scores, impact factor, and citation counts were tabulated for each article. Pearson's correlation coefficient (r) determined correlation of independent variables (number of citations or impact factor) with dependent variable (Altmetric score). For our Twitter analysis, account age was the independent variable and calculated correlation coefficients (r) were the dependent variable. Proportion of variance was determined with a coefficient of determination (R\n 2).\n Results\u2003This study included 300 articles, evenly split between 2013 and 2016. Within the 2013 cohort, three journals had significant positive correlations between citation count and Altmetric score. For the 2016 cohort, both Altmetric score and citation count (r\u2009=\u20090.583, p\u2009<\u20090.001) and Altmetric score and impact factor (r\u2009=\u20090.183, p\u2009=\u20090.025) revealed significant positive correlations. In 2016, two journals were found to have significant correlations between Altmetric score and citation number. Neither year revealed a significant correlation between the age of a journal's Twitter profile and the relationship between Altmetric score and citation count. In each year, Twitter accounted for the highest number of mentions.\n Conclusion\u2003The findings suggest that correlation between Altmetric score and traditional quality metric scores may be increasing. Altmetric score was correlated with impact factor and number of citations in 2016 but not 2013. At this time, Altmetrics are best used as an adjunct that is complementary but not an alternative to traditional bibliometrics for assessing academic productivity and impact."", 'corpus_id': 236942244, 'score': 1}, {'doc_id': '237404706', 'title': 'Highly cited papers in Takayasu arteritis on Web of Science and Scopus: cross-sectional analysis', 'abstract': 'Takayasu arteritis (TAK) is a large vessel vasculitis affecting relatively younger population. Since literature on bibliometric analysis of TAK is scarce, we analyzed top-cited articles in TAK to address this knowledge gap. We analyzed the top hundred cited articles in TAK on Web of Science and Scopus for time of publication, article type, country of origin, source journal, and authors. Furthermore, we conducted univariable- and multivariable-adjusted linear regression analyses to explore associations of rank of cited articles, mean number of annual citations, and total citations with traditional (journal impact factor, CiteScore) and alternative (PlumX) metrics. Concordance between databases was 76%. Most top-cited articles were from the USA, Japan, or the UK, and published in Annals of the Rheumatic Diseases, Arthritis and Rheumatism, and Circulation and Rheumatology (Oxford). Original articles comprised a majority of these top-cited articles. Articles describing criteria or disease management recommendations received the highest mean number of citations. Performing multivariable-adjusted linear regression analyses, years of publication associated with mean annual citations on Web of Science as well as total citations across databases (p\u2009<\u20090.01). The 2-year JIF significantly associated with mean annual citations on Web of Science (p\u2009=\u20090.047). On Scopus, the number of captures denoted under PlumX metrics consistently associated with citations (p\u2009<\u20090.001). Both traditional and alternative metrics associate with higher citations in TAK. Development of disease assessment and clinical practice guidelines and conduct and reporting of randomized controlled trials to guide TAK management are important research areas. The key points themselves are all right. Key Points • A majority of top-cited papers in TAK are original articles. • Both traditional and alternative metrics associate with number of citations for these papers. • Recommendations for disease assessment or clinical practice and clinical trials are important research agenda in TAK. Key Points • A majority of top-cited papers in TAK are original articles. • Both traditional and alternative metrics associate with number of citations for these papers. • Recommendations for disease assessment or clinical practice and clinical trials are important research agenda in TAK.', 'corpus_id': 237404706, 'score': 0}]"
163	{'doc_id': '225103287', 'title': 'A Visuospatial Dataset for Naturalistic Verb Learning', 'abstract': 'We introduce a new dataset for training and evaluating grounded language models. Our data is collected within a virtual reality environment and is designed to emulate the quality of language data to which a pre-verbal child is likely to have access: That is, naturalistic, spontaneous speech paired with richly grounded visuospatial context. We use the collected data to compare several distributional semantics models for verb learning. We evaluate neural models based on 2D (pixel) features as well as feature-engineered models based on 3D (symbolic, spatial) features, and show that neither modeling approach achieves satisfactory performance. Our results are consistent with evidence from child language acquisition that emphasizes the difficulty of learning verbs from naive distributional data. We discuss avenues for future work on cognitively-inspired grounded language learning, and release our corpus with the intent of facilitating research on the topic.', 'corpus_id': 225103287}	11650	"[{'doc_id': '222379195', 'title': 'Natural Language Rationales with Full-Stack Visual Reasoning: From Pixels to Semantic Frames to Commonsense Graphs', 'abstract': 'Natural language rationales could provide intuitive, higher-level explanations that are easily understandable by humans, complementing the more broadly studied lower-level explanations based on gradients or attention weights. We present the first study focused on generating natural language rationales across several complex visual reasoning tasks: visual commonsense reasoning, visual-textual entailment, and visual question answering. The key challenge of accurate rationalization is comprehensive image understanding at all levels: not just their explicit content at the pixel level, but their contextual contents at the semantic and pragmatic levels. We present RationaleˆVT Transformer, an integrated model that learns to generate free-text rationales by combining pretrained language models with object recognition, grounded visual semantic frames, and visual commonsense graphs. Our experiments show that free-text rationalization is a promising research direction to complement model interpretability for complex visual-textual reasoning tasks. In addition, we find that integration of richer semantic and pragmatic visual features improves visual fidelity of rationales.', 'corpus_id': 222379195, 'score': 1}, {'doc_id': '222124957', 'title': 'Which *BERT? A Survey Organizing Contextualized Encoders', 'abstract': 'Pretrained contextualized text encoders are now a staple of the NLP community. We present a survey on language representation learning with the aim of consolidating a series of shared lessons learned across a variety of recent efforts. While significant advancements continue at a rapid pace, we find that enough has now been discovered, in different directions, that we can begin to organize advances according to common themes. Through this organization, we highlight important considerations when interpreting recent contributions and choosing which model to use.', 'corpus_id': 222124957, 'score': 0}, {'doc_id': '216035815', 'title': 'Experience Grounds Language', 'abstract': ""Successful linguistic communication relies on a shared experience of the world, and it is this shared experience that makes utterances meaningful. Despite the incredible effectiveness of language processing models trained on text alone, today's best systems still make mistakes that arise from a failure to relate language to the physical world it describes and to the social interactions it facilitates. \nNatural Language Processing is a diverse field, and progress throughout its development has come from new representational theories, modeling techniques, data collection paradigms, and tasks. We posit that the present success of representation learning approaches trained on large text corpora can be deeply enriched from the parallel tradition of research on the contextual and social nature of language. \nIn this article, we consider work on the contextual foundations of language: grounding, embodiment, and social interaction. We describe a brief history and possible progression of how contextual information can factor into our representations, with an eye towards how this integration can move the field forward and where it is currently being pioneered. We believe this framing will serve as a roadmap for truly contextual language understanding."", 'corpus_id': 216035815, 'score': 1}, {'doc_id': '227339390', 'title': 'Neurosymbolic AI for Situated Language Understanding', 'abstract': 'In recent years, data-intensive AI, particularly the domain of natural language processing and understanding, has seen significant progress driven by the advent of large datasets and deep neural networks that have sidelined more classic AI approaches to the field. These systems can apparently demonstrate sophisticated linguistic understanding or generation capabilities, but often fail to transfer their skills to situations they have not encountered before. We argue that computational situated grounding provides a solution to some of these learning challenges by creating situational representations that both serve as a formal model of the salient phenomena, and contain rich amounts of exploitable, task-appropriate data for training new, flexible computational models. Our model reincorporates some ideas of classic AI into a framework of neurosymbolic intelligence, using multimodal contextual modeling of interactive situations, events, and object properties. We discuss how situated grounding provides diverse data and multiple levels of modeling for a variety of AI learning challenges, including learning how to interact with object affordances, learning semantics for novel structures and configurations, and transferring such learned knowledge to new objects and situations.', 'corpus_id': 227339390, 'score': 1}, {'doc_id': '55361473', 'title': 'Geo-structural map of the Laguna Blanca basin (Southern Central Andes, Catamarca, Argentina)', 'abstract': 'The Laguna Blanca basin is a rhomb-shaped basin located at the SE margin of the Puna plateau in the southern Central Andes (Catamarca, Argentina). An interactive analysis using remote sensing and field mapping enabled us to produce a geo-structural map at a 1:350,000 scale. Satellite images from multispectral sensors (ASTER and Landsat 7 ETM+) and medium resolution Digital Elevation Models (SRTM and ASTER GDEM) were used in order to recognize the structures and main lithologies, which were validated in the field and through laboratory tests (e.g. spectral analysis). The final result is a geo-structural map of the Laguna Blanca basin with a new geological unit subdivision, highlighting its tectonic origin, which appears to be related to a releasing stepover along N-S sinistral strike-slip master faults.', 'corpus_id': 55361473, 'score': 0}, {'doc_id': '222124849', 'title': 'Syntax Representation in Word Embeddings and Neural Networks - A Survey', 'abstract': 'Neural networks trained on natural language processing tasks capture syntax even though it is not provided as a supervision signal. This indicates that syntactic analysis is essential to the understating of language in artificial intelligence systems. This overview paper covers approaches of evaluating the amount of syntactic information included in the representations of words for different neural network architectures. We mainly summarize re-search on English monolingual data on language modeling tasks and multilingual data for neural machine translation systems and multilingual language models. We describe which pre-trained models and representations of language are best suited for transfer to syntactic tasks.', 'corpus_id': 222124849, 'score': 0}, {'doc_id': '222208810', 'title': 'ALFWorld: Aligning Text and Embodied Environments for Interactive Learning', 'abstract': ""Given a simple request (e.g., Put a washed apple in the kitchen fridge), humans can reason in purely abstract terms by imagining action sequences and scoring their likelihood of success, prototypicality, and efficiency, all without moving a muscle. Once we see the kitchen in question, we can update our abstract plans to fit the scene. Embodied agents require the same abilities, but existing work does not yet provide the infrastructure necessary for both reasoning abstractly and executing concretely. We address this limitation by introducing ALFWorld, a simulator that enables agents to learn abstract, text-based policies in TextWorld (Cote et al., 2018) and then execute goals from the ALFRED benchmark (Shridhar et al., 2020) in a rich visual environment. ALFWorld enables the creation of a new BUTLER agent whose abstract knowledge, learned in TextWorld, corresponds directly to concrete, visually grounded actions. In turn, as we demonstrate empirically, this fosters better agent generalization than training only in the visually grounded environment. BUTLER's simple, modular design factors the problem to allow researchers to focus on models for improving every piece of the pipeline (language understanding, planning, navigation, visual scene understanding, and so forth)."", 'corpus_id': 222208810, 'score': 1}, {'doc_id': '222341606', 'title': 'Vokenization: Improving Language Understanding via Contextualized, Visually-Grounded Supervision', 'abstract': 'Humans learn language by listening, speaking, writing, reading, and also, via interaction with the multimodal real world. Existing language pre-training frameworks show the effectiveness of text-only self-supervision while we explore the idea of a visually-supervised language model in this paper. We find that the main reason hindering this exploration is the large divergence in magnitude and distributions between the visually-grounded language datasets and pure-language corpora. Therefore, we develop a technique named ""vokenization"" that extrapolates multimodal alignments to language-only data by contextually mapping language tokens to their related images (which we call ""vokens""). The ""vokenizer"" is trained on relatively small image captioning datasets and we then apply it to generate vokens for large language corpora. Trained with these contextually generated vokens, our visually-supervised language models show consistent improvements over self-supervised alternatives on multiple pure-language tasks such as GLUE, SQuAD, and SWAG. Code and pre-trained models publicly available at this https URL', 'corpus_id': 222341606, 'score': 1}, {'doc_id': '222291439', 'title': 'Self-play for Data Efficient Language Acquisition', 'abstract': 'When communicating, people behave consistently across conversational roles: People understand the words they say and are able to produce the words they hear. To date, artificial agents developed for language tasks have lacked such symmetry, meaning agents trained to produce language are unable to understand it and vice-versa. In this work, we exploit the symmetric nature of communication in order to improve both the efficiency and quality of language acquisition in learning agents. Specifically, we consider the setting in which an agent must learn to both understand and generate words in an existing language, but with the assumption that access to interaction with ""oracle"" speakers of the language is very limited. We show that using self-play as a substitute for direct supervision enables the agent to transfer its knowledge across roles (e.g. training as a listener but testing as a speaker) and make better inferences about the ground truth lexicon using only a handful of interactions with the oracle.', 'corpus_id': 222291439, 'score': 0}]"
164	{'doc_id': '219037410', 'title': 'Computational Social Science and Sociology', 'abstract': 'The integration of social science with computer science and engineering fields has produced a new area of study: computational social science. This field applies computational methods to novel sour...', 'corpus_id': 219037410}	20567	[{'doc_id': '236878373', 'title': 'Social Networks Structure Analysis and Online Community Discovery', 'abstract': 'This chapter introduces the research of community discovery based on social network structure in social computing. It is further divided into the introduction of social network topology structure and model and the research of public opinion network community discovery.', 'corpus_id': 236878373, 'score': 1}, {'doc_id': '238146283', 'title': 'Participating in social work', 'abstract': 'How can the professional field of social work, decisive for democracy, integrate participation? Contributions advocate a community of expertise (scientific, professional and user-based) concerning the conditions of existence, of functionning and of evolution of social work.', 'corpus_id': 238146283, 'score': 0}, {'doc_id': '237248345', 'title': 'How social media shapes polarization', 'abstract': 'This article reviews the empirical evidence on the relationship between social media and political polarization. We argue that social media shapes polarization through the following social, cognitive, and technological processes: partisan selection, message content, and platform design and algorithms.', 'corpus_id': 237248345, 'score': 0}, {'doc_id': '237063346', 'title': 'Basic Computer Programs in Science and Engineering', 'abstract': 'Basic computer programs in science and engineering , Basic computer programs in science and engineering , مرکز فناوری اطلاعات و اطلاع رسانی کشاورزی', 'corpus_id': 237063346, 'score': 0}, {'doc_id': '237434691', 'title': 'Introduction to the special issue on COMPLEX NETWORKS 2019', 'abstract': 'This special issue of Network Science contains a collection of extended papers from the 8th International Conference on Complex Networks & their Applications (COMPLEX NETWORKS 2019). This major international event in network science brings together every year researchers from around the globe. The great diversity of the participants’ scientific backgrounds ranges from Finance and Economics, Medicine and Neuroscience, Biology and Earth Sciences, Sociology and Political Science to Mathematics and Computer Science, Physics, and many others, making it a special opportunity to review the current state of the field and formulate new directions. This edition of the conference took place at the Calouste Gulbenkian Foundation in Lisbon (Portugal) from December 10 to December 12, 2019. It attracted 470 submissions with authors from 58 countries all over the world. After thorough review, 161 papers were selected to be included in the proceedings Cherifi et al. (2020a,b). The conference program also included keynote presentations from Lada Adamic (Facebook, Inc., USA), Reka Albert (Pennsylvania State University, USA), Ulrik Brandes (ETH Zurich, Switzerland), Stefan Thurner (Medical University of Vienna, Austria), Jari Saramki (Aalto University, Finland), andMichalis Vazirgiannis (LIX, cole Polytechnique, France). Papers invited for this special issue have been selected from the accepted contributions based on relevance to the journal and excellent reviews of the conference version of the papers. The authors were asked to submit an extended version of their conference submission for journal publication in accordance with the customary practice of adding 30% new material. These submissions went through the standard double-blind review process dictated by the journal guidelines. The seven papers accepted to this special issue provide a remarkable sample illustrating the diversity of issues studied in network science research. In Horn and Nelsen (2020), the authors investigate the celebrated PageRank centrality measure from the point of view of differential geometry. More precisely, they answer the following question: how do variation in teleportation constants influence PageRank scores? Exploiting the relationship between heat flow and PageRank, they develop inequalities that bound a particular “gradient” of PageRank over the graph. Their gradient estimate shows that the PageRank vector smoothes out as the jumping constant changes. This work illustrates a beautiful mathematical theory that connects differential equations, PageRank, and diffusion processes over graphs. Liu et al. (2020) introduce a convolutional neural network framework and preprocessing techniques to estimate graphlet counts. Exploiting the correlation between the structural graph information and graphlet counts, they propose to predict graphlet count from previously studied graphs coming from a similar distribution with known graphlet counts. Extensive experiments conducted on three types of random graphs (Erdős–Rényi, Barabási–Albert, and geometric) and real-world graphs (biochemistry, collaboration, and social network) for 3,4,5-node graphlet counting show that their framework offer substantial speedup on estimating graphlet counts of new graphs with high accuracy.', 'corpus_id': 237434691, 'score': 1}, {'doc_id': '238106573', 'title': 'What is Household Crowding? And how does it affect Children’s Enrolment Rates in the Early Year’s Education (EYE): The Case of Kenya in the Covid Era.', 'abstract': 'A research article published in International Journal of Social Science and Humanities Research', 'corpus_id': 238106573, 'score': 0}, {'doc_id': '237960990', 'title': 'In Short About Bulgarian Academy of\xa0Sciences', 'abstract': 'Short notes concerning the history of computer science and informatics in Bulgarian Academy of Sciences are given.', 'corpus_id': 237960990, 'score': 0}, {'doc_id': '221342526', 'title': 'Computational social science: Obstacles and opportunities', 'abstract': 'Data sharing, research ethics, and incentives must improve The field of computational social science (CSS) has exploded in prominence over the past decade, with thousands of papers published using observational data, experimental designs, and large-scale simulations that were once unfeasible or unavailable to researchers. These studies have greatly improved our understanding of important phenomena, ranging from social inequality to the spread of infectious diseases. The institutions supporting CSS in the academy have also grown substantially, as evidenced by the proliferation of conferences, workshops, and summer schools across the globe, across disciplines, and across sources of data. But the field has also fallen short in important ways. Many institutional structures around the field—including research ethics, pedagogy, and data infrastructure—are still nascent. We suggest opportunities to address these issues, especially in improving the alignment between the organization of the 20th-century university and the intellectual requirements of the field.', 'corpus_id': 221342526, 'score': 1}, {'doc_id': '44767798', 'title': 'Computational Social Science', 'abstract': 'centered on a large database, but in this case it is entirely of living organisms, the marine bivalves. Over 28,000 records of bivalve genera and subgenera from 322 locations around the world have now been compiled by these authors, giving a global record of some 854 genera and subgenera and 5132 species. No fossils are included in the database, but because bivalves have a good fossil record, it is possible to estimate accurately the age of origin of almost all extant genera. It is then possible to plot a backward survivorship curve (8) for each of the 27 global bivalve provinces (9). On the basis of these curves, Krug et al. find that origination rates of marine bivalves increased significantly almost everywhere immediately after the K-Pg mass extinction event. The highest K-Pg origination rates all occurred in tropical and warm-temperate regions. A distinct pulse of bivalve diversification in the early Cenozoic was concentrated mainly in tropical and subtropical regions (see the figure). The steepest part of the global backward survivorship curve for bivalves lies between 65 and 50 million years ago, pointing to a major biodiversification event in the Paleogene (65 to 23 million years ago) that is perhaps not yet captured in Alroy et al.’s database (5, 7). The jury is still out on what may have caused this event. But we should not lose sight of the fact that the steep rise to prominence of many modern floral and faunal groups in the Cenozoic may bear no simple relationship to climate or any other type of environmental change (10, 11).', 'corpus_id': 44767798, 'score': 1}, {'doc_id': '3881717', 'title': 'Computational social science ≠ computer science + social data', 'abstract': 'The important intersection of computer science and social science.', 'corpus_id': 3881717, 'score': 1}]
165	{'doc_id': '231202901', 'title': 'Deep learning-enabled medical computer vision', 'abstract': 'A decade of unprecedented progress in artificial intelligence (AI) has demonstrated the potential for many fields—including medicine—to benefit from the insights that AI techniques can extract from data. Here we survey recent progress in the development of modern computer vision techniques—powered by deep learning—for medical applications, focusing on medical imaging, medical video, and clinical deployment. We start by briefly summarizing a decade of progress in convolutional neural networks, including the vision tasks they enable, in the context of healthcare. Next, we discuss several example medical imaging applications that stand to benefit—including cardiology, pathology, dermatology, ophthalmology–and propose new avenues for continued work. We then expand into general medical video, highlighting ways in which clinical workflows can integrate computer vision to enhance care. Finally, we discuss the challenges and hurdles required for real-world clinical deployment of these technologies.', 'corpus_id': 231202901}	19970	[{'doc_id': '235756782', 'title': 'Applications of interpretability in deep learning models for ophthalmology', 'abstract': 'Purpose of review In this article, we introduce the concept of model interpretability, review its applications in deep learning models for clinical ophthalmology, and discuss its role in the integration of artificial intelligence in healthcare. Recent findings The advent of deep learning in medicine has introduced models with remarkable accuracy. However, the inherent complexity of these models undermines its users’ ability to understand, debug and ultimately trust them in clinical practice. Novel methods are being increasingly explored to improve models’ ’interpretability’ and draw clearer associations between their outputs and features in the input dataset. In the field of ophthalmology, interpretability methods have enabled users to make informed adjustments, identify clinically relevant imaging patterns, and predict outcomes in deep learning models. Summary Interpretability methods support the transparency necessary to implement, operate and modify complex deep learning models. These benefits are becoming increasingly demonstrated in models for clinical ophthalmology. As quality standards for deep learning models used in healthcare continue to evolve, interpretability methods may prove influential in their path to regulatory approval and acceptance in clinical practice.', 'corpus_id': 235756782, 'score': 0}, {'doc_id': '237324580', 'title': 'Deep Learning Application for Analyzing of Constituents and Their Correlations in the Interpretations of Medical Images', 'abstract': 'The need for time and attention, given by the doctor to the patient, due to the increased volume of medical data to be interpreted and filtered for diagnostic and therapeutic purposes has encouraged the development of the option to support, constructively and effectively, deep learning models. Deep learning (DL) has experienced an exponential development in recent years, with a major impact on interpretations of the medical image. This has influenced the development, diversification and increase of the quality of scientific data, the development of knowledge construction methods and the improvement of DL models used in medical applications. All research papers focus on description, highlighting, classification of one of the constituent elements of deep learning models (DL), used in the interpretation of medical images and do not provide a unified picture of the importance and impact of each constituent in the performance of DL models. The novelty in our paper consists primarily in the unitary approach, of the constituent elements of DL models, namely, data, tools used by DL architectures or specifically constructed DL architecture combinations and highlighting their “key” features, for completion of tasks in current applications in the interpretation of medical images. The use of “key” characteristics specific to each constituent of DL models and the correct determination of their correlations, may be the subject of future research, with the aim of increasing the performance of DL models in the interpretation of medical images.', 'corpus_id': 237324580, 'score': 1}, {'doc_id': '236134515', 'title': 'A Review of Generative Adversarial Networks in Cancer Imaging: New Applications, New Solutions', 'abstract': 'Despite technological and medical advances, the detection, interpretation, and treatment of cancer based on imaging data continue to pose significant challenges. These include high inter-observer variability, difficulty of small-sized lesion detection, nodule interpretation and malignancy determination, interand intra-tumour heterogeneity, class imbalance, segmentation inaccuracies, and treatment effect uncertainty. The recent advancements in Generative Adversarial Networks (GANs) in computer vision as well as in medical imaging may provide a basis for enhanced capabilities in cancer detection and analysis. In this review, we assess the potential of GANs to address a number of key challenges of cancer imaging, including data scarcity and imbalance, domain and dataset shifts, data access and privacy, data annotation and quantification, as well as cancer detection, tumour profiling and treatment planning. We provide a critical appraisal of the existing literature of GANs applied to cancer imagery, together with suggestions on future research directions to address these challenges. We analyse and discuss 163 papers that apply adversarial training techniques in the context of cancer imaging and elaborate their methodologies, advantages and limitations. With this work, we strive to bridge the gap between the needs of the clinical cancer imaging community and the current and prospective research on GANs in the artificial intelligence community.', 'corpus_id': 236134515, 'score': 1}, {'doc_id': '235759636', 'title': 'Automated deep learning in ophthalmology: AI that can build AI', 'abstract': 'Supplemental Digital Content is available in the text Purpose of review The purpose of this review is to describe the current status of automated deep learning in healthcare and to explore and detail the development of these models using commercially available platforms. We highlight key studies demonstrating the effectiveness of this technique and discuss current challenges and future directions of automated deep learning. Recent findings There are several commercially available automated deep learning platforms. Although specific features differ between platforms, they utilise the common approach of supervised learning. Ophthalmology is an exemplar speciality in the area, with a number of recent proof-of-concept studies exploring classification of retinal fundus photographs, optical coherence tomography images and indocyanine green angiography images. Automated deep learning has also demonstrated impressive results in other specialities such as dermatology, radiology and histopathology. Summary Automated deep learning allows users without coding expertise to develop deep learning algorithms. It is rapidly establishing itself as a valuable tool for those with limited technical experience. Despite residual challenges, it offers considerable potential in the future of patient management, clinical research and medical education. Video abstract http://links.lww.com/COOP/A44', 'corpus_id': 235759636, 'score': 0}, {'doc_id': '236780627', 'title': 'AI musculoskeletal clinical applications: how can AI increase my day-to-day efficiency?', 'abstract': 'Artificial intelligence (AI) is expected to bring greater efficiency in radiology by performing tasks that would otherwise require human intelligence, also at a much faster rate than human performance. In recent years, milestone deep learning models with unprecedented low error rates and high computational efficiency have shown remarkable performance for lesion detection, classification, and segmentation tasks. However, the growing field of AI has significant implications for radiology that are not limited to visual tasks. These are essential applications for optimizing imaging workflows and improving noninterpretive tasks. This article offers an overview of the recent literature on AI, focusing on the musculoskeletal imaging chain, including initial patient scheduling, optimized protocoling, magnetic resonance imaging reconstruction, image enhancement, medical image-to-image translation, and AI-aided image interpretation. The substantial developments of advanced algorithms, the emergence of massive quantities of medical data, and the interest of researchers and clinicians reveal the potential for the growing applications of AI to augment the day-to-day efficiency of musculoskeletal radiologists.', 'corpus_id': 236780627, 'score': 1}, {'doc_id': '236984260', 'title': 'A primer on deep learning and convolutional neural networks for clinicians', 'abstract': 'Deep learning is nowadays at the forefront of artificial intelligence. More precisely, the use of convolutional neural networks has drastically improved the learning capabilities of computer vision applications, being able to directly consider raw data without any prior feature extraction. Advanced methods in the machine learning field, such as adaptive momentum algorithms or dropout regularization, have dramatically improved the convolutional neural networks predicting ability, outperforming that of conventional fully connected neural networks. This work summarizes, in an intended didactic way, the main aspects of these cutting-edge techniques from a medical imaging perspective.', 'corpus_id': 236984260, 'score': 1}, {'doc_id': '16110495', 'title': 'An X-ray view of Mrk 705: a borderline narrow-line Seyfert 1 galaxy', 'abstract': 'Mrk\xa0705\u2000exhibits optical properties of both narrow- and broad-line Seyfert 1 galaxies. We examine the X-ray properties of this borderline object utilising proprietary and public data from Chandra , ASCA , ROSAT \u2000and RXTE , spanning more than twelve years. Though long-term flux variability from the pointed observations appears rather modest (~$3\\times$), we do find examples of rare large amplitude outbursts in the RXTE \u2000monitoring data. There is very little evidence of long-term spectral variability as the low- and high-energy spectra appear constant with time. A ~$6.4{\\rm\\thinspace keV}$ emission line is detected in the ASCA \u2000spectra of Mrk\xa0705, but not during the later, higher flux state Chandra \u2000observation. However, the upper limit on the equivalent width of a line in the Chandra \u2000spectrum is consistent with a constant-flux emission line and a brighter continuum, suggesting that the line is emitted from distant material such as the putative torus. Overall, the X-ray properties of Mrk\xa0705\u2000appear typical of BLS1 activity.', 'corpus_id': 16110495, 'score': 0}, {'doc_id': '236902767', 'title': 'Artificial intelligence and ophthalmic surgery', 'abstract': 'Purpose of review Artificial intelligence and deep learning have become important tools in extracting data from ophthalmic surgery to evaluate, teach, and aid the surgeon in all phases of surgical management. The purpose of this review is to highlight the ever-increasing intersection of computer vision, machine learning, and ophthalmic microsurgery. Recent findings Deep learning algorithms are being applied to help evaluate and teach surgical trainees. Artificial intelligence tools are improving real-time surgical instrument tracking, phase segmentation, as well as enhancing the safety of robotic-assisted vitreoretinal surgery. Summary Similar to strides appreciated in ophthalmic medical disease, artificial intelligence will continue to become an important part of surgical management of ocular conditions. Machine learning applications will help push the boundaries of what surgeons can accomplish to improve patient outcomes.', 'corpus_id': 236902767, 'score': 0}, {'doc_id': '236898588', 'title': 'Images in Space and Time', 'abstract': 'Medical imaging diagnosis is mostly subjective, as it depends on medical experts. Hence, the service provided is limited by expert opinion variations and image complexity as well. However, with the increasing advancements in deep learning field, techniques are developed to help in the diagnosis and risk assessment processes. In this article, we survey different types of images in healthcare. A review of the concept and research methodology of Radiomics will highlight the potentials of integrated diagnostics. Convolutional neural networks can play an important role in next generations of automated imaging biomarker extraction and big data analytics systems. Examples are provided of what is already feasible today and also describe additional technological components required for successful clinical implementation.', 'corpus_id': 236898588, 'score': 1}, {'doc_id': '236517135', 'title': 'Generative adversarial networks in ophthalmology: what are these and how can they be used?', 'abstract': 'Purpose of review The development of deep learning (DL) systems requires a large amount of data, which may be limited by costs, protection of patient information and low prevalence of some conditions. Recent developments in artificial intelligence techniques have provided an innovative alternative to this challenge via the synthesis of biomedical images within a DL framework known as generative adversarial networks (GANs). This paper aims to introduce how GANs can be deployed for image synthesis in ophthalmology and to discuss the potential applications of GANs-produced images. Recent findings Image synthesis is the most relevant function of GANs to the medical field, and it has been widely used for generating ‘new’ medical images of various modalities. In ophthalmology, GANs have mainly been utilized for augmenting classification and predictive tasks, by synthesizing fundus images and optical coherence tomography images with and without pathologies such as age-related macular degeneration and diabetic retinopathy. Despite their ability to generate high-resolution images, the development of GANs remains data intensive, and there is a lack of consensus on how best to evaluate the outputs produced by GANs. Summary Although the problem of artificial biomedical data generation is of great interest, image synthesis by GANs represents an innovation with yet unclear relevance for ophthalmology.', 'corpus_id': 236517135, 'score': 0}]
166	{'doc_id': '218684766', 'title': 'Forward Utilities and Mean-Field Games Under Relative Performance Concerns', 'abstract': 'We introduce the concept of mean field games for agents using Forward utilities of CARA type to study a family of portfolio management problems under relative performance concerns. Under asset specialization of the fund managers, we solve the forward-utility finite player game and the forward-utility mean-field game. We study best response and equilibrium strategies in the single common stock asset and the asset specialization with common noise. As an application, we draw on the core features of the forward utility paradigm and discuss a problem of time-consistent mean-field dynamic model selection in sequential time-horizons.', 'corpus_id': 218684766}	5117	"[{'doc_id': '221112580', 'title': 'Convergence of Deep Fictitious Play for Stochastic Differential Games', 'abstract': ""Stochastic differential games have been used extensively to model agents' competitions in Finance, for instance, in P2P lending platforms from the Fintech industry, the banking system for systemic risk, and insurance markets. The recently proposed machine learning algorithm, deep fictitious play, provides a novel efficient tool for finding Markovian Nash equilibrium of large $N$-player asymmetric stochastic differential games [J. Han and R. Hu, Mathematical and Scientific Machine Learning Conference, 2020]. By incorporating the idea of fictitious play, the algorithm decouples the game into $N$ sub-optimization problems, and identifies each player's optimal strategy with the deep backward stochastic differential equation (BSDE) method parallelly and repeatedly. In this paper, under appropriate conditions, we prove the convergence of deep fictitious play (DFP) to the true Nash equilibrium. We can also show that the strategy based on DFP forms an $\\epsilon$-Nash equilibrium. We generalize the algorithm by proposing a new approach to decouple the games, and present numerical results of large population games showing the empirical convergence of the algorithm beyond the technical assumptions in the theorems."", 'corpus_id': 221112580, 'score': 1}, {'doc_id': '219670088', 'title': 'Directed hypergraph neural network', 'abstract': 'To deal with irregular data structure, graph convolution neural networks have been developed by a lot of data scientists. However, data scientists just have concentrated primarily on developing deep neural network method for un-directed graph. In this paper, we will present the novel neural network method for directed hypergraph. In the other words, we will develop not only the novel directed hypergraph neural network method but also the novel directed hypergraph based semi-supervised learning method. These methods are employed to solve the node classification task. The two datasets that are used in the experiments are the cora and the citeseer datasets. Among the classic directed graph based semi-supervised learning method, the novel directed hypergraph based semi-supervised learning method, the novel directed hypergraph neural network method that are utilized to solve this node classification task, we recognize that the novel directed hypergraph neural network achieves the highest accuracies.', 'corpus_id': 219670088, 'score': 1}, {'doc_id': '227151560', 'title': 'Remaining Useful Life Estimation Under Uncertainty with Causal GraphNets', 'abstract': 'In this work, a novel approach for the construction and training of time series models is presented that deals with the problem of learning on large time series with non-equispaced observations, which at the same time may possess features of interest that span multiple scales. The proposed method is appropriate for constructing predictive models for non-stationary stochastic time series.The efficacy of the method is demonstrated on a simulated stochastic degradation dataset and on a real-world accelerated life testing dataset for ball-bearings. The proposed method, which is based on GraphNets, implicitly learns a model that describes the evolution of the system at the level of a state-vector rather than of a raw observation. The proposed approach is compared to a recurrent network with a temporal convolutional feature extractor head (RNN-tCNN) which forms a known viable alternative for the problem context considered. Finally, by taking advantage of recent advances in the computation of reparametrization gradients for learning probability distributions, a simple yet effective technique for representing prediction uncertainty as a Gamma distribution over remaining useful life predictions is employed.', 'corpus_id': 227151560, 'score': 0}, {'doc_id': '227013048', 'title': 'Visual Forecasting of Time Series with Image-to-Image Regression', 'abstract': 'Time series forecasting is essential for agents to make decisions in many domains. Existing models rely on classical statistical methods to predict future values based on previously observed numerical information. Yet, practitioners often rely on visualizations such as charts and plots to reason about their predictions. Inspired by the end-users, we re-imagine the topic by creating a framework to produce visual forecasts, similar to the way humans intuitively do. In this work, we take a novel approach by leveraging advances in deep learning to extend the field of time series forecasting to a visual setting. We do this by transforming the numerical analysis problem into the computer vision domain. Using visualizations of time series data as input, we train a convolutional autoencoder to produce corresponding visual forecasts. We examine various synthetic and real datasets with diverse degrees of complexity. Our experiments show that visual forecasting is effective for cyclic data but somewhat less for irregular data such as stock price. Importantly, we find the proposed visual forecasting method to outperform numerical baselines. We attribute the success of the visual forecasting approach to the fact that we convert the continuous numerical regression problem into a discrete domain with quantization of the continuous target signal into pixel space.', 'corpus_id': 227013048, 'score': 0}, {'doc_id': '226299995', 'title': 'Discrete solution pools and noise-contrastive estimation for predict-and-optimize', 'abstract': 'Numerous real-life decision-making processes involve solving a combinatorial optimization problem with uncertain input that can be estimated from historic data. There is a growing interest in decision-focused learning methods, where the loss function used for learning to predict the uncertain input uses the outcome of solving the combinatorial problem over a set of predictions. Different surrogate loss functions have been identified, often using a continuous approximation of the combinatorial problem. However, a key bottleneck is that to compute the loss, one has to solve the combinatorial optimisation problem for each training instance in each epoch, which is computationally expensive even in the case of continuous approximations. \nWe propose a different solver-agnostic method for decision-focused learning, namely by considering a pool of feasible solutions as a discrete approximation of the full combinatorial problem. Solving is now trivial through a single pass over the solution pool. We design several variants of a noise-contrastive loss over the solution pool, which we substantiate theoretically and empirically. Furthermore, we show that by dynamically re-solving only a fraction of the training instances each epoch, our method performs on par with the state of the art, whilst drastically reducing the time spent solving, hence increasing the feasibility of predict-and-optimize for larger problems.', 'corpus_id': 226299995, 'score': 0}, {'doc_id': '224814213', 'title': 'Logistic $Q$-Learning', 'abstract': 'We propose a new reinforcement learning algorithm derived from a regularized linear-programming formulation of optimal control in MDPs. The method is closely related to the classic Relative Entropy Policy Search (REPS) algorithm of Peters et al. (2010), with the key difference that our method introduces a Q-function that enables efficient exact model-free implementation. The main feature of our algorithm (called QREPS) is a convex loss function for policy evaluation that serves as a theoretically sound alternative to the widely used squared Bellman error. We provide a practical saddle-point optimization method for minimizing this loss function and provide an error-propagation analysis that relates the quality of the individual updates to the performance of the output policy. Finally, we demonstrate the effectiveness of our method on a range of benchmark problems.', 'corpus_id': 224814213, 'score': 0}, {'doc_id': '227126670', 'title': 'Solving path dependent PDEs with LSTM networks and path signatures', 'abstract': 'Using a combination of recurrent neural networks and signature methods from the rough paths theory we design efficient algorithms for solving parametric families of path dependent partial differential equations (PPDEs) that arise in pricing and hedging of path-dependent derivatives or from use of non-Markovian model, such as rough volatility models in Jacquier and Oumgari, 2019. The solutions of PPDEs are functions of time, a continuous path (the asset price history) and model parameters. As the domain of the solution is infinite dimensional many recently developed deep learning techniques for solving PDEs do not apply. Similarly as in Vidales et al. 2018, we identify the objective function used to learn the PPDE by using martingale representation theorem. As a result we can de-bias and provide confidence intervals for then neural network-based algorithm. We validate our algorithm using classical models for pricing lookback and auto-callable options and report errors for approximating both prices and hedging strategies.', 'corpus_id': 227126670, 'score': 0}, {'doc_id': '212725640', 'title': 'Deep Deterministic Portfolio Optimization', 'abstract': 'Can deep reinforcement learning algorithms be exploited as solvers for optimal trading strategies? The aim of this work is to test reinforcement learning algorithms on conceptually simple, but mathematically non-trivial, trading environments. The environments are chosen such that an optimal or close-to-optimal trading strategy is known. We study the deep deterministic policy gradient algorithm and show that such a reinforcement learning agent can successfully recover the essential features of the optimal trading strategies and achieve close-to-optimal rewards.', 'corpus_id': 212725640, 'score': 1}, {'doc_id': '221266581', 'title': 'Kernel-Based Graph Learning From Smooth Signals: A Functional Viewpoint', 'abstract': 'The problem of graph learning concerns the construction of an explicit topological structure revealing the relationship between nodes representing data entities, which plays an increasingly important role in the success of many graph-based representations and algorithms in the field of machine learning and graph signal processing. In this paper, we propose a novel graph learning framework that incorporates prior information along node and observation side, and in particular the covariates that help to explain the dependency structures in graph signals. To this end, we consider graph signals as functions in the reproducing kernel Hilbert space associated with a Kronecker product kernel, and integrate functional learning with smoothness-promoting graph learning to learn a graph representing the relationship between nodes. The functional learning increases the robustness of graph learning against missing and incomplete information in the graph signals. In addition, we develop a novel graph-based regularisation method which, when combined with the Kronecker product kernel, enables our model to capture both the dependency explained by the graph and the dependency due to graph signals observed under different but related circumstances, e.g. different points in time. The latter means the graph signals are free from the i.i.d. assumptions required by the classical graph learning models. Experiments on both synthetic and real-world data show that our methods outperform the state-of-the-art models in learning a meaningful graph topology from graph signals, in particular with heavy noise, missing values, and multiple dependency.', 'corpus_id': 221266581, 'score': 1}, {'doc_id': '125681903', 'title': 'A new approach for worst-case regret portfolio optimization problem', 'abstract': 'This paper considers the worst-case regret portfolio optimization problem when the distributions of the asset returns are uncertain. In general, the solution to this problem is NP hard and approximation methods that minimise the difference between the maximum return and the sum of each portfolio return are often proposed. Applying the duality of semi-infinite programming, the worst-case regret portfolio optimization problem with uncertain distributions can be equivalently reformulated to a linear optimization problem, and the established solution approaches for linear optimization can then be applied. An example of a portfolio optimization problem is provided to show the efficiency of our method and the results demonstrate that our method can satisfy the portfolio risk diversification property under the uncertain distributions of the returns.', 'corpus_id': 125681903, 'score': 1}]"
167	{'doc_id': '237257089', 'title': 'Treatment-emergent and trajectory-based peripheral gene expression markers of antidepressant response', 'abstract': 'Identifying biomarkers of antidepressant response may advance personalized treatment of major depressive disorder (MDD). We aimed to identify longitudinal changes in gene expression associated with response to antidepressants in a sample of MDD patients treated with escitalopram. Patients ( N \u2009=\u2009153) from the CAN-BIND-1 cohort were treated for 8 weeks, and depressive symptoms were assessed using the Montgomery-Åsberg Depression Rating Scale at 0, 2, 4, 6, and 8 weeks. We identified three groups of patients according to response status: early responders (22.9%), later responders (32.0%), and nonresponders (45.1%). RNA sequencing was performed in blood obtained at weeks 0, 2, and 8. RNA expression was modeled using growth models, and differences in the longitudinal changes in expression according to response were investigated using multiple regression models. The expression of RNAs related to response was investigated in the brains of depressed individuals, as well as in neuronal cells in vitro. We identified four RNAs ( CERCAM, DARS-AS1, FAM228B, HBEGF ) whose change over time was independently associated with a response status. For all except HBEGF , responders showed higher expression over time, compared to nonresponders. While the change in all RNAs differentiated early responders from nonresponders, changes in DARS-AS1 and HBEGF also differentiated later responders from nonresponders. Additionally, HBEGF was downregulated in the brains of depressed individuals, and increased in response to escitalopram treatment in vitro. In conclusion, using longitudinal assessments of gene expression, we provide insights into biological processes involved in the intermediate stages of escitalopram response, highlighting several genes with potential utility as biomarkers of antidepressant response.', 'corpus_id': 237257089}	20166	[{'doc_id': '207934954', 'title': 'Early life stress alters transcriptomic patterning across reward circuitry in male and female mice', 'abstract': 'Abuse, neglect, and other forms of early life stress (ELS) significantly increase risk for psychiatric disorders including depression. In this study, we show that ELS in a postnatal sensitive period increases sensitivity to adult stress in female mice, consistent with our earlier findings in male mice. We used RNA-sequencing in the ventral tegmental area, nucleus accumbens, and prefrontal cortex of male and female mice to show that adult stress is distinctly represented in the brain’s transcriptome depending on ELS history. We identify: 1) biological pathways disrupted after ELS and associated with increased behavioral stress sensitivity, 2) putative transcriptional regulators of the effect of ELS on adult stress response, and 3) subsets of primed genes specifically associated with latent behavioral changes. We also provide transcriptomic evidence that ELS increases sensitivity to future stress through enhancement of known programs of cortical plasticity. Early life stress alters behavioural response to adult stress in female mice. Here authors transcriptionally profile three brain regions involved in reward (ventral tegmental area, nucleus accumbens, and prefrontal cortex) in both male and female adult mice after early life and/or adult stress', 'corpus_id': 207934954, 'score': 1}, {'doc_id': '236477002', 'title': 'Gene Expression Analysis in Three Posttraumatic Stress Disorder Cohorts Implicates Inflammation and Innate Immunity Pathways and Uncovers Shared Genetic Risk With Major Depressive Disorder', 'abstract': 'Posttraumatic stress disorder (PTSD) is a complex psychiatric disorder that can develop following exposure to traumatic events. The Psychiatric Genomics Consortium PTSD group (PGC-PTSD) has collected over 20,000 multi-ethnic PTSD cases and controls and has identified both genetic and epigenetic factors associated with PTSD risk. To further investigate biological correlates of PTSD risk, we examined three PGC-PTSD cohorts comprising 977 subjects to identify differentially expressed genes among PTSD cases and controls. Whole blood gene expression was quantified with the HumanHT-12 v4 Expression BeadChip for 726 OEF/OIF veterans from the Veterans Affairs (VA) Mental Illness Research Education and Clinical Center (MIRECC), 155 samples from the Injury and Traumatic Stress (INTRuST) Clinical Consortium, and 96 Australian Vietnam War veterans. Differential gene expression analysis was performed in each cohort separately followed by meta-analysis. In the largest cohort, we performed co-expression analysis to identify modules of genes that are associated with PTSD and MDD. We then conducted expression quantitative trait loci (eQTL) analysis and assessed the presence of eQTL interactions involving PTSD and major depressive disorder (MDD). Finally, we utilized PTSD and MDD GWAS summary statistics to identify regions that colocalize with eQTLs. Although not surpassing correction for multiple testing, the most differentially expressed genes in meta-analysis were interleukin-1 beta (IL1B), a pro-inflammatory cytokine previously associated with PTSD, and integrin-linked kinase (ILK), which is highly expressed in brain and can rescue dysregulated hippocampal neurogenesis and memory deficits. Pathway analysis revealed enrichment of toll-like receptor (TLR) and interleukin-1 receptor genes, which are integral to cellular innate immune response. Co-expression analysis identified four modules of genes associated with PTSD, two of which are also associated with MDD, demonstrating common biological pathways underlying the two conditions. Lastly, we identified four genes (UBA7, HLA-F, HSPA1B, and RERE) with high probability of a shared causal eQTL variant with PTSD and/or MDD GWAS variants, thereby providing a potential mechanism by which the GWAS variant contributes to disease risk. In summary, we provide additional evidence for genes and pathways previously reported and identified plausible novel candidates for PTSD. These data provide further insight into genetic factors and pathways involved in PTSD, as well as potential regions of pleiotropy between PTSD and MDD.', 'corpus_id': 236477002, 'score': 0}, {'doc_id': '236165131', 'title': 'Neuroepigenetics of psychiatric disorders: Focus on lncRNA', 'abstract': 'Understanding the pathology of psychiatric disorders is challenging due to their complexity and multifactorial origin. However, development of high-throughput technologies has allowed for better insight into their molecular signatures. Advancement of sequencing methodologies have made it possible to study not only the protein-coding but also the noncoding genome. It is now clear that besides the genetic component, different epigenetic mechanisms play major roles in the onset and development of psychiatric disorders. Among them, examining the role of long noncoding RNAs (lncRNAs) is a relatively new field. Here, we present an overview of what is currently known about the involvement of lncRNAs in schizophrenia, major depressive and bipolar disorders, as well as suicide. The diagnosis of psychiatric disorders mainly relies on clinical evaluation without using measurable biomarkers. In this regard, lncRNA may open new opportunities for development of molecular tests. However, so far only a small set of known lncRNAs have been characterized at molecular level, which means they have a long way to go before clinical implementation. Understanding how changes in lncRNAs affect the appearance and development of psychiatric disorders may lead to a more classified and objective diagnostic system, but also open up new therapeutic targets for these patients.', 'corpus_id': 236165131, 'score': 0}, {'doc_id': '236485111', 'title': 'Gene expression correlates of advanced epigenetic age and psychopathology in postmortem cortical tissue', 'abstract': 'Psychiatric stress has been associated with accelerated epigenetic aging (i.e., when estimates of cellular age based on DNA methylation exceed chronological age) in both blood and brain tissue. Little is known about the downstream biological effects of accelerated epigenetic age on gene expression. In this study we examined associations between DNA methylation-derived estimates of cellular age that range from decelerated to accelerated relative to chronological age (“DNAm age residuals”) and transcriptome-wide gene expression. This was examined using tissue from three post-mortem cortical regions (ventromedial and dorsolateral prefrontal cortex and motor cortex, n = 97) from the VA National PTSD Brain Bank. In addition, we examined how posttraumatic stress disorder (PTSD) and alcohol-use disorders (AUD) moderated the association between DNAm age residuals and gene expression. Transcriptome-wide results across brain regions, psychiatric diagnoses, and cohorts (full sample and male and female subsets) revealed experiment-wide differential expression of 11 genes in association with PTSD or AUD in interaction with DNAm age residuals. This included the inflammation-related genes IL1B, RCOR2, and GCNT1. Candidate gene class analyses and gene network enrichment analyses further supported differential expression of inflammation/immune gene networks as well as glucocorticoid, circadian, and oxidative stress-related genes. Gene co-expression network modules suggested enrichment of myelination related processes and oligodendrocyte enrichment in association with DNAm age residuals in the presence of psychopathology. Collectively, results suggest that psychiatric stress accentuates the association between advanced epigenetic age and expression of inflammation genes in the brain. This highlights the role of inflammatory processes in the pathophysiology of accelerated cellular aging and suggests that inflammatory pathways may link accelerated cellular aging to premature disease onset and neurodegeneration, particularly in stressed populations. This suggests that anti-inflammatory interventions may be an important direction to pursue in evaluating ways to prevent or delay cellular aging and increase resilience to diseases of aging.', 'corpus_id': 236485111, 'score': 0}, {'doc_id': '49411399', 'title': 'Improved identification of concordant and discordant gene expression signatures using an updated rank-rank hypergeometric overlap approach', 'abstract': 'Recent advances in large-scale gene expression profiling necessitate concurrent development of biostatistical approaches to reveal meaningful biological relationships. Most analyses rely on significance thresholds for identifying differentially expressed genes. We use an approach to compare gene expression datasets using ‘threshold-free’ comparisons. Significance cut-offs to identify genes shared between datasets may be too stringent and may miss concordant patterns of gene expression with potential biological relevance. A threshold-free approach gaining popularity in several research areas, including neuroscience, is Rank–Rank Hypergeometric Overlap (RRHO). Genes are ranked by their p-value and effect size direction, and ranked lists are compared to identify significantly overlapping genes across a continuous significance gradient rather than at a single arbitrary cut-off. We have updated the previous RRHO analysis by accurately detecting overlap of genes changed in the same and opposite directions between two datasets. Here, we use simulated and real data to show the drawbacks of the previous algorithm as well as the utility of our new algorithm. For example, we show the power of detecting discordant transcriptional patterns in the postmortem brain of subjects with psychiatric disorders. The new R package, RRHO2, offers a new, more intuitive visualization of concordant and discordant gene overlap.', 'corpus_id': 49411399, 'score': 1}, {'doc_id': '229321428', 'title': 'Genome-wide Signatures of Early-Life Stress: Influence of Sex', 'abstract': 'Both history of early-life stress (ELS) and female sex are associated with increased risk for depression. The complexity of how ELS interacts with brain development and sex to impart risk for multifaceted neuropsychiatric disorders is also unlikely to be understood by examining changes in single genes. Here, we review an emerging literature on genome-wide transcriptional and epigenetic signatures of ELS and the potential moderating influence of sex. We discuss evidence both that there are latent sex differences revealed by ELS and that ELS itself produces latent transcriptomic changes revealed by adult stress. In instances where there are broad similarities in global signatures of ELS among females and males, genes that contribute to these patterns are largely distinct based on sex. As this area of investigation grows, an effort should be made to better understand the sex-specific impact of ELS within the human brain, specific contributions of chromosomal versus hormonal sex, how ELS alters the time course of normal transcriptional development, and the cell-type specificity of transcriptomic and epigenomic changes in the brain. A better understanding of how ELS interacts with sex to alter transcriptomic and epigenomic signatures in the brain will inform individualized therapeutic strategies to prevent or ameliorate depression and other psychiatric disorders in this vulnerable population.', 'corpus_id': 229321428, 'score': 1}, {'doc_id': '206660069', 'title': 'Early life stress confers lifelong stress susceptibility in mice via ventral tegmental area OTX2', 'abstract': 'An early window of stress susceptibility defines a mouse’s response to stress in adulthood. Early life stress in depression susceptibility The linkage between stress early in life and behavioral depression in adulthood is complex. Peña et al. were able to define a time period in early development when mice are especially susceptible to stress. Mice subjected to stress during this time period were less resilient to stress in adulthood. Genes regulated by the transcription factor orthodenticle homeobox 2 (OTX2) primed the response toward depression in adulthood. Although early stress could establish the groundwork for later depression, that priming could be undone by intervention at the right moment. Science, this issue p. 1185 Early life stress increases risk for depression. Here we establish a “two-hit” stress model in mice wherein stress at a specific postnatal period increases susceptibility to adult social defeat stress and causes long-lasting transcriptional alterations that prime the ventral tegmental area (VTA)—a brain reward region—to be in a depression-like state. We identify a role for the developmental transcription factor orthodenticle homeobox 2 (Otx2) as an upstream mediator of these enduring effects. Transient juvenile—but not adult—knockdown of Otx2 in VTA mimics early life stress by increasing stress susceptibility, whereas its overexpression reverses the effects of early life stress. This work establishes a mechanism by which early life stress encodes lifelong susceptibility to stress via long-lasting transcriptional programming in VTA mediated by Otx2.', 'corpus_id': 206660069, 'score': 1}, {'doc_id': '3542827', 'title': 'Ketamine and Imipramine Reverse Transcriptional Signatures of Susceptibility and Induce Resilience-Specific Gene Expression Profiles', 'abstract': 'BACKGROUND\nExamining transcriptional regulation by antidepressants in key neural circuits implicated in depression and understanding the relation to transcriptional mechanisms of susceptibility and natural resilience may help in the search for new therapeutic agents. Given the heterogeneity of treatment response in human populations, examining both treatment response and nonresponse is critical.\n\n\nMETHODS\nWe compared the effects of a conventional monoamine-based tricyclic antidepressant, imipramine, and a rapidly acting, non-monoamine-based antidepressant, ketamine, in mice subjected to chronic social defeat stress, a validated depression model, and used RNA sequencing to analyze transcriptional profiles associated with susceptibility, resilience, and antidepressant response and nonresponse in the prefrontal cortex (PFC), nucleus accumbens, hippocampus, and amygdala.\n\n\nRESULTS\nWe identified similar numbers of responders and nonresponders after ketamine or imipramine treatment. Ketamine induced more expression changes in the hippocampus; imipramine induced more expression changes in the nucleus accumbens and amygdala. Transcriptional profiles in treatment responders were most similar in the PFC. Nonresponse reflected both the lack of response-associated gene expression changes and unique gene regulation. In responders, both drugs reversed susceptibility-associated transcriptional changes and induced resilience-associated transcription in the PFC.\n\n\nCONCLUSIONS\nWe generated a uniquely large resource of gene expression data in four interconnected limbic brain regions implicated in depression and its treatment with imipramine or ketamine. Our analyses highlight the PFC as a key site of common transcriptional regulation by antidepressant drugs and in both reversing susceptibility- and inducing resilience-associated molecular adaptations. In addition, we found region-specific effects of each drug, suggesting both common and unique effects of imipramine versus ketamine.', 'corpus_id': 3542827, 'score': 1}, {'doc_id': '237436833', 'title': 'Characterising the shared genetic determinants of bipolar disorder, schizophrenia and risk-taking', 'abstract': 'Increased risk-taking is a central component of bipolar disorder (BIP) and is implicated in schizophrenia (SCZ). Risky behaviours, including smoking and alcohol use, are overrepresented in both disorders and associated with poor health outcomes. Positive genetic correlations are reported but an improved understanding of the shared genetic architecture between risk phenotypes and psychiatric disorders may provide insights into underlying neurobiological mechanisms. We aimed to characterise the genetic overlap between risk phenotypes and SCZ, and BIP by estimating the total number of shared variants using the bivariate causal mixture model and identifying shared genomic loci using the conjunctional false discovery rate method. Summary statistics from genome wide association studies of SCZ, BIP, risk-taking and risky behaviours were acquired ( n \u2009=\u200982,315–466,751). Genomic loci were functionally annotated using FUMA. Of 8.6–8.7\u2009K variants predicted to influence BIP, 6.6\u2009K and 7.4\u2009K were predicted to influence risk-taking and risky behaviours, respectively. Similarly, of 10.2–10.3\u2009K variants influencing SCZ, 9.6 and 8.8\u2009K were predicted to influence risk-taking and risky behaviours, respectively. We identified 192 loci jointly associated with SCZ and risk phenotypes and 206 associated with BIP and risk phenotypes, of which 68 were common to both risk-taking and risky behaviours and 124 were novel to SCZ or BIP. Functional annotation implicated differential expression in multiple cortical and sub-cortical regions. In conclusion, we report extensive polygenic overlap between risk phenotypes and BIP and SCZ, identify specific loci contributing to this shared risk and highlight biologically plausible mechanisms that may underlie risk-taking in severe psychiatric disorders.', 'corpus_id': 237436833, 'score': 0}, {'doc_id': '237458753', 'title': 'Association between the expression of lncRNA BASP-AS1 and volume of right hippocampal tail moderated by episode duration in major depressive disorder: a CAN-BIND 1 report', 'abstract': 'The pathophysiology of major depressive disorder (MDD) encompasses an array of changes at molecular and neurobiological levels. As chronic stress promotes neurotoxicity there are alterations in the expression of genes and gene-regulatory molecules. The hippocampus is particularly sensitive to the effects of stress and its posterior volumes can deliver clinically valuable information about the outcomes of antidepressant treatment. In the present work, we analyzed individuals with MDD (N\u2009=\u2009201) and healthy controls (HC\u2009=\u2009104), as part of the CAN-BIND-1 study. We used magnetic resonance imaging (MRI) to measure hippocampal volumes, evaluated gene expression with RNA sequencing, and assessed DNA methylation with the (Infinium MethylationEpic Beadchip), in order to investigate the association between hippocampal volume and both RNA expression and DNA methylation. We identified 60 RNAs which were differentially expressed between groups. Of these, 21 displayed differential methylation, and seven displayed a correlation between methylation and expression. We found a negative association between expression of Brain Abundant Membrane Attached Signal Protein 1 antisense 1 RNA (BASP1-AS1) and right hippocampal tail volume in the MDD group (β\u2009=\u2009−0.218, p\u2009=\u20090.021). There was a moderating effect of the duration of the current episode on the association between the expression of BASP1-AS1 and right hippocampal tail volume in the MDD group (β\u2009=\u2009−0.48, 95% C.I. [−0.80, −0.16]. t\u2009=\u2009−2.95 p\u2009=\u20090.004). In conclusion, we found that overexpression of BASP1-AS1 was correlated with DNA methylation, and was negatively associated with right tail hippocampal volume in MDD.', 'corpus_id': 237458753, 'score': 0}]
168	{'doc_id': '59843021', 'title': 'Providing Outstanding Undergraduate Research Experiences and Sustainable Faculty Development in Load', 'abstract': 'When budgets are limited and teaching loads are high, colleges and universities often face challenges to provide opportunities for faculty development and superior undergraduate research experiences. However, conducting research in one’s field and allowing undergraduates to engage in this research can deeply enrich the experience of both professors and students (Kuh et al., 2007; Kuh, 2008). Therefore, the psychology department at Wofford College solved these problems by incorporating research into most psychology courses (especially lab courses) and by designing a laboratory experience which includes original team-based research designed for publication, all within the normal faculty teaching load. This fits with our departmental learning goals, stressing the scientific method and reliance on empirical research in the development and testing of psychological theories. This unique structure of our department, including research in load, gives a double benefit: (1) enhancing the ability for professors to continue research in their area, and (2) allowing students to engage in publishable research. We describe how we implemented these opportunities, hoping that some readers might adopt features into their programs.', 'corpus_id': 59843021}	20853	[{'doc_id': '225689797', 'title': 'How COVID-19 Is Affecting Undergraduate Research Experiences', 'abstract': None, 'corpus_id': 225689797, 'score': 1}, {'doc_id': '94257530', 'title': 'Improved efficiency of tocotrienol extraction from fresh and processed latex', 'abstract': 'Vitamin E, mainly in the form of tocotrienols, was extracted from Hevea brasiliensis latex with organic solvents. The content of tocotrienols and a small amount of tocopherols recovered from the latex was determined using high performance liquid chromatoghraphy (HPLC). Gas chromatoghraphy-mass spectrometry (GC-MS) confirmed the identities of the tocotrienols and tocopherols forms that were present. Gamma-tocotrienol was the most abundant form of vitamin E in Hevea latex. The yield of tocotrienols (339 ug/g of latex) was significantly increased by the use of the detergant Triton X-100 in the extraction procedure. This method improves the extraction efficiency by 83%. Through drying of the organic fraction using anhydrous magnesium sulphate following phase separation was also advantageous in the extraction procedure. On the other hand, the presence of ammonia in latex suspension reduced extraction efficiency. Vitamin E was also found in the waste serum generated from the processing of deproteinised natural rubber (DPNR). Although the yield vitamin from this source was relatively low, there is a potential to modify the processing procedure another value added end product i.e. latex vitamin E in addition to DPNR.', 'corpus_id': 94257530, 'score': 0}, {'doc_id': '198645951', 'title': 'Making the Case for Capstones and Signature Work', 'abstract': None, 'corpus_id': 198645951, 'score': 1}, {'doc_id': '11910516', 'title': 'A socioecological perspective on primate cognition, past and present', 'abstract': 'AbstractThe papers in this special issue examine the relationship between social and ecological cognition in primates. We refer to the intersection of these two domains as socioecological cognition. Examples of socioecological cognition include socially learned predator alarm calls and socially sensitive foraging decisions. In this review we consider how primate cognition may have been shaped by the interaction of social and ecological influences in their evolutionary history. The ability to remember distant, out-of-sight locations is an ancient one, shared by many mammals and widespread among primates. It seems some monkeys and apes have evolved the ability to form more complex representations of resources, integrating “what-where-how much” information. This ability allowed anthropoids to live in larger, more cohesive groups by minimizing competition for limited resources between group members. As group size increased, however, competition for resources also increased, selecting for enhanced social skills. Enhanced social skills in turn made a more sophisticated relationship to the environment possible. The interaction of social and ecological influences created a spiraling effect in the evolution of primate intelligence. In contrast, lemurs may not have evolved the ability to form complex representations which would allow them to consider the size and location of resources. This lack in lemur ecological cognition may restrict the size of frugivorous lemur social groups, thereby limiting the complexity of lemur social life. In this special issue, we have brought together two review papers, five field studies, and one laboratory study to investigate the interaction of social and ecological factors in relation to foraging. Our goal is to stimulate research that considers social and ecological factors acting together on cognitive evolution, rather than in isolation. Cross fertilization of experimental and observational studies from captivity and the field is important for increasing our understanding of this relationship.\n', 'corpus_id': 11910516, 'score': 0}, {'doc_id': '237218214', 'title': 'Two-Year College Students Report Multiple Benefits From Participation in an Integrated Geoscience Research, Coursework, and Outreach Internship Program', 'abstract': 'Objective: Despite the availability of high-paying, high-demand careers, few women and students from underrepresented racial and ethnic minorities enter undergraduate programs understanding what the geosciences are and associated available career opportunities. This problem is compounded for students from backgrounds underrepresented in the United States. High-impact practices, such as mentoring, internships, undergraduate research experiences, and cohort-building, increase recruitment and retention of underrepresented students in science, technology, engineering, and math at 4-year institutions. What is not yet clear is the impact these interventions have on underrepresented students at two-year colleges, where the STEM pathway has become a main postsecondary school entry point for these students due to the affordability, flexibility, and academic support provided. Therefore, we designed, implemented, and researched a year-long program providing underrepresented students at a two-year college exposure to several of these experiences. Methods: We interviewed program participants about their perceptions and experiences in the program. Analysis proceeded using constant comparison. Results: Participants reported benefits from networking opportunities, gains in confidence, and gains in job skills, while some reported challenges for participation such as communication and time expectation conflicts; participants also struggled to balance the program with employment needs on top of school requirements. Different aspects of the program benefited different students, suggesting that all of these experiences could support recruitment and foster interest in geoscience for underrepresented students at two-year colleges. Conclusion: We conclude with implications for future research, program enhancements, and time constraint and mentoring needs related to characteristics of two-year college students.', 'corpus_id': 237218214, 'score': 0}, {'doc_id': '237473113', 'title': 'Linking Genetics, Microbiology & Molecular Biology Courses to Provide Research Experiences in Undergraduate Biology', 'abstract': 'Abstract Course-based undergraduate research experiences (CUREs) can have benefits for many students, especially those who lack access to traditional apprenticeships for research. As part of an effort to create more opportunities for students to have access to primary research and move away from traditional cookie-cutter labs, we have created a multicourse CURE spanning three undergraduate teaching labs in which students can pick and choose to take any of the courses that most interest them. This CURE explores the essential understanding of the emergence of antibiotic-resistant bacteria as well as high-throughput sequencing and mutagenesis screens. These low-cost modular labs are designed to be flexible and integrated into any single teaching lab to increase exposure to both fundamental lab skills and primary research.', 'corpus_id': 237473113, 'score': 0}, {'doc_id': '238412294', 'title': 'Integrative and Comparative Biology', 'abstract': 'Synopsis Incorporating active research opportunities into undergraduate curricula is one of the most cited elements demonstrated to improve inclusive excellence and retention in all STEM fields. Allegheny College has a long and nationally-recognized tradition of collaborative student-faculty research within the academic curriculum and as co-curricular opportunities. We present an example of the former, a Course-based Undergraduate Research Experience (CURE), FSBio 201, that has been central to Allegheny’s biology curriculum for over two decades. The course emphasizes biological research design, execution, and communication. We have coded and analyzed feedback from student evaluations and from the national CURE project database, both of which measure students’ perceptions and attitudes toward the course. The majority of the student feedback related to the course learning outcomes of fostering independent research and communication skills was positive. However, we also see areas for improvement, such as how we employ peer-to-peer mentoring and how we teach quantitative and computerbased skills. We conclude that students’ self-reported data are in line with our learning outcomes and provide FSBio 201 as a model for introducing college undergraduates to biological research.', 'corpus_id': 238412294, 'score': 0}, {'doc_id': '73459364', 'title': 'Computer-Based and Bench-Based Undergraduate Research Experiences Produce Similar Attitudinal Outcomes', 'abstract': 'Course-based undergraduate research experiences (CUREs) have the potential to improve undergraduate biology education by involving large numbers of students in research. CUREs can take a variety of forms with different affordances and constraints, complicating the evaluation of design features that might contribute to successful outcomes. In this study, we compared students’ responses to three different research experiences offered within the same course. One of the research experiences involved purely computational work, whereas the other two offerings were bench-based research experiences. We found that students who participated in computer-based research reported at least as much interest in their research projects, a higher sense of achievement, and a higher level of satisfaction with the course compared with students who did bench-based research projects. In open-ended comments, similar proportions of students in each research area expressed some sense of project ownership as contributing positively to their course experiences. Their comments also supported the finding that experiencing a sense of achievement was a predictor of course satisfaction. We conclude that both computer-based and bench-based CUREs can have positive impacts on students’ attitudes. Development of more computer-based CUREs might allow larger numbers of students to benefit from participating in a research experience.', 'corpus_id': 73459364, 'score': 1}, {'doc_id': '235525632', 'title': 'The role of undergraduate laboratories in the formation of engineering identities: A critical review of the literature', 'abstract': 'Background: There was growing recognition worldwide by professional engineering bodies, engineering faculties and researchers on the need to pay attention to engineering students’ emerging identities and how they were formed across the trajectory of undergraduate engineering programmes. An increasing number of research studies focused on engineering identity, including systematic reviews of the research literature. Aim: Engineering laboratories were key learning spaces in undergraduate engineering programmes. In the laboratory, students learned to integrate theory and practice, engaged in problem-solving and applied experimental methods. The purpose of this critical review of the literature was to interrogate the impact that learning in engineering laboratories had on emerging professional identities across engineering disciplines and fields. Method: This review built on and extended previous systematic reviews on engineering identity by studying pedagogies in the engineering laboratory through the lens of identity formation. Search terms were consistently applied to eight databases, which yielded 57 empirical studies, after the application of relevance and quality appraisal criteria. Two reviewers independently applied a socio-materialist theoretical framework of identify formation to each study and coded each of the studies into categories aligned with the theoretical framework. Results: The findings of the critical review revealed the temporal, spatial, material, performative and discursive dimensions in engineering identity formation and showed that students’ emerging identities could be affirmed and supported by appropriate laboratory pedagogies. Conclusion: The critical review of the literature concluded that curricular and pedagogical interventions that were better aligned with the dimensions of identity formation were more likely to enhance students’ identification with engineering.', 'corpus_id': 235525632, 'score': 1}, {'doc_id': '158606584', 'title': 'A Project-Based Cornerstone Course in Civil Engineering: Student Perceptions and Identity Development.', 'abstract': None, 'corpus_id': 158606584, 'score': 1}]
169	{'doc_id': '214681486', 'title': 'A mass spectrometry‐based high‐throughput screening method for engineering fatty acid synthases with improved production of medium‐chain fatty acids', 'abstract': 'Microbial cell factories have been extensively engineered to produce free fatty acids (FFAs) as key components of crucial nutrients, soaps, industrial chemicals, and fuels. However, our ability to control the composition of microbially synthesized FFAs is still limited, particularly, for producing medium‐chain fatty acids (MCFAs). This is mainly due to the lack of high‐throughput approaches for FFA analysis to engineer enzymes with desirable product specificity. Here we report a mass spectrometry (MS)‐based method for rapid profiling of MCFAs in Saccharomyces cerevisiae by using membrane lipids as a proxy. In particular, matrix‐assisted laser desorption/ionization time‐of‐flight (MALDI‐ToF) MS was used to detect shorter acyl chain phosphatidylcholines from membrane lipids and a higher m/z peak ratio at 730 and 758 was used as an indication for improved MCFA production. This colony‐based method can be performed at a rate of ~2\u2009s per sample, representing a substantial improvement over gas chromatography‐MS (typically >30\u2009min per sample) as the gold standard method for FFA detection. To demonstrate the power of this method, we performed site‐saturation mutagenesis of the yeast fatty acid synthase and identified nine missense mutations that resulted in improved MCFA production relative to the wild‐type strain. Colony‐based MALDI‐ToF MS screening provides an effective approach for engineering microbial fatty acid compositions in a high‐throughput manner.', 'corpus_id': 214681486}	1909	[{'doc_id': '209340332', 'title': 'Cellular engineering strategies toward sustainable omega-3 long chain polyunsaturated fatty acids production: State of the art and perspectives.', 'abstract': 'Long-chain polyunsaturated fatty acids (LC-PUFAs) especially ω-3 fatty acids provide significant health benefits for human beings. However, ω-3 LC-PUFAs cannot be synthesized de novo in mammals. Traditionally, ω-3 LC-PUFAs are extracted from marine fish, and their production depends on sea fishing, which has not met ever-increasing global demand. To address the challenges, innovative cellular engineering strategies need to be developed. In nature, many fungi and microalgae are rich in ω-3 LC-PUFAs, representing promising sources of ω-3 LC-PUFAs. The latest progress in developing new cellular engineering strategies toward sustainable ω-3 LC-PUFAs production using fungi and microalga has demonstrated that they can to some extent address the supply shortage. In this review, we critically summarize the recent progress in enhancing the productivity in various ω-3 LC-PUFAs-producing organisms, as well as the latest efforts of biosynthesizing PUFAs in heterogenous biosystems. In addition, we also provide future perspectives in developing genetic toolkits for LC-PUFAs producing microbes so that cut-edging biotechnology such as gene stacking and genome editing can be further applied to increase the productivity of ω-3 LC-PUFAs.', 'corpus_id': 209340332, 'score': 1}, {'doc_id': '211138917', 'title': 'Important roles of dietary taurine, creatine, carnosine, anserine and 4-hydroxyproline in human nutrition and health', 'abstract': 'Taurine (a sulfur-containing β-amino acid), creatine (a metabolite of arginine, glycine and methionine), carnosine (a dipeptide; β-alanyl- l -histidine), and 4-hydroxyproline (an imino acid; also often referred to as an amino acid)\xa0were discovered in cattle, and the discovery of anserine (a methylated product of carnosine; β-alanyl-1-methyl- l -histidine) also originated with cattle. These five nutrients are highly abundant in beef, and have important physiological roles in anti-oxidative and anti-inflammatory reactions, as well as neurological, muscular, retinal, immunological and cardiovascular function. Of particular note, taurine, carnosine, anserine, and creatine are absent from plants, and hydroxyproline is negligible in many plant-source foods. Consumption of 30\xa0g dry beef can fully meet daily physiological needs of the healthy 70-kg adult human for taurine and carnosine, and can also provide large amounts of creatine, anserine and 4-hydroxyproline to improve human nutrition and health, including metabolic, retinal, immunological,\xa0muscular, cartilage,\xa0neurological, and cardiovascular health. The present review provides the public with the much-needed knowledge of nutritionally and physiologically significant amino acids, dipeptides and creatine in animal-source foods (including beef). Dietary taurine, creatine, carnosine, anserine and 4-hydroxyproline are beneficial for preventing and treating obesity, cardiovascular dysfunction, and ageing-related disorders, as well as inhibiting tumorigenesis, improving skin and bone health, ameliorating neurological abnormalities, and promoting well being in infants, children and adults. Furthermore, these nutrients may promote the immunological defense of humans against infections by bacteria, fungi, parasites, and viruses (including coronavirus) through enhancing the metabolism and functions of monocytes, macrophages, and other cells of the immune system. Red meat (including beef) is a functional food for optimizing human growth, development and health.', 'corpus_id': 211138917, 'score': 0}, {'doc_id': '212421062', 'title': 'Metabolic host response and therapeutic approaches to influenza infection', 'abstract': 'Based on available metabolomic studies, influenza infection affects a variety of cellular metabolic pathways to ensure an optimal environment for its replication and production of viral particles. Following infection, glucose uptake and aerobic glycolysis increase in infected cells continually, which results in higher glucose consumption. The pentose phosphate shunt, as another glucose-consuming pathway, is enhanced by influenza infection to help produce more nucleotides, especially ATP. Regarding lipid species, following infection, levels of triglycerides, phospholipids, and several lipid derivatives undergo perturbations, some of which are associated with inflammatory responses. Also, mitochondrial fatty acid β-oxidation decreases significantly simultaneously with an increase in biosynthesis of fatty acids and membrane lipids. Moreover, essential amino acids are demonstrated to decline in infected tissues due to the production of large amounts of viral and cellular proteins. Immune responses against influenza infection, on the other hand, could significantly affect metabolic pathways. Mainly, interferon (IFN) production following viral infection affects cell function via alteration in amino acid synthesis, membrane composition, and lipid metabolism. Understanding metabolic alterations required for influenza virus replication has revealed novel therapeutic methods based on targeted inhibition of these cellular metabolic pathways.', 'corpus_id': 212421062, 'score': 0}, {'doc_id': '212628375', 'title': 'Olive oil by-product as functional ingredient in bakery products. Influence of processing and evaluation of biological effects.', 'abstract': 'Nowadays, the strong demand for adequate nutrition is accompanied by concern about environmental pollution and there is a considerable emphasis on the recovery and recycling of food by-products and wastes. In this study, we focused on the exploitation of olive pomace as functional ingredient in biscuits and bread. Standard and enriched bakery products were made using different flours and fermentation protocols. After characterization, they were in vitro digested and used for supplementation of intestinal cells (Caco-2), which underwent exogenous inflammation. The enrichment caused a significant increase in the phenolic content in all products, particularly in the sourdough fermented ones. Sourdough fermentation also increased tocol concentration. The increased concentration of bioactive molecules did not reflect the anti-inflammatory effect, which was modulated by the baking procedure. Conventionally fermented bread enriched with 4% pomace and sourdough fermented, not-enriched bread had the greatest anti-inflammatory effect, significantly reducing IL-8 secretion in Caco-2 cells. The cell metabolome was modified only after supplementation with sourdough fermented bread enriched with 4% pomace, probably due to the high concentration of tocopherol that acted synergistically with polyphenols. Our data highlight that changes in chemical composition cannot predict changes in functionality. It is conceivable that matrices (including enrichment) and processing differently modulated bioactive bioaccessibility, and consequently functionality.', 'corpus_id': 212628375, 'score': 0}, {'doc_id': '53109882', 'title': 'Engineering Microbes to Produce Polyunsaturated Fatty Acids.', 'abstract': 'Polyunsaturated fatty acids (PUFAs) are important for human health. They are traditionally extracted from animals and plants but can be alternatively derived from oleaginous microbes, and engineering microbial metabolism can improve PUFA accumulation. The next frontier is to engineer more efficient PUFA-producing microbes using systems and synthetic biology tools.', 'corpus_id': 53109882, 'score': 1}, {'doc_id': '210838967', 'title': 'Dietary restriction of amino acids for Cancer therapy', 'abstract': 'Biosyntheses of proteins, nucleotides and fatty acids, are essential for the malignant proliferation and survival of cancer cells. Cumulating research findings show that amino acid restrictions are potential strategies for cancer interventions. Meanwhile, dietary strategies are popular among cancer patients. However, there is still lacking solid rationale to clarify what is the best strategy, why and how it is. Here, integrated analyses and comprehensive summaries for the abundances, signalling and functions of amino acids in proteomes, metabolism, immunity and food compositions, suggest that, intermittent dietary lysine restriction with normal maize as an intermittent staple food for days or weeks, might have the value and potential for cancer prevention or therapy. Moreover, dietary supplements were also discussed for cancer cachexia including dietary immunomodulatory.', 'corpus_id': 210838967, 'score': 0}, {'doc_id': '208038487', 'title': 'The role of fluconazole in the regulation of fatty acid and unsaponifiable matter biosynthesis in Schizochytrium sp. MYA 1381', 'abstract': 'BackgroundSchizochytrium has been widely used in industry for synthesizing polyunsaturated fatty acids (PUFAs), especially docosahexaenoic acid (DHA). However, unclear biosynthesis pathway of PUFAs inhibits further production of the Schizochytrium. Unsaponifiable matter (UM) from mevalonate pathway is crucial to cell growth and intracellular metabolism in all higher eukaryotes and microalgae. Therefore, regulation of UM biosynthesis in Schizochytrium may have important effects on fatty acids synthesis. Moreover, it is well known that UMs, such as squalene and β-carotene, are of great commercial value. Thus, regulating UM biosynthesis may also allow for an increased valuation of Schizochytrium.ResultsTo investigate the correlation of UM biosynthesis with fatty acids accumulation in Schizochytrium, fluconazole was used to block the sterols pathway. The addition of 60\u2009mg/L fluconazole at 48\u2009h increased the total lipids (TLs) at 96\u2009h by 16% without affecting cell growth, which was accompanied by remarkable changes in UMs and NADPH. Cholesterol content was reduced by 8%, and the squalene content improved by 45% at 72\u2009h, which demonstrated fluconazole’s role in inhibiting squalene flow to cholesterol. As another typical UM with antioxidant capacity, the β-carotene production was increased by 53% at 96\u2009h. The increase of squalene and β-carotene could boost intracellular oxidation resistance to protect fatty acids from oxidation. The NADPH was found to be 33% higher than that of the control at 96\u2009h, which meant that the cells had more reducing power for fatty acid synthesis. Metabolic analysis further confirmed that regulation of sterols was closely related to glucose absorption, pigment biosynthesis and fatty acid production in Schizochytrium.ConclusionThis work first reported the effect of UM biosynthesis on fatty acid accumulation in Schizochytrium. The UM was found to affect fatty acid biosynthesis by changing cell membrane function, intracellular antioxidation and reducing power. We believe that this work provides valuable insights in improving PUFA and other valuable matters in microalgae.', 'corpus_id': 208038487, 'score': 1}, {'doc_id': '211263470', 'title': 'Microbial synthesis of functional odd-chain fatty acids: a review.', 'abstract': 'Odd-chain fatty acids (OCFAs) naturally occur in bacteria, higher animals, and in plants. During recent years, they have received increasing attention due to their unique pharmacological properties and usefulness for agricultural and industrial applications. Recently, OCFAs have been identified and quantified in a few organisms, and new pharmacological functions of OCFAs have been reported. Some of the publications are related to the optimization of OCFA production through fermentation and genetic engineering. The present review aims to provide a summary on the recent progress in the field of microbial-derived OCFAs. More specifically, we outline the publications of OCFAs related to (i) different sources of OCFAs; (ii) endogenous synthesis of OCFAs; (iii) production of OCFAs through fermentation; (iv) genetic engineering related to OCFA; and (v) role of OCFAs in human health and disease. Finally, some areas that require further research are discussed.', 'corpus_id': 211263470, 'score': 1}, {'doc_id': '211063382', 'title': 'Natural pigments from microalgae grown in industrial wastewater.', 'abstract': 'The aim of this study was to investigate the cultivation of Nostoc sp., Arthrospira platensis and Porphyridium purpureum in industrial wastewater to produce phycobiliproteins. Initially, light intensity and growth medium composition were optimized, indicating that light conditions influenced the phycobiliproteins production more than the medium composition. Conditions were then selected, according to biomass growth, nutrients removal and phycobiliproteins production, to cultivate these microalgae in food-industry wastewater. The three species could efficiently remove up to 98%, 94% and 100% of COD, inorganic nitrogen and PO43--P, respectively. Phycocyanin, allophycocyanin and phycoerythrin were successfully extracted from the biomass reaching concentrations up to 103, 57 and 30\xa0mg/g dry weight, respectively. Results highlight the potential use of microalgae for industrial wastewater treatment and related high-value phycobiliproteins recovery.', 'corpus_id': 211063382, 'score': 0}, {'doc_id': '208277257', 'title': 'Fed-batch fermentation of mixed carbon source significantly enhances the production of docosahexaenoic acid in Thraustochytriidae sp. PKU#Mn16 by differentially regulating fatty acids biosynthetic pathways.', 'abstract': 'This study reports comparative evaluation of the growth and DHA productivity of the thraustochytrid strain Thraustochytriidae PKU#Mn16 fermented with seven different substrate feeding strategies. Of these strategies, fed-batch fermentation of the mixed substrate (glucose & glycerol) yielded the maximum growth (52.2\u202f±\u202f1.5\u202fg/L), DHA yield (Yp/s: 8.65) and productivity (100.7\u202f±\u202f2.9\u202fmg/L-h), comparable with those of previously reported Aurantiochytrium strains. Transcriptomics analyses revealed that glucose upregulated some genes of the fatty acid synthase pathway whereas glycerol upregulated a few genes of the polyketide synthase pathway. Co-fermentation of the mixed substrate differentially regulated genes of these two pathways and significantly enhanced the DHA productivity. Furthermore, some genes involved in DNA replication, phagosome, carbon metabolism, and β-oxidation were also found to alter significantly during the mixed-substrate fermentation. Overall, this study provides a unique strategy for enhancing growth and DHA productivity of the strain PKU#Mn16 and the first insight into the mechanisms underlying mixed-substrate fermentation.', 'corpus_id': 208277257, 'score': 1}]
170	{'doc_id': '222177200', 'title': 'Understanding Clinical Trial Reports: Extracting Medical Entities and Their Relations', 'abstract': 'The best evidence concerning comparative treatment effectiveness comes from clinical trials, the results of which are reported in unstructured articles. Medical experts must manually extract information from articles to inform decision-making, which is time-consuming and expensive. Here we consider the end-to-end task of both (a) extracting treatments and outcomes from full-text articles describing clinical trials (entity identification) and, (b) inferring the reported results for the former with respect to the latter (relation extraction). We introduce new data for this task, and evaluate models that have recently achieved state-of-the-art results on similar tasks in Natural Language Processing. We then propose a new method motivated by how trial results are typically presented that outperforms these purely data-driven baselines. Finally, we run a fielded evaluation of the model with a non-profit seeking to identify existing drugs that might be re-purposed for cancer, showing the potential utility of end-to-end evidence extraction systems.', 'corpus_id': 222177200}	9484	"[{'doc_id': '221640671', 'title': 'RadLex Normalization in Radiology Reports', 'abstract': ""Radiology reports have been widely used for extraction of various clinically significant information about patients' imaging studies. However, limited research has focused on standardizing the entities to a common radiology-specific vocabulary. Further, no study to date has attempted to leverage RadLex for standardization. In this paper, we aim to normalize a diverse set of radiological entities to RadLex terms. We manually construct a normalization corpus by annotating entities from three types of reports. This contains 1706 entity mentions. We propose two deep learning-based NLP methods based on a pre-trained language model (BERT) for automatic normalization. First, we employ BM25 to retrieve candidate concepts for the BERT-based models (re-ranker and span detector) to predict the normalized concept. The results are promising, with the best accuracy (78.44%) obtained by the span detector. Additionally, we discuss the challenges involved in corpus construction and propose new RadLex terms."", 'corpus_id': 221640671, 'score': 0}, {'doc_id': '59379420', 'title': 'Clinical Concept Embeddings Learned from Massive Sources of Multimodal Medical Data', 'abstract': 'Word embeddings are a popular approach to unsupervised learning of word relationships that are widely used in natural language processing. In this article, we present a new set of embeddings for medical concepts learned using an extremely large collection of multimodal medical data. Leaning on recent theoretical insights, we demonstrate how an insurance claims database of 60 million members, a collection of 20 million clinical notes, and 1.7 million full text biomedical journal articles can be combined to embed concepts into a common space, resulting in the largest ever set of embeddings for 108,477 medical concepts. To evaluate our approach, we present a new benchmark methodology based on statistical power specifically designed to test embeddings of medical concepts. Our approach, called cui2vec, attains state-of-the-art performance relative to previous methods in most instances. Finally, we provide a downloadable set of pre-trained embeddings for other researchers to use, as well as an online tool for interactive exploration of the cui2vec embeddings.', 'corpus_id': 59379420, 'score': 1}, {'doc_id': '188064573', 'title': 'TTrombüs aspirasyonuna sekonder gelişen bir komplikasyon: Yaygın sol sistem trombüsü ve başarılı yönetimi', 'abstract': 'Akut koroner sendromlarda (miyokard infarktusu, kararsiz angina) intrakoroner trombus gorulme orani yuksektir. Miyokard infaktusu ile gelen hastalarda intrakoroner trombus mevcudiyeti guncel mudahale tekniklerine ragmen islem basarisini dusurmekte ve cesitli komplikasyonlara neden olmaktadir. Bizim vakamizda 49 yasinda erkek hastada gelisen komplikasyonlar ve yonetimi sunulmustur. ST elevasyonlu miyokard infarktusu ile gelen hastanin son on inen koroner arterde trombusun aspirasyonu sonrasi tum sol sistemde yaygin trombus gozlemlendi. Intrakoroner heparin ve tirofiban sonrasinda trombus geriledi ve direkt stent implantasyonu ile kompikasyon basarili bir sekilde yonetildi. Son kilavuzlarda da gosterildigi uzere trombus aspirasyonu, yogun koroner arter trombusu olmasi durumunda mortaliteyi istatistiksel olarak anlamli olmasa da azalttigi gosterilmis ancak koroner arter trombus aspirasyonu rutin olarak onerilmemektedir .', 'corpus_id': 188064573, 'score': 0}, {'doc_id': '222090856', 'title': 'Extracting Concepts for Precision Oncology from the Biomedical Literature', 'abstract': 'This paper describes an initial dataset and automatic natural language processing (NLP) method for extracting concepts related to precision oncology from biomedical research articles. We extract five concept types: Cancer, Mutation, Population, Treatment, Outcome. A corpus of 250 biomedical abstracts were annotated with these concepts following standard double-annotation procedures. We then experiment with BERT-based models for concept extraction. The best-performing model achieved a precision of 63.8%, a recall of 71.9%, and an F1 of 67.1. Finally, we propose additional directions for research for improving extraction performance and utilizing the NLP system in downstream precision oncology applications.', 'corpus_id': 222090856, 'score': 1}, {'doc_id': '221738844', 'title': 'Deep Learning Approaches for Extracting Adverse Events and Indications of Dietary Supplements from Clinical Text', 'abstract': 'OBJECTIVE\nWe sought to demonstrate the feasibility of utilizing deep learning models to extract safety signals related to the use of dietary supplements (DSs) in clinical text.\n\n\nMATERIALS AND METHODS\nTwo tasks were performed in this study. For the named entity recognition (NER) task, Bi-LSTM-CRF (bidirectional long short-term memory conditional random field) and BERT (bidirectional encoder representations from transformers) models were trained and compared with CRF model as a baseline to recognize the named entities of DSs and events from clinical notes. In the relation extraction (RE) task, 2 deep learning models, including attention-based Bi-LSTM and convolutional neural network as well as a random forest model were trained to extract the relations between DSs and events, which were categorized into 3 classes: positive (ie, indication), negative (ie, adverse events), and not related. The best performed NER and RE models were further applied on clinical notes mentioning 88 DSs for discovering DSs adverse events and indications, which were compared with a DS knowledge base.\n\n\nRESULTS\nFor the NER task, deep learning models achieved a better performance than CRF, with F1 scores above 0.860. The attention-based Bi-LSTM model performed the best in the RE task, with an F1 score of 0.893. When comparing DS event pairs generated by the deep learning models with the knowledge base for DSs and event, we found both known and unknown pairs.\n\n\nCONCLUSIONS\nDeep learning models can detect adverse events and indication of DSs in clinical notes, which hold great potential for monitoring the safety of DS use.', 'corpus_id': 221738844, 'score': 0}, {'doc_id': '208639707', 'title': 'Improving reference prioritisation with PICO recognition', 'abstract': 'BackgroundMachine learning can assist with multiple tasks during systematic reviews to facilitate the rapid retrieval of relevant references during screening and to identify and extract information relevant to the study characteristics, which include the PICO elements of patient/population, intervention, comparator, and outcomes. The latter requires techniques for identifying and categorising fragments of text, known as named entity recognition.MethodsA publicly available corpus of PICO annotations on biomedical abstracts is used to train a named entity recognition model, which is implemented as a recurrent neural network. This model is then applied to a separate collection of abstracts for references from systematic reviews within biomedical and health domains. The occurrences of words tagged in the context of specific PICO contexts are used as additional features for a relevancy classification model. Simulations of the machine learning-assisted screening are used to evaluate the work saved by the relevancy model with and without the PICO features. Chi-squared and statistical significance of positive predicted values are used to identify words that are more indicative of relevancy within PICO contexts.ResultsInclusion of PICO features improves the performance metric on 15 of the 20 collections, with substantial gains on certain systematic reviews. Examples of words whose PICO context are more precise can explain this increase.ConclusionsWords within PICO tagged segments in abstracts are predictive features for determining inclusion. Combining PICO annotation model into the relevancy classification pipeline is a promising approach. The annotations may be useful on their own to aid users in pinpointing necessary information for data extraction, or to facilitate semantic search.', 'corpus_id': 208639707, 'score': 1}, {'doc_id': '221819468', 'title': 'BioALBERT: A Simple and Effective Pre-trained Language Model for Biomedical Named Entity Recognition', 'abstract': '\n Background: In recent years, with the growing amount of biomedical documents, coupled with advancement in natural language processing algorithms, the research on biomedical named entity recognition (BioNER) has increased exponentially. However, BioNER research is challenging as NER in the biomedical domain are: (i) often restricted due to limited amount of training data, (ii) an entity can refer to multiple types and concepts depending on its context and, (iii) heavy reliance on acronyms that are sub-domain specific. Existing BioNER approaches often neglect these issues and directly adopt the state-of-the-art (SOTA) models trained in general corpora which often yields unsatisfactory results. Results: We propose biomedical ALBERT (A Lite Bidirectional Encoder Representations from Transformers for Biomedical Text Mining) - bioALBERT - an effective domain-specific pre-trained language model trained on huge biomedical corpus designed to capture biomedical context-dependent NER. We adopted self-supervised loss function used in ALBERT that targets on modelling inter-sentence coherence to better learn context-dependent representations and incorporated parameter reduction strategies to minimise memory usage and enhance the training time in BioNER. In our experiments, BioALBERT outperformed comparative SOTA BioNER models on eight biomedical NER benchmark datasets with four different entity types. The performance is increased for; (i) disease type corpora by 7.47% (NCBI-disease) and 10.63% (BC5CDR-disease); (ii) drug-chem type corpora by 4.61% (BC5CDR-Chem) and 3.89 (BC4CHEMD); (iii) gene-protein type corpora by 12.25% (BC2GM) and 6.42% (JNLPBA); and (iv) Species type corpora by 6.19% (LINNAEUS) and 23.71% (Species-800) is observed which leads to a state-of-the-art results. Conclusions: The performance of proposed model on four different biomedical entity types shows that our model is robust and generalizable in recognizing biomedical entities in text. We trained four different variants of BioALBERT models which are available for the research community to be used in future research.', 'corpus_id': 221819468, 'score': 1}, {'doc_id': '195893697', 'title': 'Toward systematic review automation: a practical guide to using machine learning tools in research synthesis', 'abstract': 'Technologies and methods to speed up the production of systematic reviews by reducing the manual labour involved have recently emerged. Automation has been proposed or used to expedite most steps of the systematic review process, including search, screening, and data extraction. However, how these technologies work in practice and when (and when not) to use them is often not clear to practitioners. In this practical guide, we provide an overview of current machine learning methods that have been proposed to expedite evidence synthesis. We also offer guidance on which of these are ready for use, their strengths and weaknesses, and how a systematic review team might go about using them in practice.', 'corpus_id': 195893697, 'score': 1}, {'doc_id': '218977368', 'title': 'Subtitles to Segmentation: Improving Low-Resource Speech-to-TextTranslation Pipelines', 'abstract': 'In this work, we focus on improving ASR output segmentation in the context of low-resource language speech-to-text translation. ASR output segmentation is crucial, as ASR systems segment the input audio using purely acoustic information and are not guaranteed to output sentence-like segments. Since most MT systems expect sentences as input, feeding in longer unsegmented passages can lead to sub-optimal performance. We explore the feasibility of using datasets of subtitles from TV shows and movies to train better ASR segmentation models. We further incorporate part-of-speech (POS) tag and dependency label information (derived from the unsegmented ASR outputs) into our segmentation model. We show that this noisy syntactic information can improve model accuracy. We evaluate our models intrinsically on segmentation quality and extrinsically on downstream MT performance, as well as downstream tasks including cross-lingual information retrieval (CLIR) tasks and human relevance assessments. Our model shows improved performance on downstream tasks for Lithuanian and Bulgarian.', 'corpus_id': 218977368, 'score': 0}, {'doc_id': '15491462', 'title': 'Songwriters and song lyrics: architecture, ambiguity and repetition', 'abstract': 'Abstract This article argues for understanding popular songs and songwriting through the metaphor of architecture, an idea we draw from vernacular terms used by songwriters when comprehending and explaining their own creative practice, and which we deploy in response to those who have called for writing about music to use a non-technical vocabulary and make greater use of metaphor. By architecture we mean those recognisable characteristics of songs that exist as enduring qualities regardless of a specific performance, recording or sheet music score. We use this analogy not as a systematic model, but as a device for exploring the intricate ways in which words and music are combined and pointing to similarities in the composition of poetry and the writing of song lyrics. The art of repetition and play with ambiguity are integral to popular song architectures that endure regardless of the modifications introduced by performers who temporarily inhabit a particular song.', 'corpus_id': 15491462, 'score': 0}]"
171	"{'doc_id': '214743268', 'title': 'StyleRig: Rigging StyleGAN for 3D Control Over Portrait Images', 'abstract': ""StyleGAN generates photorealistic portrait images of faces with eyes, teeth, hair and context (neck, shoulders, background), but lacks a rig-like control over semantic face parameters that are interpretable in 3D, such as face pose, expressions, and scene illumination. Three-dimensional morphable face models (3DMMs) on the other hand offer control over the semantic parameters, but lack photorealism when rendered and only model the face interior, not other parts of a portrait image (hair, mouth interior, background). We present the first method to provide a face rig-like control over a pretrained and fixed StyleGAN via a 3DMM. A new rigging network, \\textit{RigNet} is trained between the 3DMM's semantic parameters and StyleGAN's input. The network is trained in a self-supervised manner, without the need for manual annotations. At test time, our method generates portrait images with the photorealism of StyleGAN and provides explicit control over the 3D semantic parameters of the face."", 'corpus_id': 214743268}"	12537	"[{'doc_id': '231698400', 'title': 'cGANs for Cartoon to Real-life Images', 'abstract': 'Image-to-image translation is a learning task to establish visual mapping between an input and output image. The task has several variations differentiated based on the purpose of the translation, such as synthetic− →real translation [17][21], photo− →caricature translation [23] and many others. The problem has been tackled using different approaches, either through traditional computer vision methods [7], as well as deep learning approaches in recent trends. One approach currently deemed popular and effective is using conditional generative adversarial network, also known shortly as cGAN [15]. It is adapted to perform image-to-image translation tasks with typically two networks: a generator and a discriminator[10]. The generator attempts to generate a duplicated imitation of the input data distribution from a noise distribution while the discriminator classifies whether the generator’s input is fake, i.e. imitation from the generated distribution, or real, i.e. ground truth from the original distribution. Previous research has focused on specific purpose for cross-image translation. Efros et al. [4] proposed a simple model to synthesize an image based on stitches of input images, which represents a traditional approach directly from input image. Fergus et al. [5] addressed specific problems of blurry image into higher crispness, which provided insights on retaining the crispness of duplicated imitation. Additionally, through user studies and an automated mechanism to select images, Chen et al. [3] developed a model to optimize image selection process before cross-image translation. This project is based on an existing implementation of a network called Pix2Pix[10], a cGAN implementation based on U-Net architecture and convolution Markovian discriminator. The use of U-Net architecture [19], instead of the general encoder-decoder architecture is to improve the efficiency of processing input and output with higher resolution while facilitating the transfer of shared, low-level information directly across layers. The U-Net architecture enables cross-layer communication and therefore could more effi-', 'corpus_id': 231698400, 'score': 0}, {'doc_id': '229340598', 'title': 'An Assessment of GANs for Identity-related Applications', 'abstract': 'Generative Adversarial Networks (GANs) are now capable of producing synthetic face images of exceptionally high visual quality. In parallel to the development of GANs themselves, efforts have been made to develop metrics to objectively assess the characteristics of the synthetic images, mainly focusing on visual quality and the variety of images. Little work has been done, however, to assess overfitting of GANs and their ability to generate new identities. In this paper we apply a state of the art biometric network to various datasets of synthetic images and perform a thorough assessment of their identity-related characteristics. We conclude that GANs can indeed be used to generate new, imagined identities meaning that applications such as anonymisation of image sets and augmentation of training datasets with distractor images are viable applications. We also assess the ability of GANs to disentangle identity from other image characteristics and propose a novel GAN triplet loss that we show to improve this disentanglement.', 'corpus_id': 229340598, 'score': 1}, {'doc_id': '229298022', 'title': 'Self-Supervised Sketch-to-Image Synthesis', 'abstract': ""Imagining a colored realistic image from an arbitrarily drawn sketch is one of the human capabilities that we eager machines to mimic. Unlike previous methods that either requires the sketch-image pairs or utilize low-quantity detected edges as sketches, we study the exemplar-based sketch-to-image (s2i) synthesis task in a self-supervised learning manner, eliminating the necessity of the paired sketch data. To this end, we first propose an unsupervised method to efficiently synthesize line-sketches for general RGB-only datasets. With the synthetic paired-data, we then present a self-supervised Auto-Encoder (AE) to decouple the content/style features from sketches and RGB-images, and synthesize images that are both content-faithful to the sketches and style-consistent to the RGB-images. While prior works employ either the cycle-consistence loss or dedicated attentional modules to enforce the content/style fidelity, we show AE's superior performance with pure self-supervisions. To further improve the synthesis quality in high resolution, we also leverage an adversarial network to refine the details of synthetic images. Extensive experiments on 1024*1024 resolution demonstrate a new state-of-art-art performance of the proposed model on CelebA-HQ and Wiki-Art datasets. Moreover, with the proposed sketch generator, the model shows a promising performance on style mixing and style transfer, which require synthesized images to be both style-consistent and semantically meaningful. Our code is available on this https URL, and please visit this https URL for an online demo of our model."", 'corpus_id': 229298022, 'score': 0}, {'doc_id': '221083302', 'title': 'Face identity disentanglement via latent space mapping', 'abstract': 'Learning disentangled representations of data is a fundamental problem in artificial intelligence. Specifically, disentangled latent representations allow generative models to control and compose the disentangled factors in the synthesis process. Current methods, however, require extensive supervision and training, or instead, noticeably compromise quality. In this paper, we present a method that learns how to represent data in a disentangled way, with minimal supervision, manifested solely using available pre-trained networks. Our key insight is to decouple the processes of disentanglement and synthesis, by employing a leading pre-trained unconditional image generator, such as StyleGAN. By learning to map into its latent space, we leverage both its state-of-the-art quality, and its rich and expressive latent space, without the burden of training it. We demonstrate our approach on the complex and high dimensional domain of human heads. We evaluate our method qualitatively and quantitatively, and exhibit its success with de-identification operations and with temporal identity coherency in image sequences. Through extensive experimentation, we show that our method successfully disentangles identity from other facial attributes, surpassing existing methods, even though they require more training and supervision.', 'corpus_id': 221083302, 'score': 1}, {'doc_id': '227161905', 'title': 'StyleUV: Diverse and High-fidelity UV Map Generative Model', 'abstract': 'Reconstructing 3D human faces in the wild with the 3D Morphable Model (3DMM) has become popular in recent years. While most prior work focuses on estimating more robust and accurate geometry, relatively little attention has been paid to improving the quality of the texture model. Meanwhile, with the advent of Generative Adversarial Networks (GANs), there has been great progress in reconstructing realistic 2D images. Recent work demonstrates that GANs trained with abundant high-quality UV maps can produce high-fidelity textures superior to those produced by existing methods. However, acquiring such high-quality UV maps is difficult because they are expensive to acquire, requiring laborious processes to refine. In this work, we present a novel UV map generative model that learns to generate diverse and realistic synthetic UV maps without requiring high-quality UV maps for training. Our proposed framework can be trained solely with in-the-wild images (i.e., UV maps are not required) by leveraging a combination of GANs and a differentiable renderer. Both quantitative and qualitative evaluations demonstrate that our proposed texture model produces more diverse and higher fidelity textures compared to existing methods.', 'corpus_id': 227161905, 'score': 0}, {'doc_id': '219793068', 'title': 'Differentiable Augmentation for Data-Efficient GAN Training', 'abstract': 'The performance of generative adversarial networks (GANs) heavily deteriorates given a limited amount of training data. This is mainly because the discriminator is memorizing the exact training set. To combat it, we propose Differentiable Augmentation (DiffAugment), a simple method that improves the data efficiency of GANs by imposing various types of differentiable augmentations on both real and fake samples. Previous attempts to directly augment the training data manipulate the distribution of real images, yielding little benefit; DiffAugment enables us to adopt the differentiable augmentation for the generated samples, effectively stabilizes training, and leads to better convergence. Experiments demonstrate consistent gains of our method over a variety of GAN architectures and loss functions for both unconditional and class-conditional generation. With DiffAugment, we achieve a state-of-the-art FID of 6.80 with an IS of 100.8 on ImageNet 128x128. Furthermore, with only 20% training data, we can match the top performance on CIFAR-10 and CIFAR-100. Finally, our method can generate high-fidelity images using only 100 images without pre-training, while being on par with existing transfer learning algorithms. Code is available at this https URL.', 'corpus_id': 219793068, 'score': 1}, {'doc_id': '227151907', 'title': 'Unsupervised Discovery of Disentangled Manifolds in GANs', 'abstract': 'As recent generative models can generate photo-realistic images, people seek to understand the mechanism behind the generation process. Interpretable generation process is beneficial to various image editing applications. In this work, we propose a framework to discover interpretable directions in the latent space given arbitrary pre-trained generative adversarial networks. We propose to learn the transformation from prior one-hot vectors representing different attributes to the latent space used by pre-trained models. Furthermore, we apply a centroid loss function to improve consistency and smoothness while traversing through different directions. We demonstrate the efficacy of the proposed framework on a wide range of datasets. The discovered direction vectors are shown to be visually corresponding to various distinct attributes and thus enable attribute editing.', 'corpus_id': 227151907, 'score': 1}, {'doc_id': '229924297', 'title': 'OSTeC: One-Shot Texture Completion', 'abstract': 'The last few years have witnessed the great success of non-linear generative models in synthesizing high-quality photorealistic face images. Many recent 3D facial texture reconstruction and pose manipulation from a single image approaches still rely on large and clean face datasets to train image-to-image Generative Adversarial Networks (GANs). Yet the collection of such a large scale highresolution 3D texture dataset is still very costly and difficult to maintain age/ethnicity balance. Moreover, regressionbased approaches suffer from generalization to the in-thewild conditions and are unable to fine-tune to a targetimage. In this work, we propose an unsupervised approach for one-shot 3D facial texture completion that does not require large-scale texture datasets, but rather harnesses the knowledge stored in 2D face generators. The proposed approach rotates an input image in 3D and fill-in the unseen regions by reconstructing the rotated image in a 2D face generator, based on the visible parts. Finally, we stitch the most visible textures at different angles in the UV imageplane. Further, we frontalize the target image by projecting the completed texture into the generator. The qualitative and quantitative experiments demonstrate that the completed UV textures and frontalized images are of high quality, resembles the original identity, can be used to train a texture GAN model for 3DMM fitting and improve poseinvariant face recognition.1', 'corpus_id': 229924297, 'score': 0}, {'doc_id': '227338901', 'title': 'TediGAN: Text-Guided Diverse Image Generation and Manipulation', 'abstract': 'In this work, we propose TediGAN, a novel framework for multi-modal image generation and manipulation with textual descriptions. The proposed method consists of three components: StyleGAN inversion module, visual-linguistic similarity learning, and instance-level optimization. The inversion module is to train an image encoder to map real images to the latent space of a well-trained StyleGAN. The visual-linguistic similarity is to learn the text-image matching by mapping the image and text into a common embedding space. The instance-level optimization is for identity preservation in manipulation. Our model can provide the lowest effect guarantee, and produce diverse and high-quality images with an unprecedented resolution at 1024. Using a control mechanism based on style-mixing, our TediGAN inherently supports image synthesis with multi-modal inputs, such as sketches or semantic labels with or without instance (text or real image) guidance. To facilitate text-guided multi-modal synthesis, we propose the Multi-Modal CelebA-HQ, a large-scale dataset consisting of real face images and corresponding semantic segmentation map, sketch, and textual descriptions. Extensive experiments on the introduced dataset demonstrate the superior performance of our proposed method. Code and data are available at https://github.com/weihaox/TediGAN.', 'corpus_id': 227338901, 'score': 0}, {'doc_id': '225039998', 'title': 'Few-Shot Adaptation of Generative Adversarial Networks', 'abstract': 'Generative Adversarial Networks (GANs) have shown remarkable performance in image synthesis tasks, but typically require a large number of training samples to achieve high-quality synthesis. This paper proposes a simple and effective method, Few-Shot GAN (FSGAN), for adapting GANs in few-shot settings (less than 100 images). FSGAN repurposes component analysis techniques and learns to adapt the singular values of the pre-trained weights while freezing the corresponding singular vectors. This provides a highly expressive parameter space for adaptation while constraining changes to the pretrained weights. We validate our method in a challenging few-shot setting of 5-100 images in the target domain. We show that our method has significant visual quality gains compared with existing GAN adaptation methods. We report qualitative and quantitative results showing the effectiveness of our method. We additionally highlight a problem for few-shot synthesis in the standard quantitative metric used by data-efficient image synthesis works. Code and additional results are available at this http URL.', 'corpus_id': 225039998, 'score': 1}]"
172	{'doc_id': '220252634', 'title': 'Innocent until proven guilty: Privacy-preserving search over a central CODIS criminal database from the field', 'abstract': 'The presumption of innocence (i.e., the principle that one is considered innocent until proven guilty) is a cornerstone of the criminal justice system in many countries, including the United States. DNA analysis is an important tool for criminal investigations1. In the U.S. alone, it has already aided in over half a million investigations using the Combined DNA Index System (CODIS) and associated DNA databases2. CODIS includes DNA profiles of crime scene forensic samples, convicted offenders, missing persons and more. The CODIS framework is currently used by over 50 other countries3 including much of Europe, Canada, China and more. During investigations, DNA samples can be collected from multiple individuals who may have had access to, or were found near a crime scene, in the hope of finding a single criminal match4. Controversially, CODIS samples are sometimes retained from adults and juveniles despite not yielding any database match4–6. Here we introduce a cryptographic algorithm that finds any and all matches of a person’s DNA profile against a CODIS database without revealing anything about the person’s profile to the database provider. With our protocol, matches are immediately identified as before; however, individuals who do not match anything in the database retain their full privacy. Our novel algorithm runs in 40 seconds on a CODIS database of 1,000,000 entries, enabling its use to privately screen potentially-innocent suspects even in the field.', 'corpus_id': 220252634}	16370	"[{'doc_id': '233466830', 'title': 'The First Successful Use of Simple Low Stringency Familial Searching in a French Criminal Investigation', 'abstract': 'We describe how a very simple application of familial searching was used to resolve a decade old, high profile rape/murder in France. This was the first use of familial searching in a criminal case, using the French DNA database, which contains approximately 1,800,000 profiles. When an unknown forensic profile (18 loci) was used to search that database using CODIS at low stringency, a single match was identified. This match was the father of the man who left the semen recovered from murder victim Elodie Kulik. The match was confirmed using Y-chromosome DNA from the putative father, an STR profile from the mother, and finally a tissue sample from the exhumed body of the man who left the semen. Because of this identification, the investigators are now pursuing possible co-conspirators. This article describes the combination of methods which led to the match, resulting from open discussions between scientists and investigators. POSTPRINT VERSION. The final version is published here: Pham-Hoai, E., Crispino, F., & Hampikian, G. (2014). The first successful use of a low stringency familial match in a french criminal investigation. Journal of Forensic Sciences, 59(3), 816-819. https://doi.org/10.1111/1556-4029.12372', 'corpus_id': 233466830, 'score': 0}, {'doc_id': '233323957', 'title': 'Awareness level on the role of forensic DNA database in criminal investigation in Nigeria: A case study of Benin city', 'abstract': 'In recent times, the admissibility of DNA evidence is widely used in numerous courts around the world. Its underpinning science is consistent, reproducible, accurate, and founded on authenticated technology and techniques for the generation, comparison, and interpretation of a DNA pro ile [1]. Forensic DNA databases have proved to be an indispensable tool in preventing miscarriages of justice and deterring offenders from further criminal activity [2-4]. To achieve a momentous decrease in the rate at which crimes are committed, and credible detection rates, a switch from the traditional ‘reactive’ model of crime investigation to a modern model of investigation is vital [5]. The use of DNA to trace crime suspects has been a prominent improvement in policing, and when DNA databases are ef iciently put to use, it overwhelmingly aids the conviction of culprits and exoneration of the innocent [6].', 'corpus_id': 233323957, 'score': 0}, {'doc_id': '73514951', 'title': 'Risks of compulsory genetic databases', 'abstract': 'In their Policy Forum “Is it time for a universal genetic forensic database?” (23 November 2018, p. [898][1]), J. W. Hazel et al. propose a nationwide forensic database for DNA information [Combined DNA Index System (CODIS) markers] from all citizens. Although the authors recognize that there have been similar attempts to create such a database in the past, they avoid mentioning that those efforts were met with widespread disapproval.\n\nIn 2007, when a UK court of appeal judge raised the idea, it was criticized by the director of a human rights organization as a “chilling proposal, ripe for indignity, error and abuse” ([ 1 ][2]). In 2015, Kuwait went a step further by adopting a law to implement a large compulsory database. It was universally panned by human rights agencies and experts for imposing unnecessary and disproportionate restrictions on individuals\' right to privacy ([ 2 ][3], [ 3 ][4]). Kuwait\'s constitutional court invalidated the law on these grounds ([ 4 ][5]).\n\nHazel et al. propose such a database as a way to correct issues that they recognize already exist in the current system of DNA databases, including discrimination, questionable search practices, mismanagement, and misuses of genetic information. However, creating compulsory genetic databases will not make the system more humane for minority groups or improve strained relationships with government agencies. Better regulation of existing structures and added considerations for vulnerable groups might be a more effective strategy.\n\nThe proposed database would be expensive ([ 5 ][6]) and intrusive for everyone, and it would increase the risk of abusive usage of genetic information. Such an endeavor is also likely to exacerbate the existing climate of mistrust ([ 6 ][7]) and negatively affect public perception of genetics. The analogy with public health newborn screening programs as a justification is particularly tendentious and unhelpful. These programs are developed to detect severe genetic disorders (such as phenylketonuria) in asymptomatic newborns in order to treat them early, which is in their best interest.\n\nCrime deterrence and administrative efficiency are important objectives. However, free and democratic societies must balance these goals against privacy, autonomy, and the presumption of innocence. The willingness to trade universally recognized rights for the hypothetical benefits of administrative efficiency is a common denominator of surveillance states ([ 7 ][8]).\n\n1. [↵][9]1. C. Dyer\n , “Anger over call to widen DNA database,” The Guardian (2007); [www.theguardian.com/uk/2007/sep/06/ukcrime.prisonsandprobation][10].\n \n\n2. [↵][11]Human Rights Watch, “Kuwait: New counterterror law sets mandatory DNA testing” (2015); [www.hrw.org/news/2015/07/20/kuwait-new-counterterror-law-sets-mandatory-dna-testing][12]).\n \n\n3. [↵][13]UN Human Rights Committee, “UN Human Rights Committee: Concluding observations: Kuwait” (2000); [www.refworld.org/docid/3df36be44.html][14].\n \n\n4. [↵][15]1. A. Coghlan\n , “Kuwait\'s plans for mandatory DNA database have been cancelled,” NewScientist (2017); [www.newscientist.com/article/2149830-kuwaits-plans-for-mandatory-dna-database-have-been-cancelled][16].\n \n\n5. [↵][17]Forensic Genetics Policy Initiative, “Establishing best practice for forensic DNA databases” (2017); .\n \n\n6. [↵][18]1. M. Madden, 2. L. Rainie\n , “Americans\' attitudes about privacy, security and surveillance,” Pew Research Center (2015); [www.pewinternet.org/2015/05/20/americans-attitudes-about-privacy-security-and-surveillance/][19].\n \n\n7. [↵][20]1. D. Lyon\n 1. D. Nelkin, 2. L. Andrews\n , in Surveillance as Social Sorting: Privacy, Risk and Digital Discrimination, D. Lyon, Ed. (Routledge, New York, 2003), pp. 94–110.\n\n [1]: http://www.sciencemag.org/content/362/6417/898\n [2]: #ref-1\n [3]: #ref-2\n [4]: #ref-3\n [5]: #ref-4\n [6]: #ref-5\n [7]: #ref-6\n [8]: #ref-7\n [9]: #xref-ref-1-1 ""View reference 1 in text""\n [10]: http://www.theguardian.com/uk/2007/sep/06/ukcrime.prisonsandprobation\n [11]: #xref-ref-2-1 ""View reference 2 in text""\n [12]: http://www.hrw.org/news/2015/07/20/kuwait-new-counterterror-law-sets-mandatory-dna-testing\n [13]: #xref-ref-3-1 ""View reference 3 in text""\n [14]: http://www.refworld.org/docid/3df36be44.html\n [15]: #xref-ref-4-1 ""View reference 4 in text""\n [16]: http://www.newscientist.com/article/2149830-kuwaits-plans-for-mandatory-dna-database-have-been-cancelled\n [17]: #xref-ref-5-1 ""View reference 5 in text""\n [18]: #xref-ref-6-1 ""View reference 6 in text""\n [19]: http://www.pewinternet.org/2015/05/20/americans-attitudes-about-privacy-security-and-surveillance/\n [20]: #xref-ref-7-1 ""View reference 7 in text""', 'corpus_id': 73514951, 'score': 1}, {'doc_id': '232424772', 'title': 'DNA Profiling: Forensic Science under the Microscope1', 'abstract': 'orensic scientists and crime investigators have long dreamed of being able to identify the origin of blood and other body fluids with certainty. The currently developing techniques of DNA profiling promise a degree of accuracy greater even than current methods of fingerprinting suspects. DNA profiling allows examination of human biological material at its most fundamental level the deoxyribonucleic acid (DNA) molecule. This molecule, which is found in every living cell within the body, carries the genetic information that makes one individual separate and distinct from every other individual. The DNA profiling process involves extracting the DNA from a specimen such as semen, blood or tissue and chemically dividing the DNA into fragments. Because of the naturally occurring variations in the DNA molecule from one person to another, the sequence of fragments forms a pattern, similar to a bar code found on items in supermarkets, that is to all intents and purposes unique to the individual (Phillips 1988, p. 551; Thompson & Ford 1989). The different forms of DNA testing use different numbers of various sensitive probes and differ in their percentage of result certainty. DNA testing, however, is distinguished by its particularly low false positive rate. So long as stringently developed laboratory procedures are painstakingly adhered to, DNA profiling evidence should be as reliable as any form of scientific evidence that is brought before the courts. Many claims have been made by the competing scientific houses marketing their versions of DNA profiling for the accuracy rates of their form of DNA testing. It is clear, though, that DNA profiling represents a dramatic step forward in the capacity of science to tell us whether two body samples obtained at different times come from the same individual. This paper identifies the relevance of the techniques for the legal system, especially in light of the Castro (Unreported. Supreme Court of the State of New York County of Bronx, 14 August 1989) decision in the United States, focusses on matters likely to be contentious for lawyers, and queries the civil liberties issues that accompany the development of DNA profiling techniques.', 'corpus_id': 232424772, 'score': 0}, {'doc_id': '233431163', 'title': 'The Advent of DNA Databanks: Implications for Information Privacy', 'abstract': 'Genetic identification tests — better known as DNA profiling — currently allow criminal investigators to connect suspects to physical samples retrieved from a victim or the scene of a crime. A controversial yet acclaimed expansion of DNA analysis is the creation of a massive databank of genetic codes. This Note explores the privacy concerns arising out of the collection and retention of extremely personal information in a central database. The potential for unauthorized access by those not investigating a particular crime compels the implementation of national standards and stringent security measures.', 'corpus_id': 233431163, 'score': 1}, {'doc_id': '209602460', 'title': 'The Racial Composition of Forensic DNA Databases', 'abstract': 'Forensic DNA databases have received an inordinate amount of academic and judicial attention. From their inception, scholars, advocates, and judges have wrestled with the proper reach of DNA collection, retention, and search policies. Central to these debates are concerns about racial equity in forensic genetic practices. Yet when such questions arise, critics typically just assert that forensic DNA databases are not demographically representative. Such assertions are expressed in vague or conclusory terms, without a citation to actual data or even to concrete estimates about the actual composition of DNA databases. \n \nThis Article endeavors to fill these gaps in the literature by providing demographic information about the composition of forensic DNA databases. We draw upon two sources. First, we obtained data from states in response to our requests under freedom of information laws. Second, we devised an original estimate based on public information about each state’s DNA collection policies and the demographic data that matches those policies. In other words, we reverse-engineered the national DNA database. \n \nWe then use our data on the actual and estimated racial composition of DNA databases to identify and illuminate four questions fundamental to forensic DNA policy. First, the data center racial justice concerns as critical to debates about the proper scope of collection and search policies, as well as the impact of forensic DNA database practices more generally. Second, the data cast light on the significance, determinacy, and stability of race and ethnicity as meaningful biological and social categories. Third, the data provide insight into the advantages and disadvantages of choosing among architectural approaches when collecting, storing, and searching sensitive data such as genetic profiles. And finally, the data prompt questions about genetic privacy more generally, including how to weigh the significance of criminal justice practices in an increasingly genetically transparent society.', 'corpus_id': 209602460, 'score': 1}, {'doc_id': '27289243', 'title': 'Do Health and Forensic DNA Databases Increase Racial Disparities?', 'abstract': 'Peter Chow-White and Troy Duster examine the question of whether the ""digital divide"" in health and forensic DNA databases is contributing to racial disparities.', 'corpus_id': 27289243, 'score': 1}, {'doc_id': '233743017', 'title': 'The analysis of ancestry with small-scale forensic panels of genetic markers.', 'abstract': ""In the last 10 years, forensic genetic analysis has been extended beyond identification tests that link a suspect to crime scene evidence using standard DNA profiling, to new supplementary tests that can provide information to investigators about a suspect in the absence of a database hit or eyewitness testimony. These tests now encompass the prediction of physical appearance, ancestry and age. In this review, we give a comprehensive overview of the full range of DNA-based ancestry inference tests designed to work with forensic contact traces, when the level of DNA is often very low or highly degraded. We outline recent developments in the design of ancestry-informative marker sets, forensic assays that use capillary electrophoresis or massively parallel sequencing, and the statistical analysis frameworks that examine the test profile and compares it to reference population variation. Three casework ancestry analysis examples are described which were successfully accomplished in the authors' laboratory, where the ancestry information obtained was critical to the outcome of the DNA analyses made."", 'corpus_id': 233743017, 'score': 0}, {'doc_id': '53727871', 'title': 'Is it time for a universal genetic forensic database?', 'abstract': 'Bias and privacy concerns cloud police use of genetics DNA is an increasingly useful crime-solving tool. But still quite unclear is the extent to which law enforcement should be able to obtain genetic data housed in public and private databases. How one answers that question might vary substantially, depending on the source of the data. Several countries—the United Kingdom, Kuwait, and Saudi Arabia among them—have even toyed with creating a “universal” DNA database, populated with data from every individual in society, obviating the need for any other DNA source (1). Although this move would be controversial, it may not be as dramatic as one might think. In the United States, for example, the combination of state and federal databases (containing genetic profiles of more than 16.5 million arrestees and convicts) and public and private databases (containing genetic data of tens of millions of patients, consumers, and research participants) already provides the government with potential access to genetic information that can be linked to a large segment of the country, either directly or through a relative (2, 3). We discuss here how, if correctly implemented, a universal database would likely be more productive and less discriminatory than our current system, without compromising as much privacy.', 'corpus_id': 53727871, 'score': 1}, {'doc_id': '233297362', 'title': 'Four misconceptions about investigative genetic genealogy', 'abstract': 'Abstract Investigative genetic genealogy (IGG) is a new technique for identifying criminal suspects that has sparked controversy. The technique involves uploading a crime scene DNA profile to one or more genetic genealogy databases with the intention of identifying a criminal offender’s genetic relatives and, eventually, locating the offender within the family tree. IGG was used to identify the Golden State Killer in 2018 and it is now being used in connection with hundreds of cases in the USA. Yet, as more law enforcement agencies conduct IGG, the privacy implications of the technique have come under scrutiny. While these issues deserve careful attention, we are concerned that their discussion is, at times, based on misunderstandings related to how IGG is used in criminal investigations and how IGG departs from traditional investigative techniques. Here, we aim to clarify and sharpen the public debate by addressing four misconceptions about IGG. We begin with a detailed description of IGG as it is currently practiced: what it is and—just as important—what it is not. We then examine misunderstood or not widely known aspects of IGG that are potentially confusing efforts to have constructive discussions about its future. We conclude with recommendations intended to support the productivity of those discussions.', 'corpus_id': 233297362, 'score': 0}]"
173	{'doc_id': '5981909', 'title': 'The Mythos of Model Interpretability', 'abstract': 'Supervised machine-learning models boast remarkable predictive capabilities. But can you trust your model? Will it work in deployment? What else can it tell you about the world?', 'corpus_id': 5981909}	2966	"[{'doc_id': '218869555', 'title': 'Learning to Simulate Dynamic Environments With GameGAN', 'abstract': 'Simulation is a crucial component of any robotic system. In order to simulate correctly, we need to write complex rules of the environment: how dynamic agents behave, and how the actions of each of the agents affect the behavior of others. In this paper, we aim to learn a simulator by simply watching an agent interact with an environment. We focus on graphics games as a proxy of the real environment. We introduce GameGAN, a generative model that learns to visually imitate a desired game by ingesting screenplay and keyboard actions during training. Given a key pressed by the agent, GameGAN ""renders"" the next screen using a carefully designed generative adversarial network. Our approach offers key advantages over existing work: we design a memory module that builds an internal map of the environment, allowing for the agent to return to previously visited locations with high visual consistency. In addition, GameGAN is able to disentangle static and dynamic components within an image making the behavior of the model more interpretable, and relevant for downstream tasks that require explicit reasoning over dynamic elements. This enables many interesting applications such as swapping different components of the game to build new games that do not exist. We will release the code and trained model, enabling human players to play generated games and their variations with our GameGAN.', 'corpus_id': 218869555, 'score': 0}, {'doc_id': '53741665', 'title': 'Representer Point Selection for Explaining Deep Neural Networks', 'abstract': 'We propose to explain the predictions of a deep neural network, by pointing to the set of what we call representer points in the training set, for a given test point prediction. Specifically, we show that we can decompose the pre-activation prediction of a neural network into a linear combination of activations of training points, with the weights corresponding to what we call representer values, which thus capture the importance of that training point on the learned parameters of the network. But it provides a deeper understanding of the network than simply training point influence: with positive representer values corresponding to excitatory training points, and negative values corresponding to inhibitory points, which as we show provides considerably more insight. Our method is also much more scalable, allowing for real-time feedback in a manner not feasible with influence functions.', 'corpus_id': 53741665, 'score': 1}, {'doc_id': '53731492', 'title': 'Explicability? Legibility? Predictability? Transparency? Privacy? Security? The Emerging Landscape of Interpretable Agent Behavior', 'abstract': 'There has been significant interest of late in generating behavior of agents that is interpretable to the human (observer) in the loop. However, the work in this area has typically lacked coherence on the topic, with proposed solutions for ""explicable"", ""legible"", ""predictable"" and ""transparent"" planning with overlapping, and sometimes conflicting, semantics all aimed at some notion of understanding what intentions the observer will ascribe to an agent by observing its behavior. This is also true for the recent works on ""security"" and ""privacy"" of plans which are also trying to answer the same question, but from the opposite point of view -- i.e. when the agent is trying to hide instead of revealing its intentions. This paper attempts to provide a workable taxonomy of relevant concepts in this exciting and emerging field of inquiry.', 'corpus_id': 53731492, 'score': 1}, {'doc_id': '69951972', 'title': 'Inceptionism: Going Deeper into Neural Networks', 'abstract': None, 'corpus_id': 69951972, 'score': 1}, {'doc_id': '67440606', 'title': 'The Building Blocks of Interpretability', 'abstract': None, 'corpus_id': 67440606, 'score': 1}, {'doc_id': '220793387', 'title': 'Noisy Agents: Self-supervised Exploration by Predicting Auditory Events', 'abstract': 'Humans integrate multiple sensory modalities (e.g., visual and audio) to build a causal understanding of the physical world. In this work, we propose a novel type of intrinsic motivation for Reinforcement Learning (RL) that encourages the agent to understand the causal effect of its actions through auditory event prediction. First, we allow the agent to collect a small amount of acoustic data and use K-means to discover underlying auditory event clusters. We then train a neural network to predict the auditory events and use the prediction errors as intrinsic rewards to guide RL exploration. We first conduct an in-depth analysis of our module using a set of Atari games. We then apply our model to audio-visual exploration using the Habitat simulator and active learning using the TDW simulator. Experimental results demonstrate the advantages of using audio signals over vision-based models as intrinsic rewards to guide RL explorations.', 'corpus_id': 220793387, 'score': 0}, {'doc_id': '215744839', 'title': 'Meta-Learning in Neural Networks: A Survey', 'abstract': 'The field of meta-learning, or learning-to-learn, has seen a dramatic rise in interest in recent years. Contrary to conventional approaches to AI where a given task is solved from scratch using a fixed learning algorithm, meta-learning aims to improve the learning algorithm itself, given the experience of multiple learning episodes. This paradigm provides an opportunity to tackle many of the conventional challenges of deep learning, including data and computation bottlenecks, as well as the fundamental issue of generalization. In this survey we describe the contemporary meta-learning landscape. We first discuss definitions of meta-learning and position it with respect to related fields, such as transfer learning, multi-task learning, and hyperparameter optimization. We then propose a new taxonomy that provides a more comprehensive breakdown of the space of meta-learning methods today. We survey promising applications and successes of meta-learning including few-shot learning, reinforcement learning and architecture search. Finally, we discuss outstanding challenges and promising areas for future research.', 'corpus_id': 215744839, 'score': 0}, {'doc_id': '232257833', 'title': 'PredRNN: A Recurrent Neural Network for Spatiotemporal Predictive Learning', 'abstract': 'The predictive learning of spatiotemporal sequences aims to generate future images by learning from the historical context, where the visual dynamics are believed to have modular structures that can be learned with compositional subsystems. This paper models these structures by presenting PredRNN, a new recurrent network, in which a pair of memory cells are explicitly decoupled, operate in nearly independent transition manners, and finally form unified representations of the complex environment. Concretely, besides the original memory cell of LSTM, this network is featured by a zigzag memory flow that propagates in both bottom-up and top-down directions across all layers, enabling the learned visual dynamics at different levels of RNNs to communicate. It also leverages a memory decoupling loss to keep the memory cells from learning redundant features. We further propose a new curriculum learning strategy to force PredRNN to learn long-term dynamics from context frames, which can be generalized to most sequence-to-sequence models. We provide detailed ablation studies to verify the effectiveness of each component. Our approach is shown to obtain highly competitive results on five datasets for both action-free and action-conditioned predictive learning scenarios.', 'corpus_id': 232257833, 'score': 0}, {'doc_id': '13193974', 'title': 'Understanding Black-box Predictions via Influence Functions', 'abstract': ""How can we explain the predictions of a black-box model? In this paper, we use influence functions — a classic technique from robust statistics — to trace a model's prediction through the learning algorithm and back to its training data, thereby identifying training points most responsible for a given prediction. To scale up influence functions to modern machine learning settings, we develop a simple, efficient implementation that requires only oracle access to gradients and Hessian-vector products. We show that even on non-convex and non-differentiable models where the theory breaks down, approximations to influence functions can still provide valuable information. On linear models and convolutional neural networks, we demonstrate that influence functions are useful for multiple purposes: understanding model behavior, debugging models, detecting dataset errors, and even creating visually-indistinguishable training-set attacks."", 'corpus_id': 13193974, 'score': 1}, {'doc_id': '221856630', 'title': 'Tasks, stability, architecture, and compute: Training more effective learned optimizers, and using them to train themselves', 'abstract': 'Much as replacing hand-designed features with learned functions has revolutionized how we solve perceptual tasks, we believe learned algorithms will transform how we train models. In this work we focus on general-purpose learned optimizers capable of training a wide variety of problems with no user-specified hyperparameters. We introduce a new, neural network parameterized, hierarchical optimizer with access to additional features such as validation loss to enable automatic regularization. Most learned optimizers have been trained on only a single task, or a small number of tasks. We train our optimizers on thousands of tasks, making use of orders of magnitude more compute, resulting in optimizers that generalize better to unseen tasks. The learned optimizers not only perform well, but learn behaviors that are distinct from existing first order optimizers. For instance, they generate update steps that have implicit regularization and adapt as the problem hyperparameters (e.g. batch size) or architecture (e.g. neural network width) change. Finally, these learned optimizers show evidence of being useful for out of distribution tasks such as training themselves from scratch.', 'corpus_id': 221856630, 'score': 0}]"
174	{'doc_id': '131754984', 'title': 'An integrated economics model for ISRU in support of a Mars colony - initial status report', 'abstract': 'The aim of this effort is to develop an integrated set of risk-based financial and technical models to evaluate multiple Off-Earth Mining (OEM) scenarios. This quantitative, scenarioand simulation-based tool will help identify combinations of market variables, technical parameters, and policy levers that will enable the expansion of the global economy into the solar system and return economic benefits. Human ventures in space are entering a new phase in which missions formerly driven by government agencies are now being replaced by those led by commercial enterprises – in launch, satellite deployment, resupply of the International Space Station, and space tourism. In the not-too-distant future, commercial opportunities will also include the mining of asteroids, the Moon, and Mars. This investigation will examine the role of OEM in a growing space economy. (In this investigation, the term ‘mining’ is taken to embrace minerals, ice/water, and other in situ resources.) OEM can be the engine that drives the space economy, so it would be useful to understand what OEM market conditions and technology requirements are needed for that economy to prosper. These specific elements will be studied in the wider context of creating an economy that could ultimately support a sustainable Mars Colony. Such a colony will need in situ resources not only for its own survival, but to prosper and grow, it must create viable business ventures, essentially by fulfilling the demand for in situ resources from and on Mars. This investigation will focus on understanding the role and economic prospect for OEM associated with the Human Colonization of Mars (HCM).', 'corpus_id': 131754984}	1114	"[{'doc_id': '19398128', 'title': 'Water extraction on Mars for an expanding human colony.', 'abstract': ""In-situ water extraction is necessary for an extended human presence on Mars. This study looks at the water requirements of an expanding human colony on Mars and the general systems needed to supply that water from the martian atmosphere and regolith. The proposed combination of systems in order to supply the necessary water includes a system similar to Honeybee Robotics' Mobile In-Situ Water Extractor (MISWE) that uses convection, a system similar to MISWE but that directs microwave energy down a borehole, a greenhouse or hothouse type system, and a system similar to the Mars Atmospheric Resource Recovery System (MARRS). It is demonstrated that a large water extraction system that can take advantage of large deposits of water ice at site specific locations is necessary to keep up with the demands of a growing colony."", 'corpus_id': 19398128, 'score': 1}, {'doc_id': '211678182', 'title': 'Cleaner Production in Optimized Multivariate Networks: Operations Management through a Roll of Dice', 'abstract': 'The importance of supply chain management in analyzing and later catalyzing economic expectations while simultaneously prioritizing cleaner production aspects is a vital component of modern finance. Such predictions, though, are often known to be less than accurate due to the ubiquitous uncertainty plaguing most business decisions. Starting from a multi-dimensional cost function defining the sustainability of the supply chain (SC) kernel, this article outlines a 4-component SC module - environmental, demand, economic, and social uncertainties - each ranked according to its individual weight. Our mathematical model then assesses the viability of a sustainable business by first ranking the potentially stochastic variables in order of their subjective importance, and then optimizing the cost kernel, defined from a utility function. The model will then identify conditions (as equations) validating the sustainability of a business venture. The ranking is initially obtained from an Analytical Hierarchical Process; the resultant weighted cost function is then optimized to analyze the impact of market uncertainty based on our supply chain model. Model predictions are then ratified against SME data to emphasize the importance of cleaner production in business strategies.', 'corpus_id': 211678182, 'score': 0}, {'doc_id': '211126741', 'title': 'Site-dependent levelized cost assessment for fully renewable Power-to-Methane systems', 'abstract': 'The generation of synthetic natural gas from renewable electricity enables long-term energy storage and provides clean fuels for transportation. In this article, we analyze fully renewable Power-to-Methane systems using a high-resolution energy system optimization model applied to two regions within Europe. The optimum system layout and operation depend on the availability of natural resources, which vary between locations and years. We find that much more wind than solar power is used, while the use of an intermediate battery electric storage system has little effects. The resulting levelized costs of methane vary between 0.24 and 0.30 Euro/kWh and the economic optimal utilization rate between 63% and 78%. We further discuss how the economic competitiveness of Power-to-Methane systems can be improved by the technical developments and by the use of co-products, such as oxygen and curtailed electricity. A sensitivity analysis reveals that the interest rate has the highest influence on levelized costs, followed by the investment costs for wind and electrolyzer stack.', 'corpus_id': 211126741, 'score': 0}, {'doc_id': '211204788', 'title': 'Economic Viability and Infrastructure Requirements for the Electrification of Highway Traffic', 'abstract': 'Battery electric vehicles are rapidly entering the market. Their success offers great opportunities for the decarbonization of the transport sector, but also pose new challenges to energy infrastructures. Public charging stations must be built and power grids may become congested. In this article, we analyze the optimal layout and operation of charging systems along highways using a high-resolution optimization model. We discuss the economic viability and identify potential roadblocks impeding a rapid build-up of electric mobility. We find that congestion of regional distribution grids becomes a serious issue already for a moderate market penetration of electric vehicles. While peak loads can be handled by battery electric storage systems, the grid connection fundamentally limits the total amount of cars that can be served per day. Our results further highlight the interdependency of different sectors and the importance of regional infrastructures during the transformation to a sustainable energy system. Given the long time period needed for the planning and realization of infrastructure measures, rapid decisions are imperative.', 'corpus_id': 211204788, 'score': 0}, {'doc_id': '212633620', 'title': 'Predicting the ecological outcomes of global consumption', 'abstract': 'Mapping pathways to achieving the sustainable development goals requires understanding and predicting how social, economic and political factors impact biodiversity. Trends in demography, economic growth, regional alliances and consumption behaviours can have profound effects on the environment by driving resource use and production. While these distant socio-economic drivers impact species and ecosystems at global scales, for example by driving greenhouse gas emissions and climate change, the most prevalent human impacts on biodiversity manifest through habitat loss and land use change decisions at finer scales. We provide the first integrated ecological-economic analysis pathway capable of supporting both national policy design challenges and global scale assessment of biodiversity risks posed by socio-economic drivers such as population growth, consumption and trade. To achieve this, we provide state-of-the-art integration of economic, land use, and biodiversity modelling, and illustrate its application using two case studies. We evaluate the national-level implications of change in trading conditions under a multi-lateral free trade agreement for the bird biodiversity of Vietnam. We review the implications for land-use and biodiversity under coupled socio-economic (Shared Socioeconomic Pathways) and climate (Resource Concentration Pathways) scenarios for Australia. Our study provides a roadmap for setting up high dimensional integrated analyses foe evaluating global priorities for protecting nature and livelihoods in vulnerable areas with the greatest conflicts for economic, social and environmental opportunities.', 'corpus_id': 212633620, 'score': 0}, {'doc_id': '212644500', 'title': 'A New Approach for Macroscopic Analysis in order to improve the Technical and Economic Impacts of Urban Interchanges on Traffic Networks -- A Case Study', 'abstract': 'Perusing three important elements (economic, safety and traffic) is the overall objective of decision evaluation across all transport projects. In this study, we investigate the feasibility of development of city interchanges, and road connections for network users. In order to achieve this goal, a series of smaller goals are required including determining benefits, costs of implementing new highway interchanges, quantifying the effective parameters, quantifying the increase in fuel consumption, quantifying the reduction in travel time and growth in travel speeds. In this study, geometric advancement of Hakim highway, and Yadegar-e-Emam highway was investigated just Macro from cloverleaf intersection with a low capacity to three-level directional intersection and enhanced cloverleaf. For this purpose, the simulation was done by EMME/2 software. The results of the method of net present value (NPV) were evaluated economically, and the benefit and cost of each one was stated precisely in different years (%28 improvement). The sensitivity analysis indicated that the cost of fuel, cost of travel time, cost of accidents and cost of pollutants have the highest impact factor in this assessment respectively.', 'corpus_id': 212644500, 'score': 0}, {'doc_id': '212742268', 'title': 'How China is planning to go to Mars amid the coronavirus outbreak', 'abstract': 'The launch is on track for July, as Europe and Russia announce a two-year delay in their journey to the red planet. “The launch is so important politically that they will make it happen,” says US-based planetary geologist Raymond Arvidson.', 'corpus_id': 212742268, 'score': 1}, {'doc_id': '156972953', 'title': 'Political and legal challenges in a Mars colony', 'abstract': 'Abstract In our essay we are going to briefly discuss some legal and political questions associated with the future colonization of Mars which is now being planned by NASA and the second one is known as Project MarsOne. We assume that it will be unclear as to which legal and political solutions could work in the new Martian ecological niche. Here we will show that this issue requires more attention because we unable to predict which elements of human nature will dominate the lives of the Mars colonizers.', 'corpus_id': 156972953, 'score': 1}, {'doc_id': '211572463', 'title': 'Achieving the required mobility in the solar system through direct fusion drive', 'abstract': 'Abstract To develop a spacefaring civilization, humankind must develop technologies which enable safe, affordable and repeatable mobility through the solar system. One such technology is nuclear fusion propulsion which is at present under study mostly as a breakthrough toward the first interstellar probes. The aim of the present paper is to show that direct fusion drive is even more important in human planetary exploration and constitutes the natural solution to the problem of exploring and colonizing the solar system.', 'corpus_id': 211572463, 'score': 1}, {'doc_id': '210839032', 'title': 'The Habitable Exoplanet Observatory (HabEx) Mission Concept Study Final Report', 'abstract': 'The Habitable Exoplanet Observatory, or HabEx, has been designed to be the Great Observatory of the 2030s. For the first time in human history, technologies have matured sufficiently to enable an affordable space-based telescope mission capable of discovering and characterizing Earthlike planets orbiting nearby bright sunlike stars in order to search for signs of habitability and biosignatures. Such a mission can also be equipped with instrumentation that will enable broad and exciting general astrophysics and planetary science not possible from current or planned facilities. HabEx is a space telescope with unique imaging and multi-object spectroscopic capabilities at wavelengths ranging from ultraviolet (UV) to near-IR. These capabilities allow for a broad suite of compelling science that cuts across the entire NASA astrophysics portfolio. HabEx has three primary science goals: (1) Seek out nearby worlds and explore their habitability; (2) Map out nearby planetary systems and understand the diversity of the worlds they contain; (3) Enable new explorations of astrophysical systems from our own solar system to external galaxies by extending our reach in the UV through near-IR. This Great Observatory science will be selected through a competed GO program, and will account for about 50% of the HabEx primary mission. The preferred HabEx architecture is a 4m, monolithic, off-axis telescope that is diffraction-limited at 0.4 microns and is in an L2 orbit. HabEx employs two starlight suppression systems: a coronagraph and a starshade, each with their own dedicated instrument.', 'corpus_id': 210839032, 'score': 1}]"
175	{'doc_id': '225423556', 'title': 'The Language Interpretability Tool: Extensible, Interactive Visualizations and Analysis for NLP Models', 'abstract': 'We present the Language Interpretability Tool (LIT), an open-source platform for visualization and understanding of NLP models. We focus on core questions about model behavior: Why did my model make this prediction? When does it perform poorly? What happens under a controlled change in the input? LIT integrates local explanations, aggregate analysis, and counterfactual generation into a streamlined, browser-based interface to enable rapid exploration and error analysis. We include case studies for a diverse set of workflows, including exploring counterfactuals for sentiment analysis, measuring gender bias in coreference systems, and exploring local behavior in text generation. LIT supports a wide range of models—including classification, seq2seq, and structured prediction—and is highly extensible through a declarative, framework-agnostic API. LIT is under active development, with code and full documentation available at https://github.com/pair-code/lit.', 'corpus_id': 225423556}	20581	[{'doc_id': '237158776', 'title': 'Erase and Rewind: Manual Correction of NLP Output through a Web Interface', 'abstract': 'In this paper, we present Tintful, an NLP annotation software that can be used both to manually annotate texts and to fix mistakes in NLP pipelines, such as Stanford CoreNLP. Using a paradigm similar to wiki-like systems, a user who notices some wrong annotation can easily fix it and submit the resulting (and right) entry back to the tool developers. Moreover, Tintful can be used to easily annotate data from scratch. The input documents do not need to be in a particular format: starting from the plain text, the sentences are first annotated with CoreNLP, then the user can edit the annotations and submit everything back through a user-friendly interface. A video showing Tintful and its feature is available on YouTube.1', 'corpus_id': 237158776, 'score': 1}, {'doc_id': '237513572', 'title': 'Can Machines Read Coding Manuals Yet? - A Benchmark for Building Better Language Models for Code Understanding', 'abstract': 'Code understanding is an increasingly important application of Artificial Intelligence. A fundamental aspect of understanding code is understanding text about code, e.g., documentation and forum discussions. Pre-trained language models (e.g., BERT) are a popular approach for various NLP tasks, and there are now a variety of benchmarks, such as GLUE, to help improve the development of such models for natural language understanding. However, little is known about how well such models work on textual artifacts about code, and we are unaware of any systematic set of downstream tasks for such an evaluation. In this paper, we derive a set of benchmarks (BLANCA Benchmarks for LANguage models on Coding Artifacts) that assess code understanding based on tasks such as predicting the best answer to a question in a forum post, finding related forum posts, or predicting classes related in a hierarchy from class documentation. We evaluate the performance of current state-of-the-art language models on these tasks and show that there is a significant improvement on each task from fine tuning. We also show that multi-task training over BLANCA tasks helps build better language models for code understanding.', 'corpus_id': 237513572, 'score': 0}, {'doc_id': '237194902', 'title': 'AdapterHub Playground: Simple and Flexible Few-Shot Learning with Adapters', 'abstract': 'The open-access dissemination of pretrained language models through online repositories has led to a democratization of state-of-the-art natural language processing (NLP) research. This also allows people outside of NLP to use such models and adapt them to specific usecases. However, a certain amount of technical proficiency is still required which is an entry barrier for users who want to apply these models to a certain task but lack the necessary knowledge or resources. In this work, we aim to overcome this gap by providing a tool which allows researchers to leverage pretrained models without writing a single line of code. Built upon the parameter-efficient adapter modules for transfer learning, our AdapterHub Playground provides an intuitive interface, allowing the usage of adapters for prediction, training and analysis of textual data for a variety of NLP tasks. We present the tool’s architecture and demonstrate its advantages with prototypical use-cases, where we show that predictive performance can easily be increased in a fewshot learning scenario. Finally, we evaluate its usability in a user study. We provide the code and a live interface1.', 'corpus_id': 237194902, 'score': 1}, {'doc_id': '236954897', 'title': 'An Explainable Leaderboard for NLP', 'abstract': 'With the rapid development of NLP research, leaderboards have emerged as one tool to track the performance of various systems on various NLP tasks. They are effective in this goal to some extent, but generally present a rather simplistic one-dimensional view of the submitted systems, communicated only through holistic accuracy numbers. In this paper, we present a new conceptualization and implementation of NLP evaluation: the EXPLAINABOARD, which in addition to inheriting the functionality of the standard leaderboard, also allows researchers to (i) diagnose strengths and weaknesses of a single system (e.g. what is the best-performing system bad at?) (ii) interpret relationships between multiple systems. (e.g. where does system A outperform system B? What if we combine systems A, B and C?) and (iii) examine prediction results closely (e.g. what are common errors made by multiple systems or in what contexts do particular errors occur?). So far, EXPLAINABOARD covers more than 400 systems, 50 datasets, 40 languages, and 12 tasks.1 We not only released an online platform at the website2 but also make our evaluation tool an API with MIT Licence at Github3 and PyPi4 that allows users to conveniently assess their models offline. We additionally release all output files from systems that we have run or collected to motivate “output-driven” research in the future.', 'corpus_id': 236954897, 'score': 1}, {'doc_id': '237431215', 'title': 'Augmenting Question Answering with Natural Language Explanations', 'abstract': 'Towards building annotation-efficient question answering (QA) systems for real information seeking needs, we propose a framework that efficiently augments training data by leveraging natural language explanations to annotate unanswered questions. Explanations describing how an answer is arrived for one reference QA instance are first parsed into executable rules, and then applied to large-scale unanswered questions (e.g. questions raised in search engines, online forums). Answers with high confidence will be regarded as a training instance and potentially improve QA model performance, especially in low-resource setting. We highlight (1) the generalization ability in rule matching and (2) annotation efficiency in our proposed framework. Currently, we have collected 200+ explanations for SQuAD instances. The parser can successfully parse and validate 42% of them. We expect the parser to be further improved with the expansion of predicate dictionary. Meanwhile, we have several reasonable observations in our case study of rule hardmatching, such as the trade-off between precision and coverage and variations in acceptable answers.', 'corpus_id': 237431215, 'score': 0}, {'doc_id': '237365057', 'title': 'Thermostat: A Large Collection of NLP Model Explanations and Analysis Tools', 'abstract': 'In the language domain, as in other domains, neural explainability takes an ever more important role, with feature attribution methods on the forefront. Many such methods require considerable computational resources and expert knowledge about implementation details and parameter choices. To facilitate research, we present THERMOSTAT which consists of a large collection of model explanations and accompanying analysis tools. THERMOSTAT allows easy access to over 200k explanations for the decisions of prominent stateof-the-art models spanning across different NLP tasks, generated with multiple explainers. The dataset took over 10k GPU hours (> one year) to compile; compute time that the community now saves. The accompanying software tools allow to analyse explanations instance-wise but also accumulatively on corpus level. Users can investigate and compare models, datasets and explainers without the need to orchestrate implementation details. THERMOSTAT is fully open source, democratizes explainability research in the language domain, circumvents redundant computations and increases comparability and replicability.', 'corpus_id': 237365057, 'score': 1}, {'doc_id': '237439677', 'title': 'Towards Natural Language Interfaces for Data Visualization: A Survey', 'abstract': 'Utilizing Visualization-oriented Natural Language Interfaces (V-NLI) as a complementary input modality to direct manipulation for visual analytics can provide an engaging user experience. It enables users to focus on their tasks rather than worrying about operating the interface to visualization tools. In the past two decades, leveraging advanced natural language processing technologies, numerous V-NLI systems have been developed both within academic research and commercial software, especially in recent years. In this article, we conduct a comprehensive review of the existing V-NLIs. In order to classify each paper, we develop categorical dimensions based on a classic information visualization pipeline with the extension of a V-NLI layer. The following seven stages are used: query understanding, data transformation, visual mapping, view transformation, human interaction, context management, and presentation. Finally, we also shed light on several promising directions for future work in the community.', 'corpus_id': 237439677, 'score': 0}, {'doc_id': '237562834', 'title': 'CodeQA: A Question Answering Dataset for Source Code Comprehension', 'abstract': 'We propose CodeQA, a free-form question answering dataset for the purpose of source code comprehension: given a code snippet and a question, a textual answer is required to be generated. CodeQA contains a Java dataset with 119,778 question-answer pairs and a Python dataset with 70,085 question-answer pairs. To obtain natural and faithful questions and answers, we implement syntactic rules and semantic analysis to transform code comments into question-answer pairs. We present the construction process and conduct systematic analysis of our dataset. Experiment results achieved by several neural baselines on our dataset are shown and discussed. While research on question-answering and machine reading comprehension develops rapidly, few prior work has drawn attention to code question answering. This new dataset can serve as a useful research benchmark for source code comprehension.', 'corpus_id': 237562834, 'score': 0}, {'doc_id': '238198330', 'title': 'Micromodels for Efficient, Explainable, and Reusable Systems: A Case Study on Mental Health', 'abstract': 'Many statistical models have high accuracy on test benchmarks, but are not explainable, struggle in low-resource scenarios, cannot be reused for multiple tasks, and cannot easily integrate domain expertise. These factors limit their use, particularly in settings such as mental health, where it is difficult to annotate datasets and model outputs have significant impact. We introduce a micromodel architecture to address these challenges. Our approach allows researchers to build interpretable representations that embed domain knowledge and provide explanations throughout the model’s decision process. We demonstrate the idea on multiple mental health tasks: depression classification, PTSD classification, and suicidal risk assessment. Our systems consistently produce strong results, even in low-resource scenarios, and are more interpretable than alternative methods.', 'corpus_id': 238198330, 'score': 0}, {'doc_id': '237258250', 'title': 'Teach Me to Explain: A Review of Datasets for Explainable Natural Language Processing', 'abstract': 'Explainable Natural Language Processing (EXNLP) has increasingly focused on 1 collecting human-annotated textual explanations. These explanations are used 2 downstream in three ways: as data augmentation to improve performance on a 3 predictive task, as supervision to train models to produce explanations for their 4 predictions, and as a ground-truth to evaluate model-generated explanations. In 5 this review, we identify 61 datasets with three predominant classes of textual expla6 nations (highlights, free-text, and structured), organize the literature on annotating 7 each type, identify strengths and shortcomings of existing collection methodologies, 8 and give recommendations for collecting EXNLP datasets in the future. 9', 'corpus_id': 237258250, 'score': 1}]
176	"{'doc_id': '41792210', 'title': 'From Characters to Understanding Natural Language (C2NLU): Robust End-to-End Deep Learning for NLP (Dagstuhl Seminar 17042)', 'abstract': 'This report documents the program and the outcomes of Dagstuhl Seminar 17042 ""From Characters to Understanding Natural Language (C2NLU): Robust End-to-End Deep Learning for NLP"". The seminar brought together researchers from different fields, including natural language processing, computational linguistics, deep learning and general machine learning. 31 participants from 22 academic and industrial institutions discussed advantages and challenges of using characters, i.e., ""raw text"", as input for deep learning models instead of language-specific tokens. Eight talks provided overviews of different topics, approaches and challenges in current natural language processing research. In five working groups, the participants discussed current natural language processing/understanding topics in the context of character-based modeling, namely, morphology, machine translation, representation learning, end-to-end systems and dialogue. In most of the discussions, the need for a more detailed model analysis was pointed out. Especially for character-based input, it is important to analyze what a deep learning model is able to learn about language - about tokens, morphology or syntax in general. For an efficient and effective understanding of language, it might furthermore be beneficial to share representations learned from multiple objectives to enable the models to focus on their specific understanding task instead of needing to learn syntactic regularities of language first. Therefore, benefits and challenges of transfer learning were an important topic of the working groups as well as of the panel discussion and the final plenary discussion.', 'corpus_id': 41792210}"	4011	[{'doc_id': '227229069', 'title': 'Intrinsic Knowledge Evaluation on Chinese Language Models', 'abstract': 'Recent NLP tasks have benefited a lot from pre-trained language models (LM) since they are able to encode knowledge of various aspects. However, current LM evaluations focus on downstream performance, hence lack to comprehensively inspect in which aspect and to what extent have they encoded knowledge. This paper addresses both queries by proposing four tasks on syntactic, semantic, commonsense, and factual knowledge, aggregating to a total of $39,308$ questions covering both linguistic and world knowledge in Chinese. Throughout experiments, our probes and knowledge data prove to be a reliable benchmark for evaluating pre-trained Chinese LMs. Our work is publicly available at https://github.com/ZhiruoWang/ChnEval.', 'corpus_id': 227229069, 'score': 0}, {'doc_id': '226964491', 'title': 'Pre-training Text-to-Text Transformers for Concept-centric Common Sense', 'abstract': 'Pre-trained language models (PTLM) have achieved impressive results in a range of natural language understanding (NLU) and generation (NLG) tasks. However, current pre-training objectives such as masked token prediction (for BERT-style PTLMs) and masked span infilling (for T5-style PTLMs) do not explicitly model the relational commonsense knowledge about everyday concepts, which is crucial to many downstream tasks that need common sense to understand or generate. To augment PTLMs with concept-centric commonsense knowledge, in this paper, we propose both generative and contrastive objectives for learning common sense from the text, and use them as intermediate self-supervised learning tasks for incrementally pre-training PTLMs (before task-specific fine-tuning on downstream datasets). Furthermore, we develop a joint pre-training framework to unify generative and contrastive objectives so that they can mutually reinforce each other. Extensive experimental results show that our method, concept-aware language model (CALM), can pack more commonsense knowledge into the parameters of a pre-trained text-to-text transformer without relying on external knowledge graphs, yielding better performance on both NLU and NLG tasks. We show that while only incrementally pre-trained on a relatively small corpus for a few steps, CALM outperforms baseline methods by a consistent margin and even comparable with some larger PTLMs, which suggests that CALM can serve as a general, plug-and-play method for improving the commonsense reasoning ability of a PTLM.', 'corpus_id': 226964491, 'score': 1}, {'doc_id': '28517752', 'title': 'A Data-Driven Approach to Infer Knowledge Base Representation for Natural Language Relations', 'abstract': 'This paper studies the problem of discovering the structured knowledge representation of binary natural language relations. The representation, known as the schema, generalizes the traditional path of predicates to support more complex semantics. We present a search algorithm to generate schemas over a knowledge base, and propose a data-driven learning approach to discover the most suitable representations to one relation. Evaluation results show that inferred schemas are able to represent precise semantics, and can be used to enrich manually crafted knowledge bases. 1', 'corpus_id': 28517752, 'score': 1}, {'doc_id': '227228154', 'title': 'Language-Driven Region Pointer Advancement for Controllable Image Captioning', 'abstract': 'Controllable Image Captioning is a recent sub-field in the multi-modal task of Image Captioning wherein constraints are placed on which regions in an image should be described in the generated natural language caption. This puts a stronger focus on producing more detailed descriptions, and opens the door for more end-user control over results. A vital component of the Controllable Image Captioning architecture is the mechanism that decides the timing of attending to each region through the advancement of a region pointer. In this paper, we propose a novel method for predicting the timing of region pointer advancement by treating the advancement step as a natural part of the language structure via a NEXT-token, motivated by a strong correlation to the sentence structure in the training data. We find that our timing agrees with the ground-truth timing in the Flickr30k Entities test data with a precision of 86.55% and a recall of 97.92%. Our model implementing this technique improves the state-of-the-art on standard captioning metrics while additionally demonstrating a considerably larger effective vocabulary size.', 'corpus_id': 227228154, 'score': 0}, {'doc_id': '226222033', 'title': 'SLM: Learning a Discourse Language Representation with Sentence Unshuffling', 'abstract': 'We introduce Sentence-level Language Modeling, a new pre-training objective for learning a discourse language representation in a fully self-supervised manner. Recent pre-training methods in NLP focus on learning either bottom or top-level language representations: contextualized word representations derived from language model objectives at one extreme and a whole sequence representation learned by order classification of two given textual segments at the other. However, these models are not directly encouraged to capture representations of intermediate-size structures that exist in natural languages such as sentences and the relationships among them. To that end, we propose a new approach to encourage learning of a contextualized sentence-level representation by shuffling the sequence of input sentences and training a hierarchical transformer model to reconstruct the original ordering. Through experiments on downstream tasks such as GLUE, SQuAD, and DiscoEval, we show that this feature of our model improves the performance of the original BERT by large margins.', 'corpus_id': 226222033, 'score': 0}, {'doc_id': '212725539', 'title': 'LSCP: Enhanced Large Scale Colloquial Persian Language Understanding', 'abstract': 'Language recognition has been significantly advanced in recent years by means of modern machine learning methods such as deep learning and benchmarks with rich annotations. However, research is still limited in low-resource formal languages. This consists of a significant gap in describing the colloquial language especially for low-resourced ones such as Persian. In order to target this gap for low resource languages, we propose a “Large Scale Colloquial Persian Dataset” (LSCP). LSCP is hierarchically organized in a semantic taxonomy that focuses on multi-task informal Persian language understanding as a comprehensive problem. This encompasses the recognition of multiple semantic aspects in the human-level sentences, which naturally captures from the real-world sentences. We believe that further investigations and processing, as well as the application of novel algorithms and methods, can strengthen enriching computerized understanding and processing of low resource languages. The proposed corpus consists of 120M sentences resulted from 27M tweets annotated with parsing tree, part-of-speech tags, sentiment polarity and translation in five different languages.', 'corpus_id': 212725539, 'score': 0}, {'doc_id': '212628456', 'title': 'Parsing Thai Social Data: A New Challenge for Thai NLP', 'abstract': 'Dependency parsing (DP) is a task that analyzes text for syntactic structure and relationship between words. DP is widely used to improve natural language processing (NLP) applications in many languages such as English. Previous works on DP are generally applicable to formally written languages. However, they do not apply to informal languages such as the ones used in social networks. Therefore, DP has to be researched and explored with such social network data. In this paper, we explore and identify a DP model that is suitable for Thai social network data. After that, we will identify the appropriate linguistic unit as an input. The result showed that, the transition based model called, improve Elkared dependency parser outperform the others at UAS of 81.42%.', 'corpus_id': 212628456, 'score': 0}, {'doc_id': '216869018', 'title': 'Natural Language Premise Selection: Finding Supporting Statements for Mathematical Text', 'abstract': 'Mathematical text is written using a combination of words and mathematical expressions. This combination, along with a specific way of structuring sentences makes it challenging for state-of-art NLP tools to understand and reason on top of mathematical discourse. In this work, we propose a new NLP task, the natural premise selection, which is used to retrieve supporting definitions and supporting propositions that are useful for generating an informal mathematical proof for a particular statement. We also make available a dataset, NL-PS, which can be used to evaluate different approaches for the natural premise selection task. Using different baselines, we demonstrate the underlying interpretation challenges associated with the task.', 'corpus_id': 216869018, 'score': 1}, {'doc_id': '211818180', 'title': 'Natural Language Processing Advancements By Deep Learning: A Survey', 'abstract': 'Natural Language Processing (NLP) helps empower intelligent machines by enhancing a better understanding of the human language for linguistic-based human-computer communication. Recent developments in computational power and the advent of large amounts of linguistic data have heightened the need and demand for automating semantic analysis using data-driven approaches. The utilization of data-driven strategies is pervasive now due to the significant improvements demonstrated through the usage of deep learning methods in areas such as Computer Vision, Automatic Speech Recognition, and in particular, NLP. This survey categorizes and addresses the different aspects and applications of NLP that have benefited from deep learning. It covers core NLP tasks and applications and describes how deep learning methods and models advance these areas. We further analyze and compare different approaches and state-of-the-art models.', 'corpus_id': 211818180, 'score': 1}, {'doc_id': '227238703', 'title': 'Meta-Embeddings for Natural Language Inference and Semantic Similarity tasks', 'abstract': 'Word Representations form the core component for almost all advanced Natural Language Processing (NLP) applications such as text mining, question-answering, and text summarization, etc. Over the last two decades, immense research is conducted to come up with one single model to solve all major NLP tasks. The major problem currently is that there are a plethora of choices for different NLP tasks. Thus for NLP practitioners, the task of choosing the right model to be used itself becomes a challenge. Thus combining multiple pre-trained word embeddings and forming meta embeddings has become a viable approach to improve tackle NLP tasks. Meta embedding learning is a process of producing a single word embedding from a given set of pre-trained input word embeddings. In this paper, we propose to use Meta Embedding derived from few State-of-the-Art (SOTA) models to efficiently tackle mainstream NLP tasks like classification, semantic relatedness, and text similarity. We have compared both ensemble and dynamic variants to identify an efficient approach. The results obtained show that even the best State-of-the-Art models can be bettered. Thus showing us that meta-embeddings can be used for several NLP tasks by harnessing the power of several individual representations.', 'corpus_id': 227238703, 'score': 0}]
177	{'doc_id': '122512497', 'title': 'A Nambu representation of incompressible hydrodynamics using helicity and enstrophy', 'abstract': 'Nambu mechanics generalizes discrete classical Hamiltonian dynamics. Using the Euler equations for a rotating rigid body, the connection between this representation and noncanonical Hamiltonian mechanics is shown. Nambu mechanics is extended to incompressible ideal hydrodynamical fields using energy and helicity in 3D (enstrophy in 2D). The Hamiltonian and the Casimir invariants of the non-canonical Hamiltonian theory determine the dynamics in a non-singular trilinear bracket.', 'corpus_id': 122512497}	4134	"[{'doc_id': '220403290', 'title': 'Derivation of a Relativistic Boltzmann Distribution', 'abstract': ""A framework for relativistic thermodynamics and statistical physics is built by first exploiting the symmetries between energy and momentum in the derivation of the Boltzmann distribution, then using Einstein's energy-momentum relationship to derive a PDE for the partition function. It is shown that the extended Boltzmann distribution implies the existence of an inverse four-temperature, while the form of the partition function PDE implies the existence of a quantizable field theory of classical statistics, with hints of an associated gravity like gauge theory. An adaptation of the framework is then used to derive a thermodynamic certainty relationship."", 'corpus_id': 220403290, 'score': 0}, {'doc_id': '218674151', 'title': 'Relativistic kinetic theory of classical systems of charged particles: towards the microscopic foundation of thermodynamics and kinetics', 'abstract': 'In the complete system of equations of evolution of the classical system of charges and the electromagnetic field generated by them, the field variables are excluded. An exact closed relativistic non-Hamiltonian system of nonlocal kinetic equations, that describes the evolution of a system of charges in terms of their microscopic distribution functions, is obtained . The solutions of this system of equations are non-invariant with respect to time reversal, and also have the property of hereditarity.', 'corpus_id': 218674151, 'score': 0}, {'doc_id': '9790213', 'title': 'A scaleable technique for best-match retrieval of sequential information using metrics-guided search', 'abstract': ""A new technique is described for retrieving infor mation by finding the best match or matches between a textual 'query' and a textual database. The technique uses principles of beam search with a measure of probability to guide the search and prune the search tree. Unlike many methods for comparing strings, the method gives a set of alternative matches, graded by the 'quality' of the matching achieved."", 'corpus_id': 9790213, 'score': 0}, {'doc_id': '220793790', 'title': 'A Hamiltonian Interacting Particle System for Compressible Flow', 'abstract': 'The decomposition of the energy of a compressible fluid parcel into slow (deterministic) and fast (stochastic) components is interpreted as a stochastic Hamiltonian interacting particle system (HIPS). It is shown that the McKean-Vlasov equation associated to the mean field limit yields the barotropic Navier-Stokes equation with density dependent viscosity. Capillary forces can also be treated by this approach. Due to the Hamiltonian structure the mean field system satisfies a Kelvin circulation theorem along stochastic Lagrangian paths.', 'corpus_id': 220793790, 'score': 0}, {'doc_id': '221534474', 'title': 'Quantifying uncertainty in spatio-temporal changes of upper-ocean heat content estimates: an internationally coordinated comparison', 'abstract': 'The Earth system is accumulating energy due to human-induced activities. More than 90 percent of this energy has been stored in the ocean as heat since 1970, with about 64 percent of that in the upper 700 m. Differences in upper ocean heat content anomaly (OHCA) estimates, however, exist. Here, we evaluate spread in upper OHCA estimates arising from choices in instrumental bias corrections and mapping methods, in addition to the effect of using a common ocean mask. The same dataset was mapped by six research groups for 1970 to 2008, with six instrumental bias corrections applied to expendable bathythermograph (XBT) data. We find that use of a common ocean mask may impact estimation of global OHCA by +- 13 percent. Uncertainty due to mapping method dominates over XBT bias correction at a global scale and is largest in the Indian Ocean and in the eddy-rich and frontal regions of all basins. Uncertainty due to XBT bias correction is largest in the Pacific Ocean within 30N to 30S. In both mapping and XBT cases, spread is higher since the 1990s. Important differences in spatial trends among mapping methods are found in the well-observed Northwest Atlantic and the poorly-observed Southern Ocean. Although our results cannot identify the best mapping or bias correction schemes, they identify where and when greater uncertainties exist, and so where further refinements may yield the largest improvements. Our results highlight the need for a future international coordination to evaluate performance of existing mapping methods.', 'corpus_id': 221534474, 'score': 0}, {'doc_id': '120282197', 'title': 'Sensitivity Analysis of Quality Assurance Using the Spatial Regression Approach—A Case Study of the Maximum/Minimum Air Temperature', 'abstract': 'Abstract Both the spatial regression test (SRT) and inverse distance weighting (IDW) methods have been applied to provide estimates for the maximum air temperature (Tmax) and the minimum air temperature (Tmin) in the Applied Climate Information System (ACIS). This is critical to the processes of estimating missing data and identifying suspect data and is undertaken here to ensure quality data in ACIS. The SRT method was previously found to be superior to the IDW method; however, the sensitivity of the performance of both methods to input parameters has not been evaluated. A set of analyses is presented for both methods whereby the sensitivity to the radius of inclusion, the regression time window, the regression time offset, and the number of stations used to make the estimates are examined. Comparisons were also conducted between the SRT and the IDW methods. The performance of the SRT method stabilized when 10 or more stations were applied in the estimates. The optimal number of stations for the IDW meth...', 'corpus_id': 120282197, 'score': 1}, {'doc_id': '122174376', 'title': 'Classical Dynamics: A Contemporary Approach', 'abstract': '1. Fundamentals of mechanics 2. Lagrangian formulation of mechanics 3. Topics in Lagrangian dynamics 4. Scattering and linear oscillations 5. Hamiltonian formulation of mechanics 6. Topics in Hamiltonian dynamics 7. Nonlinear dynamics 8. Virigid bodies 9. Continuum dynamics.', 'corpus_id': 122174376, 'score': 1}, {'doc_id': '124300702', 'title': 'Performance of Quality Assurance Procedures on Daily Precipitation', 'abstract': 'Abstract The search for precipitation quality control (QC) methods has proven difficult. The high spatial and temporal variability associated with precipitation data causes high uncertainty and edge creep when regression-based approaches are applied. Precipitation frequency distributions are generally skewed rather than normally distributed. The commonly assumed normal distribution in QC methods is not a good representation of the actual distribution of precipitation and is inefficient in identifying the outliers. This paper first explores the use of a single gamma distribution, fit to all precipitation data, in a quality assurance test. A second test, the multiple intervals gamma distribution (MIGD) method, is introduced. It assumes that meteorological conditions that produce a certain range in average precipitation at surrounding stations will produce a predictable range at the target station. The MIGD bins the average of precipitation at neighboring stations; then, for the events in a specific bin, an ...', 'corpus_id': 124300702, 'score': 1}, {'doc_id': '42330143', 'title': 'Nambu mechanics and its quantization.', 'abstract': 'The algebra of observables inherent in the Nambu formalism [Phys. Rev. D 7, 2405 (1973)] for a generalization of classical Hamiltonian dynamics is investigated. A consistency requirement of time evolution of the Nambu bracket leads to a five-point identity. Two types of algebras are possible at the classical level. Their composition properties under a tensor product are considered and the physical implications are analyzed. A quantum generalization of these algebras is shown to be impossible.', 'corpus_id': 42330143, 'score': 1}, {'doc_id': '118946159', 'title': 'Mathematical and Physical Ideas for Climate Science', 'abstract': ""The climate is a forced and dissipative nonlinear system featuring nontrivial dynamics on a vast range of spatial and temporal scales. The understanding of the climate's structural and multiscale properties is crucial for the provision of a unifying picture of its dynamics and for the implementation of accurate and efficient numerical models. We present some recent developments at the intersection between climate science, mathematics, and physics, which may prove fruitful in the direction of constructing a more comprehensive account of climate dynamics. We describe the Nambu formulation of fluid dynamics and the potential of such a theory for constructing sophisticated numerical models of geophysical fluids. Then, we focus on the statistical mechanics of quasi-equilibrium flows in a rotating environment, which seems crucial for constructing a robust theory of geophysical turbulence. We then discuss ideas and methods suited for approaching directly the nonequilibrium nature of the climate system. First, we describe some recent findings on the thermodynamics of climate, characterize its energy and entropy budgets, and discuss related methods for intercomparing climate models and for studying tipping points. These ideas can also create a common ground between geophysics and astrophysics by suggesting general tools for studying exoplanetary atmospheres. We conclude by focusing on nonequilibrium statistical mechanics, which allows for a unified framing of problems as different as the climate response to forcings, the effect of altering the boundary conditions or the coupling between geophysical flows, and the derivation of parametrizations for numerical models."", 'corpus_id': 118946159, 'score': 1}]"
178	{'doc_id': '152995588', 'title': 'Economic Momentum and Currency Returns', 'abstract': 'Past trends in fundamentals linked to economic activity and inflation predict currency returns. We find that a trading strategy that goes long currencies with strong economic momentum and short currencies with weak economic momentum exhibits an annualized Sharpe ratio of 0.70 and yields a significant alpha when controlling for standard carry, momentum, and value strategies. The economic momentum strategy subsumes the alpha of carry trades, suggesting that differences in past economic trends capture cross-country differences in carry.', 'corpus_id': 152995588}	14959	[{'doc_id': '229938319', 'title': 'CLO Performance', 'abstract': 'We study the performance of collateralized loan obligations (CLOs) to understand the market imperfections giving rise to these vehicles and their corresponding economic costs. CLO equity tranches earn positive abnormal returns from the risk-adjusted price differential between leveraged loans and CLO debt tranches. Debt tranches offer higher returns than similarly rated corporate bonds, making them attractive to banks and insurers that face risk-based capital requirements. Temporal variation in equity performance highlights the resilience of CLOs to market volatility due to their closed-end structure, long-term funding, and embedded options to reinvest principal proceeds.', 'corpus_id': 229938319, 'score': 0}, {'doc_id': '231699106', 'title': 'Uncertainty Network Risk and Currency Returns', 'abstract': 'We examine the pricing of a horizon specific uncertainty network risk, extracted from option implied variances on exchange rates, in the cross-section of currency returns. Buying currencies that are receivers and selling currencies that are transmitters of short-term shocks exhibits a high Sharpe ratio and yields a significant alpha when controlling for standard dollar, carry trade, volatility, variance risk premium and momentum strategies. This profitability stems primarily from the causal nature of shock propagation and not from contemporaneous dynamics. Shock propagation at longer horizons is priced less, indicating a downward-sloping term structure of uncertainty network risk in currency markets.', 'corpus_id': 231699106, 'score': 1}, {'doc_id': '230102613', 'title': 'COVID-19 and the liquidity crisis of non-banks: lessons for the future', 'abstract': 'At the same time, the crisis has been a stark reminder that there are still considerable vulnerabilities in the financial sector. In particular, there has been a divergence between the comparatively lean regulation of the non-bank financial sector and its increasing role in financial intermediation across the globe. This divergence has measurably augmented the risks of perilous macro-financial feedback loops, which may also affect the conduct of monetary policy.', 'corpus_id': 230102613, 'score': 0}, {'doc_id': '231839694', 'title': 'Liquidity Stress Testing using Optimal Portfolio Liquidation', 'abstract': 'We build an optimal portfolio liquidation model for OTC markets, aiming at minimizing the trading costs via the choice of the liquidation time. We work in the Locally Linear Order Book framework of [12] to obtain the market impact as a function of the traded volume. We find that the optimal terminal time for a linear execution of a small order is proportional to the square root of the ratio between the amount being bought or sold and the average daily volume. Numerical experiments on real market data illustrate the method on a portfolio of corporate bonds.', 'corpus_id': 231839694, 'score': 0}, {'doc_id': '231779615', 'title': 'Changes in monetary policy operating procedures over the last decade: insights from a new database', 'abstract': 'We introduce a new interactive database that allows users to easily retrieve and customise detailed information on central banks’ monetary policy operating procedures (MPOPs). These procedures govern the day-to-day implementation of monetary policy in markets. After a highlevel conceptual overview of how MPOPs have evolved over the past decade, we showcase common trends and selected cross-country differences. We discuss, in particular, how the persistent environment of excess liquidity and the effective interest rate lower bound shaped MPOPs in the aftermath of the Great Financial Crisis of 2007–09.', 'corpus_id': 231779615, 'score': 1}, {'doc_id': '210054813', 'title': 'Prospect Theory and Currency Returns ∗', 'abstract': 'We empirically investigate the role of prospect theory in the FX market. Using the historical distribution of exchange rate changes, we construct a currency-level measure of prospect theory value and show that it forecasts future currency excess returns. High prospect theory value currencies significantly underperform low prospect theory value currencies. The predictability is higher during periods of excessive speculative demand of irrational traders and when arbitrage is limited. These findings are consistent with the hypothesis that investors mentally represent the portfolio of currencies by their historical distributions or charts and evaluate this distribution in the way described by prospect theory. JEL classification: F31, G12, G15, G40', 'corpus_id': 210054813, 'score': 1}, {'doc_id': '230523654', 'title': 'Optimal Hedging with Margin Constraints and Default Aversion and its Application to Bitcoin Perpetual Futures', 'abstract': 'We consider a futures hedging problem subject to a budget constraint that limits the ability of a hedger with default aversion to meet margin requirements. We derive a semi-closed form for an optimal hedging strategy with dual objectives – to minimize both the variance of the hedged portfolio and the probability of forced liquidations due to margin calls. An empirical analysis of bitcoin shows that the optimal strategy not only achieves superior hedge effectiveness, but also reduces the probability of forced liquidations to an acceptable level. We also compare how the hedger’s default aversion impacts the performance of optimal hedging based on minute-level data across major bitcoin spot and perpetual futures markets.', 'corpus_id': 230523654, 'score': 0}, {'doc_id': '219389258', 'title': 'Business Cycles and Currency Returns', 'abstract': 'We find a strong link between currency excess returns and the relative strength of the business cycle. Buying currencies of strong economies and selling currencies of weak economies generates high returns both in the cross section and time series of countries. These returns stem primarily from spot exchange rate predictability, are uncorrelated with common currency investment strategies, and cannot be understood using traditional currency risk factors in either unconditional or conditional asset pricing tests. We also show that a business cycle factor implied by our results is priced in a broad currency cross section.', 'corpus_id': 219389258, 'score': 1}, {'doc_id': '232054128', 'title': 'Does Regulatory Cooperation Help Integrate Equity Markets?', 'abstract': 'This study tests whether cooperation between securities regulators influences global market integration. I measure cooperation using arrangements between securities regulators that enable enhanced cross-border enforcement, better regulatory decisions, and reduced compliance obligations for cross-border activities. These arrangements—formed at different times for different country pairs—are associated with an 11% increase in cross-border investment. I find similar increases using other proxies for market integration. Cross-border investment and market integration thus depend, in part, on regulators working together to extend legal and institutional capacities across borders. This reframes our understanding of the role of institutions in global capital markets.', 'corpus_id': 232054128, 'score': 0}, {'doc_id': '231632378', 'title': 'Dynamic Industry Uncertainty Networks and the Business Cycle', 'abstract': 'We argue that uncertainty network structures extracted from option prices contain valuable information for business cycles. Classifying U.S. industries according to their contribution to system-related uncertainty across business cycles, we uncover an uncertainty hub role for the communications, industrials and information technology sectors, while shocks to materials, real estate and utilities do not create strong linkages in the network. Moreover, we find that this ex-ante network of uncertainty is a useful predictor of business cycles, especially when it is based on uncertainty hubs. The industry uncertainty network behaves counter-cyclically in that a tighter network tends to associate with future business cycle contractions.', 'corpus_id': 231632378, 'score': 1}]
179	{'doc_id': '198313274', 'title': 'A Brief Introduction to Natural Language Generation within Computational Creativity', 'abstract': 'This paper is an introduction to The 3rd Workshop on Computational Creativity in Language Generation (CC-NLG 2018), collocated with The 11th International Conference on Natural Language Generation (INLG 2018). The workshop acts as an overview of projects from the Computational Creativity field present within the general field of Natural Language Generation, and references further reviews within the area. Accepted papers to the workshop will use contemporary NLG methods or approach NLG problems from a creative perspective. The workshop intends to showcase creative applications as a worthwhile and meaningful pursuit to other NLG researchers.', 'corpus_id': 198313274}	3162	"[{'doc_id': '211066248', 'title': 'Introducing Aspects of Creativity in Automatic Poetry Generation', 'abstract': 'Poetry Generation involves teaching systems to automatically generate text that resembles poetic work. A deep learning system can learn to generate poetry on its own by training on a corpus of poems and modeling the particular style of language. In this paper, we propose taking an approach that fine-tunes GPT-2, a pre-trained language model, to our downstream task of poetry generation. We extend prior work on poetry generation by introducing creative elements. Specifically, we generate poems that express emotion and elicit the same in readers, and poems that use the language of dreams—called dream poetry. We are able to produce poems that correctly elicit the emotions of sadness and joy 87.5 and 85 percent, respectively, of the time. We produce dreamlike poetry by training on a corpus of texts that describe dreams. Poems from this model are shown to capture elements of dream poetry with scores of no less than 3.2 on the Likert scale. We perform crowdsourced human-evaluation for all our poems. We also make use of the Coh-Metrix tool, outlining metrics we use to gauge the quality of text generated.', 'corpus_id': 211066248, 'score': 1}, {'doc_id': '212718021', 'title': 'MixPoet: Diverse Poetry Generation via Learning Controllable Mixed Latent Space', 'abstract': 'As an essential step towards computer creativity, automatic poetry generation has gained increasing attention these years. Though recent neural models make prominent progress in some criteria of poetry quality, generated poems still suffer from the problem of poor diversity. Related literature researches show that different factors, such as life experience, historical background, etc., would influence composition styles of poets, which considerably contributes to the high diversity of human-authored poetry. Inspired by this, we propose MixPoet, a novel model that absorbs multiple factors to create various styles and promote diversity. Based on a semi-supervised variational autoencoder, our model disentangles the latent space into some subspaces, with each conditioned on one influence factor by adversarial training. In this way, the model learns a controllable latent variable to capture and mix generalized factor-related properties. Different factor mixtures lead to diverse styles and hence further differentiate generated poems from each other. Experiment results on Chinese poetry demonstrate that MixPoet improves both diversity and quality against three state-of-the-art models.', 'corpus_id': 212718021, 'score': 1}, {'doc_id': '215416194', 'title': 'Conditional Rap Lyrics Generation with Denoising Autoencoders', 'abstract': 'We develop a method for automatically synthesizing a rap verse given an input text written in another form, such as a summary of a news article. Our approach is to train a Transformer-based denoising autoencoder to reconstruct rap lyrics from content words. We study three different approaches for automatically stripping content words that convey the essential meaning of the lyrics. Moreover, we propose a BERT-based paraphrasing scheme for rhyme enhancement and show that it increases the average rhyme density of the lyrics by 10%. Experimental results on three diverse input domains -- existing rap lyrics, news, and movie plot summaries -- show that our method is capable of generating coherent and technically fluent rap verses that preserve the input content words. Human evaluation demonstrates that our approach gives a good trade-off between content preservation and style transfer compared to a strong information retrieval baseline.', 'corpus_id': 215416194, 'score': 1}, {'doc_id': '212734256', 'title': 'PO-EMO: Conceptualization, Annotation, and Modeling of Aesthetic Emotions in German and English Poetry', 'abstract': 'Most approaches to emotion analysis of social media, literature, news, and other domains focus exclusively on basic emotion categories as defined by Ekman or Plutchik. However, art (such as literature) enables engagement in a broader range of more complex and subtle emotions. These have been shown to also include mixed emotional responses. We consider emotions in poetry as they are elicited in the reader, rather than what is expressed in the text or intended by the author. Thus, we conceptualize a set of aesthetic emotions that are predictive of aesthetic appreciation in the reader, and allow the annotation of multiple labels per line to capture mixed emotions within their context. We evaluate this novel setting in an annotation experiment both with carefully trained experts and via crowdsourcing. Our annotation with experts leads to an acceptable agreement of k = .70, resulting in a consistent dataset for future large scale analysis. Finally, we conduct first emotion classification experiments based on BERT, showing that identifying aesthetic emotions is challenging in our data, with up to .52 F1-micro on the German subset. Data and resources are available at https://github.com/tnhaider/poetry-emotion.', 'corpus_id': 212734256, 'score': 1}, {'doc_id': '210932334', 'title': 'Bringing Stories Alive: Generating Interactive Fiction Worlds', 'abstract': 'World building forms the foundation of any task that requires narrative intelligence. In this work, we focus on procedurally generating interactive fiction worlds---text-based worlds that players ""see"" and ""talk to"" using natural language. Generating these worlds requires referencing everyday and thematic commonsense priors in addition to being semantically consistent, interesting, and coherent throughout. Using existing story plots as inspiration, we present a method that first extracts a partial knowledge graph encoding basic information regarding world structure such as locations and objects. This knowledge graph is then automatically completed utilizing thematic knowledge and used to guide a neural language generation model that fleshes out the rest of the world. We perform human participant-based evaluations, testing our neural model\'s ability to extract and fill-in a knowledge graph and to generate language conditioned on it against rule-based and human-made baselines. Our code is available at this https URL.', 'corpus_id': 210932334, 'score': 0}, {'doc_id': '214802375', 'title': 'PROTOTYPE-TO-STYLE: Dialogue Generation With Style-Aware Editing on Retrieval Memory', 'abstract': 'The ability of dialogue systems to express pre-specified style during conversations has a direct, positive impact on their usability and user satisfaction. While it has attracted much research interest, existing methods often generate stylistic responses at the cost of content quality. In this work, we introduce a prototype-to-style (PS) framework to tackle the challenge of stylistic dialogue generation. The proposed framework first exploits an Information Retrieval (IR) system and extracts a response prototype from the retrieved response. A stylistic response generator then takes the response prototype and the desired style as input to produce a high-quality and stylistic response. To effectively train the proposed model and imitate the real testing environment, we introduce a new style-aware learning objective and a denoising learning strategy. Results on three benchmark datasets (gender, emotion, and sentiment) from two languages demonstrate that the proposed approach significantly outperforms existing baselines both in terms of in-domain and cross-domain evaluations.', 'corpus_id': 214802375, 'score': 0}, {'doc_id': '212628796', 'title': 'EmpTransfo: A Multi-head Transformer Architecture for Creating Empathetic Dialog Systems', 'abstract': 'Understanding emotions and responding accordingly is one of the biggest challenges of dialog systems. This paper presents EmpTransfo, a multi-head Transformer architecture for creating an empathetic dialog system. EmpTransfo utilizes state-of-the-art pre-trained models (e.g., OpenAI-GPT) for language generation, though models with different sizes can be used. We show that utilizing the history of emotions and other metadata can improve the quality of generated conversations by the dialog system. Our experimental results using a challenging language corpus show that the proposed approach outperforms other models in terms of Hit@1 and PPL (Perplexity).', 'corpus_id': 212628796, 'score': 0}, {'doc_id': '211043879', 'title': 'Stimulating Creativity with FunLines: A Case Study of Humor Generation in Headlines', 'abstract': 'Building datasets of creative text, such as humor, is quite challenging. We introduce FunLines, a competitive game where players edit news headlines to make them funny, and where they rate the funniness of headlines edited by others. FunLines makes the humor generation process fun, interactive, collaborative, rewarding and educational, keeping players engaged and providing humor data at a very low cost compared to traditional crowdsourcing approaches. FunLines offers useful performance feedback, assisting players in getting better over time at generating and assessing humor, as our analysis shows. This helps to further increase the quality of the generated dataset. We show the effectiveness of this data by training humor classification models that outperform a previous benchmark, and we release this dataset to the public.', 'corpus_id': 211043879, 'score': 0}, {'doc_id': '215745354', 'title': 'You Impress Me: Dialogue Generation via Mutual Persona Perception', 'abstract': 'Despite the continuing efforts to improve the engagingness and consistency of chit-chat dialogue systems, the majority of current work simply focus on mimicking human-like responses, leaving understudied the aspects of modeling understanding between interlocutors. The research in cognitive science, instead, suggests that understanding is an essential signal for a high-quality chit-chat conversation. Motivated by this, we propose Pˆ2 Bot, a transmitter-receiver based framework with the aim of explicitly modeling understanding. Specifically, Pˆ2 Bot incorporates mutual persona perception to enhance the quality of personalized dialogue generation. Experiments on a large public dataset, Persona-Chat, demonstrate the effectiveness of our approach, with a considerable boost over the state-of-the-art baselines across both automatic metrics and human evaluations.', 'corpus_id': 215745354, 'score': 0}, {'doc_id': '214641015', 'title': 'Generating Major Types of Chinese Classical Poetry in a Uniformed Framework', 'abstract': 'Poetry generation is an interesting research topic in the field of text generation. As one of the most valuable literary and cultural heritages of China, Chinese classical poetry is very familiar and loved by Chinese people from generation to generation. It has many particular characteristics in its language structure, ranging from form, sound to meaning, thus is regarded as an ideal testing task for text generation. In this paper, we propose a GPT-2 based uniformed framework for generating major types of Chinese classical poems. We define a unified format for formulating all types of training samples by integrating detailed form information, then present a simple form- stressed weighting method in GPT-2 to strengthen the control to the form of the generated poems, with special emphasis on those forms with longer body length. Preliminary experimental results show this enhanced model can generate Chinese classical poems of major types with high quality in both form and content, validating the effectiveness of the proposed strategy. The model has been incorporated into Jiuge, the most influential Chinese classical poetry generation system developed by Tsinghua University.', 'corpus_id': 214641015, 'score': 1}]"
180	{'doc_id': '233261827', 'title': 'Mechanical characteristics and deformation control of surrounding rock in weakly cemented siltstone', 'abstract': 'The roof of coal seams are mostly soft rock with weak cementation. To further study the mechanical characteristics and deformation control scheme of the roadway with weakly cemented siltstone as roof, this paper took Linchang coal mine as the research background and adopted field investigation methods, laboratory experiments, and theoretical analysis. Using a scanning electron microscope, it is found that the weakly cemented siltstone is composed of coarse-grained minerals with a high degree of pore development. According to the analysis, the instability factors of weakly cemented siltstone roadway include the late diagenetic age of rock, the low mechanical strength of rock, and the change of surrounding rock properties by pouring water. The selection of grouting reinforcement materials was studied in detail. The test results show that cement-bonded specimens’ strength is lower than that of Marithan polyurethane cement specimens in general. The combined support scheme based on grouting reinforcement is put forward. Field monitoring data show that the designed support scheme can effectively control the surrounding rock deformation of weakly cemented siltstone roadway.', 'corpus_id': 233261827}	16574	[{'doc_id': '112575839', 'title': 'Prevention of Borehole Diameter Shrinkage in Small Diameter Core Drilling and Treatment', 'abstract': 'Borehole diameter shrinkage is frequently encountered in small diameter core drilling,which brings serious influence of construction progress,material waste,more construction cost and even downhole accident or abandoned.With several engineering cases,the paper analyzed the causes of borehole diameter shrinkage,introduced process of accident treatment and summed up the experience of penetrating diameter shrinking formation.', 'corpus_id': 112575839, 'score': 1}, {'doc_id': '221917049', 'title': 'Borehole diameter shrinkage rule considering rheological properties and its effect on gas extraction', 'abstract': 'To study the shrinkage rule of borehole diameter and its effect on gas extraction, a visco-elastoplastic model for boreholes considering strain softening and the dilatancy characteristic was established to obtain the expressions of the coal stress, variation in diameter, and pressure relief range. The stress distribution and pressure relief effect of the boreholes in soft and hard coal seams were comparatively analyzed. The shrinkage rule of the borehole diameter was studied. The reasons for the rapid reduction in the extraction concentration of the borehole in soft coal seams were described. A technology of improving the gas extraction effect in soft coal seams was developed. The research results showed that the radius of the plastic softening zone is 0.405 m for a borehole in a soft coal seam and 0.224 m for that in a hard coal seam. This indicates that the borehole in a soft coal seam has a better pressure relief effect. The boreholes in both hard and soft coal seams will incur a shrinkage phenomenon; however, the soft coal seam has low strength and a weak ability to resist damage, and thus the surrounding coal will have a more intense creep deformation, leading to an instability failure during a short period of time and thus, a blocking of the extraction channel, thereby causing a rapid reduction in the gas extraction concentration. The borehole in a hard coal seam also shows a shrinkage phenomenon, but remains in a stable state without a blockage; thus, high-concentration gas can be extracted from this borehole for a long period of time. The geo-stress and coal strength are the two main factors controlling the amplitude of borehole shrinkage. From an increase in stress, the borehole in a hard coal seam shows a more intense creep deformation in a deep mine, which may lead to blockage. The key to improving the gas extraction effect in soft coal seams is to maintain a smooth extraction channel. The full screen pipe is installed through the drill pipe to retain an extraction channel, leading to an average gas extraction increase from 0.043 m3/min to 0.12 m3/min, an increase of 2.77 times. These research results are consistent with actual production, and can provide theoretical guidance for determining the gas extraction parameters.', 'corpus_id': 221917049, 'score': 1}, {'doc_id': '233660138', 'title': 'An Innovative Elastoplastic Analysis for Soft Surrounding Rock considering Supporting Opportunity Based on Drucker-Prager Strength Criterion', 'abstract': 'In order to study the mechanism of excavation and supporting process of equivalent circular roadway, the model of soft roadway was established firstly. The elastoplastic solutions in excavation process were deduced based on Drucker-Prager strength criterion. Then, the elastoplastic solution under supporting condition was obtained based on homogenization method under the condition of rockbolts and liner supporting. Lastly, an example was analyzed to study the effect of different factors such as “space effect,” supporting opportunity, stresses, surrounding displacement, and the radius of plastic zone. Based on theoretical research case, the change rules of considering the “space effect” and the supporting opportunity when calculating the subarea of the roadway were discussed, the control of interval distance of rockbolts on the displacement of surrounding rock mainly reflecting in the plastic residual zone and the “space effect” in excavation, and the supporting time to control the displacement of surrounding rock not being ignored are revealed. The results can provide an important theoretical basis for the stability evaluation and quantitative support design of roadway surrounding rock. Therefore, the “space effect” and the supporting time to control the displacement and stresses of surrounding rock can not being ignored in underground engineering.', 'corpus_id': 233660138, 'score': 1}, {'doc_id': '55751706', 'title': 'Application of Multiphysics Coupling FEM on Open Wellbore Shrinkage and Casing Remaining Strength in an Incomplete Borehole in Deep Salt Formation', 'abstract': 'Drilling and completing wells in deep salt stratum are technically challenging and costing, as when serving in an incomplete borehole in deep salt formation, well casing runs a high risk of collapse. To quantitatively calculate casing remaining strength under this harsh condition, a three-dimensional mechanical model is developed; then a computational model coupled with interbed salt rock-defective cement-casing and HPHT (high pressure and high temperature) is established and analyzed using multiphysics coupling FEM (finite element method); furthermore, open wellbore shrinkage and casing remaining strength under varying differential conditions in deep salt formation are discussed. The result demonstrates that the most serious shrinkage occurs at the middle of salt rock, and the combination action of salt rock creep, cement defect, and HPHT substantially lessens casing remaining strength; meanwhile, cement defect level should be taken into consideration when designing casing strength in deep salt formation, and apart from the consideration of temperature on casing the effect of temperature on cement properties also cannot be ignored. This study not only provides a theoretical basis for revealing the failure mechanism of well casing in deep complicated salt formation, but also acts as a new perspective of novel engineering applications of the multiphysics coupling FEM.', 'corpus_id': 55751706, 'score': 1}, {'doc_id': '233988763', 'title': 'Effective extraction radius of gas drilling in coal seam', 'abstract': 'In order to predict effective extraction radius of drilling hole in gas control of coal mineꎬ taking No. 6 coal seam of Sijichun mine in Guizhou Province as an exampleꎬ a gas seepage model of surrounding units of boreholes was established. Thenꎬ gas pressure distribution cloud chart along radial direction at different drilling time was obtained through Comsol numerical simulation. Finallyꎬ effective extraction radius was determined based on theoretical derivation and numerical simulation data while considering critical gas pressure (0\U001001b0 5 MPa)ꎬ and verification was made with on ̄site test. The results show that relative error rate of theoretical and simulated effective extraction radius is less than 10%ꎬ verifying accuracy of both calculation results. Effective radius is linearly dependent on extraction time. Considering its complexity to measure effective radius on siteꎬ it can be obtained through theoretical and numerical simulation methods used in this paperꎬ which provides a theoretical basis for underground gas control.', 'corpus_id': 233988763, 'score': 0}, {'doc_id': '213368464', 'title': 'Influence of drilling fluid temperature on borehole shrinkage during drilling operation in cold regions', 'abstract': 'Abstract Frozen soil is a porous elastic-plastic-viscous material with temperature-sensitive properties. Borehole shrinkage occurs due to the plasticity and creep of frozen soil that ensues during drilling operations in cold regions. Based on the change of mechanical properties of frozen soil during the heat transfer process that occurs between the drilling fluid and the formation, a heat-fluid-solid coupling model is established in this paper to analyze borehole shrinkage behaviors in frozen soil. The results show that during drilling operations, the plastic and creep deformation of frozen soil will result in borehole shrinkage. When the borehole is drilled, a certain amount of plastic deformation will occur due to stress concentration. As drilling operations continue, both plastic and creep deformation gradually accumulate. The borehole shrinkage is dominated by the initial temperature of the formation, and the higher the formation temperature, the greater the shrinkage. At the same formation temperature, the initial borehole shrinkage decreases with increasing temperature of the drilling fluid, and the final shrinkage increases nonlinearly with increasing temperature. The drilling fluid temperature should be close to 0\xa0°C when the formation temperature is high to reduce the drilling time. When the formation temperature is low, cold drilling fluid should be used to maintain borehole stability. This study provides a theoretical basis for the selection of drilling fluid temperatures during drilling operations in cold regions.', 'corpus_id': 213368464, 'score': 1}, {'doc_id': '233674898', 'title': 'Permeability Enhancement and Gas Drainage Effect in Deep High Gassy Coal Seams via Long-Distance Pressure Relief Mining: A Case Study', 'abstract': 'Coal 3 in group A is employed as a protective layer to release long-distance coal 4 in group B in Paner colliery (approximately 80\u2009m vertical interval) as the mining depth extends downward, which is the first engineering test in the Huainan coal mining area. To evaluate the validity of the scheme, the permeability distribution, and evolution law, gas pressure distribution characteristics, swelling deformation, pressure relief range, and gas drainage volume of the protected coal seam are analyzed using a FLAC3D numerical simulation and field measurements. Therefore, different stress-permeability models are adopted for caved, fractured, and continuous deformation zones, and a double-yield model is applied in the goaf based on compaction theory to improve the accuracy of the numerical simulation. The results indicate that the extraction of coal 3 has a positive effect on permeability enhancement and pressure relief gas drainage. However, the dip angle of coal measurements causes asymmetric strata movement, which leads to the pressure relief and permeability enhancement area shifting to the downhill side, where the permeability enhancement effect of the downhill side is better than that of the uphill side. The permeability enhancement zone is an inverted trapezoid, but the effective pressure relief range is a positive trapezoid. The permeability of the protected coal seam in the pressure relief zone is significantly higher than that in the compressive failure zone. The permeability in the pressure relief zone will decrease again due to the recompaction of the coal seam with an advancement of the longwall face. Thus, pressure relief gas drainage is suggested during long-distance protective coal seam mining to eliminate gas hazards.', 'corpus_id': 233674898, 'score': 0}, {'doc_id': '233532779', 'title': 'Speculum Observation and Trajectory Measurement in Gas Extraction Drilling: A Case Study of Changling Coal Mine', 'abstract': 'Coal will still be China’s basic energy for quite a long time. With the increase of mining depth, gas content and pressure also increase. The problems of gas emission and overrun affect the safety and efficient production of coal resource to a certain extent. In this work, the field test of gas drainage borehole peeping and trajectory measurement in coal seam of Changling coal mine are carried out. These coal seams include C5b coal seam, upper adjacent C5a coal seam, C6a coal seams, C6c in lower adjacent strata, and C5b coal seam in high-level borehole. The view of gas drainage borehole peeping and trajectory measurement in the working seam, upper adjacent layer, lower adjacent layer, and high position are obtained. It is found that the hole collapses at the position of about 20\u2009m in both adjacent strata and high-level boreholes, and there are a lot of cracks in the high-level boreholes before 12\u2009m. The deviation distance of high-level borehole is large, and the actual vertical deviation of upper adjacent layer is small. Finally, the strategies to prevent the deviation of drilling construction are put forward. It includes four aspects: ensuring the reliability of drilling equipment, reasonably controlling the drilling length, standardizing the drilling, and reasonably selecting the drilling process parameters.', 'corpus_id': 233532779, 'score': 0}, {'doc_id': '233693548', 'title': 'Coupling Technology of Deep-Hole Presplitting Blasting and Hydraulic Fracturing Enhance Permeability Technology in Low-Permeability and Gas Outburst Coal Seam: A Case Study in the No. 8 Mine of Pingdingshan, China', 'abstract': 'The current study aims to analyze the principles of integrated technology of explosion to tackle the problems of coal seam high gas content and pressure, developed faults, complex structure, low coal seam permeability, and high outburst risk. Based on this, we found through numerical simulation that as the inclination of the coal seam increases, the risk of coal and gas outburst increases during the tunneling process. Therefore, it is necessary to take measures to reduce the risk of coal and gas outburst. We conducted field engineering experiments. Our results show that the synergistic antireflection technology of hydraulic fracturing and deep-hole presplitting blasting has a significant antireflection effect in low-permeability coal seams. After implementing this technology, the distribution of coal moisture content was relatively uniform and improved the influence range of direction and tendency. Following 52 days of extraction, the average extraction concentration was 2.9 times that of the coal seam gas extraction concentration under the original technology. The average scalar volume of single hole gas extraction was increased by 7.7 times. Through field tests, the purpose of pressure relief and permeability enhancement in low-permeability coal seams was achieved. Moreover, the effect of gas drainage and treatment in low-permeability coal seams was improved, and the applicability, effectiveness, and safety of underground hydraulic fracturing and antireflection technology in low-permeability coal seams were verified. The new technique is promising for preventing and controlling gas hazards in the future.', 'corpus_id': 233693548, 'score': 0}, {'doc_id': '233432978', 'title': 'Permeability Enhancement Properties of High-Pressure Abrasive Water Jet Flushing and Its Application in a Soft Coal Seam', 'abstract': 'High-pressure abrasive water jet flushing (HPAWJF) is an effective method used to improve coal seam permeability. In this study, based on the theories of gas flow and coal deformation, a coupled gas-rock model is established to investigate realistic failure processes by introducing equations for the evolution of mesoscopic element damage along with coal mass deformation. Numerical simulation of the failure and pressure relief processes is carried out under different coal seam permeability and flushing length conditions. Distributions of the seepage and gas pressure fields of the realistic failure process are analyzed. The effects of flushing permeability enhancement in a soft coal seam on the gas drainage from boreholes are revealed by conducting a field experiment. Conclusions can be extracted that the gas pressure of the slotted soft coal seam is reduced and that the gas drainage volume is three times higher than that of a conventional borehole. Field tests demonstrate that the gas drainage effect of the soft coal seam is significantly improved and that tunneling speed is nearly doubled. The results obtained from this study can provide guidance to gas drainage in soft coal seams regarding the theory and practice application of the HPAWJF method.', 'corpus_id': 233432978, 'score': 0}]
181	{'doc_id': '30940331', 'title': 'The role of the tumor-microenvironment in lung cancer-metastasis and its relationship to potential therapeutic targets.', 'abstract': 'Non-small cell lung cancer (NSCLC) accounts for >80% of lung cancer cases and currently has an overall five-year survival rate of only 15%. Patients presenting with advanced stage NSCLC die within 18-months of diagnosis. Metastatic spread accounts for >70% of these deaths. Thus elucidation of the mechanistic basis of NSCLC-metastasis has potential to impact on patient quality of life and survival. Research on NSCLC metastasis has recently expanded to include non-cancer cell components of tumors-the stromal cellular compartment and extra-cellular matrix components comprising the tumor-microenvironment. Metastasis (from initial primary tumor growth through angiogenesis, intravasation, survival in the bloodstream, extravasation and metastatic growth) is an inefficient process and few released cancer cells complete the entire process. Micro-environmental interactions assist each of these steps and discovery of the mechanisms by which tumor cells co-operate with the micro-environment are uncovering key molecules providing either biomarkers or potential drug targets. The major sites of NSCLC metastasis are brain, bone, adrenal gland and the liver. The mechanistic basis of this tissue-tropism is beginning to be elucidated offering the potential to target stromal components of these tissues thus targeting therapy to the tissues affected. This review covers the principal steps involved in tumor metastasis. The role of cell-cell interactions, ECM remodeling and autocrine/paracrine signaling interactions between tumor cells and the surrounding stroma is discussed. The mechanistic basis of lung cancer metastasis to specific organs is also described. The signaling mechanisms outlined have potential to act as future drug targets minimizing lung cancer metastatic spread and morbidity.', 'corpus_id': 30940331}	5530	"[{'doc_id': '218594389', 'title': 'Colorectal carcinoma-associated carbohydrate recognition and tumor-immunological regulation by C-type lectin DC-SIGN', 'abstract': 'DC-SIGN (DC-speciˆc intercellular adhesion molecule-3-grabbing nonintegrin), which is a type II transmembrane C-type lectin expressed on dendritic cells (DCs), recognizes cell-surface carbohydrates on invading pathogens such as bacteria and viruses and plays an important role in the uptake and presentation of these antigens. To date, little is known about the correlation of DCSIGN with cancer. Recently, we reported that DC-SIGN bound to colorectal carcinoma cells by recognizing colorectal tumor-associated Lewis (Le) glycan ligands. The interaction of DC-SIGN with colorectal cancer cells increased LPS-induced IL-6 and IL-10 secretion from monocyte-derived DCs (MoDCs), resulting in inhibition of the maturation of MoDCs and diŠerentiation of naä ƒve T cells into Th1 cells. Furthermore, DC-SIGN-expressing cells were shown to colocalize with colorectal cancer-associated Le glycan ligands in primary colon cancer tissues. These results indicate that DC-SIGN-mediated dysfunction of DCs in tumor immunity is one of the critical mechanisms for escaping immune surveillance. A. Introduction DCs play professional roles in recognizing foreign antigens and inducing a T-cell dependent immune response during the shift from innate immunity to acquired immunity (1–3). DCs are the main component of the immune system for an eŠective response, therefore a defect of DC function may trigger breakdown of the whole immune system (4). It has been reported recently that pathogens such as viruses and tumor cells target DCs to escape from the host immune sytstem (5). Among a variety of receptors expressed on DCs, the C-type lectin receptor (CLR) family, like the Toll-like receptor (TLR) family, comprises a series of pathogen', 'corpus_id': 218594389, 'score': 0}, {'doc_id': '219531019', 'title': 'Histopathological imaging features- versus molecular measurements-based cancer prognosis modeling', 'abstract': 'For lung and many other cancers, prognosis is essentially important, and extensive modeling has been carried out. Cancer is a genetic disease. In the past 2 decades, diverse molecular data (such as gene expressions and DNA mutations) have been analyzed in prognosis modeling. More recently, histopathological imaging data, which is a “byproduct” of biopsy, has been suggested as informative for prognosis. In this article, with the TCGA LUAD and LUSC data, we examine and directly compare modeling lung cancer overall survival using gene expressions versus histopathological imaging features. High-dimensional penalization methods are adopted for estimation and variable selection. Our findings include that gene expressions have slightly better prognostic performance, and that most of the gene expressions are weakly correlated imaging features. This study may provide additional insight into utilizing the two types of important data in cancer prognosis modeling and into lung cancer overall survival.', 'corpus_id': 219531019, 'score': 1}, {'doc_id': '216145510', 'title': 'Identification of Human Immune Cell Subtypes Most Vulnerable to IL-1β-induced Inflammatory Signaling Using Mass Cytometry', 'abstract': 'IL-1β has emerged as a key mediator of the cytokine storm linked to high morbidity and mortality from COVID-19 and blockade of the IL-1 receptor (IL-1R) with Anakinra has entered clinical trials in COVID-19 subjects. Yet, knowledge of the specific immune cell subsets targeted by IL-1β and IL-1β-induced signaling pathways in humans is limited. Utilizing mass cytometry (CyTOF) of human peripheral blood mononuclear cells, we identified effector memory CD4 T cells and CD4−CD8low/-CD161+ T cells as the circulating immune subtypes with the greatest expression of p-NF-κB in response to IL-1β stimulation. Notably, CCR6 distinctly identified T cells most responsive to IL-1β. Other subsets including CD11c myeloid dendritic cells (mDCs), classical monocytes (CM), two subsets of natural killer cells (CD16−CD56brightCD161− and CD16−CD56dimCD161+) and a population of lineage−(Lin-) cells expressing CD161 and CD25 also showed IL-1β-induced expression of p-NF-kB. The IL-1R antagonist, Anakinra significantly inhibited IL-1β-induced p-NF-kB in the CCR6+ T cells and CD11c mDCs with a trending inhibition in CD14 monocytes and Lin−CD161+CD25+ cells. IL-1β also induced a rapid but much less robust increase in p-p38 expression as compared to p-NF-kB in the majority of these same immune cell subsets. Prolonged IL-1β stimulation greatly increased p-STAT3 and to a much lesser extent p-STAT1 and p-STAT5 in T cell subsets, monocytes, DCs and the Lin−CD161+CD25+ cells suggesting IL-1β-induced production of downstream STAT-activating cytokines, consistent with its role in cytokine storm. Interindividual heterogeneity and inhibition of this activation by Anakinra raises the intriguing possibility that assays to measure IL-1β-induced p-NF-kB in CCR6+ T cell subtypes could identify those at higher risk of cytokine storm and those most likely to benefit from Anakinra therapy.', 'corpus_id': 216145510, 'score': 1}, {'doc_id': '219538539', 'title': 'COVID-19 and androgen targeted therapy for prostate cancer patients.', 'abstract': 'The current pandemic (COVID-19) caused by severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) is a global health challenge with active development of antiviral drugs and vaccines seeking to reduce its significant disease burden. Early reports have confirmed that transmembrane serine protease 2 (TMPRSS2) and angiotensin converting enzyme 2 (ACE2) are critical targets of SARS-CoV-2 that facilitate viral entry into host cells. TMPRSS2 and ACE2 are expressed in multiple human tissues beyond the lung including the testes where predisposition to SARS-CoV-2 infection may exist. TMPRSS2 is an androgen responsive gene and its fusion represents one of the most frequent alterations in prostate cancer. Androgen suppression by androgen deprivation therapy and androgen receptor signaling inhibitors form the foundation of prostate cancer treatment. In this review, we highlight the growing evidence in support of androgen regulation of TMPRSS2 and ACE2 and the potential clinical implications of using androgen suppression to downregulate TMPRSS2 to target SARS-CoV-2. We also discuss the future directions and controversies that need to be addressed in order to establish the viability of targeting TMPRSS2 and/or ACE2 through androgen signaling regulation for COVID-19 treatment, particularly its relevance in the context of prostate cancer management.', 'corpus_id': 219538539, 'score': 0}, {'doc_id': '73196668', 'title': 'Inflammasome pathway activation in patients with non-small cell lung cancer (NSCLC): A bronchoalveolar lavage fluid study', 'abstract': 'Introduction: Inflammasome activation is mediated by NLR proteins. Among NLRs, NLRP3-inflammasome is a multiprotein molecular platform activated by infection or host-derived danger signals that trigger an innate immune response via maturation of pro-inflammatory cytokines such as interleukin-1 β (IL-1 β), in a caspase-1-dependent way. Aim of the study: Our aim was to investigate the NLRP3 pathway activation in human BALF and peripheral blood samples from NSCLC patients and healthy controls. Methods: We prospectively studied BALF and peripheral blood leukocyte samples from 19 NSCLC and 12 healthy controls. All samples were treated with LPS (250 pg/ml, 2hrs) to induce TLR4 stimulation, followed by NLRP3-inflammasome activation with pulse ATP (5mM, 20min). Secreted TNFa, IL-1β and IL-6 were measured using commercial ELISA kits. Results: The main result is that LPS treatment resulted in increased levels of IL-1b production in NSCLC patients. Moreover, LPS treatment, pulse ATP and inhibition of the NLRP3-inflammasome activation using caspase-1 inhibitor resulted in increased IL-6 levels in NSCLC in comparison with controls. On the contrary, the same treatments resulted in a significant decrease in TNF-a secretion in NSCLC in comparison with controls. Conclusion: NLRP3-inflammasome is activated in NSCLC patients in the presence of infectious stimuli thus exhibiting a possible role as a proinflammatory ""danger"" receptor.', 'corpus_id': 73196668, 'score': 1}, {'doc_id': '215741034', 'title': 'Blocking drug-induced autophagy with chloroquine in HCT-116 colon cancer cells enhances DC maturation and T cell responses induced by tumor cell lysate', 'abstract': '\n Abstract\n \n Autophagy is an important mechanism for tumor escape, allowing tumor cells to recover from the damage induced by chemotherapy, radiation therapy, and immunotherapy and contributing to the development of resistance. The pharmacological inhibition of autophagy contributes to increase the efficacy of antineoplastic agents. Exposing tumor cells to low concentrations of select autophagy-inducing antineoplastic agents increases their immunogenicity and enhances their ability to stimulate dendritic cell (DC) maturation. We tested whether the application of an autophagy-inhibiting agent, chloroquine (CQ), in combination with low concentrations of 5-fluorouracil (5-FU) increases the ability of tumor cells to induce DC maturation. DCs sensitized with the lysate of HCT-116 cells previously exposed to such a combination enhanced the DC maturation/activation ability. These matured DCs also increased the allogeneic responsiveness of both CD4+ and CD8+ T cells, which showed a greater proliferative response than those from DCs sensitized with control lysates. The T cells expanded in such cocultures were CD69+ and PD-1- and produced higher levels of IFN-γ and lower levels of IL-10, consistent with the preferential activation of Th1 cells. Cocultures of autologous DCs and lymphocytes improved the generation of cytotoxic T lymphocytes, as assessed by the expression of CD107a, perforin, and granzyme B. The drug combination increased the expression of genes related to the CEACAM family (BECN1, ATGs, MAPLC3B, ULK1, SQSTM1) and tumor suppressors (PCBP1). Furthermore, the decreased expression of genes related to metastasis and tumor progression (BNIP3, BNIP3L, FOSL2, HES1, LAMB3, LOXL2, NDRG1, P4HA1, PIK3R2) was noted. The combination of 5-FU and CQ increases the ability of tumor cells to drive DC maturation and enhances the ability of DCs to stimulate T cell responses.\n \n', 'corpus_id': 215741034, 'score': 0}, {'doc_id': '216072065', 'title': 'Immunity-and-Matrix-Regulatory Cells Derived from Human Embryonic Stem Cells Safely and Effectively Treat Mouse Lung Injury and Fibrosis', 'abstract': 'Lung injury and fibrosis represent the most significant outcomes of severe and acute lung disorders, including COVID-19. However, there are still no effective drugs to treat lung injury and fibrosis. In this study, we report the generation of clinical-grade human embryonic stem cells (hESCs)-derived immunity- and matrix-regulatory cells (IMRCs) produced under good manufacturing practice (GMP) requirements, that can treat lung injury and fibrosis in vivo. We generate IMRCs by sequentially differentiating hESCs with serum-free reagents. IMRCs possess a unique gene expression profile distinct from umbilical cord mesenchymal stem cells (UCMSCs), such as higher levels of proliferative, immunomodulatory and anti-fibrotic genes. Moreover, intravenous delivery of IMRCs inhibits both pulmonary inflammation and fibrosis in mouse models of lung injury, and significantly improves the survival rate of the recipient mice in a dose-dependent manner, likely through paracrine regulatory mechanisms. IMRCs are superior to both primary UCMSCs and FDA-approved pirfenidone, with an excellent efficacy and safety profile in mice and monkeys. In light of public health crises involving pneumonia, acute lung injury (ALI) and acute respiratory distress syndrome (ARDS), our findings suggest that IMRCs are ready for clinical trials on lung disorders.', 'corpus_id': 216072065, 'score': 0}, {'doc_id': '218648055', 'title': 'The management of patients with metastatic prostate cancer during the COVID-19 pandemic', 'abstract': 'During the ongoing global pandemic of coronavirus disease 2019 (COVID-19), the benefit of treating patients with cancer must be weighed against the COVID-19 infection risks to patients, staff and society. Prostate cancer is one of the most common cancers among men and raises particular interest during the pandemic as recent reports show that the TMPRSS2 (and other serine proteases), which facilitate the entry, replication and budding of the virion from a cell, can be inhibited using androgen deprivation therapy. Nevertheless, patients with metastatic prostate cancer commonly receive chemotherapy which may compromise their immune system. This paper aims to address the current status of the COVID-19 in patients with cancer overall and suggests an optimal approach to patients with metastatic prostate cancer.', 'corpus_id': 218648055, 'score': 0}, {'doc_id': '33724446', 'title': 'Microenvironment and Immunology IL-1 b-Mediated Repression of microRNA-101 Is Crucial for In fl ammation-Promoted Lung Tumorigenesis', 'abstract': 'Inflammatory stimuli clearly contribute to lung cancer development and progression, but the underlying pathogenic mechanisms are not fully understood. We found that the proinflammatory cytokine IL-1b is dramatically elevated in the serum of patients with non–small cell lung cancer (NSCLC). In vitro studies showed that IL-1b promoted the proliferation and migration of NSCLC cells. Mechanistically, IL-1b acted through the COX2–HIF1a pathway to repress the expression of microRNA-101 (miR-101), a microRNA with an established role in tumor suppression. Lin28Bwas identified as critical effector target ofmiR-101with its repression of Lin28B, a critical aspect of tumor suppression. Overall, IL-1b upregulated Lin28B by downregulating miR-101. Interestingly, cyclooxygenase-2 inhibition by aspirin or celecoxib abrogated IL-1b-mediated repression of miR-101 and IL-1b-mediated activation of Lin28B along with their stimulatory effects on NSCLC cell proliferation and migration. Together, our findings defined an IL-1b–miR-101–Lin28B pathway as a novel regulatory axis of pathogenic inflammatory signaling in NSCLC. Cancer Res; 74(17); 1–11. 2014 AACR.', 'corpus_id': 33724446, 'score': 1}, {'doc_id': '214802496', 'title': 'Stage I non-small cell lung cancer stratification by using a model-based clustering algorithm with covariates', 'abstract': 'Lung cancer is currently the leading cause of cancer deaths. Among various subtypes, the number of patients diagnosed with stage I non-small cell lung cancer (NSCLC), particularly adenocarcinoma, has been increasing. It is estimated that 30 - 40\\% of stage I patients will relapse, and 10 - 30\\% will die due to recurrence, clearly suggesting the presence of a subgroup that could be benefited by additional therapy. We hypothesize that current attempts to identify stage I NSCLC subgroup failed due to covariate effects, such as the age at diagnosis and differentiation, which may be masking the results. In this context, to stratify stage I NSCLC, we propose CEM-Co, a model-based clustering algorithm that removes/minimizes the effects of undesirable covariates during the clustering process. We applied CEM-Co on a gene expression data set composed of 129 subjects diagnosed with stage I NSCLC and successfully identified a subgroup with a significantly different phenotype (poor prognosis), while standard clustering algorithms failed.', 'corpus_id': 214802496, 'score': 1}]"
182	{'doc_id': '52967399', 'title': 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding', 'abstract': 'We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).', 'corpus_id': 52967399}	504	[{'doc_id': '49313245', 'title': 'Improving Language Understanding by Generative Pre-Training', 'abstract': 'Natural language understanding comprises a wide range of diverse tasks such as textual entailment, question answering, semantic similarity assessment, and document classification. Although large unlabeled text corpora are abundant, labeled data for learning these specific tasks is scarce, making it challenging for discriminatively trained models to perform adequately. We demonstrate that large gains on these tasks can be realized by generative pre-training of a language model on a diverse corpus of unlabeled text, followed by discriminative fine-tuning on each specific task. In contrast to previous approaches, we make use of task-aware input transformations during fine-tuning to achieve effective transfer while requiring minimal changes to the model architecture. We demonstrate the effectiveness of our approach on a wide range of benchmarks for natural language understanding. Our general task-agnostic model outperforms discriminatively trained models that use architectures specifically crafted for each task, significantly improving upon the state of the art in 9 out of the 12 tasks studied. For instance, we achieve absolute improvements of 8.9% on commonsense reasoning (Stories Cloze Test), 5.7% on question answering (RACE), and 1.5% on textual entailment (MultiNLI).', 'corpus_id': 49313245, 'score': 1}, {'doc_id': '57759363', 'title': 'Transformer-XL: Attentive Language Models beyond a Fixed-Length Context', 'abstract': 'Transformers have a potential of learning longer-term dependency, but are limited by a fixed-length context in the setting of language modeling. We propose a novel neural architecture Transformer-XL that enables learning dependency beyond a fixed length without disrupting temporal coherence. It consists of a segment-level recurrence mechanism and a novel positional encoding scheme. Our method not only enables capturing longer-term dependency, but also resolves the context fragmentation problem. As a result, Transformer-XL learns dependency that is 80% longer than RNNs and 450% longer than vanilla Transformers, achieves better performance on both short and long sequences, and is up to 1,800+ times faster than vanilla Transformers during evaluation. Notably, we improve the state-of-the-art results of bpc/perplexity to 0.99 on enwiki8, 1.08 on text8, 18.3 on WikiText-103, 21.8 on One Billion Word, and 54.5 on Penn Treebank (without finetuning). When trained only on WikiText-103, Transformer-XL manages to generate reasonably coherent, novel text articles with thousands of tokens. Our code, pretrained models, and hyperparameters are available in both Tensorflow and PyTorch.', 'corpus_id': 57759363, 'score': 1}, {'doc_id': '210932179', 'title': 'Deep Learning for Hindi Text Classification: A Comparison', 'abstract': 'Natural Language Processing (NLP) and especially natural language text analysis have seen great advances in recent times. Usage of deep learning in text processing has revolutionized the techniques for text processing and achieved remarkable results. Different deep learning architectures like CNN, LSTM, and very recent Transformer have been used to achieve state of the art results variety on NLP tasks. In this work, we survey a host of deep learning architectures for text classification tasks. The work is specifically concerned with the classification of Hindi text. The research in the classification of morphologically rich and low resource Hindi language written in Devanagari script has been limited due to the absence of large labeled corpus. In this work, we used translated versions of English data-sets to evaluate models based on CNN, LSTM and Attention. Multilingual pre-trained sentence embeddings based on BERT and LASER are also compared to evaluate their effectiveness for the Hindi language. The paper also serves as a tutorial for popular text classification techniques.', 'corpus_id': 210932179, 'score': 0}, {'doc_id': '160025533', 'title': 'Language Models are Unsupervised Multitask Learners', 'abstract': 'Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspecific datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underfits WebText. Samples from the model reflect these improvements and contain coherent paragraphs of text. These findings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.', 'corpus_id': 160025533, 'score': 1}, {'doc_id': '74025770', 'title': 'A case of rupture of the biceps brachii and brachialis muscles by direct violence', 'abstract': 'A Msutu, male, aged 28, and a labourer, was admitted to the Non-European Hospital, under the care of Mr. McGregor, on 18/2/35, at 10.35 a.m., complaining of injury to the left arm. History of Present Condition. At 10 a.m. that morning he was helping to demolish a building, when the wall descended upon him. He stepped aside, and the wall hit him on the left upper arm. He sustained no open wound on the arm, but noticed that a swelling appeared immediately. The arm was intensely painful, and he had to support it with his sound limb. He could move his arm, but all movements at the elbow were exceedingly painful. Movements of the wrist and fingers were free and painless. He was', 'corpus_id': 74025770, 'score': 0}, {'doc_id': '55064675', 'title': 'TRANSLATION EQUATION AND SINCOV’S EQUATION – A HISTORICAL REMARK', 'abstract': 'Gottlob Frege (1848 – 1925), the world famous logician was also a pioneer in iteration theory. His habilitation thesis “Rechnungsmethoden, die sich auf eine Erweiterung des Grössenbegriffes gründen” (“Methods of Calculation based on an Extension of the Concept of Quantity”) published 1874 yields a foundation of iteration theory and dynamical systems in one and also in several variables. He considers there the translation equation f(s, f(t, x)) = f(s + t, x) and all the three so-called Aczél-Jabotinsky equations connected with the differentiable solutions of it. By this way Frege e.g. recognized also the importance of the infinitesimal generator of a dynamical system. A comprehensive presentation of this matter may be found in Gronau [4]. Frege treated in this connection also Sincov’s equation Ψ(z, x) = Ψ(z, y) + Ψ(y, x) and gave its general solution almost 30 years before Sincov. The history and background of Sincov’s equation is described in Gronau [5]. Here we give a detailed description of the connection between the translation equation and the Sincov equation.', 'corpus_id': 55064675, 'score': 0}, {'doc_id': '209376347', 'title': 'Multilingual is not enough: BERT for Finnish', 'abstract': 'Deep learning-based language models pretrained on large unannotated text corpora have been demonstrated to allow efficient transfer learning for natural language processing, with recent approaches such as the transformer-based BERT model advancing the state of the art across a variety of tasks. While most work on these models has focused on high-resource languages, in particular English, a number of recent efforts have introduced multilingual models that can be fine-tuned to address tasks in a large number of different languages. However, we still lack a thorough understanding of the capabilities of these models, in particular for lower-resourced languages. In this paper, we focus on Finnish and thoroughly evaluate the multilingual BERT model on a range of tasks, comparing it with a new Finnish BERT model trained from scratch. The new language-specific model is shown to systematically and clearly outperform the multilingual. While the multilingual model largely fails to reach the performance of previously proposed methods, the custom Finnish BERT model establishes new state-of-the-art results on all corpora for all reference tasks: part-of-speech tagging, named entity recognition, and dependency parsing. We release the model and all related resources created for this study with open licenses at this https URL .', 'corpus_id': 209376347, 'score': 0}, {'doc_id': '195069387', 'title': 'XLNet: Generalized Autoregressive Pretraining for Language Understanding', 'abstract': 'With the capability of modeling bidirectional contexts, denoising autoencoding based pretraining like BERT achieves better performance than pretraining approaches based on autoregressive language modeling. However, relying on corrupting the input with masks, BERT neglects dependency between the masked positions and suffers from a pretrain-finetune discrepancy. In light of these pros and cons, we propose XLNet, a generalized autoregressive pretraining method that (1) enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order and (2) overcomes the limitations of BERT thanks to its autoregressive formulation. Furthermore, XLNet integrates ideas from Transformer-XL, the state-of-the-art autoregressive model, into pretraining. Empirically, under comparable experiment settings, XLNet outperforms BERT on 20 tasks, often by a large margin, including question answering, natural language inference, sentiment analysis, and document ranking.', 'corpus_id': 195069387, 'score': 1}, {'doc_id': '13756489', 'title': 'Attention is All you Need', 'abstract': 'The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.', 'corpus_id': 13756489, 'score': 1}, {'doc_id': '211677475', 'title': 'PhoBERT: Pre-trained language models for Vietnamese', 'abstract': 'We present PhoBERT with two versions, PhoBERT-base and PhoBERT-large, the first public large-scale monolingual language models pre-trained for Vietnamese. Experimental results show that PhoBERT consistently outperforms the recent best pre-trained multilingual model XLM-R (Conneau et al., 2020) and improves the state-of-the-art in multiple Vietnamese-specific NLP tasks including Part-of-speech tagging, Dependency parsing, Named-entity recognition and Natural language inference. We release PhoBERT to facilitate future research and downstream applications for Vietnamese NLP. Our PhoBERT models are available at https://github.com/VinAIResearch/PhoBERT', 'corpus_id': 211677475, 'score': 0}]
183	{'doc_id': '237807715', 'title': 'Piezo2, A Pressure Sensitive Channel Is Expressed in Select Neurons of the Mouse Brain: a Putative Mechanism for Synchronizing Neural Networks by Transducing Intracranial Pressure Pulses', 'abstract': '\n Piezo2 expression in mouse brain was examined using an anti-PIEZO2 antibody (Ab) generated against a C-terminal fragment of the human PIEZO2 protein. As a positive control for Ab staining of mouse neurons, the Ab stained a majority of mouse dorsal root ganglion (DRG) neurons, consistent with recent in situ hybridization and single cell RNA sequencing studies of Piezo2 expression. As a negative control and test for specificity, the Ab failed to stain human erythrocytes, which selectively express PIEZO1. In brain slices isolated from the same mice as the DRG, the Ab displayed high selectivity in staining specific neuron types, including pyramidal neurons in the neocortex and hippocampus, Purkinje cells in the cerebellar cortex and mitral cells in the olfactory bulb. Given the demonstrated role of Piezo2 channels in peripheral neurons as a low-threshold pressure sensor (i.e., ≤ 5 mm Hg) critical for the regulation of breathing and blood pressure, its expression in select brain neurons has interesting implications. In particular, we hypothesize that Piezo2 provides select brain neurons with an intrinsic resonance enabling their entrainment by the normal intracranial pressure (ICP) pulses (~ 5 mm Hg) associated with breathing and cardiac cycles. This mechanism could serve to increase the robustness of respiration-entrained oscillations previously reported across widely distributed neuronal networks in both rodent and human brains. This idea of a “global brain rhythm” has previously been thought to arise from the effect of nasal airflow activating mechanosensitive neurons within the olfactory epithelium, which then synaptically entrain mitral cells within the olfactory bulb and through their projections, neural networks in other brain regions, including the hippocampus and neocortex. Our proposed, non-synaptic, intrinsic mechanism in which Piezo2 tracks the “metronome-like” ICP pulses would have the advantage that spatially separated brain networks could also be synchronized by a physical force that is rapidly transmitted throughout the brain.', 'corpus_id': 237807715}	20578	"[{'doc_id': '235663508', 'title': 'The Influence of Heart Rate Variability Biofeedback on Cardiac Regulation and Functional Brain Connectivity', 'abstract': 'Background Heart rate variability (HRV) biofeedback has a beneficial impact on perceived stress and emotion regulation. However, its impact on brain function is still unclear. In this study, we aimed to investigate the effect of an 8-week HRV-biofeedback intervention on functional brain connectivity in healthy subjects. Methods HRV biofeedback was carried out in five sessions per week, including four at home and one in our lab. A control group played jump‘n’run games instead of the training. Functional magnetic resonance imaging was conducted before and after the intervention in both groups. To compute resting state functional connectivity (RSFC), we defined regions of interest in the ventral medial prefrontal cortex (VMPFC) and a total of 260 independent anatomical regions for network-based analysis. Changes of RSFC of the VMPFC to other brain regions were compared between groups. Temporal changes of HRV during the resting state recording were correlated to dynamic functional connectivity of the VMPFC. Results First, we corroborated the role of the VMPFC in cardiac autonomic regulation. We found that temporal changes of HRV were correlated to dynamic changes of prefrontal connectivity, especially to the middle cingulate cortex, the left insula, supplementary motor area, dorsal and ventral lateral prefrontal regions. The biofeedback group showed a drop in heart rate by 5.2 beats/min and an increased SDNN as a measure of HRV by 8.6 ms (18%) after the intervention. Functional connectivity of the VMPFC increased mainly to the insula, the amygdala, the middle cingulate cortex, and lateral prefrontal regions after biofeedback intervention when compared to changes in the control group. Network-based statistic showed that biofeedback had an influence on a broad functional network of brain regions. Conclusion Our results show that increased heart rate variability induced by HRV-biofeedback is accompanied by changes in functional brain connectivity during resting state.', 'corpus_id': 235663508, 'score': 1}, {'doc_id': '1700773', 'title': 'Vascular Dynamics Aid a Coupled Neurovascular Network Learn Sparse Independent Features: A Computational Model', 'abstract': 'Cerebral vascular dynamics are generally thought to be controlled by neural activity in a unidirectional fashion. However, both computational modeling and experimental evidence point to the feedback effects of vascular dynamics on neural activity. Vascular feedback in the form of glucose and oxygen controls neuronal ATP, either directly or via the agency of astrocytes, which in turn modulates neural firing. Recently, a detailed model of the neuron-astrocyte-vessel system has shown how vasomotion can modulate neural firing. Similarly, arguing from known cerebrovascular physiology, an approach known as “hemoneural hypothesis” postulates functional modulation of neural activity by vascular feedback. To instantiate this perspective, we present a computational model in which a network of “vascular units” supplies energy to a neural network. The complex dynamics of the vascular network, modeled by a network of oscillators, turns neurons ON and OFF randomly. The informational consequence of such dynamics is explored in the context of an auto-encoder network. In the proposed model, each vascular unit supplies energy to a subset of hidden neurons of an autoencoder network, which constitutes its “projective field.” Neurons that receive adequate energy in a given trial have reduced threshold, and thus are prone to fire. Dynamics of the vascular network are governed by changes in the reconstruction error of the auto-encoder network, interpreted as the neuronal demand. Vascular feedback causes random inactivation of a subset of hidden neurons in every trial. We observe that, under conditions of desynchronized vascular dynamics, the output reconstruction error is low and the feature vectors learnt are sparse and independent. Our earlier modeling study highlighted the link between desynchronized vascular dynamics and efficient energy delivery in skeletal muscle. We now show that desynchronized vascular dynamics leads to efficient training in an auto-encoder neural network.', 'corpus_id': 1700773, 'score': 1}, {'doc_id': '237406696', 'title': 'Brain Circuits Involved in the Development of Chronic Musculoskeletal Pain: Evidence From Non-invasive Brain Stimulation', 'abstract': 'It has been well-documented that the brain changes in states of chronic pain. Less is known about changes in the brain that predict the transition from acute to chronic pain. Evidence from neuroimaging studies suggests a shift from brain regions involved in nociceptive processing to corticostriatal brain regions that are instrumental in the processing of reward and emotional learning in the transition to the chronic state. In addition, dysfunction in descending pain modulatory circuits encompassing the periaqueductal gray and the rostral anterior cingulate cortex may also be a key risk factor for pain chronicity. Although longitudinal imaging studies have revealed potential predictors of pain chronicity, their causal role has not yet been determined. Here we review evidence from studies that involve non-invasive brain stimulation to elucidate to what extent they may help to elucidate the brain circuits involved in pain chronicity. Especially, we focus on studies using non-invasive brain stimulation techniques [e.g., transcranial magnetic stimulation (TMS), particularly its repetitive form (rTMS), transcranial alternating current stimulation (tACS), and transcranial direct current stimulation (tDCS)] in the context of musculoskeletal pain chronicity. We focus on the role of the motor cortex because of its known contribution to sensory components of pain via thalamic inhibition, and the role of the dorsolateral prefrontal cortex because of its role on cognitive and affective processing of pain. We will also discuss findings from studies using experimentally induced prolonged pain and studies implicating the DLPFC, which may shed light on the earliest transition phase to chronicity. We propose that combined brain stimulation and imaging studies might further advance mechanistic models of the chronicity process and involved brain circuits. Implications and challenges for translating the research on mechanistic models of the development of chronic pain to clinical practice will also be addressed.', 'corpus_id': 237406696, 'score': 0}, {'doc_id': '237156116', 'title': 'Neural Responses During Emotion Transitions and Emotion Regulation', 'abstract': 'Why are some people more susceptible to interference from previous emotional stimuli? Neural mechanisms underlying emotion regulation are typically studied with one-off positive or negative stimuli. Less is known about how they operate during dynamic emotional experiences, which more closely resemble how emotions occur in real life. Therefore, we investigated the interaction among temporal context, stimulus content, and regulatory strategy. Image sequences included either neutral to negative emotion or negative to neutral emotion. Participants were instructed to either passively watch the emotional stimuli or apply cognitive reappraisal during the image sequences presentation. Participants also reported their habitual use of cognitive reappraisal in their daily lives on a standard scale. We measured functional connectivity (FC) with electroencephalography (EEG) source localization. A three-way interaction suggested that, in addition to momentary emotional content and regulatory effort, the temporal context of stimuli impacts the FC between the ventromedial prefrontal cortex (vmPFC) and the ventral anterior cingulate cortex (ACC) in both alpha and beta frequency bands. In the reappraisal condition—but not the passive watch conditions—, individual differences in habitual reappraisal were manifested in the FC of vmPFC-ACC in alpha band. Emotion transitions may be more demanding because prefrontal-posterior FC in the beta band decreased during emotion transitions regardless of emotional content or regulation efforts. Flexible emotion regulation enables the recruiting of neural activities in response to the content of dynamic, ever-changing experiences encountered in daily life. Studying brain responses to dynamic emotional stimuli may shed light on individual differences in adaptation and psychological health. It also provides a more ecologically valid assessment of emotion regulation.', 'corpus_id': 237156116, 'score': 0}, {'doc_id': '149445874', 'title': ""The brain's resonance of breathing - decelerated breathing synchronizes heart rate and slow cortical potentials."", 'abstract': 'Numerous methods for enhancing consciousness and well-being emphasize the role of breathing. We have for the first time investigated the link between body rhythms and slow cortical brain dynamics during paced breathing. Physiological data from 37 participants are presented, who conducted paced breathing sessions with respiration rates (RR) from 6 to 14 seconds/cycle for 7 min each task. Measures of respiration, heart rate variability (HRV), and 64 channels EEG as well as subjective ratings were recorded and compared with each other. Both, the respiratory sinus arrhythmia of the HRV and the slow cortical potentials (SCPs) of the EEG correlated with the respiration cycle. The highest correlations were observed at a RR of 10 seconds/cycle especially for the SCPs. A strong positive voltage deflection during inhalation is followed by a negative variation during exhalation. This decelerated breathing rhythm matches the frequency of the baroreceptor sensitivity, leading to synchronization between breath, HRV, baroreceptors and the brain. Subjectively, participants rated this RR as the most relaxing one. This study demonstrates the importance of the speed of breathing on the brain dynamics which might help in understanding the role of the breath for mental health. .', 'corpus_id': 149445874, 'score': 1}, {'doc_id': '238232466', 'title': 'Altered Hypothalamic Functional Connectivity Following Total Sleep Deprivation in Young Adult Males', 'abstract': 'Background: Sleep deprivation can markedly influence vigilant attention that is essential to complex cognitive processes. The hypothalamus plays a critical role in arousal and attention regulation. However, the functional involvement of the hypothalamus in attentional impairments after total sleep deprivation (TSD) remains unclear. The purpose of this study is to investigate the alterations in hypothalamic functional connectivity and its association with the attentional performance following TSD. Methods: Thirty healthy adult males were recruited in the study. Participants underwent two resting-state functional magnetic resonance imaging (rs-fMRI) scans, once in rested wakefulness (RW) and once after 36 h of TSD. Seed-based functional connectivity analysis was performed using rs-fMRI for the left and right hypothalamus. Vigilant attention was measured using a psychomotor vigilance test (PVT). Furthermore, Pearson correlation analysis was conducted to investigate the relationship between altered hypothalamic functional connectivity and PVT performance after TSD. Results: After TSD, enhanced functional connectivity was observed between the left hypothalamus and bilateral thalamus, bilateral anterior cingulate cortex, right amygdala, and right insula, while reduced functional connectivity was observed between the left hypothalamus and bilateral middle frontal gyrus (AlphaSim corrected, P < 0.01). However, significant correlation between altered hypothalamic functional connectivity and PVT performance was not observed after Bonferroni correction (P > 0.05). Conclusion: Our results suggest that TSD can lead to disrupted hypothalamic circuits, which may provide new insight into neural mechanisms of attention impairments following sleep deprivation.', 'corpus_id': 238232466, 'score': 0}, {'doc_id': '235477137', 'title': 'MRI-related anxiety can induce slow BOLD oscillations coupled with cardiac oscillations', 'abstract': 'OBJECTIVE\nAlthough about 1-2% of MRI examinations must be aborted due to anxiety, there is little research on how MRI-related anxiety affects BOLD signals in resting states.\n\n\nMETHODS\nWe re-analyzed cardiac beat-to beat interval (RRI) and BOLD signals of 23 healthy fMRI participants in four resting states by calculation of phase-coupling in the 0.07-0.13\xa0Hz band and determination of positive time delays (pTDs; RRI leading neural BOLD oscillations) and negative time delays (nTDs; RRI lagging behind vascular BOLD oscillations). State anxiety of each subject was assigned to either a low anxiety (LA) or a high anxiety (HA, with most participants exhibiting moderate anxiety symptoms) category based on the inside scanner assessed anxiety score.\n\n\nRESULTS\nAlthough anxiety strongly differed between HA and LA categories, no significant difference was found for nTDs. In contrast, pTDs indicating neural BOLD oscillations exhibited a significant cumulation in the high anxiety category.\n\n\nCONCLUSIONS\nFindings may suggest that vascular BOLD oscillations related to slow cerebral blood circulation are of about similar intensity during low/no and elevated anxiety. In contrast, neural BOLD oscillations, which might be associated with a central rhythm generating mechanism (pacemaker-like activity), appear to be significantly intensified during elevated anxiety.\n\n\nSIGNIFICANCE\nThe study provides evidence that fMRI-related anxiety can activate a central rhythm generating mechanism very likely located in the brain stem, associated with slow neural BOLD oscillation.', 'corpus_id': 235477137, 'score': 1}, {'doc_id': '226291226', 'title': 'Impact of Altered Breathing Patterns on Interaction of EEG and Heart Rate Variability', 'abstract': 'Background: Altered pattern of respiration has been shown to affect both the cardiac as well as cortical activity, which is the basis of central–autonomic dual interaction concept. On the other hand, effect of this association between altered breathing with slow cortical activity, that is, electroencephalography (EEG) theta waves (associated with learning and relaxed alertness) on the cardiac autonomic balance is largely unclear. Objective: The study aims to understand this interaction in response to altered respiratory patterns, for example, voluntary apnea, bradypnea, and tachypnea in terms of EEG and heart rate variability (HRV) correlates in normal healthy subjects. Methods: This study was conducted on 32 adult male subjects. EEG from F3, F4, P3, P4, O1 and O2 cortical areas and Lead II electrocardiography for HRV analysis was continuously recorded during aforesaid respiratory interventions. Power spectral analysis of EEG for theta waves and HRV measures, that is, RMSSD, pNN50, HF, LF, and LF/HF was calculated as % change taking resting value as 100%. Results: Apnea caused decrease in theta power, whereas an increase in LF/HF was observed in HRV. Bradypnea on the other hand, did not elicit any significant change in power of theta waves. However, decreased RMSSD and pNN50 were observed in HRV. Tachypnea led to increase in theta power with HRV depicting significantly decreased RMSSD and pNN50. Besides, significant correlation between EEG and HRV measures was found during tachypnea, which shifted toward posterior cortical sites as compared to resting condition. Conclusion: Various altered respiratory patterns caused either depressed parasympathetic or increased sympathetic output, whereas increased theta power along with posterior shift of correlation between theta power and HRV measures observed during post tachypnea might be due to involvement of global brain areas due to respiration-coupled neuronal activity. Thus, a definite link between cortical activity and autonomic output in relation to altered respiratory patterns may be suggested.', 'corpus_id': 226291226, 'score': 1}, {'doc_id': '237299518', 'title': 'A neuro-cardiac self-regulation therapy to improve autonomic and neural function after SCI: a randomized controlled trial protocol', 'abstract': 'Background Spinal cord injury (SCI) is associated with autonomic imbalance and significant secondary conditions, including cardiac and brain dysfunction that adversely impact health and wellbeing. This study will investigate the effectiveness (intention-to-treat) of a neuro-cardiac self-regulation therapy to improve autonomic and neural/brain activity in adults with SCI living in the community. Methods A two-arm parallel, randomised controlled trial in which adults with SCI living in the community post-rehabilitation will be randomly assigned to a treatment or control group. The treatment group ( N \u2009=\u200960) aged 18–70\u2009years with a chronic traumatic or non-traumatic SCI, will receive intervention sessions once per week for 10\u2009weeks, designed to regulate autonomic activity using computer-based feedback of heart rate variability and controlled breathing (called HRV-F). Comprehensive neurophysiological and psychological assessment will occur at baseline, immediate post-treatment, and 6 and 12-months post-treatment. Primary outcome measures include electrocardiography/heart rate variability (to assess autonomic nervous system function) and transcranial doppler sonography (to assess cerebral blood circulation in basal cerebral arteries). Secondary outcomes measures include continuous blood pressure, electroencephalography, functional near-infrared spectroscopy, respiration/breath rate, electrooculography, cognitive capacity, psychological status, pain, fatigue, sleep and quality of life. Controls ( N \u2009=\u200960) will receive usual community care, reading material and a brief telephone call once per week for 10\u2009weeks and be similarly assessed over the same time period as the HRV-F group. Linear mixed model analysis with repeated measures will determine effectiveness of HRV-F and latent class mixture modelling used to determine trajectories for primary and selected secondary outcomes of interest. Discussion Treatments for improving autonomic function after SCI are limited. It is therefore important to establish whether a neuro-cardiac self-regulation therapy can result in improved autonomic functioning post-SCI, as well as whether HRV-F is associated with better outcomes for secondary conditions such as cardiovascular health, cognitive capacity and mental health. Trial registration The study has been prospectively registered with the Australian and New Zealand Clinical Trial Registry ( ACTRN12621000870853 .aspx). Date of Registration: 6th July 2021. Trial Sponsor: The University of Sydney, NSW 2006. Protocol version: 22/07/2021.', 'corpus_id': 237299518, 'score': 0}, {'doc_id': '237003229', 'title': 'Using Fractional Amplitude of Low-Frequency Fluctuations and Functional Connectivity in Patients With Post-stroke Cognitive Impairment for a Simulated Stimulation Program', 'abstract': 'Stroke causes alterations in local spontaneous neuronal activity and related networks functional connectivity. We hypothesized that these changes occur in patients with post-stroke cognitive impairment (PSCI). Fractional amplitude of low-frequency fluctuations (fALFF) was calculated in 36 patients with cognitive impairment, including 16 patients with hemorrhagic stroke (hPSCI group), 20 patients with ischemic stroke (iPSCI group). Twenty healthy volunteers closely matched to the patient groups with respect to age and gender were selected as the healthy control group (HC group). Regions with significant alteration were regarded as regions of interest (ROIs) using the one-way analysis of variance, and then the seed-based functional connectivity (FC) with other regions in the brain was analyzed. Pearson correlation analyses were performed to investigate the correlation between functional indexes and cognitive performance in patients with PSCI. Our results showed that fALFF values of bilateral posterior cingulate cortex (PCC)/precuneus and bilateral anterior cingulate cortex in the hPSCI group were lower than those in the HC group. Compared with the HC group, fALFF values were lower in the superior frontal gyrus and basal ganglia in the iPSCI group. Correlation analysis showed that the fALFF value of left PCC was positively correlated with MMSE scores and MoCA scores in hPSCI. Besides, the reduction of seed-based FC values was reported, especially in regions of the default-mode network (DMN) and the salience network (SN). Abnormalities of spontaneous brain activity and functional connectivity are observed in PSCI patients. The decreased fALFF and FC values in DMN of patients with hemorrhagic and SN of patients with ischemic stroke may be the pathological mechanism of cognitive impairment. Besides, we showed how to use fALFF values and functional connectivity maps to specify a target map on the cortical surface for repetitive transcranial magnetic stimulation (rTMS).', 'corpus_id': 237003229, 'score': 0}]"
184	{'doc_id': '774118', 'title': 'Noisy binary search and its applications', 'abstract': 'We study a noisy version of the classic binary search problem of inserting an element into its proper place within an ordered sequence by comparing it with elements of the sequence. In the noisy version we can not compare elements directly. Instead we are given a coin corresponding to each element of the sequence, such that as one goes through the ordered sequence the probability of observing heads when tossing the corresponding coin increases. We design online algorithms which adaptively choose a sequence of experiments, each consisting of tossing a single coin, with the goal of identifying the highest-numbered coin in the ordered sequence whose heads probability is less than some specified target value. Possible applications of such algorithms include investment planning, sponsored search advertising, admission control in queueing networks, college admissions, and admitting new members into an organization ranked by ability, such as a tennis ladder.', 'corpus_id': 774118}	1914	"[{'doc_id': '220546114', 'title': 'Optimal Vertex Fault-Tolerant Spanners in Polynomial Time', 'abstract': ""Recent work has pinned down the existentially optimal size bounds for vertex fault-tolerant spanners: for any positive integer $k$, every $n$-node graph has a $(2k-1)$-spanner on $O(f^{1-1/k} n^{1+1/k})$ edges resilient to $f$ vertex faults, and there are examples of input graphs on which this bound cannot be improved. However, these proofs work by analyzing the output spanner of a certain exponential-time greedy algorithm. In this work, we give the first algorithm that produces vertex fault tolerant spanners of optimal size and which runs in polynomial time. Specifically, we give a randomized algorithm which takes $\\widetilde{O}\\left( f^{1-1/k} n^{2+1/k} + mf^2\\right)$ time. We also derandomize our algorithm to give a deterministic algorithm with similar bounds. This reflects an exponential improvement in runtime over [Bodwin-Patel PODC '19], the only previously known algorithm for constructing optimal vertex fault-tolerant spanners."", 'corpus_id': 220546114, 'score': 0}, {'doc_id': '221265995', 'title': 'Deletion to Induced Matching', 'abstract': 'In the DELETION TO INDUCED MATCHING problem, we are given a graph $G$ on $n$ vertices, $m$ edges and a non-negative integer $k$ and asks whether there exists a set of vertices $S \\subseteq V(G) $ such that $|S|\\le k$ and the size of any connected component in $G-S$ is exactly 2. In this paper, we provide a fixed-parameter tractable (FPT) algorithm of running time $O^*(1.748^{k})$ for the DELETION TO INDUCED MATCHING problem using branch-and-reduce strategy and path decomposition. We also extend our work to the exact-exponential version of the problem.', 'corpus_id': 221265995, 'score': 0}, {'doc_id': '221292972', 'title': 'How Many Vertices Does a Random Walk Miss in a Network with Moderately Increasing the Number of Vertices?', 'abstract': 'Real networks are often dynamic. In response to it, analyses of algorithms on {\\em dynamic networks} attract more and more attentions in network science and engineering. Random walks on dynamic graphs also have been investigated actively in more than a decade, where in most cases the edge set changes but the vertex set is static. The vertex sets are also dynamic in many real networks. Motivated by a new technology of the analysis of random walks on dynamic graphs, this paper introduces a simple model of graphs with increasing the number of vertices, and presents an analysis of random walks associated with the cover time on such graphs. In particular, we reveal that a random walk asymptotically covers the vertices all but a constant number if the vertex set grows {\\em moderately}.', 'corpus_id': 221292972, 'score': 0}, {'doc_id': '41802054', 'title': 'Handbook of satisfiability', 'abstract': ""'Satisfiability (SAT) related topics have attracted researchers from various disciplines: logic, applied areas such as planning, scheduling, operations research and combinatorial optimization, but also theoretical issues on the theme of complexity and much more, they all are connected through SAT. My personal interest in SAT stems from actual solving: The increase in power of modern SAT solvers over the past 15 years has been phenomenal. It has become the key enabling technology in automated verification of both computer hardware and software' - Edmund M. Clarke (FORE Systems University Professor of Computer Science and Professor of Electrical and Computer Engineering at Carnegie Mellon University). 'Bounded Model Checking (BMC) of computer hardware is now probably the most widely used model checking technique. The counterexamples that it finds are just satisfying instances of a Boolean formula obtained by unwinding to some fixed depth a sequential circuit and its specification in linear temporal logic. Extending model checking to software verification is a much more difficult problem on the frontier of current research. One promising approach for languages like C with finite word-length integers is to use the same idea as in BMC but with a decision procedure for the theory of bit-vectors instead of SAT. All decision procedures for bit-vectors that I am familiar with ultimately make use of a fast SAT solver to handle complex formulas' - Edmund M. Clarke (FORE Systems University Professor of Computer Science and Professor of Electrical and Computer Engineering at Carnegie Mellon University). 'Decision procedures for more complicated theories, like linear real and integer arithmetic, are also used in program verification. Most of them use powerful SAT solvers in an essential way. Clearly, efficient SAT solving is a key technology for 21st century computer science. I expect this collection of papers on all theoretical and practical aspects of SAT solving will be extremely useful to both students and researchers and will lead to many further advances in the field' - Edmund M. Clarke (FORE Systems University Professor of Computer Science and Professor of Electrical and Computer Engineering at Carnegie Mellon University)."", 'corpus_id': 41802054, 'score': 1}, {'doc_id': '221266319', 'title': 'An Efficient Algorithm for Finding Sets of Optimal Routes', 'abstract': 'In several important routing contexts it is required to identify a set of routes, each of which optimizes a different criterion. For instance, in the context of vehicle routing, one route would minimize the total distance traveled, while other routes would also consider the total travel time or the total incurred cost, or combinations thereof. In general, providing such a set of diverse routes is obtained by finding optimal routes with respect to different sets of weights on the network edges. This can be simply achieved by consecutively executing a standard shortest path algorithm. However, in the case of a large number of weight sets, this may require an excessively large number of executions of such an algorithm, thus incurring a prohibitively large running time. \nWe indicate that, quite often, the different edge weights reflect different combinations of some ""raw"" performance metrics (e.g., delay, cost). In such cases, there is an inherent dependency among the different weights of the same edge. This may well result in some similarity among the shortest routes, each of which being optimal with respect to a specific set of weights. In this study, we aim to exploit such similarity in order to improve the performance of the solution scheme. \nSpecifically, we contemplate edge weights that are obtained through different linear combinations of some (``raw\'\') edge performance metrics. We establish and validate a novel algorithm that efficiently computes a shortest path for each set of edge weights. We demonstrate that, under reasonable assumptions, the algorithm significantly outperforms the standard approach. Similarly to the standard approach, the algorithm iteratively searches for routes, one per set of edge weights; however, instead of executing each iteration independently, it reduces the average running time by skillfully sharing information among the iterations.', 'corpus_id': 221266319, 'score': 1}, {'doc_id': '221186760', 'title': 'A Simple Proof of Optimal Approximations', 'abstract': ""The fundamental result of Li, Long, and Srinivasan on approximations of set systems has become a key tool across several communities such as learning theory, algorithms, combinatorics and data analysis (described as `the pinnacle of a long sequence of papers'). The goal of this paper is to give a simpler, self-contained, modular proof of this result for finite set systems. The only ingredient we assume is the standard Chernoff's concentration bound. This makes the proof accessible to a wider audience, readers not familiar with techniques from statistical learning theory, and makes it possible to be covered in a single self-contained lecture in an algorithms course."", 'corpus_id': 221186760, 'score': 1}, {'doc_id': '220646753', 'title': 'On completely factoring any integer efficiently in a single run of an order finding algorithm', 'abstract': 'We show that given the order of a single element selected uniformly at random from ZN , we can with very high probability, and for any integer N , efficiently find the complete factorization of N in polynomial time. This implies that a single run of the quantum part of Shor’s factoring algorithm is usually sufficient. All prime factors of N can then be recovered with negligible computational cost in a classical post-processing step. The classical algorithm required for this step is essentially due to Miller.', 'corpus_id': 220646753, 'score': 1}, {'doc_id': '221292996', 'title': 'A variation of the prime k-tuples conjecture with applications to quantum limits', 'abstract': 'Let $\\mathcal{H}^{*}=\\{h_1,h_2,\\ldots\\}$ be an ordered set of integers. We give sufficient conditions for the existence of increasing sequences of natural numbers $a_j$ and $n_k$ such that $n_k+h_{a_j}$ is a sum of two squares for every $k\\geq 1$ and $1\\leq j\\leq k.$ Our method uses a novel modification of the Maynard-Tao sieve together with a second moment estimate. As a special case of our result, we deduce a conjecture due to D.~Jakobson which has several implications for quantum limits on flat tori.', 'corpus_id': 221292996, 'score': 1}, {'doc_id': '221246082', 'title': 'Hyperbolic Polynomials I : Concentration and Discrepancy', 'abstract': 'Chernoff bound is a fundamental tool in theoretical computer science. It has been extensively used in randomized algorithm design and stochastic type analysis. The discrepancy theory, which deals with finding a bi-coloring of a set system such that the coloring of each set is balanced, has a huge number of applications in approximation algorithm design. A classical theorem of Spencer [Spe85] shows that any set system with $n$ sets and $n$ elements has discrepancy $O(\\sqrt{n})$ while Chernoff [Che52] only gives $O(\\sqrt{n \\log n})$. \nThe study of hyperbolic polynomial is dating back to the early 20th century, due to Garding [Gar59] for solving PDEs. In recent years, more applications are found in control theory, optimization, real algebraic geometry, and so on. In particular, the breakthrough result by Marcus, Spielman, and Srivastava [MSS15] uses the theory of hyperbolic polynomial to prove the Kadison-Singer conjecture [KS59], which is closely related to the discrepancy theory. \nIn this paper, we present two new results for hyperbolic polynomials \n$\\bullet$ We show an optimal hyperbolic Chernoff bound for any constant degree hyperbolic polynomials. \n$\\bullet$ We prove a hyperbolic Spencer theorem for any constant rank vectors. \nThe classical matrix Chernoff and discrepancy results are based on determinant polynomial which is a special case of hyperbolic polynomial. To the best of our knowledge, this paper is the first work that shows either concentration or discrepancy results for hyperbolic polynomials.', 'corpus_id': 221246082, 'score': 0}, {'doc_id': '221246257', 'title': 'Greedy Approaches to Online Stochastic Matching', 'abstract': 'Within the context of stochastic probing with commitment, we consider the online stochastic matching problem; that is, the one-sided online bipartite matching problem where edges adjacent to an online node must be probed to determine if they exist based on edge probabilities that become known when an online vertex arrives. If a probed edge exists, it must be used in the matching (if possible). We consider the competitiveness of online algorithms in the random order input model (ROM), when the offline vertices are weighted. More specifically, we consider a bipartite stochastic graph $G = (U,V,E)$ where $U$ is the set of offline vertices, $V$ is the set of online vertices and $G$ has edge probabilities $(p_{e})_{e \\in E}$ and vertex weights $(w_{u})_{u \\in U}$. Additionally, $G$ has patience values $(\\ell_{v})_{v \\in V}$, where $\\ell_v$ indicates the maximum number of edges adjacent to an online vertex $v$ which can be probed. We assume that $U$ and $(w_{u})_{u \\in U}$ are known in advance, and that the patience, adjacent edges and edge probabilities for each online vertex are only revealed when the online vertex arrives. If any one of the following three conditions is satisfied, then there is a conceptually simple deterministic greedy algorithm whose competitive ratio is $1-\\frac{1}{e}$. \n(1) When the offline vertices are unweighted. $\\\\$ \n(2) When the online vertex probabilities are ""vertex uniform""; i.e., $p_{u,v} = p_v$ for all $(u,v) \\in E$. $\\\\$ \n(3) When the patience constraint $\\ell_v$ satisfies $\\ell_v \\in \\{[1,|U|\\}$ for every online vertex; i.e., every online vertex either has unit or full patience. \nSetting the probability $p_e = 1$ for all $e \\in E$, the stochastic problem becomes the classical online bipartite matching problem. Our competitive ratios thus generalize corresponding results for the classical ROM bipartite matching setting.', 'corpus_id': 221246257, 'score': 0}]"
185	{'doc_id': '205166643', 'title': 'Several classical mouse inbred strains, including DBA/2, NOD/Lt, FVB/N, and SJL/J, carry a putative loss-of-function allele of Gpr84.', 'abstract': 'G protein-coupled receptor 84 (GPR84) is a 7-transmembrane protein expressed on myeloid cells that can bind to medium-chain free fatty acids in vitro. Here, we report the discovery of a 2-bp frameshift deletion in the second exon of the Gpr84 gene in several classical mouse inbred strains. This deletion generates a premature stop codon predicted to result in a truncated protein lacking the transmembrane domains 4-7. We sequenced Gpr84 exon 2 from 58 strains representing different groups in the mouse family tree and found that 14 strains are homozygous for the deletion. Some of these strains are DBA/1J, DBA/2J, FVB/NJ, LG/J, MRL/MpJ, NOD/LtJ, and SJL/J. However, the deletion was not found in any of the wild-derived inbred strains analyzed. Haplotype analysis suggested that the deletion originates from a unique mutation event that occurred more than 100 years ago, preceding the development of the first inbred strain (DBA), from a Mus musculus domesticus source. As GPR84 ostensibly plays a role in the biology of myeloid cells, it could be relevant 1) to consider the existence of this Gpr84 nonsense mutation in several mouse strains when choosing a mouse model to study immune processes and 2) to consider reevaluating data obtained using such strains.', 'corpus_id': 205166643}	16333	[{'doc_id': '15096396', 'title': 'Genetic influences on exercise‐induced adult hippocampal neurogenesis across 12 divergent mouse strains', 'abstract': 'New neurons are continuously born in the hippocampus of several mammalian species throughout adulthood. Adult neurogenesis represents a natural model for understanding how to grow and incorporate new nerve cells into preexisting circuits in the brain. Finding molecules or biological pathways that increase neurogenesis has broad potential for regenerative medicine. One strategy is to identify mouse strains that display large vs. small increases in neurogenesis in response to wheel running so that the strains can be contrasted to find common genes or biological pathways associated with enhanced neuron formation. Therefore, mice from 12 different isogenic strains were housed with or without running wheels for 43 days to measure the genetic regulation of exercise‐induced neurogenesis. During the first 10 days mice received daily injections of 5‐bromo‐2′‐deoxyuridine (BrdU) to label dividing cells. Neurogenesis was measured as the total number of BrdU cells co‐expressing NeuN mature neuronal marker in the hippocampal granule cell layer by immunohistochemistry. Exercise increased neurogenesis in all strains, but the magnitude significantly depended on genotype. Strain means for distance run on wheels, but not distance traveled in cages without wheels, were significantly correlated with strain mean level of neurogenesis. Furthermore, certain strains displayed greater neurogenesis than others for a fixed level of running. Strain means for neurogenesis under sedentary conditions were not correlated with neurogenesis under runner conditions suggesting that different genes influence baseline vs. exercise‐induced neurogenesis. Genetic contributions to exercise‐induced hippocampal neurogenesis suggest that it may be possible to identify genes and pathways associated with enhanced neuroplastic responses to exercise.', 'corpus_id': 15096396, 'score': 1}, {'doc_id': '232021281', 'title': 'Identifying Potentially Beneficial Genetic Mutations Associated with Monophyletic Selective Sweep and a Proof-of-Concept Study with Viral Genetic Data', 'abstract': 'In biology, research on evolution is important to understand the significance of genetic mutation. When there is a significantly beneficial mutation, a population of species with the mutation prospers and predominates, in a process called “selective sweep.” However, there are few methods that can find such a mutation causing selective sweep from genetic data. ABSTRACT Genetic mutations play a central role in evolution. For a significantly beneficial mutation, a one-time mutation event suffices for the species to prosper and predominate through the process called “monophyletic selective sweep.” However, existing methods that rely on counting the number of mutation events to detect selection are unable to find such a mutation in selective sweep. We here introduce a method to detect mutations at the single amino acid/nucleotide level that could be responsible for monophyletic selective sweep evolution. The method identifies a genetic signature associated with selective sweep using the population genetic test statistic Tajima’s D. We applied the algorithm to ebolavirus, influenza A virus, and severe acute respiratory syndrome coronavirus 2 to identify known biologically significant mutations and unrecognized mutations associated with potential selective sweep. The method can detect beneficial mutations, possibly leading to discovery of previously unknown biological functions and mechanisms related to those mutations. IMPORTANCE In biology, research on evolution is important to understand the significance of genetic mutation. When there is a significantly beneficial mutation, a population of species with the mutation prospers and predominates, in a process called “selective sweep.” However, there are few methods that can find such a mutation causing selective sweep from genetic data. We here introduce a novel method to detect such mutations. Applying the method to the genomes of ebolavirus, influenza viruses, and the novel coronavirus, we detected known biologically significant mutations and identified mutations the importance of which is previously unrecognized. The method can deepen our understanding of molecular and evolutionary biology.', 'corpus_id': 232021281, 'score': 0}, {'doc_id': '227157441', 'title': 'Mouse Genome Database (MGD): Knowledgebase for mouse–human comparative biology', 'abstract': 'Abstract The Mouse Genome Database (MGD; http://www.informatics.jax.org) is the community model organism knowledgebase for the laboratory mouse, a widely used animal model for comparative studies of the genetic and genomic basis for human health and disease. MGD is the authoritative source for biological reference data related to mouse genes, gene functions, phenotypes and mouse models of human disease. MGD is the primary source for official gene, allele, and mouse strain nomenclature based on the guidelines set by the International Committee on Standardized Nomenclature for Mice. MGD’s biocuration scientists curate information from the biomedical literature and from large and small datasets contributed directly by investigators. In this report we describe significant enhancements to the content and interfaces at MGD, including (i) improvements in the Multi Genome Viewer for exploring the genomes of multiple mouse strains, (ii) inclusion of many more mouse strains and new mouse strain pages with extended query options and (iii) integration of extensive data about mouse strain variants. We also describe improvements to the efficiency of literature curation processes and the implementation of an information portal focused on mouse models and genes for the study of COVID-19.', 'corpus_id': 227157441, 'score': 1}, {'doc_id': '232364190', 'title': 'Functional analysis of SARS-CoV-2 proteins in Drosophila identifies Orf6-induced pathogenic effects with Selinexor as an effective treatment', 'abstract': 'Background SARS-CoV-2 causes COVID-19 with a widely diverse disease profile that affects many different tissues. The mechanisms underlying its pathogenicity in host organisms remain unclear. Animal models for studying the pathogenicity of SARS-CoV-2 proteins are lacking. Methods Using bioinformatic analysis, we found that 90% of the virus-host interactions involve human proteins conserved in Drosophila . Therefore, we generated a series of transgenic fly lines for individual SARS-CoV-2 genes, and used the Gal4-UAS system to express these viral genes in Drosophila to study their pathogenicity. Results We found that the ubiquitous expression of Orf6, Nsp6 or Orf7a in Drosophila led to reduced viability and tissue defects, including reduced trachea branching as well as muscle deficits resulting in a “held-up” wing phenotype and poor climbing ability. Furthermore, muscles in these flies showed dramatically reduced mitochondria. Since Orf6 was found to interact with nucleopore proteins XPO1, we tested Selinexor, a drug that inhibits XPO1, and found that it could attenuate the Orf6-induced lethality and tissue-specific phenotypes\xa0observed in flies. Conclusions Our study established Drosophila as a model for studying the function of SARS-CoV2 genes, identified Orf6 as a highly pathogenic protein in various tissues, and demonstrated the potential of Selinexor for inhibiting Orf6 toxicity using an in vivo animal model system.', 'corpus_id': 232364190, 'score': 0}, {'doc_id': '233202919', 'title': 'Discoveries and biological implications of mammalian 45S rDNA variants and non-structural rRNAs', 'abstract': 'Repetitive nature of the ribosomal DNA (rDNA) gene makes the sequencing of hundreds copies of mammalian 45S rDNA (about 45-kb per copy) extremely difficult and its assembly is often excluded. Increasing evidence shows that 45S rDNA variations (copy number or single nucleotide), structural ribosomal RNA (rRNA) transcript variants, and non-structural rRNA transcripts (sense and anti-sense long noncoding rRNAs that include promoter rRNAs, and rRNA-derived fragments) play essential roles in mammalian development and diseases. Complete pictures of the hundreds copies of 45S rDNA and their rRNA transcripts require further innovation in sequencing techniques that include bioinformatics. The advancements in mammalian rDNA and rRNA sequencings and the discoveries of novel functions of the rDNA variants and rRNA transcripts are', 'corpus_id': 233202919, 'score': 0}, {'doc_id': '233453612', 'title': 'Identification and survival analysis of tTA/tetO-CCKR-2 double transgenic mice', 'abstract': 'BACKGROUND: The inducible forebrain-specific cholecystokinin receptor-2 (CCKR-2) double transgenic (tTA/tetO-CCKR-2 tg, abbreviated as dtg) mice are an ideal model of anxiety-related diseases. However, there is still a lack of model identification and life related data OBJECTIVE: To identify the genomic DNA of the offspring and the specific expression of CCKR-2 transgene in the forebrain, and to analyze the survival probability of dtg mice. METHODS: α-CaMKII/tTA single transgenic mice and tetO-CCKR-2 single transgenic mice were cross-fertilized to construct a dtg mouse model. The genomic DNA was extracted from the tail of the offspring, and the genotypes were detected by PCR and agarose gel electrophoresis. Wild-type (WT) mice were used as controls. In situ hybridization was used to detect the expression of CCKR-2. Survival of dtg mice and WT mice (30 females and 30 males) was observed and recorded within 2 years. The study protocol was approved by the Experimental Animal Ethics Committee of Southwest Medical University, with an approval No. 20150068. RESULTS AND CONCLUSION: Agarose gel electrophoresis results showed the molecular weight of the PCR products of dtg mice was consistent with the expected target gene fragment. In situ hybridization results showed a strong signal of CCKR-2 was detected in the forebrain of dtg mice, but hardly present in the WT mice. The median survival time of dtg mice was 76 weeks in females and 77 weeks in males. The survival probability was decreased with age in dtg mice. The survival probability of WT mice was significantly better than that of dtg mice (P < 0.001). There was no significant sex difference between males and females of dtg mice (P=0.577). Therefore, the specific expression of CCKR-2 transgene in the forebrain can be identified using PCR amplification, genomic DNA extraction, agarose gel electrophoresis, and in situ hybridization. tTA/tetO-CCKR-2 double transgenic induction may shorten the survival time of mice, but no significant difference is observed between the females and males of dtg mice.', 'corpus_id': 233453612, 'score': 0}, {'doc_id': '3446023', 'title': 'Complete overview of protein-inactivating sequence variations in 36 sequenced mouse inbred strains', 'abstract': 'Significance We have developed a bioinformatics tool that allows us to compare the sequences of all protein-coding genes of 36 sequenced mouse inbred strains with the reference mouse strain C57BL/6J. We also provide an estimate of the effect on protein function of each deviant protein sequence and have built a searchable database of all these sequences, giving researchers the opportunity to search for abnormal alleles of any protein coding gene across these strains. The database makes the enormous richness of variant alleles present in these 36 inbred strains visible, accessible, and useful to the whole mouse research community. Mouse inbred strains remain essential in science. We have analyzed the publicly available genome sequences of 36 popular inbred strains and provide lists for each strain of protein-coding genes that acquired sequence variations that cause premature STOP codons, loss of STOP codons and single nucleotide polymorphisms, and short in-frame insertions and deletions. Our data give an overview of predicted defective proteins, including predicted impact scores, of all these strains compared with the reference mouse genome of C57BL/6J. These data can also be retrieved via a searchable website (mousepost.be) and allow a global, better interpretation of genetic background effects and a source of naturally defective alleles in these 36 sequenced classical and high-priority mouse inbred strains.', 'corpus_id': 3446023, 'score': 1}, {'doc_id': '7458913', 'title': 'A comparative phenotypic and genomic analysis of C57BL/6J and C57BL/6N mouse strains', 'abstract': 'BackgroundThe mouse inbred line C57BL/6J is widely used in mouse genetics and its genome has been incorporated into many genetic reference populations. More recently large initiatives such as the International Knockout Mouse Consortium (IKMC) are using the C57BL/6N mouse strain to generate null alleles for all mouse genes. Hence both strains are now widely used in mouse genetics studies. Here we perform a comprehensive genomic and phenotypic analysis of the two strains to identify differences that may influence their underlying genetic mechanisms.ResultsWe undertake genome sequence comparisons of C57BL/6J and C57BL/6N to identify SNPs, indels and structural variants, with a focus on identifying all coding variants. We annotate 34 SNPs and 2 indels that distinguish C57BL/6J and C57BL/6N coding sequences, as well as 15 structural variants that overlap a gene. In parallel we assess the comparative phenotypes of the two inbred lines utilizing the EMPReSSslim phenotyping pipeline, a broad based assessment encompassing diverse biological systems. We perform additional secondary phenotyping assessments to explore other phenotype domains and to elaborate phenotype differences identified in the primary assessment. We uncover significant phenotypic differences between the two lines, replicated across multiple centers, in a number of physiological, biochemical and behavioral systems.ConclusionsComparison of C57BL/6J and C57BL/6N demonstrates a range of phenotypic differences that have the potential to impact upon penetrance and expressivity of mutational effects in these strains. Moreover, the sequence variants we identify provide a set of candidate genes for the phenotypic differences observed between the two strains.', 'corpus_id': 7458913, 'score': 1}, {'doc_id': '232048785', 'title': 'Advancement of chromosome science in the genomics era', 'abstract': 'In 1946, an eminent plant cytogeneticist Hitoshi Kihara coined an aphorism “The history of the earth is recorded in the layers of its crust; the history of all organisms is inscribed in the chromosomes” (Crow 1994). We are all aware that chromosomes are carriers of genetic materials that pass the exact genetic information to the next generations. Chromosomes are linear structures with genes located along the specific sites of the chromosomes. However, chromosomes are stuffed with highly non-genic repetitive sequences so that only small fractions of the genomes encode proteins. Thus chromosomes are highly dynamic structures that ensure the transfer of intact genetic materials to next generations as well as regulating the expression of genes in the ocean of repetitive DNA sequences. The wealth genome sequences enhance our understanding of the chromosome dynamics. There was a conference of the 7th AsianPacific Chromosome Colloquium in November 26–28, 2020, in Pusan, Korea. The theme was “Advancement of chromosome science in the genomics era”. The conference was held virtual online in both oral and poster presentation due to the COVID-19 pandemic. There were five sessions; Chromosome structure, Sex chromosomes and B-chromosomes, Chromosomes and evolution, genomics and chromosomes, and epigenomics and chromosomes. Of the 26 speakers in the symposium, ten speakers summarized their presentations and submitted their articles to this special issues. The followings are summaries of the articles. Tada et al. (2021) demonstrated the evidence for divergence of DNA methylation maintenance and a conserved inhibitory mechanism from DNA demethylation in chickens and mammals. The role of DNA methylation is wellknown in epigenetic gene regulation which is evolutionary conserved from single cell yeasts to mammals. The levels and patterns of DNA methylation are regulated by a balance of antagonistic enzyme functions, DNA methyltransferases DNMT1/3A/3B and methylcytosine dioxygenases TET1/2/3. In mice, TET enzyme mediates the conversion of cytosine methylation (5mC) to 5-hydroxymethylcytosine and initiation of global loss of 5mC at the beginning of fertilization until gastrulation stage. However, the 5mC level increased when the cell differentiations occur during early embryonic development. The authors checked the global loss and gain of DNA methylation whether different regulation exists in diverged species such as chicken and mammals. The results revealed that chicken and mammals shared a common chromatin-based regulation of TET-DNA success, but chicken DNMT1 was involved in different target sequence recognition systems, suggesting that factors inducing DNMT-DNA association had already diverged between egg laying bird species and placenta mammalian species. Epigenetic regulation is a fundamental mechanism in eukaryotic gene expression. Tam and Leung (2021) reviewed current states of single-cell transcriptomic and epigenomic studies in understanding the complex pathologies of chronic inflammatory diseases (CIDs). CIDs are a diverse class of autoimmune diseases characterized by dysregulated and sustained immune responses. Approximately 60% of Americans suffer from some form of CIDs and complications relating to these diseases are significant causes of death. However, exact triggers and mechanisms underlying many CIDs still remain elusive. The authors reviewed the current understanding of an association between epigenomic and transcriptomic dysregulation and the phenotypes of CIDs, specially discussed in depth on the epigenetic changes at cis-regulatory elements (CREs). Dysregulation of transposable elements (TEs) could serve as CREs or trigger molecular mimicry in CIDs. TEs constitute about 45% of human genome and a number of reports have described the aberrant expression of retrotransposons in CIDs. For CID characterization and diagnosis, the recent advancements of sequencing technologies (i.e., single-cell genomics) have generated many useful data in understanding the etiology of these complex diseases. The authors provided a most updated summary table Online ISSN 2092-9293 Print ISSN 1976-9571', 'corpus_id': 232048785, 'score': 0}, {'doc_id': '18790816', 'title': 'Development of SNP markers for C57BL/6N-derived mouse inbred strains', 'abstract': 'C57BL/6N inbred mice are used as the genetic background for producing knockout mice in large-scale projects worldwide; however, the genetic divergence among C57BL/6N-derived substrains has not been verified. Here, we identified novel single nucleotide polymorphisms (SNPs) specific to the C57BL/6NJ strain and selected useful SNPs for the genetic monitoring of C57BL/6N-derived substrains. Informative SNPs were selected from the public SNP database at the Wellcome Trust Sanger Institute by comparing sequence data from C57BL/6NJ and C57BL/6J mice. A total of 1,361 candidate SNPs from the SNP database could distinguish the C57BL/6NJ strain from 12 other inbred strains. We confirmed 277 C57BL/6NJ-specific SNPs including 10 nonsynonymous SNPs by direct sequencing, and selected 100 useful SNPs that cover all of the chromosomes except Y. Genotyping of 11 C57BL/6N-derived substrains at these 100 SNP loci demonstrated genetic differences among the substrains. This information will be useful for accurate genetic monitoring of mouse strains with a C57BL/6N-derived background.', 'corpus_id': 18790816, 'score': 1}]
186	"{'doc_id': '2264085', 'title': 'CorMet: A Computational, Corpus-Based Conventional Metaphor Extraction System', 'abstract': ""CorMet is a corpus-based system for discovering metaphorical mappings between concepts. It does this by finding systematic variations in domain-specific selectional preferences, which are inferred from large, dynamically mined Internet corpora. Metaphors transfer structure from a source domain to a target domain, making some concepts in the target domain metaphorically equivalent to concepts in the source domain. The verbs that select for a concept in the source domain tend to select for its metaphorical equivalent in the target domain. This regularity, detectable with a shallow linguistic analysis, is used to find the metaphorical interconcept mappings, which can then be used to infer the existence of higher-level conventional metaphors. Most other computational metaphor systems use small, hand-coded semantic knowledge bases and work on a few examples. Although Cor Met's only knowledge base is Word Net (Fellbaum 1998) it can find the mappings constituting many conventional metaphors and in some cases recognize sentences instantiating those mappings. CorMet is tested on its ability to find a subset of the Master Metaphor List (Lakoff, Espenson, and Schwartz 1991)."", 'corpus_id': 2264085}"	4729	"[{'doc_id': '208011706', 'title': 'Spatial Frames of Reference in Miyako: Digging into Whorfian linguistic relativity', 'abstract': None, 'corpus_id': 208011706, 'score': 0}, {'doc_id': '192413280', 'title': 'More Than Cool Reason: A Field Guide to Poetic Metaphor', 'abstract': None, 'corpus_id': 192413280, 'score': 1}, {'doc_id': '219424691', 'title': 'Patient Data-Sharing for AI: Ethical Challenges, Catholic Solutions', 'abstract': 'Recent news of Catholic and secular healthcare systems sharing electronic health record (EHR) data with technology companies for the purposes of developing artificial intelligence (AI) applications has drawn attention to the ethical and social challenges of such collaborations, including threats to patient privacy and confidentiality, undermining of patient consent, and lack of corporate transparency. Although the United States Catholic Conference of Bishops’ Ethical and Religious Directives for Health Care Services (ERDs) address collaborations between US Catholic healthcare providers and other entities, the ERDs do not adequately address the novel concerns seen in EHR data-sharing for AI development. Neither does the Health Insurance Portability and Accountability Act (HIPAA) privacy rule. This article describes ethical and social problems observed in recent patient data-sharing collaborations with AI companies and analyzes them in light of the guiding principles of the ERDs as well as the 2020 Rome Call to AI Ethics (RCAIE) document recently released by the Vatican. While both the ERDs and RCAIE guiding principles can inform future collaborations, we suggest that the next revision of the ERDs should consider addressing data-sharing and AI more directly. Summary: Electronic health record data-sharing with artificial intelligence developers presents unique ethical and social challenges that can be addressed with updated United States Catholic Conference of Bishops’ Ethical and Religious Directives and guidance from the Vatican’s 2020 Rome Call to AI Ethics.', 'corpus_id': 219424691, 'score': 0}, {'doc_id': '237204793', 'title': 'On a Chomskyan postulation in conceptual metaphor theory', 'abstract': 'Abstract This paper is an attempt to make a comparison between Lakoff and Johnson’s conceptual metaphor theory and Chomsky’s transformational generative grammar, and to demonstrate a Chomskyan postulation in the former. Although Lakoff and Johnson regard Chomsky’s linguistics as a modern representative of traditional Western philosophies of language that tend to highlight the a priori assumptions rather than empirical findings, the cognitive theory of metaphor contains a Chomskyan metaphysical assumption as its most important notion, i.e. the assumption of conceptual metaphors. Thus, what the present paper wants to argue with ample evidence is that Lakoff and Johnson’s conceptual metaphor theory resembles Chomsky’s logic and that their notion of conceptual metaphors is very much a Chomskyan postulation. What the present study tries to further demonstrate is that the abovementioned two theories actually have many points in common, which also implies that Lakoff and Johnson have failed to avoid the paradigm that they believe is conflicting with their own.', 'corpus_id': 237204793, 'score': 1}, {'doc_id': '195006290', 'title': 'Metaphors We Live By', 'abstract': 'People use metaphors every time they speak. Some of those metaphors are literary - devices for making thoughts more vivid or entertaining. But most are much more basic than that - they\'re ""metaphors we live by"", metaphors we use without even realizing we\'re using them. In this book, George Lakoff and Mark Johnson suggest that these basic metaphors not only affect the way we communicate ideas, but actually structure our perceptions and understandings from the beginning. Bringing together the perspectives of linguistics and philosophy, Lakoff and Johnson offer an intriguing and surprising guide to some of the most common metaphors and what they can tell us about the human mind. And for this new edition, they supply an afterword both extending their arguments and offering a fascinating overview of the current state of thinking on the subject of the metaphor.', 'corpus_id': 195006290, 'score': 1}, {'doc_id': '235458124', 'title': 'WaveGrad 2: Iterative Refinement for Text-to-Speech Synthesis', 'abstract': 'This paper introduces WaveGrad 2, a non-autoregressive generative model for text-to-speech synthesis. WaveGrad 2 is trained to estimate the gradient of the log conditional density of the waveform given a phoneme sequence. The model takes an input phoneme sequence, and through an iterative refinement process, generates an audio waveform. This contrasts to the original WaveGrad vocoder which conditions on mel-spectrogram features, generated by a separate model. The iterative refinement process starts from Gaussian noise, and through a series of refinement steps (e.g., 50 steps), progressively recovers the audio sequence. WaveGrad 2 offers a natural way to trade-off between inference speed and sample quality, through adjusting the number of refinement steps. Experiments show that the model can generate high fidelity audio, approaching the performance of a state-of-the-art neural TTS system. We also report various ablation studies over different model configurations. Audio samples are available at https://wavegrad.github.io/v2.', 'corpus_id': 235458124, 'score': 1}, {'doc_id': '32708543', 'title': 'The NHS: Who is attacking, who is defending?', 'abstract': '7. Schensul, J. J. and Guest, B. H. (1994). Ethics, ethnicity, and health care reform. In, ""It Just Ain\'t Fair\'raThe Ethics of Health Care for African Americans, ed. by A. Dula and S. Goering, Praeger Publishers, Westport, CT. 8. Ferguson, W. (1994). The physician\'s responsibility to medically underserved poor people. In, \'It Just Ain\'t Fair\'--The Ethics of Health Care for African Americans, ed. by A. Dula and S. Goering, Praeger Publishers, Westport, CT. 9. Radical Health Statistics Group (1977). In Defence of the NHS, Radical Health Statistics Group, London. 10. Hahn, B. and Flood, A. B. (1995). No insurance, public insurance, and private insurance: do these options contribute to differences in general health? Journal of Health Care for the Poor and Underserved 6, 41-59. 11. Sorlie, P. D., Backlund, E. and Keller, J. B. (1995). USA mortality by economic, demographic, and social characteristics: the national longitudinal mortality study. American Journal of Public Health 85, 949-956. 12. Veatch, R. M. (1995). Abandoning informed consent. Hastings Center Report 25, 5-12.', 'corpus_id': 32708543, 'score': 0}, {'doc_id': '237432347', 'title': 'Book review', 'abstract': 'This volume, edited by Ewa Dąbrowska from University of Birmingham and FAU Erlangen-Nürnberg and Dagmar Divjak from University of Birmingham, is the part of a three-volume set on Cognitive Linguistics published by Walter de Gruyter in 2019. As the first volume of this set, it discusses the cognitive processes and abilities of human beings which underlie language production, particularly concerning such concepts as embodiment, attention, and categorization, providing a state-of-the-art overview of the subfields in linguistics. Authors in this collection specially emphasize the direction of cognitive linguistic studies towards a more empirical, interdisciplinary, and social-oriented basis, and provide readers with insightful ideas and suggestions for future research in Cognitive Linguistics. To begin with, in the Introduction, the editors briefly introduce the assumptions, history, and current situation of Cognitive Linguistics, and give an outline of the topics in the three-volume set. In chapter 1, Benjamin Bergen expounds the historical conceptions of embodiment in Cognitive Science, describes some of the ways that embodiment has been used in Cognitive Linguistics, and discusses the directions that linguistic embodiment research is currently moving towards. According to the author, there have been three distinct phases in the application of the idea of embodiment to empirical work on language and cognition, containing the analytical phase, the process phase and the function phase. For the next chapter, Russell S. Tomlin and Andriy Myachykov review the evidence for a regular link between visual attention and syntactic organization. They propose that the grammatical role assignment mechanism and the positional assignment mechanism form a hierarchical dual-path system, which allows a grammatical representation of the perceptually salient referent in a sentence. In chapter 3, Dagmar Divjak and Catherine L. Caldwell-Harris present interpretations of frequency and entrenchment, and integrate perspectives from both Experimental Psychology and Cognitive Linguistics. They illustrate the origins of the interest in frequency and its applications, and also the review of the cognitive and neural mechanisms supporting language structures that vary in entrenchment. Categorization is discussed by Michael Ramscar and Robert Port in chapter 4. They suggest that human conceptual capabilities are systematic in that they are the products of a rich capacity to discriminate and learn systems of alternate responses (behaviors, affordances, words, etc.) and to use the systems in context, with a conclusion that conceptual knowledge is closely related to context of language use. As for chapter 5, R. Harald Baayen and Michael Ramscar explain three approaches that attempt to answer the question about the mechanism of structuring language, placing more emphasis on the process of abstraction, analogical reasoning, and basic principles of discrimination learning. In chapter 6, Ronald W. Langacker characterizes construal with reasonable precision and investigates their reprensentations in language. Five broad dimensions of construal are scrutinized, namely perspective, selection, promi-', 'corpus_id': 237432347, 'score': 0}, {'doc_id': '237239695', 'title': 'How to tear down the walls that separate linguists: Continuing the quest for clarity about general linguistics', 'abstract': 'The primary motivation for my target paper (“General linguistics must be based on universals”) was a perceived need for greater clarity in general linguistics. Many linguists seem to share the feeling that we often talk past each other because we understand terms like “theory”, “framework”, “explanation”, “analysis” and “description” in seemingly different ways. But even if different linguists (necessarily) prefer different methods, we need not disagree about these basic terms and concepts, and I think that some “walls” that separate linguists from different communities could be “torn down” if we became more aware of what unites all theoretical linguists: that we want to understand particular languages, and that we need them in order to understand Human Language in general. I am grateful to the commentators for their interesting contributions, and here I continue the discussion by responding to some of their remarks. To recapitulate, I made three key points in my target article:', 'corpus_id': 237239695, 'score': 0}, {'doc_id': '232185202', 'title': 'MERMAID: Metaphor Generation with Symbolism and Discriminative Decoding', 'abstract': 'Generating metaphors is a challenging task as it requires a proper understanding of abstract concepts, making connections between unrelated concepts, and deviating from the literal meaning. In this paper, we aim to generate a metaphoric sentence given a literal expression by replacing relevant verbs. Based on a theoretically-grounded connection between metaphors and symbols, we propose a method to automatically construct a parallel corpus by transforming a large number of metaphorical sentences from the Gutenberg Poetry corpus (CITATION) to their literal counterpart using recent advances in masked language modeling coupled with commonsense inference. For the generation task, we incorporate a metaphor discriminator to guide the decoding of a sequence to sequence model fine-tuned on our parallel data to generate high-quality metaphors. Human evaluation on an independent test set of literal statements shows that our best model generates metaphors better than three well-crafted baselines 66% of the time on average. A task-based evaluation shows that human-written poems enhanced with metaphors proposed by our model are preferred 68% of the time compared to poems without metaphors.', 'corpus_id': 232185202, 'score': 1}]"
187	{'doc_id': '229827417', 'title': 'William Shakespeare: Selected Sonnets', 'abstract': None, 'corpus_id': 229827417}	7386	"[{'doc_id': '221066646', 'title': 'Everyday Revolutions: Remaking Gender, Sexuality and Culture in 1970s Australia', 'abstract': 'errors of fact, at least in areas known to this reviewer. All in all the work offers a well-paced read, touching upon most well-documented cases of collaboration between European natural historians and Aboriginal collectors. I particularly enjoyed the account of George Caley and Moowattin in the Sydney region. John Gould’s empathetic interactions with Aboriginal helpers in bird and mammal studies stood out, as did the extensive expeditions of John MacGillivray with Neinmal and Paida. Central Australian work is also well covered, with accounts of the Horn Expedition, Spencer and Gillen, and Hedley Finlayson. The co-authors reveal well-known and new aspects of cross-cultural interactions on natural history expeditions. The moving demise of so many mammals, more than on any other continent over the same historical period, remains a matter of continuing regret for Australians. In the book, Gerard Krefft’s 1857 evocative illustration of a living pigfooted bandicoot from the Murray River near Mondellimin is a particularly potent reminder of what has been lost. Have the co-authors achieved their aims for the book? In general terms, yes. An overview of Aboriginal collaboration with natural history collectors in parts of Australia is undoubtedly on offer. Yet, for this reader, the clear aim of continental breadth in some ways diminishes the story of a rich history incapable of being squeezed into 189 pages. Some myths are perpetuated. For example, in the epilogue, the only map in the book, of Indigenous Protected Areas (IPA) and Working on Country ranger groups, is blank for vast areas of the continent, reflecting political realities and funding allocations. Central Australia, the western deserts and the Top End are where the biggest IPAs were found as at 2017. There is not a single entry in most of inland Queensland, in the Pilbara in northwestern Australia, nor in the southwest Noongar country where I work. Indeed, most of the southern half of Australia is bedecked with just a small scattering of tiny dots on this map. The reader might suppose that these vast regions are bereft of contemporary Aboriginal people interested in forming ranger groups or creating IPAs. Nothing is further from the truth. These are heartlands where cross-cultural interactions today are fraught due to extensive economic interests in farming and mining. Perhaps the authors could have highlighted this situation? It is ambitious these days to attempt to summarise major topics for the 250+ language groups of Indigenous Australians. If ever there was a time for localised histories, emphasising the uniqueness of bands and clans, we are in it now. Hopefully, this book will inspire readers to further investigations in their local area. To engage with Elders, respectfully, listen to their stories, hear and understand their language, and walk on country with them are life-changing experiences along the path towards reconciliation. I commend this book as an entry point. The depth and breadth of the full story is beyond its covers, but awaits those who wish to follow its lead and dig deeper.', 'corpus_id': 221066646, 'score': 0}, {'doc_id': '191490013', 'title': 'Will in the Sonnets', 'abstract': ""This essay suggests the Sonnets continues the sceptical treatment of love characteristic of earlier Elizabethan verse in which passion and its objects are frequently presented as deceptive and treacherous, aspects of a disordered will. Differing radically from the idealizing sonnets of the 1590s, Shakespeare's sequence is nevertheless attentive to the model of Astrophil and Stella. Like Sidney, Shakespeare is interested in creating a life-like voice for his poet/lover, lending him semi-autobiographical touches, and like Sidney he is interested in both man's “gift to discern beauty” and the depredations of the “infected will”. The essay argues the dark lady's kinship to the betraying Cressidas of earlier Elizabethan verse, and that the ideals embodied in the young man are repeatedly undermined. The poet/lover himself is positioned as a social outsider, an embodiment of the infected will, yearning for, yet sceptical of, the idealized perfection of the aristocratic young man."", 'corpus_id': 191490013, 'score': 1}, {'doc_id': '60294400', 'title': 'The role of similarity in similes and metaphors', 'abstract': 'The chapter by Alan Paivio and Mary Walsh is like a tool kit; it provides the potential for undertaking a serious examination of our problem. The question that now has to be asked is whether it contains the right tools. I suspect that as with most tool kits, some of the tools are useful for the problem at hand and some are not. In this chapter, I shall concentrate on the tool that I believe to be the most important, namely, similarity . I shall also discuss briefly two other issues raised by Paivio and Walsh, namely, integration and relation. Paivio and Walsh argue that the central question surrounding the comprehension (and the production) of metaphors concerns the way in which a novel conception arises from apparently disparate parts. This question, they claim, involves three important concepts, namely those of integration, relation, and similarity. Similarity is involved because the two terms in a metaphor share attributes. Relation is implicated, because a metaphor may take advantage of common relations, and also because of its involvement in integration. Integration is significant because of the emergence of something new, presumably a result of integrating certain aspects of the parts. As I have said, I think that the most important of these three concepts is that of similarity. For that reason, I shall devote most of my attention to the role of similarity in metaphors, and especially in similes.', 'corpus_id': 60294400, 'score': 0}, {'doc_id': '220058388', 'title': 'Owning reality: Reflections on Cultism and Zealotry', 'abstract': 'I am moved by the generous introduction we have just heard, but know that I must resist the temptation to believe it. My way of doing that is to turn to an avocation of mine, the creation of ostensibly humorous bird cartoons. So I would like to respond with what I consider my existential classic: A small, enthusiastic bird looks up and says, excitedly, “All of a sudden I had this wonderful feeling, I am me!” And an older, bigger, more jaundiced bird looks down at him and says, “You were wrong.” I’m delighted to be speaking to you, to this group, at this time, in this place. What I would like to do this morning is to take you along a few paths of my work, including newer reflections that connect with our agitated and dangerous world. My title, “Owning Reality,” is my organizing principle. The title suggests not only the predatory behavior I have studied but my active opposition to that behavior. That opposition is in keeping with a long-standing effort to combine scholarship and activism. In some traditions it is held that scholarship and activism must remain entirely separate, and ne’er the twain shall meet. I think the opposite is true. Scholarship is rendered meaningful by activism, and activism is significantly informed by scholarship. Both need to be undertaken with discipline.', 'corpus_id': 220058388, 'score': 0}, {'doc_id': '194423678', 'title': 'Apophatic rhetoric in Shakespeare’s Rival Poet sonnets', 'abstract': 'This article looks at Shakespeare’s paradoxical attitude to rhetoric in the sonnets, more specifically, the Rival Poet sonnets (78–80 and 82–86). Through close reading, it identifies an apophatic resistance to rhetoric and a simultaneous inherent rhetoricity in Shakespeare’s work. Apophasis is a rhetorical figure through which a speaker pretends to conceal that which he actually shows, or denies that he says or does that which he says or does. Apophasis is a self-reflexive, dynamic and ironic device, and works by inversion, highlighting that which it would seem to downplay, including the rhetorical effects of the writing itself.', 'corpus_id': 194423678, 'score': 1}, {'doc_id': '160512132', 'title': 'William Shakespeare - Sonnets', 'abstract': None, 'corpus_id': 160512132, 'score': 1}, {'doc_id': '36994808', 'title': 'Metaphor as categorisation : a connectionist implementation', 'abstract': 'A key issue for models of metaphor comprehension is to explain how in some metaphorical comparison <A is B>, only some features of B are transferred to A. The features of B that are transferred to A depend both on A and on B. This is the central thrust of Black\'s well known interaction theory of metaphor comprehension (1979). However, this theory is somewhat abstract, and it is not obvious how it may be implemented in terms of mental representations and processes. In this paper we describe a simple computational model of on-line metaphor comprehension which combines Black\'s interaction theory with the idea that metaphor comprehension is a type of categorisation process (Glucksberg & Keysar, 1990, 1993). The model is based on a distributed connectionist network depicting semantic memory (McClelland & Rumelhart, 1986). The network learns feature-based information about various concepts. A metaphor is comprehended by applying a representation of the first term A to the network storing knowledge of the second term B, in an attempt to categorise it as an exemplar of B. The output of this network is a representation of A transformed by the knowledge of B. We explain how this process embodies an interaction of knowledge between the two terms of the metaphor, how it accords with the contemporary theory of metaphor stating that comprehension for literal and metaphorical comparisons is carried out by identical mechanisms (Gibbs, 1994), and how it accounts for both existing empirical evidence (Glucksberg, McGlone, & Manfredi, 1997) and generates new predictions. In this model, the distinction between literal and metaphorical language is one of degree, not of kind. Metaphor as categorisation 4 Why use metaphor in language? Gibbs (1994) summarises three kinds of answers to this question (Fainsilber & Ortony, 1987; Ortony, 1975). First, the inexpressibility hypothesis suggests that metaphors allow us to express ideas that we cannot easily express using literal language. Second, the compactness hypothesis suggests that metaphors allow the communication of complex configurations of information to capture the richness of a particular experience. The use of literal language to communicate the same meaning would be cumbersome and inefficient. Third, the vividness hypothesis suggests that the ideas communicable via a metaphor are in fact richer than those we may achieve using literal language. When we receive information coded in the form of a metaphor (e.g., not that Richard is brave, aggressive, and so on, but that Richard is a lion), how do we process such language to extract its vivid meaning? The traditional view in philosophy and linguistics was that language comprehension and production are built around literal language, that metaphorical language is both harder to comprehend (given that it is literally false – in our example, Richard is not a lion) and requires special processing mechanisms to decode. Although it is distinguished by its communicative advantages, metaphor was seen as a purely linguistic phenomenon (Grice, 1975; Searle, 1975). More recently, this view has been challenged on two grounds (e.g. Gibbs, 1994, 1996; Lakoff, 1993). First, it is claimed that metaphor is conceptual rather than linguistic. Second, it is claimed that metaphor is not an add-on to the more primary literal language processing system, but a key aspect of language itself, sharing the same kind of processing mechanisms. In this paper, we will be focussing on the second of these claims. The argument that metaphor comprehension does not require special processing mechanisms has two strands (Gibbs & Gerrig, 1989). The first of these is Metaphor as categorisation 5 that on-line processing studies suggest that (with appropriate contextual support) metaphors and literal statements take the same amount of time to process (e.g. Inhoff, Lima, & Carroll, 1984; Ortony, Schallert, Reynolds, & Antos, 1978). This seems to rule out the possibility that metaphors are initially processed as literal statements, found to be false, and only then processed by metaphor-specific mechanisms. It does not, however, rule out the possibility that literal and metaphorical meanings of a sentence may be computed simultaneously and in parallel by separate mechanisms. The second strand suggests that literal language processing is no easier than metaphorical processing, given that both rely on a ""common ground"" between speaker and listener to comprehend what a given utterance means (Gibbs, 1994). That is, an apparently literal statement may well have an implicated meaning given a certain set of shared assumptions between speaker and listener. If both types of language involve similar problems, it makes sense to see them as engaging the same sort of mechanisms. Black (1955, 1962, 1979) outlined three views of how the metaphor comprehension process may work. In the first of these, the substitution view, to understand the metaphorical comparison Richard is a lion, this comparison must initially be replaced by a set of literal propositions that fit the same context, e.g. Richard is brave, Richard is aggressive. In the comparison view, the metaphor is taken to imply that the two terms are similar to each other in certain (communicatively relevant) respects. For example, both Richard and the lion are brave, aggressive, and so forth. The intention of the comparison is to highlight these properties in the first term Richard. In effect, the comparison is shorthand for the simile Richard is like a lion. In the interactive view, the comparison of the two terms in the metaphor is not taken to emphasise pre-existing similarities between them, but itself plays a role in Metaphor as categorisation 6 creating that similarity. The topic (first term) and vehicle (second term) interact such that the topic itself causes the selection of certain of the features of the vehicle, which may then be used in the comparison with the topic. In turn, this “parallel implication complex” may cause changes in our understanding of the vehicle in the comparison. Although the interaction view has been described as “the dominant theory in the multidisciplinary study of metaphor” (Gibbs, 1994, p. 234), it has nevertheless been criticised for the vagueness of its central terms (ibid., p. 235). One of the key issues for psycholinguistic models of metaphor comprehension is to explain the nature of the interaction between topic and vehicle that constrains the emergent meaning of the comparison. Three main models have been proposed. These are the salience imbalance model (Ortony, 1979), the structural mapping model (Gentner, 1983; Gentner & Clements, 1988), and the class inclusion model (Glucksberg & Keysar, 1990, 1993). The salience imbalance model proposes that metaphors are similarity statements whose two terms share attributes. However, the salience of these attributes is much higher in the second term than the first. The comparison serves to emphasise these attributes in the first term. The structural mapping model suggests that topic and vehicle can be matched in three ways: in terms of their relational structure (that is, in the hierarchical organisation of their properties and attributes); in terms of those properties themselves; or in terms of both relational structure and properties. People tend to show a preference for relational mappings in metaphors. Lastly, the class inclusion model proposes that metaphors are understood as categorical assertions. In a metaphor <A is B>, A is assigned to a category denoted by B (that is, Richard falls into the class of brave, aggressive things whose prototypical member is lions). Only those categories of which B is a member that could also plausibly contain A are considered as the intended meaning of the categorical assertion. Metaphor as categorisation 7 The view of metaphor as a form of categorisation seems perhaps most consistent with the claim that metaphor comprehension requires no special processes over and above literal comprehension. Both the salience imbalance model and the structural mapping model imply a property matching procedure which is engaged for non-literal comparisons (Glucksberg, McGlone, & Manfredi, 1997). Moreover, Glucksberg et al. (1997) have argued that the class inclusion theory is empirically distinguishable from these property matching models. Although literal comparisons are asymmetric (in that the similarity of two terms can be rated differently depending on the order of presentation; e.g. Tversky & Gati, 1982), class inclusion statements should be more than asymmetric, they should be non-reversible. The lion is Richard should make very much less sense than Richard is a lion, unless Richard happens to be a prototypical member of a category of which lion could also be a member. Secondly, Glucksberg et al. claimed that the topic and vehicle should make very different (though interactive) contributions to the metaphor’s meaning, and that these contributions are predictable. While the vehicle provides the properties that may be attributed to the topic, the listener’s familiarity with the topic constrains those properties that may be attributed to it. Glucksberg et al. primed comprehension of metaphorical comparisons by pre-exposure to either topic or vehicle. They predicted that only comparisons involving topics with few potentially relevant attributes, or vehicles with few properties available as candidate attributes, should benefit from preexposure. In their view, neither property-matching model should predict the nonreversibility or specific interactivity effects. Nevertheless, Glucksberg et al. (1997) found empirical support for both of their predictions. The class inclusion model contrasts with Lakoff and colleague’s theory that metaphors rely on established mappings between pairs of domains in long term Metaphor as categorisation 8 memory (Lakoff, 1987, 1990, 1993; Lakoff & Johnson, 1980; Lakoff & Turner, 1989). Thus comprehension of th', 'corpus_id': 36994808, 'score': 0}, {'doc_id': '194024496', 'title': 'Time and Immortality in William Shakespeare’s Sonnets', 'abstract': None, 'corpus_id': 194024496, 'score': 1}, {'doc_id': '160933289', 'title': 'The Complete Sonnets of William Shakespeare', 'abstract': None, 'corpus_id': 160933289, 'score': 1}, {'doc_id': '221088752', 'title': 'The Vienna Circle’s reception of Nietzsche', 'abstract': 'Friedrich Nietzsche was among the figures from the history of nineteenth century philosophy that, perhaps surprisingly, some of the Vienna Circle’s members had presented as one of their predecessors. While, primarily for political reasons, most Anglophone figures in the history of analytic philosophy had taken a dim view of Nietzsche, the Vienna Circle’s leader Moritz Schlick admired and praised Nietzsche, rejecting what he saw as a misinterpretation of Nietzsche as a militarist or proto-fascist. Schlick, Frank, Neurath, and Carnap were in different ways committed to the view that Nietzsche made a significant contribution to the overcoming of metaphysics. Some of these philosophers praised the intimate connection Nietzsche drew between his philosophical outlook and empirical studies in psychology and physiology. In his 1912 lectures on Nietzsche, Schlick maintained that Nietzsche overcame an initial Schopenhauerian metaphysical-artistic phase in his thinking, and subsequently remained a positivist until his last writings. Frank and Neurath made the weaker claim that Nietzsche contributed to the development of a positivistic or scientific conception of the world. Schlick and Frank took a further step in seeing the mature Nietzsche as an Enlightenment thinker.', 'corpus_id': 221088752, 'score': 0}]"
188	{'doc_id': '138027702', 'title': 'Microstructural and mechanical properties of cBN-Si composites prepared from the high pressure infiltration method', 'abstract': 'Abstract The grain size dependence of the mechanical properties of cBN-Si composites prepared using the high pressure infiltration method has been investigated. Indentation testing indicates that cBN-Si composites have hardness values of 38–43\xa0GPa, which increase with increasing grain size and are harder than traditional polycrystalline cBN composites (PcBNs). Thermostability analyses display that cBN-Si composites with a grain size of >\xa09\xa0μm also possess a higher temperature of oxidation, compared to traditional PcBNs, and the thermostability increases with increasing cBN grain size. Fracture toughness tests show that almost no cracks appear on the polished cBN-Si samples when the loading forces are increased to 294\xa0N and the fracture toughness is better than for commercial samples. Scanning electron microscopy illustrates that deformations and close pores occurred easily between coarse BN grains, leading to denser cBN-Si compacts with better mechanical performances.', 'corpus_id': 138027702}	14721	"[{'doc_id': '139976283', 'title': 'Superhard pcBN materials with chromium compounds as a binder', 'abstract': 'Superhard cBN-based materials with Cr3C2, Cr2N and CrB2 binders were sintered in a high-pressure high-temperature (HPHT) toroidal apparatus under a pressure of 7.7 GPa and in the temperature range of 1600-2450°С. Initial mixtures of three compositions were chosen with 60 vol.% of cBN, 35 vol.% of binder phase and 5 vol.% of Al. Phase composition and microstructure of sintered samples were investigated by X-ray analysis and scanning electron microscopy, respectively. Elastic properties were measured using the ultrasonic pulse-echo technique. Composites with Cr3C2 and CrB2 binders sintered at 2000°C have the highest values of hardness.', 'corpus_id': 139976283, 'score': 1}, {'doc_id': '4419843', 'title': 'Ultrahard nanotwinned cubic boron nitride', 'abstract': 'Cubic boron nitride (cBN) is a well known superhard material that has a wide range of industrial applications. Nanostructuring of cBN is an effective way to improve its hardness by virtue of the Hall–Petch effect—the tendency for hardness to increase with decreasing grain size. Polycrystalline cBN materials are often synthesized by using the martensitic transformation of a graphite-like BN precursor, in which high pressures and temperatures lead to puckering of the BN layers. Such approaches have led to synthetic polycrystalline cBN having grain sizes as small as ∼14\u2009nm (refs 1, 2, 4, 5). Here we report the formation of cBN with a nanostructure dominated by fine twin domains of average thickness ∼3.8\u2009nm. This nanotwinned cBN was synthesized from specially prepared BN precursor nanoparticles possessing onion-like nested structures with intrinsically puckered BN layers and numerous stacking faults. The resulting nanotwinned cBN bulk samples are optically transparent with a striking combination of physical properties: an extremely high Vickers hardness (exceeding 100\u2009GPa, the optimal hardness of synthetic diamond), a high oxidization temperature (∼1,294\u2009°C) and a large fracture toughness (>12\u2009MPa\u2009m1/2, well beyond the toughness of commercial cemented tungsten carbide, ∼10\u2009MPa\u2009m1/2). We show that hardening of cBN is continuous with decreasing twin thickness down to the smallest sizes investigated, contrasting with the expected reverse Hall–Petch effect below a critical grain size or the twin thickness of ∼10–15\u2009nm found in metals and alloys.', 'corpus_id': 4419843, 'score': 1}, {'doc_id': '231572986', 'title': 'Processing of Nanostructured Bulk Fe-Cr Alloys by Severe Plastic Deformation', 'abstract': 'The processing of binary alloys consisting of ferromagnetic Fe and antiferromagnetic Cr by severe plastic deformation (SPD) with different chemical compositions has been investigated. Although the phase diagram exhibits a large gap in the thermodynamical equilibrium at lower temperatures, it is shown that techniques based on SPD help to overcome common processing limits. Different processing routes including initial ball milling (BM) and arc melting (AM) and a concatenation with annealing treatments prior to high-pressure torsion (HPT) deformation are compared in this work. Investigation of the deformed microstructures by electron microscopy and synchrotron X-ray diffraction reveal homogeneous, nanocrystalline microstructures for HPT deformed AM alloys. HPT deformation of powder blends and BM powders leads to an exorbitant increase in hardness or an unusual fast formation of a σ-phase and therefore impede successful processing.', 'corpus_id': 231572986, 'score': 0}, {'doc_id': '784071', 'title': 'The value of diagnostic whole-body scanning and serum thyroglobulin in the presence of elevated serum thyrotropin during follow-up of anti-thyroglobulin antibody-positive patients with differentiated thyroid carcinoma who appeared to be free of disease after total thyroidectomy and radioactive iodin', 'abstract': 'BACKGROUND\nIn the presence of anti-thyroglobulin antibodies (TgAb), serum thyroglobulin (Tg) might be underestimated. Therefore, the American Thyroid Association does not recommend serum Tg after thyroid hormone withdrawal or recombinant human thyrotropin administration (stimulated Tg) and diagnostic whole-body scanning (DxWBS) in TgAb-positive patients who have serum Tg values while on thyroxine (Tg-on-T4) of <1\u2009ng/mL. The objective of this study was to determine, in patients with differentiated thyroid cancer (DTC) who appeared to be free of disease after surgery and ablative treatment, but who had positive serum TgAb, the value of performing DxWBS and obtaining serum Tg under stimulated Tg conditions.\n\n\nMETHODS\nThere were 121 women and 15 men in the study. By selection criteria, all of them had total thyroidectomy with apparent complete tumor resection, remnant ablation with (131)I (1.1-5.5\u2009GBq), and a post-(131)I therapy WBS that were negative for ectopic (131)I uptake. On assessment 8-12 months after (131)I ablation, their clinical exam needed to be normal, their Tg-on-T4 needed to be <1\u2009ng/mL, and the test for TgAb needed to be positive. Stimulated Tg, neck ultrasound (US), and DxWBS were obtained from all patients. Patients with stimulated Tg >1\u2009ng/mL without disease on US and DxWBS were evaluated by other imaging methods.\n\n\nRESULTS\nIn 10 (7.3%) patients, stimulated Tg was >1\u2009ng/mL. The DxWBS revealed metastases in two of these patients, and other imaging methods showed disease in three others. Stimulated Tg was <1\u2009ng/mL in 126 patients. DxWBS revealed metastases in three of these patients, and US detected lymph node metastases in four with a negative DxWBS. Tg stimulation combined with DxWBS revealed evidence for disease in 13 (9.5%) patients. When excluding patients with a positive US, DxWBS revealed metastases in four patients, and stimulated Tg of >1\u2009ng/mL led to detection of persistent disease by other imaging methods in two more patients.\n\n\nCONCLUSIONS\nPerforming stimulated Tg and DxWBS at the same time seems to be useful after initial therapy in DTC patients with TgAb who do not otherwise appear to have persistent disease, even when US is negative.', 'corpus_id': 784071, 'score': 0}, {'doc_id': '224939867', 'title': 'Fragmentation and stress response characteristics of cubic boron nitride under high pressure', 'abstract': 'Abstract Exploring the fragmentation behavior and stress response of cubic boron nitride (cBN) particles under high pressure offers insights into preparing polycrystalline cubic boron nitride (PcBN) with excellent properties. In this study, the distribution characteristics and micro-morphology of cBN particles with different sizes (2\xa0μm, 15\xa0μm and 40\xa0μm) with and without aluminum (Al) binder are investigated using scanning electron microscopy (SEM) and laser particle size distribution instrument after cold compression at pressures up to 10.0\xa0GPa. The results demonstrate that the fragmentation of the cBN particles without Al is more evident than that with Al. Moreover, the stress distribution among cBN grains is obtained via in-situ high-pressure synchrotron radiation X-ray diffraction experiments with diamond anvil cell (DAC). The results indicate that the stress difference in the cBN grains between the high-pressure and low-pressure zones is approximately 6.0\xa0GPa under 4.2\xa0GPa loading, while the stress difference reached to 40.0\xa0GPa when compressed to 62.0\xa0GPa. The main reason for the cBN particle breakage is the stress difference caused by mutual extrusion among particles under cold compression. These findings may provide a practical guide for preparing PcBN with excellent performance using high pressure sintering methods.', 'corpus_id': 224939867, 'score': 1}, {'doc_id': '214271567', 'title': 'Ultrahard and stable nanostructured cubic boron nitride from hexagonal boron nitride', 'abstract': 'Abstract Cubic boron nitride (cBN), a typical superhard material, has garnered significant interest for both fundamental scientific research and technical applications. Here, nanostructured cBN with high-density nanotwins and stacking faults was synthesized by rapid quenching using hexagonal boron nitride (hBN) as the starting material at a moderate temperature (2200\xa0K) and low pressure (13\xa0GPa). This synthetic nanostructured cBN exhibits high Vickers hardness (78\xa0GPa), high fracture toughness (11.8\xa0MPa·m1/2), and extremely high oxidization temperature (1565\xa0K). We propose that the refined grain structure and excellent mechanical properties of the cBN synthesized in this study is attributed mainly to dislocation slips and (111) mechanical nanotwins that dominate this phase transformation.', 'corpus_id': 214271567, 'score': 1}, {'doc_id': '99189600', 'title': ""Etude cinétique de la réduction dans l'hydrogène d'oxydes spinelles de cobalt faiblement substitués par le chrome"", 'abstract': ""La reduction par l'hydrogene a 250 °C des oxydes spinelles Co 3-x Cr x O 4 (0 ≤ x ≤ 0,5) a ete etudiee dans des conditions de regime chimique en fonction du taux de subtitution en chrome, x, d'une part, et de variations du protocole experimental d'autre part. La phase CoO pure ou substituee par Cr est formee intermediairement. Les cinetiques respectives de la reduction du spinelle en CoO et la reduction de CoO en metal sont modifiees par des changements mineurs du protocole experimental. On observe un changement graduel de regime cinetique du type lineaire au type parabolique avec l'augmentation du taux de chrome: le taux limite suppose de solubilite du chrome dans CoO separe deux domaines differenciant loi cinetique et vitesse des processus d'interface."", 'corpus_id': 99189600, 'score': 0}, {'doc_id': '231979420', 'title': 'Spinodal Decomposition Stabilizes Plastic Flow in a Nanocrystalline Cu-Ti Alloy', 'abstract': 'A combination of high strength and reasonable ductility has been achieved in a copper-1.7 at.%titanium alloy deformed by high-pressure torsion. Grain refinement and a spinodal microstructure provided a hardness of 254 ± 2 H<i>v</i>, yield strength of 800 MPa and elongation of 10%. The spinodal structure persisted during isothermal ageing, further increasing the yield strength to 890MPa while retaining an elongation of 7%. This work demonstrates the potential for spinodal microstructures to overcome the difficulties in retaining ductility in ultra-fine grained or nanocrystalline alloys, especially upon post- deformation heating where strain softening normally results in brittle behavior.', 'corpus_id': 231979420, 'score': 0}, {'doc_id': '232046311', 'title': 'High-throughput nanoindentation mapping of cast IN718 nickel-based superalloys: influence of the Nb concentration', 'abstract': 'A high-throughput correlative study of the local mechanical properties, chemical composition and crystallographic orientation has been carried out in selected areas of cast Inconel 718 specimens subjected to three different tempers. The specimens showed a strong Nb segregation at the scale of the dendrite arms, with local Nb contents that varied between 2 wt.% in the core of the dendrite arms to 8 wt.% in the interdendritic regions and 25 wt.% within the second phase particles (MC carbides, Laves phases and δ phase needles). The nanohardness was found to correlate strongly with the local Nb content and the temper condition. On the contrary, the indentation elastic moduli was not influenced by the local chemical composition or temper condition, but directly correlated with the crystallographic grain orientation, due to the high elastic anisotropy of nickel alloys.', 'corpus_id': 232046311, 'score': 0}, {'doc_id': '137641527', 'title': 'High pressure sintering of cubic boron nitride compacts with Al and AlN', 'abstract': 'Abstract Cubic boron nitride (cBN) compacts, using 15\xa0wt.% Al and 20\xa0wt.% AlN respectively as additives, were sintered in the temperature range of 1300–1700\xa0°C for 20\xa0min under high pressure of 5.0\xa0GPa. The hardness, microstructure, phase composition and cutting performance of the high pressure sintered samples were investigated. A liquid phase sintering and reaction process was observed in the cBN–Al system, which leads to the formation of AlN and AlB2 as confirmed by X-ray diffraction (XRD) in the sintered compacts. Scanning electron microscopy (SEM) analysis shows that the samples have a homogeneous microstructure. The hardness decreases with increase of sintering temperature and reaches the highest Vickers hardness of 32.1\xa0GPa at 1350\xa0°C. While in the cBN–AlN system, AlN grains agglomerate heavily at temperature below ~\xa01500\xa0°C. As the sintering temperature increasing, Al2O3 appeared and the AlN agglomeration disappeared gradually. A highest cBN–AlN composite hardness of 29\xa0GPa was achieved when sintered at 1600\xa0°C. Turning tests showed that cBN compacts with 15\xa0wt.% Al as the additive has a longer tool life as compared to that with 20\xa0wt.% AlN. Our results indicated that cBN–Al system is more favourable to obtain well-sintered cBN compacts comparing with the cBN–AlN system.', 'corpus_id': 137641527, 'score': 1}]"
189	{'doc_id': '221130443', 'title': 'Endoscope-Associated Infections (EAI): An Update and Future Directions', 'abstract': 'Kapil Gupta, MD Department of Medicine, University of Miami/JFK Medical Center Palm Beach Regional GME Consortium Internal Medicine Program, Atlantis, FL Tara Keihanian, MD Fellow, Division of Gastroenterology and Hepatology, Department of Medicine, University of Miami Miller School of Medicine, Miami, FL Amaninder S. Dhaliwal, MD Fellow, Division of Gastroenterology and Hepatology, University of Nebraska Medical Center, Omaha, NE James H. Tabibian, MD, PhD Health Sciences Clinical Associate Professor, David Geffen SOM at UCLA, Director of Endoscopy, Olive View-UCLA Medical Center, Los Angeles, CA Mohit Girotra MD FACP Associate Professor of Clinical Medicine, Division of Gastroenterology and Hepatology, University of Miami Miller School of Medicine, Miami, FL Kapil Gupta', 'corpus_id': 221130443}	5694	[{'doc_id': '215405822', 'title': 'MANAGEMENT OF ENDOSCOPIC ACCESSORIES, VALVES, AND WATER AND IRRIGATION BOTTLES IN THE GASTROENTEROLOGY SETTING.', 'abstract': 'Disclaimer The Society of Gastroenterology Nurses and Associates, Inc. (SGNA) presents this guideline for use in developing institutional policies, procedures, and/or protocols. Information contained in this guideline is based on current published data and current practice at the time of publication. The Society of Gastroenterology Nurses and Associates, Inc. assume no responsibility for the practices or recommendations of any member or other practitioner, or for the policies and practices of any practice setting. Nurses and associates function within the limits of state licensure, state nurse practice act, and/or institutional policy. The Society of Gastroenterology Nurses and Associates, Inc. does not endorse or recommend any commercial products, processes, or services. A commercial product, process, or service is recognized as being consumed by or used on patients.', 'corpus_id': 215405822, 'score': 1}, {'doc_id': '221310838', 'title': 'Did granny know best? Evaluating the antibacterial, antifungal and antiviral efficacy of acetic acid for home care procedures', 'abstract': 'Background Acetic acid has been used to clean and disinfect surfaces in the household for many decades. The antimicrobial efficacy of cleaning procedures can be considered particularly important for young, old, pregnant, immunocompromised people, but may also concern other groups, particularly with regards to the COVID-19 pandemics. This study aimed to show that acetic acid exhibit an antibacterial and antifungal activity when used for cleaning purposes and is able to destroy certain viruses. Furthermore, a disinfecting effect of laundry in a simulated washing cycle has been investigated. Results At a concentration of 10% and in presence of 1.5% citric acid, acetic acid showed a reduction of >\u20095-log steps according to the specifications of DIN EN 1040 and DIN EN 1275 for the following microorganisms: P. aeruginosa , E. coli , S. aureus , L. monocytogenes , K. pneumoniae , E. hirae and A. brasiliensis . For MRSA a logarithmic reduction of 3.19 was obtained. Tests on surfaces according to DIN EN 13697 showed a complete reduction (>\u20095-log steps) for P. aeruginosa , E. coli , S. aureus , E. hirae , A. brasiliensis and C. albicans at an acetic acid concentration of already 5%. Virucidal efficacy tests according to DIN EN 14476 and DIN EN 16777 showed a reduction of ≥4-log-steps against the Modified Vaccinia virus Ankara (MVA) for acetic acid concentrations of 5% or higher. The results suggest that acetic acid does not have a disinfecting effect on microorganisms in a dosage that is commonly used for cleaning. However, this can be achieved by increasing the concentration of acetic acid used, especially when combined with citric acid. Conclusions Our results show a disinfecting effect of acetic acid in a concentration of 10% and in presence of 1.5% citric acid against a variety of microorganisms. A virucidal effect against enveloped viruses could also be proven. Furthermore, the results showed a considerable antimicrobial effect of acetic acid when used in domestic laundry procedures.', 'corpus_id': 221310838, 'score': 0}, {'doc_id': '221137076', 'title': 'Does double high-level disinfection for duodenoscopes add any value? A mini-systematic review and meta-analysis', 'abstract': 'Duodenoscope-emerging infection especially drug-resistant bacteria is considered a major concern nowadays. Different approaches were attempted to overcome this problem, like double high-level disinfection procedure. We performed a systematic review and meta-analysis to evaluate risk difference for positive cultures from duodenoscopes between double high-level disinfection (dHLD) and single (standard) high-level disinfection (sHLD). A thorough literature search (in October and November 2019) for studies comparing dHLD and sHLD for duodenoscopes was performed by 3 researchers in the Web of Science, Scopus, PubMed, and Cochran databases. The search terms were “duodenoscope,” “ERCP endoscope,” “disinfection,” “sterilization,” and “reprocessing,” and only randomized clinical trials with the English language were accepted. Four trials were identified studying dHLD, and only 2 clinical trials comparing dHLD with standard sHLD were found reporting 6193 duodenoscope cultures. Overall sHLD cultures were 2972, and dHLD cultures were 3221; overall positive cultures were 140 in sHLD and 161 in dHLD. The results of a meta-analysis using the random-effect model showed no significant risk difference (RD) between the 2 procedures for duodenoscope positive cultures (p = 0.53, RD 0.003, 95% CI “− 0.007–0.013”). Double HLD offered no significant difference over single HLD for duodenoscope disinfection. An alternative strategy to overcome duodenoscope-transmitted infection is a big issue to be resolved.', 'corpus_id': 221137076, 'score': 1}, {'doc_id': '221178141', 'title': 'The Research Gap in Non-tuberculous Mycobacterium (NTM) and Reusable Medical Devices', 'abstract': 'Patient infections with Non-tuberculous Mycobacterium (NTM) have been attributed to some reusable medical devices (1, 2), such as heater cooler devices (3–5), dental unit waterlines (6), bronchoscopes (7), and automated endoscope reprocessors (8, 9). Such incidents can be related to insufficient reprocessing or growth of resistant organisms. For example, NTM infections can arise from patient exposure to contaminated water from established biofilms in water systems, and in some cases, aerosolization of the contaminated water (10). These medical devices are regulated by the U.S. Food and Drug Administration (FDA) and the Agency seeks to better understand the mechanism bywhich devices can transmit NTM. SomeNTM-specific challenges include potentially years-long incubation period to clinical infection, subsequent difficulty identifying the bacteria to the species-level and extended duration of treatment (11). While general awareness around clinically relevant, rapidand slow-growing NTM and infections appears to have increased (11), a literature review of Mycobacterium research reveals the trend of testing rapid-growing Mycobacterium spp. as surrogates and extrapolating data to their slow-growing counterparts (12–14). However, with more than 150 known NTM species (15), questions about the applicability of surrogate data have been voiced for decades (11, 13). Comparative analyses between slowand rapid-growing NTM tend to be limited and in some cases, have demonstrated notable variation, as well as intra-species differences (13, 14, 16, 17). This raises questions about the applicability of using one species of NTM (herein referred to as surrogate NTM) in place of another for medical device testing. The topic of appropriate surrogates can be considered in many ways. However, in the context of this manuscript we will discuss the applicability of NTM surrogates for testing intermediateand high-level disinfection1 of, and aerosolization from, reusable medical devices in the context of specific NTM outbreaks. Note that this manuscript does not address or suggests any changes in wellestablished disinfection validation methods that are routinely used for medical device testing.', 'corpus_id': 221178141, 'score': 1}, {'doc_id': '219326961', 'title': 'Evaluation of an electrostatic spray disinfectant technology for rapid decontamination of portable equipment and large open areas in the era of SARS-CoV-2', 'abstract': '\n               ABSTRACT\n               \n                  In the setting of the coronavirus disease 2019 pandemic, efficient methods are needed to decontaminate shared portable devices and large open areas such as waiting rooms. We found that wheelchairs, portable equipment, and waiting room chairs were frequently contaminated with potential pathogens. After minimal manual pre-cleaning of areas with visible soiling, application of a dilute sodium hypochlorite disinfectant using an electrostatic sprayer provided rapid and effective decontamination and eliminated the benign virus bacteriophage MS2 from inoculated surfaces.\n               \n            ', 'corpus_id': 219326961, 'score': 0}, {'doc_id': '221124748', 'title': 'Alternative Methods of Sterilization in Dental Practices Against COVID-19', 'abstract': 'SARS-CoV-2, and several other microorganisms, may be present in nasopharyngeal and salivary secretions in patients treated in dental practices, so an appropriate clinical behavior is required in order to avoid the dangerous spread of infections. COVID-19 could also be spread when patients touches a contaminated surface with infected droplets and then touch their nose, mouth, or eyes. It is time to consider a dental practice quite similar to a hospital surgery room, where particular attention should be addressed to problems related to the spreading of infections due to air and surface contamination. The effectiveness of conventional cleaning and disinfection procedures may be limited by several factors; first of all, human operator dependence seems to be the weak aspect of all procedures. The improvement of these conventional methods requires the modification of human behavior, which is difficult to achieve and sustain. As alternative sterilization methods, there are some that do not depend on the operator, because they are based on devices that perform the entire procedure on their own, with minimal human intervention. In conclusion, continued efforts to improve the traditional manual disinfection of surfaces are needed, so dentists should consider combining the use of proper disinfectants and no-touch decontamination technologies to improve sterilization procedures.', 'corpus_id': 221124748, 'score': 0}, {'doc_id': '220334092', 'title': 'Reducing Anesthesia Workstation Contamination', 'abstract': 'Healthcare-associated infections are a source of morbidity and mortality in the United States and have been shown to be more preventable than current incidence. Anesthesia providers may be a source of and vector for some of these infections. Nurse anesthetists provide direct individual care for numerous patients daily, managing airways and invasive devices that contaminate hands with secretions which then may be transferred to the anesthesia workstation. Due to its complexity, the anesthesia machine is difficult to thoroughly clean and may become a reservoir for contaminants. The purpose of this paper will be to examine new interventions being explored to reduce the contamination of the anesthesia workstation. These interventions will include hand hygiene interventions, ultraviolet (UV) radiation for workstation disinfection, and anesthesia workstation barrier devices. Analysis of which interventions are the most effective may help to guide the direction of interventions to help reduce anesthesia machine contamination. REDUCING ANESTHESIA CONTAMINATION 3 Reducing Anesthesia Workstation Contamination Introduction The Centers for Disease Control and Prevention (CDC) estimated in 2018 that healthcareassociated infections (HAIs) affected at least one in 31 hospitalized patients (CDC, 2018). Similarly, according to the World Health Organization (WHO), the incidence of HAI in 2002 was at 4.5%, affecting 1.7 million patients and causing 99,000 deaths with an estimated financial impact of $6.5 billion in 2004 (WHO, 2009). The cost of HAIs in financial terms, patient mortality, and loss of quality of life cannot be underestimated. A meta-analysis by Umscheid et al. in 2011 of data from the National Nosocomial Infections Surveillance System (NNIS), the National Hospital Discharge Survey, and the American Hospital Association identified 1,737,125 total infections in 2002, with 98,987 deaths. Central line-associated blood stream infections (CLABSI) and ventilator associated pneumonia (VAP) accounted for two-thirds of deaths and had a five-fold increase in mortality compared to other HAIs. Costs per HAI ranged from a low of $5,600 with surgical site infections (SSIs) to a high of $110,800 for CLABSIs; with potential savings ranging from $115 million from catheter-associated urinary tract infections (CAUTIs) to $18.2 billion from CLABSIs. From evidence-based practice studies examined, it was estimated that certain percentages of various HAIs are preventable. CLABSI and CAUTI could potentially be reduced by 65-70%, while VAP and SSIs could reasonably be prevented in 55% of cases. (Umscheid et al., 2011). Another study, a multistate survey including 183 hospitals and 11,290 patients by Magill et al. in 2014 found a 4% incidence of HAIs and an 11.5% rate of mortality from HAI. Infections related to invasive devices such as VAP, CAUTI, and CLABSI accounted for 25.6% of HAIs, with SSIs accounting for another 21.8%. Median time until presentation of HAI was six days, REDUCING ANESTHESIA CONTAMINATION 4 and present on admission HAIs were 19.4% of the total, with 67.3% of these being SSIs (Magill et al., 2014). These factors may help obscure the role of anesthesia in contributing to infection. As providers constantly in direct contact with patients, nurse anesthetists are in a prime position to either be a significant vector for HAIs or to help find ways to solve this costly problem. These challenges illuminate the important task of anesthesia providers becoming involved in finding unique solutions to anesthesia’s role in propagating HAIs. Solutions that work in much of the hospital may not be effective for the operating room (OR) setting. In that vein, this paper will investigate the scope and nature of the problem of anesthesia workstation contamination as well as several avenues of solutions proposed to help reduce contamination. These include novel methods of hand hygiene customized for anesthesia providers, use of UV radiation to help decontaminate the complex permanent parts of the anesthesia workstation, and a novel cover system to help prevent any contamination of the anesthesia machine. Methods Articles for inclusion in this literature review were found by searching PubMed and CINAHL. Studies were chosen from within the last five years, with exceptions made for studies with great impact on the state of the literature that provided a basis for future study.', 'corpus_id': 220334092, 'score': 0}, {'doc_id': '220907227', 'title': 'Microbial contamination of powered air purifying respirators (PAPR) used during the COVID-19 pandemic: an in situ microbiological study', 'abstract': 'OBJECTIVE To determine whether internal components of powered air purifying respirators (PAPR) used during the Corona virus 2019 disease (COVID-19) pandemic are contaminated with bacteria, fungi and/or any viral material. DESIGN In situ microbiological study. SETTING Single NHS Trust, UK. OUTCOME MEASURES Growth of any bacteria or fungi, or positive polymerase chain reaction results for common respiratory viruses and severe acute respiratory syndrome coronavirus-2 (SARS-CoV-2) RESULTS 25 PAPR hoods were swabbed; ten (40%) returned positive results. Bacterial growth was detected on six hoods (bacillus simplex, kocuria rhizophilia, bacillus weihenstephensis, microcccus luteus and staphylococcus epidermidis); five of the hoods were positive for fungal growth (non-sporulating environmental mould, NSEM); all sampled hoods tested negative for both SARS-CoV-2 and an expanded panel of respiratory viruses. There was wide variation in the storage of cleaned hoods. CONCLUSION Despite following recommended cleaning procedures, bacteria and fungi can remain on the internal components of PAPR hoods, at levels significant enough to be swabbed and cultured. PAPR hoods have the potential to cross-infect wearers and patients and are used primarily by clinicians who fail to fit disposable FFP3 respirators; the female sex and non-Caucasian people are less likely to fit FFP3 respirators. The hoods tested cannot be adequately cleaned for use in high risk healthcare environments, PAPR hoods and tubes can act as fomites, and there are evident shortcomings in their provision.', 'corpus_id': 220907227, 'score': 0}, {'doc_id': '212707954', 'title': 'Contribution of usage to endoscope working channel damage and bacterial contamination.', 'abstract': 'Biofilm formation was shown to be associated with damaged areas of endoscope channels. We hypothesis that the passage of instruments and brushes through endoscope channels during procedures and cleaning contribute to the channel damage, bacterial attachment and biofilm formation. In this study, we compared the roughness and bacterial attachment in used and new endoscope channel in vivo and in vitro. The roughness of ten clinically used retired and seven new colonoscope biopsy channels were analysed by a Surface Profiler. For in vitro study, a flexible endoscope biopsy forceps was repeatedly passed through a curved 3.0 mm diameter Teflon® tube (which endoscope channel is made of) 100, 200, 500 times. Atomic force microscopy was used to determine the degree of inner surface damage. The number of Escherichia coli or Enterococcus faecium attached to the inner surface of new Teflon® tube and tube with 500 times biopsy forceps passes in one hour at 37oC was determined by culture. Average surface roughness of clinically used colonoscope biopsy channels is 1.5 times of the new colonoscope biopsy channels (P=0.03). Surface roughness of Teflon® tubes with 100, 200 and 500 times biopsy forceps passes is 1.05, 1.12, 3.2-fold (P=0.025) of the new Teflon® tubes respectively. The bacterial number of E. coli and E. faecium attached to Teflon® tubes with 500 times biopsy forceps passes was 2.9-fold (P=0.021) and 4.3-fold (P=0.004) of those attached to the new Teflon® tubes respectively. Our study confirmed the association of endoscope usage with biopsy channel damage and increased bacterial attachment.', 'corpus_id': 212707954, 'score': 1}, {'doc_id': '221034718', 'title': 'SpyGlass application for duodenoscope working channel inspection: Impact on the microbiological surveillance', 'abstract': 'BACKGROUND Patient-ready duodenoscopes were designed with an assumed contamination rate of less than 0.4%; however, it has been reported that 5.4% of clinically used duodenoscopes remain contaminated with viable high-concern organisms despite following the manufacturer’s instructions. Visual inspection of working channels has been proposed as a quality control measure for endoscope reprocessing. There are few studies related to this issue. AIM To investigate the types, severity rate, and locations of abnormal visual inspection findings inside patient-ready duodenoscopes and their microbiological significance. METHODS Visual inspections of channels were performed in 19 patient-ready duodenoscopes using the SpyGlass visualization system in two endoscopy units of tertiary care teaching hospitals (Tri-Service General Hospital and National Taiwan University Hospital) in Taiwan. Inspections were recorded and reviewed to evaluate the presence of channel scratches, buckling, stains, debris, and fluids. These findings were used to analyze the relevance of microbiological surveillance. RESULTS Seventy-two abnormal visual inspection findings in the 19 duodenoscopes were found, including scratches (n = 10, 52.6%), buckling (n = 15, 78.9%), stains (n = 14, 73.7%), debris (n = 14, 73.7%), and fluids (n = 6, 31.6%). Duodenoscopes > 12 mo old had a significantly higher number of abnormal visual inspection findings than those ≤ 12 mo old (46 findings vs 26 findings, P < 0.001). Multivariable regression analyses demonstrated that the bending section had a significantly higher risk of being scratched, buckled, and stained, and accumulating debris than the insertion tube. Debris and fluids showed a significant positive correlation with microbiological contamination (P < 0.05). There was no significant positive Spearman’s correlation coefficient between negative bacterial cultures and debris, between that and fluids, and the concomitance of debris and fluids. This result demonstrated that the presence of fluid and debris was associated with positive cultures, but not negative cultures. Further multivariate analysis demonstrated that fluids, but not debris, is an independent factor for bacterial culture positivity. CONCLUSION In patient-ready duodenoscopes, scratches, buckling, stains, debris, and fluids inside the working channel are common, which increase the microbiological contamination susceptibility. The SpyGlass visualization system may be recommended to identify suboptimal reprocessing.', 'corpus_id': 221034718, 'score': 1}]
190	{'doc_id': '233204444', 'title': 'ACERAC: Efficient reinforcement learning in fine time discretization', 'abstract': 'We propose a framework for reinforcement learning (RL) in fine time discretization and a learning algorithm in this framework. One of the main goals of RL is to provide a way for physical machines to learn optimal behavior instead of being programmed. However, the machines are usually controlled in fine time discretization. The most common RL methods apply independent random elements to each action, which is not suitable in that setting. It is not feasible because it causes the controlled system to jerk, and does not ensure sufficient exploration since a single action is not long enough to create a significant experience that could be translated into policy improvement. In the RL framework introduced in this paper, policies are considered that produce actions based on states and random elements autocorrelated in subsequent time instants. The RL algorithm introduced here approximately optimizes such a policy. The efficiency of this algorithm is verified against three other RL methods (PPO, SAC, ACER) in four simulated learning control problems (Ant, HalfCheetah, Hopper, and Walker2D) in diverse time discretization. The algorithm introduced here outperforms the competitors in most cases considered.', 'corpus_id': 233204444}	16648	[{'doc_id': '229373012', 'title': 'Towards a Distributed Framework for Multi-Agent Reinforcement Learning Research', 'abstract': 'Some of the most important publications in deep reinforcement learning over the last few years have been fueled by access to massive amounts of computation through large scale distributed systems. The success of these approaches in achieving human-expert level performance on several complex video-game environments has motivated further exploration into the limits of these approaches as computation increases. In this paper, we present a distributed RL training framework designed for super computing infrastructures such as the MIT SuperCloud. We review a collection of challenging learning environments-such as Google Research Football, StarCraft II, and Multi-Agent Mujoco- which are at the frontier of reinforcement learning research. We provide results on these environments that illustrate the current state of the field on these problems. Finally, we also quantify and discuss the computational requirements needed for performing RL research by enumerating all experiments performed on these environments.', 'corpus_id': 229373012, 'score': 1}, {'doc_id': '53084610', 'title': 'GPU-Accelerated Robotic Simulation for Distributed Reinforcement Learning', 'abstract': 'Most Deep Reinforcement Learning (Deep RL) algorithms require a prohibitively large number of training samples for learning complex tasks. Many recent works on speeding up Deep RL have focused on distributed training and simulation. While distributed training is often done on the GPU, simulation is not. In this work, we propose using GPU-accelerated RL simulations as an alternative to CPU ones. Using NVIDIA Flex, a GPU-based physics engine, we show promising speed-ups of learning various continuous-control, locomotion tasks. With one GPU and CPU core, we are able to train the Humanoid running task in less than 20 minutes, using 10-1000x fewer CPU cores than previous works. We also demonstrate the scalability of our simulator to multi-GPU settings to train more challenging locomotion tasks.', 'corpus_id': 53084610, 'score': 1}, {'doc_id': '207870308', 'title': 'Decentralized Distributed PPO: Solving PointGoal Navigation', 'abstract': 'We present Decentralized Distributed Proximal Policy Optimization (DD-PPO), a method for distributed reinforcement learning in resource-intensive simulated environments. DD-PPO is distributed (uses multiple machines), decentralized (lacks a centralized server), and synchronous (no computation is ever stale), making it conceptually simple and easy to implement. In our experiments on training virtual robots to navigate in Habitat-Sim, DD-PPO exhibits near-linear scaling -- achieving a speedup of 107x on 128 GPUs over a serial implementation. We leverage this scaling to train an agent for 2.5 Billion steps of experience (the equivalent of 80 years of human experience) -- over 6 months of GPU-time training in under 3 days of wall-clock time with 64 GPUs. This massive-scale training not only sets the state of art on Habitat Autonomous Navigation Challenge 2019, but essentially solves the task --near-perfect autonomous navigation in an unseen environment without access to a map, directly from an RGB-D camera and a GPS+Compass sensor. Fortuitously, error vs computation exhibits a power-law-like distribution; thus, 90% of peak performance is obtained relatively early (at 100 million steps) and relatively cheaply (under 1 day with 8 GPUs). Finally, we show that the scene understanding and navigation policies learned can be transferred to other navigation tasks -- the analog of ImageNet pre-training + task-specific fine-tuning for embodied AI. Our model outperforms ImageNet pre-trained CNNs on these transfer tasks and can serve as a universal resource (all models and code are publicly available).', 'corpus_id': 207870308, 'score': 1}, {'doc_id': '232222444', 'title': 'Large Batch Simulation for Deep Reinforcement Learning', 'abstract': 'We accelerate deep reinforcement learning-based training in visually complex 3D environments by two orders of magnitude over prior work, realizing end-to-end training speeds of over 19,000 frames of experience per second on a single GPU and up to 72,000 frames per second on a single eight-GPU machine. The key idea of our approach is to design a 3D renderer and embodied navigation simulator around the principle of “batch simulation”: accepting and executing large batches of requests simultaneously. Beyond exposing large amounts of work at once, batch simulation allows implementations to amortize in-memory storage of scene assets, rendering work, data loading, and synchronization costs across many simulation requests, dramatically improving the number of simulated agents per GPU and overall simulation throughput. To balance DNN inference and training costs with faster simulation, we also build a computationally efficient policy DNN that maintains high task performance, and modify training algorithms to maintain sample efficiency when training with large mini-batches. By combining batch simulation and DNN performance optimizations, we demonstrate that PointGoal navigation agents can be trained in complex 3D environments on a single GPU in 1.5 days to 97% of the accuracy of agents trained on a prior state-of-the-art system using a 64-GPU cluster over three days. We provide open-source reference implementations of our batch 3D renderer and simulator to facilitate incorporation of these ideas into RL systems.', 'corpus_id': 232222444, 'score': 1}, {'doc_id': '232068936', 'title': 'Coverage as a Principle for Discovering Transferable Behavior in Reinforcement Learning', 'abstract': 'Designing agents that acquire knowledge autonomously and use it to solve new tasks efficiently is an important challenge in reinforcement learning, and unsupervised learning provides a useful paradigm for autonomous acquisition of task-agnostic knowledge. In supervised settings, representations discovered through unsupervised pre-training offer important benefits when transferred to downstream tasks. Given the nature of the reinforcement learning problem, we argue that representation alone is not enough for efficient transfer in challenging domains and explore how to transfer knowledge through behavior. The behavior of pre-trained policies may be used for solving the task at hand (exploitation), as well as for collecting useful data to solve the problem (exploration). We argue that policies pre-trained to maximize coverage will produce behavior that is useful for both strategies. When using these policies for both exploitation and exploration, our agents discover better solutions. The largest gains are generally observed in domains requiring structured exploration, including settings where the behavior of the pre-trained policies is misaligned with the downstream task.', 'corpus_id': 232068936, 'score': 0}, {'doc_id': '233231295', 'title': 'Decomposed Soft Actor-Critic Method for Cooperative Multi-Agent Reinforcement Learning', 'abstract': 'Deep reinforcement learning methods have shown great performance on many challenging cooperative multi-agent tasks. Two main promising research directions are multi-agent value function decomposition and multi-agent policy gradients. In this paper, we propose a new decomposed multi-agent soft actor-critic (mSAC) method, which incorporates the idea of the multi-agent value function decomposition and soft policy iteration framework effectively and is a combination of novel and existing techniques, including decomposed Q value network architecture, decentralized probabilistic policy, and counterfactual advantage function (optional). Theoretically, mSAC supports efficient offpolicy learning and addresses credit assignment problem partially in both discrete and continuous action spaces. Tested on StarCraft II micromanagement cooperative multiagent benchmark, we empirically investigate the performance of mSAC against its variants and analyze the effects of the different components. Experimental results demonstrate that mSAC significantly outperforms policybased approach COMA, and achieves competitive results with SOTA value-based approach Qmix on most tasks in terms of asymptotic perfomance metric. In addition, mSAC achieves pretty good results on large action space tasks, such as 2c vs 64zg and MMM2.', 'corpus_id': 233231295, 'score': 0}, {'doc_id': '232307382', 'title': 'Softmax with Regularization: Better Value Estimation in Multi-Agent Reinforcement Learning', 'abstract': 'Overestimation in Q-learning is an important problem that has been extensively studied in single-agent reinforcement learning, but has received comparatively little attention in the multi-agent setting. In this work, we empirically demonstrate that QMIX, a popular Q-learning algorithm for cooperative multi-agent reinforcement learning (MARL), suffers from a particularly severe overestimation problem which is not mitigated by existing approaches. We rectify this by designing a novel regularization-based update scheme that penalizes large joint action-values deviating from a baseline and demonstrate its effectiveness in stabilizing learning. We additionally propose to employ a softmax operator, which we efficiently approximate in the multi-agent setting, to further reduce the potential overestimation bias. We demonstrate that our Softmax with Regularization (SR) method, when applied to QMIX, accomplishes its goal of avoiding severe overestimation and significantly improves performance in a variety of cooperative multi-agent tasks. To demonstrate the versatility of our method, we apply it to other Q-learning based MARL algorithms and achieve similar performance gains. Finally, we show that our method provides a consistent performance improvement on a set of challenging StarCraft II micromanagement tasks.', 'corpus_id': 232307382, 'score': 0}, {'doc_id': '233423273', 'title': 'Semi-On-Policy Training for Sample Efficient Multi-Agent Policy Gradients', 'abstract': 'Policy gradient methods are an attractive approach to multi-agent reinforcement learning problems due to their convergence properties and robustness in partially observable scenarios. However, there is a significant performance gap between state-of-the-art policy gradient and value-based methods on the popular StarCraft Multi-Agent Challenge (SMAC) benchmark. In this paper, we introduce semi-onpolicy (SOP) training as an effective and computationally efficient way to address the sample inefficiency of on-policy policy gradient methods. We enhance two state-of-the-art policy gradient algorithms with SOP training, demonstrating significant performance improvements. Furthermore, we show that our methods perform as well or better than state-of-the-art value-based methods on a variety of SMAC tasks.', 'corpus_id': 233423273, 'score': 1}, {'doc_id': '234097493', 'title': 'Reward prediction for representation learning and reward shaping', 'abstract': 'One of the fundamental challenges in reinforcement learning (RL) is the one of data efficiency: modern algorithms require a very large number of training samples, especially compared to humans, for solving environments with high-dimensional observations. The severity of this problem is increased when the reward signal is sparse. In this work, we propose learning a state representation in a self-supervised manner for reward prediction. The reward predictor learns to estimate either a raw or a smoothed version of the true reward signal in environment with a single, terminating, goal state. We augment the training of out-of-the-box RL agents by shaping the reward using our reward predictor during policy learning. Using our representation for preprocessing high-dimensional observations, as well as using the predictor for reward shaping, is shown to significantly enhance Actor Critic using Kronecker-factored Trust Region and Proximal Policy Optimization in single-goal environments with visual inputs.', 'corpus_id': 234097493, 'score': 0}, {'doc_id': '232417219', 'title': 'Shaping Advice in Deep Multi-Agent Reinforcement Learning', 'abstract': 'Multi-agent reinforcement learning involves multiple agents interacting with each other and a shared environment to complete tasks. When rewards provided by the environment are sparse, agents may not receive immediate feedback on the quality of actions that they take, thereby affecting learning of policies. In this paper, we propose a method called Shaping Advice in deep Multi-agent reinforcement learning (SAM) to augment the reward signal from the environment with an additional reward termed shaping advice. The shaping advice is given by a difference of potential functions at consecutive time-steps. Each potential function is a function of observations and actions of the agents. The shaping advice needs to be specified only once at the start of training, and can be easily provided by non-experts. We show through theoretical analyses and experimental validation that the shaping advice provided by SAM does not distract agents from completing tasks specified by the environment reward. Theoretically, we prove that the convergence of policy gradients and value functions when using SAM implies the convergence of these quantities in the same environment in the absence of SAM. Experimentally, we evaluate SAM on three tasks in the multi-agent Particle World environment that have sparse rewards. We observe that using SAM results in agents learning policies to complete tasks faster, and obtain higher rewards than: i) using sparse rewards alone; ii) a state-of-the-art reward redistribution method.', 'corpus_id': 232417219, 'score': 0}]
191	{'doc_id': '5134387', 'title': 'Usability engineering', 'abstract': 'From the Publisher: \nWritten by the author of the best-selling HyperText & HyperMedia, this book provides an excellent guide to the methods of usability engineering. Special features: emphasizes cost-effective methods that will help developers improve their user interfaces immediately, shows you how to avoid the four most frequently listed reasons for delay in software projects, provides step-by-step information about which methods to use at various stages during the development life cycle, and offers information on the unique issues relating to informational usability. You do not need to have previous knowledge of usability to implement the methods provided, yet all of the latest research is covered.', 'corpus_id': 5134387}	4682	"[{'doc_id': '212633506', 'title': 'Software-testing education: A systematic literature mapping', 'abstract': 'Context: With the rising complexity and scale of software systems, there is an ever-increasing demand for sophisticated and cost-effective software testing. To meet such a demand, there is a need for a highly-skilled software testing work-force (test engineers) in the industry. To address that need, many university educators worldwide have included software-testing education in their software engineering (SE) or computer science (CS) programs. Objective: Our objective in this paper is to summarize the body of experience and knowledge in the area of software-testing education to benefit the readers (both educators and researchers) in designing and delivering software testing courses in university settings, and to also conduct further education research in this area. Method: To address the above need, we conducted a systematic literature mapping (SLM) to synthesize what the community of educators have published on this topic. After compiling a candidate pool of 307 papers, and applying a set of inclusion/exclusion criteria, our final pool included 204 papers published between 1992 and 2019. Results: The topic of software-testing education is becoming more active, as we can see by the increasing number of papers. Many pedagogical approaches (how to best teach testing), course-ware, and specific tools for testing education have been proposed. Many challenges in testing education and insights on how to overcome those challenges have been proposed. Conclusion: This paper provides educators and researchers with a classification of existing studies within software-testing education. We further synthesize challenges and insights reported when teaching software testing. The paper also provides a reference (""index"") to the vast body of knowledge and experience on teaching software testing.', 'corpus_id': 212633506, 'score': 0}, {'doc_id': '5731303', 'title': 'Using psychophysiological techniques to measure user experience with entertainment technologies', 'abstract': ""Emerging technologies offer exciting new ways of using entertainment technology to create fantastic play experiences and foster interactions between players. Evaluating entertainment technology is challenging because success isn't defined in terms of productivity and performance, but in terms of enjoyment and interaction. Current subjective methods of evaluating entertainment technology aren't sufficiently robust. This paper describes two experiments designed to test the efficacy of physiological measures as evaluators of user experience with entertainment technologies. We found evidence that there is a different physiological response in the body when playing against a computer versus playing against a friend. These physiological results are mirrored in the subjective reports provided by the participants. In addition, we provide guidelines for collecting physiological data for user experience analysis, which were informed by our empirical investigations. This research provides an initial step towards using physiological responses to objectively evaluate a user's experience with entertainment technology."", 'corpus_id': 5731303, 'score': 1}, {'doc_id': '17146837', 'title': 'User experience over time: an initial framework', 'abstract': 'A recent trend in Human-Computer Interaction (HCI) research addresses human needs that go beyond the instrumental, resulting in an increasing body of knowledge about how users form overall evaluative judgments on the quality of interactive products. An aspect largely neglected so far is that of temporality, i.e. how the quality of users\' experience develops over time. This paper presents an in-depth, five-week ethnographic study that followed 6 individuals during an actual purchase of the Apple iPhone"". We found prolonged use to be motivated by different qualities than the ones that provided positive initial experiences. Overall, while early experiences seemed to relate mostly to hedonic aspects of product use, prolonged experiences became increasingly more tied to aspects reflecting how the product becomes meaningful in one\'s life. Based on the findings, we promote three directions for CHI practice: designing for meaningful mediation, designing for daily rituals, and designing for the self.', 'corpus_id': 17146837, 'score': 1}, {'doc_id': '215768748', 'title': 'Embracing Companion Technologies', 'abstract': 'As an increasing number of interactive devices offer human-like assistance, there is a growing need to understand our experience of interactive agents. When interactive artefacts become intertwined in our everyday experience, we need to make sure that they assume the right roles and contribute to our wellbeing. In this theoretical exploration, we propose a reframing of our understanding of the experience of interactions with everyday technologies by proposing the metaphor of companion technologies. We employ theory in the philosophy of empathy to propose a framework for understanding how users develop relationships with digital agents. The experiential framework for companion technologies provides connections between the users’ psychological needs and companion features of interactive systems. Our work provides a theoretical basis for rethinking the user experience of everyday artefacts with an empathy-oriented mindset and poses future challenges for HCI.', 'corpus_id': 215768748, 'score': 0}, {'doc_id': '218684555', 'title': 'Towards Friendly Mixed Initiative Procedural Content Generation: Three Pillars of Industry', 'abstract': 'While the games industry is moving towards procedural content generation (PCG) with tools available under popular platforms such as Unreal, Unity or Houdini, and video game titles like No Man’s Sky and Horizon Zero Dawn taking advantage of PCG, the gap between academia and industry is as wide as it has ever been, in terms of communication and sharing methods. The authors have worked on both sides of this gap and in an effort to shorten it and increase the synergy between the two sectors have identified three design pillars for PCG using mixed-initiative interfaces. The three pillars are respect designer control, respect the creative process and respect existing work processes. Respecting designer control is about creating a tool that gives enough control to bring out the designer’s vision. Respecting the creative process concerns itself with having a feedback loop that is short enough, that the creative process is not disturbed. Respecting existing work processes means that a PCG tool should plug in easily to existing asset pipelines. As academics and communicators, it is surprising that publications often do not describe ways for developers to use our work or lack considerations for how a piece of work might fit into existing content pipelines.', 'corpus_id': 218684555, 'score': 0}, {'doc_id': '212415187', 'title': 'Submitting surveys via a conversational interface: an evaluation of user acceptance and approach effectiveness', 'abstract': 'Abstract Conversational interfaces are currently on the rise: more and more applications rely on a chat-like interaction pattern to increase their acceptability and to improve user experience. Also in the area of questionnaire design and administration, interaction design is increasingly looked at as an important ingredient of a digital solution. For those reasons, we designed and developed a conversational survey tool to administer questionnaires with a colloquial form through a chat-like Web interface. In this paper, we present the evaluation results of our approach, taking into account both the user point of view – by assessing user acceptance and preferences in terms of survey compilation experience – and the survey design perspective – by investigating the effectiveness of a conversational survey in comparison to a traditional questionnaire. We show that users clearly appreciate the conversational form and prefer it over a traditional approach and that, from a data collection point of view, the conversational method shows the same reliability and a higher response quality with respect to a traditional questionnaire.', 'corpus_id': 212415187, 'score': 0}, {'doc_id': '2345818', 'title': 'User experience over time', 'abstract': ""The way we experience and evaluate interactive products develops over time. An exploratory study aimed at understanding how users form evaluative judgments during the first experiences with a product as well as after four weeks of use. Goodness, an evaluative judgment related to the overall satisfaction with the product, was largely formed on the basis of pragmatic aspects (i.e. utility and usability) during the first experiences; after four weeks of use identification (i.e. what the products expresses about its owner) became a dominant aspect of how good a product is. Surprisingly, beauty judgments were largely affected by stimulation (e.g. novelty) during the first experiences. Over time stimulation lost its power to make the product beautiful in the users' eyes."", 'corpus_id': 2345818, 'score': 1}, {'doc_id': '6415845', 'title': 'User Experience Evaluation in Entertainment', 'abstract': 'Based on an overview on currently used definitions of user experience in human–computer interaction and major concepts from game development like immersion, flow, and playability, this overview describes a set of evaluation methods and their applicability in the various game development phases. Summarizing the contributions in this book, a user experience-centered development process is presented, allowing readers to understand when to use what kind of user experience evaluation methods to achieve a positive user experience.', 'corpus_id': 6415845, 'score': 1}, {'doc_id': '218571427', 'title': 'How Peripheral Interactive Systems Can Support Teachers with Differentiated Instruction: Using FireFlies as a Probe', 'abstract': ""Teachers' response to the real-time needs of diverse learners in the classroom is important for each learner's success. Teachers who give differentiated instruction (DI) provide pertinent support to each student and acknowledge their differences in learning style and pace. However, due to the already complex and intensive routines in classrooms, it is demanding and time-consuming for teachers to implement DI on-the-spot. This study aims to explore how to ease teachers' classroom differentiation by enabling effortless, low-threshold student-teacher communications through a peripheral interactive system. Namely, we present a six-week study, in which we iteratively co-designed and field-tested interaction solutions with eight school teachers, using a set of distributed, interactive LED-objects (the 'FireFlies' platform). By connecting our findings to the theories of DI, we contribute empirical knowledge about the advantages and limitations of a peripheral interactive system in supporting DI. Taken together, we summarize concrete opportunities and recommendations for future design."", 'corpus_id': 218571427, 'score': 0}, {'doc_id': '214795221', 'title': 'User Experience of Reading in Virtual Reality — Finding Values for Text Distance, Size and Contrast', 'abstract': 'Virtual Reality (VR) has an increasing impact on the market in many fields, from education and medicine to engineering and entertainment, by creating different applications that replicate or in the case of augmentation enhance real-life scenarios. Intending to present realistic environments, VR applications are including text that we are surrounded by every day. However, text can only add value to the virtual environment if it is designed and created in such a way that users can comfortably read it. With the aim to explore what values for text parameters users find comfortable while reading in virtual reality, a study was conducted allowing participants to manipulate text parameters such as font size, distance, and contrast. Therefore two different standalone virtual reality devices were used, Oculus Go and Quest, together with three different text samples: Short (2 words), medium (21 words), and long (51 words). Participants had the task of setting text parameters to the best and worst possible value. Additionally, participants were asked to rate their experience of reading in virtual reality. Results report mean values for angular size (the combination of distance and font size) and color contrast depending on the different device used as well as the varying text length, for both tasks. Significant differences were found for values of angular size, depending on the length of the displayed text. However, different device types had no significant influence on text parameters but on the experiences reported using the self-assessment manikin (SAM) scale.', 'corpus_id': 214795221, 'score': 1}]"
192	{'doc_id': '220630830', 'title': 'Immunologic Effects of Vitamin D on Human Health and Disease', 'abstract': 'Vitamin D is responsible for regulation of calcium and phosphate metabolism and maintaining a healthy mineralized skeleton. It is also known as an immunomodulatory hormone. Experimental studies have shown that 1,25-dihydroxyvitamin D, the active form of vitamin D, exerts immunologic activities on multiple components of the innate and adaptive immune system as well as endothelial membrane stability. Association between low levels of serum 25-hydroxyvitamin D and increased risk of developing several immune-related diseases and disorders, including psoriasis, type 1 diabetes, multiple sclerosis, rheumatoid arthritis, tuberculosis, sepsis, respiratory infection, and COVID-19, has been observed. Accordingly, a number of clinical trials aiming to determine the efficacy of administration of vitamin D and its metabolites for treatment of these diseases have been conducted with variable outcomes. Interestingly, recent evidence suggests that some individuals might benefit from vitamin D more or less than others as high inter-individual difference in broad gene expression in human peripheral blood mononuclear cells in response to vitamin D supplementation has been observed. Although it is still debatable what level of serum 25-hydroxyvitamin D is optimal, it is advisable to increase vitamin D intake and have sensible sunlight exposure to maintain serum 25-hydroxyvitamin D at least 30 ng/mL (75 nmol/L), and preferably at 40–60 ng/mL (100–150 nmol/L) to achieve the optimal overall health benefits of vitamin D.', 'corpus_id': 220630830}	6702	"[{'doc_id': '218609040', 'title': 'Phytochemicals as Antiviral Agents: Recent Updates', 'abstract': 'The epidemic of viral diseases is a global concern, mandating an urgent need of most promising antivirals. Some of the viral diseases can be cured by approved antiviral drugs, but for others still do not have any vaccines or drugs available. Most of the approved antiviral drugs are somehow directly or indirectly associated with side effects, which eventually raise the need for the development of antivirals based on natural phytochemicals. Globally, the development of antivirals is shifting towards the plant-derived products as they are less toxic and has less chance to develop resistance. Phytochemicals have been exploited traditionally for the cure of many diseases, and also have been reported to inhibit viral replication/transcription. Most of them inhibit the viruses either during the viral entry inside the host cell or during their replication. Moreover, 50% of the drugs derived from plants are being used in the Western nations. Plants have a variety of phytochemicals like flavonoids, terpenoids, lignins, alkaloids, and coumarins that are having antioxidant activity, and help to inhibit viral genome. Various plant-derived products have been well studied against viruses like herpes virus, human immunodeficiency virus (HIV), influenza, and hepatitis virus. More recently, Coronavirus disease (COVID-19) caused by a newly identified coronavirus has become pandemic, and affected world’s population severely. However, there are still less explored phytochemicals for the inhibition of viruses like dengue virus, chikungunya virus, and other alphaviruses. In this chapter, we will emphasize on the reported phytochemicals and their derivatives, having antiviral properties and their mechanism to treat viral diseases.', 'corpus_id': 218609040, 'score': 1}, {'doc_id': '219619785', 'title': 'Chronic Inflammation in the Context of Everyday Life: Dietary Changes as Mitigating Factors', 'abstract': 'The lifestyle adopted by most people in Western societies has an important impact on the propensity to metabolic disorders (e.g., diabetes, cancer, cardiovascular disease, neurodegenerative diseases). This is often accompanied by chronic low-grade inflammation, driven by the activation of various molecular pathways such as STAT3 (signal transducer and activator of transcription 3), IKK (IκB kinase), MMP9 (matrix metallopeptidase 9), MAPK (mitogen-activated protein kinases), COX2 (cyclooxigenase 2), and NF-Kβ (nuclear factor kappa-light-chain-enhancer of activated B cells). Multiple intervention studies have demonstrated that lifestyle changes can lead to reduced inflammation and improved health. This can be linked to the concept of real-life risk simulation, since humans are continuously exposed to dietary factors in small doses and complex combinations (e.g., polyphenols, fibers, polyunsaturated fatty acids, etc.). Inflammation biomarkers improve in patients who consume a certain amount of fiber per day; some even losing weight. Fasting in combination with calorie restriction modulates molecular mechanisms such as m-TOR, FOXO, NRF2, AMPK, and sirtuins, ultimately leads to significantly reduced inflammatory marker levels, as well as improved metabolic markers. Moving toward healthier dietary habits at the individual level and in publicly-funded institutions, such as schools or hospitals, could help improving public health, reducing healthcare costs and improving community resilience to epidemics (such as COVID-19), which predominantly affects individuals with metabolic diseases.', 'corpus_id': 219619785, 'score': 1}, {'doc_id': '219330057', 'title': 'Anti-Infectious Plants of the Thai Karen: A Meta-Analysis', 'abstract': 'Pharmacology has developed many drugs to treat infections, but many people, especially in developing countries, cannot afford to purchase them, and still depend on traditional knowledge and local medicinal plants to fight off infections. In addition, numerous microbes have developed resistance to the pharmaceutical drugs developed to fight them, and for many, such as Covid-19, effective drugs remain to be found. Ethnomedicinal knowledge is useful, not only for local people as a source of medicine for primary health care, but also for new pharmacological discoveries. This study aimed to identify the plants that the Karen, the largest hill-tribe ethnic minority in northern and western Thailand, use for treatments of infectious diseases. We present a meta-analysis of data from 16 ethnobotanical studies of 25 Karen villages with the aim of understanding traditional knowledge and treatments and point to potential plants for further pharmacological development. The Karen used 127 plant species from 59 plant families to treat infections and infectious diseases. The Cultural Important Index (CI) showed that the Leguminosae, Euphorbiaceae, Asteraceae, Lauraceae, Apocynaceae, Menispermaceae, and Lamiaceae were the most commonly used families. As for species, Cleidion javanicum, Tinospora crispa, Litsea cubeba, Aesculus assamica, Tadehagi triquetrum, Senna alata, Tithonia diversifolia, Embelia sessiliflora, and Combretum indicum were the most commonly used in treatments of infectious diseases. We suggest that these plant species should be the first to be pharmacologically tested for possible development of medicines, and the remaining species registered should subsequently undergo testing.', 'corpus_id': 219330057, 'score': 0}, {'doc_id': '219064889', 'title': 'Saudi Scientific Diabetes Society Position Statement: Management of Diabetes Mellitus in the Pandemic of COVID-19', 'abstract': 'About 10% coronavirus (COVID-19) infected patients are with diabetes comorbidity. Also, diabetes promotes severe progression in COVID-19 patients. Diabetes comorbidity is associated with significant mortality in those people with COVID-19. In this position statement, the management of diabetes in cases of COVID-19, has been presented. The impact of diabetes on the morbidity and mortality of COVID-19, as well as both the target glucose level and the method of blood glucose control have been presented in details.', 'corpus_id': 219064889, 'score': 0}, {'doc_id': '86768771', 'title': 'Role of mushrooms in gestational diabetes mellitus', 'abstract': 'Many studies have shown that plant-based diets and Mediterranean diets can lower the risk of development of gestational diabetes mellitus. Plants have been the main source of medicines since ancient times. Despite tremendous advances in medicinal chemistry, synthetic drugs have not provided cures to many diseases due to their adverse side effects or diminution in response after prolonged use. Medicinal mushrooms have been used traditionally as an anti-diabetic food for centuries especially in countries such as China, Japan, India and Korea. These are source of natural bioactive compounds. The bioactive constituents are polysaccharides, proteins, dietary fibres, lectins, lactones, alkaloids, terpenoids, sterols and phenolic compounds which have various health benefits. This review will focus on recent examples of diverse types of mushrooms that have been validated by scientific evaluation as having promising activity for the prevention and/or treatment of gestational diabetes mellitus. Dietary components and plant-derived molecules can be used in the future to complement current treatment strategies for gestational diabetes mellitus.', 'corpus_id': 86768771, 'score': 1}, {'doc_id': '218970251', 'title': 'Herbal drugs having antiviral potency can be used in treatment of corona virus disease', 'abstract': 'During the past few years herbal medicine has gained exponential growth in the field of medicine in all over the world In comparison to other countries India is the largest producer of herbal medicine The current review focuses on herbal preparation and plant recently evaluated having antiviral potency in the world This paper until focus on different beneficial aspects of herbal medicine as Antiviral activity and these herbal drugs can be used for the treatment of Corona virus disease', 'corpus_id': 218970251, 'score': 0}, {'doc_id': '219183228', 'title': 'A Summary of Proven Scientific Research Compounds Found in the Yucca Plant Used for the Treatment or Prevention of Viral Infections and Other Human Maladies', 'abstract': ""Five phenolic constituents have been identified in Yucca schidigera bark, and their structures were established by spectral (FABMS and NMR) experiments. These included two known stilbenes, trans-3,4',5-trihydroxystilbene (resveratrol) and trans-3,3',5,5'-tetrahydroxy-4'methoxystilbene, as well as three novel compounds, yuccaols A, B, and C, with spiro-structures rarely occurring in the plant kingdom. It is suggested that yuccaols A-C are biosynthethized via attachment of a stilbenic derivative to the carbocationic intermediate of the oxidative flavanone-flavonol conversion. Determination of phenolic compounds in Yucca gloriosa bark and root by LCMS/MS https://www.ncbi.nlm.nih.gov/pubmed/18502074"", 'corpus_id': 219183228, 'score': 0}, {'doc_id': '219286291', 'title': 'Intermittent fasting—The new lifestyle?', 'abstract': 'Obesity has become a pandemic that affects all populations of all ages living in countries of all income levels. Attributed to sedentary lifestyle and energy dense diet, obesity is a risk factor for cardiovascular disease, insulin resistance, type 2 diabetes, and even some types of cancer. These comorbidities also affect the immune system adversely and can exacerbate infectious diseases as observed in the most recent case of pandemic COVID-19.', 'corpus_id': 219286291, 'score': 0}, {'doc_id': '220264972', 'title': 'Anti-Viral Medicinal Plants & Their Chemical Constituents, Experimental and Clinical Pharmacology of Antiviral Plants', 'abstract': 'Almost more than thousand traditional plants show important role in the cure of health-related issues from the ancient times. Medicinal plants &amp; the herbs effective for this purpose. These plants derivative medicine have potential against different problem. In this we discussed different medicinal plants like boerhavia diffusa, Phyllanthus amarus, eclipta alba, andrographics paniculate, curcuma longa, glycyrrhiza glabra and many more which shows antiviral activities. Important phytochemical constituents which derived from the different part of the plants possess flavonoids, alkaloids, lignin’s, terpenes etc.', 'corpus_id': 220264972, 'score': 1}, {'doc_id': '219980896', 'title': 'Effects of prebiotic dietary fibers and probiotics on human health: With special focus on recent advancement in their encapsulated formulations', 'abstract': '\n               Abstract\n               \n                  Background\n                  Dietary fibers (DFs) are known as potential formulations in human health due to their beneficial effects in control of life-threatening chronic diseases including cardiovascular disease (CVD), diabetes mellitus, obesity and cancer. In recent decades scientists around the globe have shown tremendous interest to evaluate the interplay between DFs and gastrointestinal (GIT) microbiota. Evidences from various epidemiological and clinical trials have revealed that DFs modulate formation and metabolic activities of the microbial communities residing in the human GIT which in turn play significant roles in maintaining health and well-being. Furthermore, interestingly, a rapidly growing literature indicates success of DFs being prebiotics in immunomodulation, namely the stimulation of innate, cellular and humoral immune response, which could also be linked with their significant roles in modulation of the probiotics (live beneficial microorganisms).\n               \n               \n                  Scope and approach\n                  The main focus of the current review is to expressively highlight the importance of DFs being prebiotics in human health in association with their influence on gut microbiota. Now in order to significantly achieve the promising health benefits from these prebiotics, it is aimed to develop novel formulations to enhance and scale up their efficacy. Therefore, finally, herein unlike previously published articles, we highlighted different kinds of prebiotic and probiotic formulations which are being regarded as hot research topics among the scientific community now a days.\n               \n               \n                  Conclusion\n                  The information in this article will specifically provide a platform for the development of novel functional foods the demands for which has risen drastically in recent years.\n               \n            ', 'corpus_id': 219980896, 'score': 1}]"
193	{'doc_id': '233577453', 'title': 'Closing the loop on take, make, waste: Investigating circular economy practices in the Swedish fashion industry', 'abstract': 'Abstract The fashion industry is one of the most wasteful consumer industries in the world. Through the advent of fast fashion – trendy, low-cost clothing produced by global fashion brands – clothing has evolved from a durable good to a daily purchase. In recent years, the concept of the circular economy, a framework for a more efficient, closed-loop economy, has emerged as a key way forward in the transition to a more sustainable and less wasteful fashion industry. This paper investigates how the Swedish fashion industry has implemented circular economy principles. Drawing on interviews with the founders, CEOs, and/or brand sustainability managers of 19 Swedish fashion brands, this article maps circular economy strategies across key stages: take, make, and waste. Crucially, for the fashion industry to move towards circularity, this paper argues that brands must integrate these strategies across supply chains, rather than limiting them to the waste stage. The analysis explores the gaps between circular economy principles and practice, identifying challenges inherent in fashion brand approaches. It concludes with recommendations for further study of the circular economy and the fashion industry.', 'corpus_id': 233577453}	17017	[{'doc_id': '234816606', 'title': 'How Does Sustainability Affect Consumer Choices in the Fashion Industry?', 'abstract': 'The fashion industry being one of the most polluting industries in the world means that it is an industry with an immense potential for change. Consumers are central and are closely intertwined with how companies act. This research reflects consumer perspectives and practices towards the topic of sustainability implemented in the fashion industry. The relevance of sustainability in the fashion industry and the key role of consumers in its implementation are undeniable and confirmed by consumers in a representation of general awareness and concern, despite not always being translated into actual practices. A qualitative research methodology, followed by a set of interviews conducted with consumers, revealed that the great majority are implementing a variety of practices when making their buying choices towards fashion items. Barriers such as lack of education, information, knowledge and transparency were identified, and this aspect was shared by consumers as a reason why they are not motivated to make more conscious decisions. Companies should educate consumers from a general perspective and focus on the group of consumers that are not implementing sustainability in the fashion industry in their buying choices, as they represent the potential for the future.', 'corpus_id': 234816606, 'score': 1}, {'doc_id': '232401389', 'title': 'Product-Service Systems and Sustainability: Analysing the Environmental Impacts of Rental Clothing', 'abstract': 'Business models like product-service systems (PSSs) often recognise different sustainability goals and are seen as solutions for the impacts of consumption and fast fashion, but there is a lack of evidence supporting the environmental claims of such business models for clothing. The research aimed to understand if rental clothing business models such as PSSs have the environmental benefits often purported by quantifying the environmental impacts of rental formal dresses in a life-cycle assessment (LCA) in a case study in Stockholm, Sweden. The effects of varying consumer behaviour on the potential impact of a PSS vs. linear business model are explored through three functional units and 14 consumption scenarios. How users decide to engage with clothing PSSs dictates the environmental savings potential that a PSS can have, as shown in how many times consumers wear garments, how they use rental to substitute their purchasing or use needs, as well as how consumers travel to rental store locations.', 'corpus_id': 232401389, 'score': 0}, {'doc_id': '233346589', 'title': 'WHERE NOW FOR GREEN FASHION ? COVID-19 AND THE READY-MADE GARMENTS INDUSTRY ( RMG ) CRISIS IN BANGLADESH', 'abstract': 'In the past two decades the global fashion industry has been severely criticized for its failure to tackle climate change and sustainability issues across its supply chains. The response of many in the industry has addressed key criticisms by encouraging new codes of practice that apply to all stages of the supply chain. Industry-wide efforts have, however, become severely affected by the COVID-19 crisis in 2020, providing not just a crisis to green fashion, but also an existential threat to companies and organisations across the world due to the sharp decline in international trade. This article examines key issues relating to green fashion, the global COVID-19 pandemic and the crisis in the RMG industry in Bangladesh. The focus of the article is Bangladesh, the second largest centre for clothing manufacturing in the world and will concentrate on supply chain relationships relating to the manufacturing, distribution and disposal of clothing and those sustainability issues arising.', 'corpus_id': 233346589, 'score': 0}, {'doc_id': '234755829', 'title': 'Challenges for sustainability in textile craft and fashion design', 'abstract': 'India is known globally for her rich heritage of textiles and handcrafts. Consistently decreasing number of artisans indicates need to revive the craft of artisans as an important source of livelihood. Vast availability of cheap, low-quality clothing allows fast fashion, conspicuous consumption and premature disposal of fashion products. Therefore, production of sustainable traditional modes of fashion design in affordable price is a challenge. Further, lack of knowledge and tools to build capacity, especially in the areas of skill development, design intervention, technical innovation, productivity enhancement and environmental sustainability; lack of working capital and access to credit/loan facilities and lack of access to markets leaves craftsmen vulnerable to middlemen. Waste is another growing problem associated with environmental and social impacts. Promotion of handcrafted innovative and exclusive fashion designs intrinsically from the use of natural resources with collaboration of designers, technology upgradation, fusion of traditional and contemporary styles, fair trade and other business models, human resource strategies, zero waste system of manufacturing, government policy for safeguarding employment opportunities, e-marketing, eco-labeling and social-labeling are some of the ways to incorporate sustainability and ethics in textile craft and fashion designs. The paper provides a deep insight for all the above aspects.', 'corpus_id': 234755829, 'score': 1}, {'doc_id': '234873429', 'title': 'Virtual carbon and water flows embodied in denim trade', 'abstract': 'Abstract The environmental impacts of the fashion industry have been aroused wide concerns. The globalization and fragmentation of the textile and fashion system have led to the uneven distribution of environmental consequences. As denim is the fabric of jeans that is representative of fashion, this study assessed virtual carbon and water flows embodied in the global denim-product trade, and footprints of denim production were quantified by life-cycle assessment and water footprint assessment. Results indicated that virtual carbon embodied in the global denim trade increased obviously from 14.8 Mt CO2e in 2001 to 16.0 Mt CO2e in 2018, and the virtual water consumption dropped from 5.6 billion m3 to 4.7 billion m3 from 2001 to 2018. The denim fabric production and cotton fibre production respectively contribute the most of the carbon emissions and water consumption. Polyester blended denim has 5% larger carbon footprint and 72% lower water footprint than cotton denim, and contributes to increasing embodied carbon emissions (from 4% in 2001 to 43% in 2018). Increasing the utilization of polyester blended denim would save water but face more pressures on carbon emission reduction. In the past two decades, virtual carbon and water flows embodied in the global denim trade are relocating, main jean consumers (i.e., the USA, EU-15, and Japan) withdraw the denim manufacturing supply chain and developing countries (i.e., China, India, and Pakistan) with higher carbon and water footprint undertake main global denim production, facing increasing climate-related risks and water crisis. The global South cooperation helps share successful experiences, save production cost, and lessen resource consumption and environmental emissions. The production and consumption of denim should be shifted to circular and sustainable ways and new business models are required. The analysis framework can provide the basis for exploring environmental flows of product-level trade, and results can offer a basis for environmental policies and control strategies of the fashion industry, and as well as the sustainable production and consumption of garment.', 'corpus_id': 234873429, 'score': 0}, {'doc_id': '233539535', 'title': 'The need to decelerate fast fashion in a hot climate - A global sustainability perspective on the garment industry', 'abstract': 'Abstract Controversy exists regarding the scale of the impacts caused by fast fashion. This article aims to provide a robust basis for discussion about the geography, the scale and the temporal trends in the impacts of fast fashion because the globalisation of the fashion industry means original, peer-reviewed, quantitative assessments of the total impacts are relatively rare and difficult to compare. This article presents the first application of Eora, a multiregional environmentally extended input output model, to the assessment of the impacts of clothing and footwear value chain. We focus on the key environmental indicators of energy consumption, climate and water resources impacts, and social indicators of wages and employment. The results of the analysis indicate that the climate impact of clothing and footwear consumption rose from 1.0 to 1.3\xa0Gt carbon dioxide equivalent over the 15 years to 2015. China, India, the USA and Brazil dominate these figures. The trends identified in this and the other indicators represent small increases over the study period compared to the 75% increase in textile production, meaning that the impacts per garment have improved considerably. On the other hand, the climate and water use impacts are larger as a proportion of global figures than the benefits provided via employment and wages. Our analysis of energy consumption suggests most of the per-garment improvement in emissions is the result of increased fashion-industrial efficiency, with a lesser role being played by falling carbon intensity among energy suppliers. While both the social benefits and environmental impacts per mass of garment appear to have decreased in recent times, much greater improvements in the absolute carbon footprint of the fashion industry are attainable by eliminating fossil-fueled electricity supplies, and by eliminating fast fashion as a business model.', 'corpus_id': 233539535, 'score': 1}, {'doc_id': '214443970', 'title': 'Fashion Exhibitions as Scholarship: Evaluation Criteria for Peer Review', 'abstract': 'Curated exhibitions are places where research practice, creative design, storytelling, and aesthetics converge. In this article, we use the term “fashion exhibition” to refer to the organized display of extant dress-related items within museums or other public spaces. Curation, as a form of creative design research, produces numerous outcomes including museum exhibitions, digital archives, and associated publications; however, our field has not yet established a method to peer review fashion exhibitions. In this article, we build upon the work of previous scholars to propose criteria for evaluating fashion exhibitions. In doing so, we aim to elevate the scholarly status of fashion exhibitions, particularly those mounted by modestly funded institutions, and use the recent fashion exhibition, “Women Empowered: Fashions from the Frontline,” as an example to illustrate our argument.', 'corpus_id': 214443970, 'score': 1}, {'doc_id': '234313430', 'title': 'Redesigning of fashion supply chain', 'abstract': 'Abstract The fashion industry has been subject of pivotal trends over the past few decades. The industry has evolved into a complex, fragmented, global system which at its very core is based on the notion of continual consumption of the “new” and discard the old. The emergence of the “fast fashion” business model has increased the introduction of trends leading to premature product replacement and fashion obsolescence. It also has major negative environmental and social impacts, particularly on those at the bottom of the supply chain. Fashion in the 21st century is typically fast fashion, characterized by mass production, high turnover, and goods designed for a short lifespan. Like a kimono appears to be the antithesis of fast fashion in terms of production and consumption. A kimono takes time to create and usually has a long lifespan. However, in the current global fashion market, there has been a growing trend towards slow fashion, which involves longer production times, use of local materials, and a focus on quality and sustainability. Millennials nowadays are found to be more fashion-conscious, and relate themselves to the fashion brands they wear.', 'corpus_id': 234313430, 'score': 0}, {'doc_id': '195395854', 'title': 'Human Factor in Apparel and Fashion Exhibition Design', 'abstract': 'The need to preserve past cultures, textile and clothing creations, identities of a society in a certain time period, made the development of exhibition design area relevant. Exhibition spaces must give innovative answers to the aspirations of the society, which are the visitors, reason of being of its existence. In this framework, human factor is essential in the exhibition design field.', 'corpus_id': 195395854, 'score': 1}, {'doc_id': '234101778', 'title': 'Waste management strategies in fashion and textiles industry: Challenges are in governance, materials culture and design-centric', 'abstract': 'Abstract This paper aims to provide further insight into the applications and spillover of the circular economy into the fashion system. Through the systemic analysis of case histories, the research evaluates the effects of 40 circular economy actions in their relationship with sustainable development goals, by assessing how they have been able to integrate and balance the economic, social, and environmentally sustainable development dimensions into the fashion system. What emerges is that the fashion industry can be a potent ground for the implementation of a circular economy’s principle and could also provide support in understanding its evolution and adjusting its objectives accordingly. Fashion is strategic and could be a perfect field for testing a new approach to raw material and waste and for the development of a new context of the inquiry, defined as “Circular Economy for fashion.” The focus of this chapter will be various approaches to reduce waste such as the 3R approach (reduce, recycle, and reuse) for fashion wastes; donations to charity; upcycle, rent the runway; and benefits of all the newer approaches including the relevance of circular economy and sustainable development goals. This chapter deals with many subjects of sustainability from evolutionary aspects to developmental aspects. The impacts of the textile and garment industry waste generation on the environment and human well-being have been discussed. Finally, a framework to reduce waste and circular fashion future is proposed.', 'corpus_id': 234101778, 'score': 0}]
194	"{'doc_id': '83666790', 'title': 'Is my species distribution model fit for purpose? Matching data and models to applications', 'abstract': ""Species distribution models (SDMs) are used to inform a range of ecological, biogeographical and conservation applications. However, users often underestimate the strong links between data type, model output and suitability for end-use. We synthesize current knowledge and provide a simple framework that summarizes how interactions between data type and the sampling process (i.e. imperfect detection and sampling bias) determine the quantity that is estimated by a SDM. We then draw upon the published literature and simulations to illustrate and evaluate the information needs of the most common ecological, biogeographical and conservation applications of SDM outputs. We find that, while predictions of models fitted to the most commonly available observational data (presence records) suffice for some applications, others require estimates of occurrence probabilities, which are unattainable without reliable absence records. Our literature review and simulations reveal that, while converting continuous SDM outputs into categories of assumed presence or absence is common practice, it is seldom clearly justified by the application's objective and it usually degrades inference. Matching SDMs to the needs of particular applications is critical to avoid poor scientific inference and management outcomes. This paper aims to help modellers and users assess whether their intended SDM outputs are indeed fit for purpose."", 'corpus_id': 83666790}"	19732	"[{'doc_id': '54627583', 'title': 'Prediction of the distribution of Arctic-nesting pink-footed geese under a warmer climate scenario', 'abstract': 'Global climate change is expected to shift species ranges polewards, with a risk of range contractions and population declines of especially high-Arctic species. We built species distribution models for Svalbard-nesting pink-footed geese to relate their occurrence to environmental and climatic variables, and used the models to predict their distribution under a warmer climate scenario. The most parsimonious model included mean May temperature, the number of frost-free months and the proportion of moist and wet mossdominated vegetation in the area. The two climate variables are indicators for whether geese can physiologically fulfil the breeding cycle or not and the moss vegetation is an indicator of suitable feeding conditions. Projections of the distribution to warmer climate scenarios propose a large north- and eastward expansion of the potential breeding range on Svalbard even at modest temperature increases (1 and 21C increase in summer temperature, respectively). Contrary to recent suggestions regarding future distributions of Arctic wildlife, we predict that warming may lead to a further growth in population size of, at least some, Arctic breeding geese.', 'corpus_id': 54627583, 'score': 1}, {'doc_id': '19019299', 'title': 'Correlative and mechanistic models of species distribution provide congruent forecasts under climate change.', 'abstract': 'Good forecasts of climate change impacts on extinction risks are critical for effective conservation management responses. Species distribution models (SDMs) are central to extinction risk analyses. The reliability of predictions of SDMs has been questioned because models often lack a mechanistic underpinning and rely on assumptions that are untenable under climate change. We show how integrating predictions from fundamentally different modeling strategies produces robust forecasts of climate change impacts on habitat and population parameters. We illustrate the principle by applying mechanistic (Niche Mapper) and correlative (Maxent, Bioclim) SDMs to predict current and future distributions and fertility of an Australian gliding possum. The two approaches make congruent, accurate predictions of current distribution and similar, dire predictions about the impact of a warming scenario, supporting previous correlative-only predictions for similar species. We argue that convergent lines of independent evidence provide a robust basis for predicting and managing extinctions risks under climate change.', 'corpus_id': 19019299, 'score': 1}, {'doc_id': '215802454', 'title': 'Comparing and synthesizing quantitative distribution models and qualitative vulnerability assessments to project marine species distributions under climate change', 'abstract': 'Species distribution shifts are a widely reported biological consequence of climate-driven warming across marine ecosystems, creating ecological and social challenges. To meet these challenges and inform management decisions, we need accurate projections of species distributions. Quantitative species distribution models (SDMs) are routinely used to make these projections, while qualitative climate change vulnerability assessments are becoming more common. We constructed SDMs, compared SDM projections to expectations from a qualitative expert climate change vulnerability assessment, and developed a novel approach for combining the two methods to project the distribution and relative biomass of 49 marine species in the Northeast Shelf Large Marine Ecosystem under a “business as usual” climate change scenario. A forecasting experiment using SDMs highlighted their ability to capture relative biomass patterns fairly well (mean Pearson’s correlation coefficient between predicted and observed biomass = 0.24, range = 0–0.6) and pointed to areas needing improvement, including reducing prediction error and better capturing fine-scale spatial variability. SDM projections suggest the region will undergo considerable biological changes, especially in the Gulf of Maine, where commercially-important groundfish and traditional forage species are expected to decline as coastal fish species and warmer-water forage species historically found in the southern New England/Mid-Atlantic Bight area increase. The SDM projections only occasionally aligned with vulnerability assessment expectations, with agreement more common for species with adult mobility and population growth rates that showed low sensitivity to climate change. Although our blended approach tried to build from the strengths of each method, it had no noticeable improvement in predictive ability over SDMs. This work rigorously evaluates the predictive ability of SDMs, quantifies expected species distribution shifts under future climate conditions, and tests a new approach for integrating SDMs and vulnerability assessments to help address the complex challenges arising from climate-driven species distribution shifts.', 'corpus_id': 215802454, 'score': 1}, {'doc_id': '236968585', 'title': ""The evolutionary genomics of species' responses to climate change."", 'abstract': ""Climate change is a threat to biodiversity. One way that this threat manifests is through pronounced shifts in the geographical range of species over time. To predict these shifts, researchers have primarily used species distribution models. However, these models are based on assumptions of niche conservatism and do not consider evolutionary processes, potentially limiting their accuracy and value. To incorporate evolution into the prediction of species' responses to climate change, researchers have turned to landscape genomic data and examined information about local genetic adaptation using climate models. Although this is an important advancement, this approach currently does not include other evolutionary processes-such as gene flow, population dispersal and genomic load-that are critical for predicting the fate of species across the landscape. Here, we briefly review the current practices for the use of species distribution models and for incorporating local adaptation. We next discuss the rationale and theory for considering additional processes, reviewing how they can be incorporated into studies of species' responses to climate change. We summarize with a conceptual framework of how manifold layers of information can be combined to predict the potential response of specific populations to climate change. We illustrate all of the topics using an exemplar dataset and provide the source code as potential tutorials. This Perspective is intended to be a step towards a more comprehensive integration of population genomics with climate change science."", 'corpus_id': 236968585, 'score': 0}, {'doc_id': '236095454', 'title': 'Quest for New Space for Restricted Range Mammals: The Case of the Endangered Walia Ibex', 'abstract': 'Populations of large mammals have declined at alarming rates, especially in areas with intensified land use where species can only persist in small habitat fragments. To support conservation planning, we developed habitat suitability models for the Walia ibex (Capra walie), an endangered wild goat endemic to the Simen Mountains, Ethiopia. We calibrated several models that differ in statistical properties to estimate the spatial extent of suitable habitats of the Walia ibex in the Simen Mountains, as well as in other parts of the Ethiopian highlands to assess potentially suitable areas outside the current distribution range of the species. We further addressed the potential consequences of future climate change using a climate model with four emission scenarios. Model projections estimated the potential suitable habitat under current climate to 501–672 km2 in Simen and 6,251–7,732 km2 in other Ethiopian mountains. Under projected climate change by 2,080, the suitable habitat became larger in Simen but smaller in other parts of Ethiopia. The projected expansion in Simen is contrary to the general expectation of shrinking suitable habitats for high-elevation species under climate warming and may partly be due to the ruggedness of these particular mountains. The Walia ibex has a wide altitudinal range and is able to exploit very steep slopes, allowing it to track the expected vegetation shift to higher altitudes. However, this potential positive impact may not last long under continued climate warming, as the species will not have much more new space left to colonize. Our study indicates that the current distribution range can be substantially increased by reintroducing and/or translocating the species to other areas with suitable habitat. Indeed, to increase the viability and prospects for survival of this flagship species, we strongly recommend human-assisted reintroduction to other Ethiopian mountains. Emulating the successful reintroduction of the Alpine ibex that has spread from a single mountain in Italy to its historical ranges of the Alps in Europe might contribute to saving the Walia ibex from extinction.', 'corpus_id': 236095454, 'score': 0}, {'doc_id': '236436900', 'title': 'Long-term changes in populations of rainforest birds in the Australia Wet Tropics bioregion: a climate-driven biodiversity emergency', 'abstract': 'Many authors have suggested that the vulnerability of montane biodiversity to climate change worldwide is significantly higher than in most other ecosystems. Despite the extensive variety of studies predicting severe impacts of climate change globally, few studies have empirically validated the predicted changes in distribution and population density. Here, we used 17 years of bird monitoring across latitudinal/elevational gradients in the rainforest of the Australian Wet Tropics World Heritage Area to assess changes in local abundance and distribution. We used relative abundance in 1977 surveys across 114 sites ranging from 0-1500m above sea level and utilised a trend analysis approach (TRIM) to investigate elevational shifts in abundance of 42 species between 2000 – 2016. The local abundance of most mid and high elevation species has declined at the lower edges of their distribution by >40% while lowland species increased by up to 190% into higher elevation areas. Upland-specialised species and regional endemics have undergone dramatic population declines of almost 50%. The “Outstanding Universal Value” of the Australian Wet Tropics World Heritage Area, one of the most irreplaceable biodiversity hotspots on Earth, is rapidly degrading. These observed impacts are likely to be similar in many tropical montane ecosystems globally.', 'corpus_id': 236436900, 'score': 0}, {'doc_id': '236999951', 'title': 'Temporal Assessment of Eastern Spotted Skunk Geographic Distribution', 'abstract': 'Abstract Spilogale putorius (Eastern Spotted Skunk) experienced range-wide population declines beginning in the mid-1900s with no clear understanding of the causal mechanism or whether such declines were associated with range contractions. Species-distribution models can provide a powerful framework to assess changes in landscape suitability in response to changing environmental conditions. Herein, we modeled time-stepped distributions of suitable environmental conditions for Eastern Spotted Skunks from 1938 to 2016 in Maxent, incorporating climate and land-cover predictors. Climate and land-cover variables reliably predicted landscape suitability of Eastern Spotted Skunks over time. We found a 37% decline in suitable area from historic predictions, consistent with reports of population declines in these areas. Our predicted landscape-suitability maps can be used to evaluate the current distribution of environmentally suitable conditions for the species as well as guide research and conservation efforts.', 'corpus_id': 236999951, 'score': 0}, {'doc_id': '235647715', 'title': 'Coral distribution and bleaching vulnerability areas in Southwestern Atlantic under ocean warming', 'abstract': 'Global climate change is a major threat to reefs by increasing the frequency and severity of coral bleaching events over time, reducing coral cover and diversity. Ocean warming may cause shifts in coral communities by increasing temperatures above coral’s upper thermal limits in tropical regions, and by making extratropical regions (marginal reefs) more suitable and potential refugia. We used Bayesian models to project coral occurrence, cover and bleaching probabilities in Southwestern Atlantic and predicted how these probabilities will change under a high-emission scenario (RCP8.5). By overlapping these projections, we categorized areas that combine high probabilities of coral occurrence, cover and bleaching as vulnerability-hotspots. Current coral occurrence and cover probabilities were higher in the tropics (1°S–20°S) but both will decrease and shift to new suitable extratropical reefs (20°S–27°S; tropicalization) with ocean warming. Over 90% of the area present low and mild vulnerability, while the vulnerability-hotspots represent\u2009~\u20093% under current and future scenarios, but include the most biodiverse reef complex in South Atlantic (13°S–18°S; Abrolhos Bank). As bleaching probabilities increase with warming, the least vulnerable areas that could act as potential refugia are predicted to reduce by 50%. Predicting potential refugia and highly vulnerable areas can inform conservation actions to face climate change.', 'corpus_id': 235647715, 'score': 0}, {'doc_id': '2352667', 'title': 'Predicting species distribution: offering more than simple habitat models.', 'abstract': 'In the last two decades, interest in species distribution models (SDMs) of plants and animals has grown dramatically. Recent advances in SDMs allow us to potentially forecast anthropogenic effects on patterns of biodiversity at different spatial scales. However, some limitations still preclude the use of SDMs in many theoretical and practical applications. Here, we provide an overview of recent advances in this field, discuss the ecological principles and assumptions underpinning SDMs, and highlight critical limitations and decisions inherent in the construction and evaluation of SDMs. Particular emphasis is given to the use of SDMs for the assessment of climate change impacts and conservation management issues. We suggest new avenues for incorporating species migration, population dynamics, biotic interactions and community ecology into SDMs at multiple spatial scales. Addressing all these issues requires a better integration of SDMs with ecological theory.', 'corpus_id': 2352667, 'score': 1}, {'doc_id': '11283373', 'title': 'More than the sum of the parts: forest climate response from joint species distribution models.', 'abstract': 'The perceived threat of climate change is often evaluated from species distribution models that are fitted to many species independently and then added together. This approach ignores the fact that species are jointly distributed and limit one another. Species respond to the same underlying climatic variables, and the abundance of any one species can be constrained by competition; a large increase in one is inevitably linked to declines of others. Omitting this basic relationship explains why responses modeled independently do not agree with the species richness or basal areas of actual forests. We introduce a joint species distribution modeling approach (JSDM), which is unique in three ways, and apply it to forests of eastern North America. First, it accommodates the joint distribution of species. Second, this joint distribution includes both abundance and presence-absence data. We solve the common issue of large numbers of zeros in abundance data by accommodating zeros in both stem counts and basal area data, i.e., a new approach to zero inflation. Finally, inverse prediction can be applied to the joint distribution of predictions to integrate the role of climate risks across all species and identify geographic areas where communities will change most (in terms of changes in abundance) with climate change. Application to forests in the eastern United States shows that climate can have greatest impact in the Northeast, due to temperature, and in the Upper Midwest, due to temperature and precipitation. Thus, these are the regions experiencing the fastest warming and are also identified as most responsive at this scale.', 'corpus_id': 11283373, 'score': 1}]"
195	{'doc_id': '8982559', 'title': 'Protein stability: a crystallographer’s perspective', 'abstract': 'An understanding of protein stability is essential for optimizing the expression, purification and crystallization of proteins. In this review, discussion will focus on factors affecting protein stability on a somewhat practical level, particularly from the view of a protein crystallographer.', 'corpus_id': 8982559}	15364	"[{'doc_id': '207555963', 'title': 'Cosolvent effects on protein stability.', 'abstract': 'Proteins are marginally stable, and the folding/unfolding equilibrium of proteins in aqueous solution can easily be altered by the addition of small organic molecules known as cosolvents. Cosolvents that shift the equilibrium toward the unfolded ensemble are termed denaturants, whereas those that favor the folded ensemble are known as protecting osmolytes. Urea is a widely used denaturant in protein folding studies, and the molecular mechanism of its action has been vigorously debated in the literature. Here we review recent experimental as well as computational studies that show an emerging consensus in this problem. Urea has been shown to denature proteins through a direct mechanism, by interacting favorably with the peptide backbone as well as the amino acid side chains. In contrast, the molecular mechanism by which the naturally occurring protecting osmolyte trimethylamine N-oxide (TMAO) stabilizes proteins is not clear. Recent studies have established the strong interaction of TMAO with water. Detailed molecular simulations, when used with force fields that incorporate these interactions, can provide insight into this problem. We present the development of a model for TMAO that is consistent with experimental observations and that provides physical insight into the role of cosolvent-cosolvent interaction in determining its preferential interaction with proteins.', 'corpus_id': 207555963, 'score': 1}, {'doc_id': '232335411', 'title': 'Water Contribution to the Protein Folding and Its Relevance in Protein Design and Protein Aggregation', 'abstract': '\u200bWater plays a fundamental role in protein stability. However, the effect of the properties of water on the behaviour of proteins is only partially understood. Several theories have been proposed to give insight into the mechanisms of cold and pressure denaturation, or the limits of temperature and pressure above which no protein has a stable, functional state, or how unfolding and aggregation are related. Here we review our results based on a theoretical approach that can rationalise the water contribution to protein solutions’ free energy. We show, using Monte Carlo simulations, how we can rationalise experimental data with our recent results. We discuss how our findings can help develop new strategies for the design of novel synthetic biopolymers or possible approaches for mitigating neurodegenerative pathologies. Giancarlo Franzese (\u200b \u200b) · Joan Àguila Rojas Secció de Física Estadística i Interdisciplinària–Departament de Física de la Matèria Condensada, Facultat de Física, Universitat de Barcelona, Martí i Franquès 1, Barcelona 08028, Spain Institute of Nanoscience and Nanotechnology (IN2UB), Universitat de Barcelona, Martí i Franquès 1, Barcelona 08028, Spain E-mail: gfranzese@ub.edu', 'corpus_id': 232335411, 'score': 0}, {'doc_id': '232409599', 'title': 'Quantitative Analysis of Protein Unfolded State Energetics: Experimental and Computational Studies Demonstrate That Non-Native Side-Chain Interactions Stabilize Local Native Backbone Structure.', 'abstract': 'Proteins fold on relatively smooth free energy landscapes which are biased toward the native state, but even simple topologies which fold rapidly can experience roughness on their free energy landscape. The details of these interactions are difficult to elucidate experimentally. Closely related to the problem of deciphering the details of the free energy landscape is the problem of defining the interactions in the denatured state ensemble (DSE) which is populated under native conditions, that is, under conditions where the native state is stable. The DSE of many proteins deviates from random coil models, but quantifying and defining the energetics of the transiently populated interactions in this ensemble is extremely challenging. Characterization of the DSE of proteins which fold to compact structures is also relevant to studies of intrinsically disordered proteins (IDPs) since interactions in the dynamic ensemble populated by IDPs can modulate their behavior. Here we show how experimental thermodynamic and pKa measurements can be combined with computational thermodynamic integration to quantify interactions in the DSE. We show that non-native side chain interactions can stabilize native backbone structure in the DSE and demonstrate that that even rapidly folding proteins can form energetically significant non-native interactions in their DSE. As an example, we characterize a non-native salt bridge that stabilizes local native backbone structure in the DSE of a widely studied fast-folding protein, the villin headpiece helical domain. The combined computational experimental approach is applicable to other protein unfolded states and provides insight that is impossible to obtain with either method alone.', 'corpus_id': 232409599, 'score': 0}, {'doc_id': '163137357', 'title': 'Análisis del sistema electrohidraúlico de regulación de velocidad en la unidad No. 1 de la fase AB de la Central Hidroeléctrica Paute orientado a la implementación de filosofías de mantenimiento RCM', 'abstract': 'La investigacion se realizo en la Provincia de Azuay, Canton Sevilla de Oro, Parroquia Amaluza, casa de maquinas de la Central Hidroelectrica Paute, ubicada en el flanco oriental de la Cordillera de Los Andes a 1327 msnm, con una temperatura media anual 18.8o C; y una precipitacion de 2600 mm. La fase AB entro en funcionamiento en 1983. \nEl objetivo general fue analizar el sistema electro hidraulico de regulacion de velocidad de la unidad No. 1 de la fase AB de la Central Hidroelectrica Paute, orientado a la implementacion de filosofias de mantenimiento RCM; considerando la hipotesis de que este analisis proveera la informacion adecuada para garantizar el stock de repuestos y mejorar el plan de mantenimiento actual para un funcionamiento continuo y confiable. El RCM se basa en la formulacion y respuesta de siete preguntas referidas a: funciones, fallas, causas, efectos, consecuencias, tareas proactivas y tareas ""a falta de"". \nPara el efecto se dividio el sistema electrohidraulico en cuatro subsistemas; gobernador electronico, subsistema de acumulacion, valvula esferica y turbina. Se realizo un analisis historico de los equipos del gobernador electronico para determinar su numero de fallas funcionales en el periodo de 15 anos empezando en 1993 y el calculo probabilistico con la aplicacion del CB Predictor herramienta del software Crystal Ball para determinar el numero de repuestos que se podria necesitar en un periodo igual, ya que las fallas del gobernador electronico tienen la caracteristica de aleatoriedad, este calculo tambien se lo hizo extensivo a las otras unidades de la fase AB. En el nivel operativo se dividio a los subsistemas en dos grupos: el gobernador electronico y el subsistema oleodinamico, manteniendo el esquema de areas electronica y mecanica, respectivamente. \nLa propuesta del plan de mantenimiento se elaboro en base a las tareas proactivas a realizarse para eliminar o disminuir las consecuencias de las fallas funcionales, considerando la frecuencia de las tareas adicionales adaptadas a la periodicidad del plan de mantenimiento actual. El analisis de la informacion permitio concluir que el gobernador electronico concentra la gran mayoria de fallas funcionales, y que las consecuencias son de tipo: fallas ocultas por los elementos de proteccion, ambientales por las fugas de aceite hacia las aguas del rio Paute, operacionales por el lucro cesante que genera la indisponibilidad de la unidad y no operacionales porque los costos de reparacion son bajos; ademas se concluye sobre la falta de informacion veraz y codificacion de los equipos para una adecuada trazabilidad y toma de decisiones, asi como la falta de informacion tecnica de los equipos en especial del subsistema oleodinamico. \nLas recomendaciones estan orientadas al reporte adecuado de las ordenes de trabajo como fuentes de informacion, la codificacion de los equipos y al mantenimiento de un stock de repuestos de acuerdo con el informe arrojado por la aplicacion del software Crystal Ball.', 'corpus_id': 163137357, 'score': 0}, {'doc_id': '11990689', 'title': 'Optimization of protein buffer cocktails using Thermofluor.', 'abstract': 'The stability and homogeneity of a protein sample is strongly influenced by the composition of the buffer that the protein is in. A quick and easy approach to identify a buffer composition which increases the stability and possibly the conformational homogeneity of a protein sample is the fluorescence-based thermal-shift assay (Thermofluor). Here, a novel 96-condition screen for Thermofluor experiments is presented which consists of buffer and additive parts. The buffer screen comprises 23 different buffers and the additive screen includes small-molecule additives such as salts and nucleotide analogues. The utilization of small-molecule components which increase the thermal stability of a protein sample frequently results in a protein preparation of higher quality and quantity and ultimately also increases the chances of the protein crystallizing.', 'corpus_id': 11990689, 'score': 1}, {'doc_id': '30412443', 'title': 'Thermal stability of lysozyme as a function of ion concentration: A reappraisal of the relationship between the Hofmeister series and protein stability', 'abstract': 'Anion and cation effects on the structural stability of lysozyme were investigated using differential scanning calorimetry. At low concentrations (<5 mM) anions and cations alter the stability of lysozyme but they do not follow the Hofmeister (or inverse Hofmeister) series. At higher concentrations protein stabilization follows the well‐established Hofmeister series. Our hypothesis is that there are three mechanisms at work. At low concentrations the anions interact with charged side chains where the presence of the ion can alter the structural stability of the protein. At higher concentrations the low charge density anions perchlorate and iodide interact weakly with the protein. Their presence however reduces the Gibbs free energy required to hydrate the core of the protein that is exposed during unfolding therefore destabilizing the structure. At higher concentrations the high charge density anions phosphate and sulfate compete for water with the protein as it unfolds increasing the Gibbs free energy required to hydrate the newly exposed core of the protein therefore stabilizing the structure.', 'corpus_id': 30412443, 'score': 1}, {'doc_id': '23507790', 'title': 'Recent Advances in the Applications of Ionic Liquids in Protein Stability and Activity: A Review', 'abstract': 'Room temperatures ionic liquids are considered as miraculous solvents for biological system. Due to their inimitable properties and large variety of applications, they have been widely used in enzyme catalysis and protein stability and separation. The related information present in the current review is helpful to the researchers working in the field of biotechnology and biochemistry to design or choose an ionic liquid that can serve as a noble and selective solvent for any particular enzymatic reaction, protein preservation and other protein based applications. We have extensively analyzed the methods used for studying the protein–IL interaction which is useful in providing information about structural and conformational dynamics of protein. This can be helpful to develop and understanding about the effect of ionic liquids on stability and activity of proteins. In addition, the affect of physico-chemical properties of ionic liquids, viz. hydrogen bond capacity and hydrophobicity on protein stability are discussed.', 'corpus_id': 23507790, 'score': 1}, {'doc_id': '232098349', 'title': 'Macromolecular crowding effects on the kinetics of opposing reactions catalyzed by alcohol dehydrogenase', 'abstract': 'In order to better understand how the complex, densely packed, heterogeneous milieu of a cell influences enzyme kinetics, we exposed opposing reactions catalyzed by yeast alcohol dehydrogenase (YADH) to both synthetic and protein crowders ranging from 10 to 550 kDa. The results reveal that the effects from macromolecular crowding depend on the direction of the reaction. The presence of the synthetic polymers, Ficoll and dextran, decrease Vmax and Km for ethanol oxidation. In contrast, these crowders have little effect or even increase these kinetic parameters for acetaldehyde reduction. This increase in Vmax is likely due to excluded volume effects, which are partially counteracted by viscosity hindering release of the NAD+ product. Macromolecular crowding is further complicated by the presence of a depletion layer in solutions of dextran larger than YADH, which diminishes the hindrance from viscosity. The disparate effects from 25 g/L dextran or glucose compared to 25 g/L Ficoll or sucrose reveals that soft interactions must also be considered. Data from binary mixtures of glucose, dextran, and Ficoll support this “tuning” of opposing factors. While macromolecular crowding was originally proposed to influence proteins mainly through excluded volume effects, this work compliments the growing body of evidence revealing that other factors, such as preferential hydration, chemical interactions, and the presence of a depletion layer also contribute to the overall effect of crowding.', 'corpus_id': 232098349, 'score': 0}, {'doc_id': '10972802', 'title': 'Protein crowder charge and protein stability.', 'abstract': 'Macromolecular crowding effects arise from steric repulsions and weak, nonspecific, chemical interactions. Steric repulsions stabilize globular proteins, but the effect of chemical interactions depends on their nature. Repulsive interactions such as those between similarly charged species should reinforce the effect of steric repulsions, increasing the equilibrium thermodynamic stability of a test protein. Attractive chemical interactions, on the other hand, counteract the effect of hard-core repulsions, decreasing stability. We tested these ideas by using the anionic proteins from Escherichia coli as crowding agents and assessing the stability of the anionic test protein chymotrypsin inhibitor 2 at pH 7.0. The anionic protein crowders destabilize the test protein despite the similarity of their net charges. Thus, weak, nonspecific, attractive interactions between proteins can overcome the charge-charge repulsion and counterbalance the stabilizing effect of steric repulsion.', 'corpus_id': 10972802, 'score': 1}, {'doc_id': '232323985', 'title': 'Free-energy changes of bacteriorhodopsin point mutants measured by single-molecule force spectroscopy', 'abstract': 'Significance Membrane proteins are of great biological and biomedical interest; they constitute 30% of encoded proteins and are the targets of ∼50% of drugs. Measurements of membrane protein energetics by mutating individual amino acids provide information that cannot be obtained from structural studies alone. Traditional thermodynamic assays employ chemical denaturants and nonnative micelles that imperfectly reflect the biologically relevant conditions of the native lipid bilayer. Additionally, chemical denaturation is not thermodynamically reversible for many proteins, limiting its scope. Here, we present a different approach that is based on the mechanical unfolding of individual membrane proteins. Our method yields precise energetic characterization of membrane protein point mutants embedded in their native lipid bilayer without introducing confounding denaturant interactions or requiring global reversibility. Single amino acid mutations provide quantitative insight into the energetics that underlie the dynamics and folding of membrane proteins. Chemical denaturation is the most widely used assay and yields the change in unfolding free energy (ΔΔG). It has been applied to >80 different residues of bacteriorhodopsin (bR), a model membrane protein. However, such experiments have several key limitations: 1) a nonnative lipid environment, 2) a denatured state with significant secondary structure, 3) error introduced by extrapolation to zero denaturant, and 4) the requirement of globally reversible refolding. We overcame these limitations by reversibly unfolding local regions of an individual protein with mechanical force using an atomic-force-microscope assay optimized for 2 μs time resolution and 1 pN force stability. In this assay, bR was unfolded from its native bilayer into a well-defined, stretched state. To measure ΔΔG, we introduced two alanine point mutations into an 8-amino-acid region at the C-terminal end of bR’s G helix. For each, we reversibly unfolded and refolded this region hundreds of times while the rest of the protein remained folded. Our single-molecule–derived ΔΔG for mutant L223A (−2.3 ± 0.6 kcal/mol) quantitatively agreed with past chemical denaturation results while our ΔΔG for mutant V217A was 2.2-fold larger (−2.4 ± 0.6 kcal/mol). We attribute the latter result, in part, to contact between Val217 and a natively bound squalene lipid, highlighting the contribution of membrane protein–lipid contacts not present in chemical denaturation assays. More generally, we established a platform for determining ΔΔG for a fully folded membrane protein embedded in its native bilayer.', 'corpus_id': 232323985, 'score': 0}]"
196	{'doc_id': '146898991', 'title': 'The Social Functions of Group Rituals', 'abstract': 'Convergent developments across social scientific disciplines provide evidence that ritual is a psychologically prepared, culturally inherited, behavioral trademark of our species. We draw on evidence from the anthropological and evolutionary-science literatures to offer a psychological account of the social functions of ritual for group behavior. Solving the adaptive problems associated with group living requires psychological mechanisms for identifying group members, ensuring their commitment to the group, facilitating cooperation with coalitions, and maintaining group cohesion. The intersection of these lines of inquiry yields new avenues for theory and research on the evolution and ontogeny of social group cognition.', 'corpus_id': 146898991}	14379	"[{'doc_id': '232278143', 'title': 'Commentary on Alessandroni and Rodríguez', 'abstract': 'The current target article provides a robust investigation of the “cultural character” of cognitive development. This investigation has both theoretical and empirical/ methodological aspects. Methodologically, the authors argue for a unit of analysis concerning the development of object knowledge that includes other agents engaged in communication with the infant (i.e., that includes the sociocultural aspects of the infants’ developmental environment). We agree with such a position and further illustrate its utility in our own analysis of the phenomenon of overimitation. With respect to the underlying theory, we agree with the arguments against strictly cognitivist frameworks (including those with a more recent “embodied” flavor), as well as the fundamental importance ascribed to sociality and culture. However, for some aspects of the pragmatics of the object paradigm we would suggest narrowing the scope about the necessity of culture for development while in other respects we would like to suggest possible elaborations or extensions. Perhaps most fundamentally, we suggest that the physical versus cultural split that frames the target article discussion is not as metaphysically fundamental as seems to be presupposed.', 'corpus_id': 232278143, 'score': 0}, {'doc_id': '147313721', 'title': 'The Evolution and Ontogeny of Ritual', 'abstract': 'Convergent developments across social scientific disciplines provide evidence that ritual is a psychologically prepared, culturally inherited, behavioral trademark of our species. We draw on evidence from the anthropological and evolutionary science literatures to provide a psychological account of the social functions of ritual in group behavior. Solving the adaptive problems associated with group living requires psychological mechanisms for identifying group members, ensuring their commitment to the group, facilitating cooperation with their coalition, and maintaining group cohesion. We also examine evidence that the threat of social exclusion and loss of status motivates engagement in ritual throughout development and provides an account of the ontogeny of ritual cognition. The intersection of these lines of inquiry promises to provide new avenues for theory and research on the evolution and ontogeny of social group cognition. \n \n \nKeywords: \n \nritual; \ncultural evolution; \ncultural transmission; \ncooperation; \nsocial group cognition', 'corpus_id': 147313721, 'score': 1}, {'doc_id': '232327967', 'title': 'The Effects of Social Perception on Moral Judgment', 'abstract': 'When people express a moral judgment, others make inferences about their personality, such as whether they are warm or competent. People may use this interpersonal process to present themselves in a way that is socially acceptable in the current circumstances. Across four studies, we investigated this hypothesis in Chinese culture and showed that college student participants tended to associate others’ deontological moral judgments with warmth and utilitarian moral judgments with competence (Study 1, Mage = 21.1, SD = 2.45; Study 2, Mage = 20.53, SD = 1.87). In addition, participants made more deontological judgments after preparing to be interviewed for a job requiring them to be in a warm social role, and more utilitarian judgments after preparing for a job requiring them to be in a competent social role (Study 3, Mage = 19.5, SD = 1.63). This effect held true in moral dilemmas involving different degrees of hypothetical personal involvement, and appeared to be mediated by the perception of others’ expectations (Study 4, Mage = 19.92, SD = 1.97). The results suggest an important role for social cognition as an influence on moral judgments in Chinese culture.', 'corpus_id': 232327967, 'score': 0}, {'doc_id': '3900485', 'title': 'Understanding and sharing intentions: The origins of cultural cognition', 'abstract': ""We propose that the crucial difference between human cognition and that of other species is the ability to participate with others in collaborative activities with shared goals and intentions: shared intentionality. Participation in such activities requires not only especially powerful forms of intention reading and cultural learning, but also a unique motivation to share psychological states with others and unique forms of cognitive representation for doing so. The result of participating in these activities is species-unique forms of cultural cognition and evolution, enabling everything from the creation and use of linguistic symbols to the construction of social norms and individual beliefs to the establishment of social institutions. In support of this proposal we argue and present evidence that great apes (and some children with autism) understand the basics of intentional action, but they still do not participate in activities involving joint intentions and attention (shared intentionality). Human children's skills of shared intentionality develop gradually during the first 14 months of life as two ontogenetic pathways intertwine: (1) the general ape line of understanding others as animate, goal-directed, and intentional agents; and (2) a species-unique motivation to share emotions, experience, and activities with other persons. The developmental outcome is children's ability to construct dialogic cognitive representations, which enable them to participate in earnest in the collectivity that is human cognition."", 'corpus_id': 3900485, 'score': 1}, {'doc_id': '231991610', 'title': 'The Evolved Psychology of Psychedelic Set and Setting: Inferences Regarding the Roles of Shamanism and Entheogenic Ecopsychology', 'abstract': 'This review illustrates the relevance of shamanism and its evolution under effects of psilocybin as a framework for identifying evolved aspects of psychedelic set and setting. Effects of 5HT2 psychedelics on serotonin, stress adaptation, visual systems and personality illustrate adaptive mechanisms through which psychedelics could have enhanced hominin evolution as an environmental factor influencing selection for features of our evolved psychology. Evolutionary psychology perspectives on ritual, shamanism and psychedelics provides bases for inferences regarding psychedelics’ likely roles in hominin evolution as exogenous neurotransmitter sources through their effects in selection for innate dispositions for psychedelic set and setting. Psychedelics stimulate ancient brain structures and innate modular thought modules, especially self-awareness, other awareness, “mind reading,” spatial and visual intelligences. The integration of these innate modules are also core features of shamanism. Cross-cultural research illustrates shamanism is an empirical phenomenon of foraging societies, with its ancient basis in collective hominid displays, ritual alterations of consciousness, and endogenous healing responses. Shamanic practices employed psychedelics and manipulated extrapharmacological effects through stimulation of serotonin and dopamine systems and augmenting processes of the reptilian and paleomammalian brains. Differences between chimpanzee maximal displays and shamanic rituals reveal a zone of proximal development in hominin evolution. The evolution of the mimetic capacity for enactment, dance, music, and imitation provided central capacities underlying shamanic performances. Other chimp-human differences in ritualized behaviors are directly related to psychedelic effects and their integration of innate modular thought processes. Psychedelics and other ritual alterations of consciousness stimulate these and other innate responses such as soul flight and death-and-rebirth experiences. These findings provided bases for making inferences regarding foundations of our evolved set, setting and psychology. Shamanic setting is eminently communal with singing, drumming, dancing and dramatic displays. Innate modular thought structures are prominent features of the set of shamanism, exemplified in animism, animal identities, perceptions of spirits, and psychological incorporation of spirit others. A shamanic-informed psychedelic therapy includes: a preparatory set with practices such as sexual abstinence, fasting and dream incubation; a set derived from innate modular cognitive capacities and their integration expressed in a relational animistic worldview; a focus on internal imagery manifesting a presentational intelligence; and spirit relations involving incorporation of animals as personal powers. Psychedelic research and treatment can adopt this shamanic biogenetic paradigm to optimize set, setting and ritual frameworks to enhance psychedelic effects.', 'corpus_id': 231991610, 'score': 0}, {'doc_id': '232047414', 'title': 'The amoral atheist? A cross-national examination of cultural, motivational, and cognitive antecedents of disbelief, and their implications for morality', 'abstract': 'There is a widespread cross-cultural stereotype suggesting that atheists are untrustworthy and lack a moral compass. Is there any truth to this notion? Building on theory about the cultural, (de)motivational, and cognitive antecedents of disbelief, the present research investigated whether there are reliable similarities as well as differences between believers and disbelievers in the moral values and principles they endorse. Four studies examined how religious disbelief (vs. belief) relates to endorsement of various moral values and principles in a predominately religious (vs. irreligious) country (the U.S. vs. Sweden). Two U.S. M-Turk studies (Studies 1A and 1B, N = 429) and two large cross-national studies (Studies 2–3, N = 4,193), consistently show that disbelievers (vs. believers) are less inclined to endorse moral values that serve group cohesion (the binding moral foundations). By contrast, only minor differences between believers and disbelievers were found in endorsement of other moral values (individualizing moral foundations, epistemic rationality). It is also demonstrated that presumed cultural and demotivational antecedents of disbelief (limited exposure to credibility-enhancing displays, low existential threat) are associated with disbelief. Furthermore, these factors are associated with weaker endorsement of the binding moral foundations in both countries (Study 2). Most of these findings were replicated in Study 3, and results also show that disbelievers (vs. believers) have a more consequentialist view of morality in both countries. A consequentialist view of morality was also associated with another presumed antecedent of disbelief—analytic cognitive style.', 'corpus_id': 232047414, 'score': 0}, {'doc_id': '231700892', 'title': 'When alterations are violations: Moral outrage and punishment in response to (even minor) alterations to rituals.', 'abstract': ""From Catholics performing the sign of the cross since the 4th century to Americans reciting the Pledge of Allegiance since the 1890s, group rituals (i.e., predefined sequences of symbolic actions) have strikingly consistent features over time. Seven studies (N = 4,213) document the sacrosanct nature of rituals: Because group rituals symbolize sacred group values, even minor alterations to them provoke moral outrage and punishment. In Pilot Studies A and B, fraternity members who failed to complete initiation activities that were more ritualistic elicited relatively greater moral outrage and hazing from their fraternity brothers. Study 1 uses secular holiday rituals to explore the dimensions of ritual alteration-both physical and psychological-that elicit moral outrage. Study 2 suggests that altering a ritual elicits outrage even beyond the extent to which the ritual alteration is seen as violating descriptive and injunctive norms. In Study 3, group members who viewed male circumcision as more ritualistic (i.e., Jewish vs. Muslim participants) expressed greater moral outrage in response to a proposal to alter circumcision to make it safer. Study 4 uses the Pledge of Allegiance ritual to explore how the intentions of the person altering the ritual influence observers' moral outrage and punishment. Finally, in Study 5, even minor alterations elicited comparable levels of moral outrage to major alterations of the Jewish Passover ritual. Across both religious and secular rituals, the more ingroup members believed that rituals symbolize sacred group values, the more they protected their rituals-by punishing those who violated them. (PsycInfo Database Record (c) 2021 APA, all rights reserved)."", 'corpus_id': 231700892, 'score': 1}, {'doc_id': '2779137', 'title': 'The Psychology of Rituals: An Integrative Review and Process-Based Framework', 'abstract': 'Traditionally, ritual has been studied from broad sociocultural perspectives, with little consideration of the psychological processes at play. Recently, however, psychologists have begun turning their attention to the study of ritual, uncovering the causal mechanisms driving this universal aspect of human behavior. With growing interest in the psychology of ritual, this article provides an organizing framework to understand recent empirical work from social psychology, cognitive science, anthropology, behavioral economics, and neuroscience. Our framework focuses on three primary regulatory functions of rituals: regulation of (a) emotions, (b) performance goal states, and (c) social connection. We examine the possible mechanisms underlying each function by considering the bottom-up processes that emerge from the physical features of rituals and top-down processes that emerge from the psychological meaning of rituals. Our framework, by appreciating the value of psychological theory, generates novel predictions and enriches our understanding of ritual and human behavior more broadly.', 'corpus_id': 2779137, 'score': 1}, {'doc_id': '24586183', 'title': 'Evaluating ritual efficacy: Evidence from the supernatural', 'abstract': 'Rituals pose a cognitive paradox: although widely used to treat problems, rituals are causally opaque (i.e., they lack a causal explanation for their effects). How is the efficacy of ritual action evaluated in the absence of causal information? To examine this question using ecologically valid content, three studies (N=162) were conducted in Brazil, a cultural context in which rituals called simpatias are used to treat a great variety of problems ranging from asthma to infidelity. Using content from existing simpatias, experimental simpatias were designed to manipulate the kinds of information that influences perceptions of efficacy. A fourth study (N=68) with identical stimuli was conducted with a US sample to assess the generalizability of the findings across two different cultural contexts. The results provide evidence that information reflecting intuitive causal principles (i.e., repetition of procedures, number of procedural steps) and transcendental influence (i.e., presence of religious icons) affects how people evaluate ritual efficacy.', 'corpus_id': 24586183, 'score': 1}]"
197	{'doc_id': '213883397', 'title': 'Combustion stability monitoring through flame imaging and stacked sparse autoencoder based deep neural network', 'abstract': 'Combustion instability is a well-known problem in the combustion processes and closely linked to lower combustion efficiency and higher pollutant emissions. Therefore, it is important to monitor combustion stability for optimizing efficiency and maintaining furnace safety. However, it is difficult to establish a robust monitoring model with high precision through traditional data-driven methods, where prior knowledge of labeled data is required. This study proposes a novel approach for combustion stability monitoring through stacked sparse autoencoder based deep neural network. The proposed stacked sparse autoencoder is firstly utilized to extract flame representative features from the unlabeled images, and an improved loss function is used to enhance the training efficiency. The extracted features are then used to identify the classification label and stability index through clustering and statistical analysis. Classification and regression models incorporating the stacked sparse autoencoder are established for the qualitative and quantitative characterization of combustion stability. Experiments were carried out on a gas combustor to establish and evaluate the proposed models. It has been found that the classification model provides an F1-score of 0.99, whilst the R-squared of 0.98 is achieved through the regression model. Results obtained from the experiments demonstrated that the stacked sparse autoencoder model is capable of extracting flame representative features automatically without having manual interference. The results also show that the proposed model provides a higher prediction accuracy in comparison to the traditional data-driven methods and also demonstrates as a promising tool for monitoring the combustion stability accurately.', 'corpus_id': 213883397}	4663	"[{'doc_id': '126222952', 'title': 'Efficient Deep CNN-Based Fire Detection and Localization in Video Surveillance Applications', 'abstract': 'Convolutional neural networks (CNNs) have yielded state-of-the-art performance in image classification and other computer vision tasks. Their application in fire detection systems will substantially improve detection accuracy, which will eventually minimize fire disasters and reduce the ecological and social ramifications. However, the major concern with CNN-based fire detection systems is their implementation in real-world surveillance networks, due to their high memory and computational requirements for inference. In this paper, we propose an original, energy-friendly, and computationally efficient CNN architecture, inspired by the SqueezeNet architecture for fire detection, localization, and semantic understanding of the scene of the fire. It uses smaller convolutional kernels and contains no dense, fully connected layers, which helps keep the computational requirements to a minimum. Despite its low computational needs, the experimental results demonstrate that our proposed solution achieves accuracies that are comparable to other, more complex models, mainly due to its increased depth. Moreover, this paper shows how a tradeoff can be reached between fire detection accuracy and efficiency, by considering the specific characteristics of the problem of interest and the variety of fire data.', 'corpus_id': 126222952, 'score': 1}, {'doc_id': '214612507', 'title': 'Review of data analysis in vision inspection of power lines with an in-depth discussion of deep learning technology', 'abstract': 'The widespread popularity of unmanned aerial vehicles enables an immense amount of power lines inspection data to be collected. How to employ massive inspection data especially the visible images to maintain the reliability, safety, and sustainability of power transmission is a pressing issue. To date, substantial works have been conducted on the analysis of power lines inspection data. With the aim of providing a comprehensive overview for researchers who are interested in developing a deep-learning-based analysis system for power lines inspection data, this paper conducts a thorough review of the current literature and identifies the challenges for future research. Following the typical procedure of inspection data analysis, we categorize current works in this area into component detection and fault diagnosis. For each aspect, the techniques and methodologies adopted in the literature are summarized. Some valuable information is also included such as data description and method performance. Further, an in-depth discussion of existing deep-learning-related analysis methods in power lines inspection is proposed. Finally, we conclude the paper with several research trends for the future of this area, such as data quality problems, small object detection, embedded application, and evaluation baseline.', 'corpus_id': 214612507, 'score': 0}, {'doc_id': '145991179', 'title': 'Fire Detection from Images Using Faster R-CNN and Multidimensional Texture Analysis', 'abstract': 'In this paper, we propose a novel image-based fire detection approach, which combines the power of modern deep learning networks with multidimensional texture analysis based on higher-order linear dynamical systems. The candidate fire regions are identified by a Faster R-CNN network trained for the task of fire detection using a set of annotated images containing actual fire as well as selected negatives. The candidate fire regions are projected to a Grassmannian space and each image is represented as a cloud of points on the manifold. Finally, a vector representation approach is applied aiming to aggregate the Grassmannian points based on a locality criterion on the manifold. For evaluating the performance of the proposed methodology, we performed experiments with annotated images of two different databases containing fire and fire-coloured objects. Experimental results demonstrate the potential of the proposed methodology compared to other state of the art approaches.', 'corpus_id': 145991179, 'score': 1}, {'doc_id': '201878752', 'title': 'Combustion Condition Monitoring Through Deep Learning Networks', 'abstract': 'Combustion condition monitoring is essential in a power plant for maintaining stable operations and operational safety. Therefore it is crucial to develop an intelligent combustion monitoring system. Existing traditional methods not only need a large quantity of labeled data but also require rebuilding monitoring model for new conditions. Aiming these problems, the present study proposes a novel approach combining denoising auto-encoder (DAE) and generative adversarial network (GAN) to monitor combustion condition. By using the learning mechanism of the GAN, the robust feature extraction ability of DAE as a generator is improved. These features are then fed into the Gaussian process classifier (GPC) for condition identification. Especially, newly occurring conditions can be correctly classified by simply training the GPC, rather than training from scratch. Experiments performed on a gaseous combustor indicate that the proposed approach can extract representative features accurately and achieve high performance in combustion condition monitoring with the accuracy of 98.5% for original conditions and 97.8% for the new conditions.', 'corpus_id': 201878752, 'score': 1}, {'doc_id': '212628424', 'title': 'Generalizable semi-supervised learning method to estimate mass from sparsely annotated images', 'abstract': 'Mass flow estimation is of great importance to several industries, and it can be quite challenging to obtain accurate estimates due to limitation in expense or general infeasibility. In the context of agricultural applications, yield monitoring is a key component to precision agriculture and mass flow is the critical factor to measure. Measuring mass flow allows for field productivity analysis, cost minimization, and adjustments to machine efficiency. Methods such as volume or force-impact have been used to measure mass flow; however, these methods are limited in application and accuracy. In this work, we use deep learning to develop and test a vision system that can accurately estimate the mass of sugarcane while running in real-time on a sugarcane harvester during operation. The deep learning algorithm that is used to estimate mass flow is trained using very sparsely annotated images (semi-supervised) using only final load weights (aggregated weights over a certain period of time). The deep neural network (DNN) succeeds in capturing the mass of sugarcane accurately and surpasses older volumetric-based methods, despite highly varying lighting and material colors in the images. The deep neural network is initially trained to predict mass on laboratory data (bamboo) and then transfer learning is utilized to apply the same methods to estimate mass of sugarcane. Using a vision system with a relatively lightweight deep neural network we are able to estimate mass of bamboo with an average error of 4.5% and 5.9% for a select season of sugarcane.', 'corpus_id': 212628424, 'score': 0}, {'doc_id': '211677317', 'title': 'Artificial neural network based chemical mechanisms for computationally efficient modeling of kerosene combustion', 'abstract': 'To effectively simulate the combustion of hydrocarbon-fueled supersonic engines, such as rocket-based combined cycle (RBCC) engines, a detailed mechanism for chemistry is usually required but computationally prohibitive. In order to accelerate chemistry calculation, an artificial neural network (ANN) based methodology was introduced in this study. This methodology consists of two different layers: self-organizing map (SOM) and back-propagation neural network (BPNN). The SOM is for clustering the dataset into subsets to reduce the nonlinearity, while the BPNN is for regression for each subset. The entire methodology was subsequently employed to establish a skeleton mechanism of kerosene combustion with 41 species. The training data was generated by RANS simulations of the RBCC combustion chamber, and then fed into the SOM-BPNN with six different topologies (three different SOM topologies and two different BPNN topologies). By comparing the predicted results of six cases with those of the conventional ODE solver, it is found that if the topology is properly designed, high-precision results in terms of ignition, quenching and mass fraction prediction can be achieved. As for efficiency, 8~ 20 times speedup of the chemical system integration was achieved, indicating that it has great potential for application in complex chemical mechanisms for a variety of fuels.', 'corpus_id': 211677317, 'score': 1}, {'doc_id': '216422612', 'title': 'Deep Machine Learning Approach to Develop a New Asphalt Pavement Condition Index', 'abstract': 'Abstract Pavement condition assessment provides information to make more cost-effective and consistent decisions regarding management of pavement network. Generally, pavement distress inspections are performed using sophisticated data collection vehicles and/or foot-on-ground surveys. In either approach, the process of distress detection is human-dependent, expensive, inefficient, and/or unsafe. Automated pavement distress detection via road images is still a challenging issue among pavement researchers and computer-vision community. In recent years, advancement in deep learning has enabled researchers to develop robust tools for analyzing pavement images at unprecedented accuracies. Nevertheless, deep learning models necessitate a big ground truth dataset, which is often not readily accessible for pavement field. In this study, we reviewed our previous study, which a labeled pavement dataset was presented as the first step towards a more robust, easy-to-deploy pavement condition assessment system. In total, 7237 google street-view images were extracted, manually annotated for classification (nine categories of distress classes). Afterward, YOLO (you look only once) deep learning framework was implemented to train the model using the labeled dataset. In the current study, a U-net based model is developed to quantify the severity of the distresses, and finally, a hybrid model is developed by integrating the YOLO and U-net model to classify the distresses and quantify their severity simultaneously. Various pavement condition indices are developed by implementing various machine learning algorithms using the YOLO deep learning framework for distress classification and U-net for segmentation and distress densification. The output of the distress classification and segmentation models are used to develop a comprehensive pavement condition tool which rates each pavement image according to the type and severity of distress extracted. As a result, we are able to avoid over-dependence on human judgement throughout the pavement condition evaluation process. The outcome of this study could be conveniently employed to evaluate the pavement conditions during its service life and help to make valid decisions for rehabilitation or reconstruction of the roads at the right time.', 'corpus_id': 216422612, 'score': 0}, {'doc_id': '211678266', 'title': 'A deep learning framework for hydrogen-fueled turbulent combustion simulation', 'abstract': 'The high cost of high-resolution computational fluid/flame dynamics (CFD) has hindered its application in combustion related design, research and optimization. In this study, we propose a new framework for turbulent combustion simulation based on the deep learning approach. An optimized deep convolutional neural network (CNN) inspired from a U-Net architecture and inception module is designed for constructing the framework of the deep learning solver, named CFDNN. CFDNN is then trained on the simulation results of hydrogen combustion in a cavity with different inlet velocities. After training, CFDNN can not only accurately predict the flow and combustion fields within the range of the training set, but also shows an extrapolation ability for prediction outside the training set. The results from CFDNN solver show excellent consistency with the conventional CFD results in terms of both predicted spatial distributions and temporal dynamics. Meanwhile, two orders of magnitude of acceleration is achieved by using CFDNN solver compared to the conventional CFD solver. The successful development of such a deep learning-based solver opens up new possibilities of low-cost, high-accuracy simulations, fast prototyping, design optimization and real-time control of combustion systems such as gas turbines and scramjets.', 'corpus_id': 211678266, 'score': 1}, {'doc_id': '213004364', 'title': 'Toward Enabling a Reliable Quality Monitoring System for Additive Manufacturing Process using Deep Convolutional Neural Networks', 'abstract': 'Additive Manufacturing (AM) is a crucial component of the smart industry. In this paper, we propose an automated quality grading system for the AM process using a deep convolutional neural network (CNN) model. The CNN model is trained offline using the images of the internal and surface defects in the layer-by-layer deposition of materials and tested online by studying the performance of detecting and classifying the failure in AM process at different extruder speeds and temperatures. The model demonstrates the accuracy of 94% and specificity of 96%, as well as above 75% in three classifier measures of the Fscore, the sensitivity, and precision for classifying the quality of the printing process in five grades in real-time. The proposed online model adds an automated, consistent, and non-contact quality control signal to the AM process that eliminates the manual inspection of parts after they are entirely built. The quality monitoring signal can also be used by the machine to suggest remedial actions by adjusting the parameters in real-time. The proposed quality predictive model serves as a proof-of-concept for any type of AM machines to produce reliable parts with fewer quality hiccups while limiting the waste of both time and materials.', 'corpus_id': 213004364, 'score': 0}, {'doc_id': '214713625', 'title': 'Combining visible and infrared spectrum imagery using machine learning for small unmanned aerial system detection', 'abstract': ""There is an increasing demand for technology and solutions to counter commercial, off-the-shelf small unmanned aerial systems (sUAS). Advances in machine learning and deep neural networks for object detection, coupled with lower cost and power requirements of cameras, led to promising vision-based solutions for sUAS detection. However, solely relying on the visible spectrum has previously led to reliability issues in low contrast scenarios such as sUAS flying below the treeline and against bright sources of light. Alternatively, due to the relatively high heat signatures emitted from sUAS during ight, a long-wave infrared (LWIR) sensor is able to produce images that clearly contrast the sUAS from its background. However, compared to widely available visible spectrum sensors, LWIR sensors have lower resolution and may produce more false positives when exposed to birds or other heat sources. This research work proposes combining the advantages of the LWIR and visible spectrum sensors using machine learning for vision-based detection of sUAS. Utilizing the heightened background contrast from the LWIR sensor combined and synchronized with the relatively increased resolution of the visible spectrum sensor, a deep learning model was trained to detect the sUAS through previously difficult environments. More specifically, the approach demonstrated effective detection of multiple sUAS flying above and below the treeline, in the presence of heat sources, and glare from the sun. Our approach achieved a detection rate of 71.2 ± 8.3%, improving by 69% when compared to LWIR and by 30.4% when visible spectrum alone, and achieved false alarm rate of 2.7 ± 2.6%, decreasing by 74.1% and by 47.1% when compared to LWIR and visible spectrum alone, respectively, on average, for single and multiple drone scenarios, controlled for the same confidence metric of the machine learning object detector of at least 50%. With a network of these small and affordable sensors, one can accurately estimate the 3D position of the sUAS, which could then be used for elimination or further localization from more narrow sensors, like a fire-control radar (FCR). Videos of the solution's performance can be seen at https://sites.google.com/view/tamudrone-spie2020/."", 'corpus_id': 214713625, 'score': 0}]"
198	{'doc_id': '202768445', 'title': 'Retrosynthesis Prediction with Conditional Graph Logic Network', 'abstract': 'Retrosynthesis is one of the fundamental problems in organic chemistry. The task is to identify reactants that can be used to synthesize a specified product molecule. Recently, computer-aided retrosynthesis is finding renewed interest from both chemistry and computer science communities. Most existing approaches rely on template-based models that define subgraph matching rules, but whether or not a chemical reaction can proceed is not defined by hard decision rules. In this work, we propose a new approach to this task using the Conditional Graph Logic Network, a conditional graphical model built upon graph neural networks that learns when rules from reaction templates should be applied, implicitly considering whether the resulting reaction would be both chemically feasible and strategic. We also propose an efficient hierarchical sampling to alleviate the computation cost. While achieving a significant improvement of 8.2% over current state-of-the-art methods on the benchmark dataset, our model also offers interpretations for the prediction.', 'corpus_id': 202768445}	3230	[{'doc_id': '215415359', 'title': 'Predicting COVID-19 Using Hybrid AI Model', 'abstract': 'Background: The coronavirus disease 2019 (COVID-19) breaking out in late December 2019 is gradually being controlled in China, but it is still spreading in other countries and regions worldwide. It is urgent to conduct prediction research on the development and spread of the epidemic. \n \nMethods: A hybrid AI model is proposed for COVID-19 prediction. First, by analyzing the change in the infectious capacity of virus carriers within a few days after infection, an improved SI (ISI) model is proposed. Second, considering the effects of prevention and control measures and the increase of the public’s prevention awareness, the natural language processing (NLP) module and the long short-term memory (LSTM) network are embedded into the ISI model to build the hybrid AI model for COVID-19 prediction. \n \nFindings: Compared with traditional epidemic models, the proposed ISI model finds that the new confirmed cases of COVID-19 are mainly infected by the cases from previous 3 to 8 days, and the average infection time is about 5.5 days. With introduction of NLP and LSTM into the hybrid AI model, the mean absolute percentage errors (MAPE) of the prediction results are 0.52%, 0.38%, 0.05%, 0.86% for the next 6 days in Wuhan, Beijing, Shanghai and nationwide respectively, which proves our model is more in line with the actual epidemic development trend. \n \nInterpretation: The infectious capacity of virus carriers varies at different stages. Additionally, both the prevention and control measures and the public’s awareness of the epidemic have great impact on the transmission of COVID-19. \n \nFunding Statement: This study was supported by the fund from the National Key Research and Development Program of China (Grant No. 2016YFB1000900). \n \nDeclaration of Interests: The authors declare that they have no competing interests.', 'corpus_id': 215415359, 'score': 0}, {'doc_id': '214055777', 'title': 'Incubation Period and Other Epidemiological Characteristics of 2019 Novel Coronavirus Infections with Right Truncation: A Statistical Analysis of Publicly Available Case Data', 'abstract': 'The geographic spread of 2019 novel coronavirus (COVID-19) infections from the epicenter of Wuhan, China, has provided an opportunity to study the natural history of the recently emerged virus. Using publicly available event-date data from the ongoing epidemic, the present study investigated the incubation period and other time intervals that govern the epidemiological dynamics of COVID-19 infections. Our results show that the incubation period falls within the range of 2-14 days with 95% confidence and has a mean of around 5 days when approximated using the best-fit lognormal distribution. The mean time from illness onset to hospital admission (for treatment and/or isolation) was estimated at 3-4 days without truncation and at 5-9 days when right truncated. Based on the 95th percentile estimate of the incubation period, we recommend that the length of quarantine should be at least 14 days. The median time delay of 13 days from illness onset to death (17 days with right truncation) should be considered when estimating the COVID-19 case fatality risk.', 'corpus_id': 214055777, 'score': 0}, {'doc_id': '202550423', 'title': 'A Transformer Model for Retrosynthesis', 'abstract': 'We describe a Transformer model for a retrosynthetic reaction prediction task. The model is trained on 45 033 experimental reaction examples extracted from USA patents. It can successfully predict the reactants set for 42.7% of cases on the external test set. During the training procedure, we applied different learning rate schedules and snapshot learning. These techniques can prevent overfitting and thus can be a reason to get rid of internal validation dataset that is advantageous for deep models with millions of parameters. We thoroughly investigated different approaches to train Transformer models and found that snapshot learning with averaging weights on learning rates minima works best. While decoding the model output probabilities there is a strong influence of the temperature that improves at \\(\\text {T}=1.3\\) the accuracy of models up to 1–2%.', 'corpus_id': 202550423, 'score': 1}, {'doc_id': '46865246', 'title': 'Neural-Symbolic Machine Learning for Retrosynthesis and Reaction Prediction.', 'abstract': 'Reaction prediction and retrosynthesis are the cornerstones of organic chemistry. Rule-based expert systems have been the most widespread approach to computationally solve these two related challenges to date. However, reaction rules often fail because they ignore the molecular context, which leads to reactivity conflicts. Herein, we report that deep neural networks can learn to resolve reactivity conflicts and to prioritize the most suitable transformation rules. We show that by training our model on 3.5\u2005million reactions taken from the collective published knowledge of the entire discipline of chemistry, our model exhibits a top10-accuracy of 95\u2009% in retrosynthesis and 97\u2009% for reaction prediction on a validation set of almost 1\u2005million reactions.', 'corpus_id': 46865246, 'score': 1}, {'doc_id': '211476589', 'title': 'Understanding of COVID‐19 based on current evidence', 'abstract': 'Since December 2019, a series of unexplained pneumonia cases have been reported in Wuhan, China. On 12 January 2020, the World Health Organization (WHO) temporarily named this new virus as the 2019 novel coronavirus (2019‐nCoV). On 11 February 2020, the WHO officially named the disease caused by the 2019‐nCoV as coronavirus disease (COVID‐19). The COVID‐19 epidemic is spreading all over the world, especially in China. Based on the published evidence, we systematically discuss the characteristics of COVID‐19 in the hope of providing a reference for future studies and help for the prevention and control of the COVID‐19 epidemic.', 'corpus_id': 211476589, 'score': 0}, {'doc_id': '212707837', 'title': 'Coronavirus disease 2019: What we know?', 'abstract': 'In late December 2019, a cluster of unexplained pneumonia cases has been reported in Wuhan, China. A few days later, the causative agent of this mysterious pneumonia was identified as a novel coronavirus. This causative virus has been temporarily named as severe acute respiratory syndrome coronavirus 2 and the relevant infected disease has been named as coronavirus disease 2019 (COVID‐19) by the World Health Organization, respectively. The COVID‐19 epidemic is spreading in China and all over the world now. The purpose of this review is primarily to review the pathogen, clinical features, diagnosis, and treatment of COVID‐19, but also to comment briefly on the epidemiology and pathology based on the current evidence.', 'corpus_id': 212707837, 'score': 0}, {'doc_id': '211230955', 'title': 'Incubation Period and Other Epidemiological Characteristics of 2019 Novel Coronavirus Infections with Right Truncation: A Statistical Analysis of Publicly Available Case Data', 'abstract': 'The geographic spread of 2019 novel coronavirus (COVID-19) infections from the epicenter of Wuhan, China, has provided an opportunity to study the natural history of the recently emerged virus. Using publicly available event-date data from the ongoing epidemic, the present study investigated the incubation period and other time intervals that govern the epidemiological dynamics of COVID-19 infections. Our results show that the incubation period falls within the range of 2–14 days with 95% confidence and has a mean of around 5 days when approximated using the best-fit lognormal distribution. The mean time from illness onset to hospital admission (for treatment and/or isolation) was estimated at 3–4 days without truncation and at 5–9 days when right truncated. Based on the 95th percentile estimate of the incubation period, we recommend that the length of quarantine should be at least 14 days. The median time delay of 13 days from illness onset to death (17 days with right truncation) should be considered when estimating the COVID-19 case fatality risk.', 'corpus_id': 211230955, 'score': 0}, {'doc_id': '213016152', 'title': 'Serial interval of novel coronavirus (2019-nCoV) infections', 'abstract': 'Objective: To estimate the serial interval of novel coronavirus (COVID-19) from information on 28 infector-infectee pairs. Methods: We collected dates of illness onset for primary cases (infectors) and secondary cases (infectees) from published research articles and case investigation reports. We subjectively ranked the credibility of the data and performed analyses on both the full dataset (n=28) and a subset of pairs with highest certainty in reporting (n=18). In addition, we adjusting for right truncation of the data as the epidemic is still in its growth phase. Results: Accounting for right truncation and analyzing all pairs, we estimated the median serial interval at 4.0 days (95% credible interval [CrI]: 3.1, 4.9). Limiting our data to only the most certain pairs, the median serial interval was estimated at 4.6 days (95% CrI: 3.5, 5.9). Conclusions: The serial interval of COVID-19 is shorter than its median incubation period. This suggests that a substantial proportion of secondary transmission may occur prior to illness onset. The COVID-19 serial interval is also shorter than the serial interval of severe acute respiratory syndrome (SARS), indicating that calculations made using the SARS serial interval may introduce bias.', 'corpus_id': 213016152, 'score': 0}, {'doc_id': '11001549', 'title': 'Computer-Assisted Retrosynthesis Based on Molecular Similarity', 'abstract': 'We demonstrate molecular similarity to be a surprisingly effective metric for proposing and ranking one-step retrosynthetic disconnections based on analogy to precedent reactions. The developed approach mimics the retrosynthetic strategy defined implicitly by a corpus of known reactions without the need to encode any chemical knowledge. Using 40\u202f000 reactions from the patent literature as a knowledge base, the recorded reactants are among the top 10 proposed precursors in 74.1% of 5000 test reactions, providing strong quantitative support for our methodology. Extension of the one-step strategy to multistep pathway planning is demonstrated and discussed for two exemplary drug products.', 'corpus_id': 11001549, 'score': 1}, {'doc_id': '214636811', 'title': 'Title : Understanding of COVID-19 based on current evidence Running head : Current understanding of COVID-19', 'abstract': 'Since December 2019, a series of unexplained pneumonia cases has been reported in Wuhan, China. On January 12, 2020, the World Health Organization (WHO) temporarily named this new virus as the 2019 novel coronavirus (2019-nCoV). On February 11, 2020, the WHO officially named the disease caused by the 2019-nCoV as Corona Virus Disease (COVID-19). The COVID-19 epidemic is spreading all over the world, especially in China. Based on the published evidence, we systematically discuss the characteristics of COVID-19 in the hope of providing a reference for future studies and help for the prevention and control of the COVID-19 epidemic.', 'corpus_id': 214636811, 'score': 0}]
199	{'doc_id': '235727298', 'title': 'Two-phase geothermal model with fracture network and multi-branch wells', 'abstract': 'This paper focuses on the numerical simulation of geothermal systems in complex geological settings. The physical model is based on two-phase Darcy flows coupling the mass conservation of the water component with the energy conservation and the liquid vapor thermodynamical equilibrium. The discretization exploits the flexibility of unstructured meshes to model complex geology including conductive faults as well as complex wells. The polytopal and essentially nodal Vertex Approximate Gradient scheme is used for the approximation of the Darcy and Fourier fluxes combined with a Control Volume approach for the transport of mass and energy. Particular attention is paid to the faults which are modelled as two-dimensional interfaces defined as collection of faces of the mesh and to the flow inside deviated or multi-branch wells defined as collection of edges of the mesh with rooted tree data structure. By using an explicit pressure drop calculation, the well model reduces to a single equation based on complementarity constraints with only one well implicit unknown. The coupled systems are solved fully implicitely at each time step using efficient nonlinear and linear solvers on parallel distributed architectures. The convergence of the discrete model is investigated numerically on a simple test case with a Cartesian geometry and a single vertical producer well. Then, the ability of our approach to deal efficiently with realistic test cases is assessed on a high energy faulted geothermal reservoir operated using a doublet of two deviated wells.', 'corpus_id': 235727298}	20357	[{'doc_id': '235658399', 'title': 'Thermodynamic considerations on a class of dislocation-based constitutive models', 'abstract': 'Dislocations are the main carriers of plastic deformation in crystalline materials. Physically based constitutive equations of crystal plasticity typically incorporate dislocation mechanisms, using a dislocation density based description of dislocation microstructure evolution and plastic flow. Typically, such constitutive models are not formulated in a thermodynamic framework. Nevertheless, fundamental considerations of thermodynamic consistency impose constraints on the admissible range of model parameters and/or on the range of application of such models. In particular, it is mandatory to ensure that the internal energy increase associated with dislocation accumulation is properly accounted for in the local energy balance. We demonstrate on some examples taken from the literature how failure to do so can lead to constitutive equations that violate the first and second laws of thermodynamics either generally or in some particular limit cases, and we discuss how to formulate constraints that recover thermodynamic consistency.', 'corpus_id': 235658399, 'score': 1}, {'doc_id': '236965804', 'title': 'Numerical simulation of hydraulic fracturing: a hybrid FEM based algorithm', 'abstract': 'In this paper a problem of numerical simulation of hydraulic fractures is considered. An efficient algorithm of solution, based on the universal scheme introduced earlier by the authors for the fractures propagating in elastic solids, is proposed. The algorithm utilizes a FEM based subroutine to compute deformation of the fractured material. Consequently, the computational scheme retains the relative simplicity of its original version and simultaneously enables one to deal with more advanced cases of the fractured material properties and configurations. In particular, the problems of poroelasticity, plasticity and spatially varying properties of the fractured material can be analyzed. The accuracy and efficiency of the proposed algorithm are verified against analytical benchmark solutions. The algorithm capabilities are demonstrated using the example of the hydraulic fracture propagating in complex geological settings.', 'corpus_id': 236965804, 'score': 0}, {'doc_id': '3574978', 'title': 'Numerical modelling of magma dynamics coupled to tectonic deformation of lithosphere and crust', 'abstract': 'Many unresolved questions in geodynamics revolve around the physical behaviour of the two-phase system of a silicate melt percolating through and interacting with a tectonically deforming host rock. Well-accepted equations exist to describe the physics of such systems and several previous studies have successfully implemented various forms of these equations in numerical models. To date, most such models of magma dynamics have focused on mantle flow problems and therefore employed viscous creep rheologies suitable to describe the deformation properties of mantle rock under high temperatures and pressures. However, the use of such rheologies is not appropriate to model melt extraction above the lithosphere–asthenosphere boundary, where the mode of deformation of the host rock transitions from ductile viscous to brittle elasto-plastic. Here, we introduce a novel approach to numerically model magma dynamics, focusing on the conceptual study of melt extraction from an asthenospheric source of partial melt through the overlying lithosphere and crust. To this end, we introduce an adapted set of two-phase flow equations, coupled to a visco-elasto-plastic rheology for both shear and compaction deformation of the host rock in interaction with the melt phase. We describe in detail how to implement this physical model into a finite-element code, and then proceed to evaluate the functionality and potential of this methodology using a series of conceptual model setups, which demonstrate the modes of melt extraction occurring around the rheological transition from ductile to brittle host rocks. The models suggest that three principal regimes of melt extraction emerge: viscous diapirism, viscoplastic decompaction channels and elasto-plastic dyking. Thus, our model of magma dynamics interacting with active tectonics of the lithosphere and crust provides a novel framework to further investigate magmato-tectonic processes such as the formation and geometry of magma chambers and conduits, as well as the emplacement of plutonic rock complexes.', 'corpus_id': 3574978, 'score': 1}, {'doc_id': '237279349', 'title': 'Computational multiphase periporomechanics for unguided cracking in unsaturated porous media', 'abstract': 'In this article we formulate and implement a computational multiphase periporomechanics model for unguided fracturing in unsaturated porous media. The same governing equation for the solid phase applies on and off cracks. Crack formation in this framework is autonomous, requiring no prior estimates of crack topology. As a new contribution, an energy-based criterion for arbitrary crack formation is formulated using the peridynamic effective force state for unsaturated porous media. Unsaturated fluid flow in the fracture space is modeled in a simplified way in line with the nonlocal formulation of unsaturated fluid flow in bulk. The formulated unsaturated fracturing periporomechanics is numerically implemented through a fractional step algorithm in time and a two-phase mixed meshless method in space. The two-stage operator split converts the coupled periporomechanics problem into an undrained deformation and fracture problem and an unsaturated fluid flow in the deformed skeleton configuration. Numerical simulations of in-plane open and shear cracking are conducted to validate the accuracy and robustness of the fracturing unsaturated periporomechanics model. Then numerical examples of crack branching and non-planar cracking in unsaturated soil specimens are presented to demonstrate the efficacy of the proposed multiphase periporomechanics model for unguided cracking in unsaturated porous media.', 'corpus_id': 237279349, 'score': 0}, {'doc_id': '236171005', 'title': 'A micromechanics-based variational phase-field model for fracture in geomaterials with brittle-tensile and compressive-ductile behavior', 'abstract': 'This paper presents a framework for modeling failure in quasi-brittle geomaterials under different loading conditions. A micromechanics-based model is proposed in which the field variables are linked to physical mechanisms at the microcrack level: damage is related to the growth of microcracks, while plasticity is related to the frictional sliding of closed microcracks. Consequently, the hardening/softening functions and parameters entering the free energy follow from the definition of a single degradation function and the elastic material properties. The evolution of opening microcracks in tension leads to brittle behavior and mode I fracture, while the evolution of closed microcracks under frictional sliding in compression/shear leads to ductile behavior and mode II fracture. Frictional sliding is endowed with a non-associative law, a crucial aspect of the model that considers the effect of dilation and allows for realistic material responses with non-vanishing frictional energy dissipation. Despite the non-associative law, a variationally consistent formulation is presented using notions of energy balance and stability, following the energetic formulation for rate-independent systems. The material response of the model is first described, followed by several benchmark finite element simulations. The results highlight the ability of the model to describe tensile, shear, and mixed-mode fracture, as well as responses with brittle-to-ductile transition. A key result is that, by virtue of the micromechanical arguments, realistic failure modes can be captured, without resorting to the usual heuristic modifications considered in the phase-field literature. The numerical results are thoroughly discussed with reference to previous numerical studies, experimental evidence, and analytical fracture criteria.', 'corpus_id': 236171005, 'score': 1}, {'doc_id': '236976347', 'title': 'A mechanism-based gradient damage model for metallic fracture', 'abstract': 'Abstract A new gradient-based formulation for predicting fracture in elastic-plastic solids is presented. Damage is captured by means of a phase field model that considers both the elastic and plastic works as driving forces for fracture. Material deformation is characterised by a mechanism-based strain gradient constitutive model. This non-local plastic-damage formulation is numerically implemented and used to simulate fracture in several paradigmatic boundary value problems. The case studies aim at shedding light into the role of the plastic and fracture length scales. It is found that the role of plastic strain gradients is two-fold. When dealing with sharp defects like cracks, plastic strain gradients elevate local stresses and facilitate fracture. However, in the presence of non-sharp defects failure is driven by the localisation of plastic flow, which is delayed due to the additional work hardening introduced by plastic strain gradients.', 'corpus_id': 236976347, 'score': 0}, {'doc_id': '236447343', 'title': 'Height-averaged Navier-Stokes solver for hydrodynamic lubrication', 'abstract': 'Modelling hydrodynamic lubrication is crucial in the design of engineering components as well as for a fundamental understanding of friction mechanisms. The cornerstone of thin-film flow modelling is the Reynolds equation – a lower-dimensional representation of the Stokes equation. However, the derivation of the Reynolds equation is based on assumptions and fixed form constitutive relations, that may not generally be valid, especially when studying systems under extreme conditions. Furthermore, these explicit assumptions about the constitutive behaviour of the fluid prohibit applications in a multiscale scenario based on measured or atomistically simulated data. Here, we present a method that considers the full compressible Navier-Stokes equation in a height-averaged sense for arbitrary constitutive relations. We perform numerical tests by using a reformulation of the viscous stress tensor for laminar flow to validate the presented method comparing to results from conventional Reynolds solutions. The versatility of the method is shown by incorporating models for mass-conserving cavitation, wall slip and non-Newtonian fluids. This allows testing of new constitutive relations that not necessarily need to take a fixed form, and may be obtained from experimental or simulation data.', 'corpus_id': 236447343, 'score': 0}, {'doc_id': '237272174', 'title': 'Thermomechanics for Geological, Civil Engineering and Geodynamic Applications: Rate-Dependent Critical State Line Models', 'abstract': 'Equilibrium thermodynamics has been of fundamental importance to many branches of engineering including cyclical mechanical applications. However, in geomechanics and geological applications it has not yet reached a consensus in the community. Reason for the failure of establishing thermodynamic laws as a ground principle is the far from equilibrium nature of geomechanical problems which prevent the local equilibrium assumption. Problems including rate-dependence and poromechanical complexity, where deformation often occurs in a highly localized manner, were therefore thought to be not amenable to a thermodynamic approach. Here we show that the theory of thermomechanics, originally proposed for quasi-static hyperplastic deformation problems can be extended to include rate-dependent critical state-line models for porous rocks. The development therefore makes thermodynamic-consistent modeling available for civil engineering, geological and even geodynamic problems. In this two-part contribution, we present extensions of the thermomechanics theory to account for the poromechanics of path- and rate-dependent critical state line models and we cover the relevance of this thermodynamic-consistent model for civil engineering, geological and geodynamic applications. In this first part, we review the concepts behind the thermomechanics theory and present a thermodynamic extension of generic critical state line models for visco-plasticity and damage mechanics and analyze the model prediction for strain localization.', 'corpus_id': 237272174, 'score': 1}, {'doc_id': '237142362', 'title': 'An efficient split-step scheme for fluid-structure interaction involving incompressible viscous flows', 'abstract': 'Arterial blood flow, dam or ship construction and numerous other problems in biomedical and general engineering involve incompressible flows interacting with elastic structures. Such interactions heavily influence the deformation and stress states which, in turn, affect the design. Consequently, any reliable model of such physical processes must consider the coupling of fluids and solids. However, complexity increases for non-Newtonian fluids, such as blood or polymer melts. In these fluids, subtle differences in the local shear-rate can have a drastic impact on the flow. Existing numerical solution strategies devised for Newtonian fluids are either not applicable or ineffective in such scenarios. To address these shortcomings, we present here a higher-order accurate, added-mass-stable fluid–structure interaction scheme centered around a split-step fluid solver. We compare several implicit and semi-implicit variants of the algorithm and verify convergence in space and time. Numerical examples show good performance in both benchmarks and a realistic setting of blood flow through an abdominal aortic aneurysm.', 'corpus_id': 237142362, 'score': 0}, {'doc_id': '235829561', 'title': 'A convective model for poro-elastodynamics with damage and fluid flow towards Earth lithosphere modelling', 'abstract': 'Devised towards geophysical applications for various processes in the lithosphere or the crust, a model of poro-elastodynamics with inelastic strains and other internal variables like damage (aging) and porosity as well as with diffusion of water is formulated fully in the Eulerian setting. Concepts of gradient of the total strain rate as well as the additive splitting of the total strain rate are used while eliminating the displacement from the formulation. It relies on that the elastic strain is small while only the inelastic and the total strains can be large. The energetics behind this model is derived and used for analysis as far as the existence of global weak energy-conserving solutions concerns. By this way, the model in [V. Lyakhovsky et al., Pure Appl. Geophys., 171:3099–3123, 2014] and [V. Lyakhovsky et al., Izvestiya, Physics of the Solid Earth, 43:13–23, 2007] is completed to make it mechanically consistent and amenable for analysis.', 'corpus_id': 235829561, 'score': 1}]
