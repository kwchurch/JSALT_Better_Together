	query	feed_id	candidates
0	{'doc_id': '137785354', 'title': 'Mechanical properties and cutting performance of cBN -TiN composites sintered using HPHT technique', 'abstract': 'Summary Cubic boron nitride (cBN) – based ceramic composites in different modifications are still promising materials for high performance cutting applications [1-4]. CBN-TiN composites with different cBN volume ratios were formed by high pressure – high temperature sintering (HPHT) with the application of Bridgman anvils cell. From the microscopic observation it could be concluded that the samples exhibited a poreless and dense structure. Mechanical investigations of BN - TiN composites were performed using a hardness test and an ultrasonic method based on the measurement of the velocity of ultrasonic waves transition through the samples. Young’s modulus and the hardness of the investigated composites strictly depended on the cBN contents and on the synthesis parameters. Characteristic, optimum conditions which gave good mechanical properties for composites with different cBN ratios were determined. Cutting test conducted on tools made of selected cBN-TiN composites showed that a composite with volume ratio of cBN equal 65% exhibited the best cutting performance.', 'corpus_id': 137785354}	14676	[{'doc_id': '231706681', 'title': 'Effect of Powder Particle Size and Spray Parameters on the Ni/Al Reaction During Plasma Spraying of Ni-Al Composite Powders', 'abstract': 'It was known for long that Ni-Al composite powders can be used to deposit self-bonding coating as a bond coat for common ceramic coatings due to the exothermic reaction between Ni and Al. However, it was found that with commercial Ni-Al composite powders with a large particle size, it is difficult to ignite the self-propagating reaction between Ni and Al to form Ni-Al intermetallics by plasma spraying. In this study, Ni-Al composite powder particles of different sizes were used to prepare Ni-Al intermetallics-based coatings by plasma spraying. The dependencies of the exothermic reaction between Ni and Al and the coating microstructure on powder particle size and spray parameters were investigated. The phase composition, microstructure, porosity and oxide content of the coatings were characterized by x-ray diffraction, scanning electron microscope and image analyzing. The results show that particle size of Ni-Al composite powders is the dominant factor controlling the exothermic reaction for the formation of Ni-Al intermetallics during plasma spraying. When the powders larger than about 50\xa0μm are used, the reaction forming aluminide cannot complete even by heating of plasma flame generated at high plasma arc power. However, when smaller powders less than 50\xa0μm are used, the exothermic reaction can completely occur rapidly in plasma spraying, contributing to heating of Ni-Al droplets to the highest temperature for development of the self-bonding effect. The positive relationship between molten droplet temperature and tensile adhesive strength of the resultant coatings is recognized to confirm the contribution of high droplet temperature to the adhesive or cohesive strength.', 'corpus_id': 231706681, 'score': 0}, {'doc_id': '135799265', 'title': 'Grinding of PCBN cutting inserts', 'abstract': 'Abstract PCBN cutting inserts have been more often used in order to attend to the demands of an economically viable process and to lead to a proper workpiece surface quality. A proper application of this cutting material requires its adequate processing. Plunge-face grinding is used for finishing the inserts after sintering. To choose a suitable grinding tool and process parameters, the properties of the ground cutting inserts must be taken into account. Therefore, the influence of PCBN grain size and composition on the insert cutting edge and surface quality has to be investigated. This work aims to give an overview of material removal mechanisms, process forces and abrasive grain wear during grinding different PCBN inserts. It was found that the insert quality depends mainly on the material removal mechanism, which in the studied case is defined by the PCBN grain size.', 'corpus_id': 135799265, 'score': 1}, {'doc_id': '110555471', 'title': 'Tool wear when turning hardened AISI 4340 with coated PCBN tools using finishing cutting conditions', 'abstract': 'Abstract Tool wear is one of the most important aspects in metal cutting, especially when machining hardened steels. The present work shows the results of tool wear, cutting force and surface finish obtained from the turning operation on hardened AISI 4340 using PCBN coated and uncoated edges. Three different coatings were tested using finishing conditions: TiAlN, TiAlN-nanocoating and AlCrN. The lowest tool wear happened with TiAlN-nanocoating followed by TiAlN, AlCrN and uncoated PCBN. Forces followed the same pattern, increasing in the same order, after flank wear appears. At the beginning of cutting, there was no significant difference amongst the coated tools, only the uncoated one showing higher cutting force. Ra values were between 0.7 and 1.2\xa0μm with no large differences amongst the tools. Finite element method (FEM) simulations indicated that temperature at the chip–tool interface was around 800\xa0°C in absence of flank wear, independently of coating. In that range only the TiAlN coating oxidize since AlCrN needs higher than 1000\xa0°C. Therefore, due to a combination of high hardness in the cutting temperature range and the presence of an oxidizing layer, TiAlN-nanocoating performed better in terms of tool wear and surface roughness.', 'corpus_id': 110555471, 'score': 1}, {'doc_id': '231583230', 'title': 'Micropillar compression of single crystal tungsten carbide, Part 1: temperature and orientation dependence of deformation behaviour', 'abstract': 'Tungsten carbide cobalt hardmetals are commonly used as cutting tools subject to high operation temperature and pressures, where the mechanical performance of the tungsten carbide phase affects the wear and lifetime of the material. In this study, the mechanical behaviour of the isolated tungsten carbide (WC) phase was investigated using single crystal micropillar compression. Micropillars 1-5 μm in diameter, in two crystal orientations, were fabricated using focused ion beam (FIB) machining and subsequently compressed between room temperature and 600 °C. The activated plastic deformation mechanisms were strongly anisotropic and weakly temperature dependent. The flow stresses of basal-oriented pillars were about three times higher than the prismatic pillars, and pillars of both orientations soften slightly with increasing temperature. The basal pillars tended to deform by either unstable cracking or unstable yield, whereas the prismatic pillars deformed by slip-mediated cracking. However, the active deformation mechanisms were also sensitive to pillar size and shape. Slip trace analysis of the deformed pillars showed that {101̅0} prismatic planes were the dominant slip plane in WC. Basal slip was also identified as a secondary slip system, activated at high temperatures.', 'corpus_id': 231583230, 'score': 0}, {'doc_id': '231572986', 'title': 'Processing of Nanostructured Bulk Fe-Cr Alloys by Severe Plastic Deformation', 'abstract': 'The processing of binary alloys consisting of ferromagnetic Fe and antiferromagnetic Cr by severe plastic deformation (SPD) with different chemical compositions has been investigated. Although the phase diagram exhibits a large gap in the thermodynamical equilibrium at lower temperatures, it is shown that techniques based on SPD help to overcome common processing limits. Different processing routes including initial ball milling (BM) and arc melting (AM) and a concatenation with annealing treatments prior to high-pressure torsion (HPT) deformation are compared in this work. Investigation of the deformed microstructures by electron microscopy and synchrotron X-ray diffraction reveal homogeneous, nanocrystalline microstructures for HPT deformed AM alloys. HPT deformation of powder blends and BM powders leads to an exorbitant increase in hardness or an unusual fast formation of a σ-phase and therefore impede successful processing.', 'corpus_id': 231572986, 'score': 0}, {'doc_id': '135822228', 'title': 'Wear mechanisms of several cutting tool materials in hard turning of high carbon–chromium tool steel', 'abstract': 'Abstract The present study illustrates the performance of three different cutting tool materials, namely: PCBN, TiN coated PCBN, and mixed aluminum ceramic (Al 2 O 3 +TiC) in the turning of medium hardened D2 tool steel (52 HRC). Formation of Cr–O tribofilms on the ceramic tool surface as a result of interaction with the workpiece material and environment (identified by X-ray Photoelectron Spectroscopy) leads to improvement of lubricating properties at the tool/chip interface. Obtained results revealed that the mixed alumina ceramic tool can outperform both types of PCBN under different machinability criteria.', 'corpus_id': 135822228, 'score': 1}, {'doc_id': '232147443', 'title': 'Amorphous complexions alter the tensile failure of nanocrystalline Cu-Zr alloys', 'abstract': 'Abstract Grain boundary-based mechanisms are known to control the plastic deformation and failure of nanocrystalline metals, with manipulation of the boundary structure a promising path for tuning this response. In this study, the role of interfacial structural disorder on plasticity and failure of nanocrystalline Cu-Zr alloys is investigated with in situ scanning electron microscopy tensile deformation experiments. Two model materials are created, one with only the typical ordered grain boundaries and another with amorphous intergranular films interspersed into the boundary network, while the microstructures are otherwise identical. Hence, the importance of complexion type on plasticity and failure is isolated by only varying complexion structure. The tensile experiments show that failure of the samples containing amorphous films is significantly retarded, as evidenced by an increase in the cross-sectional area reduction, a decrease in the occurrence of shear-dominated failure, a decrease in strain localization, and fracture surfaces with more elongated dimple features. As a whole, this study provides direct evidence that structural disorder at the grain boundaries can be beneficial for improving the ductility of nanocrystalline metals.', 'corpus_id': 232147443, 'score': 0}, {'doc_id': '225251940', 'title': 'Porous textured ceramics with controlled grain size and orientation', 'abstract': 'Abstract Texture in ceramics can enhance their functional and structural properties. Current methods for texturation in dense ceramics employ plate-like particles or microplatelets as starting powders and processes that drive their orientation into specific directions. Using ultra-low magnetic fields combined with slip casting, it is possible to purposely orient magnetically responsive particles in any direction. Here, we explore the application of magnetic slip casting to bimodal slurries containing alumina nanoparticles and alumina microplatelets of various dimensions to create texture in ceramics with different grain sizes. After sintering, the obtained ceramics show anisotropic grains with controllable orientations, grain length of 3–7\u2009µm, mild open porosity between 30–40 %, and interesting cracking behavior. Textured porous ceramics with tunable grain dimensions and orientations are promising for filtering, biomedical or composite applications.', 'corpus_id': 225251940, 'score': 0}, {'doc_id': '218966045', 'title': 'Influence of the cutting direction angle on the tool wear behavior in face plunge grinding of PcBN', 'abstract': 'Abstract Polycrystalline cubic boron nitride (PcBN) is a highly wear resistant material. Due to its high hardness this material is typically machined with diamond grinding tools. The high hardness and high-temperature hardness of PcBN leads to a significant grinding tool wear. The applied cutting direction angle during face plunge grinding offers the possibility to influence the geometry of the contact area between the grinding tool and the PcBN workpiece. However, the underlying principal mechanisms and influences of parameters are not fully understood today. The contact zone geometry is described by the width of cut and the geometric contact length. The paper provides a mathematical description of these two parameters for S-shapes PcBN cutting inserts depending on the workpiece geometry and the cutting direction angle. It is shown that the contact length significantly determines the wear mechanism.', 'corpus_id': 218966045, 'score': 1}, {'doc_id': '139153043', 'title': 'Investigation of the mechanical properties and cutting performance of cBN-based cutting tools with Cr3C2 binder phase.', 'abstract': 'In order to investigate new materials for metal cutting applications, cubic boron nitride, cutting tools with different amounts of binder phase were sintered in HPHT toroid type apparatus under 7.7 GPa and in temperature range of 1450-2450°C. Initial mixtures of three composition were chosen with 50, 60, 65 vol. % of cBN, 5 vol. % of Al was added to mixture to prevent oxidation. Phase composition, microstructure, elastic properties, hardness, fracture toughness and cutting performance were investigated. The highest value of the mechanical properties and tool life demonstrated samples sintered in temperature range 1850-2150 °C.', 'corpus_id': 139153043, 'score': 1}]
1	{'doc_id': '202786019', 'title': 'An Overview of Remote Photoplethysmography Methods for Vital Sign Monitoring', 'abstract': 'Vital signs such as heart rate, respiratory rate, blood pressure, body temperature, and oxygen saturation are essential for early detection of any related significant illness. Many of the existing methods that are used to monitor the aforementioned vital signs are camera-based. In these methods, sensors are fixed to the body that are not sturdy to the motion of the subject. Another method to monitor vital signs is photoplethysmography (PPG), an emerging noncontact technique that maps, spatial blood volume variation in living tissue from the images captured through a video. Most of the camera-based methods are driven by three remote photoplethysmography algorithms. The camera-based methods are useful for detecting vital signs with an objective AAA, i.e., anyone, anywhere, and anytime. However, there exist few challenges in r-ppg methods and make it an open research problem. This paper presents an overview of the signal processing challenges faced by remote photoplethysmography for calculating the vital signs with a focus on heart rate estimation.', 'corpus_id': 202786019}	11159	"[{'doc_id': '224819456', 'title': 'Perspective on the increasing role of optical wearables and remote patient monitoring in the COVID-19 era and beyond', 'abstract': 'Abstract. Significance: The COVID-19 pandemic is changing the landscape of healthcare delivery in many countries, with a new shift toward remote patient monitoring (RPM). Aim: The goal of this perspective is to highlight the existing and future role of wearable and RPM optical technologies in an increasingly at-home healthcare and research environment. Approach: First, the specific changes occurring during the COVID-19 pandemic in healthcare delivery, regulations, and technological innovations related to RPM technologies are reviewed. Then, a review of the current state and potential future impact of optical physiological monitoring in portable and wearable formats is outlined. Results: New efforts from academia, industry, and regulatory agencies are advancing and encouraging at-home, portable, and wearable physiological monitors as a growing part of healthcare delivery. It is hoped that these shifts will assist with disease diagnosis, treatment, management, recovery, and rehabilitation with minimal in-person contact. Some of these trends are likely to persist for years to come. Optical technologies already account for a large portion of RPM platforms, with a good potential for future growth. Conclusions: The biomedical optics community has a potentially large role to play in developing, testing, and commercializing new wearable and RPM technologies to meet the changing healthcare and research landscape in the COVID-19 era and beyond.', 'corpus_id': 224819456, 'score': 1}, {'doc_id': '227221347', 'title': 'Implementing an embedded system to identify possible COVID-19 suspects using thermovision cameras', 'abstract': 'The main goal of this paper is to prove that by combining thermal vision cameras and image processing with many deep learning classification algorithms we developed an effective embedded system with high applicability in this critical period caused by COVID-19 pandemic disease. Using fixed and mobile thermal cameras we envisioned and developed a real time temperature screening capable of sending alarm signals over network or by SMS to local authorities along with multiple detection metrics such as the age, the gender, the facial emotion, the GPS location where the alarm went off, the temperature reading from the human face and also if the subject is wearing or not a medical face mask.', 'corpus_id': 227221347, 'score': 0}, {'doc_id': '224705978', 'title': 'MyWear: A Smart Wear for Continuous Body Vital Monitoring and Emergency Alert', 'abstract': ""Smart healthcare which is built as healthcare Cyber-Physical System (H-CPS) from Internet-of-Medical-Things (IoMT) is becoming more important than before. Medical devices and their connectivity through Internet with alongwith the electronics health record (EHR) and AI analytics making H-CPS possible. IoMT-end devices like wearables and implantables are key for H-CPS based smart healthcare. Smart garment is a specific wearable which can be used for smart healthcare. There are various smart garments that help users to monitor their body vitals in real-time. Many commercially available garments collect the vital data and transmit it to the mobile application for visualization. However, these don't perform real-time analysis for the user to comprehend their health conditions. Also, such garments are not included with an alert system to alert users and contacts in case of emergency. In MyWear, we propose a wearable body vital monitoring garment that captures physiological data and automatically analyses such heart rate, stress level, muscle activity to detect abnormalities. A copy of the physiological data is transmitted to the cloud for detecting any abnormalities in heart beats and predict any potential heart failure in future. We also propose a deep neural network (DNN) model that automatically classifies abnormal heart beat and potential heart failure. For immediate assistance in such a situation, we propose an alert system that sends an alert message to nearby medical officials. The proposed MyWear has an average accuracy of 96.9% and precision of 97.3% for detection of the abnormalities."", 'corpus_id': 224705978, 'score': 1}, {'doc_id': '3820228', 'title': 'Non-contact video-based vital sign monitoring using ambient light and auto-regressive models.', 'abstract': ""Remote sensing of the reflectance photoplethysmogram using a video camera typically positioned 1\xa0m away from the patient's face is a promising method for monitoring the vital signs of patients without attaching any electrodes or sensors to them. Most of the papers in the literature on non-contact vital sign monitoring report results on human volunteers in controlled environments. We have been able to obtain estimates of heart rate and respiratory rate and preliminary results on changes in oxygen saturation from double-monitored patients undergoing haemodialysis in the Oxford Kidney Unit. To achieve this, we have devised a novel method of cancelling out aliased frequency components caused by artificial light flicker, using auto-regressive (AR) modelling and pole cancellation. Secondly, we have been able to construct accurate maps of the spatial distribution of heart rate and respiratory rate information from the coefficients of the AR model. In stable sections with minimal patient motion, the mean absolute error between the camera-derived estimate of heart rate and the reference value from a pulse oximeter is similar to the mean absolute error between two pulse oximeter measurements at different sites (finger and earlobe). The activities of daily living affect the respiratory rate, but the camera-derived estimates of this parameter are at least as accurate as those derived from a thoracic expansion sensor (chest belt). During a period of obstructive sleep apnoea, we tracked changes in oxygen saturation using the ratio of normalized reflectance changes in two colour channels (red and blue), but this required calibration against the reference data from a pulse oximeter."", 'corpus_id': 3820228, 'score': 1}, {'doc_id': '221819388', 'title': 'DR2S: Deep Regression with Region Selection for Camera Quality Evaluation', 'abstract': 'In this work, we tackle the problem of estimating a camera capability to preserve fine texture details at a given lighting condition. Importantly, our texture preservation measurement should coincide with human perception. Consequently, we formulate our problem as a regression one and we introduce a deep convolutional network to estimate texture quality score. At training time, we use ground-truth quality scores provided by expert human annotators in order to obtain a subjective quality measure. In addition, we propose a region selection method to identify the image regions that are better suited at measuring perceptual quality. Finally, our experimental evaluation shows that our learning-based approach outperforms existing methods and that our region selection algorithm consistently improves the quality estimation.', 'corpus_id': 221819388, 'score': 0}, {'doc_id': '228090884', 'title': 'Subject-Independent Slow Fall Detection with Wearable Sensors via Deep Learning', 'abstract': 'One of the major healthcare challenges is elderly fallers. A fall can lead to disabilities and even mortality. With the current Covid-19 pandemic, insufficient resources could be provided for the care of elderlies, and care workers often may not be able to visit them. Therefore, a fall may get undetected or delayed leading to serious harm or consequences. Automatic fall detection systems could provide the necessary detection and warnings for timely intervention. Although many sensor-based fall detection systems have been proposed, most systems focus on the sudden fall and have not considered the slow fall scenario, a typical fall instance for elderly fallers. In this paper, a robust activity (RA) and slow fall detection system is proposed. The system consists of a waist-worn wearable sensor embedded with an inertial measurement unit (IMU) and a barometer, and a reference ambient barometer. A deep neural network (DNN) is developed for fusing the sensor data and classifying fall events. The results have shown that the IMU-barometer design yield better detection of fall events and the DNN approach (90.33% accuracy) outperforms traditional machine learning algorithms.', 'corpus_id': 228090884, 'score': 0}, {'doc_id': '229416234', 'title': 'Real-time Webcam Heart-Rate and Variability Estimation with Clean Ground Truth for Evaluation', 'abstract': 'Remote photo-plethysmography (rPPG) uses a camera to estimate a person’s heart rate (HR). Similar to how heart rate can provide useful information about a person’s vital signs, insights about the underlying physio/psychological conditions can be obtained from heart rate variability (HRV). HRV is a measure of the fine fluctuations in the intervals between heart beats. However, this measure requires temporally locating heart beats with a high degree of precision. We introduce a refined and efficient real-time rPPG pipeline with novel filtering and motion suppression that not only estimates heart rates, but also extracts the pulse waveform to time heart beats and measure heart rate variability. This unsupervised method requires no rPPG specific training and is able to operate in real-time. We also introduce a new multi-modal video dataset, VicarPPG 2, specifically designed to evaluate rPPG algorithms on HR and HRV estimation. We validate and study our method under various conditions on a comprehensive range of public and self-recorded datasets, showing state-of-the-art results and providing useful insights into some unique aspects. Lastly, we make available CleanerPPG, a collection of human-verified ground truth peak/heart-beat annotations for existing rPPG datasets. These verified annotations should make future evaluations and benchmarking of rPPG algorithms more accurate, standardized and fair.', 'corpus_id': 229416234, 'score': 1}, {'doc_id': '226289948', 'title': 'Deep correction of breathing-related artifacts in MR-thermometry', 'abstract': ""Real-time MR-imaging has been clinically adapted for monitoring thermal therapies since it can provide on-the-fly temperature maps simultaneously with anatomical information. However, proton resonance frequency based thermometry of moving targets remains challenging since temperature artifacts are induced by the respiratory as well as physiological motion. If left uncorrected, these artifacts lead to severe errors in temperature estimates and impair therapy guidance. In this study, we evaluated deep learning for on-line correction of motion related errors in abdominal MR-thermometry. For this, a convolutional neural network (CNN) was designed to learn the apparent temperature perturbation from images acquired during a preparative learning stage prior to hyperthermia. The input of the designed CNN is the most recent magnitude image and no surrogate of motion is needed. During the subsequent hyperthermia procedure, the recent magnitude image is used as an input for the CNN-model in order to generate an on-line correction for the current temperature map. The method's artifact suppression performance was evaluated on 12 free breathing volunteers and was found robust and artifact-free in all examined cases. Furthermore, thermometric precision and accuracy was assessed for in vivo ablation using high intensity focused ultrasound. All calculations involved at the different stages of the proposed workflow were designed to be compatible with the clinical time constraints of a therapeutic procedure."", 'corpus_id': 226289948, 'score': 0}, {'doc_id': '227407995', 'title': 'The Role of Edge Robotics As-a-Service in Monitoring COVID-19 Infection', 'abstract': 'Deep learning technology has been widely used in edge computing. However, pandemics like covid-19 require deep learning capabilities at mobile devices (detect respiratory rate using mobile robotics or conduct CT scan using a mobile scanner), which are severely constrained by the limited storage and computation resources at the device level. To solve this problem, we propose a three-tier architecture, including robot layers, edge layers, and cloud layers. We adopt this architecture to design a non-contact respiratory monitoring system to break down respiratory rate calculation tasks. Experimental results of respiratory rate monitoring show that the proposed approach in this paper significantly outperforms other approaches. It is supported by computation time costs with 2.26 ms per frame, 27.48 ms per frame, 0.78 seconds for convolution operation, similarity calculation, processing one-minute length respiratory signals, respectively. And the computation time costs of our three-tier architecture are less than that of edge+cloud architecture and cloud architecture. Moreover, we use our three-tire architecture for CT image diagnosis task decomposition. The evaluation of a CT image dataset of COVID-19 proves that our three-tire architecture is useful for resolving tasks on deep learning networks by edge equipment. There are broad application scenarios in smart hospitals in the future.', 'corpus_id': 227407995, 'score': 0}, {'doc_id': '229320256', 'title': 'Your Smart Speaker Can ""Hear"" Your Heartbeat!', 'abstract': ""Vital sign monitoring is a common practice amongst medical professionals, and plays a key role in patient care and clinical diagnosis. Traditionally, dedicated equipment is employed to monitor these vital signs. For example, electrocardiograms (ECG) with 3-12 electrodes are attached to the target chest for heartbeat monitoring. In the last few years, wireless sensing becomes a hot research topic and wireless signal itself is utilized for sensing purposes without requiring the target to wear any sensors. The contact-free nature of wireless sensing makes it particularly appealing in current COVID-19 pandemic. Recently, promising progress has been achieved and the sensing granularity has been pushed to millimeter level, fine enough to monitor respiration which causes a chest displacement of 5 mm. While a great success with respiration monitoring, it is still very challenging to monitor heartbeat due to the extremely subtle chest displacement (0.1 - 0.5 mm) - smaller than 10% of that caused by respiration. What makes it worse is that the tiny heartbeat-caused chest displacement is buried inside the respiration-caused displacement. In this paper, we show the feasibility of employing the popular smart speakers (e.g., Amazon Echo) to monitor an individual's heartbeats in a contact-free manner. To extract the submillimeter heartbeat motion in the presence of other interference movements, a series of novel signal processing schemes are employed. We successfully prototype the first real-time heartbeat monitoring system using a commodity smart speaker. Experiment results show that the proposed system can monitor a target's heartbeat accurately, achieving a median heart rate estimation error of 0.75 beat per minute (bpm), and a median heartbeat interval estimation error of 13.28 ms (less than 1.8%), outperforming even some popular commodity products available on the market."", 'corpus_id': 229320256, 'score': 1}]"
2	{'doc_id': '235358266', 'title': 'Efficient Training of Visual Transformers with Small-Size Datasets', 'abstract': 'Visual Transformers (VTs) are emerging as an architectural paradigm alternative to Convolutional networks (CNNs). Differently from CNNs, VTs can capture global relations between image elements and they potentially have a larger representation capacity. However, the lack of the typical convolutional inductive bias makes these models more data-hungry than common CNNs. In fact, some local properties of the visual domain which are embedded in the CNN architectural design, in VTs should be learned from samples. In this paper, we empirically analyse different VTs, comparing their robustness in a small training-set regime, and we show that, despite having a comparable accuracy when trained on ImageNet, their performance on smaller datasets can be largely different. Moreover, we propose a self-supervised task which can extract additional information from images with only a negligible computational overhead. This task encourages the VTs to learn spatial relations within an image and makes the VT training much more robust when training data are scarce. Our task is used jointly with the standard (supervised) training and it does not depend on specific architectural choices, thus it can be easily plugged in the existing VTs. Using an extensive evaluation with different VTs and datasets, we show that our method can improve (sometimes dramatically) the final accuracy of the VTs. The code will be available upon acceptance.', 'corpus_id': 235358266}	12718	[{'doc_id': '232290660', 'title': 'Toward Compact Deep Neural Networks via Energy-Aware Pruning', 'abstract': 'Despite of the remarkable performance, modern deep neural networks are inevitably accompanied with a significant amount of computational cost for learning and deployment, which may be incompatible with their usage on edge devices. Recent efforts to reduce these overheads involves pruning and decomposing the parameters of various layers without performance deterioration. Inspired by several decomposition studies, in this paper, we propose a novel energy-aware pruning method that quantifies the importance of each filter in the network using nuclear-norm (NN). Proposed energy-aware pruning leads to state-of-the art performance for Top-1 accuracy, FLOPs, and parameter reduction across a wide range of scenarios with multiple network architectures on CIFAR-10 and ImageNet after fine-grained classification tasks. On toy experiment, despite of no fine-tuning, we can visually observe that NN not only has little change in decision boundaries across classes, but also clearly outperforms previous popular criteria. We achieve competitive results with 40.4/49.8% of FLOPs and 45.9/52.9% of parameter reduction with 94.13/94.61% in the Top-1 accuracy with ResNet-56/110 on CIFAR-10, respectively. In addition, our observations are consistent for a variety of different pruning setting in terms of data size as well as data quality which can be emphasized in the stability of the acceleration and compression with negligible accuracy loss. Our code is available at https://github.com/ nota-github/nota-pruning-rank. Compute NN', 'corpus_id': 232290660, 'score': 0}, {'doc_id': '231699188', 'title': 'Pruning and Quantization for Deep Neural Network Acceleration: A Survey', 'abstract': 'Abstract Deep neural networks have been applied in many applications exhibiting extraordinary abilities in the field of computer vision. However, complex network architectures challenge efficient real-time deployment and require significant computation resources and energy costs. These challenges can be overcome through optimizations such as network compression. Network compression can often be realized with little loss of accuracy. In some cases accuracy may even improve. This paper provides a survey on two types of network compression: pruning and quantization. Pruning can be categorized as static if it is performed offline or dynamic if it is performed at run-time. We compare pruning techniques and describe criteria used to remove redundant computations. We discuss trade-offs in element-wise, channel-wise, shape-wise, filter-wise, layer-wise and even network-wise pruning. Quantization reduces computations by reducing the precision of the datatype. Weights, biases, and activations may be quantized typically to 8-bit integers although lower bit width implementations are also discussed including binary neural networks. Both pruning and quantization can be used independently or combined. We compare current techniques, analyze their strengths and weaknesses, present compressed network accuracy results on a number of frameworks, and provide practical guidance for compressing networks.', 'corpus_id': 231699188, 'score': 0}, {'doc_id': '235658057', 'title': 'FreeTickets: Accurate, Robust and Efficient Deep Ensemble by Training with Dynamic Sparsity', 'abstract': 'Recent works on sparse neural networks have demonstrated that it is possible to train a sparse network in isolation to match the performance of the corresponding dense networks with a fraction of parameters. However, the identification of these performant sparse neural networks (winning tickets) either involves a costly iterative train-pruneretrain process (e.g., Lottery Ticket Hypothesis) or an overextended sparse training time (e.g., Training with Dynamic Sparsity), both of which would raise financial and environmental concerns. In this work, we attempt to address this cost-reducing problem by introducing the FreeT ickets concept, as the first solution which can boost the performance of sparse convolutional neural networks over their dense network equivalents by a large margin, while using for complete training only a fraction of the computational resources required by the latter. Concretely, we instantiate the FreeT ickets concept, by proposing two novel efficient ensemble methods with dynamic sparsity, which yield in one shot many diverse and accurate tickets “for free” during the sparse training process. The combination of these free tickets into an ensemble demonstrates a significant improvement in accuracy, uncertainty estimation, robustness, and efficiency over the corresponding dense (ensemble) networks. Our results provide new insights into the strength of sparse neural networks and suggest that the benefits of sparsity go way beyond the usual training/inference expected efficiency. We will release all codes in https://github. com/Shiweiliuiiiiiii/FreeTickets.', 'corpus_id': 235658057, 'score': 1}, {'doc_id': '237532494', 'title': 'Exploiting Activation based Gradient Output Sparsity to Accelerate Backpropagation in CNNs', 'abstract': 'Machine/deep-learning (ML/DL) based techniques are emerging as a driving force behind many cutting-edge technologies, achieving high accuracy on computer vision workloads such as image classification and object detection. However, training these models involving large parameters is both time-consuming and energy-hogging. In this regard, several prior works have advocated for sparsity to speed up the of DL training and more so, the inference phase. This work begins with the observation that during training, sparsity in the forward and backward passes are correlated. In that context, we investigate two types of sparsity (input and output type) inherent in gradient descent-based optimization algorithms and propose a hardware micro-architecture to leverage the same. Our experimental results use five state-of-the-art CNN models on the Imagenet dataset, and show back propagation speedups in the range of 1.69× to 5.43×, compared to the dense baseline execution. By exploiting sparsity in both the forward and backward passes, speedup improvements range from 1.68× to 3.30× over the sparsity-agnostic baseline execution. Our work also achieves significant reduction in training iteration time over several previously proposed dense as well as sparse accelerator based platforms, in addition to achieving order of magnitude energy efficiency improvements over GPU based execution.', 'corpus_id': 237532494, 'score': 1}, {'doc_id': '231740811', 'title': 'Zen-NAS: A Zero-Shot NAS for High-Performance Deep Image Recognition', 'abstract': 'Accuracy predictor is a key component in Neural Architecture Search (NAS) for ranking architectures. Building a high-quality accuracy predictor usually costs enormous computation. To address this issue, instead of using an accuracy predictor, we propose a novel zero-shot index dubbed Zen-Score to rank the architectures. The Zen-Score represents the network expressivity and positively correlates with the model accuracy. The calculation of Zen-Score only takes a few forward inferences through a randomly initialized network, without training network parameters. Built upon the Zen-Score, we further propose a new NAS algorithm, termed as Zen-NAS, by maximizing the Zen-Score of the target network under given inference budgets. Within less than half GPU day, Zen-NAS is able to directly search high performance architectures in a data-free style. Comparing with previous NAS methods, the proposed Zen-NAS is magnitude times faster on multiple server-side and mobile-side GPU platforms with state-of-the-art accuracy on ImageNet. Searching and training code as well as pre-trained models are available from https://github.com/ idstcv/ZenNAS. *Accepted by ICCV 2021. Author home page https://minglin-home.github.io 1 ar X iv :2 10 2. 01 06 3v 4 [ cs .C V ] 2 3 A ug 2 02 1', 'corpus_id': 231740811, 'score': 0}, {'doc_id': '235658996', 'title': 'Low-Precision Training in Logarithmic Number System using Multiplicative Weight Update', 'abstract': 'Training large-scale deep neural networks (DNNs) currently requires a significant amount of energy, leading to serious environmental impacts. One promising approach to reduce the energy costs is representing DNNs with low-precision numbers. While it is common to train DNNs with forward and backward propagation in low-precision, training directly over low-precision weights, without keeping a copy of weights in high-precision, still remains to be an unsolved problem. This is due to complex interactions between learning algorithms and low-precision number systems. To address this, we jointly design a low-precision training framework involving a logarithmic number system (LNS) and a multiplicative weight update training method, termed LNS-Madam. LNS has a high dynamic range even in a low-bitwidth setting, leading to high energy efficiency and making it relevant for on-board training in energy-constrained edge devices. We design LNS to have the flexibility of choosing different bases for weights and gradients, as they usually require different quantization gaps and dynamic ranges during training. By drawing the connection between LNS and multiplicative update, LNS-Madam ensures low quantization error during weight update, leading to a stable convergence even if the bitwidth is limited. Compared to using a fixed-point or floating-point number system and training with popular learning algorithms such as SGD and Adam, our joint design with LNS and LNS-Madam optimizer achieves better accuracy while requiring smaller bitwidth. Notably, with only 5-bit for gradients, the proposed training framework achieves accuracy comparable to full-precision state-of-the-art models such as ResNet-50 and BERT. To verify the efficiency of our framework, we also conduct energy estimations by analyzing the math datapath units during training. We calculate that our design achieves over 60x energy reduction compared to FP32 on BERT models. For full training of ResNet-50 on ImageNet, our design reduces the carbon emissions by 98% around. ∗Work done during an internship at NVIDIA Research. Preprint. Under review. ar X iv :2 10 6. 13 91 4v 1 [ cs .L G ] 2 6 Ju n 20 21 1.7B 7.5B 39B 145B 530B 1TB Number of Parameters in GPT Models 1 10 10 10 10 E ne rg y C os t p er It er at io n (m J)', 'corpus_id': 235658996, 'score': 1}, {'doc_id': '235497935', 'title': 'Dynamic Precision Tunability in Low-Power Training of Deep Neural Networks', 'abstract': 'Deep neural networks (DNN) have been utilized in numerous machine learning problems due to their high inference and generalization capabilities. Their signature features including their high computational complexity and their hunger for large datasets have made Graphical Processing Units (GPU) a suitable platform for accelerating both their training and inference. However, semi-custom IP blocks for ASICs and FPGAs now compete with GPUs in terms of higher performance per watt and their ability to perform computations with variable number formats and word lengths. Our work focuses on studying the effects of different number representation on the training performance of DNNs. More specifically, we are trying to replace the floating point number representation with less accurate representations that tend to decrease the computational complexity, memory/bandwidth requirements, and power consumption of DNN training while approximating the original full precision classification accuracy of the final trained model. The goal is to dynamically tune the number of bits in fixed-point number representation during the training process and generate a hardware accelerator that can adapt to this dynamic change and utilize it for reducing power consumption. As an initial study of the impact of the dynamic fixed-point number representation, we have conducted a two-phase training of a 6-layer DNN on the CIFAR-10 dataset with fixed-point calculations to measure classification performance, and an ASIC/FPGA-based model for power consumption.', 'corpus_id': 235497935, 'score': 1}, {'doc_id': '236771815', 'title': 'Octo: INT8 Training with Loss-aware Compensation and Backward Quantization for Tiny On-device Learning', 'abstract': 'On-device learning is an emerging technique to pave the last mile of enabling edge intelligence, which eliminates the limitations of conventional in-cloud computing where dozens of computational capacities and memories are needed. A highperformance on-device learning system requires breaking the constraints of limited resources and alleviating computational overhead. In this paper, we show that employing the 8-bit fixed-point (INT8) quantization in both forward and backward passes over a deep model is a promising way to enable tiny on-device learning in practice. The key to an efficient quantization-aware training method is to exploit the hardwarelevel enabled acceleration while preserving the training quality in each layer. However, off-the-shelf quantization methods cannot handle the on-device learning paradigm of fixed-point processing. To overcome these challenges, we propose a novel INT8 training method, which optimizes the computation of forward and backward passes via the delicately designed Lossaware Compensation (LAC) and Parameterized Range Clipping (PRC), respectively. Specifically, we build a new network component, the compensation layer, to automatically counteract the quantization error of tensor arithmetic. We implement our method in Octo, a lightweight cross-platform system for tiny on-device learning. Evaluation on commercial AI chips shows that Octo holds higher training efficiency over state-of-the-art quantization training methods, while achieving adequate processing speedup and memory reduction over the full-precision training.', 'corpus_id': 236771815, 'score': 1}]
3	{'doc_id': '152214855', 'title': 'A Particular Kind of Wonder: The Experience of Magic past and Present', 'abstract': 'Wonder may be an important emotion, but the term wonder is remarkably ambiguous. For centuries, in psychological discourse, it has been defined as a variety of things. In an attempt to be more focused, and given the growing scientific interest in magic, this article describes a particular kind of wonder: the response to a magic trick. It first provides a historical perspective by considering continuity and change over time in this experience, and argues that, in certain respects, this particular kind of wonder has changed. It then describes in detail the experience of magic, considers the extent to which it might be considered acquired rather than innate, and how it relates to other emotions, such as surprise. In the process, it discusses the role of belief and offers some suggestions for future research. It concludes by noting the importance of context and meaning in shaping the nature of the experience, and argues for the value of both experimental and historical research in the attempt to understand such experiences.', 'corpus_id': 152214855}	10856	"[{'doc_id': '221824984', 'title': 'Seeing consciousness through the lens of memory', 'abstract': 'In this My Word, Joseph LeDoux and Hakwan Lau argue that everyday human conscious experiences cannot be understood separately from memory. The authors build on a tripartite model of memory as a way of fractionating consciousness into components that account for wide ranging experiences, from the simplest sensory experience of the color of an apple to a full-blown feeling of fear or other emotions.', 'corpus_id': 221824984, 'score': 0}, {'doc_id': '12053731', 'title': 'Misdirection, attention and awareness: Inattentional blindness reveals temporal relationship between eye movements and visual awareness', 'abstract': 'We designed a magic trick that could be used to investigate how misdirection can prevent people from perceiving a visually salient event, thus offering a novel paradigm to examine inattentional blindness. We demonstrate that participants’ verbal reports reflect what they saw rather than inferences about how they thought the trick was done and thus provide a reliable index of conscious perception. Eye movements revealed that for a subset of participants their conscious perception was not related to where they were looking at the time of the event and thus demonstrate how overt and covert attention can be spatially dissociated. However, detection of the event resulted in rapid shifts of eye movements towards the detected event, thus indicating a strong temporal link between overt and covert attention, and that covert attention can be allocated at least 2 or 3 saccade targets ahead of where people are fixating.', 'corpus_id': 12053731, 'score': 1}, {'doc_id': '224837798', 'title': 'Value Intentions in Future Art Teachers’ Professional Training', 'abstract': 'The article is devoted to the study of future art teachers’ value sphere. A wide range of applications of the theory of values in various fields of knowledge is shown: culturology, psychology, pedagogy, art education, economics; classification of values in science is given. A number of issues is actualized, concerning values in the context of cultural development and at the same time global crises; professional development, competitiveness and life views and needs; development of person’s abilities and his/her self-realization in art creativity. The artistic-communicative, professional-hermeneutic and motivational-need spheres in the future art teachers’ creative self-realization are singled out. These areas are subject to pedagogical influence, while becoming a conglomeration of value intentions of the individual. Theoretical substantiation is conducted and the essence of the phenomenon of future art teachers’ value intentions is determined, which combines the specified multi-vector values. The meaning of the terms “intentionality” and “intentions” is clarified. Emphasis is placed on the importance of value intentions of the individual in the economic projection and compliance of this socio-personal construct with the current concept of student-centered learning. The results of the study of future art teachers’ value intentions, which are formed during professional training and creative activity, are given. The results of the value intentions diagnostics and dynamics of their changes in accordance with future musical art teachers’ professional training are highlighted. Scientific approaches and technologies that effectively influence their value intentions are outlined. It is proved that definition of a conglomeration of value intentions of an individual allows a qualitative choice of methods for the formation of creative and competitive personality of future musical art teachers and teachers of art disciplines in general.', 'corpus_id': 224837798, 'score': 0}, {'doc_id': '15584605', 'title': 'Towards a science of magic', 'abstract': 'It is argued here that cognitive science currently neglects an important source of insight into the human mind: the effects created by magicians. Over the centuries, magicians have learned how to perform acts that are perceived as defying the laws of nature, and that induce a strong sense of wonder. This article argues that the time has come to examine the scientific bases behind such phenomena, and to create a science of magic linked to relevant areas of cognitive science. Concrete examples are taken from three areas of magic: the ability to control attention, to distort perception, and to influence choice. It is shown how such knowledge can help develop new tools and indicate new avenues of research into human perception and cognition.', 'corpus_id': 15584605, 'score': 1}, {'doc_id': '224804073', 'title': 'In the Blink of an Eye: Reading Mental States From Briefly Presented Eye Regions', 'abstract': 'Faces provide not only cues to an individual’s identity, age, gender, and ethnicity but also insight into their mental states. The aim was to investigate the temporal aspects of processing of facial expressions of complex mental states for very short presentation times ranging from 12.5 to 100 ms in a four-alternative forced choice paradigm based on Reading the Mind in the Eyes test. Results show that participants are able to recognise very subtle differences between facial expressions; performance is better than chance, even for the shortest presentation time. Importantly, we show for the first time that observers can recognise these expressions based on information contained in the eye region only. These results support the hypothesis that the eye region plays a particularly important role in social interactions and that the expressions in the eyes are a rich source of information about other peoples’ mental states. When asked to what extent the observers guessed during the task, they significantly underestimated their ability to make correct decisions, yet perform better than chance, even for very brief presentation times. These results are particularly relevant in the light of the current COVID-19 pandemic and the associated wearing of face coverings.', 'corpus_id': 224804073, 'score': 0}, {'doc_id': '226959795', 'title': 'A motion aftereffect from viewing other people’s gaze', 'abstract': 'Recent work suggests that our brains may generate subtle, false motion signals streaming from other people to the objects of their attention, aiding social cognition. For instance, brief exposure to static images depicting other people gazing at objects made subjects slower at detecting subsequent motion in the direction of gaze, suggesting that looking at someone else’s gaze caused a directional motion adaptation. Here we confirm, using a more stringent method, that viewing static images of another person gazing in a particular direction, at an object, produced motion aftereffects in the opposite direction. The aftereffect was manifested as a change in perceptual decision threshold for detecting left versus right motion. The effect disappeared when the person was looking away from the object. These findings suggest that the attentive gaze of others is encoded as an implied agent-to-object motion that is sufficiently robust to cause genuine motion aftereffects, though subtle enough to remain subthreshold.', 'corpus_id': 226959795, 'score': 0}, {'doc_id': '221510033', 'title': 'Steady-state visually evoked potentials and feature-based attention: Pre-registered null results and a focused review of methodological considerations', 'abstract': 'Feature-based attention is the ability to selectively attend to a particular feature (e.g., attend to red but not green items while looking for the ketchup bottle in your refrigerator), and steady-state visually evoked potentials (SSVEPs) measured from the human electroencephalogram (EEG) signal have been used to track the neural deployment of feature-based attention. Although many published studies suggest that we can use trial-by-trial cues to enhance relevant feature information (i.e., greater SSVEP response to the cued color), there is ongoing debate about whether participants may likewise use trial-by-trial cues to voluntarily ignore a particular feature. Here, we report the results of a pre-registered study in which participants either were cued to attend or to ignore a color. Counter to prior work, we found no attention-related modulation of the SSVEP response in either cue condition. However, positive control analyses revealed that participants paid some degree of attention to the cued color (i.e., we observed a greater P300 component to targets in the attended versus the unattended color). In light of these unexpected null results, we conducted a focused review of methodological considerations for studies of feature-based attention using SSVEPs. In the review, we quantify potentially important stimulus parameters that have been used in the past (e.g., stimulation frequency; trial counts) and we discuss the potential importance of these and other task factors (e.g., feature-based priming) for SSVEP studies.', 'corpus_id': 221510033, 'score': 0}, {'doc_id': '5325690', 'title': ""Magic and Fixation: Now You Don't See it, Now You Do"", 'abstract': 'over somatosensory modalities. And the interaction of vision and proprioception has been studied recently in animals and humans (Graziano 1999; Fink et al 1999; Farne et al 2000). Ramachandran and colleagues had the clever insight to appreciate and show that visual input of the reflection of the intact arm seen in a mirror could be used to mobilize previously immobile phantom limbs. Subsequent studies have confirmed the findings of Ramachandran and colleagues (1995) in some phantom-limb patients (Giraux and Sirigu 2003; Hunter et al 2003), and case reports and small studies have shown that therapy similarly using a mirror may be beneficial for some patients with movement deficits of other causes (Altschuler et al 1999; Sathian et al 2000; McCabe et al 2003; Moseley 2004). The effect described here suggests a rather direct effect of vision on motion in healthy subjects, the study of which may be helpful in developing improved rehabilitation methods for patients with motor deficits secondary to neurologic disease. In preparing this paper, I also noticed another paper that demonstrates a different experimental system in which there is an effect of vision via a mirror on proprioception and movement (Franz and Packman 2004).', 'corpus_id': 5325690, 'score': 1}, {'doc_id': '208209', 'title': 'Magically deceptive biological motion—the French Drop Sleight', 'abstract': 'Intentional deception, as is common in the performance of magic tricks, can provide valuable insight into the mechanisms of perception and action. Much of the recent investigations into this form of deception revolve around the attention of the observer. Here, we present experiments designed to investigate the contributions of the performer to the act of deception. An experienced magician and a naïve novice performed a classic sleight known as the French Drop. Video recordings of the performance were used to measure the quality of the deception—e.g., if a non-magician observer could discriminate instances where the sleight was performed (a deceptive performance) from those where it was not (a veridical performace). During the performance we recorded the trajectory of the hands and measured muscle activity via EMG to help understand the biomechanical mechanisms of this deception. We show that expertise plays a major role in the quality of the deception and that there are significant variations in the motion and muscular behaviors between successful and unsuccessful performances. Smooth, minimal movements with an exaggerated faux-transfer of muscular tension were characteristic of better deception. This finding is consistent with anecdotal reports and the magic performance literature.', 'corpus_id': 208209, 'score': 1}, {'doc_id': '35408290', 'title': 'Eye movements affirm: automatic overt gaze and arrow cueing for typical adults and adults with autism spectrum disorder', 'abstract': 'People with autism spectrum disorder (ASD) show reduced interest towards social aspects of the environment and a lesser tendency to follow other people’s gaze in the real world. However, most studies have shown that people with ASD do respond to eye-gaze cues in experimental paradigms, though it is possible that this behaviour is based on an atypical strategy. We tested this possibility in adults with ASD using a cueing task combined with eye-movement recording. Both eye gaze and arrow pointing distractors resulted in overt cueing effects, both in terms of increased saccadic reaction times, and in proportions of saccades executed to the cued direction instead of to the target, for both participant groups. Our results confirm previous reports that eye gaze cues as well as arrow cues result in automatic orienting of overt attention. Moreover, since there were no group differences between arrow and eye gaze cues, we conclude that overt attentional orienting in ASD, at least in response to centrally presented schematic directional distractors, is typical.', 'corpus_id': 35408290, 'score': 1}]"
4	{'doc_id': '235437005', 'title': 'Usefulness of Respiratory Mechanics and Laboratory Parameter Trends as Markers of Early Treatment Success in Mechanically Ventilated Severe Coronavirus Disease: A Single-Center Pilot Study', 'abstract': 'Whether a patient with severe coronavirus disease (COVID-19) will be successfully liberated from mechanical ventilation (MV) early is important in the COVID-19 pandemic. This study aimed to characterize the time course of parameters and outcomes of severe COVID-19 in relation to the timing of liberation from MV. This retrospective, single-center, observational study was performed using data from mechanically ventilated COVID-19 patients admitted to the ICU between 1 March 2020 and 15 December 2020. Early liberation from ventilation (EL group) was defined as successful extubation within 10 days of MV. The trends of respiratory mechanics and laboratory data were visualized and compared between the EL and prolonged MV (PMV) groups using smoothing spline and linear mixed effect models. Of 52 admitted patients, 31 mechanically ventilated COVID-19 patients were included (EL group, 20 (69%); PMV group, 11 (31%)). The patients’ median age was 71 years. While in-hospital mortality was low (6%), activities of daily living (ADL) at the time of hospital discharge were significantly impaired in the PMV group compared to the EL group (mean Barthel index (range): 30 (7.5–95) versus 2.5 (0–22.5), p = 0.048). The trends in respiratory compliance were different between patients in the EL and PMV groups. An increasing trend in the ventilatory ratio during MV until approximately 2 weeks was observed in both groups. The interaction between daily change and earlier liberation was significant in the trajectory of the thrombin–antithrombin complex, antithrombin 3, fibrinogen, C-reactive protein, lymphocyte, and positive end-expiratory pressure (PEEP) values. The indicator of physiological dead space increases during MV. The trajectory of markers of the hypercoagulation status, inflammation, and PEEP were significantly different depending on the timing of liberation from MV. These findings may provide insight into the pathophysiology of COVID-19 during treatment in the critical care setting.', 'corpus_id': 235437005}	19039	"[{'doc_id': '43111392', 'title': 'Health Informatics Tools to Improve Utilization of Laboratory Tests.', 'abstract': 'Herein, we discuss improper test utilization practices and their implications on delivery of health care, as well as providing a brief explanation of the means to reduce such practices by improvement of personnel factors, particularly involving physicians. The article also elaborates on ways to mitigate improperly utilized test practices using appropriate health informatics technologies to their maximum possible capacities.', 'corpus_id': 43111392, 'score': 1}, {'doc_id': '236034840', 'title': 'A quality improvement project to reduce antibiotic utilization and ancillary laboratory tests in the appraisal of early-onset sepsis in the NICU.', 'abstract': 'BACKGROUND\nDiagnosis and treatment of early-onset sepsis (EOS) of the newborn remains a controversial issue among providers due to the non-infectious symptomology which exists in the newborn period.\n\n\nMETHODS\nPre/post interventional quality improvement project in a level III NICU to reduce antibiotic utilization and ancillary laboratory tests with the introduction of an evidence-based guideline for the evaluation of EOS in the NICU.\n\n\nRESULTS\nPrimary outcome measures include mean number of empiric antibiotic treatment days and utilization rate (AUR), number of laboratory tests ordered, and incidence of unwarranted antibiotic therapy beyond the 48-h rule out period. Mean empiric antibiotic treatment days decreased from 2.94 to 1.58 days and overall antibiotic use decreased from 73.7% to 57.1%. Likewise, the mean AUR decreased from 212.5 to 147.6 days of therapy per 1000 patient days. There was an 86% decline in the number of ancillary tests and unwarranted antibiotic use beyond 48- h was reduced by 74%.\n\n\nDISCUSSION\nGuidelines for EOS of the newborn should include a thorough baseline evaluation of the drivers of antibiotic use to create an evidence-based foundation. Reducing unnecessary antibiotic use and EOS evaluations in a safe and effective manner have the potential to lower consumer and healthcare expenditures while improving the long-term health of the newborn in the NICU.\n\n\nCONCLUSIONS\nThese findings emphasize the importance of implementing an evidence-based protocol for antibiotic stewardship in the NICU. With further research there is the potential to improve the healthcare of newborns while reducing expenditures in a safe, effective evaluation of EOS in the newborn population.', 'corpus_id': 236034840, 'score': 1}, {'doc_id': '201712658', 'title': 'Identifying low value pathology test ordering in hospitalised patients: a retrospective cohort study across two hospitals.', 'abstract': 'The push to identify low value care has led to scrutiny of pathology test re-ordering. The objective of this study was to identify the patterns of ordering pathology tests among inpatients in teaching hospitals and model strategies to reduce unnecessary testing. This was a retrospective cohort study of all adult medical and surgical inpatients admitted to one major teaching hospital and one rural hospital in the same health district over 2 years. Obstetric, gynaecological, intensive care, elective/day procedures and dialysis admissions were excluded. Orders for electrolytes, urea and creatinine (EUC), full blood count (FBC), thyroid stimulating hormone (TSH), glycated haemoglobin (HbA1c), vitamin D, and troponin, date of order, and value of the resulting test, were obtained from a health district data warehouse. Pathology results were mapped to each inpatient day. EUC and FBC constituted over 90% of all inpatient pathology requests for these six tests. Between 40-45% of inpatients had EUC and/or FBC performed daily. After the first couple of tests, the retest interval was consistently around 24 hours, regardless of the previous value of the test, consistent with a culture of routine ordering. This was less pronounced in the rural hospital compared to the urban teaching hospital. Lockouts (applied when previous tests normal) or minimum retest intervals (applied to previously normal and abnormal tests) of various lengths were tested on the data to find optimal combinations that reduced unnecessary tests without missing too many very abnormal tests. A lockout of 48 hours for EUC and 48 hour lockout combined with a 12 hour minimum retest interval for FBC appear optimal to reduce over ordering and could save approximately AU$400/inpatient bed per year at a single teaching hospital. There is evidence of low value re-ordering of EUC and FBC pathology tests. Implementation of a computerised physician order entry system with inbuilt prompts to restrict unnecessary re-ordering of pathology tests may be a practical solution.', 'corpus_id': 201712658, 'score': 1}, {'doc_id': '20503895', 'title': 'Influence of educational, audit and feedback, system based, and incentive and penalty interventions to reduce laboratory test utilization: a systematic review', 'abstract': 'Abstract Laboratory and radiographic tests are often ordered unnecessarily. This excess testing has financial costs and is a burden on patients. We performed a systematic review to determine the effectiveness interventions to reduce test utilization by physicians. The MEDLINE and EMBASE databases were searched for the years 1946 through to September 2013 for English articles that had themes of test utilization and cost containment or optimization. Bibliographies of included papers were scanned to identify other potentially relevant studies. Our search resulted in 3236 articles of which 109 met the inclusion criteria of having an intervention aimed at reducing test utilization with results that could be expressed as a percent reduction in test use relative to the comparator. Each intervention was categorized into one or more non-exclusive category of education, audit and feedback, system based, or incentive or penalty. A rating of study quality was also performed. The percent reductions in test use ranged from a 99.7% reduction to a 27.7% increase in test use. Each category of intervention was effective in reducing test utilization. Heterogeneity between interventions, poor study quality, and limited time horizons makes generalizations difficult and calls into question the validity of results. Very few studies measure any patient safety or quality of care outcomes affected by reduced test use. There are numerous studies that use low investment strategies to reduce test utilization with one time changes in the ordering system. These low investment strategies are the most promising for achievable and durable reductions in inappropriate test use.', 'corpus_id': 20503895, 'score': 1}, {'doc_id': '235956766', 'title': 'Simple prognostic factors and change of inflammatory markers in patients with severe coronavirus disease 2019: a single‐center observational study', 'abstract': 'Aim The aim of this study was to investigate the prognostic factors and evaluate the change in inflammatory markers of patients with coronavirus disease 2019 (COVID‐19) requiring mechanical ventilation. Methods This retrospective observational study conducted from April 1, 2020, to February 18, 2021, included 97 adult patients who required mechanical ventilation for severe COVID‐19 pneumonia and excluded nonintubated patients with a positive COVID‐19 polymerase chain reaction test and those who had any obvious bacterial infection on admission. All patients were followed up to discharge or death. We obtained clinical information and laboratory data including levels of presepsin, interleukin‐6, procalcitonin, and severe acute respiratory syndrome coronavirus 2 (SARS‐CoV‐2) antibody every day. Poor outcome was defined as death or receiving a tracheostomy during hospitalization, and favorable outcome was defined as discharge after extubation. Results Differences (median [interquartile range]) were detected in age (76 [70–82] versus 66 [55–74] years), day from the onset of first symptoms to admission for mechanical ventilation (5 [3–7] versus 10 [8–12] days), and P/F ratio (i.e., ratio of arterial oxygen concentration to the fraction of inspired oxygen) after intubation (186 [149–251] versus 236 [180–296]) in patients with poor outcome versus those with favorable outcome on admission. Serum SARS‐CoV‐2 antibody levels had already increased on admission in patients with favorable outcome. We determined the day from the onset of first symptoms to admission for mechanical ventilation to be one of the independent prognostic factors of patients with COVID‐19 (adjusted odds ratio 0.69, confidence interval 0.56–0.85). Conclusion These results may contribute to understanding the mechanism of progression in severe COVID‐19 and may be helpful in devising an effective therapeutic strategy.', 'corpus_id': 235956766, 'score': 0}, {'doc_id': '205474491', 'title': 'High-Value, Cost-Conscious Care: Iterative Systems-Based Interventions to Reduce Unnecessary Laboratory Testing.', 'abstract': 'BACKGROUND\nInappropriate testing contributes to soaring healthcare costs within the United States, and teaching hospitals are vulnerable to providing care largely for academic development. Via its ""Choosing Wisely"" campaign, the American Board of Internal Medicine recommends avoiding repetitive testing for stable inpatients. We designed systems-based interventions to reduce laboratory orders for patients admitted to the wards at an academic facility.\n\n\nMETHODS\nWe identified the computer-based order entry system as an appropriate target for sustainable intervention. The admission order set had allowed multiple routine tests to be ordered repetitively each day. Our iterative study included interventions on the automated order set and cost displays at order entry. The primary outcome was number of routine tests controlled for inpatient days compared with the preceding year. Secondary outcomes included cost savings, delays in care, and adverse events.\n\n\nRESULTS\nData were collected over a 2-month period following interventions in sequential years and compared with the year prior. The first intervention led to 0.97 fewer laboratory tests per inpatient day (19.4%). The second intervention led to sustained reduction, although by less of a margin than order set modifications alone (15.3%). When extrapolating the results utilizing fees from the Centers for Medicare and Medicaid Services, there was a cost savings of $290,000 over 2 years. Qualitative survey data did not suggest an increase in care delays or near-miss events.\n\n\nCONCLUSIONS\nThis series of interventions targeting unnecessary testing demonstrated a sustained reduction in the number of routine tests ordered, without adverse effects on clinical care.', 'corpus_id': 205474491, 'score': 1}, {'doc_id': '235644109', 'title': 'Risk Stratification for ECMO Requirement in COVID-19 ICU Patients Using Quantitative Imaging Features in CT Scans on Admission', 'abstract': '(1) Background: Extracorporeal membrane oxygenation (ECMO) therapy in intensive care units (ICUs) remains the last treatment option for Coronavirus disease 2019 (COVID-19) patients with severely affected lungs but is highly resource demanding. Early risk stratification for the need of ECMO therapy upon admission to the hospital using artificial intelligence (AI)-based computed tomography (CT) assessment and clinical scores is beneficial for patient assessment and resource management; (2) Methods: Retrospective single-center study with 95 confirmed COVID-19 patients admitted to the participating ICUs. Patients requiring ECMO therapy (n = 14) during ICU stay versus patients without ECMO treatment (n = 81) were evaluated for discriminative clinical prediction parameters and AI-based CT imaging features and their diagnostic potential to predict ECMO therapy. Reported patient data include clinical scores, AI-based CT findings and patient outcomes; (3) Results: Patients subsequently allocated to ECMO therapy had significantly higher sequential organ failure (SOFA) scores (p < 0.001) and significantly lower oxygenation indices on admission (p = 0.009) than patients with standard ICU therapy. The median time from hospital admission to ECMO placement was 1.4 days (IQR 0.2–4.0). The percentage of lung involvement on AI-based CT assessment on admission to the hospital was significantly higher in ECMO patients (p < 0.001). In binary logistic regression analyses for ECMO prediction including age, sex, body mass index (BMI), SOFA score on admission, lactate on admission and percentage of lung involvement on admission CTs, only SOFA score (OR 1.32, 95% CI 1.08–1.62) and lung involvement (OR 1.06, 95% CI 1.01–1.11) were significantly associated with subsequent ECMO allocation. Receiver operating characteristic (ROC) curves showed an area under the curve (AUC) of 0.83 (95% CI 0.73–0.94) for lung involvement on admission CT and 0.82 (95% CI 0.72–0.91) for SOFA scores on ICU admission. A combined parameter of SOFA on ICU admission and lung involvement on admission CT yielded an AUC of 0.91 (0.84–0.97) with a sensitivity of 0.93 and a specificity of 0.84 for ECMO prediction; (4) Conclusions: AI-based assessment of lung involvement on CT scans on admission to the hospital and SOFA scoring, especially if combined, can be used as risk stratification tools for subsequent requirement for ECMO therapy in patients with severe COVID-19 disease to improve resource management in ICU settings.', 'corpus_id': 235644109, 'score': 0}, {'doc_id': '235410522', 'title': 'Paediatrics: how to manage septic shock', 'abstract': 'Background Septic shock is a common critical illness associated with high morbidity and mortality in children. This article provides an updated narrative review on the management of septic shock in paediatric practice. Methods A PubMed search was performed using the following Medical Subject Headings: “sepsis”, “septic shock” and “systemic inflammatory response syndrome”. The search strategy included meta-analyses, randomized controlled trials, clinical trials, observational studies and reviews. The search was limited to the English literature and specific to children. Results Septic shock is associated with high mortality and morbidity. The outcome can be improved if the diagnosis is made promptly and treatment initiated without delay. Early treatment with antimicrobial therapy, fluid therapy and vasoactive medications, and rapid recognition of the source of sepsis and control are the key recommendations from paediatric sepsis management guidelines. Conclusion Most of the current paediatric sepsis guideline recommendations are based on the adult population; therefore, the research gaps in paediatric sepsis management should be addressed.', 'corpus_id': 235410522, 'score': 0}, {'doc_id': '235717001', 'title': 'Clinical characteristics and outcomes of invasively ventilated patients with COVID-19 in Argentina (SATICOVID): a prospective, multicentre cohort study', 'abstract': '\n Background\n Although COVID-19 has greatly affected many low-income and middle-income countries, detailed information about patients admitted to the intensive care unit (ICU) is still scarce. Our aim was to examine ventilation characteristics and outcomes in invasively ventilated patients with COVID-19 in Argentina, an upper middle-income country.\n \n Methods\n In this prospective, multicentre cohort study (SATICOVID), we enrolled patients aged 18 years or older with RT-PCR-confirmed COVID-19 who were on invasive mechanical ventilation and admitted to one of 63 ICUs in Argentina. Patient demographics and clinical, laboratory, and general management variables were collected on day 1 (ICU admission); physiological respiratory and ventilation variables were collected on days 1, 3, and 7. The primary outcome was all-cause in-hospital mortality. All patients were followed until death in hospital or hospital discharge, whichever occurred first. Secondary outcomes were ICU mortality, identification of independent predictors of mortality, duration of invasive mechanical ventilation, and patterns of change in physiological respiratory and mechanical ventilation variables. The study is registered with ClinicalTrials.gov, NCT04611269, and is complete.\n \n Findings\n Between March 20, 2020, and Oct 31, 2020, we enrolled 1909 invasively ventilated patients with COVID-19, with a median age of 62 years [IQR 52–70]. 1294 (67·8%) were men, hypertension and obesity were the main comorbidities, and 939 (49·2%) patients required vasopressors. Lung-protective ventilation was widely used and median duration of ventilation was 13 days (IQR 7–22). Median tidal volume was 6·1 mL/kg predicted bodyweight (IQR 6·0–7·0) on day 1, and the value increased significantly up to day 7; positive end-expiratory pressure was 10 cm H2O (8–12) on day 1, with a slight but significant decrease to day 7. Ratio of partial pressure of arterial oxygen (PaO2) to fractional inspired oxygen (FiO2) was 160 (IQR 111–218), respiratory system compliance 36 mL/cm H2O (29–44), driving pressure 12 cm H2O (10–14), and FiO2 0·60 (0·45–0·80) on day 1. Acute respiratory distress syndrome developed in 1672 (87·6%) of patients; 1176 (61·6%) received prone positioning. In-hospital mortality was 57·7% (1101/1909 patients) and ICU mortality was 57·0% (1088/1909 patients); 462 (43·8%) patients died of refractory hypoxaemia, frequently overlapping with septic shock (n=174). Cox regression identified age (hazard ratio 1·02 [95% CI 1·01–1·03]), Charlson score (1·16 [1·11–1·23]), endotracheal intubation outside of the ICU (ie, before ICU admission; 1·37 [1·10–1·71]), vasopressor use on day 1 (1·29 [1·07–1·55]), D-dimer concentration (1·02 [1·01–1·03]), PaO2/FiO2 on day 1 (0·998 [0·997–0·999]), arterial pH on day 1 (1·01 [1·00–1·01]), driving pressure on day 1 (1·05 [1·03–1·08]), acute kidney injury (1·66 [1·36–2·03]), and month of admission (1·10 [1·03–1·18]) as independent predictors of mortality.\n \n Interpretation\n In patients with COVID-19 who required invasive mechanical ventilation, lung-protective ventilation was widely used but mortality was high. Predictors of mortality in our study broadly agreed with those identified in studies of invasively ventilated patients in high-income countries. The sustained burden of COVID-19 on scarce health-care personnel might have contributed to high mortality over the course of our study in Argentina. These data might help to identify points for improvement in the management of patients in middle-income countries and elsewhere.\n \n Funding\n None.\n \n Translation\n For the Spanish translation of the Summary see Supplementary Materials section.\n', 'corpus_id': 235717001, 'score': 0}, {'doc_id': '236012410', 'title': 'The prognostic value of extravascular lung water index in critically ill septic shock patients', 'abstract': 'ObjectiveTo investigate the prognostic value of extravascular lung water index (EVLWI) in critically ill patients with septic shock in intensive care unit (ICU). MethodsEVLWI was determined by using a PiCCO Monitor, and the daily fluid balance was recorded. ResultsFifty patients with septic shock were admitted and twenty-six patients survived. The average EVLWI at baseline was 11.7 ml/kg, and the difference was not different between survivors and nonsurvivors, P=0.551. The EVLWI of day 3 (EVLWI_ d_3 )in nonsurvivors was significantly higher than the survivors [(14.3±8.8)ml/kg vs (8.1±2.7) ml/kg, P=0.001]. If the patients were divided into three groups by the EVLWI_ d_3 , group one 0-7 ml/kg(4/16),group two 8-14 ml/kg(10/24),and group three 14 ml/kg(10/10), the hospital mortality of the third group was significantly higher than the other two groups (P=0.000, 0.002). There was a significant difference between the survivors and the nonsurvivors in the fluid balance at the first day and the following three days (P=0.000, 0.000). Negative fluid balance was associated with a lower mortality.By using receiver operating characteristic analysis, the area under the curve was 0.740±0.072 to EVLWI_ d_3 .If EVLWI 7.5 ml/kg, the sensitivity and the specificity of accurate judgment were 83.3% and 53.8%. ConclusionDynamic observation of EVLWI can be one of the factors for predicting the prognosis of patients with septic shock. A reduction of EVLWI at early treatment and a negative fluid balance were associated with a better prognosis.', 'corpus_id': 236012410, 'score': 0}]"
5	{'doc_id': '168170151', 'title': 'Correlation in Extensive-Form Games: Saddle-Point Formulation and Benchmarks', 'abstract': 'While Nash equilibrium in extensive-form games is well understood, very little is known about the properties of extensive-form correlated equilibrium (EFCE), both from a behavioral and from a computational point of view. In this setting, the strategic behavior of players is complemented by an external device that privately recommends moves to agents as the game progresses; players are free to deviate at any time, but will then not receive future recommendations. Our contributions are threefold. First, we show that an EFCE can be formulated as the solution to a bilinear saddle-point problem. To showcase how this novel formulation can inspire new algorithms to compute EFCEs, we propose a simple subgradient descent method which exploits this formulation and structural properties of EFCEs. Our method has better scalability than the prior approach based on linear programming. Second, we propose two benchmark games, which we hope will serve as the basis for future evaluation of EFCE solvers. These games were chosen so as to cover two natural application domains for EFCE: conflict resolution via a mediator, and bargaining and negotiation. Third, we document the qualitative behavior of EFCE in our proposed games. We show that the social-welfare-maximizing equilibria in these games are highly nontrivial and exhibit surprisingly subtle sequential behavior that so far has not received attention in the literature.', 'corpus_id': 168170151}	1172	"[{'doc_id': '219635890', 'title': 'First-order methods for large-scale market equilibrium computation', 'abstract': 'Market equilibrium is a solution concept with many applications such as digital ad markets, fair division, and resource sharing. For many classes of utility functions, equilibria are captured by convex programs. We develop simple first-order methods that are suitable for solving these programs for large-scale markets. We focus on three practically-relevant utility classes: linear, quasilinear, and Leontief utilities. Using structural properties of a market equilibrium under each utility class, we show that the corresponding convex programs can be reformulated as optimization of a structured smooth convex function over a polyhedral set, for which projected gradient achieves linear convergence. To do so, we utilize recent linear convergence results under weakened strong-convexity conditions, and further refine the relevant constants, both in general and for our specific setups. We then show that proximal gradient (a generalization of projected gradient) with a practical version of linesearch achieves linear convergence under the Proximal-PL condition. For quasilinear utilities, we show that Mirror Descent applied to a specific convex program achieves sublinear last-iterate convergence and recovers the Proportional Response dynamics, an elegant and efficient algorithm for computing market equilibrium under linear utilities. Numerical experiments show that proportional response is highly efficient for computing an approximate solution, while projected gradient with linesearch can be much faster when higher accuracy is required.', 'corpus_id': 219635890, 'score': 1}, {'doc_id': '211205212', 'title': 'Stochastic Regret Minimization in Extensive-Form Games', 'abstract': 'Monte-Carlo counterfactual regret minimization (MCCFR) is the state-of-the-art algorithm for solving sequential games that are too large for full tree traversals. It works by using gradient estimates that can be computed via sampling. However, stochastic methods for sequential games have not been investigated extensively beyond MCCFR. In this paper we develop a new framework for developing stochastic regret minimization methods. This framework allows us to use any regret-minimization algorithm, coupled with any gradient estimator. The MCCFR algorithm can be analyzed as a special case of our framework, and this analysis leads to significantly-stronger theoretical on convergence, while simultaneously yielding a simplified proof. Our framework allows us to instantiate several new stochastic methods for solving sequential games. We show extensive experiments on three games, where some variants of our methods outperform MCCFR.', 'corpus_id': 211205212, 'score': 1}, {'doc_id': '226281387', 'title': 'Stability of Gradient Learning Dynamics in Continuous Games: Scalar Action Spaces', 'abstract': 'Learning processes in games explain how players grapple with one another in seeking an equilibrium. We study a natural model of learning based on individual gradients in two- player continuous games. In such games, the arguably natural notion of a local equilibrium is a differential Nash equilibrium. However, the set of locally exponentially stable equilibria of the learning dynamics do not necessarily coincide with the set of differential Nash equilibria of the corresponding game. To characterize this gap, we provide formal guarantees for the stability or instability of such fixed points by leveraging the spectrum of the linearized game dynamics. We provide a comprehensive understanding of scalar games and find that equilibria that are both stable and Nash are robust to variations in learning rates.', 'corpus_id': 226281387, 'score': 1}, {'doc_id': '221135902', 'title': 'Kernel Methods for Cooperative Multi-Agent Contextual Bandits', 'abstract': ""Cooperative multi-agent decision making involves a group of agents cooperatively solving learning problems while communicating over a network with delays. In this paper, we consider the kernelised contextual bandit problem, where the reward obtained by an agent is an arbitrary linear function of the contexts' images in the related reproducing kernel Hilbert space (RKHS), and a group of agents must cooperate to collectively solve their unique decision problems. For this problem, we propose \\textsc{Coop-KernelUCB}, an algorithm that provides near-optimal bounds on the per-agent regret, and is both computationally and communicatively efficient. For special cases of the cooperative problem, we also provide variants of \\textsc{Coop-KernelUCB} that provides optimal per-agent regret. In addition, our algorithm generalizes several existing results in the multi-agent bandit setting. Finally, on a series of both synthetic and real-world multi-agent network benchmarks, we demonstrate that our algorithm significantly outperforms existing benchmarks."", 'corpus_id': 221135902, 'score': 0}, {'doc_id': '221081614', 'title': 'Efficiently Solving MDPs with Stochastic Mirror Descent', 'abstract': 'We present a unified framework based on primal-dual stochastic mirror descent for approximately solving infinite-horizon Markov decision processes (MDPs) given a generative model. When applied to an average-reward MDP with $A_{tot}$ total state-action pairs and mixing time bound $t_{mix}$ our method computes an $\\epsilon$-optimal policy with an expected $\\widetilde{O}(t_{mix}^2 A_{tot} \\epsilon^{-2})$ samples from the state-transition matrix, removing the ergodicity dependence of prior art. When applied to a $\\gamma$-discounted MDP with $A_{tot}$ total state-action pairs our method computes an $\\epsilon$-optimal policy with an expected $\\widetilde{O}((1-\\gamma)^{-4} A_{tot} \\epsilon^{-2})$ samples, matching the previous state-of-the-art up to a $(1-\\gamma)^{-1}$ factor. Both methods are model-free, update state values and policies simultaneously, and run in time linear in the number of samples taken. We achieve these results through a more general stochastic mirror descent framework for solving bilinear saddle-point problems with simplex and box domains and we demonstrate the flexibility of this framework by providing further applications to constrained MDPs.', 'corpus_id': 221081614, 'score': 0}, {'doc_id': '221266450', 'title': 'Search for a moving target in a competitive environment', 'abstract': 'We consider a discrete-time dynamic search game in which a number of players compete to find an invisible object that is moving according to a time-varying Markov chain. We examine the subgame perfect equilibria of these games. The main result of the paper is that the set of subgame perfect equilibria is exactly the set of greedy strategy profiles, i.e. those strategy profiles in which the players always choose an action that maximizes their probability of immediately finding the object. We discuss various variations and extensions of the model.', 'corpus_id': 221266450, 'score': 0}, {'doc_id': '221761151', 'title': 'The relationship between dynamic programming and active inference: the discrete, finite-horizon case', 'abstract': ""Active inference is a normative framework for generating behaviour based upon the free energy principle, a theory of self-organisation. This framework has been successfully used to solve reinforcement learning and stochastic control problems, yet, the formal relation between active inference and reward maximisation has not been fully explicated. In this paper, we consider the relation between active inference and dynamic programming under the Bellman equation, which underlies many approaches to reinforcement learning and control. We show that, on partially observable Markov decision processes, dynamic programming is a limiting case of active inference. In active inference, agents select actions to minimise expected free energy. In the absence of ambiguity about states, this reduces to matching expected states with a target distribution encoding the agent's preferences. When target states correspond to rewarding states, this maximises expected reward, as in reinforcement learning. When states are ambiguous, active inference agents will choose actions that simultaneously minimise ambiguity. This allows active inference agents to supplement their reward maximising (or exploitative) behaviour with novelty-seeking (or exploratory) behaviour. This clarifies the connection between active inference and reinforcement learning, and how both frameworks may benefit from each other."", 'corpus_id': 221761151, 'score': 0}, {'doc_id': '229371250', 'title': 'Fixed-Time Nash Equilibrium Seeking in Non-Cooperative Games', 'abstract': 'We introduce a novel class of Nash equilibrium seeking dynamics for non-cooperative games with a finite number of players, where the convergence to the Nash equilibrium is bounded by a $\\mathcal{K}\\mathcal{L}$ function with a settling time that can be upper bounded by a positive constant that is independent of the initial conditions of the players, and which can be prescribed a priori by the system designer. The dynamics are model-free, in the sense that the mathematical forms of the cost functions of the players are unknown. Instead, in order to update its own action, each player needs to have access only to real-time evaluations of its own cost, as well as to auxiliary states of neighboring players characterized by a communication graph. Stability and convergence properties are established for both potential games and strongly monotone games. Numerical examples are presented to illustrate our theoretical results.', 'corpus_id': 229371250, 'score': 1}, {'doc_id': '221319864', 'title': 'Variance-Reduced Proximal and Splitting Schemes for Monotone Stochastic Generalized Equations', 'abstract': 'We consider monotone inclusion problems where the operators may be expectation-valued. A direct application of proximal and splitting schemes is complicated by resolving problems with expectation-valued maps at each step, a concern that is addressed by using sampling. Accordingly, we propose avenues for addressing uncertainty in the mapping. (i) Variance-reduced stochastic proximal point method (vr-SPP). We develop amongst the first variance-reduced stochastic proximal-point schemes that achieves deterministic rates of convergence in terms of solving proximal-point problems. In addition, it is shown that the schemes are characterized by either optimal or near-optimal oracle (or sample) complexity guarantees. Finally, the generated sequences are shown to be convergent to a solution in an almost-sure sense in both monotone and strongly monotone regimes; (ii) Variance-reduced stochastic modified forward-backward splitting scheme (vr-SMFBS). In constrained settings, we consider structured settings when the map can be decomposed into an expectation-valued map $A$ and a maximal monotone map $B$ with a tractable resolvent. Akin to (i), we show that the proposed schemes are equipped with a.s. convergence guarantees, linear (strongly monotone $A$) and $\\mathcal{O}(1/k)$ (monotone $A$) rates of convergence while achieving optimal oracle complexity bounds. Of these, the rate statements in monotone regimes rely on leveraging the Fitzpatrick gap function for monotone inclusions. Furthermore, the schemes rely on weaker moment requirements on noise as well as allow for weakening unbiasedness requirements on oracles in strongly monotone regimes. Preliminary numerics reflect these findings and show that the variance-reduced schemes outperform stochastic approximation schemes, stochastic splitting and proximal point schemes, and sample-average approximation approaches.', 'corpus_id': 221319864, 'score': 0}, {'doc_id': '226306907', 'title': 'Recursive Regret Matching: A General Method for Solving Time-invariant Nonlinear Zero-sum Differential Games', 'abstract': 'In this paper, a new method is proposed to compute the rolling Nash equilibrium of the time-invariant nonlinear two-person zero-sum differential games. The idea is to discretize the time to transform a differential game into a sequential game with several steps, and by introducing state-value function, transform the sequential game into a recursion consisting of several normal-form games, finally, each normal-form game is solved with action abstraction and regret matching. To improve the real-time property of the proposed method, the state-value function can be kept in memory. This method can deal with the situations that the saddle point exists or does not exist, and the analysises of the existence of the saddle point can be avoided. If the saddle point does not exist, the mixed optimal control pair can be obtained. At the end of this paper, some examples are taken to illustrate the validity of the proposed method.', 'corpus_id': 226306907, 'score': 1}]"
6	{'doc_id': '25620555', 'title': 'Effects of Foliar and Root Applications of Methanol on the Growth of Arabidopsis, Tobacco, and Tomato Plants', 'abstract': 'The effects of aqueous methanol solutions applied as a foliar spray or via irrigation were investigated in Arabidopsis, tobacco, and tomato plants. Methanol applied to roots leads to phytotoxic damage in all three species tested. Foliar application causes an increase of fresh and dry weight in Arabidopsis and tobacco plants, but not in tomato plants. The increase in fresh and dry weight of Arabidopsis plants does not correlate with increased levels of soluble sugars, suggesting that increased accumulation of other products is responsible for the differences in the methanol-treated leaves. Foliar application of methanol can induce pectin methylesterase (PME) gene expression in Arabidopsis and tomato plants, activating specific PME genes.', 'corpus_id': 25620555}	10957	[{'doc_id': '225995403', 'title': 'The Surface of Tree Tissues as Source of Extractable Ice Nucleating Macromolecules during Rainfall Events', 'abstract': 'Several biological particles are able to trigger heterogeneous ice nucleation at subzero temperatures above -38°C. Many plants species such as winter rye [1], certain berries [2], pines and birches [3, 4] are known to contain biological ice-nucleating particles (BINPs) or rather icenucleating macromolecules (INMs). However, the influence of these BINPs on atmospheric processes including cloud glaciation and precipitation formation, as well as transport mechanisms of BINPs from the land surface into the atmosphere remain uncertain. If those INMs are easily available on the surfaces of a plant, they could be washed down by heavy rain events and could add an important new source for BINPs in the atmosphere, which has not received enough attention in the past.', 'corpus_id': 225995403, 'score': 1}, {'doc_id': '23641588', 'title': 'Plant Hormesis Management with Biostimulants of Biotic Origin in Agriculture', 'abstract': 'Over time plants developed complex mechanisms in order to adapt themselves to the environment. Plant innate immunity is one of the most important mechanisms for the environmental adaptation. A myriad of secondary metabolites with nutraceutical features are produced by the plant immune system in order to get adaptation to new environments that provoke stress (stressors). Hormesis is a phenomenon by which a stressor (i.e., toxins, herbicides, etc.) stimulates the cellular stress response, including secondary metabolites production, in order to help organisms to establish adaptive responses. Hormetins of biotic origin (i.e., biostimulants or biological control compounds), in certain doses might enhance plant performance, however, in excessive doses they are commonly deleterious. Biostimulants or biological control compounds of biotic origin are called “elicitors” that have widely been studied as inducers of plant tolerance to biotic and abiotic stresses. The plant response toward elicitors is reminiscent of hormetic responses toward toxins in several organisms. Thus, controlled management of hormetic responses in plants using these types of compounds is expected to be an important tool to increase nutraceutical quality of plant food and trying to minimize negative effects on yields. The aim of this review is to analyze the potential for agriculture that the use of biostimulants and biological control compounds of biotic origin could have in the management of the plant hormesis. The use of homolog DNA as biostimulant or biological control compound in crop production is also discussed.', 'corpus_id': 23641588, 'score': 1}, {'doc_id': '227068786', 'title': 'Estimation of leaf chlorophyll content in wheat using hyperspectral vegetation indices', 'abstract': 'CURRENT SCIENCE, VOL. 119, NO. 2, 25 JULY 2020 174 The available knowledge has shown that the sulphated polysaccharide is a wonder molecule with immense medicinal properties and it works on different types of bacteria and enveloped viruses which are similar to the SARS-CoV-2. Therefore in a world where more than 5 million infected people are struggling to get some relief and medical professionals and researchers are still fighting to reach any conclusive solution or effective medicine for SARS-CoV-2, sulphated polysaccharide from seaweeds can be a potent molecule to fight against COVID19 pandemic. Even molecules like carbohydrate binding proteins, lectins could also be used as tool against SARS-CoV2. A carbohydrate binding protein, Griffithsin, derived from red algae Griffithsia sp. – has shown in vitro and in vivo antiviral activity against enveloped viruses, and has registered low host toxicity, making it a candidate molecule to be studied against SARS-CoV-2. 1. Shereen, M. A., Khan, S., Kazmi, A., Bashir, N. and Siddique, R., J. Adv. Res., 2020, 24, 91–98. 2. Zhu, N. et al., New Engl. J. Med., 2020, 382(8), 727–733. 3. Singh, A. K., Singh, A., Shaika, A., Singh, R. and Misra, A., Clin. Res. Rev., 2020, 14, 241–246. 4. Wang, M. et al., Cell Res., 2020, https://doi.org/10.1038/s41422e020e0282e0. 5. Malve, H., J. Pharm. Bioallied Sci., 2016, 8(2), 83–91; doi:10.4103/09757406.171700. 6. Barahona, T. et al., Bioactive Carbohydr. Dietary Fibre, 2014, 4, 125–138. 7. Arfors, K. E. and Ley, K., J. Lab. Clin. Med., 1993, 121, 201–202. 8. Raposo, M. F., de Morais, R. M. and Bernardo de Morais, A. M., Mar. Drugs, 2013, 11(1), 233–252. 9. Shannon, E. and Abu-Ghannam, N., Mar. Drugs, 2016, 14, 81; doi:10.3390/ md14040081. 10. Ahmadi, A., Zorofchian, M. S., Abubakar, S. and Zandi, K., Biomed. Res. Int., 2015, 2015, 825203; doi:10.1155/2015/ 825203. 11. Damonte, E. B., Matulewicz, M. C. and Cerezo, A. S., Curr. Med. Chem., 2004, 11, 2399–2419. 12. Leibbrandt, A. et al., PLoS ONE, 2010, 5(12), e14320; doi:10.1371/journal.pone. 0014320. 13. Patel, S., Biotechnology, 2012, 2, 171– 185. 14. Necas, J. and Bartosikova, L., Medicina, 2013, 58(4), 187–205. 15. Lixin, H., Mingyue, S., Gordon, A., Morris and Jianhua Xie, Trends Food Sci. Technol., 2019, 92, 1–11 16. Lee, C., Mar. Drugs, 2019, 17(10), 567; doi:10.3390/md17100567.', 'corpus_id': 227068786, 'score': 0}, {'doc_id': '227126945', 'title': 'Mixing strategies combined with shape design to enhance productivity of a raceway pond', 'abstract': 'This paper focuses on mixing strategies and designing shape of the bottom topographies to enhance the growth of the microalgae in raceway ponds. A physical-biological coupled model is used to describe the growth of the algae. A simple model of a mixing device such as a paddle wheel is also considered. The complete process model was then included in an optimization problem associated with the maximization of the biomass production. The results show that non-trivial topographies can be coupled with some specific mixing strategies to improve the microalgal productivity.', 'corpus_id': 227126945, 'score': 0}, {'doc_id': '29724647', 'title': 'Linking isoprene with plant thermotolerance, antioxidants and monoterpene emissions', 'abstract': 'The purpose of the present study was to test the possible plant thermotolerance role of isoprene and to study its relationship with non-enzymatic antioxidants and terpene emissions. The gas exchange, chlorophyll fluorescence, extent of photo- and oxidative stress, leaf damage, mechanisms of photo- and antioxidant protection, and terpene emission were measured in leaves of Quercus ilex seedlings exposed to a ramp of temperatures of 5 ∞ ∞ ∞ C steps from 25 to 50 ∞ ∞ ∞ C growing with and without isoprene (10 m L L - 1 ) fumigation. The results showed that isoprene actually conferred thermotolerance (shifted the decrease of net photosynthetic rates from 35 to 45 ∞ C, increased F v / F m at 50 ∞ ∞ ∞ C from 0.38 to 0.65, and decreased the leaf area damaged from 27 to 15%), that it precluded or delayed the enhancement of the antioxidant non-enzymatic defence conferred by a tocopherol, ascorbic acid or b b b -carotene consumption in response to increasing temperatures, and that it decreased by approximately 70% the emissions of monoterpenes at the highest temperatures. This suggests that there are inducible mechanisms triggered by the initial stages of thermal damage that up-regulate these antioxidant compounds at high temperatures and that these mechanisms are somehow suppressed in the presence of exogenous isoprene, which seems to already exert an antioxidant-like behaviour.', 'corpus_id': 29724647, 'score': 1}, {'doc_id': '227121838', 'title': 'A review on hospital wastewater treatment: A special emphasis on occurrence and removal of pharmaceutically active compounds, resistant microorganisms, and SARS-CoV-2', 'abstract': '\n The hospital wastewater imposes a potent threat to the security of human health concerning its high vulnerability towards the outbreak of several diseases. Furthermore, the outbreak of COVID-19 pandemic demanded a global attention towards monitoring viruses and other infectious pathogens in hospital wastewater and their removal. Apart from that, the presence of various recalcitrant organics, pharmaceutically active compounds (PhACs), etc. imparts a complex pollution load to water resources and ecosystem. In this review, an insight into the occurrence, persistence and removal of drug-resistant microorganisms and infectious viruses as well as other micro-pollutants have been documented. The performance of various pilot/full-scale studies have been evaluated in terms of removal of biochemical oxygen demand (BOD), chemical oxygen demand (COD), total suspended solids (TSS), PhACs, pathogens, etc. It was found that many biological processes, such as membrane bioreactor, activated sludge process, constructed wetlands, etc. provided more than 80% removal of BOD, COD, TSS, etc. However, the removal of several recalcitrant organic pollutants are less responsive to those processes and demands the application of tertiary treatments, such as adsorption, ozone treatment, UV treatment, etc. Antibiotic-resistant microorganisms, viruses were found to be persistent even after the treatment of hospital wastewater, and high dose of chlorination or UV treatment was required to inactivate them. This article circumscribes the various emerging technologies, which have been used to treat PhACs and pathogens. The present review also emphasized the global concern of the presence of SARS-CoV-2 RNA in hospital wastewater and its removal by the existing treatment facilities.\n', 'corpus_id': 227121838, 'score': 0}, {'doc_id': '84893132', 'title': 'Anti-Helicobacter pylori and urease inhibitory activities of resveratrol and red wine', 'abstract': 'Abstract There is considerable interest in alternative approaches for the eradication of Helicobacter pylori using biologically active compounds including antioxidants from a wide range of natural sources. In this work we have investigated the antibacterial properties of resveratrol towards different H. pylori strains. In addition we studied the inhibition of H. pylori urease by resveratrol and red wine. In those assays, resveratrol inhibited the growth of all the 17 H. pylori strains tested, with inhibition diameters ranging from 16 to 28\xa0mm and minimum inhibitory concentration values varying from 25 to 100\xa0μg/mL, confirming its antimicrobial properties. Moreover, resveratrol and red wines showed an inhibitory effect on H. pylori urease activity, which is considered a virulence factor of this organism and essential for colonization and establishment of the infection. Further kinetic analysis revealed that inhibition occurred in a non-competitive and concentration-dependent manner. Overall, the results suggest that resveratrol and red wine may have potential for new therapy schemes that include natural products as an alternative therapeutic approach.', 'corpus_id': 84893132, 'score': 1}, {'doc_id': '3445984', 'title': 'How Plant Root Exudates Shape the Nitrogen Cycle.', 'abstract': 'Although the global nitrogen (N) cycle is largely driven by soil microbes, plant root exudates can profoundly modify soil microbial communities and influence their N transformations. A detailed understanding is now beginning to emerge regarding the control that root exudates exert over two major soil N processes - nitrification and N2 fixation. We discuss recent breakthroughs in this area, including the identification of root exudates as nitrification inhibitors and as signaling compounds facilitating N-acquisition symbioses. We indicate gaps in current knowledge, including questions of how root exudates affect newly discovered microbial players and N-cycle components. A better understanding of these processes is urgent given the widespread inefficiencies in agricultural N use and their links to N pollution and climate change.', 'corpus_id': 3445984, 'score': 1}, {'doc_id': '227151832', 'title': 'Remediation of bentazone contaminated water by Trametes versicolor: Characterization, identification of transformation products, and implementation in a trickle-bed reactor under non-sterile conditions.', 'abstract': 'Bentazone, an herbicide widely applied in rice and cereal crops, is widespread in the aquatic environment. This study evaluated the capacity of Trametes versicolor to remove bentazone from water. The fungus was able to completely remove bentazone after three days at Erlenmeyer-scale incubation. Both laccase and cytochrome P450 enzymatic systems were involved in bentazone degradation. A total of 19 transformation products (TPs) were identified to be formed during the process. The reactions involved in their formation included hydroxylations, oxidations, methylations, N-nitrosation, and dimerization. A laccase mediated radical mechanism was proposed for TP formation. In light of the results obtained at the Erlenmeyer scale, a trickle-bed reactor with T. versicolor immobilized on pine wood chips was set up to evaluate its stability during bentazone removal under non-sterile conditions. After 30 days of sequencing batch operation, an average bentazone removal of 48% was obtained, with a considerable contribution of adsorption onto the lignocellulosic support material. Bacterial contamination, which is the bottleneck in the implementation of fungal bioreactors, was successfully addressed by this particular system according to its maintained performance. This research is a pioneering step forward to the implementation of fungal bioremediation on a real scale.', 'corpus_id': 227151832, 'score': 0}, {'doc_id': '227132557', 'title': 'Panax ginseng C. A. Meyer as a potential therapeutic agent for organ fibrosis disease', 'abstract': 'Background Ginseng ( Panax ginseng C. A. Meyer), a representative Chinese herbal medicine, can improve the body’s antioxidant and anti-inflammatory capacity. Recently, scientists have shifted emphasis towards the initial stages of different malignant diseases—corresponding organ fibrosis and explored the essential role of P. ginseng in the treatment of fibrotic diseases. Main body In the first instance, the review generalizes the molecular mechanisms and common therapeutic methods of fibrosis. Next, due to the convenience and safety of individual medication, the research progress of ginseng extract and formulas in treating liver fibrosis, pulmonary fibrosis, myocardial fibrosis, and renal fibrosis has been systematically summarized. Finally, we describe active ingredients isolated from P. ginseng for their outstanding anti-fibrotic properties and further reveal the potential therapeutic prospect and limitations of P. ginseng in fibrotic diseases. Conclusions P. ginseng can be regarded as a valuable herbal medicine against fibrous tissue proliferation. Ginseng extract, derived formulas and monomers can inhibit the abundant deposition of extracellular matrix which caused by repeated damage and provide protection for fibrotic organs. Although the molecular mechanisms such as transforming growth factor β signal transduction have been confirmed, future studies should still focus on exploring the underlying mechanisms of P. ginseng in treating fibrotic disease including the therapeutic targets of synergistic action of multiple components in P. ginseng . Moreover, it is also necessary to carry out clinical trial to evaluate the feasibility of P. ginseng in combination with common fibrosis drugs.', 'corpus_id': 227132557, 'score': 0}]
7	{'doc_id': '10637317', 'title': 'SMRT compounds correct nonsense mutations in primary immunodeficiency and other genetic models', 'abstract': 'Within less than 10 years after the realization of the double helix of DNA, the ability of aminoglycosides to influence the misreading or readthrough of premature termination codons was discovered. It took another three decades to clone and sequence disease genes and appreciate the similarity of mutation spectra for most inborn errors. Nonsense mutations once again have become the target of readthrough compounds. In this brief review, we trace the development in our laboratory of the next generation of readthrough agents, small molecule readthrough (SMRT) drug‐like chemicals, and assays for comparing their in vitro activity. Possible mechanisms of action and potential clinical applications are considered.', 'corpus_id': 10637317}	5532	"[{'doc_id': '219515396', 'title': 'Pharmacological approaches for targeting cystic fibrosis nonsense mutations.', 'abstract': 'Cystic fibrosis (CF) is a monogenic autosomal recessive disorder. The clinical manifestations of the disease are caused by ∼2,000 mutations in the cystic fibrosis transmembrane conductance regulator (CFTR) protein. It is unlikely that any one approach will be efficient in correcting all defects. The recent approvals of ivacaftor, lumacaftor/ivacaftor and elexacaftor/tezacaftor/ivacaftor represent the genesis of a new era of precision combination medicine for the CF patient population. In this review, we discuss targeted translational readthrough approaches as mono and combination therapies for CFTR nonsense mutations. We examine the current status of efficacy of translational readthrough/nonsense suppression therapies and their limitations, including non-native amino acid incorporation at PTCs and nonsense-mediated mRNA decay (NMD), along with approaches to tackle these limitations. We further elaborate on combining various therapies such as readthrough agents, NMD inhibitors, and corrector/potentiators to improve the efficacy and safety of suppression therapy. These mutation specific strategies that are directed towards the basic CF defects should positively impact CF patients bearing nonsense mutations.', 'corpus_id': 219515396, 'score': 1}, {'doc_id': '219692176', 'title': 'High-throughput interrogation of programmed ribosomal frameshifting in human cells', 'abstract': 'Programmed ribosomal frameshifting (PRF) is the controlled slippage of the translating ribosome to an alternative frame. This process is widely employed by human viruses such as HIV and SARS coronavirus and is critical for their replication. Here, we developed a high-throughput approach to assess the frameshifting potential of a sequence. We designed and tested >12,000 sequences based on 15 viral and human PRF events, allowing us to systematically dissect the rules governing ribosomal frameshifting and discover novel regulatory inputs based on amino acid properties and tRNA availability. We assessed the natural variation in HIV gag-pol frameshifting rates by testing >500 clinical isolates and identified subtype-specific differences and associations between viral load in patients and the optimality of PRF rates. We devised computational models that accurately predict frameshifting potential and frameshifting rates, including subtle differences between HIV isolates. This approach can contribute to the development of antiviral agents targeting PRF. Programmed ribosomal frameshifting—the slippage of the ribosome to an alternative frame — is critical for viral replication and cellular processes. Here the authors present an approach that can assess the frameshifting potential of a sequence and elucidate the rules governing ribosomal frameshifting.', 'corpus_id': 219692176, 'score': 1}, {'doc_id': '52860843', 'title': 'Erratum to: Readthrough of nonsense mutations in Rett syndrome: evaluation of novel aminoglycosides and generation of a new mouse model', 'abstract': 'Thirty-five percent of patients with Rett syndrome carry nonsense mutations in the MECP2 gene. We have recently shown in transfected HeLa cells that read-through of nonsense mutations in the MECP2 gene can be achieved by treatment with gentamicin and geneticin. This study was performed to test if readthrough can also be achieved in cells endogenously expressing mutant MeCP2 and to evaluate potentially more effective readthrough compounds. A mouse model was generated carrying the R168X mutation in the MECP2 gene. Transfected HeLa cells expressing mutated MeCP2 fusion proteins and mouse ear fibroblasts isolated from the new mouse model were treated with gentamicin and the novel aminoglycosides NB30, NB54, and NB84. The localization of the read-through product was tested by immunofluorescence. Read-through of the R168X mutation in mouse ear fibroblasts using gentamicin was detected but at lower level than in HeLa cells. As expected, the readthrough product, full-length Mecp2 protein, was located in the nucleus. NB54 and NB84 induced readthrough more effectively than gentamicin, while NB30 was less effective. Readthrough of nonsense mutations can be achieved not only in transfected HeLa cells but also in fibroblasts of the newly generated Mecp2 R168X mouse model. NB54 and NB84 were more effective than gentamicin and are therefore promising candidates for readthrough therapy in Rett syndrome patients.', 'corpus_id': 52860843, 'score': 1}, {'doc_id': '218572031', 'title': 'Translation-associated mutational U-pressure in the first ORF of SARS-CoV-2 and other coronaviruses', 'abstract': 'Within four months of the ongoing COVID-19 pandemic caused by SARS-CoV-2, more than 250 nucleotide mutations have been detected in the ORF1 of the virus isolated from different parts of the globe. These observations open up an obvious question about the rate and direction of mutational pressure for further vaccine and therapeutics designing. In this study, we did a comparative analysis of ORF1a and ORF1b by using the first isolate (Wuhan strain) as the parent sequence. We observed that most of the nucleotide mutations are C to U transitions. The rate of synonymous C to U transitions is significantly higher than the rate of nonsynonymous ones, indicating negative selection on amino acid substitutions. Further, trends in nucleotide usage bias have been investigated in 49 coronaviruses species. A strong bias in nucleotide usage in fourfold degenerated sites towards uracil residues is seen in ORF1 of all the studied coronaviruses. A more substantial mutational U pressure is observed in ORF1a than in ORF1b owing to the translation of ORF1ab via programmed ribosomal frameshifting. Unlike other nucleotide mutations, mutational U pressure caused by cytosine deamination, mostly occurring in the RNA-plus strand, cannot be corrected by the proof-reading machinery of coronaviruses. The knowledge generated on the direction of mutational pressure during translation of viral RNA-plus strands has implications for vaccine and nucleoside analogue development for treating covid-19 and other coronavirus infections.', 'corpus_id': 218572031, 'score': 0}, {'doc_id': '23512039', 'title': 'Suppression of nonsense mutations as a therapeutic approach to treat genetic diseases', 'abstract': 'Suppression therapy is a treatment strategy for genetic diseases caused by nonsense mutations. This therapeutic approach utilizes pharmacological agents that suppress translation termination at in‐frame premature termination codons (PTCs) to restore translation of a full‐length, functional polypeptide. The efficiency of various classes of compounds to suppress PTCs in mammalian cells is discussed along with the current limitations of this therapy. We also elaborate on approaches to improve the efficiency of suppression that include methods to enhance the effectiveness of current suppression drugs and the design or discovery of new, more effective suppression agents. Finally, we discuss the role of nonsense‐mediated mRNA decay (NMD) in limiting the effectiveness of suppression therapy, and describe tactics that may allow the efficiency of NMD to be modulated in order to enhance suppression therapy. WIREs RNA 2011 2 837–852 DOI: 10.1002/wrna.95', 'corpus_id': 23512039, 'score': 1}, {'doc_id': '8830352', 'title': 'Readthrough Strategies for Therapeutic Suppression of Nonsense Mutations in Inherited Metabolic Disease', 'abstract': 'Inherited metabolic diseases (IMDs) belong to the group of rare diseases due to their low individual prevalence. Most of them are inherited in autosomal recessive fashion and represent good candidates for novel therapeutical strategies aimed at recovering partial enzyme function as they lack an effective treatment, and small levels of enzymatic activity have been shown to be associated with improved outcome and milder phenotypes. Recently, a novel therapeutic approach for genetic diseases has emerged, based on the ability of aminoglycosides and other compounds in allowing translation to proceed through a premature termination codon introduced by a nonsense mutation, which frequently constitute a significant fraction of the mutant alleles in a population. In this review we summarize the essentials of what is known as suppression therapy, the different compounds that have been identified by high-throughput screens or developed using a medicinal chemistry approach and the preclinical and clinical trials that are being conducted in general and in the field of IMDs in particular. Several IMDs have shown to be good models for evaluating readthrough compounds using patients’ cells carrying nonsense mutations, monitoring for an increase in functional recovery and/or enzyme activity. Overall, the positive results obtained indicate the feasibility of the approach for different diseases and although the levels of protein function reached are low, they may be enough to alleviate the consequences of the pathology. Nonsense suppression thus represents a potential therapy or supplementary treatment for a number of IMD patients encouraging further clinical trials with readthrough drugs with improved functionality and low toxicity.', 'corpus_id': 8830352, 'score': 1}, {'doc_id': '218582086', 'title': 'Rethinking the Optimal Duration of Immune Checkpoint Inhibitors in Non-small Cell Lung Cancer Throughout the COVID-19 Pandemic', 'abstract': ""Immune checkpoint inhibitors (ICPIs) have revolutionized the management and prognosis of fit patients with advanced non-small cell lung cancer (NSCLC). Recently, the publication of 5-year survival rates has cemented to role of ICPIs in NSCLC. An ongoing challenge is to determine the optimal treatment duration to find the balance between efficacy, toxicity and cost. From the onset of ICPI trials, different durations were used, ranging from treatment until progression or toxicity, to fixed durations of 2 years. Subsequently, exploratory analyses from a 1-year fixed duration trial failed to change practice. There are, to date, no adequately powered prospective trials addressing this important question. With today's severe acute respiratory syndrome coronavirus 2 (SARS-COV-2) pandemic, more than ever, the question resurfaces with added factors tilting the already shaky therapeutic balance. Here, we will discuss current data regarding ICPI treatment duration and incorporate this into the context of the ongoing pandemic. We conclude with a discussion of pragmatic approaches, should physicians be unable to continue standard therapy."", 'corpus_id': 218582086, 'score': 0}, {'doc_id': '214708296', 'title': 'PI3-KINASE FUSION MUTANTS AND USES THEREOF FIELD OF THE INVENTION', 'abstract': ""Polynucleotide constructs encoding growth factor indepen dent catalytically active membrane targeted PI 3-kinase mutants useful for therapeutic and research purposes are described. In addition, a method for using the polynucleotide constructs to Screen for inhibitors of PI 3-kinase, a method for making 3' phosphorylated inositol phospholipids, meth ods of reducing cell death after trauma, and methods of overcoming insulin resistance are described. Patent Application Publication May 13, 2004 US 2004/0091898A1"", 'corpus_id': 214708296, 'score': 0}, {'doc_id': '219122390', 'title': 'FDA Approves Phase III Clinical Trial of Tocilizumab for COVID-19 Pneumonia', 'abstract': 'The FDA has approved a phase III clinical trial to assess the safety and efficacy of intravenous tocilizumab (Actemra) plus standard of care in hospitalized adult patients with severe COVID-19 pneumonia.', 'corpus_id': 219122390, 'score': 0}, {'doc_id': '218892315', 'title': 'Evidence of Protective Effect of Hydroxychloroquine on COVID-19', 'abstract': 'We would like to share ideas on the report on ""Hydroxychloroquine in Patients with Rheumatic Disease Complicated by COVID-19: Clarifying Target Exposures and the Need for Clinical Trials [1].""Balevic noted that ""well-designed clinical trials that include patients with rheumatic disease are urgently needed to characterize the efficacy, safety, and target exposures for hydroxychloroquine [1].""', 'corpus_id': 218892315, 'score': 0}]"
8	{'doc_id': '3918101', 'title': 'Systematic literature reviews in software engineering - A systematic literature review', 'abstract': 'Background: In 2004 the concept of evidence-based software engineering (EBSE) was introduced at the ICSE04 conference. Aims: This study assesses the impact of systematic literature reviews (SLRs) which are the recommended EBSE method for aggregating evidence. Method: We used the standard systematic literature review method employing a manual search of 10 journals and 4 conference proceedings. Results: Of 20 relevant studies, eight addressed research trends rather than technique evaluation. Seven SLRs addressed cost estimation. The quality of SLRs was fair with only three scoring less than 2 out of 4. Conclusions: Currently, the topic areas covered by SLRs are limited. European researchers, particularly those at the Simula Laboratory appear to be the leading exponents of systematic literature reviews. The series of cost estimation SLRs demonstrate the potential value of EBSE for synthesising evidence and making it available to practitioners.', 'corpus_id': 3918101}	3460	"[{'doc_id': '39503462', 'title': 'Case Study Research in Software Engineering - Guidelines and Examples', 'abstract': 'Based on their own experiences of in-depth case studies of software projects in international corporations, in this bookthe authors present detailed practical guidelines on the preparation, conduct, design and reporting of case studies of software engineering. This is the first software engineering specific book on thecase study research method.', 'corpus_id': 39503462, 'score': 1}, {'doc_id': '212717992', 'title': 'On the Role of Software Architecture in DevOps Transformation: An Industrial Case Study', 'abstract': 'Development and Operations (DevOps), a particular type of Continuous Software Engineering, has become a popular Software System Engineering paradigm. Software architecture is critical in succeeding with DevOps. However, there is little evidence-based knowledge of how software systems are architected in the industry to enable and support DevOps. Since architectural decisions, along with their rationales and implications, are very important in the architecting process, we performed an industrial case study that has empirically identified and synthesized the key architectural decisions considered essential to DevOps transformation by two software development teams. Our study also reveals that apart from the chosen architecture style, DevOps works best with modular architectures. In addition, we found that the performance of the studied teams can improve in DevOps if operations specialists are added to the teams to perform the operations tasks that require advanced expertise. Finally, investment in testing is inevitable for the teams if they want to release software changes faster.', 'corpus_id': 212717992, 'score': 0}, {'doc_id': '15093265', 'title': 'Systematic literature reviews in software engineering - A tertiary study', 'abstract': 'Context: In a previous study, we reported on a systematic literature review (SLR), based on a manual search of 13 journals and conferences undertaken in the period 1st January 2004 to 30th June 2007. Objective: The aim of this on-going research is to provide an annotated catalogue of SLRs available to software engineering researchers and practitioners. This study updates our previous study using a broad automated search. Method: We performed a broad automated search to find SLRs published in the time period 1st January 2004 to 30th June 2008. We contrast the number, quality and source of these SLRs with SLRs found in the original study. Results: Our broad search found an additional 35 SLRs corresponding to 33 unique studies. Of these papers, 17 appeared relevant to the undergraduate educational curriculum and 12 appeared of possible interest to practitioners. The number of SLRs being published is increasing. The quality of papers in conferences and workshops has improved as more researchers use SLR guidelines. Conclusion: SLRs appear to have gone past the stage of being used solely by innovators but cannot yet be considered a main stream software engineering research methodology. They are addressing a wide range of topics but still have limitations, such as often failing to assess primary study quality.', 'corpus_id': 15093265, 'score': 1}, {'doc_id': '211010943', 'title': 'Analyzing the evolution and diversity of SBES Program Committee', 'abstract': ""The Brazilian Symposium on Software Engineering (SBES) is one of the most important Latin American Software Engineering conferences. It was first held in 1987, and in 2019 marks its 33rd edition. Over these years, many researchers have participated in SBES, attending the conference, submitting, and reviewing papers. The researchers who participate in the Program Committee (PC) and perform the reviewers' role are fundamentally important to SBES, since their evaluations (e.g., deciding whether a paper is accepted or not) have the potential of drawing what SBES is now. Knowing that diversity is an important aspect of any group work, we wanted to understand diversity in the SBES PC community. We investigated a number of characteristics of SBES PC members, including their gender and geographic location. We also analyzed the turnover and renovation of the committee. Among the findings, we observed that although the number of participants in the SBES PC has increased over the years, most of them are men (~80%) and from the Southeast and Northeast of Brazil, with very few members from the North region. We also observed that there is a small turnover: during the 2010 decade, only 11% of new members were added to the PC. Finally, we investigated the participation of the PC members publishing papers at SBES. We observed that only 24% of the papers accepted to SBES were authored by members who were not committee members of the respective year. Moreover, committee members usually do not collaborate among themselves: a significant number of the papers are authored by the PC members and students. This paper may contribute to the SBES community, in particular, its special interest group, in understanding the needs and challenges of the PC's participants."", 'corpus_id': 211010943, 'score': 0}, {'doc_id': '211082994', 'title': 'Measurement of Interpersonal Trust in Global Software Development: SLR Protocol', 'abstract': 'The purpose of this protocol is to be useful to identify, evaluate and synthesize reported knowledge about the measurement of interpersonal trust (IpT) in virtual software teams. To achieve this goal we applied a research technique known as Systematic Literature Review (SLR). The aim of a SLR is to be as objective, analytical, and repeatable as possible.', 'corpus_id': 211082994, 'score': 0}, {'doc_id': '207144526', 'title': 'Guidelines for conducting and reporting case study research in software engineering', 'abstract': 'Case study is a suitable research methodology for software engineering research since it studies contemporary phenomena in its natural context. However, the understanding of what constitutes a case study varies, and hence the quality of the resulting studies. This paper aims at providing an introduction to case study methodology and guidelines for researchers conducting case studies and readers studying reports of such studies. The content is based on the authors’ own experience from conducting and reading case studies. The terminology and guidelines are compiled from different methodology handbooks in other research domains, in particular social science and information systems, and adapted to the needs in software engineering. We present recommended practices for software engineering case studies as well as empirically derived and evaluated checklists for researchers and readers of case study research.', 'corpus_id': 207144526, 'score': 1}, {'doc_id': '214774868', 'title': 'Natural Language Processing (NLP) for Requirements Engineering: A Systematic Mapping Study', 'abstract': 'Natural language processing supported requirements engineering is an area of research and development that seeks to apply NLP techniques, tools and resources to a variety of requirements documents or artifacts to support a range of linguistic analysis tasks performed at various RE phases. Such tasks include detecting language issues, identifying key domain concepts and establishing traceability links between requirements. This article surveys the landscape of NLP4RE research to understand the state of the art and identify open problems. The systematic mapping study approach is used to conduct this survey, which identified 404 relevant primary studies and reviewed them according to five research questions, cutting across five aspects of NLP4RE research, concerning the state of the literature, the state of empirical research, the research focus, the state of the practice, and the NLP technologies used. Results: 1) NLP4RE is an active and thriving research area in RE that has amassed a large number of publications and attracted widespread attention from diverse communities; 2) most NLP4RE studies are solution proposals having only been evaluated using a laboratory experiment or an example application; 3) most studies have focused on the analysis phase, with detection as their central linguistic analysis task and requirements specification as their commonly processed document type; 4) 130 new tools have been proposed to support a range of linguistic analysis tasks, but there is little evidence of adoption in the long term, although some industrial applications have been published; 5) 140 NLP techniques, 66 NLP tools and 25 NLP resources are extracted from the selected studies.', 'corpus_id': 214774868, 'score': 0}, {'doc_id': '42173103', 'title': 'Performing systematic literature reviews in software engineering', 'abstract': 'Context: Making best use of the growing number of empirical studies in Software Engineering, for making decisions and formulating research questions, requires the ability to construct an objective summary of available research evidence. Adopting a systematic approach to assessing and aggregating the outcomes from a set of empirical studies is also particularly important in Software Engineering, given that such studies may employ very different experimental forms and be undertaken in very different experimental contexts.Objectives: To provide an introduction to the role, form and processes involved in performing Systematic Literature Reviews. After the tutorial, participants should be able to read and use such reviews, and have gained the knowledge needed to conduct systematic reviews of their own.Method: We will use a blend of information presentation (including some experiences of the problems that can arise in the Software Engineering domain), and also of interactive working, using review material prepared in advance.', 'corpus_id': 42173103, 'score': 1}, {'doc_id': '13134455', 'title': 'Guidelines for conducting systematic mapping studies in software engineering: An update', 'abstract': 'Abstract Context Systematic mapping studies are used to structure a research area, while systematic reviews are focused on gathering and synthesizing evidence. The most recent guidelines for systematic mapping are from 2008. Since that time, many suggestions have been made of how to improve systematic literature reviews (SLRs). There is a need to evaluate how researchers conduct the process of systematic mapping and identify how the guidelines should be updated based on the lessons learned from the existing systematic maps and SLR guidelines. Objective To identify how the systematic mapping process is conducted (including search, study selection, analysis and presentation of data, etc.); to identify improvement potentials in conducting the systematic mapping process and updating the guidelines accordingly. Method We conducted a systematic mapping study of systematic maps, considering some practices of systematic review guidelines as well (in particular in relation to defining the search and to conduct a quality assessment). Results In a large number of studies multiple guidelines are used and combined, which leads to different ways in conducting mapping studies. The reason for combining guidelines was that they differed in the recommendations given. Conclusion The most frequently followed guidelines are not sufficient alone. Hence, there was a need to provide an update of how to conduct systematic mapping studies. New guidelines have been proposed consolidating existing findings.', 'corpus_id': 13134455, 'score': 1}, {'doc_id': '212676032', 'title': 'Analyzing the Impact of Automated User Assistance Systems: A Systematic Review', 'abstract': 'Context: User assistance is generally defined as the guided assistance to a user of a software system in order to help accomplish tasks and enhance user experience. Automated user assistance systems are equipped with online help system that provides information to the user in an electronic format and which can be opened directly in the application. Various different automated user assistance approaches have been proposed in the literature. However, there has been no attempt to systematically review and report the impact of automated user assistance systems. Objective: The overall objective of this systematic review is to identify the state of art in automated user assistance systems, and describe the reported evidence for automated user assistance. Method: A systematic literature review is conducted by a multiphase study selection process using the published literature since 2002. Results: We reviewed 575 papers that are discovered using a well-planned review protocol, and 31 of them were assessed as primary studies related to our research questions. Conclusions: Our study shows that user assistance systems can provide important benefits for the user but still more research is required in this domain.', 'corpus_id': 212676032, 'score': 0}]"
9	{'doc_id': '232035409', 'title': 'OneStop QAMaker: Extract Question-Answer Pairs from Text in a One-Stop Approach', 'abstract': 'Large-scale question-answer (QA) pairs are critical for advancing research areas like machine reading comprehension and question answering. To construct QA pairs from documents requires determining how to ask a question and what is the corresponding answer. Existing methods for QA pair generation usually follow a pipeline approach. Namely, they first choose the most likely candidate answer span and then generate the answer-specific question. This pipeline approach, however, is undesired in mining the most appropriate QA pairs from documents since it ignores the connection between question generation and answer extraction, which may lead to incompatible QA pair generation, i.e., the selected answer span is inappropriate for question generation. However, for human annotators, we take the whole QA pair into account and consider the compatibility between question and answer. Inspired by such motivation, instead of the conventional pipeline approach, we propose a model named OneStop generate QA pairs from documents in a one-stop approach. Specifically, questions and their corresponding answer span is extracted simultaneously and the process of question generation and answer extraction mutually affect each other. Additionally, OneStop is much more efficient to be trained and deployed in industrial scenarios since it involves only one model to solve the complex QA generation task. We conduct comprehensive experiments on three large-scale machine reading comprehension datasets: SQuAD, NewsQA, and DuReader. The experimental results demonstrate that our OneStop model outperforms the baselines significantly regarding the quality of generated questions, quality of generated question-answer pairs, and model efficiency.', 'corpus_id': 232035409}	15242	"[{'doc_id': '232404901', 'title': '‘Just because you are right, doesn’t mean I am wrong’: Overcoming a bottleneck in development and evaluation of Open-Ended VQA tasks', 'abstract': 'GQA (CITATION) is a dataset for real-world visual reasoning and compositional question answering. We found that many answers predicted by the best vision-language models on the GQA dataset do not match the ground-truth answer but still are semantically meaningful and correct in the given context. In fact, this is the case with most existing visual question answering (VQA) datasets where they assume only one ground-truth answer for each question. We propose Alternative Answer Sets (AAS) of ground-truth answers to address this limitation, which is created automatically using off-the-shelf NLP tools. We introduce a semantic metric based on AAS and modify top VQA solvers to support multiple plausible answers for a question. We implement this approach on the GQA dataset and show the performance improvements.', 'corpus_id': 232404901, 'score': 0}, {'doc_id': '232185221', 'title': 'Conversational Answer Generation and Factuality for Reading Comprehension Question-Answering', 'abstract': 'Question answering (QA) is an important use case on voice assistants. A popular approach to QA is extractive reading comprehension (RC) which finds an answer span in a text passage. However, extractive answers are often unnatural in a conversational context which results in suboptimal user experience. In this work, we investigate conversational answer generation for QA. We propose AnswerBART, an end-to-end generative RC model which combines answer generation from multiple passages with passage ranking and answerability. Moreover, a hurdle in applying generative RC are hallucinations where the answer is factually inconsistent with the passage text. We leverage recent work from summarization to evaluate factuality. Experiments show that AnswerBART significantly improves over previous best published results on MS MARCO 2.1 NLGEN by 2.5 ROUGE-L and NarrativeQA by 9.4 ROUGE-L.', 'corpus_id': 232185221, 'score': 1}, {'doc_id': '232075995', 'title': 'Learning Reasoning Paths over Semantic Graphs for Video-grounded Dialogues', 'abstract': 'Compared to traditional visual question answering, video-grounded dialogues require additional reasoning over dialogue context to answer questions in a multiturn setting. Previous approaches to video-grounded dialogues mostly use dialogue context as a simple text input without modelling the inherent information flows at the turn level. In this paper, we propose a novel framework of Reasoning Paths in Dialogue Context (PDC). PDC model discovers information flows among dialogue turns through a semantic graph constructed based on lexical components in each question and answer. PDC model then learns to predict reasoning paths over this semantic graph. Our path prediction model predicts a path from the current turn through past dialogue turns that contain additional visual cues to answer the current question. Our reasoning model sequentially processes both visual and textual information through this reasoning path and the propagated features are used to generate the answer. Our experimental results demonstrate the effectiveness of our method and provide additional insights on how models use semantic dependencies in a dialogue context to retrieve visual cues.', 'corpus_id': 232075995, 'score': 0}, {'doc_id': '231855322', 'title': 'Efficient Retrieval Augmented Generation from Unstructured Knowledge for Task-Oriented Dialog', 'abstract': 'This paper summarizes our work on the first track of the ninth Dialog System Technology Challenge (DSTC 9), “Beyond Domain APIs: Task-oriented Conversational Modeling with Unstructured Knowledge Access”. The goal of the task is to generate responses to user turns in a task-oriented dialog that require knowledge from unstructured documents. The task is divided into three subtasks: detection, selection and generation. In order to be compute efficient, we formulate the selection problem in terms of hierarchical classification steps. We achieve our best results with this model. Alternatively, we employ siamese sequence embedding models, referred to as Dense Knowledge Retrieval, to retrieve relevant documents. This method further reduces the computation time by a factor of more than 100x at the cost of degradation in R@1 of 5-6% compared to the first model. Then for either approach, we use Retrieval Augmented Generation to generate responses based on multiple selected snippets and we show how the method can be used to fine-tune trained embeddings.', 'corpus_id': 231855322, 'score': 0}, {'doc_id': '232168895', 'title': 'Select, Substitute, Search: A New Benchmark for Knowledge-Augmented Visual Question Answering', 'abstract': ""Multimodal IR, spanning text corpus, knowledge graph and images, called outside knowledge visual question answering (OKVQA), is of much recent interest. However, the popular data set has serious limitations. A surprisingly large fraction of queries do not assess the ability to integrate cross-modal information. Instead, some are independent of the image, some depend on speculation, some require OCR or are otherwise answerable from the image alone. To add to the above limitations, frequency-based guessing is very effective because of (unintended) widespread answer overlaps between the train and test folds. Overall, it is hard to determine when state-of-the-art systems exploit these weaknesses rather than really infer the answers, because they are opaque and their 'reasoning' process is uninterpretable. An equally important limitation is that the dataset is designed for the quantitative assessment only of the end-to-end answer retrieval task, with no provision for assessing the correct(semantic) interpretation of the input query. In response, we identify a key structural idiom in OKVQA ,viz., S3 (select, substitute and search), and build a new data set and challenge around it. Specifically, the questioner identifies an entity in the image and asks a question involving that entity which can be answered only by consulting a knowledge graph or corpus passage mentioning the entity. Our challenge consists of (i)OKVQA_S3, a subset of OKVQA annotated based on the structural idiom and (ii)S3VQA, a new dataset built from scratch. We also present a neural but structurally transparent OKVQA system, S3, that explicitly addresses our challenge dataset, and outperforms recent competitive baselines. We make our code and data available at https://s3vqa.github.io/."", 'corpus_id': 232168895, 'score': 0}, {'doc_id': '232240655', 'title': 'Robustly Optimized and Distilled Training for Natural Language Understanding', 'abstract': 'In this paper, we explore multi-task learning (MTL) as a second pretraining step to learn enhanced universal language representation for transformer language models. We use the MTL enhanced representation across several natural language understanding tasks to improve performance and generalization. Moreover, we incorporate knowledge distillation (KD) in MTL to further boost performance and devise a KD variant that learns effectively from multiple teachers. By combining MTL and KD, we propose Robustly Optimized and Distilled (ROaD) modeling framework. We use ROaD together with the ELECTRA model to obtain state-of-the-art results for machine reading comprehension and natural language inference.', 'corpus_id': 232240655, 'score': 1}, {'doc_id': '231993458', 'title': 'Factoid and Open-Ended Question Answering with BERT in the Museum Domain', 'abstract': 'Most question answering tasks are oriented towards open domain factoid questions. In comparison, much less work has studied both factoid and open ended questions in closed domains. We have chosen a current state-of-art BERT model for our question answering experiment, and investigate the effectiveness of the BERT model for both factoid and open-ended questions in the museum domain, in a realistic setting. We conducted a web based experiment where we collected 285 questions relating to museum pictures. We manually determined the answers from the description texts of the pictures and classified them into answerable/un-answerable and factoid/open-ended. We passed the questions through a BERT model and evaluated their performance with our created dataset. Matching our expectations, BERT performed better for factoid questions, while it was only able to answer 36% of the open-ended questions. Further analysis showed that questions that can be answered from a single sentence or two are easier for the BERT model. We have also found that the individual picture and description text have some implications for the performance of the BERT model. Finally, we propose how to overcome the current limitations of out of the box question answering solutions in realistic settings and point out important factors for designing the context for getting a better question answering model using BERT.', 'corpus_id': 231993458, 'score': 1}, {'doc_id': '221516475', 'title': 'Measuring Massive Multitask Language Understanding', 'abstract': ""We propose a new test to measure a text model's multitask accuracy. The test covers 57 tasks including elementary mathematics, US history, computer science, law, and more. To attain high accuracy on this test, models must possess extensive world knowledge and problem solving ability. We find that while most recent models have near random-chance accuracy, the very largest GPT-3 model improves over random chance by almost 20 percentage points on average. However, on every one of the 57 tasks, the best models still need substantial improvements before they can reach expert-level accuracy. Models also have lopsided performance and frequently do not know when they are wrong. Worse, they still have near-random accuracy on some socially important subjects such as morality and law. By comprehensively evaluating the breadth and depth of a model's academic and professional understanding, our test can be used to analyze models across many tasks and to identify important shortcomings."", 'corpus_id': 221516475, 'score': 1}]"
10	{'doc_id': '214585479', 'title': 'YewPar: Skeletons for Exact Combinatorial Search (Operational Semantics Supplement)', 'abstract': 'This note presents an expanded version of Section 3 of the paper YewPar: Skeletons for Exact Combinatorial Search [2]. It is intended to supplement that paper with a more detailed account of the operational semantics, including detailed proof sketches and examples that were omitted for lack of space. 3 Formalising Parallel Tree Search The dynamic behaviour of the YewPar framework is modeled by a novel operational semantics formalising multi-threaded backtracking search. The semantics presented here generalises the semantics published in Archibald’s thesis [1] by classifying search types in terms of the monoids they use to accumulate information, and by providing correctness proofs. Section 3.1 specifies search tree generation and traversal. Section 3.2 introduces the monoids used to accumulate information in enumeration, optimisation and decision searches, and Sections 3.3ff. define a parallel operational semantics for multi-threaded backtracking search, including pruning (Section 3.5) andwork generation (Section 3.6). Section 3.7 proves correctness of the semantics, i.e. search always terminates and returns a correct/optimal result.', 'corpus_id': 214585479}	1173	"[{'doc_id': '211258593', 'title': 'Adaptive Propagation Graph Convolutional Network', 'abstract': 'Graph convolutional networks (GCNs) are a family of neural network models that perform inference on graph data by interleaving vertexwise operations and message-passing exchanges across nodes. Concerning the latter, two key questions arise: 1) how to design a differentiable exchange protocol (e.g., a one-hop Laplacian smoothing in the original GCN) and 2) how to characterize the tradeoff in complexity with respect to the local updates. In this brief, we show that the state-of-the-art results can be achieved by adapting the number of communication steps independently at every node. In particular, we endow each node with a halting unit (inspired by Graves’ adaptive computation time [1]) that after every exchange decides whether to continue communicating or not. We show that the proposed adaptive propagation GCN (AP-GCN) achieves superior or similar results to the best proposed models so far on a number of benchmarks while requiring a small overhead in terms of additional parameters. We also investigate a regularization term to enforce an explicit tradeoff between communication and accuracy. The code for the AP-GCN experiments is released as an open-source library.', 'corpus_id': 211258593, 'score': 1}, {'doc_id': '222208619', 'title': 'Reward-Biased Maximum Likelihood Estimation for Linear Stochastic Bandits', 'abstract': 'Modifying the reward-biased maximum likelihood method originally proposed in the adaptive control literature, we propose novel learning algorithms to handle the explore-exploit trade-off in linear bandits problems as well as generalized linear bandits problems. We develop novel index policies that we prove achieve order-optimality, and show that they achieve empirical performance competitive with the state-of-the-art benchmark methods in extensive experiments. The new policies achieve this with low computation time per pull for linear bandits, and thereby resulting in both favorable regret as well as computational efficiency.', 'corpus_id': 222208619, 'score': 0}, {'doc_id': '221340812', 'title': 'The Advantage Regret-Matching Actor-Critic', 'abstract': ""Regret minimization has played a key role in online learning, equilibrium computation in games, and reinforcement learning (RL). In this paper, we describe a general model-free RL method for no-regret learning based on repeated reconsideration of past behavior. We propose a model-free RL algorithm, the AdvantageRegret-Matching Actor-Critic (ARMAC): rather than saving past state-action data, ARMAC saves a buffer of past policies, replaying through them to reconstruct hindsight assessments of past behavior. These retrospective value estimates are used to predict conditional advantages which, combined with regret matching, produces a new policy. In particular, ARMAC learns from sampled trajectories in a centralized training setting, without requiring the application of importance sampling commonly used in Monte Carlo counterfactual regret (CFR) minimization; hence, it does not suffer from excessive variance in large environments. In the single-agent setting, ARMAC shows an interesting form of exploration by keeping past policies intact. In the multiagent setting, ARMAC in self-play approaches Nash equilibria on some partially-observable zero-sum benchmarks. We provide exploitability estimates in the significantly larger game of betting-abstracted no-limit Texas Hold'em."", 'corpus_id': 221340812, 'score': 0}, {'doc_id': '221069787', 'title': 'Improved Regret Bounds Agnostic GP Bandits Improved Regret Bounds for Agnostic Gaussian Process Bandits using Local Polynomial Estimators', 'abstract': 'We consider the problem of optimizing a black-box function f : X 7→ R under the assumption that it has bounded norm in the Reproducing Kernel Hilbert Space (RKHS) associated with a given kernel K. This problem is known to have an agnostic Gaussian Process (GP) bandit interpretation in which an appropriately constructed GP surrogate model with kernel K is used to obtain an upper confidence bound (UCB) algorithm. In this paper, we propose a new algorithm (LP-GP-UCB) where the usual GP surrogate model is augmented with Local Polynomial (LP) estimators to construct a multi-scale upper confidence bound guiding the search for the optimizer. We analyze this algorithm and derive high probability bounds on its simple and cumulative regret for a practically relevant class of kernels called the Matérn family (Kν)ν>0. We show that for certain ranges of ν, the algorithm achieves near-optimal bounds on simple and cumulative regrets, matching the algorithm-independent lower bounds up to poly-logarithmic factors, and thus closing the large gap between the existing upper and lower bounds for these values of ν.', 'corpus_id': 221069787, 'score': 1}, {'doc_id': '209314783', 'title': 'A bi-diffusion based layer-wise sampling method for deep learning in large graphs', 'abstract': 'The Graph Convolutional Network (GCN) and its variants are powerful models for graph representation learning and have recently achieved great success on many graph-based applications. However, most of them target on shallow models (e.g. 2 layers) on relatively small graphs. Very recently, although many acceleration methods have been developed for GCNs training, it still remains a severe challenge how to scale GCN-like models to larger graphs and deeper layers due to the over-expansion of neighborhoods across layers. In this paper, to address the above challenge, we propose a novel layer-wise sampling strategy, which samples the nodes layer by layer conditionally based on the factors of the bi-directional diffusion between layers. In this way, we potentially restrict the time complexity linear to the number of layers, and construct a mini-batch of nodes with high local bi-directional influence (correlation). Further, we apply the self-attention mechanism to flexibly learn suitable weights for the sampled nodes, which allows the model to be able to incorporate both the first-order and higher-order proximities during a single layer propagation process without extra recursive propagation or skip connection. Extensive experiments on three large benchmark graphs demonstrate the effectiveness and efficiency of the proposed model.', 'corpus_id': 209314783, 'score': 1}, {'doc_id': '204801263', 'title': 'Momentum in Reinforcement Learning', 'abstract': ""We adapt the optimization's concept of momentum to reinforcement learning. Seeing the state-action value functions as an analog to the gradients in optimization, we interpret momentum as an average of consecutive $q$-functions. We derive Momentum Value Iteration (MoVI), a variation of Value Iteration that incorporates this momentum idea. Our analysis shows that this allows MoVI to average errors over successive iterations. We show that the proposed approach can be readily extended to deep learning. Specifically, we propose a simple improvement on DQN based on MoVI, and experiment it on Atari games."", 'corpus_id': 204801263, 'score': 1}, {'doc_id': '222124941', 'title': 'Neural Thompson Sampling', 'abstract': 'Thompson Sampling (TS) is one of the most effective algorithms for solving contextual multi-armed bandit problems. In this paper, we propose a new algorithm, called Neural Thompson Sampling, which adapts deep neural networks for both exploration and exploitation. At the core of our algorithm is a novel posterior distribution of the reward, where its mean is the neural network approximator, and its variance is built upon the neural tangent features of the corresponding neural network. We prove that, provided the underlying reward function is bounded, the proposed algorithm is guaranteed to achieve a cumulative regret of $\\mathcal{O}(T^{1/2})$, which matches the regret of other contextual bandit algorithms in terms of total round number $T$. Experimental comparisons with other benchmark bandit algorithms on various data sets corroborate our theory.', 'corpus_id': 222124941, 'score': 0}, {'doc_id': '212414268', 'title': 'Neural Contextual Bandits with UCB-based Exploration', 'abstract': 'We study the stochastic contextual bandit problem, where the reward is generated from an unknown function with additive noise. No assumption is made about the reward function other than boundedness. We propose a new algorithm, NeuralUCB, which leverages the representation power of deep neural networks and uses a neural network-based random feature mapping to construct an upper confidence bound (UCB) of reward for efficient exploration. We prove that, under standard assumptions, NeuralUCB achieves $\\tilde O(\\sqrt{T})$ regret, where $T$ is the number of rounds. To the best of our knowledge, it is the first neural network-based contextual bandit algorithm with a near-optimal regret guarantee. We also show the algorithm is empirically competitive against representative baselines in a number of benchmarks.', 'corpus_id': 212414268, 'score': 1}]"
11	{'doc_id': '211727170', 'title': 'COVID-19 in 2 Persons with Mild Upper Respiratory Tract Symptoms on a Cruise Ship, Japan', 'abstract': 'We describe 2 cases of coronavirus disease in patients with mild upper respiratory symptoms. Both patients worked on a cruise ship quarantined off the coast of Japan. One patient had persistent, low-grade upper respiratory tract symptoms without fever. The other patient had rapid symptom cessation but persistent viral RNA detection.', 'corpus_id': 211727170}	1089	"[{'doc_id': '214611564', 'title': 'Deep Bayesian Gaussian Processes for Uncertainty Estimation in Electronic Health Records', 'abstract': 'One major impediment to the wider use of deep learning for clinical decision making is the difficulty of assigning a level of confidence to model predictions. Currently, deep Bayesian neural networks and sparse Gaussian processes are the main two scalable uncertainty estimation methods. However, deep Bayesian neural networks suffer from lack of expressiveness, and more expressive models such as deep kernel learning, which is an extension of sparse Gaussian process, captures only the uncertainty from the higher-level latent space. Therefore, the deep learning model under it lacks interpretability and ignores uncertainty from the raw data. In this paper, we merge features of the deep Bayesian learning framework with deep kernel learning to leverage the strengths of both methods for a more comprehensive uncertainty estimation. Through a series of experiments on predicting the first incidence of heart failure, diabetes and depression applied to large-scale electronic medical records, we demonstrate that our method is better at capturing uncertainty than both Gaussian processes and deep Bayesian neural networks in terms of indicating data insufficiency and identifying misclassifications, with a comparable generalization performance. Furthermore, by assessing the accuracy and area under the receiver operating characteristic curve over the predictive probability, we show that our method is less susceptible to making overconfident predictions, especially for the minority class in imbalanced datasets. Finally, we demonstrate how uncertainty information derived by the model can inform risk factor analysis towards model interpretability.', 'corpus_id': 214611564, 'score': 0}, {'doc_id': '214611552', 'title': 'How to model honeybee population dynamics: stage structure and seasonality', 'abstract': 'Western honeybees (Apis Mellifera) serve extremely important roles in our ecosystem and economics as\xa0 they are responsible for pollinating $ 215 billion dollars annually over the world.\xa0 Unfortunately,\xa0 honeybee population and their colonies have been declined dramatically. The purpose of this article is to explore how we should model honeybee population with age structure and validate the model using empirical data so that we can identify different factors that lead to the survival and healthy of the honeybee colony.\xa0 Our theoretical study combined with simulations and data validation suggests that the proper age structure incorporated in the model\xa0 and seasonality are important for modeling honeybee population.\xa0 Specifically, our work implies that the model assuming that (1) the adult bees are survived from the egg population rather than the brood population; and (2) seasonality in the queen egg laying rate, give the better fit than other honeybee models. The related theoretical and numerical analysis of the most fit model indicate that (a) the survival of honeybee colonies requires a large queen egg-laying rate and smaller values of the other life history parameter values in addition to proper initial condition; (b) both brood and adult bee populations are increasing with respect to the increase in the egg-laying rate and the decreasing in other parameter values; and (c) seasonality may promote/suppress the survival of the honeybee colony.\xa0', 'corpus_id': 214611552, 'score': 0}, {'doc_id': '213193072', 'title': '[Treatment strategies for colorectal cancer patients in tumor hospitals under the background of corona virus disease 2019].', 'abstract': 'In December 2019, a new outbreak of corona virus disease 2019 began to occur. Its pathogen is 2019-nCoV, which has the characteristics of strong infectivity and general susceptibility. The current situation of prevention and control of new coronavirus pneumonia is severe. In this context, as front-line medical workers bearing important responsibilities and pressure, while through strict management strategy, we can minimize the risk of infection exposure. By summarizing the research progress and guidelines in recent years in the fields of colorectal cancer disease screening, treatment strategies (including early colorectal cancer, locally advanced colorectal cancer, obstructive colorectal cancer, metastatic colorectal cancer and the treatment of patients after neoadjuvant therapy), the choice of medication and time limit for adjuvant therapy, the protective measures for patients undergoing emergency surgery, the re-examination of postoperative patients and the protection of medical staff, etc., authors improve treatment strategies in order to provide more choices for patients to obtain the best treatment under the severe epidemic situation of new coronavirus pneumonia. Meanwhile we hope that it can also provide more timely treatment modeling schemes for colleagues.', 'corpus_id': 213193072, 'score': 0}, {'doc_id': '214796609', 'title': 'Co-infection of Coronavirus Disease 2019 and Influenza: A Report from Iran', 'abstract': 'BACKGROUND: In late December 2019, a viral pneumonia known as coronavirus disease 2019 (COVID-19) originated from China and spread very rapidly in the world. Since then, COVID-19 has become a global concern and health problem. METHODS: We present four patients in this study, selected from among patients who presented with pneumonia symptoms and were suspicious for COVID-19. They were referred to the intended centers for COVID-19 diagnosis and management of Shiraz University of Medical Sciences in southern Iran. Two nasopharyngeal and oropharyngeal throat swab samples were collected from each patient and tested for severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) using real-time reverse-transcriptase- polymerase-chain-reaction (RT-PCR). The samples were also tested for influenza viruses and the complete respiratory panel. RESULTS: In the present report, four patients were diagnosed in the starting days of COVID-19 disease in our center in southern Iran with co-infection of SARS-CoV-2 and influenza A virus. CONCLUSION: This co-infection of COVID-19 and influenza A highlights the importance of considering SARS-CoV-2 PCR assay regardless of other positive findings for other pathogens in the primary test during the epidemic.', 'corpus_id': 214796609, 'score': 1}, {'doc_id': '215405377', 'title': ""Reply to Comments on 'Co‐infection of SARS‐CoV‐2 and HIV in a patient in Wuhan city, China’"", 'abstract': 'Abstract Coronovirus disease 2019 (COVID‐19) caused by severe acute respiratory syndrome coronovirus 2 (SARS‐CoV‐2) infection has been ranging in China throughout the world now. The World Health Organiztion declared the COVID‐19 crisis a global pandemic. The previous our case of co‐infection of SARS‐CoV‐2 and HIV has raised the interests of the HIV research community1. This article is protected by copyright. All rights reserved.', 'corpus_id': 215405377, 'score': 1}, {'doc_id': '212651602', 'title': 'Detection of Novel Coronavirus by RT-PCR in Stool Specimen from Asymptomatic Child, China', 'abstract': 'We report an asymptomatic child who was positive for a coronavirus by reverse transcription PCR in a stool specimen 17 days after the last virus exposure. The child was virus positive in stool specimens for at least an additional 9 days. Respiratory tract specimens were negative by reverse transcription PCR.', 'corpus_id': 212651602, 'score': 1}, {'doc_id': '214590831', 'title': '[Several suggestions of operation for colorectal cancer under the outbreak of corona virus disease 2019 in China].', 'abstract': 'Pneumonia caused by 2019-nCoV infection has been reported in Wuhan since December 2019, and spread rapidly across the country. The radical operation of colorectal cancer is semi-elective operation. Patients with colorectal cancer should receive operation as soon as possible after elective operation is resumed in each hospital. 2019-nCoV virus can be transmitted by asymptomatic infectors, and it has been confirmed to be transmitted by droplets and contact. However, fecal-oral transmission and aerosol transmission have not been excluded. Based on our experience with laparoscopic colorectal operation, we propose some surgery strategies for colorectal cancer patients under the corona virus disease 2019(COVID-19) situation: the screening process should be strictly carried out before surgery to reduce the risk of nosocomial infection in the later stage; laparoscopic-assisted surgery is recommended for radical surgery for patients with colorectal cancer; strict aerosol management must be made during the operation; natural orifice specimen extraction surgery and transanal total mesorectal excision are should be performed prudently; scientific and reasonable prophylactic stoma should be done; personnel protection in surgical ward and operation room must be strengthened.', 'corpus_id': 214590831, 'score': 0}, {'doc_id': '214600719', 'title': 'Profiling Early Humoral Response to Diagnose Novel Coronavirus Disease (COVID-19)', 'abstract': 'Abstract Background The emergence of coronavirus disease 2019 (COVID-19) is a major healthcare threat. The current method of detection involves a quantitative polymerase chain reaction (qPCR)–based technique, which identifies the viral nucleic acids when present in sufficient quantity. False-negative results can be achieved and failure to quarantine the infected patient would be a major setback in containing the viral transmission. We aim to describe the time kinetics of various antibodies produced against the 2019 novel coronavirus (SARS-CoV-2) and evaluate the potential of antibody testing to diagnose COVID-19. Methods The host humoral response against SARS-CoV-2, including IgA, IgM, and IgG response, was examined by using an ELISA-based assay on the recombinant viral nucleocapsid protein. 208 plasma samples were collected from 82 confirmed and 58 probable cases (qPCR negative but with typical manifestation). The diagnostic value of IgM was evaluated in this cohort. Results The median duration of IgM and IgA antibody detection was 5 (IQR, 3–6) days, while IgG was detected 14 (IQR, 10–18) days after symptom onset, with a positive rate of 85.4%, 92.7%, and 77.9%, respectively. In confirmed and probable cases, the positive rates of IgM antibodies were 75.6% and 93.1%, respectively. The detection efficiency by IgM ELISA is higher than that of qPCR after 5.5 days of symptom onset. The positive detection rate is significantly increased (98.6%) when combining IgM ELISA assay with PCR for each patient compared with a single qPCR test (51.9%). Conclusions The humoral response to SARS-CoV-2 can aid in the diagnosis of COVID-19, including subclinical cases.', 'corpus_id': 214600719, 'score': 1}, {'doc_id': '214611822', 'title': 'Design Multimedia Expert Diagnosing Diseases System Using Fuzzy Logic (MEDDSFL)', 'abstract': ""In this paper we designed an efficient expert system to diagnose diseases for human beings. The system depended on several clinical features for different diseases which will be used as knowledge base for this system. We used fuzzy logic system which is one of the most expert systems techniques that used in building knowledge base of expert systems. Fuzzy logic will be used to inference the results of disease diagnosing. We also provided the system with multimedia such as videos, pictures and information for most of disease that have been achieved in our system. The system implemented using Matlab ToolBox and fifteen diseases were studied. Five cases for normal, affected and unaffected people's different diseases have been tested on this system. The results show that system was able to predict the status whether a human has a disease or not accurately. All system results are reported in tables and discussed in detail."", 'corpus_id': 214611822, 'score': 0}, {'doc_id': '214698372', 'title': 'Coronavirus infections: Epidemiological, clinical and immunological features and hypotheses', 'abstract': 'Coronaviruses (CoVs) are a large family of enveloped, positive-strand RNA viruses. Four human CoVs (HCoVs), the non-severe acute respiratory syndrome (SARS)-like HCoVs (namely HCoV 229E, NL63, OC43, and HKU1), are globally endemic and account for a substantial fraction of upper respiratory tract infections. Non-SARS-like CoV can occasionally produce severe diseases in frail subjects but do not cause any major (fatal) epidemics. In contrast, SARS like CoVs (namely SARS-CoV and Middle-East respiratory syndrome coronavirus, MERS-CoV) can cause intense short-lived fatal outbreaks. The current epidemic caused by the highly contagious SARS-CoV-2 and its rapid spread globally is of major concern. There is scanty knowledge on the actual pandemic potential of this new SARS-like virus. It might be speculated that SARS-CoV-2 epidemic is grossly underdiagnosed and that the infection is silently spreading across the globe with two consequences: (i) clusters of severe infections among frail subjects could haphazardly occur linked to unrecognized index cases; (ii) the current epidemic could naturally fall into a low-level endemic phase when a significant number of subjects will have developed immunity. Understanding the role of paucisymptomatic subjects and stratifying patients according to the risk of developing severe clinical presentations is pivotal for implementing reasonable measures to contain the infection and to reduce its mortality. Whilst the future evolution of this epidemic remains unpredictable, classic public health strategies must follow rational patterns. The emergence of yet another global epidemic underscores the permanent challenges that infectious diseases pose and underscores the need for global cooperation and preparedness, even during inter-epidemic periods.', 'corpus_id': 214698372, 'score': 1}]"
12	{'doc_id': '214616751', 'title': 'Systematic review of COVID‐19 in children shows milder cases and a better prognosis than adults', 'abstract': 'The coronavirus disease 2019 (COVID‐19) pandemic has affected hundreds of thousands of people. Data on symptoms and prognosis in children are rare.', 'corpus_id': 214616751}	1089	"[{'doc_id': '214792057', 'title': 'Timely Review PREGNANCY AND COVID-19: A BRIEF REVIEW', 'abstract': 'COVID-19 is a disease caused by the novel coronavirus (CoV) that can result in a severe acute respiratory syndrome. Pregnant women have a complex modification of the immune system that varies through pregnancy and extends to the postpartum period. Change in the balance toward antiinflammatory pathways occurs, although some cellular mechanisms of immunity could be enhanced. Overall, pregnant women tend to have more severe viral illnesses as compared with the general population as reported in influenza, varicella, herpes simplex, and Ebola infections. Previous outbreaks of coronavirus diseases, SARS-CoV, and MERS-CoV demonstrated rather a small number of reported cases, mixed outcomes for mothers and neonates, and the absence of vertical transmission. COVID-19 appears to follow a similar pattern; however, it is more contagious, and additional data will likely emerge soon. Due to immunological and physiological changes in pregnancy, more severe outcomes for other viral infections, and presently scarce data on COVID-19, the reasonable approach is to consider pregnant women an at-risk population.', 'corpus_id': 214792057, 'score': 1}, {'doc_id': '214617043', 'title': 'Urology practice during COVID-19 pandemic.', 'abstract': 'The severe acute respiratory syndrome coronavirus 2 and the disease it causes, coronavirus disease 2019 (COVID-19) is generating a rapid and tragic health emergency in Italy due to the need to provide assistance to an overwhelming number of infected patients and, at the same time, treat all the non-deferrable oncological and benign conditions. A panel of Italian urologists has agreed on possible strategies for the reorganization of urological routine practice and on a set of recommendations that should facilitate the process of rescheduling both surgical and outpatient activities during the COVID-19 pandemic and in the subsequent phases. This document could be a valid tool to be used in routine clinical practice and, possibly, a cornerstone for further discussion on the topic also considering the further evolution of the COVID-19 pandemic. It also may provide useful recommendations for national and international urological societies in a condition of emergency.', 'corpus_id': 214617043, 'score': 0}, {'doc_id': '214698372', 'title': 'Coronavirus infections: Epidemiological, clinical and immunological features and hypotheses', 'abstract': 'Coronaviruses (CoVs) are a large family of enveloped, positive-strand RNA viruses. Four human CoVs (HCoVs), the non-severe acute respiratory syndrome (SARS)-like HCoVs (namely HCoV 229E, NL63, OC43, and HKU1), are globally endemic and account for a substantial fraction of upper respiratory tract infections. Non-SARS-like CoV can occasionally produce severe diseases in frail subjects but do not cause any major (fatal) epidemics. In contrast, SARS like CoVs (namely SARS-CoV and Middle-East respiratory syndrome coronavirus, MERS-CoV) can cause intense short-lived fatal outbreaks. The current epidemic caused by the highly contagious SARS-CoV-2 and its rapid spread globally is of major concern. There is scanty knowledge on the actual pandemic potential of this new SARS-like virus. It might be speculated that SARS-CoV-2 epidemic is grossly underdiagnosed and that the infection is silently spreading across the globe with two consequences: (i) clusters of severe infections among frail subjects could haphazardly occur linked to unrecognized index cases; (ii) the current epidemic could naturally fall into a low-level endemic phase when a significant number of subjects will have developed immunity. Understanding the role of paucisymptomatic subjects and stratifying patients according to the risk of developing severe clinical presentations is pivotal for implementing reasonable measures to contain the infection and to reduce its mortality. Whilst the future evolution of this epidemic remains unpredictable, classic public health strategies must follow rational patterns. The emergence of yet another global epidemic underscores the permanent challenges that infectious diseases pose and underscores the need for global cooperation and preparedness, even during inter-epidemic periods.', 'corpus_id': 214698372, 'score': 1}, {'doc_id': '214784496', 'title': 'Updating the diagnostic criteria of COVID-19 “suspected case” and “confirmed case” is necessary', 'abstract': 'On 6 February 2020, our team had published a rapid advice guideline for diagnosis and treatment of 2019 novel coronavirus (2019-nCoV) infection, and this guideline provided our experience and make well reference for fighting against this pandemic worldwide. However, the coronavirus disease 2019 (COVID-19) is a new disease, our awareness and knowledge are gradually increasing based on the ongoing research findings and clinical practice experience; hence, the strategies of diagnosis and treatment are also continually updated. In this letter, we answered one comment on our guideline and provided the newest diagnostic criteria of “suspected case” and “confirmed case” according to the latest Diagnosis and Treatment Guidelines for COVID-19 (seventh version) that issued by the National Health Committee of the People’s Republic of China.', 'corpus_id': 214784496, 'score': 1}, {'doc_id': '212730137', 'title': 'Equine Coronavirus-Associated Colitis in Horses: A Retrospective Study', 'abstract': '\n Abstract\n \n Equine coronavirus (ECoV) is a known cause of fever, anorexia, and lethargy in adult horses. Although there are multiple reports of ECoV outbreaks, less is known about the clinical presentation of individual horses during a nonoutbreak situation. The purpose of this study was to describe the clinical presentation of horses diagnosed with ECoV infection that were not associated with an outbreak. Medical records of all horses admitted to Washington State University, Veterinary Teaching Hospital, during an 8-year period were reviewed (2010–2018). The five horses included in this study were older than 1 year of age, were diagnosed with colitis, tested positive for ECoV using real-time polymerase chain reaction, and were negative to other enteric pathogens. Interestingly, 4 of 5 horses had moderate to severe diarrhea,\xa03 had abnormal large colon ultrasonography, 2 had transient ventricular tachycardia and 2 had clinicopathologic evidence of liver dysfunction. ECoV should be included as a differential diagnosis for individual horses presenting with anorexia, fever, lethargy, and colitis. Early identification of ECoV cases is key to implement appropriate biosecurity measures to prevent the potential spread of this disease.\n \n', 'corpus_id': 212730137, 'score': 0}, {'doc_id': '214611552', 'title': 'How to model honeybee population dynamics: stage structure and seasonality', 'abstract': 'Western honeybees (Apis Mellifera) serve extremely important roles in our ecosystem and economics as\xa0 they are responsible for pollinating $ 215 billion dollars annually over the world.\xa0 Unfortunately,\xa0 honeybee population and their colonies have been declined dramatically. The purpose of this article is to explore how we should model honeybee population with age structure and validate the model using empirical data so that we can identify different factors that lead to the survival and healthy of the honeybee colony.\xa0 Our theoretical study combined with simulations and data validation suggests that the proper age structure incorporated in the model\xa0 and seasonality are important for modeling honeybee population.\xa0 Specifically, our work implies that the model assuming that (1) the adult bees are survived from the egg population rather than the brood population; and (2) seasonality in the queen egg laying rate, give the better fit than other honeybee models. The related theoretical and numerical analysis of the most fit model indicate that (a) the survival of honeybee colonies requires a large queen egg-laying rate and smaller values of the other life history parameter values in addition to proper initial condition; (b) both brood and adult bee populations are increasing with respect to the increase in the egg-laying rate and the decreasing in other parameter values; and (c) seasonality may promote/suppress the survival of the honeybee colony.\xa0', 'corpus_id': 214611552, 'score': 0}, {'doc_id': '214791294', 'title': 'Corona Viral Epidemic: A Review', 'abstract': 'Viruses are both living and non-living organisms; living when found inside the host cell as they proliferate and grow, whereas, on occasions when they do not find any host cell, they are inactive or in a dormant stage. The virus can replicate itself and their genetic material mostly consists of either DNA or RNA. Different viral species spread in different ways. For example, plant virus spreads from plant to plant by insects and other organisms, some viruses are spread by body fluids of humans: influenza virus spread by aerosol droplets when people sneeze or cough. HIV is transmitted through body fluids; Dengue is propagated by mosquito bite. SARS virus is a respiratory disease that is spread by animals and humans.', 'corpus_id': 214791294, 'score': 1}, {'doc_id': '213193072', 'title': '[Treatment strategies for colorectal cancer patients in tumor hospitals under the background of corona virus disease 2019].', 'abstract': 'In December 2019, a new outbreak of corona virus disease 2019 began to occur. Its pathogen is 2019-nCoV, which has the characteristics of strong infectivity and general susceptibility. The current situation of prevention and control of new coronavirus pneumonia is severe. In this context, as front-line medical workers bearing important responsibilities and pressure, while through strict management strategy, we can minimize the risk of infection exposure. By summarizing the research progress and guidelines in recent years in the fields of colorectal cancer disease screening, treatment strategies (including early colorectal cancer, locally advanced colorectal cancer, obstructive colorectal cancer, metastatic colorectal cancer and the treatment of patients after neoadjuvant therapy), the choice of medication and time limit for adjuvant therapy, the protective measures for patients undergoing emergency surgery, the re-examination of postoperative patients and the protection of medical staff, etc., authors improve treatment strategies in order to provide more choices for patients to obtain the best treatment under the severe epidemic situation of new coronavirus pneumonia. Meanwhile we hope that it can also provide more timely treatment modeling schemes for colleagues.', 'corpus_id': 213193072, 'score': 0}, {'doc_id': '214590831', 'title': '[Several suggestions of operation for colorectal cancer under the outbreak of corona virus disease 2019 in China].', 'abstract': 'Pneumonia caused by 2019-nCoV infection has been reported in Wuhan since December 2019, and spread rapidly across the country. The radical operation of colorectal cancer is semi-elective operation. Patients with colorectal cancer should receive operation as soon as possible after elective operation is resumed in each hospital. 2019-nCoV virus can be transmitted by asymptomatic infectors, and it has been confirmed to be transmitted by droplets and contact. However, fecal-oral transmission and aerosol transmission have not been excluded. Based on our experience with laparoscopic colorectal operation, we propose some surgery strategies for colorectal cancer patients under the corona virus disease 2019(COVID-19) situation: the screening process should be strictly carried out before surgery to reduce the risk of nosocomial infection in the later stage; laparoscopic-assisted surgery is recommended for radical surgery for patients with colorectal cancer; strict aerosol management must be made during the operation; natural orifice specimen extraction surgery and transanal total mesorectal excision are should be performed prudently; scientific and reasonable prophylactic stoma should be done; personnel protection in surgical ward and operation room must be strengthened.', 'corpus_id': 214590831, 'score': 0}, {'doc_id': '214716080', 'title': 'Detectable SARS‐CoV‐2 viral RNA in feces of three children during recovery period of COVID‐19 pneumonia', 'abstract': ""Coronavirus Disease 2019 (COVID‐19) is a newly emerging infectious disease caused by a novel coronavirus, severe acute respiratory syndrome coronavirus 2 (SARS‐CoV‐2). After its first occurrence in Wuhan of China from December 2019, COVID‐19 rapidly spread around the world. According to the World Health Organization statement on 13 March 2020, there had been over 132\u2009500 confirmed cases globally. Nevertheless, the case reports of children are rare, which results in the lack of evidence for preventing and controlling of children's infection. Here, we report three cases of SARS‐CoV‐2 infected children diagnosed from 3 February to 17 February 2020 in Tianjin, China. All of these three cases experienced mild illness and recovered soon after the treatment, with the nucleic acid of throat swab turning negative within 14, 11, and 7 days after diagnosis, respectively. However, after been discharged, all three cases were tested SARS‐CoV‐2 positive in the stool samples within 10 days, in spite of their remained negative nucleic acid in throat swab specimens. Therefore, it is necessary to be aware of the possibility of fecal‐oral transmission of SARS‐CoV‐2 infection, especially for children cases."", 'corpus_id': 214716080, 'score': 1}]"
13	{'doc_id': '231627491', 'title': 'LIME: Learning Inductive Bias for Primitives of Mathematical Reasoning', 'abstract': 'While designing inductive bias in neural architectures has been widely studied, we hypothesize that transformer networks are flexible enough to learn inductive bias from suitable generic tasks. Here, we replace architecture engineering by encoding inductive bias in the form of datasets. Inspired by Peirce’s view that deduction, induction, and abduction form an irreducible set of reasoning primitives, we design three synthetic tasks that are intended to require the model to have these three abilities. We specifically design these synthetic tasks in a way that they are devoid of mathematical knowledge to ensure that only the fundamental reasoning biases can be learned from these tasks. This defines a new pre-training methodology called “LIME” (Learning Inductive bias for Mathematical rEasoning). Models trained with LIME significantly outperform vanilla transformers on three very different large mathematical reasoning benchmarks. Unlike dominating the computation cost as traditional pre-training approaches, LIME requires only a small fraction of the computation cost of the typical downstream task.', 'corpus_id': 231627491}	6078	"[{'doc_id': '225067910', 'title': 'ShiftAddNet: A Hardware-Inspired Deep Network', 'abstract': ""Multiplication (e.g., convolution) is arguably a cornerstone of modern deep neural networks (DNNs). However, intensive multiplications cause expensive resource costs that challenge DNNs' deployment on resource-constrained edge devices, driving several attempts for multiplication-less deep networks. This paper presented ShiftAddNet, whose main inspiration is drawn from a common practice in energy-efficient hardware implementation, that is, multiplication can be instead performed with additions and logical bit-shifts. We leverage this idea to explicitly parameterize deep networks in this way, yielding a new type of deep network that involves only bit-shift and additive weight layers. This hardware-inspired ShiftAddNet immediately leads to both energy-efficient inference and training, without compromising the expressive capacity compared to standard DNNs. The two complementary operation types (bit-shift and add) additionally enable finer-grained control of the model's learning capacity, leading to more flexible trade-off between accuracy and (training) efficiency, as well as improved robustness to quantization and pruning. We conduct extensive experiments and ablation studies, all backed up by our FPGA-based ShiftAddNet implementation and energy measurements. Compared to existing DNNs or other multiplication-less models, ShiftAddNet aggressively reduces over 80% hardware-quantified energy cost of DNNs training and inference, while offering comparable or better accuracies. Codes and pre-trained models are available at this https URL."", 'corpus_id': 225067910, 'score': 1}, {'doc_id': '218581509', 'title': 'Compact Neural Representation Using Attentive Network Pruning', 'abstract': 'Deep neural networks have evolved to become power demanding and consequently difficult to apply to small-size mobile platforms. Network parameter reduction methods have been introduced to systematically deal with the computational and memory complexity of deep networks. We propose to examine the ability of attentive connection pruning to deal with redundancy reduction in neural networks as a contribution to the reduction of computational demand. In this work, we describe a Top-Down attention mechanism that is added to a Bottom-Up feedforward network to select important connections and subsequently prune redundant ones at all parametric layers. Our method not only introduces a novel hierarchical selection mechanism as the basis of pruning but also remains competitive with previous baseline methods in the experimental evaluation. We conduct experiments using different network architectures on popular benchmark datasets to show high compression ratio is achievable with negligible loss of accuracy.', 'corpus_id': 218581509, 'score': 0}, {'doc_id': '218863030', 'title': 'PruneNet: Channel Pruning via Global Importance', 'abstract': 'Channel pruning is one of the predominant approaches for accelerating deep neural networks. Most existing pruning methods either train from scratch with a sparsity inducing term such as group lasso, or prune redundant channels in a pretrained network and then fine tune the network. Both strategies suffer from some limitations: the use of group lasso is computationally expensive, difficult to converge and often suffers from worse behavior due to the regularization bias. The methods that start with a pretrained network either prune channels uniformly across the layers or prune channels based on the basic statistics of the network parameters. These approaches either ignore the fact that some CNN layers are more redundant than others or fail to adequately identify the level of redundancy in different layers. In this work, we investigate a simple-yet-effective method for pruning channels based on a computationally light-weight yet effective data driven optimization step that discovers the necessary width per layer. Experiments conducted on ILSVRC-$12$ confirm effectiveness of our approach. With non-uniform pruning across the layers on ResNet-$50$, we are able to match the FLOP reduction of state-of-the-art channel pruning results while achieving a $0.98\\%$ higher accuracy. Further, we show that our pruned ResNet-$50$ network outperforms ResNet-$34$ and ResNet-$18$ networks, and that our pruned ResNet-$101$ outperforms ResNet-$50$.', 'corpus_id': 218863030, 'score': 0}, {'doc_id': '210859422', 'title': 'Get Rid of Suspended Animation Problem: Deep Diffusive Neural Network on Graph Semi-Supervised Classification', 'abstract': 'Existing graph neural networks may suffer from the ""suspended animation problem"" when the model architecture goes deep. Meanwhile, for some graph learning scenarios, e.g., nodes with text/image attributes or graphs with long-distance node correlations, deep graph neural networks will be necessary for effective graph representation learning. In this paper, we propose a new graph neural network, namely DIFNET (Graph Diffusive Neural Network), for graph representation learning and node classification. DIFNET utilizes both neural gates and graph residual learning for node hidden state modeling, and includes an attention mechanism for node neighborhood information diffusion. Extensive experiments will be done in this paper to compare DIFNET against several state-of-the-art graph neural network models. The experimental results can illustrate both the learning performance advantages and effectiveness of DIFNET, especially in addressing the ""suspended animation problem"".', 'corpus_id': 210859422, 'score': 1}, {'doc_id': '219303563', 'title': 'Weight Pruning via Adaptive Sparsity Loss', 'abstract': 'Pruning neural networks has regained interest in recent years as a means to compress state-of-the-art deep neural networks and enable their deployment on resource-constrained devices. In this paper, we propose a robust compressive learning framework that efficiently prunes network parameters during training with minimal computational overhead. We incorporate fast mechanisms to prune individual layers and build upon these to automatically prune the entire network under a user-defined budget constraint. Key to our end-to-end network pruning approach is the formulation of an intuitive and easy-to-implement adaptive sparsity loss that is used to explicitly control sparsity during training, enabling efficient budget-aware optimization. Extensive experiments demonstrate the effectiveness of the proposed framework for image classification on the CIFAR and ImageNet datasets using different architectures, including AlexNet, ResNets and Wide ResNets.', 'corpus_id': 219303563, 'score': 1}, {'doc_id': '229376579', 'title': 'Learning Activation Functions in Deep (Spline) Neural Networks', 'abstract': 'We develop an efficient computational solution to train deep neural networks (DNN) with free-form activation functions. To make the problem well-posed, we augment the cost functional of the DNN by adding an appropriate shape regularization: the sum of the second-order total-variations of the trainable nonlinearities. The representer theorem for DNNs tells us that the optimal activation functions are adaptive piecewise-linear splines, which allows us to recast the problem as a parametric optimization. The challenging point is that the corresponding basis functions (ReLUs) are poorly conditioned and that the determination of their number and positioning is also part of the problem. We circumvent the difficulty by using an equivalent B-spline basis to encode the activation functions and by expressing the regularization as an $\\ell _1$-penalty. This results in the specification of parametric activation function modules that can be implemented and optimized efficiently on standard development platforms. We present experimental results that demonstrate the benefit of our approach.', 'corpus_id': 229376579, 'score': 1}, {'doc_id': '226964609', 'title': 'Advances in the training, pruning and enforcement of shape constraints of Morphological Neural Networks using Tropical Algebra', 'abstract': 'In this paper we study an emerging class of neural networks based on the morphological operators of dilation and erosion. We explore these networks mathematically from a tropical geometry perspective as well as mathematical morphology. Our contributions are threefold. First, we examine the training of morphological networks via Difference-of-Convex programming methods and extend a binary morphological classifier to multiclass tasks. Second, we focus on the sparsity of dense morphological networks trained via gradient descent algorithms and compare their performance to their linear counterparts under heavy pruning, showing that the morphological networks cope far better and are characterized with superior compression capabilities. Our approach incorporates the effect of the training optimizer used and offers quantitative and qualitative explanations. Finally, we study how the architectural structure of a morphological network can affect shape constraints, focusing on monotonicity. Via Maslov Dequantization, we obtain a softened version of a known architecture and show how this approach can improve training convergence and performance.', 'corpus_id': 226964609, 'score': 1}, {'doc_id': '219963786', 'title': 'NeuralScale: Efficient Scaling of Neurons for Resource-Constrained Deep Neural Networks', 'abstract': 'Deciding the amount of neurons during the design of a deep neural network to maximize performance is not intuitive. In this work, we attempt to search for the neuron (filter) configuration of a fixed network architecture that maximizes accuracy. Using iterative pruning methods as a proxy, we parametrize the change of the neuron (filter) number of each layer with respect to the change in parameters, allowing us to efficiently scale an architecture across arbitrary sizes. We also introduce architecture descent which iteratively refines the parametrized function used for model scaling. The combination of both proposed methods is coined as NeuralScale. To prove the efficiency of NeuralScale in terms of parameters, we show empirical simulations on VGG11, MobileNetV2 and ResNet18 using CIFAR10, CIFAR100 and TinyImageNet as benchmark datasets. Our results show an increase in accuracy of 3.04%, 8.56% and 3.41% for VGG11, MobileNetV2 and ResNet18 on CIFAR10, CIFAR100 and TinyImageNet respectively under a parameter-constrained setting (output neurons (filters) of default configuration with scaling factor of 0.25).', 'corpus_id': 219963786, 'score': 0}, {'doc_id': '220265908', 'title': 'Maximum Entropy Models for Fast Adaptation', 'abstract': 'Deep Neural Networks have shown great promise on a variety of downstream tasks; but their ability to adapt to new data and tasks remains a challenging problem. The ability of a model to perform few-shot adaptation to a novel task is important for the scalability and deployment of machine learning models. Recent work has shown that the learned features in a neural network follow a normal distribution [41], which thereby results in a strong prior on the downstream task. This implicit overfitting to data from training tasks limits the ability to generalize and adapt to unseen tasks at test time. This also highlights the importance of learning task-agnostic representations from data. In this paper, we propose a regularization scheme using a max-entropy prior on the learned features of a neural network; such that the extracted features make minimal assumptions about the training data. We evaluate our method on adaptation to unseen tasks by performing experiments in 4 distinct settings. We find that our method compares favourably against multiple strong baselines across all of these experiments.', 'corpus_id': 220265908, 'score': 1}, {'doc_id': '233168737', 'title': 'Rademacher Complexity and Numerical Quadrature Analysis of Stable Neural Networks with Applications to Numerical PDEs', 'abstract': 'Methods for solving PDEs using neural networks have recently become a very important topic. We provide an error analysis for such methods which is based on an a priori constraint on the K1(D)norm of the numerical solution. We show that the resulting constrained optimization problem can be efficiently solved using a greedy algorithm, which replaces stochastic gradient descent. Following this, we show that the error arising from discretizing the energy integrals is bounded both in the deterministic case, i.e. when using numerical quadrature, and also in the stochastic case, i.e. when sampling points to approximate the integrals. In the later case, we use a Rademacher complexity analysis, and in the former we use standard numerical quadrature bounds. This extends existing results to methods which use a general dictionary of functions to learn solutions to PDEs and importantly gives a consistent analysis which incorporates the optimization, approximation, and generalization aspects of the problem. In addition, the Rademacher complexity analysis is simplified and generalized, which enables application to a wide range of problems.', 'corpus_id': 233168737, 'score': 1}]"
14	{'doc_id': '218974874', 'title': 'Remdesivir for 5 or 10 Days in Patients with Severe Covid-19', 'abstract': 'Abstract Background Remdesivir is an RNA polymerase inhibitor with potent antiviral activity in vitro and efficacy in animal models of coronavirus disease 2019 (Covid-19). Methods We conducted a randomized, open-label, phase 3 trial involving hospitalized patients with confirmed SARS-CoV-2 infection, oxygen saturation of 94% or less while they were breathing ambient air, and radiologic evidence of pneumonia. Patients were randomly assigned in a 1:1 ratio to receive intravenous remdesivir for either 5 days or 10 days. All patients received 200 mg of remdesivir on day 1 and 100 mg once daily on subsequent days. The primary end point was clinical status on day 14, assessed on a 7-point ordinal scale. Results In total, 397 patients underwent randomization and began treatment (200 patients for 5 days and 197 for 10 days). The median duration of treatment was 5 days (interquartile range, 5 to 5) in the 5-day group and 9 days (interquartile range, 5 to 10) in the 10-day group. At baseline, patients randomly assigned to the 10-day group had significantly worse clinical status than those assigned to the 5-day group (P=0.02). By day 14, a clinical improvement of 2 points or more on the ordinal scale occurred in 64% of patients in the 5-day group and in 54% in the 10-day group. After adjustment for baseline clinical status, patients in the 10-day group had a distribution in clinical status at day 14 that was similar to that among patients in the 5-day group (P=0.14). The most common adverse events were nausea (9% of patients), worsening respiratory failure (8%), elevated alanine aminotransferase level (7%), and constipation (7%). Conclusions In patients with severe Covid-19 not requiring mechanical ventilation, our trial did not show a significant difference between a 5-day course and a 10-day course of remdesivir. With no placebo control, however, the magnitude of benefit cannot be determined. (Funded by Gilead Sciences; GS-US-540-5773 ClinicalTrials.gov number, NCT04292899.)', 'corpus_id': 218974874}	5596	"[{'doc_id': '218571559', 'title': 'COVID-19: Clinical course and outcomes of 36 maintenance hemodialysis patients from a single center in Spain.', 'abstract': '\n \n SARS-CoV-2-pneumonia emerged in Wuhan, China in December 2019. Unfortunately, there is lack of evidence about the optimal management of novel coronavirus disease 2019 (COVID-19), even less in patients on maintenance hemodialysis (MHD) therapy than in the general population. In this retrospective observational single-center study we analyzed the clinical course and outcomes of all MHD patients hospitalized with COVID-19 from March 12th to April 10th, 2020 as confirmed by real time polymerase chain reaction. Baseline features, clinical course, laboratory data, and different therapies were compared between survivors and non-survivors to identify risk factors associated with mortality. Among the 36 patients, 11 (30.5%) died and 7 could be discharged within the observation period. Clinical and radiological evolution during the first week of admission were predictive of mortality. Among the 36 patients, 18 had worsening of their clinical status, as defined by severe hypoxia with oxygen therapy requirements greater than 4 Liters/minute and radiological worsening. Significantly 11 out of those 18 patients (61.1%) died. None of the classical cardiovascular risk factors in the general population were associated with higher mortality. However, a longer time on hemodialysis (hazard ratio 1.008(95% confidence interval 1.001-1.015) per year), increased LDH levels (1.006(1.001-1.011), and lower lymphocyte count (0.996 (0.992-1.000) one week after clinical onset were all significantly associated with higher mortality risk. Thus, the mortality among hospitalized hemodialysis patients diagnosed with COVID-19 is high. Lymphopenia and increased LDH levels were associated with poor prognosis.\n \n', 'corpus_id': 218571559, 'score': 0}, {'doc_id': '222209229', 'title': 'Treating COVID-19 With Hydroxychloroquine (TEACH): A Multicenter, Double-Blind Randomized Controlled Trial in Hospitalized Patients', 'abstract': 'Abstract Background Effective therapies to combat coronavirus 2019 (COVID-19) are urgently needed. Hydroxychloroquine (HCQ) has in vitro antiviral activity against severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2), but the clinical benefit of HCQ in treating COVID-19 is unclear. Randomized controlled trials are needed to determine the safety and efficacy of HCQ for the treatment of hospitalized patients with COVID-19. Methods We conducted a multicenter, double-blind randomized clinical trial of HCQ among patients hospitalized with laboratory-confirmed COVID-19. Subjects were randomized in a 1:1 ratio to HCQ or placebo for 5 days and followed for 30 days. The primary efficacy outcome was a severe disease progression composite end point (death, intensive care unit admission, mechanical ventilation, extracorporeal membrane oxygenation, and/or vasopressor use) at day 14. Results A total of 128 patients were included in the intention-to-treat analysis. Baseline demographic, clinical, and laboratory characteristics were similar between the HCQ (n\u2005=\u200567) and placebo (n\u2005=\u200561) arms. At day 14, 11 (16.4%) subjects assigned to HCQ and 6 (9.8%) subjects assigned to placebo met the severe disease progression end point, but this did not achieve statistical significance (P\u2005=\u2005.350). There were no significant differences in COVID-19 clinical scores, number of oxygen-free days, SARS-CoV-2 clearance, or adverse events between HCQ and placebo. HCQ was associated with a slight increase in mean corrected QT interval, an increased D-dimer, and a trend toward an increased length of stay. Conclusions In hospitalized patients with COVID-19, our data suggest that HCQ does not prevent severe outcomes or improve clinical scores. However, our conclusions are limited by a relatively small sample size, and larger randomized controlled trials or pooled analyses are needed.', 'corpus_id': 222209229, 'score': 1}, {'doc_id': '219526698', 'title': 'Tocilizumab as a Therapeutic Agent for Critically Ill Patients Infected with SARS-CoV-2', 'abstract': ""Background: Tocilizumab is an IL-6 receptor antagonist with the ability to suppress the cytokine storm in critically ill patients infected with SARS-CoV-2. Methods: We evaluated patients treated with tocilizumab for a SARS-CoV-2 infection who were admitted between 3/13/20 and 4/16/20. This was a multi-center study with data collected by chart review both retrospectively and concurrently. Parameters evaluated included age, sex, race, use of mechanical ventilation (MV), usage of steroids and vasopressors, inflammatory markers, and comorbidities. Early dosing was defined as a tocilizumab dose administered prior to or within one (1) day of intubation. Late dosing was defined as a dose administered greater than one (1) day after intubation. In the absence of mechanical ventilation, the timing of the dose was related to the patient's date of admission only. Results: We evaluated 145 patients. The average age was 58.1 years, 64% were male, 68.3% had comorbidities, and 60% received steroid therapy. Disposition of patients was 48.3% discharged and 29.3% expired, of which 43.9% were African American. Mechanical ventilation was required in 55.9%, of which 34.5% expired. Avoidance of MV (p value = 0.002) and increased survival (p value < 0.001) was statistically associated with early dosing. Conclusions: Tocilizumab therapy was effective at decreasing mortality and should be instituted early in the management of critically ill COVID-19 patients."", 'corpus_id': 219526698, 'score': 1}, {'doc_id': '220602322', 'title': 'COVID-19 in people with diabetes: understanding the reasons for worse outcomes', 'abstract': '\n Summary\n \n Since the initial COVID-19 outbreak in China, much attention has focused on people with diabetes because of poor prognosis in those with the infection. Initial reports were mainly on people with type 2 diabetes, although recent surveys have shown that individuals with type 1 diabetes are also at risk of severe COVID-19. The reason for worse prognosis in people with diabetes is likely to be multifactorial, thus reflecting the syndromic nature of diabetes. Age, sex, ethnicity, comorbidities such as hypertension and cardiovascular disease, obesity, and a pro-inflammatory and pro-coagulative state all probably contribute to the risk of worse outcomes. Glucose-lowering agents and anti-viral treatments can modulate the risk, but limitations to their use and potential interactions with COVID-19 treatments should be carefully assessed. Finally, severe acute respiratory syndrome coronavirus 2 infection itself might represent a worsening factor for people with diabetes, as it can precipitate acute metabolic complications through direct negative effects on β-cell function. These effects on β-cell function might also cause diabetic ketoacidosis in individuals with diabetes, hyperglycaemia at hospital admission in individuals with unknown history of diabetes, and potentially new-onset diabetes.\n \n', 'corpus_id': 220602322, 'score': 0}, {'doc_id': '219858700', 'title': 'The clinical value of two combination regimens in the Management of Patients Suffering from Covid-19 pneumonia: a single centered, retrospective, observational study', 'abstract': 'There is no identified pharmacological therapy for COVID-19 patients, where potential therapeutic strategies are underway to determine effective therapy under such unprecedented pandemic. Therefore, combination therapies may have the potential of alleviating the patient’s outcome. This study aimed at comparing the efficacy of two different combination regimens in improving outcomes of patients infected by novel coronavirus (COVID-19). This is a single centered, retrospective, observational study of 60 laboratory-confirmed COVID-19 positive inpatients (≥18 years old) at two wards of the Baqiyatallah Hospital, Tehran, Iran. Patient’s data including clinical and laboratory parameters were recorded. According to the drug regimen, the patients were divided into two groups; group I who received regimen I consisting azithromycin, prednisolone, naproxen, and lopinavir/ritonavir and group II who received regimen II including meropenem, levofloxacin, vancomycin, hydroxychloroquine, and oseltamivir. The oxygen saturation (SpO2) and temperature were positively changed in patients receiving regimen I compared to regimen II (P\u2009=\u20090.013 and P\u2009=\u20090.012, respectively). The serum level of C-reactive protein (CRP) changed positively in group I (P\u2009<\u20090.001). Although there was a significant difference in platelets between both groups (75.44 vs 51.62, P\u2009<\u20090.001), their change did not clinically differ between two groups. The findings indicated a significant difference of the average length of stay in hospitals (ALOS) between two groups, where the patients under regimen I showed a shorter ALOS (6.97 vs 9.93, P\u2009=\u20090.001). This study revealed the beneficial effect of the short-term use of low-dose prednisolone in combination with azithromycin, naproxen and lopinavir/ritonavir (regimen I), in decreasing ALOS compared to regimen II. Since there is still lack of evidence for safety of this regimen, further investigation in our ongoing follow-up to deal with COVID-19 pneumonia is underway. Graphical abstract Graphical abstract', 'corpus_id': 219858700, 'score': 1}, {'doc_id': '224811730', 'title': 'Clinical Characteristics and Outcomes of COVID-19 Patients Receiving Compassionate Use Leronlimab', 'abstract': 'Abstract Background Leronlimab, a monoclonal antibody blocker of CCR5 originally developed to treat HIV-1 infection, was administered as an open label compassionate use therapeutic for COVID-19. Methods 23 hospitalized severe/critical COVID-19 patients received 700mg leronlimab subcutaneously, repeated after seven days in 17/23 patients still hospitalized. 18/23 received other experimental treatments, including convalescent plasma, hydroxychloroquine, steroids, and/or tocilizumab. 5/23 received leronlimab after blinded placebo-controlled trials of remdesivir, sarilumab, selinexor, or tocilizumab. Outcomes and results were extracted from medical records. Results Mean age was 69.5±14.9 years. 20/23 had significant co-morbidities. At baseline, 22/23 were receiving supplemental oxygen (3/23 high flow, 7/23 mechanical ventilation). Blood showed markedly elevated inflammatory markers (ferritin, D-dimer, C-reactive protein) and elevated neutrophil:lymphocyte ratio. By day 30 after initial dosing, 17/23 were recovered, 2/23 were still hospitalized, and 4/23 had died. Of the 7 intubated at baseline, 4/7 were fully recovered off oxygen, 2/7 were still hospitalized, and 1/7 had died. Conclusions Leronlimab appeared safe and well tolerated. The high recovery rate suggested benefit, and those with lower inflammatory markers had better outcomes. Some but not all patients appeared to have dramatic clinical responses, indicating that unknown factors may determine responsiveness to leronlimab. Routine inflammatory and cell prognostic markers did not markedly change immediately after treatment, although IL-6 tended to fall. In some persons C-reactive protein clearly dropped only after the second leronlimab dose, suggesting that a higher loading dose might be more effective. Future controlled trials will be informative.', 'corpus_id': 224811730, 'score': 1}, {'doc_id': '220526615', 'title': 'Clinical Manifestations and Outcomes of Critically Ill Children and Adolescents with Coronavirus Disease 2019 in New York City', 'abstract': '\n               \n                  Objectives\n                  To describe the clinical manifestations and outcomes of critically ill children with coronavirus disease-19 (COVID-19) in New York City.\n               \n               \n                  Study design\n                  Retrospective observational study of children 1 month to 21 years admitted March 14 to May 2, 2020 to 9 New York City pediatric intensive care units (PICUs) with SARS-CoV-2 infection.\n               \n               \n                  Results\n                  Of 70 children admitted to PICUs: median age 15 [IQR 9, 19] years; 61.4% male; 38.6% Hispanic; 32.9% Black; 74.3% with comorbidities. Fever (72.9%) and cough (71.4%) were the common presenting symptoms. Twelve patients (17%) met severe sepsis criteria; 14 (20%) required vasopressor support; 21 (30%) developed acute respiratory distress syndrome (ARDS); 9 (12.9%) met acute kidney injury criteria; 1 (1.4%) required renal replacement therapy, and 2 (2.8%) had cardiac arrest. For treatment, 27 (38.6%) patients received hydroxychloroquine; 13 (18.6%) remdesivir; 23 (32.9%) corticosteroids; 3 (4.3%) tocilizumab; 1 (1.4%) anakinra; no patient was given immunoglobulin or convalescent plasma. Forty-nine (70%) patients required respiratory support: 14 (20.0%) non-invasive mechanical ventilation, 20 (28.6%) invasive mechanical ventilation (IMV), 7 (10%) prone position, 2 (2.8%) inhaled nitric oxide, and 1 (1.4%) extracorporeal membrane oxygenation. Nine (45%) of the 20 patients requiring IMV were extubated by day 14 with median IMV duration of 218 [IQR 79, 310.4] hours. Presence of ARDS was significantly associated with duration of PICU and hospital stay, and lower probability of PICU and hospital discharge at hospital day 14 (P < .05 for all).\n               \n               \n                  Conclusions\n                  Critically ill children with COVID-19 predominantly are adolescents, have comorbidities, and require some form of respiratory support. The presence of ARDS is significantly associated with prolonged PICU and hospital stay.\n               \n            ', 'corpus_id': 220526615, 'score': 0}, {'doc_id': '220405024', 'title': 'Interleukin-6 receptor blocking with intravenous tocilizumab in COVID-19 severe acute respiratory distress syndrome: A retrospective case-control survival analysis of 128 patients', 'abstract': '\n Abstract\n \n In cases of COVID-19 acute respiratory distress syndrome, an excessive host inflammatory response has been reported, with elevated serum interleukin-6 levels. In this multicenter retrospective cohort study we included adult patients with COVID-19, need of respiratory support, and elevated C-reactive protein who received intravenous tocilizumab in addition to standard of care. Control patients not receiving tocilizumab were matched for sex, age and respiratory support. We selected survival as the primary endpoint, along with need for invasive ventilation, thrombosis, hemorrhage, and infections as secondary endpoints at 30 days. We included 64 patients with COVID-19 in the tocilizumab group and 64 matched controls. At baseline the tocilizumab group had longer symptom duration (13\u202f±\u202f5 vs. 9\u202f±\u202f5 days) and received hydroxychloroquine more often than controls (100% vs. 81%). The mortality rate was similar between groups (27% with tocilizumab vs. 38%) and at multivariable analysis risk of death was not significantly influenced by tocilizumab (hazard ratio 0.61, 95% confidence interval 0.33–1.15), while being associated with the use at baseline of non invasive mechanical or invasive ventilation, and the presence of comorbidities. Among secondary outcomes, tocilizumab was associated with a lower probability of requiring invasive ventilation (hazard ratio 0.36, 95% confidence interval 0.16–0.83; P\u202f=\u202f0.017) but not with the risk of thrombosis, bleeding, or infections. The use of intravenous tocilizumab was not associated with changes in 30-day mortality in patients with COVID-19 severe respiratory impairment. Among the secondary outcomes there was less use of invasive ventilation in the tocilizumab group.\n \n', 'corpus_id': 220405024, 'score': 1}, {'doc_id': '220437021', 'title': 'Intravenous high-dose vitamin C for the treatment of severe COVID-19: study protocol for a multicentre randomised controlled trial', 'abstract': 'Introduction The rapid worldwide spread of COVID-19 has caused a global health crisis. To date, symptomatic supportive care has been the most common treatment. It has been reported that the mechanism of COVID-19 is related to cytokine storms and subsequent immunogenic damage, especially damage to the endothelium and alveolar membrane. Vitamin C (VC), also known as L-ascorbic acid, has been shown to have antimicrobial and immunomodulatory properties. A high dose of intravenous VC (HIVC) was proven to block several key components of cytokine storms, and HIVC showed safety and varying degrees of efficacy in clinical trials conducted on patients with bacterial-induced sepsis and acute respiratory distress syndrome (ARDS). Therefore, we hypothesise that HIVC could be added to the treatment of ARDS and multiorgan dysfunction related to COVID-19. Methods and analysis The investigators designed a multicentre prospective randomised placebo-controlled trial that is planned to recruit 308 adults diagnosed with COVID-19 and transferred into the intensive care unit. Participants will randomly receive HIVC diluted in sterile water or placebo for 7 days once enrolled. Patients with a history of VC allergy, end-stage pulmonary disease, advanced malignancy or glucose-6-phosphate dehydrogenase deficiency will be excluded. The primary outcome is ventilation-free days within 28 observational days. This is one of the first clinical trials applying HIVC to treat COVID-19, and it will provide credible efficacy and safety data. We predict that HIVC could suppress cytokine storms caused by COVID-19, help improve pulmonary function and reduce the risk of ARDS of COVID-19. Ethics and dissemination The study protocol was approved by the Ethics Committee of Zhongnan Hospital of Wuhan University (identifiers: Clinical Ethical Approval No. 2020001). Findings of the trial will be disseminated through peer-reviewed journals and scientific conferences. Trial registration number NCT04264533.', 'corpus_id': 220437021, 'score': 0}]"
15	{'doc_id': '237278008', 'title': 'Randomized C/C++ dynamic memory allocator', 'abstract': 'Dynamic memory management requires special attention in programming. It should be fast and secure at the same time. This paper proposes a new randomized dynamic memory management algorithm designed to meet these requirements. Randomization is a key feature intended to protect applications from “use-after-free” or similar attacks. At the same time, the state in the algorithm consists only of one pointer, so it does not consume extra memory for itself. However, our algorithm is not a universal solution. It does not solve the memory fragmentation problem and it needs further development and testing.', 'corpus_id': 237278008}	15429	"[{'doc_id': '11932674', 'title': 'Destination-passing style for efficient memory management', 'abstract': 'We show how to compile high-level functional array-processing programs, drawn from image processing and machine learning, into C code that runs as fast as hand-written C. The key idea is to transform the program to destination-passing style, which in turn enables a highly-efficient stack-like memory allocation discipline.', 'corpus_id': 11932674, 'score': 1}, {'doc_id': '61153505', 'title': 'Mesh: compacting memory management for C/C++ applications', 'abstract': 'Programs written in C/C++ can suffer from serious memory fragmentation, leading to low utilization of memory, degraded performance, and application failure due to memory exhaustion. This paper introduces Mesh, a plug-in replacement for malloc that, for the first time, eliminates fragmentation in unmodified C/C++ applications. Mesh combines novel randomized algorithms with widely-supported virtual memory operations to provably reduce fragmentation, breaking the classical Robson bounds with high probability. Mesh generally matches the runtime performance of state-of-the-art memory allocators while reducing memory consumption; in particular, it reduces the memory of consumption of Firefox by 16% and Redis by 39%.', 'corpus_id': 61153505, 'score': 1}, {'doc_id': '237362765', 'title': 'HMvisor: dynamic hybrid memory management for virtual machines', 'abstract': 'Emerging non-volatile memory (NVM) technologies promise high density, low cost and dynamic random access memory (DRAM)-like performance, at the expense of limited write endurance and high write energy consumption. It is more practical to use NVM combining with the traditional DRAM. However, the hybrid memory management such as page migration becomes more challenging in a virtualization environment because virtual machines (VMs) are unaware of the memory heterogeneity. In this paper, we propose HMvisor, a hypervisor and VM coordinated hybrid memory management mechanism to better utilize DRAM and NVM resources. HMvisor exposes the memory heterogeneity to VMs by mapping virtual NUMA nodes to different physical NUMA nodes. We propose a lightweight and efficient page migration mechanism by decoupling page hotness tracking from page migration. HMvisor performs those operations in the hypervisor and VMs separately, without disrupting the execution of VMs. We also propose a memory resource trading policy to adjust the capacity of DRAM and NVM for each VM, with the monetary cost unchanged. We implement our prototype system based on QEMU/KVM and evaluate it with several benchmarks. Experimental results show that HMvisor can reduce 50% of write traffic to NVM with less than 5% performance overhead. Moreover, the hybrid memory adjustment scheme in HMvisor can significantly improve application performance by up to 30×.', 'corpus_id': 237362765, 'score': 0}, {'doc_id': '234904205', 'title': 'An Overview of Python', 'abstract': 'In this chapter, we cover just enough of Python to get you going. If you’re new to programming, this chapter will get you started well enough to be productive and we’ll call out ways to learn more at the end. Python is a great place to learn to program because its syntax is simpler and it has less overhead (e.g. memory management) than traditional programming languages such as Java or C+ +. If you’re an experienced programmer in another language, you should skim this chapter to learn the essentials.', 'corpus_id': 234904205, 'score': 0}, {'doc_id': '237206062', 'title': 'Persistent software transactional memory in Haskell', 'abstract': ""Emerging persistent memory in commodity hardware allows byte-granular accesses to persistent state at memory speeds. However, to prevent inconsistent state in persistent memory due to unexpected system failures, different write-semantics are required compared to volatile memory. Transaction-based library solutions for persistent memory facilitate the atomic modification of persistent data in languages where memory is explicitly managed by the programmer, such as C/C++. For languages that provide extended capabilities like automatic memory management, a more native integration into the language is needed to maintain the high level of memory abstraction. It is shown in this paper how persistent software transactional memory (PSTM) can be tightly integrated into the runtime system of Haskell to atomically manage values of persistent transactional data types. PSTM has a clear interface and semantics extending that of software transactional memory (STM). Its integration with the language’s memory management retains features like garbage collection and allocation strategies, and is fully compatible with Haskell's lazy execution model. Our PSTM implementation demonstrates competitive performance with low level libraries and trivial portability of existing STM libraries to PSTM. The implementation allows further interesting use cases, such as persistent memoization and persistent Haskell expressions."", 'corpus_id': 237206062, 'score': 1}, {'doc_id': '63622661', 'title': 'Using Destination-Passing Style to Compile a Functional Language into Efficient Low-Level Code', 'abstract': 'We show how to compile high-level functional array-processing programs, drawn from image processing and machine learning, into C code that runs as fast as hand-written C. The key idea is to transform the program to destination passing style, which in turn enables a highly-efficient stack-like memory allocation discipline.', 'corpus_id': 63622661, 'score': 1}, {'doc_id': '234491186', 'title': 'Execution Dependence Extension (EDE): ISA Support for Eliminating Fences', 'abstract': 'Fence instructions are a coarse-grained mechanism to enforce the order of instruction execution in an out-of-order pipeline. They are an overkill for cases when only one instruction must wait for the completion of one other instruction. For example, this is the case when performing undo logging in Non-Volatile Memory (NVM) systems: while the update of a variable needs to wait until the corresponding undo log entry is persisted, all other instructions can be reordered. Unfortunately, current ISAs do not provide a way to describe such an execution dependence between two instructions that have no register or memory dependences. As a result, programmers must place fences, which unnecessarily serialize many unrelated instructions.To remedy this limitation, we propose an ISA extension capable of describing these execution dependences. We call the proposal Execution Dependence Extension (EDE), and add it to Arm’s AArch64 ISA. We also present two hardware realizations of EDE that enforce execution dependences at different stages of the pipeline: one in the issue queue (IQ) and another in the write buffer (WB). We implement IQ and WB in a simulator and test them with several NVM applications. Overall, by using EDE with IQ and WB rather than fences, we attain average workload speedups of 18% and 26%, respectively.', 'corpus_id': 234491186, 'score': 0}, {'doc_id': '237532306', 'title': 'Dala: a simple capability-based dynamic language design for data race-freedom', 'abstract': 'Dynamic languages like Erlang, Clojure, JavaScript, and E adopted data-race freedom by design. To enforce data-race freedom, these languages either deep copy objects during actor (thread) communication or proxy back to their owning thread. We present Dala, a simple programming model that ensures data-race freedom while supporting efficient inter-thread communication. Dala is a dynamic, concurrent, capability-based language that relies on three core capabilities: immutable values can be shared freely; isolated mutable objects can be transferred between threads but not aliased; local objects can be aliased within their owning thread but not dereferenced by other threads. Objects with capabilities can co-exist with unsafe objects, that are unchecked and may suffer data races, without compromising the safety of safe objects. We present a formal model of Dala, prove data race-freedom and state and prove a dynamic gradual guarantee. These theorems guarantee data race-freedom when using safe capabilities and show that the addition of capabilities is semantics preserving modulo permission and cast errors.', 'corpus_id': 237532306, 'score': 1}, {'doc_id': '235727314', 'title': 'Model Checking C++ Programs', 'abstract': 'In the last three decades, memory safety issues in system programming languages such as C or C++ have been one of the significant sources of security vulnerabilities. However, there exist only a few attempts with limited success to cope with the complexity of C++ program verification. Here we describe and evaluate a novel verification approach based on bounded model checking (BMC) and satisfiability modulo theories (SMT) to verify C++ programs formally. Our verification approach analyzes bounded C++ programs by encoding into SMT various sophisticated features that the C++ programming language offers, such as templates, inheritance, polymorphism, exception handling, and the Standard C++ Libraries. We formalize these features within our formal verification framework using a decidable fragment of first-order logic and then show how state-of-the-art SMT solvers can efficiently handle that. We implemented our verification approach on top of ESBMC. We compare ESBMC to LLBMC and DIVINE, which are state-of-the-art verifiers to check C++ programs directly from the LLVM bitcode. Experimental results show that ESBMC can handle a wide range of C++ programs, presenting a higher number of correct verification results. At the same time, it reduces the verification time if compared to LLBMC and DIVINE tools. Additionally, ESBMC has been applied to a commercial C++ application in the telecommunication domain and successfully detected arithmetic overflow errors, potentially leading to security vulnerabilities. Copyright© 2021 John Wiley & Sons, Ltd.', 'corpus_id': 235727314, 'score': 0}, {'doc_id': '235128580', 'title': 'Polypyus – The Firmware Historian', 'abstract': 'Binary diffing compares binaries without source code. Currently, the state-of-the-art tools for this are BinDiff [43] and Diaphora [29]. They identify similar functions in a binary even if names are stripped. To this end, they can compare disassembler and decompiler output and consider call graph statistics. While BinDiff and Diaphora work well on most binaries, this is not the case for raw firmware. Both depend on functions being identified correctly. However, disassemblers like IDA Pro and Ghidra, which support these binary diffing tools, fail at function identification on the Advanced RISC Machine (ARM) Thumb2 instruction set. Yet, fixing disassembler issues with the ARM Thumb2 instruction set is rather complex and comes with different properties per compiler setting [28]. Surprisingly, due to avoiding false positives, the most recent version of IDA Pro fails to identify significantly more functions in firmware binaries than older versions.', 'corpus_id': 235128580, 'score': 0}]"
16	{'doc_id': '219179670', 'title': 'Learning Constraints for Structured Prediction Using Rectifier Networks', 'abstract': 'Various natural language processing tasks are structured prediction problems where outputs are constructed with multiple interdependent decisions. Past work has shown that domain knowledge, framed as constraints over the output space, can help improve predictive accuracy. However, designing good constraints often relies on domain expertise. In this paper, we study the problem of learning such constraints. We frame the problem as that of training a two-layer rectifier network to identify valid structures or substructures, and show a construction for converting a trained network into a system of linear constraints over the inference variables. Our experiments on several NLP tasks show that the learned constraints can improve the prediction accuracy, especially when the number of training examples is small.', 'corpus_id': 219179670}	5226	"[{'doc_id': '218718686', 'title': 'Learning semantic program embeddings with graph interval neural network', 'abstract': 'Learning distributed representations of source code has been a challenging task for machine learning models. Earlier works treated programs as text so that natural language methods can be readily applied. Unfortunately, such approaches do not capitalize on the rich structural information possessed by source code. Of late, Graph Neural Network (GNN) was proposed to learn embeddings of programs from their graph representations. Due to the homogeneous (i.e. do not take advantage of the program-specific graph characteristics) and expensive (i.e. require heavy information exchange among nodes in the graph) message-passing procedure, GNN can suffer from precision issues, especially when dealing with programs rendered into large graphs. In this paper, we present a new graph neural architecture, called Graph Interval Neural Network (GINN), to tackle the weaknesses of the existing GNN. Unlike the standard GNN, GINN generalizes from a curated graph representation obtained through an abstraction method designed to aid models to learn. In particular, GINN focuses exclusively on intervals (generally manifested in looping construct) for mining the feature representation of a program, furthermore, GINN operates on a hierarchy of intervals for scaling the learning to large graphs. We evaluate GINN for two popular downstream applications: variable misuse prediction and method name prediction. Results show in both cases GINN outperforms the state-of-the-art models by a comfortable margin. We have also created a neural bug detector based on GINN to catch null pointer deference bugs in Java code. While learning from the same 9,000 methods extracted from 64 projects, GINN-based bug detector significantly outperforms GNN-based bug detector on 13 unseen test projects. Next, we deploy our trained GINN-based bug detector and Facebook Infer, arguably the state-of-the-art static analysis tool, to scan the codebase of 20 highly starred projects on GitHub. Through our manual inspection, we confirm 38 bugs out of 102 warnings raised by GINN-based bug detector compared to 34 bugs out of 129 warnings for Facebook Infer. We have reported 38 bugs GINN caught to developers, among which 11 have been fixed and 12 have been confirmed (fix pending). GINN has shown to be a general, powerful deep neural network for learning precise, semantic program embeddings.', 'corpus_id': 218718686, 'score': 0}, {'doc_id': '215827467', 'title': 'Graph-Structured Referring Expression Reasoning in the Wild', 'abstract': 'Grounding referring expressions aims to locate in an image an object referred to by a natural language expression. The linguistic structure of a referring expression provides a layout of reasoning over the visual contents, and it is often crucial to align and jointly understand the image and the referring expression. In this paper, we propose a scene graph guided modular network (SGMN), which performs reasoning over a semantic graph and a scene graph with neural modules under the guidance of the linguistic structure of the expression. In particular, we model the image as a structured semantic graph, and parse the expression into a language scene graph. The language scene graph not only decodes the linguistic structure of the expression, but also has a consistent representation with the image semantic graph. In addition to exploring structured solutions to grounding referring expressions, we also propose Ref-Reasoning, a large-scale real-world dataset for structured referring expression reasoning. We automatically generate referring expressions over the scene graphs of images using diverse expression templates and functional programs. This dataset is equipped with real-world visual contents as well as semantically rich expressions with different reasoning layouts. Experimental results show that our SGMN not only significantly outperforms existing state-of-the-art algorithms on the new Ref-Reasoning dataset, but also surpasses state-of-the-art structured methods on commonly used benchmark datasets. It can also provide interpretable visual evidences of reasoning.', 'corpus_id': 215827467, 'score': 0}, {'doc_id': '216641734', 'title': 'Leveraging Declarative Knowledge in Text and First-Order Logic for Fine-Grained Propaganda Detection', 'abstract': 'We study the detection of propagandistic text fragments in news articles. Instead of merely learning from input-output datapoints in training data, we introduce an approach to inject declarative knowledge of fine-grained propaganda techniques. We leverage declarative knowledge expressed in both natural language and first-order logic. The former refers to the literal definition of each propaganda technique, which is utilized to get class representations for regularizing the model parameters. The latter refers to logical consistency between coarse- and fine- grained predictions, which is used to regularize the training process with propositional Boolean expressions. We conduct experiments on Propaganda Techniques Corpus, a large manually annotated dataset for fine-grained propaganda detection. Experiments show that our method achieves superior performance, demonstrating that injecting declarative knowledge expressed in both natural language and first-order logic can help the model to make more accurate predictions.', 'corpus_id': 216641734, 'score': 1}, {'doc_id': '202775885', 'title': 'DRUM: End-To-End Differentiable Rule Mining On Knowledge Graphs', 'abstract': 'In this paper, we study the problem of learning probabilistic logical rules for inductive and interpretable link prediction. Despite the importance of inductive link prediction, most previous works focused on transductive link prediction and cannot manage previously unseen entities. Moreover, they are black-box models that are not easily explainable for humans. We propose DRUM, a scalable and differentiable approach for mining first-order logical rules from knowledge graphs that resolves these problems. We motivate our method by making a connection between learning confidence scores for each rule and low-rank tensor approximation. DRUM uses bidirectional RNNs to share useful information across the tasks of learning rules for different relations. We also empirically demonstrate the efficiency of DRUM over existing rule mining methods for inductive link prediction on a variety of benchmark datasets.', 'corpus_id': 202775885, 'score': 1}, {'doc_id': '220055965', 'title': 'Compositional Explanations of Neurons', 'abstract': 'We describe a procedure for explaining neurons in deep representations by identifying compositional logical concepts that closely approximate neuron behavior. Compared to prior work that uses atomic labels as explanations, analyzing neurons compositionally allows us to more precisely and expressively characterize their behavior. We use this procedure to answer several questions on interpretability in models for vision and natural language processing. First, we examine the kinds of abstractions learned by neurons. In image classification, we find that many neurons learn highly abstract but semantically coherent visual concepts, while other polysemantic neurons detect multiple unrelated features; in natural language inference (NLI), neurons learn shallow lexical heuristics from dataset biases. Second, we see whether compositional explanations give us insight into model performance: vision neurons that detect human-interpretable concepts are positively correlated with task performance, while NLI neurons that fire for shallow heuristics are negatively correlated with task performance. Finally, we show how compositional explanations provide an accessible way for end users to produce simple ""copy-paste"" adversarial examples that change model behavior in predictable ways.', 'corpus_id': 220055965, 'score': 1}, {'doc_id': '28851985', 'title': 'Incidental Supervision: Moving beyond Supervised Learning', 'abstract': 'Machine Learning and Inference methods have become ubiquitous in our attempt to induce more abstract representations of natural language text, visual scenes, and other messy, naturally occurring data, and support decisions that depend on it. However, learning models for these tasks is difficult partly because generating the necessary supervision signals for it is costly and does not scale. This paper describes several learning paradigms that are designed to alleviate the supervision bottleneck. It will illustrate their benefit in the context of multiple problems, all pertaining to inducing various levels of semantic representations from text.', 'corpus_id': 28851985, 'score': 0}, {'doc_id': '218665531', 'title': 'Recent Advances in SQL Query Generation: A Survey', 'abstract': 'Natural language is hypothetically the best user interface for many domains. However, general models that provide an interface between natural language and any other domain still do not exist. Providing natural language interface to relational databases could possibly attract a vast majority of users that are or are not proficient with query languages. With the rise of deep learning techniques, there is extensive ongoing research in designing a suitable natural language interface to relational databases. \nThis survey aims to overview some of the latest methods and models proposed in the area of SQL query generation from natural language. We describe models with various architectures such as convolutional neural networks, recurrent neural networks, pointer networks, reinforcement learning, etc. Several datasets intended to address the problem of SQL query generation are interpreted and briefly overviewed. In the end, evaluation metrics utilized in the field are presented mainly as a combination of execution accuracy and logical form accuracy.', 'corpus_id': 218665531, 'score': 0}, {'doc_id': '219792319', 'title': 'Compositional Generalization by Learning Analytical Expressions', 'abstract': 'Compositional generalization is a basic but essential intellective capability of human beings, which allows us to recombine known parts readily. However, existing neural network based models have been proven to be extremely deficient in such a capability. Inspired by work in cognition which argues compositionality can be captured by variable slots with symbolic functions, we present a refreshing view that connects a memory-augmented neural model with analytical expressions, to achieve compositional generalization. Our model consists of two cooperative neural modules Composer and Solver, fitting well with the cognitive argument while still being trained in an end-to-end manner via a hierarchical reinforcement learning algorithm. Experiments on a well-known benchmark SCAN demonstrate that our model seizes a great ability of compositional generalization, solving all challenges addressed by previous works with 100% accuracies.', 'corpus_id': 219792319, 'score': 1}, {'doc_id': '215745174', 'title': 'Pre-training Text Representations as Meta Learning', 'abstract': ""Pre-training text representations has recently been shown to significantly improve the state-of-the-art in many natural language processing tasks. The central goal of pre-training is to learn text representations that are useful for subsequent tasks. However, existing approaches are optimized by minimizing a proxy objective, such as the negative log likelihood of language modeling. In this work, we introduce a learning algorithm which directly optimizes model's ability to learn text representations for effective learning of downstream tasks. We show that there is an intrinsic connection between multi-task pre-training and model-agnostic meta-learning with a sequence of meta-train steps. The standard multi-task learning objective adopted in BERT is a special case of our learning algorithm where the depth of meta-train is zero. We study the problem in two settings: unsupervised pre-training and supervised pre-training with different pre-training objects to verify the generality of our approach.Experimental results show that our algorithm brings improvements and learns better initializations for a variety of downstream tasks."", 'corpus_id': 215745174, 'score': 1}, {'doc_id': '202577430', 'title': 'RuDaS: Synthetic Datasets for Rule Learning and Evaluation Tools', 'abstract': ""Logical rules are a popular knowledge representation language in many domains, representing background knowledge and encoding information that can be derived from given facts in a compact form. However, rule formulation is a complex process that requires deep domain expertise,and is further challenged by today's often large, heterogeneous, and incomplete knowledge graphs. Several approaches for learning rules automatically, given a set of input example facts,have been proposed over time, including, more recently, neural systems. Yet, the area is missing adequate datasets and evaluation approaches: existing datasets often resemble toy examples that neither cover the various kinds of dependencies between rules nor allow for testing scalability. We present a tool for generating different kinds of datasets and for evaluating rule learning systems, including new performance measures."", 'corpus_id': 202577430, 'score': 1}]"
17	{'doc_id': '26007053', 'title': 'Affinity-based drug delivery systems for tissue repair and regeneration.', 'abstract': 'Affinity-based release systems use transient interactions to sustain and control the release of a therapeutic from a polymeric matrix. The most common affinity-based systems use heparin-based scaffolds to sustain the release of heparin-binding proteins, such as fibroblast growth factor-2 (FGF2) and vascular endothelial growth factor (VEGF). However, novel affinity-based systems based on, for example, protein-protein or DNA-protein interactions, are emerging to control the release of an expanding repertoire of therapeutics. Mathematical models of affinity-based systems have provided a thorough understanding of which parameters affect release rate from these systems, and how these release rates can be tuned. In this review, recent affinity-based release systems will be described, including an overview of the various types of affinity interactions used to modulate release, the mechanisms by which release from these systems is tuned, and the time scales of sustained release. This advanced drug delivery paradigm provides tunable and predictable release rates and has expanded the scope of deliverable therapeutics for tissue repair and regeneration.', 'corpus_id': 26007053}	2620	[{'doc_id': '214137811', 'title': 'Nanotechnology and sialic acid biology', 'abstract': '\n Abstract\n \n Nanotechnology is the science of matter at size in a scale of 1/1,000,000,000 of a meter. In the last century, considerable progress has been made in the field of nanotechnology and its finds application in major spheres of human life encompassing personal products, medicines, biosensors, disease diagnosis, food, chemicals, energy, agriculture, and industry with application in human health and environment. Nanotechnology application to glycobiology is a considerable new development. In the recent times, nanotechnology finds promising applications in the study and targeting of sialic acid encompassing (i) detection of sialic acids in minute quantities and (ii) enabling their targeting in diseases. We discuss in this chapter the recent advances in the application of nanotechnology to sialic acid biology as (i) imaging agents, (ii) spectroscopic tools for their detection, (iii) monitoring of cellular systems, and (iv) application in drug delivery.\n \n', 'corpus_id': 214137811, 'score': 0}, {'doc_id': '214726225', 'title': 'The first-in-class peptide binder to the SARS-CoV-2 spike protein', 'abstract': 'Coronavirus disease 19 (COVID-19) is an emerging global health crisis. With over 200,000 confirmed cases to date, this pandemic continues to expand, spurring research to discover vaccines and therapies. SARS-CoV-2 is the novel coronavirus responsible for this disease. It initiates entry into human cells by binding to angiotensin-converting enzyme 2 (ACE2) via the receptor binding domain (RBD) of its spike protein (S). Disrupting the SARS-CoV-2-RBD binding to ACE2 with designer drugs has the potential to inhibit the virus from entering human cells, presenting a new modality for therapeutic intervention. Peptide-based binders are an attractive solution to inhibit the RBD-ACE2 interaction by adequately covering the extended protein contact interface. Using molecular dynamics simulations based on the recently solved ACE2 and SARS-CoV-2-RBD co-crystal structure, we observed that the ACE2 peptidase domain (PD) α1 helix is important for binding SARS-CoV-2-RBD. Using automated fast-flow peptide synthesis, we chemically synthesized a 23-mer peptide fragment of the ACE2 PD α1 helix composed entirely of proteinogenic amino acids. Chemical synthesis of this human derived sequence was complete in 1.5 hours and after work up and isolation >20 milligrams of pure material was obtained. Bio-layer interferometry revealed that this peptide specifically associates with the SARS-CoV-2-RBD with low nanomolar affinity. This peptide binder to SARS-CoV-2-RBD provides new avenues for COVID-19 treatment and diagnostic modalities by blocking the SARS-CoV-2 spike protein interaction with ACE2 and thus precluding virus entry into human cells.', 'corpus_id': 214726225, 'score': 1}, {'doc_id': '214763496', 'title': 'Site-specific analysis of the SARS-CoV-2 glycan shield', 'abstract': 'The emergence of the betacoronavirus, SARS-CoV-2 that causes COVID-19, represents a significant threat to global human health. Vaccine development is focused on the principal target of the humoral immune response, the spike (S) glycoprotein, that mediates cell entry and membrane fusion. SARS-CoV-2 S gene encodes 22 N-linked glycan sequons per protomer, which likely play a role in immune evasion and occluding immunogenic protein epitopes. Here, using a site-specific mass spectrometric approach, we reveal the glycan structures on a recombinant SARS-CoV-2 S immunogen. This analysis enables mapping of the glycan-processing states across the trimeric viral spike. We show how SARS-CoV-2 S glycans differ from typical host glycan processing, which may have implications in viral pathobiology and vaccine design.', 'corpus_id': 214763496, 'score': 0}, {'doc_id': '53292145', 'title': 'Controlled release strategy designed for intravitreal protein delivery to the retina', 'abstract': '&NA; Therapeutic protein delivery directly to the eye is a promising strategy to treat retinal degeneration; yet, the high risks of local drug overdose and cataracts associated with bolus injection have limited progress, requiring the development of sustained protein delivery strategies. Since the vitreous humor itself is a gel, hydrogel‐based release systems are a sensible solution for sustained intravitreal protein delivery. Using ciliary neurotrophic factor (CNTF) as a model protein for ocular treatment, we investigated the use of an intravitreal, affinity‐based release system for protein delivery. To sustain CNTF release, we took advantage of the affinity between Src homology 3 (SH3) and its peptide binding partners: CNTF was expressed as a fusion protein with SH3, and a thermogel of hyaluronan and methylcellulose (HAMC) was modified with SH3 binding peptides. Using a mathematical model, the hydrogel composition was successfully designed to release CNTF‐SH3 over 7 days. The stability and bioactivity of the released protein were similar to those of commercial CNTF. Intravitreal injections of the bioengineered thermogel showed successful delivery of CNTF‐SH3 to the mouse retina, with expected transient downregulation of phototransduction genes (e.g., rhodopsin, S‐opsin, M‐opsin, Gnat 1 and 2), upregulation of STAT1 and STAT3 expression, and upregulation of STAT3 phosphorylation. This constitutes the first demonstration of intravitreal protein release from a hydrogel. Immunohistochemical analysis of the retinal tissues of injected eyes confirmed the biocompatibility of the delivery vehicle, paving the way towards new intravitreal protein delivery strategies.', 'corpus_id': 53292145, 'score': 1}, {'doc_id': '214746186', 'title': 'Computational simulations reveal the binding dynamics between human ACE2 and the receptor binding domain of SARS-CoV-2 spike protein', 'abstract': 'A novel coronavirus (the SARS-CoV-2) has been identified in January 2020 as the causal pathogen for COVID-19 pneumonia, an outbreak started near the end of 2019 in Wuhan, China. The SARS-CoV-2 was found to be closely related to the SARS-CoV, based on the genomic analysis. The Angiotensin converting enzyme 2 protein (ACE2) utilized by the SARS-CoV as a receptor was found to facilitate the infection of SARS-CoV-2 as well, initiated by the binding of the spike protein to the human ACE2. Using homology modeling and molecular dynamics (MD) simulation methods, we report here the detailed structure of the ACE2 in complex with the receptor binding domain (RBD) of the SARS-CoV-2 spike protein. The predicted model is highly consistent with the experimentally determined complex structures. Plausible binding modes between human ACE2 and the RBD were revealed from all-atom MD simulations. The simulation data further revealed critical residues at the complex interface and provided more details about the interactions between the SARS-CoV-2 RBD and human ACE2. Two mutants mimicking rat ACE2 were modeled to study the mutation effects on RBD binding to ACE2. The simulations showed that the N-terminal helix and the K353 of the human ACE2 alter the binding modes of the CoV2-RBD to the ACE2.', 'corpus_id': 214746186, 'score': 0}, {'doc_id': '214763629', 'title': 'Structure-based modeling of SARS-CoV-2 peptide/HLA-A02 antigens', 'abstract': 'As a first step toward the development of diagnostic and therapeutic tools to fight the Coronavirus disease (COVID-19), we aim to characterize CD8+ T cell epitopes in the SARS-CoV-2 peptidome that can trigger adaptive immune responses. Here, we use RosettaMHC, a comparative modeling approach which leverages existing high-resolution X-ray structures from peptide/MHC complexes available in the Protein Data Bank, to derive physically realistic 3D models for high-affinity SARS-CoV-2 epitopes. We outline an application of our method to model 439 9mer and 279 10mer predicted epitopes displayed by the common allele HLA-A*02:01, and we make our models publicly available through an online database (https://rosettamhc.chemistry.ucsc.edu). As more detailed studies on antigen-specific T cell repertoires become available, RosettaMHC models of antigens from different strains and HLA alleles can be used as a basis to understand the link between peptide/HLA complex structure and surface chemistry with immunogenicity, in the context of SARS-CoV-2 infection.', 'corpus_id': 214763629, 'score': 0}, {'doc_id': '11444828', 'title': 'Tunable Growth Factor Delivery from Injectable Hydrogels for Tissue Engineering', 'abstract': 'Current sustained delivery strategies of protein therapeutics are limited by the fragility of the protein, resulting in minimal quantities of bioactive protein delivered. In order to achieve prolonged release of bioactive protein, an affinity-based approach was designed which exploits the specific binding of the Src homology 3 (SH3) domain with short proline-rich peptides. Specifically, methyl cellulose was modified with SH3-binding peptides (MC-peptide) with either a weak affinity or strong affinity for SH3. The release profile of SH3-rhFGF2 fusion protein from hyaluronan MC-SH3 peptide (HAMC-peptide) hydrogels was investigated and compared to unmodified controls. SH3-rhFGF2 release from HAMC-peptide was extended to 10 days using peptides with different binding affinities compared to the 48 h release from unmodified HAMC. This system is capable of delivering additional proteins with tunable rates of release, while maintaining bioactivity, and thus is broadly applicable.', 'corpus_id': 11444828, 'score': 1}, {'doc_id': '59340783', 'title': 'Local delivery of stabilized chondroitinase ABC degrades chondroitin sulfate proteoglycans in stroke‐injured rat brains', 'abstract': '&NA; Central nervous system (CNS) injuries, such as stroke and spinal cord injuries, result in the formation of a proteoglycan‐rich glial scar, which acts as a barrier to axonal regrowth and limits the regenerative capacity of the CNS. Chondroitinase ABC (ChABC) is a potent bacterial enzyme that degrades the chondroitin sulfate proteoglycan (CSPG) component of the glial scar and promotes tissue recovery; however, its use is significantly limited by its inherent instability at physiological temperatures. Here, we demonstrate that ChABC can be stabilized using site‐directed mutagenesis and covalent modification with poly(ethylene glycol) chains (i.e. PEGylation). Rosetta protein structure modeling was used to screen >20,000 single point mutations, and four potentially stabilizing mutations were tested in vitro. One of the mutations, N1000G (asparagine → glycine at residue 1000), significantly improved the long‐term activity of the protein, doubling its functional half‐life. PEGylation of this ChABC mutant inhibited unfolding and aggregation and resulted in prolonged bioactivity with a 10‐fold increase in activity compared to the unmodified protein after two days. Local, affinity‐controlled release of the modified protein (PEG‐N1000G‐ChABC) was achieved by expressing it as a fusion protein with Src homology 3 (SH3) and delivering the protein from a methylcellulose hydrogel modified with SH3 binding peptides. This affinity‐based release strategy provided sustained PEG‐N1000G‐ChABC‐SH3 release over several days in vitro. Direct implantation of the hydrogel delivery vehicle containing stabilized PEG‐N1000G‐ChABC‐SH3 onto the rat brain cortex in a sub‐acute model of stroke resulted in significantly reduced CSPG levels in the penumbra of 49% at 14 and 40% at 28 days post‐injury compared to animals treated with the vehicle alone. Graphical abstract Figure. No caption available. HighlightsComputationally‐predicted point mutation improves chondroitinase ABC bioactivity.PEGylation of chondroitinase ABC improves stability and bioactivity.Stabilized chABC reduces peri‐lesional proteoglycans in the stroke‐injured rat brain.', 'corpus_id': 59340783, 'score': 1}, {'doc_id': '210912540', 'title': 'Injectable hydrogel enables local and sustained co-delivery to the brain: Two clinically approved biomolecules, cyclosporine and erythropoietin, accelerate functional recovery in rat model of stroke.', 'abstract': 'Therapeutic delivery to the brain is limited by the blood-brain barrier and is exacerbated by off-target effects associated with systemic delivery, thereby precluding many potential therapies from even being tested. Given the systemic side effects of cyclosporine and erythropoietin, systemic administration would be precluded in the context of stroke, leaving only the possibility of local delivery. We wondered if direct delivery to the brain would allow new reparative therapeutics, such as these, to be identified for stroke. Using a rodent model of stroke, we employed an injectable drug delivery hydrogel strategy to circumvent the blood-brain barrier and thereby achieved, for the first time, local and sustained co-release to the brain of cyclosporine and erythropoietin. Both drugs diffused to the sub-cortical neural stem and progenitor cell (NSPC) niche and were present in the brain for at least 32 days post-stroke. Each drug had a different outcome on brain tissue: cyclosporine increased plasticity in the striatum while erythropoietin stimulated endogenous NSPCs. Only their co-delivery, but not either drug alone, accelerated functional recovery and improved tissue repair. This platform opens avenues for hitherto untested therapeutic combinations to promote regeneration and repair after stroke.', 'corpus_id': 210912540, 'score': 1}, {'doc_id': '210951307', 'title': 'The versatile biomedical applications of bismuth-based nanoparticles and composites: therapeutic, diagnostic, biosensing, and regenerative properties.', 'abstract': 'Studies of nanosized forms of bismuth (Bi)-containing materials have recently expanded from optical, chemical, electronic, and engineering fields towards biomedicine, as a result of their safety, cost-effective fabrication processes, large surface area, high stability, and high versatility in terms of shape, size, and porosity. Bi, as a nontoxic and inexpensive diamagnetic heavy metal, has been used for the fabrication of various nanoparticles (NPs) with unique structural, physicochemical, and compositional features to combine various properties, such as a favourably high X-ray attenuation coefficient and near-infrared (NIR) absorbance, excellent light-to-heat conversion efficiency, and a long circulation half-life. These features have rendered bismuth-containing nanoparticles (BiNPs) with desirable performance for combined cancer therapy, photothermal and radiation therapy (RT), multimodal imaging, theranostics, drug delivery, biosensing, and tissue engineering. Bismuth oxyhalides (BiOx, where X is Cl, Br or I) and bismuth chalcogenides, including bismuth oxide, bismuth sulfide, bismuth selenide, and bismuth telluride, have been heavily investigated for therapeutic purposes. The pharmacokinetics of these BiNPs can be easily improved via the facile modification of their surfaces with biocompatible polymers and proteins, resulting in enhanced colloidal stability, extended blood circulation, and reduced toxicity. Desirable antibacterial effects, bone regeneration potential, and tumor growth suppression under NIR laser radiation are the main biomedical research areas involving BiNPs that have opened up a new paradigm for their future clinical translation. This review emphasizes the synthesis and state-of-the-art progress related to the biomedical applications of BiNPs with different structures, sizes, and compositions. Furthermore, a comprehensive discussion focusing on challenges and future opportunities is presented.', 'corpus_id': 210951307, 'score': 0}]
18	{'doc_id': '233536610', 'title': 'Development of simplified models for nondestructive testing of rice (with husk) protein content using hyperspectral imaging technology', 'abstract': 'Abstract The study aimed to establish predictive models of protein content in rice (with husk) using a hyperspectral imaging (HSI) system in a collection of 87 rice varieties in China in the wavelength range of 938–2215\u2009nm and the first established multivariate calibration models over the full wavelength range by using partial least-square regression (PLSR), principal component regression (PCR) and least-square support vector regression (LS-SVR). In predictive model optimisation, the optimal wavelengths were selected by using regression coefficients (RC) as discriminating factors to establish PLSR models. According to the RC of models, the optimal wavelength combination was 17 and 7 characteristics. The model based on the 17-characteristic wavelengths was determined as the optimal optimisation model with coefficients of determination for prediction of 0.8011 and root mean square error of prediction of 0.52. The mapping of protein content was achieved by transferring a quantitative model to each pixel. According to the visualisation image, the distribution of rice protein could be understood, thus realising the possibility of on-line detection of protein content by using HSI technology.', 'corpus_id': 233536610}	17836	"[{'doc_id': '234838471', 'title': 'Food and agro-product quality evaluation based on spectroscopy and deep learning: A review', 'abstract': 'Abstract Background Rapid and non-destructive infrared spectroscopy has been applied to both internal and external quality evaluations of food and agro-products. Various linear and nonlinear chemometric methods have been developed for spectral analysis. The generalizability of previous chemometric methods is hindered by changing noise under various detection conditions and biological variabilities. Recently, deep learning approaches have been developed for spectral noise reduction, feature extraction, and calibration regression modeling. Scope and approach This review discusses the current challenges of conventional chemometric methods and the emerging deep learning approach for spectral analysis. The current state-of-the-art techniques, including unsupervised feature extraction and noise reduction models and supervised multivariate regression approaches, have been addressed in this review. The research on exploring the learning mechanism of the ‘black box’ deep learning model is also discussed. This review focuses on the application of deep learning approaches on quality evaluation of food and agro-products, lessons from current studies, and future perspectives. Key findings and conclusions The deep learning approach combined with spectroscopic sensing techniques has shown great potential for quality evaluation of food and agro-products. Current advances in deep learning-based qualitative analysis include variety identification, geographical origin detection, adulteration recognition, and bruise detection, whereas quantitative analysis includes multiple component content prediction for fruits, grains, and crops. The main advantage of deep learning approach is the decreasing the dependence on human domain knowledge by end-to-end analysis and the improved precision and generalizability.', 'corpus_id': 234838471, 'score': 0}, {'doc_id': '235531063', 'title': 'Soil organic carbon estimation using VNIR–SWIR spectroscopy: The effect of multiple sensors and scanning conditions', 'abstract': 'Abstract Visible–near infrared–shortwave infrared (VNIR–SWIR) spectroscopy is being increasingly used for soil organic carbon (SOC) assessment. Common practice consists of scanning soil samples using a single spectrometer. Considerations have rarely been documented of the effects of using multiple instruments and scanning conditions on SOC model calibration that occur when merging soil spectral libraries (SSLs), particularly in soils with low SOC concentration and using both field spectroradiometers and laboratory fixed spectrometers. To address this gap, we scanned 143 low-SOC-content soil samples using three spectrometers (ASD FieldSpec 3, ASD FieldSpec 4, and FOSS XDS) and four setup features - FOSS, contact probe (CP), dark-box (DB), and open laboratory (LAB) - at three laboratories. The application of an internal soil standard (ISS) to align one laboratory spectrum with another for spectral correction and spectral merging of various SSLs was examined. SOC models were developed using i) data from each single spectrometer – single laboratory separately and ii) merged data from multiple spectrometers – different laboratories, applying the 1st derivatives of spectra and random forest (RF) regression. The results indicate that the spectral shape and wavelength position of key features obtained from all spectrometers and setups did not show any noticeable differences, though spectra based on FOSS setup, particularly on low-SOC samples, demonstrated greater range in absolute derivative values regardless of ISS application. The derivative ISS-corrected spectra showed less variation among different spectrometers compared to their uncorrected raw reflectance spectra. All single spectrometer models predicted SOC reasonably well. However, the spectra acquired by the FOSS setup predicted SOC more accurately (R2 = 0.77, RPIQ = 3.30, RMSE = 0.22 %, and SD = 0.04) than the spectra acquired by the other setups. The models derived from merged uncorrected raw reflectance spectra yielded poor results (R2 = 0.48, RPIQ = 2.33, RMSE = 0.33 %, and SD = 0.10); nevertheless, assessment of SOC using the 1st derivative ISS-corrected merged SSLs considerably improved the prediction accuracy (R2 = 0.70, RPIQ = 3.10, RMSE = 0.25 %, and SD = 0.06). Hence, the derivative spectra coupled with the ISS correction improved the accuracy of SOC prediction models obtained from the merged soil spectra collected in different environments using different instruments. We therefore recommend application of the ISS spectral alignment method linked to the 1st derivative approach to enhance the compilation of SSLs at the regional and global scales for SOC assessment.', 'corpus_id': 235531063, 'score': 1}, {'doc_id': '233584607', 'title': 'Characterization of field-scale soil variation using a stepwise multi-sensor fusion approach and a cost-benefit analysis', 'abstract': 'Abstract The potential of a stepwise fusion of proximally sensed portable X-ray fluorescence (pXRF) spectra and electromagnetic induction (EMI) with remote Sentinel-2 bands and a digital elevation model (DEM) was investigated for predicting soil physicochemical properties in pedons and across a heterogeneous 80-ha crop field in Wisconsin, USA. We found that pXRF spectra with partial least squares regression (PLSR) models can predict sand, total nitrogen (TN), organic carbon (OC), silt contents, and clay with validation R2 of 0.81, 0.74, 0.73, 0.68, and 0.64 at the pedon scale but performed less well for soil pH (R2\xa0=\xa00.51). A combination of EMI, Sentinel-2, and DEM data showed promise in mapping sand, silt contents, and TN at two depths and Ap horizon thickness and soil depth across the field. A clustering analysis using combinations of mapped soil properties or proximal and remote sensing data suggested that data fusion improved the characterization of field-scale variability of soil properties. The cost-benefit analysis showed that the most accurate management zones (MZs) for topsoil can be generated only using estimated soil property maps while it was the most costly as compared to other data sources. For an intermediate-high (for topsoil) and high (subsoil) accuracy and a moderate economic budget, the combination of sensors (proximal\xa0+\xa0remote sensing\xa0+\xa0DEM) might be a better approach for effective MZs generation than collecting soil samples for laboratory analysis while the latter produced the most accurate maps for topsoil. It can be concluded that pXRF spectra can be useful for predicting key soil properties (e.g., sand, TN, OC, silt, clay) at different soil depths, and a combination of proximal and remote sensing provides an effective way to delineate soil MZs that are useful for decision-making.', 'corpus_id': 233584607, 'score': 1}, {'doc_id': '235364221', 'title': 'High-resolution spectral information enables phenotyping of leaf epicuticular wax in wheat', 'abstract': 'Background Epicuticular wax (EW) is the first line of defense in plants for protection against biotic and abiotic factors in the environment. In wheat, EW is associated with resilience to heat and drought stress, however, the current limitations on phenotyping EW restrict the integration of this secondary trait into wheat breeding pipelines. In this study we evaluated the use of light reflectance as a proxy for EW load and developed an efficient indirect method for the selection of genotypes with high EW density. Results Cuticular waxes affect the light that is reflected, absorbed and transmitted by plants. The narrow spectral regions statistically associated with EW overlap with bands linked to photosynthetic radiation (500\xa0nm), carotenoid absorbance (400\xa0nm) and water content (~\u2009900\xa0nm) in plants. The narrow spectral indices developed predicted 65% (EWI-13) and 44% (EWI-1) of the variation in this trait utilizing single-leaf reflectance. However, the normalized difference indices EWI-4 and EWI-9 improved the phenotyping efficiency with canopy reflectance across all field experimental trials. Indirect selection for EW with EWI-4 and EWI-9 led to a selection efficiency of 70% compared to phenotyping with the chemical method. The regression model EWM-7 integrated eight narrow wavelengths and accurately predicted 71% of the variation in the EW load (mg·dm −2 ) with leaf reflectance, but under field conditions, a single-wavelength model consistently estimated EW with an average RMSE of 1.24\xa0mg·dm −2 utilizing ground and aerial canopy reflectance. Conclusions Overall, the indices EWI-1, EWI-13 and the model EWM-7 are reliable tools for indirect selection for EW based on leaf reflectance, and the indices EWI-4, EWI-9 and the model EWM-1 are reliable for selection based on canopy reflectance. However, further research is needed to define how the background effects and geometry of the canopy impact the accuracy of these phenotyping methods.', 'corpus_id': 235364221, 'score': 1}, {'doc_id': '235241694', 'title': 'Unveiling spatial variability in herbicide soil sorption using bayesian digital mapping.', 'abstract': 'Regional mapping herbicide sorption to soil is essential for risk assessment. However, conducting analytical quantification of adsorption coefficient (Kd) in large scale studies is too costly; therefore, a research question arises on goodness of Kd spatial prediction from sampling. The application of a spatial Bayesian regression (BR) is a newer technique in agricultural and natural resources sciences that allows converting spatially discrete samples into maps covering continuous spatial domains. The objective of this work was to unveil herbicide sorption to soil at a landscape scale by developing a predictive BR model. We integrated a large set of ancillary soil and climate covariables from sites with Kd measurements into a spatial mixed model including site random effects. The models were fitted using glyphosate and atrazine Kds, determined in 80 and 120 sites from central Argentina. For model assessment, measurements of global and point-wise prediction errors were obtained by cross-validation; residual variability was estimated by bootstrap to compare BR with regression kriging (RK). Results showed that the BR spatial predictions outperformed RK. The glyphosate Kd model (RMSPE: 13% of the mean) included aluminum oxides, pH, and clay content, whereas the atrazine Kd model strongly depended on soil organic carbon and clay, as well as on climatic variables related to water availability (RMSPE: 27%). Spatial modelling of a complex edaphic process as herbicide sorption to soils enhanced environmental interpretations. An efficient approach for spatial mapping provides a modern perspective on the study of herbicide sorption to soil. This article is protected by copyright. All rights reserved.', 'corpus_id': 235241694, 'score': 0}, {'doc_id': '235288652', 'title': 'A Wavelength Selection Model Based on Successive Projections Algorithm for pH Detection of Water by VIS-NIR Spectroscopy', 'abstract': 'A novel pH detection method using VIS-NIR (visible near infrared) spectroscopy for water is introduced in this paper. In order to improve prediction accuracy and calculation speed of pH detection using VIS-NIR spectroscopy, a wavelength selection model of SPA-MLR (successive projections algorithm and multiple linear regression) is proposed. Two experiments (pH measurement experiment by VIS-NIR spectrophotometer and pH measurement experiment by VIS-NIR grating spectrograph) are employed to evaluate the performance of SPA-MLR. At meanwhile, PLS (partial least squares), GA-PLS (genetic algorithm and partial least squares) are introduced to compare with SPA-MLR. The results indicate that, the prediction accuracy and calculation speed of SPA-MLR are all better than that of PLS and GA-PLS. Under the pH measurement experiment using VIS-NIR spectrophotometer, only one wavelength is selected from 351 original wavelengths for calibration, the RMSEP (root mean square errors of prediction) is 0.35, and the model training speed is 2.36s. Under the pH measurement experiment using VIS-NIR grating spectrograph, 3 wavelengths are selected from 2860 original wavelengths for calibration, the RMSEP is 0.50, and the model training speed is 56.45s. For the two experiments, the selected wavelength variables by SPA are distributed mainly in the band of 870 nm to 990 nm.', 'corpus_id': 235288652, 'score': 0}, {'doc_id': '235553625', 'title': 'Application of Genetic Algorithm-Multiple Linear Regression and Artificial Neural Network Determinations for Prediction of Kovats Retention Index', 'abstract': ""This study aims to compare linear and nonlinear prediction methods in predicting the Kovats retention index of 126 compounds extracted from the Lippia origanoides plant. The retention index of each compound has been predicted based on its molecular descriptors. There have been 189 molecular descriptors for each compound in this study, and the best descriptors have been selected using the Genetic Algorithm (GA). It has succeeded in selecting the best five descriptors used to build the Multiple Linear Regression (MLR) and the Artificial Neural Network (ANN) model. MLR has obtained R2 of 0.959, 0.946, 0.955, and a Root Mean Square Error (RMSE) of 48.00, 50.84, 47.19 on training, validation, and testing, respectively. Meanwhile, ANN has obtained R2 of 0.963, 0.947, 0.962, and an RMSE of 45.45, 50.59, 43.20, respectively. Compared to MLR, there has been an increase in the ANN model's performance, with an increase in R2 of 0.004, 0.001, and 0.007 and a decrease in RMSE of 2.55, 0.25, and 3.99. Based on the prediction results obtained, it is known that in this case, the ANN method can provide better predictive results than MLR."", 'corpus_id': 235553625, 'score': 0}, {'doc_id': '235716344', 'title': 'Estimation of Bare Soil Moisture from Remote Sensing Indices in the 0.4–2.5 mm Spectral Range', 'abstract': 'Abstract Soil moisture content (SMC) is an important element of the environment, influencing water availability for plants and atmospheric parameters, and its monitoring is important for predicting floods or droughts and for weather and climate modeling. Optical methods for measuring soil moisture use spectral reflection analysis in the 350–2500 nm range. Remote sensing is considered to be an effective tool for monitoring soil parameters over large areas and to be more cost effective than in situ measurements. The aim of this study was to assess the SMC of bare soil on the basis of hyperspectral data from the ASD FieldSpec 4 Hi-Res field spectrometer by determining remote sensing indices and visualization based on multispectral data obtained from UAVs. Remote sensing measurements were validated on the basis of field humidity measurements with the HH2 Moisture Meter and ML3 ThetaProbe Soil Moisture Sensor. A strong correlation between terrestrial and remote sensing data was observed for 7 out of 11 selected indexes and the determination coefficient R2 values ranged from 67%– 87%. The best results were obtained for the NINSON index, with determination coefficient values of 87%, NSMI index (83.5%) and NINSOL (81.7%). We conclude that both hyperspectral and multispectral remote sensing data of bare soil moisture are valuable, providing good temporal and spatial resolution of soil moisture distribution in local areas, which is important for monitoring and forecasting local changes in climate.', 'corpus_id': 235716344, 'score': 1}, {'doc_id': '235450820', 'title': 'A best-practice guide to predicting plant traits from leaf-level hyperspectral data using partial least squares regression.', 'abstract': 'Partial least squares regression (PLSR) modelling is a statistical technique for correlating datasets, and involves the fitting of a linear regression between two matrices. One application of PLSR enables leaf traits to be estimated from hyperspectral optical reflectance data, facilitating rapid, high-throughput, non-destructive plant phenotyping. This technique is of interest and importance in a wide range of contexts including crop breeding and ecosystem monitoring. The lack of a consensus in the literature on how to perform PLSR means that interpreting model results can be challenging, applying existing models to novel datasets can be impossible, and unknown or undisclosed assumptions can lead to incorrect or spurious predictions. We address this lack of consensus by proposing best practices for using PLSR to predict plant traits from leaf-level hyperspectral data, including a discussion of when PLSR is applicable, and recommendations for data collection. We provide a tutorial to demonstrate how to develop a PLSR model, in the form of an R script accompanying this manuscript. This practical guide will assist all those interpreting and using PLSR models to predict leaf traits from spectral data, and advocates for a unified approach to using PLSR for predicting traits from spectra in the plant sciences.', 'corpus_id': 235450820, 'score': 0}, {'doc_id': '235651515', 'title': 'Can Agricultural Management Induced Changes in Soil Organic Carbon Be Detected Using Mid-Infrared Spectroscopy?', 'abstract': 'A major limitation to building credible soil carbon sequestration programs is the cost of measuring soil carbon change. Diffuse reflectance spectroscopy (DRS) is considered a viable low-cost alternative to traditional laboratory analysis of soil organic carbon (SOC). While numerous studies have shown that DRS can produce accurate and precise estimates of SOC across landscapes, whether DRS can detect subtle management induced changes in SOC at a given site has not been resolved. Here, we leverage archived soil samples from seven long-term research trials in the U.S. to test this question using mid infrared (MIR) spectroscopy coupled with the USDA-NRCS Kellogg Soil Survey Laboratory MIR spectral library. Overall, MIR-based estimates of SOC%, with samples scanned on a secondary instrument, were excellent with the root mean square error ranging from 0.10 to 0.33% across the seven sites. In all but two instances, the same statistically significant (p < 0.10) management effect was found using both the lab-based SOC% and MIR estimated SOC% data. Despite some additional uncertainty, primarily in the form of bias, these results suggest that large existing MIR spectral libraries can be operationalized in other laboratories for successful carbon monitoring.', 'corpus_id': 235651515, 'score': 1}]"
19	{'doc_id': '214385109', 'title': 'Automated continuous evolution of proteins in vivo', 'abstract': 'We present automated continuous evolution (ACE), a platform for the hands-free directed evolution of biomolecules. ACE pairs OrthoRep, a genetic system for continuous targeted mutagenesis of user-selected genes in vivo, with eVOLVER, a scalable and automated continuous culture device for precise, multi-parameter regulation of growth conditions. By implementing real-time feedback-controlled tuning of selection stringency with eVOLVER, genes of interest encoded on OrthoRep autonomously traversed multi-mutation adaptive pathways to reach desired functions, including drug resistance and improved enzyme activity. The durability, scalability, and speed of biomolecular evolution with ACE should be broadly applicable to protein engineering as well as prospective studies on how selection parameters and schedules shape adaptation.', 'corpus_id': 214385109}	16591	"[{'doc_id': '235901317', 'title': 'A Chinese hamster transcription start site atlas that enables targeted editing of CHO cells', 'abstract': 'Abstract Chinese hamster ovary (CHO) cells are widely used for producing biopharmaceuticals, and engineering gene expression in CHO is key to improving drug quality and affordability. However, engineering gene expression or activating silent genes requires accurate annotation of the underlying regulatory elements and transcription start sites (TSSs). Unfortunately, most TSSs in the published Chinese hamster genome sequence were computationally predicted and are frequently inaccurate. Here, we use nascent transcription start site sequencing methods to revise TSS annotations for 15 308 Chinese hamster genes and 3034 non-coding RNAs based on experimental data from CHO-K1 cells and 10 hamster tissues. We further capture tens of thousands of putative transcribed enhancer regions with this method. Our revised TSSs improves upon the RefSeq annotation by revealing core sequence features of gene regulation such as the TATA box and the Initiator and, as exemplified by targeting the glycosyltransferase gene Mgat3, facilitate activating silent genes by CRISPRa. Together, we envision our revised annotation and data will provide a rich resource for the CHO community, improve genome engineering efforts and aid comparative and evolutionary studies.', 'corpus_id': 235901317, 'score': 1}, {'doc_id': '235798222', 'title': 'OnePot PURE Cell-Free System.', 'abstract': 'The defined PURE (protein synthesis using recombinant elements) transcription-translation system provides an appealing chassis for cell-free synthetic biology. Unfortunately, commercially available systems are costly, and their tunability is limited. In comparison, a home-made approach can be customized based on user needs. However, the preparation of home-made systems is time-consuming and arduous due to the need for ribosomes as well as 36 medium scale protein purifications. Streamlining protein purification by coculturing and co-purification allows for minimizing time and labor requirements. Here, we present an easy, adjustable, time- and cost-effective method to produce all PURE system components within 1 week, using standard laboratory equipment. Moreover, the performance of the OnePot PURE is comparable to commercially available systems. The OnePot PURE preparation method expands the accessibility of the PURE system to more laboratories due to its simplicity and cost-effectiveness.', 'corpus_id': 235798222, 'score': 0}, {'doc_id': '235450328', 'title': 'Capsid Proteins Are Necessary for Replication of a Parvovirus', 'abstract': 'Densoviruses could be used as biological control agents to manage insect pests. Such applications require an in-depth biological understanding and associated molecular tools. ABSTRACT Despite tight genetic compression, viral genomes are often organized into functional gene clusters, a modular structure that might favor their evolvability. This has greatly facilitated biotechnological developments such as the recombinant adeno-associated virus (AAV) systems for gene therapy. Following this lead, we endeavored to engineer the related insect parvovirus Junonia coenia densovirus (JcDV) to create addressable vectors for insect pest biocontrol. To enable safer manipulation of capsid mutants, we translocated the nonstructural (ns) gene cluster outside the viral genome. To our dismay, this yielded a virtually nonreplicable clone. We linked the replication defect to an unexpected modularity breach, as ns translocation truncated the overlapping 3′ untranslated region (UTR) of the capsid transcript (vp). We found that the native vp 3′ UTR is necessary for high-level VP production but that decreased expression does not adversely impact the expression of NS proteins, which are known replication effectors. As nonsense vp mutations recapitulate the replication defect, VP proteins appear to be directly implicated in the replication process. Our findings suggest intricate replication-encapsidation couplings that favor the maintenance of genetic integrity. We discuss possible connections with an intriguing cis-packaging phenomenon previously observed in parvoviruses whereby capsids preferentially package the genome from which they were expressed. IMPORTANCE Densoviruses could be used as biological control agents to manage insect pests. Such applications require an in-depth biological understanding and associated molecular tools. However, the genomes of these viruses remain difficult to manipulate due to poorly tractable secondary structures at their extremities. We devised a construction strategy that enables precise and efficient molecular modifications. Using this approach, we endeavored to create a split clone of Junonia coenia densovirus (JcDV) that can be used to safely study the impact of capsid mutations on host specificity. Our original construct proved to be nonfunctional. Fixing this defect led us to uncover that capsid proteins and their correct expression are essential for continued rolling-hairpin replication. This points to an intriguing link between replication and packaging, which might be shared with related viruses. This serendipitous discovery illustrates the power of synthetic biology approaches to advance our knowledge of biological systems.', 'corpus_id': 235450328, 'score': 0}, {'doc_id': '236475737', 'title': 'Tetravalent SARS-CoV-2 Neutralizing Antibodies Show Enhanced Potency and Resistance to Escape Mutations', 'abstract': '\n Neutralizing antibodies (nAbs) hold promise as therapeutics against COVID-19. Here, we describe protein engineering and modular design principles that have led to the development of synthetic bivalent and tetravalent nAbs against SARS-CoV-2. The best nAb targets the host receptor binding site of the viral S-protein and tetravalent versions block entry with a potency exceeding bivalent nAbs by an order of magnitude. Structural studies show that both the bivalent and tetravalent nAbs can make multivalent interactions with a single S-protein trimer, consistent with the avidity and potency of these molecules. Significantly, we show that the tetravalent nAbs show increased tolerance to potential virus escape mutants and an emerging variant of concern. Bivalent and tetravalent nAbs can be produced at large-scale and are as stable and specific as approved antibody drugs. Our results provide a general framework for enhancing antiviral therapies against COVID-19 and related viral threats, and our strategy can be applied to virtually any antibody drug.\n', 'corpus_id': 236475737, 'score': 0}, {'doc_id': '235893676', 'title': '1LC-6: FACS-based enzyme engineering for the enhanced production of biomolecules', 'abstract': 'Currently, proteins have indeed found significant applications in various bioprocesses but the full realization of their potential has been limited. To acquire the desired properties, proteins need to be engineered and, for this purpose, directed evolution techniques has been the most powerful tool. Directed evolution consists of an iterative two-step protocol, initially generating molecular diversity by random mutagenesis, then identifying library members with improvements in desired phenotype by screening or selection, and in many cases, the success of directed evolution highly rely on the employed screening strategy. Our research is also focused on the development of efficient strategy for high throughput screening and particularly, we have developed FACS-based high throughput screening in bacterial hosts. In this presentation, I will introduce new screening strategy for the engineering of industrial enzymes towards enhanced production of biomolecules.', 'corpus_id': 235893676, 'score': 0}, {'doc_id': '236987676', 'title': 'A proline metabolism selection system and its application to the engineering of lipid biosynthesis in Chinese hamster ovary cells', 'abstract': 'Chinese hamster ovary (CHO) cells are the leading mammalian cell host employed to produce complex secreted recombinant biotherapeutics such as monoclonal antibodies (mAbs). Metabolic selection marker technologies (e.g. glutamine synthetase (GS) or dihydrofolate reductase (DHFR)) are routinely employed to generate such recombinant mammalian cell lines. Here we describe the development of a selection marker system based on the metabolic requirement of CHO cells to produce proline, and that uses pyrroline-5-carboxylase synthetase (P5CS) to complement this auxotrophy. Firstly, we showed the system can be used to generate cells that have growth kinetics in proline-free medium similar to those of the parent CHO cell line, CHOK1SV GS-KO™ grown in proline-containing medium. As we have previously described how engineering lipid metabolism can be harnessed to enhance recombinant protein productivity in CHO cells, we then used the P5CS selection system to re-engineer lipid metabolism by over-expression of either sterol regulatory element binding protein 1 (SREBF1) or stearoyl CoA desaturase 1 (SCD1). The cells with re-engineered proline and lipid metabolism showed consistent growth and P5CS, SCD1 and SREBF1 expression across 100 cell generations. Finally, we show that the P5CS and GS selection systems can be used together. A GS vector containing the light and heavy chains for a mAb was super-transfected into a CHOK1SV GS-KO™ host over-expressing SCD1 from a P5CS vector. The resulting stable transfectant pools achieved a higher concentration at harvest for a model difficult to express mAb than the CHOK1SV GS-KO™ host. This demonstrates that the P5CS and GS selection systems can be used concomitantly to enable CHO cell line genetic engineering and recombinant protein expression.', 'corpus_id': 236987676, 'score': 1}, {'doc_id': '237403634', 'title': 'Artificial introns for effective expression of transgenes in mammalian cells', 'abstract': 'Introns are widely used in the assembly of genetic constructions expressing transgenic proteins in eukaryotic cells for the enhancement of this expression. However, the choice of introns that can be applied for such purposes is limited by the excessively large size of the majority of natural introns (several thousand nucleotides) and therefore they cannot be cloned in a genetic construction. With the help of site-directed mutagenesis we have generated a library of short (99 nucleotides long) introns. The efficiency of these introns in the enhancement of gene expression was analyzed. As a result, a set of 12 introns was selected. The generated intros can be used for genetic constructions with high expression level of recombinant proteins.', 'corpus_id': 237403634, 'score': 1}, {'doc_id': '236533388', 'title': 'Efficient utilization of available sequence space : generation of hAbs for medicine through consensus human Ab library and trinucleotide-based mutagenesis method', 'abstract': ""Human antibodies (hAbs) against various diseaserelated antigens are highly desirable but not always available. Using the combinatorial approach, it is possible to generate human antibody fragments against those antigens with relatively high affinity. However, this approach requires enormous libraries which are not always feasible. It is reasoned that such large-sized libraries are necessary to represent the true functional repertoires. We have undertaken two approaches to address this problem. First, the human antibody repertoire is composed of sequences that are closely related and their diversity has little to do with the binding affinity. We have compared the available human antibody sequences in the database and generated a set of human consensus sequences which consists of 7 VII, 4V(&-) and 3 V (lambda) chains. With this set of consensus sequences, it is now possible to generate the entire human antibody repertoire with limited framework diversity. Due to their complete synthetic nature, these consensus sequences are designed to increase their expressibility in E. coli and to facilitate site-directed and random mutagenesis to imitate the affinity maturation process in nature. Secondly, even with the optimized 'rationally' designed random mutagenesis methods, it is not possible to exclude completely: (a) stop codons, (b) structure breaking residues, (c) residues that are known to lead to non-functional antibodies. Random mutagenesis using the NNK approach which is the most prevalently employed method, generates all possible combinations of sequences but at the expense of the quality of the available sequence space. We have developed the trinucleotide-based mutagenesis method with which we introduced only the residues that are known to give rise to functional antibodies, and thus were able to concentrate on the sequence space which is known to be entirely functional. The results from both of these approaches will be presented and their implications in the field of high affinity human antibody in medicine will be discussed. Development and usage of semi-synthetic scFv repertoires displayed on phage"", 'corpus_id': 236533388, 'score': 1}, {'doc_id': '237349073', 'title': 'A Combinatorial PCR Method for Efficient, Selective Oligo Retrieval from Complex Oligo Pools', 'abstract': 'With the rapidly decreasing cost of array-based oligo synthesis, large-scale oligo pools offer significant benefits for advanced applications, including gene synthesis, CRISPR-based gene editing, and DNA data storage. Selectively retrieving specific oligos from these complex pools traditionally uses Polymerase Chain Reaction (PCR), in which any selected oligos are exponentially amplified to quickly outnumber non-selected ones. In this case, the number of orthogonal PCR primers is limited due to interactions between them. This lack of specificity presents a serious challenge, particularly for DNA data storage, where the size of an oligo pool (i.e., a DNA database) is orders of magnitude larger than it is for other applications. Although a nested file address system was recently developed to increase the number of accessible files for DNA storage, it requires a more complicated lab protocol and more expensive reagents to achieve high specificity. Instead, we developed a new combinatorial PCR method that outperforms prior work without compromising the fidelity of retrieved material or complicating wet lab processes. Our method quadratically increases the number of accessible oligos while maintaining high specificity. In experiments, we accessed three arbitrarily chosen files from a DNA prototype database that contained 81 different files. Initially comprising only 1% of the original database, the selected files were enriched to over 99.9% using our combinatorial primer method. Our method thus provides a viable path for scaling up DNA data storage systems and has broader utility whenever scientists need access to a specific target oligo and can design their own primer regions.', 'corpus_id': 237349073, 'score': 0}, {'doc_id': '237121998', 'title': 'Recent Advances in the Enabling Technologies for Synthetic Biology', 'abstract': 'As a new discipline with great potential, synthetic biology has rapidly developed mainly owing to the innovation and application of various enabling technologies. From the construction and standardization of biological parts to high-throughput microchip-based gene synthesis, convenient DNA assembly methods at different scales, and high-efficiency genome-editing tools, great progress has been made in the enabling technologies for synthetic biology. Furthermore, advances in numerous other disciplines such as genetics, cancer therapy, disease surveillance, and biomanufacturing are driven by these cutting-edge engineering techniques. If these aforementioned operating tools are viewed as accessories, the corresponding main equipment, the cellular chassis, has also been greatly developed in recent years. Bioinformatic analysis and large-scale deletions in microbial genomes provide a strong basis for constructing predictable and controllable chassis strains. In addition, mammalian cells have been optimized to construct excellent chassis cells for the treatment of human diseases based on cell therapy. This review discusses the currently available versatile enabling technologies as well as their increasing application and significance in life sciences research.', 'corpus_id': 237121998, 'score': 1}]"
20	{'doc_id': '211084773', 'title': 'Towards story-based classification of movie scenes', 'abstract': 'Humans are entertained and emotionally captivated by a good story. Artworks, such as operas, theatre plays, movies, TV series, cartoons, etc., contain implicit stories, which are conveyed visually (e.g., through scenes) and audially (e.g., via music and speech). Story theorists have explored the structure of various artworks and identified forms and paradigms that are common to most well-written stories. Further, typical story structures have been formalized in different ways and used by professional screenwriters as guidelines. Currently, computers cannot yet identify such a latent narrative structure of a movie story. Therefore, in this work, we raise the novel challenge of understanding and formulating the movie story structure and introduce the first ever story-based labeled dataset—the Flintstones Scene Dataset (FSD). The dataset consists of 1, 569 scenes taken from a manual annotation of 60 episodes of a famous cartoon series, The Flintstones, by 105 distinct annotators. The various labels assigned to each scene by different annotators are summarized by a probability vector over 10 possible story elements representing the function of each scene in the advancement of the story, such as the Climax of Act One or the Midpoint. These elements are learned from guidelines for professional script-writing. The annotated dataset is used to investigate the effectiveness of various story-related features and multi-label classification algorithms for the task of predicting the probability distribution of scene labels. We use cosine similarity and KL divergence to measure the quality of predicted distributions. The best approaches demonstrated 0.81 average similarity and 0.67 KL divergence between the predicted label vectors and the ground truth vectors based on the manual annotations. These results demonstrate the ability of machine learning approaches to detect the narrative structure in movies, which could lead to the development of story-related video analytics tools, such as automatic video summarization and recommendation systems.', 'corpus_id': 211084773}	19767	"[{'doc_id': '237012351', 'title': 'Sentiment analysis of twitter data using Machine learning algorithms', 'abstract': 'The rapid increase in usage of Technology has changed the way of expressing people’s opinions, views and Sentiments about specific product, services, people and more, by using social media services such as Facebook, Instagram and Twitter. Due to this is massive amount of data gets generated. To find insights from this Data generated and make certain decision we implement web application that collects twitter data and shows it indifferent statistical forms. The main objective of the work presented with in this paper was to design and implement twitter data analysis and visualization in Python platform. Our primary approach was to focus on real-time analysis rather than historic datasets. Twitter API allow for collecting the sentiments information in the form of either positive score, negative score or neutral. We show the application of sentimental analysis and how to connect to Twitter and run sentimental analysis queries. We run experiments on different queries from politics to humanity and show the interesting results. We realized that the neutral sentiment for tweets are significantly high which clearly shows the limitations of the current works. this study focuses mainly on sentiment analysis of twitter data which is helpful to analyze the information in the tweets where opinions are highly unstructured, heterogeneous and are either positive or negative, or neutral in some cases. In this paper, we provide a survey and a comparative analyses of existing techniques for opinion mining like machine learning and lexicon-based approaches, together with evaluation metrics. Using various machine learning algorithms like Naive Bayes, XGBoost Classifier and Support Vector Machine, we provide research on twitter data streams. We have also discussed general challenges and applications of Sentiment Analysis on Twitter.', 'corpus_id': 237012351, 'score': 0}, {'doc_id': '237258432', 'title': 'Survey on Different Algorithm for Movie Recommedation System', 'abstract': 'Abstract : The key to the recommendation system is to predict user performance. Day –by-Day we see huge growth in ECommerce and this continues growth in the E-commerce field gave the birth of a recommendation system. The recommendation system is done of different methods by using usersimilarily, content-based method, collaborative filtering, Hybrid Models, and many more with various algorithms and accuracy.this essay critically examine how various algorithms perform and which of each algorithms performs with better accuracy between all of them .the most better accuracy c by this origination of recommendation system, there are various types of recommendation available in the market nowadays and each and every recommendation system works on distinct appearance like the interest of users, history of users, location of users and many more. In this process, we will generally discuss the movie recommendation system. Movie recommendation Engine will recommend movies to the users on the basis of their interest as well as rating and reviews by other users whose item interest is similar to eachother. This engine will work on the user similarity Model.', 'corpus_id': 237258432, 'score': 0}, {'doc_id': '22946196', 'title': 'Challenges, comparative analysis and a proposed methodology to predict sentiment from movie reviews using machine learning', 'abstract': 'This paper investigates a new approach of finding sentence level sentiment analysis using different machine learning algorithms. Three different machine learning algorithms — SVM (Support Vector Machine), Naïve Bayes and MLP (Multilayer Layer Perceptron) have been used both for sentiment analysis. Moreover two different classifiers of Naïve Bayes and two different types of SVM kernels have been used in this work to identify and analyze the difference in accuracy as well as to find the best outcome among all the experiments. For sentiment analysis aclimdb movie review dataset has been used. Lastly, the impact of stop words and number of attributes in accuracy for sentiment analysis has also been illustrated.', 'corpus_id': 22946196, 'score': 1}, {'doc_id': '20759660', 'title': 'A machine learning approach to predict movie box-office success', 'abstract': ""Predicting society's reaction to a new product in the sense of popularity and adaption rate has become an emerging field of data analysis. The motion picture industry is a multi-billion-dollar business, and there is a massive amount of data related to movies is available over the internet. This study proposes a decision support system for movie investment sector using machine learning techniques. This research helps investors associated with this business for avoiding investment risks. The system predicts an approximate success rate of a movie based on its profitability by analyzing historical data from different sources like IMDb, Rotten Tomatoes, Box Office Mojo and Metacritic. Using Support Vector Machine (SVM), Neural Network and Natural Language Processing the system predicts a movie box office profit based on some pre-released features and post-released features. This paper shows Neural Network gives an accuracy of 84.1% for pre-released features and 89.27% for all features while SVM has 83.44% and 88.87% accuracy for pre-released features and all features respectively when one away prediction is considered. Moreover, we figure out that budget, IMDb votes and no. of screens are the most important features which play a vital role while predicting a movie's box-office success."", 'corpus_id': 20759660, 'score': 1}, {'doc_id': '145051725', 'title': 'Movie Success Prediction using Machine Learning Algorithms and their Comparison', 'abstract': 'The number of movies produced in the world is growing at an exponential rate and success rate of movie is of utmost importance since billions of dollars are invested in the making of each of these movies. In such a scenario, prior knowledge about the success or failure of a particular movie and what factor affect the movie success will benefit the production houses since these predictions will give them a fair idea of how to go about with the advertising and campaigning, which itself is an expensive affair altogether. So, the prediction of the success of a movie is very essential to the film industry. In this proposed research, we give our detailed analysis of the Internet Movie Database (IMDb) and predict the IMDb score. This database contains categorical and numerical information such as IMDb score, director, gross, budget and so on and so forth. This research proposes a way to predict how successful a movie will be prior to its arrival at the box office instead of listening to critics and others on whether a movie will be successful or not. The proposed research provides a quite efficient approach to predict IMDb score on IMDb Movie Dataset. We will try to unveil the important factors influencing the score of IMDb Movie Data. We have used different algorithms in the research work for analysis but among all Random forest gave the best prediction accuracy which is better in comparison to the previous studies. In the exploratory analysis we found that number of voted users, number of critics for reviews, number of Facebook likes, duration of the movie and gross collection of movie affect the IMDb score strongly. Drama and Biopic movies are best in genres.', 'corpus_id': 145051725, 'score': 1}, {'doc_id': '236205880', 'title': 'Machine learning based recommendation system on movie reviews using KNN classifiers', 'abstract': 'Recommender systems are the systems that are designed to recommend items to the consumer depending on several different criteria. These systems estimate the most possible product that the consumers are most likely to buy and are of interest to. Companies like Netflix, Amazon, etc. use recommender services to allow their customers to find the right items or movies for them.In the current system recommendations, the content of ltering and collective ltering typically fall into two groups. The method is formerly Periment in our paper in all methods. We take film features such as stars, directors, for content-based ltering. Movie definition and keywords as inputs use TF-IDF and doc2vec for measuring the film resemblance. For the first time, Input to our algorithm is the film ranking encountered by users, and we use neighbours nearest K, as Factorization of matrix to estimate film scores for consumers. We find that teamwork functions better than content. Predictive error and estimation time ltering.', 'corpus_id': 236205880, 'score': 0}, {'doc_id': '45266675', 'title': 'Estimation of Interpersonal Relationships in Movies', 'abstract': 'In many movies, social conditions and awareness of the issues of the times are depicted in any form. Even if fantasy and science fiction are works far from reality, the character relationship does mirror the real world. Therefore, we try to understand social conditions of the real world by analyzing the movie. As a way to analyze the movies, we propose a method of estimating interpersonal relationships of the characters, using a machine learning technique called Markov Logic Network (MLN) from movie script databases on the Web. The MLN is a probabilistic logic network that can describe the relationships between characters, which are not necessarily satisfied on every line. In experiments, we confirmed that our proposed method can estimate favors between the characters in a movie with F-measure of 58.7%. Finally, by comparing the relationships with social indicators, we discussed the relevance of the movies to the real world. key words: Markov Logic Network, semantic analysis, Open Movie Database, perception/cognitive metrics', 'corpus_id': 45266675, 'score': 1}, {'doc_id': '236836774', 'title': 'SENTIMENT ANALYSIS OF PRODUCT REVIEWS USING SUPERVISED LEARNING', 'abstract': ""Today, Online Reviews are global communications among consumers and E-commerce businesses. When Somebody wants to make a purchase online, they read the reviews and comments that many people have written about the product. Only after customers decide whether to buy the product or not. Based on that, the Success of any Products directly depends on its Customer. Customer Likes Products It’s Success. if not, then Company needs to improve it by making some changes in it. For that, the need is to analyze the customers' written reviews and find the sentiment from that. the task of Classifying the comments and the reviews in positive or negative is known as sentiment analysis.in this paper, A Standard dataset reviews have been classified into positive and negative sentiments using Sentiment Analysis. For that different Machine Learning and Deep Learning Technique is used and also Compared the performance of word2vec-CNN Model with FastTextCNN Model on amazon unlocked mobile phone Dataset."", 'corpus_id': 236836774, 'score': 0}, {'doc_id': '236909721', 'title': 'A STUDY: SENTIMENTAL ANALYSIS FOR ELECTION RESULTS BY USING TWITTER DATA', 'abstract': 'The entire world is changing at a breakneck pace, and technology is no exception. User-generated data is abundant on social networking platforms like Twitter. Users from all around the world offer their thoughts, opinions, ideas, and feelings about a variety of topics, including products, movies, and politics. Manual sentiment analysis is a time-consuming task. Opinion mining has recently gained popularity as a result of the large volume of opinionated data available on social networking sites such as Twitter. In this paper, we used a chronological approach to data collection, data pre-processing, emotional analysis, and machine learning analysis to forecast the outcome of the US 2020 presidential election using Twitter emotional analysis. We used a Random Forest classifier after completing a literature review and comparing all supervised ensemble machine learning algorithms to determine which one was the best. The proposed technique was tested on Twitter data, and it outperformed existing approaches.', 'corpus_id': 236909721, 'score': 0}, {'doc_id': '154471234', 'title': 'Box office forecasting using machine learning algorithms based on SNS data', 'abstract': 'We propose a novel approach to the box office forecasting of motion pictures using social network service (SNS) data and machine learning-based algorithms. We begin by providing a comprehensive survey of the forecasting algorithms and explanatory variables used in the motion picture domain. Because of the importance of forecasting in early periods, we develop three sequential forecasting models for predicting the non-cumulative and cumulative box office earnings: (1) prior to, (2) a week after, and (3) two weeks after release. The numbers of SNS mentions and their weekly trends are used as input variables in addition to the screening-related information. A genetic algorithm is adopted for determining significant input variables, whereas three machine learning-based nonlinear regression algorithms and their combinations are employed for building forecasting models. Experimental results show that the utilization of SNS data, machine learning-based algorithms and their combination made noticeable improvements to the forecasting accuracies of all the three models.', 'corpus_id': 154471234, 'score': 1}]"
21	{'doc_id': '235727734', 'title': 'Neural Network Layer Algebra: A Framework to Measure Capacity and Compression in Deep Learning', 'abstract': 'We present a new framework to measure the intrinsic properties of (deep) neural networks. While we focus on convolutional networks, our framework can be extrapolated to any network architecture. In particular, we evaluate two network properties, namely, capacity (related to expressivity) and compression, both of which depend only on the network structure and are independent of the training and test data. To this end, we propose two metrics: the first one, called layer complexity, captures the architectural complexity of any network layer; and, the second one, called layer intrinsic power, encodes how data is compressed along the network. The metrics are based on the concept of layer algebra, which is also introduced in this paper. This concept is based on the idea that the global properties depend on the network topology, and the leaf nodes of any neural network can be approximated using local transfer functions, thereby, allowing a simple computation of the global metrics. We also compare the properties of the state-of-the art architectures using our metrics and use the properties to analyze the classification accuracy on benchmark datasets.', 'corpus_id': 235727734}	9851	"[{'doc_id': '9786626', 'title': 'Accelerating neural network training using weight extrapolations', 'abstract': 'The backpropagation (BP) algorithm for training feedforward neural networks has proven robust even for difficult problems. However, its high performance results are attained at the expense of a long training time to adjust the network parameters, which can be discouraging in many real-world applications. Even on relatively simple problems, standard BP often requires a lengthy training process in which the complete set of training examples is processed hundreds or thousands of times. In this paper, a universal acceleration technique for the BP algorithm based on extrapolation of each individual interconnection weight is presented. This extrapolation procedure is easy to implement and is activated only a few times in between iterations of the conventional BP algorithm. This procedure, unlike earlier acceleration procedures, minimally alters the computational structure of the BP algorithm. The viability of this new approach is demonstrated on three examples. The results suggest that it leads to significant savings in computation time of the standard BP algorithm. Moreover, the solution computed by the proposed approach is always located in close proximity to the one obtained by the conventional BP procedure. Hence, the proposed method provides a real acceleration of the BP algorithm without degrading the usefulness of its solutions. The performance of the new method is also compared with that of the conjugate gradient algorithm, which is an improved and faster version of the BP algorithm.', 'corpus_id': 9786626, 'score': 1}, {'doc_id': '1890353', 'title': 'Introspection: Accelerating Neural Network Training By Learning Weight Evolution', 'abstract': 'Neural Networks are function approximators that have achieved state-of-the-art accuracy in numerous machine learning tasks. In spite of their great success in terms of accuracy, their large training time makes it difficult to use them for various tasks. In this paper, we explore the idea of learning weight evolution pattern from a simple network for accelerating training of novel neural networks. We use a neural network to learn the training pattern from MNIST classification and utilize it to accelerate training of neural networks used for CIFAR-10 and ImageNet classification. Our method has a low memory footprint and is computationally efficient. This method can also be used with other optimizers to give faster convergence. The results indicate a general trend in the weight evolution during training of neural networks.', 'corpus_id': 1890353, 'score': 1}, {'doc_id': '22553892', 'title': 'A New Type of Neurons for Machine Learning', 'abstract': 'In machine learning, an artificial neural network is the mainstream approach. Such a network consists of many neurons. These neurons are of the same type characterized by the 2 features: (1) an inner product of an input vector and a matching weighting vector of trainable parameters and (2) a nonlinear excitation function. Here, we investigate the possibility of replacing the inner product with a quadratic function of the input vector, thereby upgrading the first-order neuron to the second-order neuron, empowering individual neurons and facilitating the optimization of neural networks. Also, numerical examples are provided to illustrate the feasibility and merits of the second-order neurons. Finally, further topics are discussed.', 'corpus_id': 22553892, 'score': 1}, {'doc_id': '235755202', 'title': 'Generalization Error Analysis of Neural networks with Gradient Based Regularization', 'abstract': 'We study gradient-based regularization methods for neural networks. We mainly focus on two regularizationmethods: the total variation and the Tikhonov regularization. Applying these methods is equivalent to using neural networks to solve some partial differential equations, mostly in high dimensions in practical applications. In this work, we introduce a general framework to analyze the generalization error of regularized networks. The error estimate relies on two assumptions on the approximation error and the quadrature error. Moreover, we conduct some experiments on the image classification tasks to show that gradient-based methods can significantly improve the generalization ability and adversarial robustness of neural networks. A graphical extension of the gradient-based methods are also considered in the experiments.', 'corpus_id': 235755202, 'score': 1}, {'doc_id': '227342566', 'title': 'Estimating Vector Fields from Noisy Time Series', 'abstract': 'While there has been a surge of recent interest in learning differential equation models from time series, methods in this area typically cannot cope with highly noisy data. We break this problem into two parts: (i) approximating the unknown vector field (or right-hand side) of the differential equation, and (ii) dealing with noise. To deal with (i), we describe a neural network architecture consisting of tensor products of one-dimensional neural shape functions. For (ii), we propose an al-ternating minimization scheme that switches between vector field training and filtering steps, together with multiple trajectories of training data. We find that the neural shape function architecture retains the approximation properties of dense neural networks, enables effective computation of vector field error, and allows for graphical interpretability, all for data/systems in any finite dimension d. We also study the combination of either our neural shape function method or existing differential equation learning methods with alternating minimization and multiple trajectories. We find that retrofitting any learning method in this way boosts the method’s robustness to noise. While in their raw form the methods struggle with 1% Gaussian noise, after retrofitting, they learn accurate vector fields from data with 10% Gaussian noise.', 'corpus_id': 227342566, 'score': 0}, {'doc_id': '227338773', 'title': 'Multivariate Density Estimation with Deep Neural Mixture Models', 'abstract': ""Albeit worryingly underrated in the recent literature on machine learning in general (and, on deep learning in particular), multivariate density estimation is a fundamental task in many applications, at least implicitly, and still an open issue. With a few exceptions, deep neural networks (DNNs) have seldom been applied to density estimation, mostly due to the unsupervised nature of the estimation task, and (especially) due to the need for constrained training algorithms that ended up realizing proper probabilistic models that satisfy Kolmogorov's axioms. Moreover, in spite of the well-known improvement in terms of modeling capabilities yielded by mixture models over plain single-density statistical estimators, no proper mixtures of multivariate DNN-based component densities have been investigated so far. The paper fills this gap by extending our previous work on Neural Mixture Densities (NMMs) to multivariate DNN mixtures. A maximum-likelihood (ML) algorithm for estimating Deep NMMs (DNMMs) is handed out, which satisfies numerically a combination of hard and soft constraints aimed at ensuring satisfaction of Kolmogorov's axioms. The class of probability density functions that can be modeled to any degree of precision via DNMMs is formally defined. A procedure for the automatic selection of the DNMM architecture, as well as of the hyperparameters for its ML training algorithm, is presented (exploiting the probabilistic nature of the DNMM). Experimental results on univariate and multivariate data are reported on, corroborating the effectiveness of the approach and its superiority to the most popular statistical estimation techniques."", 'corpus_id': 227338773, 'score': 0}, {'doc_id': '227745095', 'title': 'Statistical Mechanics of Deep Linear Neural Networks: The Back-Propagating Renormalization Group', 'abstract': 'The success of deep learning in many real-world tasks has triggered an effort to theoretically understand the power and limitations of deep learning in training and generalization of complex tasks, so far with limited progress. In this work, we study the statistical mechanics of learning in Deep Linear Neural Networks (DLNNs) in which the input-output function of an individual unit is linear. Despite the linearity of the units, learning in DLNNs is highly nonlinear, hence studying its properties reveals some of the essential features of nonlinear Deep Neural Networks (DNNs). We solve exactly the network properties following supervised learning using an equilibrium Gibbs distribution in the weight space. To do this, we introduce the Back-Propagating Renormalization Group (BPRG) which allows for the incremental integration of the network weights layer by layer from the network output layer and progressing backward. This procedure allows us to evaluate important network properties such as its generalization error, the role of network width and depth, the impact of the size of the training set, and the effects of weight regularization and learning stochasticity. Furthermore, by performing partial integration of layers, BPRG allows us to compute the emergent properties of the neural representations across the different hidden layers. We have proposed a heuristic extension of the BPRG to nonlinear DNNs with rectified linear units (ReLU). Surprisingly, our numerical simulations reveal that despite the nonlinearity, the predictions of our theory are largely shared by ReLU networks with modest depth, in a wide regime of parameters. Our work is the first exact statistical mechanical study of learning in a family of Deep Neural Networks, and the first development of the Renormalization Group approach to the weight space of these systems.', 'corpus_id': 227745095, 'score': 0}, {'doc_id': '148571926', 'title': 'AutoAssist: A Framework to Accelerate Training of Deep Neural Networks', 'abstract': 'Deep neural networks have yielded superior performance in many applications; however, the gradient computation in a deep model with millions of instances lead to a lengthy training process even with modern GPU/TPU hardware acceleration. In this paper, we propose AutoAssist, a simple framework to accelerate training of a deep neural network. Typically, as the training procedure evolves, the amount of improvement in the current model by a stochastic gradient update on each instance varies dynamically. In AutoAssist, we utilize this fact and design a simple instance shrinking operation, which is used to filter out instances with relatively low marginal improvement to the current model; thus the computationally intensive gradient computations are performed on informative instances as much as possible. We prove that the proposed technique outperforms vanilla SGD with existing importance sampling approaches for linear SVM problems, and establish an O(1/k) convergence for strongly convex problems. In order to apply the proposed techniques to accelerate training of deep models, we propose to jointly train a very lightweight Assistant network in addition to the original deep network referred to as Boss. The Assistant network is designed to gauge the importance of a given instance with respect to the current Boss such that a shrinking operation can be applied in the batch generator. With careful design, we train the Boss and Assistant in a nonblocking and asynchronous fashion such that overhead is minimal. We demonstrate that AutoAssist reduces the number of epochs by 40% for training a ResNet to reach the same test accuracy on an image classification data set and saves 30% training time needed for a transformer model to yield the same BLEU scores on a translation dataset.', 'corpus_id': 148571926, 'score': 1}]"
22	"{'doc_id': '227054259', 'title': 'Scalable Graph Neural Networks for Heterogeneous Graphs', 'abstract': 'Graph neural networks (GNNs) are a popular class of parametric model for learning over graph-structured data. Recent work has argued that GNNs primarily use the graph for feature smoothing, and have shown competitive results on benchmark tasks by simply operating on graph-smoothed node features, rather than using end-to-end learned feature hierarchies that are challenging to scale to large graphs. In this work, we ask whether these results can be extended to heterogeneous graphs, which encode multiple types of relationship between different entities. We propose Neighbor Averaging over Relation Subgraphs (NARS), which trains a classifier on neighbor-averaged features for randomly-sampled subgraphs of the ""metagraph"" of relations. We describe optimizations to allow these sets of node features to be computed in a memory-efficient way, both at training and inference time. NARS achieves a new state of the art accuracy on several benchmark datasets, outperforming more expensive GNN-based methods', 'corpus_id': 227054259}"	6078	"[{'doc_id': '220496140', 'title': 'Learning to Learn Parameterized Classification Networks for Scalable Input Images', 'abstract': 'Convolutional Neural Networks (CNNs) do not have a predictable recognition behavior with respect to the input resolution change. This prevents the feasibility of deployment on different input image resolutions for a specific model. To achieve efficient and flexible image classification at runtime, we employ meta learners to generate convolutional weights of main networks for various input scales and maintain privatized Batch Normalization layers per scale. For improved training performance, we further utilize knowledge distillation on the fly over model predictions based on different input resolutions. The learned meta network could dynamically parameterize main networks to act on input images of arbitrary size with consistently better accuracy compared to individually trained models. Extensive experiments on the ImageNet demonstrate that our method achieves an improved accuracy-efficiency trade-off during the adaptive inference process. By switching executable input resolutions, our method could satisfy the requirement of fast adaption in different resource-constrained environments. Code and models are available at this https URL.', 'corpus_id': 220496140, 'score': 0}, {'doc_id': '232404318', 'title': 'A nonlinear diffusion method for semi-supervised learning on hypergraphs', 'abstract': 'Hypergraphs are a common model for multiway relationships in data, and hypergraph semisupervised learning is the problem of assigning labels to all nodes in a hypergraph, given labels on just a few nodes. Diffusions and label spreading are classical techniques for semi-supervised learning in the graph setting, and there are some standard ways to extend them to hypergraphs. However, these methods are linear models, and do not offer an obvious way of incorporating node features for making predictions. Here, we develop a nonlinear diffusion process on hypergraphs that spreads both features and labels following the hypergraph structure, which can be interpreted as a hypergraph equilibrium network. Even though the process is nonlinear, we show global convergence to a unique limiting point for a broad class of nonlinearities, which is the global optimum of a interpretable, regularized semi-supervised learning loss function. The limiting point serves as a node embedding from which we make predictions with a linear model. Our approach is much more accurate than several hypergraph neural networks, and also takes less time to train.', 'corpus_id': 232404318, 'score': 1}, {'doc_id': '220265908', 'title': 'Maximum Entropy Models for Fast Adaptation', 'abstract': 'Deep Neural Networks have shown great promise on a variety of downstream tasks; but their ability to adapt to new data and tasks remains a challenging problem. The ability of a model to perform few-shot adaptation to a novel task is important for the scalability and deployment of machine learning models. Recent work has shown that the learned features in a neural network follow a normal distribution [41], which thereby results in a strong prior on the downstream task. This implicit overfitting to data from training tasks limits the ability to generalize and adapt to unseen tasks at test time. This also highlights the importance of learning task-agnostic representations from data. In this paper, we propose a regularization scheme using a max-entropy prior on the learned features of a neural network; such that the extracted features make minimal assumptions about the training data. We evaluate our method on adaptation to unseen tasks by performing experiments in 4 distinct settings. We find that our method compares favourably against multiple strong baselines across all of these experiments.', 'corpus_id': 220265908, 'score': 1}, {'doc_id': '227238602', 'title': 'Revisiting graph neural networks and distance encoding from a practical view', 'abstract': 'Graph neural networks (GNNs) are widely used in the applications based on graph structured data, such as node classification and link prediction. However, GNNs are often used as a black-box tool and rarely get in-depth investigated regarding whether they fit certain applications that may have various properties. A recently proposed technique distance encoding (DE) (Li et al. 2020) magically makes GNNs work well in many applications, including node classification and link prediction. The theory provided in (Li et al. 2020) supports DE by proving that DE improves the representation power of GNNs. However, it is not obvious how the theory assists the applications accordingly. Here, we revisit GNNs and DE from a more practical point of view. We want to explain how DE makes GNNs fit for node classification and link prediction. Specifically, for link prediction, DE can be viewed as a way to establish correlations between a pair of node representations. For node classification, the problem becomes more complicated as different classification tasks may hold node labels that indicate different physical meanings. We focus on the most widely-considered node classification scenarios and categorize the node labels into two types, community type and structure type, and then analyze different mechanisms that GNNs adopt to predict these two types of labels. We also run extensive experiments 1 to compare eight different configurations of GNNs paired with DE to predict node labels over eight real-world graphs. The results demonstrate the uniform effectiveness of DE to predict structure-type labels. Lastly, we reach three pieces of conclusions on how to use GNNs and DE properly in tasks of node classification.', 'corpus_id': 227238602, 'score': 1}, {'doc_id': '219558713', 'title': 'Learning to Stop While Learning to Predict', 'abstract': ""There is a recent surge of interest in designing deep architectures based on the update steps in traditional algorithms, or learning neural networks to improve and replace traditional algorithms. While traditional algorithms have certain stopping criteria for outputting results at different iterations, many algorithm-inspired deep models are restricted to a ``fixed-depth'' for all inputs. Similar to algorithms, the optimal depth of a deep architecture may be different for different input instances, either to avoid ``over-thinking'', or because we want to compute less for operations converged already. In this paper, we tackle this varying depth problem using a steerable architecture, where a feed-forward deep model and a variational stopping policy are learned together to sequentially determine the optimal number of layers for each input instance. Training such architecture is very challenging. We provide a variational Bayes perspective and design a novel and effective training procedure which decomposes the task into an oracle model learning stage and an imitation stage. Experimentally, we show that the learned deep model along with the stopping policy improves the performances on a diverse set of tasks, including learning sparse recovery, few-shot meta learning, and computer vision tasks."", 'corpus_id': 219558713, 'score': 0}, {'doc_id': '219636421', 'title': 'Tangent Space Sensitivity and Distribution of Linear Regions in ReLU Networks', 'abstract': 'Recent articles indicate that deep neural networks are efficient models for various learning problems. However they are often highly sensitive to various changes that cannot be detected by an independent observer. As our understanding of deep neural networks with traditional generalization bounds still remains incomplete, there are several measures which capture the behaviour of the model in case of small changes at a specific state. In this paper we consider adversarial stability in the tangent space and suggest tangent sensitivity in order to characterize stability. We focus on a particular kind of stability with respect to changes in parameters that are induced by individual examples without known labels. We derive several easily computable bounds and empirical measures for feed-forward fully connected ReLU (Rectified Linear Unit) networks and connect tangent sensitivity to the distribution of the activation regions in the input space realized by the network. Our experiments suggest that even simple bounds and measures are associated with the empirical generalization gap.', 'corpus_id': 219636421, 'score': 0}, {'doc_id': '216338053', 'title': 'Maxpolynomial Division with Application To Neural Network Simplification', 'abstract': 'In this work, we further the link between neural networks with piecewise linear activations and tropical algebra. To that end, we introduce the process of Maxpolynomial Division, a geometric method which simulates division of polynomials in the max-plus semiring, while highlighting its key properties and noting its connection to neural networks. Afterwards, we generalize this method and apply it in the context of neural network minimization, for two-layer networks used for binary classification problems, attempting to reduce the size of the hidden layer before the output. A tractable method to find an appropriate divisor and perform the division is introduced and evaluated in the IMDB Movie Review and MNIST datasets, with preliminary experiments demonstrating a capacity of this method to reduce the size of the network, without major loss of performance.', 'corpus_id': 216338053, 'score': 1}, {'doc_id': '231918684', 'title': 'Min-Max-Plus Neural Networks', 'abstract': 'We present a new model of neural networks called Min-Max-Plus Neural Networks (MMP-NNs) based on operations in tropical arithmetic. In general, an MMP-NN is composed of three types of alternately stacked layers, namely linear layers, min-plus layers and max-plus layers. Specifically, the latter two types of layers constitute the nonlinear part of the network which is trainable and more sophisticated compared to the nonlinear part of conventional neural networks. In addition, we show that with higher capability of nonlinearity expression, MMP-NNs are universal approximators of continuous functions, even when the number of multiplication operations is tremendously reduced (possibly to none in certain extreme cases). Furthermore, we formulate the backpropagation algorithm in the training process of MMP-NNs and introduce an algorithm of normalization to improve the rate of convergence in training.', 'corpus_id': 231918684, 'score': 1}, {'doc_id': '228064565', 'title': 'Multi-Population Phase Oscillator Networks with Higher-Order Interactions.', 'abstract': 'The classical Kuramoto model consists of finitely many pairwise coupled oscillators on the circle. In many applications a simple pairwise coupling is not sufficient to describe real-world phenomena as higher-order (or group) interactions take place. Hence, we replace the classical coupling law with a very general coupling function involving higher-order terms. Furthermore, we allow for multiple populations of oscillators interacting with each other through a very general law. In our analysis, we focus on the characteristic system and the mean-field limit of this generalized class of Kuramoto models. While there are several works studying particular aspects of our program, we propose a general framework to work with all three aspects (higher-order, multi-population, and mean-field) simultaneously. Assuming identical oscillators in each population, we derive equations for the evolution of oscillator populations in the mean-field limit. First, we clarify existence and uniqueness of our set of characteristic equations, which are formulated in the space of probability measures together with the bounded-Lipschitz metric. Then, we investigate dynamical properties within the framework of the characteristic system. We identify invariant subspaces and stability of the state, in which all oscillators are synchronized within each population. Even though it turns out that this so called all-synchronized state is never asymptotically stable, under some conditions and with a suitable definition of stability, the all-synchronized state can be proven to be at least locally stable. In summary, our work provides a rigorous mathematical framework upon which the further study of multi-population higher-order coupled particle systems can be based.', 'corpus_id': 228064565, 'score': 1}, {'doc_id': '219687601', 'title': 'Proximal Mapping for Deep Regularization', 'abstract': 'Underpinning the success of deep learning is effective regularizations that allow a variety of priors in data to be modeled. For example, robustness to adversarial perturbations, and correlations between multiple modalities. However, most regularizers are specified in terms of hidden layer outputs, which are not themselves optimization variables. In contrast to prevalent methods that optimize them indirectly through model weights, we propose inserting proximal mapping as a new layer to the deep network, which directly and explicitly produces well regularized hidden layer outputs. The resulting technique is shown well connected to kernel warping and dropout, and novel algorithms were developed for robust temporal learning and multiview modeling, both outperforming state-of-the-art methods.', 'corpus_id': 219687601, 'score': 0}]"
23	{'doc_id': '233257992', 'title': 'Generation of recombinant hyperimmune globulins from diverse B-cell repertoires', 'abstract': 'Plasma-derived polyclonal antibody therapeutics, such as intravenous immunoglobulin, have multiple drawbacks, including low potency, impurities, insufficient supply, and batch-to-batch variation. Here we describe a microfluidics and molecular genomics strategy for capturing diverse mammalian antibody repertoires to create recombinant multivalent hyperimmune globulins. Our method generates thousands-diverse mixtures of recombinant antibodies, enriched for specificity and activity against therapeutic targets. Each hyperimmune globulin product comprised thousands to tens of thousands of antibodies derived from convalescent or vaccinated human donors, or immunized mice. Using this approach, we generated hyperimmune globulins with potent neutralizing activity against Severe Acute Respiratory Syndrome Coronavirus-2 (SARS-CoV-2) in under three months, Fc-engineered hyperimmune globulins specific for Zika virus that lacked antibody-dependent enhancement of disease, and hyperimmune globulins specific for lung pathogens present in patients with primary immune deficiency. To address the limitations of rabbit-derived anti-thymocyte globulin (ATG), we generated a recombinant human version and demonstrated its efficacy in mice against graft-versus-host disease.', 'corpus_id': 233257992}	16591	"[{'doc_id': '235353135', 'title': 'One-step CRISPR-Cas9 protocol for the generation of plug & play conditional knockouts in Drosophila melanogaster', 'abstract': 'Summary This one-step method generates, for any locus, a conditional knockout allele in Drosophila. The allele carries a bright fluorescent marker for easy screening and an attP landing site for subsequent genetic manipulations. After removing the selectable marker with Cre, the attP site can be used to insert DNA fragments expressing tagged or mutant alleles to determine protein localization and function in a tissue- and stage-specific manner. Only a single round of CRISPR-Cas9-mediated editing is required. For complete details on the use and execution of this protocol, please refer to the DWnt4[cKO] example in Yu et al. (2020).', 'corpus_id': 235353135, 'score': 0}, {'doc_id': '236213020', 'title': 'Directed Evolution: Methodologies and Applications.', 'abstract': 'Directed evolution aims to expedite the natural evolution process of biological molecules and systems in a test tube through iterative rounds of gene diversifications and library screening/selection. It has become one of the most powerful and widespread tools for engineering improved or novel functions in proteins, metabolic pathways, and even whole genomes. This review describes the commonly used gene diversification strategies, screening/selection methods, and recently developed continuous evolution strategies for directed evolution. Moreover, we highlight some representative applications of directed evolution in engineering nucleic acids, proteins, pathways, genetic circuits, viruses, and whole cells. Finally, we discuss the challenges and future perspectives in directed evolution.', 'corpus_id': 236213020, 'score': 1}, {'doc_id': '222352211', 'title': 'Systematic use of synthetic 5′-UTR RNA structures to tune protein translation improves yield and quality of complex proteins in mammalian cell factories', 'abstract': 'Abstract Predictably regulating protein expression levels to improve recombinant protein production has become an important tool, but is still rarely applied to engineer mammalian cells. We therefore sought to set-up an easy-to-implement toolbox to facilitate fast and reliable regulation of protein expression in mammalian cells by introducing defined RNA hairpins, termed ‘regulation elements (RgE)’, in the 5′-untranslated region (UTR) to impact translation efficiency. RgEs varying in thermodynamic stability, GC-content and position were added to the 5′-UTR of a fluorescent reporter gene. Predictable translation dosage over two orders of magnitude in mammalian cell lines of hamster and human origin was confirmed by flow cytometry. Tuning heavy chain expression of an IgG with the RgEs to various levels eventually resulted in up to 3.5-fold increased titers and fewer IgG aggregates and fragments in CHO cells. Co-expression of a therapeutic Arylsulfatase-A with RgE-tuned levels of the required helper factor SUMF1 demonstrated that the maximum specific sulfatase activity was already attained at lower SUMF1 expression levels, while specific production rates steadily decreased with increasing helper expression. In summary, we show that defined 5′-UTR RNA-structures represent a valid tool to systematically tune protein expression levels in mammalian cells and eventually help to optimize recombinant protein expression.', 'corpus_id': 222352211, 'score': 1}, {'doc_id': '237257237', 'title': 'Synthetic Biology in Plants, a Boon for Coming Decades', 'abstract': 'Recently an enormous expansion of knowledge is seen in various disciplines of science. This surge of information has given rise to concept of interdisciplinary fields, which has resulted in emergence of newer research domains, one of them is ‘Synthetic Biology’ (SynBio). It captures basics from core biology and integrates it with concepts from the other areas of study such as chemical, electrical, and computational sciences. The essence of synthetic biology is to rewire, re-program, and re-create natural biological pathways, which are carried through genetic circuits. A genetic circuit is a functional assembly of basic biological entities (DNA, RNA, proteins), created using typical design, built, and test cycles. These circuits allow scientists to engineer nearly all biological systems for various useful purposes. The development of sophisticated molecular tools, techniques, genomic programs, and ease of nucleic acid synthesis have further fueled several innovative application of synthetic biology in areas like molecular medicines, pharmaceuticals, biofuels, drug discovery, metabolomics, developing plant biosensors, utilization of prokaryotic systems for metabolite production, and CRISPR/Cas9 in the crop improvement. These applications have largely been dominated by utilization of prokaryotic systems. However, newer researches have indicated positive growth of SynBio for the eukaryotic systems as well. This paper explores advances of synthetic biology in the plant field by elaborating on its core components and potential applications. Here, we have given a comprehensive idea of designing, development, and utilization of synthetic biology in the improvement of the present research state of plant system.', 'corpus_id': 237257237, 'score': 0}, {'doc_id': '235595463', 'title': 'Multiplexed engineering glycosyltransferase genes in CHO cells via targeted integration for producing antibodies with diverse complex-type N-glycans', 'abstract': 'Therapeutic antibodies are decorated with complex-type N-glycans that significantly affect their biodistribution and bioactivity. The N-glycan structures on antibodies are incompletely processed in wild-type CHO cells due to their limited glycosylation capacity. To improve N-glycan processing, glycosyltransferase genes have been traditionally overexpressed in CHO cells to engineer the cellular N-glycosylation pathway by using random integration, which is often associated with large clonal variations in gene expression levels. In order to minimize the clonal variations, we used recombinase-mediated-cassette-exchange (RMCE) technology to overexpress a panel of 42 human glycosyltransferase genes to screen their impact on antibody N-linked glycosylation. The bottlenecks in the N-glycosylation pathway were identified and then released by overexpressing single or multiple critical genes. Overexpressing B4GalT1 gene alone in the CHO cells produced antibodies with more than 80% galactosylated bi-antennary N-glycans. Combinatorial overexpression of B4GalT1 and ST6Gal1 produced antibodies containing more than 70% sialylated bi-antennary N-glycans. In addition, antibodies with various tri-antennary N-glycans were obtained for the first time by overexpressing MGAT5 alone or in combination with B4GalT1 and ST6Gal1. The various N-glycan structures and the method for producing them in this work provide opportunities to study the glycan structure-and-function and develop novel recombinant antibodies for addressing different therapeutic applications.', 'corpus_id': 235595463, 'score': 1}, {'doc_id': '235745046', 'title': 'Using Engineered Mammalian Cells for an Epitope-Directed Antibody Affinity Maturation System.', 'abstract': 'Antibodies have been attracting attention as therapeutic tools owing to their high affinity and specificity. To develop potent antibodies, affinity maturation, epitope regulation, and using target antigens in native form are pivotal requirements. Here we describe a method to conduct epitope-directed affinity maturation of antibodies using engineered mammalian cells. This method utilizes protein chimeras that transduce cell death signaling in response to antibody binding. As the competition of antibody binding inhibits the cell death signaling, only affinity-matured antibodies retaining the same epitope as an original one can be selected using cell survival as readout.', 'corpus_id': 235745046, 'score': 1}, {'doc_id': '235393158', 'title': 'Genome editor-directed in\xa0vivo library diversification.', 'abstract': 'The generation of a library of variant genes is a prerequisite of directed evolution, a powerful tool for biomolecular engineering. As the number of all possible sequences often far exceeds the diversity of a practical library, methods that allow efficient library diversification in living cells are essential for in\xa0vivo directed evolution technologies to effectively sample the sequence space and allow hits to emerge. While traditional whole-genome mutagenesis often results in toxicity and the emergence of ""cheater"" mutations, recent developments that exploit the targeting and editing abilities of genome editors to facilitate in\xa0vivo library diversification have allowed for precise mutagenesis focused on specific genes of interest, higher mutational density, and reduced the occurrence of cheater mutations. This minireview summarizes recent advances in genome editor-directed in\xa0vivo library diversification and provides an outlook on their future applications in chemical biology.', 'corpus_id': 235393158, 'score': 1}, {'doc_id': '235418254', 'title': 'Generation of recombinant antibodies by mammalian expression system for detecting S-metolachlor in environmental waters.', 'abstract': 'Current immunoassays for herbicide detection are usually based on polyclonal or monoclonal antibodies (MAbs) raised in animals. The mammalian expression system allows the procurement of specific and highly sensitive antibodies, avoiding animal immunization. In this study, S-metolachlor-specific IgG vectors bearing either Thosea asigna virus 2A or internal ribosome entry site (S-T2A or S-IRES) and single-chain variable fragment (scFv) vectors were designed and expressed. The recombinant antibodies (RAbs) were characterized by indirect competitive enzyme-linked immunosorbent assays (icELISA). The results showed that full-length RAbs exhibited significantly better performance than scFv, and both bicistronic vectors expressed antibodies of correct size, while RAb S-T2A elicited a higher yield than RAb S-IRES. Further analyses showed that RAb S-T2A and RAb S-IRES exhibited comparable reactivities and specificities to the parental MAb, with IC50 values of 3.44, 3.89 and 3.37\xa0ng/mL, respectively. Finally, MAb- and RAb-based icELISAs were established for the determination of S-metolachlor in environmental waters. The recoveries were in the range of 73.0-128.1%, and the coefficients of variation were mostly below 10%. This article describes the production of RAbs for S-metolachlor from mammalian cells for the first time and paves the way to develop RAb-based immunoassays for monitoring herbicide residues in the environment.', 'corpus_id': 235418254, 'score': 0}, {'doc_id': '235796372', 'title': 'Novel phage display-derived recombinant antibodies recognizing both MPT64 native and mutant (63-bp deletion) are promising tools for tuberculosis diagnosis.', 'abstract': 'Tuberculosis (TB) is one of the top 10 causes of death in humans worldwide. The most important causative agents of TB are bacteria from the Mycobacterium tuberculosis complex (MTC), although nontuberculous mycobacteria (NTM) can also cause similar infections. The ability to identify and differentiate MTC isolates from NTM is important for the selection of the correct antimicrobial therapy. Immunochromatographic assays with antibodies anti-MPT64 allow differentiation between MTC and NTM since the MPT64 protein is specific from MTC. However, studies reported false-negative results mainly due to mpt64 63-bp deletion. Considering this drawback, we selected seven human antibody fragments against MPT64 by phage display and produced them as scFv-Fc. Three antibodies reacted with rMPT64 mutant (63-bp deletion) protein and native MPT64 from M. tuberculosis H37Rv in ELISA and Western blot. These antibodies are new biological tools with the potential for the development of TB diagnosis helping to overcome limitations of the MPT64-based immunochromatographic tests currently available.', 'corpus_id': 235796372, 'score': 0}, {'doc_id': '237307774', 'title': 'Specificity and off-target effects of AAV8-TBG viral vectors for the manipulation of hepatocellular gene expression in mice', 'abstract': 'ABSTRACT Mice are a widely used pre-clinical model system in large part due to their potential for genetic manipulation. The ability to manipulate gene expression in specific cells under temporal control is a powerful experimental tool. The liver is central to metabolic homeostasis and a site of many diseases, making the targeting of hepatocytes attractive. Adeno-associated virus 8 (AAV8) vectors are valuable instruments for the manipulation of hepatocellular gene expression. However, their off-target effects in mice have not been thoroughly explored. Here, we sought to identify the short-term off-target effects of AAV8 administration in mice. To do this, we injected C57BL/6J wild-type mice with either recombinant AAV8 vectors expressing Cre recombinase or control AAV8 vectors and characterised the changes in general health and in liver physiology, histology and transcriptomics compared to uninjected controls. We observed an acute and transient trend for reduction in homeostatic liver proliferation together with induction of the DNA damage marker γH2AX following AAV8 administration. The latter was enhanced upon Cre recombinase expression by the vector. Furthermore, we observed transcriptional changes in genes involved in circadian rhythm and response to infection. Notably, there were no additional transcriptomic changes upon expression of Cre recombinase by the AAV8 vector. Overall, there was no evidence of liver injury, and only mild T-cell infiltration was observed 14\u2005days following AAV8 infection. These data advance the technique of hepatocellular genome editing through Cre-Lox recombination using Cre expressing AAV vectors, demonstrating their minimal effects on murine physiology and highlight the more subtle off target effects of these systems.', 'corpus_id': 237307774, 'score': 0}]"
24	{'doc_id': '135418842', 'title': 'Application of IoT-Enabled Smart Agriculture in Vertical Farming', 'abstract': 'Vertical farming is an unconventional farming technique that has gained relevance in recent years, as existing agricultural lands fail to meet the needs of the growing population. Smart monitoring of the ambient parameters in vertical farming can improve the productivity and quality of the crops. A system has been proposed to develop sensor arrays that can measure the ambient parameters and upload the data onto the ThingSpeak Cloud, using the Intel Edison wireless module. The web-based application can be used to analyze and monitor the light, temperature, humidity, and soil moisture of the vertical farming stacks. Using the Virtuino app, a SMS can be sent if the parameters fall below a threshold value.', 'corpus_id': 135418842}	7462	[{'doc_id': '111272111', 'title': 'Model-based predictive control of greenhouse climate for reducing energy and water consumption', 'abstract': 'This work focuses on development of control algorithms by incorporating energy and water consumption to maintain climatic conditions in greenhouse. Advanced control algorithms can supply solutions to modern exploitations. The new developments usually require accurate models (probably multivariable and non-linear ones) and control methodologies capable of using these models. As an additional requirement it is important for the final application to be easy to use, so advanced control will not mean an increase in complexity of the manipulation of the installation. This article shows an alternative to classical climate control. It is based on two fundamental elements: an accurate non-linear model and a model-based predictive control (MBPC) that incorporate energy and water consumption. Genetic algorithms (GAs) play a key role in these two elements because functions to solve are non-convex and with local minima. First of all GAs supply a way to adjust the non-linear model parameters obtained from first principles, and finally GAs open the possibility of using non-linear model in the MBPC and of establishing a flexible cost index to minimize energy and water consumption. The results on a plastic greenhouse with arch-shaped roofs and for Mediterranean area are presented, important reduction in energy and water used in the cooling system (nebulization) is obtained.', 'corpus_id': 111272111, 'score': 1}, {'doc_id': '218471177', 'title': 'Transfer of Manure from Livestock Farms to Crop Fields as Fertilizer using an Ant Inspired Approach', 'abstract': 'Abstract. Intensive livestock production might have a negative environmental impact, by producing large amounts of animal excrements, which, if not properly managed, can contaminate nearby water bodies with nutrient excess. However, if animal manure is exported to distant crop fields, to be used as organic fertilizer, pollution can be mitigated. It is a single-objective optimization problem, in regards to finding the best solution for the logistics process of satisfying nutrient crops needs by means of livestock manure. This paper proposes a dynamic approach to solve the problem, based on a decentralized nature-inspired cooperative technique, inspired by the foraging behavior of ants (AIA). Results provide important insights for policy-makers over the potential of using animal manure as fertilizer for crop fields, while AIA solves the problem effectively, in a fair way to the farmers and well balanced in terms of average transportation distances that need to be covered by each livestock farmer. Our work constitutes the first application of a decentralized AIA to this interesting real-world problem, in a domain where swarm intelligence methods are still under-exploited.', 'corpus_id': 218471177, 'score': 0}, {'doc_id': '218832204', 'title': 'Implementation of Inverse Kinematics for Crop-Harvesting Robotic Arm in Vertical Farming', 'abstract': 'The world population is expected to increase to 9.8 billion in 2050 according to United Nations. With this, scarcity of food and space will further be a major concern. This study proposes a framework which used initializing, processing, and directing applied to an inverse kinematics based robotic arm. An automatized approach in addressing the foreseeable problem on providing nutritional plant-based food considering that cities are becoming highly-urbanized was developed. Wall gardening used for vertical farming or urban farming is a technique by which there are sets of rows and columns of pockets installed over a wall. These pockets are filled with soil or other planting bases (i.e. water for hydroponics) for the seedlings to grow. A robotic arm is manually set to point on a specific pocket where a crop has grown. Using inverse kinematics, the set points determine the joint angles. This then targets the pockets and the end-effector of the robot arm performs a grip to the roots of the crops. The robotic arm then moves to its initial point, technically pulling up the crop. After positioning to the initial point, the arm directs to the side of the wall, where a container is located. The end-effector opens to drop the crop carefully into the container. The research study is simulated using MATLAB and Universal Robots. The results show that it can only yield 85.42% of the crops.', 'corpus_id': 218832204, 'score': 1}, {'doc_id': '220666099', 'title': 'Can animal manure be used to increase soil organic carbon stocks in the Mediterranean as a mitigation climate change strategy', 'abstract': 'Soil organic carbon (SOC) plays an important role on improving soil conditions and soil functions. Increasing land use changes have induced an important decline of SOC content at global scale. Increasing SOC in agricultural soils has been proposed as a strategy to mitigate climate change. Animal manure has the characteristic of enriching SOC, when applied to crop fields, while, in parallel, it could constitute a natural fertilizer for the crops. In this paper, a simulation is performed using the area of Catalonia, Spain as a case study for the characteristic low SOC in the Mediterranean, to examine whether animal manure can improve substantially the SOC of agricultural fields, when applied as organic fertilizers. Our results show that the policy goals of the 4x1000 strategy can be achieved only partially by using manure transported to the fields. This implies that the proposed approach needs to be combined with other strategies.', 'corpus_id': 220666099, 'score': 0}, {'doc_id': '219708508', 'title': 'Transfer of Manure as Fertilizer from Livestock Farms to Crop Fields: The Case of Catalonia', 'abstract': 'Abstract Intensive livestock production might have a negative environmental impact, by producing large amounts of animal manure, which, if not properly managed, can contaminate nearby water bodies with nutrient excess. However, if animal manure is exported to nearby crop fields, to be used as organic fertilizer, pollution can be mitigated. It is a single-objective optimization problem, in regards to finding the best solution for the logistics process of satisfying nutrient needs of crops by means of livestock manure. This paper proposes three different approaches to solve the problem: a centralized optimal algorithm (COA), a decentralized nature-inspired cooperative technique, based on the foraging behaviour of ants (AIA), as well as a naive neighbour-based method (NBS), which constitutes the existing practice used today in an ad hoc, uncoordinated manner in Catalonia. Results show that the COA approach is 8.5% more efficient than the AIA. However, the AIA approach is fairer to the farmers and more balanced in terms of average transportation distances that need to be covered by each livestock farmer, while it is 1.07 times more efficient than the NBS. Our work constitutes the first application of a decentralized AIA to this interesting real-world problem, in a domain where swarm intelligence methods are still under-exploited.', 'corpus_id': 219708508, 'score': 0}, {'doc_id': '214199226', 'title': 'Vertical farming in Europe', 'abstract': 'Abstract In Europe, plant factories are typically referred to as vertical or indoor farms. Vertical farming is a young, dynamic, and ever-changing sector. The number of vertical farms in Europe is still relatively small, but the sector is increasing rapidly. In this chapter, an overview is given of the different types of vertical farms found in Europe, i.e., PFALs (plant factory with artificial light), container farms, in-store farms, and appliance farms. The entrepreneurial landscape is described. This description not only includes companies growing the crops but also companies supplying the technology. Many of the European vertical farming companies are listed, including well-established and start-up companies. The potential, and risks, of vertical farming are discussed in the European context.', 'corpus_id': 214199226, 'score': 1}, {'doc_id': '226948352', 'title': 'Cherry Tomato Production in Intelligent Greenhouses—Sensors and AI for Control of Climate, Irrigation, Crop Yield, and Quality', 'abstract': 'Greenhouses and indoor farming systems play an important role in providing fresh and nutritious food for the growing global population. Farms are becoming larger and greenhouse growers need to make complex decisions to maximize production and minimize resource use while meeting market requirements. However, highly skilled labor is increasingly lacking in the greenhouse sector. Moreover, extreme events such as the COVID-19 pandemic, can make farms temporarily less accessible. This highlights the need for more autonomous and remote-control strategies for greenhouse production. This paper describes and analyzes the results of the second “Autonomous Greenhouse Challenge”. In this challenge, an experiment was conducted in six high-tech greenhouse compartments during a period of six months of cherry tomato growing. The primary goal of the greenhouse operation was to maximize net profit, by controlling the greenhouse climate and crop with AI techniques. Five international teams with backgrounds in AI and horticulture were challenged in a competition to operate their own compartment remotely. They developed intelligent algorithms and use sensor data to determine climate setpoints and crop management strategy. All AI supported teams outperformed a human-operated greenhouse that served as reference. From the results obtained by the teams and from the analysis of the different climate-crop strategies, it was possible to detect challenges and opportunities for the future implementation of remote-control systems in greenhouse production.', 'corpus_id': 226948352, 'score': 1}, {'doc_id': '220962268', 'title': 'Minimizing Electricity Cost through Smart Lighting Control for Indoor Plant Factories', 'abstract': 'Smart plant factories incorporate sensing technology, actuators and control algorithms to automate processes, reducing the cost of production while improving crop yield many times over that of traditional farms. This paper investigates the growth of lettuce (Lactuca Sativa) in a smart farming setup when exposed to red and blue light-emitting diode (LED) horticulture lighting. An image segmentation method based on K-means clustering is used to identify the size of the plant at each stage of growth, and the growth of the plant modelled in a feed forward network. Finally, an optimization algorithm based on the plant growth model is proposed to find the optimal lighting schedule for growing lettuce with respect to dynamic electricity pricing. Genetic algorithm was utilized to find solutions to the optimization problem. When compared to a baseline in a simulation setting, the schedules proposed by the genetic algorithm can achieved between 40-52% savings in energy costs, and up to a 6% increase in leaf area.', 'corpus_id': 220962268, 'score': 1}]
25	{'doc_id': '132596242', 'title': 'Does the future of the favela fit in an aerial cable car? Examining tourism mobilities and urban inequalities through a decolonial lens', 'abstract': 'ABSTRACT Promising to foster social, economic, and spatial integration, as well as tourism opportunities, the application of gondola lift systems to segregated urban areas has attracted widespread attention. In the favelas of Rio de Janeiro, the aerial cable car system is a key component of place-branding strategies that are preparing the city for global mega events. Under a decolonial lens and the inspiration of the New Mobilities Paradigm, the idealization, the uses, and disputes around this transportation device, celebrated as capable of adding touristic value to despoiled landscapes, allow for a more comprehensive understanding of heated conflicts over mobility patterns, especially concerning favelas as touristic destinations, that are contemporary but are also extensions of older inequalities played out in Brazil. The conclusion highlights the need to infer about the future of the megacities in which tourism flows are increasingly legitimating high-cost infrastructural proposals which neglect genuinely participatory and inclusive public consultation methods.', 'corpus_id': 132596242}	16753	"[{'doc_id': '110558960', 'title': 'The Past, Present, and Future of Urban Cable Propelled People Movers', 'abstract': ""The evolution of urban cable propelled people mover technology from the early 1800's through the 1990's is reviewed, with emphasis on systems developed since 1980. Technologies can be classified by the means of vehicle support and the type of service provided. The simplest systems serve as shuttles moving back-and-forth between a pair of terminal stations; more complex systems feature vehicles which are launched continuously at short headways. Advances have occurred in the design and aesthetics of guideways, vehicle operating speed, the spectrum of capacities available, and automated features. The characteristics of the individual systems in urban environments throughout the world are described."", 'corpus_id': 110558960, 'score': 1}, {'doc_id': '233475322', 'title': 'Cities and Urban Planning BUILDING FORWARD BETTER-PATHWAYS FOR A SUSTAINABLE POST-COVID RECOVERY FOR INDIA', 'abstract': 'At the same time, traffic congestion, pollution levels, and greenhouse gas (GHG) emissions decreased temporarily, and new ways of doing business emerged, supported by an accelerated shift to a digital economy. Even as there is a need to return to previous levels of growth and employment, there is an equal and simultaneous opportunity to revisit India’s long-term sustainable development challenges. For instance, green investments – including building efficiency, public transit, and solar photovoltaics (PV) – can create more jobs per dollar in the immediate term than investments in fossil fuels.5 In addition to the number of jobs, it is also critical to ensure that the measures taken in this socioeconomic reset are equitable and inclusive. To achieve this, cities and towns, which produce more than 70% of the national GDP, have to make a significant comeback in terms of productivity.6', 'corpus_id': 233475322, 'score': 0}, {'doc_id': '233304911', 'title': 'The role of urban agriculture for a resilient city', 'abstract': 'Humans are simultaneously facing challenges as climate change, epidemics and scarcity of food and water. It is estimated that by 2021 over 690 million of people will face hunger; by 2050 the global population will increase up to 10 billion with 68% of the population living in urban areas. By providing 30% of self-sufficient food in 2030, urban agriculture will be a practical concept to face these challenges. The work studies the role of agricultural land as a critical part for a resilient city. Parameters related to food production are also explored. As study case, this work aims to investigate the current food security of the Ho Chi Minh city (HCMC), offering productive green solutions at different scales from land-use planning, urban design to green roofs. For a production of 6.7 kg/day of vegetables a day, the costs of are approximately $10,000 for nearly 5.6 square meters of land; this points out A-Go-Gro technology as an effective measure for vertical farming. For example, 0.18 ha of green space can produce 2 tons of vegetables per day in the Lake View settlement (district 2 in HCMC). Moreover, due to green roofs, stormwater volumes directed into the sewer system are decreased by 65% and the penetration of electromagnetic radiation is reduced by 99.4%.', 'corpus_id': 233304911, 'score': 0}, {'doc_id': '233717807', 'title': 'The Past, Present, and Future of Special Economic Zones and Their Impact', 'abstract': 'ABSTRACT Special economic zones (SEZs) have been used by many developing countries as a policy tool to promote industrialization and economic transformation. Since the initiation of the first modern zone in Shannon, Ireland, special economic zones have evolved in many ways, from an initial ‘enclave’ nature towards today’s ‘Economic Zone 5.0’, which is built on emerging digital technologies and well integrated with urban development. The special economic zones represents a new unilateral compromise between the state and market, while still contributing to economic globalization, by presenting itself as a complementary or as an alternative approach to integrate with the global market in addition to the international economic law instruments. Despite the prevalence of special economic zones worldwide, their performance and impact on the economy and structural transformation are quite mixed. Among the many lessons learned from successful special economic zone programmes, the key elements include a strategic location, integration of zone strategy with the overall development strategy, understanding the market and leveraging comparative advantage, and, most importantly, ensuring that zones are ‘special’ in terms of a business-friendly environment—especially a sound legal and regulatory framework and an embodiment of sustainability and resiliency towards various external shocks like today’s COVID-19 pandemic.', 'corpus_id': 233717807, 'score': 0}, {'doc_id': '153827744', 'title': 'An innovative transit system and its impact on low income users: the case of the Metrocable in Medellín', 'abstract': 'The Metrocable in Medellin, Colombia, is an innovative system to improve access to deprived areas located in hilly zones. The idea to use cable cars as feeders to the metro was integrated into an ambitious urban project that, to date, has improved accessibility dramatically for some low-income residents. Using data before and after the project’s implementation, we evaluate the impact on social equity for the population in the zone of influence, considering changes in accessibility to employment and in housing-related costs. The access provided by the project to the main high-employment centres has doubled the number of opportunities that can be reached by the “target population,” even though travel-time savings and costs have seen only small changes. In fact, prior access to the CBD was poor and expensive, but time and costs were reduced with the Metrocable, although this reduction was not equal for all locations in the metropolitan area. In general, we argue that the main benefits, in terms of accessibility that differentiates the areas analysed from those used for comparison, are related to a localised ease of access to specific centres of activity according to the centralised development of the city’s job market along the mass transit lines. In terms of housing costs, we developed a set of difference-in-difference models that considered rent, transport, and public utilities costs; however, none of them have allowed us to conclude that there was a statistically valid relationship between the Metrocable and the changes in costs between the two analysed populations.', 'corpus_id': 153827744, 'score': 1}, {'doc_id': '109387262', 'title': ""Delivering the Emirates Air Line, London – Britain's first urban cable car"", 'abstract': 'The Emirates Air Line is a spectacular 1·1 km cable car which crosses the River Thames in London at up to 90 m above the ground. Forming part of the Docklands Light Railway network, the £60 million...', 'corpus_id': 109387262, 'score': 1}, {'doc_id': '202544537', 'title': 'The impact of an urban cable-car transport system on the spatial configuration of an informal settlement . The Case of', 'abstract': 'In Latin American, informality is fast becoming a recognised part of cities, increasing awareness of urban poverty and segregated communities. This has led to the rapid rise of urban interventions of which cable-car transport systems is one of the most popular. These address physical causes of segregation such as steep slopes and poor road layouts, whilst offering fast connections into the city. In Latin America these transport systems can be seen in Medellin, Caracas, Rio, Manizales and most recently La Paz. Their appeal is low cost, relatively quick construction, minimum disruption to existing urban fabric and low emission levels, making them very attractive to municipalities. However, many believe they only benefit immediate residents, offer limited socioeconomics gains and are merely political tools. Even their integrational worth is questioned, as the presumption that these alone can integrate the urban poor is queried. So how can we better interpret their impact and does this form of integration alter informal settlements? Through the case of Medellin, this paper will explore these issues, looking at the role spatial integration (in this case urban cable-cars) plays in the upgrading process and its relationship to other urban conditions that contribute to this transformation. This is done by comparing the spatial configuration of the city and the areas surrounding the cable-cars, ‘WITH’ and ‘WITHOUT’ the cablecar connection, through a variety of different metric scales. This is then correlated to a series of onsite observations into pedestrian movement patterns and land-use locations at two sites – San Javier and Santo Domingo. This demonstrates the spatial configuration of each case at a city and meso scale, is very important when implementing a new direct spatial connection, especially in terms of its integrational impact. However, this analysis also shows that whilst the cable-car clearly provides better connectivity into the city, local socio-components and possibly topology has a strong influence on the impact and benefits of the intervention at a local scale. Nevertheless, this paper argues, through its analysis and background research that the cable-car still played a pivotal role in the transformation of the informal settlement, especially in Santo Domingo, since without good spatial integration, the upgrading would have remained introvert and localised, whereas the cablecar offered this transformation the opportunity to synergize with the wider city. The lack of good citywide integration often leads to isolation, encouraging local socio-components to dominant, which is prevalent in informal settlements. This paper demonstrates the role spatial integration can play in these complex environments, so that we can better interpret and predict its impact.', 'corpus_id': 202544537, 'score': 1}, {'doc_id': '233282935', 'title': 'AN ANALYSIS OF THE POSTAL MARKET CONCERNING 24/7 COURIER SERVICE AVAILABILITY', 'abstract': 'In this paper, we analyzed the readiness and needs of the postal market for the concept of express delivery service with improved time availability, which together with the existing services includes 24/7 customer service. On the territory of the city of Belgrade, a survey was conducted on the attitudes and needs for the existing services of express shipments, as well as for the proposed new service, in order to obtain appropriate indicators. The research involved creating an appropriate questionnaire and collecting the opinions of respondents, which consisted of individuals and legal entities. The collected data were statistically analyzed, and the most significant results were presented in the paper.', 'corpus_id': 233282935, 'score': 0}, {'doc_id': '232067697', 'title': 'Ecology and Greenfield Precincts: Integrating Conservation and Bushfire Exposure Risk into Urban Planning', 'abstract': 'The rapid increase in urban expansion that is currently occurring around the world creates huge expanses of contested space on the fringes of cities and towns. Urban planners are on the frontline in trying to balance the social and economic pressures of providing affordable housing and accommodating increasing human populations, with the important challenge of meeting expectations around biodiversity conservation, a healthy environment, and a safe place to live. While there are a wide range of tools and expertise available to investigate the trade-offs between potentially competing land-uses and their spatial arrangements, there are few examples of how to draw upon these existing tools and incorporate them into the planning', 'corpus_id': 232067697, 'score': 0}, {'doc_id': '110956791', 'title': 'Public Transport and Accessibility in Informal Settlements: Aerial Cable Cars in Medellín, Colombia', 'abstract': 'The MetroCable in Medellin, the second largest city in Colombia, is perhaps one of the most prominent recent expansions of public transport infrastructure in urban Latin America. This article explores the question how and in what ways the MetroCable affects accessibility of the residents in two municipalities - Santa Cruz and Popular - where it operates since 2004. The presentation combines an analysis of the influence of the MetroCable on accessibility in general with a specific inquiry on female residents and the role of security. The paper adopts a differentiated conceptualization and indicators of accessibility. The analysis draws on statistical quantitative data from the Origin-Destination surveys for Medellin of 2005 and 2011/2 and about 30 in-depth semi structured interviews with female residents conducted in 2012. The paper concludes with a discussion on the possible lessons for planning interventions for improving accessibility of residents in low-income neighborhoods and a discussion on methodological implications.', 'corpus_id': 110956791, 'score': 1}]"
26	{'doc_id': '233405268', 'title': 'GRACE: A Compressed Communication Framework for Distributed Machine Learning', 'abstract': 'Powerful computer clusters are used nowadays to train complex deep neural networks (DNN) on large datasets. Distributed training increasingly becomes communication bound. For this reason, many lossy compression techniques have been proposed to reduce the volume of transferred data. Unfortunately, it is difficult to argue about the behavior of compression methods, because existing work relies on inconsistent evaluation testbeds and largely ignores the performance impact of practical system configurations. In this paper, we present a comprehensive survey of the most influential compressed communication methods for DNN training, together with an intuitive classification (i.e., quantization, sparsification, hybrid and low-rank). Next, we propose GRACE, a unified framework and API that allows for consistent and easy implementation of compressed communication on popular machine learning toolkits. We instantiate GRACE on TensorFlow and PyTorch, and implement 16 such methods. Finally, we present a thorough quantitative evaluation with a variety of DNNs (convolutional and recurrent), datasets and system configurations. We show that the DNN architecture affects the relative performance among methods. Interestingly, depending on the underlying communication library and computational cost of compression / decompression, we demonstrate that some methods may be impractical. GRACE and the entire benchmarking suite are available as open-source.', 'corpus_id': 233405268}	16648	"[{'doc_id': '6287870', 'title': 'TensorFlow: A system for large-scale machine learning', 'abstract': 'TensorFlow is a machine learning system that operates at large scale and in heterogeneous environments. Tensor-Flow uses dataflow graphs to represent computation, shared state, and the operations that mutate that state. It maps the nodes of a dataflow graph across many machines in a cluster, and within a machine across multiple computational devices, including multicore CPUs, general-purpose GPUs, and custom-designed ASICs known as Tensor Processing Units (TPUs). This architecture gives flexibility to the application developer: whereas in previous ""parameter server"" designs the management of shared state is built into the system, TensorFlow enables developers to experiment with novel optimizations and training algorithms. TensorFlow supports a variety of applications, with a focus on training and inference on deep neural networks. Several Google services use TensorFlow in production, we have released it as an open-source project, and it has become widely used for machine learning research. In this paper, we describe the TensorFlow dataflow model and demonstrate the compelling performance that TensorFlow achieves for several real-world applications.', 'corpus_id': 6287870, 'score': 1}, {'doc_id': '232417471', 'title': 'cuConv: A CUDA Implementation of Convolution for CNN Inference', 'abstract': 'Convolutions are the core operation of deep learning applications based on Convolutional Neural Networks (CNNs). Current GPU architectures are highly efficient for training and deploying deep CNNs, and hence, these are largely used in production for this purpose. State–of–the–art implementations, however, present a lack of efficiency for some commonly used network configurations. In this paper we propose a GPU-based implementation of the convolution operation for CNN inference that favors coalesced accesses, without requiring prior data transformations. Our experiments demonstrate that our proposal yields notable performance improvements in a range of common CNN forward propagation convolution configurations, with speedups of up to 2.29× with respect to the best implementation of convolution in cuDNN, hence covering a relevant region in currently existing approaches.', 'corpus_id': 232417471, 'score': 0}, {'doc_id': '232352854', 'title': 'Hierarchical Program-Triggered Reinforcement Learning Agents For Automated Driving', 'abstract': 'Recent advances in Reinforcement Learning (RL) combined with Deep Learning (DL) have demonstrated impressive performance in complex tasks, including autonomous driving [1]. The use of RL agents in autonomous driving leads to a smooth human-like driving experience, but the limited interpretability of Deep Reinforcement Learning (DRL) creates a verification and certification bottleneck. Instead of relying on RL agents to learn complex tasks, we propose HPRL Hierarchical Program-triggered Reinforcement Learning, which uses a hierarchy consisting of a structured program along with multiple RL agents, each trained to perform a relatively simple task. The focus of verification shifts to the master program under simple guarantees from the RL agents, leading to a significantly more interpretable and verifiable implementation as compared to a complex RL agent. The evaluation of the framework is demonstrated on different driving tasks, and NHTSA precrash scenarios using CARLA, an open-source dynamic urban simulation environment.', 'corpus_id': 232352854, 'score': 0}, {'doc_id': '233230120', 'title': 'A coevolutionary approach to deep multi-agent reinforcement learning', 'abstract': ""Traditionally, Deep Artificial Neural Networks (DNN's) are trained through gradient descent. Recent research shows that Deep Neuroevolution (DNE) is also capable of evolving multi-million-parameter DNN's, which proved to be particularly useful in the field of Reinforcement Learning (RL). This is mainly due to its excellent scalability and simplicity compared to the traditional MDP-based RL methods. So far, DNE has only been applied to complex single-agent problems. As evolutionary methods are a natural choice for multi-agent problems, the question arises whether DNE can also be applied in a complex multi-agent setting. In this paper, we describe and validate a new approach based on coevolution. To validate our approach, we benchmark two Deep coevolutionary Algorithms on a range of multi-agent Atari games and compare our results against the results of Ape-X DQN. Our results show that these Deep coevolutionary algorithms (1) can be successfully trained to play various games, (2) outperform Ape-X DQN in some of them, and therefore (3) show that coevolution can be a viable approach to solving complex multi-agent decision-making problems."", 'corpus_id': 233230120, 'score': 0}, {'doc_id': '231951561', 'title': 'Learning Memory-Dependent Continuous Control from Demonstrations', 'abstract': 'Efficient exploration has presented a long-standing challenge in reinforcement learning, especially when rewards are sparse. A developmental system can overcome this difficulty by learning from both demonstrations and self-exploration. However, existing methods are not applicable to most realworld robotic controlling problems because they assume that environments follow Markov decision processes (MDP); thus, they do not extend to partially observable environments where historical observations are necessary for decision making. This paper builds on the idea of replaying demonstrations for memorydependent continuous control, by proposing a novel algorithm, Recurrent Actor-Critic with Demonstration and Experience Replay (READER). Experiments involving several memory-crucial continuous control tasks reveal significantly reduce interactions with the environment using our method with a reasonably small number of demonstration samples. The algorithm also shows better sample efficiency and learning capabilities than a baseline reinforcement learning algorithm for memory-based control from demonstrations.', 'corpus_id': 231951561, 'score': 0}, {'doc_id': '3585888', 'title': 'Demystifying Parallel and Distributed Deep Learning', 'abstract': 'Deep Neural Networks (DNNs) are becoming an important tool in modern computing applications. Accelerating their training is a major challenge and techniques range from distributed algorithms to low-level circuit design. In this survey, we describe the problem from a theoretical perspective, followed by approaches for its parallelization. We present trends in DNN architectures and the resulting implications on parallelization strategies. We then review and model the different types of concurrency in DNNs: from the single operator, through parallelism in network inference and training, to distributed deep learning. We discuss asynchronous stochastic optimization, distributed system architectures, communication schemes, and neural architecture search. Based on those approaches, we extrapolate potential directions for parallelism in deep learning.', 'corpus_id': 3585888, 'score': 1}, {'doc_id': '233293904', 'title': 'GSHARD: SCALING GIANT MODELS', 'abstract': 'Neural network scaling has been critical for improving the model quality in many real-world machine learning applications with vast amounts of training data and compute. Although this trend of scaling is affirmed to be a sure-fire approach for better model quality, there are challenges on the path such as the computation cost, ease of programming, and efficient implementation on parallel devices. In this paper we demonstrate conditional computation as a remedy to the above mentioned impediments, and demonstrate its efficacy and utility. We make extensive use of GShard, a module composed of a set of lightweight annotation APIs and an extension to the XLA compiler to enable large scale models with up to trillions of parameters. GShard and conditional computation enable us to scale up multilingual neural machine translation Transformer model with Sparsely-Gated Mixture-ofExperts. We demonstrate that such a giant model with 600 billion parameters can efficiently be trained on 2048 TPU v3 cores in 4 days to achieve far superior quality for translation from 100 languages to English compared to the prior art.', 'corpus_id': 233293904, 'score': 1}, {'doc_id': '235433618', 'title': 'Distributed Training for Reinforcement Learning', 'abstract': 'Reinforcement learning (RL) has scaled up immensely over the last few years through the creation of innovative distributed training techniques. This paper discusses a rough timeline of the methods used to push the field forward. I begin by summarizing the problem of reinforcement learning and general solution methods. I then discuss the training environments used to evaluate model performance. I walk through a timeline of breakthroughs in distributed training used to scale up RL models, as well as other innovations in RL training. Finally, I take a look at exciting applications of distributed training processes in complex games like Go, Dota 2, and StarCraft II.', 'corpus_id': 235433618, 'score': 1}, {'doc_id': '233394573', 'title': 'Constraint-Guided Reinforcement Learning: Augmenting the Agent-Environment-Interaction', 'abstract': 'Reinforcement Learning (RL) agents have great successes in solving tasks with large observation and action spaces from limited feedback. Still, training the agents is data-intensive and there are no guarantees that the learned behavior is safe and does not violate rules of the environment, which has limitations for the practical deployment in real-world scenarios. This paper discusses the engineering of reliable agents via the integration of deep RL with constraint-based augmentation models to guide the RL agent towards safe behavior. Within the constraints set, the RL agent is free to adapt and explore, such that its effectiveness to solve the given problem is not hindered. However, once the RL agent leaves the space defined by the constraints, the outside models can provide guidance to still work reliably. We discuss integration points for constraint guidance within the RL process and perform experiments on two case studies: a strictly constrained card game and a grid world environment with additional combinatorial subgoals. Our results show that constraint-guidance does both provide reliability improvements and safer behavior, as well as accelerated training.', 'corpus_id': 233394573, 'score': 0}, {'doc_id': '233296692', 'title': 'Sync-Switch: Hybrid Parameter Synchronization for Distributed Deep Learning', 'abstract': ""Stochastic Gradient Descent (SGD) has become the de facto way to train deep neural networks in distributed clusters. A critical factor in determining the training throughput and model accuracy is the choice of the parameter synchronization protocol. For example, while Bulk Synchronous Parallel (BSP) often achieves better converged accuracy, the corresponding training throughput can be negatively impacted by stragglers. In contrast, Asynchronous Parallel (ASP) can have higher throughput, but its convergence and accuracy can be impacted by stale gradients. To improve the performance of synchronization protocol, recent work often focuses on designing new protocols with a heavy reliance on hard-to-tune hyper-parameters. In this paper, we design a hybrid synchronization approach that exploits the benefits of both BSP and ASP, i.e., reducing training time while simultaneously maintaining the converged accuracy. Based on extensive empirical profiling, we devise a collection of adaptive policies that determine how and when to switch between synchronization protocols. Our policies include both offline ones that target recurring jobs and online ones for handling transient stragglers. We implement the proposed policies in a prototype system, called Sync-Switch, on top of TensorFlow, and evaluate the training performance with popular deep learning models and datasets. Our experiments show that Sync-Switch can achieve ASP level training speedup while maintaining similar converged accuracy when comparing to BSP. Moreover, Sync-Switch's elastic-based policy can adequately mitigate the impact from transient stragglers."", 'corpus_id': 233296692, 'score': 1}]"
27	{'doc_id': '67855847', 'title': 'Structure Tree-LSTM: Structure-aware Attentional Document Encoders', 'abstract': 'We propose a method to create document representations that reflect their internal structure. We modify Tree-LSTMs to hierarchically merge basic elements like words and sentences into blocks of increasing complexity. Our Structure Tree-LSTM implements a hierarchical attention mechanism over individual components and combinations thereof. We thus emphasize the usefulness of Tree-LSTMs for texts larger than a sentence. We show that structure-aware encoders can be used to improve the performance of document classification. We demonstrate that our method is resilient to changes to the basic building blocks, as it performs well with both sentence and word embeddings. The Structure Tree-LSTM outperforms all the baselines on two datasets when structural clues like sections are available, but also in the presence of mere paragraphs. On a third dataset from the medical domain, our model achieves competitive performance with the state of the art. This result shows the Structure Tree-LSTM can leverage dependency relations other than text structure, such as a set of reports on the same patient.', 'corpus_id': 67855847}	4	"[{'doc_id': '207930656', 'title': 'Learning from a Teacher using Unlabeled Data', 'abstract': 'Knowledge distillation is a widely used technique for model compression. We posit that the teacher model used in a distillation setup, captures relationships between classes, that extend beyond the original dataset. We empirically show that a teacher model can transfer this knowledge to a student model even on an {\\it out-of-distribution} dataset. Using this approach, we show promising results on MNIST, CIFAR-10, and Caltech-256 datasets using unlabeled image data from different sources. Our results are encouraging and help shed further light from the perspective of understanding knowledge distillation and utilizing unlabeled data to improve model quality.', 'corpus_id': 207930656, 'score': 1}, {'doc_id': '67788603', 'title': 'ScispaCy: Fast and Robust Models for Biomedical Natural Language Processing', 'abstract': 'Despite recent advances in natural language processing, many statistical models for processing text perform extremely poorly under domain shift. Processing biomedical and clinical text is a critically important application area of natural language processing, for which there are few robust, practical, publicly available models. This paper describes scispaCy, a new Python library and models for practical biomedical/scientific text processing, which heavily leverages the spaCy library. We detail the performance of two packages of models released in scispaCy and demonstrate their robustness on several tasks and datasets. Models and code are available at https://allenai.github.io/scispacy/.', 'corpus_id': 67788603, 'score': 1}, {'doc_id': '219982272', 'title': 'Evaluating German Transformer Language Models with Syntactic Agreement Tests', 'abstract': 'Pre-trained transformer language models (TLMs) have recently refashioned natural language processing (NLP): Most state-of-the-art NLP models now operate on top of TLMs to benefit from contextualization and knowledge induction. To explain their success, the scientific community conducted numerous analyses. Besides other methods, syntactic agreement tests were utilized to analyse TLMs. Most of the studies were conducted for the English language, however. In this work, we analyse German TLMs. To this end, we design numerous agreement tasks, some of which consider peculiarities of the German language. Our experimental results show that state-of-the-art German TLMs generally perform well on agreement tasks, but we also identify and discuss syntactic structures that push them to their limits.', 'corpus_id': 219982272, 'score': 0}, {'doc_id': '207930679', 'title': 'Classifying Relevant Social Media Posts During Disasters Using Ensemble of Domain-agnostic and Domain-specific Word Embeddings', 'abstract': 'The use of social media as a means of communication has significantly increased over recent years. There is a plethora of information flow over the different topics of discussion, which is widespread across different domains. The ease of information sharing has increased noisy data being induced along with the relevant data stream. Finding such relevant data is important, especially when we are dealing with a time-critical domain like disasters. It is also more important to filter the relevant data in a real-time setting to timely process and leverage the information for decision support. \nHowever, the short text and sometimes ungrammatical nature of social media data challenge the extraction of contextual information cues, which could help differentiate relevant vs. non-relevant information. This paper presents a novel method to classify relevant social media posts during disaster events by ensembling the features of both domain-specific word embeddings as well as more generic domain-agnostic word embeddings. Therefore, we develop and evaluate a hybrid feature engineering framework for integrating diverse semantic representations using a combination of word embeddings to efficiently classify a relevant social media post. The application of the proposed classification framework could help in filtering public posts at large scale, given the growing usage of social media posts in recent years.', 'corpus_id': 207930679, 'score': 0}, {'doc_id': '220381366', 'title': 'scb-mt-en-th-2020: A Large English-Thai Parallel Corpus', 'abstract': ""The primary objective of our work is to build a large-scale English-Thai dataset for machine translation. We construct an English-Thai machine translation dataset with over 1 million segment pairs, curated from various sources, namely news, Wikipedia articles, SMS messages, task-based dialogs, web-crawled data and government documents. Methodology for gathering data, building parallel texts and removing noisy sentence pairs are presented in a reproducible manner. We train machine translation models based on this dataset. Our models' performance are comparable to that of Google Translation API (as of May 2020) for Thai-English and outperform Google when the Open Parallel Corpus (OPUS) is included in the training data for both Thai-English and English-Thai translation. The dataset, pre-trained models, and source code to reproduce our work are available for public use."", 'corpus_id': 220381366, 'score': 0}, {'doc_id': '199442469', 'title': 'ETNLP: A Visual-Aided Systematic Approach to Select Pre-Trained Embeddings for a Downstream Task', 'abstract': 'Given many recent advanced embedding models, selecting pre-trained word representation (i.e., word embedding) models best fit for a specific downstream NLP task is non-trivial. In this paper, we propose a systematic approach to extracting, evaluating, and visualizing multiple sets of pre-trained word embed- dings to determine which embeddings should be used in a downstream task. First, for extraction, we provide a method to extract a subset of the embeddings to be used in the downstream NLP tasks. Second, for evaluation, we analyse the quality of pre-trained embeddings using an input word analogy list. Finally, we visualize the embedding space to explore the embedded words interactively. We demonstrate the effectiveness of the proposed approach on our pre-trained word embedding models in Vietnamese to select which models are suitable for a named entity recogni- tion (NER) task. Specifically, we create a large Vietnamese word analogy list to evaluate and select the pre-trained embedding models for the task. We then utilize the selected embed- dings for the NER task and achieve the new state-of-the-art results on the task benchmark dataset. We also apply the approach to another downstream task of privacy-guaranteed embedding selection, and show that it helps users quickly select the most suitable embeddings. In addition, we create an open-source system using the proposed systematic approach to facilitate similar studies on other NLP tasks. The source code and data are available at https: //github.com/vietnlp/etnlp.', 'corpus_id': 199442469, 'score': 1}, {'doc_id': '220961667', 'title': 'Learning Visual Representations with Caption Annotations', 'abstract': 'Pretraining general-purpose visual features has become a crucial part of tackling many computer vision tasks. While one can learn such features on the extensively-annotated ImageNet dataset, recent approaches have looked at ways to allow for noisy, fewer, or even no annotations to perform such pretraining. Starting from the observation that captioned images are easily crawlable, we argue that this overlooked source of information can be exploited to supervise the training of visual representations. To do so, motivated by the recent progresses in language models, we introduce {\\em image-conditioned masked language modeling} (ICMLM) -- a proxy task to learn visual representations over image-caption pairs. ICMLM consists in predicting masked words in captions by relying on visual cues. To tackle this task, we propose hybrid models, with dedicated visual and textual encoders, and we show that the visual representations learned as a by-product of solving this task transfer well to a variety of target tasks. Our experiments confirm that image captions can be leveraged to inject global and localized semantic information into visual representations. Project website: this https URL.', 'corpus_id': 220961667, 'score': 0}, {'doc_id': '53083244', 'title': 'Neural Related Work Summarization with a Joint Context-driven Attention Mechanism', 'abstract': 'Conventional solutions to automatic related work summarization rely heavily on human-engineered features. In this paper, we develop a neural data-driven summarizer by leveraging the seq2seq paradigm, in which a joint context-driven attention mechanism is proposed to measure the contextual relevance within full texts and a heterogeneous bibliography graph simultaneously. Our motivation is to maintain the topic coherency between a related work section and its target document, where both the textual and graphic contexts play a big role in characterizing the relationship among scientific publications accurately. Experimental results on a large dataset show that our approach achieves a considerable improvement over a typical seq2seq summarizer and five classical summarization baselines.', 'corpus_id': 53083244, 'score': 1}, {'doc_id': '207930303', 'title': 'Adapting and evaluating a deep learning language model for clinical why-question answering', 'abstract': 'Abstract Objectives To adapt and evaluate a deep learning language model for answering why-questions based on patient-specific clinical text. Materials and Methods Bidirectional encoder representations from transformers (BERT) models were trained with varying data sources to perform SQuAD 2.0 style why-question answering (why-QA) on clinical notes. The evaluation focused on: (1) comparing the merits from different training data and (2) error analysis. Results The best model achieved an accuracy of 0.707 (or 0.760 by partial match). Training toward customization for the clinical language helped increase 6% in accuracy. Discussion The error analysis suggested that the model did not really perform deep reasoning and that clinical why-QA might warrant more sophisticated solutions. Conclusion The BERT model achieved moderate accuracy in clinical why-QA and should benefit from the rapidly evolving technology. Despite the identified limitations, it could serve as a competent proxy for question-driven clinical information extraction.', 'corpus_id': 207930303, 'score': 1}, {'doc_id': '220936295', 'title': 'LT@Helsinki at SemEval-2020 Task 12: Multilingual or Language-specific BERT?', 'abstract': 'This paper presents the different models submitted by the LT@Helsinki team for the SemEval 2020 Shared Task 12. Our team participated in sub-tasks A and C; titled offensive language identification and offense target identification, respectively. In both cases we used the so-called Bidirectional Encoder Representation from Transformer (BERT), a model pre-trained by Google and fine-tuned by us on the OLID and SOLID datasets. The results show that offensive tweet classification is one of several language-based tasks where BERT can achieve state-of-the-art results.', 'corpus_id': 220936295, 'score': 0}]"
28	{'doc_id': '219180114', 'title': 'Virtualization of science education: a lesson from the COVID-19 pandemic', 'abstract': 'Nowadays, e-learning and virtual labs have gained substantial popularity in science education. Amid the COVID-19 shutdowns, regular in-person classroom teaching and lab courses are suspended in several countries worldwide. In this scenario, virtual classes and online resources could serve more effectively as a possible alternative way of learning science from home.', 'corpus_id': 219180114}	14718	"[{'doc_id': '232120905', 'title': 'Science Education and the Pandemic, 1 Year On', 'abstract': 'One year has passed since my editorial about science education in the pandemic era was published online in March 2020. In one sense, it feels as though a longer period has elapsed. In another sense, it only seems a few weeks ago when I was pondering about what the impending pandemic meant for research on the applications of history, philosophy, and sociology of science in science education. Our experience of time can seem distorted and confused at times during this pandemic, as evidenced by the typographical error in that editorial where I referred to my doctoral student’s research taking place in February 2019 when only 1 month had elapsed. Time seems to have taken a different meaning in our everyday lives, but can “time” emerge from its benign presence in our research endeavours to become a focus of investigation for science education? In a recent paper, Levrini et al. (2021) have explored precisely this question by referring to concepts such has “alienation from time”, “time re-appropriation”, and “era of future shock” and drawing on perspectives from sociology of time to understand how students’ daily rituals in schools detach their science learning from broader societal issues. Such concepts are providing us with new ways of interpreting what the pandemic has meant for science education in the past year. Indeed, the pandemic context is helping researchers galvanize new and fresh perspectives on how to understand and improve science education to make it relevant for today’s circumstances. In this issue of Science & Education, Pietrocola and colleagues characterize the Covid-19 pandemic as “manufactured risk”. The authors question new roles for science education based on “risk society theory”, a conceptual framework that can act as a tool for understanding how to align science education with the contemporary demands on problemsolving. They argue that students are unable to deal with manufactured risk because of the type of problems they are usually prepared to solve at school and the limited risk perception they possess. The authors propose the integration in science courses of “wicked problems” with a multidimensional schema that they refer to as “amplified risk perception space” in order to identify and assess students’ risk perception. Science & Education https://doi.org/10.1007/s11191-021-00201-6', 'corpus_id': 232120905, 'score': 1}, {'doc_id': '231721439', 'title': 'Status of tertiary level online class in Bangladesh: students’ response on preparedness, participation and classroom activities', 'abstract': 'In the era of technology, every time the world confronts any kind of crisis or challenge, we use technology as a weapon. Like other emergencies, as COVID-19 is announced as a pandemic, all countries have started trying to control the situation with technological advancement in the medical sector, educational progress, and in the continuity of productions. As most of the educational institutions have been closed since March and the learning process in higher education has moved online, therefore, developing countries like Bangladesh are also trying to continue classes through the online platform with a lack of technological resources, readiness, and inclusiveness from the perspective of the students. This quantitative study surveyed over 844 students of different universities of Bangladesh to analyze the status of preparedness, participation, and classroom activities through online during the pandemic. The findings revealed a lack of preparedness, participation, and less scope of classroom activities through online learning. Problems of infeasible consistency of the internet and electricity, paying attention, understanding lessons through the online platform are the main constraints of online learning in the developing country. Finding ways of mitigating these problems can be the next subject for further researchers.', 'corpus_id': 231721439, 'score': 0}, {'doc_id': '231597961', 'title': 'A Call for Rethinking Schooling and Leadership in the Time of COVID-19', 'abstract': ""COVID-19 forced a sudden closure of schools, prompting a hasty and unplanned reaction of educators to deliver educational content. Inspired by Ivan Illich's book Deschooling Society, where he argues for the delivery of educational content by utilizing technology and forging intentional partnerships with parents and communities to assist in the delivery of educational content, we reflect on how these ideas impact school leadership and preparation of school leaders. This “forced” deschooling has offered educators an opportunity to rethink the true purpose of education, and redesign flexible, creative and innovative instructional strategies for delivering educational materials and knowledge, as well as rethinking the role of and preparation of educational leaders. While we do not offer quick solutions, our intent is to revisit Illich's Deschooling Society as a means to examine and question our school system introspectively and collectively."", 'corpus_id': 231597961, 'score': 0}, {'doc_id': '231639768', 'title': 'Rethinking community participation in education post Covid-19', 'abstract': 'Covid-19 has put education in a challenging place. With millions of children out of school, education needs to reinvent itself. During the pandemic, communities have used this opportunity to support children’s education in various ways, including the launching of new online classes. This article takes the social capital theoretical model as its framework and applies it to help communities strengthen their education support systems. It also looks at models from the literature as best practices to operationalize social capital. The article highlights various models for community participation that could continue after the Covid-19 pandemic. It urges the school system to be flexible and incorporate community-driven parental engagement with project-based and experiential learning. These community-driven educational programs must be supported to provide much-needed place-based supplementary education opportunities to students.', 'corpus_id': 231639768, 'score': 0}, {'doc_id': '214746217', 'title': 'Science Education in the Era of a Pandemic', 'abstract': 'In late February 2019, when the Covid-19 crisis began to spread across South Korea, my doctoral student Wonyong Park was there for his data collection in secondary schools. Unphased by the growing national epidemic at the time, he remarked: “As a Cambridge student, Newton once had to return home due to the plague outbreak in England, during which he made his greatest discoveries! However, as the situation develops, I promise I’ll keep healthy and make this time most useful for me.” Now, about a month later, the entire planet finds itself in the midst of a pandemic. Among the countries worst hit by the pandemic is Italy where one of our Associate Editors, Olivia Levrini, is based. During a recent exchange, she raised the question of how history, philosophy and sociology of science (HPS) might contribute to science education in the era of a pandemic. Given the novelty of the issues generated by a major health emergency, Science & Education invites colleagues to submit papers broadly addressing the following theme: “Science Education in the Era of a Pandemic: How can History, Philosophy and Sociology of Science Contribute to Education for Understanding and Solving the Covid-19 Crisis?” Past pandemics can point to not only the development of scientific explanations in time but also the societal contexts that harboured them. Consider the stigma associated with syphilis characterised as a French or an Italian disease depending on where the disease was observed in the fifteenth century; the framing of cholera in the colonial discourse in Asia and the impact of the disease on global trade in the nineteenth century; or the mistaken causality drawn between a country and an outbreak in the case of the Spanish flu in the early twentieth century. History is replete with countless lessons about pandemics in terms of their societal, ethical as well as their scientific and medical dimensions. The current pandemic is set against a backdrop of growing mistrust in science sometimes deliberately promoted for political ends, for instance, as is the case of climate change denial. There is an unprecedented need to educate the future scientists as well as the general public in engaging not only in evidence-based reasoning and critical thinking Science & Education https://doi.org/10.1007/s11191-020-00122-w', 'corpus_id': 214746217, 'score': 1}, {'doc_id': '221865721', 'title': 'Lessons from the pandemic about science education', 'abstract': 'The coronavirus pandemic vividly illuminates deficiencies of the Next Generation Science Standards (NGSS). The NGSS does not mention immunization, antibodies, vaccines, or the Centers for Disease Control and Prevention, nor do they ask students to investigate a topic outside the standards or read a science-related book or article. The stated goal of the standards is to prepare students for college and careers, which means that they do not prioritize how science connects to societal or personal concerns of every adult, even those who do not pursue science in higher education or their careers. Andrew Zucker and Pendred Noyce suggest several approaches to improve the NGSS.', 'corpus_id': 221865721, 'score': 1}, {'doc_id': '231844944', 'title': 'Educators’ Experiences Online: How COVID-19 Encouraged Pedagogical Change in CS Education', 'abstract': 'The COVID-19 lockdown in the spring of 2020 created a unique pedagogical change situation. Educators had to make significant and rapid changes to their teaching approaches, with the time frame being in the magnitude of hours, not weeks or months. At NTNU, a survey was conducted among the educators shortly after the lockdown to study how the educators experienced the change from campus based face-to-face learning to online learning. A total of 56 educators responded to the survey, with 22 of these affiliated with a Computer Science (CS) department. Nearly all the CS educators reported having a positive change experience during this time. More than half of the CS educators reported having prior online teaching experience, while nearly three quarters reported having sufficient or partially sufficient competence needed for the change. In this survey, CS educators highlighted pedagogical challenges as the main challenge. The findings also highlight the fact that some educators found aspects of online teaching to be better than campus based teaching and that CS educators collaborate and exchange pedagogical experience when facing change. Approximately two thirds of the CS educators reported that they consulted a more experienced person or worked closely with colleagues when making the change from a face-toface mode of delivery to that of an online only mode of delivery. Given the variety of experiences reported and the willingness to collaborate and exchange experience, it can be argued that CS staff and other departments may choose a path of knowledge sharing and communities to support future blended and online teaching opportunities.', 'corpus_id': 231844944, 'score': 0}, {'doc_id': '231662922', 'title': 'Waking up to the dawn of a new era: Reconceptualization of curriculum post Covid-19', 'abstract': 'Covid-19, the most severe global pandemic since the Spanish flu that followed World War I, threatens nearly every country, from global powers to developing nations. This threat presents a concurrent challenge for educational systems. With schools closed during the pandemic, students and teachers have had to stay at home worldwide. This shift has required us to move beyond conventional ideas regarding education. From kindergarten to higher education, schools offered web-based and online classes, both synchronous and asynchronous. However, while technology emerged as a savior, it is not possible to achieve thorough learning only by listening or watching content. Instead of championing technologies in which pedagogy is irrelevant, schools must invest in helping students become lifelong learners, enrich their learning processes, and focus on critical self-reflection, problem-solving skills, imagination, ideas, and projects involving social problems. A significant attempt to redefine the concepts that we have traditionally used must be made. The aim of this article is to develop new suggestions on how curriculum as the essence and core of all educational systems can be reconceptualized for the post-Covid-19 era.', 'corpus_id': 231662922, 'score': 0}, {'doc_id': '219747101', 'title': 'Science Teacher Education in the Times of the COVID-19 Pandemic', 'abstract': 'We write this editorial at a time when there are more than 4 million cases of COVID-19 globally, which includes over a million cases in the United States. The pandemic has caused more than 280,000 ...', 'corpus_id': 219747101, 'score': 1}, {'doc_id': '231772557', 'title': 'Science in mobile apps for education: tackling online learning within pandemic outbreak', 'abstract': 'In adapting the issues of pandemic, educational sectors in Malaysia from the lower school level till university levels are encouraged to forecast any specific tools that may help in cater the needs in utilizing online learning. It is by right the issues that been face by global outbreak for everyone on earth as Covid19 that embarked big challenges in face to face and normal daily classes and learning method. Therefore this paper is written to give a general overview, uncover the possible models in used and technologies that may overcome the needs of online education specifically focused for improving the usage of online educational tools especially with the use of mobile ap-plication for learning itself. The concept of manual for pen and paper reporting can makes the reporting very slow especially in terms of communication and sharing of information. Plus, within the outbreak patterns may will provide inaccurate information and may also late results to be achieved. This paper is discussing model in plan and the implementation proposed with overcome results of the post results questionnaire conducted to get the major feedback on usage of the current proposed app.', 'corpus_id': 231772557, 'score': 1}]"
29	"{'doc_id': '236286401', 'title': 'Stock market reactions to R&D cuts used to manage earnings', 'abstract': ""Abstract Prior research shows that stock returns are positive when firms meet or beat analysts' consensus earnings forecasts but negative when they miss. Past studies also show that managers frequently cut research and development (R&D) expenses in order to meet the consensus forecast. This study shows that the stock market penalizes this behavior and exacts a discount to the market reward if beating the forecast requires cutting R&D. However, it is only a partial discount and firms are still better off managing R&D expenditures in the short run. This study also shows that the reductions in R&D are likely temporary, as firms tend to increase R&D spending in the subsequent periods. Investors appear to recognize these short-term cuts and treat them similarly to accruals."", 'corpus_id': 236286401}"	19825	"[{'doc_id': '235719783', 'title': 'Private equity and bank capital requirements: Evidence from European firms', 'abstract': 'Using firm-level data from 16 euro-area countries over 2008-2014, we investigate how the growth and investment of bank-affiliated private equity-backed companies evolve after the European Banking Authority (EBA) increases capital requirements for their parent banks. We find that portfolio companies connected to affected banks reduce their investment, asset growth, and employment growth following the capital exercise. We further show that the effect is stronger for companies likely to face financial constraints. Finally, the findings indicate that the negative effect of the capital exercise is muted when the private equity sponsor is more experienced.', 'corpus_id': 235719783, 'score': 0}, {'doc_id': '236445861', 'title': 'Disclosure Tone and Short-Selling Pressure: Evidence from Regulation-SHO', 'abstract': 'Managers use disclosure tone as a strategic tool to manage investors’ expectation and demand for information. I provide evidence that investors’ actions can in turn exercise a disciplining effect on tone management. Using an exogenous relaxation in the short-selling constraints from Regulation-SHO, I find that short-selling pressure reduces tone management. Greater short selling pressure results in less optimism in tone unrelated to fundamentals, and in disclosures about the past rather than the future. This reduction in tone management is stronger for firms with higher short-selling constraints pre-SHO, overoptimistic and overconfident managers, and lower analyst coverage.', 'corpus_id': 236445861, 'score': 1}, {'doc_id': '236207402', 'title': 'Earnings Forecasts of Female CEOs: Quality and Consequences', 'abstract': 'This study examines the voluntary disclosure of earnings forecasts by female CEOs. We find that in the backdrop of increased pressure to perform from investors and other stakeholders, female CEOs tend to issue more earnings forecasts than male CEOs, and those forecasts are more accurate. We also find that while financial analysts generally prefer to follow companies headed by male CEOs, female CEOs’ efforts to issue accurate earnings forecasts pay off as these efforts help them close the analyst coverage gap. We provide complementary evidence on the disclosure efforts of female CEOs with regard to updates to the forecast and the 10-K report. Lastly, we show that financial analysts rely more on the earnings forecasts of female CEOs, possibly because they realize female CEOs’ superior forecasting quality. Our results are robust to the use of alternative research designs, including difference-in-difference, propensity score matching, and entropy balancing. Overall, our study documents gender differences in voluntary disclosure by senior management.', 'corpus_id': 236207402, 'score': 0}, {'doc_id': '235739851', 'title': 'Firm Fundamentals in the COVID-19 Stock Market', 'abstract': 'The effects of the COVID-19 crisis allow for an opportunity to examine what types of firms are most durable to sudden and prolonged stock market shocks. Using data on the constituents of the Russell 3000, I find that firms with more capital-intensive operations, higher leverage, and larger cash balances were most prone to drops in daily return related to increases in the growth of COVID-19 infections. Larger, more profitable, and more liquid firms saw positive interactions between daily returns and COVID-19 infection growth. However, I also find a potential reversal of these trends as riskier and more leveraged firms exhibit higher cumulative abnormal returns off vaccine hopes, which may signal a renewed interest in riskier investment profiles as the world begins and continues to move past the COVID-19 pandemic. This study presents insight on daily stock return reactions to COVID-19 infection growth throughout the course of the virus.', 'corpus_id': 235739851, 'score': 0}, {'doc_id': '17115306', 'title': 'Detecting Corporate Fraud: An Application of Machine Learning', 'abstract': 'This paper explores the application of several machine learning algorithms to published corporate data in an effort to identify patterns indicative of securities fraud. Generally Accepted Accounting Principles (GAAP) represent a conglomerate of industry reporting standards which US public companies must abide by to aid in ensuring the integrity of these companies. Notwithstanding these principles, US public companies have legal flexibility to maneuver the way they disclose certain items in the financial statements making it extremely hard to detect fraud manually. Here we test several popular methods in machine learning (logistic regression, naive Bayes and support vector machines) on a large set of financial data and evaluate their accuracy in identifying fraud. We find that some variants of SVM and logistic regression are significantly better than the currently available methods in industry. Our results are encouraging and call for a more thorough investigation into the applicability of machine learning techniques in corporate fraud detection.', 'corpus_id': 17115306, 'score': 1}, {'doc_id': '236491863', 'title': 'Post-litigation reporting conservatism', 'abstract': 'We investigate changes in financial reporting conservatism arising from disclosure-related shareholder lawsuits filed between 1996 and 2016. We find that sued firms respond to 10b-5 litigation with increased accounting conservatism. Consistent with a spillover effect, we also find that non-sued peer firms increase accounting conservatism following litigation. Despite the FASB having eliminated conservatism as an essential qualitative characteristic from the conceptual framework in 2010, we find the post-suit spike in conservatism persists after 2010, suggesting a capital market demand for conservatism even without regulator intervention. Our results, which support the widespread but untested belief that litigation events induce accounting conservatism, extend prior studies examining the disclosure effects of litigation into the accounting choice effects of litigation.', 'corpus_id': 236491863, 'score': 1}, {'doc_id': '150882345', 'title': 'Announcing the Announcement', 'abstract': ""What drives investors’ attention? We study how far in advance earnings calendars are pre-announced and find that investors are more attentive to earnings news when such details are disclosed well ahead of time. This variation in investors' attention affects short-run and long-run stock returns, thereby creating incentives for firms to strategically pre-announce the report date on short notice when the earnings news is bad. Consistent with this idea, firms pre-announce their report dates well ahead of time when earnings are good and do it at the very last moment when earnings are bad. A trading strategy that exploits such variations yields abnormal returns of 1.5% per month."", 'corpus_id': 150882345, 'score': 1}, {'doc_id': '236505302', 'title': 'Multiple large shareholders and corporate fraud: evidence from China', 'abstract': 'This study tests the effect of multiple large shareholders on the level of corporate fraud using the data of Chinese listed companies from 2010 to 2018. We find lower probabilities and lower corporate fraud frequencies when there are multiple large shareholders in Chinese listed companies, indicating that their presence plays a supervisory role in internal governance. These results persist after we control for endogeneity. Moreover, the effect of multiple large shareholders on corporate fraud is strengthened with the separation of control right and cash flow right. Further analyses reveal that companies with multiple large shareholders experience considerably reduced information disclosure fraud but no reduction in operating or leader frauds. Additionally, information asymmetry and the capital occupation of controlling shareholders both play a mediating role in the relationship between multiple large shareholders and the level of corporate fraud. This study enriches the literature on the determinants of corporate fraud and the effects of multiple large shareholders. Our findings also provide implications for companies and regulators regarding ways to reduce fraud.', 'corpus_id': 236505302, 'score': 0}, {'doc_id': '237044860', 'title': 'Litigation and Information Effects on Private Sales of Securities', 'abstract': 'We analyze the resolution of information asymmetry in transactions of private investments in public equity (PIPEs) in which the issuer has experienced class action lawsuits. We explain the associated wealth and pricing effects (information effects). We show that litigated PIPEs are associated with higher announcement wealth effects and lower levels of discounts than non-litigated PIPEs. We find that disclosure, prior investors’ ownership, early registration, and intermediation have positive information effects on incumbent and new investors when issuers concurrently pursue the desired corporate actions (proxied by the auditor changes). We conclude that PIPE issuers can mitigate information effects from prior litigation.', 'corpus_id': 237044860, 'score': 1}, {'doc_id': '236451378', 'title': 'Do algorithm traders mitigate insider trading profits?: Evidence from the Thai stock market', 'abstract': 'This paper asks whether algorithm traders (AT) mitigate insider trading profits in the Thai stock market over the period of 2010–2016. We find that in general it does but not in the case of buy side, big trades nor the executive trades. Our findings suggest that, to some extent, AT can take important role to increase an efficiency in stock market by processing the public information and incorporating it into price at ultra-fast speed. Additional robustness checks based on the instrumental variable approach confirm our findings.', 'corpus_id': 236451378, 'score': 0}]"
30	{'doc_id': '150478992', 'title': 'Data activism and social change', 'abstract': 'obtain skills and jobs), they are fundamentally different in structure and mission. The authors imply that analyzing these differentiations is key to understanding the complexities and nuances ofmodels of SIM programing, which can help to duplicate successes in other contexts. Chapter Five’s description of the creative non-profit workforce sets up the reader for Chapter Seven, where the authors describe the motivations for adult workers in non-profit youth programing like those described in Chapters Six and Eight. Again, the authors take an interdisciplinary approach to the topic by engaging theories from sociology and philosophy to read their ethnographic results from the field, leading them to determine that creative nonprofit workers do their work with reputation and reciprocal behavior in mind. They also acknowledge the role that social media play in promoting recognition for good deeds, increasing the visibility and, therein, capacity for these motivators. The book concludes with a critical discussion on “the intense preoccupation with appraisal (prospective assessment) and evaluation (retrospective assessment) in the SIM field” (p. 108), where the expectation for funding in today’s economy requires assurance in some way of a return on investment. This is a complicated task for programs with the aim of “social benefit,” and the authors recognize and critique the culture of assessment that non-profit work must cater to, such as quantifying successes and then aiming primarily for those benchmarks. That said, they provide an extensive review of methodology and conclude that participatory ethnographic evaluation, although resource-demanding, is the optimal approach to evaluating programs like those presented in this book. Using media successfully bridges multiple areas of literature to comprise an insightful applied survey to SIM. Not only do the authors provide socio-economic context alongside thick descriptions of successes in the field of Social Innovation Media, the authors skillfully integrate both contemporary and canonical social theory throughout each chapter to underscore the relevance of these applied practices to an academic audience. That said, because the text jumps from case studies to empirical data, the flow of the book suggests it is best suited as an encyclopedic resource for creative developers in the non-profit world, with a secondary audience being students of strategic or non-profit communication who should be assigned specific chapters as needed.', 'corpus_id': 150478992}	20864	"[{'doc_id': '237610208', 'title': 'Transnational Solidarity Organisations and their Main Features, before and since 2008: Adaptive and/or Autonomous?', 'abstract': 'This article highlights the importance of crisis-related transformations experienced during the 2008–2016 period by transnationally oriented, citizen-led solidarity organisations, a topic that has received scant scholarly attention. It offers an exploratory, comparative analysis of the main features of these Transnational Solidarity Organisations (TSOs) which rests on a comprehensive conceptual framework of ‘alternative forms of resilience’, referring to the ability to bounce back from hardship and meet human needs in challenging times. We apply a new methodology, Action Organisation Analysis, which is based on information coded from organisational websites of solidarity organisations retrieved from online directories. Using a sample of 1753 TSOs, we examine two types of approaches: adaptive (philanthropic, formal, or reformist) and autonomous (mutual-help, informal, or contentious) ones. We document differential transformations for adaptive and autonomous TSOs, as reflected in their major characteristics, that is, their value frames, partners, and routes to achieve their goals and supplementary actions, across time and in three different issue fields: migration, disabilities, and unemployment. Notable are the increasing shifts towards social change and protests, especially for unemployment TSOs, and less so for migration ones. The findings contribute to debates on the impact of crises on activist solidarity organisations by documenting the dialectics of autonomy and adaptation across contemporary social issues, as well as by highlighting the importance of TSOs’ hybrid features. This analysis will also be useful for future work on transnational solidarity organisations and their transitions in a rapidly evolving global society.', 'corpus_id': 237610208, 'score': 0}, {'doc_id': '237537721', 'title': 'The Environmental Justice Movement as a Model Politics of Risk', 'abstract': 'Risk carries unique significance for democratic politics today as it faces the challenges of rising inequality, neoliberalism, and systemic racism. To show how, the article divides “risk” into two complementary political models: a technocratic logic of risk allocation, concerned primarily with safety, and a forensic logic of risk attribution, concerned with holding risk takers to account. Both have had pervasive effects on a transformed welfare state, increasingly focused on “personal responsibility” and privatized risk-management. But risk has also played a key role in the way post-1968 movements have organized and challenged the logic of privatization. Risk-based movements, the article argues, especially from the political margins, are key agents in promoting a new political form founded on risk attribution. The article focuses on the exemplary case of the American environmental justice movement in the 1980s–90s as it reframed social justice around three core demands: accountability from decision makers, equitable risk distribution, and broad participation in decisions about danger and communities’ well-being.', 'corpus_id': 237537721, 'score': 0}, {'doc_id': '150586372', 'title': 'Big Data from the South(s): Beyond Data Universalism', 'abstract': 'This article introduces the tenets of a theory of datafication of and in the Souths. It calls for a de-Westernization of critical data studies, in view of promoting a reparation to the cognitive injustice that fails to recognize non-mainstream ways of knowing the world through data. It situates the “Big Data from the South” research agenda as an epistemological, ontological, and ethical program and outlines five conceptual operations to shape this agenda. First, it suggests moving past the “universalism” associated with our interpretations of datafication. Second, it advocates understanding the South as a composite and plural entity, beyond the geographical connotation (i.e., “global South”). Third, it postulates a critical engagement with the decolonial approach. Fourth, it argues for the need to bring agency to the core of our analyses. Finally, it suggests embracing the imaginaries of datafication emerging from the Souths, foregrounding empowering ways of thinking data from the margins.', 'corpus_id': 150586372, 'score': 1}, {'doc_id': '237481820', 'title': 'The Generative Power of Protest: Time and Space in Contentious Politics', 'abstract': 'How do social movements sustain themselves under authoritarian rule? This remains a crucial puzzle for scholars of comparative politics. This article gains traction on this puzzle by foregrounding the generative power of protest, namely the power of protest experiences themselves to deepen and broaden movements. Some studies have started to draw attention to those questions without yet systematically examining how the form of protest differentially affects those outcomes. I argue that different forms of protest have varying effects on movements depending on their duration and geographic scope. While short, multiple-site actions, such as marches, can broaden movements by expanding their base, extended, single-site actions, such as sit-ins, are more likely to deepen movements by fostering collective identities and building organizational capacities. This article is based on field research in Egypt, Tunisia, Jordan, and Morocco and interviews with more than 100 movement participants and civil society activists.', 'corpus_id': 237481820, 'score': 0}, {'doc_id': '158302958', 'title': 'Design Justice, A.I., and Escape from the Matrix of Domination', 'abstract': None, 'corpus_id': 158302958, 'score': 1}, {'doc_id': '152166960', 'title': 'The Alternative Epistemologies of Data Activism', 'abstract': 'Abstract As datafication progressively invades all spheres of contemporary society, citizens grow increasingly aware of the critical role of information as the new fabric of social life. This awareness triggers new forms of civic engagement and political action that we term “data activism”. Data activism indicates the range of sociotechnical practices that interrogate the fundamental paradigm shift brought about by datafication. Combining Science and Technology Studies with Social Movement Studies, this theoretical article offers a foretaste of a research agenda on data activism. It foregrounds democratic agency vis-à-vis datafication, and unites under the same label ways of affirmative engagement with data (“proactive data activism”, e. g. databased advocacy) and tactics of resistance to massive data collection (“reactive data activism”, e. g. encryption practices), understood as a continuum along which activists position and reposition themselves and their tactics. The article argues that data activism supports the emergence of novel epistemic cultures within the realm of civil society, making sense of data as a way of knowing the world and turning it into a point of intervention and generation of data countercultures. It offers the notion of data activism as a heuristic tool for the study of new forms of political participation and civil engagement in the age of datafication, and explores data activism as an evolving theoretical construct susceptible to contestation and revision.', 'corpus_id': 152166960, 'score': 1}, {'doc_id': '237951735', 'title': 'The Politics of Civil Society Forms: Urban Environmental Activists and Democracy in Jakarta', 'abstract': ""Despite the ongoing debate regarding how and to what extent civil society enhances democratic practices, it is generally agreed that there is a reasonable link between civil society and democracy under certain conditions. This paper aims to explore the politics of civil society forms and understand their contribution to the maintenance of democratic practices in Jakarta. Building on a neo-Tocquevillian understanding of civil society, this article analyses urban environmental activists' strategic adoption of voluntary associations and environmental spin-off campaigns as forms of civic engagement to improve public policy. This paper asks how and to what extent these forms of civic engagement provide alternative understandings of civil society's efforts to promote local democracy. We argue that urban environmental activists' spin-off campaigns and voluntary associations represent a particular form of civil society politics, and thus provide different routes to understand local democracy by facilitating diagonal accountability mechanisms. However, further analysis found that the forms adopted by urban environmental activists suffer horizontal and vertical accountability problems similar to those frequently found in more established forms of civil society (e.g. non-government organisations). Nonetheless, the discussion in this paper illustrates civil society's ingenuity in pushing for democratic practices amidst Indonesia's 'democratic recession'."", 'corpus_id': 237951735, 'score': 0}, {'doc_id': '228974027', 'title': 'Data-bodies and data activism: Presencing women in digital heritage research', 'abstract': 'As heritage-as-the-already-occurred folds into heritage-in-the-making practices, temporal and spatial fluidity is made more complex by digital mediation and particularly by Big Data. Such liveliness evokes ontological, epistemological and methodological challenges. Drawing on more-than-human theorizing, this article reframes the notion of data-bodies to advance data activist-oriented research in heritage. Focused primarily on women, it examines how their distributed agency and voice with respect to data practices and the (re)makings of (digital) heritage could be amplified. I describe three methodological directions, influenced by feminist work in critical data studies, which could be employed by researchers: attuning to and becoming with data, making data physical and changing narratives. From data-bodies to haunted data, performative data curation and mapping data-bodies, and attuning to data streams and re-voicing narratives, this article contributes to discussions of how to engage critically and creatively with the datafication of digital heritage practices, knowings and ontologies.', 'corpus_id': 228974027, 'score': 1}, {'doc_id': '237308759', 'title': 'What makes a global movement? Analyzing the conditions for strong participation in the climate strike', 'abstract': 'The Fridays For Future movement and their global climate strikes put climate change on political agendas worldwide and created a new wave of climate activism. The emergence of a global movement is a rare and contingent phenomenon that promises insights for political sociology and globalization research. This study consists of a qualitative comparative analysis (QCA) of 17 democratic countries to analyze the conditions for strong mobilization of the third global climate strike. Four mechanisms are identified, showing that trust in environmental movements, the availability of resources through international nongovernmental organizations (INGOs) and information and communication technologies (ICT), and frame resonance are sufficient for explaining strong mobilization. These results illustrate that global movements depend on several equifinal mechanisms for mobilization on the nation-state level. Furthermore, the findings illustrate that the global features of a global movement are necessary but not sufficient for explaining its emergence.', 'corpus_id': 237308759, 'score': 0}, {'doc_id': '199020420', 'title': 'Indigenous Data Governance: Strategies from United States Native Nations', 'abstract': 'Data have become the new global currency, and a powerful force in making decisions and wielding power. As the world engages with open data, big data reuse, and data linkage, what do data-driven futures look like for communities plagued by data inequities? Indigenous data stakeholders and non-Indigenous allies have explored this question over the last three years in a series of meetings through the Research Data Alliance (RDA). Drawing on RDA and other gatherings, and a systematic scan of literature and practice, we consider possible answers to this question in the context of Indigenous peoples vis-a-vis two emerging concepts: Indigenous data sovereignty and Indigenous data\xa0governance.\xa0Specifically,\xa0we\xa0focus\xa0on\xa0the\xa0data\xa0challenges facing Native nations and the intersection of data, tribal sovereignty, and power. Indigenous data sovereignty is the right of each Native nation to govern the collection, ownership, and application of the tribe’s data. Native nations exercise Indigenous data sovereignty through the interrelated processes of Indigenous data governance and decolonizing data. This paper explores the implications of Indigenous data sovereignty and Indigenous data governance for Native nations and others. We argue for the repositioning of authority over Indigenous data back to Indigenous peoples. At the same time, we recognize that\xa0 there are significant obstacles to rebuilding effective Indigenous data systems and the process will require resources, time, and partnerships among Native nations, other governments, and data agents.', 'corpus_id': 199020420, 'score': 1}]"
31	{'doc_id': '218604091', 'title': 'Improved Algorithms for Learning Equilibria in Simulation-Based Games', 'abstract': 'We tackle a fundamental problem in empirical game-theoretic analysis (EGTA), that of learning equilibria of simulation-based games. Such games cannot be described in analytical form; instead, a blackbox simulator can be queried to obtain noisy samples of utilities. Our first theorem establishes that uniform approximations of simulationbased games are equilibrium preserving. We then design algorithms that uniformly approximate simulation-based games with finitesample guarantees. Our first algorithm, global sampling (GS), extends previous work that constructs confidence intervals assuming bounded utilities with confidence intervals that are sensitive to variance. The second, progressive sample with pruning (PSP), samples progressively, ceasing the sampling process (i.e., pruning strategies) as soon as it determines that the corresponding utilities have been sufficiently well estimated for equilibrium computation. We experiment with our algorithms using both GAMUT, a state-ofthe-art game generator, and Gambit, a state-of-the-art game solver. For a broad swath of games, we show that GS using our variancesensitive bounds outperforms previous work, and that PSP can significantly outperform GS. Here “outperform” means achieving the same guarantees with far fewer samples.', 'corpus_id': 218604091}	1172	"[{'doc_id': '221995421', 'title': 'Zero Knowledge Games', 'abstract': 'Zero-knowledge strategies as a form of inference and reasoning operate using the concept of zero-knowledge signaling, such that any imperfect recall or incomplete information can be attenuated for. The resulting effect of structuring a continuous game within a zero-knowledge strategy demonstrates the ability to infer, within acceptable probabilities, which approximate stage a player is in. This occurs only when an uninformed player attempts non-revealing strategies, resulting in a higher probability of failing to appear informed. Thus, an opposing player understanding their opponent is uninformed can choose a more optimal strategy. In cases where an informed player chooses a non-revealing strategy, introducing a hedge algebra as a doxastic heuristic informs feasibility levels of trust. A counter strategy employing such a hedge algebra facilitates optimal outcomes for both players, provided the trust is well placed. Given indefinite, finite sub-games leading to continued interactions based on trust, extensions to continuous games are feasible.', 'corpus_id': 221995421, 'score': 0}, {'doc_id': '209439918', 'title': 'On the Convergence of Model Free Learning in Mean Field Games', 'abstract': 'Learning by experience in Multi-Agent Systems (MAS) is a difficult and exciting task, due to the lack of stationarity of the environment, whose dynamics evolves as the population learns. In order to design scalable algorithms for systems with a large population of interacting agents (e.g., swarms), this paper focuses on Mean Field MAS, where the number of agents is asymptotically infinite. Recently, a very active burgeoning field studies the effects of diverse reinforcement learning algorithms for agents with no prior information on a stationary Mean Field Game (MFG) and learn their policy through repeated experience. We adopt a high perspective on this problem and analyze in full generality the convergence of a fictitious iterative scheme using any single agent learning algorithm at each step. We quantify the quality of the computed approximate Nash equilibrium, in terms of the accumulated errors arising at each learning iteration step. Notably, we show for the first time convergence of model free learning algorithms towards non-stationary MFG equilibria, relying only on classical assumptions on the MFG dynamics. We illustrate our theoretical results with a numerical experiment in a continuous action-space environment, where the approximate best response of the iterative fictitious play scheme is computed with a deep RL algorithm.', 'corpus_id': 209439918, 'score': 1}, {'doc_id': '204915924', 'title': 'A distributed proximal-point algorithm for Nash equilibrium seeking under partial-decision information with geometric convergence', 'abstract': 'We consider the Nash equilibrium seeking problem for a group of noncooperative agents, in a partial-decision information scenario. First, we recast the problem as that of finding a zero of a monotone operator. Then, we design a novel fully distributed, single-layer, fixed-step algorithm, which is a suitably preconditioned proximal-point iteration. We prove its convergence to a Nash equilibrium with geometric rate, by leveraging restricted monotonicity properties, under strong monotonicity and Lipschitz continuity of the game mapping. Remarkably, we show that our algorithm outperforms known gradient-based schemes, both in terms of theoretical convergence rate and in practice according to our numerical experience.', 'corpus_id': 204915924, 'score': 1}, {'doc_id': '231924605', 'title': 'Discounting the Past in Stochastic Games', 'abstract': 'Stochastic games, introduced by Shapley, model adversarial interactions in stochastic environments where two players choose their moves to optimize a discounted-sum of rewards. In the traditional discounted reward semantics, long-term weights are geometrically attenuated based on the delay in their occurrence. We propose a temporally dual notion—called past-discounting—where agents have geometrically decaying memory of the rewards encountered during a play of the game. We study past-discounted weight sequences as rewards on stochastic game arenas and examine the corresponding stochastic games with discounted and mean payoff objectives. We dub these games forgetful discounted games and forgetful mean payoff games, respectively. We establish positional determinacy of these games and recover classical complexity results and a Tauberian theorem in the context of past discounted reward sequences. 2012 ACM Subject Classification Computing methodologies → Stochastic games; Theory of computation → Stochastic control and optimization', 'corpus_id': 231924605, 'score': 1}, {'doc_id': '221246066', 'title': 'Comparison of Algorithms for Simple Stochastic Games', 'abstract': 'Simple stochastic games are turn-based 2.5-player zero-sum graph games with a reachability objective. The problem is to compute the winning probability as well as the optimal strategies of both players. In this paper, we compare the three known classes of algorithms -- value iteration, strategy iteration and quadratic programming -- both theoretically and practically. Further, we suggest several improvements for all algorithms, including the first approach based on quadratic programming that avoids transforming the stochastic game to a stopping one. Our extensive experiments show that these improvements can lead to significant speed-ups. We implemented all algorithms in PRISM-games 3.0, thereby providing the first implementation of quadratic programming for solving simple stochastic games.', 'corpus_id': 221246066, 'score': 1}, {'doc_id': '220870679', 'title': 'Improving approximate pure Nash equilibria in congestion games', 'abstract': 'Congestion games constitute an important class of games to model resource allocation by different users. As computing an exact or even an approximate pure Nash equilibrium is in general PLS-complete, Caragiannis et al. (2011) present a polynomial-time algorithm that computes a ($2 + \\epsilon$)-approximate pure Nash equilibria for games with linear cost functions and further results for polynomial cost functions. We show that this factor can be improved to $(1.61+\\epsilon)$ and further improved results for polynomial cost functions, by a seemingly simple modification to their algorithm by allowing for the cost functions used during the best response dynamics be different from the overall objective function. Interestingly, our modification to the algorithm also extends to efficiently computing improved approximate pure Nash equilibria in games with arbitrary non-decreasing resource cost functions. Additionally, our analysis exhibits an interesting method to optimally compute universal load dependent taxes and using linear programming duality prove tight bounds on PoA under universal taxation, e.g, 2.012 for linear congestion games and further results for polynomial cost functions. Although our approach yield weaker results than that in Bil\\`{o} and Vinci (2016), we remark that our cost functions are locally computable and in contrast to Bil\\`{o} and Vinci (2016) are independent of the actual instance of the game.', 'corpus_id': 220870679, 'score': 0}, {'doc_id': '224706322', 'title': 'Asynchronous Optimization Over Graphs: Linear Convergence Under Error Bound Conditions', 'abstract': 'We consider convex and nonconvex constrained optimization with a partially separable objective function: Agents minimize the sum of local objective functions, each of which is known only by the associated agent and depends on the variables of that agent and those of a few others. This partitioned setting arises in several applications of practical interest. We propose what is, to the best of our knowledge, the first distributed, asynchronous algorithm with rate guarantees for this class of problems. When the objective function is nonconvex, the algorithm provably converges to a stationary solution at a sublinear rate whereas linear rate is achieved under the renowned Luo-Tseng error bound condition (which is less stringent than strong convexity). Numerical results on matrix completion and LASSO problems show the effectiveness of our method.', 'corpus_id': 224706322, 'score': 0}, {'doc_id': '221266450', 'title': 'Search for a moving target in a competitive environment', 'abstract': 'We consider a discrete-time dynamic search game in which a number of players compete to find an invisible object that is moving according to a time-varying Markov chain. We examine the subgame perfect equilibria of these games. The main result of the paper is that the set of subgame perfect equilibria is exactly the set of greedy strategy profiles, i.e. those strategy profiles in which the players always choose an action that maximizes their probability of immediately finding the object. We discuss various variations and extensions of the model.', 'corpus_id': 221266450, 'score': 0}, {'doc_id': '221761368', 'title': 'Finding Effective Security Strategies through Reinforcement Learning and Self-Play', 'abstract': 'We present a method to automatically find security strategies for the use case of intrusion prevention. Following this method, we model the interaction between an attacker and a defender as a Markov game and let attack and defense strategies evolve through reinforcement learning and self-play without human intervention. Using a simple infrastructure configuration, we demonstrate that effective security strategies can emerge from self-play. This shows that self-play, which has been applied in other domains with great success, can be effective in the context of network security. Inspection of the converged policies show that the emerged policies reflect common-sense knowledge and are similar to strategies of humans. Moreover, we address known challenges of reinforcement learning in this domain and present an approach that uses function approximation, an opponent pool, and an autoregressive policy representation. Through evaluations we show that our method is superior to two baseline methods but that policy convergence in self-play remains a challenge.', 'corpus_id': 221761368, 'score': 0}, {'doc_id': '221266317', 'title': 'LP Formulations of Two-Player Zero-Sum Stochastic Bayesian games', 'abstract': ""This paper studies two-player zero-sum stochastic Bayesian games where each player has its own dynamic state that is unknown to the other player. Using typical techniques, we provide the recursive formulas and the sufficient statistics in both the primal game and its dual games. It's also shown that with a specific initial parameter, the optimal strategy of one player in a dual game is also the optimal strategy of the player in the primal game. We, then, construct linear programs to compute the optimal strategies in both the primal game and the dual games and the special initial parameters in the dual games. The main results are demonstrated in a security problem of underwater sensor networks."", 'corpus_id': 221266317, 'score': 1}]"
32	{'doc_id': '202788490', 'title': 'NumNet: Machine Reading Comprehension with Numerical Reasoning', 'abstract': 'Numerical reasoning, such as addition, subtraction, sorting and counting is a critical skill in human’s reading comprehension, which has not been well considered in existing machine reading comprehension (MRC) systems. To address this issue, we propose a numerical MRC model named as NumNet, which utilizes a numerically-aware graph neural network to consider the comparing information and performs numerical reasoning over numbers in the question and passage. Our system achieves an EM-score of 64.56% on the DROP dataset, outperforming all existing machine reading comprehension models by considering the numerical relations among numbers.', 'corpus_id': 202788490}	3346	"[{'doc_id': '218487830', 'title': 'Probing Text Models for Common Ground with Visual Representations', 'abstract': 'Vision, as a central component of human perception, plays a fundamental role in shaping natural language. To better understand how text models are connected to our visual perceptions, we propose a method for examining the similarities between neural representations extracted from words in text and objects in images. Our approach uses a lightweight probing model that learns to map language representations of concrete words to the visual domain. We find that representations from models trained on purely textual data, such as BERT, can be nontrivially mapped to those of a vision model. Such mappings generalize to object categories that were never seen by the probe during training, unlike mappings learned from permuted or random representations. Moreover, we find that the context surrounding objects in sentences greatly impacts performance. Finally, we show that humans significantly outperform all examined models, suggesting considerable room for improvement in representation learning and grounding.', 'corpus_id': 218487830, 'score': 0}, {'doc_id': '233240851', 'title': 'NT5?! Training T5 to Perform Numerical Reasoning', 'abstract': 'Numerical reasoning over text (NRoT) presents unique challenges that are not well addressed by existing pre-training objectives. We explore five sequential training schedules that adapt a pre-trained T5 model for NRoT. Our final model is adapted from T5, but further pre-trained on three datasets designed to strengthen skills necessary for NRoT and general reading comprehension before being fine-tuned on the Discrete Reasoning over Text (DROP) dataset. The training improves DROP’s adjusted F1 performance (a numeracy-focused score) from 45.90 to 70.83. Our model closes in on GenBERT (72.4), a custom BERT-Base model using the same datasets with significantly more parameters. We show that training the T5 multitasking framework with multiple numerical reasoning datasets of increasing difficulty, good performance on DROP can be achieved without manually engineering partitioned functionality between distributed and symbol modules.', 'corpus_id': 233240851, 'score': 1}, {'doc_id': '215777958', 'title': 'A Benchmark for Structured Procedural Knowledge Extraction from Cooking Videos', 'abstract': 'Watching instructional videos are often used to learn about procedures. Video captioning is one way of automatically collecting such knowledge. However, it provides only an indirect, overall evaluation of multimodal models with no finer-grained quantitative measure of what they have learned. We propose instead, a benchmark of structured procedural knowledge extracted from cooking videos. This work is complementary to existing tasks, but requires models to produce interpretable structured knowledge in the form of verb-argument tuples. Our manually annotated open-vocabulary resource includes 356 instructional cooking videos and 15,523 video clip/sentence-level annotations. Our analysis shows that the proposed task is challenging and standard modeling approaches like unsupervised segmentation, semantic role labeling, and visual action detection perform poorly when forced to predict every action of a procedure in a structured form.', 'corpus_id': 215777958, 'score': 0}, {'doc_id': '30332033', 'title': 'It Takes Nine Days to Iron a Shirt: The Development of Cognitive Estimation Skills in School Age Children', 'abstract': 'Data are presented for 315 elementary school-aged children (K–11) who took the Biber Cognitive Estimation Test, a 20-item test with five estimation questions in each of four domains: quantity, time/duration, weight, and distance/length. Performance showed significant development yearly until around the age of nine years, with much slower development subsequently. No gender effects were found. Age and fund of knowledge correlated with overall test performance. Fund of information accounted for a large proportion of the variance in estimation skills for children 8 years and under, but not for children 9 years and older. Since estimation skills require retrieval and manipulation of relevant knowledge and inhibition of impulsive responding and are necessary in many everyday tasks, it was anticipated that this test may provide a useful measure of judgment and estimations and may correlate with other executive skills in school-aged children.', 'corpus_id': 30332033, 'score': 1}, {'doc_id': '219958898', 'title': 'Visual sense of number vs. sense of magnitude in humans and machines', 'abstract': 'Numerosity perception is thought to be foundational to mathematical learning, but its computational bases are strongly debated. Some investigators argue that humans are endowed with a specialized system supporting numerical representations; others argue that visual numerosity is estimated using continuous magnitudes, such as density or area, which usually co-vary with number. Here we reconcile these contrasting perspectives by testing deep neural networks on the same numerosity comparison task that was administered to human participants, using a stimulus space that allows the precise measurement of the contribution of non-numerical features. Our model accurately simulates the psychophysics of numerosity perception and the associated developmental changes: discrimination is driven by numerosity, but non-numerical features also have a significant impact, especially early during development. Representational similarity analysis further highlights that both numerosity and continuous magnitudes are spontaneously encoded in deep networks even when no task has to be carried out, suggesting that numerosity is a major, salient property of our visual environment.', 'corpus_id': 219958898, 'score': 1}, {'doc_id': '218486778', 'title': 'The Sensitivity of Language Models and Humans to Winograd Schema Perturbations', 'abstract': 'Large-scale pretrained language models are the major driving force behind recent improvements in perfromance on the Winograd Schema Challenge, a widely employed test of commonsense reasoning ability. We show, however, with a new diagnostic dataset, that these models are sensitive to linguistic perturbations of the Winograd examples that minimally affect human understanding. Our results highlight interesting differences between humans and language models: language models are more sensitive to number or gender alternations and synonym replacements than humans, and humans are more stable and consistent in their predictions, maintain a much higher absolute performance, and perform better on non-associative instances than associative ones.', 'corpus_id': 218486778, 'score': 1}, {'doc_id': '119786518', 'title': ""Students' Accuracy of Measurement Estimation: Context, Units, and Logical Thinking"", 'abstract': ""This study examined students' accuracy of measurement estimation for linear distances, different units of measure, task context, and the relationship between accuracy estimation and logical thinking. Middle school students completed a series of tasks that included estimating the length of various objects in different contexts and completed a test of logical thinking ability. Results found that the students were not able to give accurate estimations for the lengths of familiar objects. Students were also less accurate in estimating in metric units as compared to English or novel units. Estimation accuracy was dependent on the task context. There were significant differences in estimation accuracy for two- versus three-dimensional estimation tasks. There were no significant differences for estimating objects with different orientations or embedded objects. For the tasks requiring the students to estimate in English units, the embedded task and the three-dimensional tasks were correlated with logical thinking. For estimation tasks with novel units, three-dimensional and two-dimensional estimation tasks were significantly correlated with the logical thinking. \n \n \n \nIn order to interact effectively with our environment it is essential to possess an intuitive grasp of both dimension and scale and to be able to manipulate such information. Estimation, approximating and measuring are all components of such intuition (Forrester, Latham, & Shire, 1990, p. 283)."", 'corpus_id': 119786518, 'score': 1}, {'doc_id': '218487034', 'title': 'On Faithfulness and Factuality in Abstractive Summarization', 'abstract': 'It is well known that the standard likelihood training and approximate decoding objectives in neural text generation models lead to less human-like responses for open-ended tasks such as language modeling and story generation. In this paper we have analyzed limitations of these models for abstractive document summarization and found that these models are highly prone to hallucinate content that is unfaithful to the input document. We conducted a large scale human evaluation of several neural abstractive summarization systems to better understand the types of hallucinations they produce. Our human annotators found substantial amounts of hallucinated content in all model generated summaries. However, our analysis does show that pretrained models are better summarizers not only in terms of raw metrics, i.e., ROUGE, but also in generating faithful and factual summaries as evaluated by humans. Furthermore, we show that textual entailment measures better correlate with faithfulness than standard metrics, potentially leading the way to automatic evaluation metrics as well as training and decoding criteria.', 'corpus_id': 218487034, 'score': 1}, {'doc_id': '124652071', 'title': 'The Number Sense: How the Mind Creates Mathematics.', 'abstract': ""L'A. propose quelques observations concernant l'ouvrage de Stanislas Dehaene The number sense. How the mind creates mathematics (1997) qui explore tous les aspects de la relation entre les hommes et les nombres : la numerosite chez les autres animaux, la numerosite et le calcul simple chez les bebes, l'histoire de l'expression du nombre dans le langage, l'histoire de la notation du nombre, le circuit neuronal necessaire pour faire de l'arithmetique et du calcul, la localisation dans le cerveau, l'ordre mathematique de l'univers, etc ... L'A. examine ici en particulier les questions portant sur la relation entre les nombres et le langage dans une perspective cognitive, puis explique ce que Dehaene entend par le sens du nombre en caracterisant les mathematiques comme une formalisation progressive de nos intuitions sur les ensembles, le nombre, l'espace, le temps et la logique"", 'corpus_id': 124652071, 'score': 1}, {'doc_id': '211126663', 'title': 'Transformers as Soft Reasoners over Language', 'abstract': 'Beginning with McCarthy\'s Advice Taker (1959), AI has pursued the goal of providing a system with explicit, general knowledge and having the system reason over that knowledge. However, expressing the knowledge in a formal (logical or probabilistic) representation has been a major obstacle to this research. This paper investigates a modern approach to this problem where the facts and rules are provided as natural language sentences, thus bypassing a formal representation. We train transformers to reason (or emulate reasoning) over these sentences using synthetically generated data. Our models, that we call RuleTakers, provide the first empirical demonstration that this kind of soft reasoning over language is learnable, can achieve high (99%) accuracy, and generalizes to test data requiring substantially deeper chaining than seen during training (95%+ scores). We also demonstrate that the models transfer well to two hand-authored rulebases, and to rulebases paraphrased into more natural language. These findings are significant as it suggests a new role for transformers, namely as limited ""soft theorem provers"" operating over explicit theories in language. This in turn suggests new possibilities for explainability, correctability, and counterfactual reasoning in question-answering.', 'corpus_id': 211126663, 'score': 0}]"
33	{'doc_id': '209439016', 'title': 'A pr 2 00 1 Bootstrapping Structure using Similarity', 'abstract': 'In this paper a new similarity-based learning algorithm, inspired by string edit-distance (Wagner and Fischer, 1974), is applied to the problem of bootstrapping structure from scratch. The algorithm takes a corpus of unannotated sentences as input and returns a corpus of bracketed sentences. The method works on pairs of unstructured sentences or sentences partially bracketed by the algorithm that have one or more words in common. It finds parts of sentences that are interchangeable (i.e. the parts of the sentences that are different in both sentences). These parts are taken as possible constituents of the same type. While this corresponds to the basic bootstrapping step of the algorithm, further structure may be learned from comparison with other (similar) sentences. We used this method for bootstrapping structure from the flat sentences of the Penn Treebank ATIS corpus, and compared the resulting structured sentences to the structured sentences in the ATIS corpus. Similarly, the algorithm was tested on the OVIS corpus. We obtained 86.04 % non-crossing brackets precision on the ATIS corpus and 89.39 % non-crossing brackets precision on the OVIS corpus.', 'corpus_id': 209439016}	14659	"[{'doc_id': '121944264', 'title': 'Theoretical and Practical Experiences with Alignment-Based Learning', 'abstract': None, 'corpus_id': 121944264, 'score': 1}, {'doc_id': '233364998', 'title': 'Unsupervised Natural Language Parsing (Introductory Tutorial)', 'abstract': 'Unsupervised parsing learns a syntactic parser from training sentences without parse tree annotations. Recently, there has been a resurgence of interest in unsupervised parsing, which can be attributed to the combination of two trends in the NLP community: a general trend towards unsupervised training or pre-training, and an emerging trend towards finding or modeling linguistic structures in neural models. In this tutorial, we will introduce to the general audience what unsupervised parsing does and how it can be useful for and beyond syntactic parsing. We will then provide a systematic overview of major classes of approaches to unsupervised parsing, namely generative and discriminative approaches, and analyze their relative strengths and weaknesses. We will cover both decade-old statistical approaches and more recent neural approaches to give the audience a sense of the historical and recent development of the field. We will also discuss emerging research topics such as BERT-based approaches and visually grounded learning.', 'corpus_id': 233364998, 'score': 0}, {'doc_id': '16072122', 'title': 'An efficient implementation of a new DOP model', 'abstract': 'Two apparently opposing DOP models exist in the literature: one which computes the parse tree involving the most frequent subtrees from a treebank and one which computes the parse tree involving the fewest subtrees from a treebank. This paper proposes an integration of the two models which outperforms each of them separately. Together with a PCFG-reduction of DOP we obtain improved accuracy and efficiency on the Wall Street Journal treebank. Our results show an 11% relative reduction in error rate over previous models, and an average processing time of 3.6 seconds per WSJ sentence.', 'corpus_id': 16072122, 'score': 1}, {'doc_id': '16860802', 'title': 'Learning Structure using Alignment Based Learning', 'abstract': None, 'corpus_id': 16860802, 'score': 1}, {'doc_id': '233735504', 'title': 'Parsing the Less-configurational Georgian Language with a Context-Free Grammar', 'abstract': 'A large part of the methodology for Natural Language Processing has been developed for languages with a strong syntactic configuration. At the other end of the configurational spectrum there are languages with rich derivational and inflectional morphology. These languages for morphologically rich and less-configurational features are referred to as MR&LC. In our study we have addressed Georgian a language with less-configurational constraints, though, with a rich inflectional morphology and a very little fixed structure on the sentence level, and therefore, the most syntax-level information for the Georgian language is conveyed by its productive morphology. This paper features issues concerned with development of a crucial NLP resource for the Georgian language a Context-Free Syntactic Parser.', 'corpus_id': 233735504, 'score': 0}, {'doc_id': '14196782', 'title': 'Implementing Alignment-Based Learning', 'abstract': ""In this article, the current implementation of the Alignment-Based Learning (ABL) framework (van Zaanen, 2002) will be described. ABL is an unsupervised grammar induction system that is based on Harris's (1951) idea of substitutability. Instances of the framework can be applied to an untagged, unstructured corpus of natural language sentences, resulting in a labelled, bracketed version of that corpus.Firstly, the framework aligns all sentences in the corpus in pairs, resulting in a partition of the sentences consisting of parts of the sentences that are equal in both sentences and parts that are unequal. Since substituting one unequal part for the other results in another valid sentence, the unequal parts of the sentences are considered to be possible (possibly overlapping) constituents. Secondly, of all possible constituents found by the first phase, the best are selected."", 'corpus_id': 14196782, 'score': 1}, {'doc_id': '233386610', 'title': 'What’s in a Span? Evaluating the Creativity of a Span-Based Neural Constituency Parser', 'abstract': 'Constituency parsing is generally evaluated superficially, particularly in a multiple language setting, with only F-scores being reported. As new state-of-the-art chart-based parsers have resulted in a transition from traditional PCFG-based grammars to span-based approaches (Stern et al., 2017; Gaddy et al., 2018), we do not have a good understanding of how such fundamentally different approaches interact with various treebanks as results show improvements across treebanks (Kitaev and Klein, 2018), but it is unclear what influence annotation schemes have on various treebank performance (Kitaev et al., 2019). In particular, a span-based parser’s capability of creating novel rules is an unknown factor. We perform an analysis of how span-based parsing performs across 11 treebanks in order to examine the overall behavior of this parsing approach and the effect of the treebanks’ specific annotations on results. We find that the parser tends to prefer flatter trees, but the approach works well because it is robust enough to adapt to differences in annotation schemes across treebanks and languages.', 'corpus_id': 233386610, 'score': 0}, {'doc_id': '232268898', 'title': 'Parsing Penn Chinese Treebank Based on Lexicalized Model', 'abstract': 'Syntactic parsing is one of the most important technologies of natural language processing. The development of Penn Chinese Treebank (CTB) spurred the research of Chinese parsing. This paper describes a lexicalized statistical Chinese parser. First, a lexicalized model based on hidden Markov model is proposed for part of speech tagging. Second, a well-known lexicalized model i.e. the head-driven model is adapted to parse the automatically POS tagged Chinese sentences. The construction of the parser is described, and the effects of details that can make great difference in the parsing performance are analyzed. On sentences of length less than 100 words, the parser performances at 80.08% precision and 78.45% recall on, surpassing the best published results.', 'corpus_id': 232268898, 'score': 0}, {'doc_id': '1984129', 'title': 'Two Questions about Data-Oriented Parsing', 'abstract': 'In this paper I present ongoing work on the data-oriented parsing (DOP) model. In previous work, DOP was tested on a cleaned-up set of analyzed part-of-speech strings from the Penn Treebank, achieving excellent test results. This left, however, two important questions unanswered: (1) how does DOP perform if tested on unedited data, and (2) how can DOP be used for parsing word strings that contain unknown words? This paper addresses these questions. We show that parse results on unedited data are worse than on cleaned-up data, although still very competitive if compared to other models. As to the parsing of word strings, we show that the hardness of the problem does not so much depend on unknown words, but on previously unseen lexical categories of known words. We give a novel method for parsing these words by estimating the probabilities of unknown subtrees. The method is of general interest since it shows that good performance can be obtained without the use of a part-ofspeech tagger. To the best of our knowledge, our method outperforms other statistical parsers tested on Penn Treebank word strings.', 'corpus_id': 1984129, 'score': 1}, {'doc_id': '233740070', 'title': 'Polynomial Graph Parsing with Non-Structural Reentrancies', 'abstract': 'Graph-based semantic representations are valuable in natural language processing, where it is often simple and effective to represent linguistic concepts as nodes, and relations as edges between them. Several attempts has been made to find a generative device that is sufficiently powerful to represent languages of semantic graphs, while at the same allowing efficient parsing. We add to this line of work by introducing graph extension grammar, which consists of an algebra over graphs together with a regular tree grammar that generates expressions over the operations of the algebra. Due to the design of the operations, these grammars can generate graphs with non-structural reentrancies, a type of node-sharing that is excessively common in formalisms such as abstract meaning representation, but for which existing devices offer little support. We provide a parsing algorithm for graph extension grammars, which is proved to be correct and run in polynomial time.', 'corpus_id': 233740070, 'score': 0}]"
34	{'doc_id': '235257318', 'title': 'Stable overexpression of mutated PTEN in Chinese hamster ovary cells enhances their performance and therapeutic antibody production', 'abstract': 'Chinese hamster ovary (CHO) cells with a high viable cell density (VCD), resilience to culture stress, and the capacity to continuously express recombinant proteins are highly desirable. Phosphatase and tension homology deleted on chromosome ten (PTEN) functions as a key negative regulator of the PI3K/Akt signaling pathway, mediating cell growth and survival. Its oncogenic mutant endows cells with an enhanced proliferation rate and resistance to death. In this study, the role of oncogenic PTEN C124S or G129E on the performance of CHO‐K1 and CHO‐IgG cells was investigated. Our results showed that CHO‐K1 cells stably expressing PTEN C124S or G129E exhibited enhanced proliferation, reduced apoptosis rate, and increased transient expression of therapeutic antibodies compared to the control cells. Moreover, the stable overexpression of PTEN C124S or G129E endowed CHO‐IgG cells with higher cell viability, VCD, and antibody titers (yield increased by approximately 0.77‐fold) in the fed‐batch culture process and enhanced their performance in response to the addition of sodium lactate. Moreover, the engineering of mutated PTEN in CHO‐IgG cells did not alter antibody quality. Collectively, our data suggest that mutated PTEN is a potential target for improving the manufacture of therapeutic antibodies.', 'corpus_id': 235257318}	16591	"[{'doc_id': '237218013', 'title': 'Harnessing secretory pathway differences between HEK293 and CHO to rescue production of difficult to express proteins', 'abstract': 'Biologics represent the fastest growing group of therapeutics, but many advanced recombinant protein moieties remain difficult to produce. Here, we identify bottlenecks limiting expression of recombinant human proteins through a systems biology analysis of the transcriptomes of CHO and HEK293 during recombinant overexpression. Surprisingly, one third of the challenging human proteins displayed improved secretion upon host cell swapping from CHO to HEK293. While most components of the secretory machinery showed comparable expression levels in both expression hosts, genes with significant expression variation were identified. Among these, ATF4, SRP9, JUN, PDIA3 and HSPA8 were validated as productivity boosters in CHO. Further, more heavily glycosylated products benefitted more from the elevated activities of the N- and O-glycosyltransferases found in HEK293. Collectively, our results demonstrate the utilization of HEK293 for expression rescue of human proteins and suggest a methodology for identification of secretory pathway components improving recombinant protein yield in HEK293 and CHO.', 'corpus_id': 237218013, 'score': 1}, {'doc_id': '235232429', 'title': 'Measurement of Large Serine Integrase Enzymatic Characteristics in HEK293 Cells Reveals Variability and Influence on Downstream Reporter Expression.', 'abstract': 'Large Serine Integrases (LSIs) offer tremendous potential for rapid genetic engineering as well as building biological systems capable of responding to stimuli and integrating information. Currently, there is no unified metric for directly measuring the enzymatic characteristics of LSI function, which hinders evaluation of their suitability to specific applications. Here, we present an experimental protocol for recording DNA recombination in HEK293 cells in real-time through fluorophore expression and software which fits the kinetic data to a model tailored to LSI recombination dynamics. Our model captures the activity of LSIs as three parameters: expression level (Kexp ), catalytic rate (kcat ), and substrate affinity (Kd ). The expression level and catalytic rate for phiC31 and Bxb1 varied greatly, suggesting disparate routes to high recombination efficiencies. Moreover, the expression level and substrate affinity jointly impacted downstream reporter expression, potentially by obstructing transcriptional machinery. We validated these observations by swapping between promoters and mutating key recombinase residues and DNA recognition sites to individually modulate each parameter. Our model for identifying key LSI parameters in cellulo provides insight into selecting the optimal recombinase for various applications as well as for guiding the engineering of improved LSIs.', 'corpus_id': 235232429, 'score': 0}, {'doc_id': '235495246', 'title': 'Palindromic Sequence-Targeted (PST) PCR, Version 2: An Advanced Method for High-Throughput Targeted Gene Characterization and Transposon Display', 'abstract': 'Genome walking (GW), a strategy for capturing previously unsequenced DNA fragments that are in proximity to a known sequence tag, is currently predominantly based on PCR. Recently developed PCR-based methods allow for combining of sequence-specific primers with designed capturing primers capable of annealing to unknown DNA targets, thereby offering the rapidity and effectiveness of PCR. This study presents a methodological improvement to the previously described GW technique known as palindromic sequence-targeted PCR (PST-PCR). Like PST-PCR, this new method (called PST-PCR v.2) relies on targeting of capturing primers to palindromic sequences arbitrarily present in natural DNA templates. PST-PCR v.2 consists of two rounds of PCR. The first round uses a combination of one sequence-specific primer with one capturing (PST) primer. The second round uses a combination of a single (preferred) or two universal primers; one anneals to a 5′ tail attached to the sequence-specific primer and the other anneals to a different 5′ tail attached to the PST primer. The key advantage of PST-PCR v.2 is the convenience of using a single universal primer with invariable sequences in GW processes involving various templates. The entire procedure takes approximately 2–3 h to produce the amplified PCR fragment, which contains a portion of a template flanked by the sequence-specific and capturing primers. PST-PCR v.2 is highly suitable for simultaneous work with multiple samples. For this reason, PST-PCR v.2 can be applied beyond the classical task of GW for studies in population genetics, in which PST-PCR v.2 is a preferred alternative to amplified fragment length polymorphism (AFLP) or next-generation sequencing. Furthermore, the conditions for PST-PCR v.2 are easier to optimize, as only one sequence-specific primer is used. This reduces non-specific random amplified polymorphic DNA (RAPD)-like amplification and formation of non-templated amplification. Importantly, akin to the previous version, PST-PCR v.2 is not sensitive to template DNA sequence complexity or quality. This study illustrates the utility of PST-PCR v.2 for transposon display (TD), which is a method to characterize inter- or intra-specific variability related to transposon integration sites. The Ac transposon sequence in the maize (Zea mays) genome was used as a sequence tag during the TD procedure to characterize the Ac integration sites.', 'corpus_id': 235495246, 'score': 0}, {'doc_id': '235745430', 'title': 'Current Designs of Polymeric Platforms towards the Delivery of Nucleic Acids inside the Cells with Focus on Polyethylenimine.', 'abstract': 'BACKGROUND\nGene delivery is a promising technology for treating diseases linked to abnormal gene expression. Since nucleic acids are the therapeutic entities in such approach, a transfecting vector is required because the macromolecules are not able to efficiently enter the cells by themselves. Viral vectors have been evidenced to be highly effective in this context; however, they suffer from fundamental drawbacks, such as the ability to stimulate immune responses. The development of synthetic vectors has accordingly emerged as an alternative.\n\n\nOBJECTIVES\nGene delivery by using non-viral vectors is a multi-step process that poses many challenges, either regarding the extracellular or intracellular media. We explore the delivery pathway and afterwards, we review the main classes of non-viral gene delivery vectors. We further focus on the progresses concerning polyethylenimine-based polymer-nucleic acid polyplexes, which have emerged as one of the most efficient systems for delivering genetic material inside the cells.\n\n\nDISCUSSION\nThe complexity of the whole transfection pathway, along with a lack of fundamental understanding, particularly regarding the intracellular trafficking of nucleic acids complexed to non-viral vectors, probably justifies the current (beginning of 2021) limited number of formulations that have progressed to clinical trials. Truly, successful medical developments still require a lot of basic research.\n\n\nCONCLUSION\nAdvances in macromolecular chemistry and high-resolution imaging techniques will be useful to understand fundamental aspects towards further optimizations and future applications. More investigations concerning the dynamics, thermodynamics and structural parameters of polyplexes would be valuable since they can be connected to the different levels of transfection efficiency hitherto evidenced.', 'corpus_id': 235745430, 'score': 0}, {'doc_id': '235393158', 'title': 'Genome editor-directed in\xa0vivo library diversification.', 'abstract': 'The generation of a library of variant genes is a prerequisite of directed evolution, a powerful tool for biomolecular engineering. As the number of all possible sequences often far exceeds the diversity of a practical library, methods that allow efficient library diversification in living cells are essential for in\xa0vivo directed evolution technologies to effectively sample the sequence space and allow hits to emerge. While traditional whole-genome mutagenesis often results in toxicity and the emergence of ""cheater"" mutations, recent developments that exploit the targeting and editing abilities of genome editors to facilitate in\xa0vivo library diversification have allowed for precise mutagenesis focused on specific genes of interest, higher mutational density, and reduced the occurrence of cheater mutations. This minireview summarizes recent advances in genome editor-directed in\xa0vivo library diversification and provides an outlook on their future applications in chemical biology.', 'corpus_id': 235393158, 'score': 1}, {'doc_id': '233722512', 'title': 'Control of Multigene Expression Stoichiometry in Mammalian Cells Using Synthetic Promoters', 'abstract': 'To successfully engineer mammalian cells for a desired purpose, multiple recombinant genes are required to be coexpressed at a specific and optimal ratio. In this study, we hypothesized that synthetic promoters varying in transcriptional activity could be used to create single multigene expression vectors coexpressing recombinant genes at a predictable relative stoichiometry. A library of 27 multigene constructs was created comprising three discrete fluorescent reporter gene transcriptional units in fixed series, each under the control of either a relatively low, medium, or high transcriptional strength synthetic promoter in every possible combination. Expression of each reporter gene was determined by absolute quantitation qRT-PCR in CHO cells. The synthetic promoters did generally function as designed within a multigene vector context; however, significant divergences from predicted promoter-mediated transcriptional activity were observed. First, expression of all three genes within a multigene vector was repressed at varying levels relative to coexpression of identical reporter genes on separate single gene vectors at equivalent gene copies. Second, gene positional effects were evident across all constructs where expression of the reporter genes in positions 2 and 3 was generally reduced relative to position 1. Finally, after accounting for general repression, synthetic promoter transcriptional activity within a local multigene vector format deviated from that expected. Taken together, our data reveal that mammalian synthetic promoters can be employed in vectors to mediate expression of multiple genes at predictable relative stoichiometries. However, empirical validation of functional performance is a necessary prerequisite, as vector and promoter design features can significantly impact performance.', 'corpus_id': 233722512, 'score': 1}, {'doc_id': '236929709', 'title': 'Broadening the toolkit for quantitatively evaluating noncanonical amino acid incorporation in yeast', 'abstract': 'Genetic code expansion is a powerful approach for advancing critical fields such as biological therapeutic discovery. However, the machinery for genetically encoding noncanonical amino acids (ncAAs) is only available in limited plasmid formats, constraining potential applications. In extreme cases, the introduction of two separate plasmids, one containing an orthogonal translation system (OTS) to facilitate ncAA incorporation and a second for expressing a ncAA-containing protein of interest, is not possible due to lack of convenient selection markers. One strategy to circumvent this challenge is to express the OTS and protein of interest from a single vector. For what we believe is the first time in yeast, we describe here several sets of single plasmid systems (SPSs) for performing genetic code manipulation and compare the ncAA incorporation capabilities of these plasmids against the capabilities of previously described dual plasmid systems (DPSs). For both dual fluorescent protein reporters and yeast display reporters tested with multiple OTSs and ncAAs, measured ncAA incorporation efficiencies with SPSs were determined to be equal to or improved relative to efficiencies determined with DPSs. Click chemistry on yeast cells displaying ncAA-containing proteins was also shown to be feasible in both formats, although differences in reactivity between formats suggest the need for caution when using such approaches. Additionally, we investigated whether these reporters would support separation of yeast strains known to exhibit distinct ncAA incorporation efficiencies. Model sorts conducted with mixtures of two strains transformed with the same SPS or DPS led to enrichment of a strain known to support higher efficiency ncAA incorporation, suggesting that these reporters will be suitable for conducting screens for strains exhibiting enhanced ncAA incorporation efficiencies. Overall, our results confirm that SPSs are well-behaved in yeast and provide a convenient alternative to DPSs. In particular, SPSs are expected to be invaluable for conducting high-throughput investigations of the effects of genetic or genomic changes on ncAA incorporation efficiency and, more fundamentally, the eukaryotic translation apparatus.', 'corpus_id': 236929709, 'score': 0}, {'doc_id': '234828489', 'title': 'Recent advances in CHO cell line development for recombinant protein production', 'abstract': 'Abstract Recombinant proteins used in biomedical research, diagnostics and different therapies are mostly produced in Chinese hamster ovary cells in the pharmaceutical industry. These biotherapeutics, monoclonal antibodies in particular, have shown remarkable market growth in the past few decades. The increasing demand for high amounts of biologics requires continuous optimization and improvement of production technologies. Research aims at discovering better means and methods for reaching higher volumetric capacity, while maintaining stable product quality. An increasing number of complex novel protein therapeutics, such as viral antigens, vaccines, bi- and tri-specific monoclonal antibodies, are currently entering industrial production pipelines. These biomolecules are, in many cases, difficult to express and require tailored product-specific solutions to improve their transient or stable production. All these requirements boost the development of more efficient expression optimization systems and high-throughput screening platforms to facilitate the design of product-specific cell line engineering and production strategies. In this minireview, we provide an overview on recent advances in CHO cell line development, targeted genome manipulation techniques, selection systems and screening methods currently used in recombinant protein production.', 'corpus_id': 234828489, 'score': 1}, {'doc_id': '235718069', 'title': 'Short homology-directed repair using optimized Cas9 in the pathogen Cryptococcus neoformans enables rapid gene deletion and tagging', 'abstract': 'Cryptococcus neoformans, the most common cause of fungal meningitis, is a basidiomycete haploid budding yeast with a complete sexual cycle. Genome modification by homologous recombination is feasible using biolistic transformation and long homology arms, but the method is arduous and unreliable. Recently, multiple groups have reported the use of CRISPR-Cas9 as an alternative to biolistics, but long homology arms are still necessary, limiting the utility of this method. Since the S. pyogenes Cas9 derivatives used in prior studies were not optimized for expression in C. neoformans, we designed, synthesized, and tested a fully C. neoformans-optimized Cas9. We found that a Cas9 harboring only common C. neoformans codons and a consensus C. neoformans intron together with a TEF1 promoter and terminator and a nuclear localization signal (C. neoformans-optimized CAS9 or “CnoCAS9”) reliably enabled genome editing in the widely-used KN99α C. neoformans strain. Furthermore, editing was accomplished using donors harboring short (50 bp) homology arms attached to marker DNAs produced with synthetic oligonucleotides and PCR amplification. We also demonstrated that prior stable integration of CnoCAS9 further enhances both transformation and homologous recombination efficiency; importantly, this manipulation does not impact virulence in animals. We also implemented a universal tagging module harboring a codon-optimized fluorescent protein (mNeonGreen) and a tandem Calmodulin Binding Peptide-2X FLAG Tag that allows for both localization and purification studies of proteins for which the corresponding genes are modified by short homology-directed recombination. These tools enable short-homology genome engineering in C. neoformans.', 'corpus_id': 235718069, 'score': 0}, {'doc_id': '53209828', 'title': 'Scalable, Continuous Evolution of Genes at Mutation Rates above Genomic Error Thresholds', 'abstract': 'Directed evolution is a powerful approach for engineering biomolecules and understanding adaptation.\xa0However, experimental strategies for directed evolution are notoriously labor intensive and low throughput, limiting access to demanding functions, multiple functions in parallel, and the study of molecular evolution in replicate. We report OrthoRep, an orthogonal DNA polymerase-plasmid pair in yeast that stably mutates ∼100,000-fold faster than the host genome in\xa0vivo, exceeding the error threshold of genomic replication that causes single-generation extinction. User-defined genes in OrthoRep continuously and rapidly evolve through serial passaging, a highly straightforward and scalable process. Using OrthoRep, we evolved drug-resistant malarial dihydrofolate reductases (DHFRs) in 90 independent replicates. We uncovered a more complex fitness landscape than previously realized, including common adaptive trajectories constrained by epistasis, rare outcomes that avoid a frequent early adaptive mutation, and a suboptimal fitness peak that occasionally traps evolving populations. OrthoRep enables a new paradigm of routine, high-throughput evolution of biomolecular and cellular function.', 'corpus_id': 53209828, 'score': 1}]"
35	{'doc_id': '235469393', 'title': 'The Clinical Utility of Serial Procalcitonin and Procalcitonin Clearance in Predicting the Outcome of COVID-19 Patients', 'abstract': 'Background: The pandemic of coronavirus disease 2019 (COVID-19) represents a great threat to global health. Sensitive tests that effectively predict the disease outcome are essentially required to guide proper intervention. Objectives: To evaluate the prognostic ability of serial procalcitonin (PCT) measurement to predict the outcome of COVID-19 patients, using PCT clearance (PCT-c) as a tool to reflect its dynamic changes. Methods: A prospective observational study of inpatients diagnosed with COVID-19 at the Quarantine Hospitals of Ain-Shams University, Cairo, Egypt. During the first five days of hospitalization, serial PCT and PCT-c values were obtained and compared between survivors and non-survivors. Patients were followed up to hospital discharge or in-hospital mortality. Results: Compared to survivors, serial PCT levels of non-survivors were significantly higher (p<0.001) and progressively increased during follow-up, in contrast, PCT-c values were significantly lower (p<0.01) and progressively decreased. Receiver operating characteristic (ROC) curve analysis showed that by using the initial PCT value alone, at a cut off value of 0.80 ng/ml, the area under the curve for predicting in-hospital mortality was 0.81 with 61.1% sensitivity and 87.3% accuracy. Serial measurements showed better predictive performance and the combined prediction value was better than the single prediction by the initial PCT. Conclusions: Serial PCT measurement could be a useful laboratory tool to predict the prognosis and outcome of COVID-19 patients. Moreover, PCT-c could be a reliable tool to assess PCT progressive kinetics.', 'corpus_id': 235469393}	19039	"[{'doc_id': '235439424', 'title': 'Performance of serum procalcitonin as a biochemical predictor of death in hematology patients with febrile neutropenia.', 'abstract': 'INTRODUCTION\nHistorically, the measurement of serum procalcitonin (PCT) levels in patients with leukopenia has been rejected without sufficient prospective evidence to justify this argument. On the other hand, the accumulated use of broad spectrum antibiotics in these patients and their consequences make the use of PCT attractive in an effort to reduce its use.\n\n\nPATIENTS AND METHODS\nWe conducted a prospective study between 2016 and 2018, recruiting newly diagnosed FN patients, evaluating them with PCT levels during the first 24\xa0h. After this we evaluate them with overall survival throughout the follow-up.\n\n\nRESULTS\nA total of 81 episodes of FN in 72 patients were included. We report a mortality of 27.2% in our cohort. The mean serum PCT in these patients was 4.01\xa0ng/mL compared to 0.42\xa0ng/mL in the survivors group (p\xa0<\xa00.01). Using ROC curves, we determined a cut-off point to predict septic shock/death at 0.46\xa0ng/mL. Patients with a procalcitonin >0.46\xa0ng/mL had an increased risk of death, with a HR of 4.43, (p\xa0=\xa00.048).\n\n\nCONCLUSION\nIn conclusion, in our trial a single PCT on admission at a cut-off value of 0.46\xa0ng/mL was able to predict the occurrence of septic shock and death in FN patients.', 'corpus_id': 235439424, 'score': 0}, {'doc_id': '205474491', 'title': 'High-Value, Cost-Conscious Care: Iterative Systems-Based Interventions to Reduce Unnecessary Laboratory Testing.', 'abstract': 'BACKGROUND\nInappropriate testing contributes to soaring healthcare costs within the United States, and teaching hospitals are vulnerable to providing care largely for academic development. Via its ""Choosing Wisely"" campaign, the American Board of Internal Medicine recommends avoiding repetitive testing for stable inpatients. We designed systems-based interventions to reduce laboratory orders for patients admitted to the wards at an academic facility.\n\n\nMETHODS\nWe identified the computer-based order entry system as an appropriate target for sustainable intervention. The admission order set had allowed multiple routine tests to be ordered repetitively each day. Our iterative study included interventions on the automated order set and cost displays at order entry. The primary outcome was number of routine tests controlled for inpatient days compared with the preceding year. Secondary outcomes included cost savings, delays in care, and adverse events.\n\n\nRESULTS\nData were collected over a 2-month period following interventions in sequential years and compared with the year prior. The first intervention led to 0.97 fewer laboratory tests per inpatient day (19.4%). The second intervention led to sustained reduction, although by less of a margin than order set modifications alone (15.3%). When extrapolating the results utilizing fees from the Centers for Medicare and Medicaid Services, there was a cost savings of $290,000 over 2 years. Qualitative survey data did not suggest an increase in care delays or near-miss events.\n\n\nCONCLUSIONS\nThis series of interventions targeting unnecessary testing demonstrated a sustained reduction in the number of routine tests ordered, without adverse effects on clinical care.', 'corpus_id': 205474491, 'score': 1}, {'doc_id': '235493405', 'title': 'Detecting Sepsis in an Emergency Department: SIRS vs. qSOFA.', 'abstract': 'Sepsis is a condition that can progress to serious illness and even death. The diagnosis of sepsis is difficult because no unique biomarker exists. With this, health care providers must rely on clinical diagnostic criteria to guide diagnosis. Systemic Inflammatory Response Syndrome (SIRS) criteria have been used for diagnosis since 1992. The more recent attempt to replace SIRS with the quick Sequential Organ Failure Assessment (qSOFA) for assessment of potentially septic patients is troublesome. The qSOFA was designed as a prognostic and not diagnostic tool. Using established processes of evidence-based medicine, it is shown herein that qSOFA fails to meet the definition of a diagnostic assessment tool. Thus, the SIRS assessment should remain the gold standard tool for detecting patients at risk of ""sepsis.""', 'corpus_id': 235493405, 'score': 0}, {'doc_id': '46920884', 'title': 'Reducing Test Utilization in Hospital Settings: A Narrative Review', 'abstract': 'Background Studies addressing the appropriateness of laboratory testing have revealed approximately 20% overutilization. We conducted a narrative review to (1) describe current interventions aimed at reducing unnecessary laboratory testing, specifically in hospital settings, and (2) provide estimates of their efficacy in reducing test order volume and improving patient-related clinical outcomes. Methods The PubMed, Embase, Scopus, Web of Science, and Canadian Agency for Drugs and Technologies in Health-Health Technology Assessment databases were searched for studies describing the effects of interventions aimed at reducing unnecessary laboratory tests. Data on test order volume and clinical outcomes were extracted by one reviewer, while uncertainties were discussed with two other reviewers. Because of the heterogeneity of interventions and outcomes, no meta-analysis was performed. Results Eighty-four studies were included. Interventions were categorized into educational, (computerized) provider order entry [(C)POE], audit and feedback, or other interventions. Nearly all studies reported a reduction in test order volume. Only 15 assessed sustainability up to two years. Patient-related clinical outcomes were reported in 45 studies, two of which found negative effects. Conclusions Interventions from all categories have the potential to reduce unnecessary laboratory testing, although long-term sustainability is questionable. Owing to the heterogeneity of the interventions studied, it is difficult to conclude which approach was most successful, and for which tests. Most studies had methodological limitations, such as the absence of a control arm. Therefore, well-designed, controlled trials using clearly described interventions and relevant clinical outcomes are needed.', 'corpus_id': 46920884, 'score': 1}, {'doc_id': '235708817', 'title': 'Does admission order form design really matter? A reduction in urea blood test ordering', 'abstract': 'Introduction Laboratory blood testing is one of the most high-volume medical procedures and continues to increase steadily with instances of inappropriate testing resulting in significant financial implications. Studies have suggested that the design of a standard hospital admission order form and laboratory request forms influence physician test ordering behaviour, reducing inappropriate ordering and promoting resource stewardship. Aim/method To redesign the standard medicine admission order form-laboratory request section to reduce inappropriate blood urea nitrogen (BUN) testing. Results A redesign of the standard admission order form used by general internal medicine physicians and residents in two large teaching hospitals in one health zone in Alberta, Canada led to a significant step reduction in the ordering of the BUN test on hospital admission. Conclusions Redesigning the standard medicine admission order form-laboratory request section can have a beneficial effect on the reduction in BUN ordering altering physician ordering patterns and behaviour.', 'corpus_id': 235708817, 'score': 1}, {'doc_id': '235703544', 'title': 'Outcomes and Timing of Bedside Percutaneous Tracheostomy of COVID-19 Patients over a Year in the Intensive Care Unit', 'abstract': 'Background: The benefits and timing of percutaneous dilatational tracheostomy (PDT) in Intensive Care Unit (ICU) COVID-19 patients are still controversial. PDT is considered a high-risk procedure for the transmission of SARS-CoV-2 to healthcare workers (HCWs). The present study analyzed the optimal timing of PDT, the clinical outcomes of patients undergoing PDT, and the safety of HCWs performing PDT. Methods: Of the 133 COVID-19 patients who underwent PDT in our ICU from 1 April 2020 to 31 March 2021, 13 patients were excluded, and 120 patients were enrolled. A trained medical team was dedicated to the PDT procedure. Demographic, clinical history, and outcome data were collected. Patients who underwent PDT were stratified into two groups: an early group (PDT ≤ 12 days after orotracheal intubation (OTI) and a late group (>12 days after OTI). An HCW surveillance program was also performed. Results: The early group included 61 patients and the late group included 59 patients. The early group patients had a shorter ICU length of stay and fewer days of mechanical ventilation than the late group (p < 0.001). On day 7 after tracheostomy, early group patients required fewer intravenous anesthetic drugs and experienced an improvement of the ventilation parameters PaO2/FiO2 ratio, PEEP, and FiO2 (p < 0.001). No difference in the case fatality ratio between the two groups was observed. No SARS-CoV-2 infections were reported in the HCWs performing the PDTs. Conclusions: PDT was safe and effective for COVID-19 patients since it improved respiratory support parameters, reduced ICU length of stay and duration of mechanical ventilation, and optimized the weaning process. The procedure was safe for all HCWs involved in the dedicated medical team. The development of standardized early PDT protocols should be implemented, and PDT could be considered a first-line approach in ICU COVID-19 patients requiring prolonged mechanical ventilation.', 'corpus_id': 235703544, 'score': 0}, {'doc_id': '235717001', 'title': 'Clinical characteristics and outcomes of invasively ventilated patients with COVID-19 in Argentina (SATICOVID): a prospective, multicentre cohort study', 'abstract': '\n Background\n Although COVID-19 has greatly affected many low-income and middle-income countries, detailed information about patients admitted to the intensive care unit (ICU) is still scarce. Our aim was to examine ventilation characteristics and outcomes in invasively ventilated patients with COVID-19 in Argentina, an upper middle-income country.\n \n Methods\n In this prospective, multicentre cohort study (SATICOVID), we enrolled patients aged 18 years or older with RT-PCR-confirmed COVID-19 who were on invasive mechanical ventilation and admitted to one of 63 ICUs in Argentina. Patient demographics and clinical, laboratory, and general management variables were collected on day 1 (ICU admission); physiological respiratory and ventilation variables were collected on days 1, 3, and 7. The primary outcome was all-cause in-hospital mortality. All patients were followed until death in hospital or hospital discharge, whichever occurred first. Secondary outcomes were ICU mortality, identification of independent predictors of mortality, duration of invasive mechanical ventilation, and patterns of change in physiological respiratory and mechanical ventilation variables. The study is registered with ClinicalTrials.gov, NCT04611269, and is complete.\n \n Findings\n Between March 20, 2020, and Oct 31, 2020, we enrolled 1909 invasively ventilated patients with COVID-19, with a median age of 62 years [IQR 52–70]. 1294 (67·8%) were men, hypertension and obesity were the main comorbidities, and 939 (49·2%) patients required vasopressors. Lung-protective ventilation was widely used and median duration of ventilation was 13 days (IQR 7–22). Median tidal volume was 6·1 mL/kg predicted bodyweight (IQR 6·0–7·0) on day 1, and the value increased significantly up to day 7; positive end-expiratory pressure was 10 cm H2O (8–12) on day 1, with a slight but significant decrease to day 7. Ratio of partial pressure of arterial oxygen (PaO2) to fractional inspired oxygen (FiO2) was 160 (IQR 111–218), respiratory system compliance 36 mL/cm H2O (29–44), driving pressure 12 cm H2O (10–14), and FiO2 0·60 (0·45–0·80) on day 1. Acute respiratory distress syndrome developed in 1672 (87·6%) of patients; 1176 (61·6%) received prone positioning. In-hospital mortality was 57·7% (1101/1909 patients) and ICU mortality was 57·0% (1088/1909 patients); 462 (43·8%) patients died of refractory hypoxaemia, frequently overlapping with septic shock (n=174). Cox regression identified age (hazard ratio 1·02 [95% CI 1·01–1·03]), Charlson score (1·16 [1·11–1·23]), endotracheal intubation outside of the ICU (ie, before ICU admission; 1·37 [1·10–1·71]), vasopressor use on day 1 (1·29 [1·07–1·55]), D-dimer concentration (1·02 [1·01–1·03]), PaO2/FiO2 on day 1 (0·998 [0·997–0·999]), arterial pH on day 1 (1·01 [1·00–1·01]), driving pressure on day 1 (1·05 [1·03–1·08]), acute kidney injury (1·66 [1·36–2·03]), and month of admission (1·10 [1·03–1·18]) as independent predictors of mortality.\n \n Interpretation\n In patients with COVID-19 who required invasive mechanical ventilation, lung-protective ventilation was widely used but mortality was high. Predictors of mortality in our study broadly agreed with those identified in studies of invasively ventilated patients in high-income countries. The sustained burden of COVID-19 on scarce health-care personnel might have contributed to high mortality over the course of our study in Argentina. These data might help to identify points for improvement in the management of patients in middle-income countries and elsewhere.\n \n Funding\n None.\n \n Translation\n For the Spanish translation of the Summary see Supplementary Materials section.\n', 'corpus_id': 235717001, 'score': 0}, {'doc_id': '235822267', 'title': 'Prediction of 28-day mortality in critically ill patients with COVID-19: Development and internal validation of a clinical prediction model', 'abstract': 'Background COVID-19 pandemic has rapidly required a high demand of hospitalization and an increased number of intensive care units (ICUs) admission. Therefore, it became mandatory to develop prognostic models to evaluate critical COVID-19 patients. Materials and methods We retrospectively evaluate a cohort of consecutive COVID-19 critically ill patients admitted to ICU with a confirmed diagnosis of SARS-CoV-2 pneumonia. A multivariable Cox regression model including demographic, clinical and laboratory findings was developed to assess the predictive value of these variables. Internal validation was performed using the bootstrap resampling technique. The model’s discriminatory ability was assessed with Harrell’s C-statistic and the goodness-of-fit was evaluated with calibration plot. Results 242 patients were included [median age, 64 years (56–71 IQR), 196 (81%) males]. Hypertension was the most common comorbidity (46.7%), followed by diabetes (15.3%) and heart disease (14.5%). Eighty-five patients (35.1%) died within 28 days after ICU admission and the median time from ICU admission to death was 11 days (IQR 6–18). In multivariable model after internal validation, age, obesity, procaltitonin, SOFA score and PaO2/FiO2 resulted as independent predictors of 28-day mortality. The C-statistic of the model showed a very good discriminatory capacity (0.82). Conclusions We present the results of a multivariable prediction model for mortality of critically ill COVID-19 patients admitted to ICU. After adjustment for other factors, age, obesity, procalcitonin, SOFA and PaO2/FiO2 were independently associated with 28-day mortality in critically ill COVID-19 patients. The calibration plot revealed good agreements between the observed and expected probability of death.', 'corpus_id': 235822267, 'score': 0}, {'doc_id': '3979489', 'title': ""Academic physicians' views on low-value services and the choosing wisely campaign: A qualitative study."", 'abstract': 'BACKGROUND\nIn 2012, the American Board of Internal Medicine (ABIM) Foundation launched a campaign called Choosing Wisely which was intended to start a national dialogue on services that are not medically necessary. More research is needed on the in-depth reasons why doctors overuse low-value services, their views on Choosing Wisely specifically, and ways to help them change their practice patterns.\n\n\nMETHODS\nWe performed a qualitative study of focus groups with physicians to explore their views on the problem of overuse of low-value services, the reasons why they overuse, and ways that they think could be effective at curbing overuse. Participants were attendings in the fields of emergency medicine, internal medicine, hospital medicine, and cardiology.\n\n\nRESULTS\nAll physicians felt that overuse of low-value services was a significant problem. Physicians frequently cited that patient expectations drove the use of low-value services and lack of time was the most cited reason why behavior change was difficult. Facilitators that could promote behavior change included decision support through the electronic medical record, motivation to maintain their reputation among their colleagues, internal motivation to be a good doctor, objective data showing their rates of overuse, alignment of institutional goals, and forums to discuss evidence and new research.\n\n\nCONCLUSIONS AND IMPLICATIONS\nIn focus groups with physicians, we found that physicians perceived that overuse of low-value services was a problem. Participants cited many barriers to behavior change. Methods that help address patient expectations, physician time, and social norms may help physicians reduce their use of low-value services.', 'corpus_id': 3979489, 'score': 1}, {'doc_id': '235608549', 'title': 'Improving the Timing of Laboratory Studies in Hospitalized Children: A Quality Improvement Study.', 'abstract': ""OBJECTIVES\nFor hospitalized children and their families, laboratory study collection at night and in the early morning interrupts sleep and increases the stress of a hospitalization. To change this practice, our quality improvement (QI) study developed a rounding checklist aimed at increasing the percentage of routine laboratory studies ordered for and collected after 7 am.\n\n\nMETHODS\nOur QI study was conducted on the pediatric hospital medicine service at a single-site urban children's hospital over 28 months. Medical records from 420 randomly selected pediatric inpatients were abstracted, and 5 plan-do-study-act cycles were implemented during the intervention. Outcome measures included the percentage of routine laboratory studies ordered for and collected after 7 am. The process measure was use of the rounding checklist. Run charts were used for analysis.\n\n\nRESULTS\nThe percentage of laboratory studies ordered for after 7 am increased from a baseline median of 25.8% to a postintervention median of 75.0%, exceeding our goal of 50% and revealing special cause variation. In addition, the percentage of laboratory studies collected after 7 am increased from a baseline median of 37.1% to 76.4% post intervention, with special cause variation observed.\n\n\nCONCLUSIONS\nBy implementing a rounding checklist, our QI study successfully increased the percentage of laboratory studies ordered for and collected after 7 am and could serve as a model for other health care systems to impact provider ordering practices and behavior. In future initiatives, investigators should evaluate the effects of similar interventions on caregiver and provider perceptions of patient- and family-centeredness, satisfaction, and the quality of patient care."", 'corpus_id': 235608549, 'score': 1}]"
36	{'doc_id': '232372180', 'title': 'Chord Conditioned Melody Generation With Transformer Based Decoders', 'abstract': 'For successful artificial music composition, chords and melody must be aligned well. Yet, chord conditioned melody generation remains a challenging task mainly due to its multimodality. While few studies have focused on this task, they face difficulties in generating dynamic rhythm patterns aligned appropriately with a given chord progression. In this paper, we propose a chord conditioned melody Transformer, a K-POP melody generation model, which separately produces rhythm and pitch conditioned on a chord progression. The model is trained in two phases. A rhythm decoder (RD) is trained first, and subsequently a pitch decoder is trained by utilizing the pre-trained RD. Experimental results show that reusing RD at the pitch decoding stage and training with pitch varied rhythm data improve the performance. It was also observed that the samples produced by the model well reflected the key characteristics of dataset in terms of both pitch and rhythm related features, including chord tone ratio and rhythm distribution. Qualitative analysis reveals the model’s capability of generating various melodies in accordance with a given chord progression, as well as the presence of repetitions and variations within the generated melodies. With subjective human listening test, we come to a conclusion that the model was able to successfully produce new melodies that sound pleasant in terms of both rhythm and pitch (Source code available at https://github.com/ckycky3/CMT-pytorch).', 'corpus_id': 232372180}	16008	"[{'doc_id': '233393210', 'title': 'Ravel’s Sound', 'abstract': 'Ravel’s interwar compositions and transcriptions reveal a sophisticated engagement with timbre and orchestration. Of interest is the way he uses timbre to connect and conceal passages in his music. In this article, I look at the way Ravel manipulates instrumental timbre to create sonic illusions that transform expectations, mark the form, and create meaning. I examine how he uses instrumental groupings to create distinct or blended auditory events, which I relate to musical structure. Using an aurally based analytical approach, I develop these descriptions of timbre and auditory scenes to interpret ways in which different timbre-spaces function. Through techniques such as timbral transformations, magical effects, and timbre and contour fusion, I examine the ways in which Ravel conjures sound objects in his music that are imaginary, transformative, or illusory. DOI: 10.30535/mto.27.1.0 Volume 27, Number 1, March 2021 Copyright © 2021 Society for Music Theory', 'corpus_id': 233393210, 'score': 1}, {'doc_id': '233218730', 'title': 'Dissonance suppression during harmonic mixing', 'abstract': 'This work reviews the thesis Techniques for automatic dissonance suppression in harmonic mixing by Vittorio Maffei. Dance clubs require a continuous mix of music that does not interrupt the flow when switching between two songs. While traditionally done by a live DJ, new approaches can solve this problem fully or semi-automated. Maffei’s thesis extends previous pitch-shifting approaches by modifying the spectral composition to remove dissonances.', 'corpus_id': 233218730, 'score': 1}, {'doc_id': '232404630', 'title': 'On TasNet for Low-Latency Single-Speaker Speech Enhancement', 'abstract': 'In recent years, speech processing algorithms have seen tremendous progress primarily due to the deep learning renaissance. This is especially true for speech separation where the timedomain audio separation network (TasNet) has led to significant improvements. However, for the related task of single-speaker speech enhancement, which is of obvious importance, it is yet unknown, if the TasNet architecture is equally successful. In this paper, we show that TasNet improves state-of-the-art also for speech enhancement, and that the largest gains are achieved for modulated noise sources such as speech. Furthermore, we show that TasNet learns an efficient inner-domain representation, where target and noise signal components are highly separable. This is especially true for noise in terms of interfering speech signals, which might explain why TasNet performs so well on the separation task. Additionally, we show that TasNet performs poorly for large frame hops and conjecture that aliasing might be the main cause of this performance drop. Finally, we show that TasNet consistently outperforms a state-of-the-art single-speaker speech enhancement system.', 'corpus_id': 232404630, 'score': 0}, {'doc_id': '232377748', 'title': 'Songwriting with Iconic Notation in a Music Technology Classroom', 'abstract': 'Recognizing that music teachers may struggle to implement songwriting activities in a classroom, and that iconic notation provides an opportunity to increase access to school music for all students, the purpose of this article is to share one model of songwriting activities in a music technology class using chord diagrams, beat grids, and keyboard charts. The article outlines specific steps to the creation of drum grooves, simple chord progressions, bass lines, and melodies, using forms of notation that are appropriate for popular music instruments and styles.', 'corpus_id': 232377748, 'score': 0}, {'doc_id': '232701834', 'title': 'IMPLEMENTATION OF MACHINE LEARNING AND DEEP LEARNING IN IMPROVING SIGNAL TO NOISE RATIO FOR AUDIO ANALYSIS IN DIGITAL MEDIA PLATFORMS', 'abstract': 'Audio analysis is fast becoming a requirement of digital media for analysing multiple frequencies of sound at a time and also reduce the background noise.The urban sound challenge\xa0 is increasing day by day and the problem is meant to introduce \xa0for audio analysis and processing in the usual classification scenario. Model is implemented on the basis of keras framework and librosalibrary. Keras is capable of running of the algorithm tensorflow. Kerascan be described as an interface rather than a standalone machine learning tools. Librosa is one of the python library for music and audio analysis. It helps us with necessary music information retrieval systems. \nThe authorcollect the database and use the data and also use the graph for a better understanding of audio data analysis.', 'corpus_id': 232701834, 'score': 1}, {'doc_id': '233296970', 'title': 'Visually Guided Sound Source Separation and Localization using Self-Supervised Motion Representations', 'abstract': 'The objective of this paper is to perform audio-visual sound source separation, i.e. to separate component audios from a mixture based on the videos of sound sources. Moreover, we aim to pinpoint the source location in the input video sequence. Recent works have shown impressive audio-visual separation results when using prior knowledge of the source type (e.g. human playing instrument) and pre-trained motion detectors (e.g. keypoints or optical flows). However, at the same time, the models are limited to a certain application domain. In this paper, we address these limitations and make the following contributions: i) we propose a two-stage architecture, called Appearance and Motion network (AMnet), where the stages specialise to appearance and motion cues, respectively. The entire system is trained in a self-supervised manner; ii) we introduce an Audio-Motion Embedding (AME) framework to explicitly represent the motions that related to sound; iii) we propose an audio-motion transformer architecture for audio and motion feature fusion; iv) we demonstrate state-of-the-art performance on two challenging datasets (MUSIC-21 and AVE) despite the fact that we do not use any pre-trained keypoint detectors or optical flow estimators. Project page: https://ly-zhu.github.io/self-supervisedmotion-representations.', 'corpus_id': 233296970, 'score': 0}, {'doc_id': '232372487', 'title': 'Enhancing Local Dependencies for Transformer-Based Text-to-Speech via Hybrid Lightweight Convolution', 'abstract': 'Owing to the powerful self-attention mechanism, the Transformer network has achieved considerable successes across many sequence modeling tasks and has become one of the most popular methods in text-to-speech (TTS). The vanilla self-attention excels in capturing long-range dependencies but suffers in modeling stable short-range dependencies that are quite important for speech synthesis where the local audio signals are highly correlated. To address this problem, we propose the hybrid lightweight convolution (HLC), which is responsible for fully exploiting local structures of a sequence, and combine it with the self-attention to improve the Transformer-based TTS. The experimental results show that our modified model obtains better performance in both objective and subjective evaluations. At the same time, we also demonstrate that a more compact TTS model may be built through the combination of self-attention and proposed hybrid lightweight convolution. Besides, this method is also potentially adaptable for other sequence modeling tasks.', 'corpus_id': 232372487, 'score': 0}, {'doc_id': '233228767', 'title': ""Tracking Musical Voices in Bach's The Art of the Fugue: Timbral Heterogeneity Differentially Affects Younger Normal-Hearing Listeners and Older Hearing-Aid Users"", 'abstract': ""Auditory scene analysis is an elementary aspect of music perception, yet only little research has scrutinized auditory scene analysis under realistic musical conditions with diverse samples of listeners. This study probed the ability of younger normal-hearing listeners and older hearing-aid users in tracking individual musical voices or lines in JS Bach's The Art of the Fugue. Five-second excerpts with homogeneous or heterogenous instrumentation of 2–4 musical voices were presented from spatially separated loudspeakers and preceded by a short cue for signaling the target voice. Listeners tracked the cued voice and detected whether an amplitude modulation was imposed on the cued voice or a distractor voice. Results indicated superior performance of young normal-hearing listeners compared to older hearing-aid users. Performance was generally better in conditions with fewer voices. For young normal-hearing listeners, there was interaction between the number of voices and the instrumentation: performance degraded less drastically with an increase in the number of voices for timbrally heterogeneous mixtures compared to homogeneous mixtures. Older hearing-aid users generally showed smaller effects of the number of voices and instrumentation, but no interaction between the two factors. Moreover, tracking performance of older hearing aid users did not differ when these participants did or did not wear hearing aids. These results shed light on the role of timbral differentiation in musical scene analysis and suggest reduced musical scene analysis abilities of older hearing-impaired listeners in a realistic musical scenario."", 'corpus_id': 233228767, 'score': 1}, {'doc_id': '227953506', 'title': 'A Computational Model of Tonal Tension Profile of Chord Progressions in the Tonal Interval Space', 'abstract': 'In tonal music, musical tension is strongly associated with musical expression, particularly with expectations and emotions. Most listeners are able to perceive musical tension subjectively, yet musical tension is difficult to be measured objectively, as it is connected with musical parameters such as rhythm, dynamics, melody, harmony, and timbre. Musical tension specifically associated with melodic and harmonic motion is called tonal tension. In this article, we are interested in perceived changes of tonal tension over time for chord progressions, dubbed tonal tension profiles. We propose an objective measure capable of capturing tension profile according to different tonal music parameters, namely, tonal distance, dissonance, voice leading, and hierarchical tension. We performed two experiments to validate the proposed model of tonal tension profile and compared against Lerdahl’s model and MorpheuS across 12 chord progressions. Our results show that the considered four tonal parameters contribute differently to the perception of tonal tension. In our model, their relative importance adopts the following weights, summing to unity: dissonance (0.402), hierarchical tension (0.246), tonal distance (0.202), and voice leading (0.193). The assumption that listeners perceive global changes in tonal tension as prototypical profiles is strongly suggested in our results, which outperform the state-of-the-art models.', 'corpus_id': 227953506, 'score': 1}, {'doc_id': '233178181', 'title': 'Audio-Visual Model for Generating Eating Sounds Using Food ASMR Videos', 'abstract': 'We present an audio-visual model for generating food texture sounds from silent eating videos. We designed a deep network-based model that takes the visual features of the detected faces as input and outputs a magnitude spectrogram that aligns with the visual streams. Generating raw waveform samples directly from a given input visual stream is challenging; in this study, we used the Griffin-Lim algorithm for phase recovery from the predicted magnitude to generate raw waveform samples using inverse short-time Fourier transform. Additionally, we produced waveforms from these magnitude spectrograms using an example-based synthesis procedure. To train the model, we created a dataset containing several food autonomous sensory meridian response videos. We evaluated our model on this dataset and found that the predicted sound features exhibit appropriate temporal synchronization with the visual inputs. Our subjective evaluation experiments demonstrated that the predicted sounds are considerably realistic to fool participants in a “real” or “fake” psychophysical experiment.', 'corpus_id': 233178181, 'score': 0}]"
37	{'doc_id': '158368584', 'title': 'Global recessions and booms: what do Probit models tell us?', 'abstract': 'We present non-linear binary Probit models to capture the turning points in global economic activity as well as in advanced and emerging economies from 1980 to 2016. For that purpose, we use four different business cycle dating methods to identify the regimes (upswings, downswings). We find that especially activity-driven variables are important indicators for the turning points. Moreover, we identify similarities and differences between the different regions in this respect.', 'corpus_id': 158368584}	20162	"[{'doc_id': '154900457', 'title': 'Using financial indicators to predict turning points in the business cycle: The case of the leading economic index for the United States', 'abstract': 'In this paper, we evaluate the usefulness of financial indicators according to their ability to predict recessions (i.e., peaks in the business cycle). We then select a small set of financial indicators to aggregate into a single composite index of financial indicators, which we name the Leading Credit Index (LCI). Our approach differs from others in the literature in that we follow the composite index approach of the Leading Economic Index (LEI) of the United States and focus on a small, carefully selected set of indicators as index components, and, in addition, our selection criteria target business cycle turning points rather than financial stress or instability. We show that this leading credit index, either alone or as a component of the LEI, can be helpful in estimating recession probabilities, which it does better than the individual indicators, including some of the existing components of the LEI, especially real money supply.', 'corpus_id': 154900457, 'score': 1}, {'doc_id': '154552209', 'title': 'Predicting severe simultaneous bear stock markets using macroeconomic variables as leading indicators', 'abstract': 'Abstract This paper investigates the predictability of severe simultaneous bear stock markets in 10 industrialized countries. Based on a set of US macroeconomic variables, all of the in-sample and out-of-sample results from probit models with a single macroeconomic variable and with more than one macroeconomic variables confirm that severe simultaneous bear stock markets are indeed predictable. In particular, while the inflation rate is the strongest predictor at longer forecast horizons, the relative long-term government bond yield and the stock return perform best at shorter forecast horizons.', 'corpus_id': 154552209, 'score': 1}, {'doc_id': '157365050', 'title': 'Forecasting Stock Returns in Good and Bad Times: The Role of Market States', 'abstract': 'This paper proposes a state-dependent predictive regression model and finds that a market momentum predictor predicts the excess market return negatively in good times and positively in bad times. The out-of-sample R-square is 2.06% in NBER expansions and 1.84% in NBER recessions. There are similar predictability patterns in the cross-section of U.S. stocks and in the international markets. Our study shows the importance of market states in predicting stock returns, and finds that the concentration of return predictability in bad times, a well documented fact in the literature, is largely due to the assumption of a one-state predictive regression model.', 'corpus_id': 157365050, 'score': 1}, {'doc_id': '237447213', 'title': 'Stock Price Prediction Based on LSTM Deep Learning Model', 'abstract': 'Predicting the stock market is either the easiest or the toughest task in the field of computations. There are many factors related to prediction, physical factors vs. physiological, rational and irrational , capitalist sentiment, market , etc. All these aspects combine to make stock costs volatile and are extremely tough to predict with high accuracy. The prices of a stock market depend very much on demand and supply. High demand stocks will increase in price while heavy selling stocks will decrease. Fluctuations in stock prices affect investor perception and thus there is a need to predict future share prices and to predict stock market prices to make more acquaint and precise investment decisions. We examine data analysis in this domain as a game-changer. This paper proposes that historical value bears the impact of all other market events and can be used to predict future movement. Machine Learning techniques can detect paradigms and insights that can be used to construct surprisingly correct predictions. We propose the LSTM (Long Short Term Memory) model to examine the future price of a stock. This paper is to predict stock market prices to make more acquaint and precise investment decisions.', 'corpus_id': 237447213, 'score': 0}, {'doc_id': '6736249', 'title': 'Nowcasting Recessions with Machine Learning : New Tools for Predicting the Business Cycle', 'abstract': None, 'corpus_id': 6736249, 'score': 1}, {'doc_id': '237288561', 'title': 'Investor Impatience and Financial Markets: the Case of the Short Squeeze of Meme Stocks', 'abstract': 'In this paper, we provide a system of equations to measure investor impatience in financial markets. As in physics, we propose that there exists a measurable force created by external market factors, including investor impatience, which we equate with gravitational force. Using a physics-based Eulerian fluid flow system of equations, we model this force and associated energy conservation equation. We test this hypothesis using minute-by-minute data from meme stocks during a unique market event, the GameStop short squeeze of January 2021. Aside from the effect created by intrinsic market forces, the resulting parameters provide evidence of external force acting on stock prices. We further extend our research to the 2010 flash crash, showing that the system captures external influence on market behavior.', 'corpus_id': 237288561, 'score': 0}, {'doc_id': '211323773', 'title': 'Does machine learning help us predict banking crises?', 'abstract': 'Abstract This paper compares the out-of-sample predictive performance of different early warning models for systemic banking crises using a sample of advanced economies covering the past 45 years. We compare a benchmark logit approach to several machine learning approaches recently proposed in the literature. We find that while machine learning methods often attain a very high in-sample fit, they are outperformed by the logit approach in recursive out-of-sample evaluations. This result is robust to the choice of performance metric, crisis definition, preference parameter, and sample length, as well as to using different sets of variables and data transformations. Thus, our paper suggests that further enhancements to machine learning early warning models are needed before they are able to offer a substantial value-added for predicting systemic banking crises. Conventional logit models appear to use the available information already fairly efficiently, and would for instance have been able to predict the 2007/2008 financial crisis out-of-sample for many countries. In line with economic intuition, these models identify credit expansions, asset price booms and external imbalances as key predictors of systemic banking crises.', 'corpus_id': 211323773, 'score': 1}, {'doc_id': '237233992', 'title': 'The 2000s Housing Cycle With 2020 Hindsight: A Neo-Kindlebergerian View', 'abstract': 'With ""2020 hindsight,\'\' the 2000s housing cycle is not a boom-bust but rather a boom-bust-rebound at both the national level and across cities. We argue this pattern reflects a larger role for fundamentally-rooted explanations than previously thought. We construct a city-level long-run fundamental using a spatial equilibrium regression framework in which house prices are determined by local income, amenities, and supply. The fundamental predicts not only 1997-2019 price and rent growth but also the amplitude of the boom-bust-rebound and foreclosures. This evidence motivates our neo-Kindlebergerian model, in which an improvement in fundamentals triggers a boom-bust-rebound. Agents learn about the fundamentals by observing ""dividends\'\' but become over-optimistic due to diagnostic expectations. A bust ensues when over-optimistic beliefs start to correct, exacerbated by a price-foreclosure spiral that drives prices below their long-run level. The rebound follows as prices converge to a path commensurate with higher fundamental growth. The estimated model explains the boom-bust-rebound with a single fundamental shock and accounts quantitatively for cross-city patterns in the dynamics of prices and foreclosures.', 'corpus_id': 237233992, 'score': 0}, {'doc_id': '158378645', 'title': 'Financial cycles around the world', 'abstract': 'The study analyses ﬁnancial cycles based on a global sample of 34 advanced and developing countries over the period 1960Q1 to 2015Q4. We use dynamic factor models and state-space techniques to estimate ﬁnancial cycles in credit, housing, bond and equity markets, as well as aggregate ﬁnancial cycles for each country in the sample using a large number of variables conveying price, quantity and risk characteristics of respective markets. The analysis reveals the highly persistent and recurring nature of ﬁnancial cycles, which tend to ﬂuctuate at frequencies much lower than business cycles, 9‑15 years on average, and are indicative of major ﬁnancial distress episodes. Our results point to notable intra-regional synchronisation, as well as nontrivial co-movement tendencies between European, American and Asian ﬁnancial cycles. We also extract global and regional ﬁnancial cycles, the former closely associated with the dynamics of the US T-bill rate and the VIX index, conﬁrming the existence of common supranational factors governing the boom-bust dynamics of ﬁnancial market activity around the world.', 'corpus_id': 158378645, 'score': 0}, {'doc_id': '237213075', 'title': 'Macroeconomic Transmission of (Un-)Predictable Uncertainty Shocks', 'abstract': ""We document significant upward bias in estimates of the transmission of uncertainty shocks to real activity found in prominent studies of uncertainty's macroeconomic transmission. We show this bias is due to predictability in these uncertainty shocks. The predictability stems not from the use of ex-post revised data rather than real-time data, but from failure to control for uncertainty's endogenous response to changes in economic conditions. We demonstrate two ways of purging uncertainty shocks of their predictable component and find that uncertainty's transmission to output and employment is much smaller than traditional estimates. Instead, we find that uncertainty's relationship with aggregate real activity is more the result of its role as an amplification mechanism for other macroeconomic and financial shocks."", 'corpus_id': 237213075, 'score': 0}]"
38	{'doc_id': '35322654', 'title': 'Human Genome Ultraconserved Elements Are Ultraselected', 'abstract': 'Ultraconserved elements in the human genome are defined as stretches of at least 200 base pairs of DNA that match identically with corresponding regions in the mouse and rat genomes. Most ultraconserved elements are noncoding and have been evolutionarily conserved since mammal and bird ancestors diverged over 300 million years ago. The reason for this extreme conservation remains a mystery. It has been speculated that they are mutational cold spots or regions where every site is under weak but still detectable negative selection. However, analysis of the derived allele frequency spectrum shows that these regions are in fact under negative selection that is much stronger than that in protein coding genes.', 'corpus_id': 35322654}	16127	"[{'doc_id': '232420787', 'title': 'Ultraconserved enhancer function does not require perfect sequence conservation.', 'abstract': 'Ultraconserved enhancer sequences show perfect conservation between human and rodent genomes, suggesting that their functions are highly sensitive to mutation. However, current models of enhancer function do not sufficiently explain this extreme evolutionary constraint. We subjected 23 ultraconserved enhancers to different levels of mutagenesis, collectively introducing 1,547 mutations, and examined their activities in transgenic mouse reporter assays. Overall, we find that the regulatory properties of ultraconserved enhancers are robust to mutation. Upon mutagenesis, nearly all (19/23, 83%) still functioned as enhancers at one developmental stage, as did most of those tested again later in development (5/9, 56%). Replacement of endogenous enhancers with mutated alleles in mice corroborated results of transgenic assays, including the functional resilience of ultraconserved enhancers to mutation. Our findings show that the currently known activities of ultraconserved enhancers do not necessarily require the perfect conservation observed in evolution and suggest that additional regulatory or other functions contribute to their sequence constraint.', 'corpus_id': 232420787, 'score': 1}, {'doc_id': '237297612', 'title': 'Extreme purifying selection against point mutations in the human genome', 'abstract': 'Genome sequencing of tens of thousands of humans has enabled the measurement of large selective effects for mutations to protein-coding genes. Here we describe a new method, called ExtRaINSIGHT, for measuring similar selective effects in noncoding as well as in coding regions of the human genome. ExtRaINSIGHT estimates the prevalance of strong purifying selection, or “ultraselection” (λs), as the fractional depletion of rare single-nucleotide variants in target genomic sites relative to matched sites that are putatively free from selection, after controlling for local variation and neighbor-dependence in mutation rate. We show using simulations that λs is closely related to the average site-specific selection coefficient against heterozygous point mutations, as predicted at mutation-selection balance. Applying ExtRaINSIGHT to 71,702 whole genome sequences from gnomAD v3, we find strong evidence of ultraselection in evolutionarily ancient miRNAs and neuronal protein-coding genes, as well as at splice sites. By contrast, we find weak evidence in other noncoding RNAs and transcription factor binding sites, and only modest evidence in ultraconserved elements and human accelerated regions. We estimate that ~0.3–0.5% of the human genome is ultraselected, implying ~0.3–0.4 lethal or nearly lethal de novo mutations per potential human zygote. Overall, our study sheds new light on the genome-wide distribution of fitness effects for new point mutations by combining deep new sequencing data sets and classical theory from population genetics.', 'corpus_id': 237297612, 'score': 1}, {'doc_id': '232412244', 'title': 'Sequence and phylogenetic analysis revealed structurally conserved domains and motifs in lincRNA-p21', 'abstract': 'Long Intergenic Non-coding RNAs (lincRNAs) are the largest class of long non-coding RNAs in the eukaryotes, which originate from the intergenic regions of the genome. A ~4kb long lincRNA-p21 is derived from a transcription unit next to the p21/Cdkn1a gene locus. LincRNA-p21 plays key regulatory roles in p53 dependent transcriptional repression and translational repression through its physical association with proteins such as hnRNP-K and HuR.It is also involved in the aberrant gene expression in different cancers. However, detailed information on its structure, recognition, and trans-regulation by proteins is not well known. In this study, we have carried out a complete gene analysis and annotation of lincRNA-p21. This analysis showed that lincRNA-p21 is highly conserved in primates, and its conservation drops significantly in lower organisms. Furthermore, our analysis has revealed two structurally conserved domains in the 5’ and 3’ terminal regions of lincRNA-p21. Phylogenetic analysis has revealed discrete evolutionary dynamics in these conserved domains for orthologous sequences of lincRNA-p21, which have evolved slowly across primates compared to other mammals. Using Infernal based covariance analysis, we have computed the secondary structures of these domains. The secondary structures were further validated by energy minimization criteria for individual orthologous sequences as well as the full-length human lincRNA-p21. In summary, this analysis has led to the identification of sequence and structural motifs in the conserved fragments, indicating the functional importance for these regions.', 'corpus_id': 232412244, 'score': 0}, {'doc_id': '233246520', 'title': 'Intron Losses and Gains in Nematodes: Not Eccentric at All', 'abstract': 'The evolution of spliceosomal introns has been widely studied among various eukaryotic groups. Researchers nearly reached the consensuses on the pattern and the mechanisms of intron losses and gains across eukaryotes. However, according to previous studies that analyzed a few genes or genomes of nematodes, Nematoda seem to be an eccentric group. Taking advantage of the recent accumulation of sequenced genomes, we carried out an extensive analysis on the intron losses and gains using 104 nematodes genomes across all the five Clades of the phylum. Nematodes have a wide range of intron density, from less than one to more than nine per 1kbp coding sequence. The rates of intron losses and gains exhibit significant heterogeneity both across different nematode lineages and across different evolutionary stages of the same lineage. The frequency of intron losses far exceeds that of intron gains. Five pieces of evidence supporting the model of cDNA-mediated intron loss have been observed in ten Caenorhabditis species, the dominance of the precise intron losses, frequent loss of adjacent introns, and high-level expression of the intron-lost genes, preferential losses of short introns, and the preferential losses of introns close to 3′-ends of genes. Like studies in most eukaryotic groups, we cannot find the source sequences for the limited number of intron gains detected in the Caenorhabditis genomes. All the results indicate that nematodes are a typical eukaryotic group rather than an outlier in intron evolution.', 'corpus_id': 233246520, 'score': 0}, {'doc_id': '232279148', 'title': 'Evolution of Retrotransposable Elements', 'abstract': 'ABC Fax + 41 61 306 12 34 E-mail karger@karger.ch www.karger.com © 2005 S. Karger AG, Basel Accessible online at: www.karger.com/cgr Abstract. L1 elements are the most successful retrotransposons in mammals and are responsible for at least 30% of human DNA. Far from being indolent genomic parasites, L1 elements have evolved and amplified rapidly during human evolution. Indeed during just the last 25 million years (MY) five distinct L1 families have emerged and generated tens of thousands of copies. The most recently evolved human specific L1 family is currently active and L1 copies have been accumulating in the human genome at about the same rate per generation as the currently active L1 families in Old World rats and mice. At times during the last 25 MY L1 activity constituted a significant enough genetic load to be subject to negative selection. During these same times, and in apparent response to the host, L1 underwent adaptive evolution. Understanding the molecular basis for these evolutionary changes should help illuminate one of the least understood but most important aspects of L1 biology, namely the extent and nature of the interaction between L1 and its host.', 'corpus_id': 232279148, 'score': 0}, {'doc_id': '13920438', 'title': 'Ultraconserved Enhancers Are Required for Normal Development', 'abstract': 'Non-coding ""ultraconserved"" regions containing hundreds of consecutive bases of perfect sequence conservation across mammalian genomes can function as distant-acting enhancers. However, initial deletion studies in mice revealed that loss of such extraordinarily constrained sequences had no immediate impact on viability. Here, we show that ultraconserved enhancers are required for normal development. Focusing on some of the longest ultraconserved sites genome wide, located near the essential neuronal transcription factor Arx, we used genome editing to create an expanded series of knockout mice lacking individual or combinations of ultraconserved enhancers. Mice with single or pairwise deletions of ultraconserved enhancers were viable and fertile but in nearly all cases showed neurological or growth abnormalities, including substantial alterations of neuron populations and structural brain\xa0defects. Our results demonstrate the functional importance of ultraconserved enhancers and indicate that remarkably strong sequence conservation likely results from fitness deficits that appear subtle in a laboratory setting.', 'corpus_id': 13920438, 'score': 1}, {'doc_id': '207895693', 'title': 'Genetic Variations of Ultraconserved Elements in the Human Genome.', 'abstract': 'Ultraconserved elements (UCEs) are among the most popular DNA markers for phylogenomic analysis. In at least three of five placental mammalian genomes (human, dog, cow, mouse, and rat), 2189 UCEs of at least 200\u2009bp in length that are identical have been identified. Most of these regions have not yet been functionally annotated, and their associations with diseases remain largely unknown. This is an important knowledge gap in human genomics with regard to UCE roles in physiologically critical functions, and by extension, their relevance for shared susceptibilities to common complex diseases across several mammalian organisms in the event of their polymorphic variations. In the present study, we remapped the genomic locations of these UCEs to the latest human genome assembly, and examined them for documented polymorphisms in sequenced human genomes. We identified 29,983 polymorphisms within analyzed UCEs, but revealed that a vast majority exhibits very low minor allele frequencies. Notably, only 112 of the identified polymorphisms are associated with a phenotype in the Ensembl genome browser. Through literature analyses, we confirmed associations of 37 (i.e., out of the 112) polymorphisms within 23 UCEs with 25 diseases and phenotypic traits, including, muscular dystrophies, eye diseases, and cancers (e.g., familial adenomatous polyposis). Most reports of UCE polymorphism-disease associations appeared to be not cognizant that their candidate polymorphisms were actually within UCEs. The present study offers strategic directions and knowledge gaps for future computational and experimental work so as to better understand the thus far intriguing and puzzling role(s) of UCEs in mammalian genomes.', 'corpus_id': 207895693, 'score': 1}, {'doc_id': '233220504', 'title': 'Genome-wide discovery of G-quadruplexes in barley', 'abstract': 'G-quadruplexes (G4s) are four-stranded nucleic acid structures with closely spaced guanine bases forming square planar G-quartets. Aberrant formation of G4 structures has been associated with genomic instability. However, most plant species are lacking comprehensive studies of G4 motifs. In this study, genome-wide identification of G4 motifs in barley was performed, followed by a comparison of genomic distribution and molecular functions to other monocot species, such as wheat, maize, and rice. Similar to the reports on human and some plants like wheat, G4 motifs peaked around the 5′ untranslated region (5′ UTR), the first coding domain sequence, and the first intron start sites on antisense strands. Our comparative analyses in human, Arabidopsis, maize, rice, and sorghum demonstrated that the peak points could be erroneously merged into a single peak when large window sizes are used. We also showed that the G4 distributions around genic regions are relatively similar in the species studied, except in the case of Arabidopsis. G4 containing genes in monocots showed conserved molecular functions for transcription initiation and hydrolase activity. Additionally, we provided examples of imperfect G4 motifs.', 'corpus_id': 233220504, 'score': 0}, {'doc_id': '233299663', 'title': 'Identification of new Anopheles gambiae transcriptional enhancers using a cross‐species prediction approach', 'abstract': 'The success of transgenic mosquito vector control approaches relies on well‐targeted gene expression, requiring the identification and characterization of a diverse set of mosquito promoters and transcriptional enhancers. However, few enhancers have been characterized in Anopheles gambiae to date. Here, we employ the SCRMshaw method we previously developed to predict enhancers in the A. gambiae genome, preferentially targeting vector‐relevant tissues such as the salivary glands, midgut and nervous system. We demonstrate a high overall success rate, with at least 8 of 11 (73%) tested sequences validating as enhancers in an in vivo xenotransgenic assay. Four tested sequences drive expression in either the salivary gland or the midgut, making them directly useful for probing the biology of these infection‐relevant tissues. The success of our study suggests that computational enhancer prediction should serve as an effective means for identifying A. gambiae enhancers with activity in tissues involved in malaria propagation and transmission.', 'corpus_id': 233299663, 'score': 0}, {'doc_id': '8968747', 'title': 'Deletion of Ultraconserved Elements Yields Viable Mice', 'abstract': 'Ultraconserved elements have been suggested to retain extended perfect sequence identity between the human, mouse, and rat genomes due to essential functional properties. To investigate the necessities of these elements in vivo, we removed four noncoding ultraconserved elements (ranging in length from 222 to 731 base pairs) from the mouse genome. To maximize the likelihood of observing a phenotype, we chose to delete elements that function as enhancers in a mouse transgenic assay and that are near genes that exhibit marked phenotypes both when completely inactivated in the mouse and when their expression is altered due to other genomic modifications. Remarkably, all four resulting lines of mice lacking these ultraconserved elements were viable and fertile, and failed to reveal any critical abnormalities when assayed for a variety of phenotypes including growth, longevity, pathology, and metabolism. In addition, more targeted screens, informed by the abnormalities observed in mice in which genes in proximity to the investigated elements had been altered, also failed to reveal notable abnormalities. These results, while not inclusive of all the possible phenotypic impact of the deleted sequences, indicate that extreme sequence constraint does not necessarily reflect crucial functions required for viability.', 'corpus_id': 8968747, 'score': 1}]"
39	{'doc_id': '210971140', 'title': 'Rapid and Robust Monocular Visual-Inertial Initialization with Gravity Estimation via Vertical Edges', 'abstract': 'Monocular visual-inertial tracking without good initialization easily fails due to its non-linear nature. Rapid and accurate metric initialization is crucial. In this paper, we propose a novel monocular visual-inertial initialization method which can initialize the IMU states, camera poses, and scale in a rapid and robust way. To avoid mixing gravity and accelerometer bias, we propose to use the detected vertical edges to estimate a better gravity. This improves the observability to the underlying problem even without sufficient movement, so we can solve all the states crucial for a good initialization. We evaluate our approach on EuRoC dataset and compare with existing state-of-the-art methods. The experimental results demonstrate the effectiveness of the proposed method.', 'corpus_id': 210971140}	2533	[{'doc_id': '230434001', 'title': 'A Hybrid Learner for Simultaneous Localization and Mapping', 'abstract': 'Simultaneous localization and mapping (SLAM) is used to predict the dynamic motion path of a moving platform based on the location coordinates and the precise mapping of the physical environment. SLAM has great potential in augmented reality (AR), autonomous vehicles, viz. self-driving cars, drones, Autonomous navigation robots (ANR). This work introduces a hybrid learning model that explores beyond feature fusion and conducts a multimodal weight sewing strategy towards improving the performance of a baseline SLAM algorithm. It carries out weight enhancement of the front end feature extractor of the SLAM via mutation of different deep networks’ top layers. At the same time, the trajectory predictions from independently trained models are amalgamated to refine the location detail. Thus, the integration of the aforesaid early and late fusion techniques under a hybrid learning framework minimizes the translation and rotation errors of the SLAM model. This study exploits some well-known deep learning (DL) architectures, including ResNet18, ResNet34, ResNet50, ResNet101, VGG16, VGG19, and AlexNet for experimental analysis. An extensive experimental analysis proves that hybrid learner (HL) achieves significantly better results than the unimodal approaches and multimodal approaches with early or late fusion strategies. Hence, it is found that the Apolloscape dataset taken in this work has never been used in the literature under SLAM with fusion techniques, which makes this work unique and insightful.', 'corpus_id': 230434001, 'score': 1}, {'doc_id': '211988831', 'title': 'Redesigning SLAM for Arbitrary Multi-Camera Systems', 'abstract': 'Adding more cameras to SLAM systems improves robustness and accuracy but complicates the design of the visual front-end significantly. Thus, most systems in the literature are tailored for specific camera configurations. In this work, we aim at an adaptive SLAM system that works for arbitrary multi-camera setups. To this end, we revisit several common building blocks in visual SLAM. In particular, we propose an adaptive initialization scheme, a sensor-agnostic, information- theoretic keyframe selection algorithm, and a scalable voxel- based map. These techniques make little assumption about the actual camera setups and prefer theoretically grounded methods over heuristics. We adapt a state-of-the-art visual- inertial odometry with these modifications, and experimental results show that the modified pipeline can adapt to a wide range of camera setups (e.g., 2 to 6 cameras in one experiment) without the need of sensor-specific modifications or tuning.', 'corpus_id': 211988831, 'score': 0}, {'doc_id': '102351483', 'title': '3D LiDAR and Stereo Fusion using Stereo Matching Network with Conditional Cost Volume Normalization', 'abstract': 'The complementary characteristics of active and passive depth sensing techniques motivate the fusion of the LiDAR sensor and stereo camera for improved depth perception. Instead of directly fusing estimated depths across LiDAR and stereo modalities, we take advantages of the stereo matching network with two enhanced techniques: Input Fusion and Conditional Cost Volume Normalization (CCVNorm) on the LiDAR information. The proposed framework is generic and closely integrated with the cost volume component that is commonly utilized in stereo matching neural networks. We experimentally verify the efficacy and robustness of our method on the KITTI Stereo and Depth Completion datasets, obtaining favorable performance against various fusion strategies. Moreover, we demonstrate that, with a hierarchical extension of CCVNorm, the proposed method brings only slight overhead to the stereo matching network in terms of computation time and model size.', 'corpus_id': 102351483, 'score': 1}, {'doc_id': '211989418', 'title': 'LAMP: Large-Scale Autonomous Mapping and Positioning for Exploration of Perceptually-Degraded Subterranean Environments', 'abstract': 'Simultaneous Localization and Mapping (SLAM) in large-scale, unknown, and complex subterranean environments is a challenging problem. Sensors must operate in off-nominal conditions; uneven and slippery terrains make wheel odometry inaccurate, while long corridors without salient features make exteroceptive sensing ambiguous and prone to drift; finally, spurious loop closures that are frequent in environments with repetitive appearance, such as tunnels and mines, could result in a significant distortion of the entire map. These challenges are in stark contrast with the need to build highly-accurate 3D maps to support a wide variety of applications, ranging from disaster response to the exploration of underground extraterrestrial worlds. This paper reports on the implementation and testing of a lidar-based multi-robot SLAM system developed in the context of the DARPA Subterranean Challenge. We present a system architecture to enhance subterranean operation, including an accurate lidar-based front-end, and a flexible and robust back-end that automatically rejects outlying loop closures. We present an extensive evaluation in large-scale, challenging subterranean environments, including the results obtained in the Tunnel Circuit of the DARPA Subterranean Challenge. Finally, we discuss potential improvements, limitations of the state of the art, and future research directions.', 'corpus_id': 211989418, 'score': 0}, {'doc_id': '211069692', 'title': 'Multi-object Monocular SLAM for Dynamic Environments', 'abstract': 'In this paper, we tackle the problem of multibody SLAM from a monocular camera. The term multibody, implies that we track the motion of the camera, as well as that of other dynamic participants in the scene. The quintessential challenge in dynamic scenes is unobservability: it is not possible to unambiguously triangulate a moving object from a moving monocular camera. Existing approaches solve restricted variants of the problem, but the solutions suffer relative scale ambiguity (i.e., a family of infinitely many solutions exist for each pair of motions in the scene). We solve this rather intractable problem by leveraging single-view metrology, advances in deep learning, and category-level shape estimation. We propose a multi pose-graph optimization formulation, to resolve the relative and absolute scale factor ambiguities involved. This optimization helps us reduce the average error in trajectories of multiple bodies over real-world datasets, such as KITTI [1]. To the best of our knowledge, our method is the first practical monocular multi-body SLAM system to perform dynamic multi-object and ego localization in a unified framework in metric scale.', 'corpus_id': 211069692, 'score': 0}, {'doc_id': '210970600', 'title': 'Multi-Sensor 6-DoF Localization For Aerial Robots In Complex GNSS-Denied Environments', 'abstract': 'The need for robots autonomously navigating in more and more complex environments has motivated intense R& D efforts in making robot pose estimation more accurate and reliable. This paper presents a multi-sensor multi-hypothesis method for robust 6-DoF localization in complex environments. Robustness and accuracy requirements are addressed as follows. First, camera and LIDAR features are seamlessly integrated in the same statistical framework, benefiting from their synergies and providing robustness in scenarios with low or varying densities of LIDAR and visual features. Second, a multi-hypothesis approach is adopted to cope with scenario symmetries. The method has been carefully designed to operate in real time using feature and hypothesis filtering and efficient hypothesis refinement, and has been coded in a multi-core implementation. The proposed method has been extensively validated for closed-loop aerial robot navigation in different urban and industrial scenarios and has shown advantages over well-known single-sensor techniques.', 'corpus_id': 210970600, 'score': 1}, {'doc_id': '227334165', 'title': 'TP-TIO: A Robust Thermal-Inertial Odometry with Deep ThermalPoint', 'abstract': 'To achieve robust motion estimation in visually degraded environments, thermal odometry has been an attraction in the robotics community. However, most thermal odometry methods are purely based on classical feature extractors, which is difficult to establish robust correspondences in successive frames due to sudden photometric changes and large thermal noise. To solve this problem, we propose ThermalPoint, a lightweight feature detection network specifically tailored for producing keypoints on thermal images, providing notable anti-noise improvements compared with other state-of-the-art methods. After that, we combine ThermalPoint with a novel radiometric feature tracking method, which directly makes use of full radiometric data and establishes reliable correspondences between sequential frames. Finally, taking advantage of an optimization-based visual-inertial framework, a deep feature-based thermal-inertial odometry (TP-TIO) framework is proposed and evaluated thoroughly in various visually degraded environments. Experiments show that our method outperforms state-of-the-art visual and laser odometry methods in smoke-filled environments and achieves competitive accuracy in normal environments.', 'corpus_id': 227334165, 'score': 1}, {'doc_id': '227162656', 'title': 'Relation3DMOT: Exploiting Deep Affinity for 3D Multi-Object Tracking from View Aggregation', 'abstract': 'Autonomous systems need to localize and track surrounding objects in 3D space for safe motion planning. As a result, 3D multi-object tracking (MOT) plays a vital role in autonomous navigation. Most MOT methods use a tracking-by-detection pipeline, which includes both the object detection and data association tasks. However, many approaches detect objects in 2D RGB sequences for tracking, which lacks reliability when localizing objects in 3D space. Furthermore, it is still challenging to learn discriminative features for temporally consistent detection in different frames, and the affinity matrix is typically learned from independent object features without considering the feature interaction between detected objects in the different frames. To settle these problems, we first employ a joint feature extractor to fuse the appearance feature and the motion feature captured from 2D RGB images and 3D point clouds, and then we propose a novel convolutional operation, named RelationConv, to better exploit the correlation between each pair of objects in the adjacent frames and learn a deep affinity matrix for further data association. We finally provide extensive evaluation to reveal that our proposed model achieves state-of-the-art performance on the KITTI tracking benchmark.', 'corpus_id': 227162656, 'score': 0}, {'doc_id': '221266043', 'title': 'FastORB-SLAM: a Fast ORB-SLAM Method with Coarse-to-Fine Descriptor Independent Keypoint Matching', 'abstract': 'Indirect methods for visual SLAM are gaining popularity due to their robustness to varying environments. ORB-SLAM2 is a benchmark method in this domain, however, the computation of descriptors in ORB-SLAM2 is time-consuming and the descriptors cannot be reused unless a frame is selected as a keyframe. To overcome these problems, we present FastORB-SLAM which is light-weight and efficient as it tracks keypoints between adjacent frames without computing descriptors. To achieve this, a two-stage coarse-to-fine descriptor independent keypoint matching method is proposed based on sparse optical flow. In the first stage, we first predict initial keypoint correspondences via a uniform acceleration motion model and then robustly establish the correspondences via a pyramid-based sparse optical flow tracking method. In the second stage, we leverage motion smoothness and the epipolar constraint to refine the correspondences. In particular, our method computes descriptors only for keyframes. We test FastORB-SLAM with an RGBD camera on \\textit{TUM} and \\textit{ICL-NUIM} datasets and compare its accuracy and efficiency to nine existing RGBD SLAM methods. Qualitative and quantitative results show that our method achieves state-of-the-art performance in accuracy and is about twice as fast as the ORB-SLAM2', 'corpus_id': 221266043, 'score': 0}, {'doc_id': '222291130', 'title': 'CurbScan: Curb Detection and Tracking Using Multi-Sensor Fusion', 'abstract': 'Reliable curb detection is critical for safe autonomous driving in urban contexts. Curb detection and tracking are also useful in vehicle localization and path planning. Past work utilized a 3D LiDAR sensor to determine accurate distance information and the geometric attributes of curbs. However, such an approach requires dense point cloud data and is also vulnerable to false positives from obstacles present on both road and off-road areas. In this paper, we propose an approach to detect and track curbs by fusing together data from multiple sensors: sparse LiDAR data, a mono camera and low-cost ultrasonic sensors. The detection algorithm is based on a single 3D LiDAR and a mono camera sensor used to detect candidate curb features and it effectively removes false positives arising from surrounding static and moving obstacles. The detection accuracy of the tracking algorithm is boosted by using Kalman filter-based prediction and fusion with lateral distance information from low-cost ultrasonic sensors. We next propose a line-fitting algorithm that yields robust results for curb locations. Finally, we demonstrate the practical feasibility of our solution by testing in different road environments and evaluating our implementation in a real vehicle1. Our algorithm maintains over 90% accuracy within 4.5-22 meters and 0-14 meters for the KITTI dataset and our dataset respectively, and its average processing time per frame is approximately 10 ms on Intel i7 x86 and 100ms on NVIDIA Xavier board.', 'corpus_id': 222291130, 'score': 1}]
40	{'doc_id': '235828718', 'title': 'Calibrating Predictions to Decisions: A Novel Approach to Multi-Class Calibration', 'abstract': 'When facing uncertainty, decision-makers want predictions they can trust. A machine learning provider can convey confidence to decision-makers by guaranteeing their predictions are distribution calibrated — amongst the inputs that receive a predicted class probabilities vector q, the actual distribution over classes is q. For multi-class prediction problems, however, achieving distribution calibration tends to be infeasible, requiring sample complexity exponential in the number of classes C. In this work, we introduce a new notion—decision calibration—that requires the predicted distribution and true distribution to be “indistinguishable” to a set of downstream decision-makers. When all possible decision makers are under consideration, decision calibration is the same as distribution calibration. However, when we only consider decision makers choosing between a bounded number of actions (e.g. polynomial in C), our main result shows that decisions calibration becomes feasible — we design a recalibration algorithm that requires sample complexity polynomial in the number of actions and the number of classes. We validate our recalibration algorithm empirically: compared to existing methods, decision calibration improves decision-making on skin lesion and ImageNet classification with modern neural network predictors.', 'corpus_id': 235828718}	19320	"[{'doc_id': '236634614', 'title': 'Reliable Decisions with Threshold Calibration', 'abstract': 'Decision makers rely on probabilistic forecasts to predict the loss of different decision rules before deployment. When the forecasted probabilities match the true frequencies, predicted losses will be accurate. Although perfect forecasts are typically impossible, probabilities can be calibrated to match the true frequencies on average. However, we find that this average notion of calibration, which is typically used in practice, does not necessarily guarantee accurate decision loss prediction. Specifically in the regression setting, the loss of threshold decisions, which are decisions based on whether the forecasted outcome falls above or below a cutoff, might not be predicted accurately. We propose a stronger notion of calibration called threshold calibration, which is exactly the condition required to ensure that decision loss is predicted accurately for threshold decisions. We provide an efficient algorithm which takes an uncalibrated forecaster as input and provably outputs a threshold-calibrated forecaster. Our procedure allows downstream decision makers to confidently estimate the loss of any threshold decision under any threshold loss function. Empirically, threshold calibration improves decision loss prediction without compromising on the quality of the decisions in two real-world settings: hospital scheduling decisions and resource allocation decisions.', 'corpus_id': 236634614, 'score': 1}, {'doc_id': '237243759', 'title': 'The Fermi–Dirac distribution provides a calibrated probabilistic output for binary classifiers', 'abstract': 'Significance While it would be desirable that the output of binary classification algorithms be the probability that the classification is correct, most algorithms do not provide a method to calculate such a probability. We propose a probabilistic output for binary classifiers based on an unexpected mapping of the probability of correct classification to the probability of occupation of a fermion in a quantum system, known as the Fermi–Dirac distribution. This mapping allows us to compute the optimal threshold to separate predicted classes and to calculate statistical parameters necessary to estimate confidence intervals of performance metrics. Using this mapping we propose an ensemble learning algorithm. In short, the Fermi–Dirac distribution provides a calibrated probabilistic output for binary classification. Binary classification is one of the central problems in machine-learning research and, as such, investigations of its general statistical properties are of interest. We studied the ranking statistics of items in binary classification problems and observed that there is a formal and surprising relationship between the probability of a sample belonging to one of the two classes and the Fermi–Dirac distribution determining the probability that a fermion occupies a given single-particle quantum state in a physical system of noninteracting fermions. Using this equivalence, it is possible to compute a calibrated probabilistic output for binary classifiers. We show that the area under the receiver operating characteristics curve (AUC) in a classification problem is related to the temperature of an equivalent physical system. In a similar manner, the optimal decision threshold between the two classes is associated with the chemical potential of an equivalent physical system. Using our framework, we also derive a closed-form expression to calculate the variance for the AUC of a classifier. Finally, we introduce FiDEL (Fermi–Dirac-based ensemble learning), an ensemble learning algorithm that uses the calibrated nature of the classifier’s output probability to combine possibly very different classifiers.', 'corpus_id': 237243759, 'score': 1}, {'doc_id': '235694507', 'title': 'Well-calibrated prediction intervals for regression problems', 'abstract': 'Over the last few decades, various methods have been proposed for estimating prediction intervals in regression settings, including Bayesian methods, ensemble methods, direct interval estimation methods and conformal prediction methods. An important issue is the calibration of these methods: the generated prediction intervals should have a predefined coverage level, without being overly conservative. In this work, we review the above four classes of methods from a conceptual and experimental point of view. Results on benchmark data sets from various domains highlight large fluctuations in performance from one data set to another. These observations can be attributed to the violation of certain assumptions that are inherent to some classes of methods. We illustrate how conformal prediction can be used as a general calibration procedure for methods that deliver poor results without a calibration step.', 'corpus_id': 235694507, 'score': 1}, {'doc_id': '237940579', 'title': 'Training on Test Data with Bayesian Adaptation for Covariate Shift', 'abstract': 'When faced with distribution shift at test time, deep neural networks often make inaccurate predictions with unreliable uncertainty estimates. While improving the robustness of neural networks is one promising approach to mitigate this issue, an appealing alternate to robustifying networks against all possible test-time shifts is to instead directly adapt them to unlabeled inputs from the particular distribution shift we encounter at test time. However, this poses a challenging question: in the standard Bayesian model for supervised learning, unlabeled inputs are conditionally independent of model parameters when the labels are unobserved, so what can unlabeled data tell us about the model parameters at test-time? In this paper, we derive a Bayesian model that provides for a well-defined relationship between unlabeled inputs under distributional shift and model parameters, and show how approximate inference in this model can be instantiated with a simple regularized entropy minimization procedure at test-time. We evaluate our method on a variety of distribution shifts for image classification, including image corruptions, natural distribution shifts, and domain adaptation settings, and show that our method improves both accuracy and uncertainty estimation.', 'corpus_id': 237940579, 'score': 0}, {'doc_id': '237635041', 'title': 'Sample Efficient Model Evaluation', 'abstract': 'Labelling data is a major practical bottleneck in training and testing classifiers. Given a collection of unlabelled data points, we address how to select which subset to label to best estimate test metrics such as accuracy, F1 score or micro/macro F1. We consider two sampling based approaches, namely the well-known Importance Sampling and we introduce a novel application of Poisson Sampling. For both approaches we derive the minimal error sampling distributions and how to approximate and use them to form estimators and confidence intervals. We show that Poisson Sampling outperforms Importance Sampling both theoretically and experimentally. 1 Offline Model Evaluation How to select training examples from an unlabelled pool to minimise labelling effort is well studied (see for example [1]). However, the complementary problem of selecting which testpoints to label to estimate test performance is less well studied. Having a good estimate of test performance is vital to gain confidence in the predictive performance of a model. We focus in this initial work on the ‘offline’ setting in which we assume that a single set of data points will be selected to be labelled. The ‘online’ setting in which labelled testpoints can inform the future selection of testpoints to label is left for a separate study. We assume that we have a trained probabilistic binary classifier p(cn|xn), where xn is an input and cn ∈ {0, 1} is the predicted class, with cn = 1 being the positive’ class and cn = 0 being the ‘negative’ class. From this trained classifier we assume that a class label is produced deterministically. For example, one may use thresholding to set cn = 1 if p(c p n = 1|xn) > θ for some user specified θ. Given a set of test inputs X = {x1, . . . , xN} we would like to estimate various measures of performance such as accuracy, F1 score, etc. However, we do not a priori know the true class labels cn and assume that it is very costly (in time/effort) to obtain these true class labels. We therefore know the test set inputs and wish to estimate the test performance using as little test labelling effort as possible. Assuming for the moment that we have access to all true test labels, cn ∈ {0, 1}, we first consider test metrics of the form F ≡ ∑N n=1 f(c p n, c t n) ∑N n=1 g(c p n, cn) (1) For example, for the accuracy metric we have f(c, c) = I [c = c], g(c, c) = 1. Here I [x = y] is the indicator function, being 1 when x = y and 0 otherwise. Similarly, for the Fα metric 0 ≤ α ≤ 1 Fα ≡ ∑N n=1 I [cn = 1] I [cn = 1] α ∑N n=1 I [c p n = 1] + (1− α) ∑N n=1 I [cn = 1] (2) where f(c, c) = I [ c = 1 ] I [c = 1] , g(c, c) = αI [c = 1] + (1− α)I [ c = 1 ] . (3) ar X iv :2 10 9. 12 04 3v 1 [ cs .L G ] 2 4 Se p 20 21 The F1 metric is given by F 1 2 ; recall (F0) and precision (F1) along with other metrics are also easily defined. Where there is no ambiguity, we write fn in place of f(cn, c t n), and similarly for gn. The above metrics require us to know the true value of the test label cn. However, in our scenario we wish to get a good estimate of the test metric, without having to label all test points. A simple approach is to uniformly sample a subset of test points xn, label them and calculate the performance. However, this is sub-optimal, particularly in the case of high class imbalance which occurs frequently in practice. Consider for example the ability to correctly classify an offensive tweet for content moderation. Only a small percentage of the dataset (say less than 7%) may contain offensive tweets1. We therefore wish to actively select datapoints to label such that we obtain an accurate metric (for example, by concentrating on tweets which are likely to be offensive). We consider two sampling-based approaches to estimating test metrics – Importance Sampling and Poisson Sampling. Full derivations are in the appendix, including the more general approach in app(D) that can be used to estimate for example the macro F1 score in multi-label classification. The theory behind the approaches is similar and we begin with the better known Importance Sampling. We note that an ideal estimator would have the property that as all N test labels are known, the metric will be correctly evaluated with no uncertainty (for a deterministic true classifier p(cn|xn)). It is important to bear in mind the two stages in which we need estimates of the metric. The presampling stage estimates the metric to approximate the optimal sampler. The post-sampling stage uses the drawn samples to form an estimate of the performance. We also note that the metric used to form the samples may not be the same as the metric used for the evaluation. This is because we will typically only wish to use a single criterion to determine which test points should be labelled – however, we may wish to estimate a variety of performance metrics using those samples. For example, we may decide which datapoints to label on the basis of getting the best estimate of F1 score, but also use those samples to estimate F1, recall, precision, etc. 2 Importance Sampling We first extend the work of [2] to a more general class of test metrics. Importance Sampling (IS) can be used to estimate test performance by independently sampling (with replacement) indices m1, . . . ,mM , mi ∈ {1, . . . , N} from the test dataset using the importance distribution qn, n ∈ {1, . . . , N}. We then form the estimator using F̂ = x̂ ŷ (4) where x̂ and ŷ are obtained from M sampled datapoints:', 'corpus_id': 237635041, 'score': 0}, {'doc_id': '237593007', 'title': 'Quantifying Model Predictive Uncertainty with Perturbation Theory', 'abstract': 'We propose a framework for predictive uncertainty quantification of a neural network that replaces the conventional Bayesian notion of weight probability density function (PDF) with a physics based potential field representation of the model weights in a Gaussian reproducing kernel Hilbert space (RKHS) embedding. This allows us to use perturbation theory from quantum physics to formulate a moment decomposition problem over the model weight-output relationship. The extracted moments reveal successive degrees of regularization of the weight potential field around the local neighborhood of the model output. Such localized moments represent well the PDF tails and provide significantly greater accuracy of the model’s predictive uncertainty than the central moments characterized by Bayesian and ensemble methods or their variants. We show that this consequently leads to a better ability to detect false model predictions of test data that has undergone a covariate shift away from the training PDF learned by the model. We evaluate our approach against baseline uncertainty quantification methods on several benchmark datasets that are corrupted using common distortion techniques. Our approach provides fast model predictive uncertainty estimates with much greater precision and calibration. Deep neural network (DNN) models have become the predominant choice for pattern representation in a wide variety of machine learning applications due to their remarkable performance advantages in the presence of large amount data (LeCun et al., 2015). The increased adoption of DNNs in safety critical and high stake problems such as medical diagnosis, chemical plant control, defense sysFigure 1: Proposed approach: Moments extracted from the local interaction of the model output with the RKHS potential field of the weights quantify the output uncertainty. tems and autonomous driving has led to growing concerns within the research community on the performance trustworthiness of such models (Kendall & Gal, 2017; Lundberg & Lee, 2017). This becomes particularly imperative in situations involving data distributional shifts or the presence of out of distribution data (OOD) during testing towards which the model may lack robustness due to poor choice of training parameters or lack of sufficiently labeled training data, especially since machine learning algorithms do not have extensive prior information like humans to deal with such situations (Amodei et al., 2016). An important way through which trust in the performance of machine learning algorithms (particularly DNNs) can be established is through accurate techniques of predictive uncertainty quantification of models that allow practitioners to determine how much they should rely on model predictions. Although there have been several categories of methods developed in the recent years, the Bayesian approach ar X iv :2 10 9. 10 88 8v 1 [ cs .L G ] 2 2 Se p 20 21 (MacKay, 1992; Neal, 2012; Bishop, 1995) had for long been regarded as the gold standard for natural representation of uncertainty in neural networks. However, it has been realized that they are unable to scale to modern applications and often fail to capture the true data distribution in practice (Lakshminarayanan et al., 2017). Another fundamental limitation of Bayesian techniques is that they are only capable of selecting measurements of central tendency from the posterior. This limits their sensitivity in quantifying local uncertainty information which is especially prevalent in modern applications where models can capture very complex patterns with significant local variations. We propose an approach for model uncertainty quantification that relies on the density and local fit criteria (Leonard et al., 1992) which in this context means that a model prediction y is reliable/certain only if the model has been trained to make predictions in the local vicinity of y (i.e. the model is locally regularized around y). To quantify the model space according to this criteria, one requires a rich localized representation of the model predictive PDF p(y |w) which deems a prediction y reliable only if δ= p(y +∆y |w)−p(y |w) ≈ 0 where∆y is a small perturbation around y . Towards this end, we utilize an uncertainty decomposition framework called the quantum information potential field (QIPF) (Singh & Principe (2020); Singh & Principe (2021)) through which we represent the model weight space as a potential field by embedding the weights in a Gaussian reproducing kernel Hilbert space (RKHS). Such a representation (depicted in Fig. 1) allows us to principally use the notion of perturbation theory in quantum physics to quantify the local gradients of the model’s predictive PDF space in terms of multiple uncertainty moments, thereby giving a high resolution description of δ. In essence, these moments successively quantify the degree of regularization of the weights in the local neighborhood of the model output. The QIPF framework, which was first introduced by (Singh & Principe (2020); Singh & Principe (2021)), has shown promising (albeit very preliminary) results in model uncertainty quantification for regression problems (Singh & Principe, 2021) and in particular applications of time series analysis (Singh & Principe, 2020). It enjoys the following advantages over other methods: • The QIPF utilizes the Gaussian RKHS whose mathematical properties, specifically the kernel trick (Smola & Schölkopf, 1998) and the kernel mean embedding theory (Muandet et al., 2017), makes it a universal injective estimator of data PDF and without making any underlying assumptions. • Through its physics based moment decomposition formulation, it is able to provide a multi-scale description of the local PDF dynamics that focuses on the tail regions of the PDF thereby providing a very accurate description of uncertainty. • It is also significantly simpler to compute and more scalable than Bayesian approaches. It is non-intrusive to the training process of the model and enables a single-shot estimation of model uncertainty at each test instance thereby offering practical advantages over ensemble and Monte Carlo approaches. Our main contributions in this paper are three-fold: • We use perturbation theory to provide a more concrete description of the QIPF framework and provide a new insight into how a potential field viewpoint of the model weight space can be used to describe the degree of regularization/certainty of trained weights in the local neighborhood of the output in terms of multiple moments. • We specifically evaluate the performance of the framework in an important and unsolved problem of model uncertainty quantification in situations involving covariate shift in test data. • We analyze the performance of the QIPF against baselines of multiple UQ approaches using diverse models trained on benchmark datasets. We use both accuracy as well as calibration metrics to evaluate performance.', 'corpus_id': 237593007, 'score': 0}, {'doc_id': '237489696', 'title': 'Adaptation of CNN Classifiers to Prior Shift', 'abstract': 'In many classification tasks, the test set’s relative class frequencies (class priors probabilities) differ from the relative class frequencies at training time. Such a phenomenon, called label shift or prior shift, can negatively affect the classifier’s performance. Considering a probabilistic classifier approximating posterior probabilities, the predictions can be adapted to the label shift by re-weighting with a ratio of the test set and training set priors. Labels in the test set are usually unknown, therefore the prior ratio has to be estimated in an unsupervised manner. This thesis reviews existing methods for adapting probabilistic classifiers to label shift and for estimating test priors in an unlabeled test set. Moreover, we propose novel algorithms to address the problems of estimating new priors and prior ratio. The methods are designed to handle a known issue in confusion matrix-based methods, where inconsistent estimates of decision probabilities and confusion matrices lead to negative values in estimated priors. Experimental evaluation shows that our method improves the stability of prior estimation and the adapted classifier’s accuracy compared to the baseline confusion matrix-based methods and achieves state-of-the-art performance in prior shift adaptation.', 'corpus_id': 237489696, 'score': 0}, {'doc_id': '235829044', 'title': ""What classifiers know what they don't?"", 'abstract': 'Being uncertain when facing the unknown is key to intelligent decision making. However, machine learning algorithms lack reliable estimates about their predictive uncertainty. This leads to wrong and overly-confident decisions when encountering classes unseen during training. Despite the importance of equipping classifiers with uncertainty estimates ready for the real world, prior work has focused on small datasets and little or no class discrepancy between training and testing data. To close this gap, we introduce UIMNET: a realistic, ImageNet-scale test-bed to evaluate predictive uncertainty estimates for deep image classifiers. Our benchmark provides implementations of eight state-of-the-art algorithms, six uncertainty measures, four in-domain metrics, three out-domain metrics, and a fully automated pipeline to train, calibrate, ensemble, select, and evaluate models. Our test-bed is open-source and all of our results are reproducible from a fixed commit in our repository. Adding new datasets, algorithms, measures, or metrics is a matter of a few lines of code—in so hoping that UIMNET becomes a stepping stone towards realistic, rigorous, and reproducible research in uncertainty estimation. Our results show that ensembles of ERM classifiers as well as single MIMO classifiers are the two best alternatives currently available to measure uncertainty about both in-domain and out-domain classes.', 'corpus_id': 235829044, 'score': 0}, {'doc_id': '237213527', 'title': 'Optimally Efficient Sequential Calibration of Binary Classifiers to Minimize Classification Error', 'abstract': 'In this work, we aim to calibrate the score outputs of an estimator for the binary classification problem by finding an ’optimal’ mapping to class probabilities, where the ’optimal’ mapping is in the sense that minimizes the classification error (or equivalently, maximizes the accuracy). We show that for the given target variables and the score outputs of an estimator, an ’optimal’ soft mapping, which monotonically maps the score values to probabilities, is a hard mapping that maps the score values to 0 and 1. We show that for class weighted (where the accuracy for one class is more important) and sample weighted (where the samples’ accurate classifications are not equally important) errors, or even general linear losses; this hard mapping characteristic is preserved. We propose a sequential recursive merger approach, which produces an ’optimal’ hard mapping (for the observed samples so far) sequentially with each incoming new sample. Our approach has a logarithmic in sample size time complexity, which is optimally efficient.', 'corpus_id': 237213527, 'score': 1}, {'doc_id': '237581126', 'title': 'Bayesian Confidence Calibration for Epistemic Uncertainty Modelling', 'abstract': 'Modern neural networks have found to be miscalibrated in terms of confidence calibration, i.e., their predicted confidence scores do not reflect the observed accuracy or precision. Recent work has introduced methods for post-hoc confidence calibration for classification as well as for object detection to address this issue. Especially in safety critical applications, it is crucial to obtain a reliable self-assessment of a model. But what if the calibration method itself is uncertain, e.g., due to an insufficient knowledge base? We introduce Bayesian confidence calibration a framework to obtain calibrated confidence estimates in conjunction with an uncertainty of the calibration method. Commonly, Bayesian neural networks (BNN) are used to indicate a network’s uncertainty about a certain prediction. BNNs are interpreted as neural networks that use distributions instead of weights for inference. We transfer this idea of using distributions to confidence calibration. For this purpose, we use stochastic variational inference to build a calibration mapping that outputs a probability distribution rather than a single calibrated estimate. Using this approach, we achieve state-of-the-art calibration performance for object detection calibration. Finally, we show that this additional type of uncertainty can be used as a sufficient criterion for covariate shift detection. All code is open source and available at https://github.com/EFS-OpenSource/calibrationframework.', 'corpus_id': 237581126, 'score': 0}]"
41	"{'doc_id': '118946159', 'title': 'Mathematical and Physical Ideas for Climate Science', 'abstract': ""The climate is a forced and dissipative nonlinear system featuring nontrivial dynamics on a vast range of spatial and temporal scales. The understanding of the climate's structural and multiscale properties is crucial for the provision of a unifying picture of its dynamics and for the implementation of accurate and efficient numerical models. We present some recent developments at the intersection between climate science, mathematics, and physics, which may prove fruitful in the direction of constructing a more comprehensive account of climate dynamics. We describe the Nambu formulation of fluid dynamics and the potential of such a theory for constructing sophisticated numerical models of geophysical fluids. Then, we focus on the statistical mechanics of quasi-equilibrium flows in a rotating environment, which seems crucial for constructing a robust theory of geophysical turbulence. We then discuss ideas and methods suited for approaching directly the nonequilibrium nature of the climate system. First, we describe some recent findings on the thermodynamics of climate, characterize its energy and entropy budgets, and discuss related methods for intercomparing climate models and for studying tipping points. These ideas can also create a common ground between geophysics and astrophysics by suggesting general tools for studying exoplanetary atmospheres. We conclude by focusing on nonequilibrium statistical mechanics, which allows for a unified framing of problems as different as the climate response to forcings, the effect of altering the boundary conditions or the coupling between geophysical flows, and the derivation of parametrizations for numerical models."", 'corpus_id': 118946159}"	4134	"[{'doc_id': '218674151', 'title': 'Relativistic kinetic theory of classical systems of charged particles: towards the microscopic foundation of thermodynamics and kinetics', 'abstract': 'In the complete system of equations of evolution of the classical system of charges and the electromagnetic field generated by them, the field variables are excluded. An exact closed relativistic non-Hamiltonian system of nonlocal kinetic equations, that describes the evolution of a system of charges in terms of their microscopic distribution functions, is obtained . The solutions of this system of equations are non-invariant with respect to time reversal, and also have the property of hereditarity.', 'corpus_id': 218674151, 'score': 0}, {'doc_id': '219708794', 'title': 'Estimating Concurrent Climate Extremes: A Conditional Approach', 'abstract': 'Simultaneous concurrence of extreme values across multiple climate variables can result in large societal and environmental impacts. Therefore, there is growing interest in understanding these concurrent extremes. One way to approach this problem is to study the distribution of one climate variable given that another is extreme. In this work we develop a statistical framework for estimating bivariate concurrent extremes via a conditional approach, where univariate extreme value modeling is combined with dependence modeling of the conditional tail distribution using techniques from quantile regression and extreme value analysis to quantify concurrent extremes. We focus on the distribution of daily wind speed conditioned on daily precipitation taking its seasonal maximum. The Canadian Regional Climate Model large ensemble is used to assess the performance of the proposed framework both via a simulation study with specified dependence structure and via an analysis of the climate model-simulated dependence structure.', 'corpus_id': 219708794, 'score': 1}, {'doc_id': '42330143', 'title': 'Nambu mechanics and its quantization.', 'abstract': 'The algebra of observables inherent in the Nambu formalism [Phys. Rev. D 7, 2405 (1973)] for a generalization of classical Hamiltonian dynamics is investigated. A consistency requirement of time evolution of the Nambu bracket leads to a five-point identity. Two types of algebras are possible at the classical level. Their composition properties under a tensor product are considered and the physical implications are analyzed. A quantum generalization of these algebras is shown to be impossible.', 'corpus_id': 42330143, 'score': 1}, {'doc_id': '220403290', 'title': 'Derivation of a Relativistic Boltzmann Distribution', 'abstract': ""A framework for relativistic thermodynamics and statistical physics is built by first exploiting the symmetries between energy and momentum in the derivation of the Boltzmann distribution, then using Einstein's energy-momentum relationship to derive a PDE for the partition function. It is shown that the extended Boltzmann distribution implies the existence of an inverse four-temperature, while the form of the partition function PDE implies the existence of a quantizable field theory of classical statistics, with hints of an associated gravity like gauge theory. An adaptation of the framework is then used to derive a thermodynamic certainty relationship."", 'corpus_id': 220403290, 'score': 0}, {'doc_id': '221534474', 'title': 'Quantifying uncertainty in spatio-temporal changes of upper-ocean heat content estimates: an internationally coordinated comparison', 'abstract': 'The Earth system is accumulating energy due to human-induced activities. More than 90 percent of this energy has been stored in the ocean as heat since 1970, with about 64 percent of that in the upper 700 m. Differences in upper ocean heat content anomaly (OHCA) estimates, however, exist. Here, we evaluate spread in upper OHCA estimates arising from choices in instrumental bias corrections and mapping methods, in addition to the effect of using a common ocean mask. The same dataset was mapped by six research groups for 1970 to 2008, with six instrumental bias corrections applied to expendable bathythermograph (XBT) data. We find that use of a common ocean mask may impact estimation of global OHCA by +- 13 percent. Uncertainty due to mapping method dominates over XBT bias correction at a global scale and is largest in the Indian Ocean and in the eddy-rich and frontal regions of all basins. Uncertainty due to XBT bias correction is largest in the Pacific Ocean within 30N to 30S. In both mapping and XBT cases, spread is higher since the 1990s. Important differences in spatial trends among mapping methods are found in the well-observed Northwest Atlantic and the poorly-observed Southern Ocean. Although our results cannot identify the best mapping or bias correction schemes, they identify where and when greater uncertainties exist, and so where further refinements may yield the largest improvements. Our results highlight the need for a future international coordination to evaluate performance of existing mapping methods.', 'corpus_id': 221534474, 'score': 0}, {'doc_id': '120282197', 'title': 'Sensitivity Analysis of Quality Assurance Using the Spatial Regression Approach—A Case Study of the Maximum/Minimum Air Temperature', 'abstract': 'Abstract Both the spatial regression test (SRT) and inverse distance weighting (IDW) methods have been applied to provide estimates for the maximum air temperature (Tmax) and the minimum air temperature (Tmin) in the Applied Climate Information System (ACIS). This is critical to the processes of estimating missing data and identifying suspect data and is undertaken here to ensure quality data in ACIS. The SRT method was previously found to be superior to the IDW method; however, the sensitivity of the performance of both methods to input parameters has not been evaluated. A set of analyses is presented for both methods whereby the sensitivity to the radius of inclusion, the regression time window, the regression time offset, and the number of stations used to make the estimates are examined. Comparisons were also conducted between the SRT and the IDW methods. The performance of the SRT method stabilized when 10 or more stations were applied in the estimates. The optimal number of stations for the IDW meth...', 'corpus_id': 120282197, 'score': 1}, {'doc_id': '220793790', 'title': 'A Hamiltonian Interacting Particle System for Compressible Flow', 'abstract': 'The decomposition of the energy of a compressible fluid parcel into slow (deterministic) and fast (stochastic) components is interpreted as a stochastic Hamiltonian interacting particle system (HIPS). It is shown that the McKean-Vlasov equation associated to the mean field limit yields the barotropic Navier-Stokes equation with density dependent viscosity. Capillary forces can also be treated by this approach. Due to the Hamiltonian structure the mean field system satisfies a Kelvin circulation theorem along stochastic Lagrangian paths.', 'corpus_id': 220793790, 'score': 0}, {'doc_id': '220041872', 'title': 'Introduction to the Special Issue on the Statistical Mechanics of Climate', 'abstract': 'We introduce the special issue on the Statistical Mechanics of Climate by presenting an informal discussion of some theoretical aspects of climate dynamics that make it a topic of great interest for mathematicians and theoretical physicists. In particular, we briefly discuss its nonequilibrium and multiscale properties, the relationship between natural climate variability and climate change, the different regimes of climate response to perturbations, and critical transitions.', 'corpus_id': 220041872, 'score': 1}, {'doc_id': '124951196', 'title': 'An Improved QC Process for Temperature in the Daily Cooperative Weather Observations', 'abstract': 'Abstract TempVal is a spatial component of data quality assurance algorithms applied by the National Climatic Data Center (NCDC), and it has been used operationally for about 4 yr. A spatial regression test (SRT) approach was developed at the regional climate centers for climate data quality assurance and was found to be superior to currently used quality control (QC) procedures for the daily maximum and minimum air temperature. The performance of the spatial quality assessment procedures has been evaluated by assessing the rate with which seeded errors are identified. A complete dataset with seeded errors for the year 2003 for the contiguous United States was examined for both the maximum and minimum air temperature. The spatial regression quality assessment component (SRT), originating in the Automated Climate Information System (ACIS), and TempVal, originating in the NCDC database, were applied separately and evaluated through the ratio of identified seeded errors to the total number of seeds. The spat...', 'corpus_id': 124951196, 'score': 1}]"
42	{'doc_id': '235074375', 'title': 'Prehospital tourniquet use: An evaluation of community application and outcome', 'abstract': 'BACKGROUND There is substantial investment in layperson and first responder training involving tourniquet use for hemorrhage control. Little is known however about prehospital tourniquet application, field conversion, or outcomes in the civilian setting. We describe the experience of a metropolitan region with prehospital tourniquet application. METHODS We conducted a retrospective cohort study characterizing prehospital tourniquet use treated by emergency medical services (EMS) in King County, Washington, from January 2018 to June 2019. Emergency medical services and hospital records were abstracted for demographics, injury mechanism, tourniquet details, clinical care, and outcomes. We evaluated the incidence of tourniquet application, who applied the device (EMS, law enforcement, or layperson), and subsequent course. RESULTS A total of 168 patients received tourniquet application, an incidence of 5.1 per 100,000 person-years and 3.48 per 1,000 EMS responses for trauma. Tourniquets were applied for penetrating trauma (64%), blunt trauma (30%), and bleeding ateriovenous fistulas (7%). A subset was critically ill: 13% had systolic blood pressures of <90 mm Hg, 8% had Glasgow Coma Scale score of <13, and 3% had cardiac arrest. Among initial applications, 48% were placed by law enforcement, 33% by laypersons, and 18% by EMS. Among tourniquets applied by layperson or law enforcement (n = 137), EMS relied solely on the original tourniquet in 45% (n = 61), placed a second tourniquet in 20% (n = 28), and removed the tourniquet without replacement in 35% (n = 48). Overall, 24% required massive transfusion, 59% underwent urgent surgery, and 21% required vascular surgery. Mortality was 3% (n = 4). At hospital discharge, the tourniquet limb was fully functional in 81%, partially functional in 10%, and nonfunctional in 9%; decreased function was not attributed to tourniquet application. CONCLUSION The high rate of application, need for urgent hospital intervention in a subset, and low incidence of apparent complication suggest that efforts to increase access and early tourniquet use can provide public health benefit. LEVEL OF EVIDENCE Therapeutic, level IV.', 'corpus_id': 235074375}	16649	[{'doc_id': '235128162', 'title': 'Intelligent prediction of RBC demand in trauma patients using decision tree methods', 'abstract': 'Background The vital signs of trauma patients are complex and changeable, and the prediction of blood transfusion demand mainly depends on doctors’ experience and trauma scoring system; therefore, it cannot be accurately predicted. In this study, a machine learning decision tree algorithm [classification and regression tree (CRT) and eXtreme gradient boosting (XGBoost)] was proposed for the demand prediction of traumatic blood transfusion to provide technical support for doctors. Methods A total of 1371 trauma patients who were diverted to the Emergency Department of the First Medical Center of Chinese PLA General Hospital from January 2014 to January 2018 were collected from an emergency trauma database. The vital signs, laboratory examination parameters and blood transfusion volume were used as variables, and the non-invasive parameters and all (non-invasive + invasive) parameters were used to construct an intelligent prediction model for red blood cell (RBC) demand by logistic regression (LR), CRT and XGBoost. The prediction accuracy of the model was compared with the area under the curve (AUC). Results For non-invasive parameters, the LR method was the best, with an AUC of 0.72 [95% confidence interval (CI) 0.657–0.775], which was higher than the CRT (AUC 0.69, 95% CI 0.633–0.751) and the XGBoost (AUC 0.71, 95% CI 0.654–0.756, P \u2009<\u20090.05). The trauma location and shock index are important prediction parameters. For all the prediction parameters, XGBoost was the best, with an AUC of 0.94 (95% CI 0.893–0.981), which was higher than the LR (AUC 0.80, 95% CI 0.744–0.850) and the CRT (AUC 0.82, 95% CI 0.779–0.853, P \u2009<\u20090.05). Haematocrit (Hct) is an important prediction parameter. Conclusions The classification performance of the intelligent prediction model of red blood cell transfusion in trauma patients constructed by the decision tree algorithm is not inferior to that of the traditional LR method. It can be used as a technical support to assist doctors to make rapid and accurate blood transfusion decisions in emergency rescue environment, so as to improve the success rate of patient treatment.', 'corpus_id': 235128162, 'score': 1}, {'doc_id': '235463367', 'title': 'Whole blood for trauma patients: Outcomes at higher doses', 'abstract': 'Death due to hemorrhage remains the leading cause of mortality worldwide in patients younger than 40 years of age. Despite recent advances in massive resuscitation and hemorrhage control techniques, the morbidity and years of life lost in this population remain substantial. Whole blood (WB) transfusion is not a new concept, having been used extensively during military conflicts more than a century ago. As with many topics in medicine, as our understanding and practice management mature, certain historical concepts return to the forefront of current clinical practice. The partitioning of whole blood into component parts during the middle of the 20th century became common blood banking practice and was accompanied by an era of crystalloid-heavy resuscitation that dominated trauma transfusion practices in both military and civilian settings. In the late 1990s, the concepts of permissive hypotension and limited pre-hospital crystalloid resuscitation evolved into the practices of Damage Control Resuscitation (DCR) with ratio-based administration of packed red blood cells (PRBCs), fresh frozen plasma (FFP), and platelets (PLTs), thanks in large part to experience from military conflicts in the Middle East. The development of transfusion strategies that utilized higher ratios of FFP and early PLT administration to achieve as close as possible a 1:1:1 transfusion of PRBC, FFP, and PLT reinvigorated the interest in whole blood transfusion as a viable and efficacious method of resuscitation for hemorrhagic shock. The use of low titer group O whole blood (LTOWB) has increased in recent years in hopes its use could help better address the high rate of death from hemorrhage in trauma patients. Despite the successful implementation of massive transfusion protocols that aim for a 1:1:1 transfusion ratio, the reality of this approach is that the sum of the component parts does not equal the composition of whole blood. This is one of the many arguments in favor of whole blood transfusion in hemorrhaging trauma patients. In addition, each unit of PRBC, FFP, and PLT contains additive solutions that do not contribute to oxygen carrying capacity or coagulation function, and further dilutes the composition of transfused products. LTOWB has a reduced volume of anticoagulants and additive solutions when compared to an equivalent dose of traditional component therapy. Studies have shown that LTOWB has higher hemoglobin, platelet and coagulation factor concentrations and is less acidotic when compared to component therapy. Additionally, the use of LTOWB has been approved by the AABB and shown to be both feasible and safe in a number of studies. Given the favorable product characteristics of LTOWB, investigators have hypothesized that its use could improve survival and other outcomes in patients suffering massive bleeding in the setting of trauma. There have been multiple studies comparing LTOWB to component therapy that have sought to demonstrate the potential benefits of LTOWB with varying results. These studies have used different methodologies and varying doses of WB. A 2020 review of WB studies concluded that there is substantial evidence that WB is safe and feasible, but it has not been shown to be superior to component therapy in any large or randomized studies. More recently, published studies have shown conflicting results. Two single-center observational studies demonstrated improved mortality with patients receiving median doses of 1 and 3.3 units of WB respectively. In a third study, an analysis of a large national database found that trauma patients receiving a combination of WB plus component therapy had significantly reduced 24-h and in-hospital mortality compared with patients receiving component therapy alone. In contrast, three other observational, singlecenter studies with median WB doses of 2, 6.5, and 2 units did not show a decrease in mortality. Finally, a meta-analysis of studies that measured 24-h and 30-day in-hospital survival failed to demonstrate a survival advantage to patients treated with WB compared with CT. In this issue of Transfusion, Yazer et al. present a retrospective analysis of clinical outcomes in trauma patients receiving at least 3 units of LTOWB. Based on the varying doses used in the previously published studies and previous institutional work, the authors restricted inclusion to patients that had received a minimum of 3 units of LTOWB to focus on patients that received a relatively large dose of WB. The authors hypothesized that clinical outcomes, including mortality, thromboembolic events, and multiorgan failure, would be similar for recipients of WB and conventional component therapy. After performing two matched analyses, the authors concluded that transfusing a median of 4 units of LTOWB Received: 7 May 2021 Accepted: 7 May 2021', 'corpus_id': 235463367, 'score': 1}, {'doc_id': '235915170', 'title': 'Prehospital whole blood reduces early mortality in patients with hemorrhagic shock', 'abstract': 'Low titer O+ whole blood (LTOWB) is being increasingly used for resuscitation of hemorrhagic shock in military and civilian settings. The objective of this study was to identify the impact of prehospital LTOWB on survival for patients in shock receiving prehospital LTOWB transfusion.', 'corpus_id': 235915170, 'score': 1}, {'doc_id': '233234275', 'title': 'Pediatric Thoracolumbar Spinal Injuries in United States Trauma Centers.', 'abstract': 'OBJECTIVES\nInjuries are the leading cause of morbidity and mortality in children ages 1 to 18 years. There are limited studies about pediatric thoracolumbar (TL) spinal injuries; the purpose of this study was to characterize TL spinal injuries among pediatric patients evaluated in US trauma centers.\n\n\nMETHODS\nThis was a retrospective cohort study of the National Trauma Data Bank. Patients aged 1 to 18 years with a thoracic or lumbar spinal injury sustained by blunt trauma during calendar years 2011 through 2016 were included. Cervical spinal injuries, death before arrival, or penetrating trauma were excluded. The data was abstracted, and missing data was addressed by imputations. Data was analyzed using descriptive statistics and multinomial logistic regressions.\n\n\nRESULTS\nA total of 20,062 patients were included in the study. Thoracolumbar spinal injuries were more commonly sustained by 16- to 17-year-olds (45.7%), boys (56.6%), and White (74.8%). The injuries were often from a motor vehicle collision (MVC) (55.2%) and resulted in a bone injury (82.3%). Mechanism of injury and age were significant in predicting injury type. A fall was more likely than MVC to result in disc injury (odds ratio [OR], 1.70; 95% confidence interval [CI], 1.24-2.33), strain injury (OR, 1.18; 95% CI, 1.05-1.34), or cord injury (OR, 1.27; 95% CI, 1.12-1.45). Younger children were more likely than adolescents to present with disc injury (OR, 2.79; 95% CI, 1.75-4.45), cord injury (OR, 1.46; 95% CI, 1.18-1.81), or strain injury (OR, 1.37; 95% CI, 1.09-1.72).\n\n\nCONCLUSIONS\nTo our knowledge, this is the largest pediatric TL spinal study. Clinicians should consider TL spinal injuries when adolescents present after an MVC, and specifically, TL spinal cord injuries when young children present after a fall. Additionally, pediatric TL spinal injury prevention should highlight motor vehicle and fall safety.', 'corpus_id': 233234275, 'score': 0}, {'doc_id': '235750420', 'title': 'Prehospital time and mortality in polytrauma patients: a retrospective analysis', 'abstract': 'Background The time from injury to treatment is considered as one of the major determinants for patient outcome after trauma. Previous studies already attempted to investigate the correlation between prehospital time and trauma patient outcome. However, the outcome for severely injured patients is not clear yet, as little data is available from prehospital systems with both Emergency Medical Services (EMS) and physician staffed Helicopter Emergency Medical Services (HEMS). Therefore, the aim was to investigate the association between prehospital time and mortality in polytrauma patients in a Dutch level I trauma center. Methods A retrospective study was performed using data derived from the Dutch trauma registry of the National Network for Acute Care from Amsterdam UMC location VUmc over a 2-year period. Severely injured polytrauma patients (Injury Severity Score (ISS)\u2009≥\u200916), who were treated on-scene by EMS or both EMS and HEMS and transported to our level I trauma center, were included. Patient characteristics, prehospital time, comorbidity, mechanism of injury, type of injury, HEMS assistance, prehospital Glasgow Coma Score and ISS were analyzed using logistic regression analysis. The outcome measure was in-hospital mortality. Results In total, 342 polytrauma patients were included in the analysis. The total mortality rate was 25.7% (n\u2009=\u200988). Similar mean prehospital times were found between the surviving and non-surviving patient groups, 45.3\xa0min (SD 14.4) and 44.9\xa0min (SD 13.2) respectively ( p \u2009=\u20090.819). The confounder-adjusted analysis revealed no significant association between prehospital time and mortality ( p \u2009=\u20090.156). Conclusion This analysis found no association between prehospital time and mortality in polytrauma patients. Future research is recommended to explore factors of influence on prehospital time and mortality.', 'corpus_id': 235750420, 'score': 1}, {'doc_id': '235300117', 'title': 'Prehospital administration of blood and plasma products', 'abstract': 'Purpose of review Posttraumatic bleeding following major trauma is life threatening for the patient and remains a major global health issue. Bleeding after major trauma is worsened by trauma-induced coagulopathy (TIC). TIC consists of acute trauma coagulopathy and resuscitation coagulopathy. The early diagnosis and management of prehospital TIC management are challenging. Recent findings Concepts for early diagnosis and management of civilian prehospital TIC management are evolving. The feasibility of prehospital blood component as well as coagulation factor transfusion has been proven. Summary Due to different national guidelines and regulations of blood component therapies there is a wide heterogeneity in concepts of prehospital damage control resuscitation. Tranexamic acid administration is widely accepted, whereas the transfusion of whole blood, blood components, or coagulations factors needs further examination in the civilian setting.', 'corpus_id': 235300117, 'score': 1}, {'doc_id': '233025768', 'title': 'Incidence, Prevalence, and Outcomes of Pediatric Trauma in Rural Appalachia (West Virginia) From 2017 to 2019', 'abstract': 'Background Appalachian rural pediatric trauma has its unique incidence, presentation, and distribution due to the mechanisms of injury, geographic location, access to care, and social issues. Purpose To review, analyze, and understand pediatric trauma in West Virginia during the period 2017-2019. Materials and methods After institutional review board approval, the statewide trauma database was queried and analyzed in a retrospective cohort study for all pediatric trauma ages zero to 18 from 2017-2019 in the Appalachian regions one through four in West Virginia. The following were analyzed: gender, injury mechanism, Glasgow Coma Scale Score (GCS) at admission, injury severity score (ISS), toxicology screen results, hospital length of stay, duration of ventilatory support, number of procedures performed during admission, presence of non-accidental trauma, cardiac arrest, patient discharge disposition, and mortality. Results One-thousand eighty-two (1182) patients between the ages of zero to 18 were admitted to the trauma center. An average of 37% was female and 63% male. In the 11-18 age group, 24% were female and 76% were male. Most injuries were due to blunt force (89%), followed by penetrating injuries (7.2%) and burns (1.4%). The majority had minor or moderate injuries with 95% receiving a Glasgow Coma Scale (GCS) >13 and 72% listed as minor on the injury severity score (ISS). Children in ages 0-2 years had the highest proportion of poor (0-8) GCS scores, high ISS (>14) scores, most hospital admission days, most days on a ventilator, highest mortality, most pre-hospital cardiac arrests, child abuse, burns, and placement with child protective services. An average of 31% of children tested, and 17% in the age group of 0-2 had a positive toxicology screen. There were 3670 procedures done in total and the most common procedure performed was an ultrasound of the abdomen. Procedures were performed in 90% of the patients. Conclusions and relevance Based on our study, the zero to two-year-old pediatric trauma patients are most vulnerable to poor outcomes and may need targeted preventative interventions. Toxicology screens may need to be more widely implemented in pediatric trauma in the Appalachian region. Rural trauma in Appalachia has endemic issues related to substance abuse, poverty, and a lower degree of social support as compared to urban areas. Although the distribution of injury may follow a national distribution, mechanism, management, and outcomes can vary.', 'corpus_id': 233025768, 'score': 0}, {'doc_id': '233191163', 'title': 'Morbidity and Mortality in the Obese Trauma Intensive Care Unit Patient.', 'abstract': 'INTRODUCTION\nObesity is an epidemic in the United States, known to be associated with comorbidities. However, some data show that obesity may be a protective factor in some instances. The purpose of this study is to determine if there are differences in morbidity and mortality when comparing the obese and non-obese critically ill trauma patient populations.\n\n\nMATERIALS AND METHODS\nThis was a retrospective study conducted at Prisma Health Upstate in Greenville, South Carolina, an Adult Level 1 Trauma Center. Patients over the age of 18\xa0years admitted due to trauma from February 6, 2016 to February 28, 2019 were included in this study. Burn patients were excluded. An online trauma database was used to obtain age, sex, body mass index, Glasgow coma score (GCS), injury severity score (ISS), revised trauma score (RTS), days on mechanical ventilation, hospital length of stay (LOS), and intensive care unit (ICU) LOS.\n\n\nRESULTS\nThere were 2365 critically ill trauma patients who met inclusion criteria for this study. 1570 patients were men (66.38%) and mean age was 53.2 ± 20.9. Of the patients, 2166 patients had blunt trauma (91.59%). Median GCS was 15 (interquartilerange [IQR]: 12, 15), median RTS was 12 (IQR: 11, 12), and median ISS was 17 (IQR: 9, 22). Obese critically ill trauma patients had significantly lower odds of mortality than nonobese (OR .686, CI 0.473-.977). Penetrating traumas (OR: 4.206, CI: 2.478, 6.990), increased ISS (OR: 1.095, CI: .473, 1.112), and increased age (OR: 1.036, CI: 1.038, 1.045) were associated with significantly increased odds of mortality.\n\n\nDISCUSSION\nThe obesity paradox is observed in the obese critically ill trauma patient population.', 'corpus_id': 233191163, 'score': 0}, {'doc_id': '234498918', 'title': 'The impact of delayed time to first CT head on functional outcomes after blunt head trauma with moderately depressed GCS', 'abstract': 'Recent work suggests patients with moderately depressed Glasgow Coma Scale (GCS) score in the Emergency Department (ED) who do not undergo immediate head CT (CTH) have delayed neurosurgical intervention and longer ED stay. The present study objective was to determine the impact of time to first CTH on functional neurologic outcomes in this patient population. Blunt trauma patients presenting to our Level I trauma center (11/2015–10/2019) with first ED GCS 9–12 were retrospectively identified and included. Transfers and those with extracranial AIS\u2009≥\u20093 were excluded. The study population was stratified into Immediate (≤\u20091 h) and Delayed (1–6 h) CTH groups based on time from ED arrival to first CTH. Outcomes included functional outcomes at hospital discharge based on the Modified Rankin Scale (mRS). After exclusions, 564 patients were included: 414 (73%) with Immediate CTH and 150 (27%) Delayed CTH. Both groups arrived with median GCS 11 and alcohol/drug intoxication did not differ (p\u2009>\u20090.05). AIS Head/Neck was comparable (3[3–4] vs. 3[3–3], p\u2009=\u20090.349). Time to ED disposition decision and ED exit were significantly shorter after Immediate CTH (2.8[1.5–5.3] vs. 5.2[3.6–7.5]h, p\u2009<\u20090.001 and 5.5[3.3–8.9] vs. 8.1[5.2–11.7]h, p\u2009<\u20090.001). Functional outcomes were slightly worse after Immediate CTH (mRS 2[1–4] vs. 2[1–3], p\u2009=\u20090.002). Subgroup analysis of patients requiring neurosurgical intervention demonstrated a greater proportion of moderately disabled patients with a lower proportion of severely disabled or dead patients after Immediate CTH as compared to Delayed CTH (51 vs. 20%, p\u2009=\u20090.063 and 35 vs. 60%, p\u2009=\u20090.122). Immediate CTH shortened time to disposition decision out of the ED and ED exit. Patients requiring neurosurgical intervention after Immediate CTH had improved functional outcomes when compared to those undergoing Delayed CTH. These differences did not reach statistical significance in this single-center study and, therefore, a large, multicenter study is the next step in demonstrating the potential functional outcomes benefit of Immediate CTH after blunt head trauma.', 'corpus_id': 234498918, 'score': 0}, {'doc_id': '233351601', 'title': 'The association of hand and wrist injuries with other injuries in multiple trauma patients. A retrospective study in a UK Major Trauma Centre.', 'abstract': 'BACKGROUND\nApproximately 20,000 major trauma cases occur in England every year. However, the association with concomitant upper limb injuries is unknown. This study aims to determine the incidence, injury pattern and association of hand and wrist injuries with other body injuries and the Injury Severity Score (ISS) in multiply injured trauma patients.\n\n\nMETHODS\nSingle centre retrospective study was performed at a level-one UK Major Trauma Centre (MTC). Trauma Audit and Research Network (TARN) eligible multiply injured trauma patients that were admitted to the hospital between January 2014 and December 2018 were analysed. TARN is the national trauma registry. Eligible patients were: a trauma patient of any age who was admitted for 72\xa0h or more, or was admitted to intensive care, or died at the hospital, was transferred into the hospital for specialist care, was transferred to another hospital for specialist care or for an intensive care bed and whose isolated injuries met a set of criteria. Data extracted included: age, gender, mode of arrival, location of injuries including: hand and/or wrist and mechanism of injury. We performed a logistic regression analysis to assess the association between hand/wrist injury to ISS score of 15 points or above/below and to the presentation of other injuries.\n\n\nRESULTS\n107 patients were analysed. Hand and wrist injuries were the second most common injury (26.2%), after thoracic injuries. Distal radial injuries were found in 5.6%, carpal/carpometacarpal in 6.5%, concurrent distal radius and carpometacarpal in 0.9%, phalangeal injuries in 4.7%, tendon injuries in 0.9% and concurrent hand and wrist injuries in 7.5% cases. There was a significant association between hand or wrist injuries and lower limb injuries (Odds Ratio (OR): 3.84; 95% confidence intervals (CI): 1.09 to 13.50; p\xa0=\xa00.04) and pelvic injuries (OR: 4.78; 95% CI: 1.31 to 17.44; p\xa0=\xa00.02). There was no statistical association between hand and wrist injuries and ISS score (OR: 0.80; 95% CI: 0.11 to 5.79; p\xa0=\xa00.82).\n\n\nCONCLUSIONS\nHand and wrist injuries are prevalent in trauma patients admitted to MTCs. They should not be under-estimated but routinely screened for in multiply injured patients particularly those with a pelvic or lower limb injury.', 'corpus_id': 233351601, 'score': 0}]
43	"{'doc_id': '209386709', 'title': 'Towards fairer datasets: filtering and balancing the distribution of the people subtree in the ImageNet hierarchy', 'abstract': ""Computer vision technology is being used by many but remains representative of only a few. People have reported misbehavior of computer vision models, including offensive prediction results and lower performance for underrepresented groups. Current computer vision models are typically developed using datasets consisting of manually annotated images or videos; the data and label distributions in these datasets are critical to the models' behavior. In this paper, we examine ImageNet, a large-scale ontology of images that has spurred the development of many modern computer vision methods. We consider three key factors within the person subtree of ImageNet that may lead to problematic behavior in downstream computer vision technology: (1) the stagnant concept vocabulary of WordNet, (2) the attempt at exhaustive illustration of all categories with images, and (3) the inequality of representation in the images within concepts. We seek to illuminate the root causes of these concerns and take the first steps to mitigate them constructively."", 'corpus_id': 209386709}"	4	"[{'doc_id': '218673831', 'title': 'Exploring Software Reusability Metrics with Q&A Forum Data', 'abstract': 'Abstract Question and answer (QA it can be used to explore the relationship between software reusability metrics and difficulties encountered by users, as well as predict the number of difficulties users will face in the future. Q&A forum data can help improve understanding of software reuse, and may be harnessed as an additional resource to evaluate software reusability metrics.', 'corpus_id': 218673831, 'score': 0}, {'doc_id': '220056279', 'title': 'XREF: Entity Linking for Chinese News Comments with Supplementary Article Reference', 'abstract': ""Automatic identification of mentioned entities in social media posts facilitates quick digestion of trending topics and popular opinions. Nonetheless, this remains a challenging task due to limited context and diverse name variations. In this paper, we study the problem of entity linking for Chinese news comments given mentions' spans. We hypothesize that comments often refer to entities in the corresponding news article, as well as topics involving the entities. We therefore propose a novel model, XREF, that leverages attention mechanisms to (1) pinpoint relevant context within comments, and (2) detect supporting entities from the news article. To improve training, we make two contributions: (a) we propose a supervised attention loss in addition to the standard cross entropy, and (b) we develop a weakly supervised training scheme to utilize the large-scale unlabeled corpus. Two new datasets in entertainment and product domains are collected and annotated for experiments. Our proposed method outperforms previous methods on both datasets."", 'corpus_id': 220056279, 'score': 0}, {'doc_id': '216641618', 'title': 'Zero-shot topic generation', 'abstract': 'We present an approach to generating topics using a model trained only for document title generation, with zero examples of topics given during training. We leverage features that capture the relevance of a candidate span in a document for the generation of a title for that document. The output is a weighted collection of the phrases that are most relevant for describing the document and distinguishing it within a corpus, without requiring access to the rest of the corpus. We conducted a double-blind trial in which human annotators scored the quality of our machine-generated topics along with original human-written topics associated with news articles from The Guardian and The Huffington Post. The results show that our zero-shot model generates topic labels for news documents that are on average equal to or higher quality than those written by humans, as judged by humans.', 'corpus_id': 216641618, 'score': 1}, {'doc_id': '216562444', 'title': 'Joint Keyphrase Chunking and Salience Ranking with BERT', 'abstract': ""An effective keyphrase extraction system requires to produce self-contained high quality phrases that are also key to the document topic. This paper presents BERT-JointKPE, a multi-task BERT-based model for keyphrase extraction. JointKPE employs a chunking network to identify high-quality phrases and a ranking network to learn their salience in the document. The model is trained jointly on the chunking task and the ranking task, balancing the estimation of keyphrase quality and salience. Experiments on two benchmarks demonstrate JointKPE's robust effectiveness with different BERT variants. Our analyses show that JointKPE has advantages in predicting long keyphrases and extracting phrases that are not entities but also meaningful. The source code of this paper can be obtained from this https URL"", 'corpus_id': 216562444, 'score': 1}, {'doc_id': '208139323', 'title': 'Error Analysis for Vietnamese Named Entity Recognition on Deep Neural Network Models', 'abstract': 'In recent years, Vietnamese Named Entity Recognition (NER) systems have had a great breakthrough when using Deep Neural Network methods. This paper describes the primary errors of the state-of-the-art NER systems on Vietnamese language. After conducting experiments on BLSTM-CNN-CRF and BLSTM-CRF models with different word embeddings on the Vietnamese NER dataset. This dataset is provided by VLSP in 2016 and used to evaluate most of the current Vietnamese NER systems. We noticed that BLSTM-CNN-CRF gives better results, therefore, we analyze the errors on this model in detail. Our error-analysis results provide us thorough insights in order to increase the performance of NER for the Vietnamese language and improve the quality of the corpus in the future works.', 'corpus_id': 208139323, 'score': 0}, {'doc_id': '215754478', 'title': 'Author Name Disambiguation in Bibliographic Databases: A Survey', 'abstract': 'Entity resolution is a challenging and hot research area in the field of Information Systems since last decade. Author Name Disambiguation (AND) in Bibliographic Databases (BD) like DBLP , Citeseer , and Scopus is a specialized field of entity resolution. Given many citations of underlying authors, the AND task is to find which citations belong to the same author. In this survey, we start with three basic AND problems, followed by need for solution and challenges. A generic, five-step framework is provided for handling AND issues. These steps are; (1) Preparation of dataset (2) Selection of publication attributes (3) Selection of similarity metrics (4) Selection of models and (5) Clustering Performance evaluation. Categorization and elaboration of similarity metrics and methods are also provided. Finally, future directions and recommendations are given for this dynamic area of research.', 'corpus_id': 215754478, 'score': 1}, {'doc_id': '220961667', 'title': 'Learning Visual Representations with Caption Annotations', 'abstract': 'Pretraining general-purpose visual features has become a crucial part of tackling many computer vision tasks. While one can learn such features on the extensively-annotated ImageNet dataset, recent approaches have looked at ways to allow for noisy, fewer, or even no annotations to perform such pretraining. Starting from the observation that captioned images are easily crawlable, we argue that this overlooked source of information can be exploited to supervise the training of visual representations. To do so, motivated by the recent progresses in language models, we introduce {\\em image-conditioned masked language modeling} (ICMLM) -- a proxy task to learn visual representations over image-caption pairs. ICMLM consists in predicting masked words in captions by relying on visual cues. To tackle this task, we propose hybrid models, with dedicated visual and textual encoders, and we show that the visual representations learned as a by-product of solving this task transfer well to a variety of target tasks. Our experiments confirm that image captions can be leveraged to inject global and localized semantic information into visual representations. Project website: this https URL.', 'corpus_id': 220961667, 'score': 0}, {'doc_id': '219179714', 'title': 'Generate FAIR Literature Surveys with Scholarly Knowledge Graphs', 'abstract': 'Reviewing scientific literature is a cumbersome, time consuming but crucial activity in research. Leveraging a scholarly knowledge graph, we present a methodology and a system for comparing scholarly literature, in particular research contributions describing the addressed problem, utilized materials, employed methods and yielded results. The system can be used by researchers to quickly get familiar with existing work in a specific research domain (e.g., a concrete research question or hypothesis). Additionally, it can be used to publish literature surveys following the FAIR Data Principles. The methodology to create a research contribution comparison consists of multiple tasks, specifically: (a) finding similar contributions, (b) aligning contribution descriptions, (c) visualizing and finally (d) publishing the comparison. The methodology is implemented within the Open Research Knowledge Graph (ORKG), a scholarly infrastructure that enables researchers to collaboratively describe, find and compare research contributions. We evaluate the implementation using data extracted from published review articles. The evaluation also addresses the FAIRness of comparisons published with the ORKG.', 'corpus_id': 219179714, 'score': 1}, {'doc_id': '216641773', 'title': 'AxCell: Automatic Extraction of Results from Machine Learning Papers', 'abstract': 'Tracking progress in machine learning has become increasingly difficult with the recent explosion in the number of papers. In this paper, we present AxCell, an automatic machine learning pipeline for extracting results from papers. AxCell uses several novel components, including a table segmentation subtask, to learn relevant structural knowledge that aids extraction. When compared with existing methods, our approach significantly improves the state of the art for results extraction. We also release a structured, annotated dataset for training models for results extraction, and a dataset for evaluating the performance of models on this task. Lastly, we show the viability of our approach enables it to be used for semi-automated results extraction in production, suggesting our improvements make this task practically viable for the first time. Code is available on GitHub.', 'corpus_id': 216641773, 'score': 1}, {'doc_id': '220045918', 'title': 'Coupling Distant Annotation and Adversarial Training for Cross-Domain Chinese Word Segmentation', 'abstract': 'Fully supervised neural approaches have achieved significant progress in the task of Chinese word segmentation (CWS). Nevertheless, the performance of supervised models always drops gravely if the domain shifts due to the distribution gap across domains and the out of vocabulary (OOV) problem. In order to simultaneously alleviate the issues, this paper intuitively couples distant annotation and adversarial training for cross-domain CWS. 1) We rethink the essence of “Chinese words” and design an automatic distant annotation mechanism, which does not need any supervision or pre-defined dictionaries on the target domain. The method could effectively explore domain-specific words and distantly annotate the raw texts for the target domain. 2) We further develop a sentence-level adversarial training procedure to perform noise reduction and maximum utilization of the source domain information. Experiments on multiple real-world datasets across various domains show the superiority and robustness of our model, significantly outperforming previous state-of-the-arts cross-domain CWS methods.', 'corpus_id': 220045918, 'score': 0}]"
44	{'doc_id': '2352667', 'title': 'Predicting species distribution: offering more than simple habitat models.', 'abstract': 'In the last two decades, interest in species distribution models (SDMs) of plants and animals has grown dramatically. Recent advances in SDMs allow us to potentially forecast anthropogenic effects on patterns of biodiversity at different spatial scales. However, some limitations still preclude the use of SDMs in many theoretical and practical applications. Here, we provide an overview of recent advances in this field, discuss the ecological principles and assumptions underpinning SDMs, and highlight critical limitations and decisions inherent in the construction and evaluation of SDMs. Particular emphasis is given to the use of SDMs for the assessment of climate change impacts and conservation management issues. We suggest new avenues for incorporating species migration, population dynamics, biotic interactions and community ecology into SDMs at multiple spatial scales. Addressing all these issues requires a better integration of SDMs with ecological theory.', 'corpus_id': 2352667}	19732	"[{'doc_id': '11283373', 'title': 'More than the sum of the parts: forest climate response from joint species distribution models.', 'abstract': 'The perceived threat of climate change is often evaluated from species distribution models that are fitted to many species independently and then added together. This approach ignores the fact that species are jointly distributed and limit one another. Species respond to the same underlying climatic variables, and the abundance of any one species can be constrained by competition; a large increase in one is inevitably linked to declines of others. Omitting this basic relationship explains why responses modeled independently do not agree with the species richness or basal areas of actual forests. We introduce a joint species distribution modeling approach (JSDM), which is unique in three ways, and apply it to forests of eastern North America. First, it accommodates the joint distribution of species. Second, this joint distribution includes both abundance and presence-absence data. We solve the common issue of large numbers of zeros in abundance data by accommodating zeros in both stem counts and basal area data, i.e., a new approach to zero inflation. Finally, inverse prediction can be applied to the joint distribution of predictions to integrate the role of climate risks across all species and identify geographic areas where communities will change most (in terms of changes in abundance) with climate change. Application to forests in the eastern United States shows that climate can have greatest impact in the Northeast, due to temperature, and in the Upper Midwest, due to temperature and precipitation. Thus, these are the regions experiencing the fastest warming and are also identified as most responsive at this scale.', 'corpus_id': 11283373, 'score': 1}, {'doc_id': '236174861', 'title': 'Addressing risks to biodiversity arising from a changing climate: The need for ecosystem restoration in the Tana River Basin, Kenya', 'abstract': 'Climate change is projected to have significant effects on the distribution of species globally, but research into the implications in parts of Africa has been limited. Using species distribution modelling, this study models climate change-related risks to the terrestrial biodiversity (birds, mammals, reptiles, amphibians and plants) of Kenya’s economically-important and ecologically diverse Tana River Basin. Large reductions in species richness are projected with just 2°C warming (relative to preindustrial levels) with birds and plants seeing the greatest impact. Potential climate refugia for biodiversity are identified within the basin, but often overlap with areas already converted to agriculture or set aside for agricultural expansion, and the majority are outside protected areas. Similarly, some protected areas contain no projected refugia at higher levels of global warming, showing they may be insufficient to protect the basin’s biodiversity as climate changes. However, risks to biodiversity are much smaller if the Paris Agreement’s goal of limiting global warming to ‘well below 2°C’ warming, rather than 2°C only, is met. The potential for refugia for plants and animals decreases strongly with warming. For example, 82% of the basin remaining climatically suitable for at least 75% of the plants currently present at 1.5°C warming, as compared with 23% at 2°C and 3% at 4.5°C. This research provides the first assessment of the combined effects of development plans and climate change on biodiversity of the Tana River Basin, including identifying potential areas for restoration, and contributes to a greater understanding of biodiversity protection and adaptation options in Kenya.', 'corpus_id': 236174861, 'score': 0}, {'doc_id': '236436900', 'title': 'Long-term changes in populations of rainforest birds in the Australia Wet Tropics bioregion: a climate-driven biodiversity emergency', 'abstract': 'Many authors have suggested that the vulnerability of montane biodiversity to climate change worldwide is significantly higher than in most other ecosystems. Despite the extensive variety of studies predicting severe impacts of climate change globally, few studies have empirically validated the predicted changes in distribution and population density. Here, we used 17 years of bird monitoring across latitudinal/elevational gradients in the rainforest of the Australian Wet Tropics World Heritage Area to assess changes in local abundance and distribution. We used relative abundance in 1977 surveys across 114 sites ranging from 0-1500m above sea level and utilised a trend analysis approach (TRIM) to investigate elevational shifts in abundance of 42 species between 2000 – 2016. The local abundance of most mid and high elevation species has declined at the lower edges of their distribution by >40% while lowland species increased by up to 190% into higher elevation areas. Upland-specialised species and regional endemics have undergone dramatic population declines of almost 50%. The “Outstanding Universal Value” of the Australian Wet Tropics World Heritage Area, one of the most irreplaceable biodiversity hotspots on Earth, is rapidly degrading. These observed impacts are likely to be similar in many tropical montane ecosystems globally.', 'corpus_id': 236436900, 'score': 0}, {'doc_id': '54627583', 'title': 'Prediction of the distribution of Arctic-nesting pink-footed geese under a warmer climate scenario', 'abstract': 'Global climate change is expected to shift species ranges polewards, with a risk of range contractions and population declines of especially high-Arctic species. We built species distribution models for Svalbard-nesting pink-footed geese to relate their occurrence to environmental and climatic variables, and used the models to predict their distribution under a warmer climate scenario. The most parsimonious model included mean May temperature, the number of frost-free months and the proportion of moist and wet mossdominated vegetation in the area. The two climate variables are indicators for whether geese can physiologically fulfil the breeding cycle or not and the moss vegetation is an indicator of suitable feeding conditions. Projections of the distribution to warmer climate scenarios propose a large north- and eastward expansion of the potential breeding range on Svalbard even at modest temperature increases (1 and 21C increase in summer temperature, respectively). Contrary to recent suggestions regarding future distributions of Arctic wildlife, we predict that warming may lead to a further growth in population size of, at least some, Arctic breeding geese.', 'corpus_id': 54627583, 'score': 1}, {'doc_id': '236999951', 'title': 'Temporal Assessment of Eastern Spotted Skunk Geographic Distribution', 'abstract': 'Abstract Spilogale putorius (Eastern Spotted Skunk) experienced range-wide population declines beginning in the mid-1900s with no clear understanding of the causal mechanism or whether such declines were associated with range contractions. Species-distribution models can provide a powerful framework to assess changes in landscape suitability in response to changing environmental conditions. Herein, we modeled time-stepped distributions of suitable environmental conditions for Eastern Spotted Skunks from 1938 to 2016 in Maxent, incorporating climate and land-cover predictors. Climate and land-cover variables reliably predicted landscape suitability of Eastern Spotted Skunks over time. We found a 37% decline in suitable area from historic predictions, consistent with reports of population declines in these areas. Our predicted landscape-suitability maps can be used to evaluate the current distribution of environmentally suitable conditions for the species as well as guide research and conservation efforts.', 'corpus_id': 236999951, 'score': 0}, {'doc_id': '227279995', 'title': 'Building a better baseline to estimate 160 years of avian population change and create historically informed conservation targets.', 'abstract': 'Globally, anthropogenic land cover change has been dramatic over the last few centuries and is frequently invoked as a major cause of wildlife population declines. Baseline data currently used to assess population trends, however, began well after major changes to the landscape. In North America, breeding bird population trends are assessed by the Breeding Bird Survey, which began in the 1960s. Estimates of distribution and abundance prior to major habitat alteration would add historical perspective to contemporary trends and allow for historically based conservation targets. Here we use a hind-casting framework to estimate change in distribution and abundance of seven bird species in the Willamette Valley, Oregon. After cross-walking current land cover classes with 1850s reconstructed vegetation data, we used multi-scale species distribution models and hierarchical distance sampling models to predict spatially explicit densities on the modern and historical landscapes. We estimated that White-breasted Nuthatch and Western Meadowlark populations, two species sensitive to fragmentation of oak woodlands and grasslands, have declined since the 1850s by 93% and 97%, respectively. Five other species have had nearly stable or increasing populations, despite steep regional declines since the 1960s. Hind-casted reconstructions provide historical perspective for assessing contemporary trends and allow for historically based conservation targets that can inform current management. This article is protected by copyright. All rights reserved.', 'corpus_id': 227279995, 'score': 1}, {'doc_id': '235688499', 'title': 'Impact of climate change on the current and future distribution of threatened species of the genus Lessingianthus (Vernonieae: Asteraceae) from the Brazilian Cerrado.', 'abstract': ""Climate change has already altered global biodiversity, causing the migration of species and changes in habitat distribution. To implement a sustainable conservation strategy, it is necessary to understand the impacts of climate change on species. Lessingianthus is a South American genus that includes numerous endangered species, some of which grow in the Brazilian Cerrado, a Neotropical savanna considered a world's biodiversity hotspot. However, the impact of global climate change on these species has still not been estimated. We evaluate the effect of climate change on the habitat of 10 threatened Lessingianthus species and on their potential distribution, and assess the effectiveness of current protected areas (PAs) using ecological niche models. Based on the maximum entropy algorithm (Maxent), we first modeled the potential distribution of these species under current climatic conditions and then projected the distribution for two future scenarios of climate change (RCP 4.5 and RCP 8.5) and two time periods (2050 and 2070). We predicted current habitat suitability and identified suitable bioclimatic variables for these species. Our findings suggest that the area comprising the south and southeast of Cerrado is irreplaceable and the most biotically stable region for these endangered species; therefore, it should be considered a conservation priority area."", 'corpus_id': 235688499, 'score': 0}, {'doc_id': '235700984', 'title': 'A Test of Species Distribution Model Transferability Across Environmental and Geographic Space for 108 Western North American Tree Species', 'abstract': 'Predictions from species distribution models (SDMs) are commonly used in support of environmental decision-making to explore potential impacts of climate change on biodiversity. However, because future climates are likely to differ from current climates, there has been ongoing interest in understanding the ability of SDMs to predict species responses under novel conditions (i.e., model transferability). Here, we explore the spatial and environmental limits to extrapolation in SDMs using forest inventory data from 11 model algorithms for 108 tree species across the western United States. Algorithms performed well in predicting occurrence for plots that occurred in the same geographic region in which they were fitted. However, a substantial portion of models performed worse than random when predicting for geographic regions in which algorithms were not fitted. Our results suggest that for transfers in geographic space, no specific algorithm was better than another as there were no significant differences in predictive performance across algorithms. There were significant differences in predictive performance for algorithms transferred in environmental space with GAM performing best. However, the predictive performance of GAM declined steeply with increasing extrapolation in environmental space relative to other algorithms. The results of this study suggest that SDMs may be limited in their ability to predict species ranges beyond the environmental data used for model fitting. When predicting climate-driven range shifts, extrapolation may also not reflect important biotic and abiotic drivers of species ranges, and thus further misrepresent the realized shift in range. Future studies investigating transferability of process based SDMs or relationships between geodiversity and biodiversity may hold promise.', 'corpus_id': 235700984, 'score': 1}, {'doc_id': '19019299', 'title': 'Correlative and mechanistic models of species distribution provide congruent forecasts under climate change.', 'abstract': 'Good forecasts of climate change impacts on extinction risks are critical for effective conservation management responses. Species distribution models (SDMs) are central to extinction risk analyses. The reliability of predictions of SDMs has been questioned because models often lack a mechanistic underpinning and rely on assumptions that are untenable under climate change. We show how integrating predictions from fundamentally different modeling strategies produces robust forecasts of climate change impacts on habitat and population parameters. We illustrate the principle by applying mechanistic (Niche Mapper) and correlative (Maxent, Bioclim) SDMs to predict current and future distributions and fertility of an Australian gliding possum. The two approaches make congruent, accurate predictions of current distribution and similar, dire predictions about the impact of a warming scenario, supporting previous correlative-only predictions for similar species. We argue that convergent lines of independent evidence provide a robust basis for predicting and managing extinctions risks under climate change.', 'corpus_id': 19019299, 'score': 1}, {'doc_id': '236968585', 'title': ""The evolutionary genomics of species' responses to climate change."", 'abstract': ""Climate change is a threat to biodiversity. One way that this threat manifests is through pronounced shifts in the geographical range of species over time. To predict these shifts, researchers have primarily used species distribution models. However, these models are based on assumptions of niche conservatism and do not consider evolutionary processes, potentially limiting their accuracy and value. To incorporate evolution into the prediction of species' responses to climate change, researchers have turned to landscape genomic data and examined information about local genetic adaptation using climate models. Although this is an important advancement, this approach currently does not include other evolutionary processes-such as gene flow, population dispersal and genomic load-that are critical for predicting the fate of species across the landscape. Here, we briefly review the current practices for the use of species distribution models and for incorporating local adaptation. We next discuss the rationale and theory for considering additional processes, reviewing how they can be incorporated into studies of species' responses to climate change. We summarize with a conceptual framework of how manifold layers of information can be combined to predict the potential response of specific populations to climate change. We illustrate all of the topics using an exemplar dataset and provide the source code as potential tutorials. This Perspective is intended to be a step towards a more comprehensive integration of population genomics with climate change science."", 'corpus_id': 236968585, 'score': 0}]"
45	{'doc_id': '71907', 'title': 'Identifying Argumentative Discourse Structures in Persuasive Essays', 'abstract': 'In this paper, we present a novel approach for identifying argumentative discourse structures in persuasive essays. The structure of argumentation consists of several components (i.e. claims and premises) that are connected with argumentative relations. We consider this task in two consecutive steps. First, we identify the components of arguments using multiclass classification. Second, we classify a pair of argument components as either support or non-support for identifying the structure of argumentative discourse. For both tasks, we evaluate several classifiers and propose novel feature sets including structural, lexical, syntactic and contextual features. In our experiments, we obtain a macro F1-score of 0.726 for identifying argument components and 0.722 for argumentative relations.', 'corpus_id': 71907}	11901	"[{'doc_id': '15178809', 'title': 'A Systematic Study of Neural Discourse Models for Implicit Discourse Relation', 'abstract': 'Inferring implicit discourse relations in natural language text is the most difficult subtask in discourse parsing. Many neural network models have been proposed to tackle this problem. However, the comparison for this task is not unified, so we could hardly draw clear conclusions about the effectiveness of various architectures. Here, we propose neural network models that are based on feedforward and long-short term memory architecture and systematically study the effects of varying structures. To our surprise, the best-configured feedforward architecture outperforms LSTM-based model in most cases despite thorough tuning. Further, we compare our best feedforward system with competitive convolutional and recurrent networks and find that feedforward can actually be more effective. For the first time for this task, we compile and publish outputs from previous neural and non-neural systems to establish the standard for further comparison.', 'corpus_id': 15178809, 'score': 1}, {'doc_id': '235092325', 'title': 'A Neural Approach to Discourse Relation Signal Detection', 'abstract': ""Previous data-driven work investigating the types and distributions of discourse\n relation signals, including discourse markers such as 'however' or phrases such as 'as a\n result' has focused on the relative frequencies of signal words within and outside text\n from each discourse relation. Such approaches do not allow us to quantify the signaling\n strength of individual instances of a signal on a scale (e.g. more or less\n discourse-relevant instances of 'and'), to assess the distribution of ambiguity for\n signals, or to identify words that hinder discourse relation identification in context\n ('anti-signals' or 'distractors'). In this paper we present a data-driven approach to\n signal detection using a distantly supervised neural network and develop a metric, Δs\n (or 'delta-softmax'), to quantify signaling strength. Ranging between -1 and 1 and\n relying on recent advances in contextualized words embeddings, the metric represents\n each word's positive or negative contribution to the identifiability of a relation in\n specific instances in context. Based on an English corpus annotated for discourse\n relations using Rhetorical Structure Theory and signal type annotations anchored to\n specific tokens, our analysis examines the reliability of the metric, the places where\n it overlaps with and differs from human judgments, and the implications for identifying\n features that neural models may need in order to perform better on automatic discourse\n relation classification."", 'corpus_id': 235092325, 'score': 1}, {'doc_id': '211296688', 'title': 'Parsing Early Modern English for Linguistic Search', 'abstract': 'We investigate the question of whether advances in NLP over the last few years make it possible to vastly increase the size of data usable for research in historical syntax. This brings together many of the usual tools in NLP - word embeddings, tagging, and parsing - in the service of linguistic queries over automatically annotated corpora. We train a part-of-speech (POS) tagger and parser on a corpus of historical English, using ELMo embeddings trained over a billion words of similar text. The evaluation is based on the standard metrics, as well as on the accuracy of the query searches using the parsed data.', 'corpus_id': 211296688, 'score': 0}, {'doc_id': '212628456', 'title': 'Parsing Thai Social Data: A New Challenge for Thai NLP', 'abstract': 'Dependency parsing (DP) is a task that analyzes text for syntactic structure and relationship between words. DP is widely used to improve natural language processing (NLP) applications in many languages such as English. Previous works on DP are generally applicable to formally written languages. However, they do not apply to informal languages such as the ones used in social networks. Therefore, DP has to be researched and explored with such social network data. In this paper, we explore and identify a DP model that is suitable for Thai social network data. After that, we will identify the appropriate linguistic unit as an input. The result showed that, the transition based model called, improve Elkared dependency parser outperform the others at UAS of 81.42%.', 'corpus_id': 212628456, 'score': 0}, {'doc_id': '209516094', 'title': 'ÆTHEL: Automatically Extracted Typelogical Derivations for Dutch', 'abstract': 'We present ÆTHEL, a semantic compositionality dataset for written Dutch. ÆTHEL consists of two parts. First, it contains a lexicon of supertags for about 900 000 words in context. The supertags correspond to types of the simply typed linear lambda-calculus, enhanced with dependency decorations that capture grammatical roles supplementary to function-argument structures. On the basis of these types, ÆTHEL further provides 72 192 validated derivations, presented in four formats: natural-deduction and sequent-style proofs, linear logic proofnets and the associated programs (lambda terms) for meaning composition. ÆTHEL’s types and derivations are obtained by means of an extraction algorithm applied to the syntactic analyses of LASSY Small, the gold standard corpus of written Dutch. We discuss the extraction algorithm and show how ‘virtual elements’ in the original LASSY annotation of unbounded dependencies and coordination phenomena give rise to higher-order types. We suggest some example usecases highlighting the benefits of a type-driven approach at the syntax semantics interface. The following resources are open-sourced with ÆTHEL: the lexical mappings between words and types, a subset of the dataset consisting of 7 924 semantic parses, and the Python code that implements the extraction algorithm.', 'corpus_id': 209516094, 'score': 0}, {'doc_id': '195063854', 'title': 'Ambiguity in Explicit Discourse Connectives', 'abstract': 'Discourse connectives are known to be subject to both usage and sense ambiguity, as has already been discussed in the literature. But discourse connectives are no different from other linguistic expressions in being subject to other types of ambiguity as well. Four are illustrated and discussed here.', 'corpus_id': 195063854, 'score': 1}, {'doc_id': '226254371', 'title': 'MEGA RST Discourse Treebanks with Structure and Nuclearity from Scalable Distant Sentiment Supervision', 'abstract': 'The lack of large and diverse discourse treebanks hinders the application of data-driven approaches, such as deep-learning, to RST-style discourse parsing. In this work, we present a novel scalable methodology to automatically generate discourse treebanks using distant supervision from sentiment-annotated datasets, creating and publishing MEGA-DT, a new large-scale discourse-annotated corpus. Our approach generates discourse trees incorporating structure and nuclearity for documents of arbitrary length by relying on an efficient heuristic beam-search strategy, extended with a stochastic component. Experiments on multiple datasets indicate that a discourse parser trained on our MEGA-DT treebank delivers promising inter-domain performance gains when compared to parsers trained on human-annotated discourse corpora.', 'corpus_id': 226254371, 'score': 1}, {'doc_id': '212725806', 'title': 'CompLex — A New Corpus for Lexical Complexity Prediction from Likert Scale Data', 'abstract': 'Predicting which words are considered hard to understand for a given target population is a vital step in many NLP applications such astext simplification. This task is commonly referred to as Complex Word Identification (CWI). With a few exceptions, previous studieshave approached the task as a binary classification task in which systems predict a complexity value (complex vs. non-complex) fora set of target words in a text. This choice is motivated by the fact that all CWI datasets compiled so far have been annotated using abinary annotation scheme. Our paper addresses this limitation by presenting the first English dataset for continuous lexical complexityprediction. We use a 5-point Likert scale scheme to annotate complex words in texts from three sources/domains: the Bible, Europarl,and biomedical texts. This resulted in a corpus of 9,476 sentences each annotated by around 7 annotators.', 'corpus_id': 212725806, 'score': 0}, {'doc_id': '53394543', 'title': 'Discourse Annotation in the PDTB: The Next Generation', 'abstract': 'We present highlights from our work on enriching the Penn Discourse Treebank (PDTB), to be released to the community in Fall 2018 as the PDTB-3. We have increased its coverage of discourse relations (from ⇠40K to ⇠53k), the majority in intra-sentential contexts. Our work on these new relations has led us to augment and/or modify aspects of the annotation guidelines, including the sense hierarchy, and all changes have been propagated through the rest of the corpus.', 'corpus_id': 53394543, 'score': 1}]"
46	{'doc_id': '2808403', 'title': 'Deep Learning for Physical Processes: Incorporating Prior Scientific Knowledge', 'abstract': 'We consider the use of Deep Learning methods for modeling complex phenomena like those occurring in natural physical processes. With the large amount of data gathered on these phenomena the data intensive paradigm could begin to challenge more traditional approaches elaborated over the years in fields like maths or physics. However, despite considerable successes in a variety of application domains, the machine learning field is not yet ready to handle the level of complexity required by such problems. Using an example application, namely Sea Surface Temperature Prediction, we show how general background knowledge gained from physics could be used as a guideline for designing efficient Deep Learning models. In order to motivate the approach and to assess its generality we demonstrate a formal link between the solution of a class of differential equations underlying a large family of physical phenomena and the proposed model. Experiments and comparison with series of baselines including a state of the art numerical approach is then provided.', 'corpus_id': 2808403}	9851	"[{'doc_id': '227342566', 'title': 'Estimating Vector Fields from Noisy Time Series', 'abstract': 'While there has been a surge of recent interest in learning differential equation models from time series, methods in this area typically cannot cope with highly noisy data. We break this problem into two parts: (i) approximating the unknown vector field (or right-hand side) of the differential equation, and (ii) dealing with noise. To deal with (i), we describe a neural network architecture consisting of tensor products of one-dimensional neural shape functions. For (ii), we propose an al-ternating minimization scheme that switches between vector field training and filtering steps, together with multiple trajectories of training data. We find that the neural shape function architecture retains the approximation properties of dense neural networks, enables effective computation of vector field error, and allows for graphical interpretability, all for data/systems in any finite dimension d. We also study the combination of either our neural shape function method or existing differential equation learning methods with alternating minimization and multiple trajectories. We find that retrofitting any learning method in this way boosts the method’s robustness to noise. While in their raw form the methods struggle with 1% Gaussian noise, after retrofitting, they learn accurate vector fields from data with 10% Gaussian noise.', 'corpus_id': 227342566, 'score': 0}, {'doc_id': '208527690', 'title': 'Variational Physics-Informed Neural Networks For Solving Partial Differential Equations', 'abstract': 'Physics-informed neural networks (PINNs) [31] use automatic differentiation to solve partial differential equations (PDEs) by penalizing the PDE in the loss function at a random set of points in the domain of interest. Here, we develop a Petrov-Galerkin version of PINNs based on the nonlinear approximation of deep neural networks (DNNs) by selecting the {\\em trial space} to be the space of neural networks and the {\\em test space} to be the space of Legendre polynomials. We formulate the \\textit{variational residual} of the PDE using the DNN approximation by incorporating the variational form of the problem into the loss function of the network and construct a \\textit{variational physics-informed neural network} (VPINN). By integrating by parts the integrand in the variational form, we lower the order of the differential operators represented by the neural networks, hence effectively reducing the training cost in VPINNs while increasing their accuracy compared to PINNs that essentially employ delta test functions. For shallow networks with one hidden layer, we analytically obtain explicit forms of the \\textit{variational residual}. We demonstrate the performance of the new formulation for several examples that show clear advantages of VPINNs over PINNs in terms of both accuracy and speed.', 'corpus_id': 208527690, 'score': 1}, {'doc_id': '229331851', 'title': 'On the eigenvector bias of Fourier feature networks: From regression to solving multi-scale PDEs with physics-informed neural networks', 'abstract': 'Physics-informed neural networks (PINNs) are demonstrating remarkable promise in integrating physical models with gappy and noisy observational data, but they still struggle in cases where the target functions to be approximated exhibit high-frequency or multi-scale features. In this work we investigate this limitation through the lens of Neural Tangent Kernel (NTK) theory and elucidate how PINNs are biased towards learning functions along the dominant eigen-directions of their limiting NTK. Using this observation, we construct novel architectures that employ spatio-temporal and multi-scale random Fourier features, and justify how such coordinate embedding layers can lead to robust and accurate PINN models. Numerical examples are presented for several challenging cases where conventional PINN models fail, including wave propagation and reaction-diffusion dynamics, illustrating how the proposed methods can be used to effectively tackle both forward and inverse problems involving partial differential equations with multi-scale behavior. All code an data accompanying this manuscript will be made publicly available at https://github.com/ PredictiveIntelligenceLab/MultiscalePINNs.', 'corpus_id': 229331851, 'score': 1}, {'doc_id': '209442627', 'title': 'Approaching Neural Network Uncertainty Realism', 'abstract': 'Statistical models are inherently uncertain. Quantifying or at least upperbounding their uncertainties is vital for safety-critical systems such as autonomous vehicles. While standard neural networks do not report this information, several approaches exist to integrate uncertainty estimates into them. Assessing the quality of these uncertainty estimates is not straightforward, as no direct ground truth labels are available. Instead, implicit statistical assessments are required. For regression, we propose to evaluate uncertainty realism—a strict quality criterion—with a Mahalanobis distance-based statistical test. An empirical evaluation reveals the need for uncertainty measures that are appropriate to upper-bound heavy-tailed empirical errors. Alongside, we transfer the variational U-Net classification architecture to standard supervised image-to-image tasks. We adopt it to the automotive domain and show that it significantly improves uncertainty realism compared to a plain encoder-decoder model.', 'corpus_id': 209442627, 'score': 1}, {'doc_id': '227338773', 'title': 'Multivariate Density Estimation with Deep Neural Mixture Models', 'abstract': ""Albeit worryingly underrated in the recent literature on machine learning in general (and, on deep learning in particular), multivariate density estimation is a fundamental task in many applications, at least implicitly, and still an open issue. With a few exceptions, deep neural networks (DNNs) have seldom been applied to density estimation, mostly due to the unsupervised nature of the estimation task, and (especially) due to the need for constrained training algorithms that ended up realizing proper probabilistic models that satisfy Kolmogorov's axioms. Moreover, in spite of the well-known improvement in terms of modeling capabilities yielded by mixture models over plain single-density statistical estimators, no proper mixtures of multivariate DNN-based component densities have been investigated so far. The paper fills this gap by extending our previous work on Neural Mixture Densities (NMMs) to multivariate DNN mixtures. A maximum-likelihood (ML) algorithm for estimating Deep NMMs (DNMMs) is handed out, which satisfies numerically a combination of hard and soft constraints aimed at ensuring satisfaction of Kolmogorov's axioms. The class of probability density functions that can be modeled to any degree of precision via DNMMs is formally defined. A procedure for the automatic selection of the DNMM architecture, as well as of the hyperparameters for its ML training algorithm, is presented (exploiting the probabilistic nature of the DNMM). Experimental results on univariate and multivariate data are reported on, corroborating the effectiveness of the approach and its superiority to the most popular statistical estimation techniques."", 'corpus_id': 227338773, 'score': 0}, {'doc_id': '227745095', 'title': 'Statistical Mechanics of Deep Linear Neural Networks: The Back-Propagating Renormalization Group', 'abstract': 'The success of deep learning in many real-world tasks has triggered an effort to theoretically understand the power and limitations of deep learning in training and generalization of complex tasks, so far with limited progress. In this work, we study the statistical mechanics of learning in Deep Linear Neural Networks (DLNNs) in which the input-output function of an individual unit is linear. Despite the linearity of the units, learning in DLNNs is highly nonlinear, hence studying its properties reveals some of the essential features of nonlinear Deep Neural Networks (DNNs). We solve exactly the network properties following supervised learning using an equilibrium Gibbs distribution in the weight space. To do this, we introduce the Back-Propagating Renormalization Group (BPRG) which allows for the incremental integration of the network weights layer by layer from the network output layer and progressing backward. This procedure allows us to evaluate important network properties such as its generalization error, the role of network width and depth, the impact of the size of the training set, and the effects of weight regularization and learning stochasticity. Furthermore, by performing partial integration of layers, BPRG allows us to compute the emergent properties of the neural representations across the different hidden layers. We have proposed a heuristic extension of the BPRG to nonlinear DNNs with rectified linear units (ReLU). Surprisingly, our numerical simulations reveal that despite the nonlinearity, the predictions of our theory are largely shared by ReLU networks with modest depth, in a wide regime of parameters. Our work is the first exact statistical mechanical study of learning in a family of Deep Neural Networks, and the first development of the Renormalization Group approach to the weight space of these systems.', 'corpus_id': 227745095, 'score': 0}, {'doc_id': '218537894', 'title': 'Physics-informed neural network for ultrasound nondestructive quantification of surface breaking cracks', 'abstract': 'We introduce an optimized physics-informed neural network (PINN) trained to solve the problem of identifying and characterizing a surface breaking crack in a metal plate. PINNs are neural networks that can combine data and physics in the learning process by adding the residuals of a system of Partial Differential Equations to the loss function. Our PINN is supervised with realistic ultrasonic surface acoustic wave data acquired at a frequency of 5 MHz. The ultrasonic surface wave data is represented as a surface deformation on the top surface of a metal plate, measured by using the method of laser vibrometry. The PINN is physically informed by the acoustic wave equation and its convergence is sped up using adaptive activation functions. The adaptive activation function uses a scalable hyperparameter in the activation function, which is optimized to achieve best performance of the network as it changes dynamically the topology of the loss function involved in the optimization process. The usage of adaptive activation function significantly improves the convergence, notably observed in the current study. We use PINNs to estimate the speed of sound of the metal plate, which we do with an error of 1\\%, and then, by allowing the speed of sound to be space dependent, we identify and characterize the crack as the positions where the speed of sound has decreased. Our study also shows the effect of sub-sampling of the data on the sensitivity of sound speed estimates. More broadly, the resulting model shows a promising deep neural network model for ill-posed inverse problems.', 'corpus_id': 218537894, 'score': 1}, {'doc_id': '235441920', 'title': 'Learning nonlinear state-space models using autoencoders', 'abstract': 'We propose a methodology for the identification of nonlinear state-space models from input/output data using machine-learning techniques based on autoencoders and neural networks. Our framework simultaneously identifies the nonlinear output and state-update maps of the model. After formulating the approach and providing guidelines for tuning the related hyper-parameters (including the model order), we show its capability in fitting nonlinear models on different nonlinear system identification benchmarks. Performance is assessed in terms of open-loop prediction on test data and of controlling the system via nonlinear model predictive control (MPC) based on the identified nonlinear state-space model.', 'corpus_id': 235441920, 'score': 1}]"
47	{'doc_id': '54471801', 'title': 'The shifting politics of patient activism: From bio-sociality to bio-digital citizenship', 'abstract': 'Digital media provide novel tools for patient activists from disease- and condition-specific communities. While those with debilitating conditions or disabilities have long recognised the value of collective action for advancing their interests, digital media offer activists unparalleled opportunities to fulfil their goals. This article explores the shifting politics of ‘activism’ in the increasingly digitally mediated, commercialised context of healthcare, asking: what role have digital media played in the repertoire of activists’ strategies? And, to what extent and how has the use of such media impacted the very concept of activism? Building on sociological ideas on emergent forms of ‘biological citizenship’ and drawing on findings from an analysis of available media, including television and print news reportage, online communications, published histories and campaign material and other information produced by activists in HIV/AIDS and breast cancer communities, we argue that digital media have profoundly shaped how ‘activism’ is enacted, both the goals pursued and the strategies adopted, which serve to broadly align contemporary patient communities’ interests with those of science and business. This alignment, which we characterise as ‘bio-digital citizenship’, has involved a fundamental reorientation of ‘activism’ from less of a struggle for rights to more of a striving to achieve a public profile and attract funding. We conclude by calling for a reconceptualisation of ‘activism’ to more adequately reflect the workings of power in the digital age, whereby the agency and hopes of citizens are central to the workings of political rule.', 'corpus_id': 54471801}	20864	"[{'doc_id': '148821692', 'title': 'Counter‐mapping data science', 'abstract': ""Counter-mapping is a combination of critical ideas and practices for social change that offers a productive and promising approach for grassroots data science initiatives. Current information technologies collect, store, and analyze data with new degrees of size, speed, heterogeneity, and detail. While much work utilizing data science technologies is dedicated to generating profit or to national security, some data science projects explicitly attempt to facilitate new social relations, though with inconsistent results and consequences. This paper reviews counter-mapping's particular combination of theory and practice as a potential point of reference for such initiatives. Counter-mapping takes the tools of institutional map-making at government agencies and corporations and applies them in situated, bottom-up ways. Moreover, counter-mapping's multiple theoretical approaches and polyglot practices offer a variety of inspirations and avenues for future work in identifying and realizing alternative, ideally better, possibilities. This paper defines counter-mapping; outlines its multiple theorizations; briefly describes three relevant case studies, The Detroit Geographical Expedition and Institute, Mapping Police Violence, and the Counter-Cartographies Collective; and concludes with a few hard-learned considerations from counter-mapping that are directly pertinent for data-oriented projects focused on change."", 'corpus_id': 148821692, 'score': 1}, {'doc_id': '237525845', 'title': '“It’s to trust not trusting”: tensions and conflicts between LGBT activism and media', 'abstract': ""CAROLINA BONOTO Master in Media Communication Post-Graduation Program in Communication at Federal University of Santa Maria (UFSM). Bachelor's degree in Social Communication – Journalism at Federal University of Santa Maria (UFSM) and Law at Franciscan University (UFN). ). In this article, she contributed with the conception of the research design; development of the theoretical discussion; support in the text revision; writing of the manuscript and translation and revision of the foreign language version. Santa Maria, Rio Grande do Sul, Brasil. E-mail: c.bonoto@gmail.com. ORCID: 0000-0003-2184-7625."", 'corpus_id': 237525845, 'score': 0}, {'doc_id': '237951735', 'title': 'The Politics of Civil Society Forms: Urban Environmental Activists and Democracy in Jakarta', 'abstract': ""Despite the ongoing debate regarding how and to what extent civil society enhances democratic practices, it is generally agreed that there is a reasonable link between civil society and democracy under certain conditions. This paper aims to explore the politics of civil society forms and understand their contribution to the maintenance of democratic practices in Jakarta. Building on a neo-Tocquevillian understanding of civil society, this article analyses urban environmental activists' strategic adoption of voluntary associations and environmental spin-off campaigns as forms of civic engagement to improve public policy. This paper asks how and to what extent these forms of civic engagement provide alternative understandings of civil society's efforts to promote local democracy. We argue that urban environmental activists' spin-off campaigns and voluntary associations represent a particular form of civil society politics, and thus provide different routes to understand local democracy by facilitating diagonal accountability mechanisms. However, further analysis found that the forms adopted by urban environmental activists suffer horizontal and vertical accountability problems similar to those frequently found in more established forms of civil society (e.g. non-government organisations). Nonetheless, the discussion in this paper illustrates civil society's ingenuity in pushing for democratic practices amidst Indonesia's 'democratic recession'."", 'corpus_id': 237951735, 'score': 0}, {'doc_id': '237477456', 'title': 'The activism consumption of in the strategic dynamics of advertising campaigns: a semiotic approach', 'abstract': ""The article analyzes how brands use activism for social causes in the construction of their images in the contemporary scenario. He briefly digresses about the path that led corporations to this discursive practice, positioning themselves in the defense of causes, manifested especially in digital environments. It chooses as a corpus of analysis a set of narratives conveyed by the Brazilian beer brand Skol, which was one of the pioneers in this perspective, showing itself as a trend for many others. The theoretical-methodological basis is the sociosemiotic developed by Landowski, from Greimas, and Floch's studies in the field of plastic semiotics and advertising. The study shows how brands with a large market share have used activism as a figurative coating to make their axiologies feasible in strategic narratives that meet the demands of an audience eager for social participation and identification for the exercise of their subjectivity."", 'corpus_id': 237477456, 'score': 0}, {'doc_id': '18270347', 'title': 'Evidence-based activism: Patients’, users’ and activists’ groups in knowledge society', 'abstract': 'This article proposes the notion of ‘evidence-based activism’ to capture patients’ and health activists’ groups’ focus on knowledge production and knowledge mobilisation in the governance of health issues. It introduces empirical data and analysis on groups active in four countries (France, Ireland, Portugal and the United Kingdom), and in four condition-areas (rare diseases, Alzheimer’s disease, ADHD – Attention Deficit Hyperactivity Disorder and childbirth). It shows how these groups engage with, and articulate a variety of credentialed knowledge and ‘experiential knowledge’ with a view to explore concerned people’s situations, to make themselves part and parcel of the networks of expertise on their conditions in their national contexts, and to elaborate evidence on the issues they deem important to address both at an individual and at a collective level. This article argues that in contrast to health movements which contest institutions from the outside, patients’ and activists’ groups which embrace ‘evidence-based activism’ work ‘from within’ to imagine new epistemic and political appraisal of their causes and conditions. ‘Evidence-based activism’ entails a collective inquiry associating patients/activists and specialists/professionals in the conjoint fabrics of scientific statements and political claims. From a conceptual standpoint, ‘evidence-based activism’ sheds light on the ongoing co-production of matters of fact and matters of concern in contemporary technological democracies.', 'corpus_id': 18270347, 'score': 1}, {'doc_id': '216348451', 'title': 'Design Justice', 'abstract': None, 'corpus_id': 216348451, 'score': 1}, {'doc_id': '238054561', 'title': 'Activism in the Digital Age', 'abstract': 'Social movements have been transformed in the last decade by social networks, where the dynamics of the social protests have evolved and have been structured and viralized through social media. They are no longer just conversations between activists that stay on social platforms. The cyberactivism that takes place on Twitter or Instagram can also play a significant role in general society by influencing government decision making or shaping the relationships between citizens. In this chapter, the authors explore the main activist movements that took place in social media in the last decade: Occupy, BlackLivesMatter, and MeToo. The proposed approach used in this study facilitates the comparison of each movement while focusing on the user-generated content in social media. This study suggests the presence of four major categories to frame the content generated by the activists. The chapter concludes with the identification of three different approaches to the research of a future research agenda that should be considered for the study of the social movements from the UGC theory framework.', 'corpus_id': 238054561, 'score': 0}, {'doc_id': '237343236', 'title': 'The Ethics of Using Digital Trace Data in Education: A Thematic Review of the Research Landscape', 'abstract': 'This article presents the findings of a systematic qualitative analysis of research in the ethics of digital trace data use in learning and education. From the resulting analysis of 77 peer-reviewed studies, we (1) map the characteristics of research by study type, academic community, institutional setting, and national context; (2) identify the primary ethical concerns and related responses; and (3) highlight the research gaps. Four areas of focus are identified in this emerging area: (1) privacy, informed consent, and data ownership; (2) validity and integrity; (3) ethical decision making; and (4) governance and accountability. We highlight the lack of evidence particularly for preschool and school-aged children and the disparate communities working in this domain, and we suggest a more cohesive approach, where the wider learning and educational ecosystem is recognized, explicit engagement with ethical theory is central, and mid- to long-term ethical issues are considered alongside immediate concerns.', 'corpus_id': 237343236, 'score': 0}, {'doc_id': '228468384', 'title': 'The CARE Principles for Indigenous Data Governance', 'abstract': 'Concerns about secondary use of data and limited opportunities for benefit-sharing have focused attention on the tension that Indigenous communities feel between (1) protecting Indigenous rights and interests in Indigenous data (including traditional knowledges) and (2) supporting open data, machine learning, broad data sharing, and big data initiatives. The International Indigenous Data Sovereignty Interest Group (within the Research Data Alliance) is a network of nation-state based Indigenous data sovereignty networks and individuals that developed the ‘CARE Principles for Indigenous Data Governance’ (Collective Benefit, Authority to Control, Responsibility, and Ethics) in consultation with Indigenous Peoples, scholars, non-profit organizations, and governments. The CARE Principles are people– and purpose-oriented, reflecting the crucial role of data in advancing innovation, governance, and self-determination among Indigenous Peoples. The Principles complement the existing data-centric approach represented in the ‘FAIR Guiding Principles for scientific data management and stewardship’ (Findable, Accessible, Interoperable, Reusable). The CARE Principles build upon earlier work by the Te Mana Raraunga Maori Data Sovereignty Network, US Indigenous Data Sovereignty Network, Maiam nayri Wingara Aboriginal and Torres Strait Islander Data Sovereignty Collective, and numerous Indigenous Peoples, nations, and communities. The goal is that stewards and other users of Indigenous data will ‘Be FAIR and CARE.’ In this first formal publication of the CARE Principles, we articulate their rationale, describe their relation to the FAIR Principles, and present examples of their application.', 'corpus_id': 228468384, 'score': 1}, {'doc_id': '199020420', 'title': 'Indigenous Data Governance: Strategies from United States Native Nations', 'abstract': 'Data have become the new global currency, and a powerful force in making decisions and wielding power. As the world engages with open data, big data reuse, and data linkage, what do data-driven futures look like for communities plagued by data inequities? Indigenous data stakeholders and non-Indigenous allies have explored this question over the last three years in a series of meetings through the Research Data Alliance (RDA). Drawing on RDA and other gatherings, and a systematic scan of literature and practice, we consider possible answers to this question in the context of Indigenous peoples vis-a-vis two emerging concepts: Indigenous data sovereignty and Indigenous data\xa0governance.\xa0Specifically,\xa0we\xa0focus\xa0on\xa0the\xa0data\xa0challenges facing Native nations and the intersection of data, tribal sovereignty, and power. Indigenous data sovereignty is the right of each Native nation to govern the collection, ownership, and application of the tribe’s data. Native nations exercise Indigenous data sovereignty through the interrelated processes of Indigenous data governance and decolonizing data. This paper explores the implications of Indigenous data sovereignty and Indigenous data governance for Native nations and others. We argue for the repositioning of authority over Indigenous data back to Indigenous peoples. At the same time, we recognize that\xa0 there are significant obstacles to rebuilding effective Indigenous data systems and the process will require resources, time, and partnerships among Native nations, other governments, and data agents.', 'corpus_id': 199020420, 'score': 1}]"
48	{'doc_id': '46798026', 'title': 'beta-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework', 'abstract': 'Learning an interpretable factorised representation of the independent data generative factors of the world without supervision is an important precursor for the development of artificial intelligence that is able to learn and reason in the same way that humans do. We introduce β-VAE, a new state-of-the-art framework for automated discovery of interpretable factorised latent representations from raw image data in a completely unsupervised manner. Our approach is a modification of the variational autoencoder (VAE) framework. We introduce an adjustable hyperparameter β that balances latent channel capacity and independence constraints with reconstruction accuracy. We demonstrate that β-VAE with appropriately tuned β > 1 qualitatively outperforms VAE (β = 1), as well as state of the art unsupervised (InfoGAN) and semi-supervised (DC-IGN) approaches to disentangled factor learning on a variety of datasets (celebA, faces and chairs). Furthermore, we devise a protocol to quantitatively compare the degree of disentanglement learnt by different models, and show that our approach also significantly outperforms all baselines quantitatively. Unlike InfoGAN, β-VAE is stable to train, makes few assumptions about the data and relies on tuning a single hyperparameter β, which can be directly optimised through a hyperparameter search using weakly labelled data or through heuristic visual inspection for purely unsupervised data.', 'corpus_id': 46798026}	2966	"[{'doc_id': '221856630', 'title': 'Tasks, stability, architecture, and compute: Training more effective learned optimizers, and using them to train themselves', 'abstract': 'Much as replacing hand-designed features with learned functions has revolutionized how we solve perceptual tasks, we believe learned algorithms will transform how we train models. In this work we focus on general-purpose learned optimizers capable of training a wide variety of problems with no user-specified hyperparameters. We introduce a new, neural network parameterized, hierarchical optimizer with access to additional features such as validation loss to enable automatic regularization. Most learned optimizers have been trained on only a single task, or a small number of tasks. We train our optimizers on thousands of tasks, making use of orders of magnitude more compute, resulting in optimizers that generalize better to unseen tasks. The learned optimizers not only perform well, but learn behaviors that are distinct from existing first order optimizers. For instance, they generate update steps that have implicit regularization and adapt as the problem hyperparameters (e.g. batch size) or architecture (e.g. neural network width) change. Finally, these learned optimizers show evidence of being useful for out of distribution tasks such as training themselves from scratch.', 'corpus_id': 221856630, 'score': 0}, {'doc_id': '232233653', 'title': 'Neural Networks and Denotation', 'abstract': 'We introduce a framework for reasoning about what meaning is captured by the neurons in a trained neural network. We provide a strategy for discovering meaning by training a second model (referred to as an observer model) to classify the state of the model it observes (an object model) in relation to attributes of the underlying dataset. We implement and evaluate observer models in the context of a specific set of classification problems, employ heat maps for visualizing the relevance of components of an object model in the context of linear observer models, and use these visualizations to extract insights about the manner in which neural networks identify salient characteristics of their inputs. We identify important properties captured decisively in trained neural networks; some of these properties are denoted by individual neurons. Finally, we observe that the label proportion of a property denoted by a neuron is dependent on the depth of a neuron within a network; we analyze these dependencies, and provide an interpretation of them.', 'corpus_id': 232233653, 'score': 1}, {'doc_id': '218869555', 'title': 'Learning to Simulate Dynamic Environments With GameGAN', 'abstract': 'Simulation is a crucial component of any robotic system. In order to simulate correctly, we need to write complex rules of the environment: how dynamic agents behave, and how the actions of each of the agents affect the behavior of others. In this paper, we aim to learn a simulator by simply watching an agent interact with an environment. We focus on graphics games as a proxy of the real environment. We introduce GameGAN, a generative model that learns to visually imitate a desired game by ingesting screenplay and keyboard actions during training. Given a key pressed by the agent, GameGAN ""renders"" the next screen using a carefully designed generative adversarial network. Our approach offers key advantages over existing work: we design a memory module that builds an internal map of the environment, allowing for the agent to return to previously visited locations with high visual consistency. In addition, GameGAN is able to disentangle static and dynamic components within an image making the behavior of the model more interpretable, and relevant for downstream tasks that require explicit reasoning over dynamic elements. This enables many interesting applications such as swapping different components of the game to build new games that do not exist. We will release the code and trained model, enabling human players to play generated games and their variations with our GameGAN.', 'corpus_id': 218869555, 'score': 0}, {'doc_id': '232257725', 'title': 'Gradient Projection Memory for Continual Learning', 'abstract': 'The ability to learn continually without forgetting the past tasks is a desired attribute for artificial learning systems. Existing approaches to enable such learning in artificial neural networks usually rely on network growth, importance based weight update or replay of old data from the memory. In contrast, we propose a novel approach where a neural network learns new tasks by taking gradient steps in the orthogonal direction to the gradient subspaces deemed important for the past tasks. We find the bases of these subspaces by analyzing network representations (activations) after learning each task with Singular Value Decomposition (SVD) in a single shot manner and store them in the memory as Gradient Projection Memory (GPM). With qualitative and quantitative analyses, we show that such orthogonal gradient descent induces minimum to no interference with the past tasks, thereby mitigates forgetting. We evaluate our algorithm on diverse image classification datasets with short and long sequences of tasks and report better or on-par performance compared to the state-of-the-art approaches1.', 'corpus_id': 232257725, 'score': 0}, {'doc_id': '235446704', 'title': 'Refining Language Models with Compositional Explanations', 'abstract': 'Pre-trained language models have been successful on text classification tasks, but are prone to learning spurious correlations from biased datasets, and are thus vulnerable when making inferences in a new domain. Prior works reveal such spurious patterns via post-hoc explanation algorithms which compute the importance of input features. Further, the model is regularized to align the importance scores with human knowledge, so that the unintended model behaviors are eliminated. However, such a regularization technique lacks flexibility and coverage, since only importance scores towards a pre-defined list of features are adjusted, while more complex human knowledge such as feature interaction and pattern generalization can hardly be incorporated. In this work, we propose to refine a learned language model for a target domain by collecting human-provided compositional explanations regarding observed biases. By parsing these explanations into executable logic rules, the human-specified refinement advice from a small set of explanations can be generalized to more training examples. We additionally introduce a regularization term allowing adjustments for both importance and interaction of features to better rectify model behavior. We demonstrate the effectiveness of the proposed approach on two text classification tasks by showing improved performance in target domain as well as improved model fairness after refinement1.', 'corpus_id': 235446704, 'score': 1}, {'doc_id': '215744839', 'title': 'Meta-Learning in Neural Networks: A Survey', 'abstract': 'The field of meta-learning, or learning-to-learn, has seen a dramatic rise in interest in recent years. Contrary to conventional approaches to AI where a given task is solved from scratch using a fixed learning algorithm, meta-learning aims to improve the learning algorithm itself, given the experience of multiple learning episodes. This paradigm provides an opportunity to tackle many of the conventional challenges of deep learning, including data and computation bottlenecks, as well as the fundamental issue of generalization. In this survey we describe the contemporary meta-learning landscape. We first discuss definitions of meta-learning and position it with respect to related fields, such as transfer learning, multi-task learning, and hyperparameter optimization. We then propose a new taxonomy that provides a more comprehensive breakdown of the space of meta-learning methods today. We survey promising applications and successes of meta-learning including few-shot learning, reinforcement learning and architecture search. Finally, we discuss outstanding challenges and promising areas for future research.', 'corpus_id': 215744839, 'score': 0}, {'doc_id': '232257833', 'title': 'PredRNN: A Recurrent Neural Network for Spatiotemporal Predictive Learning', 'abstract': 'The predictive learning of spatiotemporal sequences aims to generate future images by learning from the historical context, where the visual dynamics are believed to have modular structures that can be learned with compositional subsystems. This paper models these structures by presenting PredRNN, a new recurrent network, in which a pair of memory cells are explicitly decoupled, operate in nearly independent transition manners, and finally form unified representations of the complex environment. Concretely, besides the original memory cell of LSTM, this network is featured by a zigzag memory flow that propagates in both bottom-up and top-down directions across all layers, enabling the learned visual dynamics at different levels of RNNs to communicate. It also leverages a memory decoupling loss to keep the memory cells from learning redundant features. We further propose a new curriculum learning strategy to force PredRNN to learn long-term dynamics from context frames, which can be generalized to most sequence-to-sequence models. We provide detailed ablation studies to verify the effectiveness of each component. Our approach is shown to obtain highly competitive results on five datasets for both action-free and action-conditioned predictive learning scenarios.', 'corpus_id': 232257833, 'score': 0}, {'doc_id': '227254464', 'title': 'Neural Prototype Trees for Interpretable Fine-grained Image Recognition', 'abstract': ""Interpretable machine learning addresses the black-box nature of deep neural networks. Visual prototypes have been suggested for intrinsically interpretable image recognition, instead of generating post-hoc explanations that approximate a trained model. However, a large number of prototypes can be overwhelming. To reduce explanation size and improve interpretability, we propose the Neural Prototype Tree (ProtoTree), a deep learning method that includes prototypes in an interpretable decision tree to faithfully visualize the entire model. In addition to global interpretability, a path in the tree explains a single prediction. Each node in our binary tree contains a trainable prototypical part. The presence or absence of this prototype in an image determines the routing through a node. Decision making is therefore similar to human reasoning: Does the bird have a red throat? And an elongated beak? Then it's a hummingbird! We tune the accuracy-interpretability trade-off using ensembling and pruning. We apply pruning without sacrificing accuracy, resulting in a small tree with only 8 prototypes along a path to classify a bird from 200 species. An ensemble of 5 ProtoTrees achieves competitive accuracy on the CUB-200-2011 and Stanford Cars data sets. Code is available at https://github.com/M-Nauta/ProtoTree"", 'corpus_id': 227254464, 'score': 1}, {'doc_id': '232185628', 'title': 'Analyzing the Hidden Activations of Deep Policy Networks: Why Representation Matters', 'abstract': 'We analyze the hidden activations of neural network policies of deep reinforcement learning (RL) agents and show, empirically, that it’s possible to know a-priori if a state representation will lend itself to fast learning. RL agents in high-dimensional states have two main learning burdens: (1) to learn an action-selection policy and (2) to learn to discern between useful and non-useful information in a given state. By learning a latent representation of these high-dimensional states with an auxiliary model, the latter burden is effectively removed, thereby leading to accelerated training progress. We examine this phenomenon across tasks in the PyBullet Kuka environment, where an agent must learn to control a robotic gripper to pick up an object. Our analysis reveals how neural network policies learn to organize their internal representation of the state space throughout training. The results from this analysis provide three main insights into how deep RL agents learn. First, a well-organized internal representation within the policy network is a prerequisite to learning good action-selection. Second, a poor initial representation can cause an unrecoverable collapse within a policy network. Third, a good initial representation allows an agent’s policy network to organize its internal representation even before any training begins.', 'corpus_id': 232185628, 'score': 1}, {'doc_id': '378410', 'title': 'Network Dissection: Quantifying Interpretability of Deep Visual Representations', 'abstract': 'We propose a general framework called Network Dissection for quantifying the interpretability of latent representations of CNNs by evaluating the alignment between individual hidden units and a set of semantic concepts. Given any CNN model, the proposed method draws on a data set of concepts to score the semantics of hidden units at each intermediate convolutional layer. The units with semantics are labeled across a broad range of visual concepts including objects, parts, scenes, textures, materials, and colors. We use the proposed method to test the hypothesis that interpretability is an axis-independent property of the representation space, then we apply the method to compare the latent representations of various networks when trained to solve different classification problems. We further analyze the effect of training iterations, compare networks trained with different initializations, and measure the effect of dropout and batch normalization on the interpretability of deep visual representations. We demonstrate that the proposed method can shed light on characteristics of CNN models and training methods that go beyond measurements of their discriminative power.', 'corpus_id': 378410, 'score': 1}]"
49	{'doc_id': '26851', 'title': 'Calculation of dispersion energies.', 'abstract': 'We summarize the theory of van der Waals (dispersion) forces, with emphasis on recent microscopic approaches that permit the prediction of forces between solids and nanostructures right down to intimate contact and binding. Some connections are pointed out between microscopic theory and macroscopic Lifshitz theory.', 'corpus_id': 26851}	7499	"[{'doc_id': '221186749', 'title': 'Exceptional drag enhancement of electron-phonon transport properties in 3C-SiC from fully coupled ab-initio analysis', 'abstract': 'We carry out novel ab-initio calculations of fully coupled electron and phonon transport and show that mutual drag causes the thermopower to be dominated by transport of phonons, rather than electrons, at room temperature in the case of \\textit{n}-doped 3C-SiC. The thermopower is insensitive to impurity scattering. Phonon drag also strongly boosts the intrinsic electron mobility, thermal conductivity and the Lorenz number. This work establishes the roles of microscopic scattering mechanisms in the emergence of strong drag effects in transport of the interacting electron-phonon gas.', 'corpus_id': 221186749, 'score': 0}, {'doc_id': '221507609', 'title': 'Strong zero-field Förster resonances in K-Rb Rydberg systems', 'abstract': 'We study resonant dipole-dipole coupling and the associated van der Waals energy shifts in Rydberg excited atomic rubidium and potassium and investigate Forster resonances between interspecies pair states. A comprehensive survey over experimentally accessible pair state combinations reveals multiple candidates with small Forster defects. We crucially identify the existence of an ultrastrong, ""low"" electric field K-Rb Forster resonance with a extremely large zero-field crossover distance exceeding 100 $\\mu$m between the van der Waals regime and the resonant regime. This resonance allows for a strong interaction over a wide range of distances and by investigating its dependence on the strength and orientation of external fields we show this to be largely isotropic. As a result, the resonance offers a highly favorable setting for studying long-range resonant excitation transfer and entanglement generation between atomic ensembles in a flexible geometry. The two-species K-Rb system establishes a unique way of realizing a Rydberg single-photon optical transistor with a high-input photon rate and we specifically investigate an experimental scheme with two separate ensembles.', 'corpus_id': 221507609, 'score': 0}, {'doc_id': '233131239', 'title': 'Strong Coupling, Hyperbolic Metamaterials and Optical Tamm States in Layered Dielectric-Plasmonic Media', 'abstract': 'Thin films of noble metals with thickness smaller than the wavelength of light constitute one of the most investigated structures in plasmonics. The fact that surface plasmon modes can be excited in these films by different ways and the simplicity of fabrication offer ideal conditions for applications in nanophotonics. The generation of optical modes in coupled Fabry-Pérot planar cavities and their migration to hyperbolic metamaterials is investigated. Coupled Fabry-Pérot cavities behave as simple coupled resonators. When the intra-cavity media have different refractive indices in two or more coupled cavities resonance anti-crossings arise. The application of this kind of strong coupling in sensing is foreseen. Beyond the cavity modes excited by propagating waves, also long range plasmonic guided modes can be excited using emitters or evanescent waves. A periodic structure made by multiple plasmonic films and dielectrica supports bulk plasmons, of large propagation constant and increasing field amplitude. The optical response of these structures approaches that of the hyperbolic metamaterial predicted by the effective medium theory. Light can propagate with full transmission in a structure made of a photonic crystal based on quarter wavelength layers and a second photonic crystal with an overlapping forbidden band, but presenting a non-trivial topological phase achieved by band inversion. This is due to excitation of optical Tamm states at the boundary between both crystals. The extension to multiple optical Tamm states using dielectric and plasmonic materials and the symmetries of the edge states is investigated.', 'corpus_id': 233131239, 'score': 0}, {'doc_id': '232428377', 'title': 'Controlling and focusing of in-plane hyperbolic phonon polaritons in {\\alpha}-MoO3 with plasmonic antenna', 'abstract': 'Hyperbolic phonon polaritons (HPhPs) sustained in van der Waals (vdW) materials exhibit extraordinary capabilities of confining long-wave electromagnetic fields to the deep subwavelength scale. In stark contrast to the uniaxial vdW hyperbolic materials such as hexagonal boron nitride (hBN), the recently emerging biaxial hyperbolic materials such as α-MoO3 and α-V2O5 further bring new degree of freedoms in controlling light at the flatland, due to their distinctive in-plane hyperbolic dispersion. However, the controlling and focusing of such in-plane HPhPs are to date remain elusive. Here, we propose a versatile technique for launching, controlling and focusing of in-plane HPhPs in α-MoO3 with geometrically designed plasmonic antennas. By utilizing high resolution near-field optical imaging technique, we directly excited and mapped the HPhPs wavefronts in real space. We find that subwavelength manipulating and focusing behavior are strongly dependent on the curvature of antenna extremity. This strategy operates effectively in a broadband spectral region. These findings can not only provide fundamental insights into manipulation of light by biaxial hyperbolic crystals at nanoscale, but also open up new opportunities for planar nanophotonic applications.', 'corpus_id': 232428377, 'score': 0}, {'doc_id': '233004698', 'title': 'Influence of collective scattering of light on electromagnetic induced transparency', 'abstract': 'Here we present a microscopic model that describes the Electromagnetically Induced Transparency (EIT) phenomenon in the multiple scattering regime. We consider an ensembles of cold three-level atoms, in a Λ configuration, scattering a probe and a control field to the vacuum modes of the electromagnetic field. By first considering a scalar description of the scattering, we show that the light-mediated long-range interactions that emerge between the dipoles narrow the EIT transparency window for increasing densities and sample sizes. For a vectorial description, we demonstrate that near-field interacting terms can critically affect the atomic population transfer in the Stimulated Raman Adiabatic Passage (STIRAP). This result points out that standard STIRAP-based quantum memories in dense and cold atomic ensembles would not reach efficiency high enough for quantum information processing applications.', 'corpus_id': 233004698, 'score': 1}, {'doc_id': '221246424', 'title': 'Vacuum-Induced Saturation in Plasmonic Nanoparticles', 'abstract': 'Vacuum fluctuations are a fundamental feature of quantized fields. It is usually assumed that observations connected to vacuum fluctuations require a system well isolated from other influences. In this work, we demonstrate that effects of the quantum vacuum can already occur in simple colloidal nano-assemblies prepared by wet chemistry. We claim that the electromagnetic field fluctuations at the zero-point level saturate the absorption of dye molecules self-assembled at the surface of plasmonic nano-resonators. For this effect to occur, reaching the strong coupling regime between the plasmons and excitons is not required. This intriguing effect of vacuum-induced saturation (VISA) is discussed within a simple quantum optics picture and demonstrated by comparing the optical spectra of hybrid gold-core dye-shell nanorods to electromagnetic simulations.', 'corpus_id': 221246424, 'score': 1}, {'doc_id': '233407664', 'title': 'Emerging dissipative phases in a superradiant quantum gas with tunable decay', 'abstract': 'Exposing a many-body system to external drives and losses can transform the nature of its phases and opens perspectives for engineering new properties of matter. How such characteristics are related to the underlying microscopic processes of the driven and dissipative system is a fundamental question. Here we address this point in a quantum gas that is strongly coupled to a lossy optical cavity mode using two independent Raman drives, which act on the spin and motional degrees of freedom of the atoms. This setting allows us to control the competition between coherent dynamics and dissipation by adjusting the imbalance between the drives. For strong enough coupling, the transition to a superradiant phase occurs, as is the case for a closed system. Yet, by imbalancing the drives we can enter a dissipation-stabilized normal phase and a region of multistability. Measuring the properties of excitations on top of the out-of-equilibrium phases reveals the microscopic elementary processes in the open system. Our findings provide prospects for studying squeezing in non-Hermitian systems, quantum jumps in superradiance, and dynamical spin-orbit coupling in a dissipative setting.', 'corpus_id': 233407664, 'score': 1}, {'doc_id': '232290472', 'title': 'Strong light-matter coupling in \nMoS2', 'abstract': 'Polariton-based devices require materials where light-matter coupling under ambient conditions exceeds losses, but our current selection of such materials is limited. Here we measured the dispersion of polaritons formed by the A and B excitons in thin MoS2 slabs by imaging their optical near fields. We combined fully tunable laser excitation in the visible with a scattering near-field optical microscope to excite polaritons and image their optical near fields. We obtained the properties of bulk MoS2 from fits to the slab dispersion. The in-plane excitons are in the strong regime of light-matter coupling with a coupling strength (40− 100 meV) that exceeds their losses by at least a factor of two. The coupling becomes comparable to the exciton binding energy, which is known as very strong coupling. MoS2 and other transition metal dichalcogenides are excellent materials for future polariton devices. 1 ar X iv :2 10 3. 10 86 7v 1 [ co nd -m at .m tr lsc i] 1 9 M ar 2 02 1', 'corpus_id': 232290472, 'score': 0}, {'doc_id': '226289797', 'title': 'Motional quantum states of surface electrons on liquid helium in a tilted magnetic field', 'abstract': 'The theoretical model developed by Jaynes and Cummings to describe interaction between light and a two-level atom became a paradigm of atomic and optical physics. Here, the authors demonstrate that this model can be realized in a pure condensed matter system of electrons trapped on the surface of liquid helium. In particular, they show that the motional states of such an electron, when it is placed in a strong magnetic field, realize a prototypical quantum system that exhibits a variety of phenomena common to the light dressed states of atomic and molecular systems.', 'corpus_id': 226289797, 'score': 1}, {'doc_id': '231933646', 'title': 'Plasmonic Waveguides to Enhance Quantum Electrodynamic Phenomena at the Nanoscale', 'abstract': '—The emerging field of plasmonics can lead to enhanced light-matter interactions at extremely nanoscale regions. Plasmonic (metallic) devices promise to efficiently control both classical and quantum properties of light. Plasmonic waveguides are usually used to excite confined electromagnetic modes at the nanoscale that can strongly interact with matter. The analysis of these nanowaveguides exhibits similarities with their low frequency microwave counterparts. In this article, we review ways to study plasmonic nanostructures coupled to quantum optical emitters from a classical electromagnetic perspective. These quantum emitters are mainly used to generate single-photon quantum light that can be employed as a quantum bit or “qubit’’ in the envisioned quantum information technologies. We demonstrate different ways to enhance a diverse range of quantum electrodynamic phenomena based on plasmonic configurations by using the classical dyadic tensor Green’s function formalism. More specifically, spontaneous emission and superradiance are analyzed by using the Green’s function-based field quantization. The exciting new field of quantum plasmonics will lead to a plethora of novel optical devices for communications and computing applications operating in the quantum realm, such as efficient single-photon sources, quantum sensors, and compact onchip nanophotonic circuits.', 'corpus_id': 231933646, 'score': 1}]"
50	{'doc_id': '235470492', 'title': 'Rapid detection of cellulose and hemicellulose contents of corn stover based on near-infrared spectroscopy combined with chemometrics.', 'abstract': 'The feasibility of near-infrared spectroscopy (NIRS) combined with chemometrics for the rapid detection of the cellulose and hemicellulose contents in corn stover is discussed. Competitive adaptive reweighted sampling (CARS) and genetic simulated annealing algorithm (GSA) were combined (CARS-GSA) to select the characteristic wavelengths of cellulose and hemicellulose and to reduce the dimensionality and multicollinearity of the NIRS data. The whole spectra contained 1845 wavelength variables. After CARS-GSA optimization, the number of characteristic wavelengths of cellulose (hemicellulose) was reduced to 152 (260), accounting for 8.24% (14.09%) of all wavelengths. The coefficients of determination of the regression models for predicting the cellulose and hemicellulose contents were 0.968 and 0.996, the root mean square errors of prediction (RMSEPs) were 0.683 and 0.648, and the residual predictive deviations (RPDs) were 5.213 and 16.499, respectively. The RMSEP of the cellulose and hemicellulose regression models was 0.152 and 0.190 lower for CARS-GSA than for the full-spectrum, and the RPD was increased by 0.949 and 3.47, respectively. The results showed that the CARS-GSA model substantially reduced the number of characteristic wavelengths and significantly improved the predictive ability of the regression model.', 'corpus_id': 235470492}	17836	[{'doc_id': '234314316', 'title': 'Organic fertilizer from agricultural waste: determination of phosphorus content using near infrared reflectance', 'abstract': 'Agricultural waste can be converted onto useful materials like compost or organic fertilizers. In a simply practice, those wastes were kept and mixed with additional composting materials to enrich the fertilizer nutrients and compositions. Generally, plants can growth optimally in sufficient media, that is soil. It requires adequate micro and macro nutrients like phosphorus (P). in order to determine P content and other nutrient properties, many methods have been widely used from which most of them are wet chemical analysis. The main aim of this present study is to employ the near infrared reflectance (NIRS) technique in determining P content of organic fertilizer. Spectra data were generated in wavenumbers 5000 – 10 000 cm−1 and the model were established using principal component regression (PCR) method. The results showed that P content of compost materials can be determined using NIRS with maximum correlation coefficient 0.99 and robustness index 4.14 respectively.', 'corpus_id': 234314316, 'score': 0}, {'doc_id': '233472417', 'title': 'Assessment of a Non-Destructive Method for Rapid Discrimination of Moroccan Date Palm Varieties via Mid-Infrared Spectroscopy Combined with Chemometric Models.', 'abstract': 'BACKGROUND\nMorocco is an important world producer and consumer of several varieties of date palm. In fact, the discrimination between varieties remains difficult and requires the use of complex and high-cost techniques.\n\n\nOBJECTIVE\nWe evaluated in this work the potential of mid-infrared spectroscopy (MIR) and chemometric models to discriminate eight date palm varieties.\n\n\nMETHODS\nFour chemometric models were applied for the analysis of the spectral data, including principal component analysis (PCA), support vector machine discriminant analysis (SVM-DA), linear discriminant analysis (LDA) and partial least squares (PLS). MIR spectroscopic data were recorded from the wavenumber range 4000 - 600\u2009cm-1, with a spectral resolution of 4\u2009cm-1.\n\n\nRESULTS\nThe discriminant analysis was performed by LDA and SVM-DA with a 100% correct classification rate for the date mesocarp. Partial least-squares was applied as a complementary chemometric tool aimed at quantifying moisture content, the validation of this model shows a good predictive capacity with a regression coefficient of 84% and a root mean square error of cross-validation of 0.50.\n\n\nCONCLUSIONS\nThe present study clearly demonstrates that MIR spectroscopy combined with chemometric approaches constitutes a promising analytical method to classify date palms according to their varietal origin and to establish a regression model for predicting moisture content.\n\n\nHIGHLIGHTS\nAlternative analytical method to discriminate of date palm cultivars by FTIR-ATR spectroscopy coupled with chemometric approaches.', 'corpus_id': 233472417, 'score': 0}, {'doc_id': '235265725', 'title': 'Hyperspectral Band Selection for Multispectral Image Classification with Convolutional Networks', 'abstract': 'In recent years, Hyperspectral Imaging (HSI) has become a powerful source for reliable data in applications such as remote sensing, agriculture, and biomedicine. However, hyperspectral images are highly data-dense and often benefit from methods to reduce the number of spectral bands while retaining the most useful information for a specific application. We propose a novel band selection method to select a reduced set of wavelengths, obtained from an HSI system in the context of image classification. Our approach consists of two main steps: the first utilizes a filter-based approach to find relevant spectral bands based on a collinearity analysis between a band and its neighbors. This analysis helps to remove redundant bands and dramatically reduces the search space. The second step applies a wrapper-based approach to select bands from the reduced set based on their information entropy values, and trains a compact Convolutional Neural Network (CNN) to evaluate the performance of the current selection. We present classification results obtained from our method and compare them to other feature selection methods on two hyperspectral image datasets. Additionally, we use the original hyperspectral data cube to simulate the process of using actual filters in a multispectral imager. We show that our method produces more suitable results for a multispectral sensor design.', 'corpus_id': 235265725, 'score': 0}, {'doc_id': '233548276', 'title': 'Method and system for nondestructive detection of freshness in Penaeus vannamei based on hyperspectral technology', 'abstract': 'Abstract To test the freshness of Penaeus vannamei rapidly and non-destructively, we constructed a quantitative prediction model for total volatile basic nitrogen (TVB-N) content by collecting 860–1700\xa0nm hyperspectral data and TVB-N content of samples of P. vannamei during eight consecutive days. In this study, we compared the influence of six kinds of spectral pretreatment on the partial least-squares regression modeling (PLSR), which included multiple scattering correction (MSC), standard normal variate transformation (SNV), normalized mean centering, 11-point moving-average smoothing, and Savitzky–Golay smoothing. The results demonstrate that the prediction effect of the MSC-PLSR model has the best relative model accuracy, under the pretreatment of the MSC algorithm for spectral data. In addition, principal component analysis (PCA) and the Mahalanobis distance algorithm were used to remove anomalous samples. The results reveal that the prediction effect of the MSC-PLSR model is better than other models that we considered, and the correlation coefficient (R) of the validation set reached 0.8147. Finally, this study developed a graphical user interface system to predict freshness, based on our research, which provides a method and an analysis tool to rapidly detect the freshness of P. vannamei.', 'corpus_id': 233548276, 'score': 1}, {'doc_id': '233486524', 'title': 'argoFloats: An R Package for Analyzing Argo Data', 'abstract': 'An R package named argoFloats has been developed to facilitate identifying, downloading, caching, and analyzing oceanographic data collected by Argo profiling floats. The analysis phase benefits from close connections between argoFloats and the oce package, which is likely to be familiar to those who already use R for the analysis of oceanographic data of other kinds. This paper outlines how to use argoFloats to accomplish some everyday tasks that are particular to Argo data, ranging from downloading data and finding subsets to handling quality control and producing a variety of diagnostic plots. The benefits of the R environment are sketched in the examples, and also in some notes on the future of the argoFloats package.', 'corpus_id': 233486524, 'score': 0}, {'doc_id': '234683816', 'title': 'Using Diffuse Reflectance Spectroscopy as a High Throughput Method for Quantifying Soil C and N and Their Distribution in Particulate and Mineral-Associated Organic Matter Fractions', 'abstract': 'Large-scale quantification of soil organic carbon (C) and nitrogen (N) stocks and their distribution between particulate (POM) and mineral-associated (MAOM) organic matter is deemed necessary to develop land management strategies to mitigate climate change and sustain food production. To this end, diffuse reflectance mid-infrared spectroscopy (MIR) coupled with partial least square (PLS) analysis has been proposed as a promising method because of its low labor and cost, high throughput and the potential to estimate multiple soil attributes. In this paper, we applied MIR spectroscopy to predict C and N content in bulk soils, and in POM and MAOM, as well as soil properties influencing soil C storage. A heterogeneous dataset including 349 topsoil samples were collected under different soil types, land use and climate conditions across the European Union and the United Kingdom. The samples were analyzed for various soil properties to determine the feasibility of developing MIR-based predictive calibrations. We obtained accurate predictions for total soil C and N content, MAOM C and N content, pH, clay, and sand (R2> 0.7; RPD>1.8). In contrast, POM C and N content were predicted with lower accuracies due to non-linear dependencies, suggesting the need for additional calibration across similar soils. Furthermore, the information provided by MIR spectroscopy was able to differentiate spectral bands and patterns across different C pools. The strength of the correlation between C pools, minerals, and C functional groups was land use-dependent, suggesting that the use of this approach for long-term soil C monitoring programs should use land-use specific calibrations.', 'corpus_id': 234683816, 'score': 1}, {'doc_id': '233568599', 'title': 'Generalisation of tea moisture content models based on VNIR spectra subjected to fractional differential treatment', 'abstract': 'Model generalisation for the detection of tea moisture content was investigated across different leaf surface orientations and tea varieties in this study. The micromorphology of leaves plucked from three tea bushes was analysed, and differences between different surface orientations and varieties were observed. The VNIR spectra (350–2500\xa0nm) of the leaves were collected and analysed. Excellent prediction performance was obtained for moisture detection models based on spectra for the same leaf surface orientation and variety. By contrast, the prediction performance decreased severely if the test spectra were obtained for different leaf surface orientations and varieties. To solve this issue, differential treatments with fractional order between 0 and 2 were carried out on the spectra. The results showed that the prediction performance improved for generalisation between varieties and orientations, especially for orders of 0.4 or 0.6. The mechanism by which the fractional differential treatment mines the common information from the spectra with varying characteristics was elucidated by calculating the correlation coefficients between the moisture content and the spectra treated with different differential orders. The results of this study advance tea moisture detection based on VNIR spectra and the ability to generalise across spectra with different characteristics.', 'corpus_id': 233568599, 'score': 1}, {'doc_id': '234949295', 'title': 'Erratum to: ESPEI for efficient thermodynamic database development, modification, and uncertainty quantification: application to Cu–Mg—CORRIGENDUM', 'abstract': 'In regards to the original publication of this article[1], the authors would like to correct the following: the opening sentence of the caption for Figure 5a should read “Corner plot (a) of the parameters in the liquid phase.”', 'corpus_id': 234949295, 'score': 0}, {'doc_id': '234477393', 'title': 'SELECTION OF SNP MARKERS: ANALYZING GAW17 DATA USING DIFFERENT METHODOLOGIES', 'abstract': 'The quantity and complexity of generated data due to advances in genetic sequencing technologies has made statistical analysis an essential tool for their correct study and interpretation. However, there is still no agreement about which methodologies are more appropriate for those data, especially for the selection of genetic features that influence a specific phenotype. Genetic data are usually characterized by having a number of variables which is much greater than the number of observations. These variables exhibit little variability and high correlation. These characteristics hinder the application of traditional methodologies for variable selection. In this work (i.) we present different methodologies for selecting variables Random Forest, LASSO and the traditional Stepwise method; (ii.) we apply them to genetic data to select SNP markers that characterize the presence or absence of a disease and (iii.) we compare their performances. Random Forest and Lasso show similar prediction performance, however none of them correctly select the relevant SNPs.', 'corpus_id': 234477393, 'score': 0}, {'doc_id': '233538574', 'title': 'Using autoencoders to compress soil VNIR–SWIR spectra for more robust prediction of soil properties', 'abstract': 'Abstract In the past two decades, research efforts have focused on using near infrared diffuse reflectance spectroscopy (350–2500\xa0nm) as a rapid and cost-effective method for soil analysis. Given the multi-faceted importance of the soil ecosystem, and considering the increasing pressures exerted upon it due to climate change, degradation and urbanization, the advantages of soil spectroscopy are significant. Large soil spectral libraries have been developed to this end throughout the world. The soil matrix is however complex, posing a challenge in the determination of key soil properties from the spectra. To tackle this challenge, two methodologies are generally used: a) the use of spectral pre-processing techniques to transfer the spectra into a new space in which the association between spectrum and soil property is supposed to be more clear, and b) the use of more sophisticated machine learning models (e.g. deep learning). In this paper, we propose a novel methodology using stacked autoencoders to transform the initially recorded spectra in a new compressed (i.e. latent) space which can help the chemometric models enhance the accuracy of prediction. This is an unsupervised learning approach which only depends on the input data (i.e. the spectra). Following the significant results obtained in the literature using combinations of different spectra pre-processing techniques and the simultaneous prediction of multiple soil properties, the proposed methodology is extended to facilitate these approaches. We demonstrate this capacity by applying it in the mineral samples of the LUCAS 2009 topsoil database, and simultaneously predicting eight properties (the particle size distribution, pH, CEC, organic carbon, calcium carbonate, and total nitrogen) using an artificial neural network. Compared to standard pre-processing techniques and other transformations such as the principal components space, the proposed methodology using only one spectral source as input decreases the RMSE on average by 8.4% and by 3.5%, respectively. With respect to the current state-of-the-art and in particular a multi-input convolutional neural network which was recently proposed and outperformed the compared methodologies, the results of our multi-input methodology exhibit an average RMSE decrease of 9.9%. The interpretability aspect of the transformed feature space and the compressed spectra was also examined to identify how the compressed information encodes the input data and enables better associations between input and output.', 'corpus_id': 233538574, 'score': 1}]
51	{'doc_id': '6045196', 'title': 'Vmgen—a generator of efficient virtual machine interpreters', 'abstract': 'In a virtual machine interpreter, the code for each virtual machine instruction has similarities to code for other instructions. We present an interpreter generator that takes simple virtual machine instruction descriptions as input and generates C code for processing the instructions in several ways: execution, virtual machine code generation, disassembly, tracing, and profiling. The generator is designed to support efficient interpreters: it supports threaded code, aching the top‐of‐stack item in a register, combining simple instructions into superinstructions, and other optimizations. We have used the generator to create interpreters for Forth and Java. Theresulting interpreters are faster than other interpreters for the same languages and they are typically 2–10 times slower than code produced by native‐code compilers. We also present results for the effects of the individual optimizations supported by the generator. Copyright © 2002 John Wiley & Sons, Ltd', 'corpus_id': 6045196}	17037	"[{'doc_id': '233412265', 'title': 'Sylkan: Towards a Vulkan Compute Target Platform for SYCL', 'abstract': 'SYCL is a modern high-level C++ programming interface which excels at expressing data parallelism for heterogeneous hardware platforms in a programmer-friendly way, and is standardized by the Khronos Group. The latest version of the standard, SYCL 2020, removes the previous dependence of the specification and its implementations on an underlying OpenCL target, opening the door for compliant alternative implementations. In this paper, we discuss the opportunities and challenges of mapping SYCL to Vulkan, a low-level explicit programming model for GPUs. This includes an analysis of the potential semantic mismatch between each respective standard, as well as approaches to work around some of these issues. Additionally, we present a prototype research implementation of Sylkan, a SYCL compiler and runtime targeting Vulkan. In order to evaluate our prototype qualitatively and quantitatively, we chose a variety of functional tests as well as three performance benchmarks. For the functional tests, we discuss and categorize the failures of the current prototype, noting which semantic mismatch or missing implementation causes them. For the performance benchmarks, we compare execution times against a OpenCL-based SYCL implementation and a native Vulkan version of each benchmark, on two hardware platforms.', 'corpus_id': 233412265, 'score': 0}, {'doc_id': '36436182', 'title': 'An assembler and disassembler framework for JavaTMprogrammers', 'abstract': 'Abstract The Java™\xa0programming language is primarily used for platform-independent programming. Yet it also offers many productivity, maintainability and performance benefits for platform-specific functions, such as the generation of machine code. We have created reliable assemblers for SPARC™\xa0, AMD64, IA32 and PowerPC which support all user mode and privileged instructions and with 64\xa0bit mode support for all but the latter. These assemblers are generated as Java source code by our extensible assembler framework, which itself is written in the Java language. The assembler generator also produces javadoc comments that precisely specify the legal values for each operand. Our design is based on the Klein Assembler System written in Self. Assemblers are generated from a specification, as are table-driven disassemblers and unit tests. The specifications that drive the generators are expressed as Java language objects. Thus no extra parsers are needed and developers do not need to learn any new syntax to extend the framework for additional ISAs. Every generated assembler is tested against a preexisting assembler by comparing the output of both. Each instruction’s test cases are derived from the cross product of its potential operand values. The majority of tests are positive (i.e., result in a legal instruction encoding). The framework also generates negative tests, which are expected to cause an error detection by an assembler. As with the Klein Assembler System, we have found bugs in the external assemblers as well as in ISA reference manuals. Our framework generates tens of millions of tests. For symbolic operands, our tests include all applicable predefined constants. For integral operands, the important boundary values, such as the respective minimum, maximum, 0, 1 and −1, are tested. Full testing can take hours to run but gives us a high degree of confidence regarding correctness.', 'corpus_id': 36436182, 'score': 1}, {'doc_id': '61964282', 'title': 'Automatic Code Generation using Dynamic Programming Techniques', 'abstract': 'Building compiler back ends from declarative specifications that map tree structured intermediate representations onto target machine code is the topic of this thesis. Although many tools and approaches have been devised to tackle the problem of automated code generation, there is still room for improvement. In this context we present Hburg, an implementation of a code generator generator that emits compiler back ends from concise tree pattern specifications written in our code generator description language. The language features attribute grammar style specifications and allows for great flexibility with respect to the placement of semantic actions. Our main contribution is to show that these language features can be integrated into automatically generated code generators that perform optimal instruction selection based on tree pattern matching combined with dynamic programming. In order to substantiate claims about the usefulness of our language we provide two complete examples that demonstrate how to specify code generators for Risc and Cisc architectures.', 'corpus_id': 61964282, 'score': 1}, {'doc_id': '234762891', 'title': 'HeapSafe: Securing Unprotected Heaps in RISC-V', 'abstract': 'RISC-V is a promising open-source architecture primarily targeted for embedded systems. Programs compiled using the RISC-V toolchain can run bare-metal on the system, and, as such, can be vulnerable to several memory corruption vulnerabilities. In this work, we present HeapSafe, a lightweight hardware assisted heap-buffer protection scheme to mitigate heap overflow and use-after-free vulnerabilities in a RISC-V SoC. The proposed scheme tags pointers associated with heap buffers with metadata indices and enforces tag propagation for commonly used pointer operations. The HeapSafe hardware is decoupled from the core and is designed as a configurable coprocessor and is responsible for validating the heap buffer accesses. Benchmark results show a 1.5X performance overhead and 1.59% area overhead, while being 22% faster than a software protection. We further implemented a HeapSafe-nb, an asynchronous validation design, which improves performance by 27% over the synchronous HeapSafe.', 'corpus_id': 234762891, 'score': 0}, {'doc_id': '17985041', 'title': 'The New Jersey Machine-Code Toolkit', 'abstract': ""The New Jersey Machine-Code Toolkit helps programmers write applications that process machine code. Applications that use the toolkit are written at an assembly-language level of abstraction, but they recognize and emit binary. Guided by a short instruction-set specification, the toolkit generates all the bit-manipulating code. \n \nThe toolkit's specification language uses four concepts: fields and tokens describe parts of instructions, patterns describe binary encodings of instructions or groups of instructions, and constructors map between the assembly-language and binary levels. These concepts are suitable for describing both CISC and RISC machines; we have written specifications for the MIPS R3000, SPARC, and Intel 486 instruction sets. \n \nWe have used the toolkit to help write two applications: a debugger and a linker. The toolkit generates efficient code; for example, the linker emits binary up to 15% faster than it emits assembly language, making it 1.7-2 times faster to produce an a.out directly than by using the assembler."", 'corpus_id': 17985041, 'score': 1}, {'doc_id': '6647104', 'title': 'Survey on Instruction Selection: An Extensive and Modern Literature Review', 'abstract': 'Instruction selection is one of three optimization problems – the other two areinstruction scheduling and register allocation – involved in codegeneration. The task of the instruction selector is to transform an inputprogram from its target-independent representation into a target-specific formby making best use of the available machine instructions. Hence instructionselection is a crucial component of generating code that is both correct andruns efficiently on a specific target machine.Despite on-going research since the late 1960s, the last comprehensive survey onthis field was written more than 30 years ago. As many new approaches andtechniques have appeared since its publication, there is a need for anup-to-date review of the current body of literature; this report addresses thatneed by presenting an extensive survey and categorization of both dated methodand the state-of-the-art of instruction selection. The report thereby supersedesand extends the previous surveys, and attempts to identify where future researchcould be directed.', 'corpus_id': 6647104, 'score': 1}, {'doc_id': '233747369', 'title': 'Arranging for Safety Checks with Hardware Traps', 'abstract': 'Many programming languages are untyped and perform all type checks at run-time. This requires that type information for objects is kept around for as long as the objects are live. One way of achieving this involves tagging all pointers with the type of the object to which they refer. The compiler inserts safety checks before all object references in order to provide safe semantics and error reporting. The run-time safety checks can however be expensive and compilers sometimes provide an option to use unsafe semantics. The resulting programs are faster and more compact, but there are some obvious drawbacks. This paper describes a technique whereby a compiler can generate code that arranges for the processor to perform safety checks. The technique is usable on commodity hardware.', 'corpus_id': 233747369, 'score': 0}, {'doc_id': '595407', 'title': 'Automatic Porting of Binary File Descriptor Library', 'abstract': 'Since software is playing an increasingly important role in system-on-chip, retargetable compilation has been an active research area in the last few years. However, the retargetting of equally important downstream system tools, such as assemblers, linkers and debuggers, has either been ignored, or falls short of production quality due to the complexity involved in these tools. In this paper, we present a technique that can automatically retarget the GNU BFD library, the foundation library for a suite of binary tools. Other than having all the advantages enjoyed by open-source software by aligning to a de facto standard, our technique is systematic, as a result of using a formal model of abstract binary interface (ABI) as a new element of architectural model; and simple, as a result of leveraging free software to the largest extent.', 'corpus_id': 595407, 'score': 1}]"
52	{'doc_id': '31876092', 'title': 'Detecting non-Abelian statistics with an electronic Mach-Zehnder interferometer.', 'abstract': 'Fractionally charged quasiparticles in the quantum Hall state with a filling factor nu=5/2 are expected to obey non-Abelian statistics. We demonstrate that their statistics can be probed by transport measurements in an electronic Mach-Zehnder interferometer. The tunneling current through the interferometer exhibits a characteristic dependence on the magnetic flux and a nonanalytic dependence on the tunneling amplitudes which can be controlled by gate voltages.', 'corpus_id': 31876092}	2871	"[{'doc_id': '221186812', 'title': 'Emergent particles and gauge fields in quantum matter', 'abstract': ""ABSTRACT I give a pedagogical introduction to some of the many particles and gauge fields that can emerge in correlated matter. The standard model of materials is built on Landau's foundational principles: adiabatic continuity and spontaneous symmetry breaking. These ideas lead to quasiparticles that inherit their quantum numbers from fundamental particles, Nambu-Goldstone bosons, the Anderson-Higgs mechanism, and topological defects in order parameters. I then describe the modern discovery of physics beyond the standard model. Here, quantum correlations (entanglement) and topology play key roles in defining the properties of matter. This can lead to fractionalised quasiparticles that carry only a fraction of the quantum numbers that define fundamental particles. These particles can have exotic properties: for example Majorana fermions are their own antiparticles, anyons have exchange statistics that are neither bosonic nor fermionic, and magnetic monopoles do not occur in the vacuum. Gauge fields emerge naturally in the description of highly correlated matter and can lead to gauge bosons. Relationships to the standard model of particle physics are discussed."", 'corpus_id': 221186812, 'score': 1}, {'doc_id': '212725667', 'title': 'Topological quantum control: Edge currents via Floquet depinning of skyrmions in the \nν=0\n graphene quantum Hall antiferromagnet', 'abstract': 'We propose a defect-to-edge topological quantum quench protocol that can efficiently inject electric charge from defect-core states into a chiral edge current of an induced Chern insulator. The initial state of the system is assumed to be a Mott insulator, with electrons bound to topological defects that are pinned by disorder. We show that a ""critical quench"" to a Chern insulator mass of order the Mott gap shunts charge from defects to the edge, while a second stronger quench can trap it there and boost the edge velocity, creating a controllable current. We apply this idea to a skyrmion charge in the $\\nu = 0$ quantum Hall antiferromagnet in graphene, where the quench into the Chern insulator could be accomplished via Floquet driving with circularly polarized light.', 'corpus_id': 212725667, 'score': 0}, {'doc_id': '51614563', 'title': 'The heat is on for Majorana fermions', 'abstract': 'Exotic particles called Majorana fermions have potential applications in quantum computing, but their existence has yet to be definitively confirmed. Two groups have now glimpsed these particles.Exotic particles called Majorana fermions have potential applications in quantum computing, but their existence has yet to be definitively confirmed. Two groups have now glimpsed these particles.', 'corpus_id': 51614563, 'score': 1}, {'doc_id': '119104398', 'title': 'Beyond paired quantum Hall states: parafermions and incompressible states in the first excited Landau level', 'abstract': 'The Pfaffian quantum Hall states, which can be viewed as involving pairing either of spin-polarized electrons or of composite fermions, are generalized by finding the exact ground states of certain Hamiltonians with k+1-body interactions, for all integers k > 0. The remarkably simple wavefunctions of these states involve clusters of k particles, and are related to correlators of parafermion currents in two-dimensional conformal field theory. The k=2 case is the Pfaffian. For k > 1, the quasiparticle excitations of these systems are expected to possess nonabelian statistics, like those of the Pfaffian. For k=3, these ground states have large overlaps with the ground states of the (2-body) Coulomb-interaction Hamiltonian for electrons in the first excited Landau level at total filling factors \\nu=2+3/5, 2+2/5.', 'corpus_id': 119104398, 'score': 1}, {'doc_id': '215814308', 'title': 'Fluctuation-dissipation relations for strongly correlated out-of-equilibrium circuits', 'abstract': 'We consider strongly correlated quantum circuits where a dc drive is added on top of an initial out-of-equilibrium (OE) stationary state. Within a perturbative approach, we derive unifying OE fluctuation relations for high frequency current noise, shown to be completely determined by zero-frequency noise and dc current. We apply them to the fractional quantum Hall effect at arbitrary incompressible filling factors, driven by OE sources, without knowledge of the underlying model. We show that such OE relations provide robust methods for an unambiguous determination of the fractional charge or of key interaction parameters entering in the exploration of anyonic statistics within an anyon collider.', 'corpus_id': 215814308, 'score': 0}, {'doc_id': '211043771', 'title': 'Spin Reversal of a Quantum Hall Ferromagnet at a Landau Level Crossing.', 'abstract': 'When Landau levels (LLs) become degenerate near the Fermi energy in the quantum Hall regime, interaction effects can drastically modify the electronic ground state. We study the quantum Hall ferromagnet formed in a two-dimensional hole gas around the LL filling factor ν=1 in the vicinity of a LL crossing in the heave-hole valence band. Cavity spectroscopy in the strong-coupling regime allows us to optically extract the spin polarization of the two-dimensional hole gas. By analyzing this polarization as a function of hole density and magnetic field, we observe a spin flip of the ferromagnet. Furthermore, the depolarization away from ν=1 accelerates close to the LL crossing. This is indicative of an increase in the size of skyrmion excitations as the effective Zeeman energy vanishes at the LL crossing.', 'corpus_id': 211043771, 'score': 0}, {'doc_id': '212725132', 'title': 'Fractional quantum Hall effect at \nν=2+4/9', 'abstract': 'Motivated by two independent experiments revealing a resistance minimum at the Landau level (LL) filling factor $\\nu=2+4/9$, characteristic of the fractional quantum Hall effect (FQHE) and suggesting electron condensation into a yet unknown quantum liquid, we propose that this state likely belongs in a parton sequence, put forth recently to understand the emergence of FQHE at $\\nu=2+6/13$. While the $\\nu=2+4/9$ state proposed here directly follows three simpler parton states, all known to occur in the second LL, it is topologically distinct from the Jain composite fermion (CF) state which occurs at the same $\\nu=4/9$ filling of the lowest LL. We predict experimentally measurable properties of the $4/9$ parton state that can reveal its underlying topological structure and definitively distinguish it from the $4/9$ Jain CF state.', 'corpus_id': 212725132, 'score': 0}, {'doc_id': '214668009', 'title': 'Excitonic fractional quantum Hall hierarchy in Moiré heterostructures', 'abstract': ""We consider fractional quantum Hall states in systems where two flat Chern number $C=\\pm 1$ bands are labeled by an approximately conserved 'valley' index and interchanged by time reversal symmetry. At filling factor $\\nu=1$ this setting admits an unusual hierarchy of correlated phases of excitons, neutral particle-hole pair excitations of a fully valley-polarized `orbital ferromagnet' parent state where all electrons occupy a single valley. Excitons experience an effective magnetic field due to the Chern numbers of the underlying bands. This obstructs their condensation in favor of a variety of crystalline orders and gapped and gapless liquid states. All these have the same quantized charge Hall response and are electrically incompressible, but differ in their edge structure, orbital magnetization, and hence valley and thermal responses. We explore the relevance of this scenario for Moire heterostructures of bilayer graphene on a hexagonal boron nitride substrate."", 'corpus_id': 214668009, 'score': 0}, {'doc_id': '221135808', 'title': ""It's anyon's game: the race to quantum computation"", 'abstract': 'In 1924, Satyendra Nath Bose dispatched a manuscript introducing the concept now known as Bose statistics to Albert Einstein. Bose could hardly have imagined that the exotic statistics of certain emergent particles of quantum matter would one day suggest a route to fault-tolerant quantum computation. This non-technical Commentary on ""anyons,"" namely particles whose statistics is intermediate between Bose and Fermi, aims to convey the underlying concept as well as its experimental manifestations to the uninitiated.', 'corpus_id': 221135808, 'score': 1}, {'doc_id': '215550904', 'title': 'The smallest particle collider', 'abstract': 'A new experiment finds direct evidence for anyons in a semiconductor heterostructure The standard model of particle physics classifies all elementary particles as fermions or bosons. Fermions, like electrons, avoid each other. Bosons, like photons, can bunch together. The classification rests on a fundamental spin-statistics theorem of quantum field theory and keeps these two types of particles distinct from each other. However, on page 173 of this issue, Bartolomei et al. (1) report an observation of particles that are neither bosons nor fermions.', 'corpus_id': 215550904, 'score': 1}]"
53	{'doc_id': '44099433', 'title': 'Genome-wide analysis of ionotropic receptor gene repertoire in Lepidoptera with an emphasis on its functions of Helicoverpa armigera.', 'abstract': 'The functions of the Ionotropic Receptor (IR) family have been well studied in Drosophila melanogaster, but only limited information is available in Lepidoptera. Here, we conducted a large-scale genome-wide analysis of the IR gene repertoire in 13 moths and 16 butterflies. Combining a homology-based approach and manual efforts, totally 996 IR candidates are identified including 31 pseudogenes and 825 full-length sequences, representing the most current comprehensive annotation in lepidopteran species. The phylogeny, expression and sequence characteristics classify Lepidoptera IRs into three sub-families: antennal IRs (A-IRs), divergent IRs (D-IRs) and Lepidoptera-specific IRs (LS-IRs), which is distinct from the case of Drosophila IRs. In comparison to LS-IRs and D-IRs, A-IRs members share a higher degree of protein identity and are distinguished into 16 orthologous groups in the phylogeny, showing conservation of gene structure. Analysis of selective forces on 27 orthologous groups reveals that these lepidopteran IRs have evolved under strong purifying selection (dN/dS≪1). Most notably, lineage-specific gene duplications that contribute primarily to gene number variations across Lepidoptera not only exist in D-IRs, but are present in the two other sub-families including members of IR41a, 76b, 87a, 100a and 100b. Expression profiling analysis reveals that over 80% (21/26) of Helicoverpa armigera A-IRs are expressed more highly in antennae of adults or larvae than other tissues, consistent with its proposed function in olfaction. However, some are also detected in taste organs like proboscises and legs. These results suggest that some A-IRs in H. armigera likely bear a dual function with their involvement in olfaction and gustation. Results from mating experiments show that two HarmIRs (IR1.2 and IR75d) expression is significantly up-regulated in antennae of mated female moths. However, no expression difference is observed between unmated female and male adults, suggesting an association with female host-searching behaviors. Our current study has greatly extended the IR gene repertoire resource in Lepidoptera, and more importantly, identifies potential IR candidates for olfactory, gustatory and oviposition behaviors in the cotton bollworm.', 'corpus_id': 44099433}	7660	"[{'doc_id': '220504638', 'title': 'Sexual communication in diurnal moths: behaviors and mechanisms', 'abstract': 'Butterflies and moths have substantially different daily activities; butterflies are diurnal, while moths are largely nocturnal or crepuscular. Diurnal moths are subject to different evolutionary pressures that affect several aspects of their behavior and physiology, particularly sexual communication. In this review, species of diurnal moths and the behaviors and mechanisms of their sexual communication are summarized. Diurnal moths are day–flying insects whose partner–finding strategies include visual, olfactory and auditory signals. Males of diurnal Lepidoptera find mating partners using olfactory cues (e.g., sex pheromones) over relatively long distances, or visual (e.g., compound eyes) and/or auditory cues (e.g., ears) over short distances, or even act in combination with the three types of signals. Pheromone–binding proteins and histamine and visual genes play important roles during the signal conduction of sexual communication in diurnal moths. However, the regulatory mechanisms of acoustic communication in day–flying moths are unclear. Understanding this information may help us to explore the evolution of sexual communication in Lepidoptera and to improve biotechnological control strategies against harmful day–flying moths.', 'corpus_id': 220504638, 'score': 1}, {'doc_id': '220885275', 'title': 'Rapid host response to an infection with Coronavirus. Study of transcriptional responses with Porcine Epidemic Diarrhea Virus', 'abstract': 'The transcriptional response in Vero cells (ATCC® CCL-81) infected with the coronavirus Porcine Epidemic Diarrhea Virus (PEDV) was measured by RNAseq analysis 4 and 6 hours after infection. Differential expressed genes (DEGs) in PEDV infected cells were compared to DEGs responding in Vero cells infected with Mammalian Orthoreovirus (MRV). Functional analysis of MRV and PEDV DEGs showed that MRV increased the expression level of several cytokines and chemokines (e.g. IL6, CXCL10, IL1A, CXCL8 [alias IL8]) and antiviral genes (e.g. IFI44, IFIT1, MX1, OASL), whereas for PEDV no enhanced expression was observed for these “hallmark” antiviral and immune effector genes. Pathway and Gene Ontology “enrichment analysis” revealed that PEDV infection did not stimulate expression of genes able to activate an acquired immune response, whereas MRV did so within 6h. Instead, PEDV down-regulated the expression of a set of zinc finger proteins with putative antiviral activity and enhanced the expression of the transmembrane serine protease gene TMPRSS13 (alias MSPL) to support its own infection by virus-cell membrane fusion (Shi et al, 2017, Viruses, 9(5):114). PEDV also down-regulated expression of Ectodysplasin A, a cytokine of the TNF-family able to activate the canonical NFKB-pathway responsible for transcription of inflammatory genes like IL1B, TNF, CXCL8 and PTGS2. The only 2 cytokine genes found up-regulated by PEDV were Cardiotrophin-1, an IL6-type cytokine with pleiotropic functions on different tissues and types of cells, and Endothelin 2, a neuroactive peptide with vasoconstrictive properties. Furthermore, by comprehensive datamining in biological and chemical databases and consulting related literature we identified sets of PEDV-response genes with potential to influence i) the metabolism of biogenic amines (e.g. histamine), ii) the formation of cilia and “synaptic clefts” between cells, iii) epithelial mucus production, iv) platelets activation, and v) physiological processes in the body regulated by androgenic hormones (like blood pressure, salt/water balance and energy homeostasis). The information in this study describing a “very early” response of epithelial cells to an infection with a coronavirus may provide pharmacologists, immunological and medical specialists additional insights in the underlying mechanisms of coronavirus associated severe clinical symptoms including those induced by SARS-CoV-2. This may help them to fine-tune therapeutic treatments and apply specific approved drugs to treat COVID-19 patients.', 'corpus_id': 220885275, 'score': 0}, {'doc_id': '30891403', 'title': 'Her odours make him deaf: crossmodal modulation of olfaction and hearing in a male moth', 'abstract': ""SUMMARY All animals have to cope with sensory conflicts arising from simultaneous input of incongruent data to different sensory modalities. Nocturnal activity in moths includes mate-finding behaviour by odour detection and bat predator avoidance by acoustic detection. We studied male moths that were simultaneously exposed to female sex pheromones indicating the presence of a potential mate, and artificial bat cries simulating a predation risk. We show that stimulation of one sensory modality can modulate the response to information from another, suggesting that behavioural thresholds are dynamic and depend on the behavioural context. The tendency to respond to bat sounds decreased as the quality and/or the amount of sex pheromone increased. The behavioural threshold for artificial bat cries increased by up to 40 dB when male moths where simultaneously exposed to female sex pheromones. As a consequence, a male moth that has detected the pheromone plume from a female will not try to evade an approaching bat until the bat gets close, hence incurring increased predation risk. Our results suggest that male moths' reaction to sensory conflicts is a trade-off depending on the relative intensity of the input to CNS from the two sensory modalities."", 'corpus_id': 30891403, 'score': 1}, {'doc_id': '34187895', 'title': 'Atlas of an Insect Brain', 'abstract': None, 'corpus_id': 34187895, 'score': 1}, {'doc_id': '220044488', 'title': 'Herbivore-induced activation of viral phosphatase disarms plant antiviral immunities for pathogen transmission', 'abstract': 'The survival of pathogens depends on their ability to overcome host immunity, especially arthropod-borne viruses (arboviruses) which must withstand the immune responses of both the host and the arthropod vector. Successful arboviruses often modify host immunity to accelerate pathogen transmission; however, few studies have explored the underlying mechanism. Here we report attracted herbivore infestation on the virus-infected plants promote transmission by the associated vector herbivore. This herbivore-induced defense suppression underpins a subversive mechanism used by Begomovirus, the largest genus of plant viruses, to compromise host defense for pathogen transmission. Begomovirus-infected plants accumulated βC1 proteins in the phloem where they were bound to host defense regulators, transcription factor WRKY20 and two mitogen-activated protein kinases MPK3 and MPK6. Once perceiving whitefly herbivory or endogenous secreted peptide PEP1, the plants started dephosphorylation on serine33 and stimulated βC1 protein as a phosphatase. βC1 dephosphorylated MPK3/6 and WRKY20, the latter negatively regulated salicylic acid signaling and vascular callose deposition. This viral hijacking of WRKY20 accumulated more vascular callose by which enforced whitefly prolonged salivation and phloem sap ingestion, therefore impelling more virus transmission among plants. We present a scenario in which viruses dynamically respond to the presence of their vectors, suppressing host immunity and promoting pathogen transmission only when needed.', 'corpus_id': 220044488, 'score': 0}, {'doc_id': '220715620', 'title': 'Two isoforms of the essential C. elegans Argonaute CSR-1 differentially regulate sperm and oocyte fertility through distinct small RNA classes', 'abstract': 'The C. elegans genome encodes nineteen functional Argonaute proteins that utilize 22G-RNAs, 26G-RNAs, miRNAs, or piRNAs to regulate their target transcripts. Only one of these proteins is essential under normal laboratory conditions: CSR-1. While CSR-1 has been studied in various developmental and functional contexts, nearly all studies investigating CSR-1 have overlooked the fact that the csr-1 locus encodes two isoforms. These isoforms differ by an additional 163 amino acids present in the N-terminus of CSR-1a. Using CRISPR-Cas9 genome editing to introduce GFP::3xFLAG epitopes into the long (CSR-1a) and short (CSR-1b) isoforms of CSR-1, we identified differential expression patterns for the two isoforms. CSR-1a is expressed specifically during spermatogenesis and in select somatic tissues, including the intestine. In contrast, CSR-1b, is expressed constitutively in the germline. Essential functions of csr-1 described in the literature coincide with CSR-1b. In contrast, CSR-1a plays tissue specific functions during spermatogenesis, where it integrates into a spermatogenesis sRNA regulatory network including ALG-3, ALG-4, and WAGO-10 that is necessary for male fertility. CSR-1a is also required in the intestine for the silencing of repetitive transgenes. Sequencing of small RNAs associated with each CSR-1 isoform reveals that CSR-1a engages with 22G- and 26G-RNAs, while CSR-1b interacts with only 22G-RNAs to regulate distinct groups of germline genes and regulate both sperm and oocyte-mediated fertility.', 'corpus_id': 220715620, 'score': 0}, {'doc_id': '5668436', 'title': 'Dynamics of multiple signalling systems: animal communication in a world in flux.', 'abstract': 'The ubiquity of multiple signalling is a long-standing puzzle in the study of animal communication: given the costs of producing and receiving signals, why use more than a single cue? Focusing on sexually selected signals, I argue that dynamic variation in selection pressures can often explain why multiple signals coexist. In contrast to earlier research, which has taken a largely static view of the world, new insights highlight how fluctuations in ecological and social environments, as well as non-equilibrium dynamics intrinsic to coevolutionary systems, can maintain both multiple redundant and non-redundant signals. Future challenges will include identifying the circumstances under which environmental fluctuations lead to multiple signalling, and the consequences of such fluctuations for speciation in multiple-signalling species.', 'corpus_id': 5668436, 'score': 1}, {'doc_id': '82812568', 'title': 'Neural Mechanisms of Startle Behavior', 'abstract': '1 Comparative Neuroethology of Startle, Rapid Escape, and Giant Fiber-Mediated Responses.- 2 Fast Pathways and Escape Behavior in Cnidaria.- 3 Escape Reflexes in Earthworms and Other Annelids.- 4 The Cockroach Escape Response.- 5 The Drosophila Giant Fiber System.- 6 Escape Behavior of the Locust: The Jump and Its Initiation by Visual Stimuli.- 7 The Production of Crayfish Tailflip Escape Responses.- 8 The Role of the Mauthner Cell in Fast-Starts Involving Escape in Teleost Fishes.- 9 Methodological Factors in the Behavioral Analysis of Startle: The Use of Reflex Modification Procedures and the Assessment of Threshold.- 10 The Mammalian Startle Response.- 11 Escapism: Some Startling Revelations.', 'corpus_id': 82812568, 'score': 1}, {'doc_id': '219606590', 'title': 'Host Gene Expression of Macrophages in Response to Feline Coronavirus Infection', 'abstract': 'Feline coronavirus is a highly contagious virus potentially resulting in feline infectious peritonitis (FIP), while the pathogenesis of FIP remains not well understood, particularly in the events leading to the disease. A predominant theory is that the pathogenic FIPV arises from a mutation, so that it could replicate not only in enterocytes of the intestines but also in monocytes, subsequently systemically transporting the virus. The immune status and genetics of affected cats certainly play an important role in the pathogenesis. Considering the importance of genetics and host immune responses in viral infections, the goal of this study was to elucidate host gene expression in macrophages using RNA sequencing. Macrophages from healthy male cats infected with FIPV 79-1146 ex vivo displayed a differential host gene expression. Despite the virus uptake, aligned viral reads did not increase from 2 to 17 h. The overlap of host gene expression among macrophages from different cats was limited, even though viral transcripts were detected in the cells. Interestingly, some of the downregulated genes in all macrophages were involved in immune signaling, while some upregulated genes common for all cats were found to be inhibiting immune activation. Our results highlight individual host responses playing an important role, consistent with the fact that few cats develop feline infectious peritonitis despite a common presence of enteric FCoV.', 'corpus_id': 219606590, 'score': 0}]"
54	"{'doc_id': '162197985', 'title': 'Le stockage des céréales en milieu désertique : le ghird, dune garde-manger (Egypte). Note de recherche', 'abstract': ""L'article traite d'un procede original de stockage des cereales en milieu desertique qui consiste a enfouir directement les grains dans le sable des dunes, «ghird», choisies a cet effet. Le phenomene a ete observe en Egypte dans les oasis du desert de l'ouest. Ce mode de conservation des cereales est identique pour toutes les categories sociales, quelles que soient les finalites assignees - conservation domestique differee ou stockage speculatif. Au travers de cette etude, le ghird apparait comme l'expression d'une symbiose ingenieuse entre l'etablissement humain et l'ecosysteme. Il est l'un des signes qui temoigne, ici, de l'existence d'une veritable civilisation du sable."", 'corpus_id': 162197985}"	16980	"[{'doc_id': '162681321', 'title': 'Archaeological identification and significance of ÉSAG (agricultural storage pits) at Kaman-Kalehöyük, central Anatolia', 'abstract': ""Abstract Evidence for the presence of storage pits described in Hittite texts by the Sumerogram ÉSAG is presented from Kaman-Kalehöyük, a multi-period tell site in central Turkey occupied during the second and first millennia BC. Small earthen pits matching the description of ÉSAG were part of the normal suite of domestic installations at the site throughout the period. Similar to pits seen across western Eurasia, they were probably used to store seed corn or seed for trade. Large earthen pits (>7m in diameter) were also present that matched the description of the ÉSAG form, and in some cases contained archaeological cereal remains. Evidence from Kaman shows ÉSAG were part of Anatolian life for at least 4,000 years and suggests that the term was generic for lined, earthen storage pits. The presence of so many small pits at Kaman-Kalehöyük showed that it was an agricultural production site for much of its existence. The appearance of the large pits, confined to the Hittite period, reflects centralised control of grain supply, probably by the Hittite Kingdom, and fits a pattern seen at other sites in the region during the second millennium BC. Özet Hitit metinlerinde Sumerogram ÉSAG ile tanımlanan depo çukurlarının varlığına dair kanıt, Orta Anadolu'da M.Ö. ikinci ve Birinci binde iskan edilmiş çok dönemli bir yerleşim alanı olan Kaman-Kalehöyük'ten tanınmaktadır. Küçük toprak çukurlar ÉSAG' in tanımlamasına uygun olarak bu dönem süresince normal ev düzeninin bir parçası olarak karşımıza çıkmıştır. Çakurlar, Batı Avrasya'daki benzer çukurlar gibi olasılıkla ticaret maksadı ile mısır tohumu ya da tohum muhafaza etmişlerdir. ÉSAG formunun tanımına uyan büyük toprak çukurlara (çapi 7m. den büyük) rağmen bunların tahıl depolama ile ilgili bağlantıları tam olarak belirlenmemiştir. Kaman'daki delil, ÉSAG' in en az 4,000 yıldır Anadolu yaşamının bir parçası olduğunu ve bu sözcüğgün sıvanmış toprak çukurlar için kullanıldığını işaret etmektedir. Kaman-Kalehöyük'te ele geçen birçok küçük çukur, yerleşimin varlığını sürdürdüğü sürecin büyük bir bölümünde zirai üretim yapıldığını göstermektedir. Hitit Döneminde büyük çukurların ortaya çıkması muhtemelen Hitit Krallığı tarafından gerçekleştirilen tahıl tedarikinin merkezi kontrolünü yansıtmakta ve M.ö. İkinci binde bu bölgedeki diğer yerleşim alanlarında görülen şekle uymaktadır."", 'corpus_id': 162681321, 'score': 1}, {'doc_id': '126647863', 'title': 'Controlled atmosphere storage of grains : an international symposium held from 12 to 15 May, 1980, at Castelgandolfo (Rome) Italy', 'abstract': None, 'corpus_id': 126647863, 'score': 1}, {'doc_id': '233835142', 'title': 'In praise of social distance in public spaces', 'abstract': ""In the wake of the novel coronavirus pandemic, many have voiced the concern that social distancing in public spaces could negate the ability to connect with others and thereby diminish the sense of social belonging The reason why many find social distancing so difficult, and so off-putting is that it runs counter to so much of what makes life, especially city life, worthwhile Here, Melcher argues that social distance does not threaten people's ability to connect socially;it is a critical requirement for social interactions within public spaces Public spaces work well precisely because they allow for social distance, not because they overcome or bridge it The most important connections that public spaces provide are those of a public and social nature rather than those of an intimate or personal nature"", 'corpus_id': 233835142, 'score': 0}, {'doc_id': '233412391', 'title': 'Cycles in Stone Mining and Copper Circulation in Europe 5500–2000 bc: A View from Space', 'abstract': 'The authors of this article consider the relationship in European prehistory between the procurement of high-quality stones (for axeheads, daggers, and other tools) on the one hand, and the early mining, crafting, and deposition of copper on the other. The data consist of radiocarbon dates for the exploitation of stone quarries, flint mines, and copper mines, and of information regarding the frequency through time of jade axeheads and copper artefacts. By adopting a broad perspective, spanning much of central-western Europe from 5500 to 2000 bc, they identify a general pattern in which the circulation of the first copper artefacts was associated with a decline in specialized stone quarrying. The latter re-emerged in certain regions when copper use decreased, before declining more permanently in the Bell Beaker phase, once copper became more generally available. Regional variations reflect the degrees of connectivity among overlapping copper exchange networks. The patterns revealed are in keeping with previous understandings, refine them through quantification and demonstrate their cyclical nature, with additional reference to likely local demographic trajectories.', 'corpus_id': 233412391, 'score': 0}, {'doc_id': '232422191', 'title': 'Science Shaping Modernity: Stephen Gaukroger’s Four-Volume Series Completed', 'abstract': 'Civilization and the Culture of Science: Science and the Shaping of Modernity, 1795–1935, is the final installment of a coherent effort materializing in four substantial volumes. The question the author has set out to answer is, on the face of it, quite simple: How is it that science, utterly marginal in Europe’s medieval culture, has become central to our modern culture? It is this very question that, for many a historian but also philosopher or sociologist of science, has stood in the background or even at the forefront of their decision to become one. Yet no one so far has had the courage, and the stamina, and the scholarly experience, and the vast erudition, and the organizing power, and the familiarity with a number of indispensable languages that Stephen Gaukroger displays and that are needed to engage the question on anything like the scale it deserves.', 'corpus_id': 232422191, 'score': 0}, {'doc_id': '153791944', 'title': 'Nourish the People: the State Civilian Granary System in China', 'abstract': ""The Qing state, driven by Confucian precepts of good government and urgent practical needs, committed vast resources to its granaries. Nourish the People traces the basic practices of this system, analyzes the organizational bases of its successes and failures, and examines variant practices in different regions. The volume concludes with an assessment of the granary system's social and economic impact and historical comparison with the food supply policies of other states."", 'corpus_id': 153791944, 'score': 1}, {'doc_id': '108896672', 'title': 'Underground storage pits in morocco', 'abstract': 'Abstract In Morocco, underground pits are still widely used to store grain or water on small farms. This paper discusses some aspects of grain storage in such pits and specifically describes criteria used for site selection, pit construction and exploitation and lining of pit walls. Grain losses from storage in underground pits have not yet been very well evaluated, although such losses are very high. More research is needed to identify the causes of grain losses and to help farmers store their grain adequately.', 'corpus_id': 108896672, 'score': 1}, {'doc_id': '232186163', 'title': 'Value and politics: introduction to the special issue', 'abstract': 'The papers in this special section address the relation between value and politics ethnographically, across varied locations and spaces. In doing so, they necessarily address the complexity of the concept of value. Our title here is perhaps tautological because value is always about politics. One might argue that “value and politics” is just another way of getting at political economy, for flows of value provide the deeper structure in relation to which classes exist. Writers of introductions to collections about the anthropology of value struggle to clearly articulate common themes and to define what exactly is meant by value/s and its/their associated processes (cf. Otto and Willerslev 2013). Likewise, for the papers collected here, we cannot offer a common language that glosses the diverse articulations of value. We think this is because the anthropological theory of value, in its best version, offers a holistic framework capable of bringing together the integrated character inherent in the social division of labor. It goes to the core of social relationships which keep people entangled in a given social formation. The specific cases call for highlighting one or another aspect of value and politics, employing the language most relevant to each case. In thinking about value, it is crucial to continually remind ourselves that value is always about the relations of power that are integral to social life and social reproduction; it describes who produces and who consumes in a certain equilibrium (not to be confused with fairness). Marx makes a distinction between the “real value relation” and “value” (Turner 2008). By “real value relations,” he means the proportional allocation of social labor through which a social group sustains itself in the appropriation and transformation of the environment and its members. Value points to how the relational character of such activity becomes represented and understood, and both materially and symbolically organized, in a given society. Power relations produce slippage from real value relations, such', 'corpus_id': 232186163, 'score': 0}, {'doc_id': '131218938', 'title': 'Centralized Storage in Later Prehistoric Britain.', 'abstract': 'Archaeological evidence for centralized storage facilities may provide useful information about the organization of prehistoric economies. In the background are a range of explanatory ideas. ‘Redistribution’ is a term which has been applied to the evidence from some British hillforts. Resources might be collected and then re-allocated through a permanent agency of co-ordination. They might be mobilized as tribute to elites as part of political strategy. This has been suggested for early British hillforts, and the evidence is reviewed. Much depends on the interpretation of the ‘four-poster’ structures at these sites as storehouses. A survey of these structures on British and continental sites strengthens this interpretation, and a further survey shows that, in Britain, disproportionate numbers of these structures are found at massively enclosed sites. A modified form of site catchment analysis suggests that some of the hillforts stored produce mobilized from an area which was greater than is likely to have been farmed directly from these sites. One possible inference is that resources were mobilized from subordinate settlements. It is suggested that stored grain was a critical commodity if rising populations and climatic change combined to increase the risk attached to the cereal harvest.', 'corpus_id': 131218938, 'score': 1}, {'doc_id': '233267676', 'title': 'On the Multiple Varieties,Consequences and Paradoxes of the Commodification of Nature', 'abstract': 'The article aims to characterise the variety of processes andmechanisms of nature commodification from a sociological perspective. Its general theoretical framework is based on Karl Polanyi’s The Great Transformation and the economic-sociological theory of ownership, on the basis of which the social, economic and political determinants, actual modalities, and especially the intended and unintended social and ecological consequences and paradoxes of the processes of nature commodification and decommodification are analysed. This analysis (running across unilateral typologies and approaches), tries to go beyond the narrow and one-sided characteristics of complex practices of human impact on nature, taking into account both their positive and negative consequences where the robbery policy of conquering nature is mixed with attempts of protecting it. The general theoretical argument is illustrated by concrete examples and in particular by Poland’s experiences, both from the period of real socialism and the post-socialist transformation.', 'corpus_id': 233267676, 'score': 0}]"
55	{'doc_id': '234868099', 'title': 'Metaphor can influence meta-thinking and affective levels in guided meditation', 'abstract': 'Inducing a state of meditation through conceptual metaphors used in the language of guided meditation instructions can have distinctive effects on well-being and meta-thinking. We hypothesized that the use of novel deliberate metaphor in the instructions of a guided meditation in which participants are invited to pay attention to thoughts would help novice meditators increase their meta-thinking activity and change their affective state during a guided meditation session. We conducted a study to test this hypothesis, using four experimental conditions (novel deliberate metaphor, conventional deliberate metaphor, non-metaphorical, silence) in micromeditation sessions with 324 university students. Results validate the instructions exploiting novel deliberate metaphor to activate meta-thinking activity and improve the affective state. These findings enhance our understanding of the short-term effects of guided meditation instructions, particularly regarding how the use of conceptual metaphor in the language of instructions can influence the meta-cognitive and affective levels of meditators, and open new directions both in applied metaphor studies and meditation research.', 'corpus_id': 234868099}	12955	"[{'doc_id': '53208362', 'title': 'Future directions in meditation research: Recommendations for expanding the field of contemplative science', 'abstract': 'The science of meditation has grown tremendously in the last two decades. Most studies have focused on evaluating the clinical effectiveness of mindfulness-based interventions, neural and other physiological correlates of meditation, and individual cognitive and emotional aspects of meditation. Far less research has been conducted on more challenging domains to measure, such as group and relational, transpersonal and mystical, and difficult aspects of meditation; anomalous or extraordinary phenomena related to meditation; and post-conventional stages of development associated with meditation. However, these components of meditation may be crucial to people’s psychological and spiritual development, could represent important mediators and/or mechanisms by which meditation confers benefits, and could themselves be important outcomes of meditation practices. In addition, since large numbers of novices are being introduced to meditation, it is helpful to investigate experiences they may encounter that are not well understood. Over the last four years, a task force of meditation researchers and teachers met regularly to develop recommendations for expanding the current meditation research field to include these important yet often neglected topics. These meetings led to a cross-sectional online survey to investigate the prevalence of a wide range of experiences in 1120 meditators. Results show that the majority of respondents report having had many of these anomalous and extraordinary experiences. While some of the topics are potentially controversial, they can be subjected to rigorous scientific investigation. These arenas represent largely uncharted scientific terrain and provide excellent opportunities for both new and experienced researchers. We provide suggestions for future directions, with accompanying online materials to encourage such research.', 'corpus_id': 53208362, 'score': 1}, {'doc_id': '231639718', 'title': 'Exercise for people with SCI: so important but difficult to achieve', 'abstract': 'Before the COVID-19 pandemic began, the 2020 ISCoS conference was going to be held in Tokyo during the Paralympics. This would have brought the two worlds of Sir Ludwig Guttmann together. Guttmann was the first president of ISCoS and first editor of what is now called Spinal Cord. In the 1940s he introduced sport and exercise as mandatory activities for his patients with a spinal cord injury (SCI) and started to organize the Stoke Mandeville Games, which have evolved into the Paralympics. Guttman believed in the power of sport to change lives, and viewed it as integral to helping those with a physical disability build physical and mental strength. Since the SCI world and Paralympics would meet in Tokyo, this was the perfect time to launch a special issue on SCI & Exercise. Today, science supports the role of exercise as a means of becoming healthy and fit in people with SCI and its role in improving performance of daily life activities [1], return to work [2], and even quality of life [3]. In 2007, the American College of Sports Medicine launched the vision of Exercise is Medicine (EIM). EIM is a global health initiative to make physical activity assessment and promotion standard clinical care for people of all abilities (https://www.exerciseismedicine.org/). The COVID-19 pandemic adversely impacted physical activity/participation levels, as fitness centers were closed and people were encouraged or required to stay home (and indoors) to limit transmission of the virus. The EIM vision might even be more important for people with SCI since they have higher rates of physical and mental health problems and are among the least active segments of society [4]. Although there are evidence-based exercise guidelines for people with SCI [5], achieving target amounts of exercise seems to be very hard [6]. Individuals with SCI frequently report lack of time as a barrier to sport participation [7]. The time burden of participation includes “getting to/from” the sporting location, getting on/off the equipment, as well as the actual participation. Development of time-efficient and effective exercise programs that can be conducted at home is thus critical. One such time-efficient program is High Intensity Interval Training (HIIT), which has shown positive results in the general population and has been performed by people with SCI as well. HIIT workouts are typically 10 to 30 min in duration and involve alternating bouts of intense exercise and low-intensity recovery. However, the question remains whether this high intensity training mode is feasible and safe for people with SCI. A narrative review by Astorino, Hicks, and Bilzon [8] addresses the state of the science of potential effectiveness and tolerability of HIIT in persons with SCI. McMillan et al. [9] evaluated the effectiveness of HIIT by comparing the physiological response of energy expenditure matched HIIT and continuous steady-state moderate-intensity exercise. Koontz et al. [10] round out the trio with a home-based HIIT intervention. Home or community-based exercise interventional studies are needed to establish “real-world” effectiveness and to overcome the transportation and time barriers inherent in participating in laboratory-based interventions. Five different home-based exercise/physical activities are represented in this issue. All five have three features in common, provision of exercise equipment, specific physical activity targets, and provision of support or guidance to achieve the targets. All other study design facets are highly varied. * Sonja de Groot s.d.groot@reade.nl', 'corpus_id': 231639718, 'score': 0}, {'doc_id': '231639346', 'title': 'Powering population health research: Considerations for plausible and actionable effect sizes', 'abstract': 'Evidence for Action (E4A), a signature program of the Robert Wood Johnson Foundation, funds investigator-initiated research on the impacts of social programs and policies on population health and health inequities. Across thousands of letters of intent and full proposals E4A has received since 2015, one of the most common methodological challenges faced by applicants is selecting realistic effect sizes to inform calculations of power, sample size, and minimum detectable effect (MDE). E4A prioritizes health studies that are both (1) adequately powered to detect effect sizes that may reasonably be expected for the given intervention and (2) likely to achieve intervention effects sizes that, if demonstrated, correspond to actionable evidence for population health stakeholders. However, little guidance exists to inform the selection of effect sizes for population health research proposals. We draw on examples of five rigorously evaluated population health interventions. These examples illustrate considerations for selecting realistic and actionable effect sizes as inputs to calculations of power, sample size and MDE for research proposals to study population health interventions. We show that plausible effects sizes for population health interventions may be smaller than commonly cited guidelines suggest. Effect sizes achieved with population health interventions depend on the characteristics of the intervention, the target population, and the outcomes studied. Population health impact depends on the proportion of the population receiving the intervention. When adequately powered, even studies of interventions with small effect sizes can offer valuable evidence to inform population health if such interventions can be implemented broadly. Demonstrating the effectiveness of such interventions, however, requires large sample sizes.', 'corpus_id': 231639346, 'score': 0}, {'doc_id': '232091428', 'title': 'Attentional and cognitive monitoring brain networks in long-term meditators depend on meditation states and expertise', 'abstract': 'Meditation practice is suggested to engage training of cognitive control systems in the brain. To evaluate the functional involvement of attentional and cognitive monitoring processes during meditation, the present study analysed the electroencephalographic synchronization of fronto-parietal (FP) and medial-frontal (MF) brain networks in highly experienced meditators during different meditation states (focused attention, open monitoring and loving kindness meditation). The aim was to assess whether and how the connectivity patterns of FP and MF networks are modulated by meditation style and expertise. Compared to novice meditators, (1) highly experienced meditators exhibited a strong theta synchronization of both FP and MF networks in left parietal regions in all mediation styles, and (2) only the connectivity of lateralized beta MF networks differentiated meditation styles. The connectivity of intra-hemispheric theta FP networks depended non-linearly on meditation expertise, with opposite expertise-dependent patterns found in the left and the right hemisphere. In contrast, inter-hemispheric FP connectivity in faster frequency bands (fast alpha and beta) increased linearly as a function of expertise. The results confirm that executive control systems play a major role in maintaining states of meditation. The distinctive lateralized involvement of FP and MF networks appears to represent a major functional mechanism that supports both generic and style-specific meditation states. The observed expertise-dependent effects suggest that functional plasticity within executive control networks may underpin the emergence of unique meditation states in expert meditators.', 'corpus_id': 232091428, 'score': 1}, {'doc_id': '231653767', 'title': 'Sleep in Frontline Healthcare Workers on Social Media During the COVID-19 Pandemic', 'abstract': 'Abstract Importance: During the pandemic, healthcare workers on social media are sharing their challenges, including sleep disturbances. Objective: To assess sleep using validated measures among frontline healthcare workers on social media Design: A self-selection survey was distributed on Facebook, Twitter, and Instagram for 16 days (August 31-September 15, 2020) targeting healthcare workers (HCW) who were clinically active during the pandemic. Study participants completed the Pittsburgh Sleep Quality Index (PSQI), Insomnia Severity Index (ISI), and reported demographic/career information. Poor sleep quality was defined as PSQI>5. Moderate-to-severe insomnia was defined as an ISI>14. The mini-Z was used to measure burnout. Multivariate logistic regression tested associations between demographics, career characteristics, and sleep outcomes. Setting: Online self-selection survey on social media Participants: 963 surveys were completed. Participants were predominantly White (92.8%), female (73.4%), aged 30-49 (71.9%), and physicians (64.4%). Mean sleep duration was 6.1 (SD 1.2) hours. Nearly 90% reported poor sleep (PSQI). One third (33.0%) reported moderate or severe insomnia. Many (60%) experienced sleep disruptions due to device usage or had bad dreams at least once per week (45%). Over 50% reported burnout. In multivariable logistic regressions, non-physician (OR 2.4; CI: 1.7, 3.4), caring for COVID-19 patients (OR 1.8; CI 1.2, 2.8), Hispanic ethnicity (OR 2.2; CI: 1.4, 3.5), being female (OR 1.6; CI 1.1, 2.4), and having a sleep disorder (OR 4.3; CI 2.7,6.9) were associated with increased odds of insomnia. In open-ended comments (n=310), poor sleep mapped to four categories: children and family, work demands, personal health, and pandemic-related sleep disturbances. Conclusion: During the COVID-19 pandemic, 90% of frontline healthcare workers surveyed on social media reported poor sleep, over one-third reported insomnia, and over half reported burnout. Many also reported sleep disruptions due to device usage and nightmares. Sleep interventions for frontline healthcare workers are urgently needed.', 'corpus_id': 231653767, 'score': 0}, {'doc_id': '235685950', 'title': 'Contemplative Psychology: History, Key Assumptions, and Future Directions.', 'abstract': ""Contemplative psychology is concerned with the psychological study of contemplative processes and practices, such as meditation, mindfulness, yoga, introspection, reflection, metacognition, self-regulation, self-awareness, and self-consciousness. Although contemplative psychology borders with other psychological and nonpsychological disciplines, some of its underlying assumptions distinguish it from other remits of psychological and scholarly inquiry, as do its component areas of empirical focus, conceptual nuances, and challenges. Furthermore, the discipline has tended to be somewhat disparate in its approach to investigating the core techniques and principles of which it is composed, resulting in a need for greater intradisciplinary and interdisciplinary awareness of the commonalities and differences of core contemplative psychology attributes. As a remedy to these issues, in this article, we adopt a whole-discipline perspective and aim to explicate contemplative psychology's history, breadth, key assumptions, challenges, and future directions."", 'corpus_id': 235685950, 'score': 1}, {'doc_id': '231637677', 'title': 'Obesity, Culture and Stigma in the Covid-19 Context', 'abstract': 'One of the co-morbidities of COVID-19 appears to be obesity and overweight. Although this is highly significant in epidemiological terms, we would like to draw our attention to the social role of obesity in the new global social-health scenario. Obesity needs a multifactorial approach taking into account sociocultural variables in order to explain these phenomena. But the stigmatic construction of obesity and overweight is an obstacle to understand its complexities. Health professionals, from all disciplines, must be vigilant in this regard. Obesity and overweight are not the only comorbidities presented by COVID-19, but they are the only ones, so far, that can increase the stigma on a population already punished by prejudice.', 'corpus_id': 231637677, 'score': 0}, {'doc_id': '53684930', 'title': 'Effectiveness of Self-Compassion Related Therapies: a Systematic Review and Meta-analysis', 'abstract': 'This systematic review and meta-analysis investigated whether self-compassion-related therapies, including compassion-focussed therapy, mindfulness-based cognitive therapy and acceptance and commitment therapy, are effective in promoting self-compassion and reducing psychopathology in clinical and subclinical populations. A total of 22 randomised controlled trials met inclusion criteria, with data from up to 1172 individuals included in each quantitative analysis. Effect sizes were the standardised difference in change scores between intervention and control groups. Results indicated that self-compassion-related therapies produced greater improvements in all three outcomes examined: self-compassion (g\xa0=\u20090.52, 95% CIs [0.32, 0.71]), anxiety (g\xa0=\u20090.46, 95% CIs [0.25, 0.66]) and depressive symptoms (g\xa0=\u20090.40, 95% CIs [0.23, 0.57]). However, when analysis was restricted to studies that compared self-compassion-related therapies to active control conditions, change scores were not significantly different between the intervention and control groups for any of the outcomes. Patient status (clinical vs. subclinical) and type of therapy (explicitly compassion-based vs. other compassion-related therapies, e.g. mindfulness) were not moderators of outcome. There was some evidence that self-compassion-related therapies brought about greater improvements in the negative than the positive subscales of the Self-Compassion Scale, although a statistical comparison was not possible. The methodological quality of studies was generally good, although risk of performance bias due to a lack of blinding of participants and therapists was a concern. A narrative synthesis found that changes in self-compassion and psychopathology were correlated in several studies, but this relationship was observed in both intervention and control groups. Overall, this review presents evidence that third-wave therapies bring about improvements in self-compassion and psychopathology, although not over and beyond other interventions.', 'corpus_id': 53684930, 'score': 1}, {'doc_id': '231637990', 'title': 'Narrating narratives of migration through translation, interpreting and the media', 'abstract': 'Political participation of non-resident citizens in their country of origin has become a common practice around the world. The enfranchisement of Italians abroad has allowed the participation of non-resident citizens in the general elections since 2006 (twelve MPs and six Senators). Over the years, electoral results in the foreign constituency have generated an increasing interest within media outlets in Italy due to the impact of the votes in the national elections. The aim of the paper is to understand the way in which Italian newspapers have portrayed the political engagement of Italians living abroad one month before and after the 2018 elections. The paper analyses the narratives offered by these media outlets by means of a specialized corpus composed of Italian online newspapers. Starting from the corpus-based analysis of a selection of terms related to expats, relevant discourse patterns are uncovered showing attitudes and stances towards Italians abroad and their political engagement.', 'corpus_id': 231637990, 'score': 0}, {'doc_id': '235653979', 'title': 'Self-Boundary Dissolution in Meditation: A Phenomenological Investigation', 'abstract': 'A fundamental aspect of the sense of self is its pre-reflective dimension specifying the self as a bounded and embodied knower and agent. Being a constant and tacit feature structuring consciousness, it eludes robust empirical exploration. Recently, deep meditative states involving global dissolution of the sense of self have been suggested as a promising path for advancing such an investigation. To that end, we conducted a comprehensive phenomenological inquiry into meditative self-boundary alteration. The induced states were systematically characterized by changes in six experiential features including the sense of location, agency, first-person perspective, attention, body sensations, and affective valence, as well as their interaction with meditative technique and overall degree of dissolution. Quantitative analyses of the relationships between these phenomenological categories highlighted a unitary dimension of boundary dissolution. Notably, passive meditative gestures of “letting go”, which reduce attentional engagement and sense of agency, emerged as driving the depth of dissolution. These findings are aligned with an enactive approach to the pre-reflective sense of self, linking its generation to sensorimotor activity and attention-demanding processes. Moreover, they set the stage for future phenomenologically informed analyses of neurophysiological data and highlight the utility of combining phenomenology and intense contemplative training for a scientific characterization of processes giving rise to the basic sense of being a bounded self.', 'corpus_id': 235653979, 'score': 1}]"
56	{'doc_id': '210882724', 'title': 'Beyond Sessions: Exploiting Hybrid Contextual Information for Web Search', 'abstract': 'It is essential to fully understand user intents for the optimization of downstream tasks such as document ranking and query suggestion in web search. As users tend to submit ambiguous queries, numer- ous studies utilize contextual information such as query sequence and user clicks for the auxiliary of user intent modeling. Most of these work adopted Recurrent Neural Network (RNN) based frame- works to encode sequential information within a session, which is hard to realize parallel computation. To this end, we plan to adopt attention-based units to generate context-aware representations for elements in sessions. As intra-session contexts are deficient for handling the data sparsity and cold-start problems in session search, we would also attempt to integrate cross-session dependen- cies by constructing session graphs on the whole corpus to enrich the representation of queries and documents.', 'corpus_id': 210882724}	5160	"[{'doc_id': '220381114', 'title': 'MAMO: Memory-Augmented Meta-Optimization for Cold-start Recommendation', 'abstract': 'A common challenge for most current recommender systems is the cold-start problem. Due to the lack of user-item interactions, the fine-tuned recommender systems are unable to handle situations with new users or new items. Recently, some works introduce the meta-optimization idea into the recommendation scenarios, i.e. predicting the user preference by only a few of past interacted items. The core idea is learning a global sharing initialization parameter for all users and then learning the local parameters for each user separately. However, most meta-learning based recommendation approaches adopt model-agnostic meta-learning for parameter initialization, where the global sharing parameter may lead the model into local optima for some users. In this paper, we design two memory matrices that can store task-specific memories and feature-specific memories. Specifically, the feature-specific memories are used to guide the model with personalized parameter initialization, while the task-specific memories are used to guide the model fast predicting the user preference. And we adopt a meta-optimization approach for optimizing the proposed method. We test the model on two widely used recommendation datasets and consider four cold-start situations. The experimental results show the effectiveness of the proposed methods.', 'corpus_id': 220381114, 'score': 0}, {'doc_id': '219636253', 'title': 'Incorporating User Micro-behaviors and Item Knowledge into Multi-task Learning for Session-based Recommendation', 'abstract': ""Session-based recommendation (SR) has become an important and popular component of various e-commerce platforms, which aims to predict the next interacted item based on a given session. Most of existing SR models only focus on exploiting the consecutive items in a session interacted by a certain user, to capture the transition pattern among the items. Although some of them have been proven effective, the following two insights are often neglected. First, a user's micro-behaviors, such as the manner in which the user locates an item, the activities that the user commits on an item (e.g., reading comments, adding to cart), offer fine-grained and deep understanding of the user's preference. Second, the item attributes, also known as item knowledge, provide side information to model the transition pattern among interacted items and alleviate the data sparsity problem. These insights motivate us to propose a novel SR model MKM-SR in this paper, which incorporates user Micro-behaviors and item Knowledge into Multi-task learning for Session-based Recommendation. Specifically, a given session is modeled on micro-behavior level in MKM-SR, i.e., with a sequence of item-operation pairs rather than a sequence of items, to capture the transition pattern in the session sufficiently. Furthermore, we propose a multi-task learning paradigm to involve learning knowledge embeddings which plays a role as an auxiliary task to promote the major task of SR. It enables our model to obtain better session representations, resulting in more precise SR recommendation results. The extensive evaluations on two benchmark datasets demonstrate MKM-SR's superiority over the state-of-the-art SR models, justifying the strategy of incorporating knowledge learning."", 'corpus_id': 219636253, 'score': 1}, {'doc_id': '212725651', 'title': 'Document Ranking with a Pretrained Sequence-to-Sequence Model', 'abstract': 'This work proposes the use of a pretrained sequence-to-sequence model for document ranking. Our approach is fundamentally different from a commonly adopted classification-based formulation based on encoder-only pretrained transformer architectures such as BERT. We show how a sequence-to-sequence model can be trained to generate relevance labels as “target tokens”, and how the underlying logits of these target tokens can be interpreted as relevance probabilities for ranking. Experimental results on the MS MARCO passage ranking task show that our ranking approach is superior to strong encoder-only models. On three other document retrieval test collections, we demonstrate a zero-shot transfer-based approach that outperforms previous state-of-the-art models requiring in-domain cross-validation. Furthermore, we find that our approach significantly outperforms an encoder-only architecture in a data-poor setting. We investigate this observation in more detail by varying target tokens to probe the model’s use of latent knowledge. Surprisingly, we find that the choice of target tokens impacts effectiveness, even for words that are closely related semantically. This finding sheds some light on why our sequence-to-sequence formulation for document ranking is effective. Code and models are available at pygaggle.ai.', 'corpus_id': 212725651, 'score': 1}, {'doc_id': '216641819', 'title': 'Training Curricula for Open Domain Answer Re-Ranking', 'abstract': 'In precision-oriented tasks like answer ranking, it is more important to rank many relevant answers highly than to retrieve all relevant answers. It follows that a good ranking strategy would be to learn how to identify the easiest correct answers first (i.e., assign a high ranking score to answers that have characteristics that usually indicate relevance, and a low ranking score to those with characteristics that do not), before incorporating more complex logic to handle difficult cases (e.g., semantic matching or reasoning). In this work, we apply this idea to the training of neural answer rankers using curriculum learning. We propose several heuristics to estimate the difficulty of a given training sample. We show that the proposed heuristics can be used to build a training curriculum that down-weights difficult samples early in the training process. As the training process progresses, our approach gradually shifts to weighting all samples equally, regardless of difficulty. We present a comprehensive evaluation of our proposed idea on three answer ranking datasets. Results show that our approach leads to superior performance of two leading neural ranking architectures, namely BERT and ConvKNRM, using both pointwise and pairwise losses. When applied to a BERT-based ranker, our method yields up to a 4% improvement in MRR and a 9% improvement in P@1 (compared to the model trained without a curriculum). This results in models that can achieve comparable performance to more expensive state-of-the-art techniques.', 'corpus_id': 216641819, 'score': 0}, {'doc_id': '218870421', 'title': 'Learning to Transfer Graph Embeddings for Inductive Graph based Recommendation', 'abstract': ""With the increasing availability of videos, how to edit them and present the most interesting parts to users, i.e., video highlight, has become an urgent need with many broad applications. As users' visual preferences are subjective and vary from person to person, previous generalized video highlight extraction models fail to tailor to users' unique preferences. In this paper, we study the problem of personalized video highlight recommendation with rich visual content. By dividing each video into non-overlapping segments, we formulate the problem as a personalized segment recommendation task with many new segments in the test stage. The key challenges of this problem lie in: the cold-start users with limited video highlight records in the training data and new segments without any user ratings at the test stage. To tackle these challenges, an intuitive idea is to formulate a user-item interaction graph and perform inductive graph neural network based models for better user and item embedding learning. However, the graph embedding models fail to generalize to unseen items as these models rely on the item content feature and item link information for item embedding calculation. To this end, we propose an inductive Graph based Transfer learning framework for personalized video highlight Recommendation (TransGRec). TransGRec is composed of two parts: a graph neural network followed by an item embedding transfer network. Specifically, the graph neural network part exploits the higher-order proximity between users and segments to alleviate the user cold-start problem. The transfer network is designed to approximate the learned item embeddings from graph neural networks by taking each item's visual content as input, in order to tackle the new segment problem in the test phase. We design two detailed implementations of the transfer learning optimization function, and we show how the two parts of TransGRec can be efficiently optimized with different transfer learning optimization functions. Please note that, our proposed framework is generally applicable to any inductive graph based recommendation model to address the new node problem without any link structure. Finally, extensive experimental results on a real-world dataset clearly show the effectiveness of our proposed model."", 'corpus_id': 218870421, 'score': 0}, {'doc_id': '220381441', 'title': 'PinnerSage: Multi-Modal User Embedding Framework for Recommendations at Pinterest', 'abstract': ""Latent user representations are widely adopted in the tech industry for powering personalized recommender systems. Most prior work infers a single high dimensional embedding to represent a user, which is a good starting point but falls short in delivering a full understanding of the user's interests. In this work, we introduce PinnerSage, an end-to-end recommender system that represents each user via multi-modal embeddings and leverages this rich representation of users to provides high quality personalized recommendations. PinnerSage achieves this by clustering users' actions into conceptually coherent clusters with the help of a hierarchical clustering method (Ward) and summarizes the clusters via representative pins (Medoids) for efficiency and interpretability. PinnerSage is deployed in production at Pinterest and we outline the several design decisions that makes it run seamlessly at a very large scale. We conduct several offline and online A/B experiments to show that our method significantly outperforms single embedding methods."", 'corpus_id': 220381441, 'score': 0}, {'doc_id': '220363786', 'title': 'DrugDBEmbed : Semantic Queries on Relational Database using Supervised Column Encodings', 'abstract': 'Traditional relational databases contain a lot of latent semantic information that have largely remained untapped due to the difficulty involved in automatically extracting such information. Recent works have proposed unsupervised machine learning approaches to extract such hidden information by textifying the database columns and then projecting the text tokens onto a fixed dimensional semantic vector space. However, in certain databases, task-specific class labels may be available, which unsupervised approaches are unable to lever in a principled manner. Also, when embeddings are generated at individual token level, then column encoding of multi-token text column has to be computed by taking the average of the vectors of the tokens present in that column for any given row. Such averaging approach may not produce the best semantic vector representation of the multi-token text column, as observed while encoding paragraphs or documents in natural language processing domain. With these shortcomings in mind, we propose a supervised machine learning approach using a Bi-LSTM based sequence encoder to directly generate column encodings for multi-token text columns of the DrugBank database, which contains gold standard drug-drug interaction (DDI) labels. Our text data driven encoding approach achieves very high Accuracy on the supervised DDI prediction task for some columns and we use those supervised column encodings to simulate and evaluate the Analogy SQL queries on relational data to demonstrate the efficacy of our technique.', 'corpus_id': 220363786, 'score': 0}, {'doc_id': '221083421', 'title': 'MiNet: Mixed Interest Network for Cross-Domain Click-Through Rate Prediction', 'abstract': 'Click-through rate (CTR) prediction is a critical task in online advertising systems. Existing works mainly address the single-domain CTR prediction problem and model aspects such as feature interaction, user behavior history and contextual information. Nevertheless, ads are usually displayed with natural content, which offers an opportunity for cross-domain CTR prediction. In this paper, we address this problem and leverage auxiliary data from a source domain to improve the CTR prediction performance of a target domain. Our study is based on UC Toutiao (a news feed service integrated with the UC Browser App, serving hundreds of millions of users daily), where the source domain is the news and the target domain is the ad. In order to effectively leverage news data for predicting CTRs of ads, we propose the Mixed Interest Network (MiNet) which jointly models three types of user interest: 1) long-term interest across domains, 2) short-term interest from the source domain and 3) short-term interest in the target domain. MiNet contains two levels of attentions, where the item-level attention can adaptively distill useful information from clicked news / ads and the interest-level attention can adaptively fuse different interest representations. Offline experiments show that MiNet outperforms several state-of-the-art methods for CTR prediction. We have deployed MiNet in UC Toutiao and the A/B test results show that the online CTR is also improved substantially. MiNet now serves the main ad traffic in UC Toutiao.', 'corpus_id': 221083421, 'score': 1}, {'doc_id': '220280457', 'title': 'Interactive Path Reasoning on Graph for Conversational Recommendation', 'abstract': 'Traditional recommendation systems estimate user preference on items from past interaction history, thus suffering from the limitations of obtaining fine-grained and dynamic user preference. Conversational recommendation system (CRS) brings revolutions to those limitations by enabling the system to directly ask users about their preferred attributes on items. However, existing CRS methods do not make full use of such advantage --- they only use the attribute feedback in rather implicit ways such as updating the latent user representation. In this paper, we propose Conversational Path Reasoning (CPR), a generic framework that models conversational recommendation as an interactive path reasoning problem on a graph. It walks through the attribute vertices by following user feedback, utilizing the user preferred attributes in an explicit way. By leveraging on the graph structure, CPR is able to prune off many irrelevant candidate attributes, leading to a better chance of hitting user-preferred attributes. To demonstrate how CPR works, we propose a simple yet effective instantiation named SCPR (Simple CPR). We perform empirical studies on the multi-round conversational recommendation scenario, the most realistic CRS setting so far that considers multiple rounds of asking attributes and recommending items. Through extensive experiments on two datasets Yelp and LastFM, we validate the effectiveness of our SCPR, which significantly outperforms the state-of-the-art CRS methods EAR and CRM. In particular, we find that the more attributes there are, the more advantages our method can achieve.', 'corpus_id': 220280457, 'score': 0}, {'doc_id': '220250059', 'title': 'TFNet: Multi-Semantic Feature Interaction for CTR Prediction', 'abstract': 'The CTR (Click-Through Rate) prediction plays a central role in the domain of computational advertising and recommender systems. There exists several kinds of methods proposed in this field, such as Logistic Regression (LR), Factorization Machines (FM) and deep learning based methods like Wide&Deep, Neural Factorization Machines (NFM) and DeepFM. However, such approaches generally use the vector-product of each pair of features, which have ignored the different semantic spaces of the feature interactions. In this paper, we propose a novel Tensor-based Feature interaction Network (TFNet) model, which introduces an operating tensor to elaborate feature interactions via multi-slice matrices in multiple semantic spaces. Extensive offline and online experiments show that TFNet: 1) outperforms the competitive compared methods on the typical Criteo and Avazu datasets; 2) achieves large improvement of revenue and click rate in online A/B tests in the largest Chinese App recommender system, Tencent MyApp.', 'corpus_id': 220250059, 'score': 1}]"
57	{'doc_id': '1152138', 'title': 'Predicting Economic Market Crises Using Measures of Collective Panic', 'abstract': 'Predicting panic is of critical importance in many areas of human and animal behavior, notably in the context of economics. The recent financial crisis is a case in point. Panic may be due to a specific external threat, or self-generated nervousness. Here we show that the recent economic crisis and earlier large single-day panics were preceded by extended periods of high levels of market mimicry --- direct evidence of uncertainty and nervousness, and of the comparatively weak influence of external news. High levels of mimicry can be a quite general indicator of the potential for self-organized crises.', 'corpus_id': 1152138}	20162	"[{'doc_id': '3499698', 'title': 'CHANGES IN BUYER COMPOSITION AND THE EXPANSION OF CREDIT DURING THE BOOM 1', 'abstract': 'Earlier research has suggested that distortions in the supply of mortgage credit during the run up to the 2008 financial crisis, in particular a decoupling of credit flow from income growth, may have been responsible for the rise in house prices and the subsequent collapse of the housing market. Focusing on individual mortgage transactions rather than whole zip codes, we show that the apparent decoupling of credit from income shown in previous research was driven by changes in buyer composition. In fact, the relationship between individual mortgage size and income growth during the housing boom was very similar to previous periods, independent of how we measure income. Zip codes that had large house price increases experienced significant changes in the composition of buyers, i.e. home buyers (mortgage applicants) had increasingly higher income than the average residents in an area. Poorer areas saw an expansion of credit mostly through the extensive margin, i.e. a larger numbers of mortgages originated, but at DTI levels in line with borrower income. When we break out the volume of mortgage origination from 2002 to 2006 by income deciles across the US population, we see that the distribution of mortgage debt is concentrated in middle and high income borrowers, not the poor. Middle and high income borrowers also contributed most significantly to the increase in defaults after 2007. These results are consistent with an interpretation where house price expectations led lenders and buyers to buy into an unfolding bubble based on inflated asset values, rather than a change in the lending technology.', 'corpus_id': 3499698, 'score': 1}, {'doc_id': '216272124', 'title': 'Global Macro-Financial Cycles and Spillovers', 'abstract': 'We develop a new dynamic factor model that allows us to jointly characterize global macroeconomic and financial cycles and the spillovers between them. The model decomposes macroeconomic cycles into the part driven by global and country-specific macro factors and the part driven by spillovers from financial variables. We consider cycles in macroeconomic aggregates (output, consumption, and investment) and financial variables (equity and house prices, and interest rates). We find that the global macro factor plays a major role in explaining G-7 business cycles, but there are also spillovers from equity and house price shocks onto macroeconomic aggregates. These spillovers operate mainly through the global macro factor rather than the country-specific macro factors (i.e., these spillovers affect business cycles in all G-7 economies) and are stronger in the period leading up to and following the global financial crisis. We find little evidence of spillovers from macroeconomic cycles to financial cycles.', 'corpus_id': 216272124, 'score': 0}, {'doc_id': '109693849', 'title': 'Testing for Multiple Bubbles', 'abstract': 'Identifying and dating explosive bubbles when there is periodically collapsing behavior over time has been a major concern in the economics literature and is of great importance for practitioners. The complexity of the nonlinear structure inherent in multiple bubble phenomena within the same sample period makes econometric analysis particularly difficult. The present paper develops new recursive procedures for practical implementation and surveillance strategies that may be employed by central banks and fiscal regulators. We show how the testing procedure and dating algorithm of Phillips, Wu and Yu (2011, PWY) are affected by multiple bubbles and may fail to be consistent. The present paper proposes a generalized version of the sup ADF test of PWY to address this difficulty, derives its asymptotic distribution, introduces a new date-stamping strategy for the origination and termination of multiple bubbles, and proves consistency of this dating procedure. Simulations show that the test significantly improves discriminatory power and leads to distinct power gains when multiple bubbles occur. Empirical applications are conducted to S&P 500 stock market data over a long historical period from January 1871 to December 2010. The new approach identifies many key historical episodes of exuberance and collapse over this period, whereas the strategy of PWY and the CUSUM procedure locate far fewer episodes in the same sample range.', 'corpus_id': 109693849, 'score': 1}, {'doc_id': '237252600', 'title': 'On the Construction of a Positive Sentiment Index for COVID-19: Evidence from G20 Stock Markets', 'abstract': 'The present study investigates the degree of market responses through the scope of investors’ sentiment during the COVID-19 pandemic across G20 markets, by constructing a novel positive search volume index for COVID-19 (COVID19+). Our key findings, obtained using a Panel-GARCH model, indicate that an increased COVID19+ index suggests that investors decrease their COVID-19 related crisis sentiment by escalating their Google searches for positively associated COVID-19 related keywords. Specifically, we explore the predictive power of the newly constructed index on stock returns and volatility. According to our findings, investor sentiment positively (negatively) predicts the stock return (volatility) during the COVID-19. This is the first study of its kind assessing global sentiment by proposing a novel proxy and its impacts on the G20 equity market.', 'corpus_id': 237252600, 'score': 0}, {'doc_id': '1483285', 'title': 'Higher Order Beliefs , Confidence , and Business Cycles ∗', 'abstract': 'This paper presents a model of business cycles driven by shocks to agents’ beliefs about economic fundamentals. Agents are hit both by common and idiosyncratic shocks. Common shocks act as confidence shocks, which cause economy-wide optimism or pessimism and consequently, aggregate fluctuations in real variables. Idiosyncratic shocks generate dispersed information, which prevents agents from perfectly inferring the state of the economy. Crucially, asymmetric information induces the infinite regress problem, that is, agents need to forecast the forecasts of others. We develop a method that can solve the infinite regress problem without approximation. Even though agents face a complicated learning problem, the equilibrium policy can be represented by a small number of state variables. Theoretically, we prove that the persistence of aggregate output is increasing in the degree of information frictions and strategic complementarity, and there is a hump-shaped relationship between the variance of output and the variance of the confidence shock. Quantitatively, our model with confidence shocks can match a number of the key business cycle moments.', 'corpus_id': 1483285, 'score': 1}, {'doc_id': '235316577', 'title': 'Text-Based Recession Probabilities', 'abstract': ""This paper proposes a new methodology based on textual analysis to forecast U.S. recessions. Specifically, the paper develops an index in the spirit of Baker et al. (2016) and Caldara and Iacoviello (2018) which tracks developments in U.S. real activity. When used in a standard recession probability model, the index outperforms the yield curve based forecast, a standard method to forecast recessions, at medium horizons, up to 8 months. Moreover, the index contains information not included in yield data that are useful to understand recession episodes. When included as an additional control to the slope of the yield curve, it improves the forecast accuracy by 5% to 30% depending on the horizon. These results are stable to a number of different robustness checks, including changes to the estimation method, the definition of recessions and controlling for asset purchases by major central banks. Yield and textual analysis data also outperform other popular leading indicators for the U.S. business cycle such as PMIs, consumers' surveys or employment data."", 'corpus_id': 235316577, 'score': 1}, {'doc_id': '158378645', 'title': 'Financial cycles around the world', 'abstract': 'The study analyses ﬁnancial cycles based on a global sample of 34 advanced and developing countries over the period 1960Q1 to 2015Q4. We use dynamic factor models and state-space techniques to estimate ﬁnancial cycles in credit, housing, bond and equity markets, as well as aggregate ﬁnancial cycles for each country in the sample using a large number of variables conveying price, quantity and risk characteristics of respective markets. The analysis reveals the highly persistent and recurring nature of ﬁnancial cycles, which tend to ﬂuctuate at frequencies much lower than business cycles, 9‑15 years on average, and are indicative of major ﬁnancial distress episodes. Our results point to notable intra-regional synchronisation, as well as nontrivial co-movement tendencies between European, American and Asian ﬁnancial cycles. We also extract global and regional ﬁnancial cycles, the former closely associated with the dynamics of the US T-bill rate and the VIX index, conﬁrming the existence of common supranational factors governing the boom-bust dynamics of ﬁnancial market activity around the world.', 'corpus_id': 158378645, 'score': 0}, {'doc_id': '72940954', 'title': 'Nowcasting Recessions using the SVM Machine Learning Algorithm', 'abstract': 'We introduce a novel application of Support Vector Machines (SVM), an important Machine Learning algorithm, to determine the beginning and end of recessions in real time. Nowcasting, ""forecasting"" a condition about the present time because the full information about it is not available until later, is key for recessions, which are only determined months after the fact. We show that SVM has excellent predictive performance for this task, and we provide implementation details to facilitate its use in similar problems in economics and finance.', 'corpus_id': 72940954, 'score': 1}, {'doc_id': '236159272', 'title': 'A Long Short-Term Memory Network Stock Price Prediction with Leading Indicators.', 'abstract': 'The accuracy of the prediction of stock price fluctuations is crucial for investors, and it helps investors manage funds better when formulating trading strategies. Using forecasting tools to get a predicted value that is closer to the actual value from a given financial data set has always been a major goal of financial researchers and a problem. In recent years, people have paid particular attention to stocks, and gradually used various tools to predict stock prices. There is more than one factor that affects financial trends, and people need to consider it from all aspects, so research on stock price fluctuations has also become extremely difficult. This paper mainly studies the impact of leading indicators on the stock market. The framework used in this article is proposed based on long short-term memory (LSTM). In this study, leading indicators that affect stock market volatility are added, and the proposed framework is thus named as a stock tending prediction framework based on LSTM with leading indicators (LSTMLI). This study uses stock markets in the United States and Taiwan, respectively, with historical data, futures, and options as data sets to predict stock prices in these two markets. We measure the predictive performance of LSTMLI relative to other neural network models, and the impact of leading indicators on stock prices is studied. Besides, when using LSTMLI to predict the rise and fall of stock prices in the article, the conventional regression method is not used, but the classification method is used, which can give a qualitative output based on the data set. The experimental results show that the LSTMLI model using the classification method can effectively reduce the prediction error. Also, the data set with leading indicators is better than the prediction results of the single historical data using the LSTMLI model.', 'corpus_id': 236159272, 'score': 0}, {'doc_id': '237346567', 'title': 'Return Predictability using Price-to-Earnings Ratio', 'abstract': 'In this paper, I revisit the predictive ability of the price to earnings (PE) ratio for future returns. I provide a model of expected returns by decomposing stock price into earnings and PE ratio. While the PE ratio is modeled as a mean-reverting AR(1) process, earnings follow a linear trend. Expected model returns are strongly correlated with future returns. An increase of 1% in expected returns is associated with 0.5% higher future returns. To this end, I propose an investment strategy that chooses between equity or debt investment based on predicted expected stock returns and risk-free bond returns. My strategy avoids short-selling and outperforms a buy-and-hold portfolio of stocks.', 'corpus_id': 237346567, 'score': 0}]"
58	"{'doc_id': '12973375', 'title': 'Finite-Time Bounds for Fitted Value Iteration', 'abstract': 'In this paper we develop a theoretical analysis of the performance of sampling-based fitted value iteration (FVI) to solve infinite state-space, discounted-reward Markovian decision processes (MDPs) under the assumption that a generative model of the environment is available. Our main results come in the form of finite-time bounds on the performance of two versions of sampling-based FVI. The convergence rate results obtained allow us to show that both versions of FVI are well behaving in the sense that by using a sufficiently large number of samples for a large class of MDPs, arbitrary good performance can be achieved with high probability. An important feature of our proof technique is that it permits the study of weighted Lp-norm performance bounds. As a result, our technique applies to a large class of function-approximation methods (e.g., neural networks, adaptive regression trees, kernel machines, locally weighted learning), and our bounds scale well with the effective horizon of the MDP. The bounds show a dependence on the stochastic stability properties of the MDP: they scale with the discounted-average concentrability of the future-state distributions. They also depend on a new measure of the approximation power of the function space, the inherent Bellman residual, which reflects how well the function space is ""aligned"" with the dynamics and rewards of the MDP. The conditions of the main result, as well as the concepts introduced in the analysis, are extensively discussed and compared to previous theoretical results. Numerical experiments are used to substantiate the theoretical findings.', 'corpus_id': 12973375}"	3208	"[{'doc_id': '214802092', 'title': 'Intrinsic Exploration as Multi-Objective RL', 'abstract': 'Intrinsic motivation enables reinforcement learning (RL) agents to explore when rewards are very sparse, where traditional exploration heuristics such as Boltzmann or e-greedy would typically fail. However, intrinsic exploration is generally handled in an ad-hoc manner, where exploration is not treated as a core objective of the learning process; this weak formulation leads to sub-optimal exploration performance. To overcome this problem, we propose a framework based on multi-objective RL where both exploration and exploitation are being optimized as separate objectives. This formulation brings the balance between exploration and exploitation at a policy level, resulting in advantages over traditional methods. This also allows for controlling exploration while learning, at no extra cost. Such strategies achieve a degree of control over agent exploration that was previously unattainable with classic or intrinsic rewards. We demonstrate scalability to continuous state-action spaces by presenting a method (EMU-Q) based on our framework, guiding exploration towards regions of higher value-function uncertainty. EMU-Q is experimentally shown to outperform classic exploration techniques and other intrinsic RL methods on a continuous control benchmark and on a robotic manipulator.', 'corpus_id': 214802092, 'score': 1}, {'doc_id': '214775225', 'title': 'Average Reward Adjusted Discounted Reinforcement Learning: Near-Blackwell-Optimal Policies for Real-World Applications', 'abstract': 'Although in recent years reinforcement learning has become very popular the number of successful applications to different kinds of operations research problems is rather scarce. Reinforcement learning is based on the well-studied dynamic programming technique and thus also aims at finding the best stationary policy for a given Markov Decision Process, but in contrast does not require any model knowledge. The policy is assessed solely on consecutive states (or state-action pairs), which are observed while an agent explores the solution space. The contributions of this paper are manifold. First we provide deep theoretical insights to the widely applied standard discounted reinforcement learning framework, which give rise to the understanding of why these algorithms are inappropriate when permanently provided with non-zero rewards, such as costs or profit. Second, we establish a novel near-Blackwell-optimal reinforcement learning algorithm. In contrary to former method it assesses the average reward per step separately and thus prevents the incautious combination of different types of state values. Thereby, the Laurent Series expansion of the discounted state values forms the foundation for this development and also provides the connection between the two approaches. Finally, we prove the viability of our algorithm on a challenging problem set, which includes a well-studied M/M/1 admission control queuing system. In contrast to standard discounted reinforcement learning our algorithm infers the optimal policy on all tested problems. The insights are that in the operations research domain machine learning techniques have to be adapted and advanced to successfully apply these methods in our settings.', 'corpus_id': 214775225, 'score': 0}, {'doc_id': '211069354', 'title': 'Statistically Efficient Off-Policy Policy Gradients', 'abstract': 'Policy gradient methods in reinforcement learning update policy parameters by taking steps in the direction of an estimated gradient of policy value. In this paper, we consider the statistically efficient estimation of policy gradients from off-policy data, where the estimation is particularly non-trivial. We derive the asymptotic lower bound on the feasible mean-squared error in both Markov and non-Markov decision processes and show that existing estimators fail to achieve it in general settings. We propose a meta-algorithm that achieves the lower bound without any parametric assumptions and exhibits a unique 3-way double robustness property. We discuss how to estimate nuisances that the algorithm relies on. Finally, we establish guarantees on the rate at which we approach a stationary point when we take steps in the direction of our new estimated policy gradient.', 'corpus_id': 211069354, 'score': 1}, {'doc_id': '198147531', 'title': 'LQR through the Lens of First Order Methods: Discrete-time Case', 'abstract': 'We consider the Linear-Quadratic-Regulator (LQR) problem in terms of optimizing a real-valued matrix function over the set of feedback gains. Such a setup facilitates examining the implications of a natural initial-state independent formulation of LQR in designing first order algorithms. It is shown that this cost function is smooth and coercive, and provide an alternate means of noting its gradient dominated property. In the process, we provide a number of analytic observations on the LQR cost when directly analyzed in terms of the feedback gain. We then examine three types of well-posed flows for LQR: gradient flow, natural gradient flow and the quasi-Newton flow. The coercive property suggests that these flows admit unique solutions while gradient dominated property indicates that the corresponding Lyapunov functionals decay at an exponential rate; we also prove that these flows are exponentially stable in the sense of Lyapunov. We then discuss the forward Euler discretization of these flows, realized as gradient descent, natural gradient descent and the quasi-Newton iteration. We present stepsize criteria for gradient descent and natural gradient descent, guaranteeing that both algorithms converge linearly to the global optima. An optimal stepsize for the quasi-Newton iteration is also proposed, guaranteeing a $Q$-quadratic convergence rate--and in the meantime--recovering the Hewer algorithm.', 'corpus_id': 198147531, 'score': 1}, {'doc_id': '211259299', 'title': 'Periodic Q-Learning', 'abstract': 'The use of target networks is a common practice in deep reinforcement learning for stabilizing the training; however, theoretical understanding of this technique is still limited. In this paper, we study the so-called periodic Q-learning algorithm (PQ-learning for short), which resembles the technique used in deep Q-learning for solving infinite-horizon discounted Markov decision processes (DMDP) in the tabular setting. PQ-learning maintains two separate Q-value estimates - the online estimate and target estimate. The online estimate follows the standard Q-learning update, while the target estimate is updated periodically. In contrast to the standard Q-learning, PQ-learning enjoys a simple finite time analysis and achieves better sample complexity for finding an epsilon-optimal policy. Our result provides a preliminary justification of the effectiveness of utilizing target estimates or networks in Q-learning algorithms.', 'corpus_id': 211259299, 'score': 1}, {'doc_id': '8647328', 'title': 'Error Propagation for Approximate Policy and Value Iteration', 'abstract': 'We address the question of how the approximation error/Bellman residual at each iteration of the Approximate Policy/Value Iteration algorithms influences the quality of the resulted policy. We quantify the performance loss as the Lp norm of the approximation error/Bellman residual at each iteration. Moreover, we show that the performance loss depends on the expectation of the squared Radon-Nikodym derivative of a certain distribution rather than its supremum – as opposed to what has been suggested by the previous results. Also our results indicate that the contribution of the approximation/Bellman error to the performance loss is more prominent in the later iterations of API/AVI, and the effect of an error term in the earlier iterations decays exponentially fast.', 'corpus_id': 8647328, 'score': 1}, {'doc_id': '211069386', 'title': 'Bayesian Residual Policy Optimization: Scalable Bayesian Reinforcement Learning with Clairvoyant Experts', 'abstract': ""Informed and robust decision making in the face of uncertainty is critical for robots that perform physical tasks alongside people. We formulate this as Bayesian Reinforcement Learning over latent Markov Decision Processes (MDPs). While Bayes-optimality is theoretically the gold standard, existing algorithms do not scale well to continuous state and action spaces. Our proposal builds on the following insight: in the absence of uncertainty, each latent MDP is easier to solve. We first obtain an ensemble of experts, one for each latent MDP, and fuse their advice to compute a baseline policy. Next, we train a Bayesian residual policy to improve upon the ensemble's recommendation and learn to reduce uncertainty. Our algorithm, Bayesian Residual Policy Optimization (BRPO), imports the scalability of policy gradient methods and task-specific expert skills. BRPO significantly improves the ensemble of experts and drastically outperforms existing adaptive RL methods."", 'corpus_id': 211069386, 'score': 0}, {'doc_id': '211011290', 'title': 'Regret Minimization in Partially Observable Linear Quadratic Control', 'abstract': 'We study the problem of regret minimization in partially observable linear quadratic control systems when the model dynamics are unknown a priori. We propose ExpCommit, an explore-then-commit algorithm that learns the model Markov parameters and then follows the principle of optimism in the face of uncertainty to design a controller. We propose a novel way to decompose the regret and provide an end-to-end sublinear regret upper bound for partially observable linear quadratic control. Finally, we provide stability guarantees and establish a regret upper bound of $\\tilde{\\mathcal{O}}(T^{2/3})$ for ExpCommit, where $T$ is the time horizon of the problem.', 'corpus_id': 211011290, 'score': 1}, {'doc_id': '211069385', 'title': 'Maximizing the Total Reward via Reward Tweaking', 'abstract': ""In reinforcement learning, the discount factor $\\gamma$ controls the agent's effective planning horizon. Traditionally, this parameter was considered part of the MDP; however, as deep reinforcement learning algorithms tend to become unstable when the effective planning horizon is long, recent works refer to $\\gamma$ as a hyper-parameter. In this work, we focus on the finite-horizon setting and introduce \\emph{reward tweaking}. Reward tweaking learns a surrogate reward function $\\tilde r$ for the discounted setting, which induces an optimal (undiscounted) return in the original finite-horizon task. Theoretically, we show that there exists a surrogate reward which leads to optimality in the original task and discuss the robustness of our approach. Additionally, we perform experiments in a high-dimensional continuous control task and show that reward tweaking guides the agent towards better long-horizon returns when it plans for short horizons using the tweaked reward."", 'corpus_id': 211069385, 'score': 0}, {'doc_id': '214612371', 'title': 'Reinforcement Learning in Economics and Finance', 'abstract': 'Reinforcement learning algorithms describe how an agent can learn an optimal action policy in a sequential decision process, through repeated experience. In a given environment, the agent policy provides him some running and terminal rewards. As in online learning, the agent learns sequentially. As in multi-armed bandit problems, when an agent picks an action, he can not infer ex-post the rewards induced by other action choices. In reinforcement learning, his actions have consequences: they influence not only rewards, but also future states of the world. The goal of reinforcement learning is to find an optimal policy -- a mapping from the states of the world to the set of actions, in order to maximize cumulative reward, which is a long term strategy. Exploring might be sub-optimal on a short-term horizon but could lead to optimal long-term ones. Many problems of optimal control, popular in economics for more than forty years, can be expressed in the reinforcement learning framework, and recent advances in computational science, provided in particular by deep learning algorithms, can be used by economists in order to solve complex behavioral problems. In this article, we propose a state-of-the-art of reinforcement learning techniques, and present applications in economics, game theory, operation research and finance.', 'corpus_id': 214612371, 'score': 0}]"
59	{'doc_id': '226282392', 'title': 'Deeply-Supervised Density Regression for Automatic Cell Counting in Microscopy Images', 'abstract': 'Accurately counting the number of cells in microscopy images is required in many medical diagnosis and biological studies. This task is tedious, time-consuming, and prone to subjective errors. However, designing automatic counting methods remains challenging due to low image contrast, complex background, large variance in cell shapes and counts, and significant cell occlusions in two-dimensional microscopy images. In this study, we proposed a new density regression-based method for automatically counting cells in microscopy images. The proposed method processes two innovations compared to other state-of-the-art density regression-based methods. First, the density regression model (DRM) is designed as a concatenated fully convolutional regression network (C-FCRN) to employ multi-scale image features for the estimation of cell density maps from given images. Second, auxiliary convolutional neural networks (AuxCNNs) are employed to assist in the training of intermediate layers of the designed C-FCRN to improve the DRM performance on unseen datasets. Experimental studies evaluated on four datasets demonstrate the superior performance of the proposed method.', 'corpus_id': 226282392}	11033	[{'doc_id': '235415460', 'title': 'Interactive analysis for large volume data from fluorescence microscopy at cellular precision', 'abstract': 'The main objective for understanding fluorescence microscopy data is to investigate and evaluate the fluorescent signal intensity distributions as well as their spatial relationships across multiple channels. The quantitative analysis of 3D fluorescence microscopy data needs interactive tools for researchers to select and focus on relevant biological structures. We developed an interactive tool based on volume visualization techniques and GPU computing for streamlining rapid data analysis. Our main contribution is the implementation of common data quantification functions on streamed volumes, providing interactive analyses on large data without lengthy preprocessing. Data segmentation and quantification are coupled with brushing and executed at an interactive speed. A large volume is partitioned into data bricks, and only user-selected structures are analyzed to constrain the computational load. We designed a framework to assemble a sequence of GPU programs to handle brick borders and stitch analysis results. Our tool was developed in collaboration with domain experts and has been used to identify cell types. We demonstrate a workflow to analyze cells in vestibular epithelia of transgenic mice.', 'corpus_id': 235415460, 'score': 0}, {'doc_id': '231718978', 'title': 'Utilizing Uncertainty Estimation in Deep Learning Segmentation of Fluorescence Microscopy Images with Missing Markers', 'abstract': 'Fluorescence microscopy images contain several channels, each indicating a marker staining the sample. Since many different marker combinations are utilized in practice, it has been challenging to apply deep learning based segmentation models, which expect a predefined channel combination for all training samples as well as at inference for future application. Recent work circumvents this problem using a modality attention approach to be effective across any possible marker combination. However, for combinations that do not exist in a labeled training dataset, one cannot have any estimation of potential segmentation quality if that combination is encountered during inference. Without this, not only one lacks quality assurance but one also does not know where to put any additional imaging and labeling effort. We herein propose a method to estimate segmentation quality on unlabeled images by (i) estimating both aleatoric and epistemic uncertainties of convolutional neural networks for image segmentation, and (ii) training a Random Forest model for the interpretation of uncertainty features via regression to their corresponding segmentation metrics. Additionally, we demonstrate that including these uncertainty measures during training can provide an improvement on segmentation performance.', 'corpus_id': 231718978, 'score': 1}, {'doc_id': '234146188', 'title': 'Detection and segmentation in microscopy images', 'abstract': 'Abstract The plethora of heterogeneous data generated using modern microscopy imaging techniques eliminates the possibility of manual image analysis for biologists. Consequently, reliable and robust computerized techniques are critical to analyze microscopy data. Detection problems in microscopy images focuses on accurately identifying the objects of interest in an image that can be used to investigate hypotheses about developmental or pathological processes and can be indicative of prognosis in patients. Detection is also considered to be the preliminary step for solving subsequent problems, such as segmentation and tracking for various biological applications. Segmentation of the desired structures and regions in microscopy images require pixel-level labels to uniquely identify the individual structures and regions with contours for morphological and physiological analysis. Distributions of features extracted from the segmented regions can be used to compare normal versus disease or normal versus wild-type populations. Segmentation can be considered as a precursor for solving classification, reconstruction, and tracking problems in microscopy images. In this chapter, we discuss how the field of microscopic image analysis has progressed over the years, starting with traditional approaches and then followed by the study of learning algorithms. Because there is a lot of variability in microscopy data, it is essential to study learning algorithms that can adapt to these changes. We focus on deep learning approaches with convolutional neural networks (CNNs), as well as hierarchical methods for segmentation and detection in optical and electron microscopy images. Limitation of training data is one of the significant problems; hence, we explore solutions to learn better models with minimal user annotations.', 'corpus_id': 234146188, 'score': 1}, {'doc_id': '235360823', 'title': 'A survey on applications of deep learning in microscopy image analysis', 'abstract': 'Advanced microscopy enables us to acquire quantities of time-lapse images to visualize the dynamic characteristics of tissues, cells or molecules. Microscopy images typically vary in signal-to-noise ratios and include a wealth of information which require multiple parameters and time-consuming iterative algorithms for processing. Precise analysis and statistical quantification are often needed for the understanding of the biological mechanisms underlying these dynamic image sequences, which has become a big challenge in the field. As deep learning technologies develop quickly, they have been applied in bioimage processing more and more frequently. Novel deep learning models based on convolution neural networks have been developed and illustrated to achieve inspiring outcomes. This review article introduces the applications of deep learning algorithms in microscopy image analysis, which include image classification, region segmentation, object tracking and super-resolution reconstruction. We also discuss the drawbacks of existing deep learning-based methods, especially on the challenges of training datasets acquisition and evaluation, and propose the potential solutions. Furthermore, the latest development of augmented intelligent microscopy that based on deep learning technology may lead to revolution in biomedical research.', 'corpus_id': 235360823, 'score': 1}, {'doc_id': '235258809', 'title': 'Deep Learning-Based Phenotyping of Breast Cancer Cells Using Lens-free Digital In-line Holography', 'abstract': 'Lens-free digital in-line holography (LDIH) produces cellular diffraction patterns (holograms) with a large field of view that lens-based microscopes cannot offer. It is a promising diagnostic tool allowing comprehensive cellular analysis with high-throughput capability. Holograms are, however, far more complicated to discern by the human eye, and conventional computational algorithms to reconstruct images from hologram limit the throughput of hologram analysis. To efficiently and directly analyze holographic images from LDIH, we developed a novel deep learning architecture called a holographical deep learning network (HoloNet) for cellular phenotyping. The HoloNet uses holo-branches that extract large features from diffraction patterns and integrates them with small features from convolutional layers. Compared with other state-of-the-art deep learning methods, HoloNet achieved better performance for the classification and regression of the raw holograms of the breast cancer cells stained with well-known breast cancer markers, ER/PR and HER2. Moreover, we developed the HoloNet dual embedding model to extract high-level diffraction features related to breast cancer cell types and their marker intensities of ER/PR and HER2 to identify previously unknown subclusters of breast cancer cells. This hologram embedding allowed us to identify rare and subtle subclusters of the phenotypes overlapped by multiple breast cancer cell types. We demonstrate that our HoloNet efficiently enables LDIH to perform a more detailed analysis of heterogeneity of cell phenotypes for precise breast cancer diagnosis.', 'corpus_id': 235258809, 'score': 0}, {'doc_id': '235336198', 'title': 'Revealing the spatio-phenotypic patterning of cells in healthy and tumor tissues with mLSR-3D and STAPL-3D', 'abstract': 'Despite advances in three-dimensional (3D) imaging, it remains challenging to profile all the cells within a large 3D tissue, including the morphology and organization of the many cell types present. Here, we introduce eight-color, multispectral, large-scale single-cell resolution 3D (mLSR-3D) imaging and image analysis software for the parallelized, deep learning-based segmentation of large numbers of single cells in tissues, called segmentation analysis by parallelization of 3D datasets (STAPL-3D). Applying the method to pediatric Wilms tumor, we extract molecular, spatial and morphological features of millions of cells and reconstruct the tumor’s spatio-phenotypic patterning. In situ population profiling and pseudotime ordering reveals a highly disorganized spatial pattern in Wilms tumor compared to healthy fetal kidney, yet cellular profiles closely resembling human fetal kidney cells could be observed. In addition, we identify previously unreported tumor-specific populations, uniquely characterized by their spatial embedding or morphological attributes. Our results demonstrate the use of combining mLSR-3D and STAPL-3D to generate a comprehensive cellular map of human tumors.', 'corpus_id': 235336198, 'score': 0}, {'doc_id': '233334113', 'title': 'Pyramidal Deep Neural Networks for the Accurate Segmentation and Counting of Cells in Microscopy Data', 'abstract': 'Cell segmentation and counting represent one of the most important tasks required in order to provide an exhaustive understanding of biological images. Conventional features suffer the lack of spatial consistency by causing the joining of the cells and, thus, complicating the cell counting task. We propose, in this work, a cascade of networks that take as inputs different versions of the original image. After constructing a Gaussian pyramid representation of the microscopy data, the inputs of different size and spatial resolution are given to a cascade of deep convolutional autoencoders whose task is to reconstruct the segmentation mask. The coarse masks obtained from the different networks are summed up in order to provide the final mask. The principal and main contribution of this work is to propose a novel method for the cell counting. Unlike the majority of the methods that use the obtained segmentation mask as the prior information for counting, we propose to utilize the hidden latent representations, often called the high-level features, as the inputs of a neural network based regressor. While the segmentation part of our method performs as good as the conventional deep learning methods, the proposed cell counting approach outperforms the state-of-the-art methods.', 'corpus_id': 233334113, 'score': 1}, {'doc_id': '235208091', 'title': 'Graphic: Graph-Based Hierarchical Clustering For Single-Molecule Localization Microscopy', 'abstract': 'We propose a novel method for the clustering of point-cloud data that originate from single-molecule localization microscopy (SMLM). Our scheme has the ability to infer a hierarchical structure from the data. It takes a particular relevance when quantitatively analyzing the biological particles of interest at different scales. It assumes a prior neither on the shape of particles nor on the background noise. Our multiscale clustering pipeline is built upon graph theory. At each scale, we first construct a weighted graph that represents the SMLM data. Next, we find clusters using spectral clustering. We then use the output of this clustering algorithm to build the graph in the next scale; in this way, we ensure consistency over different scales. We illustrate our method with examples that highlight some of its important properties.', 'corpus_id': 235208091, 'score': 0}, {'doc_id': '227920591', 'title': 'YeastNet: Deep Learning Enabled Accurate Segmentation of Budding Yeast Cells in Bright-field Microscopy', 'abstract': 'Accurate and efficient segmentation of live-cell images is critical in maximising data extraction and knowledge generation from high-throughput biology experiments. Despite recent development of deep learning tools for biomedical imaging applications, great demand for automated segmentation tools for high-resolution live-cell microscopy images remains in order to accelerate the analysis. YeastNet dramatically improves the performance of non-trainable classic algorithm, and performs considerably better than the current state-of-the-art yeast cell segmentation tools. We have designed and trained a U-Net convolutional network (named YeastNet) to conduct semantic segmentation on bright-field microscopy images and generate segmentation masks for cell labelling and tracking. YeastNet enables accurate automatic segmentation and tracking of yeast cells in biomedical applications. YeastNet is freely provided with model weights as a Python package on GitHub. https://github.com/kaernlab/YeastNet', 'corpus_id': 227920591, 'score': 1}, {'doc_id': '233997893', 'title': 'Neural network strategies for plasma membrane selection in quantitative fluorescence microscopy images.', 'abstract': 'In recent years there has been an explosion of fluorescence microscopy studies of live cells in the literature. The analysis of the images obtained in these studies often requires labor-intensive manual annotation to extract meaningful information. In this study, we explore the utility of a neural network approach to recognize, classify, and select plasma membranes in high resolution images, thus greatly speeding up data analysis and reducing the need for personnel training for highly repetitive tasks. Two different strategies are tested: 1) a semantic segmentation strategy and 2) a sequential application of an object detector followed by a semantic segmentation network. Multiple network architectures are evaluated for each strategy, and the best performing solutions are combined and implemented in the Recognition Of Cellular Membranes (ROC-ME) software. We show that images annotated manually and with the ROC-ME software yield identical results, by comparing Förster resonance energy transfer (FRET) binding curves for the membrane protein FGFR3. The approach that we describe in this work can be applied to other image selection tasks in cell biology.', 'corpus_id': 233997893, 'score': 0}]
60	{'doc_id': '212730106', 'title': 'Diagnosis and clinical management of severe acute respiratory syndrome Coronavirus 2 (SARS-CoV-2) infection: an operational recommendation of Peking Union Medical College Hospital (V2.0)', 'abstract': 'ABSTRACT Since December 2019, China has been experiencing an outbreak of a new infectious disease caused by severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2). The clinical features include fever, coughing, shortness of breath, and inflammatory lung infiltration. China rapidly listed SARS-CoV-2-related pneumonia as a statutory infectious disease. To standardize the diagnosis and treatment of this new infectious disease, an operational recommendation for the diagnosis and management of SARS-CoV-2 infection is developed by Peking Union Medical College Hospital.', 'corpus_id': 212730106}	1089	[{'doc_id': '212739216', 'title': 'Emerging COVID-19 coronavirus: glycan shield and structure prediction of spike glycoprotein and its interaction with human CD26', 'abstract': 'ABSTRACT The recent outbreak of pneumonia-causing COVID-19 in China is an urgent global public health issue with an increase in mortality and morbidity. Here we report our modelled homo-trimer structure of COVID-19 spike glycoprotein in both closed (ligand-free) and open (ligand-bound) conformation, which is involved in host cell adhesion. We also predict the unique N- and O-linked glycosylation sites of spike glycoprotein that distinguish it from the SARS and underlines shielding and camouflage of COVID-19 from the host the defence system. Furthermore, our study also highlights the key finding that the S1 domain of COVID-19 spike glycoprotein potentially interacts with the human CD26, a key immunoregulatory factor for hijacking and virulence. These findings accentuate the unique features of COVID-19 and assist in the development of new therapeutics.', 'corpus_id': 212739216, 'score': 1}, {'doc_id': '214698372', 'title': 'Coronavirus infections: Epidemiological, clinical and immunological features and hypotheses', 'abstract': 'Coronaviruses (CoVs) are a large family of enveloped, positive-strand RNA viruses. Four human CoVs (HCoVs), the non-severe acute respiratory syndrome (SARS)-like HCoVs (namely HCoV 229E, NL63, OC43, and HKU1), are globally endemic and account for a substantial fraction of upper respiratory tract infections. Non-SARS-like CoV can occasionally produce severe diseases in frail subjects but do not cause any major (fatal) epidemics. In contrast, SARS like CoVs (namely SARS-CoV and Middle-East respiratory syndrome coronavirus, MERS-CoV) can cause intense short-lived fatal outbreaks. The current epidemic caused by the highly contagious SARS-CoV-2 and its rapid spread globally is of major concern. There is scanty knowledge on the actual pandemic potential of this new SARS-like virus. It might be speculated that SARS-CoV-2 epidemic is grossly underdiagnosed and that the infection is silently spreading across the globe with two consequences: (i) clusters of severe infections among frail subjects could haphazardly occur linked to unrecognized index cases; (ii) the current epidemic could naturally fall into a low-level endemic phase when a significant number of subjects will have developed immunity. Understanding the role of paucisymptomatic subjects and stratifying patients according to the risk of developing severe clinical presentations is pivotal for implementing reasonable measures to contain the infection and to reduce its mortality. Whilst the future evolution of this epidemic remains unpredictable, classic public health strategies must follow rational patterns. The emergence of yet another global epidemic underscores the permanent challenges that infectious diseases pose and underscores the need for global cooperation and preparedness, even during inter-epidemic periods.', 'corpus_id': 214698372, 'score': 1}, {'doc_id': '214611874', 'title': 'Carbamazepine solubility in supercritical CO2: A comprehensive study', 'abstract': 'Abstract In this paper we present our study of carbamazepine solubility in supercritical carbon dioxide. We have calculated the solubility values along two isochores corresponding to the CO2 densities ρ=1.1ρcr(CO2) and ρ=1.3ρcr(CO2), where ρcr(CO2) is the critical density of CO2, in the temperature range from 313 to 383 K, as well as along three isotherms at T=318, 328 and 348 K by an approach based on the classical density functional theory. The solubility values were also obtained using in situ IR spectroscopy and molecular dynamics simulations along the mentioned isochores and isotherms, respectively. Because the density functional theory only takes into account the Lennard-Jones interactions, it can be expected to underestimate the solubility values when compared to the experimental ones. However, we have shown that the data calculated within the classical density functional theory qualitatively reproduce the solubility trends obtained by IR spectroscopy and molecular dynamics simulation. Moreover, the obtained position of the upper crossover pressure is in good agreement with the experimental literature results.', 'corpus_id': 214611874, 'score': 0}, {'doc_id': '214612445', 'title': 'Composite Monte Carlo decision making under high uncertainty of novel coronavirus epidemic using hybridized deep learning and fuzzy rule induction☆', 'abstract': '\n Abstract\n \n In the advent of the novel coronavirus epidemic since December 2019, governments and authorities have been struggling to make critical decisions under high uncertainty at their best efforts. In computer science, this represents a typical problem of machine learning over incomplete or limited data in early epidemic Composite Monte-Carlo (CMC) simulation is a forecasting method which extrapolates available data which are broken down from multiple correlated/casual micro-data sources into many possible future outcomes by drawing random samples from some probability distributions. For instance, the overall trend and propagation of the infested cases in China are influenced by the temporal-spatial data of the nearby cities around the Wuhan city (where the virus is originated from), in terms of the population density, travel mobility, medical resources such as hospital beds and the timeliness of quarantine control in each city etc. Hence a CMC is reliable only up to the closeness of the underlying statistical distribution of a CMC, that is supposed to represent the behaviour of the future events, and the correctness of the composite data relationships. In this paper, a case study of using CMC that is enhanced by deep learning network and fuzzy rule induction for gaining better stochastic insights about the epidemic development is experimented. Instead of applying simplistic and uniform assumptions for a MC which is a common practice, a deep learning-based CMC is used in conjunction of fuzzy rule induction techniques. As a result, decision makers are benefited from a better fitted MC outputs complemented by min–max rules that foretell about the extreme ranges of future possibilities with respect to the epidemic.\n \n', 'corpus_id': 214612445, 'score': 1}, {'doc_id': '212730296', 'title': 'Era of molecular diagnosis for pathogen identification of unexplained pneumonia, lessons to be learned', 'abstract': 'ABSTRACT Unexplained pneumonia (UP) caused by a novel coronavirus SARS-CoV-2 (severe acute respiratory syndrome coronavirus 2) emerged in China in late December 2019 and has infected more than 9000 cases by 31 January 2020. Shanghai reported the first imported case of COVID-19 (Coronavirus Disease 2019) in 20 January 2020. A combinative approach of real-time RT–PCR, CRISPR-based assay and metagenomic next-generation sequencing (mNGS) were used to diagnose this unexplained pneumonia patient. Real-time RT–PCR and CRISPR-based assay both reported positive. This sample belonged to Betacoronavirus and shared a more than 99% nucleotide (nt) identity with the Wuhan SARS-CoV-2 isolates. We further compared pros and cons of common molecular diagnostics in UP. In this study, we illustrated the importance of combining molecular diagnostics to rule out common pathogens and performed mNGS to obtain unbiased potential pathogen result for the diagnosis of UP.', 'corpus_id': 212730296, 'score': 1}, {'doc_id': '214590831', 'title': '[Several suggestions of operation for colorectal cancer under the outbreak of corona virus disease 2019 in China].', 'abstract': 'Pneumonia caused by 2019-nCoV infection has been reported in Wuhan since December 2019, and spread rapidly across the country. The radical operation of colorectal cancer is semi-elective operation. Patients with colorectal cancer should receive operation as soon as possible after elective operation is resumed in each hospital. 2019-nCoV virus can be transmitted by asymptomatic infectors, and it has been confirmed to be transmitted by droplets and contact. However, fecal-oral transmission and aerosol transmission have not been excluded. Based on our experience with laparoscopic colorectal operation, we propose some surgery strategies for colorectal cancer patients under the corona virus disease 2019(COVID-19) situation: the screening process should be strictly carried out before surgery to reduce the risk of nosocomial infection in the later stage; laparoscopic-assisted surgery is recommended for radical surgery for patients with colorectal cancer; strict aerosol management must be made during the operation; natural orifice specimen extraction surgery and transanal total mesorectal excision are should be performed prudently; scientific and reasonable prophylactic stoma should be done; personnel protection in surgical ward and operation room must be strengthened.', 'corpus_id': 214590831, 'score': 0}, {'doc_id': '214611552', 'title': 'How to model honeybee population dynamics: stage structure and seasonality', 'abstract': 'Western honeybees (Apis Mellifera) serve extremely important roles in our ecosystem and economics as\xa0 they are responsible for pollinating $ 215 billion dollars annually over the world.\xa0 Unfortunately,\xa0 honeybee population and their colonies have been declined dramatically. The purpose of this article is to explore how we should model honeybee population with age structure and validate the model using empirical data so that we can identify different factors that lead to the survival and healthy of the honeybee colony.\xa0 Our theoretical study combined with simulations and data validation suggests that the proper age structure incorporated in the model\xa0 and seasonality are important for modeling honeybee population.\xa0 Specifically, our work implies that the model assuming that (1) the adult bees are survived from the egg population rather than the brood population; and (2) seasonality in the queen egg laying rate, give the better fit than other honeybee models. The related theoretical and numerical analysis of the most fit model indicate that (a) the survival of honeybee colonies requires a large queen egg-laying rate and smaller values of the other life history parameter values in addition to proper initial condition; (b) both brood and adult bee populations are increasing with respect to the increase in the egg-laying rate and the decreasing in other parameter values; and (c) seasonality may promote/suppress the survival of the honeybee colony.\xa0', 'corpus_id': 214611552, 'score': 0}, {'doc_id': '212729125', 'title': 'All eyes on Coronavirus—What do we need to know as ophthalmologists', 'abstract': '“Of course it is (a pandemic). We’re there. It doesn’t matter what kind of terminology you use. To me, the pandemic is a mindset. We can either use pandemic as a word that makes us all quake with fear or we can use it as a rallying cry to say—This is what we’re going to do to fight it.”—Dr. Michael Osterholm, Director, Center for Infectious Disease Research and Policy, University of Minnesota, MN, USA.', 'corpus_id': 212729125, 'score': 1}, {'doc_id': '212730137', 'title': 'Equine Coronavirus-Associated Colitis in Horses: A Retrospective Study', 'abstract': '\n Abstract\n \n Equine coronavirus (ECoV) is a known cause of fever, anorexia, and lethargy in adult horses. Although there are multiple reports of ECoV outbreaks, less is known about the clinical presentation of individual horses during a nonoutbreak situation. The purpose of this study was to describe the clinical presentation of horses diagnosed with ECoV infection that were not associated with an outbreak. Medical records of all horses admitted to Washington State University, Veterinary Teaching Hospital, during an 8-year period were reviewed (2010–2018). The five horses included in this study were older than 1 year of age, were diagnosed with colitis, tested positive for ECoV using real-time polymerase chain reaction, and were negative to other enteric pathogens. Interestingly, 4 of 5 horses had moderate to severe diarrhea,\xa03 had abnormal large colon ultrasonography, 2 had transient ventricular tachycardia and 2 had clinicopathologic evidence of liver dysfunction. ECoV should be included as a differential diagnosis for individual horses presenting with anorexia, fever, lethargy, and colitis. Early identification of ECoV cases is key to implement appropriate biosecurity measures to prevent the potential spread of this disease.\n \n', 'corpus_id': 212730137, 'score': 0}, {'doc_id': '211112058', 'title': 'Risk for Transportation of Coronavirus Disease from Wuhan to Other Cities in China', 'abstract': 'On January 23, 2020, China quarantined Wuhan to contain coronavirus disease (COVID-19). We estimated the probability of transportation of COVID-19 from Wuhan to 369 other cities in China before the quarantine. Expected COVID-19 risk is >50% in 130 (95% CI 89–190) cities and >99% in the 4 largest metropolitan areas.', 'corpus_id': 211112058, 'score': 0}]
61	{'doc_id': '221266554', 'title': 'Lossy Image Compression with Normalizing Flows', 'abstract': 'Deep learning based image compression has recently witnessed exciting progress and in some cases even managed to surpass transform coding based approaches that have been established and refined over many decades. However, state-of-the-art solutions for deep image compression typically employ autoencoders which map the input to a lower dimensional latent space and thus irreversibly discard information already before quantization. Due to that, they inherently limit the range of quality levels that can be covered. In contrast, traditional approaches in image compression allow for a larger range of quality levels. Interestingly, they employ an invertible transformation before performing the quantization step which explicitly discards information. Inspired by this, we propose a deep image compression method that is able to go from low bit-rates to near lossless quality by leveraging normalizing flows to learn a bijective mapping from the image space to a latent representation. In addition to this, we demonstrate further advantages unique to our solution, such as the ability to maintain constant quality results through re-encoding, even when performed multiple times. To the best of our knowledge, this is the first work to explore the opportunities for leveraging normalizing flows for lossy image compression.', 'corpus_id': 221266554}	4391	"[{'doc_id': '222066729', 'title': 'Enhanced Standard Compatible Image Compression Framework based on Auxiliary Codec Networks', 'abstract': 'To enhance image compression performance, recent deep neural network-based research can be divided into three categories: a learnable codec, a postprocessing network, and a compact representation network. The learnable codec has been designed for an end-to-end learning beyond the conventional compression modules. The postprocessing network increases the quality of decoded images using an example-based learning. The compact representation network is learned to reduce the capacity of an input image to reduce the bitrate while keeping the quality of the decoded image. However, these approaches are not compatible with the existing codecs or not optimal to increase the coding efficiency. Specifically, it is difficult to achieve optimal learning in the previous studies using the compact representation network, due to the inaccurate consideration of the codecs. In this paper, we propose a novel standard compatible image compression framework based on Auxiliary Codec Networks (ACNs). ACNs are designed to imitate image degradation operations of the existing codec, which delivers more accurate gradients to the compact representation network. Therefore, the compact representation and the postprocessing networks can be learned effectively and optimally. We demonstrate that our proposed framework based on JPEG and High Efficiency Video Coding (HEVC) standard substantially outperforms existing image compression algorithms in a standard compatible manner.', 'corpus_id': 222066729, 'score': 1}, {'doc_id': '233210102', 'title': 'Soft then Hard: Rethinking the Quantization in Neural Image Compression', 'abstract': 'Quantization is one of the core components in lossy image compression. For neural image compression, end-to-end optimization requires differentiable approximations of quantization, which can generally be grouped into three categories: additive uniform noise, straight-through estimator and soft-to-hard annealing. Training with additive uniform noise approximates the quantization error variationally but suffers from the train-test mismatch. The other two methods do not encounter this mismatch but, as shown in this paper, hurt the rate-distortion performance since the latent representation ability is weakened. We thus propose a novel soft-then-hard quantization strategy for neural image compression that first learns an expressive latent space softly, then eliminates the train-test mismatch with hard quantization. In addition, beyond the fixed integer quantization, we apply scaled additive uniform noise to adaptively control the quantization granularity by deriving a new variational upper bound on actual rate. Experiments demonstrate that our proposed methods are easy to adopt, stable to train, and highly effective especially on complex compression models.', 'corpus_id': 233210102, 'score': 1}, {'doc_id': '232478725', 'title': 'Distributed Video Adaptive Block Compressive Sensing', 'abstract': 'Video block compressive sensing has been studied for use in resource constrained scenarios, such as wireless sensor networks, but the approach still suffers from low performance and long reconstruction time. Inspired by classical distributed video coding, we design a lightweight encoder with computationally intensive operations, such as video frame interpolation, performed at the decoder. Straying from recent trends in training end-to-end neural networks, we propose two algorithms that leverage convolutional neural network components to reconstruct video with greatly reduced reconstruction time. At the encoder, we leverage temporal correlation between frames and deploy adaptive techniques based on compressive measurements from previous frames. At the decoder, we exploit temporal correlation by using video frame interpolation and temporal differential pulse code modulation. Simulations show that our two proposed algorithms, VAL-VFI and VAL-IDA-VFI reconstruct higher quality video, achieving state-of-theart performance. INDEX TERMS Distributed Video Compressive Sensing, Adaptive Block Compressive Sensing', 'corpus_id': 232478725, 'score': 0}, {'doc_id': '226289871', 'title': 'Conceptual Compression via Deep Structure and Texture Synthesis', 'abstract': 'Existing compression methods typically focus on the removal of signal-level redundancies, while the potential and versatility of decomposing visual data into compact conceptual components still lack further study. To this end, we propose a novel conceptual compression framework that encodes visual data into compact structure and texture representations, then decodes in a deep synthesis fashion, aiming to achieve better visual reconstruction quality, flexible content manipulation, and potential support for various vision tasks. In particular, we propose to compress images by a dual-layered model consisting of two complementary visual features: 1) structure layer represented by structural maps and 2) texture layer characterized by low-dimensional deep representations. At the encoder side, the structural maps and texture representations are individually extracted and compressed, generating the compact, interpretable, inter-operable bitstreams. During the decoding stage, a hierarchical fusion GAN (HF-GAN) is proposed to learn the synthesis paradigm where the textures are rendered into the decoded structural maps, leading to high-quality reconstruction with remarkable visual realism. Extensive experiments on diverse images have demonstrated the superiority of our framework with lower bitrates, higher reconstruction quality, and increased versatility towards visual analysis and content manipulation tasks.', 'corpus_id': 226289871, 'score': 1}, {'doc_id': '218581237', 'title': 'High Resolution Face Age Editing', 'abstract': 'Face age editing has become a crucial task in film post-production, and is also becoming popular for general purpose photography. Recently, adversarial training has produced some of the most visually impressive results for image manipulation, including the face aging/de-aging task. In spite of considerable progress, current methods often present visual artifacts and can only deal with low-resolution images. In order to achieve aging/de-aging with the high quality and robustness necessary for wider use, these problems need to be addressed. This is the goal of the present work. We present an encoder-decoder architecture for face age editing. The core idea of our network is to encode a face image to age-invariant features, and learn a modulation vector corresponding to a target age. We then combine these two elements to produce a realistic image of the person with the desired target age. Our architecture is greatly simplified with respect to other approaches, and allows for fine-grained age editing on high resolution images in a single unified model. Source codes are available at https://github.com/InterDigitalInc/HRFAE.', 'corpus_id': 218581237, 'score': 0}, {'doc_id': '218595892', 'title': 'Jigsaw-VAE: Towards Balancing Features in Variational Autoencoders', 'abstract': ""The latent variables learned by VAEs have seen considerable interest as an unsupervised way of extracting features, which can then be used for downstream tasks. There is a growing interest in the question of whether features learned on one environment will generalize across different environments. We demonstrate here that VAE latent variables often focus on some factors of variation at the expense of others - in this case we refer to the features as ``imbalanced''. Feature imbalance leads to poor generalization when the latent variables are used in an environment where the presence of features changes. Similarly, latent variables trained with imbalanced features induce the VAE to generate less diverse (i.e. biased towards dominant features) samples. To address this, we propose a regularization scheme for VAEs, which we show substantially addresses the feature imbalance problem. We also introduce a simple metric to measure the balance of features in generated images."", 'corpus_id': 218595892, 'score': 0}, {'doc_id': '232223020', 'title': 'Thousand to One: Semantic Prior Modeling for Conceptual Coding', 'abstract': 'Conceptual coding has been an emerging research topic recently, which encodes natural images into disentangled conceptual representations for compression. However, the compression performance of the existing methods is still suboptimal due to the lack of comprehensive consideration of rate constraint and reconstruction quality. To this end, we propose a novel end-to-end semantic prior modeling based conceptual coding scheme towards extremely low bitrate image compression, which leverages semantic-wise deep representations as a unified prior for entropy estimation and texture synthesis. Specifically, we employ semantic segmentation maps as structural guidance for extracting deep semantic prior, which provides fine-grained texture distribution modeling for better detail construction and higher flexibility in subsequent highlevel vision tasks. Moreover, a cross-channel entropy model is proposed to further exploit the inter-channel correlation of the spatially independent semantic prior, leading to more accurate entropy estimation for rate-constrained training. The proposed scheme1 achieves an ultra-high 1000× compression ratio, while still enjoying high visual reconstruction quality and versatility towards visual processing and analysis tasks.', 'corpus_id': 232223020, 'score': 1}, {'doc_id': '233296356', 'title': 'Conditional Coding and Variable Bitrate for Practical Learned Video Coding', 'abstract': 'This paper introduces a practical learned video codec. Conditional coding and quantization gain vectors are used to provide flexibility to a single encoder/decoder pair, which is able to compress video sequences at a variable bitrate. The flexibility is leveraged at test time by choosing the rate and GOP structure to optimize a rate-distortion cost. Using the CLIC21 video test conditions, the proposed approach shows performance on par with HEVC.', 'corpus_id': 233296356, 'score': 1}, {'doc_id': '218581433', 'title': 'Hierarchical Regression Network for Spectral Reconstruction from RGB Images', 'abstract': 'Capturing visual image with a hyperspectral camera has been successfully applied to many areas due to its narrowband imaging technology. Hyperspectral reconstruction from RGB images denotes a reverse process of hyperspectral imaging by discovering an inverse response function. Current works mainly map RGB images directly to corresponding spectrum but do not consider context information explicitly. Moreover, the use of encoder-decoder pair in current algorithms leads to loss of information. To address these problems, we propose a 4-level Hierarchical Regression Network (HRNet) with PixelShuffle layer as inter-level interaction. Furthermore, we adopt a residual dense block to remove artifacts of real world RGB images and a residual global block to build attention mechanism for enlarging perceptive field. We evaluate proposed HRNet with other architectures and techniques by participating in NTIRE 2020 Challenge on Spectral Reconstruction from RGB Images. The HRNet is the winning method of track 2 - real world images and ranks 3rd on track 1 - clean images.', 'corpus_id': 218581433, 'score': 0}, {'doc_id': '233405268', 'title': 'GRACE: A Compressed Communication Framework for Distributed Machine Learning', 'abstract': 'Powerful computer clusters are used nowadays to train complex deep neural networks (DNN) on large datasets. Distributed training increasingly becomes communication bound. For this reason, many lossy compression techniques have been proposed to reduce the volume of transferred data. Unfortunately, it is difficult to argue about the behavior of compression methods, because existing work relies on inconsistent evaluation testbeds and largely ignores the performance impact of practical system configurations. In this paper, we present a comprehensive survey of the most influential compressed communication methods for DNN training, together with an intuitive classification (i.e., quantization, sparsification, hybrid and low-rank). Next, we propose GRACE, a unified framework and API that allows for consistent and easy implementation of compressed communication on popular machine learning toolkits. We instantiate GRACE on TensorFlow and PyTorch, and implement 16 such methods. Finally, we present a thorough quantitative evaluation with a variety of DNNs (convolutional and recurrent), datasets and system configurations. We show that the DNN architecture affects the relative performance among methods. Interestingly, depending on the underlying communication library and computational cost of compression / decompression, we demonstrate that some methods may be impractical. GRACE and the entire benchmarking suite are available as open-source.', 'corpus_id': 233405268, 'score': 0}]"
62	"{'doc_id': '237142449', 'title': ""Relativistic Mechanics Theory for Electrons that Exhibits Spin, Zitterbewegung, Superposition and Produces Dirac's Wave Equation"", 'abstract': 'A neo-classical relativistic mechanics model is presented where the spin of an electron is a natural part of its world space-time path as a point particle. The fourth-order equation of motion corresponds to the same Lagrangian function in proper time as in special relativity except for an additional spin energy term. The dynamic variables give a complete description of the electron in the classical mechanics tradition (hence “neo-classical"") that explains its spin, Schrödinger’s zitterbewegung and the presence of a magnetic moment. The total motion can be decomposed into a sum of a local circular motion about a point and a global motion of this point, which is called here the spin center. The local spin motion corresponds to Schrödinger’s zitterbewegung and is a perpetual circular motion relative to a reference frame fixed at the spin center. This local motion produces magnetic and electric dipoles through the Lorentz force on the electron’s point charge. The global motion is sub-luminal and described by Newton’s second law in proper time, the time for a clock fixed at the spin center, while the total motion occurs at the speed of light c, consistent with the eigenvalues of Dirac’s velocity operators having magnitude c. A spin tensor is introduced that is the angular momentum of the electron’s total motion about its spin center. The fundamental equations of motion re-written in an equivalent form using this spin tensor are identical to those of the Barut-Zanghi theory, which is then used to express the equations of motion in an equivalent operator form applied to a state function of proper time satisfying a neo-classical Dirac-Schrödinger spinor equation. This state function produces the dynamic variables from the same operators as in Dirac’s theory for the electron but with deterministic superpositions. It leads to a neo-classical wave function that satisfies Dirac’s relaPreprint submitted to arXiv.org August 18, 2021 tivistic wave equation for the free electron by applying the Lorentz transformation to express proper time in the state function in terms of an observer’s space-time coordinates. In summary, the presented neo-classical theory provides a complete hidden-variable model for spin that leads to Dirac’s relativistic wave equation for the free electron and that explains the electron’s moment coupling to an electro-magnetic field, albeit with a magnetic moment that is one half of that in Dirac’s theory.', 'corpus_id': 237142449}"	20157	[{'doc_id': '237439358', 'title': 'Influence of cosmological expansion in local experiments', 'abstract': 'Whether the cosmological expansion can influence the local dynamics, below the galaxy clusters scale, has been the subject of intense investigations in the past three decades. In this work, we consider McVittie and Kottler spacetimes, embedding a spherical object in a FLRW spacetime. We calculate the influence of the cosmological expansion on the frequency shift of a resonator and estimate its effect on the exchange of light signals between local observers. In passing, we also clarify some of the statements made in the literature. 1 ar X iv :2 10 9. 03 28 0v 1 [ gr -q c] 7 S ep 2 02 1', 'corpus_id': 237439358, 'score': 0}, {'doc_id': '120171787', 'title': 'Derivation of Sommerfeld-Dirac Fine-Structure Formula by WKB Method', 'abstract': None, 'corpus_id': 120171787, 'score': 1}, {'doc_id': '124594045', 'title': 'Central Forces and Kepler’s Laws', 'abstract': 'From the historical perspective, the derivation of three Kepler’s laws in Classical Mechanics is one of the most relevant calculations which were ever done. We shall present this calculation in details and also give a simple treatment of the effect of Precession of Perihelion for a nearly circular orbit for a weakly non-Newtonian gravitational force. This problem has very special importance in General Relativity due to the Precession of Perihelion for the Mercury and some other relativistic tests.', 'corpus_id': 124594045, 'score': 1}, {'doc_id': '236098949', 'title': 'Making relativistic quantum mechanics simple', 'abstract': 'The fundamentals of a quasi-relativistic wave equation, whose solutions match the Schrödinger results for slow-moving particles but are also valid when the particle moves at relativistic speeds, are discussed. This quasi-relativistic wave equation is then used for examining some interesting quantum problems where the introduction of relativistic considerations may produce remarkable consequences. We argue in favor of the academic use of this equation, for introducing students to the implications of the special theory of relativity in introductory quantum mechanics courses.', 'corpus_id': 236098949, 'score': 1}, {'doc_id': '236907382', 'title': 'Mechanics Notes: Landau & Lifshitz', 'abstract': 'Dynamically, Lagrangians are equivalent up to a total time derivative of a function of coordinates and time. So the Lagrangians L(q, q̇, t) and L′(q, q̇, t) = L(q, q̇, t) + d f (q, t) / dt yield the same equations of motion, because the extra term of L′ gives a constant term in the action that vanishes upon minimization. Rescaling by a constant also has no effect upon the motion (arbitrary units).', 'corpus_id': 236907382, 'score': 0}, {'doc_id': '236942570', 'title': 'An “ab initio” Model for Quantum Theory and Relativity', 'abstract': 'The paper introduces a theoretical model aimed to show how the relativity can be made consistent with the non reality and non locality of the quantum physics. The concepts of quantization and superposition of states, usually regarded as distinctive properties of the quantum world, can be extended also to the relativity.', 'corpus_id': 236942570, 'score': 1}, {'doc_id': '236973551', 'title': 'A compactified (almost popular) description of the unified fundamental interaction based on the Maxwell electromagnetism', 'abstract': 'Why two particles electrically charged with the charges of opposite polarity attract each other and those charged with the charges of the same polarity are mutually repelent? Why is the gravity exclusively the attractive force? Why there is inertia of mass, but no inertia of electric charge? What is an intrinsic relationship between the electric and gravitational interaction? How can the structure of the atom be described within the theories of macroscosm as Maxwell theory of electromagnetism and/or general relativity? This five and several further fundamental questions are attempted to be answered by the authors of the most advanced physical theories. Here, we present an extract of the main results of our recent work [1] and [2], which addresses and answers these and several further questions in the case of the interaction between two stable (or accelerating from the rest), electrically charged, point-like particles. The work does not bring any principially new theory. It is a new, unitary representation of the well-known Maxwell theory of electromagnetism, completed with some formulas of general relativity. The reader likely asks, why he or she should preferably devote his or her attention just to this work published among tens, maybe hundreds, other works published every year, which also address the above mentioned questions? After the Dirac’s quantum theory, the unitary representation is only the second theory ever providing, in an independent way, the exact theoretical determination of the energy states in the spectrum of hydrogen atom (with the same precision as Dirac’s theory provides). Moreover, one can hardly find another theory which would answer all the asked questions at the same time, within a single framework. The new representation remarkably applies the Occam’s razor: to answer the questions, one can forget, in principle, the whole physics developed after Maxwell and Einstein, a major part of quantum physics including. The representation is, in fact, a continuation of the main-stream physics of the beginning of 20-th century. Of course, the value of the knowledge achieved in the post-Maxwell and post-Einstein era is not lost, a lot of mathematical procedures and partial concepts from the post eras is utilized also within the new approach.', 'corpus_id': 236973551, 'score': 0}, {'doc_id': '119224385', 'title': 'Relativistic corrections to the central force problem in a generalized potential approach', 'abstract': 'We present a novel technique to obtain relativistic corrections to the central force problem in the Lagrangian formulation, using a generalized potential energy function. We derive a general expression for a generalized potential energy function for all powers of the velocity, which when made a part of the regular classical Lagrangian can reproduce the correct (relativistic) force equation. We then go on to derive the Hamiltonian and estimate the corrections to the total energy of the system up to the fourth power in $|\\vec{v}|/c$. We found that our work is able to provide a more comprehensive understanding of relativistic corrections to the central force results and provides corrections to both the kinetic and potential energy of the system. We employ our methodology to calculate relativistic corrections to the circular orbit under the gravitational force and also the first-order corrections to the ground state energy of the hydrogen atom using a semi-classical approach. Our predictions in both problems give reasonable agreement with the known results. Thus we feel that this work has pedagogical value and can be used by undergraduate students to better understand the central force and the relativistic corrections to it.', 'corpus_id': 119224385, 'score': 1}]
63	{'doc_id': '235391024', 'title': 'Enforcing Morphological Information in Fully Convolutional Networks to Improve Cell Instance Segmentation in Fluorescence Microscopy Images', 'abstract': 'Cell instance segmentation in fluorescence microscopy images is becoming essential for cancer dynamics and prognosis. Data extracted from cancer dynamics allows to understand and accurately model different metabolic processes such as proliferation. This enables customized and more precise cancer treatments. However, accurate cell instance segmentation, necessary for further cell tracking and behavior analysis, is still challenging in scenarios with high cell concentration and overlapping edges. Within this framework, we propose a novel cell instance segmentation approach based on the well-known U-Net architecture. To enforce the learning of morphological information per pixel, a deep distance transformer (DDT) acts as a back-bone model. The DDT output is subsequently used to train a top-model. The following top-models are considered: a three-class (e.g., foreground, background and cell border) U-net, and a watershed transform. The obtained results suggest a performance boost over traditional U-Net architectures. This opens an interesting research line around the idea of injecting morphological information into a fully convolutional model.', 'corpus_id': 235391024}	11033	"[{'doc_id': '235321093', 'title': 'U-Net Deep-Learning-Based 3D Cell Counter for the Quality Control of 3D Cell-Based Assays through Seed Cell Measurement', 'abstract': 'Conventional cell-counting software uses contour or watershed segmentations and focuses on identifying two-dimensional (2D) cells attached on the bottom of plastic plates. Recently developed software has been useful tools for the quality control of 2D cell-based assays by measuring initial seed cell numbers. These algorithms do not, however, quantitatively test in three-dimensional (3D) cell-based assays using extracellular matrix (ECM), because cells are aggregated and overlapped in the 3D structure of the ECM such as Matrigel, collagen, and alginate. Such overlapped and aggregated cells make it difficult to segment cells and to count the number of cells accurately. It is important, however, to determine the number of cells to standardize experiments and ensure the reproducibility of 3D cell-based assays. In this study, we apply a 3D cell-counting method using U-net deep learning to high-density aggregated cells in ECM to identify initial seed cell numbers. The proposed method showed a 10% counting error in high-density aggregated cells, while the contour and watershed segmentations showed 30% and 40% counting errors, respectively. Thus, the proposed method can reduce the seed cell-counting error in 3D cell-based assays by providing the exact number of cells to researchers, thereby enabling the acquisition of quality control in 3D cell-based assays.', 'corpus_id': 235321093, 'score': 0}, {'doc_id': '235636153', 'title': 'Evaluating Very Deep Convolutional Neural Networks for Nucleus Segmentation from Brightfield Cell Microscopy Images', 'abstract': 'Advances in microscopy have increased output data volumes, and powerful image analysis methods are required to match. In particular, finding and characterizing nuclei from microscopy images, a core cytometry task, remains difficult to automate. While deep learning models have given encouraging results on this problem, the most powerful approaches have not yet been tested for attacking it. Here, we review and evaluate state-of-the-art very deep convolutional neural network architectures and training strategies for segmenting nuclei from brightfield cell images. We tested U-Net as a baseline model; considered U-Net++, Tiramisu, and DeepLabv3+ as latest instances of advanced families of segmentation models; and propose PPU-Net, a novel light-weight alternative. The deeper architectures outperformed standard U-Net and results from previous studies on the challenging brightfield images, with balanced pixel-wise accuracies of up to 86%. PPU-Net achieved this performance with 20-fold fewer parameters than the comparably accurate methods. All models perform better on larger nuclei and in sparser images. We further confirmed that in the absence of plentiful training data, augmentation and pretraining on other data improve performance. In particular, using only 16 images with data augmentation is enough to achieve a pixel-wise F1 score that is within 5% of the one achieved with a full data set for all models. The remaining segmentation errors are mainly due to missed nuclei in dense regions, overlapping cells, and imaging artifacts, indicating the major outstanding challenges.', 'corpus_id': 235636153, 'score': 1}, {'doc_id': '235461497', 'title': 'SAU-Net: A Unified Network for Cell Counting in 2D and 3D Microscopy Images.', 'abstract': 'Image-based cell counting is a fundamental yet challenging task with wide applications in biological research. In this paper, we propose a novel unified deep network framework designed to solve this problem for various cell types in both 2D and 3D images. Specifically, we first propose SAU-Net for cell counting by extending the segmentation network U-Net with a Self-Attention module. Second, we design an extension of Batch Normalization (BN) to facilitate the training process for small datasets. In addition, a new 3D benchmark dataset based on the existing mouse blastocyst (MBC) dataset is developed and released to the community. Our SAU-Net achieves state-of-the-art results on four benchmark 2D datasets - synthetic fluorescence microscopy (VGG) dataset, Modified Bone Marrow (MBM) dataset, human subcutaneous adipose tissue (ADI) dataset, and Dublin Cell Counting (DCC) dataset, and the new 3D dataset, MBC. The BN extension is validated using extensive experiments on the 2D datasets, since GPU memory constraints preclude use of 3D datasets. The source code is available at https://github.com/mzlr/sau-net.', 'corpus_id': 235461497, 'score': 1}, {'doc_id': '235414098', 'title': 'Assessment of deep learning algorithms for 3D instance segmentation of confocal image datasets', 'abstract': 'Segmenting three dimensional microscopy images is essential for understanding phenomena like morphogenesis, cell division, cellular growth and genetic expression patterns. Recently, deep learning (DL) pipelines have been developed which claim to provide high accuracy segmentation of cellular images and are increasingly considered as the state-of-the-art for image segmentation problems. However, it remains difficult to define their relative performance as the concurrent diversity and lack of uniform evaluation strategies makes it difficult to know how their results compare. In this paper, we first made an inventory of the available DL methods for 3D segmentation. We next implemented and quantitatively compared a number of representative DL pipelines, alongside a highly efficient non-DL method named MARS. The DL methods were trained on a common dataset of 3D cellular confocal microscopy images. Their segmentation accuracies were also tested in the presence of different image artifacts. A new method for segmentation quality evaluation was adopted which isolates segmentation errors due to under/over segmentation. This is complemented with new visualization strategies that make interactive exploration of segmentation quality possible. Our analysis shows that the DL pipelines have very different levels of accuracy. Two of them show high performance, and offer clear advantages in terms of adaptability to new data.', 'corpus_id': 235414098, 'score': 0}, {'doc_id': '233311471', 'title': 'A deep learning approach for mitosis detection: Application in tumor proliferation prediction from whole slide images', 'abstract': ""The tumor proliferation, which is correlated with tumor grade, is a crucial biomarker indicative of breast cancer patients' prognosis. The most commonly used method in predicting tumor proliferation speed is the counting of mitotic figures in Hematoxylin and Eosin (H&E) histological slides. Manual mitosis counting is known to suffer from reproducibility problems. This paper presents a fully automated system for tumor proliferation prediction from whole slide images via mitosis counting. First, by considering the epithelial tissue as mitosis activity regions, we build a deep-learning-based region of interest detection method to select the high mitosis activity regions from whole slide images. Second, we learned a set of deep neural networks to detect mitosis detection from selected areas. The proposed mitosis detection system is designed to effectively overcome the mitosis detection challenges by two novel deep preprocessing and two-step hard negative mining approaches. Third, we trained a Support Vector Machine (SVM) classifier to predict the final tumor proliferation score. The proposed method was evaluated on the dataset of the Tumor Proliferation Assessment Challenge (TUPAC16) and achieved a 73.81 % F-measure and 0.612 weighted kappa score, respectively, outperforming all previous approaches significantly. Experimental results demonstrate that the proposed system considerably improves the tumor proliferation prediction accuracy and provides a reliable automated tool to support health care make-decisions."", 'corpus_id': 233311471, 'score': 0}, {'doc_id': '235258809', 'title': 'Deep Learning-Based Phenotyping of Breast Cancer Cells Using Lens-free Digital In-line Holography', 'abstract': 'Lens-free digital in-line holography (LDIH) produces cellular diffraction patterns (holograms) with a large field of view that lens-based microscopes cannot offer. It is a promising diagnostic tool allowing comprehensive cellular analysis with high-throughput capability. Holograms are, however, far more complicated to discern by the human eye, and conventional computational algorithms to reconstruct images from hologram limit the throughput of hologram analysis. To efficiently and directly analyze holographic images from LDIH, we developed a novel deep learning architecture called a holographical deep learning network (HoloNet) for cellular phenotyping. The HoloNet uses holo-branches that extract large features from diffraction patterns and integrates them with small features from convolutional layers. Compared with other state-of-the-art deep learning methods, HoloNet achieved better performance for the classification and regression of the raw holograms of the breast cancer cells stained with well-known breast cancer markers, ER/PR and HER2. Moreover, we developed the HoloNet dual embedding model to extract high-level diffraction features related to breast cancer cell types and their marker intensities of ER/PR and HER2 to identify previously unknown subclusters of breast cancer cells. This hologram embedding allowed us to identify rare and subtle subclusters of the phenotypes overlapped by multiple breast cancer cell types. We demonstrate that our HoloNet efficiently enables LDIH to perform a more detailed analysis of heterogeneity of cell phenotypes for precise breast cancer diagnosis.', 'corpus_id': 235258809, 'score': 0}, {'doc_id': '231856314', 'title': 'ChipSeg: An Automatic Tool to Segment Bacterial and Mammalian Cells Cultured in Microfluidic Devices', 'abstract': 'Extracting quantitative measurements from time-lapse images is necessary in external feedback control applications, where segmentation results are used to inform control algorithms. We describe ChipSeg, a computational tool that segments bacterial and mammalian cells cultured in microfluidic devices and imaged by time-lapse microscopy, which can be used also in the context of external feedback control. The method is based on thresholding and uses the same core functions for both cell types. It allows us to segment individual cells in high cell density microfluidic devices, to quantify fluorescent protein expression over a time-lapse experiment, and to track individual mammalian cells. ChipSeg enables robust segmentation in external feedback control experiments and can be easily customized for other experimental settings and research aims.', 'corpus_id': 231856314, 'score': 1}, {'doc_id': '229220699', 'title': 'TEMImageNet and AtomSegNet Deep Learning Training Library and Models for High-Precision Atom Segmentation, Localization, Denoising, and Super-resolution Processing of Atom-Resolution Scanning TEM Images', 'abstract': 'Atom segmentation and localization, noise reduction and super-resolution processing of atomic-resolution scanning transmission electron microscopy (STEM) images with high precision and robustness is a challenging task. Although several conventional algorithms, such has thresholding, edge detection and clustering, can achieve reasonable performance in some predefined sceneries, they tend to fail when interferences from the background are strong and unpredictable. Particularly, for atomic-resolution STEM images, so far there is no well-established algorithm that is robust enough to segment or detect all atomic columns when there is large thickness variation in a recorded image. Herein, we report the development of a training library and a deep learning method that can perform robust and precise atom segmentation, localization, denoising, and super-resolution processing of experimental images. Despite using simulated images as training datasets, the deep-learning model can self-adapt to experimental STEM images and shows outstanding performance in atom detection and localization in challenging contrast conditions and the precision is consistently better than the state-of-the-art two-dimensional Gaussian fit method. Taking a step further, we have deployed our deep-learning models to a desktop app with a graphical user interface and the app is free and open-source. We have also built a TEM ImageNet project website for easy browsing and downloading of the training data.', 'corpus_id': 229220699, 'score': 0}, {'doc_id': '237378594', 'title': 'Spatio-temporal feature based deep neural network for cell lineage analysis in microscopy images', 'abstract': 'Background Time-lapse microscopy has been widely used in biomedical experiments because it can visualize the molecular activities of living cells in real time. However, biomedical researchers are still conducting cell lineage analysis manually. Developing automatic lineage tracing algorithms is a challenging task. In the past two decades, deep neural networks (DNNs) became have shown outstanding performance on computer vision tasks. They can learn complex visual features, capture long-range temporal dependencies, and have the potential to be used for automatic cell lineage analysis. Methods In this study, we propose a multi-task spatio-temporal feature based deep neural network for cell lineages analysis (Cell-STN). The Cell-STN extracts spatio-temporal features from microscopy image sequences by leveraging our convolutional long short-term memory based core block. And the proposed Cell-STN utilized a task specific network to predict the cell location, the mitosis event, and the apoptosis event in a multi-task manner. Results We evaluated the Cell-STN on three in-house datasets (MCF7, U2OS, and HCT116) and one public dataset (Fluo-N2DL-HeLa). For cell tracking, we used peak-wise precision, track-wise precision, end-peak precision, and spatial distance as metrics. The overall results showed the Cell-STN models outperform other state-of-the-art cell trackers. For mitosis and apoptosis tasks, we used accuracy, F1-score, temporal distance, and spatial distance as metrics. The Cell-STN models achieved the highest performance on all datasets. Conclusion This study presented a novel DNNs approach for cell lineage analysis in microscopy images. The Cell-STN showed outstanding performance on the four datasets. Additionally, the Cell-STN required minimal training data and can be adapted to new biological event detection tasks by appending task-specific layers. This algorithm has the potential to be used in real-world biomedical research.', 'corpus_id': 237378594, 'score': 1}, {'doc_id': '227118944', 'title': 'Synthetic Image Rendering Solves Annotation Problem in Deep Learning Nanoparticle Segmentation', 'abstract': 'Nanoparticles occur in various environments as a consequence of man-made processes, which raises concerns about their impact on the environment and human health. To allow for proper risk assessment, a precise and statistically relevant analysis of particle characteristics (such as e.g. size, shape and composition) is required that would greatly benefit from automated image analysis procedures. While deep learning shows impressive results in object detection tasks, its applicability is limited by the amount of representative, experimentally collected and manually annotated training data. Here, we present an elegant, flexible and versatile method to bypass this costly and tedious data acquisition process. We show that using a rendering software allows to generate realistic, synthetic training data to train a state-of-the art deep neural network. Using this approach, we derive a segmentation accuracy that is comparable to man-made annotations for toxicologically relevant metal-oxide nanoparticle ensembles which we chose as examples. Our study paves the way towards the use of deep learning for automated, high-throughput particle detection in a variety of imaging techniques such as microscopies and spectroscopies, for a wide variety of studies and applications, including the detection of plastic micro- and nanoparticles.', 'corpus_id': 227118944, 'score': 0}]"
64	{'doc_id': '94076467', 'title': 'Effect of Silica Nanoparticles on the Photoluminescence Properties of BCNO Phosphor', 'abstract': 'Effect of additional silica nanoparticles on the photoluminescence (PL) performance of boron carbon oxy‐nitride (BCNO) phosphor was investigated. As a precursor, boric acid and urea were used as boron and nitrogen sources, respectively. The carbon sources was polyethylene glycol (PEG) with average molecule weight 20000 g/mol.. Precursor solutions were prepared by mixing these raw materials in pure water, followed by stirring to achieve homogeneous solutions. In this precursor, silica nanoparticles were added at various mass ratio from 0 to 7 %wt in the solution. The precursors were then heated at 750\u2009°C for 60 min in a ceramic crucible under atmospheric pressure. The photoluminescence (PL) spectrum that characterized by spectrophotometer showed a single, distinct, and broad emission band varied from blue to near red color, depend on the PEG, boric acid and urea ratio in the precursor. The addition of silica nanoparticles caused the increasing of PL intensity as well as the shifting of peak wavelength of P...', 'corpus_id': 94076467}	20071	[{'doc_id': '236496172', 'title': 'Efficient Full-Color Boron Nitride Quantum Dots for Thermostable Flexible Displays.', 'abstract': 'Hexagonal boron nitride quantum dot (BNQD) has aroused great interest in the optoelectronics field due to their metal-free nature with promising optical properties. However, it has been a great challenge to modulate its photoluminescence to the long-wavelength region so far. Herein, BNQDs with full-color emission (420-610 nm) have been implemented by doping diverse amino ligands in different solvents for the first attempt. This color variation from blue, green, yellow-green, yellow to red is ascribed to the surface states tunable via amination degree. Attractively, the quantum yield of our blue BNQDs has set a record at 32.27%, and rare yellow-green BNQDs have been demonstrated. Combining good thermal dissipation capability and high transparency, our full-color BNQD holds great potential for transparent flexible display and security labels at the elevated temperature.', 'corpus_id': 236496172, 'score': 1}, {'doc_id': '237008945', 'title': 'High stability ultra-narrow band self-activated KGaSiO4 long-persistent phosphors for optical anti-counterfeiting.', 'abstract': 'Optical anti-counterfeiting has been developed as a promising optical-sensing technique. A self-activated KGaSiO4 phosphor was successfully prepared using the traditional solid-state method. The photoluminescence spectra of the as-synthesized phosphors indicate that the ultra-narrow band emission with green light peak at 503 nm is obtained when phosphors are excited by 254 nm UV light. Additionally, the measured afterglow curve shows that the emission of this phosphor can last more than 1200 s after UV excitation stops, which indicates that KGaSiO4 is a potential candidate for anti-counterfeiting materials. The luminescent and decay mechanism are discussed by theoretical calculation and thermo-luminescent spectra in detail. The theoretical model can provide support for explaining the mechanism of narrow band or persistent phosphor.', 'corpus_id': 237008945, 'score': 0}, {'doc_id': '236295485', 'title': 'Luminescence investigation of a novel red-emitting Sr3NaSbO6:Eu3+ phosphor', 'abstract': 'Abstract A new red-emitting phosphor Sr3NaSbO6:Eu3+ with different Eu3+ contents has been prepared by a solid-state method. The crystal structure, photoluminescence properties and concentration quenching of Sr3NaSbO6:Eu3+ phosphors were studied in detail. Under excitation at 285\xa0nm, a sharp red light was observed originating from the 5D0→(7F1, 7F2)transition of the Eu3+. The concentration quenching mechanism has been investigated, and demonstrated to involve a dipole-dipole interaction. We also probed the decay lifetime and calculated the CIE coordinates, which we found to lie in the red region. The results propose Sr3NaSbO6:Eu3+ phosphor as a promising red light-emitting material for UV- pumped white light-emitting diodes.', 'corpus_id': 236295485, 'score': 0}, {'doc_id': '92743137', 'title': 'Microwave synthesis of homogeneous and highly luminescent BCNO nanoparticles for the light emitting polymer materials', 'abstract': 'Abstract Nano-sized boron carbon oxynitride (BCNO) phosphors around 50\xa0nm containing no rare earth metal and free from color heterogeneity were synthesized from mixtures of boric acid, urea, and citric acid by microwave heating with substantially shorter reaction times and lower temperatures than in the conventional BCNO preparation method such as electric-furnace heating. The emission wavelength of the phosphors varied with the mixing ratio of raw materials and it was found that lowering the proportion of urea to boric acid or citric acid tended to increase the internal quantum yield and shorten the emission wavelength under excitation at 365\xa0nm. It was also found for the first time that a light-emitting polymer could be synthesized from a mixture of the prepared BCNO nanoparticles and a polyvinyl alcohol. This polymer composite exhibited uniform dispersion and stabilization of the luminescence and had a high internal quantum yield of 54%, which was higher than that of the phosphor alone.', 'corpus_id': 92743137, 'score': 1}, {'doc_id': '236678805', 'title': 'Highly Stable Mn4+-Activated Red-Emitting Fluoride Phosphors and Enhanced moisture stability for White LEDs', 'abstract': 'As a highly productive narrow-band emission red phosphor, K2SiF6:Mn4+ has towardly application market in pc-WLEDs (Phosphor-converted white light-emitting diodes) and LCD (Liquid crystal display). However, the poor moisture resistance property is recognized impediment for generalizing its wider commercialization. Herein, we adopt a simply green and convenient strategy based on Fe2+, which is committed to decreasing the reaction between luminescence center and water. The K2SiF6:Mn4+ as expected exhibits highly efficient red emission upon excitation.', 'corpus_id': 236678805, 'score': 0}, {'doc_id': '236242686', 'title': 'A novel Eu3+-doped ScCaO(BO3) red phosphor for tricolor-composited high color rendering white light', 'abstract': 'Abstract Exploration of novel red emitting phosphors containing Eu3+ ions is an everlasting topic in the field of luminescent materials due to the application requirement of warm white light-emitting diodes. In this work, the Eu3+-doped ScCaO(BO3) phosphors were synthesized though the high temperature solid-state reaction method. The phase purity and detail information on crystal structure were obtained by X-ray diffraction (XRD) and the technology of Rietveld XRD refinement. Scanning electron microscopy (SEM) was applied to check the particle morphology and EDS mapping was taken to analyze element distributions. Optical properties of the phosphors were studied by measuring the steady-state excitation and emission spectra and also the transient luminescence decays. The emission spectra of ScCaO(BO3):Eu3+ majorly consisted of 5D0-7F1, 7F2 and 7F4 transitions, covering the orange (582–605\xa0nm), red (605–635\xa0nm) and deep red (690–720\xa0nm) spectral regions. The doping concentration of Eu3+ ions was optimized to 12% to maximize the optical performance of the red emitting phosphors, whose CIE coordinate and fluorescent lifetime were (0.6458, 0.3537) and 1.516\xa0ms, respectively. The investigation of the thermal quenching behavior of the ScCaO(BO3):0.12Eu3+ phosphor reveals that the emission intensity of Eu3+ at 400\xa0K can maintain 68.98% of that at room temperature due to the existence of the thermal activation energy of 0.258\xa0eV. Using such a red-emitting material, the prototype warm white LED device with the CIE coordinate of (0.3262, 0.3363), color rendering index of 86.1 and correlated color temperature of 5871\xa0K can be obtained by combining with the blue emitting BaMgAl10O17:Eu2+ and green emitting Zn2SiO4:Mn2+ phosphors and coating all three on a near-UV LED chip. All the results demonstrate that the ScCaO(BO3):0.12Eu3+ phosphor can sever as a potential red emitting candidate for near-UV excited warm WLEDs.', 'corpus_id': 236242686, 'score': 0}, {'doc_id': '236246183', 'title': 'Multi-color carbon dots from cis-butenedioic acid and urea and highly luminescent carbon dots@Ca(OH)2 hybrid phosphors with excellent thermal stability for white light-emitting diodes', 'abstract': 'Abstract Multi-color carbon dots (CDs) were obtained from cis-butenedioic acid (C-BA) and urea by a simple one-step solvothermal route. This is the first report about using C-BA as the raw material to synthesize CDs. The fluorescent properties of as-prepared CDs vary with the ratio of C-BA to urea. And the sample with 1:1 of C-BA and urea exhibits the optimal yellow emission and highest photoluminescence quantum yield (PLQY) of 16.17%. Further investigations indicate that the variation of PL properties comes from the different content of surface functional groups such as C C, C O, –COOH, and graphitic N. Both of C O and graphitic N cause the PL redshift, whereas the C C and –COOH groups lead to the blueshift. Based on the above, a solid-state luminescent CDs-based phosphor was obtained by adding alkaline Ca(OH)2 powder to as-prepared yellow CDs solution. In this case, PL intensity is increased to 1.78 times, the corresponding emission redshifts from 553 to 568\xa0nm, and PLQY is improved from 16.17% to 35.12%. Simultaneously, CDs@Ca(OH)2 hybrid phosphor exhibits good thermostability. Finally, a white light-emitting diode (LED) was successfully fabricated by combining the yellow-emission CDs-based phosphor with a blue LED chip. It exhibits a high CRI of 86.9, the CCT of 5388\xa0K, and a luminous efficacy of 15.12 lm/W. The CIE coordinate locates at (0.3341, 0.3075) which is very close to the pure white light of (0.33, 0.33). Thus, as-prepared CDs-based solid-state phosphor is a potential candidate for practical lighting devices.', 'corpus_id': 236246183, 'score': 1}, {'doc_id': '95359478', 'title': 'Photoluminescence optimization of BCNO phosphors synthesized using citric acid as a carbon source', 'abstract': 'Abstract Citric acid was used as carbon source for the optimization of the photoluminescence (PL) performance of boron carbon oxynitride (BCNO) phosphor. Citric acid was chosen as an alternative carbon source because of its simple molecular structure, low decomposition temperature, relative inexpensiveness, and environmental friendliness. The prepared sample exhibited a single, homogeneous, and broad photoluminescence emission band whose peak varied from near-UV (400\xa0nm) to yellow-visible (500\xa0nm) upon excitation at 365\xa0nm. The effects of varying the synthesis temperature, molar ratio of the carbon/boron and nitrogen/boron sources, and addition of SiO 2 nanoparticles on the PL properties were also studied. The optimized BCNO phosphors may find potential use in white LED applications.', 'corpus_id': 95359478, 'score': 1}, {'doc_id': '99522678', 'title': 'A red emitting of manganese-doped boron carbon oxynitride (BCNO) phosphor materials: facile approach and photoluminescence properties', 'abstract': 'BCNO (boron carbon oxynitride) phosphors have attracted attention due to their non toxicity, simple synthesis process, and high quantum efficiency. However, the tunable emission, particularly a red emission, is still challenging to achieve. In the present study, a bright red emission (emission wavelength 620 nm under 365 nm UV excitation) of a manganese-doped BCNO (BCNO:Mn) phosphor synthesized by a solid state method is reported. Without Mn-doping, the BCNO phosphor exhibited a bright blue emission that can be ascribed to the closed-shell BO− and BO2− anions that act as luminescence center. Via Mn-doping, a red emission was exhibited that can be ascribed to the 4T1(4G) → 6A1(6S) transition from a new luminescence center of Mn2+ incorporated into the BCNO host lattice. The optimum red PL properties (λem: 611 nm, λex: 365 nm) were obtained with BCNO:Mn at a molar ratio of Mn/B at 0.71% mol/mol synthesized at 550 °C. We believe that the BCNO:Mn is a promising red-emitting phosphor for white light diodes.', 'corpus_id': 99522678, 'score': 1}, {'doc_id': '236250503', 'title': 'Photoluminescence, thermal stability and structural properties of red-emitting phosphors Na5YSi4O12:Eu3+', 'abstract': 'Abstract A series of Na5YSi4O12:xEu3+ (x\xa0=\xa00–100\xa0mol%) red-emitting phosphors were prepared by the solid-state method. The powder X-Ray diffraction (XRD) measurement was introduced and the crystal structure of the phosphors was crystallized in the trigonal system with the space group of R -3c (167). The concentration dependent photoluminescent excitation spectra (PLE), emission spectra (PL) as well as luminescence decay curves were recorded and the optimum doping concentration of Eu3+ ions was evaluated. The synthesized phosphors can be efficiently excited by ultraviolet (UV) to blue light and emit bright red-emitting. Under the excitation of 266\xa0nm wavelength, several emission bands can be observed, ascribing to the transitions from excited state of 5D0 to the ground states of 7F0-4. It is somewhat unexpected that the PL emission from the 5D0 → 7F0 transition is remarkable, which is principally believed to be forbidden. The intensity and decay lifetime of the emission were measured to vary strongly with temperature, indicating that the as-synthesized silicates with excellent photoluminescence properties could be promised for multifunctional applications. In addition, the luminescence mechanism was briefly discussed.', 'corpus_id': 236250503, 'score': 0}]
65	{'doc_id': '218889852', 'title': 'CERT: Contrastive Self-supervised Learning for Language Understanding', 'abstract': 'Pretrained language models such as BERT, GPT have shown great effectiveness in language understanding. The auxiliary predictive tasks in existing pretraining approaches are mostly defined on tokens, thus may not be able to capture sentence-level semantics very well. To address this issue, we propose CERT: Contrastive self-supervised Encoder Representations from Transformers, which pretrains language representation models using contrastive self-supervised learning at the sentence level. CERT creates augmentations of original sentences using back-translation. Then it finetunes a pretrained language encoder (e.g., BERT) by predicting whether two augmented sentences originate from the same sentence. CERT is simple to use and can be flexibly plugged into any pretraining-finetuning NLP pipeline. We evaluate CERT on 11 natural language understanding tasks in the GLUE benchmark where CERT outperforms BERT on 7 tasks, achieves the same performance as BERT on 2 tasks, and performs worse than BERT on 2 tasks. On the averaged score of the 11 tasks, CERT outperforms BERT. The data and code are available at https://github.com/UCSD-AI4H/CERT', 'corpus_id': 218889852}	6175	"[{'doc_id': '218502328', 'title': 'Exploring Contextual Word-level Style Relevance for Unsupervised Style Transfer', 'abstract': 'Unsupervised style transfer aims to change the style of an input sentence while preserving its original content without using parallel training data. In current dominant approaches, owing to the lack of fine-grained control on the influence from the target style, they are unable to yield desirable output sentences. In this paper, we propose a novel attentional sequence-to-sequence (Seq2seq) model that dynamically exploits the relevance of each output word to the target style for unsupervised style transfer. Specifically, we first pretrain a style classifier, where the relevance of each input word to the original style can be quantified via layer-wise relevance propagation. In a denoising auto-encoding manner, we train an attentional Seq2seq model to reconstruct input sentences and repredict word-level previously-quantified style relevance simultaneously. In this way, this model is endowed with the ability to automatically predict the style relevance of each output word. Then, we equip the decoder of this model with a neural style component to exploit the predicted wordlevel style relevance for better style transfer. Particularly, we fine-tune this model using a carefully-designed objective function involving style transfer, style relevance consistency, content preservation and fluency modeling loss terms. Experimental results show that our proposed model achieves state-of-the-art performance in terms of both transfer accuracy and content preservation.', 'corpus_id': 218502328, 'score': 1}, {'doc_id': '219401872', 'title': 'CoCon: A Self-Supervised Approach for Controlled Text Generation', 'abstract': ""Pretrained Transformer-based language models (LMs) display remarkable natural language generation capabilities. With their immense potential, controlling text generation of such LMs is getting attention. While there are studies that seek to control high-level attributes (such as sentiment and topic) of generated text, there is still a lack of more precise control over its content at the word- and phrase-level. Here, we propose Content-Conditioner (CoCon) to control an LM's output text with a target content, at a fine-grained level. In our self-supervised approach, the CoCon block learns to help the LM complete a partially-observed text sequence by conditioning with content inputs that are withheld from the LM. Through experiments, we show that CoCon can naturally incorporate target content into generated texts and control high-level text attributes in a zero-shot manner."", 'corpus_id': 219401872, 'score': 1}, {'doc_id': '218665411', 'title': 'Parallel Data Augmentation for Formality Style Transfer', 'abstract': 'The main barrier to progress in the task of Formality Style Transfer is the inadequacy of training data. In this paper, we study how to augment parallel data and propose novel and simple data augmentation methods for this task to obtain useful sentence pairs with easily accessible models and systems. Experiments demonstrate that our augmented parallel data largely helps improve formality style transfer when it is used to pre-train the model, leading to the state-of-the-art results in the GYAFC benchmark dataset.', 'corpus_id': 218665411, 'score': 1}, {'doc_id': '218487448', 'title': 'Expertise Style Transfer: A New Task Towards Better Communication between Experts and Laymen', 'abstract': 'The curse of knowledge can impede communication between experts and laymen. We propose a new task of expertise style transfer and contribute a manually annotated dataset with the goal of alleviating such cognitive biases. Solving this task not only simplifies the professional language, but also improves the accuracy and expertise level of laymen descriptions using simple words. This is a challenging task, unaddressed in previous work, as it requires the models to have expert intelligence in order to modify text with a deep understanding of domain knowledge and structures. We establish the benchmark performance of five state-of-the-art models for style transfer and text simplification. The results demonstrate a significant gap between machine and human performance. We also discuss the challenges of automatic evaluation, to provide insights into future research directions. The dataset is publicly available at https://srhthu.github.io/expertise-style-transfer/.', 'corpus_id': 218487448, 'score': 1}, {'doc_id': '218516586', 'title': 'Russian Natural Language Generation: Creation of a Language Modelling Dataset and Evaluation with Modern Neural Architectures', 'abstract': 'Generating coherent, grammatically correct, and meaningful text is very challenging, however, it is crucial to many modern NLP systems. So far, research has mostly focused on English language, for other languages both standardized datasets, as well as experiments with state-of-the-art models, are rare. In this work, we i) provide a novel reference dataset for Russian language modeling, ii) experiment with popular modern methods for text generation, namely variational autoencoders, and generative adversarial networks, which we trained on the new dataset. We evaluate the generated text regarding metrics such as perplexity, grammatical correctness and lexical diversity.', 'corpus_id': 218516586, 'score': 0}, {'doc_id': '218486925', 'title': 'Improving Adversarial Text Generation by Modeling the Distant Future', 'abstract': 'Auto-regressive text generation models usually focus on local fluency, and may cause inconsistent semantic meaning in long text generation. Further, automatically generating words with similar semantics is challenging, and hand-crafted linguistic rules are difficult to apply. We consider a text planning scheme and present a model-based imitation-learning approach to alleviate the aforementioned issues. Specifically, we propose a novel guider network to focus on the generative process over a longer horizon, which can assist next-word prediction and provide intermediate rewards for generator optimization. Extensive experiments demonstrate that the proposed method leads to improved performance.', 'corpus_id': 218486925, 'score': 1}, {'doc_id': '216562404', 'title': 'MultiMix: A Robust Data Augmentation Strategy for Cross-Lingual NLP', 'abstract': 'Transfer learning has yielded state-of-the-art results in many supervised natural language processing tasks. However, annotated data for every target task in every target language is rare, especially for low-resource languages. In this work, we propose MultiMix, a novel data augmentation method for semi-supervised learning in zero-shot transfer learning scenarios. In particular, MultiMix targets to solve cross-lingual adaptation problems from a source (language) distribution to an unknown target (language) distribution assuming it has no training labels in the target language task. In its heart, MultiMix performs simultaneous self-training with data augmentation and unsupervised sample selection. To show its effectiveness, we have performed extensive experiments on zero-shot transfers for cross-lingual named entity recognition (XNER) and natural language inference (XNLI). Our experiments show sizeable improvements in both tasks outperforming the baselines by a good margin.', 'corpus_id': 216562404, 'score': 0}, {'doc_id': '218684947', 'title': 'GPT-too: A Language-Model-First Approach for AMR-to-Text Generation', 'abstract': 'Abstract Meaning Representations (AMRs) are broad-coverage sentence-level semantic graphs. Existing approaches to generating text from AMR have focused on training sequence-to-sequence or graph-to-sequence models on AMR annotated data only. In this paper, we propose an alternative approach that combines a strong pre-trained language model with cycle consistency-based re-scoring. Despite the simplicity of the approach, our experimental results show these models outperform all previous techniques on the English LDC2017T10 dataset, including the recent use of transformer architectures. In addition to the standard evaluation metrics, we provide human evaluation experiments that further substantiate the strength of our approach.', 'corpus_id': 218684947, 'score': 0}]"
66	{'doc_id': '231602925', 'title': 'Proxemics and Social Interactions in an Instrumented Virtual Reality Workshop', 'abstract': 'Virtual environments (VEs) can create collaborative and social spaces, which are increasingly important in the face of remote work and travel reduction. Recent advances, such as more open and widely available platforms, create new possibilities to observe and analyse interaction in VEs. Using a custom instrumented build of Mozilla Hubs to measure position and orientation, we conducted an academic workshop to facilitate a range of typical workshop activities. We analysed social interactions during a keynote, small group breakouts, and informal networking/hallway conversations. Our mixed-methods approach combined environment logging, observations, and semi-structured interviews. The results demonstrate how small and large spaces influenced group formation, shared attention, and personal space, where smaller rooms facilitated more cohesive groups while larger rooms made small group formation challenging but personal space more flexible. Beyond our findings, we show how the combination of data and insights can fuel collaborative spaces’ design and deliver more effective virtual workshops.', 'corpus_id': 231602925}	13715	"[{'doc_id': '228102859', 'title': 'RealitySketch: An AR interface to create responsive sketches', 'abstract': ""Researchers at University of Calgary, Adobe Research and University of Colorado Boulder have recently created an augmented reality (AR) interface that can be used to produce responsive sketches, graphics and visualizations. Their work, initially pre-published on arXiv, won the Best Paper Honorable Mention and Best Demo Honorable Mention awards at the ACM Symposium on User Interface Software and Technology (UIST'20)."", 'corpus_id': 228102859, 'score': 1}, {'doc_id': '215764429', 'title': 'BISHARE: Exploring Bidirectional Interactions Between Smartphones and Head-Mounted Augmented Reality', 'abstract': 'In pursuit of a future where HMD devices can be used in tandem with smartphones and other smart devices, we present BISHARE, a design space of cross-device interactions between smartphones and ARHMDs. Our design space is unique in that it is bidirectional in nature, as it examines how both the HMD can be used to enhance smartphone tasks, and how the smartphone can be used to enhance HMD tasks. We then present an interactive prototype that enables cross-device interactions across the proposed design space. A 12-participant user study demonstrates the promise of the design space and provides insights, observations, and guidance for the future.', 'corpus_id': 215764429, 'score': 1}, {'doc_id': '227014601', 'title': 'Combining Gesture and Voice Control for Mid-Air Manipulation of CAD Models in VR Environments', 'abstract': 'Modeling 3D objects in domains like Computer Aided Design (CAD) is time-consuming and comes with a steep learning curve needed to master the design process as well as tool complexities. In order to simplify the modeling process, we designed and implemented a prototypical system that leverages the strengths of Virtual Reality (VR) hand gesture recognition in combination with the expressiveness of a voice-based interface for the task of 3D modeling. Furthermore, we use the Constructive Solid Geometry (CSG) tree representation for 3D models within the VR environment to let the user manipulate objects from the ground up, giving an intuitive understanding of how the underlying basic shapes connect. The system uses standard mid-air 3D object manipulation techniques and adds a set of voice commands to help mitigate the deficiencies of current hand gesture recognition techniques. A user study was conducted to evaluate the proposed prototype. The combination of our hybrid input paradigm shows to be a promising step towards easier to use CAD modeling.', 'corpus_id': 227014601, 'score': 0}, {'doc_id': '210695799', 'title': 'Integrating AR and VR for Mobile Remote Collaboration', 'abstract': ""In many complex tasks, a remote expert may need to assist a local user or to guide his or her actions in the local user's environment. Existing solutions also allow multiple users to collaborate remotely using high-end Augmented Reality (AR) and Virtual Reality (VR) head-mounted displays (HMD). In this paper, we propose a portable remote collaboration approach, with the integration of AR and VR devices, both running on mobile platforms, to tackle the challenges of existing approaches. The AR mobile platform processes the live video and measures the 3D geometry of the local environment of a local user. The 3D scene is then transited and rendered in the remote side on a mobile VR device, along with a simple and effective user interface, which allows a remote expert to easily manipulate the 3D scene on the VR platform and to guide the local user to complete tasks in the local environment."", 'corpus_id': 210695799, 'score': 1}, {'doc_id': '231719722', 'title': 'Art and Science Interaction Lab - A highly flexible and modular interaction science research facility', 'abstract': 'The Art and Science Interaction Lab (“ASIL”) is a unique, highly flexible and modular “interaction science” research facility to effectively bring, analyse and test experiences and interactions in mixed virtual/augmented contexts as well as to conduct research on next-gen immersive technologies. It brings together the expertise and creativity of engineers, performers, designers and scientists creating solutions and experiences shaping the lives of people. The lab is equipped with stateof-the-art visual, auditory and user-tracking equipment, fully synchronized and connected to a central backend. This synchronization allows for highly accurate multi-sensor measurements and analysis.', 'corpus_id': 231719722, 'score': 0}, {'doc_id': '216033776', 'title': 'Social VR: A New Medium for Remote Communication and Collaboration', 'abstract': 'There is a growing need for effective remote communication, which has many positive societal impacts, such as reducing environmental pollution and travel costs, supporting rich collaboration by remotely connecting talented people. Social Virtual Reality (VR) invites multiple users to join a collaborative virtual environment, which creates new opportunities for remote communication. The goal of social VR is not to completely replicate reality, but to facilitate and extend the existing communication channels of the physical world. Apart from the benefits provided by social VR, privacy concerns and ethical risks are raised when the boundary between the real and the virtual world is blurred. This workshop is intended to spur discussions regarding technology, evaluation protocols, application areas, research ethics and legal regulations for social VR as an emerging immersive remote communication tool.', 'corpus_id': 216033776, 'score': 1}, {'doc_id': '231171502', 'title': 'Mobile Multiuser AR/VR for Training', 'abstract': 'Augmented reality (AR) is a growing technology for building immersive and interactive applications for anyone to use. Integrated development environments such as Unity when enhanced with additional plugins like Unity’s AR Foundation and Photon Network, enable the rapid development of multiuser, mobile, augmented, and mixed reality (XR) applications that can be both entertaining and useful. This report is about a mobile AR and virtual reality (VR), or XR multiuser classroom prototype application. This project was developed on Unity with Unity’s AR Foundation Kit, Photon Network, and Android AR Core plugins to implement an Android based mobile phone application and a desktop application. The final multiuser applications include a real-time multiuser AR/VR environment for an emulated AR/VR Classroom, and an AR Spinning Top Demonstration. Keywords— augmented reality (AR). virtual reality (VR), mixed reality (XR), Unity, Photon Network, AR Foundation, AR Core.', 'corpus_id': 231171502, 'score': 1}, {'doc_id': '231836916', 'title': 'Grand Challenges in Immersive Analytics', 'abstract': 'Immersive Analytics is a quickly evolving field that unites several areas such as visualisation, immersive environments, and human-computer interaction to support human data analysis with emerging technologies. This research has thrived over the past years with multiple workshops, seminars, and a growing body of publications, spanning several conferences. Given the rapid advancement of interaction technologies and novel application domains, this paper aims toward a broader research agenda to enable widespread adoption. We present 17 key research challenges developed over multiple sessions by a diverse group of 24 international experts, initiated from a virtual scientific workshop at ACM CHI 2020. These challenges aim to coordinate future work by providing a systematic roadmap of current directions and impending hurdles to facilitate productive and effective applications for Immersive Analytics.', 'corpus_id': 231836916, 'score': 0}]"
67	{'doc_id': '220514447', 'title': 'Attentive Graph Neural Networks for Few-Shot Learning', 'abstract': 'Graph Neural Networks (GNN) has demonstrated the superior performance in many challenging applications, including the few-shot learning tasks. Despite its powerful capacity to learn and generalize from few samples, GNN usually suffers from severe over-fitting and over-smoothing as the model becomes deep, which limit the model scalability. In this work, we propose a novel Attentive GNN to tackle these challenges, by incorporating a triple-attention mechanism, \\ie node self-attention, neighborhood attention, and layer memory attention. We explain why the proposed attentive modules can improve GNN for few-shot learning with theoretical analysis and illustrations. Extensive experiments show that the proposed Attentive GNN outperforms the state-of-the-art GNN-based methods for few-shot learning over the mini-ImageNet and Tiered-ImageNet datasets, with both inductive and transductive settings.', 'corpus_id': 220514447}	5139	"[{'doc_id': '216562627', 'title': 'Image Augmentation Is All You Need: Regularizing Deep Reinforcement Learning from Pixels', 'abstract': ""We propose a simple data augmentation technique that can be applied to standard model-free reinforcement learning algorithms, enabling robust learning directly from pixels without the need for auxiliary losses or pre-training. The approach leverages input perturbations commonly used in computer vision tasks to regularize the value function. Existing model-free approaches, such as Soft Actor-Critic (SAC), are not able to train deep networks effectively from image pixels. However, the addition of our augmentation method dramatically improves SAC's performance, enabling it to reach state-of-the-art performance on the DeepMind control suite, surpassing model-based (Dreamer, PlaNet, and SLAC) methods and recently proposed contrastive learning (CURL). Our approach can be combined with any model-free reinforcement learning algorithm, requiring only minor modifications. An implementation can be found at this https URL."", 'corpus_id': 216562627, 'score': 0}, {'doc_id': '221856410', 'title': 'Pruning Convolutional Filters Using Batch Bridgeout', 'abstract': 'State-of-the-art computer vision models are rapidly increasing in capacity, where the number of parameters far exceeds the number required to fit the training set. This results in better optimization and generalization performance. However, the huge size of contemporary models results in large inference costs and limits their use on resource-limited devices. In order to reduce inference costs, convolutional filters in trained neural networks could be pruned to reduce the run-time memory and computational requirements during inference. However, severe post-training pruning results in degraded performance if the training algorithm results in dense weight vectors. We propose the use of Batch Bridgeout, a sparsity inducing stochastic regularization scheme, to train neural networks so that they could be pruned efficiently with minimal degradation in performance. We evaluate the proposed method on common computer vision models VGGNet, ResNet and Wide-ResNet on the CIFAR10 and CIFAR100 image classification tasks. For all the networks, experimental results show that Batch Bridgeout trained networks achieve higher accuracy across a wide range of pruning intensities compared to Dropout and weight decay regularization.', 'corpus_id': 221856410, 'score': 0}, {'doc_id': '221971120', 'title': 'Visual Steering for One-Shot Deep Neural Network Synthesis', 'abstract': 'Recent advancements in the area of deep learning have shown the effectiveness of very large neural networks in several applications. However, as these deep neural networks continue to grow in size, it becomes more and more difficult to configure their many parameters to obtain good results. Presently, analysts must experiment with many different configurations and parameter settings, which is labor-intensive and time-consuming. On the other hand, the capacity of fully automated techniques for neural network architecture search is limited without the domain knowledge of human experts. To deal with the problem, we formulate the task of neural network architecture optimization as a graph space exploration, based on the one-shot architecture search technique. In this approach, a super-graph of all candidate architectures is trained in one-shot and the optimal neural network is identified as a sub-graph. In this paper, we present a framework that allows analysts to effectively build the solution sub-graph space and guide the network search by injecting their domain knowledge. Starting with the network architecture space composed of basic neural network components, analysts are empowered to effectively select the most promising components via our one-shot search scheme. Applying this technique in an iterative manner allows analysts to converge to the best performing neural network architecture for a given application. During the exploration, analysts can use their domain knowledge aided by cues provided from a scatterplot visualization of the search space to edit different components and guide the search for faster convergence. We designed our interface in collaboration with several deep learning researchers and its final effectiveness is evaluated with a user study and two case studies.', 'corpus_id': 221971120, 'score': 0}, {'doc_id': '222377666', 'title': 'Masked Contrastive Representation Learning for Reinforcement Learning', 'abstract': 'Improving sample efficiency is a key research problem in reinforcement learning (RL), and CURL, which uses contrastive learning to extract high-level features from raw pixels of individual video frames, is an efficient algorithm~\\citep{srinivas2020curl}. We observe that consecutive video frames in a game are highly correlated but CURL deals with them independently. To further improve data efficiency, we propose a new algorithm, masked contrastive representation learning for RL, that takes the correlation among consecutive inputs into consideration. In addition to the CNN encoder and the policy network in CURL, our method introduces an auxiliary Transformer module to leverage the correlations among video frames. During training, we randomly mask the features of several frames, and use the CNN encoder and Transformer to reconstruct them based on the context frames. The CNN encoder and Transformer are jointly trained via contrastive learning where the reconstructed features should be similar to the ground-truth ones while dissimilar to others. During inference, the CNN encoder and the policy network are used to take actions, and the Transformer module is discarded. Our method achieves consistent improvements over CURL on $14$ out of $16$ environments from DMControl suite and $21$ out of $26$ environments from Atari 2600 Games. The code is available at https://github.com/teslacool/m-curl.', 'corpus_id': 222377666, 'score': 1}, {'doc_id': '219635852', 'title': 'Mutual Information Based Knowledge Transfer Under State-Action Dimension Mismatch', 'abstract': 'Deep reinforcement learning (RL) algorithms have achieved great success on a wide variety of sequential decision-making tasks. However, many of these algorithms suffer from high sample complexity when learning from scratch using environmental rewards, due to issues such as credit-assignment and high-variance gradients, among others. Transfer learning, in which knowledge gained on a source task is applied to more efficiently learn a different but related target task, is a promising approach to improve the sample complexity in RL. Prior work has considered using pre-trained teacher policies to enhance the learning of the student policy, albeit with the constraint that the teacher and the student MDPs share the state-space or the action-space. In this paper, we propose a new framework for transfer learning where the teacher and the student can have arbitrarily different state- and action-spaces. To handle this mismatch, we produce embeddings which can systematically extract knowledge from the teacher policy and value networks, and blend it into the student networks. To train the embeddings, we use a task-aligned loss and show that the representations could be enriched further by adding a mutual information loss. Using a set of challenging simulated robotic locomotion tasks involving many-legged centipedes, we demonstrate successful transfer learning in situations when the teacher and student have different state- and action-spaces.', 'corpus_id': 219635852, 'score': 0}, {'doc_id': '215415887', 'title': 'START — Self-Tuning Adaptive Radix Tree', 'abstract': 'structures like the Adaptive Radix Tree (ART) are a central part of in-memory database systems. However, we found that radix nodes that index a single byte are not optimal for read-heavy workloads. In this work, we introduce START, a self-tuning variant of ART that uses nodes spanning multiple keybytes. To determine where to introduce these new node types, we propose a cost model and an optimizer. These components allow us to fine-tune an existing ART, reducing its overall height, and improving performance. As a result, START performs on average 85 % faster than a regular ART on a wide variety of read-only workloads and 45% faster for read-mostly workloads.', 'corpus_id': 215415887, 'score': 1}, {'doc_id': '221970176', 'title': 'An Optimal Computing Budget Allocation Tree Policy for Monte Carlo Tree Search', 'abstract': 'We analyze a tree search problem with an underlying Markov decision process, in which the goal is to identify the best action at the root that achieves the highest cumulative reward. We present a new tree policy that optimally allocates a limited computing budget to maximize a lower bound on the probability of correctly selecting the best action at each node. Compared to widely used Upper Confidence Bound (UCB) tree policies, the new tree policy presents a more balanced approach to manage the exploration and exploitation trade-off when the sampling budget is limited. Furthermore, UCB assumes that the support of reward distribution is known, whereas our algorithm relaxes this assumption. Numerical experiments demonstrate the efficiency of our algorithm in selecting the best action at the root.', 'corpus_id': 221970176, 'score': 1}, {'doc_id': '89616676', 'title': 'Annotation cost-sensitive active learning by tree sampling', 'abstract': 'Active learning is an important machine learning setup for reducing the labelling effort of humans. Although most existing works are based on a simple assumption that each labelling query has the same annotation cost, the assumption may not be realistic. That is, the annotation costs may actually vary between data instances. In addition, the costs may be unknown before making the query. Traditional active learning algorithms cannot deal with such a realistic scenario. In this work, we study annotation cost-sensitive active learning algorithms, which need to estimate the utility and cost of each query simultaneously. We propose a novel algorithm, the cost-sensitive tree sampling algorithm, that conducts the two estimation tasks together and solve it with a tree-structured model motivated from hierarchical sampling, a famous algorithm for traditional active learning. Extensive experimental results using datasets with simulated and true annotation costs validate that the proposed method is generally superior to other annotation cost-sensitive algorithms.', 'corpus_id': 89616676, 'score': 1}, {'doc_id': '208221186', 'title': 'Neural Architecture Search Using Deep Neural Networks and Monte Carlo Tree Search', 'abstract': 'Neural Architecture Search (NAS) has shown great success in automating the design of neural networks, but the prohibitive amount of computations behind current NAS methods requires further investigations in improving the sample efficiency and the network evaluation cost to get better results in a shorter time. In this paper, we present a novel scalable Monte Carlo Tree Search (MCTS) based NAS agent, named AlphaX, to tackle these two aspects. AlphaX improves the search efficiency by adaptively balancing the exploration and exploitation at the state level, and by a Meta-Deep Neural Network (DNN) to predict network accuracies for biasing the search toward a promising region. To amortize the network evaluation cost, AlphaX accelerates MCTS rollouts with a distributed design and reduces the number of epochs in evaluating a network by transfer learning, which is guided with the tree structure in MCTS. In 12 GPU days and 1000 samples, AlphaX found an architecture that reaches 97.84% top-1 accuracy on CIFAR-10, and 75.5% top-1 accuracy on ImageNet, exceeding SOTA NAS methods in both the accuracy and sampling efficiency. Particularly, we also evaluate AlphaX on NASBench-101, a large scale NAS dataset; AlphaX is 3x and 2.8x more sample efficient than Random Search and Regularized Evolution in finding the global optimum. Finally, we show the searched architecture improves a variety of vision applications from Neural Style Transfer, to Image Captioning and Object Detection.', 'corpus_id': 208221186, 'score': 1}, {'doc_id': '218538147', 'title': 'Learning, transferring, and recommending performance knowledge with Monte Carlo tree search and neural networks', 'abstract': 'Making changes to a program to optimize its performance is an unscalable task that relies entirely upon human intuition and experience. In addition, companies operating at large scale are at a stage where no single individual understands the code controlling its systems, and for this reason, making changes to improve performance can become intractably difficult. In this paper, a learning system is introduced that provides AI assistance for finding recommended changes to a program. Specifically, it is shown how the evaluative feedback, delayed-reward performance programming domain can be effectively formulated via the Monte Carlo tree search (MCTS) framework. It is then shown that established methods from computational games for using learning to expedite tree-search computation can be adapted to speed up computing recommended program alterations. Estimates of expected utility from MCTS trees built for previous problems are used to learn a sampling policy that remains effective across new problems, thus demonstrating transferability of optimization knowledge. This formulation is applied to the Apache Spark distributed computing environment, and a preliminary result is observed that the time required to build a search tree for finding recommendations is reduced by up to a factor of 10x.', 'corpus_id': 218538147, 'score': 0}]"
68	{'doc_id': '227242420', 'title': 'Biologically Inspired Spatial Representation', 'abstract': 'In this thesis I explore a biologically inspired method of encoding continuous space within a population of neurons. This method provides an extension to the Semantic Pointer Architecture (SPA) to encompass Semantic Pointers with real-valued spatial content in addition to symbol-like representations. I demonstrate how these Spatial Semantic Pointers (SSPs) can be used to generate cognitive maps containing objects at various locations. A series of operations are defined that can retrieve objects or locations from the encoded map as well as manipulate the contents of the memory. These capabilities are all implemented by a network of spiking neurons. I explore the topology of the SSP vector space and show how it preserves metric information while compressing all coordinates to unit length vectors. This allows a limitless spatial extent to be represented in a finite region. Neurons encoding space represented in this manner have firing fields similar to entorhinal grid cells. Beyond constructing biologically plausible models of spatial cognition, SSPs are applied to the domain of machine learning. I demonstrate how replacing traditional spatial encoding mechanisms with SSPs can improve performance on networks trained to compute a navigational policy. In addition, SSPs are also effective for training a network to localize within an environment based on sensor measurements as well as perform path integration. To demonstrate a practical, integrated system using SSPs, I combine a goal driven navigational policy with the localization network and cognitive map representation to produce an agent that can navigate to semantically defined goals. In addition to spatial tasks, the SSP encoding is applied to a more general class of machine learning problems involving arbitrary continuous signals. Results on a collection of 122 benchmark datasets across a variety of domains indicate that neural networks trained with SSP encoding outperform commonly used methods for the majority of the datasets. Overall, the experiments in this thesis demonstrate the importance of exploring new kinds of representations within neural networks and how they shape the kinds of functions that can be effectively computed. They provide an example of how insights regarding how the brain may encode information can inspire new ways of designing artificial neural networks.', 'corpus_id': 227242420}	2774	[{'doc_id': '232428181', 'title': 'Attention, please! A survey of Neural Attention Models in Deep Learning', 'abstract': 'In humans, Attention is a core property of all perceptual and cognitive operations. Given our limited ability to process competing sources, attention mechanisms select, modulate, and focus on the information most relevant to behavior. For decades, concepts and functions of attention have been studied in philosophy, psychology, neuroscience, and computing. For the last six years, this property has been widely explored in deep neural networks. Currently, the state-of-the-art in Deep Learning is represented by neural attention models in several application domains. This survey provides a comprehensive overview and analysis of developments in neural attention models. We systematically reviewed hundreds of architectures in the area, identifying and discussing those in which attention has shown a significant impact. We also developed and made public an automated methodology to facilitate the development of reviews in the area. By critically analyzing 650 works, we describe the primary uses of attention in convolutional, recurrent networks and generative models, identifying common subgroups of uses and applications. Furthermore, we describe the impact of attention in different application domains and their impact on neural networks’ interpretability. Finally, we list possible trends and opportunities for further research, hoping that this review will provide a succinct overview of the main attentional models in the area and guide researchers in developing future approaches that will drive further improvements.', 'corpus_id': 232428181, 'score': 0}, {'doc_id': '233034499', 'title': 'Grounding Language Processing: The Added Value of Specifying Linguistic/Compositional Representations and Processes', 'abstract': 'Abundant empirical evidence suggests that visual perception and motor responses are involved in language comprehension (‘grounding’). However, when modeling the grounding of sentence comprehension on a word-by-word basis, linguistic representations and cognitive processes are rarely made fully explicit. This article reviews representational formalisms and associated (computational) models with a view to accommodating incremental and compositional grounding effects. Are different representation formats equally suitable and what mechanisms and representations do models assume to accommodate grounding effects? I argue that we must minimally specify compositional semantic representations, a set of incremental processes/mechanisms, and an explicit link from the assumed processes to measured behavior. Different representational formats can be contrasted in psycholinguistic modeling by holding the set of processes/mechanisms constant; contrasting different processes/mechanisms is possible by holding representations constant. Such psycholinguistic modeling could be applied across a wide range of experimental investigations and complement computational modeling.', 'corpus_id': 233034499, 'score': 0}, {'doc_id': '233762187', 'title': 'Precis of A Bayesian account of learning algorithms and generalising representations in the brain', 'abstract': 'Without learning we would be limited to a set of preprogrammed behaviours. While that may be acceptable for flies1, it does not provide the basis for adaptive or intelligent behaviours familiar to humans. Learning, then, is one of the crucial components of brain operation. Learning, however, takes time. Thus, the key to adaptive behaviour is learning to systematically generalise; that is, have learned knowledge that can be flexibly recombined to understand any world in front of you. This thesis attempts to make inroads on two questions how can brain networks learn, and what are the principles behind representations of knowledge that allow generalisation. With the industrialisation of science, the twentieth century bore fruit in the form of an increasingly detailed understanding of neurons, synapses, neurotransmitters, resting potentials, action potentials, networks and so on (1–4). Though we have gained a great level of detail about many of these micro-processes as well as high-level understandings of intelligence thanks to philosophy, experimental psychology, and behavioural and cognitive neuroscience (5–9) a large gulf of understanding remains between these levels of granularity. This thesis focuses on spanning this gap by providing high-level computational frameworks that translate to low-level processes. Any high-level brain framework must have successful behaviour at its heart as that is the role of the brain. Analogously, neurons are central to low-level understanding as the basis of brain function is believed to be the transfer of information between neurons, mediated via weighted connections. Different weights lead to different functions. Thus, learning appropriate configurations of weights is the fundamental problem facing brains. There are two facets to this learning the first is how, and the second is what. The how are the learning algorithms that determine updates to these synaptic connections, and the what are the neural representations that reflect how the world works. In this vein, this thesis examines 1) the algorithmic implementation of learning in biological neural networks, and 2) a computational framework for the neural representations of task generalisation. Both these research directions are bound together by Bayesian thinking, and both of these pieces of work bridge the gap between highand lowlevel understanding, as well as between brains and machines.', 'corpus_id': 233762187, 'score': 0}, {'doc_id': '235614357', 'title': 'Hyperdimensional Computing with Learnable Projection for User Adaptation Framework', 'abstract': None, 'corpus_id': 235614357, 'score': 1}, {'doc_id': '237405701', 'title': 'Semantics in High-Dimensional Space', 'abstract': 'Geometric models are used for modelling meaning in various semantic-space models. They are seductive in their simplicity and their imaginative qualities, and for that reason, their metaphorical power risks leading our intuitions astray: human intuition works well in a three-dimensional world but is overwhelmed by higher dimensionalities. This note is intended to warn about some practical pitfalls of using high-dimensional geometric representation as a knowledge representation and a memory model—challenges that can be met by informed design of the representation and its application.', 'corpus_id': 237405701, 'score': 1}, {'doc_id': '119867013', 'title': 'Sparse Binary Distributed Encoding of Numeric Vectors', 'abstract': None, 'corpus_id': 119867013, 'score': 1}, {'doc_id': '233241236', 'title': 'Memory Capacity of Neural Turing Machines with Matrix Representation', 'abstract': 'It is well known that recurrent neural networks (RNNs) faced limitations in learning longterm dependencies that have been addressed by memory structures in long short-term memory (LSTM) networks. Matrix neural networks feature matrix representation which inherently preserves the spatial structure of data and has the potential to provide better memory structures when compared to canonical neural networks that use vector representation. Neural Turing machines (NTMs) are novel RNNs that implement notion of programmable computers with neural network controllers to feature algorithms that have copying, sorting, and associative recall tasks. In this paper, we study augmentation of memory capacity with matrix representation of RNNs and NTMs (MatNTMs). We investigate if matrix representation has a better memory capacity than the vector representations in conventional neural networks. We use a probabilistic model of the memory capacity using Fisher information and investigate how the memory capacity for matrix representation networks are limited under various constraints, and in general, without any constraints. In the case of memory capacity without any constraints, we found that the upper bound on memory capacity to be N for an N×N state matrix. The results from our experiments using synthetic algorithmic tasks show that MatNTMs have a better learning capacity when compared to its counterparts.', 'corpus_id': 233241236, 'score': 0}, {'doc_id': '234202639', 'title': 'Pregroup Grammars, their Syntax and Semantics', 'abstract': 'Pregroup grammars were developed in 1999 and stayed Lambek’s preferred algebraic model of grammar. The set-theoretic semantics of pregroups, however, faces an ambiguity problem. In his latest book, Lambek suggests that this problem might be overcome using finite dimensional vector spaces rather than sets. What is the right notion of composition in this setting, direct sum or tensor product of spaces?', 'corpus_id': 234202639, 'score': 1}, {'doc_id': '232325097', 'title': 'From convolutional neural networks to models of higher-level cognition (and back again).', 'abstract': 'The remarkable successes of convolutional neural networks (CNNs) in modern computer vision are by now well known, and they are increasingly being explored as computational models of the human visual system. In this paper, we ask whether CNNs might also provide a basis for modeling higher-level cognition, focusing on the core phenomena of similarity and categorization. The most important advance comes from the ability of CNNs to learn high-dimensional representations of complex naturalistic images, substantially extending the scope of traditional cognitive models that were previously only evaluated with simple artificial stimuli. In all cases, the most successful combinations arise when CNN representations are used with cognitive models that have the capacity to transform them to better fit human behavior. One consequence of these insights is a toolkit for the integration of cognitively motivated constraints back into CNN training paradigms in computer vision and machine learning, and we review cases where this leads to improved performance. A second consequence is a roadmap for how CNNs and cognitive models can be more fully integrated in the future, allowing for flexible end-to-end algorithms that can learn representations from data while still retaining the structured behavior characteristic of human cognition.', 'corpus_id': 232325097, 'score': 0}, {'doc_id': '237297487', 'title': 'Assessing Robustness of Hyperdimensional Computing Against Errors in Associative Memory : (Invited Paper)', 'abstract': 'Brain-inspired hyperdimensional computing (HDC) is an emerging computational paradigm that has achieved success in various domains. HDC mimics brain cognition and lever-ages hyperdimensional vectors with fully distributed holographic representation and (pseudo)randomness. Compared to the traditional machine learning methods, HDC offers several critical advantages, including smaller model size, less computation cost, and one-shot learning capability, making it a promising candidate in low-power platforms. Despite the growing popularity of HDC, the robustness of HDC models has not been systematically explored. This paper presents a study on the robustness of HDC to errors in associative memory—the key component storing the class representations in HDC. We perform extensive error injection experiments to the associative memory in a number of HDC models (and datasets), sweeping the error rates and varying HDC configurations (i.e., dimension and data width). Empirically, we observe that HDC is considerably robust to errors in the associative memory, opening up opportunities for further optimizations. Further, results show that HDC robustness varies significantly with different HDC configurations such as data width. Moreover, we explore a low-cost error masking mechanism in the associative memory to enhance its robustness.', 'corpus_id': 237297487, 'score': 1}]
69	{'doc_id': '218889535', 'title': 'Personalized Fashion Recommendation from Personal Social Media Data: An Item-to-Set Metric Learning Approach', 'abstract': 'With the growth of online shopping for fashion products, accurate fashion recommendation has become a critical problem. Meanwhile, social networks provide an open and new data source for personalized fashion analysis. In this work, we study the problem of personalized fashion recommendation from social media data, i.e. recommending new outfits to social media users that fit their fashion preferences. To this end, we present an item-to-set metric learning framework that learns to compute the similarity between a set of historical fashion items of a user to a new fashion item. To extract features from multi-modal street-view fashion items, we propose an embedding module that performs multi-modality feature extraction and cross-modality gated fusion. To validate the effectiveness of our approach, we collect a real-world social media dataset. Extensive experiments on the collected dataset show the superior performance of our proposed approach.', 'corpus_id': 218889535}	5160	"[{'doc_id': '219558508', 'title': 'Few-shot Slot Tagging with Collapsed Dependency Transfer and Label-enhanced Task-adaptive Projection Network', 'abstract': 'In this paper, we explore the slot tagging with only a few labeled support sentences (a.k.a. few-shot). Few-shot slot tagging faces a unique challenge compared to the other fewshot classification problems as it calls for modeling the dependencies between labels. But it is hard to apply previously learned label dependencies to an unseen domain, due to the discrepancy of label sets. To tackle this, we introduce a collapsed dependency transfer mechanism into the conditional random field (CRF) to transfer abstract label dependency patterns as transition scores. In the few-shot setting, the emission score of CRF can be calculated as a word’s similarity to the representation of each label. To calculate such similarity, we propose a Label-enhanced Task-Adaptive Projection Network (L-TapNet) based on the state-of-the-art few-shot classification model – TapNet, by leveraging label name semantics in representing labels. Experimental results show that our model significantly outperforms the strongest few-shot learning baseline by 14.64 F1 scores in the one-shot setting.', 'corpus_id': 219558508, 'score': 0}, {'doc_id': '218538203', 'title': 'Learning Robust Models for e-Commerce Product Search', 'abstract': 'Showing items that do not match search query intent degrades customer experience in e-commerce. These mismatches result from counterfactual biases of the ranking algorithms toward noisy behavioral signals such as clicks and purchases in the search logs. Mitigating the problem requires a large labeled dataset, which is expensive and time-consuming to obtain. In this paper, we develop a deep, end-to-end model that learns to effectively classify mismatches and to generate hard mismatched examples to improve the classifier. We train the model end-to-end by introducing a latent variable into the cross-entropy loss that alternates between using the real and generated samples. This not only makes the classifier more robust but also boosts the overall ranking performance. Our model achieves a relative gain compared to baselines by over 26% in F-score, and over 17% in Area Under PR curve. On live search traffic, our model gains significant improvement in multiple countries.', 'corpus_id': 218538203, 'score': 0}, {'doc_id': '218900643', 'title': 'How to Retrain Recommender System?: A Sequential Meta-Learning Method', 'abstract': 'Practical recommender systems need be periodically retrained to refresh the model with new interaction data. To pursue high model fidelity, it is usually desirable to retrain the model on both historical and new data, since it can account for both long-term and short-term user preference. However, a full model retraining could be very time-consuming and memory-costly, especially when the scale of historical data is large. In this work, we study the model retraining mechanism for recommender systems, a topic of high practical values but has been relatively little explored in the research community. Our first belief is that retraining the model on historical data is unnecessary, since the model has been trained on it before. Nevertheless, normal training on new data only may easily cause overfitting and forgetting issues, since the new data is of a smaller scale and contains fewer information on long-term user preference. To address this dilemma, we propose a new training method, aiming to abandon the historical data during retraining through learning to transfer the past training experience.Specifically, we design a neural network-based transfer component, which transforms the old model to a new model that is tailored for future recommendations. To learn the transfer component well, we optimize the ""future performance\'\' -- i.e., the recommendation accuracy evaluated in the next time period. Our Sequential Meta-Learning(SML) method offers a general training paradigm that is applicable to any differentiable model. We demonstrate SML on matrix factorization and conduct experiments on two real-world datasets. Empirical results show that SML not only achieves significant speed-up, but also outperforms the full model retraining in recommendation accuracy, validating the effectiveness of our proposals. We release our codes at: https://github.com/zyang1580/SML.', 'corpus_id': 218900643, 'score': 0}, {'doc_id': '232073173', 'title': 'Contextual Relevance Matching for Ad-hoc Retrieval Based on Heterogeneous Graph Embeddings', 'abstract': 'Recent deep neural networks for ad-hoc retrieval mainly focus on semantic matching between queries and documents based on content/text information. In this work, we aim to extend the relevance matching of query-document pairs by incorporating contextual information such as URLs and location into the learning framework. We present the Contextual Relevance Matching Model (CRMM) that learns a common representation for contextual information in addition to textual information as a whole. It first formulates web search clicks, consisting of multi-typed objects, as a hyper-graph using click-through data. It is trained by maximizing the conditional likelihood of observing one object given other participating objects in a click. These object embedding representations are then used as features in a learning to rank function to predict the relevance score of candidate documents given a query. We evaluate CRMM over a real-world search engine traces labeled by human judges, where it achieves ∼0.52 nDCG@1.', 'corpus_id': 232073173, 'score': 1}, {'doc_id': '220251199', 'title': 'Kernel Density Estimation based Factored Relevance Model for Multi-Contextual Point-of-Interest Recommendation', 'abstract': ""An automated contextual suggestion algorithm is likely to recommend contextually appropriate and personalized 'points-of-interest' (POIs) to a user, if it can extract information from the user's preference history (exploitation) and effectively blend it with the user's current contextual information (exploration) to predict a POI's 'appropriateness' in the current context. To balance this trade-off between exploitation and exploration, we propose an unsupervised, generic framework involving a factored relevance model (FRLM), constituting two distinct components, one pertaining to historical contexts, and the other corresponding to the current context. We further generalize the proposed FRLM by incorporating the semantic relationships between terms in POI descriptors using kernel density estimation (KDE) on embedded word vectors. Additionally, we show that trip-qualifiers, (e.g. 'trip-type', 'accompanied-by') are potentially useful information sources that could be used to improve the recommendation effectiveness. Using such information is not straight forward since users' texts/reviews of visited POIs typically do not explicitly contain such annotations. We undertake a weakly supervised approach to predict the associations between the review-texts in a user profile and the likely trip contexts. Our experiments, conducted on the TREC contextual suggestion 2016 dataset, demonstrate that factorization, KDE-based generalizations, and trip-qualifier enriched contexts of the relevance model improve POI recommendation."", 'corpus_id': 220251199, 'score': 0}, {'doc_id': '220686321', 'title': 'Exploratory Search with Sentence Embeddings', 'abstract': 'Exploratory search aims to guide users through a corpus rather than pinpointing exact information. We propose an exploratory search system based on hierarchical clusters and document summaries using sentence embeddings. With sentence embeddings, we represent documents as the mean of their embedded sentences, extract summaries containing sentences close to this document representation and extract keyphrases close to the document representation. To evaluate our search system, we scrape our personal search history over the past year and report our experience with the system. We then discuss motivating use cases of an exploratory search system of this nature and conclude with possible directions of future work.', 'corpus_id': 220686321, 'score': 0}, {'doc_id': '221341086', 'title': 'Time-based Sequence Model for Personalization and Recommendation Systems', 'abstract': 'In this paper we develop a novel recommendation model that explicitly incorporates time information. The model relies on an embedding layer and TSL attention-like mechanism with inner products in different vector spaces, that can be thought of as a modification of multi-headed attention. This mechanism allows the model to efficiently treat sequences of user behavior of different length. We study the properties of our state-of-the-art model on statistically designed data set. Also, we show that it outperforms more complex models with longer sequence length on the Taobao User Behavior dataset.', 'corpus_id': 221341086, 'score': 1}, {'doc_id': '221605962', 'title': 'Similitude Attentive Relation Network for Click-Through Rate Prediction', 'abstract': 'In online advertising systems, having a good knowledge of user behavior is crucial for click-through rate (CTR) prediction. In recent years, many researchers turn to seek a better way of user representation by modeling the behavior sequences with recurrent neural network (RNN). However, recurrent layers implicitly adopt the assumption that elements with different orders are fundamentally different, which is inefficient in many practical scenarios with much uncertainty and complicated hidden states. In this paper, we follow the paradigm of Relation Network (RN), and propose a new model called Similitude Attentive Relation Network (SARN). The user behavior is modeled as a graph, where nodes correspond to the visited items and edges correspond to the relations. To capture the latent user interest better, the model concentrates on the relations between items, rather than the translation on the time series. More specifically, the model tries to learn the similarity between items in a semantic space through a learnable dot-product operation and blend both of the item representations and relational information together as the final relations. We define our user representation on an attentive pooling of the relations directly. To verify the effectiveness of our method, extensive experiments on two public datasets and one real-world online advertising dataset are conducted. Experimental results show that our methods achieve usually better performance than others. Besides, we explore the properties of our model by controlled experiments and show the learned relational knowledge by visualizing the inner states of SARN.', 'corpus_id': 221605962, 'score': 1}, {'doc_id': '46920987', 'title': 'Learning a unified embedding space of web search from large-scale query log', 'abstract': 'Abstract In the procedure of Web search, a user first comes up with an information need and a query is issued with the need as guidance. After that, some URLs are clicked and other queries may be issued if those URLs do not meet his need well. We advocate that Web search is governed by a unified hidden space, and each involved element such as query and URL has its inborn position, i.e., projected as a vector, in this space. Each of above actions in the search procedure, i.e. issuing queries or clicking URLs, is an interaction result of those elements in the space. In this paper, we aim at uncovering such a unified hidden space of Web search that uniformly captures the hidden semantics of search queries, URLs and other involved elements in Web search. We learn the semantic space with search session data, because a search session can be regarded as an instantiation of users’ information need on a particular semantic topic and it keeps the interaction information of queries and URLs. We use a set of session graphs to represent search sessions, and the space learning task is cast as a vector learning problem for the graph vertices by maximizing the log-likelihood of a training session data set. Specifically, we developed the well-known Word2vec to perform the learning procedure. Experiments on the query log data of a commercial search engine are conducted to examine the efficacy of learnt vectors, and the results show that our framework is helpful for different finer tasks in Web search.', 'corpus_id': 46920987, 'score': 1}, {'doc_id': '221761133', 'title': 'Learning to Personalize for Web Search Sessions', 'abstract': ""The task of session search focuses on using interaction data to improve relevance for the user's next query at the session level. In this paper, we formulate session search as a personalization task under the framework of learning to rank. Personalization approaches re-rank results to match a user model. Such user models are usually accumulated over time based on the user's browsing behaviour. We use a pre-computed and transparent set of user models based on concepts from the social science literature. Interaction data are used to map each session to these user models. Novel features are then estimated based on such models as well as sessions' interaction data. Extensive experiments on test collections from the TREC session track show statistically significant improvements over current session search algorithms."", 'corpus_id': 221761133, 'score': 1}]"
70	"{'doc_id': '220265874', 'title': ""On Bellman's Optimality Principle for zs-POSGs"", 'abstract': ""Many non-trivial sequential decision-making problems are efficiently solved by relying on Bellman's optimality principle, i.e., exploiting the fact that sub-problems are nested recursively within the original problem. Here we show how it can apply to (infinite horizon) 2-player zero-sum partially observable stochastic games (zs-POSGs) by (i) taking a central planner's viewpoint, which can only reason on a sufficient statistic called occupancy state, and (ii) turning such problems into zero-sum occupancy Markov games (zs-OMGs). Then, exploiting the Lipschitz-continuity of the value function in occupancy space, one can derive a version of the HSVI algorithm (Heuristic Search Value Iteration) that provably finds an $\\epsilon$-Nash equilibrium in finite time."", 'corpus_id': 220265874}"	1172	"[{'doc_id': '220961427', 'title': 'Deep Inverse Q-learning with Constraints', 'abstract': 'Popular Maximum Entropy Inverse Reinforcement Learning approaches require the computation of expected state visitation frequencies for the optimal policy under an estimate of the reward function. This usually requires intermediate value estimation in the inner loop of the algorithm, slowing down convergence considerably. In this work, we introduce a novel class of algorithms that only needs to solve the MDP underlying the demonstrated behavior once to recover the expert policy. This is possible through a formulation that exploits a probabilistic behavior assumption for the demonstrations within the structure of Q-learning. We propose Inverse Action-value Iteration which is able to fully recover an underlying reward of an external agent in closed-form analytically. We further provide an accompanying class of sampling-based variants which do not depend on a model of the environment. We show how to extend this class of algorithms to continuous state-spaces via function approximation and how to estimate a corresponding action-value function, leading to a policy as close as possible to the policy of the external agent, while optionally satisfying a list of predefined hard constraints. We evaluate the resulting algorithms called Inverse Action-value Iteration, Inverse Q-learning and Deep Inverse Q-learning on the Objectworld benchmark, showing a speedup of up to several orders of magnitude compared to (Deep) Max-Entropy algorithms. We further apply Deep Constrained Inverse Q-learning on the task of learning autonomous lane-changes in the open-source simulator SUMO achieving competent driving after training on data corresponding to 30 minutes of demonstrations.', 'corpus_id': 220961427, 'score': 0}, {'doc_id': '225103340', 'title': 'Linear Regression Games: Convergence Guarantees to Approximate Out-of-Distribution Solutions', 'abstract': 'Recently, invariant risk minimization (IRM) (Arjovsky et al.) was proposed as a promising solution to address out-of-distribution (OOD) generalization. In Ahuja et al., it was shown that solving for the Nash equilibria of a new class of ""ensemble-games"" is equivalent to solving IRM. In this work, we extend the framework in Ahuja et al. for linear regressions by projecting the ensemble-game on an $\\ell_{\\infty}$ ball. We show that such projections help achieve non-trivial OOD guarantees despite not achieving perfect invariance. For linear models with confounders, we prove that Nash equilibria of these games are closer to the ideal OOD solutions than the standard empirical risk minimization (ERM) and we also provide learning algorithms that provably converge to these Nash Equilibria. Empirical comparisons of the proposed approach with the state-of-the-art show consistent gains in achieving OOD solutions in several settings involving anti-causal variables and confounders.', 'corpus_id': 225103340, 'score': 1}, {'doc_id': '221739125', 'title': 'Finding and Certifying (Near-)Optimal Strategies in Black-Box Extensive-Form Games', 'abstract': 'Often---for example in war games, strategy video games, and financial simulations---the game is given to us only as a black-box simulator in which we can play it. In these settings, since the game may have unknown nature action distributions (from which we can only obtain samples) and/or be too large to expand fully, it can be difficult to compute strategies with guarantees on exploitability. Recent work \\cite{Zhang20:Small} resulted in a notion of certificate for extensive-form games that allows exploitability guarantees while not expanding the full game tree. However, that work assumed that the black box could sample or expand arbitrary nodes of the game tree at any time, and that a series of exact game solves (via, for example, linear programming) can be conducted to compute the certificate. Each of those two assumptions severely restricts the practical applicability of that method. In this work, we relax both of the assumptions. We show that high-probability certificates can be obtained with a black box that can do nothing more than play through games, using only a regret minimizer as a subroutine. As a bonus, we obtain an equilibrium-finding algorithm with $\\tilde O(\\sqrt{T})$ regret bound in the extensive-form game setting that does not rely on a sampling strategy with lower-bounded reach probabilities (which MCCFR assumes). We demonstrate experimentally that, in the black-box setting, our methods are able to provide nontrivial exploitability guarantees while expanding only a small fraction of the game tree.', 'corpus_id': 221739125, 'score': 1}, {'doc_id': '221319769', 'title': 'Optimal Strategies in Weighted Limit Games (full version)', 'abstract': 'We prove the existence and computability of optimal strategies in weighted limit games, zero-sum infinite-duration games with a Buchi-style winning condition requiring to produce infinitely many play prefixes that satisfy a given regular specification. Quality of plays is measured in the maximal weight of infixes between successive play prefixes that satisfy the specification.', 'corpus_id': 221319769, 'score': 0}, {'doc_id': '220961545', 'title': 'Selection problems in Large Deviations in Games under the logit choice protocol', 'abstract': 'We study large deviations in coordination games under the logit choice protocol. A major open question that [10,11] posed is whether large deviations properties under the small noise double limit and the large population double limit are identical or not. We rephrase this open question in the PDE language as some selection problems, and we provide some definitive answers to these problems.', 'corpus_id': 220961545, 'score': 0}, {'doc_id': '231693175', 'title': 'Estimating $\\alpha$-Rank by Maximizing Information Gain', 'abstract': 'Game theory has been increasingly applied in settings where the game is not known outright, but has to be estimated by sampling. For example, meta-games that arise in multi-agent evaluation can only be accessed by running a succession of expensive experiments that may involve simultaneous deployment of several agents. In this paper, we focus on α-rank, a popular game-theoretic solution concept designed to perform well in such scenarios. We aim to estimate the α-rank of the game using as few samples as possible. Our algorithm maximizes information gain between an epistemic belief over the α-ranks and the observed payoff. This approach has two main benefits. First, it allows us to focus our sampling on the entries that matter the most for identifying the α-rank. Second, the Bayesian formulation provides a facility to build in modeling assumptions by using a prior over game payoffs. We show the benefits of using information gain as compared to the confidence interval criterion of ResponseGraphUCB (Rowland et al. 2019), and provide theoretical results justifying our method.', 'corpus_id': 231693175, 'score': 1}, {'doc_id': '221995766', 'title': 'Parameter Critic: a Model Free Variance Reduction Method Through Imperishable Samples', 'abstract': 'We consider the problem of finding a policy that maximizes an expected reward throughout the trajectory of an agent that interacts with an unknown environment. Frequently denoted Reinforcement Learning, this framework suffers from the need of large amount of samples in each step of the learning process. To this end, we introduce parameter critic, a formulation that allows samples to keep their validity even when the parameters of the policy change. In particular, we propose the use of a function approximator to directly learn the relationship between the parameters and the expected cumulative reward. Through convergence analysis, we demonstrate the parameter critic outperforms gradient-free parameter space exploration techniques as it is robust to noise. Empirically, we show that our method solves the cartpole problem which corroborates our claim as the agent can successfully learn an optimal policy while learning the relationship between the parameters and the cumulative reward.', 'corpus_id': 221995766, 'score': 0}, {'doc_id': '219721024', 'title': 'Linear Last-iterate Convergence for Matrix Games and Stochastic Games', 'abstract': 'Optimistic Gradient Descent Ascent (OGDA) algorithm for saddle-point optimization has received growing attention due to its favorable last-iterate convergence. However, its behavior for simple two-player matrix games is still not fully understood -- previous analysis lacks explicit convergence rates, only applies to exponentially small learning rate, or requires additional conditions such as uniqueness of the optimal solution. In this work, we significantly expand the understanding of OGDA, introducing a set of sufficient conditions under which OGDA exhibits concrete last-iterate convergence rates with a constant learning rate. Specifically, we show that matrix games satisfy these conditions and OGDA converges exponentially fast without any additional assumptions. More generally, our conditions hold for smooth bilinear functions and strongly-convex-strongly-concave functions over a constrained set. We provide experimental results to further support our theory. To further demonstrate the significance of our results for matrix games, we greatly generalize the ideas to finite-horizon stochastic/Markov games and provide the first algorithm that simultaneously ensures 1) linear last-iterate convergence when playing against itself and 2) low regret when playing against an arbitrary slowly-changing opponent.', 'corpus_id': 219721024, 'score': 1}, {'doc_id': '221090472', 'title': 'Online Nash Social Welfare via Promised Utilities.', 'abstract': 'We consider the problem of allocating a set of divisible goods to $N$ agents in an online manner over $T$ periods, with adversarially-chosen normalized valuations in each period. Our goal is to maximize the Nash social welfare, a widely studied objective which provides a balance between fairness and efficiency. On the positive side, we provide an online algorithm that achieves a competitive ratio of $O(\\log N)$ and $O(\\log T)$, but also a stronger competitive ratio of $O(\\log k)$ in settings where the value of any agent for her most preferred item is no more than $k$ times her average value. We complement this by showing this bound is essentially tight: no online algorithm can achieve a competitive ratio of $O(\\log^{1-\\epsilon} N)$ or $O(\\log^{1-\\epsilon} T)$ for any constant $\\epsilon>0$.', 'corpus_id': 221090472, 'score': 0}, {'doc_id': '231740410', 'title': 'Poincaré-Bendixson Limit Sets in Multi-Agent Learning', 'abstract': 'A key challenge of evolutionary game theory and multi-agent learning is to characterize the limit behaviour of game dynamics. Whereas convergence is often a property of learning algorithms in games satisfying a particular reward structure (e.g. zero-sum), it is well known, that for general payoffs even basic learning models, such as the replicator dynamics, are not guaranteed to converge. Worse yet, chaotic behavior is possible even in rather simple games, such as variants of Rock-Paper-Scissors games [35]. Although chaotic behavior in learning dynamics can be precluded by the celebrated Poincaré-Bendixson theorem, it is only applicable to low-dimensional settings. Are there other characteristics of a game, which can force regularity in the limit sets of learning? In this paper, we show that behaviors consistentwith the PoincaréBendixson theorem (limit cycles, but no chaotic attractor) follows purely based on the topological structure of the interaction graph, even for high-dimensional settings with arbitrary number of players and arbitrary payoff matrices. We prove our result for a wide class of follow-the-regularized leader (FoReL) dynamics, which generalize replicator dynamics, for games where each player has two strategies at disposal, and for interaction graphs where payoffs of each agent are only affected by one other agent (i.e. interaction graphs of indegree one). Since chaos has been observed in a game with only two players and three strategies, this class of non-chaotic games is in a sense maximal. Moreover, we provide simple conditions under which such behavior translates to social welfare guarantees, implying that FoReL learning achieves time average social welfare which is at least as good as that of a Nash equilibrium; and connecting the topology of the dynamics to the Price of Anarchy analysis.', 'corpus_id': 231740410, 'score': 1}]"
71	{'doc_id': '231662526', 'title': 'Topological bands in the continuum using Rydberg states', 'abstract': 'The quest to realize topological band structures in artificial matter is strongly focused on lattice systems, and only quantum Hall physics is known to appear naturally also in the continuum. In this letter, we present a proposal based on a two-dimensional cloud of atoms dressed to Rydberg states, where excitations propagate by dipolar exchange interaction, while the Rydberg blockade phenomenon naturally gives rise to a characteristic length scale, suppressing the hopping on short distances. Then, the system becomes independent of the atoms’ spatial arrangement and can be described by a continuum model. We demonstrate the appearance of a topological band structure in the continuum characterized by a Chern number C = 2 and show that edge states appear at interfaces tunable by the atomic density.', 'corpus_id': 231662526}	7500	[{'doc_id': '221222207', 'title': 'Ultra-sensitive and label-free detection of the measles virus using an N-heterocyclic carbene-based electrochemical biosensor.', 'abstract': 'With the current intense need for the rapid and accurate detection of viruses due to COVID-19 , we report on a platform technology that is very well suited for this purpose, using intact measles virus as a demonstration. Cases of infection due to the measles virus are rapidly increasing and yet current diagnostic tools used to monitor for the virus rely on slow (>1 hour) technologies. Here, we demonstrate the first biosensor capable of detecting the measles virus in minutes with no preprocessing steps. The key sensing element is an electrode coated with a self-assembled monolayer containing the measles antibody, immobilized through an N-heterocyclic carbene (NHC). The intact virus is detected by changes in resistance, giving a linear response to 10-100 µg/mL of the intact measles virus without the need to label or process the sample. The limit of detection is 6 µg/mL, which is at the lower limit of concentrations that can cause infections in primates. The NHC-based biosensors are shown to be superior to thiol-based systems, producing an approximately 10x larger response and significantly greater stability towards repeated measurements and long-term storage. This NHC-based biosensor thus represents an important development for both the rapid detection of the measles virus and as a platform technology for the detection of other biological targets of interest.', 'corpus_id': 221222207, 'score': 0}, {'doc_id': '232417421', 'title': 'First-order Bose-Einstein condensation with three-body interacting bosons', 'abstract': 'Bose-Einstein condensation, observed in either strongly interacting liquid helium or weakly interacting atomic Bose gases, is widely known to be a second-order phase transition. Here, we predict a first-order Bose-Einstein condensation in a cloud of harmonically trapped bosons interacting with both attractive two-body interaction and repulsive three-body interaction, characterized respectively by an $s$-wave scattering length $a 0$. It happens when the harmonic trapping potential is weak, so with increasing temperature the system changes from a low-temperature liquid-like quantum droplet to a normal gas, and therefore experiences a first-order liquid-to-gas transition. At large trapping potential, however, the quantum droplet can first turn into a superfluid gas, rendering the condensation transition occurred later from a superfluid gas to a normal gas smooth. We determine a rich phase diagram and show the existence of a tri-critical point, where the three phases - quantum droplet, superfluid gas and normal gas - meet together. We argue that an ensemble of spin-polarized tritium atoms could be a promising candidate to observe the predicted first-order Bose-Einstein condensation, across which the condensate fraction or central condensate density jumps to zero and the surface-mode frequencies diverge.', 'corpus_id': 232417421, 'score': 1}, {'doc_id': '232170120', 'title': 'Controlling anisotropic dipolar interaction with shielding resonance in a three-dimensional molecular quantum gas', 'abstract': None, 'corpus_id': 232170120, 'score': 1}, {'doc_id': '235765543', 'title': 'On the electrostatic interactions involving long-range Rydberg molecules', 'abstract': 'A ground state atom immersed in the wave function of the valence electron of a Rydberg atom can generate a long-range Rydberg molecule (LRRM). In this work, using the multipole expansion of the electrostatic interaction in prolate spheroidal coordinates, approximate and compact expressions of the electrostatic potential that determine the chemistry of trilobite and butterfly LRRM are explored. It is shown that even the prolate spheroidal monopole term can be used to describe general features of the potential generated by an LRRM at short distances. It is also shown that even at long separations that allow a perturbative description of the intermolecular interaction between two LRRM, the convergence of the multipole prolate spheroidal expansion is faster than that of its spherical analogue.', 'corpus_id': 235765543, 'score': 1}, {'doc_id': '232240433', 'title': 'Mediated interaction between polarons in a one-dimensional Bose gas', 'abstract': 'We study a weakly-interacting one-dimensional Bose gas with two impurities coupled locally to the boson density. We derive analytical results for the induced interaction between the impurities at arbitrary coupling and separation r. At r . ξ, where ξ denotes the healing length of the Bose gas, the interaction is well described by the mean-field contribution. Its form changes as the coupling is increased, approaching a linear function of r at short distances in the regime of strong coupling. The mean-field contribution decays exponentially at arbitrary coupling for r ξ. At such long distances, however, the effect of quantum fluctuations becomes important, giving rise to a long-ranged quantum contribution to the induced interaction. At longest distances it behaves as 1/r, while at strong coupling we find an intermediate distance regime with a slower decay, 1/r. The quantum contribution in the crossover regime is also calculated. The induced interaction between impurities (i.e., polarons) is attractive and leads to the formation of their bound state, known as bipolaron. We discuss its binding energy.', 'corpus_id': 232240433, 'score': 1}, {'doc_id': '230523882', 'title': 'Collisions between Ultracold Molecules and Atoms in a Magnetic Trap.', 'abstract': 'We prepare mixtures of ultracold CaF molecules and Rb atoms in a magnetic trap and study their inelastic collisions. When the atoms are prepared in the spin-stretched state and the molecules in the spin-stretched component of the first rotationally excited state, they collide inelastically with a rate coefficient k_{2}=(6.6±1.5)×10^{-11}\u2009\u2009cm^{3}/s at temperatures near 100\u2009\u2009μK. We attribute this to rotation-changing collisions. When the molecules are in the ground rotational state we see no inelastic loss and set an upper bound on the spin-relaxation rate coefficient of k_{2}<5.8×10^{-12}\u2009\u2009cm^{3}/s with 95%\xa0confidence. We compare these measurements to the results of a single-channel loss model based on quantum defect theory. The comparison suggests a short-range loss parameter close to unity for rotationally excited molecules, but below 0.04 for molecules in the rotational ground state.', 'corpus_id': 230523882, 'score': 0}, {'doc_id': '224704544', 'title': 'Sub-Poissonian atom-number distributions by means of Rydberg dressing and electromagnetically induced transparency', 'abstract': 'A method is proposed to produce atomic ensembles with sub-Poissonian atom number distributions. The method consists of removing the excess atoms using the interatomic interactions induced by Rydberg dressing. The selective removal of atoms occurs via spontaneous decay into untrapped states using an electromagnetically induced transparency scheme. Ensembles with the desired number of atoms can be produced almost deterministically. Numerical simulations predict a strong reduction of the atom number fluctuations, with the variance twenty times less than the Poisson noise level (the predicted Fano factor is F = 0.05). Strikingly, the method is suitable for both fermions and bosons. It solves the problem of the atom-number fluctuations in bosons, whose weak interactions have usually been an obstacle to controlling the number of atoms.', 'corpus_id': 224704544, 'score': 0}, {'doc_id': '220961423', 'title': 'Landau-Fermi liquids without quasiparticles', 'abstract': 'Landau-Fermi liquid theory is conventionally believed to hold whenever the interacting single-particle density of states develops a $\\delta$-like component at the Fermi surface, which is associated with quasiparticles. Here we show that a microscopic justification can be actually achieved under more general circumstances, even in case coherent quasiparticles are totally missing and the interacting single-particle density of states vanishes at the chemical potential as consequence of a pole singularity in the self-energy.', 'corpus_id': 220961423, 'score': 0}, {'doc_id': '232221222', 'title': 'Bound and Subradiant Multi-Atom Excitations in an Atomic Array with Nonreciprocal Couplings', 'abstract': 'Collective decays of multiply-excited atoms become subradiant and bound in space when they are strongly coupled to the guided modes in an atom-waveguide interface. In this interface, we analyze their average densitydensity and modified third-order correlations via Kubo cumulant expansions, which can arise and sustain for long time. The shape-preserving dimers and trimers of atomic excitations emerge in the most subradiant coupling regime of light-induced dipole-dipole interactions. This leads to a potential application of quantum information processing and quantum storage in the encoded nonreciprocal spin diffusion, where its diffusion speed depends on the initial coherence between the excited atoms and is robust to their relative phase fluctuations. The state-dependent photon routing can be viable as well in this interface.', 'corpus_id': 232221222, 'score': 1}, {'doc_id': '229221580', 'title': 'Coherent Optical Creation of a Single Molecule', 'abstract': 'We report coherent association of atoms into a single weakly bound NaCs molecule in an optical tweezer through an optical Raman transition. The Raman technique uses a deeply bound electronic excited intermediate state to achieve a large transition dipole moment while reducing photon scattering. Starting from two atoms in their relative motional ground state, we achieve an optical transfer efficiency of 69%. The molecules have a binding energy of 770.2MHz at 8.83(2)G. This technique does not rely on Feshbach resonances or narrow excited-state lines and may allow a wide range of molecular species to be assembled atom-by-atom.', 'corpus_id': 229221580, 'score': 0}]
72	{'doc_id': '212778472', 'title': '3D Epigenomic Characterization Reveals Insights Into Gene Regulation and Lineage Specification During Corticogenesis', 'abstract': 'Lineage-specific epigenomic changes during human corticogenesis have previously remained elusive due to challenges with tissue heterogeneity and sample availability. Here, we analyze cis-regulatory chromatin interactions, open chromatin regions, and transcriptomes for radial glia, intermediate progenitor cells, excitatory neurons, and interneurons isolated from mid-gestational human brain samples. We show that chromatin looping underlies transcriptional regulation for lineage-specific genes, with transcription factor motifs, families of transposable elements, and disease-associated variants enriched at distal interacting regions in a cell type-specific manner. A subset of promoters exhibit unusually high degrees of chromatin interactivity, which we term super interactive promoters. Super interactive promoters are enriched for critical lineage-specific genes, suggesting that interactions at these loci contribute to the fine-tuning of cell type-specific transcription. Finally, we present CRISPRview, a novel approach for validating distal interacting regions in primary cells. Our study presents the first characterization of cell type-specific 3D epigenomic landscapes during human corticogenesis, advancing our understanding of gene regulation and lineage specification during human brain development.', 'corpus_id': 212778472}	2118	"[{'doc_id': '72936830', 'title': 'Glucocorticoid Receptor Stimulation Resulting from Early Life Stress Affects Expression of DNA Methyltransferases in Rat Prefrontal Cortex', 'abstract': 'Early life stress initiates long-term neurobiological changes that affect stress resilience and increased susceptibility to psychopathology. Maternal separation (MS) is used to cause early life stress and it induces profound neurochemical and behavioral changes that last until adulthood. The molecular pathways of how MS affects the regulation of DNA methyltransferases (Dnmt) in brain have not been entirely characterized. We evaluated MS effects on Dnmt1, Dnmt3a and Dnmt3b expression, DNMT enzyme activity and glucocorticoid receptor (GR) recruitment to different Dnmt loci in the prefrontal cortex (PFC) of Wistar rats. We found increased plasma corticosterone levels after MS that were associated with induced Dnmt expression and enzyme activity in rat PFC at post-natal day 15 (PND15). Chromatin immunoprecipitation showed increased binding of GR at the Dnmt3b promoter after MS, suggesting that genomic signaling of GR is an important regulatory mechanism for the induced Dnmt3b expression and DNMT activity. Although GR also binds to Dnmt3a promoter and a putative regulatory region in intron 3 in rat PFC, its expression after maternal separation may be influenced by other mechanisms. Therefore, GR could be a link between early life stress experience and long-term gene expression changes induced by aberrant DNA methylation.', 'corpus_id': 72936830, 'score': 1}, {'doc_id': '152553', 'title': 'Apoptosis in the nervous system', 'abstract': ""Neuronal apoptosis sculpts the developing brain and has a potentially important role in neurodegenerative diseases. The principal molecular components of the apoptosis programme in neurons include Apaf-1 (apoptotic protease-activating factor 1) and proteins of the Bcl-2 and caspase families. Neurotrophins regulate neuronal apoptosis through the action of critical protein kinase cascades, such as the phosphoinositide 3-kinase/Akt and mitogen-activated protein kinase pathways. Similar cell-death-signalling pathways might be activated in neurodegenerative diseases by abnormal protein structures, such as amyloid fibrils in Alzheimer's disease. Elucidation of the cell death machinery in neurons promises to provide multiple points of therapeutic intervention in neurodegenerative diseases."", 'corpus_id': 152553, 'score': 1}, {'doc_id': '207196134', 'title': 'Human DNA Polymerase μ Can Use a Noncanonical Mechanism for Multiple Mn2+-Mediated Functions.', 'abstract': 'Recent research on the structure and mechanism of DNA polymerases has continued to generate fundamentally important features, including a noncanonical pathway involving ""prebinding"" of metal-bound dNTP (MdNTP) in the absence of DNA. While this noncanonical mechanism was shown to be a possible subset for African swine fever DNA polymerase X (Pol X) and human Pol λ, it remains unknown whether it could be the primary pathway for a DNA polymerase. Pol μ is a unique member of the X-family with multiple functions and with unusual Mn2+ preference. Here we report that Pol μ not only prebinds MdNTP in a catalytically active conformation but also exerts a Mn2+ over Mg2+ preference at this early stage of catalysis, for various functions: incorporation of dNTP into a single nucleotide gapped DNA, incorporation of rNTP in the nonhomologous end joining (NHEJ) repair, incorporation of dNTP to an ssDNA, and incorporation of an 8-oxo-dGTP opposite template dA (mismatched) or dC (matched). The structural basis of this noncanonical mechanism and Mn2+ over Mg2+ preference in these functions was analyzed by solving 19 structures of prebinding binary complexes, precatalytic ternary complexes, and product complexes. The results suggest that the noncanonical pathway is functionally relevant for the multiple functions of Pol μ. Overall, this work provides the structural and mechanistic basis for the long-standing puzzle in the Mn2+ preference of Pol μ and expands the landscape of the possible mechanisms of DNA polymerases to include both mechanistic pathways.', 'corpus_id': 207196134, 'score': 1}, {'doc_id': '214697436', 'title': 'Associations between immune-suppressive and stimulating drugs and novel COVID-19—a systematic review of current evidence', 'abstract': 'Background Cancer and transplant patients with COVID-19 have a higher risk of developing severe and even fatal respiratory diseases, especially as they may be treated with immune-suppressive or immune-stimulating drugs. This review focuses on the effects of these drugs on host immunity against COVID-19. Methods Using Ovid MEDLINE, we reviewed current evidence for immune-suppressing or -stimulating drugs: cytotoxic chemotherapy, low-dose steroids, tumour necrosis factorα (TNFα) blockers, interlukin-6 (IL-6) blockade, Janus kinase (JAK) inhibitors, IL-1 blockade, mycophenolate, tacrolimus, anti-CD20 and CTLA4-Ig. Results 89 studies were included. Cytotoxic chemotherapy has been shown to be a specific inhibitor for severe acute respiratory syndrome coronavirus in in vitro studies, but no specific studies exist as of yet for COVID-19. No conclusive evidence for or against the use of non-steroidal anti-inflammatory drugs (NSAIDs) in the treatment of COVID-19 patients is available, nor is there evidence indicating that TNFα blockade is harmful to patients in the context of COVID-19. COVID-19 has been observed to induce a pro-inflammatory cytokine generation and secretion of cytokines, such as IL-6, but there is no evidence of the beneficial impact of IL-6 inhibitors on the modulation of COVID-19. Although there are potential targets in the JAK-STAT pathway that can be manipulated in treatment for coronaviruses and it is evident that IL-1 is elevated in patients with a coronavirus, there is currently no evidence for a role of these drugs in treatment of COVID-19. Conclusion The COVID-19 pandemic has led to challenging decision-making about treatment of critically unwell patients. Low-dose prednisolone and tacrolimus may have beneficial impacts on COVID-19. The mycophenolate mofetil picture is less clear, with conflicting data from pre-clinical studies. There is no definitive evidence that specific cytotoxic drugs, low-dose methotrexate for auto-immune disease, NSAIDs, JAK kinase inhibitors or anti-TNFα agents are contraindicated. There is clear evidence that IL-6 peak levels are associated with severity of pulmonary complications.', 'corpus_id': 214697436, 'score': 0}, {'doc_id': '69160700', 'title': 'Interpreting polygenic scores, polygenic adaptation, and human phenotypic differences', 'abstract': 'Abstract Recent analyses of polygenic scores have opened new discussions concerning the genetic basis and evolutionary significance of differences among populations in distributions of phenotypes. Here, we highlight limitations in research on polygenic scores, polygenic adaptation and population differences. We show how genetic contributions to traits, as estimated by polygenic scores, combine with environmental contributions so that differences among populations in trait distributions need not reflect corresponding differences in genetic propensity. Under a null model in which phenotypes are selectively neutral, genetic propensity differences contributing to phenotypic differences among populations are predicted to be small. We illustrate this null hypothesis in relation to health disparities between African Americans and European Americans, discussing alternative hypotheses with selective and environmental effects. Close attention to the limitations of research on polygenic phenomena is important for the interpretation of their relationship to human population differences.', 'corpus_id': 69160700, 'score': 1}, {'doc_id': '211218317', 'title': 'A Precision Medicine Approach to Rhinitis Evaluation and Management', 'abstract': 'Purpose of Review Precision medicine (PM) represents a new paradigm in disease diagnosis, prevention, and treatment. The PM approach focuses on the characterization of different phenotypes and pathogenic pathways in order to allow the selection of specific biomarkers that will be useful in disease management. Rhinitis is a highly prevalent and heterogeneous disease, both in terms of underlying endotypes and clinical presentations. Therefore, to apply the PM principles to the various rhinitis subtypes rise as a meaningful strategy to improve evaluation and treatment. Recent Findings The technology of recombinant allergens has allowed molecular characterization of IgE reactivity of specific individual components of allergenic extracts. Recently published and ongoing clinical trials based on component resolved diagnosis (CRD) bring more precision to allergen immunotherapy for allergic rhinitis. Monoclonal antibodies against various cytokines involved in inflammatory allergic and nonallergic rhinitis endotypes show promissory results. Summary Better understanding of pathogenic pathways together with an accurate phenotype classification of patients presented with rhinitis symptoms contributes to point out clinical usefulness of biomarkers and other diagnostic tools, which leads to more accurate environmental control measures, personalized pharmacologic options, and new biological therapy developments.', 'corpus_id': 211218317, 'score': 0}, {'doc_id': '3572556', 'title': 'The impact of developmental timing for stress and recovery', 'abstract': 'Stress can have lasting effects on the brain and behavior. Delineating the impact of stress on the developing brain is fundamental for understanding mechanisms through which stress induces persistent effects on behavior that can lead to psychopathology. The growing field of translational developmental neuroscience has revealed a significant role of the timing of stress on risk, resilience, and neuroplasticity. Studies of stress across species have provided essential insight into the mechanisms by which the brain changes and the timing of those changes on outcome. In this article, we review the neurobiological effects of stress and propose a model by which sensitive periods of neural development interact with stressful life events to affect plasticity and the effects of stress on functional outcomes. We then highlight how early-life stress can alter the course of brain development. Finally, we examine mechanisms of buffering against early-life stress that may promote resilience and positive outcomes. The findings are discussed in the context of implications for early identification of risk and resilience factors and development of novel interventions that target the biological state of the developing brain to ultimately ameliorate the adverse consequences of stress during childhood and adolescence.', 'corpus_id': 3572556, 'score': 1}, {'doc_id': '212403702', 'title': 'Collaborative Cross Mice Yield Genetic Modifiers for Pseudomonas aeruginosa Infection in Human Lung Disease', 'abstract': 'Respiratory infection caused by P. aeruginosa is one of the most critical health burdens worldwide. People affected by P. aeruginosa infection include patients with a weakened immune system, such as those with cystic fibrosis (CF) genetic disease or non-CF bronchiectasis. Disease outcomes range from fatal pneumonia to chronic life-threatening infection and inflammation leading to the progressive deterioration of pulmonary function. The development of these respiratory infections is mediated by multiple causes. However, the genetic factors underlying infection susceptibility are poorly known and difficult to predict. Our study employed novel approaches and improved mouse disease models to identify genetic modifiers that affect the severity of P. aeruginosa lung infection. We identified candidate genes to enhance our understanding of P. aeruginosa infection in humans and provide a proof of concept that could be exploited for other human pathologies mediated by bacterial infection. ABSTRACT Human genetics influence a range of pathological and clinical phenotypes in respiratory infections; however, the contributions of disease modifiers remain underappreciated. We exploited the Collaborative Cross (CC) mouse genetic-reference population to map genetic modifiers that affect the severity of Pseudomonas aeruginosa lung infection. Screening for P. aeruginosa respiratory infection in a cohort of 39 CC lines exhibits distinct disease phenotypes ranging from complete resistance to lethal disease. Based on major changes in the survival times, a quantitative-trait locus (QTL) was mapped on murine chromosome 3 to the genomic interval of Mb 110.4 to 120.5. Within this locus, composed of 31 protein-coding genes, two candidate genes, namely, dihydropyrimidine dehydrogenase (Dpyd) and sphingosine-1-phosphate receptor 1 (S1pr1), were identified according to the level of genome-wide significance and disease gene prioritization. Functional validation of the S1pr1 gene by pharmacological targeting in C57BL/6NCrl mice confirmed its relevance in P. aeruginosa pathophysiology. However, in a cohort of Canadian patients with cystic fibrosis (CF) disease, regional genetic-association analysis of the syntenic human locus on chromosome 1 (Mb 97.0 to 105.0) identified two single-nucleotide polymorphisms (rs10875080 and rs11582736) annotated to the Dpyd gene that were significantly associated with age at first P. aeruginosa infection. Thus, there is evidence that both genes might be implicated in this disease. Our results demonstrate that the discovery of murine modifier loci may generate information that is relevant to human disease progression. IMPORTANCE Respiratory infection caused by P. aeruginosa is one of the most critical health burdens worldwide. People affected by P. aeruginosa infection include patients with a weakened immune system, such as those with cystic fibrosis (CF) genetic disease or non-CF bronchiectasis. Disease outcomes range from fatal pneumonia to chronic life-threatening infection and inflammation leading to the progressive deterioration of pulmonary function. The development of these respiratory infections is mediated by multiple causes. However, the genetic factors underlying infection susceptibility are poorly known and difficult to predict. Our study employed novel approaches and improved mouse disease models to identify genetic modifiers that affect the severity of P. aeruginosa lung infection. We identified candidate genes to enhance our understanding of P. aeruginosa infection in humans and provide a proof of concept that could be exploited for other human pathologies mediated by bacterial infection.', 'corpus_id': 212403702, 'score': 0}]"
73	{'doc_id': '222209229', 'title': 'Treating COVID-19 With Hydroxychloroquine (TEACH): A Multicenter, Double-Blind Randomized Controlled Trial in Hospitalized Patients', 'abstract': 'Abstract Background Effective therapies to combat coronavirus 2019 (COVID-19) are urgently needed. Hydroxychloroquine (HCQ) has in vitro antiviral activity against severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2), but the clinical benefit of HCQ in treating COVID-19 is unclear. Randomized controlled trials are needed to determine the safety and efficacy of HCQ for the treatment of hospitalized patients with COVID-19. Methods We conducted a multicenter, double-blind randomized clinical trial of HCQ among patients hospitalized with laboratory-confirmed COVID-19. Subjects were randomized in a 1:1 ratio to HCQ or placebo for 5 days and followed for 30 days. The primary efficacy outcome was a severe disease progression composite end point (death, intensive care unit admission, mechanical ventilation, extracorporeal membrane oxygenation, and/or vasopressor use) at day 14. Results A total of 128 patients were included in the intention-to-treat analysis. Baseline demographic, clinical, and laboratory characteristics were similar between the HCQ (n\u2005=\u200567) and placebo (n\u2005=\u200561) arms. At day 14, 11 (16.4%) subjects assigned to HCQ and 6 (9.8%) subjects assigned to placebo met the severe disease progression end point, but this did not achieve statistical significance (P\u2005=\u2005.350). There were no significant differences in COVID-19 clinical scores, number of oxygen-free days, SARS-CoV-2 clearance, or adverse events between HCQ and placebo. HCQ was associated with a slight increase in mean corrected QT interval, an increased D-dimer, and a trend toward an increased length of stay. Conclusions In hospitalized patients with COVID-19, our data suggest that HCQ does not prevent severe outcomes or improve clinical scores. However, our conclusions are limited by a relatively small sample size, and larger randomized controlled trials or pooled analyses are needed.', 'corpus_id': 222209229}	5596	[{'doc_id': '220437021', 'title': 'Intravenous high-dose vitamin C for the treatment of severe COVID-19: study protocol for a multicentre randomised controlled trial', 'abstract': 'Introduction The rapid worldwide spread of COVID-19 has caused a global health crisis. To date, symptomatic supportive care has been the most common treatment. It has been reported that the mechanism of COVID-19 is related to cytokine storms and subsequent immunogenic damage, especially damage to the endothelium and alveolar membrane. Vitamin C (VC), also known as L-ascorbic acid, has been shown to have antimicrobial and immunomodulatory properties. A high dose of intravenous VC (HIVC) was proven to block several key components of cytokine storms, and HIVC showed safety and varying degrees of efficacy in clinical trials conducted on patients with bacterial-induced sepsis and acute respiratory distress syndrome (ARDS). Therefore, we hypothesise that HIVC could be added to the treatment of ARDS and multiorgan dysfunction related to COVID-19. Methods and analysis The investigators designed a multicentre prospective randomised placebo-controlled trial that is planned to recruit 308 adults diagnosed with COVID-19 and transferred into the intensive care unit. Participants will randomly receive HIVC diluted in sterile water or placebo for 7 days once enrolled. Patients with a history of VC allergy, end-stage pulmonary disease, advanced malignancy or glucose-6-phosphate dehydrogenase deficiency will be excluded. The primary outcome is ventilation-free days within 28 observational days. This is one of the first clinical trials applying HIVC to treat COVID-19, and it will provide credible efficacy and safety data. We predict that HIVC could suppress cytokine storms caused by COVID-19, help improve pulmonary function and reduce the risk of ARDS of COVID-19. Ethics and dissemination The study protocol was approved by the Ethics Committee of Zhongnan Hospital of Wuhan University (identifiers: Clinical Ethical Approval No. 2020001). Findings of the trial will be disseminated through peer-reviewed journals and scientific conferences. Trial registration number NCT04264533.', 'corpus_id': 220437021, 'score': 0}, {'doc_id': '220602322', 'title': 'COVID-19 in people with diabetes: understanding the reasons for worse outcomes', 'abstract': '\n Summary\n \n Since the initial COVID-19 outbreak in China, much attention has focused on people with diabetes because of poor prognosis in those with the infection. Initial reports were mainly on people with type 2 diabetes, although recent surveys have shown that individuals with type 1 diabetes are also at risk of severe COVID-19. The reason for worse prognosis in people with diabetes is likely to be multifactorial, thus reflecting the syndromic nature of diabetes. Age, sex, ethnicity, comorbidities such as hypertension and cardiovascular disease, obesity, and a pro-inflammatory and pro-coagulative state all probably contribute to the risk of worse outcomes. Glucose-lowering agents and anti-viral treatments can modulate the risk, but limitations to their use and potential interactions with COVID-19 treatments should be carefully assessed. Finally, severe acute respiratory syndrome coronavirus 2 infection itself might represent a worsening factor for people with diabetes, as it can precipitate acute metabolic complications through direct negative effects on β-cell function. These effects on β-cell function might also cause diabetic ketoacidosis in individuals with diabetes, hyperglycaemia at hospital admission in individuals with unknown history of diabetes, and potentially new-onset diabetes.\n \n', 'corpus_id': 220602322, 'score': 0}, {'doc_id': '221863058', 'title': 'Convalescent plasma for the treatment of patients with severe coronavirus disease 2019: A preliminary report', 'abstract': 'The role of convalescent plasma therapy for patients with coronavirus disease 2019 (COVID‐19) is unclear.', 'corpus_id': 221863058, 'score': 1}, {'doc_id': '226310893', 'title': 'Lack of efficacy of standard doses of ivermectin in severe COVID-19 patients', 'abstract': 'Ivermectin has recently shown efficacy against SARS-CoV-2 in-vitro. We retrospectively reviewed severe COVID-19 patients receiving standard doses of ivermectin and we compared clinical and microbiological outcomes with a similar group of patients not receiving ivermectin. No differences were found between groups. We recommend the evaluation of high-doses of ivermectin in randomized trials against SARS-CoV-2.', 'corpus_id': 226310893, 'score': 1}, {'doc_id': '222001378', 'title': 'IL-6 serum levels predict severity and response to tocilizumab in COVID-19: An observational study', 'abstract': '\n                  Background\n                  COVID-19 patients can develop a cytokine release syndrome that eventually leads to acute respiratory distress syndrome (ARDS) requiring invasive mechanical ventilation (IMV). Since interleukin-6 (IL-6) is a relevant cytokine in ARDS, the blockade of its receptor with Tocilizumab (TCZ) could reduce mortality and/or morbidity in severe COVID-19.\n               \n                  Objective\n                  To determine whether baseline IL-6 serum levels can predict the need for IMV and the response to TCZ.\n               \n                  Methods\n                  Retrospective observational study performed in hospitalized patients diagnosed of COVID-19. Clinical information and laboratory findings, including IL-6 levels, were collected approximately 3 and 9 days after admission to be matched with pre- and post-administration of TCZ. Multivariable logistic and linear regressions, and survival analysis were performed depending on outcomes: need for IMV, evolution of arterial oxygen tension/fraction of inspired oxygen ratio (PaO2/FiO2) or mortality.\n               \n                  Results\n                  One hundred and forty-six patients were studied, predominantly male (66%); median age was 63 years. Forty-four patients (30%) required IMV, and 58 patients (40%) received treatment with TCZ. IL-6 levels>30 pg/ml was the best predictor for IMV (OR:7.1; p<0.001). Early administration of TCZ was associated with improvement of oxygenation (PaO2/FiO2) in patients with high IL-6 (p=0.048). Patients with high IL-6 not treated with TCZ showed high mortality (HR: 4.6; p=0.003), as well as those with low IL-6 treated with TCZ (HR: 3.6; p=0.016). No relevant serious adverse events were observed in TCZ-treated patients.\n               \n                  Conclusion\n                  Baseline IL-6>30 pg/ml predicts IMV requirement in patients with COVID-19 and contributes to establish an adequate indication for TCZ administration.\n               ', 'corpus_id': 222001378, 'score': 1}, {'doc_id': '220526615', 'title': 'Clinical Manifestations and Outcomes of Critically Ill Children and Adolescents with Coronavirus Disease 2019 in New York City', 'abstract': '\n               \n                  Objectives\n                  To describe the clinical manifestations and outcomes of critically ill children with coronavirus disease-19 (COVID-19) in New York City.\n               \n               \n                  Study design\n                  Retrospective observational study of children 1 month to 21 years admitted March 14 to May 2, 2020 to 9 New York City pediatric intensive care units (PICUs) with SARS-CoV-2 infection.\n               \n               \n                  Results\n                  Of 70 children admitted to PICUs: median age 15 [IQR 9, 19] years; 61.4% male; 38.6% Hispanic; 32.9% Black; 74.3% with comorbidities. Fever (72.9%) and cough (71.4%) were the common presenting symptoms. Twelve patients (17%) met severe sepsis criteria; 14 (20%) required vasopressor support; 21 (30%) developed acute respiratory distress syndrome (ARDS); 9 (12.9%) met acute kidney injury criteria; 1 (1.4%) required renal replacement therapy, and 2 (2.8%) had cardiac arrest. For treatment, 27 (38.6%) patients received hydroxychloroquine; 13 (18.6%) remdesivir; 23 (32.9%) corticosteroids; 3 (4.3%) tocilizumab; 1 (1.4%) anakinra; no patient was given immunoglobulin or convalescent plasma. Forty-nine (70%) patients required respiratory support: 14 (20.0%) non-invasive mechanical ventilation, 20 (28.6%) invasive mechanical ventilation (IMV), 7 (10%) prone position, 2 (2.8%) inhaled nitric oxide, and 1 (1.4%) extracorporeal membrane oxygenation. Nine (45%) of the 20 patients requiring IMV were extubated by day 14 with median IMV duration of 218 [IQR 79, 310.4] hours. Presence of ARDS was significantly associated with duration of PICU and hospital stay, and lower probability of PICU and hospital discharge at hospital day 14 (P < .05 for all).\n               \n               \n                  Conclusions\n                  Critically ill children with COVID-19 predominantly are adolescents, have comorbidities, and require some form of respiratory support. The presence of ARDS is significantly associated with prolonged PICU and hospital stay.\n               \n            ', 'corpus_id': 220526615, 'score': 0}, {'doc_id': '225077669', 'title': 'Comparison of antiviral effect for mild-to-moderate COVID-19 cases between lopinavir/ritonavir versus hydroxychloroquine: A nationwide propensity score-matched cohort study', 'abstract': '\n                  Objectives\n                  We aimed to compare the antiviral effect of hydroxychloroquine (HCQ) and lopinavir/ritonavir (LPV/r) in patients with COVID-19.\n               \n                  Methods\n                  Nationwide retrospective case-control study was conducted to compare the effect of HCQ and LPV/r on viral shedding duration among patients with mild-to-moderate COVID-19 using the reimbursement data of National Health Insurance Service. After propensity score matching (PSM), multivariate analysis was conducted to determine statistically significant risk factors associated with prolonged viral shedding.\n               \n                  Results\n                  Overall, 4,197 patients with mild-to-moderate COVID-19 were included. Patients were categorized into three groups: LPV/r (n\u2009=\u20091,268), HCQ (n\u2009=\u2009801), and standard care without HCQ or LPV/r (controls, n\u2009=\u20092128). The median viral shedding duration was 23 (IQR 17–32), 23 (IQR 16–32), and 18 (IQR 12–25) days in the LPV/r, HCQ, and control groups, respectively. Even after PSM, the viral shedding duration was not significantly different between LPV/r and HCQ groups: 23 (IQR, 17–32) days versus 23 (IQR, 16–32) days. On multivariate analysis, old age, malignancy, steroid use, and concomitant pneumonia were statistically significant risk factors for prolonged viral shedding.\n               \n                  Conclusion\n                  The viral shedding duration was similar between HCQ and LPV/r treatment groups. There was no benefit in improving viral clearance compared to the control group.\n               ', 'corpus_id': 225077669, 'score': 1}, {'doc_id': '227253543', 'title': 'A systematic review of corticosteroid treatment for noncritically ill patients with COVID-19', 'abstract': 'The World Health Organization (WHO) has published guidance recommending systemic corticosteroids for the treatment of patients with severe or critical COVID-19 and no corticosteroids for those with nonsevere COVID-19. Although their recommendations for critical cases were based on the results from seven randomized controlled trials (RCTs), those for noncritical cases were based on the results from only one RCT, the Randomized Evaluation of COVID-19 Therapy (RECOVERY) trial. In search of additional evidence of corticosteroids’ effect on COVID-19, we systematically reviewed controlled observational studies, besides RCTs, that assessed the impact of corticosteroid treatment on any type of mortality and/or other outcomes in noncritical patients. Of the 4037 titles and abstracts screened, we ultimately included the RECOVERY trial and five controlled observational studies using propensity score matching, (accessed on September 8, 2020). Two of the controlled observational studies assessed the association between corticosteroid treatment and in-hospital mortality, without finding statistical significance. Four of the controlled observational studies assessed corticosteroids’ effect on other outcomes, demonstrating that they were associated with reduced risk of intubation in patients requiring oxygen and with longer hospitalization and viral shedding in mild or moderate cases. These results support the WHO recommendations not to use corticosteroids for nonsevere COVID-19.', 'corpus_id': 227253543, 'score': 1}, {'doc_id': '218571559', 'title': 'COVID-19: Clinical course and outcomes of 36 maintenance hemodialysis patients from a single center in Spain.', 'abstract': '\n \n SARS-CoV-2-pneumonia emerged in Wuhan, China in December 2019. Unfortunately, there is lack of evidence about the optimal management of novel coronavirus disease 2019 (COVID-19), even less in patients on maintenance hemodialysis (MHD) therapy than in the general population. In this retrospective observational single-center study we analyzed the clinical course and outcomes of all MHD patients hospitalized with COVID-19 from March 12th to April 10th, 2020 as confirmed by real time polymerase chain reaction. Baseline features, clinical course, laboratory data, and different therapies were compared between survivors and non-survivors to identify risk factors associated with mortality. Among the 36 patients, 11 (30.5%) died and 7 could be discharged within the observation period. Clinical and radiological evolution during the first week of admission were predictive of mortality. Among the 36 patients, 18 had worsening of their clinical status, as defined by severe hypoxia with oxygen therapy requirements greater than 4 Liters/minute and radiological worsening. Significantly 11 out of those 18 patients (61.1%) died. None of the classical cardiovascular risk factors in the general population were associated with higher mortality. However, a longer time on hemodialysis (hazard ratio 1.008(95% confidence interval 1.001-1.015) per year), increased LDH levels (1.006(1.001-1.011), and lower lymphocyte count (0.996 (0.992-1.000) one week after clinical onset were all significantly associated with higher mortality risk. Thus, the mortality among hospitalized hemodialysis patients diagnosed with COVID-19 is high. Lymphopenia and increased LDH levels were associated with poor prognosis.\n \n', 'corpus_id': 218571559, 'score': 0}]
74	{'doc_id': '221293157', 'title': 'An anisotropic viscoplasticity model for shale based on layered microstructure homogenization', 'abstract': 'Viscoplastic deformation of shale is frequently observed in many subsurface applications. Many studies have suggested that this viscoplastic behavior is anisotropic---specifically, transversely isotropic---and closely linked to the layered composite structure at the microscale. In this work, we develop a two-scale constitutive model for shale in which anisotropic viscoplastic behavior naturally emerges from semi-analytical homogenization of a bi-layer microstructure. The microstructure is modeled as a composite of soft layers, representing a ductile matrix formed by clay and organics, and hard layers, corresponding to a brittle matrix composed of stiff minerals. This layered microstructure renders the macroscopic behavior anisotropic, even when the individual layers are modeled with isotropic constitutive laws. Using a common correlation between clay and organic content and magnitude of creep, we apply a viscoplastic Modified Cam-Clay plasticity model to the soft layers, while treating the hard layers as a linear elastic material to minimize the number of calibration parameters. We then describe the implementation of the proposed model in a standard material update subroutine. The model is validated with laboratory creep data on samples from three gas shale formations. We also demonstrate the computational behavior of the proposed model through simulation of time-dependent borehole closure in a shale formation with different bedding plane directions.', 'corpus_id': 221293157}	16574	[{'doc_id': '235291263', 'title': 'Numerical Simulation of Soft Rock Creep Behavior under Coupling Influence of Temperature and Stress', 'abstract': 'In order to investigate the deformation law of deep surrounding rock under the coupling influence of temperature, cracks and stress. We take the temperature, initial damage and confining pressure into consideration to simulate the deformation of deep surrounding rocks after mining. The numerical simulation of the creep characteristics of deep tunnel can be sum up as follows: a) the increase of temperature makes the strength decrease and the increase deformation of rock mass; b) With the increase of confining pressure, the anti-deformation ability of rock mass is enhanced and the failure mode of rock mass changes from brittle failure to plastic failure; c) The existence of cracks reduces the strength of rocks and intensifies the deformation of rocks; d) the stress in the tunnel mainly concentrates on the tip of the initial cracks and the existence of defects aggravates the deformation inside the tunnel.', 'corpus_id': 235291263, 'score': 1}, {'doc_id': '236379766', 'title': 'Numerical simulation and experimental verification studies on a unified strength theory-based elastoplastic damage constitutive model of shale', 'abstract': 'Abstract The purpose of this study is to establish an elastic–plastic damage constitutive model of shale and simulate the elastic–plastic damage characteristics of shale under stress. Based on the unified strength theory and the mechanics of shale rock samples characterized in laboratory tests, a new elastic–plastic damage constitutive model for shale is established by introducing compression factors and damage variables. The main considerations include the compressibility of primary fractures and pores in the shale core, and the formation of secondary cracks in the rock matrix under stress. A fully implicit backward Euler regression mapping algorithm has been used to solve the model, and the numerical simulation results are in good agreement with the experimental results. The results show that the model established in this paper can accurately simulate the elastic–plastic damage characteristics of shale under stress, and that it provides a new numerical simulation method for describing the elastic–plastic damage in shale.', 'corpus_id': 236379766, 'score': 1}, {'doc_id': '235400757', 'title': 'Application of wellbore strengthening drilling fluid technology in Lingshui gas field', 'abstract': 'Fractured reservoir is developed in Lingshui gas field and the problem of lost circulation is prominent. To improve the pressure bearing capacity of fractured formation, the wellbore strengthening technology was applied in this block. The fracture width of Lingshui gas field was predicted with finite element method, which considered the effect of wellbore pressure, in-situ stress, seepage fluid and pore pressure. The simulation results showed that the fracture width of Lingshui block ranged from 0 to 464 μm. Based on the prediction results of fracture width, the particle size distribution of lost circulation material (LCM) was selected with dynamic fracture width testing device. The results indicated that the D50 rule has the best sealing efficiency. The optimized LCM formula had better compatibility performance with water-based drilling fluid. The sealing results illustrated that invasion depth of seepage loss was smaller than 0.3 cm and the pressure bearing capacity of fracture reached to 17 MPa.', 'corpus_id': 235400757, 'score': 0}, {'doc_id': '233432978', 'title': 'Permeability Enhancement Properties of High-Pressure Abrasive Water Jet Flushing and Its Application in a Soft Coal Seam', 'abstract': 'High-pressure abrasive water jet flushing (HPAWJF) is an effective method used to improve coal seam permeability. In this study, based on the theories of gas flow and coal deformation, a coupled gas-rock model is established to investigate realistic failure processes by introducing equations for the evolution of mesoscopic element damage along with coal mass deformation. Numerical simulation of the failure and pressure relief processes is carried out under different coal seam permeability and flushing length conditions. Distributions of the seepage and gas pressure fields of the realistic failure process are analyzed. The effects of flushing permeability enhancement in a soft coal seam on the gas drainage from boreholes are revealed by conducting a field experiment. Conclusions can be extracted that the gas pressure of the slotted soft coal seam is reduced and that the gas drainage volume is three times higher than that of a conventional borehole. Field tests demonstrate that the gas drainage effect of the soft coal seam is significantly improved and that tunneling speed is nearly doubled. The results obtained from this study can provide guidance to gas drainage in soft coal seams regarding the theory and practice application of the HPAWJF method.', 'corpus_id': 233432978, 'score': 0}, {'doc_id': '236004450', 'title': 'Rock Breaking Characteristics of the Self-rotating Multi-orifice Nozzle for Sandstone Radial Jet Drilling', 'abstract': 'Radial jet drilling technology (RJD) is an economical and efficient technology for the development of unconventional oil, gas, and geothermal resources. The self-rotated multi-orifice nozzle, which is the key for the hole-forming, is introduced and tested here. Its sandstone-breaking characteristics are researched by experiment, the influence of working conditions, lithology, axis length, jet pressure (15\u2009~\u200940 MPa), and standoff distance (0\u2009~\u200910 mm) is also studied. Moreover, optimal working parameters for continuous drilling are proposed. Results show that the self-rotating multi-orifice nozzle can effectively drill a large circular hole whose diameter is about 30 mm under the condition of fixed standoff distance or continuous feeding. The submergence conditions are helpful to improve rock breaking efficiency. The short-axis nozzle has good hydraulic performance and rock breaking ability. Uniaxial compressive strength affects rock breaking diameter, while permeability affects rock breaking depth and volume. The rock breaking diameter, depth and volume of sandstone both increase with the jet pressure. The rock breaking diameter increases with the standoff distance, while the rock breaking volume and depth are opposite. The diameter of the rock breaking hole decreased with the augment of the feeding speed. For the sandstone here, the jet pressure is recommended as 35 MPa and the feeding speed 0.03 m/min (1.8 m/h); laterals with a diameter of about 30 mm can be formed. The research results are expected to promote the development of RJD technology.', 'corpus_id': 236004450, 'score': 0}, {'doc_id': '235466548', 'title': 'Effect of Borehole Positions and Depth on Pressure Relief of Cavern Surrounding Rock Mass', 'abstract': 'As the development scale and depth increasing, the frequency and intensity of rock burst and other surrounding rock dynamic disasters are also increasing, which seriously affects the safety construction and operation of cavern engineering. At present, one of the most commonly used methods to control rock burst risk is use borehole pressure relief technology. In this paper, the pressure relief effect of surrounding rock drilling with different borehole positions and borehole depth. Firstly, the stress evolution process of surrounding rock after excavation is analyzed. Then, the influence of drilling position and drilling depth on the stress evolution process of surrounding rock is explored. The research results show that drilling in the surrounding rock will reduce the stress near the borehole, while the stress of surrounding rock in the depth direction of the borehole will increase. With the increase of drilling position H, the stress in the drilling area of surrounding rock decreases gradually. On the contrary, the stress concentration in other areas near the borehole gradually increases. With the increase of drilling depth, the stress of surrounding rock of the cavern continuously transfers to the interior, and when the drilling depth is greater than 1500 mm, the pressure relief effect does not continue to improve. After drilling, the damage of surrounding rock transfers to the interior of surrounding rock, and the damage degree near the tunnel wall is less than that without drilling. Different borehole location will lead to different failure characteristics of borehole. With the increase of drilling depth, the failure depth of surrounding rock will transfer to the interior.', 'corpus_id': 235466548, 'score': 0}, {'doc_id': '233190306', 'title': 'Experimental Study on Briquette Coal Sample Mechanics and Acoustic Emission Characteristics under Different Binder Ratios', 'abstract': 'In order to find briquette coal with mechanical characteristics close to those of raw coal samples, we conducted triaxial compression experiments on raw coal and briquette coal samples with different proportions of cement contents. The mechanical characteristics and acoustic emission (AE) characteristics of the coal samples in the triaxial compression process were analyzed in detail. The test results show that the evolution of deformation and strength characteristics of the briquette coal and raw coal samples follow certain common laws. The confining pressure can improve the mechanical properties of both types of coal samples: as the confining pressure continues to increases according to the set values, the elastic modulus, peak strain, and peak strength of both samples show an increasing linear trend. When the value of the confining pressure setting was the same, the compressive strength of the raw coal samples exceeded that of briquette ones, but the deformation and shape variations of the latter exceeded those of the former. When performing triaxial compression experiments on both kinds of coal samples, the AE amplitude and counts showed a close correlation with the stress evolution curves. When the confining pressure was set to 5 MPa, 20% cement content briquette coal was seen to be the closest to raw coal samples regarding mechanical and AE characteristics.', 'corpus_id': 233190306, 'score': 0}, {'doc_id': '235262994', 'title': 'Log-based Geo-mechanical Characterization for Openhole Stability Analysis of Horizontal Wellbores', 'abstract': 'This paper presents the methodology of a rock mechanics characterization from logging data in order to examine the stability of openhole horizontal wells. This methodology investigates the borehole stability under multiple production scenarios during the projected pressure decline of the reservoir. The stability study includes the in-situ stress tensor description of the field and the mechanical property characterization of the formations based on a constitutive model describing the microscopic processes occurring in a rock sample during tri-axial loading. The micromechanical constitute model derives continuous profiles of log-based static mechanical properties and strengths at different confining conditions. An analytical program for boreholewall failure prediction and in-situ stress estimation predicted the shear failure zone around the borehole wall under progressive production scenarios. The risk of borehole collapse increases with a growing shear failure zone around the borehole wall. In order to prevent borehole collapse, the maximum allowable drawdown at the borehole wall interface was computed under progressive depletion scenarios. In order to address the effect of grain-cement disintegration after acid stimulations, several compressive strength degradation scenarios were also considered. The results of this type of analysis provide assistance in optimizing the horizontal well completion i.e. open-hole, cased and cemented, slotted liners, or expandable screens in carbonate reservoirs.', 'corpus_id': 235262994, 'score': 1}, {'doc_id': '235531564', 'title': 'Rock mechanics and wellbore stability of deep shale during drilling and completion processes', 'abstract': 'Abstract The seepage coupling effect between the borehole rock mass and borehole fluid and the effective stress field distribution around the wellbore are obviously different under different working conditions of drilling, fracturing and completion, which affect the stability of horizontal borehole of shale gas reservoir. The physical, chemical, and mechanical properties of the shale in Longmaxi formations in a block of China immersed with different working fluids were tested experimentally. Combined with the experimental data, a theoretical model was established to evaluate and analyze the wellbore stability of Longmaxi formations shale under different working conditions of drilling, fracturing and completion. The results show that the shale of the Longmaxi Formation is a typical hard and brittle shale, the bedrock is dense, with high mechanical strength and weak hydration and expansion ability. The immersion effect of different working fluids has little influence on the expansion and mechanical properties of shale bedrock. However, the mechanical strength of shale with relatively developed bedding fractures is low, and the immersion effect of different drilling fluid further weakens the mechanical strength, resulting in obvious anisotropy of mechanical strength of underground rock. The coupling effect of seepage between wellbore and formation has obvious influence on the dynamic distribution of borehole pore pressure and wellbore stability under different working conditions. The effective fluid column pressure at the bottom of the hole is the highest during the fracturing process, which leads to the increase of pore pressure near the wellbore, and then to the increase of collapse pressure around the borehole wall. The equivalent density of collapse pressure rises to the maximum of 1.93\xa0g/cm3, and the wellbore stability is the worst in the fracturing process. The wellbore stability is secondary in the drilling process, and the wellbore stability is good when drilling along the direction of the maximum horizontal principal stress, and the equivalent density of collapse pressure is 1.69\xa0g/cm3. During the completion process, the formation fluid flows into the wellbore, the pore pressure in the borehole is relieved, and the equivalent density of the collapse pressure is reduced to approximately 1.35\xa0g/cm3, the wellbore stability is the best and open hole completion can be attempted in the horizontal well section. The results provide a certain theoretical reference for the selection of the drilling and completion schemes in shale formation.', 'corpus_id': 235531564, 'score': 1}, {'doc_id': '235282080', 'title': 'Exploration and practice of stress-strain evolution law of hydraulic fracturing in coal mine', 'abstract': 'In order to explore the evolution law of stress and strain in the process of hydraulic fracturing, a hollow inclusions stress-strain gauge was used for monitoring. The results show that the influence range of high-pressure water spreads to the monitoring borehole at 47m after 2 hours of hydraulic fracturing, and its influence on the stress-strain monitoring point reaches the maximum, and the strain tends to be stable in the later stage. After 6 days of hydraulic fracturing, the strain returns to a stable state without any change, but it is still larger than the state before hydraulic fracturing. During hydraulic fracturing, the increment of principal stress increases, but the amplitude is different. Hydraulic fracturing mainly affects the increment of vertical stress, which is related to the stress-strain monitoring point and the space location of the hydraulic fracturing borehole. After hydraulic fracturing, the stress increment decreases gradually and tends to return to the original stable state due to the gradual pressure relief of fracturing fluid, but it is still larger than the state without hydraulic fracturing. Hydraulic fracturing has little effect on the azimuth and dip angle of coal and rock mass. The stress-strain law can provide reference for the exploration of the coal seam permeability increasing mechanism of hydraulic fracturing.', 'corpus_id': 235282080, 'score': 1}]
75	{'doc_id': '212718038', 'title': 'Constraining SMEFT operators with associated $h\\gamma$ production in Weak Boson Fusion', 'abstract': 'We consider the associated production of a Higgs boson and a photon in weak boson fusion in the Standard Model (SM) and the Standard Model Effective Theory (SMEFT), with the Higgs boson decaying to a pair of bottom quarks. Analysing events in a cut-based analysis and with multivariate techniques we determine the sensitivity of this process to the bottom-Yukawa coupling in the SM and to possible CP-violation mediated by dimension-6 operators in the SMEFT.', 'corpus_id': 212718038}	1866	"[{'doc_id': '214775593', 'title': 'Aggregate and Firm-Level Stock Returns During Pandemics, in Real Time', 'abstract': ""We show that unexpected changes in the trajectory of COVID-19 infections predict US stock returns, in real time. Parameter estimates indicate that an unanticipated doubling (halving) of projected infections forecasts next-day decreases (increases) in aggregate US market value of 4 to 11 percent, indicating that equity markets may begin to rebound even as infections continue to rise, if the trajectory of the disease becomes less severe than initially anticipated. Using the same variation in unanticipated projected cases, we find that COVID-19-related losses in market value at the firm level rise with capital intensity and leverage, and are deeper in industries more conducive to disease transmission. These relationships provide important insight into current record job losses. Measuring US states' drops in market value as the employment weighted average declines of the industries they produce, we find that states with milder drops in market value exhibit larger initial jobless claims per worker. This initially counter-intuitive result suggests that investors value the relative ease with which labor versus capital costs can be shed as revenues decline."", 'corpus_id': 214775593, 'score': 0}, {'doc_id': '214693393', 'title': 'Improving $t \\bar{t}$ reconstruction in the dilepton channel at future lepton colliders', 'abstract': 'A future lepton collider, such as the proposed CLIC or ILC, would allow to study top quark properties with unprecedented precision. In this paper, we present a method to reconstruct the $t \\bar{t}$ decay in the dilepton channel at future $e^+e^-$ colliders. We derive a simple, closed analytical expression for the neutrino four-momenta as a function of the $W$ boson mass and develop a maximization procedure to find the optimal solution for the reconstruction of the full $t \\bar{t}$ event. We show that our method is able to reconstruct neutrino four-momenta with an error of less than $2 \\, \\%$ in $60 \\, \\%$ of the times. Finally, we test the performance of this reconstruction method in the calculation of the helicity fractions of the $W$ boson. A precise measurement of these observables could be used to probe new physics effects in the $Wtb$ vertex. We find that, from a large $t \\bar{t}$ sample, our reconstruction method allows to calculate these observables with an accuracy better than $1 \\, \\%$.', 'corpus_id': 214693393, 'score': 1}, {'doc_id': '214792914', 'title': 'Me(a)t the Future', 'abstract': 'When I was given the opportunity to take the podium at last year’s WTO-public forum in Geneva, I raised the issue of crisis management and the necessity of coming together, more efficiently and much faster than the usual worldwide procedure. I repeated it in a Civil Dialogue Group in Brussels this January, although I have to admit that at the time, I was actually thinking about a solution for the African Swine Fever. Today, the Coronavirus puts the world in a completely new and serious situation that hardly anyone could have predicted.', 'corpus_id': 214792914, 'score': 0}, {'doc_id': '214774373', 'title': 'Animals Keep Viruses in Balance', 'abstract': 'The human body is a constant flux of thousands of chemical/biological interactions and processes connecting molecules, cells, organs, and fluids, throughout the brain, body, and nervous system. Up until recently it was thought that all these interactions operated in a linear sequence, passing on information much like a runner passing the baton to the next runner. However, the latest findings in quantum biology and biophysics have discovered that there is in fact a tremendous degree of coherence within all living systems.', 'corpus_id': 214774373, 'score': 1}, {'doc_id': '212675718', 'title': 'Radiative generation of neutrino masses in a 3-3-1 type model', 'abstract': 'A new model for tiny neutrino masses is proposed in the gauge theory of $SU(3)_C \\otimes SU(3)_L \\otimes U(1)_X$, where neutrino masses are generated via the quantum effect of new particles. In this model, the fermion content is taken to be minimal to realize the gauge anomaly cancellation, while the scalar sector is extended from the minimal 3-3-1 model to have an additional $SU(3)_L$ triplet field. After $SU(3)_L\\otimes U(1)_X$ is broken into $SU(2)_L\\otimes U(1)_Y$, the ""Zee model"" like diagrams are naturally induced, which contain sufficient lepton flavor violating interactions to reproduce current neutrino oscillation data. Furthermore, the remnant $Z_2$ symmetry appears after the electroweak symmetry breaking, which guarantees the stability of dark matter. It is confirmed that this model can satisfy current dark matter data. As an important prediction to test this model, productions and decays of doubly-charged scalar bosons at collider experiments are discussed in successful benchmark scenarios.', 'corpus_id': 212675718, 'score': 1}, {'doc_id': '214791402', 'title': 'Signal amplification and quantification on lateral flow assays by laser excitation of plasmonic nanomaterials', 'abstract': 'Lateral flow assay (LFA) has become one of the most widely used point-of-care diagnostic methods due to its simplicity and low cost. While easy to use, LFA suffers from its low sensitivity and poor quantification, which largely limits its applications for early disease diagnosis and requires further testing to eliminate false-negative results. Over the past decade, signal enhancement strategies that took advantage of the laser excitation of plasmonic nanomaterials have pushed down the detection limit and enabled quantification of analytes. Significantly, these methods amplify the signal based on the current LFA design without modification. This review highlights these strategies of signal enhancement for LFA including surface enhanced Raman scattering (SERS), photothermal and photoacoustic methods. Perspectives on the rational design of the reader systems are provided. Future translation of the research toward clinical applications is also discussed.', 'corpus_id': 214791402, 'score': 1}, {'doc_id': '214693340', 'title': 'Measuring the trilinear Higgs boson self-coupling at the 100\xa0TeV hadron collider via multivariate analysis', 'abstract': 'We perform a multivariate analysis of Higgs-pair production via the decay channel $HH \\to b\\bar b \\gamma\\gamma$ at the future 100 TeV $pp$ collider to determine the trilinear Higgs self--coupling (THSC) $\\lambda_{3H}$, which takes the value of 1 in the standard model. We consider all known background processes. For the signal we adopt the most recent event generator of {\\tt POWHEG-BOX-V2} to exploit the NLO distributions for Toolkit for Multivariate Data Analysis (TMVA). Through the technique of Boosted Decision Tree (BDT) analysis trained for $\\lambda_{3H}=1$, compared to the the conventional cut-and-count approach, the signal-to-background ratio improves tremendously from about $1/10$ to $1$ and the significance can reach up to $20.5$ with a luminosity of 3 ab$^{-1}$ without including systematic uncertainties. In addition, by implementing a likelihood fitting of the signal-plus-background $M_{\\gamma\\gamma b b}$ distribution with optimized bin sizes, it is possible to determine the THSC with the precision of 7.5\\% at 68\\% CL even at the early stage of 100 TeV hadron collider with 3 ab$^{-1}$.', 'corpus_id': 214693340, 'score': 1}, {'doc_id': '214779060', 'title': 'ESTIMATING THE PREVALENCE: PROPERTIES OF THE ESTIMATOR REGARDING SPECIFICITY AND SENSITIVITYOF THE UNDERLYING TEST', 'abstract': 'We provide a calculation tool to assess the properties of a maximumlikelihood (ML) estimator that extrapolates the true prevalence of an infectious disease from a random sample. The tools allow the researcher to correct for the specificity and sensitivity of the underlying medical test, calculate the standard deviation of the estimator and to plan the needed sample size. This document explains the underlying methods of the calculation tools and provides instructions for their proper use. We apply an adaption of the epidemiological SEIR-model to show that ML-estimators from random sampling tests provide a more realistic rate of infection than common approaches. ∗The authors greatly appreciate the research assistance of Falk Wendorff †Kiel Institute for the World Economy ‡Kiel Institute for the World Economy, Kiel University 1 Aim of the calculators During pandemic outbreaks of infectious diseases policy makers are forced to take actions against the spread quickly, often at the expense of economic activity. A recent study by Burns et al. (2006) estimates that 60% of economic damages incurred during a pandemic can be attributed to demand shocks, i.e. the indirect costs of an outbreak. Factoring in the interruption of supply chains and detrimental uncertainty likely increases the economic costs significantly. While human health must be protected, governments typically have limited information on the actual spread of the disease. Indeed, the true rate of infection in the population is rarely known. Random testing can be a remedy to achieve the needed information of the prevalence. However, given that specificity and sensitivity of a test can deviate from one, the prevalence has to be estimated from test results e.g. via a Maximum Likelihood estimation. We provide the ready-to-use tools for such a Maximum Likelihood estimation, which calculates the standard deviation of the estimator for given sensitivity, specificity, sample size and expected prevalence. Vice versa the needed sample size can be retrieved for a standard deviation or precision that shall be achieved. While the tools presented in this paper are applicable to any infectious disease, we provide examples from the COVID-19 pandemic. Indeed, the outbreak of Sars-CoV-2 is a suitable illustration for the need of statistical tests: At the moment, mainly patients who are at high risk of infection (e.g. because of contact with an infected individual) are tested for the presence of the pathogen by use of a rRT-PCR (reverse transcription polymerase chain reaction) test. This approach swiftly diagnoses COVID-19 and helps to trace the chain of infection. However, the virus has a high level of contagion, an incubation period of approximately five days (Lauer et al., 2020) and results only in minor symptoms for many people. This suggests that the true rate of', 'corpus_id': 214779060, 'score': 0}, {'doc_id': '214777187', 'title': 'Inhibitor Prevents Viral Infection', 'abstract': 'The human body is a constant flux of thousands of chemical/biological interactions and processes connecting molecules, cells, organs, and fluids, throughout the brain, body, and nervous system. Up until recently it was thought that all these interactions operated in a linear sequence, passing on information much like a runner passing the baton to the next runner. However, the latest findings in quantum biology and biophysics have discovered that there is in fact a tremendous degree of coherence within all living systems.', 'corpus_id': 214777187, 'score': 0}, {'doc_id': '214780318', 'title': 'Antibiotic Resistance Gene', 'abstract': 'A completely new resistance gene, which is likely to counteract the newest aminoglycoside-drug plazomycin, was recently discovered by scientists in Gothenburg, Sweden. [15] Now investigators at Massachusetts General Hospital (MGH) have modified the system to be nearly free of this requirement, making it possible to potentially target any location across the entire human genome. [14] An ancient group of microbes that contains some of the smallest life forms on Earth also has the smallest CRISPR gene-editing machinery discovered to date. [13] ETH scientists have been able to prove that a protein structure widespread in nature – the amyloid – is theoretically capable of multiplying itself. [12]', 'corpus_id': 214780318, 'score': 0}]"
76	{'doc_id': '58106567', 'title': 'Extensive Analysis on Generation and Consensus Mechanisms of Clustering Ensemble', 'abstract': 'Data analysis plays a prominent role in interpreting various phenomena. Data mining is the process to hypothesize useful knowledge from the extensive data. Based upon the classical statistical prototypes the data can be exploited beyond the storage and management of the data. Cluster analysis a primary investigation with little or no prior knowledge, consists of research and development across a wide variety of communities. Cluster ensembles are melange of individual solutions obtained from different clusterings to produce final quality clustering which is required in wider applications. The method arises in the perspective of increasing robustness, scalability and accuracy. This paper gives a brief overview of the generation methods and consensus functions included in cluster ensemble. The survey is to analyze the various techniques and cluster ensemble methods.', 'corpus_id': 58106567}	8619	"[{'doc_id': '173990922', 'title': 'Consensus Clustering: An Embedding Perspective, Extension and Beyond', 'abstract': 'Consensus clustering fuses diverse basic partitions (i.e., clustering results obtained from conventional clustering methods) into an integrated one, which has attracted increasing attention in both academic and industrial areas due to its robust and effective performance. Tremendous research efforts have been made to thrive this domain in terms of algorithms and applications. Although there are some survey papers to summarize the existing literature, they neglect to explore the underlying connection among different categories. Differently, in this paper we aim to provide an embedding prospective to illustrate the consensus mechanism, which transfers categorical basic partitions to other representations (e.g., binary coding, spectral embedding, etc) for the clustering purpose. To this end, we not only unify two major categories of consensus clustering, but also build an intuitive connection between consensus clustering and graph embedding. Moreover, we elaborate several extensions of classical consensus clustering from different settings and problems. Beyond this, we demonstrate how to leverage consensus clustering to address other tasks, such as constrained clustering, domain adaptation, feature selection, and outlier detection. Finally, we conclude this survey with future work in terms of interpretability, learnability and theoretical analysis.', 'corpus_id': 173990922, 'score': 1}, {'doc_id': '221246167', 'title': 'ConiVAT: Cluster Tendency Assessment and Clustering with Partial Background Knowledge', 'abstract': 'The VAT method is a visual technique for determining the potential cluster structure and the possible number of clusters in numerical data. Its improved version, iVAT, uses a path-based distance transform to improve the effectiveness of VAT for ""tough"" cases. Both VAT and iVAT have also been used in conjunction with a single-linkage(SL) hierarchical clustering algorithm. However, they are sensitive to noise and bridge points between clusters in the dataset, and consequently, the corresponding VAT/iVAT images are often in-conclusive for such cases. In this paper, we propose a constraint-based version of iVAT, which we call ConiVAT, that makes use of background knowledge in the form of constraints, to improve VAT/iVAT for challenging and complex datasets. ConiVAT uses the input constraints to learn the underlying similarity metric and builds a minimum transitive dissimilarity matrix, before applying VAT to it. We demonstrate ConiVAT approach to visual assessment and single linkage clustering on nine datasets to show that, it improves the quality of iVAT images for complex datasets, and it also overcomes the limitation of SL clustering with VAT/iVAT due to ""noisy"" bridges between clusters. Extensive experiment results on nine datasets suggest that ConiVAT outperforms the other three semi-supervised clustering algorithms in terms of improved clustering accuracy.', 'corpus_id': 221246167, 'score': 0}, {'doc_id': '220496329', 'title': 'Unsupervised feature selection for tumor profiles using autoencoders and kernel methods', 'abstract': 'Molecular data from tumor profiles is high dimensional. Tumor profiles can be characterized by tens of thousands of gene expression features. Due to the size of the gene expression feature set machine learning methods are exposed to noisy variables and complexity. Tumor types present heterogeneity and can be subdivided in tumor subtypes. In many cases tumor data does not include tumor subtype labeling thus unsupervised learning methods are necessary for tumor subtype discovery. This work aims to learn meaningful and low dimensional representations of tumor samples and find tumor subtype clusters while keeping biological signatures without using tumor labels. By using Autoencoders a low dimensional and denoised latent space is learned as a target representation to guide a Multiple Kernel Learning model that selects a subset of genes. By using the selected genes a clustering method is used to group samples. In order to evaluate the performance of the proposed unsupervised feature selection method the obtained features and clusters are analyzed by clinical significance. The proposed method has been applied on three tumor datasets which are Brain, Renal and Lung, each one composed by two tumor subtypes. The results obtained by the proposed method reveal lower redundancy in the selected features and each obtained cluster is significantly enriched by just one tumor subtype when compared with benchmark unsupervised feature selection methods. The proposed method named Latent Kernel Feature Selection (LKFS) is an unsupervised approach for gene selection in tumor gene expression profiles.', 'corpus_id': 220496329, 'score': 0}, {'doc_id': '15373514', 'title': 'Comparative study of matrix refinement approaches for ensemble clustering', 'abstract': 'Cluster ensembles or consensus clusterings have been shown to be better than any standard clustering algorithm at improving accuracy and robustness across various sets of data. This meta-learning formalism also helps users to overcome the dilemma of selecting an appropriate technique and the parameters for that technique. Since founded, different research areas have emerged with the common purpose of enhancing the effectiveness and applicability of cluster ensembles. These include the selection of ensemble members, the imputation of missing values, and the summarization of ensemble members. In particular, this paper is set to provide the review of different matrix refinement approaches that have been recently proposed in the literature for summarizing information of multiple clusterings. With various benchmark datasets and quality measures, the comparative study of these novel techniques is carried out to provide empirical findings from which a practical guideline can be drawn.', 'corpus_id': 15373514, 'score': 1}, {'doc_id': '212448441', 'title': 'A Survey : Clustering Ensemble Techniques with Consensus Function', 'abstract': 'The clustering ensembles contains multiple partitions are divided by different clustering algorithms into a single clustering solutions. Clustering ensembles used for improving robustness, stability, and accuracy of unsupervised classification solutions. The major problem of clustering ensemble is the consensus function. Consensus functions in clustering ensembles including hyperactive graph partition, mutual information, co-association based functions, voting approach and finite machine. The characteristics of clustering ensembles algorithm are computational complexity, robustness, simplicity and accuracy on different datasets in previous techniques.', 'corpus_id': 212448441, 'score': 1}, {'doc_id': '58626921', 'title': 'A Review on Consensus Clustering Methods', 'abstract': 'Unsupervised learning/clustering is one of the most common, yet computationally intense, data analysis problems in data mining. The plethora of clustering algorithms and performance measures makes the choice of optimal clustering algorithm a challenging task. In order to overcome this shortcoming consensus learning methods have been proposed in the literature. These methods try to optimally combine independently obtained clusterings into a single more robust clustering of improved quality. In this chapter we provide a review of unsupervised consensus learning techniques based on their underlying theoretical principles. We present the exact, approximation, and heuristic approaches, the relation of consensus clustering with other well-studied problems, and discuss relevant applications.', 'corpus_id': 58626921, 'score': 1}, {'doc_id': '221447840', 'title': 'reval: a Python package to determine the best number of clusters with stability-based relative clustering validation', 'abstract': 'Determining the number of clusters that best partitions a dataset can be a challenging task because of 1) the lack of a priori information within an unsupervised learning framework; and 2) the absence of a unique clustering validation approach to evaluate clustering solutions. Here we present reval: a Python package that leverages stability-based relative clustering validation methods to determine best clustering solutions. Statistical software, both in R and Python, usually rely on internal validation metrics, such as the silhouette index, to select the number of clusters that best fits the data. Meanwhile, open-source software solutions that easily implement relative clustering techniques are lacking. Internal validation methods exploit characteristics of the data itself to produce a result, whereas relative approaches attempt to leverage the unknown underlying distribution of data points looking for a replicable and generalizable clustering solution. The implementation of relative validation solutions can further the theory of clustering by enriching the already available methods that can be used to investigate clustering results in different situations and for different data distributions. This work aims at contributing to this effort by developing a stability-based method that selects the best clustering solution as the one that replicates, via supervised learning, on unseen subsets of data. The package works with multiple clustering and classification algorithms, hence allowing further assessment of the stability of different clustering mechanisms.', 'corpus_id': 221447840, 'score': 0}, {'doc_id': '220546282', 'title': 'Evaluating and Validating Cluster Results', 'abstract': 'Clustering is the technique to partition data according to their characteristics. Data that are similar in nature belong to the same cluster [1]. There are two types of evaluation methods to evaluate clustering quality. One is an external evaluation where the truth labels in the data sets are known in advance and the other is internal evaluation in which the evaluation is done with data set itself without true labels. In this paper, both external evaluation and internal evaluation are performed on the cluster results of the IRIS dataset. In the case of external evaluation Homogeneity, Correctness and V-measure scores are calculated for the dataset. For internal performance measures, the Silhouette Index and Sum of Square Errors are used. These internal performance measures along with the dendrogram (graphical tool from hierarchical Clustering) are used first to validate the number of clusters. Finally, as a statistical tool, we used the frequency distribution method to compare and provide a visual representation of the distribution of observations within a clustering result and the original data.', 'corpus_id': 220546282, 'score': 0}, {'doc_id': '16218273', 'title': 'EXCLUVIS: A MATLAB GUI Software for Comparative Study of Clustering and Visualization of Gene Expression Data', 'abstract': 'Clustering is a popular data mining technique that aims to partition an input space into multiple homogeneous regions. There exist several clustering algorithms in the literature. The performance of a clustering algorithm depends on its input parameters which can substantially affect the behavior of the algorithm. Cluster validity indices determine the partitioning that best fits the underlying data. In bioinformatics, microarray gene expression technology has made it possible to measure the gene expression levels of thousands of genes simultaneously. Many genomic studies, which aim to analyze the functions of some genes, highly rely on some clustering technique for grouping similarly expressed genes in one cluster or partitioning tissue samples based on similar expression values of genes. In this work, an application package called EXCLUVIS (gene EXpression data CLUstering and VISualization) has been developed using MATLAB Graphical User Interface (GUI) environment for analyzing the performances of different clustering algorithms on gene expression datasets. In this application package, the user needs to select a number of parameters such as internal validity indices, external validity indices and number of clusters from the active windows for evaluating the performance of the clustering algorithms. EXCLUVIS compares the performances of K-means, fuzzy C-means, hierarchical clustering and multiobjective evolutionary clustering algorithms. Heatmap and cluster profile plots are used for visualizing the results. EXCLUVIS allows the users to easily find the goodness of clustering solutions as well as provides visual representations of the clustering outcomes.', 'corpus_id': 16218273, 'score': 0}, {'doc_id': '213745971', 'title': 'Weighted consensus clustering and its application to Big data', 'abstract': 'Abstract The aim of this study is the development of a weighted consensus clustering that assigns weights to single clustering methods using the purity utility function. In the case of Big data that does not contain labels, the utility function based on the Davies-Bouldin index is proposed in this paper. The Banknote authentication, Phishing, Diabetic, Magic04, Credit card clients, Covertype, Phone accelerometer, and NSL-KDD datasets are used to assess the efficiency of the proposed consensus approach. The proposed approach is evaluated using the Euclidean, Minkowski, squared Euclidean, cosine, and Chebychev distance metrics. It is compared with single clustering algorithms (DBSCAN, OPTICS, CLARANS, k-means, and shared nearby neighbor clustering). The experimental results show the effectiveness of the proposed approach to the Big data clustering in comparison to single clustering methods. The proposed weighted consensus clustering using the squared Euclidean distance metric achieves the highest accuracy, which is a very promising result for Big data clustering. It can be applied to expert systems to help experts make group decisions based on several alternatives. The paper also provides directions for future research on consensus clustering in this area.', 'corpus_id': 213745971, 'score': 1}]"
77	{'doc_id': '204032941', 'title': 'How Culture Is Understood in Faculty Development in the Health Professions: A Scoping Review', 'abstract': 'Supplemental Digital Content is available in the text. Purpose To examine the ways in which culture is conceptualized in faculty development (FD) in the health professions. Method The authors searched PubMed, Web of Science, ERIC, and CINAHL, as well as the reference lists of identified publications, for articles on culture and FD published between 2006 and 2018. Based on inclusion criteria developed iteratively, they screened all articles. A total of 955 articles were identified, 100 were included in the full-text screen, and 70 met the inclusion criteria. Descriptive and thematic analyses of data extracted from the included articles were conducted. Results The articles emanated from 20 countries; primarily focused on teaching and learning, cultural competence, and career development; and frequently included multidisciplinary groups of health professionals. Only 1 article evaluated the cultural relevance of an FD program. The thematic analysis yielded 3 main themes: culture was frequently mentioned but not explicated; culture centered on issues of diversity, aiming to promote institutional change; and cultural consideration was not routinely described in international FD. Conclusions Culture was frequently mentioned but rarely defined in the FD literature. In programs focused on cultural competence and career development, addressing culture was understood as a way of accounting for racial and socioeconomic disparities. In international FD programs, accommodations for cultural differences were infrequently described, despite authors acknowledging the importance of national norms, values, beliefs, and practices. In a time of increasing international collaboration, an awareness of, and sensitivity to, cultural contexts is needed.', 'corpus_id': 204032941}	20843	"[{'doc_id': '122558101', 'title': 'Visualizing faculty development impact: A social network analysis', 'abstract': 'Faculty development programs have tended to focus on low levels of evaluation such as participant satisfaction rather than assess the actual changes that training has brought about in the workplace. This has prompted scholars to suggest using social network analysis as a\xa0means to provide a\xa0more rigorous method of evaluating the impact of faculty development. To test the feasibility of such a\xa0suggestion, we used the social network analysis concepts of social cohesion to assess the impact of a\xa0year-long fellowship program conducted by Duke-NUS Medical School’s Academic Medicine Education Institute (AM·EI). Specifically, we used the key metrics of connectedness and betweenness centrality to assess the changes in the AM·EI fellows’ information and collaboration networks post-fellowship. We invited three cohorts of AM·EI fellows (2013–2016; n\u202f=\u200974) to participate in a\xa0branched survey. The response rate was 64%; n\u202f=\u200947. Results showed that in terms of connectedness, the largest connected set more than doubled in size, and pair level reachability grew threefold. Betweenness centrality among the AM·EI fellows also increased, with more individuals reporting that they sought advice from the fellows as well as trusted the advice the fellows provided. In sum, this study suggests that it is indeed viable to use social network analysis to identify changes in social cohesion. As such, social network analysis serves as another tool for scholars to use to assess the impact of their faculty development efforts.', 'corpus_id': 122558101, 'score': 1}, {'doc_id': '237398076', 'title': 'Pathology Residency Program Special Expertise Tracks Meet the Needs of an Evolving Field', 'abstract': 'Pathologists who enter the workforce must have a diverse skill set beyond that of clinical diagnostics alone. Anticipating this need, the Johns Hopkins Pathology Residency Program developed Special Expertise Tracks to enhance training in relevant subspecialty domains. Using a combination of discussions and surveys, we assessed: (1) our current resident curriculum; (2) perceived curricular strengths and needs; (3) resident career preferences and ultimate career paths; (4) perceived barriers to implementing an advanced elective curriculum; and (5) available departmental/institutional resources. Additionally, we utilized the Accreditation Council for Graduate Medical Education Pathology Milestones as a curricular guide. Six professional residency training Special Expertise Tracks were established: Education, Physician-Scientist Research, Informatics, Quality Improvement/Quality Assurance/Value-Based Care, Health Policy/Hospital Management and Global Health. After implementation in 2017, the Education track has had 4 residents complete the curriculum successfully; the Physician-Scientist Research track has had 2 residents and the Informatics and Global Health tracks have each had one resident successfully complete their respective curricula. Currently, 5 residents are pursuing the Education track, one is pursuing the Physician-Scientist Research track, one is pursuing the Informatics track, and 2 residents are pursuing the Global Health track. Five residents have completed long-term projects including developing several e-learning modules, an online free digital cytopathology atlas, peer-reviewed articles, book chapters, and books. The Johns Hopkins Pathology Resident Special Expertise Track program provides pathology residents an opportunity to gain meaningful experience and additional skills tailored to their individual career interests.', 'corpus_id': 237398076, 'score': 0}, {'doc_id': '11851689', 'title': '""Incentives for Managed Growth"": A Case Study of Incentives-Based Planning and Budgeting in a Large Public Research University', 'abstract': 'Implementing an incentives-based budget system at a large public research university significantly redirected internal funds while producing notable organizational and financial surprises. For example, units did not increase their ""hoarding"" of students, contrary to some expectations. The findings point to several issues for further analysis and research.', 'corpus_id': 11851689, 'score': 1}, {'doc_id': '214346436', 'title': 'Effective Faculty Development', 'abstract': None, 'corpus_id': 214346436, 'score': 1}, {'doc_id': '147966918', 'title': ""Growing Faculty Research for Students' Success: Best Practices of a Research Institute at a Minority-Serving Undergraduate Institution"", 'abstract': 'IntroductionPuerto Rico was a colony of Spain for ~400 years until 1898, when the US occupied the Island. Transformations ushered in by World War II changed the overtly colonial relationship between the Island and the US to die current Commonwealth status as a non-incorporated US territory. Island-born Puerto Ricans are US citizens and most wish to maintain close political and economic ties to the US. However, most Puerto Ricans also view themselves as a distinct group with common history, culture, and heritage (Dâvila, 1997; Duany, 2002; Morris, 1995). Both Spanish and English are official languages in Puerto Rico, but mainly Spanish is spoken. Because the Islands economy is heavily dependent on US industry and federal funds transfers, mainland events such as the recent economic recession adversely affected this US territory. Currendy, Puerto Ricos per capita income is ~$15,200 (half that of Mississippi, the poorest state), and the unemployment rate is 15.4 percent (Alvarez, 2014). Tie resident population is estimated at 3.6 million (U.S. Census Bureau, 2013), while almost 5 million Puerto Ricans are now living in the US (U.S. Census Bureau, 2011).Since 1903, the University of Puerto Rico (UPR) has been die sole public institution charged with the mission to ""develop the latent intellectual and spiritual enrichment of our society fully,"" so diat ""the intellectual and spiritual values of exceptional personalities that surge from all its social sectors, especially from those less favored in terms of economic resources, will be put to the service of the Puerto Rican community"" (UPRRC EGCTI, 2015, our emphasis). With a total student population of61,967 students, the UPR system has to implement this mission across its eleven campuses: three ofwhich are graduate and eight ofwhich are primarily undergraduate. The University of Puerto Rico at Cayey (UPR-C) is one of the UPRs eight undergraduate campuses. It offers 27 bachelors degrees in Natural Science (Biology, Chemistry, Mathematics and the Natural Sciences General Program); Social Sciences (Psychology and Mental Health); Arts (English, Literature, Humanities, Foreign Languages, Literature and Linguistics) and Professional Schools (Education and Business Administration). Enrollment trends from 2005 to 2009 varied from 22% to 33% in Natural Sciences, including the Natural Sciences General Program, 24% to 27% in Business Administration and from 13% to 14% in Social Sciences. General fall enrollment at UPR-C has increased 5.39% from 3,634 in 2005-06 to 3,830 in 2009-10. Practically all students (99%) are Puerto Rican (UPR-C Assessment Office, 2013a, 2013b; U.S. Department of Education, 2012) and the majority (67%) is female. Most UPR-C students come from the municipalities that surround the town of Cayey, located in the central mountainous area of Puerto Rico (see Figure 1).Consistent with UPR\'s overall mission, UPR-C offers quality educational opportunities to lowincome students of its service region who meet the University\'s admissions criteria. The majority of incoming UPR-C full-time undergraduate students (75%) received Pell Grants and more than half ( 56.9 %) proceed from public schools. The average GPA for incoming freshmen is 2.87. Fulltime attendance status in 2013 was 93%. The campus has 164 full-time faculty and 32 part-time professors, 90% of whom are Puerto Rican. One hundred twenty-nine are tenured or tenuretrack, and 67 are faculty with non-tenure adjunct positions; approximately 79% of all tenure and tenure-track faculty have a Ph.D.This faculty along with administrators and staff are charged with fulfilling three UPR-C missions, which together emphasize providing a quality education that integrates: 1) interdisciplinar)\' approaches, 2) research, and 3) community engagement (UPR-C mission, 2006). The Institute of Interdisciplinary Research (HR) (http://webl.oss.cayey.upr.edu/iii/) supports this mission. Six overarching aims developed in 2004 guide the IIR initiatives: 1 ) to promote interdisciplinary research; 2) to produce knowledge that is relevant to Puerto Rico and to the UPR-C service region; 3) to facilitate research at UPR-C; 4) to promote research-informed curricular innovations; 5) to provide a supportive environment for researchers and students; and 6) to disseminate results of the research projects it sponsors. …', 'corpus_id': 147966918, 'score': 1}, {'doc_id': '211050024', 'title': 'Implementing Faculty Development Programs: Moving From Theory to Practice.', 'abstract': None, 'corpus_id': 211050024, 'score': 1}, {'doc_id': '237578752', 'title': 'Mentoring Opportunities in Computer Architecture: Analyzing the Past to Develop the Future', 'abstract': 'Academic mentoring programming is a powerful tool used for supporting, engaging, and retaining students in their fields of study. Researchers have long known the positive effects of academic mentoring, particularly for students from underrepresented and marginalized backgrounds. The computer architecture community currently hosts an assortment of mentoring programs geared toward women, underrepresented students, junior graduate students, and undergraduates alike. In this work, we describe the current state of mentoring opportunities for students in computer architecture. In addition to summarizing various mentoring programs (e.g., CWWMCA, YArch, and uArch), this work details the organization and feedback from two programs (MaSA and MaSS) that the authors currently run and organize. Based on feedback from these short-term mentoring programs, along with relevant mentoring research literature, we identify opportunities for developing more productive longer-term mentoring programming for the computer architecture community. Following mentoring literature, this work makes a strong case for offering both short-term and long-term mentoring programs in the future; in particular, mentoring literature show the need for time in forming mentoring relationships for mentees to receive the multifaceted benefits of mentoring.', 'corpus_id': 237578752, 'score': 0}, {'doc_id': '237406957', 'title': 'Activating Social Capital: How Peer and Socio-Emotional Mentoring Facilitate Resilience and Success for Community College Students', 'abstract': 'This article details the impact of the intensive mentoring model, through faculty-to-student and peer-to-peer mentoring, utilized in WAESO-LSAMP community colleges. We pay particular attention to the practice of socio-emotional mentoring, the development of a “mentoring chain,” and the impact of communities of support on student and faculty participants. Specifically, we discuss how these separate modes of mentoring impact students from underrepresented students in developing and activating social capital, developing collaborative support systems, fostering confidence and self-efficacy, combatting impostor syndrome and stereotype threat, and embracing the importance of failure in the scientific process. Methods and data include qualitative analysis of forty-six in-depth interviews with program participants, including faculty mentors and community college students, at three community college sites within the WAESO-LSAMP alliance. We address specific implications for faculty working with underrepresented STEM community college students and provide evidence of best practices for setting up a community of support that leads to academic and personal success.', 'corpus_id': 237406957, 'score': 0}, {'doc_id': '237514310', 'title': ""Engaging faculty advisors to promote students' personal and professional development."", 'abstract': ""BACKGROUND AND PURPOSE\nWith the increasing emphasis on personal and professional development in pharmacy curricula, programs often need to enhance or modify existing resources to meet standards. A major initiative of developing a consistent and standardized student advising process involving both clinical and non-clinical faculty as advisors to promote development of pharmacy students was an avenue taken at our institution.\n\n\nEDUCATIONAL ACTIVITY AND SETTING\nFaculty were identified as ideal individuals to assist in mentoring and assessing students' personal and professional development given the long-term relationships they develop with students throughout both the didactic and experiential curriculum. Development and implementation of a longitudinal faculty advising program incorporating elements of student self-assessment, reflection, and faculty-based objective assessment is described.\n\n\nFINDINGS\nWe found both students and faculty benefited from this advising program in different ways, with students feeling more supported and faculty feeling more engaged and informed about the pharmacy curricula and student career-pathways.\n\n\nSUMMARY\nA faculty advising program, as described in this article, can be utilized for both student success and faculty development in the pharmacy education setting."", 'corpus_id': 237514310, 'score': 0}]"
78	{'doc_id': '23512039', 'title': 'Suppression of nonsense mutations as a therapeutic approach to treat genetic diseases', 'abstract': 'Suppression therapy is a treatment strategy for genetic diseases caused by nonsense mutations. This therapeutic approach utilizes pharmacological agents that suppress translation termination at in‐frame premature termination codons (PTCs) to restore translation of a full‐length, functional polypeptide. The efficiency of various classes of compounds to suppress PTCs in mammalian cells is discussed along with the current limitations of this therapy. We also elaborate on approaches to improve the efficiency of suppression that include methods to enhance the effectiveness of current suppression drugs and the design or discovery of new, more effective suppression agents. Finally, we discuss the role of nonsense‐mediated mRNA decay (NMD) in limiting the effectiveness of suppression therapy, and describe tactics that may allow the efficiency of NMD to be modulated in order to enhance suppression therapy. WIREs RNA 2011 2 837–852 DOI: 10.1002/wrna.95', 'corpus_id': 23512039}	5532	"[{'doc_id': '7962232', 'title': 'Ranking insertion, deletion and nonsense mutations based on their effect on genetic information', 'abstract': 'BackgroundGenetic variations contribute to normal phenotypic differences as well as diseases, and new sequencing technologies are greatly increasing the capacity to identify these variations. Given the large number of variations now being discovered, computational methods to prioritize the functional importance of genetic variations are of growing interest. Thus far, the focus of computational tools has been mainly on the prediction of the effects of amino acid changing single nucleotide polymorphisms (SNPs) and little attention has been paid to indels or nonsense SNPs that result in premature stop codons.ResultsWe propose computational methods to rank insertion-deletion mutations in the coding as well as non-coding regions and nonsense mutations. We rank these variations by measuring the extent of their effect on biological function, based on the assumption that evolutionary conservation reflects function. Using sequence data from budding yeast and human, we show that variations which that we predict to have larger effects segregate at significantly lower allele frequencies, and occur less frequently than expected by chance, indicating stronger purifying selection. Furthermore, we find that insertions, deletions and premature stop codons associated with disease in the human have significantly larger predicted effects than those not associated with disease. Interestingly, the large-effect mutations associated with disease show a similar distribution of predicted effects to that expected for completely random mutations.ConclusionsThis demonstrates that the evolutionary conservation context of the sequences that harbour insertions, deletions and nonsense mutations can be used to predict and rank the effects of the mutations.', 'corpus_id': 7962232, 'score': 1}, {'doc_id': '216130437', 'title': 'Effect of High vs Low Doses of Chloroquine Diphosphate as Adjunctive Therapy for Patients Hospitalized With Severe Acute Respiratory Syndrome Coronavirus 2 (SARS-CoV-2) Infection: A Randomized Clinical Trial.', 'abstract': 'Importance\nThere is no specific antiviral therapy recommended for coronavirus disease 2019 (COVID-19). In vitro studies indicate that the antiviral effect of chloroquine diphosphate (CQ) requires a high concentration of the drug.\n\n\nObjective\nTo evaluate the safety and efficacy of 2 CQ dosages in patients with severe COVID-19.\n\n\nDesign, Setting, and Participants\nThis parallel, double-masked, randomized, phase IIb clinical trial with 81 adult patients who were hospitalized with severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) infection was conducted from March 23 to April 5, 2020, at a tertiary care facility in Manaus, Brazilian Amazon.\n\n\nInterventions\nPatients were allocated to receive high-dosage CQ (ie, 600 mg CQ twice daily for 10 days) or low-dosage CQ (ie, 450 mg twice daily on day 1 and once daily for 4 days).\n\n\nMain Outcomes and Measures\nPrimary outcome was reduction in lethality by at least 50% in the high-dosage group compared with the low-dosage group. Data presented here refer primarily to safety and lethality outcomes during treatment on day 13. Secondary end points included participant clinical status, laboratory examinations, and electrocardiogram results. Outcomes will be presented to day 28. Viral respiratory secretion RNA detection was performed on days 0 and 4.\n\n\nResults\nOut of a predefined sample size of 440 patients, 81 were enrolled (41 [50.6%] to high-dosage group and 40 [49.4%] to low-dosage group). Enrolled patients had a mean (SD) age of 51.1 (13.9) years, and most (60 [75.3%]) were men. Older age (mean [SD] age, 54.7 [13.7] years vs 47.4 [13.3] years) and more heart disease (5 of 28 [17.9%] vs 0) were seen in the high-dose group. Viral RNA was detected in 31 of 40 (77.5%) and 31 of 41 (75.6%) patients in the low-dosage and high-dosage groups, respectively. Lethality until day 13 was 39.0% in the high-dosage group (16 of 41) and 15.0% in the low-dosage group (6 of 40). The high-dosage group presented more instance of QTc interval greater than 500 milliseconds (7 of 37 [18.9%]) compared with the low-dosage group (4 of 36 [11.1%]). Respiratory secretion at day 4 was negative in only 6 of 27 patients (22.2%).\n\n\nConclusions and Relevance\nThe preliminary findings of this study suggest that the higher CQ dosage should not be recommended for critically ill patients with COVID-19 because of its potential safety hazards, especially when taken concurrently with azithromycin and oseltamivir. These findings cannot be extrapolated to patients with nonsevere COVID-19.\n\n\nTrial Registration\nClinicalTrials.gov Identifier: NCT04323527.', 'corpus_id': 216130437, 'score': 0}, {'doc_id': '216525501', 'title': 'The Efficacy and Safety of Fecal Microbiota Transplant for Recurrent Clostridiumdifficile Infection: Current Understanding and Gap Analysis', 'abstract': 'Abstract The leading risk factor for Clostridioides (Clostridium) difficile infection (CDI) is broad-spectrum antibiotics, which lead to low microbial diversity, or dysbiosis. Current therapeutic strategies for CDI are insufficient, as they do not address the key role of the microbiome in preventing C. difficile spore germination into toxin-producing vegetative bacteria, which leads to symptomatic disease. Fecal microbiota transplant (FMT) appears to reduce the risk of recurrent CDI through microbiome restoration. However, a wide range of efficacy rates have been reported, and few placebo-controlled trials have been conducted, limiting our understanding of FMT efficacy and safety. We discuss the current knowledge gaps driven by questions around the quality and consistency of clinical trial results, patient selection, diagnostic methodologies, use of suppressive antibiotic therapy, and methods for adverse event reporting. We provide specific recommendations for future trial designs of FMT to provide improved quality of the clinical evidence to better inform treatment guidelines.', 'corpus_id': 216525501, 'score': 0}, {'doc_id': '219692176', 'title': 'High-throughput interrogation of programmed ribosomal frameshifting in human cells', 'abstract': 'Programmed ribosomal frameshifting (PRF) is the controlled slippage of the translating ribosome to an alternative frame. This process is widely employed by human viruses such as HIV and SARS coronavirus and is critical for their replication. Here, we developed a high-throughput approach to assess the frameshifting potential of a sequence. We designed and tested >12,000 sequences based on 15 viral and human PRF events, allowing us to systematically dissect the rules governing ribosomal frameshifting and discover novel regulatory inputs based on amino acid properties and tRNA availability. We assessed the natural variation in HIV gag-pol frameshifting rates by testing >500 clinical isolates and identified subtype-specific differences and associations between viral load in patients and the optimality of PRF rates. We devised computational models that accurately predict frameshifting potential and frameshifting rates, including subtle differences between HIV isolates. This approach can contribute to the development of antiviral agents targeting PRF. Programmed ribosomal frameshifting—the slippage of the ribosome to an alternative frame — is critical for viral replication and cellular processes. Here the authors present an approach that can assess the frameshifting potential of a sequence and elucidate the rules governing ribosomal frameshifting.', 'corpus_id': 219692176, 'score': 1}, {'doc_id': '218517823', 'title': ""Does zinc supplementation enhance the clinical efficacy of chloroquine/hydroxychloroquine to win today's battle against COVID-19?"", 'abstract': '\n Abstract\n \n Currently, drug repurposing is an alternative to novel drug development for the treatment of COVID-19 patients. The antimalarial drug chloroquine (CQ) and its metabolite hydroxychloroquine (HCQ) are currently being tested in several clinical studies as potential candidates to limit SARS-CoV-2-mediated morbidity and mortality. CQ and HCQ (CQ/HCQ) inhibit pH-dependent steps of SARS-CoV-2 replication by increasing pH in intracellular vesicles and interfere with virus particle delivery into host cells. Besides direct antiviral effects, CQ/HCQ specifically target extracellular zinc to intracellular lysosomes where it interferes with RNA-dependent RNA polymerase activity and coronavirus replication. As zinc deficiency frequently occurs in elderly patients and in those with cardiovascular disease, chronic pulmonary disease, or diabetes, we hypothesize that CQ/HCQ plus zinc supplementation may be more effective in reducing COVID-19 morbidity and mortality than CQ or HCQ in monotherapy. Therefore, CQ/HCQ in combination with zinc should be considered as additional study arm for COVID-19 clinical trials.\n \n', 'corpus_id': 218517823, 'score': 0}, {'doc_id': '218892315', 'title': 'Evidence of Protective Effect of Hydroxychloroquine on COVID-19', 'abstract': 'We would like to share ideas on the report on ""Hydroxychloroquine in Patients with Rheumatic Disease Complicated by COVID-19: Clarifying Target Exposures and the Need for Clinical Trials [1].""Balevic noted that ""well-designed clinical trials that include patients with rheumatic disease are urgently needed to characterize the efficacy, safety, and target exposures for hydroxychloroquine [1].""', 'corpus_id': 218892315, 'score': 0}, {'doc_id': '218856018', 'title': 'Investigating the genomic landscape of novel coronavirus (2019-nCoV) to identify non-synonymous mutations for use in diagnosis and drug design.', 'abstract': 'Novel coronavirus has wrecked medical and health care facilities claiming ∼5% death tollsglobally. All efforts to contain the pathogenesis either using inhibitory drugs or vaccines largelyremained futile due to a lack of better understanding of the genomic feature of this virus. In thepresent study, we compared the 2019-nCoV with other coronaviruses, which indicated that batSARS like coronavirus could be a probable ancestor of the novel coronavirus. The proteinsequence similarity of pangolin-hCoV and bat-hCoV with human coronavirus was higher ascompared to their nucleotide similarity denoting the occurrence of more synonymous mutationsin the genome. Phylogenetic and alignment analysis of 591 novel coronaviruses of differentclades from Group I to Group V revealed several mutations and concomitant amino acidchanges. Detailed investigation on nucleotide substitution unfolded 100 substitutions in thecoding region of which 43 were synonymous and 57 were of non-synonymous type. The nonsynonymous substitutions resulting into 57 amino acid changes were found to be distributed overdifferent hCoV proteins with maximum on spike protein. An important di-amino acid change RGto KR was observed in ORF9 protein. Additionally, several interesting features of the novelcoronavirus genome have been highlighted in respect to various other human infecting viruseswhich may explain extreme pathogenicity, infectivity and simultaneously the reason behindfailure of the antiviral therapies. SUMMARY: This study presents a comprehensive phylogenetic analysis of SARS-CoV2 isolates to understand discrete mutations that are occurring between patient samples. The analysis unravel various amino acid mutations in the viral proteins which may provide an explanation for varying treatment efficacies of different inhibitory drugs and a future direction towards a combinatorial treatment therapies based on the kind of mutation in the viral genome.', 'corpus_id': 218856018, 'score': 0}, {'doc_id': '218856387', 'title': 'Polysomes Bypass a 50-Nucleotide Coding Gap Less Efficiently Than Monosomes Due to Attenuation of a 5′ mRNA Stem–Loop and Enhanced Drop-off', 'abstract': '\n Abstract\n \n Efficient translational bypassing of a 50\u202fnt non-coding gap in a phage T4 topoisomerase subunit gene (gp60) requires several recoding signals. Here we investigate the function of the mRNA stem loop 5′ of the take-off codon, as well as the importance of ribosome loading density on the mRNA for efficient bypassing. We show that polysomes are less efficient at mediating bypassing than monosomes, both in vitro and in vivo, due to their preventing formation of a stem loop 5′ of the take-off codon and allowing greater peptidyl-tRNA drop off. A ribosome profiling analysis of phage T4 infected E. coli yielded protected mRNA fragments within the normal size range derived from ribosomes stalled at the take-off codon. However, ribosomes at this position also yielded some 53 nucleotide fragments, 16 longer. These were due to protection of the nucleotides that form the 5′ stem loop. NMR shows that the 5′ stem loop is highly dynamic. The importance of different nucleotides in the 5′ stem loop is revealed by mutagenesis studies. These data highlight the significance of the 5′ stem loop for the 50\u202fnt bypassing, and further enhance appreciation of relevance of the extent of ribosome loading for recoding.\n \n', 'corpus_id': 218856387, 'score': 1}, {'doc_id': '8830352', 'title': 'Readthrough Strategies for Therapeutic Suppression of Nonsense Mutations in Inherited Metabolic Disease', 'abstract': 'Inherited metabolic diseases (IMDs) belong to the group of rare diseases due to their low individual prevalence. Most of them are inherited in autosomal recessive fashion and represent good candidates for novel therapeutical strategies aimed at recovering partial enzyme function as they lack an effective treatment, and small levels of enzymatic activity have been shown to be associated with improved outcome and milder phenotypes. Recently, a novel therapeutic approach for genetic diseases has emerged, based on the ability of aminoglycosides and other compounds in allowing translation to proceed through a premature termination codon introduced by a nonsense mutation, which frequently constitute a significant fraction of the mutant alleles in a population. In this review we summarize the essentials of what is known as suppression therapy, the different compounds that have been identified by high-throughput screens or developed using a medicinal chemistry approach and the preclinical and clinical trials that are being conducted in general and in the field of IMDs in particular. Several IMDs have shown to be good models for evaluating readthrough compounds using patients’ cells carrying nonsense mutations, monitoring for an increase in functional recovery and/or enzyme activity. Overall, the positive results obtained indicate the feasibility of the approach for different diseases and although the levels of protein function reached are low, they may be enough to alleviate the consequences of the pathology. Nonsense suppression thus represents a potential therapy or supplementary treatment for a number of IMD patients encouraging further clinical trials with readthrough drugs with improved functionality and low toxicity.', 'corpus_id': 8830352, 'score': 1}, {'doc_id': '7368538', 'title': 'Nonaminoglycoside compounds induce readthrough of nonsense mutations', 'abstract': 'Large numbers of genetic disorders are caused by nonsense mutations for which compound-induced readthrough of premature termination codons (PTCs) might be exploited as a potential treatment strategy. We have successfully developed a sensitive and quantitative high-throughput screening (HTS) assay, protein transcription/translation (PTT)–enzyme-linked immunosorbent assay (ELISA), for identifying novel PTC-readthrough compounds using ataxia-telangiectasia (A-T) as a genetic disease model. This HTS PTT-ELISA assay is based on a coupled PTT that uses plasmid templates containing prototypic A-T mutated (ATM) mutations for HTS. The assay is luciferase independent. We screened ∼34,000 compounds and identified 12 low-molecular-mass nonaminoglycosides with potential PTC-readthrough activity. From these, two leading compounds consistently induced functional ATM protein in ATM-deficient cells containing disease-causing nonsense mutations, as demonstrated by direct measurement of ATM protein, restored ATM kinase activity, and colony survival assays for cellular radiosensitivity. The two compounds also demonstrated readthrough activity in mdx mouse myotube cells carrying a nonsense mutation and induced significant amounts of dystrophin protein.', 'corpus_id': 7368538, 'score': 1}]"
79	{'doc_id': '227227752', 'title': 'Uncertainty Quantification in Deep Learning through Stochastic Maximum Principle', 'abstract': 'We develop a probabilistic machine learning method, which formulates a class of stochastic neural networks by a stochastic optimal control problem. An efficient stochastic gradient descent algorithm is introduced under the stochastic maximum principle framework. Convergence analysis for stochastic gradient descent optimization and numerical experiments for applications of stochastic neural networks are carried out to validate our methodology in both theory and performance.', 'corpus_id': 227227752}	5117	"[{'doc_id': '219792634', 'title': 'Deep Reinforcement Learning amidst Lifelong Non-Stationarity', 'abstract': 'As humans, our goals and our environment are persistently changing throughout our lifetime based on our experiences, actions, and internal and external drives. In contrast, typical reinforcement learning problem set-ups consider decision processes that are stationary across episodes. Can we develop reinforcement learning algorithms that can cope with the persistent change in the former, more realistic problem settings? While on-policy algorithms such as policy gradients in principle can be extended to non-stationary settings, the same cannot be said for more efficient off-policy algorithms that replay past experiences when learning. In this work, we formalize this problem setting, and draw upon ideas from the online learning and probabilistic inference literature to derive an off-policy RL algorithm that can reason about and tackle such lifelong non-stationarity. Our method leverages latent variable models to learn a representation of the environment from current and past experiences, and performs off-policy RL with this representation. We further introduce several simulation environments that exhibit lifelong non-stationarity, and empirically find that our approach substantially outperforms approaches that do not reason about environment shift.', 'corpus_id': 219792634, 'score': 1}, {'doc_id': '221397582', 'title': 'Stochastic Graph Recurrent Neural Network', 'abstract': 'Representation learning over graph structure data has been widely studied due to its wide application prospects. However, previous methods mainly focus on static graphs while many real-world graphs evolve over time. Modeling such evolution is important for predicting properties of unseen networks. To resolve this challenge, we propose SGRNN, a novel neural architecture that applies stochastic latent variables to simultaneously capture the evolution in node attributes and topology. Specifically, deterministic states are separated from stochastic states in the iterative process to suppress mutual interference. With semi-implicit variational inference integrated to SGRNN, a non-Gaussian variational distribution is proposed to help further improve the performance. In addition, to alleviate KL-vanishing problem in SGRNN, a simple and interpretable structure is proposed based on the lower bound of KL-divergence. Extensive experiments on real-world datasets demonstrate the effectiveness of the proposed model. Code is available at this https URL.', 'corpus_id': 221397582, 'score': 1}, {'doc_id': '224814213', 'title': 'Logistic $Q$-Learning', 'abstract': 'We propose a new reinforcement learning algorithm derived from a regularized linear-programming formulation of optimal control in MDPs. The method is closely related to the classic Relative Entropy Policy Search (REPS) algorithm of Peters et al. (2010), with the key difference that our method introduces a Q-function that enables efficient exact model-free implementation. The main feature of our algorithm (called QREPS) is a convex loss function for policy evaluation that serves as a theoretically sound alternative to the widely used squared Bellman error. We provide a practical saddle-point optimization method for minimizing this loss function and provide an error-propagation analysis that relates the quality of the individual updates to the performance of the output policy. Finally, we demonstrate the effectiveness of our method on a range of benchmark problems.', 'corpus_id': 224814213, 'score': 0}, {'doc_id': '227013048', 'title': 'Visual Forecasting of Time Series with Image-to-Image Regression', 'abstract': 'Time series forecasting is essential for agents to make decisions in many domains. Existing models rely on classical statistical methods to predict future values based on previously observed numerical information. Yet, practitioners often rely on visualizations such as charts and plots to reason about their predictions. Inspired by the end-users, we re-imagine the topic by creating a framework to produce visual forecasts, similar to the way humans intuitively do. In this work, we take a novel approach by leveraging advances in deep learning to extend the field of time series forecasting to a visual setting. We do this by transforming the numerical analysis problem into the computer vision domain. Using visualizations of time series data as input, we train a convolutional autoencoder to produce corresponding visual forecasts. We examine various synthetic and real datasets with diverse degrees of complexity. Our experiments show that visual forecasting is effective for cyclic data but somewhat less for irregular data such as stock price. Importantly, we find the proposed visual forecasting method to outperform numerical baselines. We attribute the success of the visual forecasting approach to the fact that we convert the continuous numerical regression problem into a discrete domain with quantization of the continuous target signal into pixel space.', 'corpus_id': 227013048, 'score': 0}, {'doc_id': '226299995', 'title': 'Discrete solution pools and noise-contrastive estimation for predict-and-optimize', 'abstract': 'Numerous real-life decision-making processes involve solving a combinatorial optimization problem with uncertain input that can be estimated from historic data. There is a growing interest in decision-focused learning methods, where the loss function used for learning to predict the uncertain input uses the outcome of solving the combinatorial problem over a set of predictions. Different surrogate loss functions have been identified, often using a continuous approximation of the combinatorial problem. However, a key bottleneck is that to compute the loss, one has to solve the combinatorial optimisation problem for each training instance in each epoch, which is computationally expensive even in the case of continuous approximations. \nWe propose a different solver-agnostic method for decision-focused learning, namely by considering a pool of feasible solutions as a discrete approximation of the full combinatorial problem. Solving is now trivial through a single pass over the solution pool. We design several variants of a noise-contrastive loss over the solution pool, which we substantiate theoretically and empirically. Furthermore, we show that by dynamically re-solving only a fraction of the training instances each epoch, our method performs on par with the state of the art, whilst drastically reducing the time spent solving, hence increasing the feasibility of predict-and-optimize for larger problems.', 'corpus_id': 226299995, 'score': 0}, {'doc_id': '235421581', 'title': 'COHORTNEY: Non-Parametric Clustering of Event Sequences', 'abstract': 'Cohort analysis is a pervasive activity in web analytics. One divides users into groups according to specific criteria and tracks their behavior over time. Despite its extensive use, academic circles do not discuss cohort analysis to evaluate user behavior online. This work introduces an unsupervised non-parametric approach to group Internet users based on their activities. In comparison, canonical methods in marketing and engineering-based techniques underperform. COHORTNEY is the first machine learning-based cohort analysis algorithm with a robust theoretical explanation.', 'corpus_id': 235421581, 'score': 1}, {'doc_id': '222290661', 'title': 'Multivariate Time Series Classification with Hierarchical Variational Graph Pooling', 'abstract': ""Over the past decade, multivariate time series classification (MTSC) has received great attention with the advance of sensing techniques. Current deep learning methods for MTSC are based on convolutional and recurrent neural network, with the assumption that time series variables have the same effect to each other. Thus they cannot model the pairwise dependencies among variables explicitly. What's more, current spatial-temporal modeling methods based on GNNs are inherently flat and lack the capability of aggregating node information in a hierarchical manner. To address this limitation and attain expressive global representation of MTS, we propose a graph pooling based framework MTPool and view MTSC task as graph classification task. With graph structure learning and temporal convolution, MTS slices are converted to graphs and spatial-temporal features are extracted. Then, we propose a novel graph pooling method, which uses an ``encoder-decoder'' mechanism to generate adaptive centroids for cluster assignments. GNNs and graph pooling layers are used for joint graph representation learning and graph coarsening. With multiple graph pooling layers, the input graphs are hierachically coarsened to one node. Finally, differentiable classifier takes this coarsened one-node graph as input to get the final predicted class. Experiments on 10 benchmark datasets demonstrate MTPool outperforms state-of-the-art methods in MTSC tasks."", 'corpus_id': 222290661, 'score': 1}, {'doc_id': '227151560', 'title': 'Remaining Useful Life Estimation Under Uncertainty with Causal GraphNets', 'abstract': 'In this work, a novel approach for the construction and training of time series models is presented that deals with the problem of learning on large time series with non-equispaced observations, which at the same time may possess features of interest that span multiple scales. The proposed method is appropriate for constructing predictive models for non-stationary stochastic time series.The efficacy of the method is demonstrated on a simulated stochastic degradation dataset and on a real-world accelerated life testing dataset for ball-bearings. The proposed method, which is based on GraphNets, implicitly learns a model that describes the evolution of the system at the level of a state-vector rather than of a raw observation. The proposed approach is compared to a recurrent network with a temporal convolutional feature extractor head (RNN-tCNN) which forms a known viable alternative for the problem context considered. Finally, by taking advantage of recent advances in the computation of reparametrization gradients for learning probability distributions, a simple yet effective technique for representing prediction uncertainty as a Gamma distribution over remaining useful life predictions is employed.', 'corpus_id': 227151560, 'score': 0}, {'doc_id': '208248131', 'title': 'Economy Statistical Recurrent Units For Inferring Nonlinear Granger Causality', 'abstract': 'Granger causality is a widely-used criterion for analyzing interactions in large-scale networks. As most physical interactions are inherently nonlinear, we consider the problem of inferring the existence of pairwise Granger causality between nonlinearly interacting stochastic processes from their time series measurements. Our proposed approach relies on modeling the embedded nonlinearities in the measurements using a component-wise time series prediction model based on Statistical Recurrent Units (SRUs). We make a case that the network topology of Granger causal relations is directly inferrable from a structured sparse estimate of the internal parameters of the SRU networks trained to predict the processes’ time series measurements. We propose a variant of SRU, called economy-SRU, which, by design has considerably fewer trainable parameters, and therefore less prone to overfitting. The economy-SRU computes a low-dimensional sketch of its high-dimensional hidden state in the form of random projections to generate the feedback for its recurrent processing. Additionally, the internal weight parameters of the economy-SRU are strategically regularized in a group-wise manner to facilitate the proposed network in extracting meaningful predictive features that are highly time-localized to mimic real-world causal events. Extensive experiments are carried out to demonstrate that the proposed economy-SRU based time series prediction model outperforms the MLP, LSTM and attention-gated CNN-based time series models considered previously for inferring Granger causality.', 'corpus_id': 208248131, 'score': 1}, {'doc_id': '227126670', 'title': 'Solving path dependent PDEs with LSTM networks and path signatures', 'abstract': 'Using a combination of recurrent neural networks and signature methods from the rough paths theory we design efficient algorithms for solving parametric families of path dependent partial differential equations (PPDEs) that arise in pricing and hedging of path-dependent derivatives or from use of non-Markovian model, such as rough volatility models in Jacquier and Oumgari, 2019. The solutions of PPDEs are functions of time, a continuous path (the asset price history) and model parameters. As the domain of the solution is infinite dimensional many recently developed deep learning techniques for solving PDEs do not apply. Similarly as in Vidales et al. 2018, we identify the objective function used to learn the PPDE by using martingale representation theorem. As a result we can de-bias and provide confidence intervals for then neural network-based algorithm. We validate our algorithm using classical models for pricing lookback and auto-callable options and report errors for approximating both prices and hedging strategies.', 'corpus_id': 227126670, 'score': 0}]"
80	{'doc_id': '792798', 'title': 'Bacteriocins from the rhizosphere microbiome – from an agriculture perspective', 'abstract': 'Bacteria produce and excrete a versatile and dynamic suit of compounds to defend against microbial competitors and mediate local population dynamics. These include a wide range of broad-spectrum non-ribosomally synthesized antibiotics, lytic enzymes, metabolic by-products, proteinaceous exotoxins, and ribosomally produced antimicrobial peptides (bacteriocins). Most bacteria produce at least one bacteriocin. Bacteriocins are of interest in the food industry as natural preservatives and in the probiotics industry, leading to extensive studies on lactic acid bacteria (colicin produced by Escherichia coli is a model bacteriocin). Recent studies have projected use of bacteriocins in veterinary medicine and in agriculture, as biostimulants of plant growth and development and as biocontrol agents. For example, bacteriocins such as Cerein 8A, Bac-GM17, putidacin, Bac 14B, amylocyclicin have been studied for their mechanisms of anti-microbial activity. Bac IH7 promotes tomato and musk melon plant growth. Thuricin 17 (Th17) is the only bacteriocin studied extensively for plant growth promotion, including at the molecular level. Th17 functions as a bacterial signal compound, promoting plant growth in legumes and non-legumes. In Arabidopsis thaliana and Glycine max Th17 increased phytohormones IAA and SA at 24 h post treatment. At the proteome level Th17 treatment of 3-week-old A. thaliana rosettes led to >2-fold changes in activation of the carbon and energy metabolism pathway proteins, 24 h post treatment. At 250 mM NaCl stress, the control plants under osmotic-shock shut down most of carbon-metabolism and activated energy-metabolism and antioxidant pathways. Th17 treated plants, at 250 mM NaCl, retained meaningful levels of the light harvesting complex, photosystems I and II proteins and energy and antioxidant pathways were activated, so that rosettes could better withstand the salt stress. In Glycine max, Th17 helped seeds germinate in the presence of NaCl stress, and was most effective at 100 mM NaCl. The 48 h post germination proteome suggested efficient and speedier partitioning of storage proteins, activation of carbon, nitrogen and energy metabolisms in Th17 treated seeds both under optimal and 100 mM NaCl. This review focuses on the bacteriocins produced by plant-rhizosphere colonizers and plant-pathogenic bacteria, that might have uses in agriculture, veterinary, and human medicine.', 'corpus_id': 792798}	8548	"[{'doc_id': '220543736', 'title': 'Selenium biofortification in the 21st century: status and challenges for healthy human nutrition', 'abstract': 'Selenium (Se) is an essential element for mammals and its deficiency in the diet is a global problem. Plants accumulate Se and thus represent a major source of Se to consumers. Agronomic biofortification intends to enrich crops with Se in order to secure its adequate supply by people. The goal of this review is to report the present knowledge of the distribution and processes of Se in soil and at the plant-soil interface, and of Se behaviour inside the plant in terms of biofortification. It aims to unravel the Se metabolic pathways that affect the nutritional value of edible plant products, various Se biofortification strategies in challenging environments, as well as the impact of Se-enriched food on human health. Agronomic biofortification and breeding are prevalent strategies for battling Se deficiency. Future research addresses nanosized Se biofortification, crop enrichment with multiple micronutrients, microbial-integrated agronomic biofortification, and optimization of Se biofortification in adverse conditions. Biofortified food of superior nutritional quality may be created, enriched with healthy Se-compounds, as well as several other valuable phytochemicals. Whether such a food source might be used as nutritional intervention for recently emerged coronavirus infections is a relevant question that deserves investigation.', 'corpus_id': 220543736, 'score': 0}, {'doc_id': '58553652', 'title': 'Bacteriocins: Classification, synthesis, mechanism of action and resistance development in food spoilage causing bacteria.', 'abstract': 'Huge demand of safe and natural preservatives has opened new area for intensive research on bacteriocins to unravel the novel range of antimicrobial compounds that could efficiently fight off the food-borne pathogens. Since food safety has become an increasingly important international concern, the application of bacteriocins from lactic acid bacteria that target food spoilage/pathogenic bacteria without major adverse effects has received great attention. Different modes of actions of these bacteriocins have been suggested and identified, like pore-forming, inhibition of cell-wall/nucleic acid/protein synthesis. However, development of resistance in the food spoilage and pathogenic bacteria against these bacteriocins is a rising concern. Emergence and spread of mutant strains resistant to bacteriocins is hampering food safety. It has spurred an interest to understand the bacteriocin resistance phenomenon displayed by the food pathogens, which will be helpful in mitigating the resistance problem. Therefore, present review is focused on the different resistance mechanisms adopted by food pathogens to overcome bacteriocin.', 'corpus_id': 58553652, 'score': 1}, {'doc_id': '221402862', 'title': 'Probiotics: Versatile Bioactive Components in Promoting Human Health', 'abstract': 'The positive impact of probiotic strains on human health has become more evident than ever before. Often delivered through food, dietary products, supplements, and drugs, different legislations for safety and efficacy issues have been prepared. Furthermore, regulatory agencies have addressed various approaches toward these products, whether they authorize claims mentioning a disease’s diagnosis, prevention, or treatment. Due to the diversity of bacteria and yeast strains, strict approaches have been designed to assess for side effects and post-market surveillance. One of the most essential delivery systems of probiotics is within food, due to the great beneficial health effects of this system compared to pharmaceutical products and also due to the increasing importance of food and nutrition. Modern lifestyle or various diseases lead to an imbalance of the intestinal flora. Nonetheless, as the amount of probiotic use needs accurate calculations, different factors should also be taken into consideration. One of the novelties of this review is the presentation of the beneficial effects of the administration of probiotics as a potential adjuvant therapy in COVID-19. Thus, this paper provides an integrative overview of different aspects of probiotics, from human health care applications to safety, quality, and control.', 'corpus_id': 221402862, 'score': 0}, {'doc_id': '92671064', 'title': 'Bacteriocins active against plant pathogenic bacteria. Biochem Soc Trans', 'abstract': None, 'corpus_id': 92671064, 'score': 1}, {'doc_id': '221691975', 'title': 'Impact of Dietary Modification on Microbiome:Exploring Therapeutic Implications', 'abstract': 'The Human Gut Microbiome: The gastro-intestinal tract and various other organs harbour large and diverse communities of bacteria, viruses, and other microscopic life. In the human gut, there inhabit microbial members as residents (autochthonous), while others (allochthonous) are from ingested food, water and other components of the environment. The adult human gut microbiota is dominated by mainly two bacteria, the Bacteroidetes and Firmicutes and an archaea, Metanobrevibacter smithii.', 'corpus_id': 221691975, 'score': 0}, {'doc_id': '221507911', 'title': 'Investigation of the Cyprus donkey milk bacterial diversity by 16SrDNA high-throughput sequencing in a Cyprus donkey farm', 'abstract': 'The interest in milk originating from donkeys is growing worldwide due to its claimed functional and nutritional properties, especially for sensitive population groups, such as infants with cow milk protein allergy. The current study aimed to assess the microbiological quality of donkey milk produced in a donkey farm in Cyprus using culture-based and high-throughput sequencing techniques. The culture-based microbiological analysis showed very low microbial counts, whereas important food-borne pathogens were not detected in any sample. In addition, high-throughput sequencing was applied to characterize the bacterial communities of donkey milk samples. Donkey milk mostly composed of gram-negative Proteobacteria, including Sphingomonas, Pseudomonas, Mesorhizobium, and Acinetobacter; lactic acid bacteria, including Lactobacillus and Streptococcus; the endospores forming Clostridium; and the environmental genera Flavobacterium and Ralstonia, detected in lower relative abundances. The results of the study support existing findings that donkey milk contains mostly gram-negative bacteria. Moreover, it raises questions regarding the contribution of (1) antimicrobial agents (i.e., lysozyme, peptides) in shaping the microbial communities and (2) bacterial microbiota to the functional value of donkey milk.', 'corpus_id': 221507911, 'score': 0}, {'doc_id': '218590331', 'title': 'Research news highlights', 'abstract': 'Many diseases caused by viruses, bacteria, and fungi affect plant crops, resulting in losses and decreasing the quality and safety of agricultural products. Plant disease control relies mainly on chemical pesticides that are currently subject to strong restrictions and regulatory requirements. Antimicrobial peptides are interesting compounds in plant health because there is a need for new products in plant protection that fit into the new regulations. Living organisms secrete a wide range of antimicrobial peptides produced through ribosomal (defensins and small bacteriocins) or non-ribosomal synthesis (peptaibols, cyclopeptides, and pseudopeptides). Several antimicrobial peptides (AMPs) are the basis for the design of new synthetic analogues. They have either been been expressed in transgenic plants to confer disease protection or are secreted by microorganisms that are active ingredients of commercial biopesticides. In this review, AMPs produced by microorganisms are described in more detail, as well as compounds that have been synthesized, produced by microbial biocontrol agents, or expressed in transgenic plants.', 'corpus_id': 218590331, 'score': 1}, {'doc_id': '210871744', 'title': 'Antimicrobials for food and feed; a bacteriocin perspective.', 'abstract': 'Bacteriocins are natural antimicrobials that have been consumed via fermented foods for millennia and have been the focus of renewed efforts to identify novel bacteriocins, and their producing microorganisms, for use as food biopreservatives and other applications. Bioengineering bacteriocins or combining bacteriocins with multiple modes of action (hurdle approach) can enhance their preservative effect and reduces the incidence of antimicrobial resistance. In addition to their role as food biopreservatives, bacteriocins are gaining credibility as health modulators, due to their ability to regulate the gut microbiota, which is strongly associated with human wellbeing. Indeed the strengthening link between the gut microbiota and obesity make bacteriocins ideal alternatives to Animal Growth Promoters (AGP) in animal feed also. Here we review recent advances in bacteriocin research that will contribute to the development of functional foods and feeds as a consequence of roles in food biopreservation and human/animal health.', 'corpus_id': 210871744, 'score': 1}, {'doc_id': '12446337', 'title': 'Bacteriocin-based strategies for food biopreservation.', 'abstract': 'Bacteriocins are ribosomally-synthesized peptides or proteins with antimicrobial activity, produced by different groups of bacteria. Many lactic acid bacteria (LAB) produce bacteriocins with rather broad spectra of inhibition. Several LAB bacteriocins offer potential applications in food preservation, and the use of bacteriocins in the food industry can help to reduce the addition of chemical preservatives as well as the intensity of heat treatments, resulting in foods which are more naturally preserved and richer in organoleptic and nutritional properties. This can be an alternative to satisfy the increasing consumers demands for safe, fresh-tasting, ready-to-eat, minimally-processed foods and also to develop ""novel"" food products (e.g. less acidic, or with a lower salt content). In addition to the available commercial preparations of nisin and pediocin PA-1/AcH, other bacteriocins (like for example lacticin 3147, enterocin AS-48 or variacin) also offer promising perspectives. Broad-spectrum bacteriocins present potential wider uses, while narrow-spectrum bacteriocins can be used more specifically to selectively inhibit certain high-risk bacteria in foods like Listeria monocytogenes without affecting harmless microbiota. Bacteriocins can be added to foods in the form of concentrated preparations as food preservatives, shelf-life extenders, additives or ingredients, or they can be produced in situ by bacteriocinogenic starters, adjunct or protective cultures. Immobilized bacteriocins can also find application for development of bioactive food packaging. In recent years, application of bacteriocins as part of hurdle technology has gained great attention. Several bacteriocins show additive or synergistic effects when used in combination with other antimicrobial agents, including chemical preservatives, natural phenolic compounds, as well as other antimicrobial proteins. This, as well as the combined use of different bacteriocins may also be an attractive approach to avoid development of resistant strains. The combination of bacteriocins and physical treatments like high pressure processing or pulsed electric fields also offer good opportunities for more effective preservation of foods, providing an additional barrier to more refractile forms like bacterial endospores as well. The effectiveness of bacteriocins is often dictated by environmental factors like pH, temperature, food composition and structure, as well as the food microbiota. Foods must be considered as complex ecosystems in which microbial interactions may have a great influence on the microbial balance and proliferation of beneficial or harmful bacteria. Recent developments in molecular microbial ecology can help to better understand the global effects of bacteriocins in food ecosystems, and the study of bacterial genomes may reveal new sources of bacteriocins.', 'corpus_id': 12446337, 'score': 1}, {'doc_id': '221728087', 'title': 'Strategies of Plant Biotechnology to Meet the Increasing Demand of Food and Nutrition in India', 'abstract': ""A groundbreaking application of biotechnology research during the recent past has been improvement of crop health and production. India being one of the most rapidly developing countries with an enormous population and remarkable biodiversity, plant biotechnology promises significant potential to contribute to characterization and conservation of the biodiversity, increasing its usefulness. However, India’s green revolution was noted to be insufficient to feed the country's teeming millions. Therefore, novel approaches in crop biotechnology had to be aimed at ensuring better productivity and quality of cultivars. This paper provides a comprehensive review of research undertaken mainly in the last couple of decades along with potential strategies in plant biotechnology focusing on specific grain and seed crops of key agricultural as well as dietary importance to meet the growing demand of food and nutrition in India, while also proposing potential application of relevant global research findings in the Indian context. The analysis would help address the ever-increasing worldwide socioeconomic necessity for greater food security, particularly during times of crisis such as the recent Coronavirus Infectious Disease 2019 (COVID-19) pandemic."", 'corpus_id': 221728087, 'score': 0}]"
81	{'doc_id': '54445170', 'title': 'Deep Learning for Aspect-Based Sentiment Analysis: A Comparative Review', 'abstract': 'Abstract The increasing volume of user-generated content on the web has made sentiment analysis an important tool for the extraction of information about the human emotional state. A current research focus for sentiment analysis is the improvement of granularity at aspect level, representing two distinct aims: aspect extraction and sentiment classification of product reviews and sentiment classification of target-dependent tweets. Deep learning approaches have emerged as a prospect for achieving these aims with their ability to capture both syntactic and semantic features of text without requirements for high-level feature engineering, as is the case in earlier methods. In this article, we aim to provide a comparative review of deep learning for aspect-based sentiment analysis to place different approaches in context.', 'corpus_id': 54445170}	9969	"[{'doc_id': '201378649', 'title': 'Algorithmic bias? An empirical study into apparent gender-based discrimination in the display of STEM career ads', 'abstract': 'We explore data from a field test of how an algorithm delivered ads promoting job opportunities in the Science, Technology, Engineering and Math (STEM) fields. This ad was explicitly intended to be gender-neutral in its delivery. Empirically, however, fewer women saw the ad than men. This happened because younger women are a prized demographic and are more expensive to show ads to. An algorithm which simply optimizes cost-effectiveness in ad delivery will deliver ads that were intended to be gender-neutral in an apparently discriminatory way, due to crowding out. We show that this empirical regularity extends to other major digital platforms.', 'corpus_id': 201378649, 'score': 1}, {'doc_id': '221806683', 'title': 'Advancing Enterprise Education for Women in Science, Technology, Engineering and Mathematics (STEM) among MENA Countries-G20 Insights', 'abstract': 'This Policy Brief is offered to the Saudi T20 process, as a recommendation to the G20 in 2020. In 2020, Saudi Arabia is host of the G20 Summit during the unprecedented COVID-19 pandemic and calls for social and political transformation among MENA countries. Calls for reform include the need to strengthen enterprise or entrepreneurship education for women in science, technology, engineering, and mathematics (STEM). This need is evidenced in below average performance of MENA countries among 80 countries surveyed in the OECD (2018) Program for International Student Assessment. The assessment measured 15year-old students’ reading, mathematics and science literacy, as well as problem solving and skills to ‘meet real-life challenges.’ With respect to enterprise education, an UNESCO (2013, p. 18) study of Arab countries recommended, “a national strategy for the integration of entrepreneurship education in the educational and training systems and curricula during general education, technical or professional education and higher education.” Similar recommendations are advanced by the World Economic Forum (2010). Challenges facing women in STEM are also reported. While agencies, such as UNESCO (2017) and the World Bank (2020), report that the engagement of women in STEM across MENA countries is high (relative to more developed countries) few women graduates pursue STEM careers. Thirteen of 15 countries with the lowest rate of female participation in the workforce are Arab countries (World Bank, 2020). This reflects ‘lost’ economic opportunity and return on investment from STEM education. In recognizing the importance of STEM credentials as the foundation for employment in the digital economy, associated challenges are twofold. First, is a need to imbed enterprise education in STEM curricula to help students develop entrepreneurial attitudes, skills and competencies, and to improve their abilities to meet ‘real life challenges’. Second, entrepreneurship education within STEM disciplines will encourage women to pursue business startup as a career option, and enhance their ability to scale and growth their businesses. Such measures will contribute to women’s economic empowerment and economic growth across the MENA region. To this extent, this Policy Brief addresses the lack of: Formalized enterprise curricula in MENA educational and training systems Enterprise education within STEM disciplines within most MENA states Small business support services beyond startup within most MENA states Small business support services for startups operating within MENA’s STEM sectors Mentors and role models for women STEM entrepreneurs in MENA states. The policy brief informs an issue of significant importance to the G20 economies, and in particular, MENA states by advancing strategies to: Educate and train a billion people in the next decade. The focus on entrepreneurship and STEM will prepare young women for careers in the Fourth Industrial Revolution. Build capacity and mobilize knowledge needed to create businesses to drive economic growth in the Fourth Industrial Revolution.', 'corpus_id': 221806683, 'score': 0}, {'doc_id': '221506642', 'title': ""The Impact Of The Secondary School Students' Integrationin Distance Learning With Its Various Platforms On Their Achievement From Their Point Of View In Na'ourdistrict Directorate"", 'abstract': '2020 The study aimed to identify the impact of the integration of secondary school students in distance learning with its various platforms on their achievement from their point of view in Na\'ourDistrict Directorate. To achieve the goal of the study, the descriptive approach was used by constructing a questionnaire, the validity and reliability of the study were verified, and the study sampleconsisted of 918 male and female students from the first and second secondary students.The study concluded the following results: there is a statistically significant effect of the level of integration of secondary school students through distance learning on the total scorein academic achievement, and integration accounts for (59.2%) of achievement. As for the domains level, the results showed that there was a statistically significant effect of the dimensions related to the student and the ones related to the teacher in academic achievement and the absence ofthe effect of the technical related dimension. The results also showed that the extent of secondary stage students\'integration in distance learning with its various platforms in Na\'our District Directorate wasvery high, the student field occupied ""the first place, and in the second place came technical – aspect related dimension."", in the third and last place came the aspects related to teacher dimension"", the results also showed that there are statistically significant differences regardingthe extent of the integration of secondary students taught through distance learning at the total score as well as fields according to variables (gender, class, educational platform).The differences were in favor of females, the second secondary class , Noorspace platform. In the light of the results, a number of recommendations were presented, the most important of which are: the necessity of encouraging secondary stage students, especially the second secondary students to follow educational platforms to improve achievement, to understand academic subjects, and to be less dependent on costlyprivate lessons. Another recommendation isteacher training on the optimal use of synchronous and asynchronous distance learning.', 'corpus_id': 221506642, 'score': 0}, {'doc_id': '4894798', 'title': 'The Influence of Affirming Kindness and Community on Broadening Participation in STEM Career Pathways.', 'abstract': ""The United States' inability to achieve equitable workforce development in science, technology, engineering, and mathematics (STEM) career pathways is well-recognized and has been attributed to the poor retention of a diverse stream of students in academia. Social science theory and research provide evidence that social contextual variables-specifically kindness cues affirming social inclusion-influence chronic underrepresentation of some groups within STEM career pathways. Review of the literature suggests that the current STEM academic context does not consistently provide cues that affirm social inclusion to all members of the academic population, and that policies that address this disparity are essential to broadening STEM workforce development in the United States."", 'corpus_id': 4894798, 'score': 1}, {'doc_id': '222508352', 'title': 'Pre-Service Mathematics Teachers’ Levels of Academic Procrastination and Online Learning Readiness', 'abstract': 'This article aims to examine the relationship between the online learning readiness and academic procrastination behaviors of the pre-service mathematics teachers. In line with this research purpose, it is examined whether the online learning readiness and academic procrastination differentiate with regard to demographic variables such as gender, grade levels...etc.; and as well as that to technical problems occurs during the online learning process, the last minute course and the instructors’ impact during the course study process. This research has been conducted with 314 pre-service mathematics teachers that currently attending the Faculty of Education of different universities in Turkey. The relevant analyses revealed that there is a low-level significant relationship between the academic procrastination tendency and online learning readiness. Besides, the scores of academic procrastination tendency and online learning readiness are found higher among males, first graders, those who have access problems and those who think an instructor is a determinant. The research findings are discussed within the light of related literature.', 'corpus_id': 222508352, 'score': 0}, {'doc_id': '215959235', 'title': 'Opinion Mining and Sentiment Analysis', 'abstract': 'An important part of our information-gathering behavior has always been to find out what other people think. With the growing availability and popularity of opinion-rich resources such as online re...', 'corpus_id': 215959235, 'score': 1}, {'doc_id': '144689028', 'title': 'Linking early science and mathematics attitudes to long-term science, technology, engineering, and mathematics career attainment: latent class analysis with proximal and distal outcomes', 'abstract': ""There is a need to identify students' early attitudes toward mathematics and science to better support their long-term persistence in science, technology, engineering, and mathematics (STEM) careers. Seventh graders from a nationally representative sample (N = 2,861) were classified based on their responses to questions about their attitudes toward mathematics and science using latent class analysis. Four distinct groups of students that differed in terms of their attitudes were identified. There were relationships between attitudinal group membership, demographic characteristics, mathematics and science achievement, and STEM career attainment. Females and underrepresented minorities were more likely to be in the positive attitude group. However, despite these early positive attitudes, females and underrepresented minorities were less likely to be employed in a STEM career some 20 years later. Information about student interests organized in this manner can be used to better target specific interventions to support and encourage persistence in STEM careers."", 'corpus_id': 144689028, 'score': 1}, {'doc_id': '43216798', 'title': ""Who's computing? Gender and race differences in young adults' decisions to pursue an information technology career."", 'abstract': 'Over the past ten years, information technology (IT) has emerged as a vitalpart of the global economy. Although IT job growth will not be as rapid asthe booming growth of the computer industry during the previous decade,as the IT sector matures and routine work is increasingly outsourced over-seas, the computer industry still offers favorable job prospects and faces ademand for qualiﬁed professionals (U.S. Department of Labor, 2004). Astechnology becomes more sophisticated and complex, these computer-basedjob prospects are especially promising for individuals with more advancedlevels of training and expertise (U.S. Department of Labor, 2004).Despite these fruitful career opportunities available in the computerﬁeld, women and minorities in the United States are vastly underrepresented', 'corpus_id': 43216798, 'score': 1}]"
82	{'doc_id': '231934142', 'title': 'Accelerated Sparse Neural Training: A Provable and Efficient Method to Find N: M Transposable Masks', 'abstract': 'Unstructured pruning reduces the memory footprint in deep neural networks (DNNs). Recently, researchers proposed different types of structural pruning intending to reduce also the computation complexity. In this work, we first suggest a new measure called mask-diversity which correlates with the expected accuracy of the different types of structural pruning. We focus on the recently suggested N : M fine-grained block sparsity mask, in which for each block of M weights, we have at least N zeros. While N :M fine-grained block sparsity allows acceleration in actual modern hardware, it can be used only to accelerate the inference phase. In order to allow for similar accelerations in the training phase, we suggest a novel transposable fine-grained sparsity mask, where the same mask can be used for both forward and backward passes. Our transposable mask guarantees that both the weight matrix and its transpose follow the same sparsity pattern; thus, the matrix multiplication required for passing the error backward can also be accelerated. We formulate the problem of finding the optimal transposable-mask as a minimum-cost flow problem. Additionally, to speed up the minimum-cost flow computation, we also introduce a fast linear-time approximation that can be used when the masks dynamically change during training. Our experiments suggest a 2x speed-up in the matrix multiplications with no accuracy degradation over vision and language models. Finally, to solve the problem of switching between different structure constraints, we suggest a method to convert a pre-trained model with unstructured sparsity to an N :M fine-grained block sparsity model with little to no training. A reference implementation can be found at https: //github.com/papers-submission/structured_transposable_masks.', 'corpus_id': 231934142}	12718	[{'doc_id': '235324652', 'title': 'Parallelizing DNN Training on GPUs: Challenges and Opportunities', 'abstract': 'In recent years, Deep Neural Networks (DNNs) have emerged as a widely adopted approach in many application domains. Training DNN models is also becoming a significant fraction of the datacenter workload. Recent evidence has demonstrated that modern DNNs are becoming more complex and the size of DNN parameters (i.e., weights) is also increasing. In addition, a large amount of input data is required to train the DNN models to reach target accuracy. As a result, the training performance becomes one of the major challenges that limit DNN adoption in real-world applications. Recent works have explored different parallelism strategies (i.e., data parallelism and model parallelism) and used multi-GPUs in datacenters to accelerate the training process. However, naively adopting data parallelism and model parallelism across multiple GPUs can lead to sub-optimal executions. The major reasons are i) the large amount of data movement that prevents the system from feeding the GPUs with the required data in a timely manner (for data parallelism); and ii) low GPU utilization caused by data dependency between layers that placed on different devices (for model parallelism). In this paper, we identify the main challenges in adopting data parallelism and model parallelism on multi-GPU platforms. Then, we conduct a survey including recent research works targeting these challenges. We also provide an overview of our work-in-progress project on optimizing DNN training on GPUs. Our results demonstrate that simple-yet-effective system optimizations can further improve the training scalability compared to prior works.', 'corpus_id': 235324652, 'score': 1}, {'doc_id': '232290660', 'title': 'Toward Compact Deep Neural Networks via Energy-Aware Pruning', 'abstract': 'Despite of the remarkable performance, modern deep neural networks are inevitably accompanied with a significant amount of computational cost for learning and deployment, which may be incompatible with their usage on edge devices. Recent efforts to reduce these overheads involves pruning and decomposing the parameters of various layers without performance deterioration. Inspired by several decomposition studies, in this paper, we propose a novel energy-aware pruning method that quantifies the importance of each filter in the network using nuclear-norm (NN). Proposed energy-aware pruning leads to state-of-the art performance for Top-1 accuracy, FLOPs, and parameter reduction across a wide range of scenarios with multiple network architectures on CIFAR-10 and ImageNet after fine-grained classification tasks. On toy experiment, despite of no fine-tuning, we can visually observe that NN not only has little change in decision boundaries across classes, but also clearly outperforms previous popular criteria. We achieve competitive results with 40.4/49.8% of FLOPs and 45.9/52.9% of parameter reduction with 94.13/94.61% in the Top-1 accuracy with ResNet-56/110 on CIFAR-10, respectively. In addition, our observations are consistent for a variety of different pruning setting in terms of data size as well as data quality which can be emphasized in the stability of the acceleration and compression with negligible accuracy loss. Our code is available at https://github.com/ nota-github/nota-pruning-rank. Compute NN', 'corpus_id': 232290660, 'score': 0}, {'doc_id': '235658057', 'title': 'FreeTickets: Accurate, Robust and Efficient Deep Ensemble by Training with Dynamic Sparsity', 'abstract': 'Recent works on sparse neural networks have demonstrated that it is possible to train a sparse network in isolation to match the performance of the corresponding dense networks with a fraction of parameters. However, the identification of these performant sparse neural networks (winning tickets) either involves a costly iterative train-pruneretrain process (e.g., Lottery Ticket Hypothesis) or an overextended sparse training time (e.g., Training with Dynamic Sparsity), both of which would raise financial and environmental concerns. In this work, we attempt to address this cost-reducing problem by introducing the FreeT ickets concept, as the first solution which can boost the performance of sparse convolutional neural networks over their dense network equivalents by a large margin, while using for complete training only a fraction of the computational resources required by the latter. Concretely, we instantiate the FreeT ickets concept, by proposing two novel efficient ensemble methods with dynamic sparsity, which yield in one shot many diverse and accurate tickets “for free” during the sparse training process. The combination of these free tickets into an ensemble demonstrates a significant improvement in accuracy, uncertainty estimation, robustness, and efficiency over the corresponding dense (ensemble) networks. Our results provide new insights into the strength of sparse neural networks and suggest that the benefits of sparsity go way beyond the usual training/inference expected efficiency. We will release all codes in https://github. com/Shiweiliuiiiiiii/FreeTickets.', 'corpus_id': 235658057, 'score': 1}, {'doc_id': '231740811', 'title': 'Zen-NAS: A Zero-Shot NAS for High-Performance Deep Image Recognition', 'abstract': 'Accuracy predictor is a key component in Neural Architecture Search (NAS) for ranking architectures. Building a high-quality accuracy predictor usually costs enormous computation. To address this issue, instead of using an accuracy predictor, we propose a novel zero-shot index dubbed Zen-Score to rank the architectures. The Zen-Score represents the network expressivity and positively correlates with the model accuracy. The calculation of Zen-Score only takes a few forward inferences through a randomly initialized network, without training network parameters. Built upon the Zen-Score, we further propose a new NAS algorithm, termed as Zen-NAS, by maximizing the Zen-Score of the target network under given inference budgets. Within less than half GPU day, Zen-NAS is able to directly search high performance architectures in a data-free style. Comparing with previous NAS methods, the proposed Zen-NAS is magnitude times faster on multiple server-side and mobile-side GPU platforms with state-of-the-art accuracy on ImageNet. Searching and training code as well as pre-trained models are available from https://github.com/ idstcv/ZenNAS. *Accepted by ICCV 2021. Author home page https://minglin-home.github.io 1 ar X iv :2 10 2. 01 06 3v 4 [ cs .C V ] 2 3 A ug 2 02 1', 'corpus_id': 231740811, 'score': 0}, {'doc_id': '235247897', 'title': 'Towards Efficient Full 8-bit Integer DNN Online Training on Resource-limited Devices without Batch Normalization', 'abstract': 'Huge computational costs brought by convolution and batch normalization (BN) have caused great challenges for the online training and corresponding applications of deep neural networks (DNNs), especially in resource-limited devices. Existing works only focus on the convolution or BN acceleration and no solution can alleviate both problems with satisfactory performance. Online training has gradually become a trend in resource-limited devices like mobile phones while there is still no complete technical scheme with acceptable model performance, processing speed, and computational cost. In this research, an efficient online-training quantization framework termed EOQ is proposed by combining Fixup initialization and a novel quantization scheme for DNN model compression and acceleration. Based on the proposed framework, we have successfully realized full 8-bit integer network training and removed BN in largescale DNNs. Especially, weight updates are quantized to 8-bit integers for the first time. Theoretical analyses of EOQ utilizing Fixup initialization for removing BN have been further given using a novel Block Dynamical Isometry theory with weaker assumptions. Benefiting from rational quantization strategies and the absence of BN, the full 8-bit networks based on EOQ can achieve state-of-the-art accuracy and immense advantages in computational cost and processing speed. In addition to the huge advantages brought by quantization in convolution operations, 8-bit networks based on EOQ without BN can realize >70× lower in power, >18× faster in the processing speed compared with the traditional 32-bit floating-point BN inference process. What’s more, the design of deep learning chips can be profoundly simplified for the absence of unfriendly square root operations in BN. Beyond this, EOQ has been evidenced to be more advantageous in small-batch online training with fewer batch samples. In summary, the EOQ framework is specially designed for reducing the high cost of convolution and BN in network training, demonstrating a broad application prospect of online training in resource-limited devices.', 'corpus_id': 235247897, 'score': 1}, {'doc_id': '219305108', 'title': 'Towards Lower Bit Multiplication for Convolutional Neural Network Training', 'abstract': 'Convolutional Neural Networks (CNNs) have been widely used in many fields. However, the training process costs much energy and time, in which the convolution operations consume the major part. In this paper, we propose a fixed-point training framework, in order to reduce the data bit-width for the convolution multiplications. Firstly, we propose two constrained group-wise scaling methods that can be implemented with low hardware cost. Secondly, to overcome the challenge of trading off overflow and rounding error, a shiftable fixed-point data format is used in this framework. Finally, we propose a double-width deployment technique to boost inference performance with the same bit-width hardware multiplier. The experimental results show that the input data of convolution in the training process can be quantized to 2-bit for CIFAR-10 dataset, 6-bit for ImageNet dataset, with negligible accuracy degradation. Furthermore, our fixed-point train-ing framework has the potential to save at least 75% energy of the computation in the training process.', 'corpus_id': 219305108, 'score': 1}, {'doc_id': '231699188', 'title': 'Pruning and Quantization for Deep Neural Network Acceleration: A Survey', 'abstract': 'Abstract Deep neural networks have been applied in many applications exhibiting extraordinary abilities in the field of computer vision. However, complex network architectures challenge efficient real-time deployment and require significant computation resources and energy costs. These challenges can be overcome through optimizations such as network compression. Network compression can often be realized with little loss of accuracy. In some cases accuracy may even improve. This paper provides a survey on two types of network compression: pruning and quantization. Pruning can be categorized as static if it is performed offline or dynamic if it is performed at run-time. We compare pruning techniques and describe criteria used to remove redundant computations. We discuss trade-offs in element-wise, channel-wise, shape-wise, filter-wise, layer-wise and even network-wise pruning. Quantization reduces computations by reducing the precision of the datatype. Weights, biases, and activations may be quantized typically to 8-bit integers although lower bit width implementations are also discussed including binary neural networks. Both pruning and quantization can be used independently or combined. We compare current techniques, analyze their strengths and weaknesses, present compressed network accuracy results on a number of frameworks, and provide practical guidance for compressing networks.', 'corpus_id': 231699188, 'score': 0}, {'doc_id': '237532494', 'title': 'Exploiting Activation based Gradient Output Sparsity to Accelerate Backpropagation in CNNs', 'abstract': 'Machine/deep-learning (ML/DL) based techniques are emerging as a driving force behind many cutting-edge technologies, achieving high accuracy on computer vision workloads such as image classification and object detection. However, training these models involving large parameters is both time-consuming and energy-hogging. In this regard, several prior works have advocated for sparsity to speed up the of DL training and more so, the inference phase. This work begins with the observation that during training, sparsity in the forward and backward passes are correlated. In that context, we investigate two types of sparsity (input and output type) inherent in gradient descent-based optimization algorithms and propose a hardware micro-architecture to leverage the same. Our experimental results use five state-of-the-art CNN models on the Imagenet dataset, and show back propagation speedups in the range of 1.69× to 5.43×, compared to the dense baseline execution. By exploiting sparsity in both the forward and backward passes, speedup improvements range from 1.68× to 3.30× over the sparsity-agnostic baseline execution. Our work also achieves significant reduction in training iteration time over several previously proposed dense as well as sparse accelerator based platforms, in addition to achieving order of magnitude energy efficiency improvements over GPU based execution.', 'corpus_id': 237532494, 'score': 1}]
83	{'doc_id': '236770385', 'title': 'Recent Automation Trends in Portugal: Implications on Industrial Productivity and Employment in Automotive Sector', 'abstract': 'Recent developments in automation and artificial intelligence (AI) are leading to a wave of innovation in organizational design and changes in the workplace. Techno-optimists even named it the “second machine age,” arguing that it now involves the substitution of the human brain. Other authors see this as just a continuation of previous ICT developments. Potentially, automation and AI can have significant technical, economic, and social implications in firms. This paper will answer the following question: What are the implications on industrial productivity and employment in the automotive sector with the recent automation trends, including AI, in Portugal? Our approach used mixed methods to conduct statistical analyses of relevant databases and interviews with experts on R&D projects related to automation and AI implementation. Results suggest that automation can have widespread adoption in the short term in the automotive sector, but AI technologies will take more time to be adopted. The findings show that adoption of automation and AI increases productivity in firms and is dephased in time with employment implications. Investments in automation are not substituting operators but rather changing work organization. Thus, negative effects of technology and unemployment were not substantiated by our results.', 'corpus_id': 236770385}	12947	"[{'doc_id': '232147300', 'title': ""Does automation erode governments' tax basis? An empirical assessment of tax revenues in Europe"", 'abstract': 'Decomposing taxes by source (labor, capital, sales), we analyze the impact of automation (1) on tax revenues, (2) the structure of taxation, and (3) identify channels of impact in 19 EU countries during 1995-2016. Robots and Information and Communication Technologies (ICT) are different technologies designed to automate manual (robots) or cognitive tasks (ICT). Until 2007, robot diffusion led to decreasing factor and tax income, and a shift from taxes on capital to goods. ICTs changed the structure of taxation from capital to labor. We find decreasing employment, but increasing wages and labor income. After 2008, robots have no effect but we find an ICT-induced increase in capital income, a rise of services, but no effect on taxation. Automation goes through different phases with different economic impacts which affect the amount and structure of taxes. Whether automation erodes taxation depends (a) on the technology type, (b) the stage of diffusion and (c) local conditions.', 'corpus_id': 232147300, 'score': 0}, {'doc_id': '236909508', 'title': 'Modelling Artificial Intelligence', 'abstract': 'IZA DP No. 14171 MARCH 2021 Modelling Artificial Intelligence in Economics Economists’ two main theoretical approaches to understanding Artificial Intelligence (AI) impacts have been the task-approach to labor markets and endogenous growth theory. Therefore, the recent integration of the task-approach into an endogenous growth model by Acemoglu and Restrepo (AR) is a useful advance. However, it is subject to the shortcoming that it does not explicitly model AI and its technological feasibility. The AR model focuses on tasks and skills but not on abilities, while abilities better characterize AI services’ nature. This paper addresses this shortcoming by elaborating the task-approach with AI abilities for use within endogenous growth models. This more ability-sensitive specification of the taskapproach allows for more nuanced and realistic impacts of progress in artificial intelligence (AI) on the economy to be captured. JEL Classification: O47, O33, J24, E21, E25', 'corpus_id': 236909508, 'score': 1}, {'doc_id': '231203954', 'title': 'The productivity paradox: policy lessons from MICROPROD', 'abstract': 'MICROPROD researchers have so far delivered 20 papers on four broad issues relevant for today’s policy debates: the measurement and effects of intangible capital on productivity; the impact of globalisation, international trade and the integration of global value chains (GVCs) on productivity; factor allocation and allocative efficiency; and finally the social consequences of the two structural shocks Europe has faced in the last two decades: globalisation and technological progress.', 'corpus_id': 231203954, 'score': 0}, {'doc_id': '236695156', 'title': 'The Present, Past, and Future of Labor-Saving Technologies', 'abstract': 'The present chapter provides a historical reappraisal of labor-saving technologies. It reviews and systematizes theoretical contributions and empirical findings documenting the presence of labor- and time-saving heuristics in innovative efforts back since the First Industrial Revolution. More in detail, with the help of various patent analyses, the chapter documents the presence of labor-saving heuristics in the latest wave of technological innovation, detecting the human functions substituted by the underlying technologies. Against a reductionist approach conceiving robots as the only threat for labor displacement, it shall be argued that labor-saving technologies consist of a complex and heterogeneous bundle of innovations uncovering a much wider set of artifacts and functions. Motivated by the recurrent debate on the threats of automation occurring in the last couple of centuries, evidence is provided on the existence of long waves and clusters in relevant innovations, discussing how the overall cluster of labor-saving technologies consists of heterogeneous and often independent innovations following remarkably different time-trajectories. The chapter closes with an outline of potential future trends in labor-saving technologies and room for policy actions.', 'corpus_id': 236695156, 'score': 1}, {'doc_id': '237303078', 'title': 'New Frontiers: The Origins and Content of New Work, 1940–2018∗', 'abstract': 'Recent theory stresses the role of new job types (‘new work’) in counterbalancing the erosive effect of task-displacing automation on labor demand. Drawing on a novel inventory of eight decades of new job titles linked to United States Census microdata, we estimate that the majority of contemporary employment is found in new job tasks added since 1940 but that the locus of new task creation has shifted—from middle-paid production and clerical occupations in the first four post-WWII decades, to high-paid professional and, secondarily, low-paid services since 1980. We hypothesize that new tasks emerge in occupations where new innovations complement their outputs (‘augmentation’) or market size expands, while conversely, employment contracts in occupations where innovations substitute for labor inputs (‘automation’) or market size contracts. Leveraging proxies for output-augmenting and task-automating innovations built from a century of patent data and harnessing occupational demand shifts stemming from trade and demographic shocks, we show that new occupational tasks emerge in response to both positive demand shifts and augmenting innovations, but not in response to negative demand shifts or automation innovations. We document that the flow of both augmentation and automation innovations is positively correlated across occupations, yet these two faces of innovation have strongly countervailing relationships with occupational labor demand.', 'corpus_id': 237303078, 'score': 1}, {'doc_id': '236632964', 'title': 'A review on the economics of artificial intelligence', 'abstract': None, 'corpus_id': 236632964, 'score': 1}, {'doc_id': '232221783', 'title': 'The Great Transition: Kuznets Facts for Family-Economists', 'abstract': 'The 20th century beheld a dramatic transformation of the family. Some Kuznets style facts regarding structural change in the family are presented. Over the course of the 20th century in the United States fertility declined, educational attainment waxed, housework fell, leisure increased, jobs shifted from blue to white collar, and marriage waned. These trends are also observed in the cross-country data. A model is developed, and then calibrated, to address the trends in the US data. The calibration procedure is closely connected to the underlying economic logic. Three drivers of the great transition are considered: neutral technological progress, skilled-biased technological change, and drops in the price of labor-saving household durables. \n \nYou can download the Kuznets facts here: https://www.ricardomarto.com/data/', 'corpus_id': 232221783, 'score': 0}, {'doc_id': '221521478', 'title': 'Industrialization Without Innovation', 'abstract': 'The introduction of labor-saving technologies in agriculture can release workers who find occupation in the manufacturing sector. The traditional view is that this structural transformation process leads to economic growth. However, if workers leaving agriculture are unskilled, the labor reallocation process reinforces comparative advantage in the least skill-intensive manufacturing industries. We embed this mechanism in a multi-sector endogenous growth model where only skill-intensive manufacturing industries innovate and generate knowledge spillovers. In this setup, the increase in the relative size of the unskilled-labor intensive industries reduces the incentives to innovate and slows down growth. We test the predictions of the model in the context of a large and exogenous increase in agricultural productivity in Brazil. We use social security data to develop a new measure of the labor input in innovation which is representative at any level of spatial aggregation. We find that regions adopting the new agricultural technology experienced a reallocation of unskilled workers away from agriculture into the least R&D-intensive manufacturing industries. The expansion of low-R&D industries attracted workers away from innovative occupations in high-R&D industries, slowing down local aggregate manufacturing productivity growth.', 'corpus_id': 221521478, 'score': 0}, {'doc_id': '236204384', 'title': 'Artificial Intelligence , Robotics , Work and Productivity : The Role of Firm Heterogeneity', 'abstract': 'We propose a model with asymmetric firms where new technologies displace workers. We show that both leading (low-cost) firms and laggard (high-cost) firms increase productivity when automating but that only laggard firms hire more automation-susceptible workers. The reason for this asymmetry is that in laggard firms, the lower incentive to invest in new technologies implies a weaker displacement effect and thus that the output-expansion effect on labor demand dominates. Using novel firm-level automation workforce probabilities, which reveal the extent to which a firms’ workforce can be replaced by new AI and robotic technology and a new shiftshare instrument to address endogeneity, we find strong empirical evidence for these predictions in Swedish matched employer-employee data.', 'corpus_id': 236204384, 'score': 1}, {'doc_id': '233294285', 'title': 'Cross-industry Productivty Growth Differences∗', 'abstract': 'In a multisector model with endogenous knowledge generation, we find that long run differences in sectoral productivity growth are mainly driven by receptivity — the extent to which firm research benefits from prior knowledge regardless of the source. R&D intensity also depends on appropriability — the fraction of receptivity that accrues from the firm’s own stock of knowledge. We show that optimal R&D subsidies should target sectors with higher receptivity but lower appropriability. Using patent data for 14 US industries, we find that quantitatively receptivity is the main factor behind differences in industry TFP growth rates and R&D intensities. JEL Codes: D24, O3, O41 .', 'corpus_id': 233294285, 'score': 0}]"
84	{'doc_id': '134115593', 'title': 'Geochemical Sourcing of Lithic Raw Materials from Secondary Deposits in South Serbia. Implications for Early Neolithic Resource Management Strategies', 'abstract': 'The valleys of the South Morava River and its tributaries in the region of Pusta Reka around the cities of Leskovac and Lebane in southern Serbia are notable for the high density of early prehistoric settlements identified archaeologically. The area represents a link between the Mediterranean and the (northern) Balkans and is therefore of key importance for understanding the processes of Neolithisation in south-eastern and central Europe, which commenced at the beginning of the 6th millennium BC. The current study is part of a larger project which aims to address issues concerning Early Neolithic resource management and production strategies, and specifically the use of the prehistoric landscape, through the characterisation of lithic materials in archaeological assemblages from this region. Lithic raw materials used for chipped stone tool production in the Pusta Reka region include a wide range of cryptocrystalline SiO2 modifications of volcanic and perivolcanic origin. These materials are found abundantly in secondary alluvial deposits in the extensive Neogene basin complexes of southern Serbia. The current pilot study was undertaken to test the viability of sourcing such lithic materials from secondary deposits to their primary origin using the Multi-Layered Chert Sourcing Approach (MLA), a method developed by the first author of this study, which combines visual, microscopic, and geochemical techniques using Laser Ablation Inductively Coupled Plasma Mass Spectrometry (LA-ICP-MS). In the light of the encouraging results, it is planned to extend the analytical work and to apply this method of provenance study to archaeological materials in order to reconstruct the economic behaviour of the Early Neolithic communities in this region.', 'corpus_id': 134115593}	5027	"[{'doc_id': '211609044', 'title': 'The Early Neolithic Complex on the Tartas-1 Site: Results of the AMS Radiocarbon Dating', 'abstract': 'AMS radiocarbon dating was applied to seven samples from Tartas-1, an Early Neolithic site in the Baraba forest-steppe, southwestern Siberia: four from pit 938, one from pit 990, and two from structure 6. Pits had been destined for fermenting fi sh, and contained offerings, such as corpses of animals (fox, hare, wolverine, dog), stone and bone artifacts, and fl at-bottomed clay vessels. On the basis of these fi nds, the Baraba culture was described. The results of the AMS radiocarbon analysis support the previous conclusion regarding the date of the complex — 7th millennium BC. A series of dates generated at the Curt Engelhorn Center for Archaeometry in Mannheim, Germany, for the Neolithic materials from Tartas-1 mostly fall within the 7th millennium, and the same applies to the dates relating to the Neolithic site of Vengerovo-2. The dates for structure 6 from Tartas-1 were generated at the Institute of Nuclear Physics SB RAS in Novosibirsk as well, agreeing with those from the Mannheim Center (for two samples, the results being virtually identical). In sum, the data obtained confi rm the correctness of dating the Early Neolithic complex from Tartas-1 to the 7th millennium BC. The Baraba culture is also dated to this time.', 'corpus_id': 211609044, 'score': 1}, {'doc_id': '216456973', 'title': 'The evolution of evangelical socio-political approaches in contemporary China (1980s-2010s)', 'abstract': 'This thesis explores the evolution of Evangelical socio-political approaches in contemporary China, arguing that Evangelicals in both the Three-Self church and the house churches have moved towards an increasing sense of social concern in the period from the 1980s to the 2010s. The period is divided into a former period (1980s to early-1990s) and a latter period (mid-1990s to 2010s). The late 1970s was the beginning of the Reform and Opening Up policy, which brought about a relatively open socio-political context and led to fast economic growth. Meanwhile, Protestant Christianity experienced fast growth in church numbers and Christian population. The mid-1990s marked a new phase of significant social change, which saw the growth of a socialist market economy partnered with moral decline and social injustice that continued to the 2010s. During this period, Protestant Christianity witnessed the rise of urban churches and growth of Christian intellectuals. The vast majority of Protestants during both periods would be considered Evangelical. This thesis is a study of historical theology, focusing on four Evangelical church figures as case studies. Among them, in the former period, the house church pastor Lin Xiangao focuses on a pious Christian life and rewards in eternity while disengaging with the socio-political context. Different from Lin’s approach, the Three-Self church leader Wang Weifan aspires to a Chinese Christianity integrated with traditional Chinese culture, taking a culture-driven engagement with the sociopolitical context. In the latter period, the house church leader Sun Yi emphases the integrity and public nature of the church. He proposes to build the church as a model of moral integrity and organisational integrity, based on which the church should openly engage with the wider society. Contrastingly, the Three-Self church pastor Wu Weiqing emphasises a Christ-centred theology, by which he proposes faith in Christ as the solution to social injustice. Different from Sun’s blueprint of church integrity, Wu directs the church’s sense of social justice towards helping the poor through practical means like charity, social service and pastoral support. The present study reveals three trends of evolution in the Chinese church. The first trend is of house church Evangelicals, moving from a privatised faith to an open engagement with society. The second trend is of Three-Self church Evangelicals, moving from a culture-driven engagement to a society-oriented approach. The third trend is of Three-Self church and house church Evangelicals moving towards an increasing social concern across the two periods. Nevertheless, this increasing social concern has encountered changes under the state’s new political leadership in the second half of the 2010s, leaving the Evangelical’s quest for socio-political engagement to face new uncertainties in the near future. LAY SUMMARY This thesis explores the evolution of Evangelical socio-political approaches in contemporary China, arguing that Evangelicals in both the Three-Self church and the house churches have moved towards an increasing sense of social concern in the period from the 1980s to the 2010s. The period is divided into a former period (1980s to early-1990s) and a latter period (mid-1990s to 2010s). The late 1970s was the beginning of the Reform and Opening Up policy, which brought about a relatively open socio-political context and led to fast economic growth. Meanwhile, Protestant Christianity experienced fast growth in church numbers and Christian population. The mid-1990s marked a new phase of significant social change, which saw the growth of a socialist market economy partnered with moral decline and social injustice that continued to the 2010s. During this period, Protestant Christianity witnessed the rise of urban churches and growth of Christian intellectuals. The vast majority of Protestants during both periods would be considered Evangelical. This thesis is a study of historical theology, focusing on four Evangelical church figures as case studies. Among them, in the former period, the house church pastor Lin Xiangao focuses on a pious Christian life and rewards in eternity while disengaging with the socio-political context. Different from Lin’s approach, the Three-Self church leader Wang Weifan aspires to a Chinese Christianity integrated with traditional Chinese culture, taking a culture-driven engagement with the sociopolitical context. In the latter period, the house church leader Sun Yi emphases the integrity and public nature of the church. He proposes to build the church as a model of moral integrity and organisational integrity, based on which the church should openly engage with the wider society. Contrastingly, the Three-Self church pastor Wu Weiqing emphasises a Christ-centred theology, by which he proposes faith in Christ as the solution to social injustice. Different from Sun’s blueprint of church integrity, Wu directs the church’s sense of social justice towards helping the poor through practical means like charity, social service and pastoral support. The present study reveals three trends of evolution in the Chinese church. The first trend is of house church Evangelicals, moving from a privatised faith to an open engagement with the society. The second trend is of Three-Self church Evangelicals, moving from a culture-driven engagement to a society-oriented approach. The third trend is of Three-Self church and house church Evangelicals moving towards an increasing social concern across the two periods. Nevertheless, this increasing social concern has encountered changes under the state’s new political leadership in the second half of the 2010s, leaving the Evangelical’s quest for socio-political engagement to face new uncertainties in the near future. TABLE OF CONTENTS', 'corpus_id': 216456973, 'score': 0}, {'doc_id': '212675147', 'title': 'Genesis of primitive Hawaiian rejuvenated-stage lavas: Evidence for carbonatite metasomatism and implications for ancient eclogite source', 'abstract': 'To constrain a contribution of deep carbonated mantle, to fractionation of Hf relative to rare earth elements (REE) in volcanic series, we examine available high-quality data on major, trace element and Nd-Hf isotope compositions of primitive lavas and glasses erupted during preshield, postshield and mostly rejuvenated stage of the Hawaiian hot spot (Pacific Ocean). Strong variations of Hf/Sm, Zr/Sm, Ti/Eu, K/Th, Nb/Th, La/K and Ba/K in the lavas are not features of the melt equilibration with residual amphibole or phlogopite, and cannot be due to variable degrees of batch or dynamic melting of uncarbonated lherzolite source. Enrichment in REE, Th and Ba relative to K, Hf, Zr, Ti and Nb together and low Si, high Na, K and Ca contents in the Hawaiian lavas are compositional features of carbonated mantle lithospheric to asthenospheric peridotite source affected by carbonatite metasomatism. In contrast, major and trace element signatures of most primitive preshield- and postshield-stage magmas require pyroxenite source. \nThe available data infer that Salt Lake Crater garnet pyroxenite xenoliths hosted by the Koolau volcano lavas on Oahu, Hawaii, were derived from deep eclogite source likely generating the carbonatite melts within the Hawaiian plume. Highly radiogenic Hf and decoupled Nd-Hf isotope systematics recorded in the Salt Lake Crater mantle xenolith series on Oahu may be explained by strong Hf fractionation relative to REE owing to ancient event of carbonatite metasomatism, which is likely related to partial melting of the deeply subducted carbonated eclogite within the Hawaiian plume.', 'corpus_id': 212675147, 'score': 0}, {'doc_id': '67864057', 'title': 'Regional diversity in subsistence among early farmers in Southeast Europe revealed by archaeological organic residues', 'abstract': 'The spread of early farming across Europe from its origins in Southwest Asia was a culturally transformative process which took place over millennia. Within regions, the pace of the transition was probably related to the particular climatic and environmental conditions encountered, as well as the nature of localized hunter–gatherer and farmer interactions. The establishment of farming in the interior of the Balkans represents the first movement of Southwest Asian livestock beyond their natural climatic range, and widespread evidence now exists for early pottery being used extensively for dairying. However, pottery lipid residues from sites in the Iron Gates region of the Danube in the northern Balkans show that here, Neolithic pottery was being used predominantly for processing aquatic resources. This stands out not only within the surrounding region but also contrasts markedly with Neolithic pottery use across wider Europe. These findings provide evidence for the strategic diversity within the wider cultural and economic practices during the Neolithic, with this exceptional environmental and cultural setting offering alternative opportunities despite the dominance of farming in the wider region.', 'corpus_id': 67864057, 'score': 1}, {'doc_id': '214802972', 'title': 'Exploring Basement Surface relationship of north-west Bengal Basin using satellite images and tectonic modeling', 'abstract': 'The Bengal basin is one of the thickest sedimentary basins and is being constantly affected by the collision of the Indian plate with the Burma and Tibetan plates. The northwest part of the basin, our study area, is one of the least explored areas where the shallowest faulted basement is present. Controversies exist about the origin of the basement and its role to the formation of surface landforms. We analyze satellite images, Bouguer anomaly data, and develop a geodynamic model to explore the relationship between the faulted basement and surface landforms. Satellite images and gravity anomalies show a spatial correlation between the surface topography and basement fault structures. The elevated tracts and the low-lying flood plains are located on top of the gravity highs (horsts) and lows (grabens). The geodynamic model suggests that conjugate thrust faults may exist beneath the horsts that push the horst block upward. Our observations suggest the regional compression and basement faults have a more considerable influence on the development of surface landforms such as the uplifted tracts and the low-lying flood plains.', 'corpus_id': 214802972, 'score': 0}, {'doc_id': '216553175', 'title': 'El Bahr: A Prospective Impact Crater', 'abstract': 'This preliminary investigation addresses the discovery of an unidentified crater located south of the Sahara Desert between Qaret Had El Bahr and Qaret El Allafa, Egypt. The unidentified crater (hereafter tentatively named El Bahr Crater) was discovered during a terrain analysis of the Sahara Desert. El Bahr Crater is located Southwest Al-Jiza Giza and is approximately 327 meters across, has a rim with a circumference of approximately 1,027 meters, and occupies a surface area of approximately 83,981 square meters. Preliminary spectral and topographic analysis reveal features characteristic of an impact crater produced by a hypervelocity event of extraterrestrial origin, including a bowl-shaped rim and a crater wall. No proximal and/or distal ejecta, however, are visible from Landsat imagery. Moreover, the geomorphic features, along with the fact that the El Bahr basalts are known to be rich in orthopyroxene while the surrounding basalts are not, imply an impact as the most plausible explanation. The El Bahr Crater is not indexed in the Earth Impact Database, and an analysis of impact structures in Africa did not identify it as either a confirmed, proposed or disproved impact crater. In collaboration with the University of Cairo, therefore, an expedition has been organized to conduct an in-situ investigation of El Bahr Crater, to ascertain if planar formations, shatter cones, and shock metamorphic and or other meteoritic properties are present.', 'corpus_id': 216553175, 'score': 0}, {'doc_id': '218470110', 'title': 'Ancient River Morphological Features on Mars versus Arizona Moenkopi Plateau', 'abstract': 'Mars is currently at the center of scientific debate regarding proposed ancient river morphological landscapes on the planet. An increased curiosity in the geomorphology of Mars and its water history, therefore, has led to an effort to better understand how those landscapes formed. Many studies, however, consist of patchwork investigations that have not thoroughly examined proposed ancient fluvial processes on Mars from an Earth analog perspective. The purpose of this investigation, therefore, is to compare known fluvial features on Moenkopi Plateau with proposed paleopotamologic features on Mars. The search for analogs along the Moenkopi Plateau was due to the similarities in fluvial erosion, influenced and modified by eolian activity, primarily from Permian through Jurassic age. By analyzing orbital imagery from two cameras onboard NASA Mars Reconnaissance Orbiter High Resolution Imaging Science Experiment and the Context Camera and paralleling it with imagery obtained from the U.S. Geological Survey and an unmanned aircraft operating over the Moenkopi Plateau, this investigation identified similar fluvial morphology. We interpret, therefore, that the same fluvial processes occurred on both planets, thereby reinforcing the history of water on Mars.', 'corpus_id': 218470110, 'score': 0}, {'doc_id': '131460444', 'title': 'Flint raw material transfers in the prehistoric Lower Danube Basin: An integrated analytical approach', 'abstract': 'The paper presents results of a research programme focused on the provenancing of flint raw materials used in the prehistory of the Lower Danube Basin of the Balkans. Field survey encompassed two adjacent regions connected by the Danube River. First, northern Bulgaria where rich flint-bearing Cretaceous deposits are known along with numerous Neolithic sites but with limited pre-Neolithic presence apart from several well-known Middle to Upper Palaeolithic sequences. Second, the Danube Gorges area on the southern, Serbian side of the river, characterized by relatively scarce deposits of flint, but with one of the best preserved concentrations of Mesolithic, transitional and Early Neolithic sites in the wider region of southeastern Europe. Focusing on both of the two selected regions allows one to follow diachronic dynamics in supply and circulation of local and non-local flint raw materials along the examined stretch of the Lower Danube Basin. In order to connect surveyed flint outcrops and different types of raw material used in archaeological contexts, an integrated approach was employed using both petrographic thin sections and LA-ICP-MS trace element chemical finger-printing analyses.', 'corpus_id': 131460444, 'score': 1}, {'doc_id': '135119855', 'title': 'Following their tears: Production and use of plant exudates in the Neolithic of North Aegean and the Balkans', 'abstract': 'Abstract Resinous and tarry materials have been valuable commodities since prehistory as their widespread use for numerous purposes indicates, but remain largely neglected by archaeological research, in part due to their poor preservation and the need for chemical analyses to identify them. This paper explores the use of these plant exudates in northern Greece and the Balkans during the Early and Middle Holocene with the aim of documenting the production and use of tarry materials and the exploitation of woodland resources. To this end tarry material found on pottery from 10 neolithic settlements located in North Greece, Bulgaria and Serbia, spanning the Early to Late Neolithic periods (7th to 5th millennia BC), were analysed by gas chromatography-mass spectrometry (GC-MS) and identified using the biomarker approach. Data from analysis of wood charcoal found at Neolithic sites together with the pollen record have also been considered in order to assess the availability of raw materials to local populations. The results of biomolecular analysis show that birch-bark tar was almost exclusively used by Neolithic communities located in the Balkan hinterland, while a more complex picture arises for the northern Aegean area. Here, in addition to the predominant birch-bark tar, pine resin and pitch have also been identified as well as beeswax. The pollen and anthracological record suggest that birch existed in northern Greece and the Balkans hinterland during the Early Holocene, but must have been restricted to the uplands. Procurement of raw material may have taken place, therefore, at some distances from the settlements, involving the movement of people and raw materials or final products within the wider region. Chemical analysis provides evidence for variability in the production of tarry materials between settlements in northern Greece, while in the Balkan interior tar-making appears to have followed a more standardised recipe.', 'corpus_id': 135119855, 'score': 1}, {'doc_id': '134368065', 'title': 'Exploitation of aquatic resources for adornment and tool processing at Măgura ‘Buduiasca’ (‘Boldul lui Moş Ivănuş’) Neolithic settlement (southern Romania)', 'abstract': ""Măgura 'Buduiasca' (‘Boldul lui Mos Ivănus’) settlement has an important place among the Prehistoric settlements from the Balkans. It is characterized by continuous habitation from the Early Neolithic through to the Early Chalcolithic allowing an understanding of the development of utilitarian objects and personal adornments in respect of the raw materials used and their processing patterns. One group consists of local bivalves (Unio sp.) which were exploited in an opportunistic manner: first as an important source of food and second as a source material for producing artefacts after recovery from domestic waste. The local gastropods shells (Lithoglyphus sp., Theodoxus danubialis, Esperiana sp., Ansius/Planorbis sp.) are without any nutritional value and used only as a source of raw materials. They could be collected during specialized expeditions organized for this purpose at certain times of the year. Imported elements are a third category which most likely arrived at Măgura ‘Buduiasca’ as finished objects. They provide evidence for complex exchange networks at this period in prehistory. Species like Mytilus or Cardium most likely come from the Black Sea, while Spondylus or Glycymeris may have their origins in the Mediterranean Sea. The presence of these raw materials demonstrates different transformation methods with their origin influencing the processing procedure: allogene valves for adornments, while Unio sp. are especially transformed into utilitarian tools."", 'corpus_id': 134368065, 'score': 1}]"
85	{'doc_id': '109092138', 'title': 'Things That Make Us Smart: Defending Human Attributes In The Age Of The Machine', 'abstract': '* A Human-Centered Technology * Experiencing the World * The Power of Representation * Fitting the Artifact to the Person * The Human Mind * Distributed Cognition * A Place for Everything, and Everything in Its Place * Predicting the Future * Soft and Hard Technology * Technology Is Not Neutral', 'corpus_id': 109092138}	13673	"[{'doc_id': '13635578', 'title': 'User experience - a research agenda', 'abstract': ""Over the last decade, 'user experience' (UX) became a buzzword in the field of human - computer interaction (HCI) and interaction design. As technology matured, interactive products became not only more useful and usable, but also fashionable, fascinating things to desire. Driven by the impression that a narrow focus on interactive products as tools does not capture the variety and emerging aspects of technology use, practitioners and researchers alike, seem to readily embrace the notion of UX as a viable alternative to traditional HCI. And, indeed, the term promises change and a fresh look, without being too specific about its definite meaning. The present introduction to the special issue on 'Empirical studies of the user experience' attempts to give a provisional answer to the question of what is meant by 'the user experience'. It provides a cursory sketch of UX and how we think UX research will look like in the future. It is not so much meant as a forecast of the future, but as a proposal - a stimulus for further UX research."", 'corpus_id': 13635578, 'score': 1}, {'doc_id': '231812703', 'title': 'Exploring UX Maturity in Software Development Environments in Saudi Arabia', 'abstract': 'User experience (UX) design is becoming increasingly crucial for developing successful software today. It can determine whether or not users stay engaged with a product or service. It is, therefore, important that organizations have their users in mind when developing software and that there is a maturity for UX work. However, there are still organizations which do not value UX highly and where UX maturity is low. This paper reported the results of a survey of 75 practitioners working in software-development environments in Saudi Arabia. The survey was conducted in July 2020 and aimed to explore practitioners\' perceptions of UX maturity, UX significance, and the challenges that face UX process in software development environments. The results show a higher than expected perception of organizational UX maturity amongst the practitioners surveyed, with the majority considering their organizations to be at an ""Integrated phase"". The degree of awareness of UX value was also found higher than anticipated. Furthermore, the study reveals important information about the most used UX methods as task analysis, prototyping, and heuristic evaluation. It also shows that UX assessment and user involvement being considered during different stages of product development, particularly in the prototyping phase. The major challenges that face UX process were found to be the need to improve UX consistency and the ability of teams and departments to collaborate. Keywords—User experience; UX maturity; Saudi Arabia', 'corpus_id': 231812703, 'score': 0}, {'doc_id': '196186602', 'title': 'Smart home and appliances: State of the art', 'abstract': None, 'corpus_id': 196186602, 'score': 1}, {'doc_id': '231778234', 'title': 'Digicampus - Preliminary Lessons from a Quadruple Helix Ecosystem for Public Service Innovation', 'abstract': 'Many governments want to harness the potential of new digital technologies for shaping an progressive and inclusive society. However, they often struggle to translate their ambitions into reality. Drawing on the quadruple helix model, Digicampus is an innovation ecosystem in the Netherlands in which government, academia, citizens and companies explore future public services. Since the launch of Digicampus, more than hundred requests for collaboration were submitted by public organizations. The objective of this ongoing research paper is to share the experiences of starting a quadruple helix ecosystem for public service innovation. We do this by discussing the innovation guidelines, research agenda and lessons learned in the first year of Digicampus. Important innovation guidelines are: embrace design thinking and an agile way of working, facilitate multidisciplinary knowledge exchange, co-create prototypes (make technologies tangible) and foster open experimentation (assess the potential). Other countries looking to explore future public services in a collaborative manner can benefit from the insights presented in this paper.', 'corpus_id': 231778234, 'score': 0}, {'doc_id': '114291710', 'title': '2015 Automated Vehicle Literature Review as Part of Preparing aPossible Oregon Road Map for Connected Vehicle/CooperativeSystems Deployment Scenarios', 'abstract': ""The goal of this project was to lay the groundwork for Oregon to be prepared to lead in the implementation of a connected vehicle/cooperative systems transportation portfolio, and/or to avoid being caught by surprise as developments in this area evolve quickly. The project assessed Oregon Department of Transportation's (ODOT’s) internal mechanisms for addressing connected vehicle/cooperative systems, scanned, reviewed and assessed the technical maturity of potential connected vehicle/cooperative system applications, developed preliminary goals, linked to prospective connected vehicle/cooperative systems applications, and refined/ranked/prioritized those that fit with potential ODOT role in advancing/leading these initiatives. The project identified opportunities for linking ODOT’s current programs with national and international connected vehicle/cooperative system research, testing and deployment initiatives, and recommended a final shared vision and “road map” for Oregon's priority connected vehicle/cooperative system applications. This volume contains a literature review and annotated bibliography regarding policy and technical questions about the potential for introducing automated vehicles in the state for research and testing purposes. This includes a discussion of the history and development of automated vehicles for highway use as well as a discussion of the relationship between automated and connected vehicles and the potential for integrating the two technologies. The review also includes an analysis along twelve Oregon-specific dimensions related to specific question about the potential introduction of automated vehicles in Oregon. These dimensions include: liability, implementation, privacy, cyber security, governance, risk, certification, data, legislation, deployment approach, financing and sustainability."", 'corpus_id': 114291710, 'score': 1}, {'doc_id': '225254180', 'title': ""How to Measure UX and Usability in Today's Connected Vehicles"", 'abstract': 'Besides hard factors like horsepower or consumption, today’s connected cars need soft factors being available as criteria for purchasing decisions. User experience (UX), as one of these soft factors, is essentially influenced by services available inside the vehicle. Almost every new vehicle offers a variety of mostly Internet-enabled services to improve the driving experience and to make customers happier. One challenge is to make UX measurable regarding experiences and knowledge inside the vehicle. It has been researched what influencing factors significantly characterize the evaluation of in-car UX. A combination of assessment of relevance, usability testing of pre-defined use cases as well as gathering of quantitative parameters helped create a characteristic variable as a measure of UX inside vehicles. The final evaluation tool is to allow the formation of a holistic opinion about experiences and knowledge inside vehicles and to enable a comparability of UX of complete vehicles as well as of developmental states. Ultimately this not only supports customers in their purchasing decision process, but also manufacturers developing and optimizing new and existing solutions.', 'corpus_id': 225254180, 'score': 1}, {'doc_id': '115122423', 'title': 'Literature Review of Behavioral Adaptations to Advanced Driver Assistance Systems', 'abstract': 'In this literature review, theories of driver behavioral adaptation (BA) are examined for the insight they can provide into how drivers will use advanced driver assistance systems (ADAS). Such systems are designed to support driving tasks formerly managed exclusively by the drivers themselves. How drivers react to this assistance will depend on the accuracy of their understanding (or mental model) of the functions and capabilities of a particular ADAS. Negative BA effects can arise when a driver’s mental model of an ADAS is incomplete or inaccurate. This may happen when an ADAS has functional limits that are reached only infrequently, and that are therefore difficult for a driver to notice and understand. Various ways to address this issue are described in the conclusions.', 'corpus_id': 115122423, 'score': 1}, {'doc_id': '227164875', 'title': 'Subject-Oriented Business Process Management. The Digital Workplace – Nucleus of Transformation: 12th International Conference, S-BPM ONE 2020, Bremen, Germany, December 2-3, 2020, Proceedings', 'abstract': 'In the business we are experiencing digital transformation by a higher speed of change and increasing complexity. Especially in the area of BPM this causesmoreprojectswhich fail. The reasons aremanifold butwell knownandpoint to the usage ofmore than 40 years old paradigms of software development. The gap between people formulation new requirements for processes and those creating the software for digitization and automation is getting larger. A solution is to involve business practitioners directly in programming. This disruptive approach is shifting the old software development paradigms and only possible if the basis for programming by businesspeople is based on subject-orientation and on a very simple and easy to use programming environment. Metasonic® Process Suite and Touch provides exactly this environment for coding the business logic of a process by businesspeople. The created process model serves both business and IT.Many examples realized on subject-oriented BPM prove this new concept pays off and is created big success. A comparison of TCO between S-BPM projects and projects using conventional approaches shows the financial advantages in more details. For BPM projects, using the S-BPM methodology and the metasonic® Process Suite & Touch yield significant time and cost savings. The savings are a direct result of the essential capabilities that set the S-BPM methodology and the metasonic® Process Suite & Touch apart from other approaches it focuses on subjects and their communication – the two key elements that are essential to any organization’s success.', 'corpus_id': 227164875, 'score': 0}, {'doc_id': '229331892', 'title': 'Smart Refrigerator using Internet of Things and Android', 'abstract': 'The kitchen is regarded as the central unit of the traditional as well as modern homes.It is where people cook meals and where our families sit together to eat food. The refrigerator is the pivotal of all that, and hence it plays an important part in our regular lives.The idea of this project is to improvise the normal refrigerfator into a smart one by making it to place order for food items and to create an virtual interactive environment between it and the user. Keywords—Food Ordering, Expiry date reminder .', 'corpus_id': 229331892, 'score': 0}, {'doc_id': '229363863', 'title': ""If This Context Then That Concern: Exploring users' concerns with IFTTT applets"", 'abstract': 'End users are increasingly using trigger-action platforms like, If-This-Then-That (IFTTT) to create applets to connect smart home devices and services. However, there are inherent risks in using such applets—even non-malicious ones—as sensitive information may leak through their use in certain contexts (e.g., where the device is located, who can observe the resultant action). This work aims to understand how well end users can assess this risk. We do so by exploring users’ concerns with using IFTTT applets and more importantly if and how those concerns change based on different contextual factors. Through a Mechanical Turk survey of 386 participants on 49 smart-home IFTTT applets, we found that nudging the participants to think about different usage contexts led them to think deeper about the associated risks and raise their concerns. Qualitative analysis reveals that participants had a nuanced understanding of contextual factors and how these factors could lead to leakage of sensitive data and allow unauthorized access to applets and data.', 'corpus_id': 229363863, 'score': 0}]"
86	{'doc_id': '231709464', 'title': 'Revisiting Locally Supervised Learning: an Alternative to End-to-end Training', 'abstract': 'Due to the need to store the intermediate activations for back-propagation, end-toend (E2E) training of deep networks usually suffers from high GPUs memory footprint. This paper aims to address this problem by revisiting the locally supervised learning, where a network is split into gradient-isolated modules and trained with local supervision. We experimentally show that simply training local modules with E2E loss tends to collapse task-relevant information at early layers, and hence hurts the performance of the full model. To avoid this issue, we propose an information propagation (InfoPro) loss, which encourages local modules to preserve as much useful information as possible, while progressively discard task-irrelevant information. As InfoPro loss is difficult to compute in its original form, we derive a feasible upper bound as a surrogate optimization objective, yielding a simple but effective algorithm. In fact, we show that the proposed method boils down to minimizing the combination of a reconstruction loss and a normal cross-entropy/contrastive term. Extensive empirical results on five datasets (i.e., CIFAR, SVHN, STL-10, ImageNet and Cityscapes) validate that InfoPro is capable of achieving competitive performance with less than 40% memory footprint compared to E2E training, while allowing using training data with higher-resolution or larger batch sizes under the same GPU memory constraint. Our method also enables training local modules asynchronously for potential training acceleration. Code is available at: https://github.com/blackfeather-wang/InfoPro-Pytorch.', 'corpus_id': 231709464}	12718	[{'doc_id': '237532494', 'title': 'Exploiting Activation based Gradient Output Sparsity to Accelerate Backpropagation in CNNs', 'abstract': 'Machine/deep-learning (ML/DL) based techniques are emerging as a driving force behind many cutting-edge technologies, achieving high accuracy on computer vision workloads such as image classification and object detection. However, training these models involving large parameters is both time-consuming and energy-hogging. In this regard, several prior works have advocated for sparsity to speed up the of DL training and more so, the inference phase. This work begins with the observation that during training, sparsity in the forward and backward passes are correlated. In that context, we investigate two types of sparsity (input and output type) inherent in gradient descent-based optimization algorithms and propose a hardware micro-architecture to leverage the same. Our experimental results use five state-of-the-art CNN models on the Imagenet dataset, and show back propagation speedups in the range of 1.69× to 5.43×, compared to the dense baseline execution. By exploiting sparsity in both the forward and backward passes, speedup improvements range from 1.68× to 3.30× over the sparsity-agnostic baseline execution. Our work also achieves significant reduction in training iteration time over several previously proposed dense as well as sparse accelerator based platforms, in addition to achieving order of magnitude energy efficiency improvements over GPU based execution.', 'corpus_id': 237532494, 'score': 1}, {'doc_id': '231847094', 'title': 'Learning N: M Fine-grained Structured Sparse Neural Networks From Scratch', 'abstract': 'Sparsity in Deep Neural Networks (DNNs) has been widely studied to compress and accelerate the models on resource-constrained environments. It can be generally categorized into unstructured fine-grained sparsity that zeroes out multiple individual weights distributed across the neural network, and structured coarse-grained sparsity which prunes blocks of sub-networks of a neural network. Fine-grained sparsity can achieve a high compression ratio but is not hardware friendly and hence receives limited speed gains. On the other hand, coarse-grained sparsity cannot simultaneously achieve both apparent acceleration on modern GPUs and decent performance. In this paper, we are the first to study training from scratch an N:M fine-grained structured sparse network, which can maintain the advantages of both unstructured fine-grained sparsity and structured coarse-grained sparsity simultaneously on specifically designed GPUs. Specifically, a 2 : 4 sparse network could achieve 2× speed-up without performance drop on Nvidia A100 GPUs. Furthermore, we propose a novel and effective ingredient, sparse-refined straight-through estimator (SR-STE), to alleviate the negative influence of the approximated gradients computed by vanilla STE during optimization. We also define a metric, Sparse Architecture Divergence (SAD), to measure the sparse network’s topology change during the training process. Finally, We justify SR-STE’s advantages with SAD and demonstrate the effectiveness of SR-STE by performing comprehensive experiments on various tasks. Anonymous code and model will be at available at https://github.com/anonymous-NM-sparsity/NM-sparsity.', 'corpus_id': 231847094, 'score': 1}, {'doc_id': '232290660', 'title': 'Toward Compact Deep Neural Networks via Energy-Aware Pruning', 'abstract': 'Despite of the remarkable performance, modern deep neural networks are inevitably accompanied with a significant amount of computational cost for learning and deployment, which may be incompatible with their usage on edge devices. Recent efforts to reduce these overheads involves pruning and decomposing the parameters of various layers without performance deterioration. Inspired by several decomposition studies, in this paper, we propose a novel energy-aware pruning method that quantifies the importance of each filter in the network using nuclear-norm (NN). Proposed energy-aware pruning leads to state-of-the art performance for Top-1 accuracy, FLOPs, and parameter reduction across a wide range of scenarios with multiple network architectures on CIFAR-10 and ImageNet after fine-grained classification tasks. On toy experiment, despite of no fine-tuning, we can visually observe that NN not only has little change in decision boundaries across classes, but also clearly outperforms previous popular criteria. We achieve competitive results with 40.4/49.8% of FLOPs and 45.9/52.9% of parameter reduction with 94.13/94.61% in the Top-1 accuracy with ResNet-56/110 on CIFAR-10, respectively. In addition, our observations are consistent for a variety of different pruning setting in terms of data size as well as data quality which can be emphasized in the stability of the acceleration and compression with negligible accuracy loss. Our code is available at https://github.com/ nota-github/nota-pruning-rank. Compute NN', 'corpus_id': 232290660, 'score': 0}, {'doc_id': '231934142', 'title': 'Accelerated Sparse Neural Training: A Provable and Efficient Method to Find N: M Transposable Masks', 'abstract': 'Unstructured pruning reduces the memory footprint in deep neural networks (DNNs). Recently, researchers proposed different types of structural pruning intending to reduce also the computation complexity. In this work, we first suggest a new measure called mask-diversity which correlates with the expected accuracy of the different types of structural pruning. We focus on the recently suggested N : M fine-grained block sparsity mask, in which for each block of M weights, we have at least N zeros. While N :M fine-grained block sparsity allows acceleration in actual modern hardware, it can be used only to accelerate the inference phase. In order to allow for similar accelerations in the training phase, we suggest a novel transposable fine-grained sparsity mask, where the same mask can be used for both forward and backward passes. Our transposable mask guarantees that both the weight matrix and its transpose follow the same sparsity pattern; thus, the matrix multiplication required for passing the error backward can also be accelerated. We formulate the problem of finding the optimal transposable-mask as a minimum-cost flow problem. Additionally, to speed up the minimum-cost flow computation, we also introduce a fast linear-time approximation that can be used when the masks dynamically change during training. Our experiments suggest a 2x speed-up in the matrix multiplications with no accuracy degradation over vision and language models. Finally, to solve the problem of switching between different structure constraints, we suggest a method to convert a pre-trained model with unstructured sparsity to an N :M fine-grained block sparsity model with little to no training. A reference implementation can be found at https: //github.com/papers-submission/structured_transposable_masks.', 'corpus_id': 231934142, 'score': 1}, {'doc_id': '231879922', 'title': 'High-Performance Large-Scale Image Recognition Without Normalization', 'abstract': 'Batch normalization is a key component of most image classification models, but it has many undesirable properties stemming from its dependence on the batch size and interactions between examples. Although recent work has succeeded in training deep ResNets without normalization layers, these models do not match the test accuracies of the best batch-normalized networks, and are often unstable for large learning rates or strong data augmentations. In this work, we develop an adaptive gradient clipping technique which overcomes these instabilities, and design a significantly improved class of Normalizer-Free ResNets. Our smaller models match the test accuracy of an EfficientNet-B7 on ImageNet while being up to 8.7× faster to train, and our largest models attain a new state-of-the-art top-1 accuracy of 86.5%. In addition, Normalizer-Free models attain significantly better performance than their batch-normalized counterparts when finetuning on ImageNet after large-scale pre-training on a dataset of 300 million labeled images, with our best models obtaining an accuracy of 89.2%.2', 'corpus_id': 231879922, 'score': 1}, {'doc_id': '231740811', 'title': 'Zen-NAS: A Zero-Shot NAS for High-Performance Deep Image Recognition', 'abstract': 'Accuracy predictor is a key component in Neural Architecture Search (NAS) for ranking architectures. Building a high-quality accuracy predictor usually costs enormous computation. To address this issue, instead of using an accuracy predictor, we propose a novel zero-shot index dubbed Zen-Score to rank the architectures. The Zen-Score represents the network expressivity and positively correlates with the model accuracy. The calculation of Zen-Score only takes a few forward inferences through a randomly initialized network, without training network parameters. Built upon the Zen-Score, we further propose a new NAS algorithm, termed as Zen-NAS, by maximizing the Zen-Score of the target network under given inference budgets. Within less than half GPU day, Zen-NAS is able to directly search high performance architectures in a data-free style. Comparing with previous NAS methods, the proposed Zen-NAS is magnitude times faster on multiple server-side and mobile-side GPU platforms with state-of-the-art accuracy on ImageNet. Searching and training code as well as pre-trained models are available from https://github.com/ idstcv/ZenNAS. *Accepted by ICCV 2021. Author home page https://minglin-home.github.io 1 ar X iv :2 10 2. 01 06 3v 4 [ cs .C V ] 2 3 A ug 2 02 1', 'corpus_id': 231740811, 'score': 0}, {'doc_id': '219305108', 'title': 'Towards Lower Bit Multiplication for Convolutional Neural Network Training', 'abstract': 'Convolutional Neural Networks (CNNs) have been widely used in many fields. However, the training process costs much energy and time, in which the convolution operations consume the major part. In this paper, we propose a fixed-point training framework, in order to reduce the data bit-width for the convolution multiplications. Firstly, we propose two constrained group-wise scaling methods that can be implemented with low hardware cost. Secondly, to overcome the challenge of trading off overflow and rounding error, a shiftable fixed-point data format is used in this framework. Finally, we propose a double-width deployment technique to boost inference performance with the same bit-width hardware multiplier. The experimental results show that the input data of convolution in the training process can be quantized to 2-bit for CIFAR-10 dataset, 6-bit for ImageNet dataset, with negligible accuracy degradation. Furthermore, our fixed-point train-ing framework has the potential to save at least 75% energy of the computation in the training process.', 'corpus_id': 219305108, 'score': 1}, {'doc_id': '231699188', 'title': 'Pruning and Quantization for Deep Neural Network Acceleration: A Survey', 'abstract': 'Abstract Deep neural networks have been applied in many applications exhibiting extraordinary abilities in the field of computer vision. However, complex network architectures challenge efficient real-time deployment and require significant computation resources and energy costs. These challenges can be overcome through optimizations such as network compression. Network compression can often be realized with little loss of accuracy. In some cases accuracy may even improve. This paper provides a survey on two types of network compression: pruning and quantization. Pruning can be categorized as static if it is performed offline or dynamic if it is performed at run-time. We compare pruning techniques and describe criteria used to remove redundant computations. We discuss trade-offs in element-wise, channel-wise, shape-wise, filter-wise, layer-wise and even network-wise pruning. Quantization reduces computations by reducing the precision of the datatype. Weights, biases, and activations may be quantized typically to 8-bit integers although lower bit width implementations are also discussed including binary neural networks. Both pruning and quantization can be used independently or combined. We compare current techniques, analyze their strengths and weaknesses, present compressed network accuracy results on a number of frameworks, and provide practical guidance for compressing networks.', 'corpus_id': 231699188, 'score': 0}]
87	{'doc_id': '219310485', 'title': 'COVID-19 and digital disruption in UK universities: afflictions and affordances of emergency online migration', 'abstract': 'COVID-19 has caused the closure of university campuses around the world and migration of all learning, teaching, and assessment into online domains. The impacts of this on the academic community as frontline providers of higher education are profound. In this article, we report the findings from a survey of n \u2009=\u20091148 academics working in universities in the United Kingdom (UK) and representing all the major disciplines and career hierarchy. Respondents report an abundance of what we call ‘afflictions’ exacted upon their role as educators and in far fewer yet no less visible ways ‘affordances’ derived from their rapid transition to online provision and early ‘entry-level’ use of digital pedagogies. Overall, they suggest that online migration is engendering significant dysfunctionality and disturbance to their pedagogical roles and their personal lives. They also signpost online migration as a major challenge for student recruitment, market sustainability, an academic labour-market, and local economies.', 'corpus_id': 219310485}	14718	"[{'doc_id': '231941188', 'title': 'Digital Teaching in Times of Covid-19: Risks and Opportunities', 'abstract': 'COVID-19: It started in one place in January 2020 and has since reached the whole world. The global pandemic has been spreading and changing our lives since. The COVID-19 crisis has also changed many things within the world of higher education. In-person teaching was no longer possible; instead, almost all courses were offered in digital formats. This sudden change poses enormous challenges for universities, students, and teachers. This paper discusses the advantages, disadvantages, and opportunities offered by digital teaching. Based on central assumptions of the ‘second digital divide,’ it examines whether certain groups of higher education students are more affected by the switch to digital teaching than others. Findings from national and international studies were used, as well as a survey from the University of Marburg (Germany). They show that there is a relationship between various socio-demographic factors and the evaluation of digital teaching. For example, university students with highly educated parents more often rate digital courses as a good substitute for face-to-face teaching than students with less educated parents. A brief overview highlights the problems faced by teachers in the transition to digital teaching. This paper ends with a discussion of the opportunities that arise from the digitalization of teaching and the wishes of students and teachers with regard to future teaching.', 'corpus_id': 231941188, 'score': 1}, {'doc_id': '219747101', 'title': 'Science Teacher Education in the Times of the COVID-19 Pandemic', 'abstract': 'We write this editorial at a time when there are more than 4 million cases of COVID-19 globally, which includes over a million cases in the United States. The pandemic has caused more than 280,000 ...', 'corpus_id': 219747101, 'score': 1}, {'doc_id': '231721439', 'title': 'Status of tertiary level online class in Bangladesh: students’ response on preparedness, participation and classroom activities', 'abstract': 'In the era of technology, every time the world confronts any kind of crisis or challenge, we use technology as a weapon. Like other emergencies, as COVID-19 is announced as a pandemic, all countries have started trying to control the situation with technological advancement in the medical sector, educational progress, and in the continuity of productions. As most of the educational institutions have been closed since March and the learning process in higher education has moved online, therefore, developing countries like Bangladesh are also trying to continue classes through the online platform with a lack of technological resources, readiness, and inclusiveness from the perspective of the students. This quantitative study surveyed over 844 students of different universities of Bangladesh to analyze the status of preparedness, participation, and classroom activities through online during the pandemic. The findings revealed a lack of preparedness, participation, and less scope of classroom activities through online learning. Problems of infeasible consistency of the internet and electricity, paying attention, understanding lessons through the online platform are the main constraints of online learning in the developing country. Finding ways of mitigating these problems can be the next subject for further researchers.', 'corpus_id': 231721439, 'score': 0}, {'doc_id': '229382696', 'title': 'Creating Innovative Structures in Workplace and Vocational Digital Learning to Ensure Social Distancing', 'abstract': 'The COVID-19 pandemic has influenced learning, including Vocational Education and Training (VET) and workplace learning in companies. Many people with special needs (social or disabilities) have not benefited from the elearning systems used in this period, and emphasized the fact that digital innovations are necessary in all types of education. This paper highlights the importance of disruptive digital innovations in education, such as personalized e-learning and e-mentors, and presents examples of structures and social measures, which can be developed around improving learning during the COVID-19 crisis to ensure social distancing. Keywords-COVID-19; workplace learning; vocational education and training; disruptive innovation.', 'corpus_id': 229382696, 'score': 0}, {'doc_id': '231690009', 'title': 'Threat or opportunity? A case study of digital-enabled redesign of entrepreneurship education in the COVID-19 emergency', 'abstract': '\n The COVID-19 crisis has forced universities worldwide to seek urgent solutions to reconfigure traditional education programs for distance learning. The transformation process faces a number of complexities deriving from both institutional and contextual factors. It may generate threats and as well as opportunities to enhance the education system and prepare for potential future emergencies. In this article, we adopted a combined research approach to describe the experience of the\xa0 Contamination Lab\xa0of the University of Salento (CLab@Salento), an entrepreneurship education program focused on innovative and technology-based entrepreneurship for university students. Moving from the analysis of the main challenges the pandemic generated for the institution, students and faculty, we illustrate the process of redesigning the entrepreneurial learning program by leveraging digital technologies. We show a new approach to entrepreneurial storytelling, pitching and business planning and development through digital technologies. We also report the outcomes of a student survey to highlight the strengths of the redesigned program and some weaknesses, especially associated with digital technologies’ limitations in education, which represent areas for future improvement. The study contributes at theory level with a new discussion on digital-supported entrepreneurship education. At practitioner level, it offers insights on redesigning traditional university programs to effectively address emergencies.\n', 'corpus_id': 231690009, 'score': 0}, {'doc_id': '231775043', 'title': 'A Critical Review by Teachers on the Online Teaching-Learning during the COVID-19', 'abstract': 'The world has witnessed a sudden change in the teaching-learning processes due to the ongoing pandemic of COVID-19. The worldwide compulsive lockdown for ensuring the preventive measures to stop the spread of this infection has equally affected education sector as other business sectors. As all of us know that quality education is the only long-term rescue for all the challenges and therefore, the need to find out the alternative solution to the traditional classroom teachinglearning is the concern of all stakeholders and the only option found is online mode of teaching-learning, which was somehow already available and had attracted an intense attention during this period. The aim of the paper is to study the teacher’s perspective in India about this mode of learning, challenges and issues faced by them in migration to online platform, experience about online tools/platforms used for instructional delivery and their suggestions to improve the process for effective teaching. This study will help in gaining insight towards the possible improvements in the ongoing mode of online teaching and in future situations also. The results obtained based on sample collection through web based questionnaire clearly gives some information, which could be an eye opener for enhancing the implementation of the online teachinglearning among the learners especially teachers, who can further help in implementation of the large. Although, the online mode was already in place and was utilized in blended form to a substantial level in the developed countries, but in developing countries like India, where teachers are not familiar with online platforms/tools, lack of knowledge and skills to handle the online ICT infrastructure in a challenging situation. The results also give an impression about the need of professional development with special focus on digital literacy skills and awareness among the teacher community about the merits of online platforms for the teaching-learning process.', 'corpus_id': 231775043, 'score': 1}, {'doc_id': '231784860', 'title': 'The Use of Technology to Continue Learning in Palestine Disrupted with COVID-19', 'abstract': ""This qualitative study examined how decision-makers and teachers have responded to offer education for all Palestinian students at the immediate onset of the COVID-19 outbreak and how technology is being used to continue education online. Semi-structured interviews were conducted with 20 participants from parents, teachers and decision-makers in Palestine. Interview transcripts were coded using a grounded theory design with a constant comparative method. The findings show that participants identified that technologies such as mobile devices, social media and cloud computing would be useful for design and delivery of educational materials as well as raising safety awareness, and communication during the COVID-19 pandemic in Palestine. The findings also identify various challenges including the widening of the education's digital divide and an increasingly negative attitude towards online education. The data also indicate that the first wave of the COVID-19 experience could be the roadmap for wave two and for the transition to sustainable online learning as a supplement to the traditional learning methods and not as a replacement. This research further demonstrates that teachers who are early adopters have a significant role in influencing both students and other teachers to adopt the transformation to online learning. In addition, the national and international initiatives with a multi-stakeholder partnership could provide sustained, long-term, real solutions for online learning."", 'corpus_id': 231784860, 'score': 1}, {'doc_id': '231639768', 'title': 'Rethinking community participation in education post Covid-19', 'abstract': 'Covid-19 has put education in a challenging place. With millions of children out of school, education needs to reinvent itself. During the pandemic, communities have used this opportunity to support children’s education in various ways, including the launching of new online classes. This article takes the social capital theoretical model as its framework and applies it to help communities strengthen their education support systems. It also looks at models from the literature as best practices to operationalize social capital. The article highlights various models for community participation that could continue after the Covid-19 pandemic. It urges the school system to be flexible and incorporate community-driven parental engagement with project-based and experiential learning. These community-driven educational programs must be supported to provide much-needed place-based supplementary education opportunities to students.', 'corpus_id': 231639768, 'score': 0}, {'doc_id': '215803166', 'title': 'The Impact of the Covid‐19 Pandemic on Current Anatomy Education and Future Careers: A Student’s Perspective', 'abstract': 'As both an anatomy student and anatomy demonstrator, Anatomical Sciences Education has become my go-to journal to further my knowledge of the latest developments in anatomy education, innovations in pedagogical practice, and indeed perspectives on current affairs in the field.', 'corpus_id': 215803166, 'score': 1}, {'doc_id': '232134635', 'title': 'Towards a hybrid ecosystem of blended learning within university contexts', 'abstract': 'The socio-health emergency of covid-19 has affected the use of distance learning The massive use of technologies and digital media in higher education has profoundly revolutionized the perceptions of the actors involved and creating innovations within practices that are already in use Starting from the didactic experiences in digital learning environments that have already been tested in recent years, it was possible to summarize in a single model the trajectories of future experimentation in e-learning for universities The purpose of this article is in fact to propose an overall ecosystem of blended learning for university teaching, within the new phase of resumption of activities in the post-covid world The key elements brought into play by the pandemic concern training, i e , (1) users, (2) market rules and (3) didactics, further modified in relation to time, place, technology and teaching content (for practical purposes, we will call these last four categories extrinsic characteristics of the educational process), which are affected by the contextual needs that have emerged The result is a proposal for the application of a hybrid ecosystem of higher university education © 2021 Copyright for this paper by its authors Use permitted under Creative Commons License Attribution 4 0 International (CC BY 4 0)', 'corpus_id': 232134635, 'score': 0}]"
88	{'doc_id': '229923038', 'title': 'LOCUS: A Multi-Sensor Lidar-Centric Solution for High-Precision Odometry and 3D Mapping in Real-Time', 'abstract': 'A reliable odometry source is a prerequisite to enable complex autonomy behaviour in next-generation robots operating in extreme environments. In this work, we present a high-precision lidar odometry system to achieve robust and real-time operation under challenging perceptual conditions. LOCUS (Lidar Odometry for Consistent operation in Uncertain Settings), provides an accurate multi-stage scan matching unit equipped with an health-aware sensor integration module for seamless fusion of additional sensing modalities. We evaluate the performance of the proposed system against state-of-the-art techniques in perceptually challenging environments, and demonstrate top-class localization accuracy along with substantial improvements in robustness to sensor failures. We then demonstrate real-time performance of LOCUS on various types of robotic mobility platforms involved in the autonomous exploration of the Satsop power plant in Elma, WA where the proposed system was a key element of the CoSTAR team’s solution that won first place in the Urban Circuit of the DARPA Subterranean Challenge.', 'corpus_id': 229923038}	2533	"[{'doc_id': '237445307', 'title': 'Depth-Camera-Aided Inertial Navigation Utilizing Directional Constraints', 'abstract': 'This paper presents a practical yet effective solution for integrating an RGB-D camera and an inertial sensor to handle the depth dropouts that frequently happen in outdoor environments, due to the short detection range and sunlight interference. In depth drop conditions, only the partial 5-degrees-of-freedom pose information (attitude and position with an unknown scale) is available from the RGB-D sensor. To enable continuous fusion with the inertial solutions, the scale ambiguous position is cast into a directional constraint of the vehicle motion, which is, in essence, an epipolar constraint in multi-view geometry. Unlike other visual navigation approaches, this can effectively reduce the drift in the inertial solutions without delay or under small parallax motion. If a depth image is available, a window-based feature map is maintained to compute the RGB-D odometry, which is then fused with inertial outputs in an extended Kalman filter framework. Flight results from the indoor and outdoor environments, as well as public datasets, demonstrate the improved navigation performance of the proposed approach.', 'corpus_id': 237445307, 'score': 1}, {'doc_id': '216552918', 'title': 'EAO-SLAM: Monocular Semi-Dense Object SLAM Based on Ensemble Data Association', 'abstract': 'Object-level data association and pose estimation play a fundamental role in semantic SLAM, which remain unsolved due to the lack of robust and accurate algorithms. In this work, we propose an ensemble data associate strategy for integrating the parametric and nonparametric statistic tests. By exploiting the nature of different statistics, our method can effectively aggregate the information of different measurements, and thus significantly improve the robustness and accuracy of data association. We then present an accurate object pose estimation framework, in which an outliers-robust centroid and scale estimation algorithm and an object pose initialization algorithm are developed to help improve the optimality of pose estimation results. Furthermore, we build a SLAM system that can generate semi-dense or lightweight object-oriented maps with a monocular camera. Extensive experiments are conducted on three publicly available datasets and a real scenario. The results show that our approach significantly outperforms state-of-the-art techniques in accuracy and robustness. The source code is available on https://github.com/yanmin-wu/EAO-SLAM.', 'corpus_id': 216552918, 'score': 0}, {'doc_id': '218502155', 'title': 'RadarSLAM: Radar based Large-Scale SLAM in All Weathers', 'abstract': 'Numerous Simultaneous Localization and Mapping (SLAM) algorithms have been presented in last decade using different sensor modalities. However, robust SLAM in extreme weather conditions is still an open research problem. In this paper, RadarSLAM, a full radar based graph SLAM system, is proposed for reliable localization and mapping in large-scale environments. It is composed of pose tracking, local mapping, loop closure detection and pose graph optimization, enhanced by novel feature matching and probabilistic point cloud generation on radar images. Extensive experiments are conducted on a public radar dataset and several self-collected radar sequences, demonstrating the state-of-the-art reliability and localization accuracy in various adverse weather conditions, such as dark night, dense fog and heavy snowfall.', 'corpus_id': 218502155, 'score': 0}, {'doc_id': '212736991', 'title': 'An Experimental Evaluation of Robustness and Precision for Long-term LiDAR-based Localization in Highly Changing Environments', 'abstract': 'One of the hardest challenges to face in the development of a non GPS-based localization system for autonomous vehicles is the changes of the environment. LiDAR-based systems typically try to match the last measurements obtained with a previously recorded map of the area. If the existing map is not updated along time, there is a good chance that the measures will not match the environment well enough, causing the vehicle to lose track of its location. In this paper, we present and analyze experimental results regarding the robustness and precision of a map-matching based localization system over a certain period of time in the following three cases: (1) without any update of the initial map, (2) updating the map as the vehicle moves and (3) with map updates that take into account surrounding structures labeled as ""fixed"" which are treated differently. The environment of the tests is a busy parking area, which ensures drastic changes from one day to the next. The precision is obtained by comparing the positions computed using the map with the ones provided by a Real-Time Kinematic GPS system. The experimental results reveal a positioning error of about 6cm which remains stable even after 23 days when using fixed structures on the working area.', 'corpus_id': 212736991, 'score': 0}, {'doc_id': '232069056', 'title': 'Accurate Visual-Inertial SLAM by Feature Re-identification', 'abstract': 'We propose a novel feature re-identification method for real-time visual-inertial SLAM. The front-end module of the state-of-the-art visual-inertial SLAM methods (e.g. visual feature extraction and matching schemes) relies on feature tracks across image frames, which are easily broken in challenging scenarios, resulting in insufficient visual measurement and accumulated error in pose estimation. In this paper, we propose an efficient drift-less SLAM method by reidentifying existing features from a spatial-temporal sensitive sub-global map. The re-identified features over a long time span serve as augmented visual measurements and are incorporated into the optimization module which can gradually decrease the accumulative error in the long run, and further build a drift-less global map in the system. Extensive experiments show that our feature re-identification method is both effective and efficient. Specifically, when combining the feature re-identification with the state-of-the-art SLAM method [11], our method achieves 67.3% and 87.5% absolute translation error reduction with only a small additional computational cost on two public SLAM benchmark DBs: EuRoC and TUM-VI respectively.', 'corpus_id': 232069056, 'score': 1}, {'doc_id': '237383396', 'title': 'DEEPLIO: DEEP LIDAR INERTIAL SENSOR FUSION FOR ODOMETRY ESTIMATION', 'abstract': 'Having a good estimate of the position and orientation of a mobile agent is essential for many application domains such as robotics, autonomous driving, and virtual and augmented reality. In particular, when using LiDAR and IMU sensors as the inputs, most existing methods still use classical filter-based fusion methods to achieve this task. In this work, we propose DeepLIO, a modular, end-to-end learning-based fusion framework for odometry estimation using LiDAR and IMU sensors. For this task, our network learns an appropriate fusion function by considering different modalities of its input latent feature vectors. We also formulate a loss function, where we combine both global and local pose information over an input sequence to improve the accuracy of the network predictions. Furthermore, we design three sub-networks with different modules and architectures derived from DeepLIO to analyze the effect of each sensory input on the task of odometry estimation. Experiments on the benchmark dataset demonstrate that DeepLIO outperforms existing learning-based and model-based methods regarding orientation estimation and shows a marginal position accuracy difference.', 'corpus_id': 237383396, 'score': 1}, {'doc_id': '236482187', 'title': 'PLF-VINS: Real-Time Monocular Visual-Inertial SLAM With Point-Line Fusion and Parallel-Line Fusion', 'abstract': ""This letter presents a real-time monocular visual-inertial simultaneous localization and mapping with point-line fusion and parallel-line fusion. The corner and line features provide plenty of information about object structures. In the 2D image plane, such corner and line features have a positional similarity. The corner feature represents an endpoint of an object's edge, and the line feature represents a straight edge. We propose two novel methods for fusing corner and line features to improve localization accuracy. The first method is for fusing corner and line features. Using the positional similarity of corner and line features, we search the relationship between two features by utilizing the proposed point-line coupled residual. The second method is for fusing parallel 3D lines. First, we search for line features clustered based on a vanishing point. Then, the outliers in the parallel 3D lines are removed using the proposed consistency check during the multi-view line clustering. Finally, the parallel 3D lines are used in the proposed parallel 3D line residual. Experimental results show that real-time localization accuracy is improved when two proposed residuals are integrated with the sliding-window optimization. The proposed PLF-VINS is compared with other state-of-the-art algorithms using the public EuRoC dataset."", 'corpus_id': 236482187, 'score': 1}, {'doc_id': '211259382', 'title': 'Monocular Direct Sparse Localization in a Prior 3D Surfel Map', 'abstract': 'In this paper, we introduce an approach to tracking the pose of a monocular camera in a prior surfel map. By rendering vertex and normal maps from the prior surfel map, the global planar information for the sparse tracked points in the image frame is obtained. The tracked points with and without the global planar information involve both global and local constraints of frames to the system. Our approach formulates all constraints in the form of direct photometric errors within a local window of the frames. The final optimization utilizes these constraints to provide the accurate estimation of global 6-DoF camera poses with the absolute scale. The extensive simulation and real-world experiments demonstrate that our monocular method can provide accurate camera localization results under various conditions.', 'corpus_id': 211259382, 'score': 0}, {'doc_id': '221266431', 'title': 'TORNADO-Net: mulTiview tOtal vaRiatioN semAntic segmentation with Diamond inceptiOn module', 'abstract': 'Semantic segmentation of point clouds is a key component of scene understanding for robotics and autonomous driving. In this paper, we introduce TORNADO-Net - a neural network for 3D LiDAR point cloud semantic segmentation. We incorporate a multi-view (bird-eye and range) projection feature extraction with an encoder-decoder ResNet architecture with a novel diamond context block. Current projection-based methods do not take into account that neighboring points usually belong to the same class. To better utilize this local neighbourhood information and reduce noisy predictions, we introduce a combination of Total Variation, Lovász-Softmax, and Weighted Cross-Entropy losses. We also take advantage of the fact that the LiDAR data encompasses 360◦ field of view and use circular padding. We demonstrate state-of-the-art results on the SemanticKITTI dataset and also provide thorough quantitative evaluations and ablation results.', 'corpus_id': 221266431, 'score': 0}, {'doc_id': '233347061', 'title': 'LVI-SAM: Tightly-coupled Lidar-Visual-Inertial Odometry via Smoothing and Mapping', 'abstract': 'We propose a framework for tightly-coupled lidar-visual-inertial odometry via smoothing and mapping, LVI-SAM, that achieves real-time state estimation and map-building with high accuracy and robustness. LVI-SAM is built atop a factor graph and is composed of two sub-systems: a visual-inertial system (VIS) and a lidar-inertial system (LIS). The two sub-systems are designed in a tightly-coupled manner, in which the VIS leverages LIS estimation to facilitate initialization. The accuracy of the VIS is improved by extracting depth information for visual features using lidar measurements. In turn, the LIS utilizes VIS estimation for initial guesses to support scan-matching. Loop closures are first identified by the VIS and further refined by the LIS. LVI-SAM can also function when one of the two sub-systems fails, which increases its robustness in both texture-less and feature-less environments. LVI-SAM is extensively evaluated on datasets gathered from several platforms over a variety of scales and environments. Our implementation is available at https://git.io/lvi-sam.', 'corpus_id': 233347061, 'score': 1}]"
89	{'doc_id': '234816606', 'title': 'How Does Sustainability Affect Consumer Choices in the Fashion Industry?', 'abstract': 'The fashion industry being one of the most polluting industries in the world means that it is an industry with an immense potential for change. Consumers are central and are closely intertwined with how companies act. This research reflects consumer perspectives and practices towards the topic of sustainability implemented in the fashion industry. The relevance of sustainability in the fashion industry and the key role of consumers in its implementation are undeniable and confirmed by consumers in a representation of general awareness and concern, despite not always being translated into actual practices. A qualitative research methodology, followed by a set of interviews conducted with consumers, revealed that the great majority are implementing a variety of practices when making their buying choices towards fashion items. Barriers such as lack of education, information, knowledge and transparency were identified, and this aspect was shared by consumers as a reason why they are not motivated to make more conscious decisions. Companies should educate consumers from a general perspective and focus on the group of consumers that are not implementing sustainability in the fashion industry in their buying choices, as they represent the potential for the future.', 'corpus_id': 234816606}	17017	[{'doc_id': '234304207', 'title': 'Practices for garment industry s post-consumer textile waste management in the circular economy context: ananalysis on literature', 'abstract': 'Goal: This study aimed to identify and describe garment’s post-consumer textile waste management practices and to analyse them according to environmental, economic and social criteria in the circular economy context. Design / Methodology / Approach: A literature review was conducted to identify, collect and organize practices from garment’s post-consumer textile waste management and the environmental, economic and social criteria taken into account for the analysis of such practices in a circular context. Results: There were eleven collection practices, three sorting practices, five reuse practices, and six recycling practices. Additionally, even circularity is presented as a new solution to environmental problems, those practices identified in literature are pulled mainly downstream, promoting shortterm waste management approaches, while the initial production chain’s links continue to extract and use several non-renewable resources from the excessive way. Limitations of the investigation: The limitations of a literature review of this nature is the complete reliance on the defined strings to search the previously published research and the adopted procedures to select and evaluate these studies (data base, search period, exclusion criteria) Practical implications: For researchers and garment’s industry professionals, the identified practices should provide new solutions that could be tested in the current post-consumer textile waste management model. Moreover, this research allowed understanding the way those postconsumer textile waste management’s practices are interpreted under a circular context. Originality/Value: there is almost no detailed study of post-consumer textile waste management’s practices. Furthermore, it is very rare to find those textile waste management practices related in a circular context.', 'corpus_id': 234304207, 'score': 0}, {'doc_id': '147022401', 'title': 'Touring the fashion: Branding the city', 'abstract': 'This article aims to explain the process through which the fashion phenomenon creates city representations. The city is thus examined by means of a framework of tourism practices and related consumer patterns. Consequently, fashion is also understood as a consumer approach that influences the production of consumer patterns. Three perspectives reflect the ways in which representations are constructed and emphasise the possibility of reorganising space and creating consumer scripts. Drawing on examples of the representations of Paris and Amsterdam, this article explains variations in representations that are dependent on the fashion phenomenon’s focus, the type of tourist practices and access to tourist activities. Suggestions are provided for further investigation of the interdependency between institutional organisations and their influence on represented cities.', 'corpus_id': 147022401, 'score': 1}, {'doc_id': '214443970', 'title': 'Fashion Exhibitions as Scholarship: Evaluation Criteria for Peer Review', 'abstract': 'Curated exhibitions are places where research practice, creative design, storytelling, and aesthetics converge. In this article, we use the term “fashion exhibition” to refer to the organized display of extant dress-related items within museums or other public spaces. Curation, as a form of creative design research, produces numerous outcomes including museum exhibitions, digital archives, and associated publications; however, our field has not yet established a method to peer review fashion exhibitions. In this article, we build upon the work of previous scholars to propose criteria for evaluating fashion exhibitions. In doing so, we aim to elevate the scholarly status of fashion exhibitions, particularly those mounted by modestly funded institutions, and use the recent fashion exhibition, “Women Empowered: Fashions from the Frontline,” as an example to illustrate our argument.', 'corpus_id': 214443970, 'score': 1}, {'doc_id': '233992331', 'title': 'Fashion Designers as Entrepreneurs: Investigating the Perception and Challenges', 'abstract': 'This paper examines fashion entrepreneurship critically from a designer’s perspective. The aim is to identify the different kinds of challenges that fashion designers face in their simultaneous roles as designers and entrepreneurs, along with the potential advantages of that position. The article aims to supplement existing research on micro-scale fashion design businesses that has focused primarily on the encountered challenges. A total of 30 entrepreneurial fashion designers were interviewed. Thematic analysis was used in order to identify patterns in the diverse experiences of the entrepreneurial designers, and to conceptualize them into concise themes and subcategories. Another purpose of the study is to bring a broader understanding of the designer’s viewpoint, and thus it highlights also personal perspectives and motivations behind fashion entrepreneurship. The results show that entrepreneurial fashion designers have to have multilevel managerial skills to run their business. Besides another important factor for success is the creating of balance between the business and private life. This study identifies fashion companies as businesses where creativity is a successful combination of fashion creativity and entrepreneurial creativity. Furthermore, the study shows that the obstacle for business growth might be the designer’s business orientation rather than the lack of investors. Therefore, it is crucial to recognize the different drivers behind the business, acknowledge the importance of intrinsic values (e.g. aesthetic and creative aspects), and allow them to shape the business.', 'corpus_id': 233992331, 'score': 1}, {'doc_id': '214212672', 'title': 'Book review: Fabricating Transnational Capitalism: A Collaborative Ethnography of Italian-Chinese Global Fashion', 'abstract': None, 'corpus_id': 214212672, 'score': 1}, {'doc_id': '234313430', 'title': 'Redesigning of fashion supply chain', 'abstract': 'Abstract The fashion industry has been subject of pivotal trends over the past few decades. The industry has evolved into a complex, fragmented, global system which at its very core is based on the notion of continual consumption of the “new” and discard the old. The emergence of the “fast fashion” business model has increased the introduction of trends leading to premature product replacement and fashion obsolescence. It also has major negative environmental and social impacts, particularly on those at the bottom of the supply chain. Fashion in the 21st century is typically fast fashion, characterized by mass production, high turnover, and goods designed for a short lifespan. Like a kimono appears to be the antithesis of fast fashion in terms of production and consumption. A kimono takes time to create and usually has a long lifespan. However, in the current global fashion market, there has been a growing trend towards slow fashion, which involves longer production times, use of local materials, and a focus on quality and sustainability. Millennials nowadays are found to be more fashion-conscious, and relate themselves to the fashion brands they wear.', 'corpus_id': 234313430, 'score': 0}, {'doc_id': '195395854', 'title': 'Human Factor in Apparel and Fashion Exhibition Design', 'abstract': 'The need to preserve past cultures, textile and clothing creations, identities of a society in a certain time period, made the development of exhibition design area relevant. Exhibition spaces must give innovative answers to the aspirations of the society, which are the visitors, reason of being of its existence. In this framework, human factor is essential in the exhibition design field.', 'corpus_id': 195395854, 'score': 1}, {'doc_id': '234101778', 'title': 'Waste management strategies in fashion and textiles industry: Challenges are in governance, materials culture and design-centric', 'abstract': 'Abstract This paper aims to provide further insight into the applications and spillover of the circular economy into the fashion system. Through the systemic analysis of case histories, the research evaluates the effects of 40 circular economy actions in their relationship with sustainable development goals, by assessing how they have been able to integrate and balance the economic, social, and environmentally sustainable development dimensions into the fashion system. What emerges is that the fashion industry can be a potent ground for the implementation of a circular economy’s principle and could also provide support in understanding its evolution and adjusting its objectives accordingly. Fashion is strategic and could be a perfect field for testing a new approach to raw material and waste and for the development of a new context of the inquiry, defined as “Circular Economy for fashion.” The focus of this chapter will be various approaches to reduce waste such as the 3R approach (reduce, recycle, and reuse) for fashion wastes; donations to charity; upcycle, rent the runway; and benefits of all the newer approaches including the relevance of circular economy and sustainable development goals. This chapter deals with many subjects of sustainability from evolutionary aspects to developmental aspects. The impacts of the textile and garment industry waste generation on the environment and human well-being have been discussed. Finally, a framework to reduce waste and circular fashion future is proposed.', 'corpus_id': 234101778, 'score': 0}, {'doc_id': '234873429', 'title': 'Virtual carbon and water flows embodied in denim trade', 'abstract': 'Abstract The environmental impacts of the fashion industry have been aroused wide concerns. The globalization and fragmentation of the textile and fashion system have led to the uneven distribution of environmental consequences. As denim is the fabric of jeans that is representative of fashion, this study assessed virtual carbon and water flows embodied in the global denim-product trade, and footprints of denim production were quantified by life-cycle assessment and water footprint assessment. Results indicated that virtual carbon embodied in the global denim trade increased obviously from 14.8 Mt CO2e in 2001 to 16.0 Mt CO2e in 2018, and the virtual water consumption dropped from 5.6 billion m3 to 4.7 billion m3 from 2001 to 2018. The denim fabric production and cotton fibre production respectively contribute the most of the carbon emissions and water consumption. Polyester blended denim has 5% larger carbon footprint and 72% lower water footprint than cotton denim, and contributes to increasing embodied carbon emissions (from 4% in 2001 to 43% in 2018). Increasing the utilization of polyester blended denim would save water but face more pressures on carbon emission reduction. In the past two decades, virtual carbon and water flows embodied in the global denim trade are relocating, main jean consumers (i.e., the USA, EU-15, and Japan) withdraw the denim manufacturing supply chain and developing countries (i.e., China, India, and Pakistan) with higher carbon and water footprint undertake main global denim production, facing increasing climate-related risks and water crisis. The global South cooperation helps share successful experiences, save production cost, and lessen resource consumption and environmental emissions. The production and consumption of denim should be shifted to circular and sustainable ways and new business models are required. The analysis framework can provide the basis for exploring environmental flows of product-level trade, and results can offer a basis for environmental policies and control strategies of the fashion industry, and as well as the sustainable production and consumption of garment.', 'corpus_id': 234873429, 'score': 0}, {'doc_id': '233440053', 'title': 'Insufficiency for the unregistered protection for the fashion designs under US legislation', 'abstract': 'This article is devoted to a comprehensive study of the issues and specifics of the protection for the unregistered fashion designs in the US. The essence and content of the legal nature of unregistered fashion designs is determined. Author states, that one of the main reasons why clothing design calls for more protection nowadays than before is a result of a fact that fashion designs can be copied so easily these days that sometimes they can reach retailor shops before the original products will be available for the consumers. Counterfeit goods are usually created as a simple copy of the original design without any input, so the time consumption is not as high as during the creation of the original design because there is no need in creating something new, thus the innovative process decreases. As well, in order to sell counterfeited products at a lower price point than original one usually cheap labour is used and working hours are limited in order not to pay more for the production. It is mistakenly concluded that counterfeit products and original models cannot compete because from the very beginning they were made for different markets. Nowadays some designs are so well imitated that sometimes it is impossible to make a distinction. Thus, for some consumers who can afford original product there is a choice either to buy a good copy but cheaper or to buy a product from an authorised boutique. As long as this choice exist it is impossible to claim that counterfeits do not cause any damages for fashion industry. By choosing the counterfeit products consumers make a demand for a future production of it and this circle is growing rapidly. The issue of formation and current state of the legal provisions for the protection of the unregistered fashion designs in US is determined and analyzed. The case law is analyzed. On the basis of the following research, ways of improving the legislation of the US in this field are proposed. The key words: fashion design, separability doctrine, copyright, counterfeit.', 'corpus_id': 233440053, 'score': 0}]
90	{'doc_id': '221761685', 'title': 'Self-Supervised Meta-Learning for Few-Shot Natural Language Classification Tasks', 'abstract': 'Self-supervised pre-training of transformer models has revolutionized NLP applications. Such pre-training with language modeling objectives provides a useful initial point for parameters that generalize well to new tasks with fine-tuning. However, fine-tuning is still data inefficient -- when there are few labeled examples, accuracy can be low. Data efficiency can be improved by optimizing pre-training directly for future fine-tuning with few examples; this can be treated as a meta-learning problem. However, standard meta-learning techniques require many training tasks in order to generalize; unfortunately, finding a diverse set of such supervised tasks is usually difficult. This paper proposes a self-supervised approach to generate a large, rich, meta-learning task distribution from unlabeled text. This is achieved using a cloze-style objective, but creating separate multi-class classification tasks by gathering tokens-to-be blanked from among only a handful of vocabulary terms. This yields as many unique meta-training tasks as the number of subsets of vocabulary terms. We meta-train a transformer model on this distribution of tasks using a recent meta-learning framework. On 17 NLP tasks, we show that this meta-training leads to better few-shot generalization than language-model pre-training followed by finetuning. Furthermore, we show how the self-supervised tasks can be combined with supervised tasks for meta-learning, providing substantial accuracy gains over previous supervised meta-learning.', 'corpus_id': 221761685}	13742	"[{'doc_id': '231740610', 'title': 'Can We Automate Scientific Reviewing?', 'abstract': 'The rapid development of science and technology has been accompanied by an exponential growth in peer-reviewed scientific publications. At the same time, the review of each paper is a laborious process that must be carried out by subject matter experts. Thus, providing high-quality reviews of this growing number of papers is a significant challenge. In this work, we ask the question “can we automate scientific reviewing?”, discussing the possibility of using state-of-the-art natural language processing (NLP) models to generate first-pass peer reviews for scientific papers. Arguably the most difficult part of this is defining what a “good” review is in the first place, so we first discuss possible evaluation measures for such reviews. We then collect a dataset of papers in the machine learning domain, annotate them with different aspects of content covered in each review, and train targeted ∗Corresponding author. summarization models that take in papers to generate reviews. Comprehensive experimental results show that system-generated reviews tend to touch upon more aspects of the paper than human-written reviews, but the generated text can suffer from lower constructiveness for all aspects except the explanation of the core ideas of the papers, which are largely factually correct. We finally summarize eight challenges in the pursuit of a good review generation system together with potential solutions, which, hopefully, will inspire more future research on this subject. We make all code, and the dataset publicly available: https://github. com/neulab/ReviewAdvisor as well as a ReviewAdvisor system: http://review.nlpedia.ai/ (See demo screenshot in A.2). The review of this paper (without TL;QR section) written by the system of this paper can be found A.1', 'corpus_id': 231740610, 'score': 1}, {'doc_id': '230799347', 'title': 'Did Aristotle Use a Laptop? A Question Answering Benchmark with Implicit Reasoning Strategies', 'abstract': 'Abstract A key limitation in current datasets for multi-hop reasoning is that the required steps for answering the question are mentioned in it explicitly. In this work, we introduce StrategyQA, a question answering (QA) benchmark where the required reasoning steps are implicit in the question, and should be inferred using a strategy. A fundamental challenge in this setup is how to elicit such creative questions from crowdsourcing workers, while covering a broad range of potential strategies. We propose a data collection procedure that combines term-based priming to inspire annotators, careful control over the annotator population, and adversarial filtering for eliminating reasoning shortcuts. Moreover, we annotate each question with (1) a decomposition into reasoning steps for answering it, and (2) Wikipedia paragraphs that contain the answers to each step. Overall, StrategyQA includes 2,780 examples, each consisting of a strategy question, its decomposition, and evidence paragraphs. Analysis shows that questions in StrategyQA are short, topic-diverse, and cover a wide range of strategies. Empirically, we show that humans perform well (87%) on this task, while our best baseline reaches an accuracy of ∼ 66%.', 'corpus_id': 230799347, 'score': 0}, {'doc_id': '231639356', 'title': 'Situation and Behavior Understanding by Trope Detection on Films', 'abstract': 'The human ability of deep cognitive skills is crucial for the development of various real-world applications that process diverse and abundant user generated input. While recent progress of deep learning and natural language processing have enabled learning system to reach human performance on some benchmarks requiring shallow semantics, such human ability still remains challenging for even modern contextual embedding models, as pointed out by many recent studies [9, 10, 22, 24, 32]. Existing machine comprehension datasets assume sentence-level input, lack of casual or motivational inferences, or can be answered with question-answer bias. Here, we present a challenging novel task, trope detection on films, in an effort to create a situation and behavior understanding for machines. Tropes are frequently used storytelling devices for creative works. Comparing to existing movie tag prediction tasks, tropes are more sophisticated as they can vary widely, from a moral concept to a series of circumstances, and embedded with motivations and cause-and-effects. We introduce a new dataset, Tropes in Movie Synopses (TiMoS), with 5623 movie synopses and 95 different tropes collecting from a Wikipedia-style database, TVTropes. We present a multi-stream comprehension network (MulCom) leveraging multi-level attention of words, sentences, and role relations. Experimental result demonstrates that modern models including BERT contextual embedding, movie tag prediction systems, and relational networks, perform at most 37% of human performance (23.97/64.87) in terms of F1 score. Our MulCom outperforms all modern baselines, by 1.5 to 5.0 F1 score and 1.5 to 3.0 mean of average precision (mAP) score. We also provide a detailed analysis and human evaluation to pave ways for future research.', 'corpus_id': 231639356, 'score': 0}, {'doc_id': '222208963', 'title': 'Catch Me if I Can: Detecting Strategic Behaviour in Peer Assessment', 'abstract': 'We consider the issue of strategic behaviour in various peer-assessment tasks, including peer grading of exams or homeworks and peer review in hiring or promotions. When a peer-assessment task is competitive (e.g., when students are graded on a curve), agents may be incentivized to misreport evaluations in order to improve their own final standing. Our focus is on designing methods for detection of such manipulations. Specifically, we consider a setting in which agents evaluate a subset of their peers and output rankings that are later aggregated to form a final ordering. In this paper, we investigate a statistical framework for this problem and design a principled test for detecting strategic behaviour. We prove that our test has strong false alarm guarantees and evaluate its detection ability in practical settings. For this, we design and execute an experiment that elicits strategic behaviour from subjects and release a dataset of patterns of strategic behaviour that may be of independent interest. We then use the collected data to conduct a series of real and semi-synthetic evaluations that demonstrate a strong detection power of our test.', 'corpus_id': 222208963, 'score': 1}, {'doc_id': '211481529', 'title': 'Learning Conversational Web Interfaces', 'abstract': 'Automating user tasks with natural language instructions, such as booking a movie ticket, while keeping them engaged is a nontrivial and open problem. Previous work has focused on a particular scenario where users need to give entire instructions before a task can be handled. Aside from the difficulty of uttering a long instruction, this setup is also less realistic as the instructions could depend on future observations and needs to be delayed. In this work, we introduce the dialogue-based web navigation problem where the objective is to fulfill a hidden user goal by having multi-turn conversations with users and navigating a given web page simultaneously. We study joint learning of dialogue and navigation policies using reinforcement learning with actor-critic method. An architecture where user dialogue and web page observations are attentively encoded into policy actions is developed. We build a novel dialogue-based web environment by wrapping a user simulator and the Fandango movie ticket booking website into a single environment. We evaluate the performance of our models and discuss their biases and shortcomings.', 'corpus_id': 211481529, 'score': 1}, {'doc_id': '231861515', 'title': 'Civil Rephrases Of Toxic Texts With Self-Supervised Transformers', 'abstract': 'Platforms that support online commentary, from social networks to news sites, are increasingly leveraging machine learning to assist their moderation efforts. But this process does not typically provide feedback to the author that would help them contribute according to the community guidelines. This is prohibitively time-consuming for human moderators to do, and computational approaches are still nascent. This work focuses on models that can help suggest rephrasings of toxic comments in a more civil manner. Inspired by recent progress in unpaired sequence-to-sequence tasks, a self-supervised learning model is introduced, called CAE-T5. CAE-T5 employs a pre-trained text-to-text transformer, which is fine tuned with a denoising and cyclic auto-encoder loss. Experimenting with the largest toxicity detection dataset to date (Civil Comments) our model generates sentences that are more fluent and better at preserving the initial content compared to earlier text style transfer systems which we compare with using several scoring systems and human evaluation.', 'corpus_id': 231861515, 'score': 0}, {'doc_id': '1211869', 'title': 'Steering user behavior with badges', 'abstract': ""An increasingly common feature of online communities and social media sites is a mechanism for rewarding user achievements based on a system of badges. Badges are given to users for particular contributions to a site, such as performing a certain number of actions of a given type. They have been employed in many domains, including news sites like the Huffington Post, educational sites like Khan Academy, and knowledge-creation sites like Wikipedia and Stack Overflow. At the most basic level, badges serve as a summary of a user's key accomplishments; however, experience with these sites also shows that users will put in non-trivial amounts of work to achieve particular badges, and as such, badges can act as powerful incentives. Thus far, however, the incentive structures created by badges have not been well understood, making it difficult to deploy badges with an eye toward the incentives they are likely to create.\n In this paper, we study how badges can influence and steer user behavior on a site---leading both to increased participation and to changes in the mix of activities a user pursues on the site. We introduce a formal model for reasoning about user behavior in the presence of badges, and in particular for analyzing the ways in which badges can steer users to change their behavior. To evaluate the main predictions of our model, we study the use of badges and their effects on the widely used Stack Overflow question-answering site, and find evidence that their badges steer behavior in ways closely consistent with the predictions of our model. Finally, we investigate the problem of how to optimally place badges in order to induce particular user behaviors. Several robust design principles emerge from our framework that could potentially aid in the design of incentives for a broad range of sites."", 'corpus_id': 1211869, 'score': 1}, {'doc_id': '227230484', 'title': 'Facts2Story: Controlling Text Generation by Key Facts', 'abstract': 'Recent advancements in self-attention neural network architectures have raised the bar for open-ended text generation. Yet, while current methods are capable of producing a coherent text which is several hundred words long, attaining control over the content that is being generated—as well as evaluating it—are still open questions. We propose a controlled generation task which is based on expanding a sequence of facts, expressed in natural language, into a longer narrative. We introduce human-based evaluation metrics for this task, as well as a method for deriving a large training dataset. We evaluate three methods on this task, based on fine-tuning pre-trained models. We show that while auto-regressive, unidirectional Language Models such as GPT2 produce better fluency, they struggle to adhere to the requested facts. We propose a plan-and-cloze model (using fine-tuned XLNet) which produces competitive fluency while adhering to the requested content.', 'corpus_id': 227230484, 'score': 1}, {'doc_id': '231749675', 'title': 'NBSearch: Semantic Search and Visual Exploration of Computational Notebooks', 'abstract': 'Code search is an important and frequent activity for developers using computational notebooks (e.g., Jupyter). The flexibility of notebooks brings challenges for effective code search, where classic search interfaces for traditional software code may be limited. In this paper, we propose, NBSearch, a novel system that supports semantic code search in notebook collections and interactive visual exploration of search results. NBSearch leverages advanced machine learning models to enable natural language search queries and intuitive visualizations to present complicated intra- and inter-notebook relationships in the returned results. We developed NBSearch through an iterative participatory design process with two experts from a large software company. We evaluated the models with a series of experiments and the whole system with a controlled user study. The results indicate the feasibility of our analytical pipeline and the effectiveness of NBSearch to support code search in large notebook collections.', 'corpus_id': 231749675, 'score': 0}]"
91	{'doc_id': '220962832', 'title': 'Promising effects of tocilizumab in COVID-19: A non-controlled, prospective clinical trial', 'abstract': '\n               Abstract\n               \n                  Background\n                  The clinical presentation of SARS-CoV-2 infection ranges from mild symptoms to severe complications, including acute respiratory distress syndrome. In this syndrome, inflammatory cytokines are released after activation of the inflammatory cascade, with the predominant role of interleukin (IL)-6. The aim of this study was to evaluate the effects of tocilizumab, as an IL-6 antagonist, in patients with severe or critical SARS-CoV-2 infection.\n               \n               \n                  Methods\n                  In this prospective clinical trial, 76 patients with severe or critical SARS-CoV-2 infection were evaluated for eligibility, and ultimately, 42 patients were included. Tocilizumab was administered at a dose of 400 mg as a single dose via intravenous infusion. Primary outcomes included changes in oxygenation support, need for invasive mechanical ventilation, and death. Secondary outcomes included radiological changes in the lungs, IL-6 plasma levels, C-reactive protein levels, and adverse drug reactions. The data were analyzed using SPSS software.\n               \n               \n                  Results\n                  Of the 42 included patients, 20 (48%) patients presented the severe infection stage and 22 (52%) were in the critical stage. The median age of patients was 56 years, and the median IL-6 level was 28.55 pg/mL. After tocilizumab administration, only 6 patients (14%) required invasive ventilation. Additionally, 35 patients (83.33%) showed clinical improvement. By day 28, a total of 7 patients died (6 patients in the critical stage and 1 patient in the severe stage). Neurological adverse effects were observed in 3 patients.\n               \n               \n                  Conclusions\n                  Based on the current results, tocilizumab may be a promising agent for patients with severe or critical SARS-CoV-2 infection, if promptly initiated during the severe stage.\n               \n            ', 'corpus_id': 220962832}	5596	"[{'doc_id': '218571559', 'title': 'COVID-19: Clinical course and outcomes of 36 maintenance hemodialysis patients from a single center in Spain.', 'abstract': '\n \n SARS-CoV-2-pneumonia emerged in Wuhan, China in December 2019. Unfortunately, there is lack of evidence about the optimal management of novel coronavirus disease 2019 (COVID-19), even less in patients on maintenance hemodialysis (MHD) therapy than in the general population. In this retrospective observational single-center study we analyzed the clinical course and outcomes of all MHD patients hospitalized with COVID-19 from March 12th to April 10th, 2020 as confirmed by real time polymerase chain reaction. Baseline features, clinical course, laboratory data, and different therapies were compared between survivors and non-survivors to identify risk factors associated with mortality. Among the 36 patients, 11 (30.5%) died and 7 could be discharged within the observation period. Clinical and radiological evolution during the first week of admission were predictive of mortality. Among the 36 patients, 18 had worsening of their clinical status, as defined by severe hypoxia with oxygen therapy requirements greater than 4 Liters/minute and radiological worsening. Significantly 11 out of those 18 patients (61.1%) died. None of the classical cardiovascular risk factors in the general population were associated with higher mortality. However, a longer time on hemodialysis (hazard ratio 1.008(95% confidence interval 1.001-1.015) per year), increased LDH levels (1.006(1.001-1.011), and lower lymphocyte count (0.996 (0.992-1.000) one week after clinical onset were all significantly associated with higher mortality risk. Thus, the mortality among hospitalized hemodialysis patients diagnosed with COVID-19 is high. Lymphopenia and increased LDH levels were associated with poor prognosis.\n \n', 'corpus_id': 218571559, 'score': 0}, {'doc_id': '221858141', 'title': 'Long-term hydroxychloroquine use in patients with rheumatic conditions and development of SARS-CoV-2 infection: a retrospective cohort study', 'abstract': '\n Background\n Hydroxychloroquine is one of several agents being evaluated in the treatment of severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) infection. We aimed to examine whether patients with rheumatological conditions receiving chronic hydroxychloroquine therapy are at less risk of developing SARS-CoV-2 infection than those not receiving hydroxychloroquine.\n \n Methods\n This retrospective cohort study included de-identified information of all veterans in the US Veterans Health Administration clinical administrative database aged 18 years or older with rheumatoid arthritis, systemic lupus erythematosus, or associated rheumatological conditions (based on International Classification of Diseases, 10th edition, diagnostic codes) who were alive on March 1, 2020. A propensity score was calculated for each patient, and each patient who was receiving hydroxychloroquine was matched to two patients who were not receiving hydroxychloroquine (controls). The primary endpoint was the proportion of patients with PCR-confirmed SARS-CoV-2 infection among those receiving chronic hydroxychloroquine versus the propensity-matched patients not receiving chronic hydroxychloroquine between March 1 and June 30, 2020. Secondary outcomes were hospital admission associated with SARS-CoV-2 infection; intensive care requirement associated with SARS-CoV-2 infection; mortality associated with SARS-CoV-2 infection; and overall rates of any hospital admission and mortality (ie, all cause). Multivariate logistic regression analysis was done to determine independent variables for the development of active SARS-CoV-2 infection.\n \n Findings\n Between March 1 and June 30, 2020, 10\u2008703 patients receiving hydroxychloroquine and 21\u2008406 patients not receiving hydroxychloroquine were included in the primary analysis. The incidence of active SARS-CoV-2 infections during the study period did not differ between patients receiving hydroxychloroquine and patients not receiving hydroxychloroquine (31 [0·3%] of 10\u2008703 vs 78 [0·4%] of 21\u2008406; odds ratio 0·79, 95% CI 0·52–1·20, p=0·27). There were no significant differences in secondary outcomes between the two groups in patients who developed active SARS-CoV-2 infection. For all patients in the study, overall mortality was lower in the hydroxychloroquine group than in the group of patients who did not receive hydroxychloroquine (odds ratio 0·70, 95% CI 0·55–0·89, p=0·0031). In multivariate logistic regression analysis, receipt of hydroxychloroquine was not associated with the development of active SARS-CoV-2 infection (odds ratio 0·79, 95% CI 0·51–1·42).\n \n Interpretation\n Hydroxychloroquine was not associated with a preventive effect against SARS-CoV-2 infection in a large group of patients with rheumatological conditions.\n \n Funding\n None.\n', 'corpus_id': 221858141, 'score': 1}, {'doc_id': '220602322', 'title': 'COVID-19 in people with diabetes: understanding the reasons for worse outcomes', 'abstract': '\n Summary\n \n Since the initial COVID-19 outbreak in China, much attention has focused on people with diabetes because of poor prognosis in those with the infection. Initial reports were mainly on people with type 2 diabetes, although recent surveys have shown that individuals with type 1 diabetes are also at risk of severe COVID-19. The reason for worse prognosis in people with diabetes is likely to be multifactorial, thus reflecting the syndromic nature of diabetes. Age, sex, ethnicity, comorbidities such as hypertension and cardiovascular disease, obesity, and a pro-inflammatory and pro-coagulative state all probably contribute to the risk of worse outcomes. Glucose-lowering agents and anti-viral treatments can modulate the risk, but limitations to their use and potential interactions with COVID-19 treatments should be carefully assessed. Finally, severe acute respiratory syndrome coronavirus 2 infection itself might represent a worsening factor for people with diabetes, as it can precipitate acute metabolic complications through direct negative effects on β-cell function. These effects on β-cell function might also cause diabetic ketoacidosis in individuals with diabetes, hyperglycaemia at hospital admission in individuals with unknown history of diabetes, and potentially new-onset diabetes.\n \n', 'corpus_id': 220602322, 'score': 0}, {'doc_id': '220526615', 'title': 'Clinical Manifestations and Outcomes of Critically Ill Children and Adolescents with Coronavirus Disease 2019 in New York City', 'abstract': '\n               \n                  Objectives\n                  To describe the clinical manifestations and outcomes of critically ill children with coronavirus disease-19 (COVID-19) in New York City.\n               \n               \n                  Study design\n                  Retrospective observational study of children 1 month to 21 years admitted March 14 to May 2, 2020 to 9 New York City pediatric intensive care units (PICUs) with SARS-CoV-2 infection.\n               \n               \n                  Results\n                  Of 70 children admitted to PICUs: median age 15 [IQR 9, 19] years; 61.4% male; 38.6% Hispanic; 32.9% Black; 74.3% with comorbidities. Fever (72.9%) and cough (71.4%) were the common presenting symptoms. Twelve patients (17%) met severe sepsis criteria; 14 (20%) required vasopressor support; 21 (30%) developed acute respiratory distress syndrome (ARDS); 9 (12.9%) met acute kidney injury criteria; 1 (1.4%) required renal replacement therapy, and 2 (2.8%) had cardiac arrest. For treatment, 27 (38.6%) patients received hydroxychloroquine; 13 (18.6%) remdesivir; 23 (32.9%) corticosteroids; 3 (4.3%) tocilizumab; 1 (1.4%) anakinra; no patient was given immunoglobulin or convalescent plasma. Forty-nine (70%) patients required respiratory support: 14 (20.0%) non-invasive mechanical ventilation, 20 (28.6%) invasive mechanical ventilation (IMV), 7 (10%) prone position, 2 (2.8%) inhaled nitric oxide, and 1 (1.4%) extracorporeal membrane oxygenation. Nine (45%) of the 20 patients requiring IMV were extubated by day 14 with median IMV duration of 218 [IQR 79, 310.4] hours. Presence of ARDS was significantly associated with duration of PICU and hospital stay, and lower probability of PICU and hospital discharge at hospital day 14 (P < .05 for all).\n               \n               \n                  Conclusions\n                  Critically ill children with COVID-19 predominantly are adolescents, have comorbidities, and require some form of respiratory support. The presence of ARDS is significantly associated with prolonged PICU and hospital stay.\n               \n            ', 'corpus_id': 220526615, 'score': 0}, {'doc_id': '221497715', 'title': 'Observational study of azithromycin in hospitalized patients with COVID-19', 'abstract': 'Background The rapid spread of the disease caused by the novel SARS-CoV-2 virus has led to the use of multiple therapeutic agents whose efficacy has not been previously demonstrated. The objective of this study was to analyze whether there is an association between the use of azithromycin and the evolution of the pulmonary disease or the time to discharge, in patients hospitalized with COVID-19. Methods This was an observational study on a cohort of 418 patients admitted to three regional hospitals in Catalonia, Spain. As primary outcomes, we studied the evolution of SAFI ratio (oxygen saturation/fraction of inspired oxygen) in the first 48 hours of treatment and the time to discharge. The results were compared between patients treated and untreated with the study drug through subcohort analyses matched for multiple clinical and prognostic factors, as well as through analysis of non-matched subcohorts, using Cox multivariate models adjusted for prognostic factors. Results There were 239 patients treated with azithromycin. Of these, 29 patients treated with azithromycin could be matched with an equivalent number of control patients. In the analysis of these matched subcohorts, SAFI at 48h had no significant changes associated to the use of azithromycin, though azithromycin treatment was associated with a longer time to discharge (10.0 days vs 6.7 days; log rank: p = 0.039). However, in the unmatched cohorts, the increased hospital stay associated to azithromycin use, was no significant after adjustment using Multivariate Cox regression models: hazard ratio 1.45 (IC95%: 0.88–2.41; p = 0.150). This study is limited by its small sample size and its observational nature; despite the strong pairing of the matched subcohorts and the adjustment of the Cox regression for multiple factors, the results may be affected by residual confusion. Conclusions We did not find a clinical benefit associated with the use of azithromycin, in terms of lung function 48 hours after treatment or length of hospital stay.', 'corpus_id': 221497715, 'score': 1}, {'doc_id': '222799310', 'title': 'Comparative Survival Analysis of Immunomodulatory Therapy for Coronavirus Disease 2019 Cytokine Storm', 'abstract': '\n Background\n Cytokine storm is a marker of COVID-19 illness severity and increased mortality. Immunomodulatory treatments have been repurposed to improve mortality outcomes.\n \n Research Question\n To identify if immunomodulatory therapies improve survival in patients with COVID-19 cytokine storm.\n \n Study Design and Methods\n We conducted a retrospective analysis of electronic health records across the Northwell Health system. COVID-19 patients hospitalized between March 1, 2020 and April 24, 2020 were included. Cytokine storm was defined by inflammatory markers: ferritin >700ng/mL, C-reactive protein >30mg/dL or lactate dehydrogenase >300U/L. Patients were subdivided into six groups—no immunomodulatory treatment (standard of care) and five groups that received either corticosteroids, anti-interleukin 6 antibody (tocilizumab) or anti-interleukin-1 therapy (anakinra) alone or in combination with corticosteroids. The primary outcome was hospital mortality.\n \n Results\n 5,776 patients met the inclusion criteria. The most common comorbidities were hypertension (44-59%), diabetes (32-46%) and cardiovascular disease (5-14%). Patients most frequently met criteria with high lactate dehydrogenase (76.2%) alone or in combination, followed by ferritin (63.2%) and C-reactive protein (8.4%). More than 80% of patients had an elevated D-dimer. Patients treated with corticosteroids and tocilizumab combination had lower mortality compared to standard of care (Hazard Ratio (HR):0.44, 95% confidence interval (CI): 0.35-0.55; p<0.0001) and when compared to corticosteroids alone (HR:0.66, 95%CI: 0.53-0.83; p-value=0.004), or in combination with anakinra (HR:0.64, 95%CI:0.50-0.81; p-value=0.003) . Corticosteroids when administered alone (HR:0.66, 95%CI:0.57-0.76; p<0.0001) or in combination with tocilizumab (HR:0.43, 95%CI:0.35-0.55; p<0.0001) or anakinra (HR:0.68, 95%CI:0.57-0.81; p<0.0001) improved hospital survival compared to standard of care.\n \n Interpretation\n The combination of corticosteroids with tocilizumab had superior survival outcome when compared to standard of care and corticosteroids alone or in combination with anakinra. Furthermore, corticosteroid use either alone or in combination with tocilizumab or anakinra was associated with reduced hospital mortality for patients with COVID-19 cytokine storm compared to standard of care.\n', 'corpus_id': 222799310, 'score': 1}, {'doc_id': '220437021', 'title': 'Intravenous high-dose vitamin C for the treatment of severe COVID-19: study protocol for a multicentre randomised controlled trial', 'abstract': 'Introduction The rapid worldwide spread of COVID-19 has caused a global health crisis. To date, symptomatic supportive care has been the most common treatment. It has been reported that the mechanism of COVID-19 is related to cytokine storms and subsequent immunogenic damage, especially damage to the endothelium and alveolar membrane. Vitamin C (VC), also known as L-ascorbic acid, has been shown to have antimicrobial and immunomodulatory properties. A high dose of intravenous VC (HIVC) was proven to block several key components of cytokine storms, and HIVC showed safety and varying degrees of efficacy in clinical trials conducted on patients with bacterial-induced sepsis and acute respiratory distress syndrome (ARDS). Therefore, we hypothesise that HIVC could be added to the treatment of ARDS and multiorgan dysfunction related to COVID-19. Methods and analysis The investigators designed a multicentre prospective randomised placebo-controlled trial that is planned to recruit 308 adults diagnosed with COVID-19 and transferred into the intensive care unit. Participants will randomly receive HIVC diluted in sterile water or placebo for 7 days once enrolled. Patients with a history of VC allergy, end-stage pulmonary disease, advanced malignancy or glucose-6-phosphate dehydrogenase deficiency will be excluded. The primary outcome is ventilation-free days within 28 observational days. This is one of the first clinical trials applying HIVC to treat COVID-19, and it will provide credible efficacy and safety data. We predict that HIVC could suppress cytokine storms caused by COVID-19, help improve pulmonary function and reduce the risk of ARDS of COVID-19. Ethics and dissemination The study protocol was approved by the Ethics Committee of Zhongnan Hospital of Wuhan University (identifiers: Clinical Ethical Approval No. 2020001). Findings of the trial will be disseminated through peer-reviewed journals and scientific conferences. Trial registration number NCT04264533.', 'corpus_id': 220437021, 'score': 0}, {'doc_id': '226281669', 'title': 'The effect of tocilizumab on cytokine release syndrome in COVID-19 patients', 'abstract': 'Background This study was aimed to assess the efficacy and safety of tocilizumab (TCZ) and to investigate the factors related to the progress and mortality of patients with a secondary cytokine release syndrome caused by SARS-CoV-2. Methods A retrospective descriptive observational study of hospitalised patients with a positive polymerase chain reaction (PCR) result for SARS-CoV-2 and whose clinical evolution required the administration of one or more doses of TCZ was conducted. Demographic variables, clinical evolution, radiologic progress and analytical parameters were analysed on days 1, 3 and 5 after administration the first dose of TCZ. Results A total of 75 patients with a clinical history of Accurate Respiratory Distress Syndrome (ARDS) were analysed, among whom, 19 had mild ARDS (25.3%), 37 moderate ARDS (49.4%) and 19 severe ARDS (25.3%). Lymphocytopenia and high levels of PCR, d-Dimer and IL-6 were observed in almost all the patients (91.8%). Treatment with TCZ was associated with a reduction of lymphocytopenia, C-reactive protein (CRP) levels, severe ARDS cases and fever. Although a better evolution of PaO2/FiO2 was observed in patients who received two or more doses of TCZ (38/75), there was an increase in their mortality (47.4%) and ICU admission (86.8%). The 30-day mortality rate was 30.7% (20.5–42.4% CI) being hypertension, high initial d-dimer levels and ICU admission the only predictive factors found. Conclusion Based on our results, treatment with TCZ was associated with a fever, swelling and ventilator support improvement. However, there is no evidence that the administration of two or more doses of TCZ was related to a mortality decrease.', 'corpus_id': 226281669, 'score': 1}, {'doc_id': '221559999', 'title': 'Hydroxychloroquine and dexamethasone in COVID-19: who won and who lost?', 'abstract': ""Background On June 30, 2020, the WHO reported over 10 millions of COVID-19 cases worldwide with over half a million deaths. In severe cases the disease progresses into an Acute Respiratory Distress Syndrome (ARDS), which in turn depends on an overproduction of cytokines (IL-6, TNFα, IL-12, IL-8, CCL-2 and IL1) that causes alveolar and vascular lung damage. Clearly, it is essential to find an immunological treatment that controls the “cytokine storm”. In the meantime, however, it is essential to have effective antiviral and anti-inflammatory drugs available immediately. Pharmacologic therapy for COVID-19 Hydroxychloroquine or chloroquine have been widely adopted worldwide for the treatment of SARS-CoV-2 pneumonia. However, the choice of this treatment was based on low quality of evidence, i.e. retrospective, non-randomized controlled studies. Recently, four large Randomized Controlled Trials (RCTs) have been performed in record time delivering reliable data: (1) the National Institutes of Health (NIH) RCT included 60 hospitals participating all over the world and showed the efficacy of remdesivir in reducing the recovery time in hospitalized adults with COVID-19 pneumonia; (2) three large RCTs already completed, for hydroxychloroquine, dexamethasone and Lopinavir and Ritonavir respectively. These trials were done under the umbrella of the 'Recovery' project, headed by the University of Oxford. The project includes 176 participating hospitals in the UK and was set up to verify the efficacy of some of the treatments used for COVID-19. These three ‘Recovery’ RCTs concluded definitely: (a) that treatment with hydroxychloroquine provides no benefits in patients hospitalized with COVID-19; (b) that treatment with dexamethasone reduced deaths by one-third in COVID-19 patients that were mechanically ventilated, and by one-fifth in patients receiving oxygen only; (c) that the combination of Lopinavir and Ritonavir is not effective in reducing mortality in COVID-19 hospitalized patients. Conclusions The results of these four large RCTs have provided sound indications to doctors for the treatment of patients with COVID-19 and prompted the correction of many institutional provisions and guidelines on COVID-19 treatments (i.e. FDA, NIH, UK Health Service, etc.). Even though a definitive treatment for COVID-19 has not yet been found, large RCTs stand as the Gold Standards for COVID-19 therapy and offer a solid scientific base on which to base treatment decisions."", 'corpus_id': 221559999, 'score': 1}]"
92	{'doc_id': '85511878', 'title': 'Planning for a Corpus of Continuous Ratings of Spoken Dialog Quality', 'abstract': 'While many aspects of speech processing, including speech recognition and speech synthesis, have seen enormous advances over the past few years, advances in dialog have been more modest. This difference is largely attributable to the lack of resources that can support machine learning of dialog models and dialog phenomena. The research community accordingly needs a corpus of spoken dialogs with quality annotations every 100 milliseconds or so. We envisage a large and diverse collection: on the order of fifty hours of data, representing hundreds of speakers and many genres, with every instant labeled for interaction quality by one or more human judges. To make it maximally useful, its design will be a community effort. This technical report is an edited version of a proposal to the National Science Foundation, submitted to the CISE Community Research Infrastructure Program in February 2019. I thank David DeVault, Milica Gasic, Kallirroi Georgila, Svetlana Stoyanchev, Tatsuya Kawahara, Olac Fuentes, and David Novick for helpful discussions. 1 Motivation: Enabling Faster Progress in Spoken Dialog Systems Research and Development Dialog is a uniquely powerful form of interaction. Text-based interactions have their place, but if we want to get to know someone, negotiate plans, make lasting decisions, get considered advice, resolve a workplace issue, or have fun together, we usually seek a realtime spoken dialog, face to face or by phone. Sometimes it would be helpful if computers could interact with us in the same way, for example as a coaches, tutors, or workplace assistants, but this is currently beyond the state of the art, and spoken language systems today are mostly confined to a few usage niches. Siri, for example, was initially designed to avoid dialog if at all possible, to instead get the job done with one response to one input, and this is still a common strategy. Indeed, at SLT 2018 (the IEEE Spoken Language Technology Workshop), one presenter, the lead developer of a well-known personal assistant system, revealed that for his system the average dialog length is 1.1 turns. While impressive in some ways, this is also testament to the difficulty of supporting true dialogs today. In the research arena, researchers have shown how we can do better, producing prototype systems with amazing responsiveness. For example, Gratch produced a system capable of active listening better than most people, DeVault produced a system capable of in-game collaboration as fast as the average human, Litman demonstrated a system that could pick up on subtle indications of a student’s cognitive state and respond in ways that increased learning gains, Acosta produced a system that prosodically tailored its utterances to show empathy and thereby establish rapport, and Yu demonstrated how a robot through dialog could shape the user’s attention [1, 2, 3, 4, 5]. These illustrate that there are many ways in which the functionality and usability of dialog systems could be greatly improved. However such capabilities have not been taken up in commercial systems. One major reason is that their development involved custom corpora, careful policy design, and intense engineering and tuning. These do not scale. In contrast, the astounding recent advances in other areas of speech and language technology — speech recognition, speech synthesis, machine translation, speaker identification, emotion recognition, and many others — have been enabled by the application of deep learning models to large corpora. To enable rapid advances also in spoken dialog, the research community needs large corpora of data with suitable annotations to support deep learning approaches. In this project we propose to lay the groundwork for developing such corpora. 2 Research Landscape: How Spoken Dialog Systems are Built and Trained Most task-oriented dialog systems are today designed by hand. This is because creating effective dialog today, even for something as familiar as automated banking, is as much an art as a science, both in the creation phase and in the refinement/tuning phase. Developers tend to be conservative and follow heuristics, such as avoid overlap at all costs, and avoid prosodic variation in favor of conveying everything explicitly with words. Such heuristics originated in the early days of dialog systems development, when the component technologies were far inferior to where they are today. They are ultimately rooted in informal quality judgments made by developers: they apply their intuitions about what users like and don’t like, and may augment these with simple call-log statistics, for example to find reasons for glaring quality issues, such as those leading to dialog breakdown or call abandonment [6, 7]. In the research arena, by contrast, the current mainstream eschews questions of design in favor of developing ways to tune dialog systems from data. The general strategy is to devise cost functions and then create models that can be trained to optimize them. There are three main families of approaches. 1. For developing chat systems, standard practice is to train a system to match observed behavior, for example in choosing word sequences based on the degree of match to sequences seen in response to similar inputs in training data, such as chat corpora or movie screenplays. This approach is the core of neural conversational models, end-to-end dialog models and related approaches, which over the past three years have, from a few seminal papers [8, 9, 10, 11] grown to a tidal wave of work, with many hundreds of publications so far. While to date applied, as far as we know, only to text and to the content aspects of spoken dialog, it is only a matter of time before we see work generalizing these models to handle spoken interaction. At the same time, this family of approaches suffers from crippling problems with the metrics [12] (such as the commonly-used Bleu), which relate to simplistic concepts of match to observed behavior. In particular, human-human dialog interaction quality is not uniformly high, and there are many speakers and many actions which we do not want our dialog systems to mimic. (There are of course partial workarounds for this issue, including recording dialogs with an exemplary speaker, such as a champion-level customer service representative, and obtaining multiple response tracks, and then training the system to produce consensus behavior [13].) 2. For developing task-oriented dialog systems, there are more principled ways to optimize', 'corpus_id': 85511878}	19767	"[{'doc_id': '236836774', 'title': 'SENTIMENT ANALYSIS OF PRODUCT REVIEWS USING SUPERVISED LEARNING', 'abstract': ""Today, Online Reviews are global communications among consumers and E-commerce businesses. When Somebody wants to make a purchase online, they read the reviews and comments that many people have written about the product. Only after customers decide whether to buy the product or not. Based on that, the Success of any Products directly depends on its Customer. Customer Likes Products It’s Success. if not, then Company needs to improve it by making some changes in it. For that, the need is to analyze the customers' written reviews and find the sentiment from that. the task of Classifying the comments and the reviews in positive or negative is known as sentiment analysis.in this paper, A Standard dataset reviews have been classified into positive and negative sentiments using Sentiment Analysis. For that different Machine Learning and Deep Learning Technique is used and also Compared the performance of word2vec-CNN Model with FastTextCNN Model on amazon unlocked mobile phone Dataset."", 'corpus_id': 236836774, 'score': 0}, {'doc_id': '237258432', 'title': 'Survey on Different Algorithm for Movie Recommedation System', 'abstract': 'Abstract : The key to the recommendation system is to predict user performance. Day –by-Day we see huge growth in ECommerce and this continues growth in the E-commerce field gave the birth of a recommendation system. The recommendation system is done of different methods by using usersimilarily, content-based method, collaborative filtering, Hybrid Models, and many more with various algorithms and accuracy.this essay critically examine how various algorithms perform and which of each algorithms performs with better accuracy between all of them .the most better accuracy c by this origination of recommendation system, there are various types of recommendation available in the market nowadays and each and every recommendation system works on distinct appearance like the interest of users, history of users, location of users and many more. In this process, we will generally discuss the movie recommendation system. Movie recommendation Engine will recommend movies to the users on the basis of their interest as well as rating and reviews by other users whose item interest is similar to eachother. This engine will work on the user similarity Model.', 'corpus_id': 237258432, 'score': 0}, {'doc_id': '45266675', 'title': 'Estimation of Interpersonal Relationships in Movies', 'abstract': 'In many movies, social conditions and awareness of the issues of the times are depicted in any form. Even if fantasy and science fiction are works far from reality, the character relationship does mirror the real world. Therefore, we try to understand social conditions of the real world by analyzing the movie. As a way to analyze the movies, we propose a method of estimating interpersonal relationships of the characters, using a machine learning technique called Markov Logic Network (MLN) from movie script databases on the Web. The MLN is a probabilistic logic network that can describe the relationships between characters, which are not necessarily satisfied on every line. In experiments, we confirmed that our proposed method can estimate favors between the characters in a movie with F-measure of 58.7%. Finally, by comparing the relationships with social indicators, we discussed the relevance of the movies to the real world. key words: Markov Logic Network, semantic analysis, Open Movie Database, perception/cognitive metrics', 'corpus_id': 45266675, 'score': 1}, {'doc_id': '211519415', 'title': 'Film Genre Prediction Based on Film Content and Screenplay Structure', 'abstract': 'In this study, we propose a method to classify genre-based films using film screenplays. The proposed method vectorizes films into two aspects, i.e., film content (i.e., what films tell viewers) and screenplay structure (i.e., how the films narrate stories), and classifies film genres using the support vector machine method. We applied the Doc2Vec algorithm to screenplay structure and to handle film content. In film production, for vectorizing films, we used the statistics of the four screenplay elements: scene, action, dialogue, and transition. Compared with baseline methods, the evaluation showed that the proposed method is better for classifying films of specific genres.', 'corpus_id': 211519415, 'score': 1}, {'doc_id': '196153442', 'title': 'Automatic movie analysis and summarisation', 'abstract': 'Automatic movie analysis is the task of employing Machine Learning methods to the field of screenplays, movie scripts, and motion pictures to facilitate or enable various tasks throughout the entirety of a movie’s life-cycle. From helping with making informed decisions about a new movie script with respect to aspects such as its originality, similarity to other movies, or even commercial viability, all the way to offering consumers new and interesting ways of viewing the final movie, many stages in the life-cycle of a movie stand to benefit from Machine Learning techniques that promise to reduce human effort, time, or both. Within this field of automatic movie analysis, this thesis addresses the task of summarising the content of screenplays, enabling users at any stage to gain a broad understanding of a movie from greatly reduced data. The contributions of this thesis are four-fold: (i) We introduce ScriptBase, a new large-scale data set of original movie scripts, annotated with additional meta-information such as genre and plot tags, cast information, and logand tag-lines. To our knowledge, ScriptBase is the largest data set of its kind, containing scripts and information for almost 1,000 Hollywood movies. (ii) We present a dynamic summarisation model for the screenplay domain, which allows for extraction of highly informative and important scenes from movie scripts. The extracted summaries allow for the content of the original script to stay largely intact and provide the user with its important parts, while greatly reducing the script-reading time. (iii) We extend our summarisation model to capture additional modalities beyond the screenplay text. The model is rendered multi-modal by introducing visual information obtained from the actual movie and by extracting scenes from the movie, allowing users to generate visual summaries of motion pictures. (iv) We devise a novel end-to-end neural network model for generating natural language screenplay overviews. This model enables the user to generate short descriptive and informative texts that capture certain aspects of a movie script, such as its genres, approximate content, or style, allowing them to gain a fast, high-level understanding of the screenplay. Multiple automatic and human evaluations were carried out to assess the performance of our models, demonstrating that they are well-suited for the tasks set out in this thesis, outperforming strong baselines. Furthermore, the ScriptBase data set has started to gain traction, and is currently used by a number of other researchers in the field to tackle various tasks relating to screenplays and their analysis.', 'corpus_id': 196153442, 'score': 1}, {'doc_id': '236909721', 'title': 'A STUDY: SENTIMENTAL ANALYSIS FOR ELECTION RESULTS BY USING TWITTER DATA', 'abstract': 'The entire world is changing at a breakneck pace, and technology is no exception. User-generated data is abundant on social networking platforms like Twitter. Users from all around the world offer their thoughts, opinions, ideas, and feelings about a variety of topics, including products, movies, and politics. Manual sentiment analysis is a time-consuming task. Opinion mining has recently gained popularity as a result of the large volume of opinionated data available on social networking sites such as Twitter. In this paper, we used a chronological approach to data collection, data pre-processing, emotional analysis, and machine learning analysis to forecast the outcome of the US 2020 presidential election using Twitter emotional analysis. We used a Random Forest classifier after completing a literature review and comparing all supervised ensemble machine learning algorithms to determine which one was the best. The proposed technique was tested on Twitter data, and it outperformed existing approaches.', 'corpus_id': 236909721, 'score': 0}, {'doc_id': '145051725', 'title': 'Movie Success Prediction using Machine Learning Algorithms and their Comparison', 'abstract': 'The number of movies produced in the world is growing at an exponential rate and success rate of movie is of utmost importance since billions of dollars are invested in the making of each of these movies. In such a scenario, prior knowledge about the success or failure of a particular movie and what factor affect the movie success will benefit the production houses since these predictions will give them a fair idea of how to go about with the advertising and campaigning, which itself is an expensive affair altogether. So, the prediction of the success of a movie is very essential to the film industry. In this proposed research, we give our detailed analysis of the Internet Movie Database (IMDb) and predict the IMDb score. This database contains categorical and numerical information such as IMDb score, director, gross, budget and so on and so forth. This research proposes a way to predict how successful a movie will be prior to its arrival at the box office instead of listening to critics and others on whether a movie will be successful or not. The proposed research provides a quite efficient approach to predict IMDb score on IMDb Movie Dataset. We will try to unveil the important factors influencing the score of IMDb Movie Data. We have used different algorithms in the research work for analysis but among all Random forest gave the best prediction accuracy which is better in comparison to the previous studies. In the exploratory analysis we found that number of voted users, number of critics for reviews, number of Facebook likes, duration of the movie and gross collection of movie affect the IMDb score strongly. Drama and Biopic movies are best in genres.', 'corpus_id': 145051725, 'score': 1}, {'doc_id': '236205880', 'title': 'Machine learning based recommendation system on movie reviews using KNN classifiers', 'abstract': 'Recommender systems are the systems that are designed to recommend items to the consumer depending on several different criteria. These systems estimate the most possible product that the consumers are most likely to buy and are of interest to. Companies like Netflix, Amazon, etc. use recommender services to allow their customers to find the right items or movies for them.In the current system recommendations, the content of ltering and collective ltering typically fall into two groups. The method is formerly Periment in our paper in all methods. We take film features such as stars, directors, for content-based ltering. Movie definition and keywords as inputs use TF-IDF and doc2vec for measuring the film resemblance. For the first time, Input to our algorithm is the film ranking encountered by users, and we use neighbours nearest K, as Factorization of matrix to estimate film scores for consumers. We find that teamwork functions better than content. Predictive error and estimation time ltering.', 'corpus_id': 236205880, 'score': 0}, {'doc_id': '237257749', 'title': 'Predicting the sentiment analysis for the customer reviews to analyze the text analytics', 'abstract': 'The social web has generated huge amounts of data for the users across the globe with just the click of a button. Even in the age of digitalization other’s opinions are considered while making a decision. This reliability is found in the form of opinions and experiences regarding a particular product or service. Sentiment analysis discusses these opinions. The information gathered through the World Wide Web via forums, blogs, social networks and content-sharing services is not structured which leads to the rise of fields like opinion mining, text analysis and sentiment analysis. This paper discusses the different methods of sentiment analysis and highlights its importance in understanding customer reviews to assess text analytics. Since reviews based on sentiment analysis have been included so this paper will focus on reviewing some previous review works of sentiment analysis for customer reviews.', 'corpus_id': 237257749, 'score': 0}, {'doc_id': '20759660', 'title': 'A machine learning approach to predict movie box-office success', 'abstract': ""Predicting society's reaction to a new product in the sense of popularity and adaption rate has become an emerging field of data analysis. The motion picture industry is a multi-billion-dollar business, and there is a massive amount of data related to movies is available over the internet. This study proposes a decision support system for movie investment sector using machine learning techniques. This research helps investors associated with this business for avoiding investment risks. The system predicts an approximate success rate of a movie based on its profitability by analyzing historical data from different sources like IMDb, Rotten Tomatoes, Box Office Mojo and Metacritic. Using Support Vector Machine (SVM), Neural Network and Natural Language Processing the system predicts a movie box office profit based on some pre-released features and post-released features. This paper shows Neural Network gives an accuracy of 84.1% for pre-released features and 89.27% for all features while SVM has 83.44% and 88.87% accuracy for pre-released features and all features respectively when one away prediction is considered. Moreover, we figure out that budget, IMDb votes and no. of screens are the most important features which play a vital role while predicting a movie's box-office success."", 'corpus_id': 20759660, 'score': 1}]"
93	{'doc_id': '56174514', 'title': 'Toward Multimodal Model-Agnostic Meta-Learning', 'abstract': 'Gradient-based meta-learners such as MAML are able to learn a meta-prior from similar tasks to adapt to novel tasks from the same distribution with few gradient updates. One important limitation of such frameworks is that they seek a common initialization shared across the entire task distribution, substantially limiting the diversity of the task distributions that they are able to learn from. In this paper, we augment MAML with the capability to identify tasks sampled from a multimodal task distribution and adapt quickly through gradient updates. Specifically, we propose a multimodal MAML algorithm that is able to modulate its meta-learned prior according to the identified task, allowing faster adaptation. We evaluate the proposed model on a diverse set of problems including regression, few-shot image classification, and reinforcement learning. The results demonstrate the effectiveness of our model in modulating the meta-learned prior in response to the characteristics of tasks sampled from a multimodal distribution.', 'corpus_id': 56174514}	14859	[{'doc_id': '235313639', 'title': 'Adversarially Adaptive Normalization for Single Domain Generalization', 'abstract': 'Single domain generalization aims to learn a model that performs well on many unseen domains with only one domain data for training. Existing works focus on studying the adversarial domain augmentation (ADA) to improve the model’s generalization capability. The impact on domain generalization of the statistics of normalization layers is still underinvestigated. In this paper, we propose a generic normalization approach, adaptive standardization and rescaling normalization (ASR-Norm), to complement the missing part in previous works. ASR-Norm learns both the standardization and rescaling statistics via neural networks. This new form of normalization can be viewed as a generic form of the traditional normalizations. When trained with ADA, the statistics in ASR-Norm are learned to be adaptive to the data coming from different domains, and hence improves the model generalization performance across domains, especially on the target domain with large discrepancy from the source domain. The experimental results show that ASR-Norm can bring consistent improvement to the state-of-the-art ADA approaches by 1.6%, 2.7%, and 6.3% averagely on the Digits, CIFAR-10-C, and PACS benchmarks, respectively. As a generic tool, the improvement introduced by ASR-Norm is agnostic to the choice of ADA methods.', 'corpus_id': 235313639, 'score': 0}, {'doc_id': '235344366', 'title': 'Meta-learning with few-shot models Analysis Final Project', 'abstract': 'This project focuses on understanding the various elements of Meta-learning and few-shot models and the effectiveness of the different detailed implementation approaches. Using the default RobustQA project as a baseline, we explored the different implementations of the Meta-learning algorithm, LEOPARD[1], and evaluate the impact on performance of the prediction accuracy. We have also experimented with the eval-every parameter to understand how fast each implementation can learn when presented with the out of domain questions initially. We found that the multiple datasets implementation of the Leopard algorithm yields the best few-shot result. On the first evaluation at step O (after 1 batch of data for learning) this implementation already achieving a result of a EM score of 34.55 (on the validation set) compared to the 32 EM scores that the other implementation and the baseline are getting. However, after the model is trained for a longer time, we found that the baseline can actually achieve a better EM score overall with 42.202 on the test set. Although, the difference in the overall accuracy of the test set score are very small for different implementations, we found the more simple implementation yields better accuracy in the long run. Our key finding is that the design of a few-shot learning algorithm or model is actually a trade off between few-shot accuracy and the overall highest achievable accuracy. 1 Key Information to include ¢ Mentor: Rui Wang (ruil @stanford.edu) ¢ External Collaborators (if you have any): None ¢ Sharing project: None', 'corpus_id': 235344366, 'score': 0}, {'doc_id': '235658346', 'title': 'R-Drop: Regularized Dropout for Neural Networks', 'abstract': 'Dropout is a powerful and widely used technique to regularize the training of deep neural networks. In this paper, we introduce a simple regularization strategy upon dropout in model training, namely R-Drop, which forces the output distributions of different sub models generated by dropout to be consistent with each other. Specifically, for each training sample, R-Drop minimizes the bidirectional KL-divergence between the output distributions of two sub models sampled by dropout. Theoretical analysis reveals that R-Drop reduces the freedom of the model parameters and complements dropout. Experiments on 5 widely used deep learning tasks (18 datasets in total), including neural machine translation, abstractive summarization, language understanding, language modeling, and image classification, show that R-Drop is universally effective. In particular, it yields substantial improvements when applied to fine-tune large-scale pre-trained models, e.g., ViT, RoBERTa-large, and BART, and achieves state-of-the-art (SOTA) performances with the vanilla Transformer model on WMT14 English→German translation (30.91 BLEU) and WMT14 English→French translation (43.95 BLEU), even surpassing models trained with extra large-scale data and expert-designed advanced variants of Transformer models. Our code is available at GitHub1.', 'corpus_id': 235658346, 'score': 0}, {'doc_id': '219635819', 'title': 'A Brief Look at Generalization in Visual Meta-Reinforcement Learning', 'abstract': 'Due to the realization that deep reinforcement learning algorithms trained on high-dimensional tasks can strongly overfit to their training environments, there have been several studies that investigated the generalization performance of these algorithms. However, there has been no similar study that evaluated the generalization performance of algorithms that were specifically designed for generalization, i.e. meta-reinforcement learning algorithms. In this paper, we assess the generalization performance of these algorithms by leveraging high-dimensional, procedurally generated environments. We find that these algorithms can display strong overfitting when they are evaluated on challenging tasks. We also observe that scalability to high-dimensional tasks with sparse rewards remains a significant problem among many of the current meta-reinforcement learning algorithms. With these results, we highlight the need for developing meta-reinforcement learning algorithms that can both generalize and scale.', 'corpus_id': 219635819, 'score': 1}, {'doc_id': '236166305', 'title': 'Semi-Supervised Few-Shot Learning with Pseudo Label Refinement', 'abstract': 'Few-shot classification aims at recognising novel categories with very limited labelled samples. Although substantial achievements have been obtained, few-shot classification remains challenging due to the scarcity of labelled examples. Recent studies resort to leveraging unlabelled data to expand the training set using pseudo labelling, but this strategy often yields significant label noise. In this work, we introduce a new baseline method for semi-supervised few-shot learning by iterative pseudo label refinement to reduce noise. Then, we investigate the label noise propagation problem and improve the baseline with a denoising network to learn distributions of clean and noisy pseudo-labelled examples via a mixture model. This helps to estimate confidence values of pseudo labelled examples and to select the reliable ones with less noise for iteratively refining a few-shot classifier. Extensive experiments on three widely used benchmarks, miniImagenet, tieredImagenet and CIFAR-FS, show the superiority of the proposed methods over the state-of-the-art methods.', 'corpus_id': 236166305, 'score': 1}, {'doc_id': '235358483', 'title': 'DAMSL: Domain Agnostic Meta Score-based Learning', 'abstract': 'In this paper, we propose Domain Agnostic Meta Score-based Learning (DAMSL), a novel, versatile and highly effective solution that delivers significant out-performance over state-of-the-art methods for cross-domain few-shot learning. We identify key problems in previous meta-learning methods over-fitting to the source domain, and previous transfer-learning methods under-utilizing the structure of the support set. The core idea behind our method is that instead of directly using the scores from a fine-tuned feature encoder, we use these scores to create input coordinates for a domain agnostic metric space. A graph neural network is applied to learn an embedding and relation function over these coordinates to process all information contained in the score distribution of the support set. We test our model on both established CD-FSL benchmarks and new domains and show that our method overcomes the limitations of previous meta-learning and transfer-learning methods to deliver substantial improvements in accuracy across both smaller and larger domain shifts.', 'corpus_id': 235358483, 'score': 1}, {'doc_id': '235490524', 'title': 'Iterative Network Pruning with Uncertainty Regularization for Lifelong Sentiment Classification', 'abstract': 'Lifelong learning capabilities are crucial for sentiment classifiers to process continuous streams of opinioned information on the Web. However, performing lifelong learning is non-trivial for deep neural networks as continually training of incrementally available information inevitably results in catastrophic forgetting or interference. In this paper, we propose a novel i terative network p runing with uncertainty r egularization method for l ifelong s entiment classification (IPRLS), which leverages the principles of network pruning and weight regularization. By performing network pruning with uncertainty regularization in an iterative manner, IPRLS can adapt a single BERT model to work with continuously arriving data from multiple domains while avoiding catastrophic forgetting and interference. Specifically, we leverage an iterative pruning method to remove redundant parameters in large deep networks so that the freed-up space can then be employed to learn new tasks, tackling the catastrophic forgetting problem. Instead of keeping the old-tasks fixed when learning new tasks, we also use an uncertainty regularization based on the Bayesian online learning framework to constrain the update of old tasks weights in BERT, which enables positive backward transfer, i.e. learning new tasks improves performance on past tasks while protecting old knowledge from being lost. In addition, we propose a task-specific low-dimensional residual function in parallel to each layer of BERT, which makes IPRLS less prone to losing the knowledge saved in the base BERT network when learning a new task. Extensive experiments on 16 popular review corpora demonstrate that the proposed IPRLS method significantly outperforms the strong baselines for lifelong sentiment classification. For reproducibility, we submit the code and data at: \\urlhttps://github.com/siat-nlp/IPRLS .', 'corpus_id': 235490524, 'score': 0}, {'doc_id': '235691634', 'title': 'Generalization on Unseen Domains via Inference-Time Label-Preserving Target Projections', 'abstract': 'Generalization of machine learning models trained on a set of source domains on unseen target domains with different statistics, is a challenging problem. While many approaches have been proposed to solve this problem, they only utilize source data during training but do not take advantage of the fact that a single target example is available at the time of inference. Motivated by this, we propose a method that effectively uses the target sample during inference beyond mere classification. Our method has three components (i) A label-preserving feature or metric transformation on source data such that the source samples are clustered in accordance with their class irrespective of their domain (ii) A generative model trained on the these features (iii) A label-preserving projection of the target point on the source-feature manifold during inference via solving an optimization problem on the input space of the generative model using the learned metric. Finally, the projected target is used in the classifier. Since the projected target feature comes from the source manifold and has the same label as the real target by design, the classifier is expected to perform better on it than the true target. We demonstrate that our method outperforms the state-of-the-art Domain Generalization methods on multiple datasets and tasks.', 'corpus_id': 235691634, 'score': 0}, {'doc_id': '69787226', 'title': 'Towards Faster Development of Deep Learning Models Using Meta-Learning', 'abstract': 'Deep learning has, in relatively few years, improved significantly the performance of many machine learning applications. Even though its popularity has surged, it’s not always easy to apply deep learning to a real-world problem. Developing a good deep learning model is a process that most likely will include several iterations of data collection, training and hyperparameter tuning. One big obstacle in this process is the hunger for data and compute power. Supervised learning often requires a massive amount of annotated examples, and training usually extends over hours or days. This makes the process of developing deep learning models very time and resource consuming. In this thesis we investigate if recent advances in few-shot learning can be used to speed up this process, and look specifically at object detection as an example. Such methods could potentially decrease both the necessary number of examples and the training time. MAML (Finn et al. 2017) is a promising few-shot learning method based on metalearning that optimizes the initial parameters of a model to be best possibly suited for fine-tuning. It’s model-agnostic by nature and can in principle be applied to most deep learning models. But through extensive exploration we show that it’s far from trivial to apply MAML to object detection on natural images. However, we are able to use a simpler method inspired by MAML, Reptile (Nichol et al. 2018b). We show that a model pretrained using Reptile can be fine-tuned considerably faster than a model pretrained normally on object detection, but surprisingly it does not enable using fewer examples. In addition, we show that Reptile is able to speed up the development of deep learning models in practice. This is done by building a proof of concept tool and use this to test some example use cases.', 'corpus_id': 69787226, 'score': 1}, {'doc_id': '4587331', 'title': 'On First-Order Meta-Learning Algorithms', 'abstract': 'This paper considers meta-learning problems, where there is a distribution of tasks, and we would like to obtain an agent that performs well (i.e., learns quickly) when presented with a previously unseen task sampled from this distribution. We analyze a family of algorithms for learning a parameter initialization that can be fine-tuned quickly on a new task, using only first-order derivatives for the meta-learning updates. This family includes and generalizes first-order MAML, an approximation to MAML obtained by ignoring second-order derivatives. It also includes Reptile, a new algorithm that we introduce here, which works by repeatedly sampling a task, training on it, and moving the initialization towards the trained weights on that task. We expand on the results from Finn et al. showing that first-order meta-learning algorithms perform well on some well-established benchmarks for few-shot classification, and we provide theoretical analysis aimed at understanding why these algorithms work.', 'corpus_id': 4587331, 'score': 1}]
94	{'doc_id': '232170380', 'title': 'The AI Arena: A Framework for Distributed Multi-Agent Reinforcement Learning', 'abstract': 'Advances in reinforcement learning (RL) have resulted in recent breakthroughs in the application of artificial intelligence (AI) across many different domains. An emerging landscape of development environments is making powerful RL techniques more accessible for a growing community of researchers. However, most existing frameworks do not directly address the problem of learning in complex operating environments, such as dense urban settings or defense-related scenarios, that incorporate distributed, heterogeneous teams of agents. To help enable AI research for this important class of applications, we introduce the AI Arena: a scalable framework with flexible abstractions for distributed multi-agent reinforcement learning. The AI Arena extends the OpenAI Gym interface to allow greater flexibility in learning control policies across multiple agents with heterogeneous learning strategies and localized views of the environment. To illustrate the utility of our framework, we present experimental results that demonstrate performance gains due to a distributed multi-agent learning approach over commonly-used RL techniques in several different learning environments.', 'corpus_id': 232170380}	16648	"[{'doc_id': '232307382', 'title': 'Softmax with Regularization: Better Value Estimation in Multi-Agent Reinforcement Learning', 'abstract': 'Overestimation in Q-learning is an important problem that has been extensively studied in single-agent reinforcement learning, but has received comparatively little attention in the multi-agent setting. In this work, we empirically demonstrate that QMIX, a popular Q-learning algorithm for cooperative multi-agent reinforcement learning (MARL), suffers from a particularly severe overestimation problem which is not mitigated by existing approaches. We rectify this by designing a novel regularization-based update scheme that penalizes large joint action-values deviating from a baseline and demonstrate its effectiveness in stabilizing learning. We additionally propose to employ a softmax operator, which we efficiently approximate in the multi-agent setting, to further reduce the potential overestimation bias. We demonstrate that our Softmax with Regularization (SR) method, when applied to QMIX, accomplishes its goal of avoiding severe overestimation and significantly improves performance in a variety of cooperative multi-agent tasks. To demonstrate the versatility of our method, we apply it to other Q-learning based MARL algorithms and achieve similar performance gains. Finally, we show that our method provides a consistent performance improvement on a set of challenging StarCraft II micromanagement tasks.', 'corpus_id': 232307382, 'score': 0}, {'doc_id': '235433618', 'title': 'Distributed Training for Reinforcement Learning', 'abstract': 'Reinforcement learning (RL) has scaled up immensely over the last few years through the creation of innovative distributed training techniques. This paper discusses a rough timeline of the methods used to push the field forward. I begin by summarizing the problem of reinforcement learning and general solution methods. I then discuss the training environments used to evaluate model performance. I walk through a timeline of breakthroughs in distributed training used to scale up RL models, as well as other innovations in RL training. Finally, I take a look at exciting applications of distributed training processes in complex games like Go, Dota 2, and StarCraft II.', 'corpus_id': 235433618, 'score': 1}, {'doc_id': '233347091', 'title': 'Reinforcement Learning using Guided Observability', 'abstract': 'Due to recent breakthroughs, reinforcement learning (RL) has demonstrated impressive performance in challenging sequential decision-making problems. However, an open question is how to make RL cope with partial observability which is prevalent in many real-world problems. Contrary to contemporary RL approaches, which focus mostly on improved memory representations or strong assumptions about the type of partial observability, we propose a simple but efficient approach that can be applied together with a wide variety of RL methods. Our main insight is that smoothly transitioning from full observability to partial observability during the training process yields a high performance policy. The approach, called partially observable guided reinforcement learning (PO-GRL), allows to utilize full state information during policy optimization without compromising the optimality of the final policy. A comprehensive evaluation in discrete partially observable Markov decision process (POMDP) benchmark problems and continuous partially observable MuJoCo and OpenAI gym tasks shows that PO-GRL improves performance. Finally, we demonstrate PO-GRL in the ball-in-the-cup task on a real Barrett WAM robot under partial observability.', 'corpus_id': 233347091, 'score': 0}, {'doc_id': '6287870', 'title': 'TensorFlow: A system for large-scale machine learning', 'abstract': 'TensorFlow is a machine learning system that operates at large scale and in heterogeneous environments. Tensor-Flow uses dataflow graphs to represent computation, shared state, and the operations that mutate that state. It maps the nodes of a dataflow graph across many machines in a cluster, and within a machine across multiple computational devices, including multicore CPUs, general-purpose GPUs, and custom-designed ASICs known as Tensor Processing Units (TPUs). This architecture gives flexibility to the application developer: whereas in previous ""parameter server"" designs the management of shared state is built into the system, TensorFlow enables developers to experiment with novel optimizations and training algorithms. TensorFlow supports a variety of applications, with a focus on training and inference on deep neural networks. Several Google services use TensorFlow in production, we have released it as an open-source project, and it has become widely used for machine learning research. In this paper, we describe the TensorFlow dataflow model and demonstrate the compelling performance that TensorFlow achieves for several real-world applications.', 'corpus_id': 6287870, 'score': 1}, {'doc_id': '233204546', 'title': 'Learning to Reweight Imaginary Transitions for Model-Based Reinforcement Learning', 'abstract': 'Model-based reinforcement learning (RL) is more sample efficient than modelfree RL by using imaginary trajectories generated by the learned dynamics model. When the model is inaccurate or biased, imaginary trajectories may be deleterious for training the action-value and policy functions. To alleviate such problem, this paper proposes to adaptively reweight the imaginary transitions, so as to reduce the negative effects of poorly generated trajectories. More specifically, we evaluate the effect of an imaginary transition by calculating the change of the loss computed on the real samples when we use the transition to train the action-value and policy functions. Based on this evaluation criterion, we construct the idea of reweighting each imaginary transition by a well-designed meta-gradient algorithm. Extensive experimental results demonstrate that our method outperforms state-of-the-art model-based and model-free RL algorithms on multiple tasks. Visualization of our changing weights further validates the necessity of utilizing reweight scheme.', 'corpus_id': 233204546, 'score': 0}, {'doc_id': '3585888', 'title': 'Demystifying Parallel and Distributed Deep Learning', 'abstract': 'Deep Neural Networks (DNNs) are becoming an important tool in modern computing applications. Accelerating their training is a major challenge and techniques range from distributed algorithms to low-level circuit design. In this survey, we describe the problem from a theoretical perspective, followed by approaches for its parallelization. We present trends in DNN architectures and the resulting implications on parallelization strategies. We then review and model the different types of concurrency in DNNs: from the single operator, through parallelism in network inference and training, to distributed deep learning. We discuss asynchronous stochastic optimization, distributed system architectures, communication schemes, and neural architecture search. Based on those approaches, we extrapolate potential directions for parallelism in deep learning.', 'corpus_id': 3585888, 'score': 1}, {'doc_id': '232478635', 'title': 'Optimizer Fusion: Efficient Training with Better Locality and Parallelism', 'abstract': 'Machine learning frameworks adopt iterative optimizers to train neural networks. Conventional eager execution separates the updating of trainable parameters from forward and backward computations. However, this approach introduces nontrivial training time overhead due to the lack of data locality and computation parallelism. In this work, we propose to fuse the optimizer with forward or backward computation to better leverage locality and parallelism during training. By reordering the forward computation, gradient calculation, and parameter updating, our proposed method improves the efficiency of iterative optimizers. Experimental results demonstrate that we can achieve an up to 20% training time reduction on various configurations. Since our methods do not alter the optimizer algorithm, they can be used as a general “plug-in” technique to the training process.', 'corpus_id': 232478635, 'score': 0}, {'doc_id': '49546141', 'title': 'RLlib: Abstractions for Distributed Reinforcement Learning', 'abstract': 'Reinforcement learning (RL) algorithms involve the deep nesting of highly irregular computation patterns, each of which typically exhibits opportunities for distributed computation. We argue for distributing RL components in a composable way by adapting algorithms for top-down hierarchical control, thereby encapsulating parallelism and resource requirements within short-running compute tasks. We demonstrate the benefits of this principle through RLlib: a library that provides scalable software primitives for RL. These primitives enable a broad range of algorithms to be implemented with high performance, scalability, and substantial code reuse. RLlib is available at this https URL.', 'corpus_id': 49546141, 'score': 1}, {'doc_id': '232035672', 'title': 'Memory-based Deep Reinforcement Learning for POMDP', 'abstract': 'A promising characteristic of Deep Reinforcement Learning (DRL) is its capability to learn optimal policy in an end-to-end manner without relying on feature engineering. However, most approaches assume a fully observable state space, i.e. fully observable Markov Decision Process (MDP). In real-world robotics, this assumption is unpractical, because of the sensor issues such as sensors’ capacity limitation and sensor noise, and the lack of knowledge about if the observation design is complete or not. These scenarios lead to Partially Observable MDP (POMDP) and need special treatment. In this paper, we propose Long-Short-Term-Memory-based Twin Delayed Deep Deterministic Policy Gradient (LSTM-TD3) by introducing a memory component to TD3, and compare its performance with other DRL algorithms in both MDPs and POMDPs. Our results demonstrate the significant advantages of the memory component in addressing POMDPs, including the ability to handle missing and noisy observation data.', 'corpus_id': 232035672, 'score': 0}, {'doc_id': '233405268', 'title': 'GRACE: A Compressed Communication Framework for Distributed Machine Learning', 'abstract': 'Powerful computer clusters are used nowadays to train complex deep neural networks (DNN) on large datasets. Distributed training increasingly becomes communication bound. For this reason, many lossy compression techniques have been proposed to reduce the volume of transferred data. Unfortunately, it is difficult to argue about the behavior of compression methods, because existing work relies on inconsistent evaluation testbeds and largely ignores the performance impact of practical system configurations. In this paper, we present a comprehensive survey of the most influential compressed communication methods for DNN training, together with an intuitive classification (i.e., quantization, sparsification, hybrid and low-rank). Next, we propose GRACE, a unified framework and API that allows for consistent and easy implementation of compressed communication on popular machine learning toolkits. We instantiate GRACE on TensorFlow and PyTorch, and implement 16 such methods. Finally, we present a thorough quantitative evaluation with a variety of DNNs (convolutional and recurrent), datasets and system configurations. We show that the DNN architecture affects the relative performance among methods. Interestingly, depending on the underlying communication library and computational cost of compression / decompression, we demonstrate that some methods may be impractical. GRACE and the entire benchmarking suite are available as open-source.', 'corpus_id': 233405268, 'score': 0}]"
95	{'doc_id': '1890353', 'title': 'Introspection: Accelerating Neural Network Training By Learning Weight Evolution', 'abstract': 'Neural Networks are function approximators that have achieved state-of-the-art accuracy in numerous machine learning tasks. In spite of their great success in terms of accuracy, their large training time makes it difficult to use them for various tasks. In this paper, we explore the idea of learning weight evolution pattern from a simple network for accelerating training of novel neural networks. We use a neural network to learn the training pattern from MNIST classification and utilize it to accelerate training of neural networks used for CIFAR-10 and ImageNet classification. Our method has a low memory footprint and is computationally efficient. This method can also be used with other optimizers to give faster convergence. The results indicate a general trend in the weight evolution during training of neural networks.', 'corpus_id': 1890353}	9851	"[{'doc_id': '227338773', 'title': 'Multivariate Density Estimation with Deep Neural Mixture Models', 'abstract': ""Albeit worryingly underrated in the recent literature on machine learning in general (and, on deep learning in particular), multivariate density estimation is a fundamental task in many applications, at least implicitly, and still an open issue. With a few exceptions, deep neural networks (DNNs) have seldom been applied to density estimation, mostly due to the unsupervised nature of the estimation task, and (especially) due to the need for constrained training algorithms that ended up realizing proper probabilistic models that satisfy Kolmogorov's axioms. Moreover, in spite of the well-known improvement in terms of modeling capabilities yielded by mixture models over plain single-density statistical estimators, no proper mixtures of multivariate DNN-based component densities have been investigated so far. The paper fills this gap by extending our previous work on Neural Mixture Densities (NMMs) to multivariate DNN mixtures. A maximum-likelihood (ML) algorithm for estimating Deep NMMs (DNMMs) is handed out, which satisfies numerically a combination of hard and soft constraints aimed at ensuring satisfaction of Kolmogorov's axioms. The class of probability density functions that can be modeled to any degree of precision via DNMMs is formally defined. A procedure for the automatic selection of the DNMM architecture, as well as of the hyperparameters for its ML training algorithm, is presented (exploiting the probabilistic nature of the DNMM). Experimental results on univariate and multivariate data are reported on, corroborating the effectiveness of the approach and its superiority to the most popular statistical estimation techniques."", 'corpus_id': 227338773, 'score': 0}, {'doc_id': '22553892', 'title': 'A New Type of Neurons for Machine Learning', 'abstract': 'In machine learning, an artificial neural network is the mainstream approach. Such a network consists of many neurons. These neurons are of the same type characterized by the 2 features: (1) an inner product of an input vector and a matching weighting vector of trainable parameters and (2) a nonlinear excitation function. Here, we investigate the possibility of replacing the inner product with a quadratic function of the input vector, thereby upgrading the first-order neuron to the second-order neuron, empowering individual neurons and facilitating the optimization of neural networks. Also, numerical examples are provided to illustrate the feasibility and merits of the second-order neurons. Finally, further topics are discussed.', 'corpus_id': 22553892, 'score': 1}, {'doc_id': '67877133', 'title': 'Streaming Batch Eigenupdates for Hardware Neural Networks', 'abstract': 'Neural networks based on nanodevices, such as metal oxide memristors, phase change memories, and flash memory cells, have generated considerable interest for their increased energy efficiency and density in comparison to graphics processing units (GPUs) and central processing units (CPUs). Though immense acceleration of the training process can be achieved by leveraging the fact that the time complexity of training does not scale with the network size, it is limited by the space complexity of stochastic gradient descent, which grows quadratically. The main objective of this work is to reduce this space complexity by using low-rank approximations of stochastic gradient descent. This low spatial complexity combined with streaming methods allows for significant reductions in memory and compute overhead, opening the door for improvements in area, time and energy efficiency of training. We refer to this algorithm and architecture to implement it as the streaming batch eigenupdate (SBE) approach.', 'corpus_id': 67877133, 'score': 1}, {'doc_id': '148571926', 'title': 'AutoAssist: A Framework to Accelerate Training of Deep Neural Networks', 'abstract': 'Deep neural networks have yielded superior performance in many applications; however, the gradient computation in a deep model with millions of instances lead to a lengthy training process even with modern GPU/TPU hardware acceleration. In this paper, we propose AutoAssist, a simple framework to accelerate training of a deep neural network. Typically, as the training procedure evolves, the amount of improvement in the current model by a stochastic gradient update on each instance varies dynamically. In AutoAssist, we utilize this fact and design a simple instance shrinking operation, which is used to filter out instances with relatively low marginal improvement to the current model; thus the computationally intensive gradient computations are performed on informative instances as much as possible. We prove that the proposed technique outperforms vanilla SGD with existing importance sampling approaches for linear SVM problems, and establish an O(1/k) convergence for strongly convex problems. In order to apply the proposed techniques to accelerate training of deep models, we propose to jointly train a very lightweight Assistant network in addition to the original deep network referred to as Boss. The Assistant network is designed to gauge the importance of a given instance with respect to the current Boss such that a shrinking operation can be applied in the batch generator. With careful design, we train the Boss and Assistant in a nonblocking and asynchronous fashion such that overhead is minimal. We demonstrate that AutoAssist reduces the number of epochs by 40% for training a ResNet to reach the same test accuracy on an image classification data set and saves 30% training time needed for a transformer model to yield the same BLEU scores on a translation dataset.', 'corpus_id': 148571926, 'score': 1}, {'doc_id': '227342566', 'title': 'Estimating Vector Fields from Noisy Time Series', 'abstract': 'While there has been a surge of recent interest in learning differential equation models from time series, methods in this area typically cannot cope with highly noisy data. We break this problem into two parts: (i) approximating the unknown vector field (or right-hand side) of the differential equation, and (ii) dealing with noise. To deal with (i), we describe a neural network architecture consisting of tensor products of one-dimensional neural shape functions. For (ii), we propose an al-ternating minimization scheme that switches between vector field training and filtering steps, together with multiple trajectories of training data. We find that the neural shape function architecture retains the approximation properties of dense neural networks, enables effective computation of vector field error, and allows for graphical interpretability, all for data/systems in any finite dimension d. We also study the combination of either our neural shape function method or existing differential equation learning methods with alternating minimization and multiple trajectories. We find that retrofitting any learning method in this way boosts the method’s robustness to noise. While in their raw form the methods struggle with 1% Gaussian noise, after retrofitting, they learn accurate vector fields from data with 10% Gaussian noise.', 'corpus_id': 227342566, 'score': 0}, {'doc_id': '229331851', 'title': 'On the eigenvector bias of Fourier feature networks: From regression to solving multi-scale PDEs with physics-informed neural networks', 'abstract': 'Physics-informed neural networks (PINNs) are demonstrating remarkable promise in integrating physical models with gappy and noisy observational data, but they still struggle in cases where the target functions to be approximated exhibit high-frequency or multi-scale features. In this work we investigate this limitation through the lens of Neural Tangent Kernel (NTK) theory and elucidate how PINNs are biased towards learning functions along the dominant eigen-directions of their limiting NTK. Using this observation, we construct novel architectures that employ spatio-temporal and multi-scale random Fourier features, and justify how such coordinate embedding layers can lead to robust and accurate PINN models. Numerical examples are presented for several challenging cases where conventional PINN models fail, including wave propagation and reaction-diffusion dynamics, illustrating how the proposed methods can be used to effectively tackle both forward and inverse problems involving partial differential equations with multi-scale behavior. All code an data accompanying this manuscript will be made publicly available at https://github.com/ PredictiveIntelligenceLab/MultiscalePINNs.', 'corpus_id': 229331851, 'score': 1}, {'doc_id': '227745095', 'title': 'Statistical Mechanics of Deep Linear Neural Networks: The Back-Propagating Renormalization Group', 'abstract': 'The success of deep learning in many real-world tasks has triggered an effort to theoretically understand the power and limitations of deep learning in training and generalization of complex tasks, so far with limited progress. In this work, we study the statistical mechanics of learning in Deep Linear Neural Networks (DLNNs) in which the input-output function of an individual unit is linear. Despite the linearity of the units, learning in DLNNs is highly nonlinear, hence studying its properties reveals some of the essential features of nonlinear Deep Neural Networks (DNNs). We solve exactly the network properties following supervised learning using an equilibrium Gibbs distribution in the weight space. To do this, we introduce the Back-Propagating Renormalization Group (BPRG) which allows for the incremental integration of the network weights layer by layer from the network output layer and progressing backward. This procedure allows us to evaluate important network properties such as its generalization error, the role of network width and depth, the impact of the size of the training set, and the effects of weight regularization and learning stochasticity. Furthermore, by performing partial integration of layers, BPRG allows us to compute the emergent properties of the neural representations across the different hidden layers. We have proposed a heuristic extension of the BPRG to nonlinear DNNs with rectified linear units (ReLU). Surprisingly, our numerical simulations reveal that despite the nonlinearity, the predictions of our theory are largely shared by ReLU networks with modest depth, in a wide regime of parameters. Our work is the first exact statistical mechanical study of learning in a family of Deep Neural Networks, and the first development of the Renormalization Group approach to the weight space of these systems.', 'corpus_id': 227745095, 'score': 0}, {'doc_id': '9786626', 'title': 'Accelerating neural network training using weight extrapolations', 'abstract': 'The backpropagation (BP) algorithm for training feedforward neural networks has proven robust even for difficult problems. However, its high performance results are attained at the expense of a long training time to adjust the network parameters, which can be discouraging in many real-world applications. Even on relatively simple problems, standard BP often requires a lengthy training process in which the complete set of training examples is processed hundreds or thousands of times. In this paper, a universal acceleration technique for the BP algorithm based on extrapolation of each individual interconnection weight is presented. This extrapolation procedure is easy to implement and is activated only a few times in between iterations of the conventional BP algorithm. This procedure, unlike earlier acceleration procedures, minimally alters the computational structure of the BP algorithm. The viability of this new approach is demonstrated on three examples. The results suggest that it leads to significant savings in computation time of the standard BP algorithm. Moreover, the solution computed by the proposed approach is always located in close proximity to the one obtained by the conventional BP procedure. Hence, the proposed method provides a real acceleration of the BP algorithm without degrading the usefulness of its solutions. The performance of the new method is also compared with that of the conjugate gradient algorithm, which is an improved and faster version of the BP algorithm.', 'corpus_id': 9786626, 'score': 1}]"
96	{'doc_id': '235458588', 'title': 'Transductive Few-Shot Learning: Clustering is All You Need?', 'abstract': 'We investigate a general formulation for clustering and transductive few-shot learning, which integrates prototype-based objectives, Laplacian regularization and supervision constraints from a few labeled data points. We propose a concave-convex relaxation of the problem, and derive a computationally efficient block-coordinate bound optimizer, with convergence guarantee. At each iteration, our optimizer computes independent (parallel) updates for each point-to-cluster assignment. Therefore, it could be trivially distributed for large-scale clustering and few-shot tasks. Furthermore, we provides a thorough convergence analysis based on point-to-set maps. We report comprehensive clustering and few-shot learning experiments over various data sets, showing that our method yields competitive performances, in term of accuracy and optimization quality, while scaling up to large problems. Using standard training on the base classes, without resorting to complex meta-learning and episodic-training strategies, our approach outperforms state-of-the-art few-shot methods by significant margins, across various models, settings and data sets. Surprisingly, we found that even standard clustering procedures (e.g., K-means), which correspond to particular, non-regularized cases of our general model, already achieve competitive performances in comparison to the state-of-the-art in few-shot learning. These surprising results point to the limitations of the current few-shot benchmarks, and question the viability of a large body of convoluted few-shot learning techniques in the recent literature. Our code is publicly available at https://github.com/imtiazziko/SLK-few-shot.', 'corpus_id': 235458588}	14859	[{'doc_id': '235313639', 'title': 'Adversarially Adaptive Normalization for Single Domain Generalization', 'abstract': 'Single domain generalization aims to learn a model that performs well on many unseen domains with only one domain data for training. Existing works focus on studying the adversarial domain augmentation (ADA) to improve the model’s generalization capability. The impact on domain generalization of the statistics of normalization layers is still underinvestigated. In this paper, we propose a generic normalization approach, adaptive standardization and rescaling normalization (ASR-Norm), to complement the missing part in previous works. ASR-Norm learns both the standardization and rescaling statistics via neural networks. This new form of normalization can be viewed as a generic form of the traditional normalizations. When trained with ADA, the statistics in ASR-Norm are learned to be adaptive to the data coming from different domains, and hence improves the model generalization performance across domains, especially on the target domain with large discrepancy from the source domain. The experimental results show that ASR-Norm can bring consistent improvement to the state-of-the-art ADA approaches by 1.6%, 2.7%, and 6.3% averagely on the Digits, CIFAR-10-C, and PACS benchmarks, respectively. As a generic tool, the improvement introduced by ASR-Norm is agnostic to the choice of ADA methods.', 'corpus_id': 235313639, 'score': 0}, {'doc_id': '211205120', 'title': 'A Structured Prediction Approach for Conditional Meta-Learning', 'abstract': 'Optimization-based meta-learning algorithms are a powerful class of methods for learning-to-learn applications such as few-shot learning. They tackle the limited availability of training data by leveraging the experience gained from previously observed tasks. However, when the complexity of the tasks distribution cannot be captured by a single set of shared meta-parameters, existing methods may fail to fully adapt to a target task. We address this issue with a novel perspective on conditional meta-learning based on structured prediction. We propose task-adaptive structured meta-learning (TASML), a principled estimator that weighs meta-training data conditioned on the target task to design tailored meta-learning objectives. In addition, we introduce algorithmic improvements to tackle key computational limitations of existing methods. Experimentally, we show that TASML outperforms state-of-the-art methods on benchmark datasets both in terms of accuracy and efficiency. An ablation study quantifies the individual contribution of model components and suggests useful practices for meta-learning.', 'corpus_id': 211205120, 'score': 1}, {'doc_id': '235658346', 'title': 'R-Drop: Regularized Dropout for Neural Networks', 'abstract': 'Dropout is a powerful and widely used technique to regularize the training of deep neural networks. In this paper, we introduce a simple regularization strategy upon dropout in model training, namely R-Drop, which forces the output distributions of different sub models generated by dropout to be consistent with each other. Specifically, for each training sample, R-Drop minimizes the bidirectional KL-divergence between the output distributions of two sub models sampled by dropout. Theoretical analysis reveals that R-Drop reduces the freedom of the model parameters and complements dropout. Experiments on 5 widely used deep learning tasks (18 datasets in total), including neural machine translation, abstractive summarization, language understanding, language modeling, and image classification, show that R-Drop is universally effective. In particular, it yields substantial improvements when applied to fine-tune large-scale pre-trained models, e.g., ViT, RoBERTa-large, and BART, and achieves state-of-the-art (SOTA) performances with the vanilla Transformer model on WMT14 English→German translation (30.91 BLEU) and WMT14 English→French translation (43.95 BLEU), even surpassing models trained with extra large-scale data and expert-designed advanced variants of Transformer models. Our code is available at GitHub1.', 'corpus_id': 235658346, 'score': 0}, {'doc_id': '235422615', 'title': 'Knowledge Consolidation based Class Incremental Online Learning with Limited Data', 'abstract': 'We propose a novel approach for class incremental online learning in a limited data setting. This problem setting is challenging because of the following constraints: (1) Classes are given incrementally, which necessitates a class incremental learning approach; (2) Data for each class is given in an online fashion, i.e., each training example is seen only once during training; (3) Each class has very few training examples; and (4) We do not use or assume access to any replay/memory to store data from previous classes. Therefore, in this setting, we have to handle twofold problems of catastrophic forgetting and overfitting. In our approach, we learn robust representations that are generalizable across tasks without suffering from the problems of catastrophic forgetting and overfitting to accommodate future classes with limited samples. Our proposed method leverages the meta-learning framework with knowledge consolidation. The meta-learning framework helps the model for rapid learning when samples appear in an online fashion. Simultaneously, knowledge consolidation helps to learn a robust representation against forgetting under online updates to facilitate future learning. Our approach significantly outperforms other methods on several benchmarks.', 'corpus_id': 235422615, 'score': 0}, {'doc_id': '235825483', 'title': 'Memory Efficient Online Meta Learning', 'abstract': 'We propose a novel algorithm for online meta learning where task instances are sequentially revealed with limited supervision and a learner is expected to meta learn them in each round, so as to allow the learner to customize a task-specific model rapidly with little task-level supervision. A fundamental concern arising in online metalearning is the scalability of memory as more tasks are viewed over time. Heretofore, prior works have allowed for perfect recall leading to linear increase in memory with time. Different from prior works, in our method, prior task instances are allowed to be deleted. We propose to leverage prior task instances by means of a fixed-size state-vector, which is updated sequentially. Our theoretical analysis demonstrates that our proposed memory efficient online learning (MOML) method suffers sub-linear regret with convex loss functions and sub-linear local regret for nonconvex losses. On benchmark datasets we show that our method can outperform prior works even though they allow for perfect recall.', 'corpus_id': 235825483, 'score': 1}, {'doc_id': '235490524', 'title': 'Iterative Network Pruning with Uncertainty Regularization for Lifelong Sentiment Classification', 'abstract': 'Lifelong learning capabilities are crucial for sentiment classifiers to process continuous streams of opinioned information on the Web. However, performing lifelong learning is non-trivial for deep neural networks as continually training of incrementally available information inevitably results in catastrophic forgetting or interference. In this paper, we propose a novel i terative network p runing with uncertainty r egularization method for l ifelong s entiment classification (IPRLS), which leverages the principles of network pruning and weight regularization. By performing network pruning with uncertainty regularization in an iterative manner, IPRLS can adapt a single BERT model to work with continuously arriving data from multiple domains while avoiding catastrophic forgetting and interference. Specifically, we leverage an iterative pruning method to remove redundant parameters in large deep networks so that the freed-up space can then be employed to learn new tasks, tackling the catastrophic forgetting problem. Instead of keeping the old-tasks fixed when learning new tasks, we also use an uncertainty regularization based on the Bayesian online learning framework to constrain the update of old tasks weights in BERT, which enables positive backward transfer, i.e. learning new tasks improves performance on past tasks while protecting old knowledge from being lost. In addition, we propose a task-specific low-dimensional residual function in parallel to each layer of BERT, which makes IPRLS less prone to losing the knowledge saved in the base BERT network when learning a new task. Extensive experiments on 16 popular review corpora demonstrate that the proposed IPRLS method significantly outperforms the strong baselines for lifelong sentiment classification. For reproducibility, we submit the code and data at: \\urlhttps://github.com/siat-nlp/IPRLS .', 'corpus_id': 235490524, 'score': 0}, {'doc_id': '235826257', 'title': 'Parameterless Transductive Feature Re-representation for Few-Shot Learning', 'abstract': 'Recent literature in few-shot learning (FSL) has shown that transductive methods often outperform their inductive counterparts. However, most transductive solutions, particularly the meta-learning based ones, require inserting trainable parameters on top of some inductive baselines to facilitate transduction. In this paper, we propose a parameterless transductive feature re-representation framework that differs from all existing solutions from the following perspectives. (1) It is widely compatible with existing FSL methods, including meta-learning and fine tuning based models. (2) The framework is simple and introduces no extra training parameters when applied to any architecture. We conduct experiments on three benchmark datasets by applying the framework to both representative meta-learning baselines and state-ofthe-art FSL methods. Our framework consistently improves performances in all experiments and refreshes the state-of-the-art FSL results.', 'corpus_id': 235826257, 'score': 1}, {'doc_id': '215752787', 'title': 'Research Project Proposal: Structured Meta-Learning for Heterogeneous Tasks', 'abstract': 'Machine Learning is an application of Artificial Intelligence that sets as its goal to develop methods that can detect patterns in data, and then use the uncovered patterns to predict future data or other outcomes of interest [20]. In recent years, a class of parametric techniques called Deep Learning has led to astonishing achievements in the field. The key aspect of these models is the ability to learn how to extract relevant features from data through many layers of general-purpose neural networks, thus not requiring field-specific expertise [17]. Nonetheless, Deep Learning currently faces some obstacles that still hinder the technology to be exploited in many application domains. Indeed, humans have a remarkable capacity to learn new concepts when provided with few examples; conversely, current popular deep learning techniques are data-hungry, needing thousands of samples to be able to generalize their knowledge and make predictions on unseen data. Meta-Learning [4, 29], also known as “learning-to-learn”, is a sub-field of Machine Learning that exploits previous experience to optimize learning algorithms to work well on novel tasks [10]. The experience is often formalized as a collection of tasks, upon which meta-learning techniques build general, task-agnostic knowledge that can be reused. The approach has been shown to address some of the challenges posed by Few-Shot Learning, where very few task-specific training datapoints are available and the problem of overfitting is particularly insidious [18]. The literature has recently provided promising results also thanks to the leverage of deep learning techniques, achieving human-like performance also in simple meta-learning tasks [16]. Beyond its recent achievements, Meta-Learning itself currently faces many challenges. Many popular approaches struggle when scaling to more powerful learners, which constrains them to poor performance when dealing with complex tasks. Another challenge is the transfer of knowledge among tasks that are particularly different. Our brain builds powerful abstractions that can be used to identify an object, no matter how it is depicted, either as a natural image, a clip-art, or another visual representation. Conversely, a problem that has been observed to occur in many state-of-the-art meta-learning approaches is the inability to generalize the knowledge when presented with a heterogeneous distribution of domains. As also analyzed in [31], we identified three partially overlapping issues that should be addressed to scale meta-learning. Firstly, most of the current meta-learning algorithms are designed considering the simplistic assumption that the distribution of tasks is homogeneous, namely that tasks are coming from a single source [3], and they share the same characteristics. In contrast, real-life learning experiences are heterogeneous: for instance, classification tasks may vary in terms of the number of classes or examples per class and are often unbalanced. Secondly, benchmarks in Meta-Learning only measure within-dataset generalization. However, we are interested in having models that can learn from multiple sources and generalize to entirely new distributions, namely new datasets or domains. Lastly, most of the current models and benchmarks ignore the relationships between tasks and classes, disregarding structures that could be useful to share knowledge across multiple tasks. In light of these issues, our goal is to determine whether the currently provided techniques can be extended to better scale Meta-Learning with respect to data and task heterogeneity. A model capable of operating among different data domains would be able to transfer knowledge among widely different tasks, solving the lack of training samples that is observed in certain data domains. As a practical example, the desired model would be able to generalize the recognition of malignant tumors in x-ray images to images obtained through other less popular techniques or instruments which may feature different colors and shades.', 'corpus_id': 215752787, 'score': 1}, {'doc_id': '235691634', 'title': 'Generalization on Unseen Domains via Inference-Time Label-Preserving Target Projections', 'abstract': 'Generalization of machine learning models trained on a set of source domains on unseen target domains with different statistics, is a challenging problem. While many approaches have been proposed to solve this problem, they only utilize source data during training but do not take advantage of the fact that a single target example is available at the time of inference. Motivated by this, we propose a method that effectively uses the target sample during inference beyond mere classification. Our method has three components (i) A label-preserving feature or metric transformation on source data such that the source samples are clustered in accordance with their class irrespective of their domain (ii) A generative model trained on the these features (iii) A label-preserving projection of the target point on the source-feature manifold during inference via solving an optimization problem on the input space of the generative model using the learned metric. Finally, the projected target is used in the classifier. Since the projected target feature comes from the source manifold and has the same label as the real target by design, the classifier is expected to perform better on it than the true target. We demonstrate that our method outperforms the state-of-the-art Domain Generalization methods on multiple datasets and tasks.', 'corpus_id': 235691634, 'score': 0}, {'doc_id': '52811286', 'title': 'A Meta-Learning Approach for Custom Model Training', 'abstract': 'Transfer-learning and meta-learning are two effective methods to apply knowledge learned from large data sources to new tasks. In few-class, few-shot target task settings (i.e. when there are only a few classes and training examples available in the target task), meta-learning approaches that optimize for future task learning have outperformed the typical transfer approach of initializing model weights from a pretrained starting point. But as we experimentally show, metalearning algorithms that work well in the few-class setting do not generalize well in many-shot and many-class cases. In this paper, we propose a joint training approach that combines both transfer-learning and meta-learning. Benefiting from the advantages of each, our method obtains improved generalization performance on unseen target tasks in both few- and many-class and few- and many-shot scenarios.', 'corpus_id': 52811286, 'score': 1}]
97	{'doc_id': '202853165', 'title': 'Lipids in xylem sap of woody plants across the angiosperm phylogeny', 'abstract': 'Lipids have been observed attached to lumen-facing surfaces of mature xylem conduits of several plant species, but there has been little research on their functions or effects on water transport, and only one lipidomic study of the xylem apoplast. Therefore, we conducted lipidomic analyses of xylem sap from woody stems of seven plants representing six major angiosperm clades, including basal magnoliids, monocots, and eudicots, to characterize and quantify phospholipids, galactolipids, and sulfolipids in sap using mass spectrometry. Locations of lipids in vessels of Laurus nobilis were imaged using TEM and confocal microscopy. Xylem sap contained the galactolipids di- and mono-galactosyldiacylglycerol (DGDG and MGDG), as well as all common plant phospholipids, but only traces of sulfolipids, with total lipid concentrations in extracted sap ranging from 0.18 to 0.63 nmol / mL across all seven species. Contamination of extracted sap from lipids in cut living cells was found to be negligible. Lipid composition of sap was compared to wood in two species and was largely similar, suggesting that sap lipids, including galactolipids, originate from cell content of living vessels. Seasonal changes in lipid composition of sap were observed for one species. Lipid layers coated all lumen-facing vessel surfaces of Laurus nobilis, and lipids were highly concentrated in inter-vessel pits. The findings suggest that apoplastic, amphiphilic xylem lipids are a universal feature of angiosperms. The findings require a reinterpretation of the cohesion-tension theory of water transport to account for the effects of apoplastic lipids on dynamic surface tension and hydraulic conductance in xylem.', 'corpus_id': 202853165}	10957	[{'doc_id': '227122083', 'title': 'Global crop waste burning – micro-biochar; how a small community development organization learned experientially to address a huge problem one tiny field at a time', 'abstract': 'The world’s 2.5 billion poorest people - small farmers living at the far fringe of the developing world – and their billion or so slightly better off neighbors burn 10.5 billion metric tonnes (tonnes) of crop waste annually. Smoke from their fires reddens the sun, closes airports, shuts schools and governments – and kills millions of people (World Health Organization (WHO). who.int/health-topics/air-pollution#tab=tab_1). Their fires release 16.6 billion tonnes of CO 2 , and emit 9.8 billion tonnes CO 2 e, 1.1 billion tonnes of smog precursors and 66 million tonnes of PM2.5. (Akagi et al., Atmospheric Chem Physics 4039-4071, 2011; Environmental Protection Agency, epa.gov/ghgemissions/understanding-global-warming-potentials; Food and Agriculture Organization, FAOSTAT, http://www.fao.org/faostat/en/#data ) [See Attachments 1–3. For details of the Attachments, please see the section below entitled “Availability of data and materials.”]. No one yet has stopped the burning. Seminars, health warnings, bans, threats, jailings, shootings – nothing has worked, because not one has offered farmers a better alternative. This is the story of how Warm Heart, a small, community development NGO, learned enough about small farmers’ plight to collaborate with them to develop the technology, training and social organization to mobilize villages to form biochar social enterprises. These make it profitable for farmers to convert crop waste into biochar, reducing CO 2 e, smog precursor and PM2.5 emissions, improving health and generating new local income – in short, to address the big three SDGs (1, 2 and 3) from the bottom. Warm Heart, however, wanted more; it wanted a system so appealing that it would spread by imitation and not require outside intervention. Based on what it has learned, Warm Heart wants to teach others that the knowledge to stop the smoke and improve the quality of one’s life does not require outside experts and lots of money. It wants to teach that anyone can learn to create a more sustainable world by themselves. This article traces the experiential learning process by which Warm Heart and its partners achieved their goals and shares Warm Heart’s open source solution. It serves four purposes. The article closely explores an experiential learning process. It details the underlying logic, workings and consequences of crop waste burning in the developing world. It demonstrates the application of this knowledge to the development of a sustainable – even profitable – solution to this global problem that does not require costly outside intervention but can be undertaken by local communities and small NGOs anywhere. Finally, it models how local communities, small NGOs and social investors can turn this global problem into a profitable business opportunity.', 'corpus_id': 227122083, 'score': 0}, {'doc_id': '227070180', 'title': 'EMPIRICAL STUDY OF KEY SUCCESS FACTORS OF CLOSED SUPPLY CHAIN FOR VEGETABLES: THE CASE OF EXPORT FROM YUNAN TO THAILAND', 'abstract': 'This paper analyzes the key success factors of the closed supply chain for vegetables exported from Yunnan and Thailand by empirical study, with the aim of guaranteeing the quality and safety of vegetables. After the introduction, literature reviews with the key term of vegetable quality and safety are presented. Followed section 3 indicates data sources and methodologies from both quantitative and qualitative approaches. Then operation results and discussion are shown to determine the key success factors. The last section proposes some recommendations for member enterprises and related government departments.', 'corpus_id': 227070180, 'score': 0}, {'doc_id': '232238535', 'title': 'Soybean yield and quality as affected by spraying NPK fertilizers compound with amino acids and micronutrients', 'abstract': 'This study aimed to investigation the effects of foliar NPK fertilizers compound with micronutrients spraying at different levels and amino acids on soybean plants. The field experiment was conducted at Al Sharkia Governorate, Egypt in a private farm through a project of soil and water use Dept. of the National Research Center This experiment design with three replicates. Soybean seeds (Giza 21) were sown on the 15 th of June, 2014. The NPK fertilizers contain macronutrients from N, P and K as (2020 -20) foliar application. The micronutrients were mixed from (Fe, Zn, Mn, Mg and B) and added every 15 days with irrigation at two levels 1.5 and 2.0 Kgfed -1 . The amino acids were applied at two levels as 1.5 and 2.0 g/l. The results are as follows: 1-The application of foliar NPK fertilizers with amino acids at 2.0g/l compound with mixed micronutrients as 2.0 Kgfed-1 increased the vegetative growth of soybean plants compared with control. 2The best results of yield and its compound and concentrations of N, P and K were recorded with foliar NPK application compound with increasing the rate of amino acids and mixed micronutrients. Key word : Soybean, foliar fertilization, NPK, Micronutrients, Amino acids.', 'corpus_id': 232238535, 'score': 1}, {'doc_id': '227125383', 'title': 'Integrating Tradition and Technology for Fermented Foods for Maternal Nutrition A Grand Challenge', 'abstract': 'Embracing the tradition of microbial fermentation to transform locally available foods into naturally vitamin-fortified, toxin-free, flavorful, and shelf-stable products could empower local communities to mitigate the impact of COVID-19 on supply chain/food security and improve the health and nutrition of mothers and children in the most vulnerable settings. Historical advances in food processing have largely employed strategies that involve supplementation with micronutrients and additives to improve nutritional content and stability, but these approaches require highly centralized supply chains.1 In addition, chemical additives for preservation, flavor and texture purposes may have unintended consequences of contributing to compromised gut health and increased prevalence of metabolic disease (hypertension, diabetes, obesity).2', 'corpus_id': 227125383, 'score': 0}, {'doc_id': '1793416', 'title': 'Uptake of organic nitrogen by plants.', 'abstract': 'Languishing for many years in the shadow of plant inorganic nitrogen (N) nutrition research, studies of organic N uptake have attracted increased attention during the last decade. The capacity of plants to acquire organic N, demonstrated in laboratory and field settings, has thereby been well established. Even so, the ecological significance of organic N uptake for plant N nutrition is still a matter of discussion. Several lines of evidence suggest that plants growing in various ecosystems may access organic N species. Many soils display amino acid concentrations similar to, or higher than, those of inorganic N, mainly as a result of rapid hydrolysis of soil proteins. Transporters mediating amino acid uptake have been identified both in mycorrhizal fungi and in plant roots. Studies of endogenous metabolism of absorbed amino acids suggest that L- but not D-enantiomers are efficiently utilized. Dual labelled amino acids supplied to soil have provided strong evidence for plant uptake of organic N in the field but have failed to provide information on the quantitative importance of this process. Thus, direct evidence that organic N contributes significantly to plant N nutrition is still lacking. Recent progress in our understanding of the mechanisms underlying plant organic N uptake may open new avenues for the exploration of this subject.', 'corpus_id': 1793416, 'score': 1}, {'doc_id': '227132579', 'title': 'Black cumin (Nigella sativa) seeds: Chemistry, Technology, Functionality, and Applications', 'abstract': 'Nigella sativa L. (botanical family, Ranunculaceae) is one of the most admired medical oilseeds in history. Nigella sativa seeds have been mentioned in the words of the Prophet Mohammed. Nigella sativa seeds contain active phytochemicals (i.e., phenolics, thymoquinone, fatty acids, tocols, sterols, polar lipids, amino acids...etc) with diverse biological effects. Functional extracts, essential oil, and fixed oil from Nigella sativa have been used in novel foods, nutraceuticals and pharmaceuticals. Nigella sativa is evident to promote health and it might serve to be a novel source for modern phytomedicine. Recently, black cumin has become an important topic for research worldwide. This book project aims to build a multidisciplinary discussion on the development and advances in Nigella sativa phytochemistry, cultivation practices, technology, functional characteristics, health-promoting activities as well as the food and non-food applications.', 'corpus_id': 227132579, 'score': 0}, {'doc_id': '231580256', 'title': 'The application of a biostimulant based on tannins affects root architecture and improves tolerance to salinity in tomato plants', 'abstract': 'Roots have important roles for plants to withstand adverse environmental conditions, including salt stress. Biostimulant application was shown to enhance plant resilience towards abiotic stresses. Here, we studied the effect of a tannin-based biostimulant on tomato ( Solanum lycopersicum L.) grown under salt stress conditions. We investigated the related changes at both root architecture (via imaging and biometric analysis) and gene expression (RNA-Seq/qPCR) levels. Moreover, in order to identify the main compounds potentially involved in the observed effects, the chemical composition\xa0of the biostimulant was evaluated by UV/Vis and HPLC-ESI-Orbitrap analysis. Sixteen compounds, known to be involved in root development and having a potential antioxidant properties were identified. Significant increase of root weight (+\u200924%) and length (+\u200923%) was observed when the plants were grown under salt stress and treated with the biostimulant. Moreover, transcriptome analysis revealed that the application of the biostimulant upregulated 285 genes, most of which correlated to root development and salt stress tolerance. The 171 downregulated genes were mainly involved in nutrient uptake. These data demonstrated that the biostimulant is able not only to restore root growth in salty soils, but also to provide the adequate plant nourishment by regulating the expression of essential transcription factors and stress responsive genes.', 'corpus_id': 231580256, 'score': 1}, {'doc_id': '82407421', 'title': 'Can a diazotrophic endophyte originally isolated from lodgepole pine colonize an agricultural crop (corn) and promote its growth', 'abstract': 'Abstract Several diazotrophic Paenibacillus strains were isolated from extracts of surface-sterilized lodgepole pine seedling and tree tissues. One strain, Paenibacillus polymyxa P2b-2R, was found to fix high amounts of nitrogen when reintroduced to the gymnosperms, lodgepole pine and western red cedar. We wanted to determine if P. polymyxa P2b-2R could colonize rhizosphere and internal tissues, fix N and promote growth of corn (Zea mays L), an important agricultural crop. We inoculated corn seeds with P.\xa0polymyxa strain P2b-2R and grew seedlings for 30 days. Corn seedlings were harvested 10, 20 and 30 days after inoculation for evaluation of endophytic and rhizospheric colonization as well as nitrogen fixation and growth promotion. P2b-2R successfully colonized the rhizosphere as well as internal root tissues of corn (i.e., endophytically) with population densities near 106\xa0cfu. Corn seedling growth was promoted significantly by inoculation with P2b-2R with an increase of up to 35% in length and up to 30% in biomass after 30 days of inoculation. Seedlings inoculated with P2b-2R derived up to 20% of foliar nitrogen from atmosphere after 30 days of inoculation, which is significant considering the fact that this was a short growth trial. These results suggest that P. polymyxa P2b-2R might have a broad range of plant hosts and is able to fix N and promote the growth of at least one important agricultural crop i.e. Corn.', 'corpus_id': 82407421, 'score': 1}, {'doc_id': '227127231', 'title': 'Fabrication of graphene carbon nanotubes/zinc oxide composite with enhanced dye-removal ability.', 'abstract': 'A 3-D composite of zinc oxide/graphene carbon nanotubes was fabricated to observe the photocatalytic adsorption and hydrogen evolution to prevent water pollution and to find the application in the energy storage system.', 'corpus_id': 227127231, 'score': 0}, {'doc_id': '44335773', 'title': 'The Plant Hormones: Their Nature, Occurrence, and Functions', 'abstract': 'Plant hormones are a group of naturally occurring, organic substances which influence physiological processes at low concentrations. The processes influenced consist mainly of growth, differentiation and development, though other processes, such as stomatal movement, may also be affected. Plant hormones1 have also been referred to as ‘phytohormones’ though this term is infrequently used.', 'corpus_id': 44335773, 'score': 1}]
98	{'doc_id': '86380746', 'title': 'Toward Contextual Information Retrieval: A Review And Trends', 'abstract': 'Abstract With the growth of electronic data and the expansion of the World Wide Web (WWW), many classic existing retrieval models and systems ignore information about the actual user and search context. Due to the constraints imposed by this fact, context has received more attention in the information retrieval (IR) literature and its interactions over the past decade. In this paper, we emphasize on the importance and implications of context in information retrieval and how can it affect the retrieval systems to operate and behave more intelligently; we highlight some emerging trends of context; we present variety of practical uses of context along with its taxonomies and levels; we discuss how can we model context by these systems along with proposing some practical recommendations to enhance this research area for future research.', 'corpus_id': 86380746}	5160	"[{'doc_id': '225062388', 'title': 'NGAT4Rec: Neighbor-Aware Graph Attention Network For Recommendation', 'abstract': ""Learning informative representations (aka. embeddings) of users and items is the core of modern recommender systems. Previous works exploit user-item relationships of one-hop neighbors in the user-item interaction graph to improve the quality of representation. Recently, the research of Graph Neural Network (GNN) for recommendation considers the implicit collaborative information of multi-hop neighbors to enrich the representation. However, most works of GNN for recommendation systems do not consider the relational information which implies the expression differences of different neighbors in the neighborhood explicitly. The influence of each neighboring item to the representation of the user's preference can be represented by the correlation between the item and neighboring items of the user. Symmetrically, for a given item, the correlation between one neighboring user and neighboring users can reflect the strength of signal about the item's characteristic. To modeling the implicit correlations of neighbors in graph embedding aggregating, we propose a Neighbor-Aware Graph Attention Network for recommendation task, termed NGAT4Rec. It employs a novel neighbor-aware graph attention layer that assigns different neighbor-aware attention coefficients to different neighbors of a given node by computing the attention among these neighbors pairwisely. Then NGAT4Rec aggregates the embeddings of neighbors according to the corresponding neighbor-aware attention coefficients to generate next layer embedding for every node. Furthermore, we combine more neighbor-aware graph attention layer to gather the influential signals from multi-hop neighbors. We remove feature transformation and nonlinear activation that proved to be useless on collaborative filtering. Extensive experiments on three benchmark datasets show that our model outperforms various state-of-the-art models consistently."", 'corpus_id': 225062388, 'score': 1}, {'doc_id': '219569372', 'title': 'A Feature Analysis for Multimodal News Retrieval', 'abstract': 'Content-based information retrieval is based on the information contained in documents rather than using metadata such as keywords. Most information retrieval methods are either based on text or image. In this paper, we investigate the usefulness of multimodal features for cross-lingual news search in various domains: politics, health, environment, sport, and finance. To this end, we consider five feature types for image and text and compare the performance of the retrieval system using different combinations. Experimental results show that retrieval results can be improved when considering both visual and textual information. In addition, it is observed that among textual features entity overlap outperforms word embeddings, while geolocation embeddings achieve better performance among visual features in the retrieval task.', 'corpus_id': 219569372, 'score': 0}, {'doc_id': '220280457', 'title': 'Interactive Path Reasoning on Graph for Conversational Recommendation', 'abstract': 'Traditional recommendation systems estimate user preference on items from past interaction history, thus suffering from the limitations of obtaining fine-grained and dynamic user preference. Conversational recommendation system (CRS) brings revolutions to those limitations by enabling the system to directly ask users about their preferred attributes on items. However, existing CRS methods do not make full use of such advantage --- they only use the attribute feedback in rather implicit ways such as updating the latent user representation. In this paper, we propose Conversational Path Reasoning (CPR), a generic framework that models conversational recommendation as an interactive path reasoning problem on a graph. It walks through the attribute vertices by following user feedback, utilizing the user preferred attributes in an explicit way. By leveraging on the graph structure, CPR is able to prune off many irrelevant candidate attributes, leading to a better chance of hitting user-preferred attributes. To demonstrate how CPR works, we propose a simple yet effective instantiation named SCPR (Simple CPR). We perform empirical studies on the multi-round conversational recommendation scenario, the most realistic CRS setting so far that considers multiple rounds of asking attributes and recommending items. Through extensive experiments on two datasets Yelp and LastFM, we validate the effectiveness of our SCPR, which significantly outperforms the state-of-the-art CRS methods EAR and CRM. In particular, we find that the more attributes there are, the more advantages our method can achieve.', 'corpus_id': 220280457, 'score': 0}, {'doc_id': '220686321', 'title': 'Exploratory Search with Sentence Embeddings', 'abstract': 'Exploratory search aims to guide users through a corpus rather than pinpointing exact information. We propose an exploratory search system based on hierarchical clusters and document summaries using sentence embeddings. With sentence embeddings, we represent documents as the mean of their embedded sentences, extract summaries containing sentences close to this document representation and extract keyphrases close to the document representation. To evaluate our search system, we scrape our personal search history over the past year and report our experience with the system. We then discuss motivating use cases of an exploratory search system of this nature and conclude with possible directions of future work.', 'corpus_id': 220686321, 'score': 0}, {'doc_id': '8312085', 'title': 'Exploring Query Auto-Completion and Click Logs for Contextual-Aware Web Search and Query Suggestion', 'abstract': ""Contextual data plays an important role in modeling search engine users' behaviors on both query auto-completion (QAC) log and normal query (click) log. User's recent search history on each log has been widely studied individually as the context to benefit the modeling of users' behaviors on that log. However, there is no existing work that explores or incorporates both logs together for contextual data. As QAC and click logs actually record users' sequential behaviors while interacting with a search engine, the available context of a user's current behavior based on the same type of log can be strengthened from the user's recent search history shown on the other type of log. Our paper proposes to model users' behaviors on both QAC and click logs simultaneously by utilizing both logs as the contextual data of each other. The key idea is to capture the correlation between users' behavior patterns on both logs. We model such correlation through a novel probabilistic model based on the Latent Dirichlet allocation (LDA) model. The learned users' behavior patterns on both logs are utilized to address not only the application of query auto-completion on QAC logs, but also the click prediction and relevance ranking of web documents on click logs. Experiments on real-world logs demonstrate the effectiveness of the proposed model on both applications."", 'corpus_id': 8312085, 'score': 1}, {'doc_id': '220686459', 'title': 'Understanding BERT Rankers Under Distillation', 'abstract': 'Deep language models, such as BERT pre-trained on large corpora, have given a huge performance boost to state-of-the-art information retrieval ranking systems. Knowledge embedded in such models allows them to pick up complex matching signals between passages and queries. However, the high computation cost during inference limits their deployment in real-world search scenarios. In this paper, we study if and how the knowledge for search within BERT can be transferred to a smaller ranker through distillation. Our experiments demonstrate that it is crucial to use a proper distillation procedure, which produces up to nine times speedup while preserving the state-of-the-art performance.', 'corpus_id': 220686459, 'score': 0}, {'doc_id': '220666080', 'title': 'Conformer-Kernel with Query Term Independence for Document Retrieval', 'abstract': ""The Transformer-Kernel (TK) model has demonstrated strong reranking performance on the TREC Deep Learning benchmark---and can be considered to be an efficient (but slightly less effective) alternative to BERT-based ranking models. In this work, we extend the TK architecture to the full retrieval setting by incorporating the query term independence assumption. Furthermore, to reduce the memory complexity of the Transformer layers with respect to the input sequence length, we propose a new Conformer layer. We show that the Conformer's GPU memory requirement scales linearly with input sequence length, making it a more viable option when ranking long documents. Finally, we demonstrate that incorporating explicit term matching signal into the model can be particularly useful in the full retrieval setting. We present preliminary results from our work in this paper."", 'corpus_id': 220666080, 'score': 1}, {'doc_id': '220713472', 'title': 'METEOR: Learning Memory and Time Efficient Representations from Multi-modal Data Streams', 'abstract': 'Many learning tasks involve multi-modal data streams, where continuous data from different modes convey a comprehensive description about objects. A major challenge in this context is how to efficiently interpret multi-modal information in complex environments. This has motivated numerous studies on learning unsupervised representations from multi-modal data streams. These studies aim to understand higher-level contextual information (e.g., a Twitter message) by jointly learning embeddings for the lower-level semantic units in different modalities (e.g., text, user, and location of a Twitter message). However, these methods directly associate each low-level semantic unit with a continuous embedding vector, which results in high memory requirements. Hence, deploying and continuously learning such models in low-memory devices (e.g., mobile devices) becomes a problem. To address this problem, we present METEOR, a novel MEmory and Time Efficient Online Representation learning technique, which: (1) learns compact representations for multi-modal data by sharing parameters within semantically meaningful groups and preserves the domain-agnostic semantics; (2) can be accelerated using parallel processes to accommodate different stream rates while capturing the temporal changes of the units; and (3) can be easily extended to capture implicit/explicit external knowledge related to multi-modal data streams. We evaluate METEOR using two types of multi-modal data streams (i.e., social media streams and shopping transaction streams) to demonstrate its ability to adapt to different domains. Our results show that METEOR preserves the quality of the representations while reducing memory usage by around 80% compared to the conventional memory-intensive embeddings.', 'corpus_id': 220713472, 'score': 0}, {'doc_id': '209527523', 'title': 'Learning a Joint Search and Recommendation Model from User-Item Interactions', 'abstract': 'Existing learning to rank models for information retrieval are trained based on explicit or implicit query-document relevance information. In this paper, we study the task of learning a retrieval model based on user-item interactions. Our model has potential applications to the systems with rich user-item interaction data, such as browsing and recommendation, in which having an accurate search engine is desired. This includes media streaming services and e-commerce websites among others. Inspired by the neural approaches to collaborative filtering and the language modeling approaches to information retrieval, our model is jointly optimized to predict user-item interactions and reconstruct the item textual descriptions. In more details, our model learns user and item representations such that they can accurately predict future user-item interactions, while generating an effective unigram language model for each item. Our experiments on four diverse datasets in the context of movie and product search and recommendation demonstrate that our model substantially outperforms competitive retrieval baselines, in addition to providing comparable performance to state-of-the-art hybrid recommendation models.', 'corpus_id': 209527523, 'score': 1}, {'doc_id': '219178269', 'title': 'Contextual Re-Ranking with Behavior Aware Transformers', 'abstract': 'In this work, we focus on the contextual document ranking task, which deals with the challenge of user interaction modeling for conversational search. Given a history of user feedback behaviors, such as issuing a query, clicking a document, and skipping a document, we propose to introduce behavior awareness to a neural ranker, resulting in a Hierarchical Behavior Aware Transformers (HBA-Transformers) model. The hierarchy is composed of an intra-behavior attention layer and an inter-behavior attention layer to let the system effectively distinguish and model different user behaviors. Our extensive experiments on the AOL session dataset demonstrate that the hierarchical behavior aware architecture is more powerful than a simple combination of history behaviors. Besides, we analyze the conversational property of queries. We show that coherent sessions tend to be more conversational and thus are more demanding in terms of considering history user behaviors.', 'corpus_id': 219178269, 'score': 1}]"
99	{'doc_id': '235198275', 'title': 'Comparative study of three fingerprint analytical approaches based on spectroscopic sensors and chemometrics for the detection and quantification of argan oil adulteration.', 'abstract': 'BACKGROUND\nArgan oil is one of the purest and rarest oils in the world so that the addition of any further product is strictly prohibited by international regulations. Consequently, it is necessary to establish reliable analytical methods to ensure its authenticity. In this study, three multivariate approaches have been developed and validated using fluorescence, UV-visible, and ATR-FT-MIR spectroscopies.\n\n\nRESULT\nThe application of partial least squares discriminant analysis (PLS-DA) model showed an accuracy of 100%. The quantification of adulteration have been evaluated using partial least square regression (PLS). The PLS model developed from fluorescence spectroscopy provided the best results for the calibration and cross-validation sets, as it showed the highest R2 0.99 and the lowest root mean square error (RMSE) of calibration and cross-validation 0.55, 0.79. The external validation of the three multivariate approaches by the accuracy profile shows that these approaches guarantee reliable and valid results between 0.5% -32%, 7%-32%, and 10%-32% using fluorescence, FT-MIR, and UV-visible spectroscopies respectively.\n\n\nCONCLUSION\nThis study confirmed the feasibility of using spectroscopic sensors (routine technique) for rapid determination of argan oil falsification. This article is protected by copyright. All rights reserved.', 'corpus_id': 235198275}	17836	[{'doc_id': '235308394', 'title': 'A comparison between PLSR, SVMR and NARX network for the mint treatment day prediction based on multisensor system', 'abstract': 'The ability to distinguish between edible aromatic plants treated with insecticides holds the attention of researchers in view of the toxicity of insecticides in human health. The malathion has a distinctive smell it an insecticide widely used to protect mint crops. In the present paper, three regression and artificial intelligence (AI)-based methods such as partial least squares (PLS) regression, support vector machine (SVM) regression, and the nonlinear autoregressive with exogenous input (NARX) were investigated to predict the mint treatment day with malathion. The data used in this work are collected using a multi-sensor system designed based on commercial gas sensors. In this case, the nonlinear autoregressive with exogenous input (NARX) was found the most effective achieving a correlation coefficient (R) of 0.99 with a very minimal mean squared error (MSE) of about 1.10288e-14. Thanks to the right choice of the appropriate algorithm, the mint treatment day could be predicted with a simple multisensor gas array.', 'corpus_id': 235308394, 'score': 0}, {'doc_id': '235300158', 'title': 'Artificial Intelligence Empowered Multispectral Vision Based System for Non-Contact Monitoring of Large Yellow Croaker (Larimichthys crocea) Fillets', 'abstract': 'A non-contact method was proposed to monitor the freshness (based on TVB-N and TBA values) of large yellow croaker fillets (Larimichthys crocea) by using a visible and near-infrared hyperspectral imaging system (400–1000 nm). In this work, the quantitative calibration models were built by using feed-forward neural networks (FNN) and partial least squares regression (PLSR). In addition, it was established that using a regression coefficient on the data can be further compressed by selecting optimal wavelengths (35 for TVB-N and 18 for TBA). The results validated that FNN has higher prediction accuracies than PLSR for both cases using full and selected reflectance spectra. Moreover, our FNN based model has showcased excellent performance even with selected reflectance spectra with rp = 0.978, R2p = 0.981, and RMSEP = 2.292 for TVB-N, and rp = 0.957, R2p = 0.916, and RMSEP = 0.341 for TBA, respectively. This optimal FNN model was then utilized for pixel-wise visualization maps of TVB-N and TBA contents in fillets.', 'corpus_id': 235300158, 'score': 1}, {'doc_id': '234295646', 'title': 'Application of spectra pre-treatments on firmness assessment of intact sapodilla using vis-nir spectroscopy', 'abstract': 'This study aimed to obtain the best calibration model from various spectra pre-treatment methods to assess sapodilla fruit firmness using vis-nir spectroscopy. Before the spectra data measurement, samples were treated with storage of 0, 5 and 10 days at room temperature. Spectra data measurement was carried out using the NirVana AG410 visible and near infrared spectrometer from 312 to 1050 nm with interval of 3 nm. RAW spectra were pre-treated using the multiplicative scatter correction (MSC), standard normal variate (SNV), and Savitzky-Golay first derivative (dg1) with 9 points of smoothing. The calibration model was developed using PLS (partial least squares) method. Validation was done by K fold cross validation method. The results showed the MSC and SNV spectra were able to eliminate noises of RAW spectra, whereas in the dg1 spectra, noises were still visible. The best model was acquired by SNV spectra with R2 (coefficient of determination) of calibration and validation of 0.882 and 0.870, root mean square error of calibration (RMSEC) and root mean square error of cross validation (RMSECV) values of 2.92 and 3.08, and the ratio of performance to deviation (RPD) of 2.76. The result indicated the spectra pre-treatments were able to improve the accuracy of calibration model on assessment of sapodilla fruit firmness.', 'corpus_id': 234295646, 'score': 1}, {'doc_id': '233542127', 'title': 'Classification of aflatoxin B1 naturally contaminated peanut using visible and near-infrared hyperspectral imaging by integrating spectral and texture features', 'abstract': 'Abstract The aim of this study is to carry out a non-destructive, hyperspectral imaging-based method to discriminate between normal and naturally aflatoxin B1 (AFB1) contaminated peanuts. Two varieties of peanut were imaged under a hyperspectral imaging system in the spectral range from 400 to 1000\u202fnm. The reference AFB1 levels were measured by enzyme linked immunosorbent\xa0assay\xa0(ELISA) method, then the peanuts were divided into two categories according to the threshold of 20\u202fppb. The spectral, color and texture features were extracted and integrated to discriminate AFB1 contaminated from normal peanuts. Different pretreatment methods were conducted on the full spectra, the linear discriminant analysis (LDA) results indicated that firstly Savitzky-Golay smoothing (SGS) then standard normal transformation (SNV) could achieve the best discrimination with an accuracy of 90% and 92% for calibration and validation sets respectively. The LDA results of feature integration proved that the optimized spectral features combined with the texture features realized the best classification, with an accuracy of 91% and 94% for calibration and validation sets respectively. Finally, the performance of partial least squares discrimination analysis (PLS-DA) and support vector machine (SVM) was compared with the LDA, the SVM with RBF kernel revealed the best results with an accuracy of 93% and 94% for calibration and validation sets respectively. This study presented the potential of hyperspectral imaging in direct AFB1 contamination classification of peanut, and demonstrated that the combination of texture and spectra features could improve the modelling results.', 'corpus_id': 233542127, 'score': 1}, {'doc_id': '234824365', 'title': 'Enhancing the accuracy of machine learning models using the super learner technique in digital soil mapping', 'abstract': 'Abstract Digital soil mapping approaches predict soil properties based on the relationships between soil observations and related environmental covariates using techniques such as machine learning (ML) models. In this research, a wide range of ML models (12 base learners) were tested in predicting and mapping soil properties. Furthermore, a super learner approach was used to improve model accuracy by combining the predictions of the base learners. A major challenge of using super learner and complex models is that the exact contribution of individual covariates in the overall prediction is not always known. To address this issue, permutation feature importance (PFI) analysis was applied as a model-agnostic interpretation tool. The weights assigned to each ML base learner obtained from super learner, and feature importance values obtained from each ML base learner were used to quantify the contribution of individual covariates on the final prediction. The super learner and PFI techniques were tested by predicting a variety of soil physical and chemical properties of the Urmia Lake playa in Iran. As expected, the results indicated that the super learner had substantially higher accuracies for predicting soil properties in comparison to the individual base learners. For instance, the super learner showed an improved performance in comparison to linear regression by decreasing the root mean square error by an average of 46%. The PFI analysis revealed the important contribution of geomorphic and groundwater data in predicting soil properties. Overall, the proposed approach may be used for improving accuracy of ML models in digital soil mapping.', 'corpus_id': 234824365, 'score': 0}, {'doc_id': '233486524', 'title': 'argoFloats: An R Package for Analyzing Argo Data', 'abstract': 'An R package named argoFloats has been developed to facilitate identifying, downloading, caching, and analyzing oceanographic data collected by Argo profiling floats. The analysis phase benefits from close connections between argoFloats and the oce package, which is likely to be familiar to those who already use R for the analysis of oceanographic data of other kinds. This paper outlines how to use argoFloats to accomplish some everyday tasks that are particular to Argo data, ranging from downloading data and finding subsets to handling quality control and producing a variety of diagnostic plots. The benefits of the R environment are sketched in the examples, and also in some notes on the future of the argoFloats package.', 'corpus_id': 233486524, 'score': 0}, {'doc_id': '235626075', 'title': 'A simple approach to the prediction of soil sorption of organophosphorus pesticides', 'abstract': 'Abstract Organophosphorus pesticides (OP) affect the crops and environments, and the reliable approach to the prediction of soil sorption of pesticides is required. In this respect, we proposed a simple Chemometrics approach, in which the Tchebichef image moment (TM) method was used to extract useful information from the greyscale images of molecular structures and the quantitative model was established by stepwise regression to predict the soil sorption of OPs. Different squared correlation coefficients including the leave-one-out cross-validation (LOO-CV) (Q 2) that concerns the training set and the (R 2 test) which concerns the external independent test set are more than 0.96. This reflects that the established model has considerably high accuracy and reliability. Compared with the literature on the strategies of quantitative structure–property relationship (QSPR), the proposed method is more suitable, in which the established model shows a high predictive ability. Our study provides another effective approach to predict the soil sorption of OPs and also extends the innovative pathway of QSPR modelling.', 'corpus_id': 235626075, 'score': 0}, {'doc_id': '235528816', 'title': 'Quality control of mint species based on UV-VIS and FTIR spectral data supported by chemometric tools', 'abstract': 'Abstract Mints are valued for their specific essential oil used in food, pharmaceutical and cosmetic industry. Chemical compounds differing between species, cause changes in medicinal/pharmacological properties, antioxidant activities or smell sensations. For this reason fast procedure for quality control of at least two most popular mint species, peppermint and spearmint, is the issue at hand. UV-VIS spectrophotometry and FTIR-ATR spectroscopy were used for recording the spectral fingerprints of a collection of more than 20 mint varieties harvested in three periods. Two-step chemometric approach for mints quality control involved SIMCA (Soft Independent Modeling of Class Analogy) to filter out species other than peppermint and spearmint. The samples suspected to be either peppermint or spearmint underwent final discrimination using adequate discrimination tools PLS-DA (Partial Least Squares-Discriminant Analysis) or SVM (Support Vector Machines). The model performance ranged between 60 and 80% depending on spectroscopic data used for model training and the harvest season.', 'corpus_id': 235528816, 'score': 1}, {'doc_id': '235204494', 'title': 'A hyperspectral method of inverting copper signals in mineral deposits based on an improved gradient-boosting regression tree', 'abstract': 'ABSTRACT In the process of mineral deposits formation, elements often show abnormal enrichment. Therefore, the inversion of element contents (the prediction of element contents in mineral deposits through hyperspectral data) has some indicative significance to the exploration of mineral resources. In order to solve the problem of nonlinear copper contents inversion using hyperspectral remote sensing, this paper put forward an improved Gradient Boosting Regression Tree (IGBRT) method to invert copper contents. The main innovations of this paper are: (1) replacing the simple average function with the k-nearest-neighbour weighted average function as the node prediction function to improve the accuracy; (2) using the adaptive reduction step instead of the fixed reduction step to improve the efficiency. At the end of the paper, taking the Altun region, Xinjiang province, China as the experimental area, the iron element which was high correlated with the copper was chosen as the intermediate variable, and the contents relationship between them was established to solve the problem of the small number of copper samples in the study area. Using the improved Gradient Boosting Regression Tree method to predict the iron contents, then the copper contents could be predicted through the relationship between them. The results showed that the IGBRT algorithm had the coefficient of determination (R2) of 0.744, which were 0.185 more than the traditional one, and the learning efficiency was increased by 39.4%. The results of this study can provide a reference for remote sensing inversion of copper contents in mineral deposits of uninhabited areas, and have some important significance for the delineation of copper prospects.', 'corpus_id': 235204494, 'score': 0}, {'doc_id': '235220001', 'title': 'Using Spectral Reflectance to Estimate the Leaf Chlorophyll Content of Maize Inoculated With Arbuscular Mycorrhizal Fungi Under Water Stress', 'abstract': 'Leaf chlorophyll content is an important indicator of the growth and photosynthesis of maize under water stress. The promotion of maize physiological growth by (AMF) has been studied. However, studies of the effects of AMF on the leaf chlorophyll content of maize under water stress as observed through spectral information are rare. In this study, a pot experiment was carried out to spectrally estimate the leaf chlorophyll content of maize subjected to different durations (20, 35, and 55 days); degrees of water stress (75%, 55% and 35% water supply) and two inoculation treatments (inoculation with Funneliformis mosseae and no inoculation). Three machine learning algorithms, including the back propagation (BP) method, least square support vector machine (LSSVM) and random forest (RF) method, were used to estimate the leaf chlorophyll content of maize. The results showed that AMF increased the leaf chlorophyll content, net photosynthetic rate (A), stomatal conductance (gs), transpiration rate (E), and water use efficiency (WUE) of maize but decreased the intercellular carbon dioxide concentration (Ci) of maize and atmospheric vapor pressure deficit (VPD) regardless of the water stress duration and degree. The first-order differential spectral data can better reflect the correlation between leaf chlorophyll content and spectrum of inoculated maize when compared with original spectral data. The BP model performed bestin modeling the maize leaf chlorophyll content, yielding the largest R2-values and smallest root mean square error (RMSE) values, regardless of stress duration. These results provide a reliable basis for the effective monitoring of the leaf chlorophyll content of maize under water stress.', 'corpus_id': 235220001, 'score': 1}]
100	{'doc_id': '155727714', 'title': 'The Role of Credit in Predicting US Recessions', 'abstract': 'We study the role of credit in forecasting US recession periods with probit models. We employ both classical recession predictors and common factors based on a large panel of financial and macroeconomic variables as control variables. Our findings suggest that a number of credit variables are useful predictors of US recessions over and above the control variables both in and out of sample. Especially the excess bond premium, capturing the cyclical changes in the relationship between default risk and credit spreads, is found to be a powerful predictor. Overall, models that combine credit variables, common factors, and classic recession predictors, are found to have the best forecasting performance.', 'corpus_id': 155727714}	20162	"[{'doc_id': '237604470', 'title': 'The Equity Share Cycle', 'abstract': 'Standard financial portfolio theory recommends increasing the equity share of the portfolio as the equity premium rises. On the other hand, purely mechanically high stock prices imply low expected returns. Motivated by these opposite predictions I use data from 16 developed economies between 1873 to 2015 to study the composition of aggregate household wealth portfolio and document that in most countries the equity share has moved in very low-frequency cycles, which take decades to mean revert. I document a negative relationship between equity share and subsequent stock market returns with equity share having significant predictive power over future returns while outperforming the historical mean and traditional predictors in- and out-of-sample. I derive two new decompositions based on present value identities that help to understand these results in a framework of multiple assets classes. Furthermore, a high level of equity share is associated with a higher probability of a financial crisis. Standard asset pricing models have difficulty explaining the presented facts but a behavioral model where households have extrapolative expectations driven by unobserved market sentiment can offer a solution.', 'corpus_id': 237604470, 'score': 0}, {'doc_id': '211323773', 'title': 'Does machine learning help us predict banking crises?', 'abstract': 'Abstract This paper compares the out-of-sample predictive performance of different early warning models for systemic banking crises using a sample of advanced economies covering the past 45 years. We compare a benchmark logit approach to several machine learning approaches recently proposed in the literature. We find that while machine learning methods often attain a very high in-sample fit, they are outperformed by the logit approach in recursive out-of-sample evaluations. This result is robust to the choice of performance metric, crisis definition, preference parameter, and sample length, as well as to using different sets of variables and data transformations. Thus, our paper suggests that further enhancements to machine learning early warning models are needed before they are able to offer a substantial value-added for predicting systemic banking crises. Conventional logit models appear to use the available information already fairly efficiently, and would for instance have been able to predict the 2007/2008 financial crisis out-of-sample for many countries. In line with economic intuition, these models identify credit expansions, asset price booms and external imbalances as key predictors of systemic banking crises.', 'corpus_id': 211323773, 'score': 1}, {'doc_id': '124629997', 'title': 'Identifying business cycle turning points in real time with vector quantization', 'abstract': 'We propose a simple machine-learning algorithm known as Learning Vector Quantization (LVQ) for the purpose of identifying new U.S. business cycle turning points quickly in real time. LVQ is used widely for real-time statistical classification in many other fields, but has not previously been applied to the classification of economic variables, to the best of our knowledge. The algorithm is intuitive and simple to implement, and easily incorporates salient features of the real-time nowcasting environment, such as differences in data reporting lags across series. We evaluate the algorithm’s real-time ability to establish new business cycle turning points in the United States quickly and accurately over the past five NBER recessions. Despite its relative simplicity, the algorithm’s performance appears to be very competitive with those of commonly used alternatives.', 'corpus_id': 124629997, 'score': 1}, {'doc_id': '237233992', 'title': 'The 2000s Housing Cycle With 2020 Hindsight: A Neo-Kindlebergerian View', 'abstract': 'With ""2020 hindsight,\'\' the 2000s housing cycle is not a boom-bust but rather a boom-bust-rebound at both the national level and across cities. We argue this pattern reflects a larger role for fundamentally-rooted explanations than previously thought. We construct a city-level long-run fundamental using a spatial equilibrium regression framework in which house prices are determined by local income, amenities, and supply. The fundamental predicts not only 1997-2019 price and rent growth but also the amplitude of the boom-bust-rebound and foreclosures. This evidence motivates our neo-Kindlebergerian model, in which an improvement in fundamentals triggers a boom-bust-rebound. Agents learn about the fundamentals by observing ""dividends\'\' but become over-optimistic due to diagnostic expectations. A bust ensues when over-optimistic beliefs start to correct, exacerbated by a price-foreclosure spiral that drives prices below their long-run level. The rebound follows as prices converge to a path commensurate with higher fundamental growth. The estimated model explains the boom-bust-rebound with a single fundamental shock and accounts quantitatively for cross-city patterns in the dynamics of prices and foreclosures.', 'corpus_id': 237233992, 'score': 0}, {'doc_id': '36685073', 'title': 'Model Averaging in Markov-Switching Models: Predicting National Recessions with Regional Data', 'abstract': 'This paper estimates and forecasts U.S. business cycle turning points with state-level data. The probabilities of recession are obtained from univariate and multivariate regime-switching models based on a pairwise combination of national and state-level data. We use two classes of combination schemes to summarize the information from these models: Bayesian Model Averaging and Dynamic Model Averaging. In addition, we suggest the use of combination schemes based on the past predictive ability of a given model to estimate regimes. Both simulation and empirical exercises underline the utility of such combination schemes. Moreover, our best specification provides timely updates of the U.S. business cycles. In particular, the estimated turning points from this specification largely precede the announcements of business cycle turning points from the NBER business cycle dating committee, and compare favorably with competing models.', 'corpus_id': 36685073, 'score': 1}, {'doc_id': '237447213', 'title': 'Stock Price Prediction Based on LSTM Deep Learning Model', 'abstract': 'Predicting the stock market is either the easiest or the toughest task in the field of computations. There are many factors related to prediction, physical factors vs. physiological, rational and irrational , capitalist sentiment, market , etc. All these aspects combine to make stock costs volatile and are extremely tough to predict with high accuracy. The prices of a stock market depend very much on demand and supply. High demand stocks will increase in price while heavy selling stocks will decrease. Fluctuations in stock prices affect investor perception and thus there is a need to predict future share prices and to predict stock market prices to make more acquaint and precise investment decisions. We examine data analysis in this domain as a game-changer. This paper proposes that historical value bears the impact of all other market events and can be used to predict future movement. Machine Learning techniques can detect paradigms and insights that can be used to construct surprisingly correct predictions. We propose the LSTM (Long Short Term Memory) model to examine the future price of a stock. This paper is to predict stock market prices to make more acquaint and precise investment decisions.', 'corpus_id': 237447213, 'score': 0}, {'doc_id': '216272124', 'title': 'Global Macro-Financial Cycles and Spillovers', 'abstract': 'We develop a new dynamic factor model that allows us to jointly characterize global macroeconomic and financial cycles and the spillovers between them. The model decomposes macroeconomic cycles into the part driven by global and country-specific macro factors and the part driven by spillovers from financial variables. We consider cycles in macroeconomic aggregates (output, consumption, and investment) and financial variables (equity and house prices, and interest rates). We find that the global macro factor plays a major role in explaining G-7 business cycles, but there are also spillovers from equity and house price shocks onto macroeconomic aggregates. These spillovers operate mainly through the global macro factor rather than the country-specific macro factors (i.e., these spillovers affect business cycles in all G-7 economies) and are stronger in the period leading up to and following the global financial crisis. We find little evidence of spillovers from macroeconomic cycles to financial cycles.', 'corpus_id': 216272124, 'score': 0}, {'doc_id': '236687029', 'title': 'Do the Stock Market Indices Follow a Random Walk?', 'abstract': 'This chapter aims to test the hypothesis of an efficient market, in its weak form, in the stock markets of Brazil, China, South Korea, USA, Spain, Italy, in the period from December 2, 2020 to May 12, 2020. The results show that the market efficiency hypothesis is rejected in all markets. In corroboration the DFA exponents show long memories, which put in question the market efficiency, in its weak form, suggesting that the stock markets analyzed show some predictability. In conclusion, investors should avoid investing in stock markets, at least while this pandemic lasts, and invest in less risky markets in order to mitigate risk and improve the efficiency of their portfolios.', 'corpus_id': 236687029, 'score': 0}, {'doc_id': '55403977', 'title': 'Forecasting Recessions in Real Time', 'abstract': 'We review several methods to define and forecast classical business cycle turning points in Norway. In the paper we compare the Bry - Boschan rule (BB) with a Markov Switching model (MS), using alternative vintages of Norwegian Gross Domestic Product (GDP) as the business cycle indicator. The timing of business cycles depends on the vintage and the method used. BB provides the most reasonable definition of business cycles. The forecasting exercise, where the models are augmented with surveys or financial indicators, respectively, leads to the conclusion that the BB rule applied to density forecasts of GDP augmented with either the consumer confidence index or a financial conditions index provides the most timely predictions of peaks. For troughs, augmenting with surveys or financial indicators does not increase forecastability.', 'corpus_id': 55403977, 'score': 1}, {'doc_id': '211352732', 'title': 'Predicting Recessions: Financial Cycle versus Term Spread', 'abstract': 'Financial cycles can be important drivers of real activity, but there is scant evidence about how well they signal recession risks. We run a horse race between the term spread - the most widely used indicator in the literature - and a range of financial cycle measures. Unlike most papers, ours assesses forecasting performance not just for the United States but also for a panel of advanced and emerging market economies. We find that financial cycle measures have significant forecasting power both in and out of sample, even for a three-year horizon. Moreover, they outperform the term spread in nearly all specifications. These results are robust to different recession specifications.', 'corpus_id': 211352732, 'score': 1}]"
101	{'doc_id': '231933646', 'title': 'Plasmonic Waveguides to Enhance Quantum Electrodynamic Phenomena at the Nanoscale', 'abstract': '—The emerging field of plasmonics can lead to enhanced light-matter interactions at extremely nanoscale regions. Plasmonic (metallic) devices promise to efficiently control both classical and quantum properties of light. Plasmonic waveguides are usually used to excite confined electromagnetic modes at the nanoscale that can strongly interact with matter. The analysis of these nanowaveguides exhibits similarities with their low frequency microwave counterparts. In this article, we review ways to study plasmonic nanostructures coupled to quantum optical emitters from a classical electromagnetic perspective. These quantum emitters are mainly used to generate single-photon quantum light that can be employed as a quantum bit or “qubit’’ in the envisioned quantum information technologies. We demonstrate different ways to enhance a diverse range of quantum electrodynamic phenomena based on plasmonic configurations by using the classical dyadic tensor Green’s function formalism. More specifically, spontaneous emission and superradiance are analyzed by using the Green’s function-based field quantization. The exciting new field of quantum plasmonics will lead to a plethora of novel optical devices for communications and computing applications operating in the quantum realm, such as efficient single-photon sources, quantum sensors, and compact onchip nanophotonic circuits.', 'corpus_id': 231933646}	7499	"[{'doc_id': '221186749', 'title': 'Exceptional drag enhancement of electron-phonon transport properties in 3C-SiC from fully coupled ab-initio analysis', 'abstract': 'We carry out novel ab-initio calculations of fully coupled electron and phonon transport and show that mutual drag causes the thermopower to be dominated by transport of phonons, rather than electrons, at room temperature in the case of \\textit{n}-doped 3C-SiC. The thermopower is insensitive to impurity scattering. Phonon drag also strongly boosts the intrinsic electron mobility, thermal conductivity and the Lorenz number. This work establishes the roles of microscopic scattering mechanisms in the emergence of strong drag effects in transport of the interacting electron-phonon gas.', 'corpus_id': 221186749, 'score': 0}, {'doc_id': '233407664', 'title': 'Emerging dissipative phases in a superradiant quantum gas with tunable decay', 'abstract': 'Exposing a many-body system to external drives and losses can transform the nature of its phases and opens perspectives for engineering new properties of matter. How such characteristics are related to the underlying microscopic processes of the driven and dissipative system is a fundamental question. Here we address this point in a quantum gas that is strongly coupled to a lossy optical cavity mode using two independent Raman drives, which act on the spin and motional degrees of freedom of the atoms. This setting allows us to control the competition between coherent dynamics and dissipation by adjusting the imbalance between the drives. For strong enough coupling, the transition to a superradiant phase occurs, as is the case for a closed system. Yet, by imbalancing the drives we can enter a dissipation-stabilized normal phase and a region of multistability. Measuring the properties of excitations on top of the out-of-equilibrium phases reveals the microscopic elementary processes in the open system. Our findings provide prospects for studying squeezing in non-Hermitian systems, quantum jumps in superradiance, and dynamical spin-orbit coupling in a dissipative setting.', 'corpus_id': 233407664, 'score': 1}, {'doc_id': '233004698', 'title': 'Influence of collective scattering of light on electromagnetic induced transparency', 'abstract': 'Here we present a microscopic model that describes the Electromagnetically Induced Transparency (EIT) phenomenon in the multiple scattering regime. We consider an ensembles of cold three-level atoms, in a Λ configuration, scattering a probe and a control field to the vacuum modes of the electromagnetic field. By first considering a scalar description of the scattering, we show that the light-mediated long-range interactions that emerge between the dipoles narrow the EIT transparency window for increasing densities and sample sizes. For a vectorial description, we demonstrate that near-field interacting terms can critically affect the atomic population transfer in the Stimulated Raman Adiabatic Passage (STIRAP). This result points out that standard STIRAP-based quantum memories in dense and cold atomic ensembles would not reach efficiency high enough for quantum information processing applications.', 'corpus_id': 233004698, 'score': 1}, {'doc_id': '232428089', 'title': 'Quantum corrections to the classical electrostatic interaction between induced dipoles', 'abstract': 'We study, in the presence of an external electrostatic field, the interatomic interaction between two ground-state atoms coupled with vacuum electromagnetic fluctuations within the dipole coupling approximation based on the perturbation theory. We show that, up to the fourth order, the electrostatic-field-induced interatomic interaction is just the classical dipole-dipole interaction, which disagrees with the recent result from Fiscelli et al. [G. Fiscelli et al., Phys. Rev. Lett. 124, 013604 (2020)]. However, to higher orders, there exist external-field-related quantum corrections to the induced classical electrostatic dipole-dipole interaction. In the sixth order, the external field effectively modifies the atomic polarizability to give rise to a two-photon-exchange quantum correction, while in the eighth order, the external field enables an additional process of three-photon exchange which is not allowed in the absence of the external field, and this process generates an r −11 term in the interaction potential in the far regime, where r is the interatomic separation. Numerical estimations show that these external-field-related quantum corrections are much smaller than the two-photon-exchange Casimir-Polder interaction. ∗ ys-hu@qq.com † Corresponding author. jwhu@hunnu.edu.cn ‡ Corresponding author. hwyu@hunnu.edu.cn', 'corpus_id': 232428089, 'score': 1}, {'doc_id': '232138262', 'title': 'OPTICS, ATOMS AND MOLECULES', 'abstract': 'The structure of a solution of the generalized Maxwell–Bloch system of equations describing the strongly pumped interacting two-level atoms is discussed. This structure is represented by means of the corresponding differential equations for each contributing process. The interaction between the processes is introduced through the interaction integral and is illustrated by the specific system of graphs. The method allows one to describe the quantum-field-induced long-range interaction prevailing over short-range collisions and causing the broadening, narrowing, and shifts of an absorption line shape. The description is given in terms of the interaction integrals which couple the collective atomic polarization and population inversion. The contributions from different effects are analyzed with the use of the additivity of the corresponding absorption/reemission rates.', 'corpus_id': 232138262, 'score': 0}, {'doc_id': '232290472', 'title': 'Strong light-matter coupling in \nMoS2', 'abstract': 'Polariton-based devices require materials where light-matter coupling under ambient conditions exceeds losses, but our current selection of such materials is limited. Here we measured the dispersion of polaritons formed by the A and B excitons in thin MoS2 slabs by imaging their optical near fields. We combined fully tunable laser excitation in the visible with a scattering near-field optical microscope to excite polaritons and image their optical near fields. We obtained the properties of bulk MoS2 from fits to the slab dispersion. The in-plane excitons are in the strong regime of light-matter coupling with a coupling strength (40− 100 meV) that exceeds their losses by at least a factor of two. The coupling becomes comparable to the exciton binding energy, which is known as very strong coupling. MoS2 and other transition metal dichalcogenides are excellent materials for future polariton devices. 1 ar X iv :2 10 3. 10 86 7v 1 [ co nd -m at .m tr lsc i] 1 9 M ar 2 02 1', 'corpus_id': 232290472, 'score': 0}, {'doc_id': '231924620', 'title': ""Casimir-Lifshitz forces and plasmons in a structure of two dielectric rods: Green's function method of electrodynamics"", 'abstract': 'A new model for calculating the Casimir-Lifshitz force per unit length for two dielectric rods is proposed, based on the Green’s function method of classical electrodynamics and the Lorentz model for permittivity. For metal rods, it is proposed to use the Drude-Smith model. It is shown that the origin of the force is associated with the fluctuation excitation of slow coupled long-wave surface polaritons. A simple model of the dispersion forces for two CNTs is proposed.', 'corpus_id': 231924620, 'score': 1}, {'doc_id': '232428377', 'title': 'Controlling and focusing of in-plane hyperbolic phonon polaritons in {\\alpha}-MoO3 with plasmonic antenna', 'abstract': 'Hyperbolic phonon polaritons (HPhPs) sustained in van der Waals (vdW) materials exhibit extraordinary capabilities of confining long-wave electromagnetic fields to the deep subwavelength scale. In stark contrast to the uniaxial vdW hyperbolic materials such as hexagonal boron nitride (hBN), the recently emerging biaxial hyperbolic materials such as α-MoO3 and α-V2O5 further bring new degree of freedoms in controlling light at the flatland, due to their distinctive in-plane hyperbolic dispersion. However, the controlling and focusing of such in-plane HPhPs are to date remain elusive. Here, we propose a versatile technique for launching, controlling and focusing of in-plane HPhPs in α-MoO3 with geometrically designed plasmonic antennas. By utilizing high resolution near-field optical imaging technique, we directly excited and mapped the HPhPs wavefronts in real space. We find that subwavelength manipulating and focusing behavior are strongly dependent on the curvature of antenna extremity. This strategy operates effectively in a broadband spectral region. These findings can not only provide fundamental insights into manipulation of light by biaxial hyperbolic crystals at nanoscale, but also open up new opportunities for planar nanophotonic applications.', 'corpus_id': 232428377, 'score': 0}, {'doc_id': '232185562', 'title': 'Imaging the transverse spin density of light via electromagnetically induced transparency', 'abstract': 'When a light beam is strongly laterally confined, its field vector spins in a plane not perpendicular to the propagation direction, leading to the presence of transverse spin angular momentum, which plays a crucial role in the field of chiral quantum optics. The existing techniques to measure the transverse spin density require complex setups and sophisticated time-consuming procedures. Here, we propose a scheme to measure the transverse spin density of an optical field in real time using a multi-level atomic medium. The susceptibility of the medium is spatially modulated by the transverse spin via electromagnetically induced transparency. The distribution of the transverse spin is then extracted by measuring the distributions of the Stokes parameters of another collimated probe field.', 'corpus_id': 232185562, 'score': 0}, {'doc_id': '232417548', 'title': 'Molecular Interactions Induced by an Electric Field in Quantum Mechanics and Quantum Electrodynamics', 'abstract': 'We study the interaction between two neutral atoms or molecules subject to a uniform static electric field, using quantum mechanics (QM) and quantum electrodynamics (QED) applied to coupled harmonic Drude oscillators. Our focus is to understand the interplay between dispersion interactions and field-induced electrostatics and polarization in both retarded and non-retarded regimes. We present an exact solution for two coupled oscillators using QM and Rayleigh-Schrödinger perturbation theory, demonstrating that the external field controls the strength of different intermolecular interactions and relative orientations of the molecules. In the retarded regime described by QED and rationalized by stochastic electrodynamics, our analysis shows that field-induced electrostatics and polarization terms remain unchanged (in isotropic and homogeneous vacuum) compared to the non-retarded QM description, in contrast to a recent work. Our framework combining four complementary theoretical approaches paves the way to a systematic description and enhanced understanding of molecular interactions under the combined action of external and vacuum fields.', 'corpus_id': 232417548, 'score': 1}]"
102	"{'doc_id': '216135732', 'title': 'Toppling democracy', 'abstract': ""Abstract Thailand's 2006 royalist coup is best understood by reference to the historical context of democratisation. The dominant historiography of Thai democratisation is either a simplistic liberal view of anti-military democracy or a royalist one that is ultimately anti-democratic. This article offers a serial history of democratisation that allows us to see the long duration of layered historical processes. As democratisation is fundamentally a break from the centralised absolute monarchy, the monarchy and the monarchists, despite their up and down political fortunes, have probably played the most significant role in shaping Thai democracy since 1932. Despite that, their role and place in history has been overlooked due to the perception that they are “above politics.” This article argues that, since 1973 in particular, the monarchists have assumed the status of the superior realm in Thai politics that claims the high moral ground above politicians and normal politics. With distaste for electoral politics, and in tacit collaboration with the so-called people's sector, activists and intellectuals, they have undermined electoral democracy in the name of “clean politics” versus the corruption of politicians. The 2006 coup that toppled democracy was the latest effort of the monarchists to take control of the democratisation process."", 'corpus_id': 216135732}"	17690	"[{'doc_id': '159316615', 'title': ""Thailand's Hyper-royalism: Its Past Success and Present Predicament"", 'abstract': None, 'corpus_id': 159316615, 'score': 1}, {'doc_id': '233764950', 'title': 'Liberalism and Great Upheaval: What Did Classical Liberals Do in the Tsarist Russia?', 'abstract': 'Efficient constitutional change depends on ability of bargaining parties to overcome such inherent problems of political change as commitment and credibility (Galiani, Torrens, and Yanguas, 2014; Congleton, 2011; Boettke and Coyne, 2009). This paper studies how constitutional bargaining leads to a negative sum-game in a transitional illiberal environment. We use a historical example of the Russian constitutional monarchy (1905-1917) to demonstrate that an exchange-based constitutional change within the king-council model leads to an inefficient outcome when bargaining parties fail to trade political authority for policy results. The historical example of the Russian constitutional monarchy shows how both radicalization of the liberal parliamentary majority and pseudo-constitutionalism of Nicholas II undermined efficiency of the legislative assembly. We also find that nationality-based politics undermined the constitutional bargaining by radicalizing both the liberal movement and the tsar.', 'corpus_id': 233764950, 'score': 0}, {'doc_id': '163956036', 'title': 'Peter A. Jackson, Buddhadāsa: Theravada Buddhism and Modernist Reform in Thailand', 'abstract': None, 'corpus_id': 163956036, 'score': 1}, {'doc_id': '235236448', 'title': 'DEMOCRATISATION AND THE MEDIA A PRELIMINARY DISCUSSION OF EXPERIENCES IN EUROPE AND ASIA', 'abstract': 'Studies of the relationship of the mass media and democracy either rely very heavily on American and European experience, or they are focused studies of national cases. There are relatively few attempts to generalise from the rich diversity of the last thirty years. This article is an initial attempt to compare the experiences of the former communist countries of Central and Eastern Europe with the various dictatorships of East and South East Asia. It is argued that, despite the fact that there are important differences between communist and capitalist dictatorships, it is nevertheless theoretically possible to compare the two categories. An outline the-ory of the relationship between media and social power in both is introduced. In comparing Europe and Asia, it is argued that oppositions between \x93Western\x94 and \x93Asian\x94 values are not at all useful, despite their considerable popular currency at the time of writing (September 2001). In both cases, key democratic values like reason and media freedom have a contradictory status that defies continental generalisations. The article continues with an examination of the decay of the undemocratic regimes in the two continents and points to their differences and similarities. In particular, it is noted that there are surprising similarities of outcome in the media democratisation process despite the differences in starting point. The absence of radical democratic movements in Europe is contrasted with their apparently greater prominence in some parts of Asia. It is speculated that the future in China will involve political crises that will provide further opportunities for radical democratic movements. COLIN SPARKS Colin Sparks is Professor at CCIS University of', 'corpus_id': 235236448, 'score': 0}, {'doc_id': '85460298', 'title': 'The Quest for “Siwilai”: A Geographical Discourse of Civilizational Thinking in the Late Nineteenth and Early Twentieth-Century Siam', 'abstract': 'On 27 december 1932, prince bhidayalongkorn, the President of the Royal Institute of Siam, delivered a special lecture titled “What are the conditions called ‘siwilai’?” [Phawa yangrai no thi riakwa khwam siwilai]. Transliterated from the English word civilized, the term was widely used in public without elaboration. Bhidayalongkorn reported that there was a debate whether Siam was or was not yet siwilai, often referring to England, China, Haiti, Tibet, and many other countries, but it was not clear what made them siwilai or not siwilai. He went on debunking the general understanding that wealth, power, territory, monogamy, gender equity, cleanliness, dress, etiquette, or mechanization constituted the notion of siwilai. The meaning was slippery, no matter how anybody tried to claim or use it politically (Bhidayalongkorn 1970).', 'corpus_id': 85460298, 'score': 1}, {'doc_id': '234000703', 'title': 'Relationship between Literature & Politics in Selected Malay Novels', 'abstract': 'It cannot be denied that politics has a connection with the field of literature as politics is often a source of ideas and literature a ""tool"" to politicians especially in Malaysia. Baha Zain once said, ""writers or literates often make politicians a screen to show their dreams. Politicians usually see writers as the dreamers who are like branches of life in a tree of more important matters""(1982: 3). When discussing the two areas, the socio-economic issues that spark the themes of creative work are inevitable. In general, fiction writers cannot escape the political, socio-economic environment of a country, especially in Malaysia. The more political the situation of a country, the more political the literary works becomes as they are used as a voice for the community to support or protest the socio-economic administration of a government. Thus, a society’s ideology and thoughts or expectations are often reflected in various genres of creative and flow works such as novels, short stories, poetry and drama. This research explores the relationship between literature and politics of Malaysia in the 1980sinparticular, the novel genre written by selected Malay authors in the mainstream Malay literature. Introduction These recent decades have shown a tremendous tendency among Malaysian writers to tackle political questions in their works, either expressly or implicitly. Literary authors often make their work as a medium of political pronunciation. Literary works often produce political insights as well as their political ideology in their creative reality. Political novels are written expressly to raise issues, events and characters linked to society and indirectly to politics, government and power. Faulks (1992), states that politics is linked to institutions developed in a country and formed between the will of society divided into rulers and citizens. The Oxford Dictionary (1994) describes a more comprehensive and political understanding of the relationship between literature and politics; that is “knowledge and governmental arts, knowledge related to the form, organization and administration of the country or part thereof and its relationship with other countries” (Ariff, 2016). International Journal of Academic Research in Business and Social Sciences Vol. 9 , No. 12, December, 2019, E-ISSN: 2222-6990 © 2019 HRMARS 907 Bearing in mind these definitions, literary works are not consensual depictions of a multitude nor a power but often expressed controversial views on sensitive topics related to religion or views in opposition of the government depending largely on its creator, in this case the writers. Conflict that is transposed into pages and confined in a fictional world is “allowed” therefore to enjoy a certain amount of freedom between the rulers and the general public in the ambivalent relationship which authors who thrive upon and generally literature survives upon. As Leo Tolstoy, George Bernard Shaw or Salman Rushdie have done, the act of writing can and will interfere with the power play in politics of any nation, government, society, or a particular institution. This is further reinforced by Milton C. Albrecht’s (1954) hypothesis that: literature “reflects” society; its supposed converse is that literature influences or “shapes” society. A third hypothesis is that literature functions socially to maintain and stabilize, if not to justify and sanctify, the social order, which may be called the “social-control” theory (Milton C. Albrecht, 1954). Ideally, it can be interpreted as a belief that literature is the platform in which people, the community or the state responds. As mentioned by (Sapardi, 2000), it means that literature should not be separated from political attitudes, authors\' ideologies, publishers and readers. According to Sapardi in his study of Indonesian literature, the prohibition and restriction of literary criticism of the government is a recognition of the power of the written word, as hypothesized by Albrecht. More importantly, it acknowledges that literature has a broad influence on the way people think and act. Literature is definitely more than an aesthetic work but a form of control on ideology, politics, economy, socio-culture, law and religion of a nation, Malaysia literature notwithstanding. Malaysian Literature As stated by Sikana (2004) in his scrutiny of Malay Literature, he observes that: ""Literature and politics are like hunters with tigers. What hunters are trying to hunt is not a tiger, maybe a deer or a jungle. But it turns out that the hunters in the game have not always been tortured by tigers. To the tiger, the hunter became his rival. The food is a dream hunter. For the hunter the tiger becomes a barrier."" His analogy of the hunter and the tiger is an apt and accurate description of the relationship between the Malay authors and politics in Malaysian Malay literature. The ambivalence is balanced by an unacknowledged dependence between the political scenario in Malaysia and Malay literature as allegorized in there being no hunt if there was no tiger and vice versa. The parley between literature and politics is also reflected in the use of Malay literature in the political arena. For example, the symbolic betrayal and guilt between the classical friendship of Hang Tuah and Hang Jebat, two classical Malay heroes of the Malacca Sultanate often used as a symbol of ideal Malay Heroism that has been used countless times by Malay writers when alluding to the sacrifice of Hang Tuah in preserving Malay sovereignity. According International Journal of Academic Research in Business and Social Sciences Vol. 9 , No. 12, December, 2019, E-ISSN: 2222-6990 © 2019 HRMARS 908 to Hassan (2016), the impact of Korban Hang Tuah the poem which inspired Raja Raja Muda Perak, Raja Nazrin Shah ibni Sultan Azlan Muhibuddin Shah when he launched the book The Malays at Universiti Kebangsaan Malaysia in 2009 and also Hishamuddin Tun Hussein who went on to quote it during his opening speech at the annual Umno Youth convention in 1999, was unforseen. As stated by Zurinah (2009), in her own blog claims that it was written as a call to reevaluate the Malay circumstances; if Hang Tuah did not stop Hang Jebat from committing treason by killing the King. The allusion to the conflict arising from loyalty to King and country over loyalty to friend and comrade is lauded by those in government and the Malay rulers. Clearly, Zurinah Hassan has highlighted the pride of the Malays in their loyalty in unity. But Shaharuddin (1993) mentioned in his book entitled Konsep Wira Dalam masyarakat Melayu, the evocation of Hang Tuah’s act as an example of blind unthinking loyalty provides other avenues of interpretation to the poem. The ambivalent nature of the relationship between Malaysian politics and Malaysian Malay literature appears to be harnessed by the fluidity of interpretation enabling a two-way power confluence depending on who is in power. This research will focus on a cross section of selected Malay novels that received greatest critical response in regards to the political themes that permeated the plot, theme, characterization and critical points that it sent across at the point of political changes in Malaysia. A. Samad Said’s critically acclaimed nationalistic novel Salina (1961) which became a text-book for the secondary level represents the post-merdeka writers, while SHIT (1999), a controversial novel by National Laureate Shahnon Ahmad is representative of the height of Malay power in the form of UMNO’s superiority in the governance of Barisan National coallition in Malaysia. Faisal Tehrani’s Perempuan Politikus Melayu (2002), the work of Faisal Tehrani, a prolific writer’s opposition views of the current governance at that time and Azizi Hj Abdullah’s Kawin-Kawin (2001) winner of the Sako 2 awards representative of the prevalent Islamic writings before millennial. Ariff (2016) mentioned that political questions in Malay society have been presented in many classical traditional works such as Malay History (Sulalatus Salatin) and Hikayat Hang Tuah. They like the early Greek mythologies. Since the 1920s Malay novels reflected a local colour genre that proliferated storytelling stories, history books, constitutions, folklore, poems and poems. As mentioned by Ariff (2016), in this article “Malay political novels” or “Malay Protest novels” refer to novels written in Malay that are produced by Malaysian authors with critical views of political matters, presented in cynical, figurative or unambiguous form. Throughout the eight decades of development, modern Malay literature has changed not only in technique and focus but in its role in society pertaining to the political climate of Malaysia. While most works of literature stayed away from politically sensitive issues pertaining to Malaysian politics, there were some who managed to free themselves from being vehicles of government propaganda. According to Talib (2010), observed that Shahnon Ahmad\'s narratology in his novels express current political issues that target readers’ experiences specifically their memory, power of recollection and understanding of the present political issues occurring in the country. The International Journal of Academic Research in Business and Social Sciences Vol. 9 , No. 12, December, 2019, E-ISSN: 2222-6990 © 2019 HRMARS 909 current political issues presented in writing as a narrative strategy can create political awareness on certain aspects of the reader\'s heart, functioning to mobilize the reader\'s mind to immediately remember and interpret the actual events or events that have passed whether involving political, economic or religious matters. The author creatively translates his interpretation of events and political issues based on his own experience of the issue or perspective directly and indirectly. The changing political scenario undoubtedly influences the type of writing produced in a nation, Malaysia is no exceptio', 'corpus_id': 234000703, 'score': 0}, {'doc_id': '145569503', 'title': 'Thailand Buddhism, Legitimation, and Conflict: The Political Functions of Urban Thai Buddhism . By Peter A. Jackson. Singapore: Institute of Southeast Asian Studies, 1989, Pp. xiii, 245. Notes, Glossary, Bibliography, Index.', 'abstract': None, 'corpus_id': 145569503, 'score': 1}, {'doc_id': '235384779', 'title': ""Buddhist Law in Burma: A History of Dhammasattha Texts and Jurisprudence, 1250–1850 By D. Christian Lammert. Honolulu: University of Hawai'i Press, 2018. 304pp. ISBN 9780824872601, URL: https://uhpress.hawaii.edu/product/buddhist-law-in-burma-a-history-of-dhammasattha-texts-and-jurisprudence-1250-18"", 'abstract': 'missible). I wonder whether it would be possible to recognize a halal entertainer like Siti in stricter Islamic countries. How are Arab nations characterizing Umm Kulthum among increasingly fundamental Muslims today? I am somewhat perplexed as to why the editors decided to include the last chapter, which focuses on Hatsune Miku, as this particular phenomenon has no subjectivity and is instead merely a software operated by those who purchase it. Although I agree that it is important for popular music studies to research the creation of virtual idols based on individual user preferences, we should introduce different perspectives to analyze this phenomenon. In particular, we should focus on those who operate Hatsume Miku. In this sense, the distinction that was made previously between idols and fans could be transformed into more ambiguous and multitiered relations mediated by an operable idol. The main themes of this book (female singers’ subjectivities and agencies in Asia at the time of modernization) do not emerge from the software itself but through the operators, whose gender is not always recognized by the public. I think that this topic should have been treated as another project. However, there can be no doubt that this book provides various ideas and inspirations for carrying out further projects on musical performance, gendered relations and Asian modernities.', 'corpus_id': 235384779, 'score': 0}, {'doc_id': '153698050', 'title': 'Sufficient Citizens: Moderation and the Politics of Sustainable Development in Thailand', 'abstract': ""After the 1997 Asian markets crash, the theory of the “sufficiency economy” altered the discourse of development in Thailand. Emphasizing Buddhist notions of moderation and “enough” sufficiency recast development as a means to temper socio-economic volatility by reforming consumer affect. Sufficiency theory pinned the nation's economic upheaval on excessive desires, attempting to intervene in these impulses through projects rooted in personal moderation and communality. In this article, I explore the relationship between politics, citizenship, and sufficiency. Through ethnographic analysis of cases from urban squatter settlements taking part in state-driven participatory urban planning policies, I argue that sufficiency has become central to debates over citizenship in contemporary Thailand. Urban planners use sufficiency to attempt to produce what they term “personal development” and to attempt to defuse political claims by training the poor to moderate their demands by learning “enough.” I also show how residents of squatter communities use the language and practices of sufficiency to make political claims and to demonstrate their legitimacy as citizens with rights to the city. In doing so, I demonstrate how notions of sustainable development tied to moderation extend and potentially interrupt social inequalities."", 'corpus_id': 153698050, 'score': 1}, {'doc_id': '234683661', 'title': 'Wondrous Modernity: Refashioning Religion in Urban India', 'abstract': 'vival has to be ensured by a democratic state? Auerbach has some striking insights on party work as a form of employment. For instance, he finds that during elections, “a slum leader in Jaipur . . . was able to make Rs. 30,000—an amount equal to almost six months of income for most of his neighbors . . . These incentives can go up to a lakh (Rs. 100,000) depending on the importance of the slum leader” (p. 108). Even if it is not election time, everyday claims-making is also a source of income, with seemingly fixed rates for gaining access to various forms of entitlements (p. 108). In short, the party workers are a clear triumph of democratic claims-making, but do they also reveal deeper structural insights on India’s transition narrative? Both of these books will be invaluable reading for scholars of South Asia and for anyone interested in “slums” in the Global South. Not only do they pry open the category and workings of “slums,” they also raise seminal questions on the constitutive role of slums in the capitalist transformations in the Global South.', 'corpus_id': 234683661, 'score': 0}]"
103	{'doc_id': '235360823', 'title': 'A survey on applications of deep learning in microscopy image analysis', 'abstract': 'Advanced microscopy enables us to acquire quantities of time-lapse images to visualize the dynamic characteristics of tissues, cells or molecules. Microscopy images typically vary in signal-to-noise ratios and include a wealth of information which require multiple parameters and time-consuming iterative algorithms for processing. Precise analysis and statistical quantification are often needed for the understanding of the biological mechanisms underlying these dynamic image sequences, which has become a big challenge in the field. As deep learning technologies develop quickly, they have been applied in bioimage processing more and more frequently. Novel deep learning models based on convolution neural networks have been developed and illustrated to achieve inspiring outcomes. This review article introduces the applications of deep learning algorithms in microscopy image analysis, which include image classification, region segmentation, object tracking and super-resolution reconstruction. We also discuss the drawbacks of existing deep learning-based methods, especially on the challenges of training datasets acquisition and evaluation, and propose the potential solutions. Furthermore, the latest development of augmented intelligent microscopy that based on deep learning technology may lead to revolution in biomedical research.', 'corpus_id': 235360823}	11033	"[{'doc_id': '235400219', 'title': 'Automated cell tracking using StarDist and TrackMate [version 1; peer review: 2 approved, 1 approved with reservations]', 'abstract': ""The ability of cells to migrate is a fundamental physiological process involved in embryonic development, tissue homeostasis, immune surveillance, and wound healing. Therefore, the mechanisms governing cellular locomotion have been under intense scrutiny over the last 50 years. One of the main tools of this scrutiny is live-cell quantitative imaging, where researchers image cells over time to study their migration and quantitatively analyze their dynamics by tracking them using the recorded images. Despite the availability of computational tools, manual tracking remains widely used among researchers due to the difficulty setting up robust automated cell tracking and large-scale analysis. Here we provide a detailed analysis pipeline illustrating how the deep learning network StarDist can be combined with the popular tracking software TrackMate to perform 2D automated cell tracking and provide fully quantitative readouts. Our proposed protocol is compatible with both fluorescent and widefield images. It only requires freely available and open-source software (ZeroCostDL4Mic and Fiji), and does not require any coding knowledge from the users, making it a versatile and powerful tool for the field. We demonstrate this pipeline's usability by automatically tracking cancer cells and T cells using fluorescent and brightfield images. Importantly, we provide, as supplementary information, a detailed step-by-step protocol to allow researchers to implement it with their images. Open Peer Review"", 'corpus_id': 235400219, 'score': 1}, {'doc_id': '226307068', 'title': 'Deep machine learning-assisted multiphoton microscopy to reduce light exposure and expedite imaging', 'abstract': 'Two-photon excitation fluorescence (2PEF) allows imaging of tissue up to about one millimeter in thickness. Typically, reducing fluorescence excitation exposure reduces the quality of the image. However, using deep learning super resolution techniques, these low-resolution images can be converted to high-resolution images. This work explores improving human tissue imaging by applying deep learning to maximize image quality while reducing fluorescence excitation exposure. We analyze two methods: a method based on U-Net, and a patch-based regression method. Both methods are evaluated on a skin dataset and an eye dataset. The eye dataset includes 1200 paired high power and low power images of retinal organoids. The skin dataset contains multiple frames of each sample of human skin. High-resolution images were formed by averaging 70 frames for each sample and low-resolution images were formed by averaging the first 7 and 15 frames for each sample. The skin dataset includes 550 images for each of the resolution levels. We track two measures of performance for the two methods: mean squared error (MSE) and structural similarity index measure (SSIM). For the eye dataset, the patches method achieves an average MSE of 27,611 compared to 146,855 for the U-Net method, and an average SSIM of 0.636 compared to 0.607 for the U-Net method. For the skin dataset, the patches method achieves an average MSE of 3.768 compared to 4.032 for the U-Net method, and an average SSIM of 0.824 compared to 0.783 for the U-Net method. Despite better performance on image quality, the patches method is worse than the U-Net method when comparing the speed of prediction, taking 303 seconds to predict one image compared to less than one second for the U-Net method.', 'corpus_id': 226307068, 'score': 0}, {'doc_id': '233997893', 'title': 'Neural network strategies for plasma membrane selection in quantitative fluorescence microscopy images.', 'abstract': 'In recent years there has been an explosion of fluorescence microscopy studies of live cells in the literature. The analysis of the images obtained in these studies often requires labor-intensive manual annotation to extract meaningful information. In this study, we explore the utility of a neural network approach to recognize, classify, and select plasma membranes in high resolution images, thus greatly speeding up data analysis and reducing the need for personnel training for highly repetitive tasks. Two different strategies are tested: 1) a semantic segmentation strategy and 2) a sequential application of an object detector followed by a semantic segmentation network. Multiple network architectures are evaluated for each strategy, and the best performing solutions are combined and implemented in the Recognition Of Cellular Membranes (ROC-ME) software. We show that images annotated manually and with the ROC-ME software yield identical results, by comparing Förster resonance energy transfer (FRET) binding curves for the membrane protein FGFR3. The approach that we describe in this work can be applied to other image selection tasks in cell biology.', 'corpus_id': 233997893, 'score': 0}, {'doc_id': '201208973', 'title': 'DeLTA: Automated cell segmentation, tracking, and lineage reconstruction using deep learning', 'abstract': 'Microscopy image analysis is a major bottleneck in quantification of single-cell microscopy data, typically requiring human supervision and curation, which limit both accuracy and throughput. To address this, we developed a deep learning-based image analysis pipeline that performs segmentation, tracking, and lineage reconstruction. Our analysis focuses on time-lapse movies of Escherichia coli cells trapped in a “mother machine” microfluidic device, a scalable platform for long-term single-cell analysis that is widely used in the field. While deep learning has been applied to cell segmentation problems before, our approach is fundamentally innovative in that it also uses machine learning to perform cell tracking and lineage reconstruction. With this framework we are able to get high fidelity results (1% error rate), without human supervision. Further, the algorithm is fast, with complete analysis of a typical frame containing ∼150 cells taking <700msec. The framework is not constrained to a particular experimental set up and has the potential to generalize to time-lapse images of other organisms or different experimental configurations. These advances open the door to a myriad of applications including real-time tracking of gene expression and high throughput analysis of strain libraries at single-cell resolution. Author Summary Automated microscopy experiments can generate massive data sets, allowing for detailed analysis of cell physiology and properties such as gene expression. In particular, dynamic measurements of gene expression with time-lapse microscopy have proved invaluable for understanding how gene regulatory networks operate. However, image analysis remains a key bottleneck in the analysis pipeline, typically requiring human supervision and a posteriori processing. Recently, machine learning-based approaches have ushered in a new era of rapid, unsupervised image analysis. In this work, we use and repurpose the U-Net deep learning algorithm to develop an image processing pipeline that can not only accurately identify the location of cells in an image, but also track them over time as they grow and divide. As an application, we focus on multi-hour time-lapse movies of bacteria growing in a microfluidic device. Our algorithm is accurate and fast, with error rates near 1% and requiring less than a second to analyze a typical movie frame. This increase in speed and fidelity has the potential to open new experimental avenues, e.g. where images are analyzed on-the-fly so that experimental conditions can be updated in real time.', 'corpus_id': 201208973, 'score': 1}, {'doc_id': '235414098', 'title': 'Assessment of deep learning algorithms for 3D instance segmentation of confocal image datasets', 'abstract': 'Segmenting three dimensional microscopy images is essential for understanding phenomena like morphogenesis, cell division, cellular growth and genetic expression patterns. Recently, deep learning (DL) pipelines have been developed which claim to provide high accuracy segmentation of cellular images and are increasingly considered as the state-of-the-art for image segmentation problems. However, it remains difficult to define their relative performance as the concurrent diversity and lack of uniform evaluation strategies makes it difficult to know how their results compare. In this paper, we first made an inventory of the available DL methods for 3D segmentation. We next implemented and quantitatively compared a number of representative DL pipelines, alongside a highly efficient non-DL method named MARS. The DL methods were trained on a common dataset of 3D cellular confocal microscopy images. Their segmentation accuracies were also tested in the presence of different image artifacts. A new method for segmentation quality evaluation was adopted which isolates segmentation errors due to under/over segmentation. This is complemented with new visualization strategies that make interactive exploration of segmentation quality possible. Our analysis shows that the DL pipelines have very different levels of accuracy. Two of them show high performance, and offer clear advantages in terms of adaptability to new data.', 'corpus_id': 235414098, 'score': 0}, {'doc_id': '235200861', 'title': 'ImageJ and CellProfiler: Complements in Open-Source Bioimage Analysis.', 'abstract': ""ImageJ and CellProfiler have long been leading open-source platforms in the field of bioimage analysis. ImageJ's traditional strength is in single-image processing and investigation, while CellProfiler is designed for building large-scale, modular analysis pipelines. Although many image analysis problems can be well solved with one or the other, using these two platforms together in a single workflow can be powerful. Here, we share two pipelines demonstrating mechanisms for productively and conveniently integrating ImageJ and CellProfiler for (1) studying cell morphology and migration via tracking, and (2) advanced stitching techniques for handling large, tiled image sets to improve segmentation. No single platform can provide all the key and most efficient functionality needed for all studies. While both programs can be and are often used separately, these pipelines demonstrate the benefits of using them together for image analysis workflows. ImageJ and CellProfiler are both committed to interoperability between their platforms, with ongoing development to improve how both are leveraged from the other. © 2021 Wiley Periodicals LLC. Basic Protocol 1: Studying cell morphology and cell migration in time-lapse datasets using TrackMate (Fiji) and CellProfiler Basic Protocol 2: Creating whole plate montages to easily assess adaptability of segmentation parameters."", 'corpus_id': 235200861, 'score': 1}, {'doc_id': '233745104', 'title': 'Volumetric Semantic Instance Segmentation of the Plasma Membrane of HeLa Cells', 'abstract': 'In this work, the unsupervised volumetric semantic segmentation of the plasma membrane of HeLa cells as observed with Serial Block Face Scanning Electron Microscopy is described. The resin background of the images was segmented at different slices of a 3D stack of 518 slices with 8, 192 × 8, 192 pixels each. The background was used to create a distance map which helped identify and rank the cells by their size at each slice. The centroids of the cells detected at different slices were linked to identify them as a single cell that spanned a number of slices. A subset of these cells, i.e., largest ones and those not close to the edges were selected for further processing. The selected cells were then automatically cropped to smaller regions of interest of 2, 000 × 2, 000 × 300 voxels that were treated as cell instances. Then, for each of these volumes the nucleus was segmented and the cell was separated from any neighbouring cells through a series of traditional image processing steps that followed the plasma membrane. The segmentation process was repeated for all the regions selected. For one cell for which the ground truth was available, the algorithm provided excellent results in Accuracy (AC) and Jaccard Index (JI): Nucleus: JI = 0.9665, AC= 0.9975, Cell and Nucleus JI = 0.8711, AC = 0.9655, Cell only JI = 0.8094, AC = 0.9629. A limitation of the algorithm for the plasma membrane segmentation was the presence of background, as in cases of tightly packed cells. When tested for these conditions, the segmentation of the nuclear envelope was still possible. All the code and data are released openly through GitHub, Zenodo and EMPIAR.', 'corpus_id': 233745104, 'score': 0}, {'doc_id': '236991924', 'title': 'DeLTA 2.0: A deep learning pipeline for quantifying single-cell spatial and temporal dynamics', 'abstract': 'Improvements in microscopy software and hardware have dramatically increased the pace of image acquisition, making analysis a major bottleneck in generating quantitative, single-cell data. Although tools for segmenting and tracking bacteria within time-lapse images exist, most require human input, are specialized to the experimental set up, or lack accuracy. Here, we introduce DeLTA 2.0, a purely Python workflow that can rapidly and accurately analyze single cells on two-dimensional surfaces to quantify gene expression and cell growth. The algorithm uses deep convolutional neural networks to extract single-cell information from time-lapse images, requiring no human input after training. DeLTA 2.0 retains all the functionality of the original version, which was optimized for bacteria growing in the mother machine microfluidic device, but extends results to two-dimensional growth environments. Two-dimensional environments represent an important class of data because they are more straightforward to implement experimentally, they offer the potential for studies using co-cultures of cells, and they can be used to quantify spatial effects and multi-generational phenomena. However, segmentation and tracking are significantly more challenging tasks in two-dimensions due to exponential increases in the number of cells that must be tracked. To showcase this new functionality, we analyze mixed populations of antibiotic resistant and susceptible cells, and also track pole age and growth rate across generations. In addition to the two-dimensional capabilities, we also introduce several major improvements to the code that increase accessibility, including the ability to accept many standard microscopy file formats and arbitrary image sizes as inputs. DeLTA 2.0 is rapid, with run times of less than 10 minutes for complete movies with hundreds of cells, and is highly accurate, with error rates around 1%, making it a powerful tool for analyzing time-lapse microscopy data. Author Summary Time-lapse microscopy can generate large image datasets which track single-cell properties like gene expression or growth rate over time. Deep learning tools are very useful for analyzing these data and can identify the location of cells and track their position over time. In this work, we introduce a new version of our Deep Learning for Time-lapse Analysis (DeLTA) software, which includes the ability to robustly segment and track bacteria that are growing in two dimensions, such as on agarose pads or within microfluidic environments. This capability is essential for experiments where spatial and positional effects are important, such as conditions with microbial co-cultures, cell-to-cell interactions, or spatial patterning. The software also tracks pole age and can be used to analyze replicative aging. These new features join other improvements, such as the ability to work directly with many common microscope file formats. DeLTA 2.0 can reliably track hundreds of cells with low error rates, making it an ideal tool for high throughput analysis of microscopy data.', 'corpus_id': 236991924, 'score': 1}, {'doc_id': '235243158', 'title': 'A deep learning algorithm for 3D cell detection in whole mouse brain image datasets', 'abstract': 'Understanding the function of the nervous system necessitates mapping the spatial distributions of its constituent cells defined by function, anatomy or gene expression. Recently, developments in tissue preparation and microscopy allow cellular populations to be imaged throughout the entire rodent brain. However, mapping these neurons manually is prone to bias and is often impractically time consuming. Here we present an open-source algorithm for fully automated 3D detection of neuronal somata in mouse whole-brain microscopy images using standard desktop computer hardware. We demonstrate the applicability and power of our approach by mapping the brain-wide locations of large populations of cells labeled with cytoplasmic fluorescent proteins expressed via retrograde trans-synaptic viral infection.', 'corpus_id': 235243158, 'score': 0}, {'doc_id': '232412385', 'title': 'Unsupervised temporal consistency improvement for microscopy video segmentation with Siamese networks', 'abstract': 'We introduce a simple mechanism by which a CNN trained to perform semantic segmentation of individual images can be re-trained - with no additional annotations - to improve its performance for segmentation of videos. We put the segmentation CNN in a Siamese setup with shared weights and train both for segmentation accuracy on annotated images and for segmentation similarity on unlabelled consecutive video frames. Our main application is live microscopy imaging of membrane-less organelles where the fluorescent groundtruth for virtual staining can only be acquired for individual frames. The method is directly applicable to other microscopy modalities, as we demonstrate by experiments on the Cell Segmentation Benchmark. Our code is available at https://github.com/kreshuklab/learning-temporal-consistency.', 'corpus_id': 232412385, 'score': 0}]"
104	{'doc_id': '25946397', 'title': 'Investigating survivability of configuration management tools in unreliable and hostile networks', 'abstract': 'A configuration management system (CMS) can control large networks of computers. A modern CMS is idempotent and describes infrastructure as code, so that it uses a description of the desired state of a system to automatically correct any deviations from a defined goal. As this requires both complete control of the slave systems and unquestioned ability to provide new instructions to slaves, the private key of the master is highly valuable target for attackers. Criminal malware networks already survive in hostile, heterogeneous networks, and therefore, the concepts from those systems could be applied to benign enterprise CMSs. We describe one such concept, the hidden master architecture, and compare its survivability to existing systems using attack trees.', 'corpus_id': 25946397}	20012	"[{'doc_id': '235829722', 'title': 'The Master and Parasite Attack', 'abstract': 'We explore a new type of malicious script attacks: the persistent parasite attack. Persistent parasites are stealthy scripts, which persist for a long time in the browser’s cache. We show to infect the caches of victims with parasite scripts via TCP injection.Once the cache is infected, we implement methodologies for propagation of the parasites to other popular domains on the victim client as well as to other caches on the network. We show how to design the parasites so that they stay long time in the victim’s cache not restricted to the duration of the user’s visit to the web site. We develop covert channels for communication between the attacker and the parasites, which allows the attacker to control which scripts are executed and when, and to exfiltrate private information to the attacker, such as cookies and passwords. We then demonstrate how to leverage the parasites to perform sophisticated attacks, and evaluate the attacks against a range of applications and security mechanisms on popular browsers. Finally we provide recommendations for countermeasures.', 'corpus_id': 235829722, 'score': 0}, {'doc_id': '167199882', 'title': 'Institutional and Economic Foundations of Cybercrime Business Models', 'abstract': 'Cybercrime business models are rapidly evolving. It is argued that cyber-criminals closely imitate business models of legitimate corporations. Cybercrime firms and legitimate businesses, however, differ in terms of the important sources of core competence. Legitimate businesses’ core processes are centered around creating the most value for customers. Most cyber-criminals’ core processes, however, involve extorting and defrauding prospective victims and minimizing the odds of getting caught. Cyber-criminals and legitimate businesses also differ in terms of the legitimacy related to regulative institutions and inter-organizational arrangements. This chapter disentangles the mechanisms behind the cybercrime business models and examines the contexts and processes associated with such models.', 'corpus_id': 167199882, 'score': 1}, {'doc_id': '234332346', 'title': 'An Econometric Overview on Growth and Impact of Online Crime and Analytics View to Combat Them', 'abstract': ""In this chapter, the authors take a closer look into the economic relation with cybercrime and an analytics method to combat that. At first, they examine whether the increase in the unemployment rate among youths is the prime cause of the growth of cybercrime or not. They proposed a model with the help of the Phillips curve and Okun's law to get hold of the assumptions. A brief discussion of the impact of cybercrime in economic growth is also presented in this paper. Crime pattern detection and the impact of bitcoin in the current digital currency market have also been discussed. They have proposed an analytic method to combat the crime using the concept of game theory. They have tested the vulnerability of the cloud datacenter using game theory where two players will play the game in non-cooperative strategy in the Nash equilibrium state. Through the rational state decisions of the players and implementation MSWA algorithm, they have simulated the results through which they can check the dysfunctionality probabilities of the datacenters."", 'corpus_id': 234332346, 'score': 1}, {'doc_id': '149144930', 'title': 'An Endeavour in the Domain of Cybercrime: Exploring the Structural and Cultural Features of the Darknet Market AlphaBay', 'abstract': None, 'corpus_id': 149144930, 'score': 1}, {'doc_id': '220742558', 'title': 'THE IMPACT OF CYBERCRIME ON THE NIGERIAN ECONOMY AND BANKING SYSTEM BY UMARU IBRAHIM, FCIB, mni,', 'abstract': '1.0 Introduction The role that Information and Communication Technology (ICT) plays in all aspects of human endeavors is well documented and evident. ICT has integrated different economies of the world, through the aid of electronics via the internet. Many corporate organizations, including banks now depend on ICT and computer networks to perform basic as well as complex tasks. The electronic market is now open to everybody, including criminals. It is projected that by 2020, global Cyber security spending will reach $170bn, a 126% increase from $75bn in 2015.', 'corpus_id': 220742558, 'score': 1}, {'doc_id': '236881734', 'title': 'Botnet Detection through DNS based approach', 'abstract': 'Volume 2, Issue 6, June 2013 Page 497 ABSTRACT Botnets is group of compromised computers controlled remotely by attackers. Botnets create widespread security and data safety issues and areeffective tools for propagating cyber-crime. It is imperative for the IT community to develop effective means of detecting andmitigating the malicious behavior of botnets. In this paper, we will understanding about the botnet and botnet detection by (a) creating a simple botnet and studying the flow of information among the bot and C&C server through activity diagram; and (b) study the detection of botnet using the DNS query patterns generated by the botnet.', 'corpus_id': 236881734, 'score': 0}, {'doc_id': '237099462', 'title': 'An exploratory analysis on the impact of Shodan scanning tool on the network attacks', 'abstract': 'Network flexibility, openness and systems integration has brought in the last years many advantages in the society in terms of communication and information sharing. Beside that, new issues are emerging related to vulnerabilities in the Internet, which can affect not only virtual environments in an isolated way but this can have serious repercussions in the real world. That is why, the identification of new system vulnerabilities represents an important information for malicious parties. Currently, several tools, known as Online Public Scanning Tools (OPSTs) represent for attackers an attractive source of information from which to draw in order to plan and launch attacks. Indeed, they can automatically scan services, platforms and IoT devices connected to the Internet in order to retrieve information related to them, by including those related to vulnerabilities. In this context, this work aims to investigate how such OPSTs impact the launch of attacks on the network. To this purpose, a model centered on 3 main actors, (i.e. the attack, the hacker and the OPST) has been proposed by defining a set of features which aims to support the evaluation. Shodan was chosen as the OPST, as it is the most popular based on the related review works, while a honey-based approach was adopted to support monitoring and information extraction related to attacks. The results of these analyzes, which show how Shodan influence the attackers in carrying out network attacks are presented and discussed.', 'corpus_id': 237099462, 'score': 0}, {'doc_id': '236880625', 'title': 'Malware Incident Prevention and Handling', 'abstract': ""Malware, also known as malicious code, is software that is hidden inside another programme with the aim of causing data loss, running disruptive or intrusive programmes, or jeopardising the protection, integrity, or availability of the victim's data, applications, or operating system. Malware is the most prevalent external threat to most hosts, causing widespread damage and destruction as well as substantial recovery efforts within most organisations. This guide will show you how to make your company's malware incident response procedures better. It also includes concrete suggestions for strengthening an organization's current crisis management capabilities. It also includes detailed recommendations for enhancing an organization's current incident response capabilities so that it can better manage malware incidents, particularly those that are widespread."", 'corpus_id': 236880625, 'score': 0}, {'doc_id': '236909988', 'title': 'Study of Blacklisted Malicious Domains from a Microsoft Windows End-user Perspective: Is It Safe Behind the Wall?', 'abstract': 'The Internet is a dangerous place, filled with different cyber threats, including malware. To withstand this, blacklists have been utilized for a long time to block known infection and delivery sources. However, through blacklisting the domain names we are leaving a landscape of threats to be unknown and forgotten. In this paper, first, we investigate the current state-of-the-art in cyber threats available on such blacklists. Then, we study the corresponding malicious actors and reveal that those persistently appear since 2006. By shedding light on this part of the cyber threat landscape we target increased information security perception of the landscape from the perspective of the average end-user. Moreover, it is clear that the blacklisting the domains should not be one-way function and need to be regularly re-evaluated. Moreover, blacklisting might not be enforced by client applications in addition to outdated system software leaving real danger. For practical evaluation, we created a multi-focused experimental setup employing different MS Windows OS and browser versions. This allowed us to perform a thorough analysis of blacklisted domains from the perspective of the published information, content retrieved and possible malware distribution campaigns. We believe that this paper serves as a stepping stone in a re-evaluation of the once found and then blacklisted domains from the perspective of minimal security protection of a general user, who might not be equipped with a blacklisting mechanism.', 'corpus_id': 236909988, 'score': 0}, {'doc_id': '155115646', 'title': 'Dis-Organised Crime: Towards a Distributed Model of the Organization of Cybercrime', 'abstract': 'There exists a widespread and uncritical assumption that the internet and society have been brought to their knees by Mafia-driven organised crime groups. Yet, this rhetorical narrative is not supported by research into the organisation of online crime groupings which finds that the organisation of crime online follows a different logic to the organisation of crime offline, a difference which is also reflected in other organised crime groupings though with different actors. Such findings identify instead a ‘disorganised’ or distributed model of organization, rather than a hierarchical command and control structure. This article maps out the logic behind the organisation of criminal behaviour online before looking critically at the organised cybercrime debates. It then draws upon a simple analysis of the structures of known cybercrime groups and then three case studies of different cybercrime types to explore their organization.', 'corpus_id': 155115646, 'score': 1}]"
105	{'doc_id': '8830352', 'title': 'Readthrough Strategies for Therapeutic Suppression of Nonsense Mutations in Inherited Metabolic Disease', 'abstract': 'Inherited metabolic diseases (IMDs) belong to the group of rare diseases due to their low individual prevalence. Most of them are inherited in autosomal recessive fashion and represent good candidates for novel therapeutical strategies aimed at recovering partial enzyme function as they lack an effective treatment, and small levels of enzymatic activity have been shown to be associated with improved outcome and milder phenotypes. Recently, a novel therapeutic approach for genetic diseases has emerged, based on the ability of aminoglycosides and other compounds in allowing translation to proceed through a premature termination codon introduced by a nonsense mutation, which frequently constitute a significant fraction of the mutant alleles in a population. In this review we summarize the essentials of what is known as suppression therapy, the different compounds that have been identified by high-throughput screens or developed using a medicinal chemistry approach and the preclinical and clinical trials that are being conducted in general and in the field of IMDs in particular. Several IMDs have shown to be good models for evaluating readthrough compounds using patients’ cells carrying nonsense mutations, monitoring for an increase in functional recovery and/or enzyme activity. Overall, the positive results obtained indicate the feasibility of the approach for different diseases and although the levels of protein function reached are low, they may be enough to alleviate the consequences of the pathology. Nonsense suppression thus represents a potential therapy or supplementary treatment for a number of IMD patients encouraging further clinical trials with readthrough drugs with improved functionality and low toxicity.', 'corpus_id': 8830352}	5532	"[{'doc_id': '52860843', 'title': 'Erratum to: Readthrough of nonsense mutations in Rett syndrome: evaluation of novel aminoglycosides and generation of a new mouse model', 'abstract': 'Thirty-five percent of patients with Rett syndrome carry nonsense mutations in the MECP2 gene. We have recently shown in transfected HeLa cells that read-through of nonsense mutations in the MECP2 gene can be achieved by treatment with gentamicin and geneticin. This study was performed to test if readthrough can also be achieved in cells endogenously expressing mutant MeCP2 and to evaluate potentially more effective readthrough compounds. A mouse model was generated carrying the R168X mutation in the MECP2 gene. Transfected HeLa cells expressing mutated MeCP2 fusion proteins and mouse ear fibroblasts isolated from the new mouse model were treated with gentamicin and the novel aminoglycosides NB30, NB54, and NB84. The localization of the read-through product was tested by immunofluorescence. Read-through of the R168X mutation in mouse ear fibroblasts using gentamicin was detected but at lower level than in HeLa cells. As expected, the readthrough product, full-length Mecp2 protein, was located in the nucleus. NB54 and NB84 induced readthrough more effectively than gentamicin, while NB30 was less effective. Readthrough of nonsense mutations can be achieved not only in transfected HeLa cells but also in fibroblasts of the newly generated Mecp2 R168X mouse model. NB54 and NB84 were more effective than gentamicin and are therefore promising candidates for readthrough therapy in Rett syndrome patients.', 'corpus_id': 52860843, 'score': 1}, {'doc_id': '216595557', 'title': 'Effect of High vs Low Doses of Chloroquine Diphosphate as Adjunctive Therapy for Patients Hospitalized With Severe Acute Respiratory Syndrome Coronavirus 2 (SARS-CoV-2) Infection: A Randomized Clinical Trial.', 'abstract': 'Importance\nThere is no specific antiviral therapy recommended for coronavirus disease 2019 (COVID-19). In vitro studies indicate that the antiviral effect of chloroquine diphosphate (CQ) requires a high concentration of the drug.\n\n\nObjective\nTo evaluate the safety and efficacy of 2 CQ dosages in patients with severe COVID-19.\n\n\nDesign, Setting, and Participants\nThis parallel, double-masked, randomized, phase IIb clinical trial with 81 adult patients who were hospitalized with severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) infection was conducted from March 23 to April 5, 2020, at a tertiary care facility in Manaus, Brazilian Amazon.\n\n\nInterventions\nPatients were allocated to receive high-dosage CQ (ie, 600 mg CQ twice daily for 10 days) or low-dosage CQ (ie, 450 mg twice daily on day 1 and once daily for 4 days).\n\n\nMain Outcomes and Measures\nPrimary outcome was reduction in lethality by at least 50% in the high-dosage group compared with the low-dosage group. Data presented here refer primarily to safety and lethality outcomes during treatment on day 13. Secondary end points included participant clinical status, laboratory examinations, and electrocardiogram results. Outcomes will be presented to day 28. Viral respiratory secretion RNA detection was performed on days 0 and 4.\n\n\nResults\nOut of a predefined sample size of 440 patients, 81 were enrolled (41 [50.6%] to high-dosage group and 40 [49.4%] to low-dosage group). Enrolled patients had a mean (SD) age of 51.1 (13.9) years, and most (60 [75.3%]) were men. Older age (mean [SD] age, 54.7 [13.7] years vs 47.4 [13.3] years) and more heart disease (5 of 28 [17.9%] vs 0) were seen in the high-dose group. Viral RNA was detected in 31 of 40 (77.5%) and 31 of 41 (75.6%) patients in the low-dosage and high-dosage groups, respectively. Lethality until day 13 was 39.0% in the high-dosage group (16 of 41) and 15.0% in the low-dosage group (6 of 40). The high-dosage group presented more instance of QTc interval greater than 500 milliseconds (7 of 37 [18.9%]) compared with the low-dosage group (4 of 36 [11.1%]). Respiratory secretion at day 4 was negative in only 6 of 27 patients (22.2%).\n\n\nConclusions and Relevance\nThe preliminary findings of this study suggest that the higher CQ dosage should not be recommended for critically ill patients with COVID-19 because of its potential safety hazards, especially when taken concurrently with azithromycin and oseltamivir. These findings cannot be extrapolated to patients with nonsevere COVID-19.\n\n\nTrial Registration\nClinicalTrials.gov Identifier: NCT04323527.', 'corpus_id': 216595557, 'score': 0}, {'doc_id': '219515396', 'title': 'Pharmacological approaches for targeting cystic fibrosis nonsense mutations.', 'abstract': 'Cystic fibrosis (CF) is a monogenic autosomal recessive disorder. The clinical manifestations of the disease are caused by ∼2,000 mutations in the cystic fibrosis transmembrane conductance regulator (CFTR) protein. It is unlikely that any one approach will be efficient in correcting all defects. The recent approvals of ivacaftor, lumacaftor/ivacaftor and elexacaftor/tezacaftor/ivacaftor represent the genesis of a new era of precision combination medicine for the CF patient population. In this review, we discuss targeted translational readthrough approaches as mono and combination therapies for CFTR nonsense mutations. We examine the current status of efficacy of translational readthrough/nonsense suppression therapies and their limitations, including non-native amino acid incorporation at PTCs and nonsense-mediated mRNA decay (NMD), along with approaches to tackle these limitations. We further elaborate on combining various therapies such as readthrough agents, NMD inhibitors, and corrector/potentiators to improve the efficacy and safety of suppression therapy. These mutation specific strategies that are directed towards the basic CF defects should positively impact CF patients bearing nonsense mutations.', 'corpus_id': 219515396, 'score': 1}, {'doc_id': '218599573', 'title': 'Hydroxychloroquine in Patients with Rheumatic Disease Complicated by COVID-19: Clarifying Target Exposures and the Need for Clinical Trials', 'abstract': 'Objective. To characterize hydroxychloroquine (HCQ) exposure in patients with rheumatic disease receiving longterm HCQ compared to target concentrations with reported antiviral activity against the coronavirus disease 2019 caused by SARS-CoV-2 (COVID-19). Method. We evaluated total HCQ concentrations in serum and plasma from published literature values, frozen serum samples from a pediatric systemic lupus erythematosus trial, and simulated concentrations using a published pharmacokinetic model during pregnancy. For each source, we compared observed or predicted HCQ concentrations to target concentrations with reported antiviral activity against SARS-CoV-2. Results. The average total serum/plasma HCQ concentrations were below the lowest SARS-CoV-2 target of 0.48 mg/l in all studies. Assuming the highest antiviral target exposure (total plasma concentration of 4.1 mg/l), all studies had about one-tenth the necessary concentration for in vitro viral inhibition. Pharmacokinetic model simulations confirmed that pregnant adults receiving common dosing for rheumatic diseases did not achieve target exposures; however, the models predict that a dosage of 600 mg once a day during pregnancy would obtain the lowest median target exposure for most patients after the first dose. Conclusion. We found that the average patient receiving treatment with HCQ for rheumatic diseases, including children and non-pregnant/pregnant adults, are unlikely to achieve total serum or plasma concentrations shown to inhibit SARS-CoV-2 in vitro. Nevertheless, patients receiving HCQ long term may have tissue concentrations far exceeding that of serum/plasma. Because the therapeutic window for HCQ in the setting of SARS-CoV-2 is unknown, well-designed clinical trials that include patients with rheumatic disease are urgently needed to characterize the efficacy, safety, and target exposures for HCQ.', 'corpus_id': 218599573, 'score': 0}, {'doc_id': '218856387', 'title': 'Polysomes Bypass a 50-Nucleotide Coding Gap Less Efficiently Than Monosomes Due to Attenuation of a 5′ mRNA Stem–Loop and Enhanced Drop-off', 'abstract': '\n Abstract\n \n Efficient translational bypassing of a 50\u202fnt non-coding gap in a phage T4 topoisomerase subunit gene (gp60) requires several recoding signals. Here we investigate the function of the mRNA stem loop 5′ of the take-off codon, as well as the importance of ribosome loading density on the mRNA for efficient bypassing. We show that polysomes are less efficient at mediating bypassing than monosomes, both in vitro and in vivo, due to their preventing formation of a stem loop 5′ of the take-off codon and allowing greater peptidyl-tRNA drop off. A ribosome profiling analysis of phage T4 infected E. coli yielded protected mRNA fragments within the normal size range derived from ribosomes stalled at the take-off codon. However, ribosomes at this position also yielded some 53 nucleotide fragments, 16 longer. These were due to protection of the nucleotides that form the 5′ stem loop. NMR shows that the 5′ stem loop is highly dynamic. The importance of different nucleotides in the 5′ stem loop is revealed by mutagenesis studies. These data highlight the significance of the 5′ stem loop for the 50\u202fnt bypassing, and further enhance appreciation of relevance of the extent of ribosome loading for recoding.\n \n', 'corpus_id': 218856387, 'score': 1}, {'doc_id': '218892315', 'title': 'Evidence of Protective Effect of Hydroxychloroquine on COVID-19', 'abstract': 'We would like to share ideas on the report on ""Hydroxychloroquine in Patients with Rheumatic Disease Complicated by COVID-19: Clarifying Target Exposures and the Need for Clinical Trials [1].""Balevic noted that ""well-designed clinical trials that include patients with rheumatic disease are urgently needed to characterize the efficacy, safety, and target exposures for hydroxychloroquine [1].""', 'corpus_id': 218892315, 'score': 0}, {'doc_id': '218517823', 'title': ""Does zinc supplementation enhance the clinical efficacy of chloroquine/hydroxychloroquine to win today's battle against COVID-19?"", 'abstract': '\n Abstract\n \n Currently, drug repurposing is an alternative to novel drug development for the treatment of COVID-19 patients. The antimalarial drug chloroquine (CQ) and its metabolite hydroxychloroquine (HCQ) are currently being tested in several clinical studies as potential candidates to limit SARS-CoV-2-mediated morbidity and mortality. CQ and HCQ (CQ/HCQ) inhibit pH-dependent steps of SARS-CoV-2 replication by increasing pH in intracellular vesicles and interfere with virus particle delivery into host cells. Besides direct antiviral effects, CQ/HCQ specifically target extracellular zinc to intracellular lysosomes where it interferes with RNA-dependent RNA polymerase activity and coronavirus replication. As zinc deficiency frequently occurs in elderly patients and in those with cardiovascular disease, chronic pulmonary disease, or diabetes, we hypothesize that CQ/HCQ plus zinc supplementation may be more effective in reducing COVID-19 morbidity and mortality than CQ or HCQ in monotherapy. Therefore, CQ/HCQ in combination with zinc should be considered as additional study arm for COVID-19 clinical trials.\n \n', 'corpus_id': 218517823, 'score': 0}, {'doc_id': '218572031', 'title': 'Translation-associated mutational U-pressure in the first ORF of SARS-CoV-2 and other coronaviruses', 'abstract': 'Within four months of the ongoing COVID-19 pandemic caused by SARS-CoV-2, more than 250 nucleotide mutations have been detected in the ORF1 of the virus isolated from different parts of the globe. These observations open up an obvious question about the rate and direction of mutational pressure for further vaccine and therapeutics designing. In this study, we did a comparative analysis of ORF1a and ORF1b by using the first isolate (Wuhan strain) as the parent sequence. We observed that most of the nucleotide mutations are C to U transitions. The rate of synonymous C to U transitions is significantly higher than the rate of nonsynonymous ones, indicating negative selection on amino acid substitutions. Further, trends in nucleotide usage bias have been investigated in 49 coronaviruses species. A strong bias in nucleotide usage in fourfold degenerated sites towards uracil residues is seen in ORF1 of all the studied coronaviruses. A more substantial mutational U pressure is observed in ORF1a than in ORF1b owing to the translation of ORF1ab via programmed ribosomal frameshifting. Unlike other nucleotide mutations, mutational U pressure caused by cytosine deamination, mostly occurring in the RNA-plus strand, cannot be corrected by the proof-reading machinery of coronaviruses. The knowledge generated on the direction of mutational pressure during translation of viral RNA-plus strands has implications for vaccine and nucleoside analogue development for treating covid-19 and other coronavirus infections.', 'corpus_id': 218572031, 'score': 0}, {'doc_id': '7368538', 'title': 'Nonaminoglycoside compounds induce readthrough of nonsense mutations', 'abstract': 'Large numbers of genetic disorders are caused by nonsense mutations for which compound-induced readthrough of premature termination codons (PTCs) might be exploited as a potential treatment strategy. We have successfully developed a sensitive and quantitative high-throughput screening (HTS) assay, protein transcription/translation (PTT)–enzyme-linked immunosorbent assay (ELISA), for identifying novel PTC-readthrough compounds using ataxia-telangiectasia (A-T) as a genetic disease model. This HTS PTT-ELISA assay is based on a coupled PTT that uses plasmid templates containing prototypic A-T mutated (ATM) mutations for HTS. The assay is luciferase independent. We screened ∼34,000 compounds and identified 12 low-molecular-mass nonaminoglycosides with potential PTC-readthrough activity. From these, two leading compounds consistently induced functional ATM protein in ATM-deficient cells containing disease-causing nonsense mutations, as demonstrated by direct measurement of ATM protein, restored ATM kinase activity, and colony survival assays for cellular radiosensitivity. The two compounds also demonstrated readthrough activity in mdx mouse myotube cells carrying a nonsense mutation and induced significant amounts of dystrophin protein.', 'corpus_id': 7368538, 'score': 1}, {'doc_id': '219692176', 'title': 'High-throughput interrogation of programmed ribosomal frameshifting in human cells', 'abstract': 'Programmed ribosomal frameshifting (PRF) is the controlled slippage of the translating ribosome to an alternative frame. This process is widely employed by human viruses such as HIV and SARS coronavirus and is critical for their replication. Here, we developed a high-throughput approach to assess the frameshifting potential of a sequence. We designed and tested >12,000 sequences based on 15 viral and human PRF events, allowing us to systematically dissect the rules governing ribosomal frameshifting and discover novel regulatory inputs based on amino acid properties and tRNA availability. We assessed the natural variation in HIV gag-pol frameshifting rates by testing >500 clinical isolates and identified subtype-specific differences and associations between viral load in patients and the optimality of PRF rates. We devised computational models that accurately predict frameshifting potential and frameshifting rates, including subtle differences between HIV isolates. This approach can contribute to the development of antiviral agents targeting PRF. Programmed ribosomal frameshifting—the slippage of the ribosome to an alternative frame — is critical for viral replication and cellular processes. Here the authors present an approach that can assess the frameshifting potential of a sequence and elucidate the rules governing ribosomal frameshifting.', 'corpus_id': 219692176, 'score': 1}]"
106	"{'doc_id': '118584311', 'title': 'Infinite RAAM: Initial Explorations into a Fractal Basis for Cognition', 'abstract': ""In nite RAAM: Initial Explorations into a Fra tal Basis for Cognition A dissertation presented to the Fa ulty of the Graduate S hool of Arts and S ien es of Brandeis University, Waltham, Massa husetts by Simon D. Levy This thesis attempts to provide an answer to the question What is the mathemati al basis of ognitive representations? The answer we present is a novel onne tionist framework alled In nite RAAM. We show how this framework satis es the ognitive requirements of systemati ity, ompositionality, and s alable representational apa ity, while also exhibiting natural properties like learnability, generalization, and indu tive bias. We begin with the requirements for ognitive representations, in luding traditional ritiisms about the inability of standard onne tionist models to satisfy these requirements. We then review some re ent onne tionist approa hes and their limitations, and des ribe Polla k's Re ursive Auto-Asso iative Memory (RAAM) a re urrent neural network model of hierar hi al symboli stru ture that addresses some of these limitations su essfully. S alability problems with RAAM lead us to an exploration of the network's behavior as an iterated fun tion system (IFS), fo ussed on the on ept of the attra tor, whi h des ribes the behavior of the IFS in the limit. We show how exploiting the attra tor over omes the pra ti al limitations of RAAM, leading to a model with provably in nite representational apa ity, whi h we all In nite RAAM, or IRAAM. The subsequent hapters detail how the model an represent trees over a potentially innite lexi on of terminals; the bias that the model exhibits in the relationship between these terminals and the tree stru tures; and the dire t me hanism that the model provides for the uni ation algorithm. We then present experiments on IRAAM learning, followed by some appli ations of IRAAM to logi programming and language representation. We on lude with a dis ussion of the impli ations and urrent limitations of the model, with prospe ts for over oming these limitations in future work. The ontributions of this work are twofold: First, In nite RAAM shows how onne tionist models an exhibit in nite ompeten e for interesting ognitive domains like language. Se ond, our attra tor-based learning algorithm provides a way of learning stru tured ognitive representations, with robust de oding and generalization. Both results ome from allowing the dynami s of the network to devise emergent representations during learning. An appendix provides Matlab ode for the experiments des ribed in the thesis. ix"", 'corpus_id': 118584311}"	2774	[{'doc_id': '232325097', 'title': 'From convolutional neural networks to models of higher-level cognition (and back again).', 'abstract': 'The remarkable successes of convolutional neural networks (CNNs) in modern computer vision are by now well known, and they are increasingly being explored as computational models of the human visual system. In this paper, we ask whether CNNs might also provide a basis for modeling higher-level cognition, focusing on the core phenomena of similarity and categorization. The most important advance comes from the ability of CNNs to learn high-dimensional representations of complex naturalistic images, substantially extending the scope of traditional cognitive models that were previously only evaluated with simple artificial stimuli. In all cases, the most successful combinations arise when CNN representations are used with cognitive models that have the capacity to transform them to better fit human behavior. One consequence of these insights is a toolkit for the integration of cognitively motivated constraints back into CNN training paradigms in computer vision and machine learning, and we review cases where this leads to improved performance. A second consequence is a roadmap for how CNNs and cognitive models can be more fully integrated in the future, allowing for flexible end-to-end algorithms that can learn representations from data while still retaining the structured behavior characteristic of human cognition.', 'corpus_id': 232325097, 'score': 0}, {'doc_id': '221911732', 'title': 'Designing a Neural Network Primitive for Conditional Structural Transformations', 'abstract': None, 'corpus_id': 221911732, 'score': 1}, {'doc_id': '233762187', 'title': 'Precis of A Bayesian account of learning algorithms and generalising representations in the brain', 'abstract': 'Without learning we would be limited to a set of preprogrammed behaviours. While that may be acceptable for flies1, it does not provide the basis for adaptive or intelligent behaviours familiar to humans. Learning, then, is one of the crucial components of brain operation. Learning, however, takes time. Thus, the key to adaptive behaviour is learning to systematically generalise; that is, have learned knowledge that can be flexibly recombined to understand any world in front of you. This thesis attempts to make inroads on two questions how can brain networks learn, and what are the principles behind representations of knowledge that allow generalisation. With the industrialisation of science, the twentieth century bore fruit in the form of an increasingly detailed understanding of neurons, synapses, neurotransmitters, resting potentials, action potentials, networks and so on (1–4). Though we have gained a great level of detail about many of these micro-processes as well as high-level understandings of intelligence thanks to philosophy, experimental psychology, and behavioural and cognitive neuroscience (5–9) a large gulf of understanding remains between these levels of granularity. This thesis focuses on spanning this gap by providing high-level computational frameworks that translate to low-level processes. Any high-level brain framework must have successful behaviour at its heart as that is the role of the brain. Analogously, neurons are central to low-level understanding as the basis of brain function is believed to be the transfer of information between neurons, mediated via weighted connections. Different weights lead to different functions. Thus, learning appropriate configurations of weights is the fundamental problem facing brains. There are two facets to this learning the first is how, and the second is what. The how are the learning algorithms that determine updates to these synaptic connections, and the what are the neural representations that reflect how the world works. In this vein, this thesis examines 1) the algorithmic implementation of learning in biological neural networks, and 2) a computational framework for the neural representations of task generalisation. Both these research directions are bound together by Bayesian thinking, and both of these pieces of work bridge the gap between highand lowlevel understanding, as well as between brains and machines.', 'corpus_id': 233762187, 'score': 0}, {'doc_id': '233303578', 'title': 'Encoding and Decoding of Recursive Structures in Neural-Symbolic Systems', 'abstract': 'Abstract One of the ways to join the connectionist approach and the symbolic paradigm is Tensor Product Variable Binding. It was initially devoted to building distributed representation of recursive structures for neural networks to use it as the input. Structures are an essential part of both formal and natural languages and appear in syntactic trees, grammar, semantic interpretation. A human mind smoothly operates with the appearing problems on the neural level, and it is naturally scalable and robust. The question arises of whether it is possible to translate traditional symbolic algorithms to the sub-symbolic level to reuse performance and computational gain of the neural networks for general tasks. However, several aspects of Tensor Product Variable Binding lack attention in public research, especially in building such a neural architecture that performs computations according to the mathematical model without preliminary training. In this paper, those implementation aspects are addressed. A proposed novel design for the decoding network translates a tensor to a corresponding recursive structure with the arbitrary level of nesting. Also, several complex topics about encoding such structures in the distributed representation or tensor are addressed. Both encoding and decoding neural networks are built with the Keras framework’s help and are analyzed from the perspective of applied value. The proposed design continues the series of papers dedicated to building a robust bridge between two computational paradigms: connectionist and symbolic.', 'corpus_id': 233303578, 'score': 1}, {'doc_id': '233241202', 'title': 'Syntactic Perturbations Reveal Representational Correlates of Hierarchical Phrase Structure in Pretrained Language Models', 'abstract': 'While vector-based language representations from pretrained language models have set a new standard for many NLP tasks, there is not yet a complete accounting of their inner workings. In particular, it is not entirely clear what aspects of sentence-level syntax are captured by these representations, nor how (if at all) they are built along the stacked layers of the network. In this paper, we aim to address such questions with a general class of interventional, input perturbation-based analyses of representations from pretrained language models. Importing from computational and cognitive neuroscience the notion of representational invariance, we perform a series of probes designed to test the sensitivity of these representations to several kinds of structure in sentences. Each probe involves swapping words in a sentence and comparing the representations from perturbed sentences against the original. We experiment with three different perturbations: (1) random permutations of n-grams of varying width, to test the scale at which a representation is sensitive to word position; (2) swapping of two spans which do or do not form a syntactic phrase, to test sensitivity to global phrase structure; and (3) swapping of two adjacent words which do or do not break apart a syntactic phrase, to test sensitivity to local phrase structure. Results from these probes collectively suggest that Transformers build sensitivity to larger parts of the sentence along their layers, and that hierarchical phrase structure plays a role in this process. More broadly, our results also indicate that structured input perturbations widens the scope of analyses that can be performed on often-opaque deep learning systems, and can serve as a complement to existing tools (such as supervised linear probes) for interpreting complex black-box models.', 'corpus_id': 233241202, 'score': 0}, {'doc_id': '237149108', 'title': 'Near-channel classifier: symbiotic communication and classification in high-dimensional space', 'abstract': 'Brain-inspired high-dimensional (HD) computing represents and manipulates data using very long, random vectors with dimensionality in the thousands. This representation provides great robustness for various classification tasks where classifiers operate at low signal-to-noise ratio (SNR) conditions. Similarly, hyperdimensional modulation (HDM) leverages the robustness of complex-valued HD representations to reliably transmit information over a wireless channel, achieving a similar SNR gain compared to state-of-the-art codes. Here, we first propose methods to improve HDM in two ways: (1) reducing the complexity of encoding and decoding operations by generating, manipulating, and transmitting bipolar or integer vectors instead of complex vectors; (2) increasing the SNR gain by 0.2\xa0dB using a new soft-feedback decoder; it can also increase the additive superposition capacity of HD vectors up to 1.7 $$\\times$$ × in noise-free cases. Secondly, we propose to combine encoding/decoding aspects of communication with classification into a single framework by relying on multifaceted HD representations. This leads to a near-channel classification (NCC) approach that avoids transformations between different representations and the overhead of multiple layers of encoding/decoding, hence reducing latency and complexity of a wireless smart distributed system while providing robustness against noise and interference from other nodes. We provide a use-case for wearable hand gesture recognition with 5 classes from 64 EMG sensors, where the encoded vectors are transmitted to a remote node for either performing NCC, or reconstruction of the encoded data. In NCC mode, the original classification accuracy of 94% is maintained, even in the channel at SNR of 0\xa0dB, by transmitting 10,000-bit vectors. We remove the redundancy by reducing the vector dimensionality to 2048-bit that still exhibits a graceful degradation: less than 6% accuracy loss is occurred in the channel at −\xa05\xa0dB, and with the interference from 6 nodes that simultaneously transmit their encoded vectors. In the reconstruction mode, it improves the mean-squared error by up to 20\xa0dB, compared to standard decoding, when transmitting 2048-dimensional vectors.', 'corpus_id': 237149108, 'score': 1}, {'doc_id': '235485318', 'title': 'On Effects of Compression with Hyperdimensional Computing in Distributed Randomized Neural Networks', 'abstract': 'A change of the prevalent supervised learning techniques is foreseeable in the near future: from the complex, computational expensive algorithms to more flexible and elementary training ones. The strong revitalization of randomized algorithms can be framed in this prospect steering. We recently proposed a model for distributed classification based on randomized neural networks and hyperdimensional computing, which takes into account cost of information exchange between agents using compression. The use of compression is important as it addresses the issues related to the communication bottleneck, however, the original approach is rigid in the way the compression is used. Therefore, in this work, we propose a more flexible approach to compression and compare it to conventional compression algorithms, dimensionality reduction, and quantization techniques.', 'corpus_id': 235485318, 'score': 1}, {'doc_id': '232428181', 'title': 'Attention, please! A survey of Neural Attention Models in Deep Learning', 'abstract': 'In humans, Attention is a core property of all perceptual and cognitive operations. Given our limited ability to process competing sources, attention mechanisms select, modulate, and focus on the information most relevant to behavior. For decades, concepts and functions of attention have been studied in philosophy, psychology, neuroscience, and computing. For the last six years, this property has been widely explored in deep neural networks. Currently, the state-of-the-art in Deep Learning is represented by neural attention models in several application domains. This survey provides a comprehensive overview and analysis of developments in neural attention models. We systematically reviewed hundreds of architectures in the area, identifying and discussing those in which attention has shown a significant impact. We also developed and made public an automated methodology to facilitate the development of reviews in the area. By critically analyzing 650 works, we describe the primary uses of attention in convolutional, recurrent networks and generative models, identifying common subgroups of uses and applications. Furthermore, we describe the impact of attention in different application domains and their impact on neural networks’ interpretability. Finally, we list possible trends and opportunities for further research, hoping that this review will provide a succinct overview of the main attentional models in the area and guide researchers in developing future approaches that will drive further improvements.', 'corpus_id': 232428181, 'score': 0}, {'doc_id': '231685306', 'title': 'Incremental Composition in Distributional Semantics', 'abstract': 'Despite the incremental nature of Dynamic Syntax (DS), the semantic grounding of it remains that of predicate logic, itself grounded in set theory, so is poorly suited to expressing the rampantly context-relative nature of word meaning, and related phenomena such as incremental judgements of similarity needed for the modelling of disambiguation. Here, we show how DS can be assigned a compositional distributional semantics which enables such judgements and makes it possible to incrementally disambiguate language constructs using vector space semantics. Building on a proposal in our previous work, we implement and evaluate our model on real data, showing that it outperforms a commonly used additive baseline. In conclusion, we argue that these results set the ground for an account of the non-determinism of lexical content, in which the nature of word meaning is its dependence on surrounding context for its construal.', 'corpus_id': 231685306, 'score': 1}, {'doc_id': '233219674', 'title': 'Neuro-Symbolic VQA: A review from the perspective of AGI desiderata', 'abstract': 'An ultimate goal of the AI and ML fields is artificial general intelligence (AGI); although such systems remain science fiction, various models exhibit aspects of AGI. In this work, we look at neuro-symbolic (NS) approaches to visual question answering (VQA) from the perspective of AGI desiderata. We see how well these systems meet these desiderata, and how the desiderata often pull the scientist in opposing directions. It is my hope that through this work we can temper model evaluation on benchmarks with a discussion of the properties of these systems and their potential for future extension.', 'corpus_id': 233219674, 'score': 0}]
107	{'doc_id': '11313423', 'title': 'Testing Convergent Evolution in Auditory Processing Genes between Echolocating Mammals and the Aye-Aye, a Percussive-Foraging Primate', 'abstract': 'Abstract Several taxonomically distinct mammalian groups—certain microbats and cetaceans (e.g., dolphins)—share both morphological adaptations related to echolocation behavior and strong signatures of convergent evolution at the amino acid level across seven genes related to auditory processing. Aye-ayes (Daubentonia madagascariensis) are nocturnal lemurs with a specialized auditory processing system. Aye-ayes tap rapidly along the surfaces of trees, listening to reverberations to identify the mines of wood-boring insect larvae; this behavior has been hypothesized to functionally mimic echolocation. Here we investigated whether there are signals of convergence in auditory processing genes between aye-ayes and known mammalian echolocators. We developed a computational pipeline (Basic Exon Assembly Tool) that produces consensus sequences for regions of interest from shotgun genomic sequencing data for nonmodel organisms without requiring de novo genome assembly. We reconstructed complete coding region sequences for the seven convergent echolocating bat–dolphin genes for aye-ayes and another lemur. We compared sequences from these two lemurs in a phylogenetic framework with those of bat and dolphin echolocators and appropriate nonecholocating outgroups. Our analysis reaffirms the existence of amino acid convergence at these loci among echolocating bats and dolphins; some methods also detected signals of convergence between echolocating bats and both mice and elephants. However, we observed no significant signal of amino acid convergence between aye-ayes and echolocating bats and dolphins, suggesting that aye-aye tap-foraging auditory adaptations represent distinct evolutionary innovations. These results are also consistent with a developing consensus that convergent behavioral ecology does not reliably predict convergent molecular evolution.', 'corpus_id': 11313423}	16011	"[{'doc_id': '212622617', 'title': 'Convergent losses of TLR5 suggest altered extracellular flagellin detection in four mammalian lineages.', 'abstract': 'Toll-like receptors (TLRs) play an important role for the innate immune system by detecting pathogen-associated molecular patterns. TLR5 encodes the major extracellular receptor for bacterial flagellin and frequently evolves under positive selection, consistent with coevolutionary arms races between the host and pathogens. Furthermore, TLR5 is inactivated in several vertebrates and a TLR5 stop codon polymorphism is widespread in human populations. Here, we analyzed the genomes of 120 mammals and discovered that TLR5 is convergently lost in four independent lineages, comprising guinea pigs, Yangtze river dolphin, pinnipeds, and pangolins. Validated inactivating mutations, absence of protein-coding transcript expression, and relaxed selection on the TLR5 remnants confirm these losses. PCR analysis further confirmed the loss of TLR5 in the pinniped stem lineage. Finally, we show that TLR11, encoding a second extracellular flagellin receptor, is also absent in these four lineages. Independent losses of TLR5 and TLR11 suggests that a major pathway for detecting flagellated bacteria is not essential for different mammals and predicts an impaired capacity to sense extracellular flagellin.', 'corpus_id': 212622617, 'score': 1}, {'doc_id': '3868703', 'title': 'Transition to an Aquatic Habitat Permitted the Repeated Loss of the Pleiotropic KLK8 Gene in Mammals', 'abstract': 'Abstract Kallikrein related peptidase 8 (KLK8; also called neuropsin) is a serine protease that plays distinct roles in the skin and hippocampus. In the skin, KLK8 influences keratinocyte proliferation and desquamation, and activates antimicrobial peptides in sweat. In the hippocampus, KLK8 affects memory acquisition. Here, we examined the evolution of KLK8 in mammals and discovered that, out of 70 placental mammals, KLK8 is exclusively lost in three independent fully-aquatic lineages, comprising dolphin, killer whale, minke whale, and manatee. In addition, while the sperm whale has an intact KLK8 reading frame, the gene evolves neutrally in this species. We suggest that the distinct functions of KLK8 likely became obsolete in the aquatic environment, leading to the subsequent loss of KLK8 in several fully-aquatic mammalian lineages. First, the cetacean and manatee skin lacks sweat glands as an adaptation to the aquatic environment, which likely made the epidermal function of KLK8 obsolete. Second, cetaceans and manatees exhibit a proportionally small hippocampus, which may have rendered the hippocampal functions of KLK8 obsolete. Together, our results shed light on the genomic changes that correlate with skin and neuroanatomical differences of aquatic mammals, and show that even pleiotropic genes can be lost during evolution if an environmental change nullifies the need for the different functions of such genes.', 'corpus_id': 3868703, 'score': 1}, {'doc_id': '232412244', 'title': 'Sequence and phylogenetic analysis revealed structurally conserved domains and motifs in lincRNA-p21', 'abstract': 'Long Intergenic Non-coding RNAs (lincRNAs) are the largest class of long non-coding RNAs in the eukaryotes, which originate from the intergenic regions of the genome. A ~4kb long lincRNA-p21 is derived from a transcription unit next to the p21/Cdkn1a gene locus. LincRNA-p21 plays key regulatory roles in p53 dependent transcriptional repression and translational repression through its physical association with proteins such as hnRNP-K and HuR.It is also involved in the aberrant gene expression in different cancers. However, detailed information on its structure, recognition, and trans-regulation by proteins is not well known. In this study, we have carried out a complete gene analysis and annotation of lincRNA-p21. This analysis showed that lincRNA-p21 is highly conserved in primates, and its conservation drops significantly in lower organisms. Furthermore, our analysis has revealed two structurally conserved domains in the 5’ and 3’ terminal regions of lincRNA-p21. Phylogenetic analysis has revealed discrete evolutionary dynamics in these conserved domains for orthologous sequences of lincRNA-p21, which have evolved slowly across primates compared to other mammals. Using Infernal based covariance analysis, we have computed the secondary structures of these domains. The secondary structures were further validated by energy minimization criteria for individual orthologous sequences as well as the full-length human lincRNA-p21. In summary, this analysis has led to the identification of sequence and structural motifs in the conserved fragments, indicating the functional importance for these regions.', 'corpus_id': 232412244, 'score': 0}, {'doc_id': '233246520', 'title': 'Intron Losses and Gains in Nematodes: Not Eccentric at All', 'abstract': 'The evolution of spliceosomal introns has been widely studied among various eukaryotic groups. Researchers nearly reached the consensuses on the pattern and the mechanisms of intron losses and gains across eukaryotes. However, according to previous studies that analyzed a few genes or genomes of nematodes, Nematoda seem to be an eccentric group. Taking advantage of the recent accumulation of sequenced genomes, we carried out an extensive analysis on the intron losses and gains using 104 nematodes genomes across all the five Clades of the phylum. Nematodes have a wide range of intron density, from less than one to more than nine per 1kbp coding sequence. The rates of intron losses and gains exhibit significant heterogeneity both across different nematode lineages and across different evolutionary stages of the same lineage. The frequency of intron losses far exceeds that of intron gains. Five pieces of evidence supporting the model of cDNA-mediated intron loss have been observed in ten Caenorhabditis species, the dominance of the precise intron losses, frequent loss of adjacent introns, and high-level expression of the intron-lost genes, preferential losses of short introns, and the preferential losses of introns close to 3′-ends of genes. Like studies in most eukaryotic groups, we cannot find the source sequences for the limited number of intron gains detected in the Caenorhabditis genomes. All the results indicate that nematodes are a typical eukaryotic group rather than an outlier in intron evolution.', 'corpus_id': 233246520, 'score': 0}, {'doc_id': '248525', 'title': 'Genome-wide signatures of convergent evolution in echolocating mammals', 'abstract': 'Evolution is typically thought to proceed through divergence of genes, proteins and ultimately phenotypes. However, similar traits might also evolve convergently in unrelated taxa owing to similar selection pressures. Adaptive phenotypic convergence is widespread in nature, and recent results from several genes have suggested that this phenomenon is powerful enough to also drive recurrent evolution at the sequence level. Where homoplasious substitutions do occur these have long been considered the result of neutral processes. However, recent studies have demonstrated that adaptive convergent sequence evolution can be detected in vertebrates using statistical methods that model parallel evolution, although the extent to which sequence convergence between genera occurs across genomes is unknown. Here we analyse genomic sequence data in mammals that have independently evolved echolocation and show that convergence is not a rare process restricted to several loci but is instead widespread, continuously distributed and commonly driven by natural selection acting on a small number of sites per locus. Systematic analyses of convergent sequence evolution in 805,053 amino acids within 2,326 orthologous coding gene sequences compared across 22 mammals (including four newly sequenced bat genomes) revealed signatures consistent with convergence in nearly 200 loci. Strong and significant support for convergence among bats and the bottlenose dolphin was seen in numerous genes linked to hearing or deafness, consistent with an involvement in echolocation. Unexpectedly, we also found convergence in many genes linked to vision: the convergent signal of many sensory genes was robustly correlated with the strength of natural selection. This first attempt to detect genome-wide convergent sequence evolution across divergent taxa reveals the phenomenon to be much more pervasive than previously recognized.', 'corpus_id': 248525, 'score': 1}, {'doc_id': '232308283', 'title': 'Shifts in morphology, gene expression, and selection underlie web loss in Hawaiian Tetragnatha spiders', 'abstract': 'Background A striking aspect of evolution is that it often converges on similar trajectories. Evolutionary convergence can occur in deep time or over short time scales, and is associated with the imposition of similar selective pressures. Repeated convergent events provide a framework to infer the genetic basis of adaptive traits. The current study examines the genetic basis of secondary web loss within web-building spiders (Araneoidea). Specifically, we use a lineage of spiders in the genus Tetragnatha (Tetragnathidae) that has diverged into two clades associated with the relatively recent (5\xa0mya) colonization of, and subsequent adaptive radiation within, the Hawaiian Islands. One clade has adopted a cursorial lifestyle, and the other has retained the ancestral behavior of capturing prey with sticky orb webs. We explore how these behavioral phenotypes are reflected in the morphology of the spinning apparatus and internal silk glands, and the expression of silk genes. Several sister families to the Tetragnathidae have undergone similar web loss, so we also ask whether convergent patterns of selection can be detected in these lineages. Results The cursorial clade has lost spigots associated with the sticky spiral of the orb web. This appears to have been accompanied by loss of silk glands themselves. We generated phylogenies of silk proteins (spidroins), which showed that the transcriptomes of cursorial Tetragnatha contain all major spidroins except for flagelliform. We also found an uncharacterized spidroin that has higher expression in cursorial species. We found evidence for convergent selection acting on this spidroin, as well as genes involved in protein metabolism, in the cursorial Tetragnatha and divergent cursorial lineages in the families Malkaridae and Mimetidae. Conclusions Our results provide strong evidence that independent web loss events and the associated adoption of a cursorial lifestyle are based on similar genetic mechanisms. Many genes we identified as having evolved convergently are associated with protein synthesis, degradation, and processing, which are processes that play important roles in silk production. This study demonstrates, in the case of independent evolution of web loss, that similar selective pressures act on many of the same genes to produce the same phenotypes and behaviors.', 'corpus_id': 232308283, 'score': 0}, {'doc_id': '232283165', 'title': 'Ecological correlates of gene family size: the draft genome of the redheaded', 'abstract': None, 'corpus_id': 232283165, 'score': 0}, {'doc_id': '237469401', 'title': 'Comparative genomics provides insights into the aquatic adaptations of mammals', 'abstract': 'Significance Divergent lineages can respond to common environmental factors through convergent processes involving shared genomic components or pathways, but the molecular mechanisms are poorly understood. Here, we provide genomic resources and insights into the evolution of mammalian lineages adapting to aquatic life. Our data suggest convergent evolution, for example, in association with thermoregulation through genes associated with a surface heat barrier (NFIA) and internal heat exchange (SEMA3E). Combined with the support of previous reports showing that the UCP1 locus has been lost in many marine mammals independently, our results suggest that the thermostatic strategy of marine mammals shifted from enhancing heat production to limiting heat loss. The ancestors of marine mammals once roamed the land and independently committed to an aquatic lifestyle. These macroevolutionary transitions have intrigued scientists for centuries. Here, we generated high-quality genome assemblies of 17 marine mammals (11 cetaceans and six pinnipeds), including eight assemblies at the chromosome level. Incorporating previously published data, we reconstructed the marine mammal phylogeny and population histories and identified numerous idiosyncratic and convergent genomic variations that possibly contributed to the transition from land to water in marine mammal lineages. Genes associated with the formation of blubber (NFIA), vascular development (SEMA3E), and heat production by brown adipose tissue (UCP1) had unique changes that may contribute to marine mammal thermoregulation. We also observed many lineage-specific changes in the marine mammals, including genes associated with deep diving and navigation. Our study advances understanding of the timing, pattern, and molecular changes associated with the evolution of mammalian lineages adapting to aquatic life.', 'corpus_id': 237469401, 'score': 1}, {'doc_id': '59017841', 'title': 'Genome-Wide Screens for Molecular Convergent Evolution in Mammals', 'abstract': 'Convergent evolution can occur at both the phenotypic and molecular level. Of particular interest are cases of convergent molecular changes that underlie convergent phenotypic changes, as they highlight the genomic differences that underlie phenotypic adaptations and can inform us on why evolution has repeatedly chosen the same solution in lineages that have evolved independently. Many approaches to identify convergent molecular evolution have focused on candidate genes with known functions as well as lineages with known convergent phenotypes. The growing amount of genomic sequence data makes it now possible to systematically detect molecular convergence genome-wide. Here, we highlight the advantages and drawbacks of using genomic screens to identify molecular convergence. We present our method to detect convergent substitutions between any pair of lineages in a genome-wide manner, ways of enriching for convergence that are more likely to affect protein function, and present novel cases of convergence in echolocating mammals. Our results suggest that genomic screens have the potential to generate new hypotheses of associations between molecular convergence and phenotypic convergence. Together with experimental assays to test for functional convergence, this will contribute to revealing the genomic changes that underlie convergent phenotypic changes.', 'corpus_id': 59017841, 'score': 1}, {'doc_id': '232321328', 'title': 'Macroevolutionary dynamics of dentition in Mesozoic birds reveal no long-term selection towards tooth loss', 'abstract': ""Summary Several potential drivers of avian tooth loss have been proposed, although consensus remains elusive as fully toothless jaws arose independently numerous times among Mesozoic avialans and dinosaurs more broadly. The origin of crown bird edentulism has been discussed in terms of a broad-scale selective pressure or trend toward toothlessness, although this has never been quantitatively tested. Here, we find no evidence for models whereby iterative acquisitions of toothlessness among Mesozoic Avialae were driven by an overarching selective trend. Instead, our results support modularity among jaw regions underlying heterogeneous tooth loss patterns and indicate a substantially later transition to complete crown bird edentulism than previously hypothesized (∼90 mya). We show that patterns of avialan tooth loss adhere to Dollo's law and suggest that the exclusive survival of toothless birds to the present represents lineage-specific selective pressures, irreversibility of tooth loss, and the filter of the Cretaceous-Paleogene (K–Pg) mass extinction."", 'corpus_id': 232321328, 'score': 0}]"
108	{'doc_id': '229309616', 'title': 'GAPP: Inventory Tracking Applications in Mobile Networks', 'abstract': 'On a daily basis people waste so many items, and what better way to help them from wasting but to create an app that tracks their groceries, creates less waste in the world and saves money at the same time in mobile networks. People waste food on average 1 pound per person each day they live. The application GAPP will cut down costs and waste in everyday life. People also misplace or lose track of items daily. Location of items is a big deal and knowing where they are at all times on a device would help.', 'corpus_id': 229309616}	20633	"[{'doc_id': '227181021', 'title': 'Food pantries select healthier foods after nutrition information is available on their food bank’s ordering platform', 'abstract': 'Abstract Objective: In the USA, community-based food pantries provide free groceries to people struggling with food insecurity. Many pantries obtain food from regional food banks using an online shopping platform. A food bank introduced a visible nutrition rank (i.e. green, yellow or red) onto its platform. The hypothesis was that pantry orders would increase for the healthiest options (green) and decrease for the least healthy options (red). Design: Interrupted time series (ITS) analysis of a natural experiment. Monthly data included nutrition ranks of available inventory and itemised records of all products ordered during the 15-month baseline period and 14-month intervention. Setting: A New England food bank. Participants: The twenty-five largest food pantries in the network based on pounds of food ordered. Results: Descriptive analyses of 63 922 pantry ordering records before and after the visible ranks identified an increase in the proportion of green items ordered (39·3–45·4 %) and a decrease in the proportion of red items ordered (10·5–5·1 %). ITS analyses controlling for monthly changes in inventory available and pantry variables indicated that average monthly orders of green items increased by 1286 pounds (P < 0·001) and red orders decreased by 631 pounds (P = 0·045). Among the largest changes were increases in orders of fresh produce, brown rice, low-fat dairy and low-fat meats and decreases in orders of sugary juice drinks, canned fruit with added sugar, higher fat dairy and higher fat meats. Conclusions: This promising practice can support system-wide efforts to promote healthier foods within the food banking network.', 'corpus_id': 227181021, 'score': 1}, {'doc_id': '199039643', 'title': ""P140 The University of Arizona's SNAP-Ed Used the Health Food Pantry Assessment Tool to Identify Policy, Systems, and Environmental Changes in Food Pantries"", 'abstract': ""Objective To use the Health Food Pantry Assessment Tool (HFPAT) to identify policy, systems, and environmental (PSE) changes in Arizona's food pantries. Use of Theory or Research The HFPAT was selected as an observational survey that measured the healthfulness of food pantry environments and provided PSE resources to guide food pantries and the University of Arizona Supplemental Nutrition Education - Program (UA SNAP-Ed) programming. Target Audience Four food pantries (Apache-2, Maricopa-2 Counties) that participated in SNAP-Ed completed the HFPAT. Program Description The SNAP-Ed sent the HFPAT in advance of site visits. Two Arizona SNAP-Ed staff independently completed the observational portions of the survey and results were compared. The survey was completed by interviewing the food pantry staff. Reports were generated using Qualtrics® and given to the food pantries. The HFPAT Resource Guide, which provides a resource for each section, was included. Evaluation Methods The HFPAT contained 69 questions divided into five sections: pantry location and entrance, food available to clients, policies of the food pantry, frozen chilled, dry storage and food safety, and services for clients. It provided a numeric score on a scale of 0-100; with a score of 100 signifying the healthiest food pantry environment. Results Four food pantries were assessed. The total scores ranged from 41 to 50 (mean: 46.25 out of 100). The food pantries’ mean scores for each section were: a) 9.25 out of 15 possible points; b) 19.25 out of 57 possible points; c) 6 out of 12 possible points; d) 6.25 out of 10 possible points; and e) 4.5 out of 6 possible points. Conclusions The UA SNAP-Ed used the HFPAT to assist food pantries in identifying potential PSE interventions. This evaluation tool provided direction to the UA SNAP-Ed to give specified technical assistance to the food pantries, and resources to improve their low-scoring areas. Funding None."", 'corpus_id': 199039643, 'score': 1}, {'doc_id': '237380176', 'title': 'Integration of Food and Nutrition Education Across the Secondary School Curriculum: Two Experiential Models as Two Case Studies', 'abstract': 'The aim of this paper is to present the implementation and evaluation of two recognised programs, one from Australia and one from Denmark, that endeavour to integrate and enhance food and nutrition education across the secondary school curriculum and whole school programs. This paper details descriptions of design, delivery mode, core components and evaluation of each program based on existing detailed reports and original research investigations. Resultantly, one program in Australia (Stephanie Alexander Kitchen Garden Program) and one program in Denmark (LOMA or LOkal MAd = local food) are reported as two case studies. The target group for both programs is secondary school students in Years 7–12; both programs are conducted within secondary schools and within school hours. Both interventions focus on developing secondary students’ food production and food preparation knowledge and skills. Their evaluation methods have consisted of pre- and post-intervention surveys, single case study, and focus groups with both students and teachers. Both programs have reported possible integration across secondary school subjects and modifications in students’ knowledge and skills in food and nutrition. These programs have focused on developing an experiential and localised learning model for food and nutrition education, which may also address food insecurity concerns among adolescents which has been shown to correlate with poor nutrient intake and consequential health complications. Their overall model can be adapted taking into account the social, economic, and environmental context of a secondary school.', 'corpus_id': 237380176, 'score': 0}, {'doc_id': '214402534', 'title': 'How Urban Food Pantries are Stocked and Food Is Distributed: Food Pantry Manager Perspectives from Baltimore', 'abstract': 'ABSTRACT Low-income, food-insecure Baltimore residents frequently rely on food pantries. In this study, pantry managers were key informants who shared information on how and why certain products were obtained and distributed and their perceptions around the need for nutritious products. Managers prioritized providing “staple” foods that could comprise a meal, and most of these foods were shelf-stable. Most pantries distributed pre-assembled, uniform bags, rather than using a client choice method. Managers did not perceive that their clients wanted healthy foods, despite clients informing them of diet-related health conditions. Manager-level training may be necessary to align pantry operations with clients’ food needs.', 'corpus_id': 214402534, 'score': 1}, {'doc_id': '235681581', 'title': 'Associations between Food Pantry Size and Distribution Method and Healthfulness of Foods Received by Clients in Baltimore City Food Pantries', 'abstract': 'This study aimed to evaluate the association of the overall nutritional quality and the weight share of specific types of foods received by food pantry clients with food pantry size and distribution method. Data on healthy food weights using the gross weight share (GWS) of select foods and the validated Food Assortment Score Tool (FAST) were collected from 75 food pantry clients in Baltimore, Maryland. The average FAST score across the study population was 63.0 (SD: 10.4). Overall, no statistically significant differences in average FAST scores by pantry size and distribution method were found. However, among client-choice pantries, clients of small pantries had higher scores (p < 0.05) while among medium pantries, clients of traditional pantries had higher scores (p < 0.01). Subgroup analysis of GWS was stratified by pantry size and distribution methods. Findings suggested multi-level, multi-component interventions combining environmental strategies are needed to enhance the healthfulness of foods received by clients. Our analysis provided data to consider further refinements of pantry interventions and planning of more rigorous research on factors influencing the effectiveness of pantry interventions.', 'corpus_id': 235681581, 'score': 1}, {'doc_id': '237460165', 'title': 'The role of integrated marketing communication for ustainable development in food production', 'abstract': ""The decrease in food production output, the suspension of production, and the decrease in product demand have influenced the operation of producers and their communication with customers in 2020. This brings to the forefront the producer's role in the use of IMC for sustainable development in Latvia. The purpose of the survey of leading specialists at Latvian food producers was to find out their opinion on the trends of development and a sustainable use of IMC in business. The object of the research: IMC for sustainable development. The subject: IMC for sustainable marketing at Latvian food producers. The study uses monographic, quantitative, qualitative methods – interviews of leading specialists of producers. It represents a follow-up to the author's previous studies in the food retail industry where she researched food retail chains and conducted a survey of buyers. She developed a conceptual model of IMC for sustainable business development and found that each sector has peculiarities in product selling, service provision, etc., yet there are also common trends that apply to all industries. The author urges further market research, covering producers. The results show some trends: 1) the motivation to use IMC for sustainable development has grown due to the increased use of technologies; 2) extended periods of sedentarism have exacerbated the problem of overweight in society and given rise to demand for healthy ecological products, including natural ingredients in production; 3) the risk of employee illness and the reorganization of production has contributed to the use of digital marketing."", 'corpus_id': 237460165, 'score': 0}, {'doc_id': '237733828', 'title': 'Food insecurity, food waste, food behaviours and cooking confidence of UK citizens at the start of the COVID-19 lockdown', 'abstract': 'PurposeThe current pilot study explored food insecurity, food waste, food related behaviours and cooking confidence of UK consumers following the COVID-19 lockdown.Design/methodology/approachData were collected from 473 UK-based consumers (63% female) in March 2020. A cross-sectional online survey measured variables including food insecurity prevalence, self-reported food waste, food management behaviours, confidence and frequency of use of a range of cooking methods, type of food eaten (ultra-processed, semi-finished, unprocessed) and packaging type foods are purchased in.Findings39% of participants have experienced some food insecurity in the last 12\xa0months. Being younger, having a greater BMI and living in a smaller household were associated with food insecurity. Green leaves, carrots, potatoes and sliced bread are the most wasted of purchased foods. Polenta, green leaves and white rice are the most wasted cooked foods. Food secure participants reported wasting a smaller percentage of purchased and cooked foods compared to food insecure participants. Overall, participants were most confident about boiling, microwaving and stir-frying and least confident with using a pressure cooker or sous vide. Food secure participants were more confident with boiling, stir-frying, grilling and roasting than insecure food participants.Practical implicationsThis has implications for post lockdown policy, including food policies and guidance for public-facing communications.Originality/valueWe identified novel differences in self-report food waste behaviours and cooking confidence between the food secure and insecure consumers and observed demographics associated with food insecurity.', 'corpus_id': 237733828, 'score': 0}, {'doc_id': '237771192', 'title': 'Food Supply Chain Disruptions Owing to Covid-19', 'abstract': 'The Covid-19 pandemic has shown that food supply chains are the most critical component of economic and human activities. It has also created a lot of interest among researchers, practitioners and policymakers to study the significant challenges of the food supply chain caused by the pandemic. Therefore this work wanted to investigate the critical supply chain challenges due to Covid-19 with the help of a systematic literature review of well-established articles published in interdisciplinary journals. The selection of thirty one papers was made through a research protocol that helped select and identify research papers which were coded with the help of qualitative software Atlasti 9.0 to study which supply chain challenge amongst disruption, forecasting and inventory was most prominently studied in the literature. Results of software coding revealed that disruption was coded 170 times whereas forecasting 10 times and inventory 37 times as challenges of food Supply Chain Management (SCM). Therefore, it was concluded that most of the researchers considered disruption as one of the significant food supply chain challenges. Further coding also revealed that lockdown and labour related issues were the primary reasons for food supply chain disruption.', 'corpus_id': 237771192, 'score': 0}, {'doc_id': '216147108', 'title': 'IoT Based Food Inventory Tracking System for Domestic and Commercial Kitchens', 'abstract': 'A main component in effective kitchen management is inventory control. Keep the track of the kitchen inventory leads to more informed planning and decision-making. Using advanced technology in a fast pace and everything around us becoming automated, people prefer to monitor and perform their day-to-day activities by using the smart devices they carry everywhere rather than manually recording and monitoring things. In various households, restaurants and food chains maintaining and keeping track of everyday common food inventory is becoming one of the major problems. The major concern is replenishing the containers at the right moment and also knowing the expiry of foods. Busy restaurants and working people find it difficult to keep track because it requires human intervention at the right time. Hence, it is easy to keep an eye on potential problems related to waste and pilferage. In this problem statement we propose an IOT (Internet of Things) based food inventory tracking system, which ensures real time monitoring of the kitchen inventory. To understand the daily or weekly consumption collected data can be analysed in real time and also predict usage/consumption patterns. There is also provision to check the real time status, history of consumption through a application. The system contains a Microcontroller, load cell and wireless Module, MQTT broker, a hybrid application through which real time inventory tracking is performed. The proposed solution is absolutely wireless and reliable for both domestic and commercial purposes.', 'corpus_id': 216147108, 'score': 1}, {'doc_id': '237461473', 'title': ""Consumers' Intention Towards Online Food Ordering and Delivery Service"", 'abstract': 'Online food ordering and delivery services are the platforms that provide a system and service to the consumer to order and buy food products from foodservice or restaurant operators. The service is an up-and-coming trend among Millennials and the trend has ballooned with the introduction of movement control order (MCO) due to the pandemic COVID-19. The consumer’s demand towards online food ordering and delivery service has increased markedly due to the prohibition of dine-in at restaurant premises. This study focuses on highlighting the factors that influence consumer’s intention to use online food ordering and delivery services. The survey questionnaires were distributed among 384 respondents that represent the customers of restaurants in Shah Alam, Selangor. Data analyses were conducted using SPSS and multiple regression analysis. Findings revealed that usefulness, ease of use, and consumer’s enjoyment are the factors that influence consumer’s intention to use online food ordering services and conclusively, usefulness is the most significant factor that affects consumer’s intention to order food via online food ordering and delivery service. The findings from this study are beneficial for foodservice or restaurant operators in improving their businesses and staying competitive. Recommendations for future studies are included based on the findings of this study.', 'corpus_id': 237461473, 'score': 0}]"
109	{'doc_id': '216562574', 'title': 'Extending Multilingual BERT to Low-Resource Languages', 'abstract': 'Multilingual BERT (M-BERT) has been a huge success in both supervised and zero-shot cross-lingual transfer learning. However, this success is focused only on the top 104 languages in Wikipedia it was trained on. In this paper, we propose a simple but effective approach to extend M-BERT E-MBERT so it can benefit any new language, and show that our approach aids languages that are already in M-BERT as well. We perform an extensive set of experiments with Named Entity Recognition (NER) on 27 languages, only 16 of which are in M-BERT, and show an average increase of about 6% F1 on M-BERT languages and 23% F1 increase on new languages. We release models and code at http://cogcomp.org/page/publication_view/912.', 'corpus_id': 216562574}	15237	[{'doc_id': '231879790', 'title': 'Customizing Contextualized Language Models for Legal Document Reviews', 'abstract': 'Inspired by the inductive transfer learning on computer vision, many efforts have been made to train contextualized language models that boost the performance of natural language processing tasks. These models are mostly trained on large general-domain corpora such as news, books, or Wikipedia. Although these pre-trained generic language models well perceive the semantic and syntactic essence of a language structure, exploiting them in a real-world domain-specific scenario still needs some practical considerations to be taken into account such as token distribution shifts, inference time, memory, and their simultaneous proficiency in multiple tasks. In this paper, we focus on the legal domain and present how different language models trained on general-domain corpora can be best customized for multiple legal document reviewing tasks. We compare their efficiencies with respect to task performances and present practical considerations.', 'corpus_id': 231879790, 'score': 1}, {'doc_id': '232257615', 'title': 'Investigating Monolingual and Multilingual BERTModels for Vietnamese Aspect Category Detection', 'abstract': 'Aspect category detection (ACD) is one of the challenging tasks in the Aspect-based sentiment Analysis problem. The purpose of this task is to identify the aspect categories mentioned in user-generated reviews from a set of pre-defined categories. In this paper, we investigate the performance of various monolingual pre-trained language models compared with multilingual models on the Vietnamese aspect category detection problem. We conduct the experiments on two benchmark datasets for the restaurant and hotel domain. The experimental results demonstrated the effectiveness of the monolingual PhoBERT model than others on two datasets. We also evaluate the performance of the multilingual model based on the combination of whole SemEval-2016 datasets in other languages with the Vietnamese dataset. To the best of our knowledge, our research study is the first attempt at performing various available pretrained language models on aspect category detection task and utilize the datasets from other languages based on multilingual models.', 'corpus_id': 232257615, 'score': 0}, {'doc_id': '41480412', 'title': 'Context Models for OOV Word Translation in Low-Resource Languages', 'abstract': 'Out-of-vocabulary word translation is a major problem for the translation of low-resource languages that suffer from a lack of parallel training data. This paper evaluates the contributions of target-language context models towards the translation of OOV words, specifically in those cases where OOV translations are derived from external knowledge sources, such as dictionaries. We develop both neural and non-neural context models and evaluate them within both phrase-based and self-attention based neural machine translation systems. Our results show that neural language models that integrate additional context beyond the current sentence are the most effective in disambiguating possible OOV word translations. We present an efficient second-pass lattice-rescoring method for wide-context neural language models and demonstrate performance improvements over state-of-the-art self-attention based neural MT systems in five out of six low-resource language pairs.', 'corpus_id': 41480412, 'score': 1}, {'doc_id': '232238865', 'title': 'NAMED ENTITY RECOGNITION IN THANGKA FIELD BASED ON BERT-BiLSTM-CRF-a', 'abstract': 'Thangka is one of the precious intangible cultural heritages, which is closely related to Tibetan Buddhism. However, Tibetan Buddhism has a complex system, and the naming patterns of various deities are not fixed and difficult to identify from Chinese texts. In this paper, we propose a multi-neural network fusion named entity recognition model BERT-BiLSTM-CRF-a which is based on the BERT pre-training language model, Bidirectional Long-and-Short Term Memory (BiLSTM) and Conditional Random Field (CRF). Specifically, the model uses the BERT to enhance the dynamic representation ability. Then, a weighting method from attention mechanism is introduced to weight the forward and backward BiLSTM hidden layer vectors before concatenating to further improve the effective utilization of context features. Finally, CRF model is used to output the global optimal annotation results. Experimental results on the test sets show that the recall of the BERT-BiLSTM-CRFa model is 87.4%, 8.2% higher than the traditional named entity recognition model BiLSTM-CRF, and the F1 value is also 4.8% higher. Therefore, the model we proposed can be effectively used in the task of named entity recognition in thangka field.', 'corpus_id': 232238865, 'score': 0}, {'doc_id': '195767029', 'title': 'Evaluating Language Model Finetuning Techniques for Low-resource Languages', 'abstract': 'Unlike mainstream languages (such as English and French), low-resource languages often suffer from a lack of expert-annotated corpora and benchmark resources that make it hard to apply state-of-the-art techniques directly. In this paper, we alleviate this scarcity problem for the low-resourced Filipino language in two ways. First, we introduce a new benchmark language modeling dataset in Filipino which we call WikiText-TL-39. Second, we show that language model finetuning techniques such as BERT and ULMFiT can be used to consistently train robust classifiers in low-resource settings, experiencing at most a 0.0782 increase in validation error when the number of training examples is decreased from 10K to 1K while finetuning using a privately-held sentiment dataset.', 'corpus_id': 195767029, 'score': 1}, {'doc_id': '231918577', 'title': 'Continuous Learning in Neural Machine Translation using Bilingual Dictionaries', 'abstract': 'While recent advances in deep learning led to significant improvements in machine translation, neural machine translation is often still not able to continuously adapt to the environment. For humans, as well as for machine translation, bilingual dictionaries are a promising knowledge source to continuously integrate new knowledge. However, their exploitation poses several challenges: The system needs to be able to perform one-shot learning as well as model the morphology of source and target language. In this work, we proposed an evaluation framework to assess the ability of neural machine translation to continuously learn new phrases. We integrate one-shot learning methods for neural machine translation with different word representations and show that it is important to address both in order to successfully make use of bilingual dictionaries. By addressing both challenges we are able to improve the ability to translate new, rare words and phrases from 30% to up to 70%. The correct lemma is even generated by more than 90%.', 'corpus_id': 231918577, 'score': 0}, {'doc_id': '232417503', 'title': 'Retraining DistilBERT for a Voice Shopping Assistant by Using Universal Dependencies', 'abstract': 'In this work, we retrained the distilled BERT language model for Walmart’s voice shopping assistant on retail domainspecific data. We also injected universal syntactic dependencies to improve the performance of the model further. The Natural Language Understanding (NLU) components of the voice assistants available today are heavily dependent on language models for various tasks. The generic language models such as BERT and RoBERTa are useful for domainindependent assistants but have limitations when they cater to a specific domain. For example, in the shopping domain, the token ‘horizon’ means a brand instead of its literal meaning. Generic models are not able to capture such subtleties. So, in this work, we retrained a distilled version of the BERT language model on retail domain-specific data for Walmart’s voice shopping assistant. We also included universal dependency-based features in the retraining process further to improve the performance of the model on downstream tasks. We evaluated the performance of the retrained language model on four downstream tasks, including intent-entity detection, sentiment analysis, voice title shortening and proactive intent suggestion. We observed an increase in the performance of all the downstream tasks of up to 1.31% on average.', 'corpus_id': 232417503, 'score': 0}, {'doc_id': '232320349', 'title': 'Multilingual Autoregressive Entity Linking', 'abstract': 'We present mGENRE, a sequence-to-sequence system for the Multilingual Entity Linking (MEL) problem -- the task of resolving language-specific mentions to a multilingual Knowledge Base (KB). For a mention in a given language, mGENRE predicts the name of the target entity left-to-right, token-by-token in an autoregressive fashion. The autoregressive formulation allows us to effectively cross-encode mention string and entity names to capture more interactions than the standard dot product between mention and entity vectors. It also enables fast search within a large KB even for mentions that do not appear in mention tables and with no need for large-scale vector indices. While prior MEL works use a single representation for each entity, we match against entity names of as many languages as possible, which allows exploiting language connections between source input and target name. Moreover, in a zero-shot setting on languages with no training data at all, mGENRE treats the target language as a latent variable that is marginalized at prediction time. This leads to over 50% improvements in average accuracy. We show the efficacy of our approach through extensive evaluation including experiments on three popular MEL benchmarks where mGENRE establishes new state-of-the-art results. Code and pre-trained models at this https URL', 'corpus_id': 232320349, 'score': 0}, {'doc_id': '232307525', 'title': 'BERT: A Review of Applications in Natural Language Processing and Understanding', 'abstract': 'Koroteev M.V., Financial University under the government of the Russian Federation, Moscow, Russia mvkoroteev@fa.ru Abstract: In this review, we describe the application of one of the most popular deep learning-based language models BERT. The paper describes the mechanism of operation of this model, the main areas of its application to the tasks of text analytics, comparisons with similar models in each task, as well as a description of some proprietary models. In preparing this review, the data of several dozen original scientific articles published over the past few years, which attracted the most attention in the scientific community, were systematized. This survey will be useful to all students and researchers who want to get acquainted with the latest advances in the field of natural language text analysis.', 'corpus_id': 232307525, 'score': 1}, {'doc_id': '233004275', 'title': 'Low-Resource Language Modelling of South African Languages', 'abstract': 'Language models are the foundation of current neural network-based models for natural language understanding and generation. However, research on the intrinsic performance of language models on African languages has been extremely limited, which is made more challenging by the lack of large or standardised training and evaluation sets that exist for English and other high-resource languages. In this paper, we evaluate the performance of open-vocabulary language models on lowresource South African languages, using bytepair encoding to handle the rich morphology of these languages. We evaluate different variants of n-gram models, feedforward neural networks, recurrent neural networks (RNNs), and Transformers on small-scale datasets. Overall, well-regularized RNNs give the best performance across two isiZulu and one Sepedi datasets. Multilingual training further improve performance on these datasets. We hope that this research will open new avenues for research into multilingual and low-resource language modelling for African languages.', 'corpus_id': 233004275, 'score': 1}]
110	{'doc_id': '235189698', 'title': 'Decontamination Of Endospores By Plasma Sources On Dried Surfaces: A Review Of Key Parameters And Inactivation Results', 'abstract': 'The efficiency of plasmas sources for the decontamination of heat-sensitive devices has been proven for more than 20 years, but commercial plasma-based sterilizers still have a narrow range of applications. This can be partially explained by difficulties to determine reliable bio-indicators and standardized microbiological test procedures required by industrial uses. In this paper, we examine the influence of environmental factors on the inactivation rate of microorganisms deposited on surfaces and treated by plasma sources. In addition, we present a literature review showing that several in-discharge and afterglow plasma sterilizers offer shorter treatment times than conventional low-temperature sterilizers to reduce the concentration of endospores on contaminated surfaces by 6-log. Finally we make a few recommendations for future plasma decontamination standards.', 'corpus_id': 235189698}	5694	"[{'doc_id': '232295874', 'title': 'Safety and protection in endoscopic services during phase II of COVID-19 pandemic: a national survey', 'abstract': 'Background The coronavirus disease 2019 (COVID-19) pandemic requires appropriate measures for containing infection spreading. Endoscopic procedures are considered at increased risk of infection transmission. We evaluated organizational aspects and personal behaviours in Italian Endoscopic Units during phase II of the pandemic. Methods A questionnaire on organizational aspects and use of personal protective equipment (PPE) were e-mailed to gastroenterologists working in Endoscopic Units. Data were analysed accordingly to the National Health Institute and Gastroenterology Societies recommendations. Results Data of 117 centres were collected, and different shortcomings emerged. Specific protocols for containing infection and training programs for operators were lacking in 20 and 30% of centres, respectively, and telephone triage 24–72 h before the endoscopy was not implemented in 25% of hospitals. In 30% of centres, the slot time for endoscopies and between examinations was not prolonged. PPE, masks, shirts and gloves were universally adopted, although with some differences. In 20% of centres, a FFPE-FFP3 mask was not adopted during endoscopic examinations. Postendoscopy patient tracking/contact was completed in only one-third of centres. Conclusions Our survey provides information on organizational and medical behaviours during COVID-19 phase II in Italy, which could be useful for adopting appropriate measures for containing COVID-19 spread during phase II.', 'corpus_id': 232295874, 'score': 0}, {'doc_id': '233872954', 'title': 'Endoscopy mitigation strategy with telemedicine and low-cost device use for COVID-19 prevention: A fourth-level Colombian center experience', 'abstract': '\n                  Background\n                  \n                     and study aims: The COVID-19 outbreak has reorganized surgical team conditions regarding endoscopy. The number of interventions has been reduced, the number of healthcare professionals must be limited, and both the patients and physicians are more protected than ever.\n               \n                  Patients and Methods\n                  In the highest peak of contagion in Colombia, endoscopy, colonoscopy, and esophagogastroduodenoscopy were performed using a low-cost disposable device. A total of 1388 procedures were performed. Every patient was assessed for symptoms via a telephone call, at the health center, and after the procedure, following specific attention routes.\n               \n                  Results\n                  After procedure follow-up, no positive cases of COVID-19 were noted.\n               \n                  Conclusion\n                  The methodology reduced the risk of infection during the COVID-19 pandemic.\n               ', 'corpus_id': 233872954, 'score': 0}, {'doc_id': '221124748', 'title': 'Alternative Methods of Sterilization in Dental Practices Against COVID-19', 'abstract': 'SARS-CoV-2, and several other microorganisms, may be present in nasopharyngeal and salivary secretions in patients treated in dental practices, so an appropriate clinical behavior is required in order to avoid the dangerous spread of infections. COVID-19 could also be spread when patients touches a contaminated surface with infected droplets and then touch their nose, mouth, or eyes. It is time to consider a dental practice quite similar to a hospital surgery room, where particular attention should be addressed to problems related to the spreading of infections due to air and surface contamination. The effectiveness of conventional cleaning and disinfection procedures may be limited by several factors; first of all, human operator dependence seems to be the weak aspect of all procedures. The improvement of these conventional methods requires the modification of human behavior, which is difficult to achieve and sustain. As alternative sterilization methods, there are some that do not depend on the operator, because they are based on devices that perform the entire procedure on their own, with minimal human intervention. In conclusion, continued efforts to improve the traditional manual disinfection of surfaces are needed, so dentists should consider combining the use of proper disinfectants and no-touch decontamination technologies to improve sterilization procedures.', 'corpus_id': 221124748, 'score': 0}, {'doc_id': '234792703', 'title': 'Single-use duodenoscopes: where are we and where are we going?', 'abstract': 'Purpose of review Given the growing concerns about infection transmission from use of contaminated reusable duodenoscopes, technological advancements have been made that vary from modifications of existing designs to development of single-use devices. Recent findings To circumvent mechanical limitations that preclude access to critical areas of a duodenoscope to perform thorough cleaning and disinfection, single-use disposable duodenoscopes have been developed. A thorough assessment of this technology is limited by the minimal published data that is currently available. This opinion assesses the current technical functionality of these devices, potential for further improvements, implications for healthcare economics and the future of gastrointestinal endoscopy. Summary Currently available data suggest that majority of endoscopic retrograde cholangiopancreatography procedures can be safely performed using single-use duodenoscopes. The ability to improve technical functionality, incorporate futuristic technology and secure financial reimbursement from insurance carriers will largely define the future prospects of this recent innovation.', 'corpus_id': 234792703, 'score': 1}, {'doc_id': '235961694', 'title': 'Multisocieties position paper: Microbiological surveillance on flexible endoscopes.', 'abstract': 'Transmission with endoscopes, particularly duodenoscope, of potential lethal infections prompted different scientific societies to deliver recommendations aimed reducing this risk. Some International societies extended recommendations on microbial surveillance to all the endoscopes and devices used in the reprocessing procedure. Considering the relevance of the topic, 8 Italian scientific societies of physicians, nurses and technical operators prepared a concerted document taking into account Institutional advisories and facilities in Italy. The rules for a correct microbial surveillance on endoscopes were detailed in term of what, how and when to perform the procedure, also suggesting behaviors in case of contamination.', 'corpus_id': 235961694, 'score': 1}, {'doc_id': '221310838', 'title': 'Did granny know best? Evaluating the antibacterial, antifungal and antiviral efficacy of acetic acid for home care procedures', 'abstract': 'Background Acetic acid has been used to clean and disinfect surfaces in the household for many decades. The antimicrobial efficacy of cleaning procedures can be considered particularly important for young, old, pregnant, immunocompromised people, but may also concern other groups, particularly with regards to the COVID-19 pandemics. This study aimed to show that acetic acid exhibit an antibacterial and antifungal activity when used for cleaning purposes and is able to destroy certain viruses. Furthermore, a disinfecting effect of laundry in a simulated washing cycle has been investigated. Results At a concentration of 10% and in presence of 1.5% citric acid, acetic acid showed a reduction of >\u20095-log steps according to the specifications of DIN EN 1040 and DIN EN 1275 for the following microorganisms: P. aeruginosa , E. coli , S. aureus , L. monocytogenes , K. pneumoniae , E. hirae and A. brasiliensis . For MRSA a logarithmic reduction of 3.19 was obtained. Tests on surfaces according to DIN EN 13697 showed a complete reduction (>\u20095-log steps) for P. aeruginosa , E. coli , S. aureus , E. hirae , A. brasiliensis and C. albicans at an acetic acid concentration of already 5%. Virucidal efficacy tests according to DIN EN 14476 and DIN EN 16777 showed a reduction of ≥4-log-steps against the Modified Vaccinia virus Ankara (MVA) for acetic acid concentrations of 5% or higher. The results suggest that acetic acid does not have a disinfecting effect on microorganisms in a dosage that is commonly used for cleaning. However, this can be achieved by increasing the concentration of acetic acid used, especially when combined with citric acid. Conclusions Our results show a disinfecting effect of acetic acid in a concentration of 10% and in presence of 1.5% citric acid against a variety of microorganisms. A virucidal effect against enveloped viruses could also be proven. Furthermore, the results showed a considerable antimicrobial effect of acetic acid when used in domestic laundry procedures.', 'corpus_id': 221310838, 'score': 0}, {'doc_id': '220976246', 'title': 'Endoscopic transmission of carbapenem-resistant Enterobacteriaceae: implications for Food and Drug Administration approval and postmarket surveillance of endoscopic devices.', 'abstract': 'Since the first widely reported case cluster of duodenoscope-associated transmission of carbapenem-resistant Enterobacteriaceae (CRE) in 2013 that affected 38 patients, similar outbreaks have occurred throughout the world. The U.S. Food and Drug Administration (FDA), Centers for Disease Control and Prevention, professional gastroenterology societies, and endoscope manufacturers have taken multiple steps to address this issue. Unlike prior outbreaks attributed to lapses in cleaning and reprocessing, transmission and outbreaks have continued to occur despite compliance with current reprocessing guidelines. A definitive method of duodenoscope reprocessing remains elusive, and the FDA recently recommended transition to new designs with disposable components that do not require reprocessing. The first fully disposable duodenoscope received FDA clearance as a ""breakthrough"" device in December 2019. Although the human, microbiologic, and endoscopic design factors responsible for infectious transmissions and disinfecting techniques to avoid them have been examined, discussion has not included the critical role of FDA regulation of duodenoscopes through the 510(k) clearance pathway and the mechanisms of postmarket surveillance, including adverse event reporting. We present an overview of the FDA approval of duodenoscopes by analyzing the FDA\'s 510(k) premarket notification database for data supporting clearance of duodenoscope models implicated in CRE-related outbreaks as well as subsequently required postmarket studies. We address the policy implications of CRE outbreaks on postmarketing surveillance and the need for increased gastroenterologist involvement in the life cycle of duodenoscopes and other medical devices. This includes reporting thorough adverse event data to the FDA and device manufacturers, supporting active surveillance studies to ensure\xa0safety and effectiveness, and evaluating implementation of recommendations to reduce adverse events.', 'corpus_id': 220976246, 'score': 1}, {'doc_id': '235406617', 'title': 'SARS-COV-2 in endoscopy: still a long way to go', 'abstract': 'We thank Chaussade et al that raised important questions regarding the potential ways of airborne transmission of microorganisms through endoscopes light source processors 2 and their discussion on our previously published work on endoscopes used in positive and critically ill patients with SARSCoV-2. Today we know that high viral loads on nasal and throat specimens characterise the early stage of COVID-19 disease, with viral load peaks during the first 7–10 days after symptoms onset and subsequent progressive decline over time. The dynamic of SARSCoV2 infection is not yet completely understood but can be strongly influenced by clinical factors such as age, comorbidities, serological response and many other factors. For instance, elderly patients might have higher viral loads. Our analysis included 12 patients with moderatetosevere COVID-19 disease and a mean age of 73 (53–93) years, who underwent a digestive or pulmonary endoscopic procedure after a mean time of 22.7 (IQR 9.75– 32.5) days from symptoms onset or first positive PCR for SARSCoV-2 on a nasopharyngeal swab. Notably, three patients underwent the procedure within 9 days from the diagnosis, while one patient had a positive swab 5 days after the endoscopic procedure. Nevertheless, all swabs collected from the endoscopes immediately after the procedure were negative. Our pilot study aimed to validate the efficacy of high disinfection of endoscopes with peracetic acid (PAA) on eliminating SARSCoV-2, based on the assumption that the endoscopes can be crucial in the transmission of SARSCoV-2 due to the direct contact with mucosal surfaces. Surprisingly, the virus could not be detected on any part of endoscopes immediately after the procedure, regardless of the kind of procedure and scope used. Therefore, we could not validate the endoscope reprocessing with PAA, though we could postulate that the role of the endoscope as an infection vehicle is lower than we could expect. To our knowledge, no cases of direct SARSCoV2 transmission related to infected endoscopes have been reported, even if this is difficult to verify. Nevertheless, endoscopy in positive patients cannot be considered safe since infections are mainly related to an airborne viral transmission. The potential risk of SARSCoV2 transmission described by Chaussade et al due to the environmental contamination derived from the circulation of air inside, from and to the light source processor is certainly not negligible. The primary aim of our study was the validation of the reprocessing method with PAA; thus, all microbiological tests were performed only on the endoscopes. However, the potential risk related to light source processor and environmental aerosolisation should be definitively further investigated, and not only for SARSCOV-2 but for all kind of microorganisms. If a potential risk is confirmed, the development of modified ventilation systems suggested by Chaussade et al is undoubtedly advisable to reduce as much as possible the putative risk of viral spreading in the endoscopic room. The pandemic definitively opened many questions and problems, especially in endoscopy, and still, there is a long way to go before answers and solutions are given.', 'corpus_id': 235406617, 'score': 1}, {'doc_id': '220334092', 'title': 'Reducing Anesthesia Workstation Contamination', 'abstract': 'Healthcare-associated infections are a source of morbidity and mortality in the United States and have been shown to be more preventable than current incidence. Anesthesia providers may be a source of and vector for some of these infections. Nurse anesthetists provide direct individual care for numerous patients daily, managing airways and invasive devices that contaminate hands with secretions which then may be transferred to the anesthesia workstation. Due to its complexity, the anesthesia machine is difficult to thoroughly clean and may become a reservoir for contaminants. The purpose of this paper will be to examine new interventions being explored to reduce the contamination of the anesthesia workstation. These interventions will include hand hygiene interventions, ultraviolet (UV) radiation for workstation disinfection, and anesthesia workstation barrier devices. Analysis of which interventions are the most effective may help to guide the direction of interventions to help reduce anesthesia machine contamination. REDUCING ANESTHESIA CONTAMINATION 3 Reducing Anesthesia Workstation Contamination Introduction The Centers for Disease Control and Prevention (CDC) estimated in 2018 that healthcareassociated infections (HAIs) affected at least one in 31 hospitalized patients (CDC, 2018). Similarly, according to the World Health Organization (WHO), the incidence of HAI in 2002 was at 4.5%, affecting 1.7 million patients and causing 99,000 deaths with an estimated financial impact of $6.5 billion in 2004 (WHO, 2009). The cost of HAIs in financial terms, patient mortality, and loss of quality of life cannot be underestimated. A meta-analysis by Umscheid et al. in 2011 of data from the National Nosocomial Infections Surveillance System (NNIS), the National Hospital Discharge Survey, and the American Hospital Association identified 1,737,125 total infections in 2002, with 98,987 deaths. Central line-associated blood stream infections (CLABSI) and ventilator associated pneumonia (VAP) accounted for two-thirds of deaths and had a five-fold increase in mortality compared to other HAIs. Costs per HAI ranged from a low of $5,600 with surgical site infections (SSIs) to a high of $110,800 for CLABSIs; with potential savings ranging from $115 million from catheter-associated urinary tract infections (CAUTIs) to $18.2 billion from CLABSIs. From evidence-based practice studies examined, it was estimated that certain percentages of various HAIs are preventable. CLABSI and CAUTI could potentially be reduced by 65-70%, while VAP and SSIs could reasonably be prevented in 55% of cases. (Umscheid et al., 2011). Another study, a multistate survey including 183 hospitals and 11,290 patients by Magill et al. in 2014 found a 4% incidence of HAIs and an 11.5% rate of mortality from HAI. Infections related to invasive devices such as VAP, CAUTI, and CLABSI accounted for 25.6% of HAIs, with SSIs accounting for another 21.8%. Median time until presentation of HAI was six days, REDUCING ANESTHESIA CONTAMINATION 4 and present on admission HAIs were 19.4% of the total, with 67.3% of these being SSIs (Magill et al., 2014). These factors may help obscure the role of anesthesia in contributing to infection. As providers constantly in direct contact with patients, nurse anesthetists are in a prime position to either be a significant vector for HAIs or to help find ways to solve this costly problem. These challenges illuminate the important task of anesthesia providers becoming involved in finding unique solutions to anesthesia’s role in propagating HAIs. Solutions that work in much of the hospital may not be effective for the operating room (OR) setting. In that vein, this paper will investigate the scope and nature of the problem of anesthesia workstation contamination as well as several avenues of solutions proposed to help reduce contamination. These include novel methods of hand hygiene customized for anesthesia providers, use of UV radiation to help decontaminate the complex permanent parts of the anesthesia workstation, and a novel cover system to help prevent any contamination of the anesthesia machine. Methods Articles for inclusion in this literature review were found by searching PubMed and CINAHL. Studies were chosen from within the last five years, with exceptions made for studies with great impact on the state of the literature that provided a basis for future study.', 'corpus_id': 220334092, 'score': 0}, {'doc_id': '237468607', 'title': 'Both microbiological surveillance and audit of procedures improve reprocessing of flexible bronchoscopes and patient safety.', 'abstract': 'BACKGROUND\nMicrobiological surveillance of bronchoscopes and automatic endoscope reprocessors (AERs)/washer disinfectors as a quality control measure is controversial. Experts also are divided on the infection risks associated with bronchoscopic procedures.\n\n\nOBJECTIVE\nWe evaluated the impact of routine microbiological surveillance and audits of cleaning/disinfection practices on contamination rates of reprocessed bronchoscopes.\n\n\nDESIGN\nAudits were conducted of reprocessing procedures and microbiological surveillance on all flexible bronchoscopes used from January 2007 to June 2020 at a teaching hospital in France. Contamination rates per year were calculated and analyzed using a Poisson regression model. The risk factors for microbiological contamination were analyzed using a multivariable logistical regression model.\n\n\nRESULTS\nIn total, 478 microbiological tests were conducted on 91 different bronchoscopes and 57 on AERs. The rate of bronchoscope contamination significantly decreased between 2007 and 2020, varying from 30.2 to 0% (P < .0001). Multivariate analysis confirmed that retesting after a previous contaminated test was significantly associated with higher risk of bronchoscope contamination (OR, 2.58; P = .015). This finding was explained by the persistence of microorganisms in bronchoscopes despite repeated disinfections. However, the risk of persistent contamination was not associated with the age of the bronchoscope.\n\n\nCONCLUSIONS\nOur results confirm that bronchoscopes can remain contaminated despite repeated reprocessing. Routine microbial testing of bronchoscopes for quality assurance and audit of decontamination and disinfection procedures can improve the reprocessing of bronchoscopes and minimize the rate of persistent contamination.', 'corpus_id': 237468607, 'score': 1}]"
111	{'doc_id': '214605669', 'title': 'VECSEL systems for quantum information processing with trapped beryllium ions', 'abstract': 'Two vertical-external-cavity surface-emitting laser (VECSEL) systems producing ultraviolet (UV) radiation at 235 nm and 313 nm are demonstrated. The systems are suitable for quantum information processing applications with trapped beryllium ions. Each system consists of a compact, single-frequency, continuous-wave VECSEL producing high-power near-infrared light, tunable over tens of nanometers. One system generates 2.4 W at 940 nm, using a gain mirror based on GaInAs/GaAs quantum wells, which is converted to 54 mW of 235 nm light for photoionization of neutral beryllium atoms. The other system uses a novel gain mirror based on GaInNAs/GaAs quantum-wells, enabling wavelength extension with manageable strain in the GaAs lattice. This system generates 1.6 W at 1252 nm, which is converted to 41 mW of 313 nm light that is used to laser cool trapped $^{9}$Be$^{+}$ ions and to implement quantum state preparation and detection. The 313 nm system is also suitable for implementing high-fidelity quantum gates, and more broadly, our results extend the capabilities of VECSEL systems for applications in atomic, molecular, and optical physics.', 'corpus_id': 214605669}	5505	[{'doc_id': '215814520', 'title': 'Benchmarking a High-Fidelity Mixed-Species Entangling Gate.', 'abstract': 'We implement a two-qubit logic gate between a ^{43}Ca^{+} hyperfine qubit and a ^{88}Sr^{+} Zeeman qubit. For this pair of ion species, the S-P optical transitions are close enough that a single laser of wavelength 402\xa0nm can be used to drive the gate but sufficiently well separated to give good spectral isolation and low photon scattering errors. We characterize the gate by full randomized benchmarking, gate set tomography, and Bell state analysis. The latter method gives a fidelity of 99.8(1)%, comparable to that of the best same-species gates and consistent with known sources of error.', 'corpus_id': 215814520, 'score': 1}, {'doc_id': '218527805', 'title': 'Detection of metastable electronic states by Penning trap mass spectrometry', 'abstract': 'State-of-the-art optical clocks 1 achieve precisions of 10 −18 or better using ensembles of atoms in optical lattices 2 , 3 or individual ions in radio-frequency traps 4 , 5 . Promising candidates for use in atomic clocks are highly charged ions 6 (HCIs) and nuclear transitions 7 , which are largely insensitive to external perturbations and reach wavelengths beyond the optical range 8 that are accessible to frequency combs 9 . However, insufficiently accurate atomic structure calculations hinder the identification of suitable transitions in HCIs. Here we report the observation of a long-lived metastable electronic state in an HCI by measuring the mass difference between the ground and excited states in rhenium, providing a non-destructive, direct determination of an electronic excitation energy. The result is in agreement with advanced calculations. We use the high-precision Penning trap mass spectrometer PENTATRAP to measure the cyclotron frequency ratio of the ground state to the metastable state of the ion with a precision of 10 −11 —an improvement by a factor of ten compared with previous measurements 10 , 11 . With a lifetime of about 130 days, the potential soft-X-ray frequency reference at 4.96\xa0×\xa010 16 hertz\xa0(corresponding to a transition energy of 202 electronvolts) has a linewidth of only 5\xa0×\xa010 −8 hertz and one of the highest electronic quality factors (10 24 ) measured experimentally so far. The low uncertainty of our method will enable searches for further soft-X-ray clock transitions 8 , 12 in HCIs, which are required for precision studies of fundamental physics 6 . Penning trap mass spectrometry is used to measure the electronic transition energy from a long-lived metastable state to the ground state in highly charged rhenium ions with a precision of 10 −11 .', 'corpus_id': 218527805, 'score': 0}, {'doc_id': '215548456', 'title': 'Low-requirement fast gates enable quantum computation in long ion chains', 'abstract': 'We present a model for implementing fast entangling gates (${\\sim}1\\mu$s) with ultra-fast pulses in arbitrarily long ion chains, that requires low numbers of pulses and can be implemented with laser repetition rates well within experimental capability. We demonstrate that we are able to optimise pulse sequences that have theoretical fidelities above $99.99\\%$ in arbitrarily long ion-chains, for laser repetition rates on the order of $100-300$~MHz. Notably, we find higher repetition rates are not required for gates in longer ion chains, which is in contrast to scaling analyses with other gate schemes. When pulse imperfections are considered in our calculations, we find that achievable gate fidelity is independent of the number of ions in the chain. We also show that pulse control requirements do not scale up with the number of ions. We find that population transfer efficiencies of above $99.9\\%$ from individual ultra-fast pulses is the threshold for realising high-fidelity gates, which may be achievable in near-future experiments.', 'corpus_id': 215548456, 'score': 1}, {'doc_id': '218971713', 'title': 'Studies of thorium and ytterbium ion trap loading from laser ablation for gravity monitoring with nuclear clocks', 'abstract': 'Compact and robust ion traps for thorium are enabling technology for the next generation of atomic clocks based on a low-energy isomeric transition in the thorium-229 nucleus. We aim at a laser ablation loading of single triply ionized thorium in a radio-frequency electromagnetic linear Paul trap. Detection of ions is based on a modified mass spectrometer and a channeltron with single-ion sensitivity. In this study, we successfully created and detected 232Th+ and 232Th2+ ions from plasma plumes, studied their yield evolution, and compared the loading to a quadrupole ion trap with Yb. We explore the feasibility of laser ablation loading for future low-cost 229Th3+ trapping. The thorium ablation yield shows a strong depletion, suggesting that we have ablated oxide layers from the surface and the ions were a result of the plasma plume evolution and collisions. Our results are in good agreement with similar experiments for other elements and their oxides.', 'corpus_id': 218971713, 'score': 1}, {'doc_id': '231718882', 'title': 'Fast high-fidelity single-qubit gates for flip-flop qubits in silicon', 'abstract': 'The flip-flop qubit, encoded in the states with antiparallel donor-bound electron and donor nuclear spins in silicon, showcases long coherence times, good controllability, and, in contrast to other donorspin-based schemes, long-distance coupling. Electron spin control near the interface, however, is likely to shorten the relaxation time by many orders of magnitude, reducing the overall qubit quality factor. Here, we theoretically study the multilevel system that is formed by the interacting electron and nuclear spins and derive analytical effective two-level Hamiltonians with and without periodic driving. We then propose an optimal control scheme that produces fast and robust single-qubit gates in the presence of low-frequency noise and relatively weak magnetic fields without relying on parametrically restrictive sweet spots. This scheme increases considerably both the relaxation time and the qubit quality factor.', 'corpus_id': 231718882, 'score': 0}, {'doc_id': '213175483', 'title': 'Trapped Rydberg ions: A new platform for quantum information processing', 'abstract': 'Abstract In this chapter, we present an overview of experiments with trapped Rydberg ions and outline the advantages and challenges of developing applications of this new platform for quantum computing, sensing, and simulation. Trapped Rydberg ions feature several important properties, unique in their combination: they are tightly bound in a harmonic potential of a Paul trap, in which their internal and external degrees of freedom can be controlled in a precise fashion. High fidelity state preparation of both internal and motional states of the ions has been demonstrated, and the internal states have been employed to store and manipulate qubit information. Furthermore, strong dipolar interactions can be realized between ions in Rydberg states and be explored for investigating correlated many body systems. By laser coupling to Rydberg states, the polarizability of the ions can be both enhanced and tuned. This can be used to control the interactions with the trapping fields in a Paul trap as well as dipolar interactions between the ions. Thus, trapped Rydberg ions present an attractive alternative for fast entangling operations as compared to those mediated by normal modes of trapped ions, which are advantageous for a future quantum computer or a quantum simulator.', 'corpus_id': 213175483, 'score': 1}, {'doc_id': '212747779', 'title': '2D Linear Trap Array for Quantum Information Processing', 'abstract': 'We present an ion-lattice quantum processor based on a two-dimensional arrangement of linear surface traps. Our design features a tunable coupling between ions in adjacent lattice sites and a configurable ion-lattice connectivity, allowing one, e.g., to realize rectangular and triangular lattices with the same trap chip. We present detailed trap simulations of a simplest-instance ion array with $2\\times9$ trapping sites and report on the fabrication of a prototype device in an industrial facility. The design and the employed fabrication processes are scalable to larger array sizes. We demonstrate trapping of ions in rectangular and triangular lattices and demonstrate transport of a $2\\times2$ ion-lattice over one lattice period.', 'corpus_id': 212747779, 'score': 1}, {'doc_id': '215745031', 'title': 'Identification of molecular quantum states using phase-sensitive forces', 'abstract': 'Quantum-logic techniques used to manipulate quantum systems are now increasingly being applied to molecules. Previous experiments on single trapped diatomic species have enabled state detection with excellent fidelities and highly precise spectroscopic measurements. However, for complex molecules with a dense energy-level structure improved methods are necessary. Here, we demonstrate an enhanced quantum protocol for molecular state detection using state-dependent forces. Our approach is based on interfering a reference and a signal force applied to a single atomic and molecular ion. By changing the relative phase of the forces, we identify states embedded in a dense molecular energy-level structure and monitor state-to-state inelastic scattering processes. This method can also be used to exclude a large number of states in a single measurement when the initial state preparation is imperfect and information on the molecular properties is incomplete. While the present experiments focus on N\\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$${}_{2}^{+}$$\\end{document}2+, the method is general and is expected to be of particular benefit for polyatomic systems.', 'corpus_id': 215745031, 'score': 0}, {'doc_id': '214775130', 'title': 'Three-Dimensional Cooling of an Atom-Beam Source for High-Contrast Atom Interferometry', 'abstract': 'We present a compact, two-stage atomic beam source that produces a continuous, narrow, collimated and high-flux beam of rubidium atoms with sub-Doppler temperatures in three dimensions, which features very low emission of near-resonance fluorescence along the atomic trajectory. The atom beam source originates in a pushed two-dimensional magneto-optical trap (2D$^+$ MOT) feeding a slightly off-axis three-dimensional moving optical molasses stage that continuously cools and redirects the atom beam. The capture velocity of the moving optical molasses is deliberately chosen to be low, $\\sim 3$ m/s, to reduce fluorescence, and the cooling light is detuned by several atomic linewidths from resonance to reduce the absorption cross-section of cooling-induced fluorescence. Near-resonance light from the 2D$^+$ MOT and the push beam does not propagate to the output atomic trajectory due to a 10 degree bend in the atomic trajectory. The atomic beam emitted from the two-stage source has a flux up to $1.6(3)\\times 10^9\\;\\textrm{atoms/s}$, with an optimized temperature of $15.0(2)\\;\\mu$K. We employ continuous Raman-Ramsey interference measurements at the atom beam output to study the sources of decoherence in the presence of continuous cooling, and demonstrate that the atom beam source effectively preserves high fringe contrast even during cooling. This cold-atom beam source is appropriate for use in atom interferometers and clocks, where continuous operation eliminates dead time, the slow atom beam velocity (6 - 16 m/s) improves sensitivity, the narrow 3D velocity distribution improves fringe contrast, and the low reabsorption of scattered light mitigates decoherence caused by the continuous cooling process.', 'corpus_id': 214775130, 'score': 0}]
112	{'doc_id': '236087353', 'title': 'Semi-supervised Cell Detection in Time-lapse Images Using Temporal Consistency', 'abstract': 'Cell detection is the task of detecting the approximate positions of cell centroids from microscopy images. Recently, convolutional neural network-based approaches have achieved promising performance. However, these methods require a certain amount of annotation for each imaging condition. This annotation is a time-consuming and labor-intensive task. To overcome this problem, we propose a semi-supervised cell-detection method that effectively uses a time-lapse sequence with one labeled image and the other images unlabeled. First, we train a cell-detection network with a one-labeled image and estimate the unlabeled images with the trained network. We then select high-confidence positions from the estimations by tracking the detected cells from the the labeled frame to those far from it. Next, we generate pseudo-labels from the tracking results and train the network by using pseudo-labels. We evaluated our method for seven conditions of public datasets, and we achieved the best results relative to other semi-supervised methods. Our code is available at https://github.com/naivete5656/SCDTC', 'corpus_id': 236087353}	11033	[{'doc_id': '235636153', 'title': 'Evaluating Very Deep Convolutional Neural Networks for Nucleus Segmentation from Brightfield Cell Microscopy Images', 'abstract': 'Advances in microscopy have increased output data volumes, and powerful image analysis methods are required to match. In particular, finding and characterizing nuclei from microscopy images, a core cytometry task, remains difficult to automate. While deep learning models have given encouraging results on this problem, the most powerful approaches have not yet been tested for attacking it. Here, we review and evaluate state-of-the-art very deep convolutional neural network architectures and training strategies for segmenting nuclei from brightfield cell images. We tested U-Net as a baseline model; considered U-Net++, Tiramisu, and DeepLabv3+ as latest instances of advanced families of segmentation models; and propose PPU-Net, a novel light-weight alternative. The deeper architectures outperformed standard U-Net and results from previous studies on the challenging brightfield images, with balanced pixel-wise accuracies of up to 86%. PPU-Net achieved this performance with 20-fold fewer parameters than the comparably accurate methods. All models perform better on larger nuclei and in sparser images. We further confirmed that in the absence of plentiful training data, augmentation and pretraining on other data improve performance. In particular, using only 16 images with data augmentation is enough to achieve a pixel-wise F1 score that is within 5% of the one achieved with a full data set for all models. The remaining segmentation errors are mainly due to missed nuclei in dense regions, overlapping cells, and imaging artifacts, indicating the major outstanding challenges.', 'corpus_id': 235636153, 'score': 1}, {'doc_id': '233988673', 'title': 'Deep learning-enhanced light-field imaging with continuous validation', 'abstract': 'Visualizing dynamic processes over large, three-dimensional fields of view at high speed is essential for many applications in the life sciences. Light-field microscopy (LFM) has emerged as a tool for fast volumetric image acquisition, but its effective throughput and widespread use in biology has been hampered by a computationally demanding and artifact-prone image reconstruction process. Here, we present a framework for artificial intelligence–enhanced microscopy, integrating a hybrid light-field light-sheet microscope and deep learning–based volume reconstruction. In our approach, concomitantly acquired, high-resolution two-dimensional light-sheet images continuously serve as training data and validation for the convolutional neural network reconstructing the raw LFM data during extended volumetric time-lapse imaging experiments. Our network delivers high-quality three-dimensional reconstructions at video-rate throughput, which can be further refined based on the high-resolution light-sheet images. We demonstrate the capabilities of our approach by imaging medaka heart dynamics and zebrafish neural activity with volumetric imaging rates up to 100\u2009Hz. A deep learning–based algorithm enables efficient reconstruction of light-field microscopy data at video rate. In addition, concurrently acquired light-sheet microscopy data provide ground truth data for training, validation and refinement of the algorithm.', 'corpus_id': 233988673, 'score': 0}, {'doc_id': '235336198', 'title': 'Revealing the spatio-phenotypic patterning of cells in healthy and tumor tissues with mLSR-3D and STAPL-3D', 'abstract': 'Despite advances in three-dimensional (3D) imaging, it remains challenging to profile all the cells within a large 3D tissue, including the morphology and organization of the many cell types present. Here, we introduce eight-color, multispectral, large-scale single-cell resolution 3D (mLSR-3D) imaging and image analysis software for the parallelized, deep learning-based segmentation of large numbers of single cells in tissues, called segmentation analysis by parallelization of 3D datasets (STAPL-3D). Applying the method to pediatric Wilms tumor, we extract molecular, spatial and morphological features of millions of cells and reconstruct the tumor’s spatio-phenotypic patterning. In situ population profiling and pseudotime ordering reveals a highly disorganized spatial pattern in Wilms tumor compared to healthy fetal kidney, yet cellular profiles closely resembling human fetal kidney cells could be observed. In addition, we identify previously unreported tumor-specific populations, uniquely characterized by their spatial embedding or morphological attributes. Our results demonstrate the use of combining mLSR-3D and STAPL-3D to generate a comprehensive cellular map of human tumors.', 'corpus_id': 235336198, 'score': 0}, {'doc_id': '236991924', 'title': 'DeLTA 2.0: A deep learning pipeline for quantifying single-cell spatial and temporal dynamics', 'abstract': 'Improvements in microscopy software and hardware have dramatically increased the pace of image acquisition, making analysis a major bottleneck in generating quantitative, single-cell data. Although tools for segmenting and tracking bacteria within time-lapse images exist, most require human input, are specialized to the experimental set up, or lack accuracy. Here, we introduce DeLTA 2.0, a purely Python workflow that can rapidly and accurately analyze single cells on two-dimensional surfaces to quantify gene expression and cell growth. The algorithm uses deep convolutional neural networks to extract single-cell information from time-lapse images, requiring no human input after training. DeLTA 2.0 retains all the functionality of the original version, which was optimized for bacteria growing in the mother machine microfluidic device, but extends results to two-dimensional growth environments. Two-dimensional environments represent an important class of data because they are more straightforward to implement experimentally, they offer the potential for studies using co-cultures of cells, and they can be used to quantify spatial effects and multi-generational phenomena. However, segmentation and tracking are significantly more challenging tasks in two-dimensions due to exponential increases in the number of cells that must be tracked. To showcase this new functionality, we analyze mixed populations of antibiotic resistant and susceptible cells, and also track pole age and growth rate across generations. In addition to the two-dimensional capabilities, we also introduce several major improvements to the code that increase accessibility, including the ability to accept many standard microscopy file formats and arbitrary image sizes as inputs. DeLTA 2.0 is rapid, with run times of less than 10 minutes for complete movies with hundreds of cells, and is highly accurate, with error rates around 1%, making it a powerful tool for analyzing time-lapse microscopy data. Author Summary Time-lapse microscopy can generate large image datasets which track single-cell properties like gene expression or growth rate over time. Deep learning tools are very useful for analyzing these data and can identify the location of cells and track their position over time. In this work, we introduce a new version of our Deep Learning for Time-lapse Analysis (DeLTA) software, which includes the ability to robustly segment and track bacteria that are growing in two dimensions, such as on agarose pads or within microfluidic environments. This capability is essential for experiments where spatial and positional effects are important, such as conditions with microbial co-cultures, cell-to-cell interactions, or spatial patterning. The software also tracks pole age and can be used to analyze replicative aging. These new features join other improvements, such as the ability to work directly with many common microscope file formats. DeLTA 2.0 can reliably track hundreds of cells with low error rates, making it an ideal tool for high throughput analysis of microscopy data.', 'corpus_id': 236991924, 'score': 1}, {'doc_id': '235690115', 'title': 'On Improving an Already Competitive Segmentation Algorithm for the Cell Tracking Challenge - Lessons Learned', 'abstract': 'The virtually error-free segmentation and tracking of densely packed cells and cell nuclei is still a challenging task. Especially in low-resolution and low signal-to-noise-ratio microscopy images erroneously merged and missing cells are common segmentation errors making the subsequent cell tracking even more difficult. In 2020, we successfully participated as team KIT-Sch-GE (1) in the 5th edition of the ISBI Cell Tracking Challenge. With our deep learning-based distance map regression segmentation and our graph-based cell tracking, we achieved multiple top 3 rankings on the diverse data sets. In this manuscript, we show how our approach can be further improved by using another optimizer and by fine-tuning training data augmentation parameters, learning rate schedules, and the training data representation. The fine-tuned segmentation in combination with an improved tracking enabled to further improve our performance in the 6th edition of the Cell Tracking Challenge 2021 as team KIT-Sch-GE (2).', 'corpus_id': 235690115, 'score': 1}, {'doc_id': '237378594', 'title': 'Spatio-temporal feature based deep neural network for cell lineage analysis in microscopy images', 'abstract': 'Background Time-lapse microscopy has been widely used in biomedical experiments because it can visualize the molecular activities of living cells in real time. However, biomedical researchers are still conducting cell lineage analysis manually. Developing automatic lineage tracing algorithms is a challenging task. In the past two decades, deep neural networks (DNNs) became have shown outstanding performance on computer vision tasks. They can learn complex visual features, capture long-range temporal dependencies, and have the potential to be used for automatic cell lineage analysis. Methods In this study, we propose a multi-task spatio-temporal feature based deep neural network for cell lineages analysis (Cell-STN). The Cell-STN extracts spatio-temporal features from microscopy image sequences by leveraging our convolutional long short-term memory based core block. And the proposed Cell-STN utilized a task specific network to predict the cell location, the mitosis event, and the apoptosis event in a multi-task manner. Results We evaluated the Cell-STN on three in-house datasets (MCF7, U2OS, and HCT116) and one public dataset (Fluo-N2DL-HeLa). For cell tracking, we used peak-wise precision, track-wise precision, end-peak precision, and spatial distance as metrics. The overall results showed the Cell-STN models outperform other state-of-the-art cell trackers. For mitosis and apoptosis tasks, we used accuracy, F1-score, temporal distance, and spatial distance as metrics. The Cell-STN models achieved the highest performance on all datasets. Conclusion This study presented a novel DNNs approach for cell lineage analysis in microscopy images. The Cell-STN showed outstanding performance on the four datasets. Additionally, the Cell-STN required minimal training data and can be adapted to new biological event detection tasks by appending task-specific layers. This algorithm has the potential to be used in real-world biomedical research.', 'corpus_id': 237378594, 'score': 1}, {'doc_id': '235305739', 'title': 'Cell Tracking for Organoids: Lessons From Developmental Biology', 'abstract': 'Organoids have emerged as powerful model systems to study organ development and regeneration at the cellular level. Recently developed microscopy techniques that track individual cells through space and time hold great promise to elucidate the organizational principles of organs and organoids. Applied extensively in the past decade to embryo development and 2D cell cultures, cell tracking can reveal the cellular lineage trees, proliferation rates, and their spatial distributions, while fluorescent markers indicate differentiation events and other cellular processes. Here, we review a number of recent studies that exemplify the power of this approach, and illustrate its potential to organoid research. We will discuss promising future routes, and the key technical challenges that need to be overcome to apply cell tracking techniques to organoid biology.', 'corpus_id': 235305739, 'score': 0}, {'doc_id': '235382366', 'title': 'Dice-XMBD: Deep learning-based cell segmentation for imaging mass cytometry', 'abstract': 'Highly multiplexed imaging technology is a powerful tool to facilitate understanding cells composition and interaction in tumor microenvironment at subcellular resolution, which is crucial for both basic research and clinical applications. Imaging mass cytometry (IMC), a multiplex imaging method recently introduced, can measure up to 40 markers simultaneously in one tissue section by using a high-resolution laser with a mass cytometer. However, due to its high resolution and large number of channels, how to process and interpret the image data from IMC remains a key challenge for its further applications. Accurate and reliable single cell segmentation is the first and a critical step to process IMC image data. Unfortunately, existing segmentation pipelines either produce inaccurate cell segmentation results, or require manual annotation which is very time-consuming. Here, we developed Dice-XMBD, a Deep learnIng-based Cell sEgmentation algorithm for tissue multiplexed imaging data. In comparison with other state-of-the-art cell segmentation methods currently used in IMC, Dice-XMBD generates more accurate single cell masks efficiently on IMC images produced with different nuclear, membrane and cytoplasm markers. All codes and datasets are available at https://github.com/xmuyulab/Dice-XMBD.', 'corpus_id': 235382366, 'score': 0}, {'doc_id': '235313943', 'title': 'Fast improvement of TEM image with low-dose electrons by deep learning', 'abstract': 'Low-electron-dose observation is indispensable for observing various samples using a transmission electron microscope; consequently, image processing has been used to improve transmission electron microscopy (TEM) images. To apply such image processing to in situ observations, we here apply a convolutional neural network to TEM imaging. Using a dataset that includes short-exposure images and long-exposure images, we develop a pipeline for processed short-exposure images, based on end-to-end training. The quality of images acquired with a total dose of approximately 5 e− per pixel becomes comparable to that of images acquired with a total dose of approximately 1000 e− per pixel. Because the conversion time is approximately 8 ms, in situ observation at 125 fps is possible. This imaging technique enables in situ observation of electron-beam-sensitive specimens.', 'corpus_id': 235313943, 'score': 0}, {'doc_id': '235774970', 'title': 'GeNePy3D: a quantitative geometry python toolbox for bioimaging', 'abstract': 'The advent of large-scale fluorescence and electronic microscopy techniques along with maturing image analysis is giving life sciences a deluge of geometrical objects in 2D/3D(+t) to deal with. These objects take the form of large scale, localised, precise, single cell, quantitative data such as cells’ positions, shapes, trajectories or lineages, axon traces in whole brains atlases or varied intracellular protein localisations, often in multiple experimental conditions. The data mining of those geometrical objects requires a variety of mathematical and computational tools of diverse accessibility and complexity. Here we present a new Python library for quantitative 3D geometry called GeNePy3D which helps handle and mine information and knowledge from geometric data, providing a unified application programming interface (API) to methods from several domains including computational geometry, scale space methods or spatial statistics. By framing this library as generically as possible, and by linking it to as many state-of-the-art reference algorithms and projects as needed, we help render those often specialist methods accessible to a larger community. We exemplify the usefulness of the GeNePy3D toolbox by re-analysing a recently published whole-brain zebrafish neuronal atlas, with other applications and examples available online. Along with an open source, documented and exemplified code, we release reusable containers to allow for convenient and wide usability and increased reproducibility.', 'corpus_id': 235774970, 'score': 1}]
113	{'doc_id': '235694619', 'title': 'SinGAN-Seg: Synthetic Training Data Generation for Medical Image Segmentation', 'abstract': 'Processing medical data to find abnormalities is a time-consuming and costly task, requiring tremendous efforts from medical experts. Therefore, artificial intelligence (AI) has become a popular tool for the automatic processing of medical data, acting as a supportive tool for doctors. AI tools highly depend on data for training the models. However, there are several constraints to access to large amounts of medical data to train machine learning algorithms in the medical domain, e.g., due to privacy concerns and the costly, time-consuming medical data annotation process. To address this, in this paper we present a novel synthetic data generation pipeline called SinGAN-Seg to produce synthetic medical data with the corresponding annotated ground truth masks. We show that these synthetic data generation pipelines can be used as an alternative to bypass privacy concerns and as an alternative way to produce artificial segmentation datasets with corresponding ground truth masks to avoid the tedious medical data annotation process. As a proof of concept, we used an open polyp segmentation dataset. By training UNet++ using both real polyp segmentation dataset and the corresponding synthetic dataset generated from the SinGAN-Seg pipeline, we show that the synthetic data can achieve a very close performance to the real data when the real segmentation datasets are large enough. In addition, we show that synthetic data generated from the SinGAN-Seg pipeline improving the performance of segmentation algorithms when the training dataset is very small. Since our SinGAN-Seg pipeline is applicable for any medical dataset, this pipeline can be used with any other segmentation datasets.', 'corpus_id': 235694619}	18638	"[{'doc_id': '218487714', 'title': 'The Newspaper Navigator Dataset: Extracting And Analyzing Visual Content from 16 Million Historic Newspaper Pages in Chronicling America', 'abstract': ""Chronicling America is a product of the National Digital Newspaper Program, a partnership between the Library of Congress and the National Endowment for the Humanities to digitize historic newspapers. Over 16 million pages of historic American newspapers have been digitized for Chronicling America to date, complete with high-resolution images and machine-readable METS/ALTO OCR. Of considerable interest to Chronicling America users is a semantified corpus, complete with extracted visual content and headlines. To accomplish this, we introduce a visual content recognition model trained on bounding box annotations of photographs, illustrations, maps, comics, and editorial cartoons collected as part of the Library of Congress's Beyond Words crowdsourcing initiative and augmented with additional annotations including those of headlines and advertisements. We describe our pipeline that utilizes this deep learning model to extract 7 classes of visual content: headlines, photographs, illustrations, maps, comics, editorial cartoons, and advertisements, complete with textual content such as captions derived from the METS/ALTO OCR, as well as image embeddings for fast image similarity querying. We report the results of running the pipeline on 16.3 million pages from the Chronicling America corpus and describe the resulting Newspaper Navigator dataset, the largest dataset of extracted visual content from historic newspapers ever produced. The Newspaper Navigator dataset, finetuned visual content recognition model, and all source code are placed in the public domain for unrestricted re-use."", 'corpus_id': 218487714, 'score': 1}, {'doc_id': '235657622', 'title': 'Transformation Invariant Few-Shot Object Detection', 'abstract': 'Few-shot object detection (FSOD) aims to learn detectors that can be generalized to novel classes with only a few instances. Unlike previous attempts that exploit metalearning techniques to facilitate FSOD, this work tackles the problem from the perspective of sample expansion. To this end, we propose a simple yet effective Transformation Invariant Principle (TIP) that can be flexibly applied to various meta-learning models for boosting the detection performance on novel class objects. Specifically, by introducing consistency regularization on predictions from various transformed images, we augment vanilla FSOD models with the generalization ability to objects perturbed by various transformation, such as occlusion and noise. Importantly, our approach can extend supervised FSOD models to naturally cope with unlabeled data, thus addressing a more practical and challenging semi-supervised FSOD problem. Extensive experiments on PASCAL VOC and MSCOCO datasets demonstrate the effectiveness of our TIP under both of the two FSOD settings.', 'corpus_id': 235657622, 'score': 0}, {'doc_id': '235606234', 'title': 'Open Images V5 Text Annotation and Yet Another Mask Text Spotter', 'abstract': 'A large scale human-labeled dataset plays an important role in creating high quality deep learning models. In this paper we present text annotation for Open Images V5 dataset. To our knowledge it is the largest among publicly available manually created text annotations. Having this annotation we trained a simple Mask-RCNN-based network, referred as Yet Another Mask Text Spotter (YAMTS), which achieves competitive performance or even outperforms current state-of-the-art approaches in some cases on ICDAR 2013, ICDAR 2015 and Total-Text datasets. Code for text spotting model available online at: https://github.com/openvinotoolkit/training_extensions. The model can be exported to OpenVINOTM-format and run on Intel® CPUs.', 'corpus_id': 235606234, 'score': 1}, {'doc_id': '235485156', 'title': 'How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers', 'abstract': 'Vision Transformers (ViT) have been shown to attain highly competitive performance for a wide range of vision applications, such as image classification, object detection and semantic image segmentation. In comparison to convolutional neural networks, the Vision Transformer’s weaker inductive bias is generally found to cause an increased reliance on model regularization or data augmentation (“AugReg” for short) when training on smaller training datasets. We conduct a systematic empirical study in order to better understand the interplay between the amount of training data, AugReg, model size and compute budget. 1 As one result of this study we find that the combination of increased compute and AugReg can yield models with the same performance as models trained on an order of magnitude more training data: we train ViT models of various sizes on the public ImageNet-21k dataset which either match or outperform their counterparts trained on the larger, but not publicly available JFT-300M dataset.', 'corpus_id': 235485156, 'score': 0}, {'doc_id': '235447001', 'title': '2nd Place Solution for Waymo Open Dataset Challenge - Real-time 2D Object Detection', 'abstract': 'In an autonomous driving system, it is essential to recognize vehicles, pedestrians and cyclists from images. Besides the high accuracy of the prediction, the requirement of real-time running brings new challenges for convolutional network models. In this report, we introduce a real-time method to detect the 2D objects from images. We aggregate several popular one-stage object detectors and train the models of variety input strategies independently, to yield better performance for accurate multi-scale detection of each category, especially for small objects. For model acceleration, we leverage TensorRT to optimize the inference time of our detection pipeline. As shown in the leaderboard, our proposed detection framework ranks the 2nd place with 75.00% L1 mAP and 69.72% L2 mAP in the real-time 2D detection track of the Waymo Open Dataset Challenges, while our framework achieves the latency of 45.8ms/frame on an Nvidia Tesla V100 GPU.', 'corpus_id': 235447001, 'score': 0}, {'doc_id': '235670060', 'title': 'Efficient Realistic Data Generation Framework leveraging Deep Learning-based Human Digitization', 'abstract': 'The performance of supervised deep learning algorithms depends significantly on the scale, quality and diversity of the data used for their training. Collecting and manually annotating large amount of data can be both time-consuming and costly tasks to perform. In the case of tasks related to visual human-centric perception, the collection and distribution of such data may also face restrictions due to legislation regarding privacy. In addition, the design and testing of complex systems, e.g., robots, which often employ deep learning-based perception models, may face severe difficulties as even state-of-the-art methods trained on real and large-scale datasets cannot always perform adequately due to not having been adapted to the visual differences between the virtual and the real world data. As an attempt to tackle and mitigate the effect of these issues, we present a method that automatically generates realistic synthetic data with annotations for a) person detection, b) face recognition, and c) human pose estimation. The proposed method takes as input real background images and populates them with human figures in various poses. Instead of using hand-made 3D human models, we propose the use of models generated through deep learning methods, further reducing the dataset creation costs, while maintaining a high level of realism. In addition, we provide open-source and easy to use tools that implement the proposed pipeline, allowing for generating highly-realistic synthetic datasets for a variety of tasks. A benchmarking and evaluation in the corresponding tasks shows that synthetic data can be effectively used as a supplement to real data.', 'corpus_id': 235670060, 'score': 1}, {'doc_id': '235446969', 'title': 'The Oxford Road Boundaries Dataset', 'abstract': 'In this paper we present The Oxford Road Boundaries Dataset, designed for training and testing machine-learningbased road-boundary detection and inference approaches. We have hand-annotated two of the 10 km-long forays from the Oxford Robotcar Dataset and generated from other forays several thousand further examples with semi-annotated road-boundary masks. To boost the number of training samples in this way, we used a vision-based localiser to project labels from the annotated datasets to other traversals at different times and weather conditions. As a result, we release 62 605 labelled samples, of which 47 639 samples are curated. Each of these samples contain both raw and classified masks for left and right lenses. Our data contains images from a diverse set of scenarios such as straight roads, parked cars, junctions, etc. Files for download and tools for manipulating the labelled data are available at: oxford-robotics-institute.github. io/road-boundaries-dataset Keywords— road boundary, curb, kerb, dataset', 'corpus_id': 235446969, 'score': 1}, {'doc_id': '235790490', 'title': 'Learning Cascaded Detection Tasks with Weakly-Supervised Domain Adaptation', 'abstract': 'In order to handle the challenges of autonomous driving, deep learning has proven to be crucial in tackling increasingly complex tasks, such as 3D detection or instance segmentation. State-of-the-art approaches for image-based detection tasks tackle this complexity by operating in a cascaded fashion: they first extract a 2D bounding box based on which additional attributes, e.g. instance masks, are inferred. While these methods perform well, a key challenge remains the lack of accurate and cheap annotations for the growing variety of tasks. Synthetic data presents a promising solution but, despite the effort in domain adaptation research, the gap between synthetic and real data remains an open problem. In this work, we propose a weakly supervised domain adaptation setting which exploits the structure of cascaded detection tasks. In particular, we learn to infer the attributes solely from the source domain while leveraging 2D bounding boxes as weak labels in both domains to explain the domain shift. We further encourage domain-invariant features through class-wise feature alignment using ground-truth class information, which is not available in the unsupervised setting. As our experiments demonstrate, the approach is competitive with fully supervised settings while outperforming unsupervised adaptation approaches by a large margin.', 'corpus_id': 235790490, 'score': 0}, {'doc_id': '235743138', 'title': '""Garbage In, Garbage Out"" Revisited: What Do Machine Learning Application Papers Report About Human-Labeled Training Data?', 'abstract': '\n Supervised machine learning, in which models are automatically derived from labeled training data, is only as good as the quality of that data. This study builds on prior work that investigated to what extent “best practices” around labeling training data were followed in applied ML publications within a single domain (social media platforms). In this paper, we expand by studying publications that apply supervised ML in a far broader spectrum of disciplines, focusing on human-labeled data. We report to what extent a random sample of ML application papers across disciplines give specific details about whether best practices were followed, while acknowledging that a greater range of application fields necessarily produces greater diversity of labeling and annotation methods. Because much of machine learning research and education only focuses on what is done once a “ground truth” or “gold standard” of training data is available, it is especially relevant to discuss issues around the equally important aspect of whether such data is reliable in the first place. This determination becomes increasingly complex when applied to a variety of specialized fields, as labeling can range from a task requiring little-to-no background knowledge to one that must be performed by someone with career expertise.', 'corpus_id': 235743138, 'score': 1}]"
114	{'doc_id': '201221359', 'title': 'AlradSpectra: a Quantification Tool for Soil Properties Using Spectroscopic Data in R', 'abstract': 'ABSTRACT Soil reflectance spectroscopy has become an innovative method for soil property quantification supplying data for studies in soil fertility, soil classification, digital soil mapping, while reducing laboratory time and applying a clean technology. This paper describes the implementation of a Graphical User Interface (GUI) using R named AlradSpectra. It contains several tools to process spectroscopic data and generate models to predict soil properties. The GUI was developed to accomplish tasks such as perform a large range of spectral preprocessing [...]', 'corpus_id': 201221359}	17836	[{'doc_id': '235564458', 'title': 'Milk infrared spectra from multiple instruments improve performance of prediction models', 'abstract': 'Abstract Milk data predicted from infrared spectra are the basis of selection programmes for dairy species, but several factors influence the robustness of prediction models, e.g., (i) variability of collected samples, (ii) number and quality of reference data, and (iii) influence traits on milk spectral fingerprint. Use of single or multiple instruments spectra from the same set of samples to develop mid infrared calibration models was investigated. Fitting statistics for milk coagulation properties were low to fair. Better results were achieved for titratable acidity and cheese yield; coefficients of determination in cross validation ranged from 0.52 to 0.77, depending on the trait analysed and training set. Merging spectral data from two instruments did not improve calibration performance but provided the best fitting statistics in external validation for most of the traits on different validation sets. Merging spectral data from different instruments is a cost-effective method to improve calibration performance and robustness.', 'corpus_id': 235564458, 'score': 1}, {'doc_id': '235241694', 'title': 'Unveiling spatial variability in herbicide soil sorption using bayesian digital mapping.', 'abstract': 'Regional mapping herbicide sorption to soil is essential for risk assessment. However, conducting analytical quantification of adsorption coefficient (Kd) in large scale studies is too costly; therefore, a research question arises on goodness of Kd spatial prediction from sampling. The application of a spatial Bayesian regression (BR) is a newer technique in agricultural and natural resources sciences that allows converting spatially discrete samples into maps covering continuous spatial domains. The objective of this work was to unveil herbicide sorption to soil at a landscape scale by developing a predictive BR model. We integrated a large set of ancillary soil and climate covariables from sites with Kd measurements into a spatial mixed model including site random effects. The models were fitted using glyphosate and atrazine Kds, determined in 80 and 120 sites from central Argentina. For model assessment, measurements of global and point-wise prediction errors were obtained by cross-validation; residual variability was estimated by bootstrap to compare BR with regression kriging (RK). Results showed that the BR spatial predictions outperformed RK. The glyphosate Kd model (RMSPE: 13% of the mean) included aluminum oxides, pH, and clay content, whereas the atrazine Kd model strongly depended on soil organic carbon and clay, as well as on climatic variables related to water availability (RMSPE: 27%). Spatial modelling of a complex edaphic process as herbicide sorption to soils enhanced environmental interpretations. An efficient approach for spatial mapping provides a modern perspective on the study of herbicide sorption to soil. This article is protected by copyright. All rights reserved.', 'corpus_id': 235241694, 'score': 0}, {'doc_id': '233567950', 'title': 'Determination of main raw material source in bar soaps using mid-infrared spectroscopy combined with classification tools', 'abstract': 'Abstract The personal care industry is one of the fastest-growing markets in the world in terms of revenue and sales volume. In addition, this market has been going through some modifications to serve its consumers, a fact that has generated the development of a new line of products. Nowadays several products, including bar soaps, are commercialized as free of animal and synthetic materials. In this context, the development of methodologies to verify the source of the raw material (animal, synthetic, or vegetable) used in the production of soaps can be a valuable tool to supplement the quality control routines. Infrared spectroscopy combined with pattern recognition methods, i.e. Principal Component Analysis (PCA), Partial Least Squares Discriminatory Analysis (PLS-DA) and Data Driven Soft Independent Modeling of Class Analogy (DD-SIMCA), were applied to verify the main source of raw materials used in the soaps production. Analyzing the scores plot for the first two Principal Components of PCA it was possible to observe distinct clusters of soap samples grouped based on the main sources of raw material. Both supervised methods, PLS-DA and DD-SIMCA, were able to correctly classify all samples into their corresponding classes. The most influential variables for the discrimination of groups by PLS-DA were the spectral bands related to changes in the carbon chains and in the amount of carboxyl or glycerol groups in the chemical structure. While DD-SIMCA demonstrated similarity between soaps containing mainly animal and vegetal raw material. This study indicates the potential of chemometric tools in the development of robust predictive models that may be integrated to identify the raw materials used in bar soaps.', 'corpus_id': 233567950, 'score': 0}, {'doc_id': '233534142', 'title': 'Assessing black tea quality based on visible–near infrared spectra and kernel-based methods', 'abstract': 'Abstract Current evaluation systems for tea quality assessment are difficult to use and unstable. A rapid and cheap assessment method is required to distinguish the quality levels of tea. In this study, a visible-near-infrared (Vis–NIR) spectrometer and support vector machine (SVM)-based kernels were used for the qualitative categorization of black tea. First, a Vis–NIR system was used to acquire the spectral data of seven levels of tea samples. The obtained spectra were preprocessed using the Savitzky–Golay smoothing combined with the first derivative and standard normal variate transformation. Then, four characteristic wavelength selection algorithms, namely synergy interval partial least square, competitive adaptive reweighted sampling (CARS), the variable iterative space shrinkage approach (VISSA), and the interval VISSA, were used to obtain preprocessed spectral features. Finally, the SVM was used with four kernel functions, namely the linear, Gaussian, quadratic, and cubic functions, to develop models based on the variables obtained from the selected features for tea quality classification. The results revealed that the CARS–linear kernel SVM model exhibited the best results, with a correct identification rate of 91.85 % in the validation process. Our findings demonstrate that Vis-NIR spectroscopy can be a rapid, inexpensive, efficient, and alternative method for predicting the quality of black tea.', 'corpus_id': 233534142, 'score': 1}, {'doc_id': '235450820', 'title': 'A best-practice guide to predicting plant traits from leaf-level hyperspectral data using partial least squares regression.', 'abstract': 'Partial least squares regression (PLSR) modelling is a statistical technique for correlating datasets, and involves the fitting of a linear regression between two matrices. One application of PLSR enables leaf traits to be estimated from hyperspectral optical reflectance data, facilitating rapid, high-throughput, non-destructive plant phenotyping. This technique is of interest and importance in a wide range of contexts including crop breeding and ecosystem monitoring. The lack of a consensus in the literature on how to perform PLSR means that interpreting model results can be challenging, applying existing models to novel datasets can be impossible, and unknown or undisclosed assumptions can lead to incorrect or spurious predictions. We address this lack of consensus by proposing best practices for using PLSR to predict plant traits from leaf-level hyperspectral data, including a discussion of when PLSR is applicable, and recommendations for data collection. We provide a tutorial to demonstrate how to develop a PLSR model, in the form of an R script accompanying this manuscript. This practical guide will assist all those interpreting and using PLSR models to predict leaf traits from spectral data, and advocates for a unified approach to using PLSR for predicting traits from spectra in the plant sciences.', 'corpus_id': 235450820, 'score': 0}, {'doc_id': '235424485', 'title': 'Identification of zinc pollution in rice plants based on two characteristic variables.', 'abstract': 'Traditional chemical methods used to measure the zinc content in rice plants are time-consuming, laborious, requires reagents, and have a limited monitoring range, while the Raman spectroscopy method has the advantage of being fast, non-destructive, and requires no reagents. Unfortunately, the identification accuracy of the Raman partial least squares (PLS) model based on principal components is only 53.33%. To boost this, a One-Way ANOVA method was used to extract the characteristic variables in the Raman spectra. Based on these Raman variables, a model for identifying zinc stressed samples was established. The identification accuracy was improved to 70% but still fell short of the measurement requirements. To further enhance these results, the Raman spectrum was decomposed into components based on the Hilbert Vibration Decomposition (HVD) method. Using characteristic variables of the Raman spectrum and its HVD components to establish a PLS model, the identification accuracy of the test set is raised to 90.25%. These results are a significant improvement from those obtained using a model solely based on the Raman spectral characteristic variables, revealing that HVD components provide highly effective identification information. A Raman modeling method based on the characteristic variables of the HVD component is an innovative way for improving the accuracy of Raman detection, especially for the measurement of trace substances.', 'corpus_id': 235424485, 'score': 0}, {'doc_id': '233411700', 'title': 'Reliable Model Selection without Reference Values by Utilizing Model Diversity with Prediction Similarity', 'abstract': 'Predictive modeling (calibration or training) with various data formats, such as near-infrared (NIR) spectra and quantitative structure-activity relationship (QSAR) data, provides essential information if a proper model is selected. Similarly, with a general model selection approach, spectral model maintenance (updating) from original modeling conditions to new conditions can be performed for dynamic modeling. Fundamental modeling (partial least-squares (PLS) and others) and maintenance processes (domain adaptation or transfer learning and others) require selection of tuning parameter(s) values to isolate models that can accurately predict new samples or molecules, e.g., number of PLS latent variables to predict analyte concentration. Regardless of the modeling task, model selection is complex and without a reliable protocol. Tuning parameter selection typically depends on only one model quality measure assessing model bias using prediction accuracy. Developed in this paper is a generic model selection process using concepts from consensus modeling and QSAR activity landscapes. It is a consensus filtering approach that prioritizes model diversity (MD) while conserving prediction similarity (PS) fused with a common bias-variance trade-off measure. A significant feature of MDPS is that a cross-validation scheme is not needed because models are selected relative to predicting new samples or molecules, i.e., model selection uses unlabeled samples (without reference values) for active predictions. The versatility and reliability of MDPS model selection is shown using four NIR data sets and a QSAR data set. The study also substantiates the Rashomon effect where there is not one best model tuning parameter value that provides accurate predictions.', 'corpus_id': 233411700, 'score': 0}, {'doc_id': '235531063', 'title': 'Soil organic carbon estimation using VNIR–SWIR spectroscopy: The effect of multiple sensors and scanning conditions', 'abstract': 'Abstract Visible–near infrared–shortwave infrared (VNIR–SWIR) spectroscopy is being increasingly used for soil organic carbon (SOC) assessment. Common practice consists of scanning soil samples using a single spectrometer. Considerations have rarely been documented of the effects of using multiple instruments and scanning conditions on SOC model calibration that occur when merging soil spectral libraries (SSLs), particularly in soils with low SOC concentration and using both field spectroradiometers and laboratory fixed spectrometers. To address this gap, we scanned 143 low-SOC-content soil samples using three spectrometers (ASD FieldSpec 3, ASD FieldSpec 4, and FOSS XDS) and four setup features - FOSS, contact probe (CP), dark-box (DB), and open laboratory (LAB) - at three laboratories. The application of an internal soil standard (ISS) to align one laboratory spectrum with another for spectral correction and spectral merging of various SSLs was examined. SOC models were developed using i) data from each single spectrometer – single laboratory separately and ii) merged data from multiple spectrometers – different laboratories, applying the 1st derivatives of spectra and random forest (RF) regression. The results indicate that the spectral shape and wavelength position of key features obtained from all spectrometers and setups did not show any noticeable differences, though spectra based on FOSS setup, particularly on low-SOC samples, demonstrated greater range in absolute derivative values regardless of ISS application. The derivative ISS-corrected spectra showed less variation among different spectrometers compared to their uncorrected raw reflectance spectra. All single spectrometer models predicted SOC reasonably well. However, the spectra acquired by the FOSS setup predicted SOC more accurately (R2 = 0.77, RPIQ = 3.30, RMSE = 0.22 %, and SD = 0.04) than the spectra acquired by the other setups. The models derived from merged uncorrected raw reflectance spectra yielded poor results (R2 = 0.48, RPIQ = 2.33, RMSE = 0.33 %, and SD = 0.10); nevertheless, assessment of SOC using the 1st derivative ISS-corrected merged SSLs considerably improved the prediction accuracy (R2 = 0.70, RPIQ = 3.10, RMSE = 0.25 %, and SD = 0.06). Hence, the derivative spectra coupled with the ISS correction improved the accuracy of SOC prediction models obtained from the merged soil spectra collected in different environments using different instruments. We therefore recommend application of the ISS spectral alignment method linked to the 1st derivative approach to enhance the compilation of SSLs at the regional and global scales for SOC assessment.', 'corpus_id': 235531063, 'score': 1}, {'doc_id': '235436904', 'title': 'Application of Low-Cost MEMS Spectrometers for Forest Topsoil Properties Prediction', 'abstract': 'Increasing temperatures and drought occurrences recently led to soil moisture depletion and increasing tree mortality. In the interest of sustainable forest management, the monitoring of forest soil properties will be of increasing importance in the future. Vis-NIR spectroscopy can be used as fast, non-destructive and cost-efficient method for soil parameter estimations. Microelectromechanical system devices (MEMS) have become available that are suitable for many application fields due to their low cost as well as their small size and weight. We investigated the performance of MEMS spectrometers in the visual and NIR range to estimate forest soil samples total C and N content of Ah and Oh horizons at the lab. The results were compared to a full-range device using PLSR and Cubist regression models at local (2.3 ha, n: Ah = 60, Oh = 50) and regional scale (State of Saxony, Germany, 184,000 km2, n: Ah = 186 and Oh = 176). For each sample, spectral reflectance was collected using MEMS spectrometer in the visual (Hamamatsu C12880MA) and NIR (NeoSpetrac SWS62231) range and using a conventional full range device (Veris Spectrophotometer). Both data sets were split into a calibration (70%) and a validation set (30%) to evaluate prediction power. Models were calibrated for Oh and Ah horizon separately for both data sets. Using the regional data, we also used a combination of both horizons. Our results show that MEMS devices are suitable for C and N prediction of forest topsoil on regional scale. On local scale, only models for the Ah horizon yielded sufficient results. We found moderate and good model results using MEMS devices for Ah horizons at local scale (R2≥ 0.71, RPIQ ≥ 2.41) using Cubist regression. At regional scale, we achieved moderate results for C and N content using data from MEMS devices in Oh (R2≥ 0.57, RPIQ ≥ 2.42) and Ah horizon (R2≥ 0.54, RPIQ ≥2.15). When combining Oh and Ah horizons, we achieved good prediction results using the MEMS sensors and Cubist (R2≥ 0.85, RPIQ ≥ 4.69). For the regional data, models using data derived by the Hamamatsu device in the visual range only were least precise. Combining visual and NIR data derived from MEMS spectrometers did in most cases improve the prediction accuracy. We directly compared our results to models based on data from a conventional full range device. Our results showed that the combination of both MEMS devices can compete with models based on full range spectrometers. MEMS approaches reached between 68% and 105% of the corresponding full ranges devices R2 values. Local models tended to be more accurate than regional approaches for the Ah horizon. Our results suggest that MEMS spectrometers are suitable for forest soil C and N content estimation. They can contribute to improved monitoring in the future as their small size and weight could make in situ measurements feasible.', 'corpus_id': 235436904, 'score': 1}, {'doc_id': '233553620', 'title': 'Monitoring of leaf nitrogen content of winter wheat using multi-angle hyperspectral data', 'abstract': 'ABSTRACT Hyperspectral technology, which has been used in rapid and non-destructive monitoring of crop nitrogen status, is of great significance for the nitrogen fertilization management in modern agriculture. However, most researches collect hyperspectral data from the vertical angle, always leading to the inaccurate estimation of crop nitrogen. Studies have found that using multi-angle spectral data could improve the accuracy in estimating crop leaf nitrogen content (LNC). In this study, the LNC at different leaf positions of winter wheat under five nitrogen treatments was measured and the multi-angle spectral reflectance of leaves were collected. The results showed that the top third leaf were the most sensitive to nitrogen application rate. Correlation analysis showed that the correlation between LNC and spectral reflectance obtained at 0° was the highest, followed by that between LNC and spectral reflectance obtained at 10°, 30°, 40°, and 20°. Moreover, a model based on the multi-angle composite vegetation index (MACVI) for LNC estimation was constructed through combining the difference vegetable index (DVI), the normalized difference vegetable index (NDVI), and the ratio vegetable index (RVI) and using the spectral data obtained from multiple leaf inclination angles, finding that this model could improve the estimation accuracy. The accuracy of the model based on the spectral reflectance obtained at 0°, 10°, and 20° was higher than the others, and the coefficient of determination (R 2) for the MACVID,R-based model was the highest. The MACVI-based model proposed in this study could effectively improve the estimation accuracy of winter wheat nitrogen content, and provide scientific guidance for the nitrogen fertilization management in winter wheat cultivation.', 'corpus_id': 233553620, 'score': 1}]
115	{'doc_id': '235657894', 'title': 'Lightweight and modular resource leak verification', 'abstract': 'A resource leak occurs when a program allocates a resource, such as a socket or file handle, but fails to deallocate it. Resource leaks cause resource starvation, slowdowns, and crashes. Previous techniques to prevent resource leaks are either unsound, imprecise, inapplicable to existing code, slow, or a combination of these. Static detection of resource leaks requires checking that de-allocation methods are always invoked on relevant objects before they become unreachable. Our key insight is that leak detection can be reduced to an accumulation problem, a class of typestate problems amenable to sound and modular checking without the need for a heavyweight, whole-program alias analysis. The precision of an accumulation analysis can be improved by computing targeted aliasing information, and we augmented our baseline checker with three such novel techniques: a lightweight ownership transfer system; a specialized resource alias analysis; and a system to create a fresh obligation when a non-final resource field is updated. Our approach occupies a unique slice of the design space: it is sound and runs relatively quickly (taking minutes on programs that a state-of-the-art approach took hours to analyze). We implemented our techniques for Java in an open-source tool called the Resource Leak Checker. The Resource Leak Checker revealed 49 real resource leaks in widely-deployed software. It scales well, has a manageable false positive rate (comparable to the high-confidence resource leak analysis built into the Eclipse IDE), and imposes only a small annotation burden (1/1500 LoC) for developers.', 'corpus_id': 235657894}	15429	[{'doc_id': '63622661', 'title': 'Using Destination-Passing Style to Compile a Functional Language into Efficient Low-Level Code', 'abstract': 'We show how to compile high-level functional array-processing programs, drawn from image processing and machine learning, into C code that runs as fast as hand-written C. The key idea is to transform the program to destination passing style, which in turn enables a highly-efficient stack-like memory allocation discipline.', 'corpus_id': 63622661, 'score': 1}, {'doc_id': '237455767', 'title': 'Memory Management via Ownership Concept Rust and Swift: Experimental Study', 'abstract': 'One of the most important features to help facilitate reliable design in a programming language is memory management design. There are two wide-spread approaches: manual and automatic memory management, known as garbage collection (GC). Recently, a third approach which is ownership design has been fully adapted in new modern programming languages such as Rust and Swift. Rust uses ownership to eliminate high degree memory problems such as memory leak, dangling pointer, and use after free. Rust follows deterministic syntax-driven memory management depending on static ownership rules implemented and enforced by the rustc compiler. Swift also implements ownership concept in automatic reference counting (ARC). Though the ownership concept is adapted in Swift, it is not a memory-safe language because of the possibility of strong reference cycles. In this paper, we will illustrate the fundamental of ownership and the consequences of memory safety guarantees and issues related to Rust and Swift.We also conducted an experiment to compare the elapsed time binary tree allocation and deallocation in five programming languages C, C++, Java, Swift and Rust.', 'corpus_id': 237455767, 'score': 1}, {'doc_id': '235456568', 'title': 'Static Detection of Unsafe DMA Accesses in Device Drivers', 'abstract': 'Direct Memory Access (DMA) is a popular mechanism for improving hardware I/O performance, and it has been widely used by many existing device drivers. However, DMA accesses can be unsafe, from two aspects. First, without proper synchronization of DMA buffers with hardware registers and CPU cache, the buffer data stored in CPU cache and hardware registers can be inconsistent, which can cause unexpected hardware behaviors. Second, a malfunctioning or untrusted hardware device can write bad data into system memory, which can trigger security bugs (such as buffer overflow and invalid-pointer access), if the driver uses the data without correct validation. To detect unsafe DMA accesses, some key challenges need to be solved. For example, because each DMA access is implemented as a regular variable access in the driver code, identifying DMA accesses is difficult. In this paper, we propose a static-analysis approach named SADA, to automatically and accurately detect unsafe DMA accesses in device drivers. SADA consists of three basic steps. First, SADA uses a field-based alias analysis to identify DMA accesses, according to the information of DMA-buffer creation. Second, SADA uses a flow-sensitive and patternbased analysis to check the safety of each DMA access, to detect possible unsafe DMA accesses. Finally, SADA uses an SMT solver to validate the code-path condition of each possible unsafe DMA access, to drop false positives. We have evaluated SADA on the driver code of Linux 5.6, and found 284 real unsafe DMA accesses. Among them, we highlight that 121 can trigger buffer-overflow bugs and 36 can trigger invalid-pointer accesses causing arbitrary read or write. We have reported these unsafe DMA accesses to Linux driver developers, and 105 of them have been confirmed.', 'corpus_id': 235456568, 'score': 0}, {'doc_id': '222260869', 'title': 'Safe, Flexible Aliasing with Deferred Borrows', 'abstract': 'In recent years, programming-language support for static memory safety has developed significantly. In particular, borrowing and ownership systems, such as the one pioneered by the Rust language, require the programmer to abide by certain aliasing restrictions but in return guarantee that no unsafe aliasing can ever occur. This allows parallel code to be written, or existing code to be parallelized, safely and easily, and the aliasing restrictions also statically prevent a whole class of bugs such as iterator invalidation. Borrowing is easy to reason about because it matches the intuitive ownership-passing conventions often used in systems languages. Unfortunately, a borrowing-based system can sometimes be too restrictive. Because borrows enforce aliasing rules for their entire lifetimes, they cannot be used to implement some common patterns that pointers would allow. Programs often use pseudo-pointers, such as indices into an array of nodes or objects, instead, which can be error-prone: the program is still memory-safe by construction, but it is not logically memory-safe, because an object access may reach the wrong object. In this work, we propose deferred borrows, which provide the type-safety benefits of borrows without the constraints on usage patterns that they otherwise impose. Deferred borrows work by encapsulating enough state at creation time to perform the actual borrow later, while statically guaranteeing that the eventual borrow will reach the same object it would have otherwise. The static guarantee is made with a path-dependent type tying the deferred borrow to the container (struct, vector, etc.) of the borrowed object. This combines the type-safety of borrowing with the flexibility of traditional pointers, while retaining logical memory-safety. 2012 ACM Subject Classification Software and its engineering → General programming languages', 'corpus_id': 222260869, 'score': 1}, {'doc_id': '235434572', 'title': 'ARCUS: Symbolic Root Cause Analysis of Exploits in Production Systems', 'abstract': 'End-host runtime monitors (e.g., CFI, system call IDS) flag processes in response to symptoms of a possible attack. Unfortunately, the symptom (e.g., invalid control transfer) may occur long after the root cause (e.g., buffer overflow), creating a gap whereby bug reports received by developers contain (at best) a snapshot of the process long after it executed the buggy instructions. To help system administrators provide developers with more concise reports, we propose ARCUS, an automated framework that performs root cause analysis over the execution flagged by the end-host monitor. ARCUS works by testing “what if” questions to detect vulnerable states, systematically localizing bugs to their concise root cause while finding additional enforceable checks at the program binary level to demonstrably block them. Using hardware-supported processor tracing, ARCUS decouples the cost of analysis from host performance. We have implemented ARCUS and evaluated it on 31 vulnerabilities across 20 programs along with over 9,000 test cases from the RIPE and Juliet suites. ARCUS identifies the root cause of all tested exploits — with 0 false positives or negatives — and even finds 4 new 0-day vulnerabilities in traces averaging 4,000,000 basic blocks. ARCUS handles programs compiled from upwards of 810,000 lines of C/C++ code without needing concrete inputs or re-execution.', 'corpus_id': 235434572, 'score': 0}, {'doc_id': '235497686', 'title': 'Fusuma: Double-Ended Threaded Compaction(preprint)', 'abstract': 'Jonkers’s threaded compaction is attractive in the context of memory-constrained embedded systems because of its space efficiency. However, it cannot be applied to a heap where ordinary objects and meta-objects are intermingled for the following reason. It requires the object layout information, which is often stored in meta-objects, to update pointer fields inside objects correctly. Because Jonkers’s threaded compaction reverses pointer directions during garbage collection (GC), it cannot follow the pointers to obtain the object layout. This paper proposes Fusuma, a double-ended threaded compaction that allows ordinary objects and metaobjects to be allocated in the same heap. Its key idea is to segregate ordinary objects at one end of the monolithic heap and meta-objects at the other to make it possible to separate the phases of threading pointers in ordinary objects and meta-objects. Much like Jonkers’s threaded compaction, Fusuma does not require any additional space for each object. We implemented it in eJSVM, a JavaScript virtual machine for embedded systems, and compared its performance with eJSVM using mark-sweep GC. As a result, compaction enabled an IoT-oriented benchmark program to run in a 28-KiB heap, which is 20 KiB smaller than mark-sweep GC. We also confirmed that the GC overhead of Fusuma was less than 2.50× that of mark-sweep GC.', 'corpus_id': 235497686, 'score': 0}, {'doc_id': '237300507', 'title': 'Using Move Semantics in C++ to Minimize Aliasing and Simplify Reasoning', 'abstract': 'Funding information Most modern programming languages rely on pointer and reference copying for efficient datamovement. When references to mutable objects are copied, aliases are introduced, often complicating formal and informal behavioral reasoning. While some aliasing (and related complexity in reasoning) is unavoidable in software development, this paper explains how aliasing, and its impact on reasoning, can be minimized for practical software development by leveraging the relatively recent introduction of move semantics in C++ (as of C++ 11), simultaneously maintaining important performance properties of traditional programming. Our central contribution lies in a carefully-developed discipline for programming based on move semantics to avoid most routine introduction of aliasing in programming, thereby leading to simpler reasoning about the behavior of software. The discipline requires attention to interface and implementation development. The ideas are illustrated using components that we have built with andwithout the use of unique pointers.', 'corpus_id': 237300507, 'score': 1}, {'doc_id': '232177318', 'title': 'Functional Reactive Programming with nothing but Promises Implementing Push/Pull FRP using JavaScript Promises', 'abstract': 'Functional Reactive Programming (FRP) is a model of reactive programming defined by having a well-defined semantics given by time-indexed values. Promises are one-shot communication channels which allow asynchronous programs to be written in a synchronous style. In this paper, we show how timed promise lists, a timestamped linked list structure using promises rather than pointers, can be used to implement FRP. This idea originated with Elliott’s Push/Pull FRP, and we show that it can be expressed idiomatically in a strict functional language with promises, JavaScript. We identify a potential space leak with JavaScript’s built-in promises and propose an alternative implementation that avoids the leak.', 'corpus_id': 232177318, 'score': 0}, {'doc_id': '235914212', 'title': 'J un 2 02 1 Functional Pearl : Zero-Cost , Meta-Programmed , Dependently-Typed Stateful Functors in F ★', 'abstract': 'Writing code is hard; proving it correct is even harder. As the scale of verified software projects reaches new heights, the problem of efficiently verifying large amounts of software becomes more and more salient. Nowhere is this issue more evident than in the context of verified cryptographic libraries. To achieve featureparity and be competitive with unverified cryptographic libraries, a very large number of algorithms and APIs need to be verified. However, the task is oftentimes repetitive, and factoring out commonality between algorithms is fraught with difficulties, requiring until now a significant amount of manual effort. This paper shows how a judicious combination of known functional programming techniques leads to an order-of-magnitude improvement in the amount of verified code produced by the popular HACL cryptographic library, without compromising performance. We review three techniques that build upon each other, in order of increasing sophistication. First, we use dependent types to crisply capture the specification and state machine of a block algorithm, a cryptographic notion that was until now only informally and imprecisely specified. Next, we rely on partial evaluation to author a higher-order, stateful functor that transforms any unsafe block API into a safe counterpart. Finally, we rely on elaborator reflection to automate the very process of authoring a functor, using a code-rewriting tactic. This culminates in a style akin to templatized C++ code, but relying on a userland tactic and partial evaluation, rather than built-in compiler support. We demonstrate our techniques in the F programming language, and show that they require no change in the F compiler whatsoever. Our techniques require no user intervention either; the proofs are carried once and for all, and users can instantiate our higher-order functors without incurring any proof obligations. A distinguishing feature of our approach is that rather than introducing a novel extraction pipeline or incorporating program synthesis techniques, we instead rely on the established Letouzey-style erasure and extraction pipeline of F, with an extra early meta-programming stage added. Our encoding of higher-order functors in F thus provides the programmer with a very high level of abstraction while incurring no run-time costs thanks to careful partial evaluation. Our approach is in no way specific to cryptography, and as such, this paper requires no cryptographic knowledge. It simply turns out that the existing HACL library provides a grand challenge in terms of complexity; an ideal playground to evaluate the effectiveness of our techniques; and a concrete setting to identify patterns of verifying in the large. We have contributed six higher-order functors to the HACL library, for a total of nearly 40 specialized functor applications. Individually verifying each one of these 40 algorithms would have been impossible; our method thus significantly improves programmer productivity while providing a much higher level of assurance.', 'corpus_id': 235914212, 'score': 0}, {'doc_id': '11932674', 'title': 'Destination-passing style for efficient memory management', 'abstract': 'We show how to compile high-level functional array-processing programs, drawn from image processing and machine learning, into C code that runs as fast as hand-written C. The key idea is to transform the program to destination-passing style, which in turn enables a highly-efficient stack-like memory allocation discipline.', 'corpus_id': 11932674, 'score': 1}]
116	{'doc_id': '226981672', 'title': 'Comparison of SARS-CoV-2 serological tests with different antigen targets', 'abstract': '\n                  Background\n                  These last months, dozens of SARS-CoV-2 serological tests have become available with varying performances. A major effort was completed to compare 17 serological tests available in April 2020 in Switzerland.\n               \n                  Methods\n                  In a preliminary phase, we compared 17 IgG, IgM, IgA and pan Ig serological tests including ELISA, LFA, CLIA and ECLIA on a panel of 182 sera, comprising 113 sera from hospitalized patients with a positive RT-PCR, and 69 sampled before 1\u2009st November 2019, expected to give a positive and negative results, respectively. In a second phase, the five best performing and most available tests were further evaluated on a total of 582 sera (178 and 404 expected positive and negative, respectively), allowing the assessment of 20 possible cross-reactions with other virus.\n               \n                  Results\n                  In the preliminary phase, among eight IgG/pan-Ig ELISA or CLIA/ECLIA tests, five had a sensitivity and specificity above 90% and 98% respectively, and on six IgM/IgA tests, only one was acceptable. Only one LFA test on three showed good performances for both IgG and IgM. For all the tests IgM and IgG aroused concomitantly. In the second phase, no tests showed particular cross-reaction. We observed an important heterogeneity in the development of the antibody response.\n               \n                  Conclusions\n                  The majority of the evaluated tests exhibited high performances of IgG/pan-Ig sensitivity and specificity to detect the serological response of moderately to critically ill hospitalized patients. The IgM and IgA tests showed mostly insufficient performance with no added value for the early diagnostic on the cohort tested in this study.\n               ', 'corpus_id': 226981672}	11427	"[{'doc_id': '226306757', 'title': 'Quantitative measurement of IgG to SARS-CoV-2 proteins using ImmunoCAP', 'abstract': 'Background: Detailed understanding of the immune response to SARS-CoV-2, the cause of coronavirus disease 2019 (COVID-19), has been hampered by a lack of quantitative antibody assays. Objective: To develop a quantitative assay for IgG to SARS-CoV-2 proteins that could readily be implemented in clinical and research laboratories. Methods: The biotin-streptavidin technique was used to conjugate SARS-CoV-2 spike receptor-binding-domain (RBD) or nucleocapsid protein to the solid-phase of the ImmunoCAP resin. Plasma and serum samples from patients with COVID-19 (n=51) and samples from donors banked prior to the emergence of COVID-19 (n=109) were used in the assay. SARS-CoV-2 IgG levels were followed longitudinally in a subset of samples and were related to total IgG and IgG to reference antigens using an ImmunoCAP 250 platform. Results: Performance characteristics demonstrated 100% sensitivity and 99% specificity at a cut-off level of 2.5 g/mL for both SARS-CoV-2 proteins. Among 36 patients evaluated in a post-hospital follow-up clinic, median levels of IgG to spike-RBD and nucleocapsid were 34.7 g/mL (IQR 18-52) and 24.5 g/mL (IQR 9-59), respectively. Among 17 patients with longitudinal samples there was a wide variation in the magnitude of IgG responses, but generally the response to spike-RBD and to nucleocapsid occurred in parallel, with peak levels approaching 100 g/mL, or 1% of total IgG. Conclusions: We have described a quantitative assay to measure IgG to SARS-CoV-2 that could be used in clinical and research laboratories and implemented at scale. The assay can easily be adapted to measure IgG to novel antigens, has good performance characteristics and a read-out in standardized units.', 'corpus_id': 226306757, 'score': 1}, {'doc_id': '227216961', 'title': 'Automated Western immunoblotting detection of anti-SARS-CoV-2 serum antibodies.', 'abstract': ""ELISA and chemiluminescence serological assays for COVID-19 are currently incorporating only one or two SARS-CoV-2 antigens. We developed an automated Western immunoblotting as a complementary serologic assay for COVID-19. The Jess Simple Western system, an automated capillary-based assay was used, incorporating an inactivated SARS-CoV-2 lineage 20a strain as antigen, and IgT detection. In total, 602 sera were tested including 223 from RT-PCR-confirmed COVID-19 patients, 76 from patients diagnosed with seasonal HCoVs and 303 from coronavirus-negative control sera. We also compared this assay with the EUROIMMUN(R) SARS-CoV-2 IgG ELISA kit. Among 223 sera obtained from RT-PCR-confirmed COVID-19 patients, 180/223 (81%) exhibited reactivity against the nucleocapsid and 70/223 (31%) against the spike protein. Nucleocapsid reactivity was further detected in 9/76 (14%) samples collected from patients diagnosed with seasonal HCoVs and in 15/303 (5%) coronavirus-negative control samples. In the subset of sera collected more than 2 weeks after the onset of symptoms, the sensitivity was 94% and the specificity 93%, the latter value probably reflecting cross-reactivity of SARS-CoV-2 with other coronaviruses. The automated Western immunoblotting presented a substantial agreement (90%) with the compared ELISA (Cohen's Kappa=0.64). Automated Western immunoblotting may be used as a second line test to monitor exposition of people to HCoVs including SARS-CoV-2."", 'corpus_id': 227216961, 'score': 0}, {'doc_id': '227183905', 'title': 'Development and Head-to-Head Comparison of Two Colloidal Gold Based Serologic Lateral Flow Assays for SARS-CoV-2 Antibody Tests', 'abstract': 'To combat the COVID-19 pandemic, serologic lateral flow immunoassays are required to facilitate accurate diagnosis of SARS-CoV-2 infection and confirmation of molecular results This study evaluated sensitivity of two different designs of colloidal gold serologic tests (antigen based total antibody test and antibody based IgG test) by using residual serum samples from patients who were evaluated for SARS-CoV-2 infection status by polymerase chain reaction (PCR) The results showed 100% specificity for both tests, while when testing of 16 positive patients, the data showed 90% sensitivity for total antibody test and 30% for IgG test This study demonstrates high diagnostic accuracy for anti-SARS-CoV-2 total antibody tests and will facilitate further development and selection of serological assays', 'corpus_id': 227183905, 'score': 0}, {'doc_id': '227162937', 'title': 'Comparison of various serological assays for novel SARS-COV-2', 'abstract': 'Coronavirus disease-19 (COVID19), the novel respiratory illness caused by severe acute respiratory syndrome coronavirus (SARS-CoV-2), is associated with severe morbidity and mortality. The aim of our study was to compare different immunoassays. We evaluated three immunochromatographic test (The StrongStep®SARS-CoV-2 IgG/IgM kit, AllTest COV-19 IgG/IgM kit, and Wondfo® SARS-CoV-2 Antibody) and two chemiluminescence immunoassays (CMIA) (Covid-19 VIRCLIA® IgM+IgA/IgG monotest and the Abbott SARS-CoV-2 IgG assay) in COVID-19 patients. The assays were performed using serum samples of three group patients, i.e., healthy controls, patients with SARS-CoV-2 PCR positive, and patients with SARS-CoV-2 PCR negative clinically diagnosed of COVID-19 infection. The detection percentages of IgG with the StrongStep® SARS-CoV-2 IgG/IgM kit and AllTest COV-19 IgG/IgM kit were similar in both groups (83.3% and 80.6%, respectively in group 2, p = 0.766) and (42.9% and 50.0%, respectively in group 3, p = 0.706). There were some differences on IgM detection between StrongStep® SARS-CoV-2 IgG/IgM kit and AllTest COV-19 IgG/IgM kit (11.1% and 30.6%, respectively in group 2, p = 0.042 and 0.0% and 28.6%, respectively in group 3, p = 0.031). The positive rate of IgG in group 2 is higher compared to group 3 with the two immunoassays tested. We observe the same positive rates of IgG with the two CMIA. Our study shows excellent performance of CMIA compared to immunochromatographic test and confirms its potential use in the diagnosis of the new SARS-CoV-2.', 'corpus_id': 227162937, 'score': 1}, {'doc_id': '226307711', 'title': 'Comparison of SARS-COV-2 nasal antigen test to nasopharyngeal RT-PCR in mildly symptomatic patients', 'abstract': 'Introduction COVID 19 has been vastly spreading since December 2019 and the medical teams worldwide are doing their best to limit its spread. In the absence of a vaccine the best way to fight it is by detecting infected cases early and isolate them to prevent its spread. Therefore, a readily available, rapid, and cost-effective test with high specificity and sensitivity for early detection of COVID 19 is required. In this study, we are testing the diagnostic performance of a rapid antigen detection test in mildly symptomatic cases. (RADT). Methods The study included 4183 male patients who were mildly symptomatic. A nasal sample for the rapid antigen test and a nasopharyngeal sample was taken from each patient. Statistical analysis was conducted to calculate the sensitivity, specificity, positive predictive value, negative predictive value and kappa coefficient of agreement. Results The prevalence of COVID 19 in the study population was 17.5% (733/4183). The calculated sensitivity and specificity were 82.1% and 99.1% respectively. Kappa coefficient of agreement between the rapid antigen test and RT-PCR was 0.859 (p < 0.001). A stratified analysis was performed and it showed that the sensitivity of the test improved significantly with lowering the cutoff Ct value to 24. Conclusion The results of the diagnostic assessment of nasal swabs in the RADT used in our study are promising regarding the potential benefit of using them as a screening tool in mildly symptomatic patients. The diagnostic ability was especially high in cases with high viral load. The rapid antigen test is intended to be used alongside RT-PCR and not replace it. RADT can be of benefit in reducing the use of PCR.', 'corpus_id': 226307711, 'score': 0}, {'doc_id': '226945944', 'title': 'Asymptomatic and symptomatic SARS-CoV-2 infections elicit polyfunctional antibodies.', 'abstract': 'A large proportion of SARS-CoV-2 infected individuals remains asymptomatic. Little is known about the extent and quality of their antiviral humoral response. Here, we analyzed antibody functions in 52 asymptomatic infected individuals, 119 mild and 21 hospitalized COVID-19 patients. We measured anti- Spike antibody levels with the S-Flow assay and mapped SARS-CoV-2 Spike- and N-targeted regions by Luminex. Neutralization, complement deposition and Antibody-Dependent Cellular Cytotoxicity (ADCC) were evaluated using replication-competent SARS-CoV-2 or reporter cell systems. We show that COVID-19 sera mediate complement deposition and kill infected cells by ADCC. Sera from asymptomatic individuals neutralize the virus, activate ADCC and trigger complement deposition. Antibody levels and activities are slightly lower in asymptomatic individuals. The different functions of the antibodies are correlated, independently of disease severity. Longitudinal samplings show that antibody functions follow similar kinetics of induction and contraction, with minor variations. Overall, asymptomatic SARS-CoV-2 infection elicits polyfunctional antibodies neutralizing the virus and targeting infected cells. - Sera from convalescent COVID-19 patients activate the complement and kill infected cells by ADCC. - Asymptomatic and symptomatic SARS-CoV-2-infected individuals harbor polyfunctional antibodies. - Antibody levels and functions are slightly lower in asymptomatic individuals - The different antiviral activities of anti-Spike antibodies are correlated regardless of disease severity. - Functions of anti-Spike antibodies have similar kinetics of induction and contraction.', 'corpus_id': 226945944, 'score': 0}, {'doc_id': '227188089', 'title': 'Comparison of 12 Molecular Detection Assays for Severe Acute Respiratory Syndrome Coronavirus 2 (SARS-CoV-2)', 'abstract': '\n Molecular testing for severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) is the mainstay for accurate diagnosis of the infection, but the diagnostic performances of available assays have not been defined. We compared 12 molecular diagnostic assays, including 8 commercial kits using 155 respiratory samples (65 nasopharyngeal swabs, 45 oropharyngeal swabs, and 45 sputum) collected at 2 Japanese hospitals. Sixty-eight samples were positive for more than one assay and one genetic locus and were defined as true positive samples. All the assays showed a specificity of 100% (95% confidence interval, 95.8 to 100). The N2 assay kit of the US Centers for Disease Control and Prevention (CDC) and the N2 assay of the Japanese National Institute of Infectious Disease (NIID) were the most sensitive assays with 100% sensitivity (95% confidence interval, 94.7 to 100), followed by the CDC N1 kit, E assay by Corman, and NIID N2 assay multiplex with internal control reactions. These assays are reliable as first-line molecular assays in laboratories when combined with appropriate internal control reactions.\n', 'corpus_id': 227188089, 'score': 0}, {'doc_id': '227253411', 'title': 'Anti–SARS-CoV-2 Antibody Responses in Convalescent Plasma Donors Are Increased in Hospitalized Patients; Subanalyses of a Phase 2 Clinical Study', 'abstract': 'We evaluated the antibody responses in 259 potential convalescent plasma donors for Covid-19 patients. Different assays were used: a commercial ELISA detecting antibodies against the recombinant spike protein (S1); a multiplex assay detecting total and specific antibody isotypes against three SARS-CoV-2 antigens (S1, basic nucleocapsid (N) protein and receptor-binding domain (RBD)); and an in-house ELISA detecting antibodies to complete spike, RBD and N in 60 of these donors. Neutralizing antibodies (NAb) were also evaluated in these 60 donors. Analyzed samples were collected at a median time of 62 (14–104) days from the day of first symptoms or positive PCR (for asymptomatic patients). Anti-SARS-CoV-2 antibodies were detected in 88% and 87.8% of donors using the ELISA and the multiplex assay, respectively. The multivariate analysis showed that age ≥50 years (p < 0.001) and need for hospitalization (p < 0.001) correlated with higher antibody titers, while asymptomatic status (p < 0.001) and testing >60 days after symptom onset (p = 0.001) correlated with lower titers. Interestingly, pseudotype virus-neutralizing antibodies (PsNAbs) significantly correlated with spike and with RBD antibodies by ELISA. Sera with high PsNAb also showed a strong ability to neutralize active SARS-CoV-2 virus, with hospitalized patients showing higher titers. Therefore, convalescent plasma donors can be selected based on the presence of high RBD antibody titers.', 'corpus_id': 227253411, 'score': 1}, {'doc_id': '227055889', 'title': 'Temporal Course of SARS-CoV-2 Antibody Positivity in Patients with COVID-19 following the First Clinical Presentation', 'abstract': 'Knowledge of the sensitivities of severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) antibody tests beyond 35 days after the clinical onset of COVID-19 is insufficient. We aimed to describe positivity rate of SARS-CoV-2 assays employing three different measurement principles over a prolonged period. Two hundred sixty-eight samples from 180 symptomatic patients with COVID-19 and a reverse transcription polymerase chain reaction (RT-PCR) test followed by serological investigation of SARS-CoV-2 antibodies were included. We conducted three chemiluminescence (including electrochemiluminescence assay (ECLIA)), four enzyme-linked immunosorbent assay (ELISA), and one lateral flow immunoassay (LFIA) test formats. Positivity rates, as well as positive (PPVs) and negative predictive values (NPVs), were calculated for each week after the first clinical presentation for COVID-19. Furthermore, combinations of tests were assessed within an orthogonal testing approach employing two independent assays and predictive values were calculated. Heat maps were constructed to graphically illustrate operational test characteristics. During a follow-up period of more than 9 weeks, chemiluminescence assays and one ELISA IgG test showed stable positivity rates after the third week. With the exception of ECLIA, the PPVs of the other chemiluminescence assays were ≥95% for COVID-19 only after the second week. ELISA and LFIA had somewhat lower PPVs. IgM exhibited insufficient predictive characteristics. An orthogonal testing approach provided PPVs ≥ 95% for patients with a moderate pretest probability (e.g., symptomatic patients), even for tests with a low single test performance. After the second week, NPVs of all but IgM assays were ≥95% for patients with low to moderate pretest probability. The confirmation of negative results using an orthogonal algorithm with another assay provided lower NPVs than the single assays. When interpreting results from SARS-CoV-2 tests, the pretest probability, time of blood draw, and assay characteristics must be carefully considered. An orthogonal testing approach increases the accuracy of positive, but not negative, predictions.', 'corpus_id': 227055889, 'score': 1}, {'doc_id': '226985026', 'title': 'Clinical performance of three fully automated anti‐SARS‐CoV‐2 immunoassays targeting the nucleocapsid or spike proteins', 'abstract': 'This study assesses the clinical performance of three anti‐SARS‐CoV‐2 assays, namely EUROIMMUN anti‐SARS‐CoV‐2 nucleocapsid (IgG) ELISA, Elecsys anti‐SARS‐CoV‐2 nucleocapsid (total antibodies) assay, and LIAISON anti‐SARS‐CoV‐2 spike proteins S1 and S2 (IgG) assay. One hundred and thirty‐seven coronavirus disease 2019 (COVID‐19) samples from 96 reverse‐transcription polymerase chain reaction confirmed patients were chosen to perform the sensitivity analysis. Non‐SARS‐CoV‐2 sera (n\u2009=\u2009141) with a potential cross‐reaction to SARS‐CoV‐2 immunoassays were included in the specificity analysis. None of these tests demonstrated a sufficiently high clinical sensitivity to diagnose acute infection. Fourteen days since symptom onset, we did not find any significant difference between the three techniques in terms of sensitivities. However, Elecsys performed better in terms of specificity. All three anti‐SARS‐CoV‐2 assays had equivalent sensitivities 14 days from symptom onset to diagnose past‐COVID‐19 infection. We also confirmed that anti‐SARS‐CoV‐2 determination before Day 14 is of less clinical interest.', 'corpus_id': 226985026, 'score': 1}]"
117	"{'doc_id': '111668002', 'title': 'PREHISTORIC MAORI STORAGE PITS. PROBLEMS IN INTERPRETATION', 'abstract': 'Nearly 10 years have now elapsed since Les Groube, in a paper on prehistoric settlement patterns in New Zealand, dealt the death blow to the underground house or pit dwelling and established in its place the kumara storage pit as an essential component of Maori settlement and economy.(1) There is no reason to re-open this controversy: it is now accepted that the pits supply the necessary insulated frost-free storage and controlled humidity for kumara, and the darkness needed in the case of white potatoes to prevent their turning green or sprouting. This article will examine the evidence that has accumulated from the many excavations of pits in different parts of the North Island since 1955, pick out some distinctive types, and interpret them as three-dimensional structures: in so doing, some outstanding problems may become apparent which only further excavations can resolve. First, there are numerous small ""bin"" pits that were no more than a hole in the ground, sometimes covered with a lid, such as those at Sarah\'s Gully, Coromandel, and at Otakanini, South Kaipara,(2) as well as the carefully excavated beehive or domed pit known as a rua - with these, this paper is not concerned. The subject of the enquiry is the large rec tangular pit, usually of the playing-card shape, with an approximate 2:1 ratio of length to breadth, but exceptionally near-square. Typical measure ments are from 10 to 20 ft long, from 6 to 12 ft wide, and from 3 to 6 ft deep, though there are some smaller and some larger examples. Many have drains round the bottom, often with an external exit or connecting to a sump or soakaway; there is also a series of postholes in the floor, the posts of which, as is generally recognised, supported a roof. We are not then dealing with a species of ""clamp""?an earth structure with a flat top as used for storing root vegetables in Britain?but with cellars which had a pitched roof, in order that people could enter to store or remove the contents.', 'corpus_id': 111668002}"	16980	[{'doc_id': '54202333', 'title': 'Containers and grains: food storage and symbolism in the Central Balkans (Vinča period)', 'abstract': 'Since Flannery, who showed that types and locations of storage facilities provide a variety of associations for explaining social change, the economic and social role of storage has been reviewed numerous times. So far, no research pertaining to storage practice has been conducted in the Central Balkans. However, storage strategies play an important role in the agricultural history of the region. Similar, or exactly the same storage techniques have been practiced from the Early Neolithic until modern times, and even today some are practiced by traditional farming communities. Hence this article, which is intended to lay the foundations for understanding the relation between economic and social processes as reflected by storage behaviour in the Late Neolithic-Early Eneolithic in the Central Balkans region.', 'corpus_id': 54202333, 'score': 1}, {'doc_id': '162276142', 'title': 'PITS, PRECONCEPTIONS and PROPITIATION IN the BRITISH IRON AGE', 'abstract': 'Summary: Large pits have long been known to be a characteristic of the British Iron Age. Originally they were thought to be habitations but since the 1930s they have been assumed to have served as grain silos. This paper reviews our changing conceptions and then considers a range of new data for special burials within the pits. A model is developed which sees the storage of seed grain in pits as a deliberate act designed to place the grain in the protection of the chthonic deities. the chronological and spatial implications of pit storage in Britain are briefly considered.', 'corpus_id': 162276142, 'score': 1}, {'doc_id': '233412391', 'title': 'Cycles in Stone Mining and Copper Circulation in Europe 5500–2000 bc: A View from Space', 'abstract': 'The authors of this article consider the relationship in European prehistory between the procurement of high-quality stones (for axeheads, daggers, and other tools) on the one hand, and the early mining, crafting, and deposition of copper on the other. The data consist of radiocarbon dates for the exploitation of stone quarries, flint mines, and copper mines, and of information regarding the frequency through time of jade axeheads and copper artefacts. By adopting a broad perspective, spanning much of central-western Europe from 5500 to 2000 bc, they identify a general pattern in which the circulation of the first copper artefacts was associated with a decline in specialized stone quarrying. The latter re-emerged in certain regions when copper use decreased, before declining more permanently in the Bell Beaker phase, once copper became more generally available. Regional variations reflect the degrees of connectivity among overlapping copper exchange networks. The patterns revealed are in keeping with previous understandings, refine them through quantification and demonstrate their cyclical nature, with additional reference to likely local demographic trajectories.', 'corpus_id': 233412391, 'score': 0}, {'doc_id': '234349796', 'title': 'Book Review: James Rees, Marco Pomati and Elke Heins (eds) Social Policy Review 32', 'abstract': 'based upon exclusion rather than rights. Indeed, this is a key argument running through the book – that the proliferation and naturalisation of charitable food assistance as a front-line response to food insecurity risks not only plugging the holes left by state retrenchment, it also risks further curtailment of citizens’ social rights. The editors use the conclusion to tease out various key themes across all seven national case studies with the use of tables and diagrams to facilitate ease of comparison. In short, this collection provides a rich and nuanced comparative study, affording readers the opportunity to augment their understanding of the evolution of food poverty and of the charitable responses to it across the EU. Its investigation of the entanglement of food giants who fulfil their corporate social responsibility through divesting surplus food for charitable redistribution, and businesses who enhance their corporate image through in-kind donations is significant for comprehension of the global food charity system. Finally, the book’s examination of the role the state has played in diminishing former entitlements that have increased vulnerability to hardship, and ultimately shaped the food charity landscape across Europe, make it essential reading for researchers, students and social policy analysts alike.', 'corpus_id': 234349796, 'score': 0}, {'doc_id': '233216989', 'title': 'Forest Saami heritage and history', 'abstract': 'In this paper, I will briefly present my research aiming to define, localise and interpret the archaeological remains that can be connected to Forest Saami culture and economy in the Swedish part of Sápmi, focusing on the early modern and modern period. With the help of an ethnoarchaeological method, I use ethnographic sources to understand the link between the people and the archaeological remains, and to get information on where to find them. One of the main questions is how and why Forest Saami archaeology differs from Mountain Saami archaeology, and how that is related to differences in economy. In three defined research areas, i.e. two earlier Forest Saami skatteland (Sw: ‘tax paying districts’) in the Lule river valley and a third research area, Forsa Parish in Hälsingland, new aspects of Forest Saami history have been investigated, using a combination of archaeological, ethnographic and historical sources. One aim is to bring a Saami perspective into archaeological studies, both concerning the geopolitical framework, the investigation and the interpretation of the results. Methods to be used in Saami archaeology are presented, methods that will be a part of a combined personal, academic, activist and archaeological struggle to enlighten and reclaim this heritage and history.', 'corpus_id': 233216989, 'score': 0}, {'doc_id': '232387135', 'title': 'Money, capital, and theology', 'abstract': 'How do we think about the relationship between money and Christianity? Some may recall Jesus’ teaching of paying taxes to Caesar (Matt. 22:15–22) and his overthrowing the tables of the money changers at the temple (Matt. 21:12–13). Others may think of the ransom theory of atonement or the sale of indulgences during Luther’s time. In our contemporary world, some scholars have examined the relationship between money, the spirit of capitalism, and Christianity (Wariboko 2008; Tanner 2019). But not many have looked at how money has been inscribed historically in the cultural logic of the ways Christian doctrines have been formulated. Devin Singh’s book Divine Currency: The Theological Power of Money in the West (Singh 2018) makes an important contribution by filling this gap. Divine Currency argues that the Christian imagination has been infused with economic ideas, practices, and traditions since the patristic periods when some of the foundational Christian doctrines about God, Christ, and salvation were formulated. These theological doctrines, in turn, formed what Peter Berger (1967) has called a “sacred canopy” by providing divine sanction for the normalcy of the social and political order. Singh writes: “If money lends its logic to the structuring of theology, God-talk repays by offering its prestige and sacred power to the world of exchange” (2018, 2). Divine Currency is important for scholars interested in the postcolonial study of religion for it expands our scope of research on the relationship between empire and theology. First, previous works tend to focus on the early church or the modern period. But there are comparatively fewer works that investigate the inscription of economics and empire in patristic theology. Singh’s book focuses on the writings of Eusebius, the Cappadocian fathers, and other theologians in the patristic period and, in doing so, extends our discussion to an important period of the formulation of Christian doctrines. Second, while many works on empire and theology have chastised imperial aggression, colonialism, and exploitation, the authors often focus on theological discourses and not so much on economic discourses and try to provide a “transcendent” or a moral critique.', 'corpus_id': 232387135, 'score': 0}, {'doc_id': '131218938', 'title': 'Centralized Storage in Later Prehistoric Britain.', 'abstract': 'Archaeological evidence for centralized storage facilities may provide useful information about the organization of prehistoric economies. In the background are a range of explanatory ideas. ‘Redistribution’ is a term which has been applied to the evidence from some British hillforts. Resources might be collected and then re-allocated through a permanent agency of co-ordination. They might be mobilized as tribute to elites as part of political strategy. This has been suggested for early British hillforts, and the evidence is reviewed. Much depends on the interpretation of the ‘four-poster’ structures at these sites as storehouses. A survey of these structures on British and continental sites strengthens this interpretation, and a further survey shows that, in Britain, disproportionate numbers of these structures are found at massively enclosed sites. A modified form of site catchment analysis suggests that some of the hillforts stored produce mobilized from an area which was greater than is likely to have been farmed directly from these sites. One possible inference is that resources were mobilized from subordinate settlements. It is suggested that stored grain was a critical commodity if rising populations and climatic change combined to increase the risk attached to the cereal harvest.', 'corpus_id': 131218938, 'score': 1}, {'doc_id': '161818539', 'title': 'Barely Surviving or More than Enough?. The environmental archaeology of subsistence, specialisation and surplus food production', 'abstract': None, 'corpus_id': 161818539, 'score': 1}, {'doc_id': '234708460', 'title': 'Test Excavations at Sites 41LK284 and 41LK294, FM 1042 at the Nueces River, Live Oak County, Texas', 'abstract': 'The planned extension of FM 1042 in Live Oak County includes a crossing at the Nueces River. Phase II archaeological testing of two sites (41LK284 and 41LK294) on the north and south terraces of the Nueces River was undertaken by TxDOT archaeologists, prior to construction, to determine eligibility for inclusion in the National Register of Historic Places(in accordance with 36 CFR Part 800) and State Landmark status. Portions of both sites are located within the right-of-way. Surveys conducted in 1988 and 1992 recorded a light scatter of mussel shell and chert flakes on the surface of both sites. Cultural debris found in the roadcut was reported at depths of up to 1 m at 41LK284 and up to .5 m at 41LK294. A possible Matamoros point fragment and an end-scraper were also recovered from the surface of 41LK294, suggesting a Late Prehistoric occupation. Results of testing indicate that the portions of both sites within the right-of-way are disturbed and do not meet the criteria for designation as a State Archaeological Landmark or for listing on the National Register of Historic Places.', 'corpus_id': 234708460, 'score': 0}, {'doc_id': '220829201', 'title': 'Experimental Archaeology: 1. Early Bronze Age Cremation Pyres; 2. Iron Age Grain Storage', 'abstract': None, 'corpus_id': 220829201, 'score': 1}]
118	{'doc_id': '124300702', 'title': 'Performance of Quality Assurance Procedures on Daily Precipitation', 'abstract': 'Abstract The search for precipitation quality control (QC) methods has proven difficult. The high spatial and temporal variability associated with precipitation data causes high uncertainty and edge creep when regression-based approaches are applied. Precipitation frequency distributions are generally skewed rather than normally distributed. The commonly assumed normal distribution in QC methods is not a good representation of the actual distribution of precipitation and is inefficient in identifying the outliers. This paper first explores the use of a single gamma distribution, fit to all precipitation data, in a quality assurance test. A second test, the multiple intervals gamma distribution (MIGD) method, is introduced. It assumes that meteorological conditions that produce a certain range in average precipitation at surrounding stations will produce a predictable range at the target station. The MIGD bins the average of precipitation at neighboring stations; then, for the events in a specific bin, an ...', 'corpus_id': 124300702}	4134	"[{'doc_id': '124822741', 'title': 'Quality Control of Weather Data during Extreme Events', 'abstract': 'Abstract Quality assurance (QA) procedures have been automated to reduce the time and labor necessary to discover outliers in weather data. Measurements from neighboring stations are used in this study in a spatial regression test to provide preliminary estimates of the measured data points. The new method does not assign the largest weight to the nearest estimate but, instead, assigns the weights according to the standard error of estimate. In this paper, the spatial test was employed to study patterns in flagged data in the following extreme events: the 1993 Midwest floods, the 2002 drought, Hurricane Andrew (1992), and a series of cold fronts during October 1990. The location of flagged records and the influence zones for such events relative to QA were compared. The behavior of the spatial test in these events provides important information on the probability of making a type I error in the assignment of the quality control flag. Simple pattern recognition tools that identify zones wherein frequent fl...', 'corpus_id': 124822741, 'score': 1}, {'doc_id': '218869858', 'title': 'Bayesian non-asymptotic extreme value models for environmental data', 'abstract': 'Motivated by the analysis of extreme rainfall data, we introduce a general Bayesian hierarchical model for estimating the probability distribution of extreme values of intermittent random sequences, a common problem in geophysical and environmental science settings. The approach presented here relaxes the asymptotic assumption typical of the traditional extreme value (EV) theory, and accounts for the possible underlying variability in the distribution of event magnitudes and occurrences, which are described through a latent temporal process. Focusing on daily rainfall extremes, the structure of the proposed model lends itself to incorporating prior geo-physical understanding of the rainfall process. By means of an extensive simulation study, we show that this methodology can significantly reduce estimation uncertainty with respect to Bayesian formulations of traditional asymptotic EV methods, particularly in the case of relatively small samples. The benefits of the approach are further illustrated with an application to a large data set of 479 long daily rainfall historical records from across the continental United States. By comparing measures of in-sample and out-of-sample predictive accuracy, we find that the model structure developed here, combined with the use of all available observations for inference, significantly improves robustness with respect to overfitting to the specific sample.', 'corpus_id': 218869858, 'score': 1}, {'doc_id': '220793790', 'title': 'A Hamiltonian Interacting Particle System for Compressible Flow', 'abstract': 'The decomposition of the energy of a compressible fluid parcel into slow (deterministic) and fast (stochastic) components is interpreted as a stochastic Hamiltonian interacting particle system (HIPS). It is shown that the McKean-Vlasov equation associated to the mean field limit yields the barotropic Navier-Stokes equation with density dependent viscosity. Capillary forces can also be treated by this approach. Due to the Hamiltonian structure the mean field system satisfies a Kelvin circulation theorem along stochastic Lagrangian paths.', 'corpus_id': 220793790, 'score': 0}, {'doc_id': '218674151', 'title': 'Relativistic kinetic theory of classical systems of charged particles: towards the microscopic foundation of thermodynamics and kinetics', 'abstract': 'In the complete system of equations of evolution of the classical system of charges and the electromagnetic field generated by them, the field variables are excluded. An exact closed relativistic non-Hamiltonian system of nonlocal kinetic equations, that describes the evolution of a system of charges in terms of their microscopic distribution functions, is obtained . The solutions of this system of equations are non-invariant with respect to time reversal, and also have the property of hereditarity.', 'corpus_id': 218674151, 'score': 0}, {'doc_id': '219708794', 'title': 'Estimating Concurrent Climate Extremes: A Conditional Approach', 'abstract': 'Simultaneous concurrence of extreme values across multiple climate variables can result in large societal and environmental impacts. Therefore, there is growing interest in understanding these concurrent extremes. One way to approach this problem is to study the distribution of one climate variable given that another is extreme. In this work we develop a statistical framework for estimating bivariate concurrent extremes via a conditional approach, where univariate extreme value modeling is combined with dependence modeling of the conditional tail distribution using techniques from quantile regression and extreme value analysis to quantify concurrent extremes. We focus on the distribution of daily wind speed conditioned on daily precipitation taking its seasonal maximum. The Canadian Regional Climate Model large ensemble is used to assess the performance of the proposed framework both via a simulation study with specified dependence structure and via an analysis of the climate model-simulated dependence structure.', 'corpus_id': 219708794, 'score': 1}, {'doc_id': '219260134', 'title': 'Hybrid Scheme of Kinematic Analysis and Lagrangian Koopman Operator Analysis for Short-term Precipitation Forecasting', 'abstract': 'With the accumulation of meteorological big data, data-driven models for short-term precipitation forecasting have shown increasing promise. We focus on Koopman operator analysis, which is a data-driven scheme to discover governing laws in observed data. We propose a method to apply this scheme to phenomena accompanying advection currents such as precipitation. The proposed method decomposes time evolutions of the phenomena between advection currents under a velocity field and changes in physical quantities under Lagrangian coordinates. The advection currents are estimated by kinematic analysis, and the changes in physical quantities are estimated by Koopman operator analysis. The proposed method is applied to actual precipitation distribution data, and the results show that the development and decay of precipitation are properly captured relative to conventional methods and that stable predictions over long periods are possible.', 'corpus_id': 219260134, 'score': 1}, {'doc_id': '220403290', 'title': 'Derivation of a Relativistic Boltzmann Distribution', 'abstract': ""A framework for relativistic thermodynamics and statistical physics is built by first exploiting the symmetries between energy and momentum in the derivation of the Boltzmann distribution, then using Einstein's energy-momentum relationship to derive a PDE for the partition function. It is shown that the extended Boltzmann distribution implies the existence of an inverse four-temperature, while the form of the partition function PDE implies the existence of a quantizable field theory of classical statistics, with hints of an associated gravity like gauge theory. An adaptation of the framework is then used to derive a thermodynamic certainty relationship."", 'corpus_id': 220403290, 'score': 0}, {'doc_id': '120282197', 'title': 'Sensitivity Analysis of Quality Assurance Using the Spatial Regression Approach—A Case Study of the Maximum/Minimum Air Temperature', 'abstract': 'Abstract Both the spatial regression test (SRT) and inverse distance weighting (IDW) methods have been applied to provide estimates for the maximum air temperature (Tmax) and the minimum air temperature (Tmin) in the Applied Climate Information System (ACIS). This is critical to the processes of estimating missing data and identifying suspect data and is undertaken here to ensure quality data in ACIS. The SRT method was previously found to be superior to the IDW method; however, the sensitivity of the performance of both methods to input parameters has not been evaluated. A set of analyses is presented for both methods whereby the sensitivity to the radius of inclusion, the regression time window, the regression time offset, and the number of stations used to make the estimates are examined. Comparisons were also conducted between the SRT and the IDW methods. The performance of the SRT method stabilized when 10 or more stations were applied in the estimates. The optimal number of stations for the IDW meth...', 'corpus_id': 120282197, 'score': 1}, {'doc_id': '221534474', 'title': 'Quantifying uncertainty in spatio-temporal changes of upper-ocean heat content estimates: an internationally coordinated comparison', 'abstract': 'The Earth system is accumulating energy due to human-induced activities. More than 90 percent of this energy has been stored in the ocean as heat since 1970, with about 64 percent of that in the upper 700 m. Differences in upper ocean heat content anomaly (OHCA) estimates, however, exist. Here, we evaluate spread in upper OHCA estimates arising from choices in instrumental bias corrections and mapping methods, in addition to the effect of using a common ocean mask. The same dataset was mapped by six research groups for 1970 to 2008, with six instrumental bias corrections applied to expendable bathythermograph (XBT) data. We find that use of a common ocean mask may impact estimation of global OHCA by +- 13 percent. Uncertainty due to mapping method dominates over XBT bias correction at a global scale and is largest in the Indian Ocean and in the eddy-rich and frontal regions of all basins. Uncertainty due to XBT bias correction is largest in the Pacific Ocean within 30N to 30S. In both mapping and XBT cases, spread is higher since the 1990s. Important differences in spatial trends among mapping methods are found in the well-observed Northwest Atlantic and the poorly-observed Southern Ocean. Although our results cannot identify the best mapping or bias correction schemes, they identify where and when greater uncertainties exist, and so where further refinements may yield the largest improvements. Our results highlight the need for a future international coordination to evaluate performance of existing mapping methods.', 'corpus_id': 221534474, 'score': 0}]"
119	{'doc_id': '222140788', 'title': 'Denoising Diffusion Implicit Models', 'abstract': 'Denoising diffusion probabilistic models (DDPMs) have achieved high quality image generation without adversarial training, yet they require simulating a Markov chain for many steps to produce a sample. To accelerate sampling, we present denoising diffusion implicit models (DDIMs), a more efficient class of iterative implicit probabilistic models with the same training procedure as DDPMs. In DDPMs, the generative process is defined as the reverse of a Markovian diffusion process. We construct a class of non-Markovian diffusion processes that lead to the same training objective, but whose reverse process can be much faster to sample from. We empirically demonstrate that DDIMs can produce high quality samples $10 \\times$ to $50 \\times$ faster in terms of wall-clock time compared to DDPMs, allow us to trade off computation for sample quality, and can perform semantically meaningful image interpolation directly in the latent space.', 'corpus_id': 222140788}	1173	"[{'doc_id': '222208619', 'title': 'Reward-Biased Maximum Likelihood Estimation for Linear Stochastic Bandits', 'abstract': 'Modifying the reward-biased maximum likelihood method originally proposed in the adaptive control literature, we propose novel learning algorithms to handle the explore-exploit trade-off in linear bandits problems as well as generalized linear bandits problems. We develop novel index policies that we prove achieve order-optimality, and show that they achieve empirical performance competitive with the state-of-the-art benchmark methods in extensive experiments. The new policies achieve this with low computation time per pull for linear bandits, and thereby resulting in both favorable regret as well as computational efficiency.', 'corpus_id': 222208619, 'score': 0}, {'doc_id': '222067084', 'title': 'Reannealing of Decaying Exploration Based On Heuristic Measure in Deep Q-Network', 'abstract': 'Existing exploration strategies in reinforcement learning (RL) often either ignore the history or feedback of search, or are complicated to implement. There is also a very limited literature showing their effectiveness over diverse domains. We propose an algorithm based on the idea of reannealing, that aims at encouraging exploration only when it is needed, for example, when the algorithm detects that the agent is stuck in a local optimum. The approach is simple to implement. We perform an illustrative case study showing that it has potential to both accelerate training and obtain a better policy.', 'corpus_id': 222067084, 'score': 0}, {'doc_id': '221340812', 'title': 'The Advantage Regret-Matching Actor-Critic', 'abstract': ""Regret minimization has played a key role in online learning, equilibrium computation in games, and reinforcement learning (RL). In this paper, we describe a general model-free RL method for no-regret learning based on repeated reconsideration of past behavior. We propose a model-free RL algorithm, the AdvantageRegret-Matching Actor-Critic (ARMAC): rather than saving past state-action data, ARMAC saves a buffer of past policies, replaying through them to reconstruct hindsight assessments of past behavior. These retrospective value estimates are used to predict conditional advantages which, combined with regret matching, produces a new policy. In particular, ARMAC learns from sampled trajectories in a centralized training setting, without requiring the application of importance sampling commonly used in Monte Carlo counterfactual regret (CFR) minimization; hence, it does not suffer from excessive variance in large environments. In the single-agent setting, ARMAC shows an interesting form of exploration by keeping past policies intact. In the multiagent setting, ARMAC in self-play approaches Nash equilibria on some partially-observable zero-sum benchmarks. We provide exploitability estimates in the significantly larger game of betting-abstracted no-limit Texas Hold'em."", 'corpus_id': 221340812, 'score': 0}, {'doc_id': '212717881', 'title': 'Analysis of Hyper-Parameters for Small Games: Iterations or Epochs in Self-Play?', 'abstract': 'The landmark achievements of AlphaGo Zero have created great research interest into self-play in reinforcement learning. In self-play, Monte Carlo Tree Search is used to train a deep neural network, that is then used in tree searches. Training itself is governed by many hyperparameters.There has been surprisingly little research on design choices for hyper-parameter values and loss-functions, presumably because of the prohibitive computational cost to explore the parameter space. In this paper, we investigate 12 hyper-parameters in an AlphaZero-like self-play algorithm and evaluate how these parameters contribute to training. We use small games, to achieve meaningful exploration with moderate computational effort. The experimental results show that training is highly sensitive to hyper-parameter choices. Through multi-objective analysis we identify 4 important hyper-parameters to further assess. To start, we find surprising results where too much training can sometimes lead to lower performance. Our main result is that the number of self-play iterations subsumes MCTS-search simulations, game-episodes, and training epochs. The intuition is that these three increase together as self-play iterations increase, and that increasing them individually is sub-optimal. A consequence of our experiments is a direct recommendation for setting hyper-parameter values in self-play: the overarching outer-loop of self-play iterations should be maximized, in favor of the three inner-loop hyper-parameters, which should be set at lower values. A secondary result of our experiments concerns the choice of optimization goals, for which we also provide recommendations.', 'corpus_id': 212717881, 'score': 1}, {'doc_id': '222124941', 'title': 'Neural Thompson Sampling', 'abstract': 'Thompson Sampling (TS) is one of the most effective algorithms for solving contextual multi-armed bandit problems. In this paper, we propose a new algorithm, called Neural Thompson Sampling, which adapts deep neural networks for both exploration and exploitation. At the core of our algorithm is a novel posterior distribution of the reward, where its mean is the neural network approximator, and its variance is built upon the neural tangent features of the corresponding neural network. We prove that, provided the underlying reward function is bounded, the proposed algorithm is guaranteed to achieve a cumulative regret of $\\mathcal{O}(T^{1/2})$, which matches the regret of other contextual bandit algorithms in terms of total round number $T$. Experimental comparisons with other benchmark bandit algorithms on various data sets corroborate our theory.', 'corpus_id': 222124941, 'score': 0}, {'doc_id': '221971149', 'title': 'Best Policy Identification in discounted MDPs: Problem-specific Sample Complexity', 'abstract': 'We investigate the problem of best-policy identification in discounted Markov Decision Processes (MDPs) with finite state and action spaces. We assume that the agent has access to a generative model and that the MDP possesses a unique optimal policy. In this setting, we derive a problem-specific lower bound of the sample complexity satisfied by any learning algorithm. This lower bound corresponds to an optimal sample allocation that solves a non-convex program, and hence, is hard to exploit in the design of efficient algorithms. We provide a simple and tight upper bound of the sample complexity lower bound, whose corresponding nearly-optimal sample allocation becomes explicit. The upper bound depends on specific functionals of the MDP such as the sub-optimal gaps and the variance of the next-state value function, and thus really summarizes the hardness of the MDP. We devise KLB-TS (KL Ball Track-and-Stop), an algorithm tracking this nearly-optimal allocation, and provide asymptotic guarantees for its sample complexity (both almost surely and in expectation). The advantages of KLB-TS against state-of-the-art algorithms are finally discussed.', 'corpus_id': 221971149, 'score': 0}, {'doc_id': '230104155', 'title': 'Spectral bandits', 'abstract': 'Smooth functions on graphs have wide applications in manifold and semi-supervised learning. In this work, we study a bandit problem where the payoffs of arms are smooth on a graph. This framework is suitable for solving online learning problems that involve graphs, such as content-based recommendation. In this problem, each item we can recommend is a node of an undirected graph and its expected rating is similar to the one of its neighbors. The goal is to recommend items that have high expected ratings. We aim for the algorithms where the cumulative regret with respect to the optimal policy would not scale poorly with the number of nodes. In particular, we introduce the notion of an effective dimension, which is small in real-world graphs, and propose three algorithms for solving our problem that scale linearly and sublinearly in this dimension. Our experiments on content recommendation problem show that a good estimator of user preferences for thousands of items can be learned from just tens of node evaluations.', 'corpus_id': 230104155, 'score': 1}, {'doc_id': '222310112', 'title': 'Provable Benefits of Representation Learning in Linear Bandits', 'abstract': 'We study how representation learning can improve the efficiency of bandit problems. We study the setting where we play $T$ linear bandits with dimension $d$ concurrently, and these $T$ bandit tasks share a common $k (\\ll d)$ dimensional linear representation. For the finite-action setting, we present a new algorithm which achieves $\\widetilde{O}(T\\sqrt{kN} + \\sqrt{dkNT})$ regret, where $N$ is the number of rounds we play for each bandit. When $T$ is sufficiently large, our algorithm significantly outperforms the naive algorithm (playing $T$ bandits independently) that achieves $\\widetilde{O}(T\\sqrt{d N})$ regret. We also provide an $\\Omega(T\\sqrt{kN} + \\sqrt{dkNT})$ regret lower bound, showing that our algorithm is minimax-optimal up to poly-logarithmic factors. Furthermore, we extend our algorithm to the infinite-action setting and obtain a corresponding regret bound which demonstrates the benefit of representation learning in certain regimes. We also present experiments on synthetic and real-world data to illustrate our theoretical findings and demonstrate the effectiveness of our proposed algorithms.', 'corpus_id': 222310112, 'score': 0}]"
120	{'doc_id': '231979519', 'title': 'A Deep Graph Wavelet Convolutional Neural Network for Semi-supervised Node Classification', 'abstract': 'Graph convolutional neural network provides good solutions for node classification and other tasks with non-Euclidean data. There are several graph convolutional models that attempt to develop deep networks but do not cause serious over-smoothing at the same time. Considering that the wavelet transform generally has a stronger ability to extract useful information than the Fourier transform, we propose a new deep graph wavelet convolutional network (DeepGWC) for semi-supervised node classification tasks. Based on the optimized static filtering matrix parameters of vanilla graph wavelet neural networks and the combination of Fourier bases and wavelet ones, DeepGWC is constructed together with the reuse of residual connection and identity mappings in network architectures. Extensive experiments on three benchmark datasets including Cora, Citeseer, and Pubmed are conducted. The experimental results demonstrate that our DeepGWC outperforms existing graph deep models with the help of additional wavelet bases and achieves new state-of-the-art performances eventually.', 'corpus_id': 231979519}	6078	"[{'doc_id': '219981522', 'title': 'Data Augmentation View on Graph Convolutional Network and the Proposal of Monte Carlo Graph Learning', 'abstract': ""Today, there are two major understandings for graph convolutional networks, i.e., in the spectral and spatial domain. But both lack transparency. In this work, we introduce a new understanding for it -- data augmentation, which is more transparent than the previous understandings. Inspired by it, we propose a new graph learning paradigm -- Monte Carlo Graph Learning (MCGL). The core idea of MCGL contains: (1) Data augmentation: propagate the labels of the training set through the graph structure and expand the training set; (2) Model training: use the expanded training set to train traditional classifiers. We use synthetic datasets to compare the strengths of MCGL and graph convolutional operation on clean graphs. In addition, we show that MCGL's tolerance to graph structure noise is weaker than GCN on noisy graphs (four real-world datasets). Moreover, inspired by MCGL, we re-analyze the reasons why the performance of GCN becomes worse when deepened too much: rather than the mainstream view of over-smoothing, we argue that the main reason is the graph structure noise, and experimentally verify our view. The code is available at this https URL."", 'corpus_id': 219981522, 'score': 1}, {'doc_id': '219259832', 'title': 'Learning to Branch for Multi-Task Learning', 'abstract': 'Training multiple tasks jointly in one deep network yields reduced latency during inference and better performance over the single-task counterpart by sharing certain layers of a network. However, over-sharing a network could erroneously enforce over-generalization, causing negative knowledge transfer across tasks. Prior works rely on human intuition or pre-computed task relatedness scores for ad hoc branching structures. They provide sub-optimal end results and often require huge efforts for the trial-and-error process. In this work, we present an automated multi-task learning algorithm that learns where to share or branch within a network, designing an effective network topology that is directly optimized for multiple objectives across tasks. Specifically, we propose a novel tree-structured design space that casts a tree branching operation as a gumbel-softmax sampling procedure. This enables differentiable network splitting that is end-to-end trainable. We validate the proposed method on controlled synthetic data, CelebA, and Taskonomy.', 'corpus_id': 219259832, 'score': 1}, {'doc_id': '219720969', 'title': 'MetaSDF: Meta-learning Signed Distance Functions', 'abstract': 'Neural implicit shape representations are an emerging paradigm that offers many potential benefits over conventional discrete representations, including memory efficiency at a high spatial resolution. Generalizing across shapes with such neural implicit representations amounts to learning priors over the respective function space and enables geometry reconstruction from partial or noisy observations. Existing generalization methods rely on conditioning a neural network on a low-dimensional latent code that is either regressed by an encoder or jointly optimized in the auto-decoder framework. Here, we formalize learning of a shape space as a meta-learning problem and leverage gradient-based meta-learning algorithms to solve this task. We demonstrate that this approach performs on par with auto-decoder based approaches while being an order of magnitude faster at test-time inference. We further demonstrate that the proposed gradient-based method outperforms encoder-decoder based methods that leverage pooling-based set encoders.', 'corpus_id': 219720969, 'score': 0}, {'doc_id': '229188972', 'title': 'Hierarchical Graph Capsule Network', 'abstract': 'Graph Neural Networks (GNNs) draw their strength from explicitly modeling the topological information of structured data. However, existing GNNs suffer from limited capability in capturing the hierarchical graph representation which plays an important role in graph classification. In this paper, we innovatively propose hierarchical graph capsule network (HGCN) that can jointly learn node embeddings and extract graph hierarchies. Specifically, disentangled graph capsules are established by identifying heterogeneous factors underlying each node, such that their instantiation parameters represent different properties of the same entity. To learn the hierarchical representation, HGCN characterizes the part-whole relationship between lower-level capsules (part) and higher-level capsules (whole) by explicitly considering the structure information among the parts. Experimental studies demonstrate the effectiveness of HGCN and the contribution of each component.', 'corpus_id': 229188972, 'score': 1}, {'doc_id': '231955136', 'title': 'On the Theory of Implicit Deep Learning: Global Convergence with Implicit Layers', 'abstract': 'A deep equilibrium model uses implicit layers, which are implicitly defined through an equilibrium point of an infinite sequence of computation. It avoids any explicit computation of the infinite sequence by finding an equilibrium point directly via root-finding and by computing gradients via implicit differentiation. In this paper, we analyze the gradient dynamics of deep equilibrium models with nonlinearity only on weight matrices and non-convex objective functions of weights for regression and classification. Despite non-convexity, convergence to global optimum at a linear rate is guaranteed without any assumption on the width of the models, allowing the width to be smaller than the output dimension and the number of data points. Moreover, we prove a relation between the gradient dynamics of the deep implicit layer and the dynamics of trust region Newton method of a shallow explicit layer. This mathematically proven relation along with our numerical observation suggests the importance of understanding implicit bias of implicit layers and an open problem on the topic. Our proofs deal with implicit layers, weight tying and nonlinearity on weights, and differ from those in the related literature.', 'corpus_id': 231955136, 'score': 1}, {'doc_id': '228064565', 'title': 'Multi-Population Phase Oscillator Networks with Higher-Order Interactions.', 'abstract': 'The classical Kuramoto model consists of finitely many pairwise coupled oscillators on the circle. In many applications a simple pairwise coupling is not sufficient to describe real-world phenomena as higher-order (or group) interactions take place. Hence, we replace the classical coupling law with a very general coupling function involving higher-order terms. Furthermore, we allow for multiple populations of oscillators interacting with each other through a very general law. In our analysis, we focus on the characteristic system and the mean-field limit of this generalized class of Kuramoto models. While there are several works studying particular aspects of our program, we propose a general framework to work with all three aspects (higher-order, multi-population, and mean-field) simultaneously. Assuming identical oscillators in each population, we derive equations for the evolution of oscillator populations in the mean-field limit. First, we clarify existence and uniqueness of our set of characteristic equations, which are formulated in the space of probability measures together with the bounded-Lipschitz metric. Then, we investigate dynamical properties within the framework of the characteristic system. We identify invariant subspaces and stability of the state, in which all oscillators are synchronized within each population. Even though it turns out that this so called all-synchronized state is never asymptotically stable, under some conditions and with a suitable definition of stability, the all-synchronized state can be proven to be at least locally stable. In summary, our work provides a rigorous mathematical framework upon which the further study of multi-population higher-order coupled particle systems can be based.', 'corpus_id': 228064565, 'score': 1}, {'doc_id': '219636421', 'title': 'Tangent Space Sensitivity and Distribution of Linear Regions in ReLU Networks', 'abstract': 'Recent articles indicate that deep neural networks are efficient models for various learning problems. However they are often highly sensitive to various changes that cannot be detected by an independent observer. As our understanding of deep neural networks with traditional generalization bounds still remains incomplete, there are several measures which capture the behaviour of the model in case of small changes at a specific state. In this paper we consider adversarial stability in the tangent space and suggest tangent sensitivity in order to characterize stability. We focus on a particular kind of stability with respect to changes in parameters that are induced by individual examples without known labels. We derive several easily computable bounds and empirical measures for feed-forward fully connected ReLU (Rectified Linear Unit) networks and connect tangent sensitivity to the distribution of the activation regions in the input space realized by the network. Our experiments suggest that even simple bounds and measures are associated with the empirical generalization gap.', 'corpus_id': 219636421, 'score': 0}, {'doc_id': '220514221', 'title': 'Alpha-Net: Architecture, Models, and Applications', 'abstract': 'Deep learning network training is usually computationally expensive and intuitively complex. We present a novel network architecture for custom training and weight evaluations. We reformulate the layers as ResNet-similar blocks with certain inputs and outputs of their own, the blocks (called Alpha blocks) on their connection configuration form their own network, combined with our novel loss function and normalization function form the complete Alpha-Net architecture. We provided the empirical mathematical formulation of network loss function for more understanding of accuracy estimation and further optimizations. We implemented Alpha-Net with 4 different layer configurations to express the architecture behavior comprehensively. On a custom dataset based on ImageNet benchmark, we evaluate Alpha-Net v1, v2, v3, and v4 for image recognition to give the accuracy of 78.2%, 79.1%, 79.5%, and 78.3% respectively. The Alpha-Net v3 gives improved accuracy of approx. 3% over the last state-of-the-art network ResNet 50 on ImageNet benchmark. We also present an analysis of our dataset with 256, 512, and 1024 layers and different versions of the loss function. Input representation is also crucial for training as initial preprocessing will take only a handful of features to make training less complex than it needs to be. We also compared network behavior with different layer structures, different loss functions, and different normalization functions for better quantitative modeling of Alpha-Net.', 'corpus_id': 220514221, 'score': 0}, {'doc_id': '213729382', 'title': 'Dynamic Model Pruning with Feedback', 'abstract': 'Deep neural networks often have millions of parameters. This can hinder their deployment to low-end devices, not only due to high memory requirements but also because of increased latency at inference. We propose a novel model compression method that generates a sparse trained model without additional overhead: by allowing (i) dynamic allocation of the sparsity pattern and (ii) incorporating feedback signal to reactivate prematurely pruned weights we obtain a performant sparse model in one single training pass (retraining is not needed, but can further improve the performance). We evaluate the method on CIFAR-10 and ImageNet, and show that the obtained sparse models can reach the state-of-the-art performance of dense models and further that their performance surpasses all previously proposed pruning schemes (that come without feedback mechanisms).', 'corpus_id': 213729382, 'score': 0}, {'doc_id': '231617285', 'title': 'Selective Fine-Tuning on a Classifier Ensemble: Realizing Adaptive Neural Networks With a Diversified Multi-Exit Architecture', 'abstract': 'Adaptive neural networks that provide a trade-off between computing costs and inference performance can be a crucial solution for edge artificial intelligence (AI) computing where resource and energy consumption are significantly constrained. Edge AIs require a fine-tuning technique to achieve target accuracy with less computation for pre-trained models on the cloud. However, a multi-exit network, which realizes adaptive inference costs, requires significant training costs because it has many classifiers that need to be fine-tuned. In this study, we propose a novel fine-tuning method for an ensemble of classifiers that efficiently retrain the multi-exit network. The proposed fine-tuning method exploits individualities by assembling the output of the intermediate classifiers trained with distinct preprocessed data. The evaluation results show that the proposed method achieved 0.2%-5.8%, 0.2%-4.6% higher accuracy with only 77%-93%, 73%-84% training computation compared to the entire fine-tuning of classifiers on the pre-modified CIFAR-100 and Imagenet, respectively, although it depends on assumed edge environments.', 'corpus_id': 231617285, 'score': 1}]"
121	"{'doc_id': '216412528', 'title': 'Long-range interaction of \nLi(2S2)−Li(2S2)−Li+(1S1)', 'abstract': ""The long-range interactions among two- or three-atom systems are of considerable importance in the cold and ultracold research areas for many-body systems. For an ion and an atom, the long-range interaction potential is dominated by the induction (or polarization) potential resulting from the (classical) effect of the ion's electric field on the atom and the leading term of the induction potential is much stronger than the (quantum mechanical) dispersion (or van der Waals) interaction. The present paper focuses on the long-range interaction of the $\\mathrm{Li}(2\\phantom{\\rule{0.16em}{0ex}}^{2}S)\\ensuremath{-}\\mathrm{Li}(2\\phantom{\\rule{0.16em}{0ex}}^{2}S)\\ensuremath{-}{\\mathrm{Li}}^{+}(1\\phantom{\\rule{0.16em}{0ex}}^{1}S)$ system, to see what changes this induction effect (originating in the electric field of the ${\\mathrm{Li}}^{+}$ ion) yields in the long-range additive and nonadditive interactions of this three-body system. Using perturbation theory for energies, we evaluate the coefficients ${C}_{n}$ in the potential energy for the three well-separated constituents, where $n$ refers to the corresponding order in inverse powers of distance, obtaining the additive interaction coefficients ${C}_{4}, {C}_{6}, {C}_{7}, {C}_{8}, {C}_{9}$ and the nonadditive interaction coefficients ${C}_{7}, {C}_{9}$. The obtained coefficients ${C}_{n}$ are calculated with highly accurate variationally generated nonrelativistic wave functions in Hylleraas coordinates. Our calculations may be of interest for the study of three-body recombination and for constructing precise potential energy surfaces. We also provide precise evaluations of the long-range potentials for the two-body $\\mathrm{Li}(2\\phantom{\\rule{0.16em}{0ex}}^{2}S)\\ensuremath{-}{\\mathrm{Li}}^{+}(1\\phantom{\\rule{0.16em}{0ex}}^{1}S)$ system. For both the two-body and three-body cases, we provide results for the like-nuclei cases of $^{6}\\mathrm{Li}$ and $^{7}\\mathrm{Li}$."", 'corpus_id': 216412528}"	7500	[{'doc_id': '232290427', 'title': 'Delta-kick Squeezing', 'abstract': 'We explore the possibility to overcome the standard quantum limit (SQL) in a free-fall atom interferometer using a Bose-Einstein condensate (BEC) in either of the two relevant cases of Bragg or Raman scattering light pulses. The generation of entanglement in the BEC is dramatically enhanced by amplifying the atom-atom interactions via the rapid action of an external trap focusing the matter-waves to significantly increase the atomic densities during a preparation stage – a technique we refer to as delta-kick squeezing (DKS). The action of a second DKS operation at the end of the interferometry sequence allows to implement a non-linear readout scheme making the sub-SQL sensitivity highly robust against imperfect atom counting detection. We predict more than 30 dB of sensitivity gain beyond the SQL for the variance, assuming realistic parameters and 106 atoms.', 'corpus_id': 232290427, 'score': 0}, {'doc_id': '218889749', 'title': 'Transition Strength Measurements to Guide Magic Wavelength Selection in Optically Trapped Molecules.', 'abstract': 'Optical trapping of molecules with long coherence times is crucial for many protocols in quantum information and metrology. However, the factors that limit the lifetimes of the trapped molecules remain elusive and require improved understanding of the underlying molecular structure. Here we show that measurements of vibronic line strengths in weakly and deeply bound ^{88}Sr_{2} molecules, combined with ab\xa0initio calculations, allow for unambiguous identification of vibrational quantum numbers. This, in turn, enables the construction of refined excited potential energy curves, informing the selection of magic wavelengths that facilitate long vibrational coherence. We demonstrate Rabi oscillations between far-separated vibrational states that persist for nearly 100\xa0ms.', 'corpus_id': 218889749, 'score': 0}, {'doc_id': '226289797', 'title': 'Motional quantum states of surface electrons on liquid helium in a tilted magnetic field', 'abstract': 'The theoretical model developed by Jaynes and Cummings to describe interaction between light and a two-level atom became a paradigm of atomic and optical physics. Here, the authors demonstrate that this model can be realized in a pure condensed matter system of electrons trapped on the surface of liquid helium. In particular, they show that the motional states of such an electron, when it is placed in a strong magnetic field, realize a prototypical quantum system that exhibits a variety of phenomena common to the light dressed states of atomic and molecular systems.', 'corpus_id': 226289797, 'score': 1}, {'doc_id': '213126278', 'title': 'Destiny of optical lattices with strong intersite interactions', 'abstract': 'Optical lattices are considered loaded by atoms or molecules that can exhibit strong interactions between different lattice sites. The strength of these interactions can be sufficient for generating collective phonon excitations above the ground-state energy level. Varying the interaction strength makes it possible to create several equilibrium three-dimensional phases, including conducting optical lattices, insulating optical lattices, delocalized quantum crystals, and localized quantum crystals. Also, there can exist finite one- and two-dimensional lattices of chains and planes.', 'corpus_id': 213126278, 'score': 1}, {'doc_id': '219635997', 'title': 'Fast multi-qubit gates by adiabatic evolution in interacting excited state manifolds.', 'abstract': 'Quantum computing and quantum simulation can be implemented by concatenation of one- and two-qubit gates and interactions. For most physical implementations, however, it may be advantageous to explore state components and interactions that depart from this universal paradigm and offer faster or more robust access to more advanced operations on the system. In this article, we show that adiabatic passage along the dark eigenstate of excitation exchange interactions can be used to implement fast multi-qubit Toffoli (C$_k$-NOT) and fan-out (C-NOT$^k$) gates. This mechanism can be realized by simultaneous excitation of atoms to Rydberg levels, featuring resonant exchange interaction. Our theoretical estimates and numerical simulations show that these multi-qubit Rydberg gates are possible with errors below 1% for up to 20 qubits. The excitation exchange mechanism is ubiquitous across experimental platforms and we show that similar multi-qubit gates can be implemented in superconducting circuits.', 'corpus_id': 219635997, 'score': 1}, {'doc_id': '220646452', 'title': 'Bound states of an ultracold atom interacting with a set of stationary impurities', 'abstract': 'In this manuscript we analyse properties of bound states of an atom interacting with a set of static impurities. We begin with the simplest system of a single atom interacting with two static impurities. We consider two types of atom-impurity interaction: (i) zero-range potential represented by regularized delta, (ii) more realistic polarization potential, representing long-range part of the atom-ion interaction. For the former we obtain analytical results for energies of bound states. For the latter we perform numerical calculations based on the application of finite element method. Then, we move to the case of a single atom interacting with one-dimensional (1D) infinite chain of static ions. Such a setup resembles Kronig-Penney model of a 1D crystalline solid, where energy spectrum exhibits band structure behaviour. For this system, we derive analytical results for the band structure of bound states assuming regularized delta interaction, and perform numerical calculations, considering polarization potential to model atom-impurity interaction. Both approaches agree quite well when separation between impurities is much larger than characteristic range of the interaction potential.', 'corpus_id': 220646452, 'score': 1}, {'doc_id': '221222207', 'title': 'Ultra-sensitive and label-free detection of the measles virus using an N-heterocyclic carbene-based electrochemical biosensor.', 'abstract': 'With the current intense need for the rapid and accurate detection of viruses due to COVID-19 , we report on a platform technology that is very well suited for this purpose, using intact measles virus as a demonstration. Cases of infection due to the measles virus are rapidly increasing and yet current diagnostic tools used to monitor for the virus rely on slow (>1 hour) technologies. Here, we demonstrate the first biosensor capable of detecting the measles virus in minutes with no preprocessing steps. The key sensing element is an electrode coated with a self-assembled monolayer containing the measles antibody, immobilized through an N-heterocyclic carbene (NHC). The intact virus is detected by changes in resistance, giving a linear response to 10-100 µg/mL of the intact measles virus without the need to label or process the sample. The limit of detection is 6 µg/mL, which is at the lower limit of concentrations that can cause infections in primates. The NHC-based biosensors are shown to be superior to thiol-based systems, producing an approximately 10x larger response and significantly greater stability towards repeated measurements and long-term storage. This NHC-based biosensor thus represents an important development for both the rapid detection of the measles virus and as a platform technology for the detection of other biological targets of interest.', 'corpus_id': 221222207, 'score': 0}, {'doc_id': '221217526', 'title': 'COVID-19 lockdown and its impact on tropospheric NO2 concentrations over India using satellite-based data', 'abstract': '\n Abstract\n \n The World Health Organization has declared the COVID-19 pandemic a global public health emergency. Many countries of the world, including India, closed their borders and imposed a nationwide lockdown. In India, the lockdown was declared on March 24 for 21 days (March 25–April 14, 2020) and was later extended until May 3, 2020. During the lockdown, all major anthropogenic activities, which contribute to atmospheric pollution (such as industries, vehicles, and businesses), were restricted. The current study examines the impact of the lockdown on tropospheric NO2 concentrations. Satellite-based ozone monitoring instrument sensor data were analyzed in order to investigate the variations in tropospheric NO2 concentrations. The results showed that from March 1 to 21, 2020, the average tropospheric NO2 concentration was 214.4 ×1013 molecule cm-2 over India, and it subsequently decreased by 12.1% over the next four weeks. An increase of 0.8% in tropospheric NO2 concentrations was observed for the same period in 2019 and hence, the reduced tropospheric NO2 concentrations can be attributed to restricted anthropogenic activities during the lockdown. In the absence of significant activities, the contribution of various sources was estimated, and the emissions from biomass burning were identified as a major source of tropospheric NO2 during the lockdown. The findings of this study provide an opportunity to understand the mechanism of tropospheric NO2 emissions over India, in order to improve air quality modeling and management strategies.\n \n', 'corpus_id': 221217526, 'score': 0}, {'doc_id': '233423310', 'title': 'Strain and pseudo-magnetic fields in optical lattices from density-assisted tunneling', 'abstract': 'Applying time-periodic modulations is routinely used to control and design synthetic matter in quantum-engineered settings. In lattice systems, this approach is explored to engineer band structures with non-trivial topological properties, but also to generate exotic interaction processes. A prime example is density-assisted tunneling, by which the hopping amplitude of a particle between neighboring sites explicitly depends on their respective occupations. Here, we show how densityassisted tunneling can be tailored in view of simulating the effects of strain in synthetic graphene-type systems. Specifically, we consider a mixture of two atomic species on a honeycomb optical lattice: one species forms a Bose-Einstein condensate in an anisotropic harmonic trap, whose inhomogeneous density profile induces an effective uniaxial strain for the second species through density-assisted tunneling processes. In direct analogy with strained graphene, the second species experiences a pseudo magnetic field, hence exhibiting relativistic Landau levels and the valley Hall effect. Our proposed scheme introduces a unique platform for the investigation of strain-induced gauge fields and their possible interplay with quantum fluctuations and collective excitations.', 'corpus_id': 233423310, 'score': 1}, {'doc_id': '33911627', 'title': 'Universal three-body physics for fermionic dipoles.', 'abstract': 'Our present study of the universal physics for three oriented fermionic dipoles in the hyperspherical adiabatic representation predicts a single long-lived three-dipole state, which exists in only one three-body symmetry and forms near a two-dipole resonance. Our analysis reveals the spatial configuration of the universal state and the scaling of its binding energy and lifetime with the strength of the dipolar interaction. In addition, three-body recombination of fermionic dipoles is found to be important even at ultracold energies. An additional finding is that an effective long-range repulsion arises between a dipole and a dipolar dimer that is tunable via dipolar interactions.', 'corpus_id': 33911627, 'score': 1}]
122	{'doc_id': '139162222', 'title': 'Determination Optimum Inventory Level for Material Using Genetic Algorithm', 'abstract': 'The integration of decision-making will lead to the robust of its decisions, and then determination optimum inventory level to the required materials to produce and reduce the total cost by the cooperation of purchasing department with inventory department and also with other company,s departments. Two models are suggested to determine Optimum Inventory Level (OIL), the first model (OIL-model 1) assumed that the inventory level for materials quantities equal to the required materials, while the second model (OIL-model 2) assumed that the inventory level for materials quantities more than the required materials for the next period.\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0 \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0This study was applied in Wasit Company for Textile Manufacturing in the Textile Factory, where it produces five products, which are printed striped, plain, poplin, dyed poplin and Naba weave.\xa0 The products are made from cotton and they are passing through several stages to transfer to the final product. A genetic algorithm is used to determine the optimum quantity of the purchase a cotton and colors for each month and with minimum cost. Where the purchasing and transportation costs were either constant or variable with respect to purchased quantities while holding cost is kept constant. The results showed that the total cost of the first model is minimum than the second model because the holding cost for this model is less from the second model, while the purchasing and transportation costs from two models are equals. The percentage of purchasing cost for cotton is the biggest value, more 99% of purchasing cost for two models.', 'corpus_id': 139162222}	20633	"[{'doc_id': '44095901', 'title': 'Refining and implementing the Food Assortment Scoring Tool (FAST) in food pantries', 'abstract': 'Abstract Objective Hunger relief agencies have a limited capacity to monitor the nutritional quality of their food. Validated measures of food environments, such as the Healthy Eating Index-2010 (HEI-2010), are challenging to use due to their time intensity and requirement for precise nutrient information. A previous study used out-of-sample predictions to demonstrate that an alternative measure correlated well with the HEI-2010. The present study revised the Food Assortment Scoring Tool (FAST) to facilitate implementation and tested the tool’s performance in a real-world food pantry setting. Design We developed a FAST measure with thirteen scored categories and thirty-one sub-categories. FAST scores were generated by sorting and weighing foods in categories, multiplying each category’s weight share by a healthfulness parameter and summing the categories (range 0–100). FAST was implemented by recording all food products moved over five days. Researchers collected FAST and HEI-2010 scores for food availability and foods selected by clients, to calculate correlations. Setting Five food pantries in greater Minneapolis/St. Paul, Minnesota, USA. Subjects Food carts of sixty food pantry clients. Results The thirteen-category FAST correlated well with the HEI-2010 in prediction models (r = 0·68). FAST scores averaged 61·5 for food products moved, 63·8 for availability and 62·5 for client carts. As implemented in the real world, FAST demonstrated good correlation with the HEI-2010 (r = 0·66). Conclusions The FAST is a flexible, valid tool to monitor the nutritional quality of food in pantries. Future studies are needed to test its use in monitoring improvements in food pantry nutritional quality over time.', 'corpus_id': 44095901, 'score': 1}, {'doc_id': '235754383', 'title': 'Opportunities for and Limits to Cooperation between School and Families in Sustainable Public Food Procurement', 'abstract': 'This paper describes a research project, carried out in an Italian public school, to assess whether parents were willing to take part in food procurement decisions, as well as their ability to accurately predict what foods children would pick at school lunch and their propensity to support sustainable food choices made by the school. The methodology included a questionnaire, issued to 500 parents, and an in-depth study of 138 child/parent pairs. The study comprised: (i) presentation of an innovative recipe in the weekly menu of the school canteen;(ii) meal observations of children’s intake at school lunch during the week of the menu modification;(iii) collection of both parents’ and children’s reports on their choices of recipes from the modified weekly menu. The results are commented in light of two important changes that have recently affected Italian public school food procurement: the opening of school canteens to lunches brought from home and the measures adopted since 2020 to contain the COVID-19 pandemic. Both events go in the direction of delegating to parents the multifaceted role of the school in the food arena. The article concludes that the results of the study should discourage this approach. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.', 'corpus_id': 235754383, 'score': 0}, {'doc_id': '236923502', 'title': 'Readiness of the South African Agricultural Sector to Implement IoT', 'abstract': 'As the world’s population increases, so does the demand for food. This demand for food in turn puts pressure on agriculture in many countries. The impact of climate change on the environment has made it difficult to produce food that may be necessary to accommodate the growing population. Due to these concerns, the agriculture sector is forced to move towards more efficient and sustainable methods of farming to increase productivity. There is evidence that the use of technology in agriculture has the potential to improve food production and food sustainability; thereby addressing the concerns of food security. The Internet of Things (IoT) has been suggested as a potential tool for farmers to overcome the impact of climate change on food security. However, there is dearth of research on the readiness of implementing IoT in South Africa’s agricultural sector. Therefore, this research aims to explore the readiness of the agricultural sector of South Africa for a wide implementation of IoT. This research conducts a desktop study through the lens of the PEST framework on the special case of South Africa. A thematic literature and documents review was deployed to examine the political, economic, societal and technological factors that may facilitate or impede the implementation of IoT in the agricultural sectors of South Africa. The findings suggest that the wide ranging political, economic, societal and technological constructs enable the implementation of IoT within South Africa’s agricultural sector. The most important include current policies, technological infrastructure, access to internet, and mobile technology which places South Africa in a good position to implement IoT in agriculture.', 'corpus_id': 236923502, 'score': 0}, {'doc_id': '233331610', 'title': ""Inventory Management of the Refrigerator's Produce Bins Using Classification Algorithms and Hand Analysis"", 'abstract': ""Tracking the inventory of one's refrigerator has been a mission for consumers since the advent of the refrigerator. With the improvement of computer vision capabilities, automatic inventory systems are within reach. One inventory area with many potential benefits is the fresh food produce bins. The bins are a unique storage area due to their deep size. A user cannot easily see what is in the bins without opening the drawer. Produce items are also some of the quickest foods in the refrigerator to spoil, despite being temperature and humidity controlled to have the fruits and vegetables last longer. This research addresses the challenges presented by getting a full inventory of all items within the produce bins by observing if the hand can provide useful information. The research proposes that all items must go in or out of the refrigerator by the main door, and by using a single camera to observe the hand-object interactions, a more complete inventory list can be created. As an initial proof-of-concept, two types of produce, an apple and an orange, will be used as a testing ground. The accuracy of the hand analysis (e.g., hand with apple or orange vs. hand empty) was determined by comparing the model predictions to a 301-frame video with ground truth labels. The hand analysis system was 87% accurate for classifying an empty hand, 85% accurate on a hand holding an apple, and 74% accurate on a hand holding an orange. The system was 93% accurate at detecting what was added or removed from the refrigerator."", 'corpus_id': 233331610, 'score': 1}, {'doc_id': '32218424', 'title': 'Reliability of the ecSatter Inventory as a tool to measure eating competence.', 'abstract': ""OBJECTIVE\nTo examine the reliability of the ecSatter Inventory (ecSI), a measure of eating competence.\n\n\nDESIGN\nSelf-report questionnaires were administered in person or by mail. Retesting occurred 2 to 6 weeks after completion of the first questionnaire.\n\n\nPARTICIPANTS\nBoth administrations of the questionnaire were completed by 259 participants who were mostly food secure, white females with some college education; mean age was 26.9 +/- 10.4 years.\n\n\nMEASURES\nTest-retest reliability and internal consistency.\n\n\nANALYSIS\nSpearman's rank correlation coefficients to estimate test-retest reliability and Cronbach alpha coefficients to estimate internal consistency.\n\n\nRESULTS\nSpearman's rank correlation coefficient for ecSI total score was 0.68; subscale coefficients were 0.70 for eating attitudes, 0.70 for contextual skills, 0.65 for food acceptance, and 0.52 for internal regulation. Cronbach alpha coefficient for ecSI total score was 0.77. Subscale alphas coefficients were 0.80 for eating attitudes, 0.69 for contextual skills, 0.68 for food acceptance, and 0.66 for internal regulation.\n\n\nCONCLUSIONS AND IMPLICATIONS\nThis study provides psychometric evidence about the reliability of ecSI as a measure of eating competence in this sample. Although some ecSI items may require revision, results suggest that the instrument may be used to evaluate nutrition education designed to improve eating competence."", 'corpus_id': 32218424, 'score': 1}, {'doc_id': '237461473', 'title': ""Consumers' Intention Towards Online Food Ordering and Delivery Service"", 'abstract': 'Online food ordering and delivery services are the platforms that provide a system and service to the consumer to order and buy food products from foodservice or restaurant operators. The service is an up-and-coming trend among Millennials and the trend has ballooned with the introduction of movement control order (MCO) due to the pandemic COVID-19. The consumer’s demand towards online food ordering and delivery service has increased markedly due to the prohibition of dine-in at restaurant premises. This study focuses on highlighting the factors that influence consumer’s intention to use online food ordering and delivery services. The survey questionnaires were distributed among 384 respondents that represent the customers of restaurants in Shah Alam, Selangor. Data analyses were conducted using SPSS and multiple regression analysis. Findings revealed that usefulness, ease of use, and consumer’s enjoyment are the factors that influence consumer’s intention to use online food ordering services and conclusively, usefulness is the most significant factor that affects consumer’s intention to order food via online food ordering and delivery service. The findings from this study are beneficial for foodservice or restaurant operators in improving their businesses and staying competitive. Recommendations for future studies are included based on the findings of this study.', 'corpus_id': 237461473, 'score': 0}, {'doc_id': '73482319', 'title': 'A systematic review of food pantry-based interventions in the USA', 'abstract': 'Abstract Objective Food pantries play a critical role in combating food insecurity. The objective of the present work was to systematically review and synthesize scientific evidence regarding the effectiveness of food pantry-based interventions in the USA. Design Keyword/reference search was conducted in PubMed, Web of Science, Scopus, Cochrane Library and CINAHL for peer-reviewed articles published until May 2018 that met the following criteria. Setting: food pantry and/or food bank in the USA; study design: randomized controlled trial (RCT) or pre–post study; outcomes: diet-related outcomes (e.g. nutrition knowledge, food choice, food security, diet quality); study subjects: food pantry/bank clients. Results Fourteen articles evaluating twelve distinct interventions identified from the keyword/reference search met the eligibility criteria and were included in the review. Five were RCT and the remaining seven were pre–post studies. All studies found that food pantry-based interventions were effective in improving participants’ diet-related outcomes. In particular, the nutrition education interventions and the client-choice intervention enhanced participants’ nutrition knowledge, cooking skills, food security status and fresh produce intake. The food display intervention helped pantry clients select healthier food items. The diabetes management intervention reduced participants’ glycaemic level. Conclusions Food pantry-based interventions were found to be effective in improving participants’ diet-related outcomes. Interventions were modest in scale and usually short in follow-up duration. Future studies are warranted to address the challenges of conducting interventions in food pantries, such as shortage in personnel and resources, to ensure intervention sustainability and long-term effectiveness.', 'corpus_id': 73482319, 'score': 1}, {'doc_id': '237382971', 'title': 'SIMULATION OF A SCHOOL CANTEEN TO UNDERSTAND MEAL DURATION IMPACT ON FOOD WASTE', 'abstract': 'A system simulation is a one of the approaches to understand business processes or to explain them to other people. It is an excellent decision making solution to provide data-driven conclusions based on system modelling and experiments. This paper proposes simulation results of a school canteen. The aim of the research was to investigate the relation between a food waste amount and meal time duration. The proposed simulation was based on business process analysis, business process modelling, a Monte Carlo method and expert knowledge. The frequency distributions were constructed based on children meal duration observation completed by their mothers. It is a magnificent citizen science solution to involve mothers in the research because they can additionally better understand their children meal preferences and habits. Therefore, a questionnaire for citizens was developed, which can be applied to collect statistical data for model accuracy improvement and extension.', 'corpus_id': 237382971, 'score': 0}, {'doc_id': '168258485', 'title': 'The Impact of Computer Use on Food Service Organizations', 'abstract': ""An investigation was conducted in college and university food services to assess the present situation and probable future develop\xad ments in the use of computers as well as to identify and evaluate the effect of computer use on employee skill requirements and employment trends. Results were obtained by two mail questionnaires. The first questionnaire inquired about the present and future use of the com\xad puter. The second questionnaire was used to evaluate the effect of computer utilization on employee skill requirements and labor trends. Eighty-five or 40% of the college and university food service direc\xad tors �esponded to the questionnaire. Forty-seven percent, or forty of the respondents did not utilize the computer. Fifty-three percent or forty-five of the respondents did utilize the computer. Twelve of the food service facilities uti\xad lizing the computer were making use of output pertaining to accounting transactions only. Food service employees of these facilities were not directly involved with the utilization of the computer. Computer functions least utilized and showing little potential for utilization, were sophisticated decision making functions such as simulation models and menu planning. Accounting and inventory func\xad tions were utilized to the greatest extent. Functions showing the most potential for future implementation were reports pertaining to production, sales and cost analysis, purchasing, and the use of stan\xad dardized recipes. Correlations were made between: (1) the number of meals served per day and the use of the computer; and (2) the number of meals iii served per day and the number of functions utilized. Results indi\xad cated that a significant correlation did not exist for either rela\xad tionship at a significance level of . 05. iv Overall, there was an upgrading of skill requirements for the non-supervisory, supervisory, and administrative p'ersonnel. Although overall skill levels were rated as having increased for supervisory and administrative personnel only. Results indicated that there was no routinization, and no elimination of jobs for the non-supervisory employee. Routine supervisory tasks were eliminated as a result of utilization. Supervisory employees were felt to have more free time. Job elimination did not result at the supervisory level. Administra\xad tive decision making activities were shown to have increased. Routine tasks had been eliminated due to the use of the computer and there was no indication of job elimination at this level."", 'corpus_id': 168258485, 'score': 1}, {'doc_id': '238214890', 'title': 'Title Lean 6 S in Food Production . HACCP as a benchmark for the sixth S "" Safety ""', 'abstract': 'This article presents the integration of lean 6S methodologies and hazard analysis and critical control points (HACCP) in the food production sector. Through the study, it is seen that non-food industrial production is not very different from that of food and, in many cases, it assimilates protocols and ideas that are already working in the food industry; Such is the case of risk analysis, critical control points or hygiene, which are part of the food production protocol and, increasingly, of the industry in general. After the integrative analysis, the article proposes a common lean 6S HACCP model, which can be used both in food production and in non-food industrial production.', 'corpus_id': 238214890, 'score': 0}]"
123	{'doc_id': '215416176', 'title': 'Understanding Knowledge Gaps in Visual Question Answering: Implications for Gap Identification and Testing', 'abstract': 'Traditional Visual Question Answering (VQA) datasets typically contain questions related to the spatial information of objects, object attributes, or general scene questions. Recently, researchers have recognized the need to improve the balance of such datasets to reduce the system’s dependency on memorized linguistic features and statistical biases, while aiming for enhanced visual understanding. However, it is unclear whether any latent patterns exist to quantify and explain these failures. As an initial step towards better quantifying our understanding of the performance of VQA models, we use a taxonomy of Knowledge Gaps (KGs) to tag questions with one or more types of KGs. Each KG describes the reasoning abilities needed to arrive at a resolution, and failure to resolve gaps indicates an absence of the required reasoning ability. After identifying KGs for each question, we examine the skew in the distribution of questions for each KG. We then introduce a targeted question generation model to reduce this skew, which allows us to generate new types of questions for an image.', 'corpus_id': 215416176}	706	"[{'doc_id': '216868342', 'title': 'Question Rewriting for Conversational Question Answering', 'abstract': 'Conversational question answering (QA) requires the ability to correctly interpret a question in the context of previous conversation turns. We address the conversational QA task by decomposing it into question rewriting and question answering subtasks. The question rewriting (QR) subtask is specifically designed to reformulate ambiguous questions, which depend on the conversational context, into unambiguous questions that can be correctly interpreted outside of the conversational context. We introduce a conversational QA architecture that sets the new state of the art on the TREC CAsT 2019 passage retrieval dataset. Moreover, we show that the same QR model improves QA performance on the QuAC dataset with respect to answer span extraction, which is the next step in QA after passage retrieval. Our evaluation results indicate that the QR model we proposed achieves near human-level performance on both datasets and the gap in performance on the end-to-end conversational QA task is attributed mostly to the errors in QA.', 'corpus_id': 216868342, 'score': 1}, {'doc_id': '215768725', 'title': 'A Simple Yet Strong Pipeline for HotpotQA', 'abstract': 'State-of-the-art models for multi-hop question answering typically augment large-scale language models like BERT with additional, intuitively useful capabilities such as named entity recognition, graph-based reasoning, and question decomposition. However, does their strong performance on popular multi-hop datasets really justify this added design complexity? Our results suggest that the answer may be no, because even our simple pipeline based on BERT, named Quark, performs surprisingly well. Specifically, on HotpotQA, Quark outperforms these models on both question answering and support identification (and achieves performance very close to a RoBERTa model). Our pipeline has three steps: 1) use BERT to identify potentially relevant sentences independently of each other; 2) feed the set of selected sentences as context into a standard BERT span prediction model to choose an answer; and 3) use the sentence selection model, now with the chosen answer, to produce supporting sentences. The strong performance of Quark resurfaces the importance of carefully exploring simple model designs before using popular benchmarks to justify the value of complex techniques.', 'corpus_id': 215768725, 'score': 0}, {'doc_id': '219559147', 'title': 'Unsupervised Paraphrase Generation using Pre-trained Language Models', 'abstract': ""Large scale Pre-trained Language Models have proven to be very powerful approach in various Natural language tasks. OpenAI's GPT-2 \\cite{radford2019language} is notable for its capability to generate fluent, well formulated, grammatically consistent text and for phrase completions. In this paper we leverage this generation capability of GPT-2 to generate paraphrases without any supervision from labelled data. We examine how the results compare with other supervised and unsupervised approaches and the effect of using paraphrases for data augmentation on downstream tasks such as classification. Our experiments show that paraphrases generated with our model are of good quality, are diverse and improves the downstream task performance when used for data augmentation."", 'corpus_id': 219559147, 'score': 1}, {'doc_id': '214611567', 'title': 'Fast Cross-domain Data Augmentation through Neural Sentence Editing', 'abstract': 'Data augmentation promises to alleviate data scarcity. This is most important in cases where the initial data is in short supply. This is, for existing methods, also where augmenting is the most difficult, as learning the full data distribution is impossible. For natural language, sentence editing offers a solution - relying on small but meaningful changes to the original ones. Learning which changes are meaningful also requires large amounts of training data. We thus aim to learn this in a source domain where data is abundant and apply it in a different, target domain, where data is scarce - cross-domain augmentation. \nWe create the Edit-transformer, a Transformer-based sentence editor that is significantly faster than the state of the art and also works cross-domain. We argue that, due to its structure, the Edit-transformer is better suited for cross-domain environments than its edit-based predecessors. We show this performance gap on the Yelp-Wikipedia domain pairs. Finally, we show that due to this cross-domain performance advantage, the Edit-transformer leads to meaningful performance gains in several downstream tasks.', 'corpus_id': 214611567, 'score': 0}, {'doc_id': '222133874', 'title': 'Inquisitive Question Generation for High Level Text Comprehension', 'abstract': 'Inquisitive probing questions come naturally to humans in a variety of settings, but is a challenging task for automatic systems. One natural type of question to ask tries to fill a gap in knowledge during text comprehension, like reading a news article: we might ask about background information, deeper reasons behind things occurring, or more. Despite recent progress with data-driven approaches, generating such questions is beyond the range of models trained on existing datasets. \nWe introduce INQUISITIVE, a dataset of ~19K questions that are elicited while a person is reading through a document. Compared to existing datasets, INQUISITIVE questions target more towards high-level (semantic and discourse) comprehension of text. We show that readers engage in a series of pragmatic strategies to seek information. Finally, we evaluate question generation models based on GPT-2 and show that our model is able to generate reasonable questions although the task is challenging, and highlight the importance of context to generate INQUISITIVE questions.', 'corpus_id': 222133874, 'score': 1}, {'doc_id': '218487030', 'title': 'Teaching Machine Comprehension with Compositional Explanations', 'abstract': 'Advances in machine reading comprehension (MRC) rely heavily on the collection of large scale human-annotated examples in the form of (question, paragraph, answer) triples. In contrast, humans are typically able to generalize with only a few examples, relying on deeper underlying world knowledge, linguistic sophistication, and/or simply superior deductive powers. In this paper, we focus on “teaching” machines reading comprehension, using a small number of semi-structured explanations that explicitly inform machines why answer spans are correct. We extract structured variables and rules from explanations and compose neural module teachers that annotate instances for training downstream MRC models. We use learnable neural modules and soft logic to handle linguistic variation and overcome sparse coverage; the modules are jointly optimized with the MRC model to improve final performance. On the SQuAD dataset, our proposed method achieves 70.14% F1 score with supervision from 26 explanations, comparable to plain supervised learning using 1,100 labeled instances, yielding a 12x speed up.', 'corpus_id': 218487030, 'score': 0}, {'doc_id': '218870217', 'title': 'A Question Type Driven and Copy Loss Enhanced Frameworkfor Answer-Agnostic Neural Question Generation', 'abstract': 'The answer-agnostic question generation is a significant and challenging task, which aims to automatically generate questions for a given sentence but without an answer. In this paper, we propose two new strategies to deal with this task: question type prediction and copy loss mechanism. The question type module is to predict the types of questions that should be asked, which allows our model to generate multiple types of questions for the same source sentence. The new copy loss enhances the original copy mechanism to make sure that every important word in the source sentence has been copied when generating questions. Our integrated model outperforms the state-of-the-art approach in answer-agnostic question generation, achieving a BLEU-4 score of 13.9 on SQuAD. Human evaluation further validates the high quality of our generated questions. We will make our code public available for further research.', 'corpus_id': 218870217, 'score': 1}, {'doc_id': '219179320', 'title': 'Question Answering on Scholarly Knowledge Graphs', 'abstract': 'Answering questions on scholarly knowledge comprising text and other artifacts is a vital part of any research life cycle. Querying scholarly knowledge and retrieving suitable answers is currently hardly possible due to the following primary reason: machine inactionable, ambiguous and unstructured content in publications. We present JarvisQA, a BERT based system to answer questions on tabular views of scholarly knowledge graphs. Such tables can be found in a variety of shapes in the scholarly literature (e.g., surveys, comparisons or results). Our system can retrieve direct answers to a variety of different questions asked on tabular data in articles. Furthermore, we present a preliminary dataset of related tables and a corresponding set of natural language questions. This dataset is used as a benchmark for our system and can be reused by others. Additionally, JarvisQA is evaluated on two datasets against other baselines and shows an improvement of two to three folds in performance compared to related methods.', 'corpus_id': 219179320, 'score': 0}, {'doc_id': '216056509', 'title': 'Logical Natural Language Generation from Open-Domain Tables', 'abstract': 'Neural natural language generation (NLG) models have recently shown remarkable progress in fluency and coherence. However, existing studies on neural NLG are primarily focused on surface-level realizations with limited emphasis on logical inference, an important aspect of human thinking and language. In this paper, we suggest a new NLG task where a model is tasked with generating natural language statements that can be logically entailed by the facts in an open-domain semi-structured table. To facilitate the study of the proposed logical NLG problem, we use the existing TabFact dataset~(CITATION) featured with a wide range of logical/symbolic inferences as our testbed, and propose new automatic metrics to evaluate the fidelity of generation models w.r.t. logical inference. The new task poses challenges to the existing monotonic generation frameworks due to the mismatch between sequence order and logical order. In our experiments, we comprehensively survey different generation architectures (LSTM, Transformer, Pre-Trained LM) trained with different algorithms (RL, Adversarial Training, Coarse-to-Fine) on the dataset and made following observations: 1) Pre-Trained LM can significantly boost both the fluency and logical fidelity metrics, 2) RL and Adversarial Training are trading fluency for fidelity, 3) Coarse-to-Fine generation can help partially alleviate the fidelity issue while maintaining high language fluency. The code and data are available at https://github.com/wenhuchen/LogicNLG.', 'corpus_id': 216056509, 'score': 0}, {'doc_id': '235358586', 'title': 'GTM: A Generative Triple-Wise Model for Conversational Question Generation', 'abstract': 'Generating some appealing questions in opendomain conversations is an effective way to improve human-machine interactions and lead the topic to a broader or deeper direction. To avoid dull or deviated questions, some researchers tried to utilize answer, the “future” information, to guide question generation. However, they separate a post-questionanswer (PQA) triple into two parts: postquestion (PQ) and question-answer (QA) pairs, which may hurt the overall coherence. Besides, the QA relationship is modeled as a one-to-one mapping that is not reasonable in open-domain conversations. To tackle these problems, we propose a generative triple-wise model with hierarchical variations for open-domain conversational question generation (CQG). Latent variables in three hierarchies are used to represent the shared background of a triple and one-to-many semantic mappings in both PQ and QA pairs. Experimental results on a largescale CQG dataset show that our method significantly improves the quality of questions in terms of fluency, coherence and diversity over competitive baselines.', 'corpus_id': 235358586, 'score': 1}]"
124	{'doc_id': '220485147', 'title': 'Online Portfolio Selection with Cardinality Constraint and Transaction Costs based on Contextual Bandit', 'abstract': 'Online portfolio selection (OLPS) is a fundamental and challenging problem in financial engineering, which faces two practical constraints during the real trading, i.e., cardinality constraint and non-zero transaction costs. In order to achieve greater feasibility in financial markets, in this paper, we propose a novel online portfolio selection method named LExp4.TCGP with theoretical guarantee of sublinear regret to address the OLPS problem with the two constraints. In addition, we incorporate side information into our method based on contextual bandit, which further improves the effectiveness of our method. Extensive experiments conducted on four representative real-world datasets demonstrate that our method significantly outperforms the state-of-the-art methods when cardinality constraint and non-zero transaction costs co-exist.', 'corpus_id': 220485147}	5117	[{'doc_id': '126202438', 'title': 'Towards forecast techniques for business analysts of large commercial data sets using matrix factorization methods', 'abstract': 'This research article suggests that there are significant benefits in exposing demand planners to forecasting methods using matrix completion techniques. This study aims to contribute to a better understanding of the field of forecasting with multivariate time series prediction by focusing on the dimension of large commercial data sets with hierarchies. This research highlights that there has neither been sufficient academic research in this sub-field nor dissemination among practitioners in the business sector. This study seeks to innovate by presenting a matrix completion method for short-term demand forecast of time series data on relevant commercial problems. Albeit computing intensive, this method outperforms the state of the art while remaining accessible to business users. The object of research is matrix completion for time series in a big data context within the industry. The subject of the research is forecasting product demand using techniques for multivariate hierarchical time series prediction that are both precise and accessible to non-technical business experts. Apart from a methodological innovation, this research seeks to introduce practitioners to novel methods for hierarchical multivariate time series prediction. The research outcome is of interest for organizations requiring precise forecasts yet lacking the appropriate human capital to develop them.', 'corpus_id': 126202438, 'score': 1}, {'doc_id': '227013048', 'title': 'Visual Forecasting of Time Series with Image-to-Image Regression', 'abstract': 'Time series forecasting is essential for agents to make decisions in many domains. Existing models rely on classical statistical methods to predict future values based on previously observed numerical information. Yet, practitioners often rely on visualizations such as charts and plots to reason about their predictions. Inspired by the end-users, we re-imagine the topic by creating a framework to produce visual forecasts, similar to the way humans intuitively do. In this work, we take a novel approach by leveraging advances in deep learning to extend the field of time series forecasting to a visual setting. We do this by transforming the numerical analysis problem into the computer vision domain. Using visualizations of time series data as input, we train a convolutional autoencoder to produce corresponding visual forecasts. We examine various synthetic and real datasets with diverse degrees of complexity. Our experiments show that visual forecasting is effective for cyclic data but somewhat less for irregular data such as stock price. Importantly, we find the proposed visual forecasting method to outperform numerical baselines. We attribute the success of the visual forecasting approach to the fact that we convert the continuous numerical regression problem into a discrete domain with quantization of the continuous target signal into pixel space.', 'corpus_id': 227013048, 'score': 0}, {'doc_id': '227151560', 'title': 'Remaining Useful Life Estimation Under Uncertainty with Causal GraphNets', 'abstract': 'In this work, a novel approach for the construction and training of time series models is presented that deals with the problem of learning on large time series with non-equispaced observations, which at the same time may possess features of interest that span multiple scales. The proposed method is appropriate for constructing predictive models for non-stationary stochastic time series.The efficacy of the method is demonstrated on a simulated stochastic degradation dataset and on a real-world accelerated life testing dataset for ball-bearings. The proposed method, which is based on GraphNets, implicitly learns a model that describes the evolution of the system at the level of a state-vector rather than of a raw observation. The proposed approach is compared to a recurrent network with a temporal convolutional feature extractor head (RNN-tCNN) which forms a known viable alternative for the problem context considered. Finally, by taking advantage of recent advances in the computation of reparametrization gradients for learning probability distributions, a simple yet effective technique for representing prediction uncertainty as a Gamma distribution over remaining useful life predictions is employed.', 'corpus_id': 227151560, 'score': 0}, {'doc_id': '209751283', 'title': 'Forecasting stock prices with long-short term memory neural network based on attention mechanism', 'abstract': 'The stock market is known for its extreme complexity and volatility, and people are always looking for an accurate and effective way to guide stock trading. Long short-term memory (LSTM) neural networks are developed by recurrent neural networks (RNN) and have significant application value in many fields. In addition, LSTM avoids long-term dependence issues due to its unique storage unit structure, and it helps predict financial time series. Based on LSTM and an attention mechanism, a wavelet transform is used to denoise historical stock data, extract and train its features, and establish the prediction model of a stock price. We compared the results with the other three models, including the LSTM model, the LSTM model with wavelet denoising and the gated recurrent unit(GRU) neural network model on S&P 500, DJIA, HSI datasets. Results from experiments on the S&P 500 and DJIA datasets show that the coefficient of determination of the attention-based LSTM model is both higher than 0.94, and the mean square error of our model is both lower than 0.05.', 'corpus_id': 209751283, 'score': 1}, {'doc_id': '218848236', 'title': 'A Score-Driven Conditional Correlation Model for Noisy and Asynchronous Data: An Application to High-Frequency Covariance Dynamics', 'abstract': 'Abstract The analysis of the intraday dynamics of covariances among high-frequency returns is challenging due to asynchronous trading and market microstructure noise. Both effects lead to significant data reduction and may severely affect the estimation of the covariances if traditional methods for low-frequency data are employed. We propose to model intraday log-prices through a multivariate local-level model with score-driven covariance matrices and to treat asynchronicity as a missing value problem. The main advantages of this approach are: (i) all available data are used when filtering the covariances, (ii) market microstructure noise is taken into account, (iii) estimation is performed by standard maximum likelihood. Our empirical analysis, performed on 1-sec NYSE data, shows that opening hours are dominated by idiosyncratic risk and that a market factor progressively emerges in the second part of the day. The method can be used as a nowcasting tool for high-frequency data, allowing to study the real-time response of covariances to macro-news announcements and to build intraday portfolios with very short optimization horizons.', 'corpus_id': 218848236, 'score': 1}, {'doc_id': '221397582', 'title': 'Stochastic Graph Recurrent Neural Network', 'abstract': 'Representation learning over graph structure data has been widely studied due to its wide application prospects. However, previous methods mainly focus on static graphs while many real-world graphs evolve over time. Modeling such evolution is important for predicting properties of unseen networks. To resolve this challenge, we propose SGRNN, a novel neural architecture that applies stochastic latent variables to simultaneously capture the evolution in node attributes and topology. Specifically, deterministic states are separated from stochastic states in the iterative process to suppress mutual interference. With semi-implicit variational inference integrated to SGRNN, a non-Gaussian variational distribution is proposed to help further improve the performance. In addition, to alleviate KL-vanishing problem in SGRNN, a simple and interpretable structure is proposed based on the lower bound of KL-divergence. Extensive experiments on real-world datasets demonstrate the effectiveness of the proposed model. Code is available at this https URL.', 'corpus_id': 221397582, 'score': 1}, {'doc_id': '226299995', 'title': 'Discrete solution pools and noise-contrastive estimation for predict-and-optimize', 'abstract': 'Numerous real-life decision-making processes involve solving a combinatorial optimization problem with uncertain input that can be estimated from historic data. There is a growing interest in decision-focused learning methods, where the loss function used for learning to predict the uncertain input uses the outcome of solving the combinatorial problem over a set of predictions. Different surrogate loss functions have been identified, often using a continuous approximation of the combinatorial problem. However, a key bottleneck is that to compute the loss, one has to solve the combinatorial optimisation problem for each training instance in each epoch, which is computationally expensive even in the case of continuous approximations. \nWe propose a different solver-agnostic method for decision-focused learning, namely by considering a pool of feasible solutions as a discrete approximation of the full combinatorial problem. Solving is now trivial through a single pass over the solution pool. We design several variants of a noise-contrastive loss over the solution pool, which we substantiate theoretically and empirically. Furthermore, we show that by dynamically re-solving only a fraction of the training instances each epoch, our method performs on par with the state of the art, whilst drastically reducing the time spent solving, hence increasing the feasibility of predict-and-optimize for larger problems.', 'corpus_id': 226299995, 'score': 0}, {'doc_id': '227126580', 'title': 'The Interconnectivity Vector: A Finite-Dimensional Vector Representation of Persistent Homology', 'abstract': 'Persistent Homology (PH) is a useful tool to study the underlying structure of a data set. Persistence Diagrams (PDs), which are 2D multisets of points, are a concise summary of the information found by studying the PH of a data set. However, PDs are difficult to incorporate into a typical machine learning workflow. To that end, two main methods for representing PDs have been developed: kernel methods and vectorization methods. In this paper we propose a new finite-dimensional vector, called the interconnectivity vector, representation of a PD adapted from Bag-of-Words (BoW). This new representation is constructed to demonstrate the connections between the homological features of a data set. This initial definition of the interconnectivity vector proves to be unstable, but we introduce a stabilized version of the vector and prove its stability with respect to small perturbations in the inputs. We evaluate both versions of the presented vectorization on several data sets and show their high discriminative power.', 'corpus_id': 227126580, 'score': 1}, {'doc_id': '224814213', 'title': 'Logistic $Q$-Learning', 'abstract': 'We propose a new reinforcement learning algorithm derived from a regularized linear-programming formulation of optimal control in MDPs. The method is closely related to the classic Relative Entropy Policy Search (REPS) algorithm of Peters et al. (2010), with the key difference that our method introduces a Q-function that enables efficient exact model-free implementation. The main feature of our algorithm (called QREPS) is a convex loss function for policy evaluation that serves as a theoretically sound alternative to the widely used squared Bellman error. We provide a practical saddle-point optimization method for minimizing this loss function and provide an error-propagation analysis that relates the quality of the individual updates to the performance of the output policy. Finally, we demonstrate the effectiveness of our method on a range of benchmark problems.', 'corpus_id': 224814213, 'score': 0}, {'doc_id': '227126670', 'title': 'Solving path dependent PDEs with LSTM networks and path signatures', 'abstract': 'Using a combination of recurrent neural networks and signature methods from the rough paths theory we design efficient algorithms for solving parametric families of path dependent partial differential equations (PPDEs) that arise in pricing and hedging of path-dependent derivatives or from use of non-Markovian model, such as rough volatility models in Jacquier and Oumgari, 2019. The solutions of PPDEs are functions of time, a continuous path (the asset price history) and model parameters. As the domain of the solution is infinite dimensional many recently developed deep learning techniques for solving PDEs do not apply. Similarly as in Vidales et al. 2018, we identify the objective function used to learn the PPDE by using martingale representation theorem. As a result we can de-bias and provide confidence intervals for then neural network-based algorithm. We validate our algorithm using classical models for pricing lookback and auto-callable options and report errors for approximating both prices and hedging strategies.', 'corpus_id': 227126670, 'score': 0}]
125	{'doc_id': '59449323', 'title': 'How does purchasing behaviour vary between SME companies and larger companies', 'abstract': 'Background problem: Procurement has been recognised as important to small companies. However, there remains a lack of focus in the literature on procurement for SMEs and purchasing within the smaller firms themselves receives little or no attention. The literature on purchasing practices in SMEs has typically drawn from work on larger firms. Prior research has not sufficiently explored what small firms ‘do’, consequently critiquing SME practices without fully appreciating what these practices are. Models and approaches used to describe an organisation’s position and progress in procurement are focused too much on large organisations and are not sufficiently relevant to SMEs.\n\nPurpose: The purpose of this study is to investigate whether the purchasing behaviour of large enterprises is transferable into the world of Small & Medium Enterprises (SMEs)\n\nMethod: In this thesis secondary data was collected. The study focused on existing literature from various purchasing professionals and SME owner managers to form an in-depth comparison of the different facets of the purchasing department and how they impact upon the overall success of a firm. Data from both SME and large enterprises from different business sectors were compared to obtain a general overview of differing behaviour and how firm size influences this.\n\nFindings: The research reveals that that there are a number of significant factors that differentiate the behaviour of SMEs from those of large enterprises. The major obstacles that SMEs face when trying to adopt the purchasing practises of large enterprises are attributed to: lack of access to resources, management competence, lack of skilled labour, lack of trust amongst suppliers etc.', 'corpus_id': 59449323}	4782	"[{'doc_id': '214714491', 'title': 'Sustainable Banking; Evaluation of the European Business Models', 'abstract': 'Sustainability has become one of the challenges of today’s banks. Since sustainable business models are responsible for the environment and society along with generating economic benefits, they are an attractive approach to sustainability. Sustainable business models also offer banks competitive advantages such as increasing brand reputation and cost reduction. However, no framework is presented to evaluate the sustainability of banking business models. To bridge this theoretical gap, the current study using A Delphi-Analytic Hierarchy Process method, firstly, developed a sustainable business model to evaluate the sustainability of the business model of banks. In the second step, the sustainability performance of sixteen banks from eight European countries including Norway, The UK, Poland, Hungary, Germany, France, Spain, and Italy, assessed. The proposed business model components of this study were ranked in terms of their impact on achieving sustainability goals. Consequently, the proposed model components of this study, based on their impact on sustainability, are respectively value proposition, core competencies, financial aspects, business processes, target customers, resources, technology, customer interface, and partner network. The results of the comparison of the banks studied by each country disclosed that the sustainability of the Norwegian and German banks’ business models is higher than in other counties. The studied banks of Hungary and Spain came in second, the banks of The UK, Poland, and France ranked third, and finally, the Italian banks ranked fourth in the sustainability of their business models.', 'corpus_id': 214714491, 'score': 0}, {'doc_id': '73672760', 'title': 'Tool for identifying critical control points in embedded purchasing activities in SMEs : Working paper WP 69 for the 24th IPSERA conference, Amsterdam 2015', 'abstract': 'This paper discusses risk and uncertainty aspects and proposes an assessment tool leading to identification of critical control points (CCPs) within purchasing-oriented activities of small and medium enterprises (SMEs). Identifying such CCPs is the basis for developing SME purchasing instruments to support purchasing-oriented activities. The identification of such CCPs will be theoretically approached from a systems perspective using four management functions which are needed to operate as a viable system: implementation, control, intelligence and coordination. When applied to the development of purchasing instruments, these instruments can be used for supporting one of these four management control functions.', 'corpus_id': 73672760, 'score': 1}, {'doc_id': '215403709', 'title': 'Insurance uptake among small and medium-sized tourism and hospitality enterprises in a resource-scarce environment', 'abstract': ""\n Abstract\n \n Small and medium-sized tourism and hospitality enterprises (SMTHEs) are often susceptible to various hazards, which result in risk concerns. Insurance is recognised as one of the risk management strategies, but evidence indicates that insurance uptake among SMTHEs has been low. Yet, researchers have hardly researched into the factors that influence insurance uptake among SMTHEs. Two-hundred and fifty (250) respondents were selected using a multi-stage sampling technique. Confirmatory factor analysis, multivariate logit and probit regression techniques were used to determine factors underlying SMTHEs' insurance uptake. Risk concerns, the firm's characteristics, the perceived benefits of insurance and other informal risk coping mechanisms, as well as insurance service provision concerns were identified as determinants of insurance uptake. This is one of the first papers to offer a holistic understanding of the factors influencing SMTHEs' insurance subscription in a resource-scarce destination of Sub-Saharan Africa. The practical and theoretical implications of the paper are discussed.\n \n"", 'corpus_id': 215403709, 'score': 0}, {'doc_id': '168477584', 'title': 'Purchasing practices in small hospitals.', 'abstract': None, 'corpus_id': 168477584, 'score': 1}, {'doc_id': '216081373', 'title': 'Digitalization in management accounting and control: an editorial', 'abstract': 'Digitalization has the potential to disrupt the management accounting domain. It may not only affect the digital landscape of the organization and the associated business models, but also management accounting and control practices as well as the role of the controller. This editorial discusses these developments by introducing the concept of digitalization and describing its impact on the field of management accounting and control.', 'corpus_id': 216081373, 'score': 0}, {'doc_id': '166587944', 'title': 'Purchasing management in the smaller company', 'abstract': None, 'corpus_id': 166587944, 'score': 1}, {'doc_id': '167000220', 'title': 'Cooperative Purchasing in Small and Medium-Sized Enterprises', 'abstract': 'Despite the increasing research interests in the purchasing group, cooperative purchasing in small and medium-sized enterprises (SMEs) has not received significant attention from the operations and supply chain management researcher. This study investigates the typical advantages of cooperative purchasing for SME retailers, and critical success factors for managing a purchasing group, using a case study of a purchasing group established by Chinese SME retailers. The study suggests that a successful purchasing group can help SME retailers to survive in today’s competitive marketplace. The main advantages of cooperative purchasing for SME retailers are lower purchasing prices, mutual learning and support, dealing with illegal bribes, and quality improvement. The success factors for SME retailers to manage a purchasing group are good guanxi (personal and business relationships) among group members, similar characteristics of group members, similar personality traits of top executives, and the role of a “big brother” (group leader) in the group. This study also provides practical insights for retail managers to consider when developing a purchasing group in dynamic environments, in order to achieve the benefits of cooperative purchasing.', 'corpus_id': 167000220, 'score': 1}, {'doc_id': '215737230', 'title': 'Current Practices in the Information Collection for Enterprise Architecture Management', 'abstract': ""The digital transformation influences business models, processes, and enterprise IT landscape as a whole. Therefore, business-IT alignment is becoming more important than ever before. Enterprise architecture management (EAM) is designed to support and improve this business-IT alignment. The success of EAM crucially depends on the information available about a company's enterprise architecture, such as infrastructure components, applications, and business processes. This paper discusses the results of a qualitative expert survey with 26 experts in the field of EAM. The goal of this survey was to highlight current practices in the information collection for EAM and identify relevant information from enterprise-external data sources. The results provide a comprehensive overview of collected and utilized information in the industry, including an assessment of the relevance of such information. Furthermore, the results highlight challenges in practice and point out investments that organizations plan in the field of EAM."", 'corpus_id': 215737230, 'score': 0}, {'doc_id': '169426538', 'title': 'Purchasing and Supply Strategies for SMEs in the Construction Industry', 'abstract': 'Since the purchasing expenditure of many companies is such a large part of the total cost and 99,8 % of companies are classified as Small and Medium Sized Enterprises (SME), one might expect that theories on purchasing strategy would not only be conducted for large companies but also SMEs. In order to fill this gap in theory, this thesis aims to create a purchasing strategy framework for SMEs in the construction and specialist contractor industry built on current purchasing theories of large companies and theories of SME behaviour and actions. The study has been conducted with repli- cation logic through a literature review with the briefsearch and pearl growing method to come up with a framework for SME purchasing. The framework was later tested empirically with four case studies through interviews to find key success factors of SME purchasing strategy. The study re- sulted in a framework called the ”SME Purchasing Cloud” which looks at four elements of SME purchasing: environment, resources, dynamic capabilities and purchasing strategy. The fourth ele- ment; purchasing strategy is summarized in a sub framework called the ”SME Strategic Purchasing Framework” with seven steps of purchasing actions for SMEs and a core of overall corporate strategy and management priority. To complete the framework, four key success factors for SME purchas- ing strategies have been identified: relationship, trust, communication and risk mitigation. The framework will help SMEs in the construction industry to structure their purchasing function and actions. The results could possibly be generalizable for other industries but have in this thesis been specified for the construction industry.', 'corpus_id': 169426538, 'score': 1}]"
126	{'doc_id': '168347690', 'title': 'Cybercrime insurance and its relevance to the Maltese market', 'abstract': None, 'corpus_id': 168347690}	20012	"[{'doc_id': '235829722', 'title': 'The Master and Parasite Attack', 'abstract': 'We explore a new type of malicious script attacks: the persistent parasite attack. Persistent parasites are stealthy scripts, which persist for a long time in the browser’s cache. We show to infect the caches of victims with parasite scripts via TCP injection.Once the cache is infected, we implement methodologies for propagation of the parasites to other popular domains on the victim client as well as to other caches on the network. We show how to design the parasites so that they stay long time in the victim’s cache not restricted to the duration of the user’s visit to the web site. We develop covert channels for communication between the attacker and the parasites, which allows the attacker to control which scripts are executed and when, and to exfiltrate private information to the attacker, such as cookies and passwords. We then demonstrate how to leverage the parasites to perform sophisticated attacks, and evaluate the attacks against a range of applications and security mechanisms on popular browsers. Finally we provide recommendations for countermeasures.', 'corpus_id': 235829722, 'score': 0}, {'doc_id': '182168277', 'title': 'Assessing the Capacity of DRDoS-For-Hire Services in Cybercrime Markets', 'abstract': 'ABSTRACT Over the last few years the market for distributed denial of service (DDoS) attacks has changed from a pay-per-attack model executed by botnets, to a subscription service of booters and stressers where “subscribers” launch their own attacks through a web-based front end. The DDoS attack strength of booters and stressers has significantly increased to rival that of the largest botnets, making them an ideal resource for attacks. The size of attacks offered depends on reflection and amplification (DRDoS) attacks where vulnerable servers are used to reflect attacks towards their victim. This increases the volume of attack traffic, while masking the source of the attack from the victim. To better understand this new form of cybercrime as a service, this study provides a comparative analysis of 155 unique reflective attacks performed by 21 DRDoS service providers against a real target. The underlying infrastructure of reflection servers was analyzed across the different providers, along with the type of attacks advertised relative to the actual type of attacks launched. The findings demonstrate there are distinct differences in the quality and capacity of service providers, and the language in posted advertisements does not necessarily conform to the realities of their real-time attacks. Implications for the disruption and mitigation of booter services are discussed in detail.', 'corpus_id': 182168277, 'score': 1}, {'doc_id': '235753584', 'title': 'Web Apps Security 2019: Analysis of Malware Samples and Code Injections Vulnerabilities in Tanzania Cyber Space', 'abstract': ""Malware is one of the most serious security threats on mobile devices , electronic medical devices , and the Internet [3] as a whole. Spam e-mails, phishing SMS, malicious internet links and denial of service attacks have malware as their underlying cause. Criminals are using malware as their financial gain by circulating Ransomware – a malicious software designed to block access to a computer system until a sum of money is paid. State sponsored hackers are using malware in the form of spyware – a software that enables a user to obtain secret information about another's computer activities by transmitting data secretly from their computer systems. In order for a malware to successfully infect an electronic device or a particular web application, there must exist some kind of security vulnerabilities in them; the most common vulnerability exploited by hackers to inject malware is code injections [4] exploitation. Code injection is a result of poor programming in computer (or mobile phone) application hence making it send untrusted data to an interpreter."", 'corpus_id': 235753584, 'score': 0}, {'doc_id': '153734797', 'title': 'Cybersecurity: Stakeholder incentives, externalities, and policy options', 'abstract': 'Information security breaches are increasingly motivated by fraudulent and criminal motives. Reducing their considerable costs has become a pressing issue. Although cybersecurity has strong public good characteristics, most information security decisions are made by individual stakeholders. Due to the interconnectedness of cyberspace, these decentralized decisions are afflicted with externalities that can result in sub-optimal security levels. Devising effective solutions to this problem is complicated by the global nature of cyberspace, the interdependence of stakeholders, as well as the diversity and heterogeneity of players. The paper develops a framework for studying the co-evolution of the markets for cybercrime and cybersecurity. It examines the incentives of stakeholders to provide for security and their implications for the ICT ecosystem. The findings show that market and non-market relations in the information infrastructure generate many security-enhancing incentives. However, pervasive externalities remain that can only be corrected by voluntary or government-led collective measures.', 'corpus_id': 153734797, 'score': 1}, {'doc_id': '237321108', 'title': 'Hybrid rule-based botnet detection approach using machine learning for analysing DNS traffic', 'abstract': 'Botnets can simultaneously control millions of Internet-connected devices to launch damaging cyber-attacks that pose significant threats to the Internet. In a botnet, bot-masters communicate with the command and control server using various communication protocols. One of the widely used communication protocols is the ‘Domain Name System’ (DNS) service, an essential Internet service. Bot-masters utilise Domain Generation Algorithms (DGA) and fast-flux techniques to avoid static blacklists and reverse engineering while remaining flexible. However, botnet’s DNS communication generates anomalous DNS traffic throughout the botnet life cycle, and such anomaly is considered an indicator of DNS-based botnets presence in the network. Despite several approaches proposed to detect botnets based on DNS traffic analysis; however, the problem still exists and is challenging due to several reasons, such as not considering significant features and rules that contribute to the detection of DNS-based botnet. Therefore, this paper examines the abnormality of DNS traffic during the botnet lifecycle to extract significant enriched features. These features are further analysed using two machine learning algorithms. The union of the output of two algorithms proposes a novel hybrid rule detection model approach. Two benchmark datasets are used to evaluate the performance of the proposed approach in terms of detection accuracy and false-positive rate. The experimental results show that the proposed approach has a 99.96% accuracy and a 1.6% false-positive rate, outperforming other state-of-the-art DNS-based botnet detection approaches.', 'corpus_id': 237321108, 'score': 0}, {'doc_id': '235780326', 'title': 'Brief Industry Paper: Catching IoT Malware in the Wild Using HoneyIoT', 'abstract': 'Constantly increasing botnets powered by vulnerable IoT devices perform record-breaking DDoS attacks to critical infrastructures. Therefore, it is imperative to find vulnerabilities in IoT devices ahead of attackers. In this paper, we first present a systematic analysis on various kinds of IoT malware to further explore the challenges of IoT Honeypot design. We then propose HoneyIoT, a scalable IoT Honeypot framework, which aims at attracting IoT attacks and recording malicious behaviors with configurable vulnerabilities and firmware support. During a 7day real-world industrial experiment, HoneyIoT observed over 12,500 malicious connections and 3,423 distinct login attempts.', 'corpus_id': 235780326, 'score': 0}, {'doc_id': '167335687', 'title': 'The implications of economic cybercrime for policing', 'abstract': None, 'corpus_id': 167335687, 'score': 1}, {'doc_id': '236881734', 'title': 'Botnet Detection through DNS based approach', 'abstract': 'Volume 2, Issue 6, June 2013 Page 497 ABSTRACT Botnets is group of compromised computers controlled remotely by attackers. Botnets create widespread security and data safety issues and areeffective tools for propagating cyber-crime. It is imperative for the IT community to develop effective means of detecting andmitigating the malicious behavior of botnets. In this paper, we will understanding about the botnet and botnet detection by (a) creating a simple botnet and studying the flow of information among the bot and C&C server through activity diagram; and (b) study the detection of botnet using the DNS query patterns generated by the botnet.', 'corpus_id': 236881734, 'score': 0}, {'doc_id': '166457302', 'title': 'The Global Cybercrime Industry: Economic, Institutional and Strategic Perspectives', 'abstract': 'This book is about the global cybercrime industry, which according to some estimates, is a US$1 trillion industry and is growing rapidly. It examines economic and institutional processes in the cybercrime industry, provides insights into the entrepreneurial aspect of firms engaged in cyber-criminal activities, takes a close look at cybercrime business models, explains the global variation in the pattern of cybercrimes and seeks to understand threats and countermeasures taken by key actors in this industry. This books distinguishing features include the newness, importance, controversiality and complexity of the topic; cross-disciplinary focus, orientation and scope; theory-based but practical and accessible to the wider audience; and illustration of various qualitative and quantitative aspects of the global cybercrime industry.', 'corpus_id': 166457302, 'score': 1}, {'doc_id': '141699595', 'title': 'Cybercrime: The creation and exploration of a model', 'abstract': None, 'corpus_id': 141699595, 'score': 1}]"
127	{'doc_id': '209516262', 'title': 'Encoding word order in complex embeddings', 'abstract': 'Sequential word order is important when processing text. Currently, neural networks (NNs) address this by modeling word position using position embeddings. The problem is that position embeddings capture the position of individual words, but not the ordered relationship (e.g., adjacency or precedence) between individual word positions. We present a novel and principled solution for modeling both the global absolute positions of words and their order relationships. Our solution generalizes word embeddings, previously defined as independent vectors, to continuous word functions over a variable (position). The benefit of continuous functions over variable positions is that word representations shift smoothly with increasing positions. Hence, word representations in different positions can correlate with each other in a continuous function. The general solution of these functions can be extended to complex-valued variants. We extend CNN, RNN and Transformer NNs to complex-valued versions to incorporate our complex embedding (we make all code available). Experiments on text classification, machine translation and language modeling show gains over both classical word embeddings and position-enriched word embeddings. To our knowledge, this is the first work in NLP to link imaginary numbers in complex-valued representations to concrete meanings (i.e., word order).', 'corpus_id': 209516262}	2774	"[{'doc_id': '212644755', 'title': 'Reservoir memory machines', 'abstract': 'In recent years, Neural Turing Machines have gathered attention by joining the flexibility of neural networks with the computational capabilities of Turing machines. However, Neural Turing Machines are notoriously hard to train, which limits their applicability. We propose reservoir memory machines, which are still able to solve some of the benchmark tests for Neural Turing Machines, but are much faster to train, requiring only an alignment algorithm and linear regression. Our model can also be seen as an extension of echo state networks with an external memory, enabling arbitrarily long storage without interference.', 'corpus_id': 212644755, 'score': 1}, {'doc_id': '233168647', 'title': 'Recognizing and Verifying Mathematical Equations using Multiplicative Differential Neural Units', 'abstract': 'Automated mathematical reasoning is a challenging problem that requires an agent to learn algebraic patterns that contain long-range dependencies. Two particular tasks that test this type of reasoning are (1) mathematical equation verification, which requires determining whether trigonometric and linear algebraic statements are valid identities or not, and (2) equation completion, which entails filling in a blank within an expression to make it true. Solving these tasks with deep learning requires that the neural model learn how to manipulate and compose various algebraic symbols, carrying this ability over to previously unseen expressions. Artificial neural networks, including recurrent networks and transformers, struggle to generalize on these kinds of difficult compositional problems, often exhibiting poor extrapolation performance. In contrast, recursive neural networks (recursive-NNs) are, theoretically, capable of achieving better extrapolation due to their tree-like design but are difficult to optimize as the depth of their underlying tree structure increases. To overcome this issue, we extend recursive-NNs to utilize multiplicative, higher-order synaptic connections and, furthermore, to learn to dynamically control and manipulate an external memory. We argue that this key modification gives the neural system the ability to capture powerful transition functions for each possible input. We demonstrate the effectiveness of our proposed higher-order, memory-augmented recursive-NN models on two challenging mathematical equation tasks, showing improved extrapolation, stable performance, and faster convergence. Our models achieve a 1.53% average improvement over current state-of-the-art methods in equation verification and achieve a 2.22% Top-1 average accuracy and 2.96% Top5 average accuracy for equation completion.', 'corpus_id': 233168647, 'score': 0}, {'doc_id': '227316369', 'title': 'Multi-modal actuation with the activation bit vector machine', 'abstract': 'Abstract Research towards a new approach to the abstract symbol grounding problem showed that through model counting there is a correspondence between logical/linguistic and coordinate representation in the visuospatial domain. The logical/verbal description of a spatial layout directly gives rise to a coordinate representation that can be drawn, with the drawing reflecting what is described. The main characteristic of this logical property is that it does not need any semantic information or ontology apart from a separation into symbols/words referring to relations and symbols/words referring to objects. Moreover, the complete mechanism can be implemented efficiently on a brain inspired cognitive architecture, the Activation Bit Vector Machine (ABVM), an architecture that belongs to the Vector Symbolic Architectures. However, the natural language fragment captured previously was restricted to simple predication sentences, with the corresponding logical fragment being atomic Context Logic (CLA), and the only actuation modality leveraged was visualization. This article extends the approach on all three aspects: adding a third category of action verbs we move to a fragment of first-order Context Logic (CL1), with modalities requiring a temporal dimension, such as film and music, becoming available. The article presents an ABVM generating sequences of images from texts.', 'corpus_id': 227316369, 'score': 1}, {'doc_id': '232428181', 'title': 'Attention, please! A survey of Neural Attention Models in Deep Learning', 'abstract': 'In humans, Attention is a core property of all perceptual and cognitive operations. Given our limited ability to process competing sources, attention mechanisms select, modulate, and focus on the information most relevant to behavior. For decades, concepts and functions of attention have been studied in philosophy, psychology, neuroscience, and computing. For the last six years, this property has been widely explored in deep neural networks. Currently, the state-of-the-art in Deep Learning is represented by neural attention models in several application domains. This survey provides a comprehensive overview and analysis of developments in neural attention models. We systematically reviewed hundreds of architectures in the area, identifying and discussing those in which attention has shown a significant impact. We also developed and made public an automated methodology to facilitate the development of reviews in the area. By critically analyzing 650 works, we describe the primary uses of attention in convolutional, recurrent networks and generative models, identifying common subgroups of uses and applications. Furthermore, we describe the impact of attention in different application domains and their impact on neural networks’ interpretability. Finally, we list possible trends and opportunities for further research, hoping that this review will provide a succinct overview of the main attentional models in the area and guide researchers in developing future approaches that will drive further improvements.', 'corpus_id': 232428181, 'score': 0}, {'doc_id': '233762187', 'title': 'Precis of A Bayesian account of learning algorithms and generalising representations in the brain', 'abstract': 'Without learning we would be limited to a set of preprogrammed behaviours. While that may be acceptable for flies1, it does not provide the basis for adaptive or intelligent behaviours familiar to humans. Learning, then, is one of the crucial components of brain operation. Learning, however, takes time. Thus, the key to adaptive behaviour is learning to systematically generalise; that is, have learned knowledge that can be flexibly recombined to understand any world in front of you. This thesis attempts to make inroads on two questions how can brain networks learn, and what are the principles behind representations of knowledge that allow generalisation. With the industrialisation of science, the twentieth century bore fruit in the form of an increasingly detailed understanding of neurons, synapses, neurotransmitters, resting potentials, action potentials, networks and so on (1–4). Though we have gained a great level of detail about many of these micro-processes as well as high-level understandings of intelligence thanks to philosophy, experimental psychology, and behavioural and cognitive neuroscience (5–9) a large gulf of understanding remains between these levels of granularity. This thesis focuses on spanning this gap by providing high-level computational frameworks that translate to low-level processes. Any high-level brain framework must have successful behaviour at its heart as that is the role of the brain. Analogously, neurons are central to low-level understanding as the basis of brain function is believed to be the transfer of information between neurons, mediated via weighted connections. Different weights lead to different functions. Thus, learning appropriate configurations of weights is the fundamental problem facing brains. There are two facets to this learning the first is how, and the second is what. The how are the learning algorithms that determine updates to these synaptic connections, and the what are the neural representations that reflect how the world works. In this vein, this thesis examines 1) the algorithmic implementation of learning in biological neural networks, and 2) a computational framework for the neural representations of task generalisation. Both these research directions are bound together by Bayesian thinking, and both of these pieces of work bridge the gap between highand lowlevel understanding, as well as between brains and machines.', 'corpus_id': 233762187, 'score': 0}, {'doc_id': '233241236', 'title': 'Memory Capacity of Neural Turing Machines with Matrix Representation', 'abstract': 'It is well known that recurrent neural networks (RNNs) faced limitations in learning longterm dependencies that have been addressed by memory structures in long short-term memory (LSTM) networks. Matrix neural networks feature matrix representation which inherently preserves the spatial structure of data and has the potential to provide better memory structures when compared to canonical neural networks that use vector representation. Neural Turing machines (NTMs) are novel RNNs that implement notion of programmable computers with neural network controllers to feature algorithms that have copying, sorting, and associative recall tasks. In this paper, we study augmentation of memory capacity with matrix representation of RNNs and NTMs (MatNTMs). We investigate if matrix representation has a better memory capacity than the vector representations in conventional neural networks. We use a probabilistic model of the memory capacity using Fisher information and investigate how the memory capacity for matrix representation networks are limited under various constraints, and in general, without any constraints. In the case of memory capacity without any constraints, we found that the upper bound on memory capacity to be N for an N×N state matrix. The results from our experiments using synthetic algorithmic tasks show that MatNTMs have a better learning capacity when compared to its counterparts.', 'corpus_id': 233241236, 'score': 0}, {'doc_id': '207870512', 'title': 'A Simple Recurrent Unit with Reduced Tensor Product Representations', 'abstract': 'idely used recurrent units, including Long-short Term Memory (LSTM) and the Gated Recurrent Unit (GRU), perform well on natural language tasks, but their ability to learn structured representations is still questionable. Exploiting reduced Tensor Product Representations (TPRs) --- distributed representations of symbolic structure in which vector-embedded symbols are bound to vector-embedded structural positions --- we propose the TPRU, a simple recurrent unit that, at each time step, explicitly executes structural-role binding and unbinding operations to incorporate structural information into learning. A gradient analysis of our proposed TPRU is conducted to support our model design, and its performance on multiple datasets shows the effectiveness of our design choices. Furthermore, observations on a linguistically grounded study demonstrate the interpretability of our TPRU.', 'corpus_id': 207870512, 'score': 1}, {'doc_id': '232092793', 'title': 'Distributional Formal Semantics', 'abstract': 'Abstract Natural language semantics has recently sought to combine the complementary strengths of formal and distributional approaches to meaning. More specifically, proposals have been put forward to augment formal semantic machinery with distributional meaning representations, thereby introducing the notion of semantic similarity into formal semantics, or to define distributional systems that aim to incorporate formal notions such as entailment and compositionality. However, given the fundamentally different ‘representational currency’ underlying formal and distributional approaches—models of the world versus linguistic co-occurrence—their unification has proven extremely difficult. Here, we define a Distributional Formal Semantics that integrates distributionality into a formal semantic system on the level of formal models. This approach offers probabilistic, distributed meaning representations that are also inherently compositional, and that naturally capture fundamental semantic notions such as quantification and entailment. Furthermore, we show how the probabilistic nature of these representations allows for probabilistic inference, and how the information-theoretic notion of “information” (measured in terms of Entropy and Surprisal) naturally follows from it. Finally, we illustrate how meaning representations can be derived incrementally from linguistic input using a recurrent neural network model, and how the resultant incremental semantic construction procedure intuitively captures key semantic phenomena, including negation, presupposition, and anaphoricity.', 'corpus_id': 232092793, 'score': 1}, {'doc_id': '235186433', 'title': 'Class-Modeling of Septic Shock With Hyperdimensional Computing', 'abstract': ""Sepsis arises when a patient's immune system has an extreme reaction to an infection. This is followed by septic shock if damage to organ tissue is so extensive that it causes a total systemic failure. Early detection of septic shock among septic patients could save critical time for preparation and prevention treatment. Due to the high variance in symptoms and patient state before shock, it is challenging to create a protocol that would be effective across patients. However, since septic shock is an acute change in patient state, modeling patient stability could be more effective in detecting a condition that departs from it. In this paper we present a one-class classification approach to septic shock using hyperdimensional computing. We built various models that consider different contexts and can be adapted according to a target priority. Among septic patients, the models can detect septic shock accurately with 90% sensitivity and overall accuracy of 60% of the cases up to three hours before the onset of septic shock, with the ability to adjust predictions according to incoming data. Additionally, the models can be easily adapted to prioritize sensitivity (increase true positives) or specificity (decrease false positives)."", 'corpus_id': 235186433, 'score': 1}, {'doc_id': '232325097', 'title': 'From convolutional neural networks to models of higher-level cognition (and back again).', 'abstract': 'The remarkable successes of convolutional neural networks (CNNs) in modern computer vision are by now well known, and they are increasingly being explored as computational models of the human visual system. In this paper, we ask whether CNNs might also provide a basis for modeling higher-level cognition, focusing on the core phenomena of similarity and categorization. The most important advance comes from the ability of CNNs to learn high-dimensional representations of complex naturalistic images, substantially extending the scope of traditional cognitive models that were previously only evaluated with simple artificial stimuli. In all cases, the most successful combinations arise when CNN representations are used with cognitive models that have the capacity to transform them to better fit human behavior. One consequence of these insights is a toolkit for the integration of cognitively motivated constraints back into CNN training paradigms in computer vision and machine learning, and we review cases where this leads to improved performance. A second consequence is a roadmap for how CNNs and cognitive models can be more fully integrated in the future, allowing for flexible end-to-end algorithms that can learn representations from data while still retaining the structured behavior characteristic of human cognition.', 'corpus_id': 232325097, 'score': 0}]"
128	{'doc_id': '221139536', 'title': 'Accountable Off-Policy Evaluation With Kernel Bellman Statistics', 'abstract': 'We consider off-policy evaluation (OPE), which evaluates the performance of a new policy from observed data collected from previous experiments, without requiring the execution of the new policy. This finds important applications in areas with high execution cost or safety concerns, such as medical diagnosis, recommendation systems and robotics. In practice, due to the limited information from off-policy data, it is highly desirable to construct rigorous confidence intervals, not just point estimation, for the policy performance. In this work, we propose a new variational framework which reduces the problem of calculating tight confidence bounds in OPE into an optimization problem on a feasible set that catches the true state-action value function with high probability. The feasible set is constructed by leveraging statistical properties of a recently proposed kernel Bellman loss (Feng et al., 2019). We design an efficient computational approach for calculating our bounds, and extend it to perform post-hoc diagnosis and correction for existing estimators. Empirical results show that our method yields tight confidence intervals in different settings.', 'corpus_id': 221139536}	5115	"[{'doc_id': '216562662', 'title': 'Pitfalls of learning a reward function online', 'abstract': ""In some agent designs like inverse reinforcement learning an agent needs to learn its own reward function. Learning the reward function and optimising for it are typically two different processes, usually performed at different stages. We consider a continual (``one life'') learning approach where the agent both learns the reward function and optimises for it at the same time. We show that this comes with a number of pitfalls, such as deliberately manipulating the learning process in one direction, refusing to learn, ``learning'' facts already known to the agent, and making decisions that are strictly dominated (for all relevant reward functions). We formally introduce two desirable properties: the first is `unriggability', which prevents the agent from steering the learning process in the direction of a reward function that is easier to optimise. The second is `uninfluenceability', whereby the reward-function learning process operates by learning facts about the environment. We show that an uninfluenceable process is automatically unriggable, and if the set of possible environments is sufficiently rich, the converse is true too."", 'corpus_id': 216562662, 'score': 0}, {'doc_id': '218595928', 'title': 'Delay-Aware Multi-Agent Reinforcement Learning', 'abstract': 'Action and observation delays exist prevalently in the real-world cyber-physical systems which may pose challenges in reinforcement learning design. It is particularly an arduous task when handling multi-agent systems where the delay of one agent could spread to other agents. To resolve this problem, this paper proposes a novel framework to deal with delays as well as the non-stationary training issue of multi-agent tasks with model-free deep reinforcement learning. We formally define the Delay-Aware Markov Game that incorporates the delays of all agents in the environment. To solve Delay-Aware Markov Games, we apply centralized training and decentralized execution that allows agents to use extra information to ease the non-stationary issue of the multi-agent systems during training, without the need of a centralized controller during execution. Experiments are conducted in multi-agent particle environments including cooperative communication, cooperative navigation, and competitive experiments. We also test the proposed algorithm in traffic scenarios that require coordination of all autonomous vehicles to show the practical value of delay-awareness. Results show that the proposed delay-aware multi-agent reinforcement learning algorithm greatly alleviates the performance degradation introduced by delay. Codes available at: this https URL.', 'corpus_id': 218595928, 'score': 0}, {'doc_id': '218684985', 'title': 'Experience Augmentation: Boosting and Accelerating Off-Policy Multi-Agent Reinforcement Learning', 'abstract': 'Exploration of the high-dimensional state action space is one of the biggest challenges in Reinforcement Learning (RL), especially in multi-agent domain. We present a novel technique called Experience Augmentation, which enables a time-efficient and boosted learning based on a fast, fair and thorough exploration to the environment. It can be combined with arbitrary off-policy MARL algorithms and is applicable to either homogeneous or heterogeneous environments. We demonstrate our approach by combining it with MADDPG and verifing the performance in two homogeneous and one heterogeneous environments. In the best performing scenario, the MADDPG with experience augmentation reaches to the convergence reward of vanilla MADDPG with 1/4 realistic time, and its convergence beats the original model by a significant margin. Our ablation studies show that experience augmentation is a crucial ingredient which accelerates the training process and boosts the convergence.', 'corpus_id': 218684985, 'score': 0}, {'doc_id': '226245964', 'title': 'On the Convergence of Gradient Descent in GANs: MMD GAN As a Gradient Flow', 'abstract': 'We consider the maximum mean discrepancy ($\\mathrm{MMD}$) GAN problem and propose a parametric kernelized gradient flow that mimics the min-max game in gradient regularized $\\mathrm{MMD}$ GAN. We show that this flow provides a descent direction minimizing the $\\mathrm{MMD}$ on a statistical manifold of probability distributions. We then derive an explicit condition which ensures that gradient descent on the parameter space of the generator in gradient regularized $\\mathrm{MMD}$ GAN is globally convergent to the target distribution. Under this condition, we give non asymptotic convergence results of gradient descent in MMD GAN. Another contribution of this paper is the introduction of a dynamic formulation of a regularization of $\\mathrm{MMD}$ and demonstrating that the parametric kernelized descent for $\\mathrm{MMD}$ is the gradient flow of this functional with respect to the new Riemannian structure. Our obtained theoretical result allows ones to treat gradient flows for quite general functionals and thus has potential applications to other types of variational inferences on a statistical manifold beyond GANs. Finally, numerical experiments suggest that our parametric kernelized gradient flow stabilizes GAN training and guarantees convergence.', 'corpus_id': 226245964, 'score': 1}, {'doc_id': '232239127', 'title': 'Counterfactual-Free Regret Minimization for Sequential Decision Making and Extensive-Form Games', 'abstract': 'Sequential decision processes (SDPs) model the multi-stage online decision-making problems that each player faces in an extensive-form game, as well as MDPs and POMDPs where the agent conditions on observed history. Prior regret minimization approaches for sequential decision processes typically rely heavily on having access to counterfactuals, that is, information on what would have happened had the agent chosen a different action at any decision point. While this assumption is reasonable when regret minimization algorithms are used in self-play (for instance, as a way to converge to a Nash equilibrium in an extensive-form game), it is unrealistic in online decision-making settings, where the algorithm is deployed to learn strategies against an unknown environment. In this paper, we give the first efficient algorithm for the bandit linear optimization problem on SDPs—and therefore also extensive-form games—and show that it achieves O( √ T ) cumulative regret in expectation against any strategy.', 'corpus_id': 232239127, 'score': 1}, {'doc_id': '232350474', 'title': 'A Functional Mirror Descent Perspective on Reinforcement Learning', 'abstract': 'Functional mirror descent offers a unifying perspective on optimization of statistical models and provides numerous advantages for the design and analysis of learning algorithms. It brings the concepts from optimization—such as surrogate models, constraints, projections, conjugate duality, momentum—into the realm of optimization over probability distributions. So far, only a fraction of these insights have been utilized in reinforcement learning (RL), with most progress achieved in the bandit setting and in discrete MDPs, but not in the full RL setup with continuous states, observations, goals, tasks, etc. We argue for a much tighter integration of the ideas from (online) convex optimization into the design of RL algorithms for continuous MDPs by (i) showing how a number of existing approaches can be framed as approximate mirror descent on the space of probability measures and (ii) indicating yet unexplored directions uncovered by this perspective. We hope that our exposition will stimulate a wider use of advanced optimization tools in reinforcement learning and at the same time encourage the development of novel optimization approaches motivated by the RL problem.', 'corpus_id': 232350474, 'score': 1}, {'doc_id': '210702914', 'title': 'Faster Regret Matching', 'abstract': 'The regret matching algorithm proposed by Sergiu Hart is one of the most powerful iterative methods in finding correlated equilibrium. However, it is possibly not efficient enough, especially in large scale problems. We first rewrite the algorithm in a computationally practical way based on the idea of the regret matrix. Moreover, the rewriting makes the original algorithm more easy to understand. Then by some modification to the original algorithm, we introduce a novel variant, namely faster regret matching. The experiment result shows that the novel algorithm has a speed advantage comparing to the original one.', 'corpus_id': 210702914, 'score': 1}, {'doc_id': '218628747', 'title': 'Probabilistic Guarantees for Safe Deep Reinforcement Learning', 'abstract': ""Deep reinforcement learning has been successfully applied to many control tasks, but the application of such agents in safety-critical scenarios has been limited due to safety concerns. Rigorous testing of these controllers is challenging, particularly when they operate in probabilistic environments due to, for example, hardware faults or noisy sensors. We propose MOSAIC, an algorithm for measuring the safety of deep reinforcement learning agents in stochastic settings. Our approach is based on the iterative construction of a formal abstraction of a controller's execution in an environment, and leverages probabilistic model checking of Markov decision processes to produce probabilistic guarantees on safe behaviour over a finite time horizon. It produces bounds on the probability of safe operation of the controller for different initial configurations and identifies regions where correct behaviour can be guaranteed. We implement and evaluate our approach on agents trained for several benchmark control problems."", 'corpus_id': 218628747, 'score': 0}, {'doc_id': '232147739', 'title': 'Model-Free Online Learning in Unknown Sequential Decision Making Problems and Games', 'abstract': 'Regret minimization has proved to be a versatile tool for se1 quential decision making and extensive-form games. In large 2 two-player zero-sum imperfect-information games, modern 3 extensions of counterfactual regret minimization (CFR) are 4 currently the practical state of the art for computing a Nash 5 equilibrium. Most regret-minimization algorithms for sequen6 tial decision making, including CFR, require (i) an exact model 7 of the player’s decision nodes, observation nodes, and how 8 they are linked, and (ii) full knowledge, at all times t, about 9 the payoffs—even in parts of the decision space that are not 10 encountered at time t. Recently, there has been growing in11 terest towards relaxing some of those restrictions and mak12 ing regret minimization applicable to settings for which rein13 forcement learning methods have traditionally been used—for 14 example, those in which only black-box access to the envi15 ronment is available. We give the first, to our knowledge, 16 regret-minimization algorithm that guarantees sublinear regret 17 with high probability even when requirement (i)—and thus 18 also (ii)—is dropped. We formalize an online learning setting 19 in which the strategy space is not known to the agent and gets 20 revealed incrementally whenever the agent encounters new 21 decision points. We give an efficient algorithm that achieves 22 O(T ) regret with high probability for that setting, even 23 when the agent faces an adversarial environment. Our experi24 ments show it significantly outperforms the prior algorithms 25 for the problem, which do not have such guarantees. It can 26 be used in any application for which regret minimization is 27 useful: approximating Nash equilibrium or quantal response 28 equilibrium, approximating coarse correlated equilibrium in 29 multi-player games, learning a best response, learning safe 30 opponent exploitation, and online play against an unknown 31 opponent/environment. 32', 'corpus_id': 232147739, 'score': 1}, {'doc_id': '212725640', 'title': 'Deep Deterministic Portfolio Optimization', 'abstract': 'Can deep reinforcement learning algorithms be exploited as solvers for optimal trading strategies? The aim of this work is to test reinforcement learning algorithms on conceptually simple, but mathematically non-trivial, trading environments. The environments are chosen such that an optimal or close-to-optimal trading strategy is known. We study the deep deterministic policy gradient algorithm and show that such a reinforcement learning agent can successfully recover the essential features of the optimal trading strategies and achieve close-to-optimal rewards.', 'corpus_id': 212725640, 'score': 1}]"
129	{'doc_id': '190874494', 'title': 'A single light-responsive sizer can control multiple-fission cycles in Chlamydomonas', 'abstract': 'Proliferating cells need to coordinate cell division and growth to maintain size homeostasis. Any systematic deviation from a balance between growth and division results in progressive changes of cell size over subsequent generations. While most eukaryotic cells execute binary division after a mass doubling, the photosynthetic green alga Chlamydomonas can grow more than eight-fold during daytime before undergoing rapid cycles of DNA replication, mitosis and cell division at night, which produce up to 16 daughter cells. Here, we propose a mechanistic model for multiple fission and size control in Chlamydomonas. The model comprises a light-sensitive and size-dependent biochemical toggle switch that acts as a sizer and guards transitions into and exit from a phase of cell-division cycle oscillations. We show that this simple ‘sizer-oscillator’ arrangement reproduces the experimentally observed features of multiple-fission cycles and the response of Chlamydomonas cells to different light-dark regimes. Our model also makes testable predictions about the dynamical properties of the biochemical network that controls these features and about the network’s makeup. Collectively, these results provide a new perspective on the concept of a ‘commitment point’ during the growth of Chlamydomonas cells and hint at an intriguing continuity of cell-size control in different eukaryotic lineages. Graphical abstract G1-sizer and S/M-oscillator can give rise to multiple-fission cycles in Chlamydomonas Light-responsive bistable switch may guard transition between G1 and S/M-cycles Illumination increases S/M-entry threshold, causing multiple-fission cycles Dark shift lowers S/M-entry threshold, allowing small cells to commit to fewer divisions', 'corpus_id': 190874494}	17132	[{'doc_id': '233460971', 'title': 'The Consequences of Budding versus Binary Fission on Adaptation and Aging in Primitive Multicellularity', 'abstract': 'Early multicellular organisms must gain adaptations to outcompete their unicellular ancestors, as well as other multicellular lineages. The tempo and mode of multicellular adaptation is influenced by many factors including the traits of individual cells. We consider how a fundamental aspect of cells, whether they reproduce via binary fission or budding, can affect the rate of adaptation in primitive multicellularity. We use mathematical models to study the spread of beneficial, growth rate mutations in unicellular populations and populations of multicellular filaments reproducing via binary fission or budding. Comparing populations once they reach carrying capacity, we find that the spread of mutations in multicellular budding populations is qualitatively distinct from the other populations and in general slower. Since budding and binary fission distribute age-accumulated damage differently, we consider the effects of cellular senescence. When growth rate decreases with cell age, we find that beneficial mutations can spread significantly faster in a multicellular budding population than its corresponding unicellular population or a population reproducing via binary fission. Our results demonstrate that basic aspects of the cell cycle can give rise to different rates of adaptation in multicellular organisms.', 'corpus_id': 233460971, 'score': 0}, {'doc_id': '234785120', 'title': 'Growth and division mode plasticity by cell density in marine-derived black yeasts', 'abstract': 'The diversity and ecological contribution of the fungus kingdom in the marine environment remain under-studied. A recent survey in the Atlantic (Woods Hole, MA, USA) brought to light the diversity and unique biological features of marine fungi. The study revealed that black yeast species undergo an unconventional cell division cycle, which has not been documented in conventional model yeast species such as Saccharomyces cerevisiae (budding yeast) and Schizosaccharomyces pombe (fission yeast). The prevalence of this unusual property is unknown. Inspired by the findings in Woods Hole, I collected and identified >50 marine fungi species across 40 genera from the ocean surface, sediment, and macroalgal surface in the Pacific (Sugashima, Toba, Japan). The Sugashima collection largely did not overlap with the Woods Hole collection and included several unidentifiable species, further illustrating the diversity of marine fungi. Three black yeast species were isolated, two of which were commonly found in Woods Hole (Aureobasidium pullulans, Hortaea werneckii). Surprisingly, I observed that their cell division mode was dependent on cell density, and the previously reported unconventional division mode was reproduced only at a certain cell density. For all three black yeast species, cells underwent filamentous growth with septations at low cell density and immediately formed buds at high cell density. At intermediate cell density, two black yeasts showed rod cells undergoing septation at the cell equator, in a manner similar to S. pombe. In contrast, all eight budding yeast species showed a consistent division pattern regardless of cell density. In five budding yeast species, the mother cell formed a single bud at a time at an apparently random site, similar to S. cerevisiae. The other three budding yeast species possessed a fixed budding site. This study illustrates the plastic nature of the growth/division mode of marine-derived black yeast.', 'corpus_id': 234785120, 'score': 0}, {'doc_id': '233874168', 'title': 'How Many Is Enough? - Challenges of Multinucleated Cell Division in Malaria Parasites', 'abstract': 'Regulating the number of progeny generated by replicative cell cycles is critical for any organism to best adapt to its environment. Classically, the decision whether to divide further is made after cell division is completed by cytokinesis and can be triggered by intrinsic or extrinsic factors. Contrarily, cell cycles of some species, such as the malaria-causing parasites, go through multinucleated cell stages. Hence, their number of progeny is determined prior to the completion of cell division. This should fundamentally affect how the process is regulated and raises questions about advantages and challenges of multinucleation in eukaryotes. Throughout their life cycle Plasmodium spp. parasites undergo four phases of extensive proliferation, which differ over three orders of magnitude in the amount of daughter cells that are produced by a single progenitor. Even during the asexual blood stage proliferation parasites can produce very variable numbers of progeny within one replicative cycle. Here, we review the few factors that have been shown to affect those numbers. We further provide a comparative quantification of merozoite numbers in several P. knowlesi and P. falciparum parasite strains, and we discuss the general processes that may regulate progeny number in the context of host-parasite interactions. Finally, we provide a perspective of the critical knowledge gaps hindering our understanding of the molecular mechanisms underlying this exciting and atypical mode of parasite multiplication.', 'corpus_id': 233874168, 'score': 1}, {'doc_id': '234744015', 'title': 'Role of Hypoxia in the Control of the Cell Cycle', 'abstract': 'The cell cycle is an important cellular process whereby the cell attempts to replicate its genome in an error-free manner. As such, mechanisms must exist for the cell cycle to respond to stress signals such as those elicited by hypoxia or reduced oxygen availability. This review focuses on the role of transcriptional and post-transcriptional mechanisms initiated in hypoxia that interface with cell cycle control. In addition, we discuss how the cell cycle can alter the hypoxia response. Overall, the cellular response to hypoxia and the cell cycle are linked through a variety of mechanisms, allowing cells to respond to hypoxia in a manner that ensures survival and minimal errors throughout cell division.', 'corpus_id': 234744015, 'score': 0}, {'doc_id': '235714378', 'title': 'Growth under Different Trophic Regimes and Synchronization of the Red Microalga Galdieria sulphuraria', 'abstract': 'The extremophilic unicellular red microalga Galdieria sulphuraria (Cyanidiophyceae) is able to grow autotrophically, or mixo- and heterotrophically with 1% glycerol as a carbon source. The alga divides by multiple fission into more than two cells within one cell cycle. The optimal conditions of light, temperature and pH (500 µmol photons m−2 s−1, 40 °C, and pH 3; respectively) for the strain Galdieria sulphuraria (Galdieri) Merola 002 were determined as a basis for synchronization experiments. For synchronization, the specific light/dark cycle, 16/8 h was identified as the precondition for investigating the cell cycle. The alga was successfully synchronized and the cell cycle was evaluated. G. sulphuraria attained two commitment points with midpoints at 10 and 13 h of the cell cycle, leading to two nuclear divisions, followed subsequently by division into four daughter cells. The daughter cells stayed in the mother cell wall until the beginning of the next light phase, when they were released. Accumulation of glycogen throughout the cell cycle was also described. The findings presented here bring a new contribution to our general understanding of the cell cycle in cyanidialean red algae, and specifically of the biotechnologically important species G. sulphuraria.', 'corpus_id': 235714378, 'score': 1}, {'doc_id': '235468657', 'title': 'Characterization of Growth and Cell Cycle Events Affected by Light Intensity in the Green Alga Parachlorella kessleri: A New Model for Cell Cycle Research', 'abstract': 'Multiple fission is a cell cycle variation leading to the production of more than two daughter cells. Here, we used synchronized cultures of the chlorococcal green alga Parachlorella kessleri to study its growth and pattern of cell division under varying light intensities. The time courses of DNA replication, nuclear and cellular division, cell size, total RNA, protein content, dry matter and accumulation of starch were observed at incident light intensities of 110, 250 and 500 µmol photons m−2s−1. Furthermore, we studied the effect of deuterated water on Parachlorella kessleri growth and division, to mimic the effect of stress. We describe a novel multiple fission cell cycle pattern characterized by multiple rounds of DNA replication leading to cell polyploidization. Once completed, multiple nuclear divisions were performed with each of them, immediately followed by protoplast fission, terminated by the formation of daughter cells. The multiple fission cell cycle was represented by several consecutive doublings of growth parameters, each leading to the start of a reproductive sequence. The number of growth doublings increased with increasing light intensity and led to division into more daughter cells. This study establishes the baseline for cell cycle research at the molecular level as well as for potential biotechnological applications, particularly directed synthesis of (deuterated) starch and/or neutral lipids as carbon and energy reserves.', 'corpus_id': 235468657, 'score': 1}, {'doc_id': '235645247', 'title': 'To Divide or Not to Divide? How Deuterium Affects Growth and Division of Chlamydomonas reinhardtii', 'abstract': 'Extensive in vivo replacement of hydrogen by deuterium, a stable isotope of hydrogen, induces a distinct stress response, reduces cell growth and impairs cell division in various organisms. Microalgae, including Chlamydomonas reinhardtii, a well-established model organism in cell cycle studies, are no exception. Chlamydomonas reinhardtii, a green unicellular alga of the Chlorophyceae class, divides by multiple fission, grows autotrophically and can be synchronized by alternating light/dark regimes; this makes it a model of first choice to discriminate the effect of deuterium on growth and/or division. Here, we investigate the effects of high doses of deuterium on cell cycle progression in C. reinhardtii. Synchronous cultures of C. reinhardtii were cultivated in growth medium containing 70 or 90% D2O. We characterize specific deuterium-induced shifts in attainment of commitment points during growth and/or division of C. reinhardtii, contradicting the role of the “sizer” in regulating the cell cycle. Consequently, impaired cell cycle progression in deuterated cultures causes (over)accumulation of starch and lipids, suggesting a promising potential for microalgae to produce deuterated organic compounds.', 'corpus_id': 235645247, 'score': 1}, {'doc_id': '97001121', 'title': 'A novel mathematical model to simulate the size-structured growth of microalgae strains dividing by multiple fission', 'abstract': 'Abstract Several microalgae strains are capable to divide by multiple fission, namely they can give rise to variable number of daughter cells after cytokinesis. Such behavior may have implications on the overall growth and productivities of microalgal cultures that are difficult to infer intuitively. Consequently, a novel mathematical model to simulate the dynamics of the size-structured growth of microalgal strains characterized by multiple fission, is proposed in this work. The model relies on the use of population balance equations (PBEs) to describe the evolution of the size distribution of microalgae cells during growth and permits to decouple the single cell growth phase, which is known to take place in the light, from the division one, that on the contrary is assumed to occur under dark conditions according to well corroborated experimental observations. Moreover, the effect of light intensity, photoperiod and nutrients concentration on the continuous growth of the cells, are suitably accounted for by the model. Furthermore, in order to describe the partition of newborn cells after division, a new approach, which relies on suitable experimental observations, is developed to formulate a novel birth term related to PBEs which takes into account the possibility of multiple fission to take place. Model results and literature experimental data pertaining a strain capable to divide by multiple fission, are successfully compared in terms of biomass concentration evolution, thus highlighting a good predictive capability of the model. Subsequently, specific numerical experiments are performed in order to examine the potential improvements arising from this model with respect to the ones currently available in the literature. Finally, suitable simulation-based inferences are formulated about the potential implications of multiple fission on photobioreactor’s productivity.', 'corpus_id': 97001121, 'score': 1}, {'doc_id': '234785082', 'title': 'Density Fluctuations Yield Distinct Growth and Fitness Effects in Single Bacteria', 'abstract': 'Single-cells grow by increasing their biomass and size. Here, we report that while mass and size accumulation rates of single Escherichia coli cells are exponential, their density fluctuates during growth. As such, the rates of mass and size accumulation of a single-cell are generally not the same, but rather cells differentiate into increasing one rate with respect to the other. This differentiation yields a previously unknown density homeostasis mechanism, which we support mathematically. Further, growth differentiation challenges ongoing efforts to predict single-cell reproduction rates (or fitness-levels), through the accumulation rates of size or mass. In contrast, we observe that density fluctuations can predict fitness, with only high fitness individuals existing in the high density fluctuation regime. We detail our imaging approach and the ‘invisible’ microfluidic arrays that critically enabled increased precision and throughput. Biochemical production, infections, and natural communities start from few, growing, cells, thus, underscoring the significance of density-fluctuations when considering non-genetic variability.', 'corpus_id': 234785082, 'score': 1}, {'doc_id': '233206086', 'title': 'The Modular Circuitry of Apicomplexan Cell Division Plasticity', 'abstract': 'The close-knit group of apicomplexan parasites displays a wide variety of cell division modes, which differ between parasites as well as between different life stages within a single parasite species. The beginning and endpoint of the asexual replication cycles is a ‘zoite’ harboring the defining apical organelles required for host cell invasion. However, the number of zoites produced per division round varies dramatically and can unfold in several different ways. This plasticity of the cell division cycle originates from a combination of hard-wired developmental programs modulated by environmental triggers. Although the environmental triggers and sensors differ between species and developmental stages, widely conserved secondary messengers mediate the signal transduction pathways. These environmental and genetic input integrate in division-mode specific chromosome organization and chromatin modifications that set the stage for each division mode. Cell cycle progression is conveyed by a smorgasbord of positively and negatively acting transcription factors, often acting in concert with epigenetic reader complexes, that can vary dramatically between species as well as division modes. A unique set of cell cycle regulators with spatially distinct localization patterns insert discrete check points which permit individual control and can uncouple general cell cycle progression from nuclear amplification. Clusters of expressed genes are grouped into four functional modules seen in all division modes: 1. mother cytoskeleton disassembly; 2. DNA replication and segregation (D&S); 3. karyokinesis; 4. zoite assembly. A plug-and-play strategy results in the variety of extant division modes. The timing of mother cytoskeleton disassembly is hard-wired at the species level for asexual division modes: it is either the first step, or it is the last step. In the former scenario zoite assembly occurs at the plasma membrane (external budding), and in the latter scenario zoites are assembled in the cytoplasm (internal budding). The number of times each other module is repeated can vary regardless of this first decision, and defines the modes of cell division: schizogony, binary fission, endodyogeny, endopolygeny.', 'corpus_id': 233206086, 'score': 0}]
130	{'doc_id': '150842695', 'title': 'Program and Economic Outcomes by TANF Work Exemption Status', 'abstract': 'Objective: Welfare-reform research has treated the Temporary Assistance for Needy Families (TANF) program as a homogenous work program despite the commonality of exemptions from work requirements. To address this shortcoming, we compare program and economic outcomes between three groups of TANF participants: work-exposed (workers), work-exempt to care for an infant (new mothers), and work-exempt due to a disability (participants with a disability). Method: Using 2006 administrative data for a cohort of Wisconsin TANF entrants (N = 682), we describe differences in individual characteristics, TANF participation, and work and income patterns between groups. We use discrete-time regression models to examine the between-group correlates of exiting TANF and exiting TANF to work. Results: Descriptive analyses reveal significant differences between groups in individual characteristics and TANF, work, and income patterns. On average, new mothers have more education and work experience, fewer and shorter TANF spells, and greater earnings and income in the year following TANF entry compared to other participants. Discrete-time regression analyses, however, suggest that pre-entry education and work experience are the strongest predictors of employment outcomes. Conclusions: To address the diversity of outcomes between TANF groups, policymakers should consider program alternatives such as disability and paid-leave programs.', 'corpus_id': 150842695}	6922	"[{'doc_id': '219970762', 'title': 'The Distributional Impacts of Early Employment Losses from Covid-19', 'abstract': 'COVID-19 substantially decreased employment, but the effects vary among demographic and socioeconomic groups. We document the employment losses in April 2020 across various groups using the U.S. Current Population Survey. The unemployment rate understates employment losses. We focus on the percentage of the civilian population that is employed and at work. Young persons experienced the largest employment losses. Individuals with less education and lower family income experienced much larger employment losses than their more educated and higher income counterparts. Hispanics and blacks were more adversely affected than whites.', 'corpus_id': 219970762, 'score': 0}, {'doc_id': '153878188', 'title': 'Standing Still or Moving Up? Evidence from Wisconsin on the Long-Term Employment and Earnings of TANF Participants', 'abstract': 'This study identified the employment and earnings trajectories of welfare recipients over six years for a sample of 14,150 women who entered the Temporary Assistance for Needy Families program (TANF) in Wisconsin in its first year. Wisconsin longitudinal administrative data were used to examine differential patterns of mid-term (three years) and long-term (six years) employment and earnings success. We developed a conceptual approach to categorizing participants\' employment and earnings trajectory groups. Results indicate substantial diversity in employment and earnings patterns. Some women have consistently positive outcomes, others show steady improvements over time, and others have inconsistent patterns that end strong. We found that 46% of the sample fit into one of three successful employment trajectories, and 22% fit into one of three successful earnings trajectories. Results also reveal that many women who were successful in the mid-term were not able to sustain their progress. For example, only 56% of those who were earning successes in the mid-term were still successful in the long-term. Final]y, logistic regression models were used to compare the factors associated with mid-term and long-term success and with employment success and earnings success. Implications of findings are discussed. KEY WORDS: employment and earnings; poverty; social policy; TANF; welfare reform ********** Since the passage of the Personal Responsibility and Work Opportunity Reconciliation Act (PRWORA) of 1996, welfare programs have aimed to move low-income women with children from welfare to work. In the past decade, the number of single-mother families receiving traditional cash assistance (now called Temporary Assistance for Needy Families [TANF]) has fallen dramatically, and employment rates have risen. However, much of the literature on the economic well-being of welfare recipients and welfare leavers suggests that many who have moved from welfare to work move in and out of the labor market frequently, are working for low wages, and have insufficient earnings to support a family above the poverty line without receiving public means-tested benefits. (For a discussion of post-TANF economic status, see Blank, 2006; Cancian & Meyer, 2004; Danziger, Heflin, Corcoran, Oltmans, & Wang, 2002; Grogger & Karoly, 2005; Johnson & Corcoran, 2003.) Most of the early studies on economic well-being of welfare recipients after welfare reform have examined employment, earnings, and income after leaving cash benefits over fairly short periods of time. Less is known about whether the short-term economic success (or lack of success) has persisted in the long-term. In this article, we use longitudinal administrative data to examine the employment and earnings trajectories of welfare recipients over six years for a sample of approximately 17,000 women who entered TANF in Wisconsin in its first year. We propose a method to characterize the six-year patterns of employment and earnings and consider differential patterns of mid-term (three years) and long-term (six years) employment and earnings success. We compare the factors associated with mid-term and long-term success. POLICY CONTEXT This article focuses on TANF participants in Wisconsin. Wisconsin\'s TANF program, called Wisconsin Works (W-2), was instituted in September 1997. W-2 consists of several ""tiers"" and is structured to mirror the world of employment. Thus, individuals do not receive a cash payment unless they are working in a community service job, are engaged in a work-like activity (W-2 Transitions), or have a child younger than 13 weeks old (caretaker of newborn). In addition, individuals who are the most work-ready can receive a variety of services without receiving cash (case management). Individuals are expected to begin in the tier that corresponds to their level of work-readiness and to progress up the tiers until they no longer need any W-2 services. …', 'corpus_id': 153878188, 'score': 1}, {'doc_id': '150572543', 'title': 'Vocational training programs and youth labor market outcomes: Evidence from Nepal', 'abstract': 'Lack of skills is arguably one of the most important determinants for high levels of unemployment and poverty. Targeting youth unemployment and also important because of its strong influence on other important social outcomes. Using a “fuzzy” regression discontinuity design, we examine the employment effects of a vocational training program in Nepal launched in 2009 over a three-year period. We find program participation generated an increase in non-farm employment of 28 percentage points for an overall gain of 95 percent, three years into the program. The program also generated an average monthly earnings gain of 2,167NRs (˜ 29 USD) or 171 percent. Applying heterogeneous local average treatment effect (HLATE) estimators, we find striking differences in the impacts by gender: program impacts are almost double the size for women than for men.', 'corpus_id': 150572543, 'score': 0}, {'doc_id': '154812840', 'title': 'The Extended TANF Application Period and Applicant Outcomes: Evidence from Wisconsin', 'abstract': 'This article examines the characteristics and income patterns associated with welfare entry and nonentry in the context of an extended application period for a sample of 1,664 women who applied for Temporary Assistance for Needy Families services in Wisconsin in the fall of 2006. The study uses data derived from the systematic review of caseworker notes as well as administrative data on welfare participation, earnings, child support, the Supplemental Nutrition Assistance Program, Earned Income Tax Credit estimates, and Supplemental Security Income receipt. It finds that about one-half of those who applied for services did not enter the welfare program within a 60-day time frame. Results indicate that those with consistent employment prior to applying for services, those with very young children, and pregnant applicants are less likely to drop out. In examining differences in economic well-being in the year following an application, the study finds that those who applied for and entered welfare (participants) and those who applied for but did not enter (dropouts) generally had similarly high levels of poverty. However, dropouts had higher rates of deep poverty, with some subgroups--such as those who had declined services--fating better than others. KEY WORDS: poverty; TANF; welfare; welfare application ********** The Personal Responsibility and Work Opportunity Reconciliation Act of 1996 (P.L. 104-110) ushered in a new era of work-based welfare. Benefits are time limited and are no longer an entitlement; work activities are generally mandatory in exchange for cash assistance. These components of postreform Temporary Assistance for Needy Families (TANF) programs were designed to move low-income families off the welfare rolls and into the labor market, reducing caseloads and reliance on welfare. An expansive literature on welfare leavers has documented the dramatic decline in TANF caseloads and increased employment among single mothers, although findings point to poverty-level earnings and future spells of welfare receipt for many (for a review of welfare leaver outcomes, see Acs & Loprest, 2004; Cancian, Haveman, Meyer, & Wolfe, 2002; Cancian & Meyer, 2004; Danziger, Heflin, Corcoran, Oltmans, & Wang, 2002; Grogger & Karoly, 2005; Wu, Cancian, & Meyer, 2008). Although scholars continue to debate the impact of welfare reform policies relative to the economy on postreform caseload decline (see Danielson & Klerman, 2008), the strong work-first approach of modern welfare programs, which generally offer lower cash benefits than those earned through unsubsidized employment, shares some credit for the sharp reduction in caseloads and the increase in low-income women\'s employment (Blank & Schmidt, 2001; Grogger & Karoly, 2005; Moffit, 2002). There is another contributor to reduced welfare caseloads: TANF policies that redirect applicants before they enroll, often referred to as ""diversion"" (Besharov & Germanis, 2007; Danielson & Klerman, 2008; Meyers & Lurie, 2005; Ridzi & London, 2006). Two popular diversion tools are (1) one-time cash payments offered in lieu of TANF participation and (2) extended application periods during which applicants participate in mandatory job-search activities over a prescribed number of days prior to eligibility determination. Diversion procedures are intended to attach unemployed applicants to the labor market or maintain employment for the working poor prior to TANF enrollment, thereby promoting self-sufficiency among target groups. There are, however, concerns regarding the effectiveness of diversion tools in moving applicants into employment and targeting the appropriate groups. For example, scholarship on lump-sum cash payments suggests that those who are both the best and least equipped for the labor market are targeted, and a substantial number of recipients cycle onto TANF soon after receipt of the payment (Hetling, Ovwigho, & Born, 2007; Helting, Tracy, & Born, 2006; London, 2003). …', 'corpus_id': 154812840, 'score': 1}, {'doc_id': '219490502', 'title': 'Self-employment in the Covid-19 crisis: a CEP Covid-19 analysis', 'abstract': 'We report results from the LSE-CEP Survey of UK Self-employment May 2020.1 The self-employed have been hit particularly hard by the Covid-19 crisis, with approximately three quarters reporting less work in April 2020 than usual. The largest reductions in self-employment hours and income are among lower-income, older individuals without employees. There are no gender differences in the aggregate, however this is due to self-employed women being more likely to be able to work from home. Comparing men who can work from home against women who can work from home, women are more negatively affected than men. A third of self-employed workers have felt that their health was at risk while working during the coronavirus crisis, and this was significantly higher among those who work in app-based jobs. On average, higher-income workers are more likely to apply for the Coronavirus Self-employment Income Support Scheme. Over 40% of those who had not applied are unsure whether they are eligible. On average, the self-employed expect their work to return to normal in September 2020. A fifth think it will take until 2021 and 1 in 20 expect that their work will never return back to normal. The self-employed value income support highly. On average they are prepared to sacrifice 10% of their regular income to be guaranteed similar support for future shocks. Solo self-employed individuals value income support by twice as much as the self-employed with employees.', 'corpus_id': 219490502, 'score': 0}, {'doc_id': '155395953', 'title': 'Workfare and the Great Recession: Socioeconomic Outcomes among Black, White, and Hispanic Mothers in the Era of Work-First Welfare', 'abstract': None, 'corpus_id': 155395953, 'score': 1}, {'doc_id': '220792363', 'title': 'Employment Effects of Unemployment Insurance Generosity During the Pandemic', 'abstract': 'In response to the Covid-19 pandemic, the United States enacted the CARES Act, which expanded unemployment insurance (UI) benefits by providing a $600 weekly payment in addition to state unemployment benefits. We test whether changes in UI benefit generosity are associated with decreased employment, both at the onset of the benefits expansion and as businesses began to reopen. We use data from Homebase, a private firm that provides scheduling and time clock software to small businesses, which allows us to exploit high-frequency observations to understand how firms and workers respond to policy changes in real time. While our results show that relative declines in employment and hours occurred in mid-March, we find that the workers with higher post-CARES replacement rates did not experience larger declines in employment or hours of work when the benefits expansion went into effect. They have also returned to their previous jobs over time at similar rates as others.', 'corpus_id': 220792363, 'score': 1}, {'doc_id': '219441956', 'title': 'Gendered Understanding of Ebola Crisis in Sierra Leone. Lessons for COVID-19', 'abstract': 'This case study provides evidence in response to the research question: ""How did males and females in the bottom of the socio-economic hierarchy i.e. rural poor, respond to the 2014/2015 Ebola outbreak in Sierra Leone?"" This case study focuses on the following research sub-questions: â€¢ Caretaking Responsibilities: What kind of additional burdens were placed on rural men and women who took on the responsibility for orphaned children? â€¢ Income Earnings: How did market closures and restrictions on movements affect income earnings of males and females in the rural area? Based on the analysis of the Ebola crisis on rural poor men and women income and caretaking responsibilities highlighted in this case study, we can state that most women bore the costs of caretaking responsibilities. However, both women and men of Sierra Leone bore serious socio-economic costs at the level of their productive labor (income in this case study). The international community is presently dealing with the effect of the COVID-19 pandemic on marginalized communities, and the outcomes are still unknown. This brief case study helps to understand the gendered outcomes of previous public health epidemics in the context of social stratification. It is likely that many international humanitarian organizations will eventually aim at building economic and social resilience of impoverished communities with the focus on specific needs of different genders. An evidence from previous public health crises on the economy can help design the most efficient program interventions.', 'corpus_id': 219441956, 'score': 0}, {'doc_id': '218594699', 'title': 'Impact of Unemployment Insurance Benefit Generosity on Re-employment Wages during the Great Recession', 'abstract': 'This paper examines the impact of the replacement ratio on re-employment wages during the great recession. This is done using a data set from IPUMS CPS displaced workers supplement between 2005 and 2012. Using OLS analysis, I estimated the impacts of the replacement ratio on the ratio of re-employment wages to pre-unemployment wages. I found that a replacement ratio of one would lead to a 77.6% increase in the ratio of re-employment wages to pre-unemployment wages, without the consideration of any other variables. The findings of the replacement ratio support economic theory and contradict the findings of some major papers on the subject, including the paper I based my model off. The analysis in this paper could be strengthened with weekly wage data as well as with the addition of more variables controlling for the impacts of a recession.', 'corpus_id': 218594699, 'score': 1}]"
131	"{'doc_id': '235243446', 'title': ""Simple Reciprocal Fairness Message to Enhance Non-Donor's Willingness to Donate Blood."", 'abstract': ""BACKGROUND\nAgainst a background of declining blood donor numbers, recruiting new donors is critical for the effective operations of healthcare providers. Thus, interventions are needed to recruit new blood donors.\n\n\nPURPOSE\nWe provide initial evidence for Voluntary Reciprocal Altruism (VRA) to enhance nondonors' willingness to become blood donors. VRA interventions involve asking two questions: one on accepting a blood transfusion if needed and one on willingness to donate. As early trials often use self-reports of willingness to perform blood donation behavior, we derive a correction factor to better estimate actual behavior. Finally, we explore the effect of VRA interventions on two prosocial emotions: gratitude and guilt.\n\n\nMETHODS\nAcross three experiments (two in the UK and one in Australia: Total N = 1,208 nondonors) we manipulate VRA messages and explore how they affect both reported willingness to make a one-off or repeat blood donation and influence click through to blood donation, organ donation and volunteering registration sites (behavioral proxies). We report data from a longitudinal cohort (N = 809) that enables us to derive a correction for self-reported behavioral willingness.\n\n\nRESULTS\nAcross the three experiments, we show that exposure to a question that asks about accepting a transfusion if needed increased willingness to donate blood with some spillover to organ donor registration. We also show that gratitude has an independent effect on donation and report a behavioral correction factor of .10.\n\n\nCONCLUSIONS\nAsking nondonors about accepting a transfusion if needed is likely to be an effective strategy to increase new donor numbers."", 'corpus_id': 235243446}"	18965	"[{'doc_id': '233556062', 'title': 'Demystifying the effects of perceived risk and fear on customer engagement, co-creation and revisit intention during COVID-19: A protection motivation theory approach', 'abstract': ""Abstract Applying protection motivation theory, the purpose of this study is to investigate the effects of social media on customer brand engagement (CBE) and their consequent impact on co-creation and revisit intention in pandemic environment. This study also examines the moderation impact of fear of COVID-19 and perceived risk on associations between social media, CBE, and co-creation/revisit intention, thus further contributing to existing literature. Partial least squares structural equation modeling (PLS-SEM) was adopted to examine the data collected from key tourism-destinations in Jammu/Kashmir. The findings propose that social media positively and significantly effects the CBE, which subsequently affects co-creation and revisit intention in COVID-19 times. Second, findings found that CBE's positive impact on co-creation and revisit intention. Third, findings indicated the social media's indirect effect on co-creation and revisit intention, as mediated via CBE. Thus, social media's effect on co-creation and revisit intention are more prominent under elevated CBE in pandemics. Finally, fear of COVID-19 and perceived risk negatively moderates the linkage between social media, CBE, and co-creation/revisit intention. This study concludes with key implications arising from the analyses and further research opportunities."", 'corpus_id': 233556062, 'score': 0}, {'doc_id': '235950066', 'title': 'Charitable Giving: Framing and the Role of Information', 'abstract': 'We study the impact of information on the effectiveness of a taking frame in the context of charitable giving. In our laboratory experiment, either the decision maker (giving frame) or the recipient (taking frame) receives an endowment. In both cases, the decision maker can freely decide the final allocation of the money. In addition to the frame, we vary the level of information that we provide about the worthiness of the receiving charity. In keeping with our theoretical prediction, participants donate significantly more, when the decision is framed as taking rather than as giving. However, this framing effect is smaller, the more information we provide on the charity.', 'corpus_id': 235950066, 'score': 1}, {'doc_id': '235534174', 'title': 'Psychological antecedents of consumer trust in CRM campaigns and donation intentions: The moderating role of creativity', 'abstract': 'Abstract Cause-related marketing (CRM) refers to charitable donation contingent on consumer purchase of a product by supporting a specific cause that is linked to a non-profit organization (NPO). The present research examines the influence of consumer psychographic traits on trust in CRM campaign and its resulting impact on intention to donate money to NPO. The results indicate that collectivism and hedonism have positive effect on trust in CRM campaign, but individualism and utilitarianism have negative effect on trust in CRM campaign. Moreover, the result shows that trust in the CRM campaign had significant positive effect on donation intentions. We further have investigated the role of cognitive process and demonstrated the moderating effect of creativity on the impact of trust in CRM campaign and donation intentions, such that the higher level of creativity led to higher level of donation intentions, if consumers have trust in CRM campaign. This research offers marketers and advertising professionals’ practical insights to design effective CRM campaigns. Additionally, it assists NPO managers to understand the crucial role of trust and creativity in CRM campaigns and its positive impact on donation intentions. Academic and managerial implications of this research study along with future directions are discussed.', 'corpus_id': 235534174, 'score': 1}, {'doc_id': '235755049', 'title': 'Utilitas 1 Effectiveness and Demandingness', 'abstract': 'It has been argued in some recent work that there are many cases in which individuals are subject to conditional obligations to give to more effective rather than less effective charities, despite not being unconditionally obligated to give. These conditional obligations, it has been suggested, can allow effective altruists (EAs) to make the central claims about the ethics of charitable giving that characterize the movement without taking any particular position on morality’s demandingness. I argue that the range of cases involving charitable giving in which individuals are subject to conditional effectiveness obligations is in fact quite narrow. Because of this, I claim, EAs must endorse the view that well off people have at least fairly demanding unconditional obligations.', 'corpus_id': 235755049, 'score': 1}, {'doc_id': '235797315', 'title': 'Moral Judgments of COVID-19 Social Distancing Violations: The Roles of Perceived Harm and Impurity.', 'abstract': ""Can perceptions of impurity uniquely explain moral judgment? Or is moral judgment reducible to perceptions of harm? Whereas some perspectives posit that purity violations may drive moral judgment distinctly from harm violations, other perspectives contend that perceived harm is an essential precursor of moral condemnation. We tested these competing hypotheses through five preregistered experiments (total N = 2,944) investigating U.S. adults' perceptions of social distancing violations during the COVID-19 pandemic. Perceived harm was more strongly related to moral judgment than was perceived impurity. Nevertheless, over and above perceived harm, perceived impurity reliably explained unique variance in moral judgment. Effects of perceived harm and impurity were significant among both liberal and conservative participants but were larger among liberals. Results suggest that appraisals of both harm and impurity provide valuable insights into moral cognition. We discuss implications of these findings for dyadic morality, moral foundations, act versus character judgments, and political ideology."", 'corpus_id': 235797315, 'score': 0}, {'doc_id': '233480956', 'title': ""VIDEO ADS' CREATIVITY AND STRUCTURE INFLUENCE ON BRAND CONGRUENCE AND ENGAGEMENT"", 'abstract': ""Social networks play an important role in the life of today's societies and consumers are engaging more online with brands. Brands create and share information and special video ads on social networks, and, in the context of the COVID-19 pandemic, social networks allow brands to communicate with their consumers simply and quickly. Therefore, creativity and narrative structures are important for consumers. Brands are producing video ads that show consumers’ day context in order to obtain greater social media engagement. Considering that, this paper aims to study whether that goal is being achieved. The empirical research, from which we obtained 427 responses and which was tested using structural equations using the AMOS software, allows to conclude that creativity, the structure of the narrative and the consumer's congruence with the brand are determinants of engagement in social media. Further, presents practical and theoretical recommendations."", 'corpus_id': 233480956, 'score': 0}, {'doc_id': '236163314', 'title': '“Don’t Worry, We Are Here for You”: Brands as External Source of Control during the Covid-19 Pandemic', 'abstract': 'Abstract The Covid-19 pandemic increases consumers’ worries and makes them experience a loss of control over their lives. We investigate how these factors affect the roles that brands play in consumers’ lives. Results of a longitudinal survey (N\u2009=\u20095,393) and an online experiment (N\u2009=\u2009387) show that brands gain relevance and are more firmly included in consumers’ self-concepts if consumers experience more worries about Covid-19 and a lack of control. Brands can benefit from this by addressing worries associated with the crisis in their advertisements. This is particularly effective for consumers who express greater worry about the Covid-19 pandemic.', 'corpus_id': 236163314, 'score': 0}, {'doc_id': '235816638', 'title': 'Charity preferences and perceived impact moderate charitable giving and associated neural response', 'abstract': ""Charitable giving depends on individuals' abilities to make altruistic decisions. Previous studies suggest that altruism involves recruitment of neural resources in regions including social processing, reward/reinforcement learning, emotional response, and cognition. Despite evolutionary and social benefits to altruism, we know that humans do not always engage in altruistic behavior, like charitable giving. Understanding the underlying processes leading to decisions to donate is vital to improve prosocial community engagement. The present study examined how characteristics of the charitable giving opportunity influence an individual's decision to give and the neural engagement underlying these features. Twenty-nine participants subjectively rated ten charities on their value, effectiveness, and the subject's personal chance of donating. Participants then completed an fMRI task requiring them to decide to donate to certain charities given the probability of the donation helping, their personal preference for the charity, and whether the donation came at cost to themselves. There was a significant reduction in donating when the probability of helping was low versus high, and subjects were significantly less likely to donate to their lowest-rated charities. Further, probability of a donation being helpful and how much the subject favored a charity moderated PCC and left IFG engagement. Interestingly, reward neurocircuitry did not demonstrate similar sensitivity to these variations. These results may suggest individuals engage motivated reasoning to justify failure to donate, while donations are driven by emotion mentalizing that focuses on the welfare of others. This may provide valuable insight into how to engage individuals in altruistic giving."", 'corpus_id': 235816638, 'score': 1}, {'doc_id': '235469305', 'title': 'Determinants of Fan Engagement in Social Media-Based Brand Communities: A Brand Relationship Quality Perspective', 'abstract': 'This study adopts a brand relationship quality (BRQ) perspective to reveal the reason firms’ investments in social media-based brand communities should increase their social relationship marketing performances. An empirical analysis with 234 Facebook users who joined brand communities was conducted to examine the proposed hypotheses, revealing that fan needs fulfillments—information, entertainment, social interaction, and monetary ones—had positive effects on BRQ. Further, BRQ was found to have positive effects on fans’ engagement behavioral intentions toward brands, including willingness to buy, member continuance intention, and electronic word of mouth intention. This study contributes to existing research that indicates a new mechanism of BRQ improvement via the social media-based brand community. Implications corresponding to the research findings as well as study limitations and future directions are also addressed.', 'corpus_id': 235469305, 'score': 0}, {'doc_id': '235512188', 'title': 'How reward uncertainty influences subsequent donations: The role of mental accounting', 'abstract': 'Abstract It is common in the marketplace for merchants to reward consumers whose spending reaches a certain level. This study investigates how reward uncertainty affects consumers’ intentions to donate money to charity after they receive a reward. Across one field experiment and three laboratory experiments, this study demonstrates that consumers are more likely to donate after receiving uncertain rewards than certain ones, because they treat uncertain rewards as financial gains, separate from their payment accounts; certain rewards are expected during consumption, and thus perceived as payment deductions. Therefore, uncertain rewards (vs. certain rewards) induce lower (vs. higher) psychological pain from donating money, which leads to higher (vs. lower) donation intentions. These effects do not apply to people with a calculative mindset (i.e., those who calculated their true expenses before being asked to donate).', 'corpus_id': 235512188, 'score': 1}]"
132	{'doc_id': '152166960', 'title': 'The Alternative Epistemologies of Data Activism', 'abstract': 'Abstract As datafication progressively invades all spheres of contemporary society, citizens grow increasingly aware of the critical role of information as the new fabric of social life. This awareness triggers new forms of civic engagement and political action that we term “data activism”. Data activism indicates the range of sociotechnical practices that interrogate the fundamental paradigm shift brought about by datafication. Combining Science and Technology Studies with Social Movement Studies, this theoretical article offers a foretaste of a research agenda on data activism. It foregrounds democratic agency vis-à-vis datafication, and unites under the same label ways of affirmative engagement with data (“proactive data activism”, e. g. databased advocacy) and tactics of resistance to massive data collection (“reactive data activism”, e. g. encryption practices), understood as a continuum along which activists position and reposition themselves and their tactics. The article argues that data activism supports the emergence of novel epistemic cultures within the realm of civil society, making sense of data as a way of knowing the world and turning it into a point of intervention and generation of data countercultures. It offers the notion of data activism as a heuristic tool for the study of new forms of political participation and civil engagement in the age of datafication, and explores data activism as an evolving theoretical construct susceptible to contestation and revision.', 'corpus_id': 152166960}	20864	"[{'doc_id': '53207813', 'title': 'Data Science as Political Action: Grounding Data Science in a Politics of Justice', 'abstract': 'In response to recent controversies, the field of data science has rushed to adopt codes of ethics. Such professional codes, however, are ill-equipped to address broad matters of social justice. Instead of ethics codes, I argue, the field must embrace politics. Data scientists must recognize themselves as political actors engaged in normative constructions of society and, as befits political work, evaluate their work according to its downstream material impacts on people\'s lives. I justify this notion in two parts: first, by articulating why data scientists must recognize themselves as political actors, and second, by describing how the field can evolve toward a deliberative and rigorous grounding in a politics of social justice. Part 1 responds to three arguments that are commonly invoked by data scientists when they are challenged to take political positions regarding their work. In confronting these arguments, I will demonstrate why attempting to remain apolitical is itself a political stance--a fundamentally conservative one--and why the field\'s current attempts to promote ""social good"" dangerously rely on vague and unarticulated political assumptions. Part 2 proposes a framework for what a politically-engaged data science could look like and how to achieve it, recognizing the challenge of reforming the field in this manner. I conceptualize the process of incorporating politics into data science in four stages: becoming interested in directly addressing social issues, recognizing the politics underlying these issues, redirecting existing methods toward new applications, and, finally, developing new practices and methods that orient data science around a mission of social justice. The path ahead does not require data scientists to abandon their technical expertise, but it does entail expanding their notions of what problems to work on and how to engage with society.', 'corpus_id': 53207813, 'score': 1}, {'doc_id': '237393717', 'title': 'Crowdfunding Platforms in the Political Economy of Alternative Media', 'abstract': 'Alternative media appeared in the digital environment, taking advantage of the low distribution costs and the potential for public participation. Increasingly, they use crowdfunding or collective financing platforms as a model for maintaining activities. This article maps how alternative media from Portugal, Spain and Brazil articulate crowdfunding platforms with their financing strategies, on the one hand, and with social media platforms, on the other. The study aims to discuss the possibilities and limitations of the different crowdfunding platforms for these means. The methodological design includes personal interviews, online observation about alternative media and a content analysis about their financing and specifically the digital financing platforms. Alternative media articulate their use of crowdfunding platforms with those of social networks, in a constant work of demonstrating the relevance of the type of journalism they practice and their social contribution in search of public involvement as a funder.', 'corpus_id': 237393717, 'score': 0}, {'doc_id': '216348451', 'title': 'Design Justice', 'abstract': None, 'corpus_id': 216348451, 'score': 1}, {'doc_id': '237967613', 'title': 'Using Big Data in Social Science - Perspectives and Limitations', 'abstract': 'The article provides an overview of the development of big data technologies in terms of the potential of their use in the study of social processes. The development of these technologies makes it necessary to transform the usual methods of scientific research and revise the models of social reality. To meet the demands of the modern world, the researcher needs to adopt digital tools. However, the relevance of the stated topic is not limited solely to the possibilities, since the use of digital technologies in the study of society is associated with many risks that can lead to negative consequences. Speaking about the sphere of big data, it is important to remember that one of the main risks is the violation of the rights and freedoms of other people, therefore, a researcher of social processes must understand and assess the consequences of his actions, guided, first of all, by ethical norms that allow the use of new technologies for the public. the benefits and suppression of the threats of a technogenic society. The authors propose to consider the complex of risks associated with the use of big data technologies, and also present their own approach to their systematization and classification.', 'corpus_id': 237967613, 'score': 0}, {'doc_id': '199020420', 'title': 'Indigenous Data Governance: Strategies from United States Native Nations', 'abstract': 'Data have become the new global currency, and a powerful force in making decisions and wielding power. As the world engages with open data, big data reuse, and data linkage, what do data-driven futures look like for communities plagued by data inequities? Indigenous data stakeholders and non-Indigenous allies have explored this question over the last three years in a series of meetings through the Research Data Alliance (RDA). Drawing on RDA and other gatherings, and a systematic scan of literature and practice, we consider possible answers to this question in the context of Indigenous peoples vis-a-vis two emerging concepts: Indigenous data sovereignty and Indigenous data\xa0governance.\xa0Specifically,\xa0we\xa0focus\xa0on\xa0the\xa0data\xa0challenges facing Native nations and the intersection of data, tribal sovereignty, and power. Indigenous data sovereignty is the right of each Native nation to govern the collection, ownership, and application of the tribe’s data. Native nations exercise Indigenous data sovereignty through the interrelated processes of Indigenous data governance and decolonizing data. This paper explores the implications of Indigenous data sovereignty and Indigenous data governance for Native nations and others. We argue for the repositioning of authority over Indigenous data back to Indigenous peoples. At the same time, we recognize that\xa0 there are significant obstacles to rebuilding effective Indigenous data systems and the process will require resources, time, and partnerships among Native nations, other governments, and data agents.', 'corpus_id': 199020420, 'score': 1}, {'doc_id': '237334743', 'title': 'Social News Use & Citizen Participation among Young Activists in Singapore', 'abstract': 'This article presents a study of how civically engaged young adults engage with news on social media, within the context of a developing democracy – Singapore. Based on in-depth interviews with 20 young activists, it discusses how they approach social media as a source of news, what motivates them to engage in more than one social news platform, and how social news use fits into their political lexicon. The results reveal that despite their affinity towards news-related content on social media, they are neither partial towards mainstream, nor alternative news providers on this medium. Their primary social news platform is perceived to offer the best means to disseminate news-related information. However, they are also concerned about their privacy and practice certain strategies to mitigate this. Despite its drawbacks, the activists accept social news use as a viable means of political socialisation and mobilisation.', 'corpus_id': 237334743, 'score': 0}, {'doc_id': '225739086', 'title': 'Indigenous Data Sovereignty: Tools for Transparency', 'abstract': '""Indigenous Data Sovereignty: Tools for Transparency"" was presented on June 9, 2020 via webinar.', 'corpus_id': 225739086, 'score': 1}, {'doc_id': '224895413', 'title': 'Indigenous Data Sovereignty and Policy', 'abstract': 'This book examines how Indigenous Peoples around the world are demanding greater data sovereignty, and challenging the ways in which governments have historically used Indigenous data to develop policies and programs. In the digital age, governments are increasingly dependent on data and data analytics to inform their policies and decision-making. However, Indigenous Peoples have often been the unwilling targets of policy interventions and have had little say over the collection, use and application of data about them, their lands and cultures. At the heart of Indigenous Peoples’ demands for change are the enduring aspirations of self-determination over their institutions, resources, knowledge and information systems. With contributors from Australia, Aotearoa New Zealand, North and South America and Europe, this book offers a rich account of the potential for Indigenous data sovereignty to support human flourishing and to protect against the ever-growing threats of data-related risks and harms.', 'corpus_id': 224895413, 'score': 1}, {'doc_id': '237055539', 'title': 'Corporate Actors as Translators in Transnational Lawmaking', 'abstract': 'International legal scholars have long been concerned with the transnational lawmaking process, including the development, interpretation, and implementation of international norms. Yet there has been insufficient attention devoted to the micro-level details by which international law operates. Anthropologists can shed unique insights to this process by uncovering power dynamics, disaggregating institutions and actors, and revealing local practices on the ground. In this essay, I will analyze global supply chain governance through an ethnographic lens in order to examine the role of corporate actors as translators of international law. I argue that an anthropological approach can illuminate how corporations shape international law in practice by uncovering technologies of governance, relations of power, and chains of translation in the transnational lawmaking process.', 'corpus_id': 237055539, 'score': 0}, {'doc_id': '238054561', 'title': 'Activism in the Digital Age', 'abstract': 'Social movements have been transformed in the last decade by social networks, where the dynamics of the social protests have evolved and have been structured and viralized through social media. They are no longer just conversations between activists that stay on social platforms. The cyberactivism that takes place on Twitter or Instagram can also play a significant role in general society by influencing government decision making or shaping the relationships between citizens. In this chapter, the authors explore the main activist movements that took place in social media in the last decade: Occupy, BlackLivesMatter, and MeToo. The proposed approach used in this study facilitates the comparison of each movement while focusing on the user-generated content in social media. This study suggests the presence of four major categories to frame the content generated by the activists. The chapter concludes with the identification of three different approaches to the research of a future research agenda that should be considered for the study of the social movements from the UGC theory framework.', 'corpus_id': 238054561, 'score': 0}]"
133	{'doc_id': '222290746', 'title': 'High-Fidelity 3D Digital Human Creation from RGB-D Selfies', 'abstract': 'We present a fully automatic system that can produce high-fidelity, photo-realistic 3D digital human characters with a consumer RGB-D selfie camera. The system only needs the user to take a short selfie RGB-D video while rotating his/her head, and can produce a high quality reconstruction in less than 30 seconds. Our main contribution is a new facial geometry modeling and reflectance synthesis procedure that significantly improves the state-of-the-art. Specifically, given the input video a two-stage frame selection algorithm is first employed to select a few high-quality frames for reconstruction. A novel, differentiable renderer based 3D Morphable Model (3DMM) fitting method is then applied to recover facial geometries from multiview RGB-D data, which takes advantages of extensive data generation and perturbation. Our 3DMM has much larger expressive capacities than conventional 3DMM, allowing us to recover more accurate facial geometry using merely linear bases. For reflectance synthesis, we present a hybrid approach that combines parametric fitting and CNNs to synthesize high-resolution albedo/normal maps with realistic hair/pore/wrinkle details. Results show that our system can produce faithful 3D characters with extremely realistic details. Code and the constructed 3DMM is publicly available.', 'corpus_id': 222290746}	16690	"[{'doc_id': '233210608', 'title': 'Pixel Codec Avatars', 'abstract': 'Telecommunication with photorealistic avatars in virtual or augmented reality is a promising path for achieving authentic face-to-face communication in 3D over remote physical distances. In this work, we present the Pixel Codec Avatars (PiCA): a deep generative model of 3D human faces that achieves state of the art reconstruction performance while being computationally efficient and adaptive to the rendering conditions during execution. Our model combines two core ideas: (1) a fully convolutional architecture for decoding spatially varying features, and (2) a renderingadaptive per-pixel decoder. Both techniques are integrated via a dense surface representation that is learned in a weakly-supervised manner from low-topology mesh tracking over training images. We demonstrate that PiCA improves reconstruction over existing techniques across testing expressions and views on persons of different gender and skin tone. Importantly, we show that the PiCA model is much smaller than the state-of-art baseline model, and makes multi-person telecommunicaiton possible: on a single Oculus Quest 2 mobile VR headset, 5 avatars are rendered in realtime in the same scene.', 'corpus_id': 233210608, 'score': 1}, {'doc_id': '235593318', 'title': 'Normalized Avatar Synthesis Using StyleGAN and Perceptual Refinement', 'abstract': ""We introduce a highly robust GAN-based framework for digitizing a normalized 3D avatar of a person from a single unconstrained photo. While the input image can be of a smiling person or taken in extreme lighting conditions, our method can reliably produce a high-quality textured model of a person's face in neutral expression and skin textures under diffuse lighting condition. Cutting-edge 3D face reconstruction methods use non-linear morphable face models combined with GAN-based decoders to capture the likeness and details of a person but fail to produce neutral head models with unshaded albedo textures which is critical for creating relightable and animation-friendly avatars for integration in virtual environments. The key challenges for existing methods to work is the lack of training and ground truth data containing normalized 3D faces. We propose a two-stage approach to address this problem. First, we adopt a highly robust normalized 3D face generator by embedding a non-linear morphable face model into a StyleGAN2 network. This allows us to generate detailed but normalized facial assets. This inference is then followed by a perceptual refinement step that uses the generated assets as regularization to cope with the limited available training samples of normalized faces. We further introduce a Normalized Face Dataset, which consists of a combination photogrammetry scans, carefully selected photographs, and generated fake people with neutral expressions in diffuse lighting conditions. While our prepared dataset contains two orders of magnitude less subjects than cutting edge GAN-based 3D facial reconstruction methods, we show that it is possible to produce high-quality normalized face models for very challenging unconstrained input images, and demonstrate superior performance to the current state-of-the-art."", 'corpus_id': 235593318, 'score': 1}, {'doc_id': '235651790', 'title': 'Real-time 3D neural facial animation from binocular video', 'abstract': 'We present a method for performing real-time facial animation of a 3D avatar from binocular video. Existing facial animation methods fail to automatically capture precise and subtle facial motions for driving a photo-realistic 3D avatar ""in-the-wild"" (i.e., variability in illumination, camera noise). The novelty of our approach lies in a light-weight process for specializing a personalized face model to new environments that enables extremely accurate real-time face tracking anywhere. Our method uses a pre-trained high-fidelity personalized model of the face that we complement with a novel illumination model to account for variations due to lighting and other factors often encountered in-the-wild (e.g., facial hair growth, makeup, skin blemishes). Our approach comprises two steps. First, we solve for our illumination model\'s parameters by applying analysis-by-synthesis on a short video recording. Using the pairs of model parameters (rigid, non-rigid) and the original images, we learn a regression for real-time inference from the image space to the 3D shape and texture of the avatar. Second, given a new video, we fine-tune the real-time regression model with a few-shot learning strategy to adapt the regression model to the new environment. We demonstrate our system\'s ability to precisely capture subtle facial motions in unconstrained scenarios, in comparison to competing methods, on a diverse collection of identities, expressions, and real-world environments.', 'corpus_id': 235651790, 'score': 0}, {'doc_id': '234742487', 'title': 'Fast-GANFIT: Generative Adversarial Network for High Fidelity 3D Face Reconstruction', 'abstract': 'A lot of work has been done towards reconstructing the 3D facial structure from single images by capitalizing on the power of Deep Convolutional Neural Networks (DCNNs). In the recent works, the texture features either correspond to components of a linear texture space or are learned by auto-encoders directly from in-the-wild images. In all cases, the quality of the facial texture reconstruction is still not capable of modeling facial texture with high-frequency details. In this paper, we take a radically different approach and harness the power of Generative Adversarial Networks (GANs) and DCNNs in order to reconstruct the facial texture and shape from single images. That is, we utilize GANs to train a very powerful facial texture prior from a large-scale 3D texture dataset. Then, we revisit the original 3D Morphable Models (3DMMs) fitting making use of non-linear optimization to find the optimal latent parameters that best reconstruct the test image but under a new perspective. In order to be robust towards initialisation and expedite the fitting process, we propose a novel self-supervised regression based approach. We demonstrate excellent results in photorealistic and identity preserving 3D face reconstructions and achieve for the first time, to the best of our knowledge, facial texture reconstruction with high-frequency details.', 'corpus_id': 234742487, 'score': 1}, {'doc_id': '233864889', 'title': 'Inverting Generative Adversarial Renderer for Face Reconstruction', 'abstract': 'Given a monocular face image as input, 3D face geometry reconstruction aims to recover a corresponding 3D face mesh. Recently, both optimization-based and learningbased face reconstruction methods have taken advantage of the emerging differentiable renderer and shown promising results. However, the differentiable renderer, mainly based on graphics rules, simplifies the realistic mechanism of the illumination, reflection, etc., of the real world, thus cannot produce realistic images. This brings a lot of domainshift noise to the optimization or training process. In this work, we introduce a novel Generative Adversarial Renderer (GAR) and propose to tailor its inverted version to the general fitting pipeline, to tackle the above problem. Specifically, the carefully designed neural renderer takes a face normal map and a latent code representing other factors as inputs and renders a realistic face image. Since the GAR learns to model the complicated real-world image, instead of relying on the simplified graphics rules, it is capable of producing realistic images, which essentially inhibits the domain-shift noise in training and optimization. Equipped with the elaborated GAR, we further proposed a novel approach to predict 3D face parameters, in which we first obtain fine initial parameters via Renderer Inverting and then refine it with gradient-based optimizers. Extensive experiments have been conducted to demonstrate the effectiveness of the proposed generative adversarial renderer and the novel optimization-based face reconstruction framework. Our method achieves state-of-the-art performances on multiple face reconstruction datasets.', 'corpus_id': 233864889, 'score': 1}, {'doc_id': '235606219', 'title': '3D human tongue reconstruction from single ""in-the-wild"" images', 'abstract': '3D face reconstruction from a single image is a task that has garnered increased interest in the Computer Vision community, especially due to its broad use in a number of applications such as realistic 3D avatar creation, pose invariant face recognition and face hallucination. Since the introduction of the 3D Morphable Model in the late 90’s, we witnessed an explosion of research aiming at particularly tackling this task. Nevertheless, despite the increasing level of detail in the 3D face reconstructions from single images mainly attributed to deep learning advances, finer and highly deformable components of the face such as the tongue are still absent from all 3D face models in the literature, although being very important for the realness of the 3D avatar representations. In this work we present the first, to the best of our knowledge, end-to-end trainable pipeline that accurately reconstructs the 3D face together with the tongue. Moreover, we make this pipeline robust in “in-thewild” images by introducing a novel GAN method tailored for 3D tongue surface generation. Finally, we make publicly available to the community the first diverse tongue *Authors contributed equally. dataset, consisting of 1, 800 raw scans of 700 individuals varying in gender, age, and ethnicity backgrounds *. As we demonstrate in an extensive series of quantitative as well as qualitative experiments, our model proves to be robust and realistically captures the 3D tongue structure, even in adverse “in-the-wild” conditions.', 'corpus_id': 235606219, 'score': 0}, {'doc_id': '233219657', 'title': 'Shape and Material Capture at Home', 'abstract': 'In this paper, we present a technique for estimating the geometry and reflectance of objects using only a camera, flashlight, and optionally a tripod. We propose a simple data capture technique in which the user goes around the object, illuminating it with a flashlight and capturing only a few images.Our main technical contribution is the introduction of a recursive neural architecture, which can predict geometry and reflectance at 2 × 2 resolution given an input image at 2×2 and estimated geometry and reflectance from the previous step at 2k−1×2k−1. This recursive architecture, termed RecNet, is trained with 256×256 resolution but can easily operate on 1024×1024 images during inference. We show that our method produces more accurate surface normal and albedo, especially in regions of specular highlights and cast shadows, compared to previous approaches, given three or fewer input images. Our model and code is available at https://dlichy.github.io/ ShapeAndMaterialAtHome/.', 'corpus_id': 233219657, 'score': 0}, {'doc_id': '229332112', 'title': 'Relightable 3D Head Portraits from a Smartphone Video', 'abstract': 'In this work, a system for creating a relightable 3D portrait of a human head is presented. Our neural pipeline operates on a sequence of frames captured by a smartphone camera with the flash blinking (“flash-no flash” sequence). A coarse point cloud reconstructed via structurefrom-motion software and multi-view denoising is then used as a geometric proxy. Afterwards, a deep rendering network is trained to regress dense albedo, normals, and environmental lighting maps for arbitrary new viewpoints. Effectively, the proxy geometry and the rendering network constitute a relightable 3D portrait model, that can be synthesized from an arbitrary viewpoint and under arbitrary lighting, e.g. directional light, point light, or an environment map. The model is fitted to the sequence of frames with human face-specific priors that enforce the plausability of albedo-lighting decomposition and operates at the interactive frame rate. We evaluate the performance of the method under varying lighting conditions and at the extrapolated viewpoints and compare with existing relighting methods.', 'corpus_id': 229332112, 'score': 1}, {'doc_id': '233296899', 'title': 'StylePeople: A Generative Model of Fullbody Human Avatars', 'abstract': 'We propose a new type of full-body human avatars, which combines parametric mesh-based body model with a neural texture. We show that with the help of neural textures, such avatars can successfully model clothing and hair, which usually poses a problem for mesh-based approaches. We also show how these avatars can be created from multiple frames of a video using backpropagation. We then propose a generative model for such avatars that can be trained from datasets of images and videos of people. The generative model allows us to sample random avatars as well as to create dressed avatars of people from one or few images. The code for the project is available at saic-violet.github.io/style-people.', 'corpus_id': 233296899, 'score': 0}, {'doc_id': '236986882', 'title': 'SIDER: Single-Image Neural Optimization for Facial Geometric Detail Recovery', 'abstract': 'We present SIDER (Single-Image neural optimization for facial geometric DEtail Recovery), a novel photometric optimization method that recovers detailed facial geometry from a single image in an unsupervised manner. Inspired by classical techniques of coarse-to-fine optimization and recent advances in implicit neural representations of 3D shape, SIDER combines a geometry prior based on statistical models and Signed Distance Functions (SDFs) to recover facial details from single images. First, it estimates a coarse geometry using a morphable model represented as *Equal Contribution an SDF. Next, it reconstructs facial geometry details by optimizing a photometric loss with respect to the ground truth image. In contrast to prior work, SIDER does not rely on any dataset priors and does not require additional supervision from multiple views, lighting changes or ground truth 3D shape. Extensive qualitative and quantitative evaluation demonstrates that our method achieves state-of-the-art on facial geometric detail recovery, using only a single inthe-wild image. 1 ar X iv :2 10 8. 05 46 5v 1 [ cs .C V ] 1 1 A ug 2 02 1', 'corpus_id': 236986882, 'score': 0}]"
134	{'doc_id': '213580854', 'title': 'E-procurement in small and medium sized enterprises; facilitators, obstacles and effect on performance', 'abstract': 'The purpose of this paper is to analyze e-procurement in small and medium-sized enterprises (SMEs) and its relationship with top management support, IT obstacles and strategic purchasing and the effect of e-procurement on performance (procurement performance and business performance).,The hypotheses were tested using a sample of 199 managers from SMEs in manufacturing.,The results indicated a significant relationship between e-procurement in SMEs and top management support, IT obstacles and strategic purchasing. Similarly, the authors found a positive relationship between e-procurement and procurement process performance and business performance.,The findings stress to SME managers, the need to pay attention to top management support, IT obstacles and strategic purchasing when implementing e-procurement. Similarly, it provides evidence of the benefits of e-procurement on procurement process performance and business performance.,This study fills a gap in the literature regarding e-procurement in SMEs and its impact on performance. SMEs constitute a significant part of today’s economies and e-procurement can significantly impact the performance of these organizations.', 'corpus_id': 213580854}	4782	"[{'doc_id': '214714491', 'title': 'Sustainable Banking; Evaluation of the European Business Models', 'abstract': 'Sustainability has become one of the challenges of today’s banks. Since sustainable business models are responsible for the environment and society along with generating economic benefits, they are an attractive approach to sustainability. Sustainable business models also offer banks competitive advantages such as increasing brand reputation and cost reduction. However, no framework is presented to evaluate the sustainability of banking business models. To bridge this theoretical gap, the current study using A Delphi-Analytic Hierarchy Process method, firstly, developed a sustainable business model to evaluate the sustainability of the business model of banks. In the second step, the sustainability performance of sixteen banks from eight European countries including Norway, The UK, Poland, Hungary, Germany, France, Spain, and Italy, assessed. The proposed business model components of this study were ranked in terms of their impact on achieving sustainability goals. Consequently, the proposed model components of this study, based on their impact on sustainability, are respectively value proposition, core competencies, financial aspects, business processes, target customers, resources, technology, customer interface, and partner network. The results of the comparison of the banks studied by each country disclosed that the sustainability of the Norwegian and German banks’ business models is higher than in other counties. The studied banks of Hungary and Spain came in second, the banks of The UK, Poland, and France ranked third, and finally, the Italian banks ranked fourth in the sustainability of their business models.', 'corpus_id': 214714491, 'score': 0}, {'doc_id': '54956479', 'title': 'The nature of embedded purchasing activities in SMEs: results from a Dutch multiple case study : Working paper WP 131 for the 24th IPSERA conference, Amsterdam 2015', 'abstract': 'Aims: identify and explain purchasing-oriented patterns in Small and Medium Sized Enterprises (SMEs) via case study research.\nScope: Using a conceptual framework and empirical research this article proposes a series of purchasing-oriented patterns in SMEs. These patterns align activities to achieve the SMEs proposed value proposition towards customers and activities to purchase resources needed for realizing the value proposition.\nStructure: This paper introduces the research topic. It discusses a conceptual framework and theory. It then continues with the methodology to collect and analyse case study data and describes empirical finding. It discusses these findings related to the framework and literature and ends with summarizing first conclusions.\nConclusion: The SMEs in the dataset use four types of purchasing-oriented patterns related to their customer value propositions These SMEs can strive for low transaction costs can but invest in extrinsic product attributes to realize their value proposition. Both the transaction cost theory and the resource based view help to explain the purchasing-oriented patterns. Further research is needed to strengthen and validate findings.', 'corpus_id': 54956479, 'score': 1}, {'doc_id': '166910121', 'title': 'Implementation of Customized EPM for Collaborative Resource Procurement in Small and Medium Size Plant Construction Enterprises', 'abstract': 'The need for total enterprise management that covers from accepting orders, design, cost, procurement, manufacture, and delivery has been recently raised in order for efficient project management of small and medium size plant construction enterprises. This trend is reflected in the introduction of EPM(Enterprise Project Management) solutions by some shipbuilding enterprises and large construction corporations. In step with the trend, small and medium size plant construction enterprises are enabled to provide knowledge base in terms of reducing cost and production period and managing portfolio for accepting orders and attaining contract by inducing customized EPM This study presents a design of customized EPM based on a case study of a company. It exhibits a collaborative/collaboration resource-supply model which can be universally applied to small and medium size job-order production enterprises. The research discusses that under the assumption that information is shared internally between departments on planning, supply, manufacture, and quality control in order to create collaborative environment and close participation is created externally between suppliers and orderers, productivity can be improved in cost reduction through planning efficient collaborative supply model.', 'corpus_id': 166910121, 'score': 1}, {'doc_id': '167776889', 'title': 'Study on Logistics Purchase of SMEs in Ningde Motor Industry Clusters', 'abstract': 'Logistics purchasing management communicating the production and supply, is the key content of the supply chain.This paper analysis the role of purchase in the logistics system, and then put forward the problems in logistics purchase of Ningde motor enterprises combining with the current situation of Ningde motor industry clusters, and advanced the countermeasures of the logistics purchasing process.', 'corpus_id': 167776889, 'score': 1}, {'doc_id': '215737230', 'title': 'Current Practices in the Information Collection for Enterprise Architecture Management', 'abstract': ""The digital transformation influences business models, processes, and enterprise IT landscape as a whole. Therefore, business-IT alignment is becoming more important than ever before. Enterprise architecture management (EAM) is designed to support and improve this business-IT alignment. The success of EAM crucially depends on the information available about a company's enterprise architecture, such as infrastructure components, applications, and business processes. This paper discusses the results of a qualitative expert survey with 26 experts in the field of EAM. The goal of this survey was to highlight current practices in the information collection for EAM and identify relevant information from enterprise-external data sources. The results provide a comprehensive overview of collected and utilized information in the industry, including an assessment of the relevance of such information. Furthermore, the results highlight challenges in practice and point out investments that organizations plan in the field of EAM."", 'corpus_id': 215737230, 'score': 0}, {'doc_id': '215403709', 'title': 'Insurance uptake among small and medium-sized tourism and hospitality enterprises in a resource-scarce environment', 'abstract': ""\n Abstract\n \n Small and medium-sized tourism and hospitality enterprises (SMTHEs) are often susceptible to various hazards, which result in risk concerns. Insurance is recognised as one of the risk management strategies, but evidence indicates that insurance uptake among SMTHEs has been low. Yet, researchers have hardly researched into the factors that influence insurance uptake among SMTHEs. Two-hundred and fifty (250) respondents were selected using a multi-stage sampling technique. Confirmatory factor analysis, multivariate logit and probit regression techniques were used to determine factors underlying SMTHEs' insurance uptake. Risk concerns, the firm's characteristics, the perceived benefits of insurance and other informal risk coping mechanisms, as well as insurance service provision concerns were identified as determinants of insurance uptake. This is one of the first papers to offer a holistic understanding of the factors influencing SMTHEs' insurance subscription in a resource-scarce destination of Sub-Saharan Africa. The practical and theoretical implications of the paper are discussed.\n \n"", 'corpus_id': 215403709, 'score': 0}, {'doc_id': '216081373', 'title': 'Digitalization in management accounting and control: an editorial', 'abstract': 'Digitalization has the potential to disrupt the management accounting domain. It may not only affect the digital landscape of the organization and the associated business models, but also management accounting and control practices as well as the role of the controller. This editorial discusses these developments by introducing the concept of digitalization and describing its impact on the field of management accounting and control.', 'corpus_id': 216081373, 'score': 0}, {'doc_id': '213923578', 'title': 'Management of Green Procurement in Small and Medium-Sized Manufacturing Enterprises in Developing Economies', 'abstract': 'This article explores the issue of managing green procurement at the level of small and medium-sized enterprises in developing countries. Green procurement remains a relatively new trend for small and medium-sized enterprises (SMEs). Therefore, a thorough analysis of this issue is a topic theme. In this perspective, the following hypotheses are highlighted: green procurement affects the competitiveness of an enterprise, its economic indicators, the environment and society. The data collected from SMEs in Kyiv and Kyiv region. As a result, we analysed the questionnaires provided by the managers of 181 companies operating in different industries and different forms of ownership. Factor analysis was conducted to evaluate the scales used in the study, and hypotheses were tested using structural equation modelling. The results show that green procurement has a positive impact on competitiveness, economic performance of SMEs, the environment and society. Therefore, small and medium-sized enterprises in developing countries should pay attention to the development and implementation of green supplies, as this will have a positive impact on their performance. The research conducted can serve as a benchmark for conducting similar studies on other enterprises.', 'corpus_id': 213923578, 'score': 1}, {'doc_id': '167175787', 'title': 'Factors that influence the use of e-procurement in manufacturing SMEs at Northern Region', 'abstract': 'E-procurement is one of the ranges of government measures to facilitate the process of purchasing goods online. E- \nprocurement is an important aspect for manufacturing \ncompanies to improve the buying process in their supply chain while improving the performance of their companies. With the advancement of internet technology, it can be \nused in all places, including in the company or business. Small Medium Enterprise is a major contributor to the national economy. SME provide business opportunities and \nincrease Gross Domestic Product by the opportunity given to the local community. The use of e -procurement within the company allows them to reduce costs and save time in \npurchasing process. E -procurement introduced by the government in 2000 under the MSC Flagship. In this study, researchers conducted a study on the factors influencing the \nuse of e -procurement in the manufacturing SMEs in the northern corridor. In this study, TAM model is used to study the factors that influence the use of e -procurement. Among the factors that influence the use is perceive ease of use, perceive usefulness, attitude and behavioral intention. In this study, researchers used SPSS 20 to analyze the data and using non-parametric statistics to measure the influence and strength of the relationship between the independent variables.', 'corpus_id': 167175787, 'score': 1}]"
135	{'doc_id': '149335569', 'title': 'Baseline Explorations in the Spatial Construal of Time', 'abstract': 'Baseline Explorations in the Spatial Construal of Time Benjamin A. Motz (bmotz@cogsci.ucsd.edu) Rafael E. Nunez (nunez@cogsci.ucsd.edu) Department of Cognitive Science, University of California, San Diego 9500 Gilman Dr., La Jolla, CA 92093-0515 Introduction Due in part to theoretical advances in cognitive linguistics (e.g., Lakoff & Johnson, 1980; Nunez, 1999) and recent studies in cognitive psychology (Boroditsky, 2000; Gentner, 2001), it has become well accepted that humans apply spatial principles to their conception of time. Statements such as, “Cogsci 2004 has arrived,” reflect a mental model in which temporal abstractions are given meaning via spatial metaphors. Examining these metaphors, Boroditsky (2000) used spatial priming to influence subjects’ interpretation of “forward” (as earlier or later) when disambiguating the meaning of Move the meeting forward. She reported the baseline interpretation of “forward” to be “about evenly split” (p. 9) between earlier (45.7%) and later (54.3%) when subjects received no priming. As part of a larger research project, the present report focuses on these baseline interpretations. In order to standardize the responses and improve replicability, subjects were shown a graphic display with no reference to objects moving toward an observer, or an observer moving toward objects—the two situations widely believed to exclusively influence conceptual models when disambiguating the meaning of “forward” (Gentner, 2001). Methods 66 undergraduate students at the University of California, San Diego participated in the study as part of their course requirements. Subjects were shown a static display of five stationary colored boxes. Two of the boxes contained balls; the others were empty. During this presentation, subjects were asked to respond to five questions, including: What is the color of the box containing the black ball? and How many boxes are in the display? Immediately following these presentations, subjects were instructed to turn to subsequent pages in their questionnaires containing the following two target questions, the order of which was balanced across subjects. Next Wednesday’s meeting has been moved forward two days. On what day will the meeting now take place? Tomorrow’s 12:00 (noon) meeting has been moved forward two hours. At what time will the meeting now take place? Results and Discussion Subjects’ responses to the target question are shown in Table 1. Chi-square analyses indicated that responses are significantly different from those expected by chance when the question on the scale of days is asked first (p < 0.005). This is not the case when preceded by the question on the scale of hours. Table 1: Number of responses by order of target questions. Monday Friday 10:00am 2:00pm Day then Hour Hour then Day It is widely believed that time is construed specifically in terms of observers and motion (Gentner, 2001). Our methodology, which bore no explicit reference to such concepts, elicited responses significantly different from those expected by chance. Furthermore, this difference is sensitive to the time scale of the target question. These findings suggest that the specific spatial metaphors for time are more complex than previously assumed. The data in the upper left cells of Table 1, which are analogous to Boroditsky’s (2000) baseline data, can hardly be interpreted as “about evenly split.” This suggests that there may be many pragmatic constraints to be considered and that there are more complex variations of conceptual mappings from space to time. Without making claims as to exact baseline responses to ambiguous questions about time, the present findings suggest a reconsideration of what is necessary and sufficient to elicit priming effects in the spatial construal of time. Acknowledgements The authors are grateful to Ursina Teuscher for her helpful comments and involvement. References Boroditsky, L. (2000). Metaphoric structuring: Understanding time through spatial metaphors. Cognition, Gentner, D. (2001). Spatial metaphors in temporal reasoning. In M. Gattis (Ed.) Spatial Schemas and Abstract Thought (Chapter 8, pp. 203-222). Cambridge: MIT Press. Lakoff, G. & Johnson, M. (1980). Metaphors we live by. Chicago: University of Chicago Press. Nunez, R. (1999). Could the future taste purple? Journal of Consiousness Studies, 6, 41-60.', 'corpus_id': 149335569}	4729	"[{'doc_id': '237946294', 'title': 'More Instructions Make Fewer Subtractions', 'abstract': 'Research on problem solving offers insights into how humans process task-related information and which strategies they use (Newell and Simon, 1972; Öllinger et al., 2014). Problem solving can be defined as the search for possible changes in one’s mind (Kahneman, 2003). In a recent study, Adams et al. (2021) assessed whether the predominant problem solving strategy when making changes involves adding or subtracting elements. In order to do this, they used several examples of simple problems, such as editing text or making visual patterns symmetrical, either in naturalistic settings or on-line. The essence of the authors’ findings is a strong preference to add rather than subtract elements across a diverse range of problems, including the stabilizing of artifacts, creating symmetrical patterns, or editing texts. More specifically, they succeeded in demonstrating that “participants were less likely to identify advantageous subtractive changes when the task did not (vs. did) cue them to consider subtraction, when they had only one opportunity (vs. several) to recognize the shortcomings of an additive search strategy or when they were under a higher (vs. lower) cognitive load” (Adams et al., 2021, p. 258). Addition and subtraction are generally defined as de-contextualized mathematical operations using abstract symbols (Russell, 1903/1938). Nevertheless, understanding of both symbols and operations is informed by everyday activities, such as making or breaking objects (Lakoff and Núñez, 2000; Fischer and Shaki, 2018). The universal attribution of “addition bias” or “subtraction neglect” to problem solving activities is perhaps a convenient shorthand but it overlooks influential framing effects beyond those already acknowledged in the report and the accompanying commentary (Meyvis and Yoon, 2021). Most importantly, while Adams et al.’s study addresses an important issue, their very method of verbally instructing participants, together with lack of control over several known biases, might render their findings less than conclusive. Below, we discuss our concerns that emerged from the identified biases, namely those regarding the instructions and the experimental materials. Moreover, we refer to research frommathematical cognition that provides new insights into Adams et al.’s findings.', 'corpus_id': 237946294, 'score': 1}, {'doc_id': '237372080', 'title': 'Embodiment and learning of abstract concepts (such as algebraic topology and regression to the mean).', 'abstract': ""This video is a proof of concept that ideas from embodied cognition can be used to understand how the brain and cognitive systems deal with very abstract concepts. The video teaches regression to the mean using three ideas. The first idea is directly related to embodied cognition: abstract concepts are grounded in perceptual, motor, and emotional systems by using successive levels of grounding within an extended procedure. The second idea is that this sort of grounding often requires formal instruction: a teacher needs to develop the sequence in which the concepts are grounded and the methods of grounding. That is, at least some abstract concepts are unlikely to be learned through an individual's unstructured interactions with the world. The third idea is that humans are hyper-social, thus making formal instruction possible. To the extent that the viewer learns the abstract concept of regression to the mean, then the video demonstrates how an embodied theory of abstract concepts could work."", 'corpus_id': 237372080, 'score': 1}, {'doc_id': '219424691', 'title': 'Patient Data-Sharing for AI: Ethical Challenges, Catholic Solutions', 'abstract': 'Recent news of Catholic and secular healthcare systems sharing electronic health record (EHR) data with technology companies for the purposes of developing artificial intelligence (AI) applications has drawn attention to the ethical and social challenges of such collaborations, including threats to patient privacy and confidentiality, undermining of patient consent, and lack of corporate transparency. Although the United States Catholic Conference of Bishops’ Ethical and Religious Directives for Health Care Services (ERDs) address collaborations between US Catholic healthcare providers and other entities, the ERDs do not adequately address the novel concerns seen in EHR data-sharing for AI development. Neither does the Health Insurance Portability and Accountability Act (HIPAA) privacy rule. This article describes ethical and social problems observed in recent patient data-sharing collaborations with AI companies and analyzes them in light of the guiding principles of the ERDs as well as the 2020 Rome Call to AI Ethics (RCAIE) document recently released by the Vatican. While both the ERDs and RCAIE guiding principles can inform future collaborations, we suggest that the next revision of the ERDs should consider addressing data-sharing and AI more directly. Summary: Electronic health record data-sharing with artificial intelligence developers presents unique ethical and social challenges that can be addressed with updated United States Catholic Conference of Bishops’ Ethical and Religious Directives and guidance from the Vatican’s 2020 Rome Call to AI Ethics.', 'corpus_id': 219424691, 'score': 0}, {'doc_id': '237344119', 'title': 'Szabó Exploring equivalence frames : Metaphorical lexical items as means of determining equivalence frames', 'abstract': 'This paper explores lexical items as possible units with which to detect equivalence-based framing, which shows how a given piece of information is framed. In line with frame semantics and figurative framing, individual words used metaphorically are discussed as tools to determine equivalence frames. The study applies conceptual metaphor theory (CMT) to determine the metaphors evoked by words referring to the demonstrators and their movement used in 81 news articles about the Women’s March in the USA published on the websites of the most widely circulated American newspapers between 2017 and 2019. The analysis reveals that lexical items (e.g., flood, sea, wave) evoking the WATER domain were used to frame the protest; this finding is in contrast to the use of words from other source domains, such as WAR, which are commonly used to frame demonstrations.', 'corpus_id': 237344119, 'score': 0}, {'doc_id': '231581714', 'title': 'A new methodology for conceptual metaphor detection and formulation in corpora: A case study on a mental health corpus', 'abstract': 'We describe a new methodology for conceptual metaphor detection and formulation in corpora, developed within the framework of the MOMENT project for analysing mental health metaphors. We critically review state-of-the-art methods for metaphor identification in texts, highlighting their main drawbacks for metaphor analysis in large corpora, mainly practical applicability and analytical subjectivity. Our method aims at mitigating existing drawbacks on the basis of applying the following principles: (i) working hypothesis formulation and verification at the metaphorical expression detection stage; (ii) partial use of standard methods for metaphorical focus identification; (iii) use of external expert knowledge in the form of more extensive use of dictionaries and the additional use of metaphor compendia; and (iv) the implementation of strategies for conceptual metaphor formulation, including domain formulation at two levels of generalization. Satisfactory reliability test results were obtained when we tested our method for inter-annotator agreement regardingmetaphor detection and formulation using texts about mental disorders as a test corpus.', 'corpus_id': 231581714, 'score': 1}, {'doc_id': '32708543', 'title': 'The NHS: Who is attacking, who is defending?', 'abstract': '7. Schensul, J. J. and Guest, B. H. (1994). Ethics, ethnicity, and health care reform. In, ""It Just Ain\'t Fair\'raThe Ethics of Health Care for African Americans, ed. by A. Dula and S. Goering, Praeger Publishers, Westport, CT. 8. Ferguson, W. (1994). The physician\'s responsibility to medically underserved poor people. In, \'It Just Ain\'t Fair\'--The Ethics of Health Care for African Americans, ed. by A. Dula and S. Goering, Praeger Publishers, Westport, CT. 9. Radical Health Statistics Group (1977). In Defence of the NHS, Radical Health Statistics Group, London. 10. Hahn, B. and Flood, A. B. (1995). No insurance, public insurance, and private insurance: do these options contribute to differences in general health? Journal of Health Care for the Poor and Underserved 6, 41-59. 11. Sorlie, P. D., Backlund, E. and Keller, J. B. (1995). USA mortality by economic, demographic, and social characteristics: the national longitudinal mortality study. American Journal of Public Health 85, 949-956. 12. Veatch, R. M. (1995). Abandoning informed consent. Hastings Center Report 25, 5-12.', 'corpus_id': 32708543, 'score': 0}, {'doc_id': '237216366', 'title': 'Metaphors as models: Towards a typology of metaphor in ancient science', 'abstract': 'Metaphors play a crucial role in the understanding of science. Since antiquity, metaphors have been used in technical texts to describe structures unknown or unnamed; besides establishing a terminology of science, metaphors are also important for the expression of concepts. However, a concise terminology to classify metaphors in the language of science has not been established yet. But in the context of studying the history of a science and its concepts, a precise typology of metaphors can be helpful. Metaphors have a lot in common with models in science, as has been observed already. In this paper, therefore, I suggest a typology of metaphor in ancient science to fill this terminological gap by using concepts applied to the classification of models in science, as coined by Rom Harré. I propose to differentiate between homeoconceptual metaphors (with the same conceptual frame between source and target) and paraconceptual metaphors (mapped via a different conceptual frame). Furthermore, functional and structural aspects of metaphors in ancient science are taken into account. Case studies from ancient texts displaying metaphors in ancient science are presented and classified following the outlined typology of metaphors.', 'corpus_id': 237216366, 'score': 1}, {'doc_id': '237235593', 'title': 'Mathematical Linguistics and Cognitive Complexity', 'abstract': 'The complexity of linguistic patterns has been object of extensive debate in research programs focused on probing the inherent structure of human language abilities. But in what sense is a linguistic phenomenon more complex than another, and what can complexity tell us about the connection between linguistic typology and human cognition? This chapter approaches these questions by presenting a broad and informal introduction to the vast literature on formal language theory, computational learning theory, and artificial grammar learning. In doing so, it hopes to provide readers with an understanding of the relevance of mathematically grounded approaches to cognitive investigations into linguistic complexity, and thus further fruitful collaborations between cognitive scientists and mathematically inclined linguist and psychologist.', 'corpus_id': 237235593, 'score': 1}, {'doc_id': '237239695', 'title': 'How to tear down the walls that separate linguists: Continuing the quest for clarity about general linguistics', 'abstract': 'The primary motivation for my target paper (“General linguistics must be based on universals”) was a perceived need for greater clarity in general linguistics. Many linguists seem to share the feeling that we often talk past each other because we understand terms like “theory”, “framework”, “explanation”, “analysis” and “description” in seemingly different ways. But even if different linguists (necessarily) prefer different methods, we need not disagree about these basic terms and concepts, and I think that some “walls” that separate linguists from different communities could be “torn down” if we became more aware of what unites all theoretical linguists: that we want to understand particular languages, and that we need them in order to understand Human Language in general. I am grateful to the commentators for their interesting contributions, and here I continue the discussion by responding to some of their remarks. To recapitulate, I made three key points in my target article:', 'corpus_id': 237239695, 'score': 0}, {'doc_id': '237432347', 'title': 'Book review', 'abstract': 'This volume, edited by Ewa Dąbrowska from University of Birmingham and FAU Erlangen-Nürnberg and Dagmar Divjak from University of Birmingham, is the part of a three-volume set on Cognitive Linguistics published by Walter de Gruyter in 2019. As the first volume of this set, it discusses the cognitive processes and abilities of human beings which underlie language production, particularly concerning such concepts as embodiment, attention, and categorization, providing a state-of-the-art overview of the subfields in linguistics. Authors in this collection specially emphasize the direction of cognitive linguistic studies towards a more empirical, interdisciplinary, and social-oriented basis, and provide readers with insightful ideas and suggestions for future research in Cognitive Linguistics. To begin with, in the Introduction, the editors briefly introduce the assumptions, history, and current situation of Cognitive Linguistics, and give an outline of the topics in the three-volume set. In chapter 1, Benjamin Bergen expounds the historical conceptions of embodiment in Cognitive Science, describes some of the ways that embodiment has been used in Cognitive Linguistics, and discusses the directions that linguistic embodiment research is currently moving towards. According to the author, there have been three distinct phases in the application of the idea of embodiment to empirical work on language and cognition, containing the analytical phase, the process phase and the function phase. For the next chapter, Russell S. Tomlin and Andriy Myachykov review the evidence for a regular link between visual attention and syntactic organization. They propose that the grammatical role assignment mechanism and the positional assignment mechanism form a hierarchical dual-path system, which allows a grammatical representation of the perceptually salient referent in a sentence. In chapter 3, Dagmar Divjak and Catherine L. Caldwell-Harris present interpretations of frequency and entrenchment, and integrate perspectives from both Experimental Psychology and Cognitive Linguistics. They illustrate the origins of the interest in frequency and its applications, and also the review of the cognitive and neural mechanisms supporting language structures that vary in entrenchment. Categorization is discussed by Michael Ramscar and Robert Port in chapter 4. They suggest that human conceptual capabilities are systematic in that they are the products of a rich capacity to discriminate and learn systems of alternate responses (behaviors, affordances, words, etc.) and to use the systems in context, with a conclusion that conceptual knowledge is closely related to context of language use. As for chapter 5, R. Harald Baayen and Michael Ramscar explain three approaches that attempt to answer the question about the mechanism of structuring language, placing more emphasis on the process of abstraction, analogical reasoning, and basic principles of discrimination learning. In chapter 6, Ronald W. Langacker characterizes construal with reasonable precision and investigates their reprensentations in language. Five broad dimensions of construal are scrutinized, namely perspective, selection, promi-', 'corpus_id': 237432347, 'score': 0}]"
136	{'doc_id': '233438060', 'title': 'Productivity and employment effects of digital complementarities', 'abstract': 'Modern economic growth is no longer found in total factor productivity (TFP) because there are gains from technological change that are never recorded in the returns from innovation or in the National Accounts. The existence of complementarities among technologies derived from the use of robotics, electronic commerce, or innovation is difficult to assess through country-level records. Because the literature has mainly focused on robotisation at an aggregate or industry level, research focusing on a firm level and complementarities analysis have been limited. To fill the gap, in this paper, we intend to provide new eviJ23', 'corpus_id': 233438060}	12947	[{'doc_id': '231958242', 'title': 'Financial Frictions, Allocative Efficiency, and Unemployment: A Quantitative Analysis for Argentina∗', 'abstract': 'Argentina is characterized by low levels of private credit and persistent labor market rigidities. Furthermore, financial development remained stagnant in Argentina even during episodes of fast economic growth, in stark contrast with the experience of sustained growth accelerations around the world. The goals of the paper are twofold. Firstly, it is concerned with quantifying the productivity losses associated with such low levels of private credit penetration and characterizing its implications for different subsets of firms in the economy. The latter is important in light of various policy interventions aimed at mitigating the impact of low access to credit based on firm-size thresholds. Secondly, it studies the dynamics of hypothetical reforms to credit markets in a context of rigid labor markets, which seems to be the adequate scenario in which structural reforms will have to be implemented, given the stickiness that labor market regulations have shown to reform efforts in the past. It finds sizable productivity losses from financial frictions, in the order of 13%. At the micro level it finds that it is the youngest firms, whose average marginal return to capital is far above the riskfree rate in the economy, that are more prone to become financially constrained. Turning to reform scenarios, we investigate sudden reforms that are implemented abruptly and more plausible reform paths that gradually dismantle financial frictions. In the former, productivity and the investment rate rise sharply on impact, while it also does the rate of unemployment, going from 5 to almost 12%. In the latter, the rise of unemployment is more gradual and less sharp, peaking at 7%. On the flipside, the investment rate declines on impact, although the contraction is short-lived.', 'corpus_id': 231958242, 'score': 0}, {'doc_id': '234817315', 'title': 'The four smarts of Industry 4.0: Evolution of ten years of research and future perspectives', 'abstract': 'Abstract The Industry 4.0 literature has exponentially grown in the past decade. We aim to understand how this literature has evolved and propose future research opportunities. We focus on four smart dimensions of Industry 4.0: Smart Manufacturing, Smart Products and Services, Smart Supply Chain, and Smart Working. We perform a machine learning-based systematic literature review. Our analysis included 4,973 papers published from 2011 to 2020. We conducted a chronological network analysis considering the growth of these four dimensions and the connections between them. We also analyzed keywords and the main journals publishing on these four smart dimensions. We show that the literature has mainly been devoted to the study of Smart Manufacturing, although attention to the other smart dimensions has been growing in recent years. Smart Working is the less explored dimension, with many opportunities for future research. We show that research opportunities are concentrated in the interfaces between the different smart dimensions. Our findings support the vision of Industry 4.0 as a concept transcending the Smart Manufacturing field, thus creating opportunities for synergies with other related fields. Scholars can use our findings to understand the orientation of journals and gaps that can be fulfilled by future research.', 'corpus_id': 234817315, 'score': 1}, {'doc_id': '233667104', 'title': 'Digital adoption, automation, and labor markets in developing countries', 'abstract': 'Abstract We study how digital adoption by firms—a precursor to automation—shapes the labor market structure and worker outcomes in developing countries. Using a large sample of developing countries, we document a strong and negative link between firm digital adoption and self-employment rates. This relationship persists even after controlling for the level of development and other factors associated with the distinct employment structure of developing countries. In contrast, there is no link between digital adoption and unemployment rates. We develop a model with equilibrium unemployment, self-employment, endogenous firm entry, and information-and-communications technology (ICT) adoption, and show that positive linkages between the cost of technology adoption (and therefore technology adoption itself) and salaried-firm entry costs are crucial for rationalizing the empirical relationship between firm digital adoption and self-employment.', 'corpus_id': 233667104, 'score': 1}, {'doc_id': '232221783', 'title': 'The Great Transition: Kuznets Facts for Family-Economists', 'abstract': 'The 20th century beheld a dramatic transformation of the family. Some Kuznets style facts regarding structural change in the family are presented. Over the course of the 20th century in the United States fertility declined, educational attainment waxed, housework fell, leisure increased, jobs shifted from blue to white collar, and marriage waned. These trends are also observed in the cross-country data. A model is developed, and then calibrated, to address the trends in the US data. The calibration procedure is closely connected to the underlying economic logic. Three drivers of the great transition are considered: neutral technological progress, skilled-biased technological change, and drops in the price of labor-saving household durables. \n \nYou can download the Kuznets facts here: https://www.ricardomarto.com/data/', 'corpus_id': 232221783, 'score': 0}, {'doc_id': '233217845', 'title': 'Optimal Fiscal Policy in the Presence of Declining Labor Share∗', 'abstract': 'Numerous recent studies have documented that the labor’s share in national income, which has been quite stable until the early 1980’s, has been declining at a considerable rate since then. In this paper, we analyze the implications of this decline on the optimal capital and labor income taxation from the perspective of a government that needs to finance spending. Our main qualitative finding is that the optimal tax implications of the decline in the labor share depend on the mechanism responsible for it. In particular, if the labor share declines because of rising market power or other mechanisms that raise the share of profits in national income, then the decline in the labor share should optimally be accompanied with a rise in capital income taxes. If, on the other hand, the labor share declines because of a rise in capital share, then it has no bearing on optimal capital income taxation. In our baseline calibration, we find that the optimal tax rate on capital income rises about 5-10% from the early 1980’s to 2020 depending on the increase in profit share.', 'corpus_id': 233217845, 'score': 0}, {'doc_id': '236770385', 'title': 'Recent Automation Trends in Portugal: Implications on Industrial Productivity and Employment in Automotive Sector', 'abstract': 'Recent developments in automation and artificial intelligence (AI) are leading to a wave of innovation in organizational design and changes in the workplace. Techno-optimists even named it the “second machine age,” arguing that it now involves the substitution of the human brain. Other authors see this as just a continuation of previous ICT developments. Potentially, automation and AI can have significant technical, economic, and social implications in firms. This paper will answer the following question: What are the implications on industrial productivity and employment in the automotive sector with the recent automation trends, including AI, in Portugal? Our approach used mixed methods to conduct statistical analyses of relevant databases and interviews with experts on R&D projects related to automation and AI implementation. Results suggest that automation can have widespread adoption in the short term in the automotive sector, but AI technologies will take more time to be adopted. The findings show that adoption of automation and AI increases productivity in firms and is dephased in time with employment implications. Investments in automation are not substituting operators but rather changing work organization. Thus, negative effects of technology and unemployment were not substantiated by our results.', 'corpus_id': 236770385, 'score': 1}, {'doc_id': '236909508', 'title': 'Modelling Artificial Intelligence', 'abstract': 'IZA DP No. 14171 MARCH 2021 Modelling Artificial Intelligence in Economics Economists’ two main theoretical approaches to understanding Artificial Intelligence (AI) impacts have been the task-approach to labor markets and endogenous growth theory. Therefore, the recent integration of the task-approach into an endogenous growth model by Acemoglu and Restrepo (AR) is a useful advance. However, it is subject to the shortcoming that it does not explicitly model AI and its technological feasibility. The AR model focuses on tasks and skills but not on abilities, while abilities better characterize AI services’ nature. This paper addresses this shortcoming by elaborating the task-approach with AI abilities for use within endogenous growth models. This more ability-sensitive specification of the taskapproach allows for more nuanced and realistic impacts of progress in artificial intelligence (AI) on the economy to be captured. JEL Classification: O47, O33, J24, E21, E25', 'corpus_id': 236909508, 'score': 1}, {'doc_id': '232360545', 'title': 'The UK as a Technological Follower: Higher Education Expansion, Technological Adoption, and the Labour Market', 'abstract': 'The proportion of UK people with university degrees tripled between 1993 and 2015. However, over the same period the time trend in the college wage premium has been extraordinarily flat. We show that these patterns cannot be explained by composition changes. Instead, we present a model in which firms choose between centralized and decentralized organizational forms and demonstrate that it can explain the main patterns. We also show the model has implications that differentiate it from both the exogenous skill-biased technological change model and the endogenous invention model, and that UK data fit with those implications. The result is a consistent picture of the transformation of the UK labour market in the last two decades.', 'corpus_id': 232360545, 'score': 0}, {'doc_id': '232023526', 'title': 'The American System of economic growth', 'abstract': 'The early history of industrialization in the United States—famously known as “The American System of Manufactures”—exhibited four key features: the substitution of specialized intermediate inputs for skilled work in assembling final goods, the freedom with which knowledge has long been shared in the United States, a learning technology that leverages existing mechanical know-how in human capital accumulation, and increasing returns to intermediate inputs in processing final goods. Our endogenous growth model embodies these components and utilizes historical time series data on labor force “operatives” and the Census of Manufactures to calibrate the model’s parameters. Our simulation closely matches the 1.88% average per capita product growth in the United States from 1860 to date. The simulation predicts that growth will peak in 1980 and ultimately converge to 1.31%—a growth slowdown rooted from the beginning in the economization of skilled labor inherent in the American System. By 2000, simulated per capita product is 2.21 times larger than a counterfactual in which the American System of manufactures never existed.', 'corpus_id': 232023526, 'score': 0}, {'doc_id': '234776229', 'title': 'Automation and sectoral reallocation', 'abstract': 'Empirical evidence in Dauth et al. (J Eur Econ Assoc, 2021) suggests that industrial robot adoption in Germany has led to a sectoral reallocation of employment from manufacturing to services, leaving total employment unaffected. We rationalize this evidence through the lens of a general equilibrium model with two sectors, matching frictions and endogenous participation. Automation induces firms to create fewer vacancies and job seekers to search less in the automatable sector (manufacturing). The service sector expands due to the sectoral complementarity in the production of the final good and a positive wealth effect for the household. Analysis across steady states shows that the reduction in manufacturing employment can be offset by the increase in service employment. The model can also replicate the magnitude of the decline in the ratio of manufacturing employment to service employment in Germany between 1994 and 2014. Supplementary Information The online version supplementary material available at 10.1007/s13209-021-00240-w.', 'corpus_id': 234776229, 'score': 1}]
137	{'doc_id': '221970581', 'title': 'Learning to Improve Image Compression without Changing the Standard Decoder', 'abstract': 'In recent years we have witnessed an increasing interest in applying Deep Neural Networks (DNNs) to improve the rate-distortion performance in image compression. However, the existing approaches either train a post-processing DNN on the decoder side, or propose learning for image compression in an end-to-end manner. This way, the trained DNNs are required in the decoder, leading to the incompatibility to the standard image decoders (e.g., JPEG) in personal computers and mobiles. Therefore, we propose learning to improve the encoding performance with the standard decoder. In this paper, We work on JPEG as an example. Specifically, a frequency-domain pre-editing method is proposed to optimize the distribution of DCT coefficients, aiming at facilitating the JPEG compression. Moreover, we propose learning the JPEG quantization table jointly with the pre-editing network. Most importantly, we do not modify the JPEG decoder and therefore our approach is applicable when viewing images with the widely used standard JPEG decoder. The experiments validate that our approach successfully improves the rate-distortion performance of JPEG in terms of various quality metrics, such as PSNR, MS-SSIM and LPIPS. Visually, this translates to better overall color retention especially when strong compression is applied. The codes are available at this https URL.', 'corpus_id': 221970581}	4391	"[{'doc_id': '226254313', 'title': 'CompressAI: a PyTorch library and evaluation platform for end-to-end compression research', 'abstract': 'This paper presents CompressAI, a platform that provides custom operations, layers, models and tools to research, develop and evaluate end-to-end image and video compression codecs. In particular, CompressAI includes pre-trained models and evaluation tools to compare learned methods with traditional codecs. Multiple models from the state-of-the-art on learned end-to-end compression have thus been reimplemented in PyTorch and trained from scratch. We also report objective comparison results using PSNR and MS-SSIM metrics vs. bit-rate, using the Kodak image dataset as test set. Although this framework currently implements models for still-picture compression, it is intended to be soon extended to the video compression domain.', 'corpus_id': 226254313, 'score': 1}, {'doc_id': '229340375', 'title': 'DAQ: Distribution-Aware Quantization for Deep Image Super-Resolution Networks', 'abstract': 'Quantizing deep convolutional neural networks for image super-resolution substantially reduces their computational costs. However, existing works either suffer from a severe performance drop in ultra-low precision of 4 or lower bit-widths, or require a heavy fine-tuning process to recover the performance. To our knowledge, this vulnerability to low precisions relies on two statistical observations of feature map values. First, distribution of feature map values varies significantly per channel and per input image. Second, feature maps have outliers that can dominate the quantization error. Based on these observations, we propose a novel distribution-aware quantization scheme (DAQ) which facilitates accurate training-free quantization in ultra-low precision. A simple function of DAQ determines dynamic range of feature maps and weights with low computational burden. Furthermore, our method enables mixed-precision quantization by calculating the relative sensitivity of each channel, without any training process involved. Nonetheless, quantization-aware training is also applicable for auxiliary performance gain. Our new method outperforms recent training-free and even trainingbased quantization methods to the state-of-the-art image super-resolution networks in ultra-low precision.', 'corpus_id': 229340375, 'score': 1}, {'doc_id': '233264987', 'title': 'Learned Image Compression with Discretized Gaussian Mixture Likelihoods and Attention Modules', 'abstract': 'There have been many compression standards developed during the past few decades and technological advances has resulted in introducing many methodologies with promising results. As far as PSNR metric is concerned, there is a performance gap between reigning compression standards and learned compression algorithms. Based on research, we experimented using an accurate entropy model on the learned compression algorithms to determine the rate-distortion performance. In this paper, discretized Gaussian Mixture likelihood is proposed to determine the latent code parameters in order to attain a more flexible and accurate model of entropy. Moreover, we have also enhanced the performance of the work by introducing recent attention modules in the network architecture. Simulation results indicate that when compared with the previously existing techniques using high-resolution and Kodak datasets, the proposed work achieves a higher rate of performance. When MS-SSIM is used for optimization, our work generates a more visually pleasant image.', 'corpus_id': 233264987, 'score': 1}, {'doc_id': '233293574', 'title': 'END-TO-END IMAGE COMPRESSION OPTIMIZATION WITH DEEP NEURAL NETWORKS', 'abstract': 'Convolutional neural networks (CNNs) are a popular deep learning architecture used to address image-based tasks and have also generated a growing interest to address low-level tasks such as image coding. Thus, in this paper is proposed a CNN-based coding framework, where a CNN is used for image simplification (i.e. down-sampling) before the encoder and other CNN for image enhancement (i.e. down-sampling) after the decoder, this is based on similar frameworks. Also, a third CNN, so-called CNN-FakeCodec, is introduced, to model coding distortion, for an improved training procedure. Since related solutions, mostly focus on minimizing distortion, here it is also defined a new loss function that uses the discrete cosine transform to estimate the rate required to encode an image, thus allowing to minimize both rate and distortion. The assessment results show that the proposed solutions can outperform the selected benchmarks, although depending on the image content and target quality.', 'corpus_id': 233293574, 'score': 1}, {'doc_id': '218581297', 'title': 'Domain Adaptation for Image Dehazing', 'abstract': 'Image dehazing using learning-based methods has achieved state-of-the-art performance in recent years. However, most existing methods train a dehazing model on synthetic hazy images, which are less able to generalize well to real hazy images due to domain shift. To address this issue, we propose a domain adaptation paradigm, which consists of an image translation module and two image dehazing modules. Specifically, we first apply a bidirectional translation network to bridge the gap between the synthetic and real domains by translating images from one domain to another. And then, we use images before and after translation to train the proposed two image dehazing networks with a consistency constraint. In this phase, we incorporate the real hazy image into the dehazing training via exploiting the properties of the clear image (e.g., dark channel prior and image gradient smoothing) to further improve the domain adaptivity. By training image translation and dehazing network in an end-to-end manner, we can obtain better effects of both image translation and dehazing. Experimental results on both synthetic and real-world images demonstrate that our model performs favorably against the state-of-the-art dehazing algorithms.', 'corpus_id': 218581297, 'score': 0}, {'doc_id': '232127793', 'title': 'neural compression workshop COIN : CO MPRESSION WITH I MPLICIT N EURAL REPRESENTATIONS', 'abstract': 'We propose a new simple approach for image compression: instead of storing the RGB values for each pixel of an image, we store the weights of a neural network overfitted to the image. Specifically, to encode an image, we fit it with an MLP which maps pixel locations to RGB values. We then quantize and store the weights of this MLP as a code for the image. To decode the image, we simply evaluate the MLP at every pixel location. We found that this simple approach outperforms JPEG at low bit-rates, even without entropy coding or learning a distribution over weights. While our framework is not yet competitive with state of the art compression methods, we show that it has various attractive properties which could make it a viable alternative to other neural data compression approaches.', 'corpus_id': 232127793, 'score': 1}, {'doc_id': '218571034', 'title': 'Data-Free Network Quantization With Adversarial Knowledge Distillation', 'abstract': 'Network quantization is an essential procedure in deep learning for development of efficient fixed-point inference models on mobile or edge platforms. However, as datasets grow larger and privacy regulations become stricter, data sharing for model compression gets more difficult and restricted. In this paper, we consider data-free network quantization with synthetic data. The synthetic data are generated from a generator, while no data are used in training the generator and in quantization. To this end, we propose data-free adversarial knowledge distillation, which minimizes the maximum distance between the outputs of the teacher and the (quantized) student for any adversarial samples from a generator. To generate adversarial samples similar to the original data, we additionally propose matching statistics from the batch normalization layers for generated data and the original data in the teacher. Furthermore, we show the gain of producing diverse adversarial samples by using multiple generators and multiple students. Our experiments show the state-of-the-art data-free model compression and quantization results for (wide) residual networks and MobileNet on SVHN, CIFAR-10, CIFAR-100, and Tiny-ImageNet datasets. The accuracy losses compared to using the original datasets are shown to be very minimal.', 'corpus_id': 218571034, 'score': 0}, {'doc_id': '218581237', 'title': 'High Resolution Face Age Editing', 'abstract': 'Face age editing has become a crucial task in film post-production, and is also becoming popular for general purpose photography. Recently, adversarial training has produced some of the most visually impressive results for image manipulation, including the face aging/de-aging task. In spite of considerable progress, current methods often present visual artifacts and can only deal with low-resolution images. In order to achieve aging/de-aging with the high quality and robustness necessary for wider use, these problems need to be addressed. This is the goal of the present work. We present an encoder-decoder architecture for face age editing. The core idea of our network is to encode a face image to age-invariant features, and learn a modulation vector corresponding to a target age. We then combine these two elements to produce a realistic image of the person with the desired target age. Our architecture is greatly simplified with respect to other approaches, and allows for fine-grained age editing on high resolution images in a single unified model. Source codes are available at https://github.com/InterDigitalInc/HRFAE.', 'corpus_id': 218581237, 'score': 0}, {'doc_id': '218571711', 'title': 'Décision kinésithérapique : Alexandre L. 57 ans : Kinésithérapie et Covid-19 en réanimation, de la phase aiguë à la réhabilitation', 'abstract': '\n Résumé\n \n Un homme de 57 ans est hospitalisé en réanimation pour un syndrome de détresse respiratoire aigüe (SDRA) lié à une infection à Covid-19. Après une première phase au cours de laquelle le patient est sédaté et curarisé, la kinésithérapie consiste à mobiliser passivement le patient, à participer au décubitus ventral; la kinésithérapie respiratoire n’étant pas forcément nécessaire. Dans un second temps, l’extubation est possible et plusieurs aspects sont développés\xa0: la kinésithérapie respiratoire, l’oxygénation, la déglutition et surtout la réhabilitation. Cependant, des atteintes du parenchyme pulmonaire abaissent de façon importante la saturation en oxygène au cours des exercices. L’oxygénation à haut débit et/ou la ventilation non-invasive (VNI) permettent d’optimiser la réhabilitation chez ce patient avec une réserve respiratoire encore précaire.\n Indice de factualité (i-FACT): 3.2\n \n \n Abstract\n \n A 57-year-old man is hospitalized in intensive care for an acute respiratory distress syndrome related to a Covid-19 infection. After a first phase during which the patient is sedated and nerve-blocked, physiotherapy consists in passively mobilizing the patient, participating in the prone position, respiratory physiotherapy is not necessary. In a second step, extubation is possible and several aspects are developed: respiratory physiotherapy, oxygenation, swallowing and rehabilitation. However, damage to the lung significantly decreases oxygen saturation during exercise. High-flow nasal oxygenation and / or non-invasive ventilation (NIV) can optimize rehabilitation in this patient with a still precarious respiratory function.\n Evidence index (EVID-i): 3.2\n \n', 'corpus_id': 218571711, 'score': 0}, {'doc_id': '218595892', 'title': 'Jigsaw-VAE: Towards Balancing Features in Variational Autoencoders', 'abstract': ""The latent variables learned by VAEs have seen considerable interest as an unsupervised way of extracting features, which can then be used for downstream tasks. There is a growing interest in the question of whether features learned on one environment will generalize across different environments. We demonstrate here that VAE latent variables often focus on some factors of variation at the expense of others - in this case we refer to the features as ``imbalanced''. Feature imbalance leads to poor generalization when the latent variables are used in an environment where the presence of features changes. Similarly, latent variables trained with imbalanced features induce the VAE to generate less diverse (i.e. biased towards dominant features) samples. To address this, we propose a regularization scheme for VAEs, which we show substantially addresses the feature imbalance problem. We also introduce a simple metric to measure the balance of features in generated images."", 'corpus_id': 218595892, 'score': 0}]"
138	{'doc_id': '220793559', 'title': 'Train Like a (Var)Pro: Efficient Training of Neural Networks with Variable Projection', 'abstract': 'Deep neural networks (DNNs) have achieved state-of-the-art performance across a variety of traditional machine learning tasks, e.g., speech recognition, image classification, and segmentation. The ability of DNNs to efficiently approximate high-dimensional functions has also motivated their use in scientific applications, e.g., to solve partial differential equations (PDE) and to generate surrogate models. In this paper, we consider the supervised training of DNNs, which arises in many of the above applications. We focus on the central problem of optimizing the weights of the given DNN such that it accurately approximates the relation between observed input and target data. Devising effective solvers for this optimization problem is notoriously challenging due to the large number of weights, non-convexity, data-sparsity, and non-trivial choice of hyperparameters. To solve the optimization problem more efficiently, we propose the use of variable projection (VarPro), a method originally designed for separable nonlinear least-squares problems. Our main contribution is the Gauss-Newton VarPro method (GNvpro) that extends the reach of the VarPro idea to non-quadratic objective functions, most notably, cross-entropy loss functions arising in classification. These extensions make GNvpro applicable to all training problems that involve a DNN whose last layer is an affine mapping, which is common in many state-of-the-art architectures. In numerical experiments from classification and surrogate modeling, GNvpro not only solves the optimization problem more efficiently but also yields DNNs that generalize better than commonly-used optimization schemes.', 'corpus_id': 220793559}	6078	[{'doc_id': '219687601', 'title': 'Proximal Mapping for Deep Regularization', 'abstract': 'Underpinning the success of deep learning is effective regularizations that allow a variety of priors in data to be modeled. For example, robustness to adversarial perturbations, and correlations between multiple modalities. However, most regularizers are specified in terms of hidden layer outputs, which are not themselves optimization variables. In contrast to prevalent methods that optimize them indirectly through model weights, we propose inserting proximal mapping as a new layer to the deep network, which directly and explicitly produces well regularized hidden layer outputs. The resulting technique is shown well connected to kernel warping and dropout, and novel algorithms were developed for robust temporal learning and multiview modeling, both outperforming state-of-the-art methods.', 'corpus_id': 219687601, 'score': 0}, {'doc_id': '222310432', 'title': 'ISTA-NAS: Efficient and Consistent Neural Architecture Search by Sparse Coding', 'abstract': 'Neural architecture search (NAS) aims to produce the optimal sparse solution from a high-dimensional space spanned by all candidate connections. Current gradient-based NAS methods commonly ignore the constraint of sparsity in the search phase, but project the optimized solution onto a sparse one by post-processing. As a result, the dense super-net for search is inefficient to train and has a gap with the projected architecture for evaluation. In this paper, we formulate neural architecture search as a sparse coding problem. We perform the differentiable search on a compressed lower-dimensional space that has the same validation loss as the original sparse solution space, and recover an architecture by solving the sparse coding problem. The differentiable search and architecture recovery are optimized in an alternate manner. By doing so, our network for search at each update satisfies the sparsity constraint and is efficient to train. In order to also eliminate the depth and width gap between the network in search and the target-net in evaluation, we further propose a method to search and evaluate in one stage under the target-net settings. When training finishes, architecture variables are absorbed into network weights. Thus we get the searched architecture and optimized parameters in a single run. In experiments, our two-stage method on CIFAR-10 requires only 0.05 GPU-day for search. Our one-stage method produces state-of-the-art performances on both CIFAR-10 and ImageNet at the cost of only evaluation time.', 'corpus_id': 222310432, 'score': 1}, {'doc_id': '219259832', 'title': 'Learning to Branch for Multi-Task Learning', 'abstract': 'Training multiple tasks jointly in one deep network yields reduced latency during inference and better performance over the single-task counterpart by sharing certain layers of a network. However, over-sharing a network could erroneously enforce over-generalization, causing negative knowledge transfer across tasks. Prior works rely on human intuition or pre-computed task relatedness scores for ad hoc branching structures. They provide sub-optimal end results and often require huge efforts for the trial-and-error process. In this work, we present an automated multi-task learning algorithm that learns where to share or branch within a network, designing an effective network topology that is directly optimized for multiple objectives across tasks. Specifically, we propose a novel tree-structured design space that casts a tree branching operation as a gumbel-softmax sampling procedure. This enables differentiable network splitting that is end-to-end trainable. We validate the proposed method on controlled synthetic data, CelebA, and Taskonomy.', 'corpus_id': 219259832, 'score': 1}, {'doc_id': '219179403', 'title': 'Meta Learning as Bayes Risk Minimization', 'abstract': 'Meta-Learning is a family of methods that use a set of interrelated tasks to learn a model that can quickly learn a new query task from a possibly small contextual dataset. In this study, we use a probabilistic framework to formalize what it means for two tasks to be related and reframe the meta-learning problem into the problem of Bayesian risk minimization (BRM). In our formulation, the BRM optimal solution is given by the predictive distribution computed from the posterior distribution of the task-specific latent variable conditioned on the contextual dataset, and this justifies the philosophy of Neural Process. However, the posterior distribution in Neural Process violates the way the posterior distribution changes with the contextual dataset. To address this problem, we present a novel Gaussian approximation for the posterior distribution that generalizes the posterior of the linear Gaussian model. Unlike that of the Neural Process, our approximation of the posterior distributions converges to the maximum likelihood estimate with the same rate as the true posterior distribution. We also demonstrate the competitiveness of our approach on benchmark datasets.', 'corpus_id': 219179403, 'score': 1}, {'doc_id': '220380824', 'title': 'RIFLE: Backpropagation in Depth for Deep Transfer Learning through Re-Initializing the Fully-connected LayEr', 'abstract': 'Fine-tuning the deep convolution neural network(CNN) using a pre-trained model helps transfer knowledge learned from larger datasets to the target task. While the accuracy could be largely improved even when the training dataset is small, the transfer learning outcome is usually constrained by the pre-trained model with close CNN weights (Liu et al., 2019), as the backpropagation here brings smaller updates to deeper CNN layers. In this work, we propose RIFLE - a simple yet effective strategy that deepens backpropagation in transfer learning settings, through periodically Re-Initializing the Fully-connected LayEr with random scratch during the fine-tuning procedure. RIFLE brings meaningful updates to the weights of deep CNN layers and improves low-level feature learning, while the effects of randomization can be easily converged throughout the overall learning procedure. The experiments show that the use of RIFLE significantly improves deep transfer learning accuracy on a wide range of datasets, out-performing known tricks for the similar purpose, such as Dropout, DropConnect, StochasticDepth, Disturb Label and Cyclic Learning Rate, under the same settings with 0.5% -2% higher testing accuracy. Empirical cases and ablation studies further indicate RIFLE brings meaningful updates to deep CNN layers with accuracy improved.', 'corpus_id': 220380824, 'score': 0}, {'doc_id': '210835507', 'title': '3PXNet: Pruned-Permuted-Packed XNOR Networks for Edge Machine Learning', 'abstract': 'As the adoption of Neural Networks continues to proliferate different classes of applications and systems, edge devices have been left behind. Their strict energy and storage limitations make them unable to cope with the sizes of common network models. While many compression methods such as precision reduction and sparsity have been proposed to alleviate this, they don’t go quite far enough. To push size reduction to its absolute limits, we combine binarization with sparsity in Pruned-Permuted-Packed XNOR Networks (3PXNet), which can be efficiently implemented on even the smallest of embedded microcontrollers. 3PXNets can reduce model sizes by up to 38X and reduce runtime by up to 3X compared with already compact conventional binarized implementations with less than 3% accuracy reduction. We have created the first software implementation of sparse-binarized Neural Networks, released as open source library targeting edge devices. Our library is complete with training methodology and model generating scripts, making it easy and fast to deploy.', 'corpus_id': 210835507, 'score': 1}, {'doc_id': '219963786', 'title': 'NeuralScale: Efficient Scaling of Neurons for Resource-Constrained Deep Neural Networks', 'abstract': 'Deciding the amount of neurons during the design of a deep neural network to maximize performance is not intuitive. In this work, we attempt to search for the neuron (filter) configuration of a fixed network architecture that maximizes accuracy. Using iterative pruning methods as a proxy, we parametrize the change of the neuron (filter) number of each layer with respect to the change in parameters, allowing us to efficiently scale an architecture across arbitrary sizes. We also introduce architecture descent which iteratively refines the parametrized function used for model scaling. The combination of both proposed methods is coined as NeuralScale. To prove the efficiency of NeuralScale in terms of parameters, we show empirical simulations on VGG11, MobileNetV2 and ResNet18 using CIFAR10, CIFAR100 and TinyImageNet as benchmark datasets. Our results show an increase in accuracy of 3.04%, 8.56% and 3.41% for VGG11, MobileNetV2 and ResNet18 on CIFAR10, CIFAR100 and TinyImageNet respectively under a parameter-constrained setting (output neurons (filters) of default configuration with scaling factor of 0.25).', 'corpus_id': 219963786, 'score': 0}, {'doc_id': '213556752', 'title': 'Deep Adversarial Data Augmentation for Extremely Low Data Regimes', 'abstract': 'Deep learning has revolutionized the performance of classification and object detection, but meanwhile demands sufficient labeled data for training. Given insufficient data, while many techniques have been developed to help combat overfitting, the challenge remains if one tries to train deep networks, especially in the ill-posed extremely low data regimes: only a small set of labeled data are available, and nothing – including unlabeled data – else. Such regimes arise from practical situations where not only data labeling but also data collection itself is expensive. We propose a deep adversarial data augmentation (DADA) technique to address the problem, in which we elaborately formulate data augmentation as a problem of training a class-conditional and supervised generative adversarial network (GAN). Specifically, a new discriminator loss is proposed to fit the goal of data augmentation, through which both real and augmented samples are enforced to contribute to and be consistent in finding the decision boundaries. Tailored training techniques are developed accordingly. To quantitatively validate its effectiveness, we first perform extensive simulations to show that DADA substantially outperforms both traditional data augmentation and a few GAN-based options. We then extend experiments to three real-world small labeled classification datasets where existing data augmentation and/or transfer learning strategies are either less effective or infeasible. We also demonstrate that DADA to can be extended to the detection task. We improve the pedestrian synthesis work by substitute for our discriminator and training scheme. Validation experiment shows that DADA can improve the detection mean average precision (mAP) compared with some traditional data augmentation techniques in object detection. Source code is available at https://github.com/SchafferZhang/DADA.', 'corpus_id': 213556752, 'score': 1}, {'doc_id': '219964346', 'title': 'A Structural Transformer with Relative Positions in Trees for Code-to-Sequence Tasks', 'abstract': 'We suggest two approaches to incorporate syntactic information into transformer models encoding trees (e.g. abstract syntax trees) and generating sequences. First, we use self-attention with relative position representations to consider structural relationships between nodes using a representation that encodes movements between any pair of nodes in the tree, and demonstrate how those movements can be computed efficiently on the fly. Second, we suggest an auxiliary loss enforcing the network to predict the lowest common ancestor of node pairs. We apply both methods to source code summarization tasks, where we outperform the state-of-the-art by up to 6 % F1. On natural language machine translation, our models yield competitive results. We also consistently outperform sequence-based transformers, and demonstrate that our method yields representations that are more closely aligned with the AST structure.', 'corpus_id': 219964346, 'score': 1}, {'doc_id': '220265908', 'title': 'Maximum Entropy Models for Fast Adaptation', 'abstract': 'Deep Neural Networks have shown great promise on a variety of downstream tasks; but their ability to adapt to new data and tasks remains a challenging problem. The ability of a model to perform few-shot adaptation to a novel task is important for the scalability and deployment of machine learning models. Recent work has shown that the learned features in a neural network follow a normal distribution [41], which thereby results in a strong prior on the downstream task. This implicit overfitting to data from training tasks limits the ability to generalize and adapt to unseen tasks at test time. This also highlights the importance of learning task-agnostic representations from data. In this paper, we propose a regularization scheme using a max-entropy prior on the learned features of a neural network; such that the extracted features make minimal assumptions about the training data. We evaluate our method on adaptation to unseen tasks by performing experiments in 4 distinct settings. We find that our method compares favourably against multiple strong baselines across all of these experiments.', 'corpus_id': 220265908, 'score': 1}]
139	{'doc_id': '46915027', 'title': 'Techniques for extraction of bioactive compounds from plant materials: A review', 'abstract': 'The use of bioactive compounds in different commercial sectors such as pharmaceutical, food and chemical industries signifies the need of the most appropriate and standard method to extract these active components from plant materials. Along with conventional methods, numerous new methods have been established but till now no single method is regarded as standard for extracting bioactive compounds from plants. The efficiencies of conventional and non-conventional extraction methods mostly depend on the critical input parameters; understanding the nature of plant matrix; chemistry of bioactive compounds and scientific expertise. This review is aimed to discuss different extraction techniques along with their basic mechanism for extracting bioactive compounds from medicinal plants.', 'corpus_id': 46915027}	12304	[{'doc_id': '138820508', 'title': 'A study of yield characteristics during mechanical oil extraction of preheated and ground soybeans', 'abstract': 'Soybeans were extracted for oil by compressing a ground sample at various operating pressures, pressing durations and product bulk temperatures. The oil yield from the various operations was measured and e xpressed as a percentage of the original mass of crushed seeds. It was found t hat the oil yields increased linearly with increase in pressure as the compression p ressure was increased from 40 to 80 kgf/m and that oil yield also increased linearly with i ncrease in t he d uration of pressing wi thin t he range 2 It was also f ound beneficial to dry the seeds to a moisture content slightly below t he am bient moisture content of 9.3% (d.b.) although reducing moisture to a value lower that 5% (d.b.) r esulted in a reduction in oi l yield. A si ngle e mpirical model fo r estimating the oil yield f or v aried condi tions of pressure, duration of pressing and the bulk temperature of oil bearing material was developed which could estimated the yield wi th g ood accu racy within t he experimental range.', 'corpus_id': 138820508, 'score': 1}, {'doc_id': '235282439', 'title': 'Degradation of Highly Filled Biocomposites Based on Synthetic Polymers and Natural Polysaccharides Under the Action of Climatic Weathering and Biodegradation (Review)', 'abstract': 'The abiotic and biological weathering of highly filled composites based on polyolefins and natural fillers was considered. It was found that majority of studies were devoted to artificial weathering of wood polymer composites (WPC) and its effect on composites properties. The biodegradability investigations of such materials were limited by outdoor weathering and model studies under laboratory conditions. The fungal decay was the main topical issue. In the case of natural filler composites (NFC) the biodegradation studies under the natural and laboratory conditions were discussed wider.', 'corpus_id': 235282439, 'score': 0}, {'doc_id': '237252553', 'title': 'Plant Extract Loaded Nanoparticles', 'abstract': 'Plant extract, a natural source containing complex mixture of compounds, offers many properties such as antiparasitic, antibiotic, antioxidant, anti-hypertensive, antiviral, insecticide, anticancer, antifungal, hypoglycemic properties. Recent research has been focused on developing formulation the plant extracts not only to deliver them safely but also to enhance its therapeutic efficacy. Nanotechnology-based strategies have been proposed as a system that can be used to formulate plant extracts. Plant extract loaded nanoparticles (NPs) is aimed to facilitate in crossing the biological barriers, to increase bioavailability of poorly water-soluble phytochemicals, to encapsulate mixture compounds of different phytochemicals, to provide targeted delivery of phytochemicals to specific organs resulting in low toxicity, to get effective purification process, to mask unpleasant taste and odor, to protect sensitive phytochemicals from biological (e.g. enzyme, pH) and environmental (e.g. light, temperature, humidity) degradation, to control release of encapsulated phytochemicals, and to provide a more flexible control over the size and shape of the NPs. This review is focused on plant extract loaded NPs including its advantages, stages for developing formulation of plant extract loaded NPs, and nanosystem used to loading plant extract. In addition, this review also depicts studies which have been conducted by many researchers in developing plant extract loaded NPs. The data were collected from published journals with 21 and 39 journals as primary and supporting literatures, respectively. Plant extracts loaded NPs could be a promising delivery system for active phytochemical contained in the plant extract which is not only to deliver them safely but also to enhance its therapeutic efficacy.', 'corpus_id': 237252553, 'score': 1}, {'doc_id': '189776828', 'title': 'Oilseeds: Extrusion for solvent extraction', 'abstract': None, 'corpus_id': 189776828, 'score': 1}, {'doc_id': '235290464', 'title': 'Effect of Additive on Microstructure, Hydrophilicity and Ultrafiltration Performance of Polyethylene Terephthalate Membranes', 'abstract': 'Ultrafiltration is a pressure-driven separation process through a porous membrane that can separate particles or macromolecules from a solution. Ultrafiltration is mostly applied for water treatment processes in various industries such as pharmaceutical, chemical, food and beverage industries. Commercial ultrafiltration membranes are mostly fabricated from polymer materials such as polysulfone, polyethersulfone and cellulose acetate. These polymers are expensive, and for the time being they are not produced in Indonesia. In this work, polyethylene terephthalate (PET) bottle waste was used as the polymer material to prepare ultrafiltration membranes. PET is commonly used as a packaging material for foods and beverages due to its low price and excellent mechanical properties. The PET membranes were prepared via a phase-inversion technique using polyethylene glycol (PEG) as the additive. It was observed that the addition of PEG improved the flexibility and the hydrophilicity of the membranes. The water contact angle decreased to 61.5° by the addition of PEG. The microstructures of the membranes could be controlled by the molecular weight of the PEG. The result of the ultrafiltration experiment showed that the membrane with a high porosity, a large pore size and a high hydrophilicity exhibited a high water permeate flux. In the ultrafiltration experiment using a model feed solution of an aqueous solution containing Bovine Serum Albumin with a molecular weight of 66,000 Da, the membranes showed high rejection values of up to 90% with water permeate fluxes of 14.7 - 23.0 kg/(m2h).', 'corpus_id': 235290464, 'score': 0}, {'doc_id': '235126702', 'title': 'Microwave-induced in situ drug amorphization using a mixture of polyethylene glycol and polyvinylpyrrolidone.', 'abstract': 'The use of a mixture of polyethylene glycol (PEG) and polyvinylpyrrolidone (PVP) was investigated for microwave-induced in situ amorphization of celecoxib (CCX) inside compacts. Such amorphization requires the presence of a dipolar excipient in the formulation to ensure heating of the compact by absorption of the microwaves. Previously, the hygroscopic nature of PVP was exploited for this purpose. By exposing PVP-based compacts for set time intervals at defined relative humidity, controlled water sorption into the compacts was achieved. In the present study, PEG was proposed as the microwave absorbing excipient instead of water, to avoid the water sorption step. However, it was found that PEG alone melted upon exposure to microwave radiation and caused the compact to deform. Furthermore, CCX was found to recrystallize upon cooling in PEG-based formulations. Hence, a mixture of PEG and PVP was used, where the presence of PVP preserved the physical shape of the compact, and the physical state of the amorphous solid dispersion. To study the impact of the polymer mixture, different compact compositions of CCX, PEG and PVP were prepared. When exposing the compacts to microwave radiation, it was found that the PEG:PVP ratio was critical for in situ amorphization and that complete amorphization was only achieved above a certain temperature threshold.', 'corpus_id': 235126702, 'score': 0}, {'doc_id': '225185838', 'title': 'A laboratory study of a novel bio-based nonionic surfactant to mitigate clay swelling', 'abstract': 'Abstract This study reports the applicability of quillaja saponin (QS) as a vigorous and environmentally friendly shale swelling inhibitor. QS is a natural surfactant, which is extracted from herbal sources. The inhibition strength of this surfactant was assessed through various experiments, such as sedimentation, inhibition, filtration, particle size, Scanning Electron Microscope (SEM) images, and cutting recovery. Data obtained from these tests illustrated that QS greatly inhibits clays from swelling. The optimal concentration for QS in this intend was 10\xa0g/L. Compatibility of this surfactant with other common additives was also investigated, which showed that it is totally compatible. Finally, the potential inhibition mechanism was assessed through thermal gravimetric analysis (TGA), zeta potential, and contact angle measurement experiments. Surface coating, and wettability alteration of clay particles to the oil-wet state was recognized as the most probable mechanism.', 'corpus_id': 225185838, 'score': 1}, {'doc_id': '93351677', 'title': 'Mathematical simulation of an oilseed press', 'abstract': 'A mathematical model of an oilseed press was developed by superimposition of filtration analysis on screw extrusion theory to calculate press throughput and residual oil content in presscake for a given press geometry and physical properties of oilseed. The model predicted that press performance would improve, i.e., the throughput would increase and residual oil would decrease, if the press were cooled during operation. Longer presses would also give higher throughputs with lower residual oil contents. The predicted effects of changes in shaft speed and choke opening on press performance agreed reasonably well with experimental results obtained on a small laboratory press. A relatively large error of 9.0% in the prediction of throughput could be attributed to changes in viscosity of oilseed mass occurring during its passage through the press. It is expected that use of ‘expression’ analysis in place of the simple filtration analysis would improve the predictive ability of the model.', 'corpus_id': 93351677, 'score': 1}, {'doc_id': '235436664', 'title': 'Bio-Based Polyurethane Networks Derived from Liquefied Sawdust', 'abstract': 'The utilization of forestry waste resources in the production of polyurethane resins is a promising green alternative to the use of unsustainable resources. Liquefaction of wood-based biomass gives polyols with properties depending on the reagents used. In this article, the liquefaction of forestry wastes, including sawdust, in solvents such as glycerol and polyethylene glycol was investigated. The liquefaction process was carried out at temperatures of 120, 150, and 170 °C. The resulting bio-polyols were analyzed for process efficiency, hydroxyl number, water content, viscosity, and structural features using the Fourier transform infrared spectroscopy (FTIR). The optimum liquefaction temperature was 150 °C and the time of 6 h. Comprehensive analysis of polyol properties shows high biomass conversion and hydroxyl number in the range of 238–815 mg KOH/g. This may indicate that bio-polyols may be used as a potential substitute for petrochemical polyols. During polyurethane synthesis, materials with more than 80 wt% of bio-polyol were obtained. The materials were obtained by a one-step method by hot-pressing for 15 min at 100 °C and a pressure of 5 MPa with an NCO:OH ratio of 1:1 and 1.2:1. Dynamical-mechanical analysis (DMA) showed a high modulus of elasticity in the range of 62–839 MPa which depends on the reaction conditions.', 'corpus_id': 235436664, 'score': 0}]
140	{'doc_id': '235483364', 'title': 'Using Chemometric Methods', 'abstract': 'Multivariate models were developed for the simultaneous spectrophotometric determination of copper (II), nickel (II) and zinc (II) in water with 1-(2-thiazolylazo)-2-naphthol as chromogenic reagent in the presence of Triton X-100. To overcome the drawback of spectral interferences, principal component regression (PCR) and partial least square (PLS) multivariate calibration approaches were applied. Performances were validated with several test sets, and their results were then compared. In general, no significant difference in analytical performance between PLS and PCR models. The root mean square error of prediction (RMSEP) using three components for Cu2+, Ni2+ and Zn2+ were 0.018, 0.010, 0.011 ppm, respectively. Figures of merit such as sensitivity, analytical sensitivity, limit of detection (LOD) were also estimated. High reliability was achieved when the proposed procedure was applied to simultaneous determination of Cu2+, Ni2+ and Zn2+ in synthetic mixture and tap water.', 'corpus_id': 235483364}	17836	"[{'doc_id': '235531063', 'title': 'Soil organic carbon estimation using VNIR–SWIR spectroscopy: The effect of multiple sensors and scanning conditions', 'abstract': 'Abstract Visible–near infrared–shortwave infrared (VNIR–SWIR) spectroscopy is being increasingly used for soil organic carbon (SOC) assessment. Common practice consists of scanning soil samples using a single spectrometer. Considerations have rarely been documented of the effects of using multiple instruments and scanning conditions on SOC model calibration that occur when merging soil spectral libraries (SSLs), particularly in soils with low SOC concentration and using both field spectroradiometers and laboratory fixed spectrometers. To address this gap, we scanned 143 low-SOC-content soil samples using three spectrometers (ASD FieldSpec 3, ASD FieldSpec 4, and FOSS XDS) and four setup features - FOSS, contact probe (CP), dark-box (DB), and open laboratory (LAB) - at three laboratories. The application of an internal soil standard (ISS) to align one laboratory spectrum with another for spectral correction and spectral merging of various SSLs was examined. SOC models were developed using i) data from each single spectrometer – single laboratory separately and ii) merged data from multiple spectrometers – different laboratories, applying the 1st derivatives of spectra and random forest (RF) regression. The results indicate that the spectral shape and wavelength position of key features obtained from all spectrometers and setups did not show any noticeable differences, though spectra based on FOSS setup, particularly on low-SOC samples, demonstrated greater range in absolute derivative values regardless of ISS application. The derivative ISS-corrected spectra showed less variation among different spectrometers compared to their uncorrected raw reflectance spectra. All single spectrometer models predicted SOC reasonably well. However, the spectra acquired by the FOSS setup predicted SOC more accurately (R2 = 0.77, RPIQ = 3.30, RMSE = 0.22 %, and SD = 0.04) than the spectra acquired by the other setups. The models derived from merged uncorrected raw reflectance spectra yielded poor results (R2 = 0.48, RPIQ = 2.33, RMSE = 0.33 %, and SD = 0.10); nevertheless, assessment of SOC using the 1st derivative ISS-corrected merged SSLs considerably improved the prediction accuracy (R2 = 0.70, RPIQ = 3.10, RMSE = 0.25 %, and SD = 0.06). Hence, the derivative spectra coupled with the ISS correction improved the accuracy of SOC prediction models obtained from the merged soil spectra collected in different environments using different instruments. We therefore recommend application of the ISS spectral alignment method linked to the 1st derivative approach to enhance the compilation of SSLs at the regional and global scales for SOC assessment.', 'corpus_id': 235531063, 'score': 1}, {'doc_id': '233584607', 'title': 'Characterization of field-scale soil variation using a stepwise multi-sensor fusion approach and a cost-benefit analysis', 'abstract': 'Abstract The potential of a stepwise fusion of proximally sensed portable X-ray fluorescence (pXRF) spectra and electromagnetic induction (EMI) with remote Sentinel-2 bands and a digital elevation model (DEM) was investigated for predicting soil physicochemical properties in pedons and across a heterogeneous 80-ha crop field in Wisconsin, USA. We found that pXRF spectra with partial least squares regression (PLSR) models can predict sand, total nitrogen (TN), organic carbon (OC), silt contents, and clay with validation R2 of 0.81, 0.74, 0.73, 0.68, and 0.64 at the pedon scale but performed less well for soil pH (R2\xa0=\xa00.51). A combination of EMI, Sentinel-2, and DEM data showed promise in mapping sand, silt contents, and TN at two depths and Ap horizon thickness and soil depth across the field. A clustering analysis using combinations of mapped soil properties or proximal and remote sensing data suggested that data fusion improved the characterization of field-scale variability of soil properties. The cost-benefit analysis showed that the most accurate management zones (MZs) for topsoil can be generated only using estimated soil property maps while it was the most costly as compared to other data sources. For an intermediate-high (for topsoil) and high (subsoil) accuracy and a moderate economic budget, the combination of sensors (proximal\xa0+\xa0remote sensing\xa0+\xa0DEM) might be a better approach for effective MZs generation than collecting soil samples for laboratory analysis while the latter produced the most accurate maps for topsoil. It can be concluded that pXRF spectra can be useful for predicting key soil properties (e.g., sand, TN, OC, silt, clay) at different soil depths, and a combination of proximal and remote sensing provides an effective way to delineate soil MZs that are useful for decision-making.', 'corpus_id': 233584607, 'score': 1}, {'doc_id': '233582797', 'title': 'Fish survival prediction in an aquatic environment using random forest model', 'abstract': 'In the real world, it is very difficult for fish farmers to select the perfect fish species for aquaculture in a specific aquatic environment. The main goal of this research is to build a Machine Learning that can predict the perfect fish species in an aquatic environment.\xa0\xa0In this paper, we have utilized a model using Random Forest. To validate the model, we have used a dataset of aquatic environment for 11 different fishes. To predict the fish species, we utilized the different characterics of aquiatic environment including ph, temperature, turbidity. As a performance metrics, we measured accuacry, TP rate, kappa statistics. Experimental results demonstrate that the proposed Random Forest based prediction model shows accuracy 88.48%, kappa statistic 87.11% and TP rate 88.5% for the tested dataset. In addition, we compare the proposed model with the state-of-art models- J48, Random Forest, KNN, Classification and Regression (CART). The proposed model outperforms than the existing models by exhibiting the higher accuracy score, TP rate and kappa statistics.', 'corpus_id': 233582797, 'score': 0}, {'doc_id': '235117251', 'title': ""Correlation analysis and partial least square modeling to quantify typical minerals with Chang'E-3 visible and near-infrared imaging spectrometer's ground validation data"", 'abstract': ""In 2013, Chang'E-3 program will develop lunar mineral resources in-situ detection. A Visible and Near-infrared Imaging Spectrometer(VNIS) has been selected as one payload of CE-3 lunar rover to achieve this goal. It is critical and urgent to evaluate VNIS' spectrum data quality and validate quantification methods for mineral composition before its launch. Ground validation experiment of VNIS was carried out to complete the two goals, by simulating CE-3 lunar rover's detection environment on lunar surface in the laboratory. Based on the hyperspectral reflectance data derived, Correlation Analysis and Partial Least Square(CA-PLS) algorithm is applied to predict abundance of four lunar typical minerals(pyroxene, plagioclase, ilmenite and olivine) in their mixture. We firstly selected a set of VNIS' spectral parameters which highly correlated with minerals' abundance by correlation analysis(CA), and then stepwise regression method was used to find out spectral parameters which make the largest contributions to the mineral contents. At last, functions were derived to link minerals' abundance and spectral parameters by partial least square(PLS) algorithm. Not considering the effect of maturity, agglutinate and Fe0, we found that there are wonderful correlations between these four minerals and VNIS' spectral parameters, e.g. the abundance of pyroxene correlates positively with the mixture's absorption depth, the value of absorption depth added as the increasing of pyroxene's abundance. But the abundance of plagioclase correlates negatively with the spectral parameters of band ratio, the value of band ratio would decrease when the abundance of plagioclase increased. Similar to plagioclase, the abundance of ilmenite and olivine has a negative correlation with the mixture's reflectance data, if the abundance of ilmenite or olivine increase, the reflectance values of the mixture will decrease. Through model validation, better estimates of pyroxene, plagioclase and ilmenite's abundances are given. It is concluded that VNIS has the capability to be applied on lunar minerals' identification, and CA-PLS algorithm has the potential to be used on lunar surface's in-situ detection for minerals' abundance prediction."", 'corpus_id': 235117251, 'score': 1}, {'doc_id': '234819107', 'title': 'Mapping soil organic carbon stock by hyperspectral and time-series multispectral remote sensing images in low-relief agricultural areas', 'abstract': 'Abstract High-precision digital soil organic carbon (SOC) stocks mapping is very important for agricultural production management and global carbon cycle. The spatial heterogeneity of farmland SOC is not only influence by the environmental factors of soil formation but also the management practices of tillage, fertilization, and irrigation. However, the traditional modeling covariates of digital soil mapping, such as terrain factors, land use types and climate factors have weak spatial variations in low-relief agricultural areas, and they cannot reflect the large spatial variation of SOC. Thus the time-series multispectral remote sensing images will be used for mapping soil properties in low relief regions in this study, meanwhile a new collaborative verification strategy was put forward to evaluate the spatial distribution characteristics of soil maps. The current study was performed in a nearly flat agricultural region southeast of Iowa (with an area of approximately 385.45\xa0ha), where 195 surface soil samples (0–15\xa0cm) were collected. A hyperspectral image (Headwall-Hyperspec, 380–1700\xa0nm) and the time-series multispectral remote sensing images of Sentinel 2 and Landsat 8 were used to construct the prediction models of SOC stock and its relevant soil properties of SOC and soil bulk density (SBD) through partial least square regression (PLSR) and extreme learning machine (ELM) models. The collected soil samples and evaluation indexes of root mean square error (RMSE), R2, and ratio of performance to interquartile range (RPIQ) were used to evaluate the model performance. Results are as follows: (1) hyperspectral images were successfully used to predict the SOC stock, SOC, and SBD through PLSR and ELM, while ELM (RPIQ\xa0=\xa02.03, 1.97, 1.64) outperformed PLSR (RPIQ\xa0=\xa01.83, 1.97, 1.53); (2) the time-series multispectral remote sensing images of Sentinel 2 and Landsat 8 can reflect the spatial distribution characteristics of the SOC stock, SOC and SBD by PLSR and ELM, but the combination of Sentinel 2 images and ELM obtained the best prediction results (RPIQ\xa0=\xa01.45, 1.25, 1.26); and (3) the differences of the soil maps predicted by the hyperspectral image and time-series multispectral remote sensing images were small, and the largest percentage errors nearly appeared on the edges of the farmland patches owing to mixed pixels. This study further confirmed the good prediction abilities of the time-series multispectral remote sensing images in low relief farmland regions. Lastly, this mapping strategy can provide additional valuable information for agricultural management and carbon cycle.', 'corpus_id': 234819107, 'score': 0}, {'doc_id': '233537051', 'title': 'Is this melon sweet? A quantitative classification for near-infrared spectroscopy', 'abstract': 'Abstract Melons are nutritious, healthy, and one of the most eatable summer fruits in South Asia, especially in Pakistan. A melon is delicious if it is sweet, however, the gauge of its sweetness depends on the individual taste buds. In this paper, a direct sweetness classifier is proposed as a quantitative measure, to predict the sweetness of melon as opposed to indirect measure of soluble solid content (SSC/°Brix) based thresholding for near-infrared (NIR) spectroscopy. To provide guidance for fruit sweetness classification, sensory test was conducted, and sweetness standards were established as; very sweet (with °Brix over 10), sweet (with °Brix between 7 and 10), and flat (with °Brix below 7) class. NIR spectral data obtained using F-750 produce quality meter (310–1100\xa0nm) was analyzed to build SSC prediction model and direct sweetness classification model. The best SSC model was obtained using multiple linear regression on second derivative of spectral data (for wavelength range 729–975\xa0nm) with correlation coefficient\xa0=\xa00.93, and root mean square error\xa0=\xa01.63 on test samples. Sweetness of test samples were obtained using °Brix thresholding with an accuracy of 55.45% for three classes. The best direct sweetness classifier was obtained using K nearest neighbor (KNN) on second derivative of spectral data (for wavelength range 729–975\xa0nm) with an accuracy of 70.3% for three classes on test samples. It was further observed that classification accuracy for sweet and flat melon can be improved by combining sweet and very sweet class samples into one ‘satisfactory’ class (with °Brix over 7). For °Brix thresholding-based classification the accuracy was improved to 80.2% and for KNN based direct sweetness classification the accuracy was improved to 88.12%. Extensive evaluation validates our argument that modeling a direct sweetness classifier is a better approach as compared to °Brix based thresholding for sweetness classification using NIR spectroscopy.', 'corpus_id': 233537051, 'score': 1}, {'doc_id': '233410271', 'title': 'Superlatives in Teaching General Chemistry.', 'abstract': 'What is the strongest Brønsted acid, the strongest base, the strongest oxidizing agent? If not understood in an absolute, once-and-forever sense, the answers to such questions may help at extending and reinforcing the meaning of simple concepts in first-year chemistry courses. Moreover, they serve the purpose of introducing research aspects and linking them to general chemistry.', 'corpus_id': 233410271, 'score': 0}, {'doc_id': '233545568', 'title': 'ennemi: Non-linear correlation detection with mutual information', 'abstract': 'Abstract We present ennemi, a Python package for correlation analysis based on mutual information (MI). MI is a measure of relationship between variables. Unlike Pearson correlation it is valid also for non-linear relationships, yet in the linear case the two are equivalent. The effect of other variables can be removed like with partial correlation, with the same equivalence. These features make MI a better correlation measure for exploratory analysis of many variable pairs. Our package provides methods for common correlation analysis tasks using MI. It is scalable, integrated with the Python data science ecosystem, and requires minimal configuration.', 'corpus_id': 233545568, 'score': 0}, {'doc_id': '234353050', 'title': 'Journal of Applied Structural Equation Modeling RELATIVE IMPORTANCE OF WATER QUALITY PARAMETERS ON FISH GROWTH USING PLS REGRESSION', 'abstract': 'Partial least squares (PLS) is a multivariate dimension reduction technique which is not based on ordinary regression assumptions. The use of PLS regression in life sciences is still a novel concept despite having many scientific applications. This paper analyses the relative importance of physicochemical parameters on the growth of Oreochromis jipe using Oreochromis niloticus as a control. Modelling and the graphical display of the regression coefficients were performed using a suite of open access R-software packages. The modelling hypotheses were assessed using experimental data collected from 270 fingerlings cultured for the period of 84 days. The findings revealed that significant linear correlation exists between water quality variables and the mean body weight of both O. jipe and O. niloticus fish species. The study provides baseline information to assess the growth of O. jipe under aquaculture conditions; therefore, we recommend a further study to be conducted on several other predictor variables that can be measured under controlled aquaculture conditions.', 'corpus_id': 234353050, 'score': 0}, {'doc_id': '234781612', 'title': 'iPick: Multiprocessing software for integrated NMR signal detection and validation.', 'abstract': 'Peak picking is a critical step in biomolecular NMR spectroscopy. The program, iPick, presented here provides a scripting tool and a graphical user interface (GUI), which allow the user to perform interactive and intuitive peak picking and validation. The click-and-run GUI requires no computer programming skills, while the scripting tool can be used by more advanced users to customize the application. If used with a multi-core CPU, the multiprocessing feature of iPick reduces the processing time significantly by invoking parallel computing. The GUI is a plugin, compatible with the popular NMRFAM-SPARKY software package and its newly released successor, the POKY software. Features implemented in iPick include automated noise level detection and threshold setting, cross-validation against multiple spectra, and a method for quantifying peak reliability. The iPick software is cross-platform, open-source, and freely available from https://github.com/pokynmr/ipick.', 'corpus_id': 234781612, 'score': 0}]"
141	{'doc_id': '149174557', 'title': 'A diasporic right to the city: the production of a Moroccan diaspora space in Granada, Spain', 'abstract': 'Abstract In this paper, I bring together ideas of ‘diaspora space’ and ‘the right to the city’ and empirically demonstrate how the formation of diasporas is frequently dependent on migrants attaining certain rights to the city. These rights, I argue, are conditioned and attained by the interplay of urban structural context with the place-making strategies of migrants. Drawing on 8 months of ethnographic fieldwork, I demonstrate that Moroccan migrants in Granada, Spain, have achieved a partial right to a neighbourhood of the city, producing a multi-sensory, self-orientalised diaspora space. First, I show that certain urban conditions in Granada provided a foothold for Moroccan migrants to begin to form a diaspora and transform urban space. Second, I demonstrate that through the mobilisation of a strategically self-orientalised cultural capital, the diaspora have partly appropriated the valuable history of Al-Andalus, a key component in the city’s tourist imagery. These factors and strategies have enabled Moroccan migrants to gain a right to have a visible presence in the city, a right to produce and transform urban space and a right to spatalise diverse identities – all key rights, I argue, in the formation of a diaspora.', 'corpus_id': 149174557}	7060	"[{'doc_id': '218654538', 'title': 'The Altar of National Prosperity: Extractivism and Sacrifice Zones in Argentine Patagonia', 'abstract': '\u200b: The advances in extractive technologies in the 21st century has led to the creation of a new powerful global actor, the Multinational. These multinationals have no allegiance to a state, as earlier forms of capitalism did, rather they are ventures in the industries of agribusiness and mining that operate in countries throughout Laitn America. These global actors are able to effectively dominate economies through the reprimarization of the countries that host them. Countries like Argentina have welcomed multinationals like Monsanto and Patagonia Gold into their territories, which has proven to be a detriment to the communities and environments in which they take place. These industries promise the creation of jobs, development of economies, and state revenue through taxes and royalties. Upon further inspection of these promises, it is revealed that these goals are misleading and these extractive operations are only able to succeed by preying on the preexisting social, political, and economic inequalities of communities in Argentina. I offer a vignette of socio-environmental conflicts that take place in rural, urban, and Indigenous communities. By analyzing these conflicts across space, identity reconfiguration and articulation such as that of the Mapuche in Río Negro is visibilized. As Mapuche and non-Mapuche community members come together to contest their positions within this extractive paradigm, the persisting logic and legacy of colonialism is revealed.', 'corpus_id': 218654538, 'score': 0}, {'doc_id': '133778308', 'title': ""The 'Orient' in the 'Occident' : the social, cultural and spatial dynamics of Moroccan diaspora formations in Granada, Spain"", 'abstract': None, 'corpus_id': 133778308, 'score': 1}, {'doc_id': '220444451', 'title': 'Constructing the National Identity Discourse in Citizenship Education Policy The Case of Citizenship Education in England', 'abstract': 'The thesis examines the governmental construction of national identity through its citizenship education policy in England, the state with heightened tensions in diversity and identity re-construction aligning with its mandatory citizenship classes since 2002. Theoretically framing the study on the Foucauldian post-structuralism, the thesis utilises Foucauldian-influenced ‘What is the problem represented to be?’ (WPR) method by Bacchi that presents the government as a problem-producer. Conducting qualitative research methods, the study analyses the current National Curriculum in England with the explanatory and foundational state documents of Crick and Ajegbo Reports. The thesis identifies that the government primarily aims to re-construct the inclusive and integrative national identity based on the acknowledgement of multiple identities and a plurality of nations in the citizenship education curriculum in England. Our study, however, also reveals that the English citizenship education policy implicitly presents a few assimilationist elements in the national identity discourse through exclusion and unrepresentativeness of the ethnic and racial identities, hierarchical establishment between native English and minorities, and the division of ‘whites’ and ‘non-whites’. Comparatively examining the documents, the thesis, therefore, concludes that the government has a powerful position in socially and politically reconstructing the discourses, concepts, and meanings over time.', 'corpus_id': 220444451, 'score': 0}, {'doc_id': '219956472', 'title': 'Introduction: Changing Tourism in the Cities of Post-communist Central and Eastern Europe', 'abstract': 'ABSTRACT This special edition examines various aspects of urban tourism in the post-communist cities of Central and Eastern Europe (CEE). It begins by examining the nature of tourism restructuring in the region since the end of communism and the way that this unfolds in cities. It then examines major global changes in the nature of tourism and their impacts on urban tourism in CEE. These include the growing demand among tourists for new experiences and destinations; the impact of budget airlines on tourism in smaller cities; the impacts of the sharing economy (particularly Airbnb); and the growing emphasis on events and festivals as a means of attracting visitors to cities. The article ends by introducing the six articles that make up this special edition.', 'corpus_id': 219956472, 'score': 0}, {'doc_id': '192180694', 'title': 'Hunters, Sufis, soldiers, and minstrels: The diaspora aesthetics of the Moroccan Gnawa', 'abstract': None, 'corpus_id': 192180694, 'score': 1}, {'doc_id': '219979369', 'title': 'Out of the Urban Shadows: Uneven Development and Spatial Politics in Immigrant Suburbs', 'abstract': 'It is now well established that the concentric zone model, developed by Ernest Burgess and elaborated by others in the Chicago School of Sociology to explain the distribution of social groups in metropolitan areas, was wrong. In the past several decades, immigrants have not only moved out of the centers of U.S. metropolitan areas, many have bypassed central cities altogether and settled directly in suburbs. Increasingly, they have done so in nontraditional gateway cities, such as those in the American South and Rustbelt, and in smaller metropolitan or nonmetropolitan areas (Singer et al. 2008). Suburban settlement has also not clearly been associated with immigrants’ “move up” or integration into the so-called Americanmainstream, as Chicago school authors argued. In many rapidly growing metropolitan areas, rising housing prices have pushed many immigrants out of their historic urban neighborhoods. While post-World War II visions of the American Dream may still pull immigrants to suburbia, the communities into which many have settled hardly reflect that dream.While Asian immigrants have high rates of settlement in middle-class, affluent, and white suburban neighborhoods, other immigrants more commonly settle into suburbs with relatively high rates of foreclosure, poverty, segregation, and other measures of disadvantage (Farrell 2016; Logan 2014). These are not the touted “opportunity neighborhoods” that provide pathways to economic mobility. In fact, compared to central city ethnic enclaves, many provide less of the social, cultural and institutional supports that have traditionally promoted the economic advancement of immigrants and their children. Chicago School scholars also failed to account for the politics within suburbs that challenge not only immigrants’ ability to settle within particular communities, but also to achieve their own purposes and pursuits within them. My research on immigrants in suburbia has sought to fill some of these gaps. It has investigated the struggles of educated, professional Asian immigrants to establish a place for themselves within largely white, middle-class suburbs in Silicon Valley. In the Washington, DC suburbs, I have examined how lower-income, primarily Latino and African immigrants have fought to maintain a presence within redeveloping neighborhoods with rising gentrification and displacement pressures.', 'corpus_id': 219979369, 'score': 0}, {'doc_id': '134125583', 'title': 'Morocco and Diaspora Engagement: A Contemporary Portrait', 'abstract': 'The world is becoming increasingly globalized with populations emigrating widely and in large numbers. Morocco is no exception to this general trend, with over five million of its citizens residing outside the Kingdom. Drawing on academic literature, policies, and legislation that have been disseminated by the Moroccan government, and diaspora-related statistics, Morocco and Diaspora Engagement: A Contemporary Portrait maps the diaspora, including geographic and demographic trends; examines the evolution of policies from “control” to “courting”; and analyzes the exchange of culture, legal rights and obligations, and economic capital. Lastly, it considers the specific application of diaspora legal policy to women living abroad in the context of their changing personal status under domestic law.', 'corpus_id': 134125583, 'score': 1}, {'doc_id': '219769704', 'title': 'European Identity Politics', 'abstract': 'Social and political sciences use the term ‘identity’ in describing a wide range of phenomena, whether these be personal explanations of self-understanding, descriptions of common interests or the shared experiences of a larger group. It has been used in the recent analyses of countries or larger communities, but also in the historical studies of very different societies in developing or industrialized countries. To make the concept more operational and open to empirical research, we dichotomize it into an inclusive versus an exclusive type. This enables us to carve out the different policy conclusions associated with each type. We then apply the concepts for analysing the emergence of European identity over the past decades, as well as its limits and recent headwinds. We present survey data on national and supranational identity and country differences concerning trust in national and European institutions. As a counter-strategy to populism and the exclusive type of identity, political observers, from scientists to members of the media, are split into suggesting either a ""cordon sanitaire"" to discourage voting for such ideas versus an embracement strategy by including their representatives into government, thereby controlling them or revealing their incompetence. This paper, in contrast, ventures a proactive strategy of four steps to localize the root causes of the success of populism, offering an inclusive vision for the long run, policy instruments for economic improvements and a new narrative.', 'corpus_id': 219769704, 'score': 0}, {'doc_id': '151421442', 'title': 'Moroccan Diaspora in France and the February 20 Movement in Morocco', 'abstract': ""ABSTRACT This article analyzes the links between the so-called 20 February 2011 Movement in Morocco and the Moroccan migrant organizations in France. The research examines how the 50-year-long history of these organizations shapes Moroccan migrants' political experiences and their ties to homeland in order to explain why they do not play a significant role in the events unfolding in Morocco in 2011. To this end, concepts such as diaspora and transnationalism are mobilized to grasp how activists are connecting to places and territories and reducing the distance between them, while preserving a certain unity in their collective action."", 'corpus_id': 151421442, 'score': 1}, {'doc_id': '220646677', 'title': 'Migration and Refugee Crisis: a Critical Analysis of Online Public Perception', 'abstract': 'The migration rate and the level of resentments towards migrants are an important issue in modern civilisation. The infamous EU refugee crisis caught many countries unprepared, leading to sporadic and rudimentary containment measures that, in turn, led to significant public discourse. Decades of offline data collected via traditional survey methods have been utilised earlier to understand public opinion to foster peaceful coexistence. Capturing and understanding online public opinion via social media is crucial towards a joint strategic regulation spanning safety, rights of migrants and cordial integration for economic prosperity. We present a analysis of opinions on migrants and refugees expressed by the users of a very popular social platform, Twitter. We analyse sentiment and the associated context of expressions in a vast collection of tweets related to the EU refugee crisis. Our study reveals a marginally higher proportion of negative sentiments vis-a-vis migrants and a large proportion of the negative sentiments is more reflected among the ordinary users. Users with many followers and non-governmental organisations (NGO) tend to tweet favourably about the topic, offsetting the distribution of negative sentiment. We opine that they can be encouraged to be more proactive in neutralising negative attitudes that may arise concerning similar incidences.', 'corpus_id': 220646677, 'score': 1}]"
142	{'doc_id': '216868834', 'title': 'Reinforcement Learning with Augmented Data', 'abstract': 'Learning from visual observations is a fundamental yet challenging problem in Reinforcement Learning (RL). Although algorithmic advances combined with convolutional neural networks have proved to be a recipe for success, current methods are still lacking on two fronts: (a) data-efficiency of learning and (b) generalization to new environments. To this end, we present Reinforcement Learning with Augmented Data (RAD), a simple plug-and-play module that can enhance most RL algorithms. We perform the first extensive study of general data augmentations for RL on both pixel-based and state-based inputs, and introduce two new data augmentations - random translate and random amplitude scale. We show that augmentations such as random translate, crop, color jitter, patch cutout, random convolutions, and amplitude scale can enable simple RL algorithms to outperform complex state-of-the-art methods across common benchmarks. RAD sets a new state-of-the-art in terms of data-efficiency and final performance on the DeepMind Control Suite benchmark for pixel-based control as well as OpenAI Gym benchmark for state-based control. We further demonstrate that RAD significantly improves test-time generalization over existing methods on several OpenAI ProcGen benchmarks. Our RAD module and training code are available at this https URL.', 'corpus_id': 216868834}	5115	"[{'doc_id': '221819254', 'title': 'Regret Bounds and Reinforcement Learning Exploration of EXP-based Algorithms', 'abstract': 'EXP-based algorithms are often used for exploration in multi-armed bandit. We revisit the EXP3.P algorithm and establish both the lower and upper bounds of regret in the Gaussian multi-armed bandit setting, as well as a more general distribution option. The analyses do not require bounded rewards compared to classical regret assumptions. We also extend EXP4 from multi-armed bandit to reinforcement learning to incentivize exploration by multiple agents. The resulting algorithm has been tested on hard-to-explore games and it shows an improvement on exploration compared to state-of-the-art.', 'corpus_id': 221819254, 'score': 1}, {'doc_id': '220666200', 'title': 'Unlocking the Potential of Deep Counterfactual Value Networks', 'abstract': 'Deep counterfactual value networks combined with continual resolving provide a way to conduct depth-limited search in imperfect-information games. However, since their introduction in the DeepStack poker AI, deep counterfactual value networks have not seen widespread adoption. In this paper we introduce several improvements to deep counterfactual value networks, as well as counterfactual regret minimization, and analyze the effects of each change. We combined these improvements to create the poker AI Supremus. We show that while a reimplementation of DeepStack loses head-to-head against the strong benchmark agent Slumbot, Supremus successfully beats Slumbot by an extremely large margin and also achieves a lower exploitability than DeepStack against a local best response. Together, these results show that with our key improvements, deep counterfactual value networks can achieve state-of-the-art performance.', 'corpus_id': 220666200, 'score': 1}, {'doc_id': '221655230', 'title': 'Oracle-Efficient Reinforcement Learning in Factored MDPs with Unknown Structure', 'abstract': 'We consider provably-efficient reinforcement learning (RL) in non-episodic factored Markov decision processes (FMDPs). All previous algorithms for regret minimization in this setting made the strong assumption that the factored structure of the FMDP is known to the learner in advance. In this paper, we provide the first provably-efficient algorithm that has to learn the structure of the FMDP while minimizing its regret. Our algorithm is based on the optimism in face of uncertainty principle, combined with a simple statistical method for structure learning, and can be implemented efficiently given oracle-access to an FMDP planner. It maintains its computational efficiency even though the number of possible structures is exponential.', 'corpus_id': 221655230, 'score': 0}, {'doc_id': '220665531', 'title': 'Deep vs. Deep Bayesian: Reinforcement Learning on a Multi-Robot Competitive Experiment', 'abstract': 'Deep Reinforcement Learning (RL) experiments are commonly performed in simulated environment, due to the tremendous training sample demand from deep neural networks. However, model-based Deep Bayesian RL, such as Deep PILCO, allows a robot to learn good policies within few trials in the real world. Although Deep PILCO has been applied on many single-robot tasks, in here we propose, for the first time, an application of Deep PILCO on a multi-robot confrontation game, and compare the algorithm with a model-free Deep RL algorithm, Deep Q-Learning. Our experiments show that Deep PILCO significantly outperforms Deep Q-Learning in learning efficiency and scalability. We conclude that sample-efficient Deep Bayesian learning algorithms have great prospects on competitive games where the agent aims to win the opponents in the real world, as opposed to simulated applications.', 'corpus_id': 220665531, 'score': 1}, {'doc_id': '222090758', 'title': 'Entropy Regularization for Mean Field Games with Learning', 'abstract': 'Entropy regularization has been extensively adopted to improve the efficiency, the stability, and the convergence of algorithms in reinforcement learning. This paper analyzes both quantitatively and qualitatively the impact of entropy regularization for Mean Field Game (MFG) with learning in a finite time horizon. Our study provides a theoretical justification that entropy regularization yields time-dependent policies and, furthermore, helps stabilizing and accelerating convergence to the game equilibrium. In addition, this study leads to a policy-gradient algorithm for exploration in MFG. Under this algorithm, agents are able to learn the optimal exploration scheduling, with stable and fast convergence to the game equilibrium.', 'corpus_id': 222090758, 'score': 1}, {'doc_id': '199515334', 'title': 'J ul 2 01 9 Approximating Nash Equilibrium Via Multilinear Minimax', 'abstract': 'Nash equilibrium (NE) can be stated as a formal theorem on a multilinear form, free of game theory terminology. On the other hand, inspired by this formalism, we state and prove a multilinear minimax theorem, a generalization of von Neumann’s bilinear minimax theorem. As in the bilinear case, the proof is based on relating the underlying optimizations to a primal-dual pair of linear programming problems, albeit more complicated LPs. The theorem together with its proof is of independent interest. Next, we use the theorem to associate to a multilinear form in NE a multilinear minimax relaxation (MMR), where the primal-dual pair of solutions induce an approximate equilibrium point that provides a nontrivial upper bound on a convex combination of expected payoffs in any NE solution. In fact we show any positive probability vector associated to the players induces a corresponding diagonally-scaled MMR approximate equilibrium with its associated upper bound. By virtue of the proof of the multilinear minimax theorem, MMR solution can be computed in polynomial-time. On the other hand, it is known that even in bimatrix games NE is PPAD-complete, a complexity class in NP not known to be in P. The quality of MMR solution and the efficiency of solving the underlying LPs are the subject of further investigation. However, as shown in a separate article, for a large set of test problems in bimatrix games, not only the MMR payoffs for both players are better than any NE payoffs, so is the computing time of MMR in contrast with that of Lemke-Howsen algorithm. In large size problems the latter algorithm even fails to produce a Nash equilibrium. In summary, solving MMR provides a worthy approximation even if Nash equilibrium is shown to be computable in polynomial-time.', 'corpus_id': 199515334, 'score': 1}, {'doc_id': '211252404', 'title': 'Safe Imitation Learning via Fast Bayesian Reward Inference from Preferences', 'abstract': 'Bayesian reward learning from demonstrations enables rigorous safety and uncertainty analysis when performing imitation learning. However, Bayesian reward learning methods are typically computationally intractable for complex control problems. We propose Bayesian Reward Extrapolation (Bayesian REX), a highly efficient Bayesian reward learning algorithm that scales to high-dimensional imitation learning problems by pre-training a low-dimensional feature encoding via self-supervised tasks and then leveraging preferences over demonstrations to perform fast Bayesian inference. Bayesian REX can learn to play Atari games from demonstrations, without access to the game score and can generate 100,000 samples from the posterior over reward functions in only 5 minutes on a personal laptop. Bayesian REX also results in imitation learning performance that is competitive with or better than state-of-the-art methods that only learn point estimates of the reward function. Finally, Bayesian REX enables efficient high-confidence policy evaluation without having access to samples of the reward function. These high-confidence performance bounds can be used to rank the performance and risk of a variety of evaluation policies and provide a way to detect reward hacking behaviors.', 'corpus_id': 211252404, 'score': 0}, {'doc_id': '212725921', 'title': 'Stability and Learning in Strategic Queuing Systems', 'abstract': 'Bounding the price of anarchy, which quantifies the damage to social welfare due to selfish behavior of the participants, has been an important area of research in algorithmic game theory. In this paper, we study this phenomenon in the context of a game modeling queuing systems: routers compete for servers, where packets that do not get service will be resent at future rounds, resulting in a system where the number of packets at each round depends on the success of the routers in the previous rounds. We model this as an (infinitely) repeated game, where the system holds a state (number of packets held by each queue) that arises from the results of the previous rounds. We assume that routers satisfy the no-regret condition, e.g. they use learning strategies to identify the server where their packets get the best service. Classical work on repeated games makes the strong assumption that the subsequent rounds of the repeated games are independent (beyond the influence on learning from past history). The carryover effect caused by packets remaining in this system makes learning in our context result in a highly dependent random process. We analyze this random process and find that if the capacity of the servers is high enough to allow a centralized and knowledgeable scheduler to get all packets served even when service rates are halved, and queues use no-regret learning algorithms, then the expected number of packets in the queues will remain bounded throughout time, assuming older packets have priority. This paper is the first to study the effect of selfish learning in a queuing system, where the learners compete for resources, but rounds are not all independent: the number of packets to be routed at each round depends on the success of the routers in the previous rounds.', 'corpus_id': 212725921, 'score': 0}, {'doc_id': '219965923', 'title': 'QOPT: Optimistic Value Function Decentralization for Cooperative Multi-Agent Reinforcement Learning', 'abstract': 'We propose a novel value-based algorithm for cooperative multi-agent reinforcement learning, under the paradigm of centralized training with decentralized execution. The proposed algorithm, coined QOPT, is based on the ""optimistic"" training scheme using two action-value estimators with separate roles: (i) true action-value estimation and (ii) decentralization of optimal action. By construction, our framework allows the latter action-value estimator to achieve (ii) while representing a richer class of joint action-value estimators than that of the state-of-the-art algorithm, i.e., QMIX. Our experiments demonstrate that QOPT newly achieves state-of-the-art performance in the StarCraft Multi-Agent Challenge environment. In particular, ours significantly outperform the baselines for the case where non-cooperative behaviors are penalized more aggressively.', 'corpus_id': 219965923, 'score': 0}, {'doc_id': '212628699', 'title': 'Covariance Steering for Discrete-Time Linear-Quadratic Stochastic Dynamic Games*', 'abstract': 'This paper addresses the problem of steering a discrete-time linear dynamical system from an initial Gaussian distribution to a final distribution in a game-theoretic setting. One of the two players strives to minimize a quadratic payoff, while at the same time tries to meet a given mean and covariance constraints at the final time-step. The other player maximizes the same payoff, but it is assumed to be indifferent to the terminal constraint. At first, the unconstrained version of the game is examined, and the necessary conditions for the existence of a saddle point are obtained. We show that obtaining a solution for the one-sided constrained dynamic game is not guaranteed, and subsequently the players’ best responses are analyzed. Finally, we propose to numerically solve the problem of steering the distribution under adversarial scenarios using the Jacobi iteration method.', 'corpus_id': 212628699, 'score': 0}]"
143	{'doc_id': '235458445', 'title': 'On the Dark Side of Calibration for Modern Neural Networks', 'abstract': 'Modern neural networks are highly uncalibrated. It poses a significant challenge for safety-critical systems to utilise deep neural networks (DNNs), reliably. Many recently proposed approaches have demonstrated substantial progress in improving DNN calibration. However, they hardly touch upon refinement, which historically has been an essential aspect of calibration. Refinement indicates separability of a network’s correct and incorrect predictions. This paper presents a theoretically and empirically supported exposition for reviewing a model’s calibration and refinement. Firstly, we show the breakdown of expected calibration error (ECE), into predicted confidence and refinement. Connecting with this result, we highlight that regularisation based calibration only focuses on naively reducing a model’s confidence. This logically has a severe downside to a model’s refinement. We support our claims through rigorous empirical evaluations of many state of the art calibration approaches on standard datasets. We find that many calibration approaches with the likes of label smoothing, mixup etc. lower the utility of a DNN by degrading its refinement. Even under natural data shift, this calibrationrefinement trade-off holds for the majority of calibration methods. These findings call for an urgent retrospective into some popular pathways taken for modern DNN calibration.', 'corpus_id': 235458445}	19320	"[{'doc_id': '237386240', 'title': 'Effect of the output activation function on the probabilities and errors in medical image segmentation', 'abstract': 'The sigmoid activation is the standard output activation function in binary classification and segmentation with neural networks. Still, there exist a variety of other potential output activation functions, which may lead to improved results in medical image segmentation. In this work, we consider how the asymptotic behavior of different output activation and loss functions affects the prediction probabilities and the corresponding segmentation errors. For cross entropy, we show that a faster rate of change of the activation function correlates with better predictions, while a slower rate of change can improve the calibration of probabilities. For dice loss, we found that the arctangent activation function is superior to the sigmoid function. Furthermore, we provide a test space for arbitrary output activation functions in the area of medical image segmentation. We tested seven activation functions in combination with three loss functions on four different medical image segmentation tasks to provide a classification of which function is best suited in this application scenario.', 'corpus_id': 237386240, 'score': 1}, {'doc_id': '174802983', 'title': 'When Does Label Smoothing Help?', 'abstract': ""The generalization and learning speed of a multi-class neural network can often be significantly improved by using soft targets that are a weighted average of the hard targets and the uniform distribution over labels. Smoothing the labels in this way prevents the network from becoming over-confident and label smoothing has been used in many state-of-the-art models, including image classification, language translation and speech recognition. Despite its widespread use, label smoothing is still poorly understood. Here we show empirically that in addition to improving generalization, label smoothing improves model calibration which can significantly improve beam-search. However, we also observe that if a teacher network is trained with label smoothing, knowledge distillation into a student network is much less effective. To explain these observations, we visualize how label smoothing changes the representations learned by the penultimate layer of the network. We show that label smoothing encourages the representations of training examples from the same class to group in tight clusters. This results in loss of information in the logits about resemblances between instances of different classes, which is necessary for distillation, but does not hurt generalization or calibration of the model's predictions."", 'corpus_id': 174802983, 'score': 1}, {'doc_id': '237613066', 'title': 'Decision Trees and Random Decision Forests', 'abstract': 'Linear regressors from X = R to Y = R have a limited expressive power, because they fit K affine functions to the training set. Functions that are more complex than that are approximated poorly. Similarly, K-class linear classifiers partition the data space X into K convex regions. If the true decision regions are not convex, performance suffers. This limited expressive power has also a silver lining: The number m of parameters of linear predictors is relatively small, m = K(d + 1), so that a linear predictor requires fewer training samples to train, relative to a more expressive predictor, in order to achieve a certain value for the training risk: Linear predictors have low sample complexity. As a result, they generalize better than more expressive ones. When we studied linear classifiers, we also saw the advantages of score-based classifiers: The K scores output by a K-class logistic-regression classifier can be interpreted as a probability distribution of the predicted label ŷ = h(x) given the input x, and this distribution can be used to compute some measure of the confidence of the answer. Decision trees combine the advantages of a score-based predictor (for both classifiers and regressors!) with the expressiveness deriving from a very flexible partition of X. Specifically, they recursively split X with hyperplanes, and they assign a single value (real number or label, depending on whether the problem is one of regression or classification) to each subset of the partition. This recursive splitting leads to effectively arbitrary expressiveness, as long as the partition is fine enough. Since the splitting is recursive, the resulting partition of the data space X can be represented by a strictly binary tree. As will be seen in more detail below, the root of the tree represents all of X, and contains the parameters of the hyper-plane used for the first split. The two children of the root represent the two half-spaces that that hyper-plane divides X into. Each of the two children in turn contains an additional hyper-plane that splits the child’s half-space, and the structure continues recursively. The tree can then be used for prediction: Given a data point x, check which side of the root partition it belongs to, and send it to the corresponding child. This check is repeated at every internal tree node encountered in this way, until a leaf is reached. The leaf corresponds to a (possibly small) convex region of X, and a single value is associated to that region. That value is the prediction y = h(x) returned by the tree. We will see in the next section how the hyper-planes are constructed, and how the prediction value at each leaf is computed. Staying at a high level for now, decision trees determine each of the hyper-planes greedily, by choosing the hyper-plane (out of a set of choices that is typically restricted, as we will see) that', 'corpus_id': 237613066, 'score': 0}, {'doc_id': '236447335', 'title': 'Energy-Based Open-World Uncertainty Modeling for Confidence Calibration', 'abstract': 'Confidence calibration is of great importance to the reliability of decisions made by machine learning systems. However, discriminative classifiers based on deep neural networks are often criticized for producing overconfident predictions that fail to reflect the true correctness likelihood of classification accuracy. We argue that such an inability to model uncertainty is mainly caused by the closed-world nature in softmax: a model trained by the cross-entropy loss will be forced to classify input into one of K pre-defined categories with high probability. To address this problem, we for the first time propose a novel K+1-way softmax formulation, which incorporates the modeling of open-world uncertainty as the extra dimension. To unify the learning of the original K-way classification task and the extra dimension that models uncertainty, we 1) propose a novel energy-based objective function, and moreover, 2) theoretically prove that optimizing such an objective essentially forces the extra dimension to capture the marginal data distribution. Extensive experiments show that our approach, Energy-based Open-World Softmax (EOW-Softmax), is superior to existing state-of-the-art methods in improving confidence calibration.', 'corpus_id': 236447335, 'score': 1}, {'doc_id': '238198064', 'title': 'When in Doubt: Improving Classification Performance with Alternating Normalization', 'abstract': 'We introduce Classification with Alternating Normalization (CAN), a non-parametric postprocessing step for classification. CAN improves classification accuracy for challenging examples by re-adjusting their predicted class probability distribution using the predicted class distributions of high-confidence validation examples. CAN is easily applicable to any probabilistic classifier, with minimal computation overhead. We analyze the properties of CAN using simulated experiments, and empirically demonstrate its effectiveness across a diverse set of classification tasks 1.', 'corpus_id': 238198064, 'score': 0}, {'doc_id': '237593007', 'title': 'Quantifying Model Predictive Uncertainty with Perturbation Theory', 'abstract': 'We propose a framework for predictive uncertainty quantification of a neural network that replaces the conventional Bayesian notion of weight probability density function (PDF) with a physics based potential field representation of the model weights in a Gaussian reproducing kernel Hilbert space (RKHS) embedding. This allows us to use perturbation theory from quantum physics to formulate a moment decomposition problem over the model weight-output relationship. The extracted moments reveal successive degrees of regularization of the weight potential field around the local neighborhood of the model output. Such localized moments represent well the PDF tails and provide significantly greater accuracy of the model’s predictive uncertainty than the central moments characterized by Bayesian and ensemble methods or their variants. We show that this consequently leads to a better ability to detect false model predictions of test data that has undergone a covariate shift away from the training PDF learned by the model. We evaluate our approach against baseline uncertainty quantification methods on several benchmark datasets that are corrupted using common distortion techniques. Our approach provides fast model predictive uncertainty estimates with much greater precision and calibration. Deep neural network (DNN) models have become the predominant choice for pattern representation in a wide variety of machine learning applications due to their remarkable performance advantages in the presence of large amount data (LeCun et al., 2015). The increased adoption of DNNs in safety critical and high stake problems such as medical diagnosis, chemical plant control, defense sysFigure 1: Proposed approach: Moments extracted from the local interaction of the model output with the RKHS potential field of the weights quantify the output uncertainty. tems and autonomous driving has led to growing concerns within the research community on the performance trustworthiness of such models (Kendall & Gal, 2017; Lundberg & Lee, 2017). This becomes particularly imperative in situations involving data distributional shifts or the presence of out of distribution data (OOD) during testing towards which the model may lack robustness due to poor choice of training parameters or lack of sufficiently labeled training data, especially since machine learning algorithms do not have extensive prior information like humans to deal with such situations (Amodei et al., 2016). An important way through which trust in the performance of machine learning algorithms (particularly DNNs) can be established is through accurate techniques of predictive uncertainty quantification of models that allow practitioners to determine how much they should rely on model predictions. Although there have been several categories of methods developed in the recent years, the Bayesian approach ar X iv :2 10 9. 10 88 8v 1 [ cs .L G ] 2 2 Se p 20 21 (MacKay, 1992; Neal, 2012; Bishop, 1995) had for long been regarded as the gold standard for natural representation of uncertainty in neural networks. However, it has been realized that they are unable to scale to modern applications and often fail to capture the true data distribution in practice (Lakshminarayanan et al., 2017). Another fundamental limitation of Bayesian techniques is that they are only capable of selecting measurements of central tendency from the posterior. This limits their sensitivity in quantifying local uncertainty information which is especially prevalent in modern applications where models can capture very complex patterns with significant local variations. We propose an approach for model uncertainty quantification that relies on the density and local fit criteria (Leonard et al., 1992) which in this context means that a model prediction y is reliable/certain only if the model has been trained to make predictions in the local vicinity of y (i.e. the model is locally regularized around y). To quantify the model space according to this criteria, one requires a rich localized representation of the model predictive PDF p(y |w) which deems a prediction y reliable only if δ= p(y +∆y |w)−p(y |w) ≈ 0 where∆y is a small perturbation around y . Towards this end, we utilize an uncertainty decomposition framework called the quantum information potential field (QIPF) (Singh & Principe (2020); Singh & Principe (2021)) through which we represent the model weight space as a potential field by embedding the weights in a Gaussian reproducing kernel Hilbert space (RKHS). Such a representation (depicted in Fig. 1) allows us to principally use the notion of perturbation theory in quantum physics to quantify the local gradients of the model’s predictive PDF space in terms of multiple uncertainty moments, thereby giving a high resolution description of δ. In essence, these moments successively quantify the degree of regularization of the weights in the local neighborhood of the model output. The QIPF framework, which was first introduced by (Singh & Principe (2020); Singh & Principe (2021)), has shown promising (albeit very preliminary) results in model uncertainty quantification for regression problems (Singh & Principe, 2021) and in particular applications of time series analysis (Singh & Principe, 2020). It enjoys the following advantages over other methods: • The QIPF utilizes the Gaussian RKHS whose mathematical properties, specifically the kernel trick (Smola & Schölkopf, 1998) and the kernel mean embedding theory (Muandet et al., 2017), makes it a universal injective estimator of data PDF and without making any underlying assumptions. • Through its physics based moment decomposition formulation, it is able to provide a multi-scale description of the local PDF dynamics that focuses on the tail regions of the PDF thereby providing a very accurate description of uncertainty. • It is also significantly simpler to compute and more scalable than Bayesian approaches. It is non-intrusive to the training process of the model and enables a single-shot estimation of model uncertainty at each test instance thereby offering practical advantages over ensemble and Monte Carlo approaches. Our main contributions in this paper are three-fold: • We use perturbation theory to provide a more concrete description of the QIPF framework and provide a new insight into how a potential field viewpoint of the model weight space can be used to describe the degree of regularization/certainty of trained weights in the local neighborhood of the output in terms of multiple moments. • We specifically evaluate the performance of the framework in an important and unsolved problem of model uncertainty quantification in situations involving covariate shift in test data. • We analyze the performance of the QIPF against baselines of multiple UQ approaches using diverse models trained on benchmark datasets. We use both accuracy as well as calibration metrics to evaluate performance.', 'corpus_id': 237593007, 'score': 0}, {'doc_id': '237416610', 'title': 'MACEst: The reliable and trustworthy Model Agnostic Confidence Estimator', 'abstract': 'Reliable Confidence Estimates are hugely important for any machine learning model to be truly useful. In this paper we argue that any confidence estimates based upon standard machine learning point prediction algorithms are fundamentally flawed and under situations with a large amount of epistemic uncertainty are likely to be untrustworthy. To address these issues, we present MACEst, a Model Agnostic Confidence Estimator, which provides reliable and trustworthy confidence estimates. The algorithm differs from current methods by estimating confidence independently as a local quantity which explicitly accounts for both aleatoric and epistemic uncertainty. This approach differs from standard calibration methods that use a global point prediction model as a starting point for the confidence estimate.', 'corpus_id': 237416610, 'score': 1}, {'doc_id': '237563214', 'title': 'Improving Regression Uncertainty Estimation Under Statistical Change', 'abstract': 'While deep neural networks are highly performant and successful in a wide range of real-world problems, estimating their predictive uncertainty remains a challenging task. To address this challenge, we propose and implement a loss function for regression uncertainty estimation based on the Bayesian Validation Metric (BVM) framework while using ensemble learning. A series of experiments on indistribution data show that the proposed method is competitive with existing state-of-the-art methods. In addition, experiments on out-of-distribution data show that the proposed method is robust to statistical change and exhibits superior predictive capability.', 'corpus_id': 237563214, 'score': 0}, {'doc_id': '235694507', 'title': 'Well-calibrated prediction intervals for regression problems', 'abstract': 'Over the last few decades, various methods have been proposed for estimating prediction intervals in regression settings, including Bayesian methods, ensemble methods, direct interval estimation methods and conformal prediction methods. An important issue is the calibration of these methods: the generated prediction intervals should have a predefined coverage level, without being overly conservative. In this work, we review the above four classes of methods from a conceptual and experimental point of view. Results on benchmark data sets from various domains highlight large fluctuations in performance from one data set to another. These observations can be attributed to the violation of certain assumptions that are inherent to some classes of methods. We illustrate how conformal prediction can be used as a general calibration procedure for methods that deliver poor results without a calibration step.', 'corpus_id': 235694507, 'score': 1}, {'doc_id': '237353103', 'title': 'Noisy Labels for Weakly Supervised Gamma Hadron Classification', 'abstract': 'Gamma hadron classification, a central machine learning task in gamma ray astronomy, is conventionally tackled with supervised learning. However, the supervised approach requires annotated training data to be produced in sophisticated and costly simulations. We propose to instead solve gamma hadron classification with a noisy label approach that only uses unlabeled data recorded by the real telescope. To this end, we employ the significance of detection as a learning criterion which addresses this form of weak supervision. We show that models which are based on the significance of detection deliver state-of-the-art results, despite being exclusively trained with noisy labels; put differently, our models do not require the costly simulated ground-truth labels that astronomers otherwise employ for classifier training. Our weakly supervised models exhibit competitive performances also on imbalanced data sets that stem from a variety of other application domains. In contrast to existing work on class-conditional label noise, we assume that only one of the class-wise noise rates is known.', 'corpus_id': 237353103, 'score': 0}]"
144	{'doc_id': '213197447', 'title': 'Asynchronous SGD for DNN training on Shared-memory Parallel Architectures', 'abstract': 'We present a parallel asynchronous Stochastic Gradient Descent algorithm for shared memory architectures. Different from previous asynchronous algorithms, we consider the case where the gradient updates are not particularly sparse. In the context of the MagmaDNN framework, we compare the parallel efficiency of the asynchronous implementation with that of the traditional synchronous implementation. Tests are performed for training deep neural networks on multicore CPUs and GPU devices.', 'corpus_id': 213197447}	5139	"[{'doc_id': '213556752', 'title': 'Deep Adversarial Data Augmentation for Extremely Low Data Regimes', 'abstract': 'Deep learning has revolutionized the performance of classification and object detection, but meanwhile demands sufficient labeled data for training. Given insufficient data, while many techniques have been developed to help combat overfitting, the challenge remains if one tries to train deep networks, especially in the ill-posed extremely low data regimes: only a small set of labeled data are available, and nothing – including unlabeled data – else. Such regimes arise from practical situations where not only data labeling but also data collection itself is expensive. We propose a deep adversarial data augmentation (DADA) technique to address the problem, in which we elaborately formulate data augmentation as a problem of training a class-conditional and supervised generative adversarial network (GAN). Specifically, a new discriminator loss is proposed to fit the goal of data augmentation, through which both real and augmented samples are enforced to contribute to and be consistent in finding the decision boundaries. Tailored training techniques are developed accordingly. To quantitatively validate its effectiveness, we first perform extensive simulations to show that DADA substantially outperforms both traditional data augmentation and a few GAN-based options. We then extend experiments to three real-world small labeled classification datasets where existing data augmentation and/or transfer learning strategies are either less effective or infeasible. We also demonstrate that DADA to can be extended to the detection task. We improve the pedestrian synthesis work by substitute for our discriminator and training scheme. Validation experiment shows that DADA can improve the detection mean average precision (mAP) compared with some traditional data augmentation techniques in object detection. Source code is available at https://github.com/SchafferZhang/DADA.', 'corpus_id': 213556752, 'score': 1}, {'doc_id': '232147062', 'title': 'Self-Supervised Online Reward Shaping in Sparse-Reward Environments', 'abstract': 'We propose a novel reinforcement learning framework that performs self-supervised online reward shaping, yielding faster, sample efficient performance in sparse-reward environments. The proposed framework alternates between updating a policy and inferring a reward function. While the policy update is performed with the inferred, potentially dense reward function, the original sparse reward is used to provide a self-supervisory signal for the reward update by serving as an ordering over the observed trajectories. The proposed framework is based on the theory that altering the reward function does not affect the optimal policy of the original MDP as long as certain relations between the altered and the original reward are maintained. We name the proposed framework ClAssification-based Reward Shaping (CaReS), since the altered reward is learned in a self-supervised manner using classifier-based reward inference. Experimental results on several sparse-reward environments demonstrate that the proposed algorithm is not only significantly more sample efficient than the state-of-the-art reinforcement learning baseline but also achieves a similar sample efficiency to a baseline that uses hand-designed dense reward functions.', 'corpus_id': 232147062, 'score': 1}, {'doc_id': '11476131', 'title': 'Large-Scale Optimization for Evaluation Functions with Minimax Search', 'abstract': ""This paper presents a new method, Minimax Tree Optimization (MMTO), to learn a heuristic evaluation function of a practical alpha-beta search program. The evaluation function may be a linear or non-linear combination of weighted features, and the weights are the parameters to be optimized. To control the search results so that the move decisions agree with the game records of human experts, a well-modeled objective function to be minimized is designed. Moreover, a numerical iterative method is used to find local minima of the objective function, and more than forty million parameters are adjusted by using a small number of hyper parameters. This method was applied to shogi, a major variant of chess in which the evaluation function must handle a larger state space than in chess. Experimental results show that the large-scale optimization of the evaluation function improves the playing strength of shogi programs, and the new method performs significantly better than other methods. Implementation of the new method in our shogi program Bonanza made substantial contributions to the program's first-place finish in the 2013 World Computer Shogi Championship. Additionally, we present preliminary evidence of broader applicability of our method to other two-player games such as chess."", 'corpus_id': 11476131, 'score': 1}, {'doc_id': '219721048', 'title': 'Learning What to Defer for Maximum Independent Sets', 'abstract': 'Designing efficient algorithms for combinatorial optimization appears ubiquitously in various scientific fields. Recently, deep reinforcement learning (DRL) frameworks have gained considerable attention as a new approach: they can automate the design of a solver while relying less on sophisticated domain knowledge of the target problem. However, the existing DRL solvers determine the solution using a number of stages proportional to the number of elements in the solution, which severely limits their applicability to large-scale graphs. In this paper, we seek to resolve this issue by proposing a novel DRL scheme, coined learning what to defer (LwD), where the agent adaptively shrinks or stretch the number of stages by learning to distribute the element-wise decisions of the solution at each stage. We apply the proposed framework to the maximum independent set (MIS) problem, and demonstrate its significant improvement over the current state-of-the-art DRL scheme. We also show that LwD can outperform the conventional MIS solvers on large-scale graphs having millions of vertices, under a limited time budget.', 'corpus_id': 219721048, 'score': 0}, {'doc_id': '221971120', 'title': 'Visual Steering for One-Shot Deep Neural Network Synthesis', 'abstract': 'Recent advancements in the area of deep learning have shown the effectiveness of very large neural networks in several applications. However, as these deep neural networks continue to grow in size, it becomes more and more difficult to configure their many parameters to obtain good results. Presently, analysts must experiment with many different configurations and parameter settings, which is labor-intensive and time-consuming. On the other hand, the capacity of fully automated techniques for neural network architecture search is limited without the domain knowledge of human experts. To deal with the problem, we formulate the task of neural network architecture optimization as a graph space exploration, based on the one-shot architecture search technique. In this approach, a super-graph of all candidate architectures is trained in one-shot and the optimal neural network is identified as a sub-graph. In this paper, we present a framework that allows analysts to effectively build the solution sub-graph space and guide the network search by injecting their domain knowledge. Starting with the network architecture space composed of basic neural network components, analysts are empowered to effectively select the most promising components via our one-shot search scheme. Applying this technique in an iterative manner allows analysts to converge to the best performing neural network architecture for a given application. During the exploration, analysts can use their domain knowledge aided by cues provided from a scatterplot visualization of the search space to edit different components and guide the search for faster convergence. We designed our interface in collaboration with several deep learning researchers and its final effectiveness is evaluated with a user study and two case studies.', 'corpus_id': 221971120, 'score': 0}, {'doc_id': '221761540', 'title': 'MoPro: Webly Supervised Learning with Momentum Prototypes', 'abstract': 'We propose a webly-supervised representation learning method that does not suffer from the annotation unscalability of supervised learning, nor the computation unscalability of self-supervised learning. Most existing works on webly-supervised representation learning adopt a vanilla supervised learning method without accounting for the prevalent noise in the training data, whereas most prior methods in learning with label noise are less effective for real-world large-scale noisy data. We propose momentum prototypes (MoPro), a simple contrastive learning method that achieves online label noise correction, out-of-distribution sample removal, and representation learning. MoPro achieves state-of-the-art performance on WebVision, a weakly-labeled noisy dataset. MoPro also shows superior performance when the pretrained model is transferred to down-stream image classification and detection tasks. It outperforms the ImageNet supervised pretrained model by +10.5 on 1-shot classification on VOC, and outperforms the best self-supervised pretrained model by +17.3 when finetuned on 1\\% of ImageNet labeled samples. Furthermore, MoPro is more robust to distribution shifts. Code and pretrained models are available at this https URL.', 'corpus_id': 221761540, 'score': 0}, {'doc_id': '218538147', 'title': 'Learning, transferring, and recommending performance knowledge with Monte Carlo tree search and neural networks', 'abstract': 'Making changes to a program to optimize its performance is an unscalable task that relies entirely upon human intuition and experience. In addition, companies operating at large scale are at a stage where no single individual understands the code controlling its systems, and for this reason, making changes to improve performance can become intractably difficult. In this paper, a learning system is introduced that provides AI assistance for finding recommended changes to a program. Specifically, it is shown how the evaluative feedback, delayed-reward performance programming domain can be effectively formulated via the Monte Carlo tree search (MCTS) framework. It is then shown that established methods from computational games for using learning to expedite tree-search computation can be adapted to speed up computing recommended program alterations. Estimates of expected utility from MCTS trees built for previous problems are used to learn a sampling policy that remains effective across new problems, thus demonstrating transferability of optimization knowledge. This formulation is applied to the Apache Spark distributed computing environment, and a preliminary result is observed that the time required to build a search tree for finding recommendations is reduced by up to a factor of 10x.', 'corpus_id': 218538147, 'score': 0}, {'doc_id': '231617285', 'title': 'Selective Fine-Tuning on a Classifier Ensemble: Realizing Adaptive Neural Networks With a Diversified Multi-Exit Architecture', 'abstract': 'Adaptive neural networks that provide a trade-off between computing costs and inference performance can be a crucial solution for edge artificial intelligence (AI) computing where resource and energy consumption are significantly constrained. Edge AIs require a fine-tuning technique to achieve target accuracy with less computation for pre-trained models on the cloud. However, a multi-exit network, which realizes adaptive inference costs, requires significant training costs because it has many classifiers that need to be fine-tuned. In this study, we propose a novel fine-tuning method for an ensemble of classifiers that efficiently retrain the multi-exit network. The proposed fine-tuning method exploits individualities by assembling the output of the intermediate classifiers trained with distinct preprocessed data. The evaluation results show that the proposed method achieved 0.2%-5.8%, 0.2%-4.6% higher accuracy with only 77%-93%, 73%-84% training computation compared to the entire fine-tuning of classifiers on the pre-modified CIFAR-100 and Imagenet, respectively, although it depends on assumed edge environments.', 'corpus_id': 231617285, 'score': 1}, {'doc_id': '201893351', 'title': 'A Game Model for Gomoku Based on Deep Learning and Monte Carlo Tree Search', 'abstract': 'Alpha Zero has made remarkable achievements in Go, Chess and Japanese Chess without human knowledge. Generally, the hardware resources have much influence on the effect of model training significantly. It is important to study game model that do not rely excessively on high-performance computing capabilities. In view of this, by referring to the methods used in AlphaGo Zero, this paper studies the model applying deep learning (DL) and monte carlo tree search (MCTS) with a simple deep neural network (DNN) structure on the Game of Gomoku Model, without considering human expert knowledge. Additionally, an improved method to accelerate MCTS search is proposed on the base of the characteristics of Gomoku. Experiments show that this model can improve the chess power in a short training time with limited hardware resources.', 'corpus_id': 201893351, 'score': 1}, {'doc_id': '216562627', 'title': 'Image Augmentation Is All You Need: Regularizing Deep Reinforcement Learning from Pixels', 'abstract': ""We propose a simple data augmentation technique that can be applied to standard model-free reinforcement learning algorithms, enabling robust learning directly from pixels without the need for auxiliary losses or pre-training. The approach leverages input perturbations commonly used in computer vision tasks to regularize the value function. Existing model-free approaches, such as Soft Actor-Critic (SAC), are not able to train deep networks effectively from image pixels. However, the addition of our augmentation method dramatically improves SAC's performance, enabling it to reach state-of-the-art performance on the DeepMind control suite, surpassing model-based (Dreamer, PlaNet, and SLAC) methods and recently proposed contrastive learning (CURL). Our approach can be combined with any model-free reinforcement learning algorithm, requiring only minor modifications. An implementation can be found at this https URL."", 'corpus_id': 216562627, 'score': 0}]"
145	{'doc_id': '218673862', 'title': 'Optimizing for the Future in Non-Stationary MDPs', 'abstract': 'Most reinforcement learning methods are based upon the key assumption that the transition dynamics and reward functions are fixed, that is, the underlying Markov decision process is stationary. However, in many real-world applications, this assumption is violated, and using existing algorithms may result in a performance lag. To proactively search for a good future policy, we present a policy gradient algorithm that maximizes a forecast of future performance. This forecast is obtained by fitting a curve to the counter-factual estimates of policy performance over time, without explicitly modeling the underlying non-stationarity. The resulting algorithm amounts to a non-uniform reweighting of past data, and we observe that minimizing performance over some of the data from past episodes can be beneficial when searching for a policy that maximizes future performance. We show that our algorithm, called Prognosticator, is more robust to non-stationarity than two online adaptation techniques, on three simulated problems motivated by real-world applications.', 'corpus_id': 218673862}	5117	[{'doc_id': '227126670', 'title': 'Solving path dependent PDEs with LSTM networks and path signatures', 'abstract': 'Using a combination of recurrent neural networks and signature methods from the rough paths theory we design efficient algorithms for solving parametric families of path dependent partial differential equations (PPDEs) that arise in pricing and hedging of path-dependent derivatives or from use of non-Markovian model, such as rough volatility models in Jacquier and Oumgari, 2019. The solutions of PPDEs are functions of time, a continuous path (the asset price history) and model parameters. As the domain of the solution is infinite dimensional many recently developed deep learning techniques for solving PDEs do not apply. Similarly as in Vidales et al. 2018, we identify the objective function used to learn the PPDE by using martingale representation theorem. As a result we can de-bias and provide confidence intervals for then neural network-based algorithm. We validate our algorithm using classical models for pricing lookback and auto-callable options and report errors for approximating both prices and hedging strategies.', 'corpus_id': 227126670, 'score': 0}, {'doc_id': '224814213', 'title': 'Logistic $Q$-Learning', 'abstract': 'We propose a new reinforcement learning algorithm derived from a regularized linear-programming formulation of optimal control in MDPs. The method is closely related to the classic Relative Entropy Policy Search (REPS) algorithm of Peters et al. (2010), with the key difference that our method introduces a Q-function that enables efficient exact model-free implementation. The main feature of our algorithm (called QREPS) is a convex loss function for policy evaluation that serves as a theoretically sound alternative to the widely used squared Bellman error. We provide a practical saddle-point optimization method for minimizing this loss function and provide an error-propagation analysis that relates the quality of the individual updates to the performance of the output policy. Finally, we demonstrate the effectiveness of our method on a range of benchmark problems.', 'corpus_id': 224814213, 'score': 0}, {'doc_id': '227151560', 'title': 'Remaining Useful Life Estimation Under Uncertainty with Causal GraphNets', 'abstract': 'In this work, a novel approach for the construction and training of time series models is presented that deals with the problem of learning on large time series with non-equispaced observations, which at the same time may possess features of interest that span multiple scales. The proposed method is appropriate for constructing predictive models for non-stationary stochastic time series.The efficacy of the method is demonstrated on a simulated stochastic degradation dataset and on a real-world accelerated life testing dataset for ball-bearings. The proposed method, which is based on GraphNets, implicitly learns a model that describes the evolution of the system at the level of a state-vector rather than of a raw observation. The proposed approach is compared to a recurrent network with a temporal convolutional feature extractor head (RNN-tCNN) which forms a known viable alternative for the problem context considered. Finally, by taking advantage of recent advances in the computation of reparametrization gradients for learning probability distributions, a simple yet effective technique for representing prediction uncertainty as a Gamma distribution over remaining useful life predictions is employed.', 'corpus_id': 227151560, 'score': 0}, {'doc_id': '231879647', 'title': 'Causal Inference for Time series Analysis: Problems, Methods and Evaluation', 'abstract': 'Time series data is a collection of chronological observations which are generated by several domains such as medical and financial fields. Over the years, different tasks such as classification, forecasting and clustering have been proposed to analyze this type of data. Time series data has been also used to study the effect of interventions overtime. Moreover, in many fields of science, learning the causal structure of dynamic systems and time series data is considered an interesting task which plays an important role in scientific discoveries. Estimating the effect of an intervention and identifying the causal relations from the data can be performed via causal inference. Existing surveys on time series discuss traditional tasks such as classification and forecasting or explain the details of the approaches proposed to solve a specific task. In this paper, we focus on two causal inference tasks, i.e., treatment effect estimation and causal discovery for time series data and provide a comprehensive review of the approaches in each task. Furthermore, we curate a list of commonly used evaluation metrics and datasets for each task and provide an in-depth insight. These metrics and datasets can serve as benchmark for research in the field.', 'corpus_id': 231879647, 'score': 1}, {'doc_id': '235421581', 'title': 'COHORTNEY: Non-Parametric Clustering of Event Sequences', 'abstract': 'Cohort analysis is a pervasive activity in web analytics. One divides users into groups according to specific criteria and tracks their behavior over time. Despite its extensive use, academic circles do not discuss cohort analysis to evaluate user behavior online. This work introduces an unsupervised non-parametric approach to group Internet users based on their activities. In comparison, canonical methods in marketing and engineering-based techniques underperform. COHORTNEY is the first machine learning-based cohort analysis algorithm with a robust theoretical explanation.', 'corpus_id': 235421581, 'score': 1}, {'doc_id': '226299995', 'title': 'Discrete solution pools and noise-contrastive estimation for predict-and-optimize', 'abstract': 'Numerous real-life decision-making processes involve solving a combinatorial optimization problem with uncertain input that can be estimated from historic data. There is a growing interest in decision-focused learning methods, where the loss function used for learning to predict the uncertain input uses the outcome of solving the combinatorial problem over a set of predictions. Different surrogate loss functions have been identified, often using a continuous approximation of the combinatorial problem. However, a key bottleneck is that to compute the loss, one has to solve the combinatorial optimisation problem for each training instance in each epoch, which is computationally expensive even in the case of continuous approximations. \nWe propose a different solver-agnostic method for decision-focused learning, namely by considering a pool of feasible solutions as a discrete approximation of the full combinatorial problem. Solving is now trivial through a single pass over the solution pool. We design several variants of a noise-contrastive loss over the solution pool, which we substantiate theoretically and empirically. Furthermore, we show that by dynamically re-solving only a fraction of the training instances each epoch, our method performs on par with the state of the art, whilst drastically reducing the time spent solving, hence increasing the feasibility of predict-and-optimize for larger problems.', 'corpus_id': 226299995, 'score': 0}, {'doc_id': '220686670', 'title': 'Complex Sequential Data Analysis: A Systematic Literature Review of Existing Algorithms', 'abstract': 'This paper provides a review of past approaches to the use of deep-learning frameworks for the analysis of discrete irregular-patterned complex sequential datasets. A typical example of such a dataset is financial data where specific events trigger sudden irregular changes in the sequence of the data. Traditional deep-learning methods perform poorly or even fail when trying to analyse these datasets. The results of a systematic literature review reveal the dominance of frameworks based on recurrent neural networks. The performance of deep-learning frameworks was found to be evaluated mainly using mean absolute error and root mean square error accuracy metrics. Underlying challenges that were identified are: lack of performance robustness, non-transparency of the methodology, internal and external architectural design and configuration issues. These challenges provide an opportunity to improve the framework for complex irregular-patterned sequential datasets.', 'corpus_id': 220686670, 'score': 1}, {'doc_id': '229679944', 'title': 'Towards Continual Reinforcement Learning: A Review and Perspectives', 'abstract': 'In this article, we aim to provide a literature review of different formulations and approaches to continual reinforcement learning (RL), also known as lifelong or non-stationary RL. We begin by discussing our perspective on why RL is a natural fit for studying continual learning. We then provide a taxonomy of different continual RL formulations and mathematically characterize the non-stationary dynamics of each setting. We go on to discuss evaluation of continual RL agents, providing an overview of benchmarks used in the literature and important metrics for understanding agent performance. Finally, we highlight open problems and challenges in bridging the gap between the current state of continual RL and findings in neuroscience. While still in its early days, the study of continual RL has the promise to develop better incremental reinforcement learners that can function in increasingly realistic applications where non-stationarity plays a vital role. These include applications such as those in the fields of healthcare, education, logistics, and robotics.', 'corpus_id': 229679944, 'score': 1}, {'doc_id': '208248131', 'title': 'Economy Statistical Recurrent Units For Inferring Nonlinear Granger Causality', 'abstract': 'Granger causality is a widely-used criterion for analyzing interactions in large-scale networks. As most physical interactions are inherently nonlinear, we consider the problem of inferring the existence of pairwise Granger causality between nonlinearly interacting stochastic processes from their time series measurements. Our proposed approach relies on modeling the embedded nonlinearities in the measurements using a component-wise time series prediction model based on Statistical Recurrent Units (SRUs). We make a case that the network topology of Granger causal relations is directly inferrable from a structured sparse estimate of the internal parameters of the SRU networks trained to predict the processes’ time series measurements. We propose a variant of SRU, called economy-SRU, which, by design has considerably fewer trainable parameters, and therefore less prone to overfitting. The economy-SRU computes a low-dimensional sketch of its high-dimensional hidden state in the form of random projections to generate the feedback for its recurrent processing. Additionally, the internal weight parameters of the economy-SRU are strategically regularized in a group-wise manner to facilitate the proposed network in extracting meaningful predictive features that are highly time-localized to mimic real-world causal events. Extensive experiments are carried out to demonstrate that the proposed economy-SRU based time series prediction model outperforms the MLP, LSTM and attention-gated CNN-based time series models considered previously for inferring Granger causality.', 'corpus_id': 208248131, 'score': 1}, {'doc_id': '227013048', 'title': 'Visual Forecasting of Time Series with Image-to-Image Regression', 'abstract': 'Time series forecasting is essential for agents to make decisions in many domains. Existing models rely on classical statistical methods to predict future values based on previously observed numerical information. Yet, practitioners often rely on visualizations such as charts and plots to reason about their predictions. Inspired by the end-users, we re-imagine the topic by creating a framework to produce visual forecasts, similar to the way humans intuitively do. In this work, we take a novel approach by leveraging advances in deep learning to extend the field of time series forecasting to a visual setting. We do this by transforming the numerical analysis problem into the computer vision domain. Using visualizations of time series data as input, we train a convolutional autoencoder to produce corresponding visual forecasts. We examine various synthetic and real datasets with diverse degrees of complexity. Our experiments show that visual forecasting is effective for cyclic data but somewhat less for irregular data such as stock price. Importantly, we find the proposed visual forecasting method to outperform numerical baselines. We attribute the success of the visual forecasting approach to the fact that we convert the continuous numerical regression problem into a discrete domain with quantization of the continuous target signal into pixel space.', 'corpus_id': 227013048, 'score': 0}]
146	{'doc_id': '226965637', 'title': 'No-Regret Reinforcement Learning with Value Function Approximation: a Kernel Embedding Approach', 'abstract': 'We consider the regret minimisation problem in reinforcement learning (RL) in the episodic setting. In many real-world RL environments, the state and action spaces are continuous or very large. Existing approaches establish regret guarantees by either a low-dimensional representation of the probability transition model or a functional approximation of Q functions. However, the understanding of function approximation schemes for state value functions largely remains missing. In this paper, we propose an online model-based RL algorithm, namely the CME-RL, that learns representations of transition distributions as embeddings in a reproducing kernel Hilbert space while carefully balancing the exploitation-exploration tradeoff. We demonstrate the efficiency of our algorithm by proving a frequentist (worst-case) regret bound that is of order $\\tilde{O}\\big(H\\gamma_N\\sqrt{N}\\big)$\\footnote{ $\\tilde{O}(\\cdot)$ hides only absolute constant and poly-logarithmic factors}, where $H$ is the episode length, $N$ is the total number of time steps and $\\gamma_N$ is an information theoretic quantity relating the effective dimension of the state-action feature space. Our method bypasses the need for estimating transition probabilities and applies to any domain on which kernels can be defined. It also brings new insights into the general theory of kernel methods for approximate inference and RL regret minimization.', 'corpus_id': 226965637}	1173	"[{'doc_id': '220250568', 'title': 'Lookahead-Bounded Q-Learning', 'abstract': ""We introduce the lookahead-bounded Q-learning (LBQL) algorithm, a new, provably convergent variant of Q-learning that seeks to improve the performance of standard Q-learning in stochastic environments through the use of ``lookahead'' upper and lower bounds. To do this, LBQL employs previously collected experience and each iteration's state-action values as dual feasible penalties to construct a sequence of sampled information relaxation problems. The solutions to these problems provide estimated upper and lower bounds on the optimal value, which we track via stochastic approximation. These quantities are then used to constrain the iterates to stay within the bounds at every iteration. Numerical experiments on benchmark problems show that LBQL exhibits faster convergence and more robustness to hyperparameters when compared to standard Q-learning and several related techniques. Our approach is particularly appealing in problems that require expensive simulations or real-world interactions."", 'corpus_id': 220250568, 'score': 1}, {'doc_id': '13425545', 'title': 'Monte Carlo Tree Search with Sampled Information Relaxation Dual Bounds', 'abstract': 'In the paper, “Optimistic Monte Carlo Tree Search with Sampled Information Relaxation Dual Bounds,” the authors propose an extension to Monte Carlo tree search that uses the idea of “sampling the future” to produce noisy upper bounds on nodes in the decision tree. These upper bounds can help guide the tree expansion process and produce decision trees that are deeper rather than wider, in effect concentrating computation toward more useful parts of the state space. The algorithm’s effectiveness is illustrated in a ride-sharing setting, where a driver/vehicle needs to make dynamic decisions regarding trip acceptance and relocations.', 'corpus_id': 13425545, 'score': 1}, {'doc_id': '221340812', 'title': 'The Advantage Regret-Matching Actor-Critic', 'abstract': ""Regret minimization has played a key role in online learning, equilibrium computation in games, and reinforcement learning (RL). In this paper, we describe a general model-free RL method for no-regret learning based on repeated reconsideration of past behavior. We propose a model-free RL algorithm, the AdvantageRegret-Matching Actor-Critic (ARMAC): rather than saving past state-action data, ARMAC saves a buffer of past policies, replaying through them to reconstruct hindsight assessments of past behavior. These retrospective value estimates are used to predict conditional advantages which, combined with regret matching, produces a new policy. In particular, ARMAC learns from sampled trajectories in a centralized training setting, without requiring the application of importance sampling commonly used in Monte Carlo counterfactual regret (CFR) minimization; hence, it does not suffer from excessive variance in large environments. In the single-agent setting, ARMAC shows an interesting form of exploration by keeping past policies intact. In the multiagent setting, ARMAC in self-play approaches Nash equilibria on some partially-observable zero-sum benchmarks. We provide exploitability estimates in the significantly larger game of betting-abstracted no-limit Texas Hold'em."", 'corpus_id': 221340812, 'score': 0}, {'doc_id': '204801263', 'title': 'Momentum in Reinforcement Learning', 'abstract': ""We adapt the optimization's concept of momentum to reinforcement learning. Seeing the state-action value functions as an analog to the gradients in optimization, we interpret momentum as an average of consecutive $q$-functions. We derive Momentum Value Iteration (MoVI), a variation of Value Iteration that incorporates this momentum idea. Our analysis shows that this allows MoVI to average errors over successive iterations. We show that the proposed approach can be readily extended to deep learning. Specifically, we propose a simple improvement on DQN based on MoVI, and experiment it on Atari games."", 'corpus_id': 204801263, 'score': 1}, {'doc_id': '222124941', 'title': 'Neural Thompson Sampling', 'abstract': 'Thompson Sampling (TS) is one of the most effective algorithms for solving contextual multi-armed bandit problems. In this paper, we propose a new algorithm, called Neural Thompson Sampling, which adapts deep neural networks for both exploration and exploitation. At the core of our algorithm is a novel posterior distribution of the reward, where its mean is the neural network approximator, and its variance is built upon the neural tangent features of the corresponding neural network. We prove that, provided the underlying reward function is bounded, the proposed algorithm is guaranteed to achieve a cumulative regret of $\\mathcal{O}(T^{1/2})$, which matches the regret of other contextual bandit algorithms in terms of total round number $T$. Experimental comparisons with other benchmark bandit algorithms on various data sets corroborate our theory.', 'corpus_id': 222124941, 'score': 0}, {'doc_id': '221970176', 'title': 'An Optimal Computing Budget Allocation Tree Policy for Monte Carlo Tree Search', 'abstract': 'We analyze a tree search problem with an underlying Markov decision process, in which the goal is to identify the best action at the root that achieves the highest cumulative reward. We present a new tree policy that optimally allocates a limited computing budget to maximize a lower bound on the probability of correctly selecting the best action at each node. Compared to widely used Upper Confidence Bound (UCB) tree policies, the new tree policy presents a more balanced approach to manage the exploration and exploitation trade-off when the sampling budget is limited. Furthermore, UCB assumes that the support of reward distribution is known, whereas our algorithm relaxes this assumption. Numerical experiments demonstrate the efficiency of our algorithm in selecting the best action at the root.', 'corpus_id': 221970176, 'score': 1}, {'doc_id': '222208619', 'title': 'Reward-Biased Maximum Likelihood Estimation for Linear Stochastic Bandits', 'abstract': 'Modifying the reward-biased maximum likelihood method originally proposed in the adaptive control literature, we propose novel learning algorithms to handle the explore-exploit trade-off in linear bandits problems as well as generalized linear bandits problems. We develop novel index policies that we prove achieve order-optimality, and show that they achieve empirical performance competitive with the state-of-the-art benchmark methods in extensive experiments. The new policies achieve this with low computation time per pull for linear bandits, and thereby resulting in both favorable regret as well as computational efficiency.', 'corpus_id': 222208619, 'score': 0}, {'doc_id': '226328531', 'title': 'Heuristic Sensing: An Uncertainty Exploration Method in Imperfect Information Games', 'abstract': 'Imperfect information games have served as benchmarks and milestones in fields of artificial intelligence (AI) and game theory for decades. Sensing and exploiting information to effectively describe the game environment is of critical importance for game solving, besides computing or approximating an optimal strategy. Reconnaissance blind chess (RBC), a new variant of chess, is a quintessential game of imperfect information where the player’s actions are definitely unobserved by the opponent. This characteristic of RBC exponentially expands the scale of the information set and extremely invokes uncertainty of the game environment. In this paper, we introduce a novel sense method, Heuristic Search of Uncertainty Control (HSUC), to significantly reduce the uncertainty of real-time information set. The key idea of HSUC is to consider the whole uncertainty of the environment rather than predicting the opponents’ strategy. Furthermore, we realize a practical framework for RBC game that incorporates our HSUC method with Monte Carlo Tree Search (MCTS). In the experiments, HSUC has shown better effectiveness and robustness than comparison opponents in information sensing. It is worth mentioning that our RBC game agent has won the first place in terms of uncertainty management in NeurIPS 2019 RBC tournament.', 'corpus_id': 226328531, 'score': 1}]"
147	{'doc_id': '36920242', 'title': 'Representation and Processing of Structures with Binary Sparse Distributed Codes', 'abstract': 'The schemes for compositional distributed representations include those allowing on-the-fly construction of fixed dimensionality codevectors to encode structures of various complexity. Similarity of such codevectors takes into account both structural and semantic similarity of represented structures. We provide a comparative description of sparse binary distributed representation developed in the framework of the associative-projective neural network architecture and the more well known holographic reduced representations of T.A. Plate (1995) and binary spatter codes of P. Kanerva (1996). The key procedure in associative-projective neural networks is context-dependent thinning which binds codevectors and maintains their sparseness. The codevectors are stored in structured memory array which can be realized as distributed auto-associative memory. Examples of distributed representation of structured data are given. Fast estimation of the similarity of analogical episodes by the overlap of their codevectors is used in the modeling of analogical reasoning both for retrieval of analogs from memory and for analogical mapping.', 'corpus_id': 36920242}	1214	"[{'doc_id': '211096977', 'title': 'Classifying the classifier: dissecting the weight space of neural networks', 'abstract': 'This paper presents an empirical study on the weights of neural networks, where we interpret each model as a point in a high-dimensional space -- the neural weight space. To explore the complex structure of this space, we sample from a diverse selection of training variations (dataset, optimization procedure, architecture, etc.) of neural network classifiers, and train a large number of models to represent the weight space. Then, we use a machine learning approach for analyzing and extracting information from this space. Most centrally, we train a number of novel deep meta-classifiers with the objective of classifying different properties of the training setup by identifying their footprints in the weight space. Thus, the meta-classifiers probe for patterns induced by hyper-parameters, so that we can quantify how much, where, and when these are encoded through the optimization process. This provides a novel and complementary view for explainable AI, and we show how meta-classifiers can reveal a great deal of information about the training setup and optimization, by only considering a small subset of randomly selected consecutive weights. To promote further research on the weight space, we release the neural weight space (NWS) dataset -- a collection of 320K weight snapshots from 16K individually trained deep neural networks.', 'corpus_id': 211096977, 'score': 0}, {'doc_id': '211296509', 'title': 'Declarative Memory-based Structure for the Representation of Text Data', 'abstract': 'In the era of intelligent computing, computational progress in text processing is an essential consideration. Many systems have been developed to process text over different languages. Though, there is considerable development, they still lack in understanding of the text, i.e., instead of keeping text as knowledge, many treat text as a data. In this work we introduce a text representation scheme which is influenced by human memory infrastructure. Since texts are declarative in nature, a structural organization would foster efficient computation over text. We exploit long term episodic memory to keep text information observed over time. This not only keep fragments of text in an organized fashion but also reduces redundancy and stores the temporal relation among them. Wordnet has been used to imitate semantic memory, which works at word level to facilitate the understanding about individual words within text. Experimental results of various operation performed over episodic memory and growth of knowledge infrastructure over time is reported.', 'corpus_id': 211296509, 'score': 0}, {'doc_id': '216080530', 'title': 'Classification Using Hyperdimensional Computing: A Review', 'abstract': 'Hyperdimensional (HD) computing is built upon its unique data type referred to as hypervectors. The dimension of these hypervectors is typically in the range of tens of thousands. Proposed to solve cognitive tasks, HD computing aims at calculating similarity among its data. Data transformation is realized by three operations, including addition, multiplication and permutation. Its ultra-wide data representation introduces redundancy against noise. Since information is evenly distributed over every bit of the hypervectors, HD computing is inherently robust. Additionally, due to the nature of those three operations, HD computing leads to fast learning ability, high energy efficiency and acceptable accuracy in learning and classification tasks. This paper introduces the background of HD computing, and reviews the data representation, data transformation, and similarity measurement. The orthogonality in high dimensions presents opportunities for flexible computing. To balance the tradeoff between accuracy and efficiency, strategies include but are not limited to encoding, retraining, binarization and hardware acceleration. Evaluations indicate that HD computing shows great potential in addressing problems using data in the form of letters, signals and images. HD computing especially shows significant promise to replace machine learning algorithms as a light-weight classifier in the field of internet of things (IoTs).', 'corpus_id': 216080530, 'score': 1}, {'doc_id': '195855432', 'title': 'A neural representation of continuous space using fractional binding', 'abstract': 'We present a novel method for constructing neurally implemented spatial representations that we show to be useful for building models of spatial cognition. This method represents continuous (i.e., real-valued) spaces using neurons, and identifies a set of operations for manipulating these representations. Specifically, we use “fractional binding” to construct “spatial semantic pointers” (SSPs) that we use to generate and manipulate representations of spatial maps encoding the positions of objects. We show how these representations can be transformed to answer queries about the location and identities of objects, move the relative or global position of items, and answer queries about regions of space, among other things. We demonstrate that the neural implementation in spiking networks of SSPs have similar accuracy and capacity as the mathematical ideal.', 'corpus_id': 195855432, 'score': 1}, {'doc_id': '211082582', 'title': 'Neuroevolution of Neural Network Architectures Using CoDeepNEAT and Keras', 'abstract': 'Machine learning is a huge field of study in computer science and statistics dedicated to the execution of computational tasks through algorithms that do not require explicit instructions but instead rely on learning patterns from data samples to automate inferences. A large portion of the work involved in a machine learning project is to define the best type of algorithm to solve a given problem. Neural networks - especially deep neural networks - are the predominant type of solution in the field. However, the networks themselves can produce very different results according to the architectural choices made for them. Finding the optimal network topology and configurations for a given problem is a challenge that requires domain knowledge and testing efforts due to a large number of parameters that need to be considered. The purpose of this work is to propose an adapted implementation of a well-established evolutionary technique from the neuroevolution field that manages to automate the tasks of topology and hyperparameter selection. It uses a popular and accessible machine learning framework - Keras - as the back-end, presenting results and proposed changes concerning the original algorithm. The implementation is available at GitHub (this https URL) with documentation and examples to reproduce the experiments performed for this work.', 'corpus_id': 211082582, 'score': 0}, {'doc_id': '216642177', 'title': 'Task-Projected Hyperdimensional Computing for Multi-task Learning', 'abstract': 'Brain-inspired Hyperdimensional (HD) computing is an emerging technique for cognitive tasks in the field of low-power design. As an energy-efficient and fast learning computational paradigm, HD computing has shown great success in many real-world applications. However, an HD model incrementally trained on multiple tasks suffers from the negative impacts of catastrophic forgetting. The model forgets the knowledge learned from previous tasks and only focuses on the current one. To the best of our knowledge, no study has been conducted to investigate the feasibility of applying multi-task learning to HD computing. In this paper, we propose Task-Projected Hyperdimensional Computing (TP-HDC) to make the HD model simultaneously support multiple tasks by exploiting the redundant dimensionality in the hyperspace. To mitigate the interferences between different tasks, we project each task into a separate subspace for learning. Compared with the baseline method, our approach efficiently utilizes the unused capacity in the hyperspace and shows a 12.8% improvement in averaged accuracy with negligible memory overhead.', 'corpus_id': 216642177, 'score': 1}, {'doc_id': '209497828', 'title': 'SemiHD: Semi-Supervised Learning Using Hyperdimensional Computing', 'abstract': ""In the Internet of Things (IoT), the large volume of data generated by sensors poses significant computational challenges in resource-constrained environments. Most existing machine learning algorithms are unable to train a proper model using a significantly small amount of labeled data available in practice. In this paper, we propose SemiHD, a novel semi-supervised algorithm based on brain-inspired HyperDimensional (HD) computing. SemiHD performs the cognitive task by emulating neuron's activity in high-dimensional space. SemiHD maps data points into high-dimensional space and trains a model based on the available labeled data. To improve the quality of the model, SemiHD iteratively expands the training data by labeling data points which can be classified by the current model with high confidence. We also proposed a framework which enables users to trade accuracy for efficiency and select the desired reliability of the model in detecting out of scope data. We have evaluated SemiHD's accuracy and efficiency on a wide range of classification applications and two types of embedded devices: Raspberry Pi 3 and Kintex-7 FPGA. Our evaluation shows that SemiHD can improve the classification accuracy of supervised HD by 10.2% on average (up to 27.3%). In addition, we observe that SemiHD FPGA implementation achieves 7.11× faster and 12.6× energy efficiency as compared to the CPU implementation."", 'corpus_id': 209497828, 'score': 1}, {'doc_id': '212658019', 'title': 'Text classification with word embedding regularization and soft similarity measure', 'abstract': ""Since the seminal work of Mikolov et al., word embeddings have become the preferred word representations for many natural language processing tasks. Document similarity measures extracted from word embeddings, such as the soft cosine measure (SCM) and the Word Mover's Distance (WMD), were reported to achieve state-of-the-art performance on semantic text similarity and text classification. \nDespite the strong performance of the WMD on text classification and semantic text similarity, its super-cubic average time complexity is impractical. The SCM has quadratic worst-case time complexity, but its performance on text classification has never been compared with the WMD. Recently, two word embedding regularization techniques were shown to reduce storage and memory costs, and to improve training speed, document processing speed, and task performance on word analogy, word similarity, and semantic text similarity. However, the effect of these techniques on text classification has not yet been studied. \nIn our work, we investigate the individual and joint effect of the two word embedding regularization techniques on the document processing speed and the task performance of the SCM and the WMD on text classification. For evaluation, we use the $k$NN classifier and six standard datasets: BBCSPORT, TWITTER, OHSUMED, REUTERS-21578, AMAZON, and 20NEWS. \nWe show 39% average $k$NN test error reduction with regularized word embeddings compared to non-regularized word embeddings. We describe a practical procedure for deriving such regularized embeddings through Cholesky factorization. We also show that the SCM with regularized word embeddings significantly outperforms the WMD on text classification and is over 10,000 times faster."", 'corpus_id': 212658019, 'score': 0}, {'doc_id': '216553168', 'title': ""Neuromorphic Nearest Neighbor Search Using Intel's Pohoiki Springs"", 'abstract': 'Neuromorphic computing applies insights from neuroscience to uncover innovations in computing technology. In the brain, billions of interconnected neurons perform rapid computations at extremely low energy levels by leveraging properties that are foreign to conventional computing systems, such as temporal spiking codes and finely parallelized processing units integrating both memory and computation. Here, we showcase the Pohoiki Springs neuromorphic system, a mesh of 768 interconnected Loihi chips that collectively implement 100 million spiking neurons in silicon. We demonstrate a scalable approximate k-nearest neighbor (k-NN) algorithm for searching large databases that exploits neuromorphic principles. Compared to state-of-the-art conventional CPU-based implementations, we achieve superior latency, index build time, and energy efficiency when evaluated on several standard datasets containing over 1 million high-dimensional patterns. Further, the system supports adding new data points to the indexed database online in O(1) time unlike all but brute force conventional k-NN implementations.', 'corpus_id': 216553168, 'score': 0}, {'doc_id': '9175579', 'title': 'Golden Ratio Sequences for Low-Discrepancy Sampling', 'abstract': 'Abstract Most classical constructions of low-discrepancy point sets are based on generalizations of the one-dimensional binary van der Corput sequence, whose implementation requires nontrivial bit-operations. As an alternative, we introduce the quasi-regular golden ratio sequences, which are based on the fractional part of successive integer multiples of the golden ratio. By leveraging results from number theory, we show that point sets, which evenly cover the unit square or disc, can be computed by a simple incremental permutation of a generator golden ratio sequence. We compare ambient occlusion images generated with a Monte Carlo ray tracer based on random, Hammersley, blue noise, and golden ratio point sets. The source code of the ray tracer used for our experiments is available online at the address provided at the end of this article.', 'corpus_id': 9175579, 'score': 1}]"
148	"{'doc_id': '109955119', 'title': 'Not craft, not couture, not ‘home sewing’: teaching creative patternmaking to the iPod generation', 'abstract': ""Students in apparel programmes today are denizens of a world of computers, social media, and popular culture manifestations such as Project Runway. They have a different focus and different expectations and skills than students from previous generations. Current textbooks and teaching methods for apparel patternmaking were developed by and for these previous generations, and are rooted in the era of home economics. As educators we need to rethink how we introduce students to creative technical apparel design. The methods discussed here were developed over the last 30 years and evolved based on frequent student feedback. Teaching methods that resonate best with current students emphasise the conceptual, visual, and practical; they focus on the creative process and individual development. Exercises are important, but only to introduce basic concepts. Assignments quickly shift to individual creative projects. Technology skills are increasingly important, but the teacher's role is to guide and assess the learning process."", 'corpus_id': 109955119}"	7675	"[{'doc_id': '221226085', 'title': '! ""', 'abstract': 'poetry as well. This was a new initiative of the Department of Culture in which the programme of poetry was organised. Badr Vasti remembered the contribution of the patriots in this. Shashikant Yadav Shashi saluted the valor of military warriors in his long poem. The land of India was bowed down in the lofty composition and composition of Abdul Ghaffar. It was an emotional experience to hear the pain and sorrow of the common man in a poignant voice in the poem by Hariom Pawar, the most senior poet of this project. ! ""', 'corpus_id': 221226085, 'score': 0}, {'doc_id': '221134600', 'title': 'A Parametric Method to Customize Surfboard and Stand Up Paddle Board Fins for Additive Manufacturing', 'abstract': 'At all levels of sporting competition, from recreational to elite, athletes desire products that improve their performance, comfort, safety, or enjoyment. Additive manufacturing, also known as 3D printing, provides a new method of producing sporting equipment that can be customized to the unique needs of an individual. This study examines the opportunity to produce surfboard and stand up paddle (SUP) board fins through 3D printing and details a parametric computer-aided design (CAD) system for surfers to modify the geometry of a surf fin in real-time. Specifically, the fin system, fin position on the board, cant, fin depth, sweep, base length, base foil profile, tip sharpness, tip thickness, as well as the overall dimensions can be modified using simple interactive controls that do not require any CAD experience. Indicative cost data was collected for ten virtual fins designed through this system intended for fused filament fabrication (FFF), selective laser sintering (SLS) and multi jet fusion (MJF) technologies. Two designs were 3D printed and trialled on a SUP board. The method of creating the parametric system is sufficiently detailed in order to be replicable and built upon by designers, with future research directions outlined in order to improve surfing performance and extend the wellestablished culture of experimentation within the sport.', 'corpus_id': 221134600, 'score': 0}, {'doc_id': '148756083', 'title': 'Self-sewn identity: How female home sewers use garment sewing to control self-presentation', 'abstract': 'Garment sewing was once a necessity for women to present themselves and their families in a socially acceptable manner. Despite societal and economics changes, as well as, an abundance of readily available cheap clothing, there is a resurging interest in personal garment sewing by women. This qualitative study explored the control gained by women who sew their own clothing finding that among the women interviewed personal garment sewing allowed them control over their clothing’s style, fit, and quality, which was not found through purchasing ready-to-wear clothes. Garment sewing permitted these women to present themselves in clothing that they felt more accurately represented their personality and taste. These findings provide insights into the usage of garment sewing by women to control their appearance which allowed them more authority over their clothing selection than their non-sewing peers. Validation of the women’s time spent sewing was established as the findings postulate noteworthy benefits that include increased satisfaction with both their appearance and their presentation of self to others. The findings are explained using theories related to self-presentation and identity.', 'corpus_id': 148756083, 'score': 1}, {'doc_id': '220323801', 'title': 'Are your hands clean? Pollen retention on the human hand after washing', 'abstract': '\n Abstract\n \n Pollen retention on clothes, footwear, hair and body has been used to link people to localities with distinctive vegetation, or soils containing distinctive palynomorphs. Little attention has been given to human skin as a possible medium for carrying a forensically important pollen load and whether this might survive attempts to remove it. We report here the results of experiments testing the retention of pollen of 10 flowering plant species on the human skin through repeated cycles of washing and drying hands, using the WHO protocol to standardize hand-washing and drying. Between 0.36% and 2.74% (mean 0.93%) of the initial pollen load was retained through a single hand-wash. Trace amounts of some species survived multiple hand-wash cycles. It is concluded that forensic analyses can be made of the pollen load of those parts of the skin that may have been in contact with palynologically distinctive vegetation, even in cases where the person involved has washed, or been washed. These observations may also be of relevance in cases where human skin became contaminated with other microscopic particulates.\n \n', 'corpus_id': 220323801, 'score': 0}, {'doc_id': '221224719', 'title': 'Fourth drive-through swabbing centre planned MoPH launches ‘ Heroes of the White Army ’ colouring e-booklet for children', 'abstract': 'Sidra Medicine to accept only card payments His Highness the Amir Sheikh Tamim bin Hamad al-Thani sent yesterday a cable of congratulations to the Amir of Kuwait Sheikh Sabah al-Ahmad al-Jaber al-Sabah on his successful surgery, wishing him good health and well-being. His Highness the Deputy Amir Sheikh Abdullah bin Hamad al-Thani and HE the Prime Minister and Minister of Interior Sheikh Khalid bin Khalifa bin Abdulaziz al-Thani also sent similar cables of congratulations to the Kuwaiti Amir. Earlier, the off icial Qatar News Agency, quoting Kuwait news agency said the Amir of Kuwait underwent a successful surgery yesterday morning.', 'corpus_id': 221224719, 'score': 0}, {'doc_id': '149542967', 'title': 'Dressmaking : how a clothing practice made girls in New Zealand, 1945 to 1965 : a thesis submitted in partial fulfilment of the requirements for the degree of Doctor of Philosophy at Massey University, Wellington, New Zealand', 'abstract': '.................................................................................................................................... ii Preface and acknowledgements ........................................................................................... iv List of figures ......................................................................................................................... vii Abbreviations .......................................................................................................................... ix Introduction: “This vital activity” ........................................................................................ 1 Chapter 1 A modern mechanism for fabrication of the self .........................................11 Chapter 2 Dressmaking in society: “A life-long asset to every Woman” ....................43 Chapter 3 Dressmaking at home: “She’d never had a piece of bought clothing in her life” ..........................................................................................................................................67 Chapter 4 Experimenting with tools: “I sat down at the machine” .......................... 100 Chapter 5 The regulation of sewing at school: “A room in the basement has been fitted out” ............................................................................................................................ 136 Chapter 6 A necessary discipline: “Miss Smith made me mend every single one” . 170 Chapter 7 The motivation to keep dressmaking: “I shook it off and made the dress up” ........................................................................................................................................ 201 Conclusion: “The skill comes only from practice” ...................................................... 233 Reference lists ..................................................................................................................... 243 Primary sources .......................................................................................................................... 252 Interviewees ......................................................................................................................... 252 Archives New Zealand....................................................................................................... 252 Newspapers and magazines .............................................................................................. 254 New Zealand secondary schools ...................................................................................... 257 Appendices .......................................................................................................................... 259 Appendix 1: Ethics Committee approval .............................................................................. 260 Appendix 2: Questionnaires and agreements ....................................................................... 262 Appendix 3: Biographies of interviewees .............................................................................. 270 Appendix 4: Archives New Zealand permission .................................................................. 285 Appendix 5: Sewing in the education system ........................................................................ 287', 'corpus_id': 149542967, 'score': 1}, {'doc_id': '221295519', 'title': 'Odor Identification in Older Adults: Evidence from the Yakumo (2019)- Results by Gender and Age', 'abstract': ""An examination of taste and olfactometry in Yakumo-Cho inhabitants’ examination carried out in 2005, and the result reported in Academia Journal of Medicinal Plants 2018. This study examined olfactory function. A personal function test is calculated from the Yakumo study database, and the odor stick identification tests administered to healthy older adults. The participants were community dwellers who voluntarily participated in the Yakumo Study and had managed everyday life by themselves. The participants were engaged in a variety of jobs, not only white-collar but also agriculture, fishery, and forestry. Therefore, the city regarded as a representative of today's Japanese society. From the database, 298 participants (169 females and 129 males) were selected form data in August, 2019. The Odor Stick Identification Test (OSIT-J) was used to assess odor perception. The aromas used in the OSIT-J include curry, perfume, Japanese cypress, India ink, menthol, rose, wood, Smelly socks/ sweat, fried garlic, condensed milk, gas for cooking, and Japanese mandarin aromas."", 'corpus_id': 221295519, 'score': 0}, {'doc_id': '190311922', 'title': '""What a deal of work there is in a dress!’’: Englishness and home dressmaking in the age of the sewing machine', 'abstract': 'This essay explores the home-production and consumption of clothing in relation to Englishness, from the mid. 19th century to the present day. It situates the practice of home dressmaking specifically within the construction and maintenance of social class and gender identities that were so marked within the experience of Englishness during this period. At the same time, given the advent of the domestic sewing machine and the mass-produced paper pattern, and conversely, the growth in ready-made clothing, the domestic practices of English home dressmaking became the target of competing and newly expanding international commercial interests, particularly from the USA and the essay questions the extent to which formations of identity could co-exist with these trends.', 'corpus_id': 190311922, 'score': 1}, {'doc_id': '168485854', 'title': 'The Tailoring Industry, 1850–1914', 'abstract': 'A detailed study of the tailoring industry is particularly appropriate in relation to homework since the industry employed many women in their homes in most of the tailoring centres in the country. Tailoring had a rigid sexual division of labour based on differentials of strength and skill and the position of women in the badly-paid sectors of the industry remained unchanged with the introduction of machinery to the industry. Women could buy or rent sewing machines and use them in their homes. The expansion in demand for ready-made clothing in the early twentieth century was not met by improvements in technology but by employing more women as home workers. The numbers of women homeworkers as a proportion of the total work-force in tailoring varied according to where the industry was situated. In this chapter, the effects of the introduction of the sewing machine in tailoring will be discussed. This is followed by a comparison of three geographical areas where tailoring was carried out in the late nineteenth and early twentieth centuries.', 'corpus_id': 168485854, 'score': 1}, {'doc_id': '115578282', 'title': 'Women’s motivations to sew clothing for themselves', 'abstract': 'Participation in home sewing sewing is undergoing a resurgence in participation with a growing number of women choosing to sew their garments instead of buying readily available fast fashion. The purpose of this grounded theory study was to understand the current motivations of an ethnically diverse sample of women who sew garments for themselves. The in-depth interview data from 15 interviews revealed personal fulfillment to be the overarching reason for their sewing participation. Personal fulfillment was achieved through investment, control, and empowerment that provided the women with accomplishment, a creative outlet, and stress relief. Theoretical perspectives of prosumerism, craft consumer and do-it-yourself consumption uses and gratification theory were applied to better understand the motivations of these women to sew garments for themselves.', 'corpus_id': 115578282, 'score': 1}]"
149	{'doc_id': '221970625', 'title': 'A Weighted Quiver Kernel using Functor Homology', 'abstract': 'In this paper, we propose a new homological method to study weighted directed networks. Our model of such networks is a directed graph $Q$ equipped with a weight function $w$ on the set $Q_{1}$ of arrows in $Q$. We require that the range $W$ of our weight function is equipped with an addition or a multiplication, i.e., $W$ is a monoid in the mathematical terminology. When $W$ is equipped with a representation on a vector space $M$, the standard method of homological algebra allows us to define the homology groups $H_{*}(Q,w;M)$. It is known that when $Q$ has no oriented cycles, $H_{n}(Q,w;M)=0$ for $n\\ge 2$ and $H_{1}(Q,w;M)$ can be easily computed. This fact allows us to define a new graph kernel for weighted directed graphs. We made two sample computations with real data and found that our method is practically applicable.', 'corpus_id': 221970625}	5139	"[{'doc_id': '224705377', 'title': 'i-Mix: A Strategy for Regularizing Contrastive Representation Learning', 'abstract': 'Contrastive representation learning has shown to be an effective way of learning representations from unlabeled data. However, much progress has been made in vision domains relying on data augmentations carefully designed using domain knowledge. In this work, we propose i-Mix, a simple yet effective regularization strategy for improving contrastive representation learning in both vision and non-vision domains. We cast contrastive learning as training a non-parametric classifier by assigning a unique virtual class to each data in a batch. Then, data instances are mixed in both the input and virtual label spaces, providing more augmented data during training. In experiments, we demonstrate that i-Mix consistently improves the quality of self-supervised representations across domains, resulting in significant performance gains on downstream tasks. Furthermore, we confirm its regularization effect via extensive ablation studies across model and dataset sizes.', 'corpus_id': 224705377, 'score': 1}, {'doc_id': '218538147', 'title': 'Learning, transferring, and recommending performance knowledge with Monte Carlo tree search and neural networks', 'abstract': 'Making changes to a program to optimize its performance is an unscalable task that relies entirely upon human intuition and experience. In addition, companies operating at large scale are at a stage where no single individual understands the code controlling its systems, and for this reason, making changes to improve performance can become intractably difficult. In this paper, a learning system is introduced that provides AI assistance for finding recommended changes to a program. Specifically, it is shown how the evaluative feedback, delayed-reward performance programming domain can be effectively formulated via the Monte Carlo tree search (MCTS) framework. It is then shown that established methods from computational games for using learning to expedite tree-search computation can be adapted to speed up computing recommended program alterations. Estimates of expected utility from MCTS trees built for previous problems are used to learn a sampling policy that remains effective across new problems, thus demonstrating transferability of optimization knowledge. This formulation is applied to the Apache Spark distributed computing environment, and a preliminary result is observed that the time required to build a search tree for finding recommendations is reduced by up to a factor of 10x.', 'corpus_id': 218538147, 'score': 0}, {'doc_id': '221856410', 'title': 'Pruning Convolutional Filters Using Batch Bridgeout', 'abstract': 'State-of-the-art computer vision models are rapidly increasing in capacity, where the number of parameters far exceeds the number required to fit the training set. This results in better optimization and generalization performance. However, the huge size of contemporary models results in large inference costs and limits their use on resource-limited devices. In order to reduce inference costs, convolutional filters in trained neural networks could be pruned to reduce the run-time memory and computational requirements during inference. However, severe post-training pruning results in degraded performance if the training algorithm results in dense weight vectors. We propose the use of Batch Bridgeout, a sparsity inducing stochastic regularization scheme, to train neural networks so that they could be pruned efficiently with minimal degradation in performance. We evaluate the proposed method on common computer vision models VGGNet, ResNet and Wide-ResNet on the CIFAR10 and CIFAR100 image classification tasks. For all the networks, experimental results show that Batch Bridgeout trained networks achieve higher accuracy across a wide range of pruning intensities compared to Dropout and weight decay regularization.', 'corpus_id': 221856410, 'score': 0}, {'doc_id': '195317051', 'title': 'Neural Stored-program Memory', 'abstract': 'Neural networks powered with external memory simulate computer behaviors. These models, which use the memory to store data for a neural controller, can learn algorithms and other complex tasks. In this paper, we introduce a new memory to store weights for the controller, analogous to the stored-program memory in modern computer architectures. The proposed model, dubbed Neural Stored-program Memory, augments current memory-augmented neural networks, creating differentiable machines that can switch programs through time, adapt to variable contexts and thus resemble the Universal Turing Machine. A wide range of experiments demonstrate that the resulting machines not only excel in classical algorithmic problems, but also have potential for compositional, continual, few-shot learning and question-answering tasks.', 'corpus_id': 195317051, 'score': 1}, {'doc_id': '218516818', 'title': 'Learning Adaptive Exploration Strategies in Dynamic Environments Through Informed Policy Regularization', 'abstract': 'We study the problem of learning exploration-exploitation strategies that effectively adapt to dynamic environments, where the task may change over time. While RNN-based policies could in principle represent such strategies, in practice their training time is prohibitive and the learning process often converges to poor solutions. In this paper, we consider the case where the agent has access to a description of the task (e.g., a task id or task parameters) at training time, but not at test time. We propose a novel algorithm that regularizes the training of an RNN-based policy using informed policies trained to maximize the reward in each task. This dramatically reduces the sample complexity of training RNN-based policies, without losing their representational power. As a result, our method learns exploration strategies that efficiently balance between gathering information about the unknown and changing task and maximizing the reward over time. We test the performance of our algorithm in a variety of environments where tasks may vary within each episode.', 'corpus_id': 218516818, 'score': 0}, {'doc_id': '216562627', 'title': 'Image Augmentation Is All You Need: Regularizing Deep Reinforcement Learning from Pixels', 'abstract': ""We propose a simple data augmentation technique that can be applied to standard model-free reinforcement learning algorithms, enabling robust learning directly from pixels without the need for auxiliary losses or pre-training. The approach leverages input perturbations commonly used in computer vision tasks to regularize the value function. Existing model-free approaches, such as Soft Actor-Critic (SAC), are not able to train deep networks effectively from image pixels. However, the addition of our augmentation method dramatically improves SAC's performance, enabling it to reach state-of-the-art performance on the DeepMind control suite, surpassing model-based (Dreamer, PlaNet, and SLAC) methods and recently proposed contrastive learning (CURL). Our approach can be combined with any model-free reinforcement learning algorithm, requiring only minor modifications. An implementation can be found at this https URL."", 'corpus_id': 216562627, 'score': 0}, {'doc_id': '219635852', 'title': 'Mutual Information Based Knowledge Transfer Under State-Action Dimension Mismatch', 'abstract': 'Deep reinforcement learning (RL) algorithms have achieved great success on a wide variety of sequential decision-making tasks. However, many of these algorithms suffer from high sample complexity when learning from scratch using environmental rewards, due to issues such as credit-assignment and high-variance gradients, among others. Transfer learning, in which knowledge gained on a source task is applied to more efficiently learn a different but related target task, is a promising approach to improve the sample complexity in RL. Prior work has considered using pre-trained teacher policies to enhance the learning of the student policy, albeit with the constraint that the teacher and the student MDPs share the state-space or the action-space. In this paper, we propose a new framework for transfer learning where the teacher and the student can have arbitrarily different state- and action-spaces. To handle this mismatch, we produce embeddings which can systematically extract knowledge from the teacher policy and value networks, and blend it into the student networks. To train the embeddings, we use a task-aligned loss and show that the representations could be enriched further by adding a mutual information loss. Using a set of challenging simulated robotic locomotion tasks involving many-legged centipedes, we demonstrate successful transfer learning in situations when the teacher and student have different state- and action-spaces.', 'corpus_id': 219635852, 'score': 0}, {'doc_id': '219559361', 'title': 'Bandit Samplers for Training Graph Neural Networks', 'abstract': 'Several sampling algorithms with variance reduction have been proposed for accelerating the training of Graph Convolution Networks (GCNs). However, due to the intractable computation of optimal sampling distribution, these sampling algorithms are suboptimal for GCNs and are not applicable to more general graph neural networks (GNNs) where the message aggregator contains learned weights rather than fixed weights, such as Graph Attention Networks (GAT). The fundamental reason is that the embeddings of the neighbors or learned weights involved in the optimal sampling distribution are changing during the training and not known a priori, but only partially observed when sampled, thus making the derivation of an optimal variance reduced samplers non-trivial. In this paper, we formulate the optimization of the sampling variance as an adversary bandit problem, where the rewards are related to the node embeddings and learned weights, and can vary constantly. Thus a good sampler needs to acquire variance information about more neighbors (exploration) while at the same time optimizing the immediate sampling variance (exploit). We theoretically show that our algorithm asymptotically approaches the optimal variance within a factor of 3. We show the efficiency and effectiveness of our approach on multiple datasets.', 'corpus_id': 219559361, 'score': 1}, {'doc_id': '220252950', 'title': 'Neural architecture search for sparse DenseNets with dynamic compression', 'abstract': 'Neural Architecture Search (NAS) algorithms have discovered highly novel state-of-the-art Convolutional Neural Networks (CNNs) for image classification, and are beginning to improve our understanding of CNN architectures. However, within NAS research, there are limited studies focussing on the role of skip-connections, and how the configurations of connections between layers can be optimised to improve CNN performance. Our work focusses on developing a new evolutionary NAS algorithm based on adjacency matrices to optimise skip-connection structures, creating more specialised and powerful skip-connection structures within a DenseNet-BC network than previously seen in the literature. Our work further demonstrates how simple adjacency matrices can be interpreted in a way which allows for a more dynamic variant of DenseNet-BC. The final algorithm, using this novel interpretation of adjacency matrices for architecture design and evolved on the CIFAR100 dataset, finds networks with improved performance relative to a baseline DenseNet-BC network on both the CIFAR10 and CIFAR100 datasets, being the first, to our knowledge, NAS algorithm for skip-connection optimisation to do so. Finally, skip-connection structures discovered by our algorithm are analysed, and some important skip-connection patterns are highlighted.', 'corpus_id': 220252950, 'score': 1}, {'doc_id': '215815336', 'title': 'Neural architecture search under black-box objectives with deep reinforcement learning and increasingly-sparse rewards', 'abstract': 'In this paper, we address the problem of neural architecture search (NAS) in a context where the optimality policy is driven by a black-box Oracle $\\mathcal{O}$ with unknown form and derivatives. In this scenario, $\\mathcal{O}(A_{C})$ typically provides readings from a set of sensors on how a neural network architecture $A_{C}$ fares in a target hardware, including its: power consumption, working temperature, $\\mathbf{cpu}/\\mathbf{gpu}$ usage, central bus occupancy, and more. Current differentiable NAS approaches fail in this problem context due to lack of access to derivatives, whereas traditional reinforcement learning NAS approaches remain too expensive computationally. As solution, we propose a reinforcement learning NAS strategy based on policy gradient with increasingly sparse rewards. We rely on the fact [1] that one does not need to fully train the weights of two neural networks to compare them. Our solution starts by comparing architecture candidates with almost fixed weights and no training, and progressively shifts toward comparisons under full weights training. Experimental results confirmed both the accuracy and training efficiency of our solution, as well as its compliance with soft/hard constraints imposed on the sensors feedback. Our strategy allows finding near-optimal architectures significantly faster, in approximately 1/3 of the time it would take otherwise.', 'corpus_id': 215815336, 'score': 1}]"
150	{'doc_id': '7228830', 'title': 'Adversarial Examples for Evaluating Reading Comprehension Systems', 'abstract': 'Standard accuracy metrics indicate that reading comprehension systems are making rapid progress, but the extent to which these systems truly understand language remains unclear. To reward systems with real language understanding abilities, we propose an adversarial evaluation scheme for the Stanford Question Answering Dataset (SQuAD). Our method tests whether systems can answer questions about paragraphs that contain adversarially inserted sentences, which are automatically generated to distract computer systems without changing the correct answer or misleading humans. In this adversarial setting, the accuracy of sixteen published models drops from an average of 75% F1 score to 36%; when the adversary is allowed to add ungrammatical sequences of words, average accuracy on four models decreases further to 7%. We hope our insights will motivate the development of new models that understand language more precisely.', 'corpus_id': 7228830}	7310	"[{'doc_id': '226226804', 'title': 'Reasoning Over History: Context Aware Visual Dialog', 'abstract': 'While neural models have been shown to exhibit strong performance on single-turn visual question answering (VQA) tasks, extending VQA to a multi-turn, conversational setting remains a challenge. One way to address this challenge is to augment existing strong neural VQA models with the mechanisms that allow them to retain information from previous dialog turns. One strong VQA model is the MAC network, which decomposes a task into a series of attention-based reasoning steps. However, since the MAC network is designed for single-turn question answering, it is not capable of referring to past dialog turns. More specifically, it struggles with tasks that require reasoning over the dialog history, particularly coreference resolution. We extend the MAC network architecture with Context-aware Attention and Memory (CAM), which attends over control states in past dialog turns to determine the necessary reasoning operations for the current question. MAC nets with CAM achieve up to 98.25% accuracy on the CLEVR-Dialog dataset, beating the existing state-of-the-art by 30% (absolute). Our error analysis indicates that with CAM, the model’s performance particularly improved on questions that required coreference resolution.', 'corpus_id': 226226804, 'score': 0}, {'doc_id': '202750077', 'title': 'Attention Interpretability Across NLP Tasks', 'abstract': ""The attention layer in a neural network model provides insights into the model's reasoning behind its prediction, which are usually criticized for being opaque. Recently, seemingly contradictory viewpoints have emerged about the interpretability of attention weights (Jain & Wallace, 2019; Vig & Belinkov, 2019). Amid such confusion arises the need to understand attention mechanism more systematically. In this work, we attempt to fill this gap by giving a comprehensive explanation which justifies both kinds of observations (i.e., when is attention interpretable and when it is not). Through a series of experiments on diverse NLP tasks, we validate our observations and reinforce our claim of interpretability of attention through manual evaluation."", 'corpus_id': 202750077, 'score': 1}, {'doc_id': '222124841', 'title': 'LUKE: Deep Contextualized Entity Representations with Entity-aware Self-attention', 'abstract': 'Entity representations are useful in natural language tasks involving entities. In this paper, we propose new pretrained contextualized representations of words and entities based on the bidirectional transformer. The proposed model treats words and entities in a given text as independent tokens, and outputs contextualized representations of them. Our model is trained using a new pretraining task based on the masked language model of BERT. The task involves predicting randomly masked words and entities in a large entity-annotated corpus retrieved from Wikipedia. We also propose an entity-aware self-attention mechanism that is an extension of the self-attention mechanism of the transformer, and considers the types of tokens (words or entities) when computing attention scores. The proposed model achieves impressive empirical performance on a wide range of entity-related tasks. In particular, it obtains state-of-the-art results on five well-known datasets: Open Entity (entity typing), TACRED (relation classification), CoNLL-2003 (named entity recognition), ReCoRD (cloze-style question answering), and SQuAD 1.1 (extractive question answering). Our source code and pretrained representations are available at this https URL.', 'corpus_id': 222124841, 'score': 0}, {'doc_id': '219955933', 'title': 'How does this interaction affect me? Interpretable attribution for feature interactions', 'abstract': 'Machine learning transparency calls for interpretable explanations of how inputs relate to predictions. Feature attribution is a way to analyze the impact of features on predictions. Feature interactions are the contextual dependence between features that jointly impact predictions. There are a number of methods that extract feature interactions in prediction models; however, the methods that assign attributions to interactions are either uninterpretable, model-specific, or non-axiomatic. We propose an interaction attribution and detection framework called Archipelago which addresses these problems and is also scalable in real-world settings. Our experiments on standard annotation labels indicate our approach provides significantly more interpretable explanations than comparable methods, which is important for analyzing the impact of interactions on predictions. We also provide accompanying visualizations of our approach that give new insights into deep neural networks.', 'corpus_id': 219955933, 'score': 1}, {'doc_id': '225062431', 'title': 'Generating Plausible Counterfactual Explanations for Deep Transformers in Financial Text Classification', 'abstract': 'Corporate mergers and acquisitions (M&A) account for billions of dollars of investment globally every year and offer an interesting and challenging domain for artificial intelligence. However, in these highly sensitive domains, it is crucial to not only have a highly robust/accurate model, but be able to generate useful explanations to garner a user’s trust in the automated system. Regrettably, the recent research regarding eXplainable AI (XAI) in financial text classification has received little to no attention, and many current methods for generating textual-based explanations result in highly implausible explanations, which damage a user’s trust in the system. To address these issues, this paper proposes a novel methodology for producing plausible counterfactual explanations, whilst exploring the regularization benefits of adversarial training on language models in the domain of FinTech. Exhaustive quantitative experiments demonstrate that not only does this approach improve the model accuracy when compared to the current state-of-the-art and human performance, but it also generates counterfactual explanations which are significantly more plausible based on human trials.', 'corpus_id': 225062431, 'score': 0}, {'doc_id': '226237434', 'title': 'MACE: Model Agnostic Concept Extractor for Explaining Image Classification Networks', 'abstract': ""Deep convolutional networks have been quite successful at various image classification tasks. The current methods to explain the predictions of a pre-trained model rely on gradient information, often resulting in saliency maps that focus on the foreground object as a whole. However, humans typically reason by dissecting an image and pointing out the presence of smaller concepts. The final output is often an aggregation of the presence or absence of these smaller concepts. In this work, we propose MACE: a Model Agnostic Concept Extractor, which can explain the working of a convolutional network through smaller concepts. The MACE framework dissects the feature maps generated by a convolution network for an image to extract concept based prototypical explanations. Further, it estimates the relevance of the extracted concepts to the pre-trained model's predictions, a critical aspect required for explaining the individual class predictions, missing in existing approaches. We validate our framework using VGG16 and ResNet50 CNN architectures, and on datasets like Animals With Attributes 2 (AWA2) and Places365. Our experiments demonstrate that the concepts extracted by the MACE framework increase the human interpretability of the explanations, and are faithful to the underlying pre-trained black-box model."", 'corpus_id': 226237434, 'score': 0}, {'doc_id': '216641945', 'title': 'Towards Transparent and Explainable Attention Models', 'abstract': 'Recent studies on interpretability of attention distributions have led to notions of faithful and plausible explanations for a model’s predictions. Attention distributions can be considered a faithful explanation if a higher attention weight implies a greater impact on the model’s prediction. They can be considered a plausible explanation if they provide a human-understandable justification for the model’s predictions. In this work, we first explain why current attention mechanisms in LSTM based encoders can neither provide a faithful nor a plausible explanation of the model’s predictions. We observe that in LSTM based encoders the hidden representations at different time-steps are very similar to each other (high conicity) and attention weights in these situations do not carry much meaning because even a random permutation of the attention weights does not affect the model’s predictions. Based on experiments on a wide variety of tasks and datasets, we observe attention distributions often attribute the model’s predictions to unimportant words such as punctuation and fail to offer a plausible explanation for the predictions. To make attention mechanisms more faithful and plausible, we propose a modified LSTM cell with a diversity-driven training objective that ensures that the hidden representations learned at different time steps are diverse. We show that the resulting attention distributions offer more transparency as they (i) provide a more precise importance ranking of the hidden states (ii) are better indicative of words important for the model’s predictions (iii) correlate better with gradient-based attribution methods. Human evaluations indicate that the attention distributions learned by our model offer a plausible explanation of the model’s predictions. Our code has been made publicly available at https://github.com/akashkm99/Interpretable-Attention', 'corpus_id': 216641945, 'score': 1}, {'doc_id': '214107001', 'title': 'Evaluations and Methods for Explanation through Robustness Analysis', 'abstract': 'Among multiple ways of interpreting a machine learning model, measuring the importance of a set of features tied to a prediction is probably one of the most intuitive ways to explain a model. In this paper, we establish the link between a set of features to a prediction with a new evaluation criterion, robustness analysis, which measures the minimum distortion distance of adversarial perturbation. By measuring the tolerance level for an adversarial attack, we can extract a set of features that provides the most robust support for a prediction, and also can extract a set of features that contrasts the current prediction to a target class by setting a targeted adversarial attack. By applying this methodology to various prediction tasks across multiple domains, we observe the derived explanations are indeed capturing the significant feature set qualitatively and quantitatively.', 'corpus_id': 214107001, 'score': 1}, {'doc_id': '202583616', 'title': 'Learning to Deceive with Attention-Based Explanations', 'abstract': 'Attention mechanisms are ubiquitous components in neural architectures applied to natural language processing. In addition to yielding gains in predictive accuracy, attention weights are often claimed to confer interpretability, purportedly useful both for providing insights to practitioners and for explaining why a model makes its decisions to stakeholders. We call the latter use of attention mechanisms into question by demonstrating a simple method for training models to produce deceptive attention masks. Our method diminishes the total weight assigned to designated impermissible tokens, even when the models can be shown to nevertheless rely on these features to drive predictions. Across multiple models and tasks, our approach manipulates attention weights while paying surprisingly little cost in accuracy. Through a human study, we show that our manipulated attention-based explanations deceive people into thinking that predictions from a model biased against gender minorities do not rely on the gender. Consequently, our results cast doubt on attention’s reliability as a tool for auditing algorithms in the context of fairness and accountability.', 'corpus_id': 202583616, 'score': 1}, {'doc_id': '222133166', 'title': 'Learning from Context or Names? An Empirical Study on Neural Relation Extraction', 'abstract': 'Neural models have achieved remarkable success on relation extraction (RE) benchmarks. However, there is no clear understanding which type of information affects existing RE models to make decisions and how to further improve the performance of these models. To this end, we empirically study the effect of two main information sources in text: textual context and entity mentions (names). We find that (i) while context is the main source to support the predictions, RE models also heavily rely on the information from entity mentions, most of which is type information, and (ii) existing datasets may leak shallow heuristics via entity mentions and thus contribute to the high performance on RE benchmarks. Based on the analyses, we propose an entity-masked contrastive pre-training framework for RE to gain a deeper understanding on both textual context and type information while avoiding rote memorization of entities or use of superficial cues in mentions. We carry out extensive experiments to support our views, and show that our framework can improve the effectiveness and robustness of neural models in different RE scenarios. All the code and datasets are released at this https URL.', 'corpus_id': 222133166, 'score': 0}]"
151	"{'doc_id': '145217640', 'title': 'The Wages of Whiteness: Race and the Making of the American Working Class', 'abstract': ""This is the new, fully updated edition of this now-classic study of working-class racism. Combining classical Marxism, psychoanalysis and the new labor history pioneered by E. P. Thompson and Herbert Gutman, David Roediger's widely acclaimed book provides an original study of the formative years of working-class racism in the United States. This, he argues, cannot be explained simply with reference to economic advantage; rather, white working-class racism is underpinned by a complex series of psychological and ideological mechanisms that reinforce racial stereotypes, and thus help to forge the identities of white workers in opposition to blacks. In a lengthy new introduction, Roediger surveys recent scholarship on whiteness, and discusses the changing face of labor in the twenty-first century."", 'corpus_id': 145217640}"	13605	"[{'doc_id': '230577791', 'title': 'These Stories Must Be Told: Preliminary Observations by a Black Scholar Practitioner on Silences in the Archive', 'abstract': 'As a scholar practitioner, a trained philosophical theologian, Methodist clergywoman, and social enterprise founder who is conducting oral histories as part of my doctoral internship in the IUPUI Arts and Humanities Institute, my scholarly lens and methodological skills are being defined as I interrogate the COVID-19 archive. In this article I attempt to offer some preliminary reflections on my oral history curation focused on how Black and brown artists and activists, primarily based in Indianapolis, IN, frame their lived experiences of death, dying, mourning, and bereavement in the wake of COVID-19 utilizing critical archival practices: those practices that take seriously the methods of critical race theory, critical gender theory, Womanist, mujerista, and feminist methodologies, to name a few. The COVID-19 archive is a collection of oral histories, stories and artifacts depicting the times in which we are living, through the lenses of storytellers grappling with the pandemics of systemic racism, COVID-19, distrust in government, and various relics representing the idea of the United States of America in 2020, as such, I conclude with a brief exploration of how art emerges as both an outlet for creators and a mode of illumination for consumers.', 'corpus_id': 230577791, 'score': 0}, {'doc_id': '227948592', 'title': 'Race, Power, and COVID-19: A Call for Advocacy within Bioethics', 'abstract': 'Abstract Events in 2020 have sparked a reimagination of how both individuals and institutions should consider race, power, health, and marginalization in society. In a response to these developments, we examine the current and past limitations of the ways in which bioethicists have considered race and, more generally, discourses of marginalization. We argue that the foundational principle of justice necessitates that bioethics, as an institution, maintain an active voice against systemic injustice. To carry out this charge, bioethics as a field should promote alternative narratives—“counter storytelling”—to the mainstream voices that have traditionally been heard and accepted, largely without opposition. Additionally, we engage with both Post-Colonial and Critical Race Theory, which we believe are important tools for bioethics in pursuit of equity. Ultimately, we advocate for a proactive form of bioethics that actively resists and denounces injustice and which considers a much wider variety of voices about justice than bioethics has historically considered.', 'corpus_id': 227948592, 'score': 0}, {'doc_id': '229290702', 'title': 'A Discussion of Russell Muirhead and Nancy L. Rosenblum’s A Lot of People Are Saying: The New Conspiracism and the Assault on Democracy', 'abstract': 'Charge: More than a half-century ago, Richard Hofstadter identified the “paranoid style” as an important feature of American politics. However, in A Lot of People are Saying, Russell Muirhead and Nancy L. Rosenblum argue that a “new” form of conspiracism has begun to infect contemporary American political life. Whereas “old” conspiracy theorists sought hidden evidence to describe why things are not as they seem, Muirhead and Rosenblum argue that purveyors of the new conspiracism make no attempt to substantiate their theories. In light of this fact-free approach, the authors thus warn that contemporary conspiracy theorists pose an unprecedented danger to foundational elements of American democracy, including political parties and knowledge-producing institutions. Moreover, Muirhead and Rosenblum assert, “The new conspiracism moved into the White House with the inauguration of Donald Trump” (p. 1), “the conspiracist in chief” (p. ix). If there is merit to this argument, then the fate of Trump’s reelection bid carries monumental consequences for the future of American democracy, as well as the way in which the United States responds to the unprecedented coronavirus pandemic. We therefore asked a range of scholars to comment on Muirhead’s and Rosenblum’s bold set of claims.', 'corpus_id': 229290702, 'score': 0}, {'doc_id': '142949286', 'title': 'Imperial White: Race, Diaspora, and the British Empire', 'abstract': 'Radhika Mohanram shows not just how British imperial culture shaped the colonies but how the imperial rule of colonies shifted—and gave new meanings to—what it meant to be British. \n \nImperial White looks at literary, social, and cultural texts on the racialization of the British body and investigates British whiteness in the colonies to address such questions as: How was the whiteness in Britishness constructed by the presence of Empire? How was whiteness incorporated into the idea of masculinity? Does heterosexuality have a color? And does domestic race differ from colonial race? In addition to these inquiries on the issues of race, class, and sexuality, Mohanram effectively applies the methods of whiteness studies to British imperial material culture to critically racialize the relationship between the metropole and the peripheral colonies. \n \nConsidering whether whiteness, like theory, can travel, Mohanram also provides a new perspective on white diaspora, a phenomenon of the nineteenth century that has been largely absent in diaspora studies, ultimately rereading—and rethinking—British imperial whiteness.', 'corpus_id': 142949286, 'score': 1}, {'doc_id': '190960479', 'title': 'Sight Unseen: Whiteness and American Visual Culture', 'abstract': 'List of Illustrations Acknowledgments Introduction: White Like Me 1. Genre Painting and the Foundations of Modern Race 2. Landscape Photography and the White Gaze 3. Museum Architecture and the Imperialism of Whiteness 4. Silent Cinema and the Gradations of Whiteness Epilogue: The Triumph of Racialized Thought Notes Bibliography Index', 'corpus_id': 190960479, 'score': 1}, {'doc_id': '231830644', 'title': 'Racial Security: The Unobserved Threat in IR', 'abstract': 'Since the development of the modern state in the seventeenth century, race has directly influenced human understanding of peace and order. Racist perceptions of anarchy associated non-Western and non-white communities with savagery. In turn, a racist cycle of global knowledge upon which the international community is based has developed. Additionally, imperialistic reign over the following centuries dictated the inability for society to challenge racist ideals and norms assumed by the powers that be. Unfortunately, the concepts of race and security have run counter-intuitively in respect to human development. Using the supporting research to construct a preliminary definition of Racial Security and its implications, this essay aims to show how race has compromised the theoretical understanding of international relations in its applicability to the fields of security and strategy.', 'corpus_id': 231830644, 'score': 0}, {'doc_id': '149516280', 'title': 'Playing in the Dark : Whiteness and the Literary Imagination', 'abstract': 'Pulitzer Prize-winning novelist Toni Morrison provides a personal inquiry into the significance of African-American literary imagination. Her goal, she states at the outset, is to ""put forth an argument for extending the study of American literature"". Author of ""Beloved"", ""The Bluest Eye"", ""Song of Solomon"", and other vivid portrayals of black American experience, Morrison ponders the effect that living in a historically racialized society has had on American writing in the 19th and 20th centuries. She argues that race has become a metaphor, a way of referring to forces, events, and forms of social decay, economic division, and human panic. Her argument is that the central characteristics of American literature - individualism, masculinity, the insistence upon innocence coupled to an obsession with figurations of death and hell - are responses to a dark and abiding Africanist presence.', 'corpus_id': 149516280, 'score': 1}, {'doc_id': '191337285', 'title': ""Picturing Whiteness: Nikki S. Lee's Yuppie Project"", 'abstract': ""Nikki S. Lee, a Korean-born artist who lives in New York, has made it her project to infiltrate and mimic the subcultures of American life. For weeks or even months she immerses herself in a community or cultural milieu—lesbians, drag queens, Ohio trailer-park dwellers, skateboarders, senior citizens, Hispanic or Japanese street kids—meticulously adopting its codes of dress and behavior and its living habits. Throughout a self-defined residency in which she lives and interacts with these people, Lee has herself photographed—by a friend who accompanies her, by a member of her adopted social group, or even by a passing stranger to whom she hands her point-and-shoot camera. The photographs documenting Lee's effort to blend into these communities at first appear to be crude snapshots, replete with date stamp and flash-triggered red eye. Closer scrutiny reveals their visual and intellectual sophistication, their raw, uncanny ability to represent the complexity and fluidity of human identity."", 'corpus_id': 191337285, 'score': 1}, {'doc_id': '144847733', 'title': 'Forum: Blinded by the White: Art and History at the Limits of Whiteness', 'abstract': 'Unquestioned, whiteness provides the models by which the Western subject judges culture. As the norm, whiteness passes unremarked, perpetuating the canonical conventions and traditions that sustain its privilege;1 whiteness is assumed, while only otherness is pronounced.2 Any study of whiteness, therefore, has to consider the role it plays as the universalist measure against which all other identities become particular—made too particular to be applied universally. Although woven into the very fabric of Western culture, it is a discourse that makes bodies matter and that gives matter meaning.3 The situation of whiteness is that of a body historicized and racialized to the point where its material particularity is obscured. Otherness is violently suppressed in order to promote the idea of a universal figure of disembodied, metaphysical transcendence.4', 'corpus_id': 144847733, 'score': 1}, {'doc_id': '231742773', 'title': 'BLACK LIVES MATTER IN THE UNITED STATES OF AMERICA', 'abstract': ""This paper examines affiliation with the Black Lives Matter (BLM) movement using the constructivism theory. The main finding presented in the paper is that the discrimination experienced by African Americans in the United States in the past two decades. The BLM movement's history was a response to the death of two black teenagers, Trayvon Martin and Michael Brown, who were both unarmed and shot and killed. The most famous one happened this year, the death of George Floyd for the brutal police action by pressing the victim's neck with his leg until Floyd died. The second key finding is that BLM organizations generated more to frame the movement as a struggle for individual rights. Still, many youths assume that this movement is just a trend on social media. Finally, social media's influence where the spread of news, content, videos is the important point of the black lives matter movement in the US. Keyword: BlackLivesMatter, Social Media, Constructivism, Discrimination"", 'corpus_id': 231742773, 'score': 0}]"
152	{'doc_id': '235436067', 'title': 'A Spacecraft Dataset for Detection, Segmentation and Parts Recognition', 'abstract': 'Virtually all aspects of modern life depend on space technology. Thanks to the great advancement of computer vision in general and deep learning-based techniques in particular, over the decades, the world witnessed the growing use of deep learning in solving problems for space applications, such as self-driving robot, tracers, insect-like robot on cosmos and health monitoring of spacecraft. These are just some prominent examples that has advanced space industry with the help of deep learning. However, the success of deep learning models requires a lot of training data in order to have decent performance, while on the other hand, there are very limited amount of publicly available space datasets for the training of deep learning models. Currently, there is no public datasets for space-based object detection or instance segmentation, partly because manually annotating object segmentation masks is very time consuming as they require pixel-level labelling, not to mention the challenge of obtaining images from space. In this paper, we aim to fill this gap by releasing a dataset for spacecraft detection, instance segmentation and part recognition. The main contribution of this work is the development of the dataset using images of space stations and satellites, with rich annotations including bounding boxes of spacecrafts and masks to the level of object parts, which are obtained with a mixture of automatic processes and manual efforts. We also provide evaluations with state-of-the-art methods in object detection and instance segmentation as a benchmark for the dataset. The link for downloading the proposed dataset can be found on https://github.com/Yurushia1998/SatelliteDataset.', 'corpus_id': 235436067}	18638	"[{'doc_id': '235702967', 'title': 'Unlocking the Full Potential of Small Data with Diverse Supervision', 'abstract': 'Virtually all of deep learning literature relies on the assumption of large amounts of available training data. Indeed, even the majority of few-shot learning methods rely on a large set of ""base classes"" for pre-training. This assumption, however, does not always hold. For some tasks, annotating a large number of classes can be infeasible, and even collecting the images themselves can be a challenge in some scenarios. In this paper, we study this problem and call it ""Small Data"" setting, in contrast to ""Big Data."" To unlock the full potential of small data, we propose to augment the models with annotations for other related tasks, thus increasing their generalization abilities. In particular, we use the richly annotated scene parsing dataset ADE20K to construct our realistic Long-tail Recognition with Diverse Supervision (LRDS) benchmark, by splitting the object categories into head and tail based on their distribution. Following the standard few-shot learning protocol, we use the head classes for representation learning and the tail classes for evaluation. Moreover, we further subsample the head categories and images to generate two novel settings which we call ""Scarce-Class"" and ""Scarce-Image,"" respectively corresponding to the shortage of training classes and images. Finally, we analyze the effect of applying various additional supervision sources under the proposed settings. Our experiments demonstrate that densely labeling a small set of images can indeed largely remedy the small data constraints. Our code and benchmark are available at https://github.com/BinahHu/ADE-FewShot.', 'corpus_id': 235702967, 'score': 1}, {'doc_id': '235292986', 'title': 'Reduction of annotation efforts for multiclass object detection by using a domain awareness data combination strategy', 'abstract': 'To train convolutional neural networks (CNN) it is common practise to collect a huge amount of data. This is cost intensive and often not applicable. Up to date several studies have investigated the concept of few shoot learning, e.g. 1-3 samples per class. Suboptimal is still the over fitting resulting from the gap between training data and representative test data in the application. Since this is still a field of intensive research, an alternative and common approach is transfer learning with data- and image augmented pictures. However, collecting and labelling data for fine-tuning can still take an enormous amount of time, when it comes to multiclass pictures in industrial applications like assembly kit verification. The kits often contain stock lists with a small interclass and a high intraclass-distance. A specific characteristic of stock lists is that parts are easily adaptable and exchangeable. To bring object detection closer to the industry, we successfully show a dataset driven approach that combines a single class collection of pictures, which we call single class (SC) dataset and adapt with a few samples the specific multiclass use case. In result, we use a model trained on a huge SC dataset that can easily and fast be adapted to specific industrial use cases.', 'corpus_id': 235292986, 'score': 1}, {'doc_id': '235446969', 'title': 'The Oxford Road Boundaries Dataset', 'abstract': 'In this paper we present The Oxford Road Boundaries Dataset, designed for training and testing machine-learningbased road-boundary detection and inference approaches. We have hand-annotated two of the 10 km-long forays from the Oxford Robotcar Dataset and generated from other forays several thousand further examples with semi-annotated road-boundary masks. To boost the number of training samples in this way, we used a vision-based localiser to project labels from the annotated datasets to other traversals at different times and weather conditions. As a result, we release 62 605 labelled samples, of which 47 639 samples are curated. Each of these samples contain both raw and classified masks for left and right lenses. Our data contains images from a diverse set of scenarios such as straight roads, parked cars, junctions, etc. Files for download and tools for manipulating the labelled data are available at: oxford-robotics-institute.github. io/road-boundaries-dataset Keywords— road boundary, curb, kerb, dataset', 'corpus_id': 235446969, 'score': 1}, {'doc_id': '235693250', 'title': 'AI4MARS: A Dataset for Terrain-Aware Autonomous Driving on Mars', 'abstract': 'Deep learning has quickly become a necessity for self-driving vehicles on Earth. In contrast, the self-driving vehicles on Mars, including NASA’s latest rover, Perseverance, which is planned to land on Mars in February 2021, are still driven by classical machine vision systems. Deep learning capabilities, such as semantic segmentation and object recognition, would substantially benefit the safety and productivity of ongoing and future missions to the red planet. To this end, we created the first large-scale dataset, AI4Mars, for training and validating terrain classification models for Mars, consisting of ~326K semantic segmentation full image labels on 35K images from Curiosity, Opportunity, and Spirit rovers, collected through crowdsourcing. Each image was labeled by ~10 people to ensure greater quality and agreement of the crowdsourced labels. It also includes ~1.5K validation labels annotated by the rover planners and scientists from NASA’s MSL (Mars Science Laboratory) mission, which operates the Curiosity rover, and MER (Mars Exploration Rovers) mission, which operated the Spirit and Opportunity rovers. We trained a DeepLabv3 model on the AI4Mars training dataset and achieved over 96% overall classification accuracy on the test set. The dataset is made publicly available.1 2', 'corpus_id': 235693250, 'score': 1}, {'doc_id': '235490565', 'title': 'SODA10M: Towards Large-Scale Object Detection Benchmark for Autonomous Driving', 'abstract': 'Aiming at facilitating a real-world, ever-evolving and scalable autonomous driving system, we present a large-scale benchmark for standardizing the evaluation of different self-supervised and semi-supervised approaches by learning from raw data, which is the first and largest benchmark to date. Existing autonomous driving systems heavily rely on ‘perfect’ visual perception models (e.g., detection) trained using extensive annotated data to ensure the safety. However, it is unrealistic to elaborately label instances of all scenarios and circumstances (e.g., night, extreme weather, cities) when deploying a robust autonomous driving system. Motivated by recent powerful advances of self-supervised and semi-supervised learning, a promising direction is to learn a robust detection model by collaboratively exploiting large-scale unlabeled data and few labeled data. Existing dataset (e.g., KITTI, Waymo) either provides only a small amount of data or covers limited domains with full annotation, hindering the exploration of large-scale pre-trained models. Here, we release a Large-Scale Object Detection benchmark for Autonomous driving, named as SODA10M, containing 10 million unlabeled images and 20K images labeled with 6 representative object categories. To improve diversity, the images are collected every ten seconds per frame within 32 different cities under different weather conditions, periods and location scenes. We provide extensive experiments and deep analyses of existing supervised state-of-the-art detection models, popular self-supervised and semi-supervised approaches, and some insights about how to develop future models. We show that SODA10M can serve as a promising pretraining dataset for different self-supervised learning methods, which gives superior performance when finetuning autonomous driving downstream tasks. This benchmark will be used to hold the ICCV2021 SSLAD challenge. The data and more up-to-date information have been released at https://soda-2d.github.io.', 'corpus_id': 235490565, 'score': 1}, {'doc_id': '235485156', 'title': 'How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers', 'abstract': 'Vision Transformers (ViT) have been shown to attain highly competitive performance for a wide range of vision applications, such as image classification, object detection and semantic image segmentation. In comparison to convolutional neural networks, the Vision Transformer’s weaker inductive bias is generally found to cause an increased reliance on model regularization or data augmentation (“AugReg” for short) when training on smaller training datasets. We conduct a systematic empirical study in order to better understand the interplay between the amount of training data, AugReg, model size and compute budget. 1 As one result of this study we find that the combination of increased compute and AugReg can yield models with the same performance as models trained on an order of magnitude more training data: we train ViT models of various sizes on the public ImageNet-21k dataset which either match or outperform their counterparts trained on the larger, but not publicly available JFT-300M dataset.', 'corpus_id': 235485156, 'score': 0}, {'doc_id': '235657622', 'title': 'Transformation Invariant Few-Shot Object Detection', 'abstract': 'Few-shot object detection (FSOD) aims to learn detectors that can be generalized to novel classes with only a few instances. Unlike previous attempts that exploit metalearning techniques to facilitate FSOD, this work tackles the problem from the perspective of sample expansion. To this end, we propose a simple yet effective Transformation Invariant Principle (TIP) that can be flexibly applied to various meta-learning models for boosting the detection performance on novel class objects. Specifically, by introducing consistency regularization on predictions from various transformed images, we augment vanilla FSOD models with the generalization ability to objects perturbed by various transformation, such as occlusion and noise. Importantly, our approach can extend supervised FSOD models to naturally cope with unlabeled data, thus addressing a more practical and challenging semi-supervised FSOD problem. Extensive experiments on PASCAL VOC and MSCOCO datasets demonstrate the effectiveness of our TIP under both of the two FSOD settings.', 'corpus_id': 235657622, 'score': 0}, {'doc_id': '235790490', 'title': 'Learning Cascaded Detection Tasks with Weakly-Supervised Domain Adaptation', 'abstract': 'In order to handle the challenges of autonomous driving, deep learning has proven to be crucial in tackling increasingly complex tasks, such as 3D detection or instance segmentation. State-of-the-art approaches for image-based detection tasks tackle this complexity by operating in a cascaded fashion: they first extract a 2D bounding box based on which additional attributes, e.g. instance masks, are inferred. While these methods perform well, a key challenge remains the lack of accurate and cheap annotations for the growing variety of tasks. Synthetic data presents a promising solution but, despite the effort in domain adaptation research, the gap between synthetic and real data remains an open problem. In this work, we propose a weakly supervised domain adaptation setting which exploits the structure of cascaded detection tasks. In particular, we learn to infer the attributes solely from the source domain while leveraging 2D bounding boxes as weak labels in both domains to explain the domain shift. We further encourage domain-invariant features through class-wise feature alignment using ground-truth class information, which is not available in the unsupervised setting. As our experiments demonstrate, the approach is competitive with fully supervised settings while outperforming unsupervised adaptation approaches by a large margin.', 'corpus_id': 235790490, 'score': 0}, {'doc_id': '235447001', 'title': '2nd Place Solution for Waymo Open Dataset Challenge - Real-time 2D Object Detection', 'abstract': 'In an autonomous driving system, it is essential to recognize vehicles, pedestrians and cyclists from images. Besides the high accuracy of the prediction, the requirement of real-time running brings new challenges for convolutional network models. In this report, we introduce a real-time method to detect the 2D objects from images. We aggregate several popular one-stage object detectors and train the models of variety input strategies independently, to yield better performance for accurate multi-scale detection of each category, especially for small objects. For model acceleration, we leverage TensorRT to optimize the inference time of our detection pipeline. As shown in the leaderboard, our proposed detection framework ranks the 2nd place with 75.00% L1 mAP and 69.72% L2 mAP in the real-time 2D detection track of the Waymo Open Dataset Challenges, while our framework achieves the latency of 45.8ms/frame on an Nvidia Tesla V100 GPU.', 'corpus_id': 235447001, 'score': 0}]"
153	{'doc_id': '227231779', 'title': 'A Semi-Supervised BERT Approach for Arabic Named Entity Recognition', 'abstract': 'Named entity recognition (NER) plays a significant role in many applications such as information extraction, information retrieval, question answering, and even machine translation. Most of the work on NER using deep learning was done for non-Arabic languages like English and French, and only few studies focused on Arabic. This paper proposes a semi-supervised learning approach to train a BERT-based NER model using labeled and semi-labeled datasets. We compared our approach against various baselines, and state-of-the-art Arabic NER tools on three datasets: AQMAR, NEWS, and TWEETS. We report a significant improvement in F-measure for the AQMAR and the NEWS datasets, which are written in Modern Standard Arabic (MSA), and competitive results for the TWEETS dataset, which contains tweets that are mostly in the Egyptian dialect and contain many mistakes or misspellings.', 'corpus_id': 227231779}	12457	"[{'doc_id': '226262399', 'title': 'Entity Enhanced BERT Pre-training for Chinese NER', 'abstract': 'Character-level BERT pre-trained in Chinese suffers a limitation of lacking lexicon information, which shows effectiveness for Chinese NER. To integrate the lexicon into pre-trained LMs for Chinese NER, we investigate a semi-supervised entity enhanced BERT pre-training method. In particular, we first extract an entity lexicon from the relevant raw text using a new-word discovery method. We then integrate the entity information into BERT using Char-Entity-Transformer, which augments the self-attention using a combination of character and entity representations. In addition, an entity classification task helps inject the entity information into model parameters in pre-training. The pre-trained models are used for NER fine-tuning. Experiments on a news dataset and two datasets annotated by ourselves for NER in long-text show that our method is highly effective and achieves the best results.', 'corpus_id': 226262399, 'score': 1}, {'doc_id': '226965681', 'title': 'SAG-GAN: Semi-Supervised Attention-Guided GANs for Data Augmentation on Medical Images', 'abstract': 'Recently deep learning methods, in particular, convolutional neural networks (CNNs), have led to a massive breakthrough in the range of computer vision. Also, the large-scale annotated dataset is the essential key to a successful training procedure. However, it is a huge challenge to get such datasets in the medical domain. Towards this, we present a data augmentation method for generating synthetic medical images using cycle-consistency Generative Adversarial Networks (GANs). We add semi-supervised attention modules to generate images with convincing details. We treat tumor images and normal images as two domains. The proposed GANs-based model can generate a tumor image from a normal image, and in turn, it can also generate a normal image from a tumor image. Furthermore, we show that generated medical images can be used for improving the performance of ResNet18 for medical image classification. Our model is applied to three limited datasets of tumor MRI images. We first generate MRI images on limited datasets, then we trained three popular classification models to get the best model for tumor classification. Finally, we train the classification model using real images with classic data augmentation methods and classification models using synthetic images. The classification results between those trained models showed that the proposed SAG-GAN data augmentation method can boost Accuracy and AUC compare with classic data augmentation methods. We believe the proposed data augmentation method can apply to other medical image domains, and improve the accuracy of computer-assisted diagnosis.', 'corpus_id': 226965681, 'score': 0}, {'doc_id': '225103327', 'title': 'Exploring Generative Adversarial Networks for Image-to-Image Translation in STEM Simulation', 'abstract': 'The use of accurate scanning transmission electron microscopy (STEM) image simulation methods require large computation times that can make their use infeasible for the simulation of many images. Other simulation methods based on linear imaging models, such as the convolution method, are much faster but are too inaccurate to be used in application. In this paper, we explore deep learning models that attempt to translate a STEM image produced by the convolution method to a prediction of the high accuracy multislice image. We then compare our results to those of regression methods. We find that using the deep learning model Generative Adversarial Network (GAN) provides us with the best results and performs at a similar accuracy level to previous regression models on the same dataset. Codes and data for this project can be found in this GitHub repository, this https URL.', 'corpus_id': 225103327, 'score': 0}, {'doc_id': '223957158', 'title': 'Coarse-to-Fine Pre-training for Named Entity Recognition', 'abstract': 'More recently, Named Entity Recognition hasachieved great advances aided by pre-trainingapproaches such as BERT. However, currentpre-training techniques focus on building lan-guage modeling objectives to learn a gen-eral representation, ignoring the named entity-related knowledge. To this end, we proposea NER-specific pre-training framework to in-ject coarse-to-fine automatically mined entityknowledge into pre-trained models. Specifi-cally, we first warm-up the model via an en-tity span identification task by training it withWikipedia anchors, which can be deemed asgeneral-typed entities. Then we leverage thegazetteer-based distant supervision strategy totrain the model extract coarse-grained typedentities. Finally, we devise a self-supervisedauxiliary task to mine the fine-grained namedentity knowledge via clustering.Empiricalstudies on three public NER datasets demon-strate that our framework achieves significantimprovements against several pre-trained base-lines, establishing the new state-of-the-art per-formance on three benchmarks. Besides, weshow that our framework gains promising re-sults without using human-labeled trainingdata, demonstrating its effectiveness in label-few and low-resource scenarios', 'corpus_id': 223957158, 'score': 1}, {'doc_id': '229923960', 'title': 'Generative Adversarial Network for Image Synthesis', 'abstract': 'This chapter reviews recent developments of generative adversarial networks (GAN)-based methods for medical and biomedical image synthesis tasks. These methods are classified into conditional GAN and Cycle-GAN according to the network architecture designs. For each category, a literature survey is given, which covers discussions of the network architecture designs, highlights important contributions and identifies specific challenges. keywords: Image synthesis, deep learning, Generative Adversarial Network, GAN.', 'corpus_id': 229923960, 'score': 0}, {'doc_id': '225076095', 'title': 'To BERT or Not to BERT: Comparing Task-specific and Task-agnostic Semi-Supervised Approaches for Sequence Tagging', 'abstract': 'Leveraging large amounts of unlabeled data using Transformer-like architectures, like BERT, has gained popularity in recent times owing to their effectiveness in learning general representations that can then be further fine-tuned for downstream tasks to much success. However, training these models can be costly both from an economic and environmental standpoint. In this work, we investigate how to effectively use unlabeled data: by exploring the task-specific semi-supervised approach, Cross-View Training (CVT) and comparing it with task-agnostic BERT in multiple settings that include domain and task relevant English data. CVT uses a much lighter model architecture and we show that it achieves similar performance to BERT on a set of sequence tagging tasks, with lesser financial and environmental impact.', 'corpus_id': 225076095, 'score': 1}, {'doc_id': '229923303', 'title': 'Few-Shot Named Entity Recognition: A Comprehensive Study', 'abstract': 'This paper presents a comprehensive study to efficiently build named entity recognition (NER) systems when a small number of indomain labeled data is available. Based upon recent Transformer-based self-supervised pretrained language models (PLMs), we investigate three orthogonal schemes to improve the model generalization ability for few-shot settings: (1) meta-learning to construct prototypes for different entity types, (2) supervised pre-training on noisy web data to extract entity-related generic representations and (3) self-training to leverage unlabeled in-domain data. Different combinations of these schemes are also considered. We perform extensive empirical comparisons on 10 public NER datasets with various proportions of labeled data, suggesting useful insights for future research. Our experiments show that (i) in the few-shot learning setting, the proposed NER schemes significantly improve or outperform the commonly used baseline, a PLM-based linear classifier fine-tuned on domain labels. (ii) We create new state-of-the-art results on both few-shot and training-free settings compared with existing methods. We will release our code and pretrained models for reproducible research.', 'corpus_id': 229923303, 'score': 1}, {'doc_id': '225066912', 'title': 'Geometrically Matched Multi-source Microscopic Image Synthesis Using Bidirectional Adversarial Networks', 'abstract': 'Microscopic images from different modality can provide more complete experimental information. In practice, biological and physical limitations may prohibit the acquisition of enough microscopic images at a given observation period. Image synthesis is one promising solution. However, most existing data synthesis methods only translate the image from a source domain to a target domain without strong geometric correlations. To address this issue, we propose a novel model to synthesize diversified microscopic images from multi-sources with different geometric features. The application of our model to a 3D live time-lapse embryonic images of C. elegans presents favorable results. To the best of our knowledge, it is the first effort to synthesize microscopic images with strong underlie geometric correlations from multi-source domains that of entirely separated spatial features.', 'corpus_id': 225066912, 'score': 0}, {'doc_id': '226964487', 'title': 'Mode Penalty Generative Adversarial Network with adapted Auto-encoder', 'abstract': ""Generative Adversarial Networks (GAN) are trained to generate sample images of interest distribution. To this end, generator network of GAN learns implicit distribution of real data set from the classification with candidate generated samples. Recently, various GANs have suggested novel ideas for stable optimizing of its networks. However, in real implementation, sometimes they still represent a only narrow part of true distribution or fail to converge. We assume this ill posed problem comes from poor gradient from objective function of discriminator, which easily trap the generator in a bad situation. To address this problem, we propose a mode penalty GAN combined with pre-trained auto encoder for explicit representation of generated and real data samples in the encoded space. In this space, we make a generator manifold to follow a real manifold by finding entire modes of target distribution. In addition, penalty for uncovered modes of target distribution is given to the generator which encourages it to find overall target distribution. We demonstrate that applying the proposed method to GANs helps generator's optimization becoming more stable and having faster convergence through experimental evaluations."", 'corpus_id': 226964487, 'score': 0}, {'doc_id': '227151785', 'title': 'Domain-Transferable Method for Named Entity Recognition Task', 'abstract': 'Named Entity Recognition (NER) is a fundamental task in the fields of natural language processing and information extraction. NER has been widely used as a standalone tool or an essential component in a variety of applications such as question answering, dialogue assistants and knowledge graphs development. However, training reliable NER models requires a large amount of labelled data which is expensive to obtain, particularly in specialized domains. This paper describes a method to learn a domain-specific NER model for an arbitrary set of named entities when domain-specific supervision is not available. We assume that the supervision can be obtained with no human effort, and neural models can learn from each other. The code, data and models are publicly available.', 'corpus_id': 227151785, 'score': 1}]"
154	{'doc_id': '232320302', 'title': 'Generative Minimization Networks: Training GANs Without Competition', 'abstract': 'Many applications in machine learning can be framed as minimization problems and solved efficiently using gradient-based techniques. However, recent applications of generative models, particularly GANs, have triggered interest for solving min-max games for which standard optimization techniques are often not suitable. Among known problems experienced by practitioners are the lack of convergence guarantees or convergence to a non-optimum cycle. At the heart of these problems is the min-max structure of the GAN objective which creates non-trivial dependencies between the players. We propose to address this problem by optimizing a different objective that circumvents the min-max structure using the notion of duality gap from game theory. We provide novel convergence guarantees on this objective and demonstrate why the obtained limit point solves the problem better than known techniques.', 'corpus_id': 232320302}	6078	[{'doc_id': '229368261', 'title': 'Symbolic Music Generation with Transformer-GANs', 'abstract': 'Autoregressive models using Transformers have emerged as the dominant approach for music generation with the goal of synthesizing minute-long compositions that exhibit largescale musical structure. These models are commonly trained by minimizing the negative log-likelihood (NLL) of the observed sequence in an autoregressive manner. Unfortunately, the quality of samples from these models tends to degrade significantly for long sequences, a phenomenon attributed to exposure bias. Fortunately, we are able to detect these failures with classifiers trained to distinguish between real and sampled sequences, an observation that motivates our exploration of adversarial losses to complement the NLL objective. We use a pre-trained Span-BERT model for the discriminator of the GAN, which in our experiments helped with training stability. We use the Gumbel-Softmax trick to obtain a differentiable approximation of the sampling process. This makes discrete sequences amenable to optimization in GANs. In addition, we broke the sequences into smaller chunks to ensure that we stay within a given memory budget. We demonstrate via human evaluations and a new discriminative metric that the music generated by our approach outperforms a baseline trained with likelihood maximization, the state-of-the-art Music Transformer, and other GANs used for sequence generation. 57% of people prefer music generated via our approach while 43% prefer Music Transformer.', 'corpus_id': 229368261, 'score': 1}, {'doc_id': '220265908', 'title': 'Maximum Entropy Models for Fast Adaptation', 'abstract': 'Deep Neural Networks have shown great promise on a variety of downstream tasks; but their ability to adapt to new data and tasks remains a challenging problem. The ability of a model to perform few-shot adaptation to a novel task is important for the scalability and deployment of machine learning models. Recent work has shown that the learned features in a neural network follow a normal distribution [41], which thereby results in a strong prior on the downstream task. This implicit overfitting to data from training tasks limits the ability to generalize and adapt to unseen tasks at test time. This also highlights the importance of learning task-agnostic representations from data. In this paper, we propose a regularization scheme using a max-entropy prior on the learned features of a neural network; such that the extracted features make minimal assumptions about the training data. We evaluate our method on adaptation to unseen tasks by performing experiments in 4 distinct settings. We find that our method compares favourably against multiple strong baselines across all of these experiments.', 'corpus_id': 220265908, 'score': 1}, {'doc_id': '219303563', 'title': 'Weight Pruning via Adaptive Sparsity Loss', 'abstract': 'Pruning neural networks has regained interest in recent years as a means to compress state-of-the-art deep neural networks and enable their deployment on resource-constrained devices. In this paper, we propose a robust compressive learning framework that efficiently prunes network parameters during training with minimal computational overhead. We incorporate fast mechanisms to prune individual layers and build upon these to automatically prune the entire network under a user-defined budget constraint. Key to our end-to-end network pruning approach is the formulation of an intuitive and easy-to-implement adaptive sparsity loss that is used to explicitly control sparsity during training, enabling efficient budget-aware optimization. Extensive experiments demonstrate the effectiveness of the proposed framework for image classification on the CIFAR and ImageNet datasets using different architectures, including AlexNet, ResNets and Wide ResNets.', 'corpus_id': 219303563, 'score': 1}, {'doc_id': '233346698', 'title': 'Accelerating SpMM Kernel with Cache-First Edge Sampling for GNN Inference', 'abstract': 'Graph neural networks (GNNs), an emerging deep learning model class, can extract meaningful representations from highly expressive graph-structured data and are therefore gaining popularity for wider ranges of applications. However, current GNNs suffer from the poor performance of their sparsedense matrix multiplication (SpMM) operator, even when using powerful GPUs. Our analysis shows that 95% of the inference time could be spent on SpMM when running popular GNN models on NVIDIA’s advanced V100 GPU. Such SpMM performance bottleneck hinders GNNs’ applicability to large-scale problems or the development of more sophisticated GNN models. To address this inference time bottleneck, we introduce ESSpMM, a cache-first edge sampling mechanism and codesigned SpMM kernel. ES-SpMM uses edge sampling to downsize the graph to fit into GPU’s shared memory. It thus reduces the computation cost and improves SpMM’s cache locality. To evaluate ES-SpMM’s performance, we integrated it with a popular GNN framework, DGL, and tested it using representative GNN models and datasets. Our results show that ES-SpMM outperforms the highly optimized cuSPARSE SpMM kernel by up to 4.35x with no accuracy loss and by 45.3x with less than a 1% accuracy loss.', 'corpus_id': 233346698, 'score': 1}, {'doc_id': '219720969', 'title': 'MetaSDF: Meta-learning Signed Distance Functions', 'abstract': 'Neural implicit shape representations are an emerging paradigm that offers many potential benefits over conventional discrete representations, including memory efficiency at a high spatial resolution. Generalizing across shapes with such neural implicit representations amounts to learning priors over the respective function space and enables geometry reconstruction from partial or noisy observations. Existing generalization methods rely on conditioning a neural network on a low-dimensional latent code that is either regressed by an encoder or jointly optimized in the auto-decoder framework. Here, we formalize learning of a shape space as a meta-learning problem and leverage gradient-based meta-learning algorithms to solve this task. We demonstrate that this approach performs on par with auto-decoder based approaches while being an order of magnitude faster at test-time inference. We further demonstrate that the proposed gradient-based method outperforms encoder-decoder based methods that leverage pooling-based set encoders.', 'corpus_id': 219720969, 'score': 0}, {'doc_id': '228064565', 'title': 'Multi-Population Phase Oscillator Networks with Higher-Order Interactions.', 'abstract': 'The classical Kuramoto model consists of finitely many pairwise coupled oscillators on the circle. In many applications a simple pairwise coupling is not sufficient to describe real-world phenomena as higher-order (or group) interactions take place. Hence, we replace the classical coupling law with a very general coupling function involving higher-order terms. Furthermore, we allow for multiple populations of oscillators interacting with each other through a very general law. In our analysis, we focus on the characteristic system and the mean-field limit of this generalized class of Kuramoto models. While there are several works studying particular aspects of our program, we propose a general framework to work with all three aspects (higher-order, multi-population, and mean-field) simultaneously. Assuming identical oscillators in each population, we derive equations for the evolution of oscillator populations in the mean-field limit. First, we clarify existence and uniqueness of our set of characteristic equations, which are formulated in the space of probability measures together with the bounded-Lipschitz metric. Then, we investigate dynamical properties within the framework of the characteristic system. We identify invariant subspaces and stability of the state, in which all oscillators are synchronized within each population. Even though it turns out that this so called all-synchronized state is never asymptotically stable, under some conditions and with a suitable definition of stability, the all-synchronized state can be proven to be at least locally stable. In summary, our work provides a rigorous mathematical framework upon which the further study of multi-population higher-order coupled particle systems can be based.', 'corpus_id': 228064565, 'score': 1}, {'doc_id': '220380824', 'title': 'RIFLE: Backpropagation in Depth for Deep Transfer Learning through Re-Initializing the Fully-connected LayEr', 'abstract': 'Fine-tuning the deep convolution neural network(CNN) using a pre-trained model helps transfer knowledge learned from larger datasets to the target task. While the accuracy could be largely improved even when the training dataset is small, the transfer learning outcome is usually constrained by the pre-trained model with close CNN weights (Liu et al., 2019), as the backpropagation here brings smaller updates to deeper CNN layers. In this work, we propose RIFLE - a simple yet effective strategy that deepens backpropagation in transfer learning settings, through periodically Re-Initializing the Fully-connected LayEr with random scratch during the fine-tuning procedure. RIFLE brings meaningful updates to the weights of deep CNN layers and improves low-level feature learning, while the effects of randomization can be easily converged throughout the overall learning procedure. The experiments show that the use of RIFLE significantly improves deep transfer learning accuracy on a wide range of datasets, out-performing known tricks for the similar purpose, such as Dropout, DropConnect, StochasticDepth, Disturb Label and Cyclic Learning Rate, under the same settings with 0.5% -2% higher testing accuracy. Empirical cases and ablation studies further indicate RIFLE brings meaningful updates to deep CNN layers with accuracy improved.', 'corpus_id': 220380824, 'score': 0}, {'doc_id': '232427850', 'title': 'Parameterized Hypercomplex Graph Neural Networks for Graph Classification', 'abstract': 'Despite recent advances in representation learning in hypercomplex (HC) space, this subject is still vastly unexplored in the context of graphs. Motivated by the complex and quaternion algebras, which have been found in several contexts to enable effective representation learning that inherently incorporates a weight-sharing mechanism, we develop graph neural networks that leverage the properties of hypercomplex feature transformation. In particular, in our proposed class of models, the multiplication rule specifying the algebra itself is inferred from the data during training. Given a fixed model architecture, we present empirical evidence that our proposed model incorporates a regularization effect, alleviating the risk of overfitting. We also show that for fixed model capacity, our proposed method outperforms its corresponding real-formulated GNN, providing additional confirmation for the enhanced expressivity of HC embeddings. Finally, we test our proposed hypercomplex GNN on several open graph benchmark datasets and show that our models reach state-of-the-art performance while consuming a much lower memory footprint with 70% fewer parameters. Our implementations are available at https://github.com/bayer-science-for-a-better-life/phc-gnn.', 'corpus_id': 232427850, 'score': 1}, {'doc_id': '219963786', 'title': 'NeuralScale: Efficient Scaling of Neurons for Resource-Constrained Deep Neural Networks', 'abstract': 'Deciding the amount of neurons during the design of a deep neural network to maximize performance is not intuitive. In this work, we attempt to search for the neuron (filter) configuration of a fixed network architecture that maximizes accuracy. Using iterative pruning methods as a proxy, we parametrize the change of the neuron (filter) number of each layer with respect to the change in parameters, allowing us to efficiently scale an architecture across arbitrary sizes. We also introduce architecture descent which iteratively refines the parametrized function used for model scaling. The combination of both proposed methods is coined as NeuralScale. To prove the efficiency of NeuralScale in terms of parameters, we show empirical simulations on VGG11, MobileNetV2 and ResNet18 using CIFAR10, CIFAR100 and TinyImageNet as benchmark datasets. Our results show an increase in accuracy of 3.04%, 8.56% and 3.41% for VGG11, MobileNetV2 and ResNet18 on CIFAR10, CIFAR100 and TinyImageNet respectively under a parameter-constrained setting (output neurons (filters) of default configuration with scaling factor of 0.25).', 'corpus_id': 219963786, 'score': 0}, {'doc_id': '233444179', 'title': 'Biased Edge Dropout for Enhancing Fairness in Graph Representation Learning', 'abstract': 'Graph representation learning has become a ubiquitous component in many scenarios, ranging from social network analysis to energy forecasting in smart grids. In several applications, ensuring the fairness of the node (or graph) representations with respect to some protected attributes is crucial for their correct deployment. Yet, fairness in graph deep learning remains under-explored, with few solutions available. In particular, the tendency of similar nodes to cluster on several real-world graphs (i.e., homophily) can dramatically worsen the fairness of these procedures. In this paper, we propose a biased edge dropout algorithm (FairDrop) to counter-act homophily and improve fairness in graph representation learning. FairDrop can be plugged in easily on many existing algorithms, is efficient, adaptable, and can be combined with other fairness-inducing solutions. After describing the general algorithm, we demonstrate its application on two benchmark tasks, specifically, as a random walk model for producing node embeddings, and to a graph convolutional network for link prediction. We prove that the proposed algorithm can successfully improve the fairness of all models up to a small or negligible drop in accuracy, and compares favourably with existing state-of-the-art solutions. In an ablation study, we demonstrate that our algorithm can flexibly interpolate between biasing towards fairness and an unbiased edge dropout. Furthermore, to better evaluate the gains, we propose a new dyadic group definition to measure the bias of a link prediction task when paired with group-based fairness metrics. In particular, we extend the metric used to measure the bias in the node embeddings to take into account the graph structure. Impact Statement—Fairness in graph representation learning is under-explored. Yet, the algorithms working with these types of data have a fundamental impact on our digital life. Therefore, despite the law prohibits unfair treatment based on sensitive traits, social networks and recommender systems systematically discriminate against minorities. Current solutions are computationally intensive or significantly lower the accuracy score. To solve the fairness problem, we propose FairDrop, a biased edge dropout. Our approach provides protection against unfairness generated from the network’s homophily w.r.t the sensitive attributes. It is easy to integrate FairDrop into today’s solutions for learning network embeddings or downstream tasks. We believe that the lack of expensive computations and the flexibility of our fairness constraint will spread the awareness of the fairness issue.', 'corpus_id': 233444179, 'score': 1}]
155	"{'doc_id': '219792675', 'title': 'Shapeshifter Networks: Cross-layer Parameter Sharing for Scalable and Effective Deep Learning', 'abstract': ""We present Shapeshifter Networks (SSNs), a flexible neural network framework that improves performance and reduces memory requirements on a diverse set of scenarios over standard neural networks. Our approach is based on the observation that many neural networks are severely overparameterized, resulting in significant waste in computational resources as well as being susceptible to overfitting. SSNs address this by learning where and how to share parameters between layers in a neural network while avoiding degenerate solutions that result in underfitting. Specifically, we automatically construct parameter groups that identify where parameter sharing is most beneficial. Then, we map each group's weights to construct layers with learned combinations of candidates from a shared parameter pool. SSNs can share parameters across layers even when they have different sizes, perform different operations, and/or operate on features from different modalities. We evaluate our approach on a diverse set of tasks, including image classification, bidirectional image-sentence retrieval, and phrase grounding, creating high performing models even when using as little as 1% of the parameters. We also apply SSNs to knowledge distillation, where we obtain state-of-the-art results when combined with traditional distillation methods."", 'corpus_id': 219792675}"	6078	[{'doc_id': '220514221', 'title': 'Alpha-Net: Architecture, Models, and Applications', 'abstract': 'Deep learning network training is usually computationally expensive and intuitively complex. We present a novel network architecture for custom training and weight evaluations. We reformulate the layers as ResNet-similar blocks with certain inputs and outputs of their own, the blocks (called Alpha blocks) on their connection configuration form their own network, combined with our novel loss function and normalization function form the complete Alpha-Net architecture. We provided the empirical mathematical formulation of network loss function for more understanding of accuracy estimation and further optimizations. We implemented Alpha-Net with 4 different layer configurations to express the architecture behavior comprehensively. On a custom dataset based on ImageNet benchmark, we evaluate Alpha-Net v1, v2, v3, and v4 for image recognition to give the accuracy of 78.2%, 79.1%, 79.5%, and 78.3% respectively. The Alpha-Net v3 gives improved accuracy of approx. 3% over the last state-of-the-art network ResNet 50 on ImageNet benchmark. We also present an analysis of our dataset with 256, 512, and 1024 layers and different versions of the loss function. Input representation is also crucial for training as initial preprocessing will take only a handful of features to make training less complex than it needs to be. We also compared network behavior with different layer structures, different loss functions, and different normalization functions for better quantitative modeling of Alpha-Net.', 'corpus_id': 220514221, 'score': 0}, {'doc_id': '219530738', 'title': 'Revisiting the Train Loss: an Efficient Performance Estimator for Neural Architecture Search', 'abstract': 'Reliable yet efficient evaluation of generalisation performance of a proposed architecture is crucial to the success of neural architecture search (NAS). Traditional approaches face a variety of limitations: training each architecture to completion is prohibitively expensive, early stopping estimates may correlate poorly with fully trained performance, and model-based estimators require large training sets. Instead, motivated by recent results linking training speed and generalisation with stochastic gradient descent, we propose to estimate the final test performance based on the sum of training losses. Our estimator is inspired by the marginal likelihood, which is used for Bayesian model selection. Our model-free estimator is simple, efficient, and cheap to implement, and does not require hyperparameter-tuning or surrogate training before deployment. We demonstrate empirically that our estimator consistently outperforms other baselines and can achieve a rank correlation of 0.95 with final test accuracy on the NAS-Bench201 dataset within 50 epochs.', 'corpus_id': 219530738, 'score': 1}, {'doc_id': '211011032', 'title': 'Structure-Feature based Graph Self-adaptive Pooling', 'abstract': 'Various methods to deal with graph data have been proposed in recent years. However, most of these methods focus on graph feature aggregation rather than graph pooling. Besides, the existing top-k selection graph pooling methods have a few problems. First, to construct the pooled graph topology, current top-k selection methods evaluate the importance of the node from a single perspective only, which is simplistic and unobjective. Second, the feature information of unselected nodes is directly lost during the pooling process, which inevitably leads to a massive loss of graph feature information. To solve these problems mentioned above, we propose a novel graph self-adaptive pooling method with the following objectives: (1) to construct a reasonable pooled graph topology, structure and feature information of the graph are considered simultaneously, which provide additional veracity and objectivity in node selection; and (2) to make the pooled nodes contain sufficiently effective graph information, node feature information is aggregated before discarding the unimportant nodes; thus, the selected nodes contain information from neighbor nodes, which can enhance the use of features of the unselected nodes. Experimental results on four different datasets demonstrate that our method is effective in graph classification and outperforms state-of-the-art graph pooling methods.', 'corpus_id': 211011032, 'score': 1}, {'doc_id': '232427850', 'title': 'Parameterized Hypercomplex Graph Neural Networks for Graph Classification', 'abstract': 'Despite recent advances in representation learning in hypercomplex (HC) space, this subject is still vastly unexplored in the context of graphs. Motivated by the complex and quaternion algebras, which have been found in several contexts to enable effective representation learning that inherently incorporates a weight-sharing mechanism, we develop graph neural networks that leverage the properties of hypercomplex feature transformation. In particular, in our proposed class of models, the multiplication rule specifying the algebra itself is inferred from the data during training. Given a fixed model architecture, we present empirical evidence that our proposed model incorporates a regularization effect, alleviating the risk of overfitting. We also show that for fixed model capacity, our proposed method outperforms its corresponding real-formulated GNN, providing additional confirmation for the enhanced expressivity of HC embeddings. Finally, we test our proposed hypercomplex GNN on several open graph benchmark datasets and show that our models reach state-of-the-art performance while consuming a much lower memory footprint with 70% fewer parameters. Our implementations are available at https://github.com/bayer-science-for-a-better-life/phc-gnn.', 'corpus_id': 232427850, 'score': 1}, {'doc_id': '218863030', 'title': 'PruneNet: Channel Pruning via Global Importance', 'abstract': 'Channel pruning is one of the predominant approaches for accelerating deep neural networks. Most existing pruning methods either train from scratch with a sparsity inducing term such as group lasso, or prune redundant channels in a pretrained network and then fine tune the network. Both strategies suffer from some limitations: the use of group lasso is computationally expensive, difficult to converge and often suffers from worse behavior due to the regularization bias. The methods that start with a pretrained network either prune channels uniformly across the layers or prune channels based on the basic statistics of the network parameters. These approaches either ignore the fact that some CNN layers are more redundant than others or fail to adequately identify the level of redundancy in different layers. In this work, we investigate a simple-yet-effective method for pruning channels based on a computationally light-weight yet effective data driven optimization step that discovers the necessary width per layer. Experiments conducted on ILSVRC-$12$ confirm effectiveness of our approach. With non-uniform pruning across the layers on ResNet-$50$, we are able to match the FLOP reduction of state-of-the-art channel pruning results while achieving a $0.98\\%$ higher accuracy. Further, we show that our pruned ResNet-$50$ network outperforms ResNet-$34$ and ResNet-$18$ networks, and that our pruned ResNet-$101$ outperforms ResNet-$50$.', 'corpus_id': 218863030, 'score': 0}, {'doc_id': '220380824', 'title': 'RIFLE: Backpropagation in Depth for Deep Transfer Learning through Re-Initializing the Fully-connected LayEr', 'abstract': 'Fine-tuning the deep convolution neural network(CNN) using a pre-trained model helps transfer knowledge learned from larger datasets to the target task. While the accuracy could be largely improved even when the training dataset is small, the transfer learning outcome is usually constrained by the pre-trained model with close CNN weights (Liu et al., 2019), as the backpropagation here brings smaller updates to deeper CNN layers. In this work, we propose RIFLE - a simple yet effective strategy that deepens backpropagation in transfer learning settings, through periodically Re-Initializing the Fully-connected LayEr with random scratch during the fine-tuning procedure. RIFLE brings meaningful updates to the weights of deep CNN layers and improves low-level feature learning, while the effects of randomization can be easily converged throughout the overall learning procedure. The experiments show that the use of RIFLE significantly improves deep transfer learning accuracy on a wide range of datasets, out-performing known tricks for the similar purpose, such as Dropout, DropConnect, StochasticDepth, Disturb Label and Cyclic Learning Rate, under the same settings with 0.5% -2% higher testing accuracy. Empirical cases and ablation studies further indicate RIFLE brings meaningful updates to deep CNN layers with accuracy improved.', 'corpus_id': 220380824, 'score': 0}, {'doc_id': '220381288', 'title': 'Exploring Heterogeneous Information Networks via Pre-Training', 'abstract': 'To explore heterogeneous information networks (HINs), network representation learning (NRL) is proposed, which represents a network in a low-dimension space. Recently, graph neural networks (GNNs) have drawn a lot of attention which are very expressive for mining a HIN, while they suffer from low efficiency issue. In this paper, we propose a pre-training and fine-tuning framework PF-HIN to capture the features of a HIN. Unlike traditional GNNs that have to train the whole model for each downstream task, PF-HIN only needs to fine-tune the model using the pre-trained parameters and minimal extra task-specific parameters, thus improving the model efficiency and effectiveness. Specifically, in pre-training phase, we first use a ranking-based BFS strategy to form the input node sequence. Then inspired by BERT, we adopt deep bi-directional transformer encoders to train the model, which is a variant of GNN aggregator that is more powerful than traditional deep neural networks like CNN and LSTM. The model is pre-trained based on two tasks, i.e., masked node modeling (MNM) and adjacent node prediction (ANP). Additionally, we leverage factorized embedding parameterization and cross-layer parameter sharing to reduce the parameters. In fine-tuning stage, we choose four benchmark downstream tasks, i.e., link prediction, similarity search, node classification and node clustering. We use node sequence pairs as input for link prediction and similarity search, and a single node sequence as input for node classification and clustering. The experimental results of the above tasks on four real-world datasets verify the advancement of PF-HIN, as it outperforms state-of-the-art alternatives consistently and significantly.', 'corpus_id': 220381288, 'score': 0}, {'doc_id': '231662453', 'title': 'Dive into Decision Trees and Forests: A Theoretical Demonstration', 'abstract': 'Based on decision trees, many fields have arguably made tremendous progress in recent years. In simple words, decision trees use the strategy of “divide-and-conquer” to divide the complex problem on the dependency between input features and labels into smaller ones. While decision trees have a long history, recent advances have greatly improved their performance in computational advertising, recommender system, information retrieval, etc. We introduce common tree-based models (e.g., Bayesian CART, Bayesian regression splines) and training techniques (e.g., mixed integer programming, alternating optimization, gradient descent). Along the way, we highlight probabilistic characteristics of tree-based models and explain their practical and theoretical benefits. Except machine learning and data mining, we try to show theoretical advances on tree-based models from other fields such as statistics and operation research. We list the reproducible resource at the end of each method. Keywords— Decision trees, decision forests, Bayesian trees, soft trees, differentiable trees', 'corpus_id': 231662453, 'score': 1}, {'doc_id': '222141004', 'title': 'A Novel Neural Network Training Framework with Data Assimilation', 'abstract': 'In recent years, the prosperity of deep learning has revolutionized the Artificial Neural Networks. However, the dependence of gradients and the offline training mechanism in the learning algorithms prevents the ANN for further improvement. In this study, a gradient-free training framework based on data assimilation is proposed to avoid the calculation of gradients. In data assimilation algorithms, the error covariance between the forecasts and observations is used to optimize the parameters. Feedforward Neural Networks (FNNs) are trained by gradient decent, data assimilation algorithms (Ensemble Kalman Filter (EnKF) and Ensemble Smoother with Multiple Data Assimilation (ESMDA)), respectively. ESMDA trains FNN with pre-defined iterations by updating the parameters using all the available observations which can be regard as offline learning. EnKF optimize FNN when new observation available by updating parameters which can be regard as online learning. Two synthetic cases with the regression of a Sine Function and a Mexican Hat function are assumed to validate the effectiveness of the proposed framework. The Root Mean Square Error (RMSE) and coefficient of determination (R2) are used as criteria to assess the performance of different methods. The results show that the proposed training framework performed better than the gradient decent method. The proposed framework provides alternatives for online/offline training the existing ANNs (e.g., Convolutional Neural Networks, Recurrent Neural Networks) without the dependence of gradients.', 'corpus_id': 222141004, 'score': 1}, {'doc_id': '218581509', 'title': 'Compact Neural Representation Using Attentive Network Pruning', 'abstract': 'Deep neural networks have evolved to become power demanding and consequently difficult to apply to small-size mobile platforms. Network parameter reduction methods have been introduced to systematically deal with the computational and memory complexity of deep networks. We propose to examine the ability of attentive connection pruning to deal with redundancy reduction in neural networks as a contribution to the reduction of computational demand. In this work, we describe a Top-Down attention mechanism that is added to a Bottom-Up feedforward network to select important connections and subsequently prune redundant ones at all parametric layers. Our method not only introduces a novel hierarchical selection mechanism as the basis of pruning but also remains competitive with previous baseline methods in the experimental evaluation. We conduct experiments using different network architectures on popular benchmark datasets to show high compression ratio is achievable with negligible loss of accuracy.', 'corpus_id': 218581509, 'score': 0}]
156	{'doc_id': '218551201', 'title': 'Beyond Accuracy: Behavioral Testing of NLP Models with CheckList', 'abstract': 'Although measuring held-out accuracy has been the primary approach to evaluate generalization, it often overestimates the performance of NLP models, while alternative approaches for evaluating models either focus on individual tasks or on specific behaviors. Inspired by principles of behavioral testing in software engineering, we introduce CheckList, a task-agnostic methodology for testing NLP models. CheckList includes a matrix of general linguistic capabilities and test types that facilitate comprehensive test ideation, as well as a software tool to generate a large and diverse number of test cases quickly. We illustrate the utility of CheckList with tests for three tasks, identifying critical failures in both commercial and state-of-art models. In a user study, a team responsible for a commercial sentiment analysis model found new and actionable bugs in an extensively tested model. In another user study, NLP practitioners with CheckList created twice as many tests, and found almost three times as many bugs as users without it.', 'corpus_id': 218551201}	7310	"[{'doc_id': '52003282', 'title': 'Pathologies of Neural Models Make Interpretations Difficult', 'abstract': 'One way to interpret neural model predictions is to highlight the most important input features—for example, a heatmap visualization over the words in an input sentence. In existing interpretation methods for NLP, a word’s importance is determined by either input perturbation—measuring the decrease in model confidence when that word is removed—or by the gradient with respect to that word. To understand the limitations of these methods, we use input reduction, which iteratively removes the least important word from the input. This exposes pathological behaviors of neural models: the remaining words appear nonsensical to humans and are not the ones determined as important by interpretation methods. As we confirm with human experiments, the reduced examples lack information to support the prediction of any label, but models still make the same predictions with high confidence. To explain these counterintuitive results, we draw connections to adversarial examples and confidence calibration: pathological behaviors reveal difficulties in interpreting neural models trained with maximum likelihood. To mitigate their deficiencies, we fine-tune the models by encouraging high entropy outputs on reduced examples. Fine-tuned models become more interpretable under input reduction, without accuracy loss on regular examples.', 'corpus_id': 52003282, 'score': 1}, {'doc_id': '222208874', 'title': 'Discriminatively-Tuned Generative Classifiers for Robust Natural Language Inference', 'abstract': 'While discriminative neural network classifiers are generally preferred, recent work has shown advantages of generative classifiers in term of data efficiency and robustness. In this paper, we focus on natural language inference (NLI). We propose GenNLI, a generative classifier for NLI tasks, and empirically characterize its performance by comparing it to five baselines, including discriminative models and large-scale pretrained language representation models like BERT. We explore training objectives for discriminative fine-tuning of our generative classifiers, showing improvements over log loss fine-tuning from prior work . In particular, we find strong results with a simple unbounded modification to log loss, which we call the ""infinilog loss"". Our experiments show that GenNLI outperforms both discriminative and pretrained baselines across several challenging NLI experimental settings, including small training sets, imbalanced label distributions, and label noise.', 'corpus_id': 222208874, 'score': 0}, {'doc_id': '226237381', 'title': 'Weakly- and Semi-supervised Evidence Extraction', 'abstract': 'For many prediction tasks, stakeholders desire not only predictions but also supporting evidence that a human can use to verify its correctness. However, in practice, evidence annotations may only be available for a minority of training examples (if available at all). In this paper, we propose new methods to combine few evidence annotations (strong semi-supervision) with abundant document-level labels (weak supervision) for the task of evidence extraction. Evaluating on two classification tasks that feature evidence annotations, we find that our methods outperform baselines adapted from the interpretability literature to our task. Our approach yields gains with as few as hundred evidence annotations.', 'corpus_id': 226237381, 'score': 1}, {'doc_id': '222090330', 'title': 'Understanding tables with intermediate pre-training', 'abstract': 'Table entailment, the binary classification task of finding if a sentence is supported or refuted by the content of a table, requires parsing language and table structure as well as numerical and discrete reasoning. While there is extensive work on textual entailment, table entailment is less well studied. We adapt TAPAS (Herzig et al., 2020), a table-based BERT model, to recognize entailment. Motivated by the benefits of data augmentation, we create a balanced dataset of millions of automatically created training examples which are learned in an intermediate step prior to fine-tuning. This new data is not only useful for table entailment, but also for SQA (Iyyer et al., 2017), a sequential table QA task. To be able to use long examples as input of BERT models, we evaluate table pruning techniques as a pre-processing step to drastically improve the training and prediction efficiency at a moderate drop in accuracy. The different methods set the new state-of-the-art on the TabFact (Chen et al., 2020) and SQA datasets.', 'corpus_id': 222090330, 'score': 0}, {'doc_id': '7053611', 'title': 'Right for the Right Reasons: Training Differentiable Models by Constraining their Explanations', 'abstract': 'Neural networks are among the most accurate supervised learning methods in use today, but their opacity makes them difficult to trust in critical applications, especially when conditions in training differ from those in test. Recent work on explanations for black-box models has produced tools (e.g. LIME) to show the implicit rules behind predictions, which can help us identify when models are right for the wrong reasons. However, these methods do not scale to explaining entire datasets and cannot correct the problems they reveal. We introduce a method for efficiently explaining and regularizing differentiable models by examining and selectively penalizing their input gradients, which provide a normal to the decision boundary. We apply these penalties both based on expert annotation and in an unsupervised fashion that encourages diverse models with qualitatively different decision boundaries for the same classification problem. On multiple datasets, we show our approach generates faithful explanations and models that generalize much better when conditions differ between training and test.', 'corpus_id': 7053611, 'score': 1}, {'doc_id': '19204066', 'title': 'Breaking NLI Systems with Sentences that Require Simple Lexical Inferences', 'abstract': 'We create a new NLI test set that shows the deficiency of state-of-the-art models in inferences that require lexical and world knowledge. The new examples are simpler than the SNLI test set, containing sentences that differ by at most one word from sentences in the training set. Yet, the performance on the new test set is substantially worse across systems trained on SNLI, demonstrating that these systems are limited in their generalization ability, failing to capture many simple inferences.', 'corpus_id': 19204066, 'score': 1}, {'doc_id': '222133166', 'title': 'Learning from Context or Names? An Empirical Study on Neural Relation Extraction', 'abstract': 'Neural models have achieved remarkable success on relation extraction (RE) benchmarks. However, there is no clear understanding which type of information affects existing RE models to make decisions and how to further improve the performance of these models. To this end, we empirically study the effect of two main information sources in text: textual context and entity mentions (names). We find that (i) while context is the main source to support the predictions, RE models also heavily rely on the information from entity mentions, most of which is type information, and (ii) existing datasets may leak shallow heuristics via entity mentions and thus contribute to the high performance on RE benchmarks. Based on the analyses, we propose an entity-masked contrastive pre-training framework for RE to gain a deeper understanding on both textual context and type information while avoiding rote memorization of entities or use of superficial cues in mentions. We carry out extensive experiments to support our views, and show that our framework can improve the effectiveness and robustness of neural models in different RE scenarios. All the code and datasets are released at this https URL.', 'corpus_id': 222133166, 'score': 0}, {'doc_id': '226237434', 'title': 'MACE: Model Agnostic Concept Extractor for Explaining Image Classification Networks', 'abstract': ""Deep convolutional networks have been quite successful at various image classification tasks. The current methods to explain the predictions of a pre-trained model rely on gradient information, often resulting in saliency maps that focus on the foreground object as a whole. However, humans typically reason by dissecting an image and pointing out the presence of smaller concepts. The final output is often an aggregation of the presence or absence of these smaller concepts. In this work, we propose MACE: a Model Agnostic Concept Extractor, which can explain the working of a convolutional network through smaller concepts. The MACE framework dissects the feature maps generated by a convolution network for an image to extract concept based prototypical explanations. Further, it estimates the relevance of the extracted concepts to the pre-trained model's predictions, a critical aspect required for explaining the individual class predictions, missing in existing approaches. We validate our framework using VGG16 and ResNet50 CNN architectures, and on datasets like Animals With Attributes 2 (AWA2) and Places365. Our experiments demonstrate that the concepts extracted by the MACE framework increase the human interpretability of the explanations, and are faithful to the underlying pre-trained black-box model."", 'corpus_id': 226237434, 'score': 0}, {'doc_id': '226226804', 'title': 'Reasoning Over History: Context Aware Visual Dialog', 'abstract': 'While neural models have been shown to exhibit strong performance on single-turn visual question answering (VQA) tasks, extending VQA to a multi-turn, conversational setting remains a challenge. One way to address this challenge is to augment existing strong neural VQA models with the mechanisms that allow them to retain information from previous dialog turns. One strong VQA model is the MAC network, which decomposes a task into a series of attention-based reasoning steps. However, since the MAC network is designed for single-turn question answering, it is not capable of referring to past dialog turns. More specifically, it struggles with tasks that require reasoning over the dialog history, particularly coreference resolution. We extend the MAC network architecture with Context-aware Attention and Memory (CAM), which attends over control states in past dialog turns to determine the necessary reasoning operations for the current question. MAC nets with CAM achieve up to 98.25% accuracy on the CLEVR-Dialog dataset, beating the existing state-of-the-art by 30% (absolute). Our error analysis indicates that with CAM, the model’s performance particularly improved on questions that required coreference resolution.', 'corpus_id': 226226804, 'score': 0}, {'doc_id': '237364609', 'title': 'Enjoy the Salience: Towards Better Transformer-based Faithful Explanations with Word Salience', 'abstract': 'Pretrained transformer-based models such as BERT have demonstrated state-of-the-art predictive performance when adapted into a range of natural language processing tasks. An open problem is how to improve the faithfulness of explanations (rationales) for the predictions of these models. In this paper, we hypothesize that salient information extracted a priori from the training data can complement the task-specific information learned by the model during fine-tuning on a downstream task. In this way, we aim to help BERT not to forget assigning importance to informative input tokens when making predictions by proposing SALOSS; an auxiliary loss function for guiding the multi-head attention mechanism during training to be close to salient information extracted a priori using TextRank. Experiments for explanation faithfulness across five datasets, show that models trained with SALOSS consistently provide more faithful explanations across four different feature attribution methods compared to vanilla BERT. Using the rationales extracted from vanilla BERT and SALOSS models to train inherently faithful classifiers, we further show that the latter result in higher predictive performance in downstream tasks.1', 'corpus_id': 237364609, 'score': 1}]"
157	{'doc_id': '67320926', 'title': 'Solving an Inventory Models Involving Lead Time Crashing Cost as an Exponential Function in Food Processing and Distribution Industry Using Matlab', 'abstract': '556 Solving an Inventory Models Involving Lead Time Crashing Cost as an Exponential Function in Food Processing and Distribution Industry Using Matlab S. Rekha1, P. Pavithra2 1Assistant Professor, Department of Mathematics, Dr. SNS Rajalakshmi College of Arts and Science, Coimbatore, Tamil Nadu, India 2Research Scholar,Department of Mathematics, Dr. SNS Rajalakshmi College of Arts and Science. Coimbatore, Tamil Nadu, India ABSTRACT', 'corpus_id': 67320926}	20633	"[{'doc_id': '237323608', 'title': 'Applying a Multi-Dimensional Digital Food and Nutrition Literacy Model to Inform Research and Policies to Enable Adults in the U.S. Supplemental Nutrition Assistance Program to Make Healthy Purchases in the Online Food Retail Ecosystem', 'abstract': 'The United States (U.S.) Department of Agriculture (USDA)-administered Supplemental Nutrition Assistance Program (SNAP) made substantial changes in response to the coronavirus disease 2019 (COVID-19) pandemic. These changes highlight the need to identify the digital literacy skills and capacities of SNAP adults to purchase healthy groceries online. We conducted a scoping review of four electronic databases, Google and Google Scholar to identify studies that measured food and nutrition literacy outcomes for U.S. adults. We applied a multi-dimensional digital food and nutrition literacy (MDFNL) model to assess six literacy levels and components. Of 18 studies published from 2006–2021, all measured functional and interactive literacy but no study measured communicative, critical, translational, or digital literacy. Six studies examined SNAP or SNAP-Education outcomes. Adults with higher food or nutrition literacy scores had better cognitive, behavioral, food security and health outcomes. We suggest how these findings may inform research, policies, and actions to strengthen the multi-dimensional literacy skills of SNAP participants and SNAP-eligible adults to support healthy purchases in the online food retail ecosystem.', 'corpus_id': 237323608, 'score': 1}, {'doc_id': '236288781', 'title': 'Role of mobile food-ordering applications in developing restaurants’ brand satisfaction and loyalty in the pandemic period', 'abstract': ""Abstract Retailers and their supply-chain partners should reconsider their competitive advantages in today's technology-enhanced environment and search for opportunities to collaborate. Mobile applications have emerged as a special form of e-commerce that provide convenience for consumers by saving time and effort. In the pandemic days, people prefer using mobile applications in particular to order food from restaurants and protect themselves from COVID-19. Even though there are many previous studies regarding mobile application usage, they have investigated the antecedents of mobile app adoption. Today, consumers already have adopted them, yet there is a need to understand the outcomes of mobile application usage, especially for mobile food-ordering apps (MFOAs). To fill this gap, this study first positions the MFOAs among other mobile food applications regarding their business models. Then, a structural model was developed and tested by focusing on the outcomes of MFOA usage. A survey of 217 participants was conducted, and the data were analyzed using partial least squares path modeling (PLS-PM). The main contribution of the results is the finding that MFOA satisfaction plays a critical role in developing restaurants' brand satisfaction and loyalty. Thus, brands should cooperate with MFOA providers."", 'corpus_id': 236288781, 'score': 0}, {'doc_id': '237679714', 'title': 'Review article: Food safety culture from the perspective of the Australian horticulture industry', 'abstract': 'Background: \n \nFoodborne illness outbreaks associated with fresh produce suggest a focus on food safety culture within food safety management systems throughout supply chains would benefit the horticulture industry. The recent inclusion of food safety culture in horticulture standards will drive the need for better understanding, integration into business activities, and monitoring, to help mitigate foodborne incidents in horticulture. \n \n Scope and approach: \n \nThe purpose of this review was to identify definitions of food safety culture and methods of measuring its performance in the context of the Australian horticulture industry. Investigation of how to better apply and integrate a positive food safety culture into existing food safety management systems was conducted. A roadmap for food safety culture improvement in Australian horticulture is presented, highlighting the challenges and opportunities. Key findings and conclusions: \n \nTo guide the development of a mature culture of food safety, mixed-method approaches to performance assessment were found to be the most comprehensive, valid, and offer the most potential for use by horticulture businesses. Food safety culture can be developed by using feedback from regular culture assessments that identify weaknesses and opportunities for improvement, leading to increased knowledge, alignment of attitudes, and better food safety and hygiene behaviour. To this end, the development of measurement tools specific to horticulture operations would be beneficial. Despite unforeseen challenges like the COVID-19 pandemic, the Australian horticulture industry continues to investigate the efficacy of its food safety management.', 'corpus_id': 237679714, 'score': 0}, {'doc_id': '165113804', 'title': 'Managing Your Supply Chain Pantry: Food Waste Mitigation Through Inventory Control', 'abstract': 'In this paper, we report through case study findings that the food supply chain can be improved by implementing a proper inventory control policy. The case study involves an online retailer who sells predominantly food products. The improvement of the case company is shown via simulation in economic, social, and environmental aspects. The findings indicate the role of supply chain risk as an obstacle in achieving supply chain sustainability, and the benefit of effective inventory control as a cost-advantageous and easy-to-implement waste reduction method.', 'corpus_id': 165113804, 'score': 1}, {'doc_id': '221050425', 'title': 'Supporting Wellness at Pantries (SWAP): changes to inventory in six food pantries over one year', 'abstract': 'The aim of the Supporting Wellness at Pantries (SWAP) system is to rank, label, and organize food pantry items according to whether they should be consumed often (green), sometimes (yellow), or rarely (red), using a stoplight system in accordance with the 2015–2020 Dietary Guidelines for Americans. This study assessed the nutritional quality of inventory available at six food pantries before and after implementing SWAP. The hypothesis was that the intervention would encourage efforts to procure healthier foods. Six food pantries participated in the study. At baseline, the inventory was assessed over 4 weeks in the summer of 2016. The percentage of red, yellow, and green foods was calculated by food category. The intervention was implemented from October 2016 to June 2017. The follow-up inventory assessment occurred during 4 weeks in the summer of 2017. Multivariate regression analyses were performed to assess whether the nutritional quality of food pantry inventory (measured by SWAP rank) improved post-intervention, adjusting for time trends and food category fixed effects. Results revealed that post-intervention, the average weekly pantry inventory contained 28.35 (p\u2009=\u2009.037) more pounds of green food in each food category. There were no significant changes in the pounds of yellow (β\u2009=\u200913.77, p\u2009=\u2009.31) or red (β\u2009=\u2009−2.89, p\u2009=\u2009.78) food available. One year post-intervention, the nutritional quality of food pantry inventories improved. These findings support continued structural changes to promote healthy food access to people experiencing food insecurity.', 'corpus_id': 221050425, 'score': 1}, {'doc_id': '237461473', 'title': ""Consumers' Intention Towards Online Food Ordering and Delivery Service"", 'abstract': 'Online food ordering and delivery services are the platforms that provide a system and service to the consumer to order and buy food products from foodservice or restaurant operators. The service is an up-and-coming trend among Millennials and the trend has ballooned with the introduction of movement control order (MCO) due to the pandemic COVID-19. The consumer’s demand towards online food ordering and delivery service has increased markedly due to the prohibition of dine-in at restaurant premises. This study focuses on highlighting the factors that influence consumer’s intention to use online food ordering and delivery services. The survey questionnaires were distributed among 384 respondents that represent the customers of restaurants in Shah Alam, Selangor. Data analyses were conducted using SPSS and multiple regression analysis. Findings revealed that usefulness, ease of use, and consumer’s enjoyment are the factors that influence consumer’s intention to use online food ordering services and conclusively, usefulness is the most significant factor that affects consumer’s intention to order food via online food ordering and delivery service. The findings from this study are beneficial for foodservice or restaurant operators in improving their businesses and staying competitive. Recommendations for future studies are included based on the findings of this study.', 'corpus_id': 237461473, 'score': 0}, {'doc_id': '235828625', 'title': 'Research on Innovative Business Plan. Smart Cattle Farming Using Artificial Intelligent Robotic Process Automation', 'abstract': 'Integrating livestock management with the required devices and sensors is now seen as a critical factor in the agricultural sector’s long-term success. The findings revealed that the agricultural business sector is open to implementing Information and Communication Technology (ICT) solutions, so the aim of this paper is to determine how advantageous it is for Romanian farmers to invest in a project that employs smart cattle farming methods that incorporate Artificial Intelligence (AI), Robotic Process Automation (RPA) and the Internet of Things (IOT). An unstructured interview was used to gather empirical evidence during a focus group meeting. Analyzing the selected primary performance metrics, it was projected that the farm’s profitability would increase by 19 percent, productivity would increase by 21 percent, and the farm’s environmental impact would decrease by 22 percent. Automation and remote work would help minimize the farm’s worker burden while also making control panels, decision-making files, and data analysis more available. In order for the domain to be as prosperous as possible, farmers must be made aware of the benefits of using these emerging technologies for closing the gap between farmers and Information Technology (IT) solution providers, and this can be accomplished through continuous training for both farmers and their technology vendors.', 'corpus_id': 235828625, 'score': 0}, {'doc_id': '237382971', 'title': 'SIMULATION OF A SCHOOL CANTEEN TO UNDERSTAND MEAL DURATION IMPACT ON FOOD WASTE', 'abstract': 'A system simulation is a one of the approaches to understand business processes or to explain them to other people. It is an excellent decision making solution to provide data-driven conclusions based on system modelling and experiments. This paper proposes simulation results of a school canteen. The aim of the research was to investigate the relation between a food waste amount and meal time duration. The proposed simulation was based on business process analysis, business process modelling, a Monte Carlo method and expert knowledge. The frequency distributions were constructed based on children meal duration observation completed by their mothers. It is a magnificent citizen science solution to involve mothers in the research because they can additionally better understand their children meal preferences and habits. Therefore, a questionnaire for citizens was developed, which can be applied to collect statistical data for model accuracy improvement and extension.', 'corpus_id': 237382971, 'score': 0}, {'doc_id': '214402534', 'title': 'How Urban Food Pantries are Stocked and Food Is Distributed: Food Pantry Manager Perspectives from Baltimore', 'abstract': 'ABSTRACT Low-income, food-insecure Baltimore residents frequently rely on food pantries. In this study, pantry managers were key informants who shared information on how and why certain products were obtained and distributed and their perceptions around the need for nutritious products. Managers prioritized providing “staple” foods that could comprise a meal, and most of these foods were shelf-stable. Most pantries distributed pre-assembled, uniform bags, rather than using a client choice method. Managers did not perceive that their clients wanted healthy foods, despite clients informing them of diet-related health conditions. Manager-level training may be necessary to align pantry operations with clients’ food needs.', 'corpus_id': 214402534, 'score': 1}, {'doc_id': '73482319', 'title': 'A systematic review of food pantry-based interventions in the USA', 'abstract': 'Abstract Objective Food pantries play a critical role in combating food insecurity. The objective of the present work was to systematically review and synthesize scientific evidence regarding the effectiveness of food pantry-based interventions in the USA. Design Keyword/reference search was conducted in PubMed, Web of Science, Scopus, Cochrane Library and CINAHL for peer-reviewed articles published until May 2018 that met the following criteria. Setting: food pantry and/or food bank in the USA; study design: randomized controlled trial (RCT) or pre–post study; outcomes: diet-related outcomes (e.g. nutrition knowledge, food choice, food security, diet quality); study subjects: food pantry/bank clients. Results Fourteen articles evaluating twelve distinct interventions identified from the keyword/reference search met the eligibility criteria and were included in the review. Five were RCT and the remaining seven were pre–post studies. All studies found that food pantry-based interventions were effective in improving participants’ diet-related outcomes. In particular, the nutrition education interventions and the client-choice intervention enhanced participants’ nutrition knowledge, cooking skills, food security status and fresh produce intake. The food display intervention helped pantry clients select healthier food items. The diabetes management intervention reduced participants’ glycaemic level. Conclusions Food pantry-based interventions were found to be effective in improving participants’ diet-related outcomes. Interventions were modest in scale and usually short in follow-up duration. Future studies are warranted to address the challenges of conducting interventions in food pantries, such as shortage in personnel and resources, to ensure intervention sustainability and long-term effectiveness.', 'corpus_id': 73482319, 'score': 1}]"
158	{'doc_id': '9776000', 'title': 'Reduction in Unnecessary Clinical Laboratory Testing Through Utilization Management at a US Government Veterans Affairs Hospital.', 'abstract': 'OBJECTIVES\nTo implement an electronic laboratory utilization management system (laboratory expert system [LES]) to provide safe and effective reductions in unnecessary clinical laboratory testing.\n\n\nMETHODS\nThe LES is a set of frequency filter subroutines within the Veterans Affairs hospital and laboratory information system that was formulated by an interdisciplinary medical team.\n\n\nRESULTS\nSince implementing the LES, total test volume has decreased by a mean of 11.18% per year compared with our pre-LES test volume. This change was not attributable to fluctuations in outpatient visits or inpatient days of care. Laboratory cost savings were estimated at $151,184 and $163,751 for 2012 and 2013, respectively. A significant portion of these cost savings was attributable to reductions in high-volume, large panel testing. No adverse effects on patient care were reported, and mean length of stay for patients remained unchanged.\n\n\nCONCLUSIONS\nElectronic laboratory utilization systems can effectively reduce unnecessary laboratory testing without compromising patient care.', 'corpus_id': 9776000}	19039	[{'doc_id': '235956766', 'title': 'Simple prognostic factors and change of inflammatory markers in patients with severe coronavirus disease 2019: a single‐center observational study', 'abstract': 'Aim The aim of this study was to investigate the prognostic factors and evaluate the change in inflammatory markers of patients with coronavirus disease 2019 (COVID‐19) requiring mechanical ventilation. Methods This retrospective observational study conducted from April 1, 2020, to February 18, 2021, included 97 adult patients who required mechanical ventilation for severe COVID‐19 pneumonia and excluded nonintubated patients with a positive COVID‐19 polymerase chain reaction test and those who had any obvious bacterial infection on admission. All patients were followed up to discharge or death. We obtained clinical information and laboratory data including levels of presepsin, interleukin‐6, procalcitonin, and severe acute respiratory syndrome coronavirus 2 (SARS‐CoV‐2) antibody every day. Poor outcome was defined as death or receiving a tracheostomy during hospitalization, and favorable outcome was defined as discharge after extubation. Results Differences (median [interquartile range]) were detected in age (76 [70–82] versus 66 [55–74] years), day from the onset of first symptoms to admission for mechanical ventilation (5 [3–7] versus 10 [8–12] days), and P/F ratio (i.e., ratio of arterial oxygen concentration to the fraction of inspired oxygen) after intubation (186 [149–251] versus 236 [180–296]) in patients with poor outcome versus those with favorable outcome on admission. Serum SARS‐CoV‐2 antibody levels had already increased on admission in patients with favorable outcome. We determined the day from the onset of first symptoms to admission for mechanical ventilation to be one of the independent prognostic factors of patients with COVID‐19 (adjusted odds ratio 0.69, confidence interval 0.56–0.85). Conclusion These results may contribute to understanding the mechanism of progression in severe COVID‐19 and may be helpful in devising an effective therapeutic strategy.', 'corpus_id': 235956766, 'score': 0}, {'doc_id': '226248426', 'title': 'Why clinicians overtest: development of a thematic framework', 'abstract': 'Medical tests provide important information to guide clinical management. Overtesting, however, may cause harm to patients and the healthcare system, including through misdiagnosis, false positives, false negatives and overdiagnosis. Clinicians are ultimately responsible for test requests, and are therefore ideally positioned to prevent overtesting and its unintended consequences. Through this narrative literature review and workshop discussion with experts at the Preventing Overdiagnosis Conference (Sydney, 2019), we aimed to identify and establish a thematic framework of factors that influence clinicians to request non-recommended and unnecessary tests. Articles exploring factors affecting clinician test ordering behaviour were identified through a systematic search of MedLine in April 2019, forward and backward citation searches and content experts. Two authors screened abstract titles and abstracts, and two authors screened full text for inclusion. Identified factors were categorised into a preliminary framework which was subsequently presented at the PODC for iterative development. The MedLine search yielded 542 articles; 55 were included. Another 10 articles identified by forward-backward citation and content experts were included, resulting in 65 articles in total. Following small group discussion with workshop participants, a revised thematic framework of factors was developed: “Intrapersonal” – fear of malpractice and litigation; clinician knowledge and understanding; intolerance of uncertainty and risk aversion; cognitive biases and experiences; sense of medical obligation “Interpersonal” – pressure from patients and doctor-patient relationship; pressure from colleagues and medical culture; “Environment/context” – guidelines, protocols and policies; financial incentives and ownership of tests; time constraints, physical vulnerabilities and language barriers; availability and ease of access to tests; pre-emptive testing to facilitate subsequent care; contemporary medical practice and new technology “Intrapersonal” – fear of malpractice and litigation; clinician knowledge and understanding; intolerance of uncertainty and risk aversion; cognitive biases and experiences; sense of medical obligation “Interpersonal” – pressure from patients and doctor-patient relationship; pressure from colleagues and medical culture; “Environment/context” – guidelines, protocols and policies; financial incentives and ownership of tests; time constraints, physical vulnerabilities and language barriers; availability and ease of access to tests; pre-emptive testing to facilitate subsequent care; contemporary medical practice and new technology This thematic framework may raise awareness of overtesting and prompt clinicians to change their test request behaviour. The development of a scale to assess clinician knowledge, attitudes and practices is planned to allow evaluation of clinician-targeted interventions to reduce overtesting.', 'corpus_id': 226248426, 'score': 1}, {'doc_id': '235607691', 'title': 'Implementation of point-of-care blood gas testing at a large community hospital: Cost analysis, sepsis bundle compliance, and employee engagement', 'abstract': 'Introduction/Background Point-of-care testing (POCT) platforms support patient-centered approaches to health care delivery and may improve patient care. We evaluated implementation of a POCT platform at a large, acute care hospital in the Midwestern United States. Methods We used lactate testing as part of a sepsis bundle protocol to evaluate compliance and mortality outcomes. Respiratory team members were surveyed to assess perception of efficiency, ease of use, timely patient care, and overall engagement with the POCT system. Annualized cost per test of a benchtop analyzer and a POCT platform were compared across 3 years for each platform. Results Lactate testing volume increased from 61% to 91%, which was associated with improved sepsis bundle protocol compliance. Employees reported high levels of engagement, improvements in efficiency and time savings, and better patient care with POCT. Average cost per test was $10.02 for the benchtop system and $6.21 for the POCT platform. POCT saved our institution $88,476 annually in labor costs. Discussion Combined with a robust training program emphasizing the use of lactate testing in the context of the overall clinical picture, POCT enabled adherence to the sepsis bundle protocol and may have contributed to lower mortality. Additionally, the COVID-19 pandemic has provided us with unanticipated benefits of using POCT; it has enhanced our ability to deal with stringent infectious disease protocols, saving time and minimizing patient and staff exposure. Conclusions Implementation of a POCT platform was associated with improved compliance to our sepsis protocol, reduced sepsis mortality, high employee engagement, and cost savings.', 'corpus_id': 235607691, 'score': 1}, {'doc_id': '236012410', 'title': 'The prognostic value of extravascular lung water index in critically ill septic shock patients', 'abstract': 'ObjectiveTo investigate the prognostic value of extravascular lung water index (EVLWI) in critically ill patients with septic shock in intensive care unit (ICU). MethodsEVLWI was determined by using a PiCCO Monitor, and the daily fluid balance was recorded. ResultsFifty patients with septic shock were admitted and twenty-six patients survived. The average EVLWI at baseline was 11.7 ml/kg, and the difference was not different between survivors and nonsurvivors, P=0.551. The EVLWI of day 3 (EVLWI_ d_3 )in nonsurvivors was significantly higher than the survivors [(14.3±8.8)ml/kg vs (8.1±2.7) ml/kg, P=0.001]. If the patients were divided into three groups by the EVLWI_ d_3 , group one 0-7 ml/kg(4/16),group two 8-14 ml/kg(10/24),and group three 14 ml/kg(10/10), the hospital mortality of the third group was significantly higher than the other two groups (P=0.000, 0.002). There was a significant difference between the survivors and the nonsurvivors in the fluid balance at the first day and the following three days (P=0.000, 0.000). Negative fluid balance was associated with a lower mortality.By using receiver operating characteristic analysis, the area under the curve was 0.740±0.072 to EVLWI_ d_3 .If EVLWI 7.5 ml/kg, the sensitivity and the specificity of accurate judgment were 83.3% and 53.8%. ConclusionDynamic observation of EVLWI can be one of the factors for predicting the prognosis of patients with septic shock. A reduction of EVLWI at early treatment and a negative fluid balance were associated with a better prognosis.', 'corpus_id': 236012410, 'score': 0}, {'doc_id': '20503895', 'title': 'Influence of educational, audit and feedback, system based, and incentive and penalty interventions to reduce laboratory test utilization: a systematic review', 'abstract': 'Abstract Laboratory and radiographic tests are often ordered unnecessarily. This excess testing has financial costs and is a burden on patients. We performed a systematic review to determine the effectiveness interventions to reduce test utilization by physicians. The MEDLINE and EMBASE databases were searched for the years 1946 through to September 2013 for English articles that had themes of test utilization and cost containment or optimization. Bibliographies of included papers were scanned to identify other potentially relevant studies. Our search resulted in 3236 articles of which 109 met the inclusion criteria of having an intervention aimed at reducing test utilization with results that could be expressed as a percent reduction in test use relative to the comparator. Each intervention was categorized into one or more non-exclusive category of education, audit and feedback, system based, or incentive or penalty. A rating of study quality was also performed. The percent reductions in test use ranged from a 99.7% reduction to a 27.7% increase in test use. Each category of intervention was effective in reducing test utilization. Heterogeneity between interventions, poor study quality, and limited time horizons makes generalizations difficult and calls into question the validity of results. Very few studies measure any patient safety or quality of care outcomes affected by reduced test use. There are numerous studies that use low investment strategies to reduce test utilization with one time changes in the ordering system. These low investment strategies are the most promising for achievable and durable reductions in inappropriate test use.', 'corpus_id': 20503895, 'score': 1}, {'doc_id': '5190443', 'title': 'Utilization management in a large urban academic medical center: a 10-year experience.', 'abstract': 'Management of laboratory test utilization presents an ongoing challenge. Most studies reported in the literature have described efforts to control one or a few tests, but the results cannot be generalized to a broader utilization management strategy. Herein we report our experiences with an organizational utilization management program during a 10-year period. Cumulatively, our program has achieved significant success, saving millions of dollars in blood components and reducing inpatient tests per discharge by 26%. Highlights from our experiences include the importance of implementing an institutional organizational structure to support utilization management, the central role fulfilled by clinical pathologists as leaders of the program, the ability to obtain timely utilization data, and careful selection of the most appropriate implementation tools tailored to the unique circumstances of each utilization management initiative.', 'corpus_id': 5190443, 'score': 1}, {'doc_id': '235483923', 'title': 'Tracheostomy Timing and Outcome in Severe COVID-19: The WeanTrach Multicenter Study', 'abstract': 'Background: Tracheostomy can be performed safely in patients with coronavirus disease 2019 (COVID-19). However, little is known about the optimal timing, effects on outcome, and complications. Methods: A multicenter, retrospective, observational study. This study included 153 tracheostomized COVID-19 patients from 11 intensive care units (ICUs). The primary endpoint was the median time to tracheostomy in critically ill COVID-19 patients. Secondary endpoints were survival rate, length of ICU stay, and post-tracheostomy complications, stratified by tracheostomy timing (early versus late) and technique (surgical versus percutaneous). Results: The median time to tracheostomy was 15 (1–64) days. There was no significant difference in survival between critically ill COVID-19 patients who received tracheostomy before versus after day 15, nor between surgical and percutaneous techniques. ICU length of stay was shorter with early compared to late tracheostomy (p < 0.001) and percutaneous compared to surgical tracheostomy (p = 0.050). The rate of lower respiratory tract infections was higher with surgical versus percutaneous technique (p = 0.007). Conclusions: Among critically ill patients with COVID-19, neither early nor percutaneous tracheostomy improved outcomes, but did shorten ICU stay. Infectious complications were less frequent with percutaneous than surgical tracheostomy.', 'corpus_id': 235483923, 'score': 0}, {'doc_id': '236034840', 'title': 'A quality improvement project to reduce antibiotic utilization and ancillary laboratory tests in the appraisal of early-onset sepsis in the NICU.', 'abstract': 'BACKGROUND\nDiagnosis and treatment of early-onset sepsis (EOS) of the newborn remains a controversial issue among providers due to the non-infectious symptomology which exists in the newborn period.\n\n\nMETHODS\nPre/post interventional quality improvement project in a level III NICU to reduce antibiotic utilization and ancillary laboratory tests with the introduction of an evidence-based guideline for the evaluation of EOS in the NICU.\n\n\nRESULTS\nPrimary outcome measures include mean number of empiric antibiotic treatment days and utilization rate (AUR), number of laboratory tests ordered, and incidence of unwarranted antibiotic therapy beyond the 48-h rule out period. Mean empiric antibiotic treatment days decreased from 2.94 to 1.58 days and overall antibiotic use decreased from 73.7% to 57.1%. Likewise, the mean AUR decreased from 212.5 to 147.6 days of therapy per 1000 patient days. There was an 86% decline in the number of ancillary tests and unwarranted antibiotic use beyond 48- h was reduced by 74%.\n\n\nDISCUSSION\nGuidelines for EOS of the newborn should include a thorough baseline evaluation of the drivers of antibiotic use to create an evidence-based foundation. Reducing unnecessary antibiotic use and EOS evaluations in a safe and effective manner have the potential to lower consumer and healthcare expenditures while improving the long-term health of the newborn in the NICU.\n\n\nCONCLUSIONS\nThese findings emphasize the importance of implementing an evidence-based protocol for antibiotic stewardship in the NICU. With further research there is the potential to improve the healthcare of newborns while reducing expenditures in a safe, effective evaluation of EOS in the newborn population.', 'corpus_id': 236034840, 'score': 1}, {'doc_id': '235338658', 'title': 'A retrospective cohort study assessing acute kidney injury and renal recovery among septic patients empirically treated with vancomycin piperacillin–tazobactam versus vancomycin cefepime', 'abstract': 'Vancomycin plus piperacillin–tazobactam (VPT) is a commonly used antimicrobial regimen for septic patients. VPT is more nephrotoxic than other regimens such as vancomycin plus cefepime (VC) when given over several days. This risk of nephrotoxicity is less clear when VPT is given for initial empiric therapy in sepsis and de-escalated quickly based on evolving clinical information. The objective of this study was to assess nephrotoxicity among septic patients empirically treated with either VPT or VC at initial clinical presentation. We conducted a retrospective study of septic patients who received VPT or VC within 12 h of presentation to the emergency department. The primary outcomes were acute kidney injury (AKI) and renal recovery 72 h after presentation. For the total of 418 patients, 306 received VPT and 112 received VC. Rates of AKI at 72 h were 15.2% for VPT patients and 11.0% for VC patients [p\u2009=\u20090.44]. Among patients with AKI at presentation, 16.3% of VPT patients had AKI at 72 h compared to 8.9% of VC patients [p\u2009=\u20090.19]. Among those without AKI at presentation, 14.2% VPT patients and 16.7% VC patients had AKI at 72 h [p\u2009=\u20090.71]. Renal recovery rates for patients with AKI at presentation were 42.3% for VPT patients versus 40.3% for VC patients [p\u2009=\u20090.78]. In-hospital renal replacement therapy occurred in 6.2% VPT patients and 0.9% VC patients [p\u2009=\u20090.024]. Therefore, initial empiric therapy with VPT in sepsis may not confer increased risk of AKI when de-escalated appropriately.', 'corpus_id': 235338658, 'score': 0}, {'doc_id': '235717001', 'title': 'Clinical characteristics and outcomes of invasively ventilated patients with COVID-19 in Argentina (SATICOVID): a prospective, multicentre cohort study', 'abstract': '\n Background\n Although COVID-19 has greatly affected many low-income and middle-income countries, detailed information about patients admitted to the intensive care unit (ICU) is still scarce. Our aim was to examine ventilation characteristics and outcomes in invasively ventilated patients with COVID-19 in Argentina, an upper middle-income country.\n \n Methods\n In this prospective, multicentre cohort study (SATICOVID), we enrolled patients aged 18 years or older with RT-PCR-confirmed COVID-19 who were on invasive mechanical ventilation and admitted to one of 63 ICUs in Argentina. Patient demographics and clinical, laboratory, and general management variables were collected on day 1 (ICU admission); physiological respiratory and ventilation variables were collected on days 1, 3, and 7. The primary outcome was all-cause in-hospital mortality. All patients were followed until death in hospital or hospital discharge, whichever occurred first. Secondary outcomes were ICU mortality, identification of independent predictors of mortality, duration of invasive mechanical ventilation, and patterns of change in physiological respiratory and mechanical ventilation variables. The study is registered with ClinicalTrials.gov, NCT04611269, and is complete.\n \n Findings\n Between March 20, 2020, and Oct 31, 2020, we enrolled 1909 invasively ventilated patients with COVID-19, with a median age of 62 years [IQR 52–70]. 1294 (67·8%) were men, hypertension and obesity were the main comorbidities, and 939 (49·2%) patients required vasopressors. Lung-protective ventilation was widely used and median duration of ventilation was 13 days (IQR 7–22). Median tidal volume was 6·1 mL/kg predicted bodyweight (IQR 6·0–7·0) on day 1, and the value increased significantly up to day 7; positive end-expiratory pressure was 10 cm H2O (8–12) on day 1, with a slight but significant decrease to day 7. Ratio of partial pressure of arterial oxygen (PaO2) to fractional inspired oxygen (FiO2) was 160 (IQR 111–218), respiratory system compliance 36 mL/cm H2O (29–44), driving pressure 12 cm H2O (10–14), and FiO2 0·60 (0·45–0·80) on day 1. Acute respiratory distress syndrome developed in 1672 (87·6%) of patients; 1176 (61·6%) received prone positioning. In-hospital mortality was 57·7% (1101/1909 patients) and ICU mortality was 57·0% (1088/1909 patients); 462 (43·8%) patients died of refractory hypoxaemia, frequently overlapping with septic shock (n=174). Cox regression identified age (hazard ratio 1·02 [95% CI 1·01–1·03]), Charlson score (1·16 [1·11–1·23]), endotracheal intubation outside of the ICU (ie, before ICU admission; 1·37 [1·10–1·71]), vasopressor use on day 1 (1·29 [1·07–1·55]), D-dimer concentration (1·02 [1·01–1·03]), PaO2/FiO2 on day 1 (0·998 [0·997–0·999]), arterial pH on day 1 (1·01 [1·00–1·01]), driving pressure on day 1 (1·05 [1·03–1·08]), acute kidney injury (1·66 [1·36–2·03]), and month of admission (1·10 [1·03–1·18]) as independent predictors of mortality.\n \n Interpretation\n In patients with COVID-19 who required invasive mechanical ventilation, lung-protective ventilation was widely used but mortality was high. Predictors of mortality in our study broadly agreed with those identified in studies of invasively ventilated patients in high-income countries. The sustained burden of COVID-19 on scarce health-care personnel might have contributed to high mortality over the course of our study in Argentina. These data might help to identify points for improvement in the management of patients in middle-income countries and elsewhere.\n \n Funding\n None.\n \n Translation\n For the Spanish translation of the Summary see Supplementary Materials section.\n', 'corpus_id': 235717001, 'score': 0}]
159	{'doc_id': '211205110', 'title': 'Bayesian Deep Learning and a Probabilistic Perspective of Generalization', 'abstract': 'The key distinguishing property of a Bayesian approach is marginalization, rather than using a single setting of weights. Bayesian marginalization can particularly improve the accuracy and calibration of modern deep neural networks, which are typically underspecified by the data, and can represent many compelling but different solutions. We show that deep ensembles provide an effective mechanism for approximate Bayesian marginalization, and propose a related approach that further improves the predictive distribution by marginalizing within basins of attraction, without significant overhead. We also investigate the prior over functions implied by a vague distribution over neural network weights, explaining the generalization properties of such models from a probabilistic perspective. From this perspective, we explain results that have been presented as mysterious and distinct to neural network generalization, such as the ability to fit images with random labels, and show that these results can be reproduced with Gaussian processes. We also show that Bayesian model averaging alleviates double descent, resulting in monotonic performance improvements with increased flexibility. Finally, we provide a Bayesian perspective on tempering for calibrating predictive distributions.', 'corpus_id': 211205110}	6078	[{'doc_id': '211117443', 'title': 'Memory-Based Graph Networks', 'abstract': 'Graph Neural Networks (GNNs) are a class of deep models that operates on data with arbitrary topology and order-invariant structure represented as graphs. We introduce an efficient memory layer for GNNs that can learn to jointly perform graph representation learning and graph pooling. We also introduce two new networks based on our memory layer: Memory-Based Graph Neural Network (MemGNN) and Graph Memory Network (GMN) that can learn hierarchical graph representations by coarsening the graph throughout the layers of memory. The experimental results demonstrate that the proposed models achieve state-of-the-art results in six out of seven graph classification and regression benchmarks. We also show that the learned representations could correspond to chemical features in the molecule data.', 'corpus_id': 211117443, 'score': 1}, {'doc_id': '221836090', 'title': 'Evolutionary Architecture Search for Graph Neural Networks', 'abstract': 'Automated machine learning (AutoML) has seen a resurgence in interest with the boom of deep learning over the past decade. In particular, Neural Architecture Search (NAS) has seen significant attention throughout the AutoML research community, and has pushed forward the state-of-the-art in a number of neural models to address grid-like data such as texts and images. However, very litter work has been done about Graph Neural Networks (GNN) learning on unstructured network data. Given the huge number of choices and combinations of components such as aggregator and activation function, determining the suitable GNN structure for a specific problem normally necessitates tremendous expert knowledge and laborious trails. In addition, the slight variation of hyper parameters such as learning rate and dropout rate could dramatically hurt the learning capacity of GNN. In this paper, we propose a novel AutoML framework through the evolution of individual models in a large GNN architecture space involving both neural structures and learning parameters. Instead of optimizing only the model structures with fixed parameter settings as existing work, an alternating evolution process is performed between GNN structures and learning parameters to dynamically find the best fit of each other. To the best of our knowledge, this is the first work to introduce and evaluate evolutionary architecture search for GNN models. Experiments and validations demonstrate that evolutionary NAS is capable of matching existing state-of-the-art reinforcement learning approaches for both the semi-supervised transductive and inductive node representation learning and classification.', 'corpus_id': 221836090, 'score': 1}, {'doc_id': '222209152', 'title': 'Simplicial Neural Networks', 'abstract': 'We present simplicial neural networks (SNNs), a generalization of graph neural networks to data that live on a class of topological spaces called simplicial complexes. These are natural multi-dimensional extensions of graphs that encode not only pairwise relationships but also higher-order interactions between vertices - allowing us to consider richer data, including vector fields and $n$-fold collaboration networks. We define an appropriate notion of convolution that we leverage to construct the desired convolutional neural networks. We test the SNNs on the task of imputing missing data on coauthorship complexes.', 'corpus_id': 222209152, 'score': 1}, {'doc_id': '219303563', 'title': 'Weight Pruning via Adaptive Sparsity Loss', 'abstract': 'Pruning neural networks has regained interest in recent years as a means to compress state-of-the-art deep neural networks and enable their deployment on resource-constrained devices. In this paper, we propose a robust compressive learning framework that efficiently prunes network parameters during training with minimal computational overhead. We incorporate fast mechanisms to prune individual layers and build upon these to automatically prune the entire network under a user-defined budget constraint. Key to our end-to-end network pruning approach is the formulation of an intuitive and easy-to-implement adaptive sparsity loss that is used to explicitly control sparsity during training, enabling efficient budget-aware optimization. Extensive experiments demonstrate the effectiveness of the proposed framework for image classification on the CIFAR and ImageNet datasets using different architectures, including AlexNet, ResNets and Wide ResNets.', 'corpus_id': 219303563, 'score': 1}, {'doc_id': '220514221', 'title': 'Alpha-Net: Architecture, Models, and Applications', 'abstract': 'Deep learning network training is usually computationally expensive and intuitively complex. We present a novel network architecture for custom training and weight evaluations. We reformulate the layers as ResNet-similar blocks with certain inputs and outputs of their own, the blocks (called Alpha blocks) on their connection configuration form their own network, combined with our novel loss function and normalization function form the complete Alpha-Net architecture. We provided the empirical mathematical formulation of network loss function for more understanding of accuracy estimation and further optimizations. We implemented Alpha-Net with 4 different layer configurations to express the architecture behavior comprehensively. On a custom dataset based on ImageNet benchmark, we evaluate Alpha-Net v1, v2, v3, and v4 for image recognition to give the accuracy of 78.2%, 79.1%, 79.5%, and 78.3% respectively. The Alpha-Net v3 gives improved accuracy of approx. 3% over the last state-of-the-art network ResNet 50 on ImageNet benchmark. We also present an analysis of our dataset with 256, 512, and 1024 layers and different versions of the loss function. Input representation is also crucial for training as initial preprocessing will take only a handful of features to make training less complex than it needs to be. We also compared network behavior with different layer structures, different loss functions, and different normalization functions for better quantitative modeling of Alpha-Net.', 'corpus_id': 220514221, 'score': 0}, {'doc_id': '218523571', 'title': 'Probabilistic Logic Graph Attention Networks for Reasoning', 'abstract': 'Knowledge base completion, which involves the prediction of missing relations between entities in a knowledge graph, has been an active area of research. Markov logic networks, which combine probabilistic graphical models and first order logic, have proven to be effective on knowledge graph tasks like link prediction and question answering. However, their intractable inference limits their scalability and wider applicability across various tasks. In recent times, graph attention neural networks, which capture features of neighbouring entities, have achieved superior results on highly complex graph problems like node classification and link prediction. Combining the best of both worlds, we propose Probabilistic Logic Graph Attention Network (pGAT) for reasoning. In the proposed model, the joint distribution of all possible triplets defined by a Markov logic network is optimized with a variational EM algorithm. This helps us to efficiently combine first-order logic and graph attention networks. With the goal of establishing strong baselines for future research on link prediction, we evaluate our model on various standard link prediction benchmarks, and obtain competitive results.', 'corpus_id': 218523571, 'score': 1}, {'doc_id': '218863030', 'title': 'PruneNet: Channel Pruning via Global Importance', 'abstract': 'Channel pruning is one of the predominant approaches for accelerating deep neural networks. Most existing pruning methods either train from scratch with a sparsity inducing term such as group lasso, or prune redundant channels in a pretrained network and then fine tune the network. Both strategies suffer from some limitations: the use of group lasso is computationally expensive, difficult to converge and often suffers from worse behavior due to the regularization bias. The methods that start with a pretrained network either prune channels uniformly across the layers or prune channels based on the basic statistics of the network parameters. These approaches either ignore the fact that some CNN layers are more redundant than others or fail to adequately identify the level of redundancy in different layers. In this work, we investigate a simple-yet-effective method for pruning channels based on a computationally light-weight yet effective data driven optimization step that discovers the necessary width per layer. Experiments conducted on ILSVRC-$12$ confirm effectiveness of our approach. With non-uniform pruning across the layers on ResNet-$50$, we are able to match the FLOP reduction of state-of-the-art channel pruning results while achieving a $0.98\\%$ higher accuracy. Further, we show that our pruned ResNet-$50$ network outperforms ResNet-$34$ and ResNet-$18$ networks, and that our pruned ResNet-$101$ outperforms ResNet-$50$.', 'corpus_id': 218863030, 'score': 0}, {'doc_id': '232404613', 'title': 'Graph Classification by Mixture of Diverse Experts', 'abstract': 'Graph classification is a challenging research problem in many applications across a broad range of domains. In these applications, it is very common that class distribution is imbalanced. Recently, Graph Neural Network (GNN) models have achieved superior performance on various real-world datasets. Despite their success, most of current GNN models largely overlook the important setting of imbalanced class distribution, which typically results in prediction bias towards majority classes. To alleviate the prediction bias, we propose to leverage semantic structure of dataset based on the distribution of node embedding. Specifically, we present GraphDIVE, a general framework leveraging mixture of diverse experts (i.e., graph classifiers) for imbalanced graph classification. With a divide-andconquer principle, GraphDIVE employs a gating network to partition an imbalanced graph dataset into several subsets. Then each expert network is trained based on its corresponding subset. Experiments on real-world imbalanced graph datasets demonstrate the effectiveness of GraphDIVE.', 'corpus_id': 232404613, 'score': 1}, {'doc_id': '219636421', 'title': 'Tangent Space Sensitivity and Distribution of Linear Regions in ReLU Networks', 'abstract': 'Recent articles indicate that deep neural networks are efficient models for various learning problems. However they are often highly sensitive to various changes that cannot be detected by an independent observer. As our understanding of deep neural networks with traditional generalization bounds still remains incomplete, there are several measures which capture the behaviour of the model in case of small changes at a specific state. In this paper we consider adversarial stability in the tangent space and suggest tangent sensitivity in order to characterize stability. We focus on a particular kind of stability with respect to changes in parameters that are induced by individual examples without known labels. We derive several easily computable bounds and empirical measures for feed-forward fully connected ReLU (Rectified Linear Unit) networks and connect tangent sensitivity to the distribution of the activation regions in the input space realized by the network. Our experiments suggest that even simple bounds and measures are associated with the empirical generalization gap.', 'corpus_id': 219636421, 'score': 0}, {'doc_id': '218581509', 'title': 'Compact Neural Representation Using Attentive Network Pruning', 'abstract': 'Deep neural networks have evolved to become power demanding and consequently difficult to apply to small-size mobile platforms. Network parameter reduction methods have been introduced to systematically deal with the computational and memory complexity of deep networks. We propose to examine the ability of attentive connection pruning to deal with redundancy reduction in neural networks as a contribution to the reduction of computational demand. In this work, we describe a Top-Down attention mechanism that is added to a Bottom-Up feedforward network to select important connections and subsequently prune redundant ones at all parametric layers. Our method not only introduces a novel hierarchical selection mechanism as the basis of pruning but also remains competitive with previous baseline methods in the experimental evaluation. We conduct experiments using different network architectures on popular benchmark datasets to show high compression ratio is achievable with negligible loss of accuracy.', 'corpus_id': 218581509, 'score': 0}]
160	{'doc_id': '232320804', 'title': 'Decomposing Normal and Abnormal Features of Medical Images into Discrete Latent Codes for Content-Based Image Retrieval', 'abstract': 'In medical imaging, the characteristics purely derived from a disease should reflect the extent to which abnormal findings deviate from the normal features. Indeed, physicians often need corresponding images without abnormal findings of interest or, conversely, images that contain similar abnormal findings regardless of normal anatomical context. This is called comparative diagnostic reading of medical images, which is essential for a correct diagnosis. To support comparative diagnostic reading, content-based image retrieval (CBIR), which can selectively utilize normal and abnormal features in medical images as two separable semantic components, will be useful. Therefore, we propose a neural network architecture to decompose the semantic components of medical images into two latent codes: normal anatomy code and abnormal anatomy code. The normal anatomy code represents normal anatomies that should have existed if the sample is healthy, whereas the abnormal anatomy code attributes to abnormal changes that reflect deviation from the normal baseline. These latent codes are discretized through vector quantization to enable binary hashing, which can reduce the computational burden at the time of similarity search. By calculating the similarity based on either normal or abnormal anatomy codes or the combination of the two codes, our algorithm can retrieve images according to the selected semantic component from a dataset consisting of brain magnetic resonance images of gliomas. Our CBIR system qualitatively and quantitatively achieves remarkable results.', 'corpus_id': 232320804}	12683	"[{'doc_id': '233581147', 'title': 'A comparative analysis of automatic deep neural networks for image retrieval', 'abstract': 'Feature descriptor and similarity measures are the two core components in content-based image retrieval and crucial issues due to “semantic gap” between human conceptual meaning and a machine low-level feature. Recently, deep learning techniques have shown a great interest in image recognition especially in extracting features information about the images. In this paper, we investigated, compared, and evaluated different deep convolutional neural networks and their applications for image classification and automatic image retrieval. The approaches are: simple convolutional neural network, AlexNet, GoogleNet, ResNet-50, Vgg-16, and Vgg-19. We compared the performance of the different approaches to prior works in this domain by using known accuracy metrics and analyzed the differences between the approaches. The performances of these approaches are investigated using public image datasets corel 1K, corel 10K, and Caltech 256. Hence, we deduced that GoogleNet approach yields the best overall results. In addition, we investigated and compared different similarity measures. Based on exhausted mentioned investigations, we developed a novel algorithm for image retrieval.', 'corpus_id': 233581147, 'score': 1}, {'doc_id': '235235362', 'title': 'Variable-Length Metric Learning for Fast Image Retrieval', 'abstract': 'Learning a powerful distance metric is the key component of image retrieval. Recently, deep metric learning has been an active research topic for image retrieval. However, most existing metric learning approaches treat all the input images equally and learn all image embeddings at equal lengths. These methods ignore those easy examples that can be encoded as the shorter features, which is search-inefficient. We propose a simple but efficient variable-length metric learning method for search efficiency, in which the different query samples have different feature-length. First, we propose to learn the ranked (prioritized) list of features. The more distinguishing features, the higher the rank. We show that the proposed prioritized features can be used to perform fast retrieval in different feature-length configurations. Further, we propose an adaptive feature-length selection policy to determine the amount of feature-length for each query sample. Extensive experiments are conducted on three benchmark datasets. The results demonstrate that the proposed method can reduce computational costs without incurring a decrease in accuracy.', 'corpus_id': 235235362, 'score': 1}, {'doc_id': '232765168', 'title': 'Fully automated segmentation of brain tumor from multiparametric MRI using 3D context U-Net with deep supervision', 'abstract': 'Gliomas are very heterogenous set of tumors that grow within the substance of brain and often mix with normal brain tissues. Due to its histologic complexity and irregular shapes, multiparametric magnetic resonance imaging is used to accurately diagnose brain tumor and their subregions. Current practice requires physicians to manually segment these regions on a large image dataset, which can be a very time consuming and complicated task especially with large variations among different tumor regions. Automatic segmentation of brain tumors in multimodal MRI holds a great potential in developing an effective treatment plan and improving brain tumor radiotherapy workflow. Despite continuous investigations on DL-based brain tumor segmentation, irregular shapes and histologic complexities of brain tumors introduces a major challenge in developing an effective automatic segmentation method. In this study, we develop a novel context U-net with deep supervision to segment both the whole brain tumor and their subregions of tumors. The context module was formed by an inception-like structure to extract more information regarding the brain tumors. The deep supervision in the encoder path was achieved by adding up the segmentation outputs at different levels of the network. We evaluated our method on Brain Tumor Segmentation Challenge (BraTS) 2019 training dataset in which 80% was used for training while the remaining 20% was used for performance testing. Our method achieved Dice similarity coefficients (DSC) were 0.8693, 0.8013 and 0.7782 for the whole tumor (WT), tumor core (TC) and enhancing tumor (ET), respectively. The results attained by our proposed network suggested that this technique could be used for segmentation of brain tumors and their subregions to facilitate the brain tumor radiotherapy workflow.', 'corpus_id': 232765168, 'score': 0}, {'doc_id': '232076039', 'title': 'Medical Image Segmentation With Limited Supervision: A Review of Deep Network Models', 'abstract': 'Despite the remarkable performance of deep learning methods on various tasks, most cutting-edge models rely heavily on large-scale annotated training examples, which are often unavailable for clinical and health care tasks. The labeling costs for medical images are very high, especially in medical image segmentation, which typically requires intensive pixel/voxel-wise labeling. Therefore, the strong capability of learning and generalizing from limited supervision, including a limited amount of annotations, sparse annotations, and inaccurate annotations, is crucial for the successful application of deep learning models in medical image segmentation. However, due to its intrinsic difficulty, segmentation with limited supervision is challenging and specific model design and/or learning strategies are needed. In this paper, we provide a systematic and up-to-date review of the solutions above, with summaries and comments about the methodologies. We also highlight several problems in this field, discussed future directions observing further investigations.', 'corpus_id': 232076039, 'score': 0}, {'doc_id': '233289851', 'title': 'Histopathology WSI Encoding based on GCNs for Scalable and Efficient Retrieval of Diagnostically Relevant Regions', 'abstract': 'Content-based histopathological image retrieval (CBHIR) has become popular in recent years in the domain of histopathological image analysis. CBHIR systems provide auxiliary diagnosis information for pathologists by searching for and returning regions that are contently similar to the region of interest (ROI) from a pre-established database. While, it is challenging and yet significant in clinical applications to retrieve diagnostically relevant regions from a database that consists of histopathological whole slide images (WSIs) for a query ROI. In this paper, we propose a novel framework for regions retrieval from WSI-database based on hierarchical graph convolutional networks (GCNs) and Hash technique. Compared to the present CBHIR framework, the structural information of WSI is preserved through graph embedding of GCNs, which makes the retrieval framework more sensitive to regions that are similar in tissue distribution. Moreover, benefited from the hierarchical GCN structures, the proposed framework has good scalability for both the size and shape variation of ROIs. It allows the pathologist defining query regions using free curves according to the appearance of tissue. Thirdly, the retrieval is achieved based on Hash technique, which ensures the framework is efficient and thereby adequate for practical large-scale WSI-database. The proposed method was validated on two public datasets for histopathological WSI analysis and compared to the state-of-the-art methods. The proposed method achieved mean average precision above 0.857 on the ACDC-LungHP dataset and above 0.864 on the Camelyon16 dataset in the irregular region retrieval tasks, which are superior to the state-of-the-art methods. The average retrieval time from a database within 120 WSIs is 0.802 ms.', 'corpus_id': 233289851, 'score': 1}, {'doc_id': '234810779', 'title': 'Content Based Medical Image Retrieval Based on Salient Regions Combined with Deep Learning', 'abstract': 'In traditional text based medical image retrieval system, it is hard to find visually similar images in large medical image database. Content-based image retrieval is developed to retrieve similar images and it is based on visual attributes describing the content of an image. Developing a method for content based medical image retrieval is a challenging task. This paper proposed a new method for content-based medical image retrieval based on salient regions and deep learning. The proposed method includes two stages: an offline task to extract local object features and an online task for content-based image retrieval in database. In first stage, we extract local object features of medical image depending on shape, texture and intensity, and features extracted by deep learning applied in saliency of decomposition. Secondly, we make online task for content-based image retrieval in database. The user gives query image as an input and the system will retrieve n top most similar images by similarity comparison with bag of code words feature values obtained in the first stage. Evaluation of the proposed method is based on Precision and Recall values. Our dataset includes 5 groups of medical images with their quality varying from low to high. With the best medical image quality group, the accuray can be 91.61% for Precision and 89.61% for Recall. Comparing the average values with others methods, the results of the proposed method are more than 2 to 5% better.', 'corpus_id': 234810779, 'score': 1}, {'doc_id': '234487606', 'title': 'Shape retrieval using angle-wise contour variance', 'abstract': 'Abstract In this study, we propose a geometric feature set for 2D shape retrieval. Conventional Hough feature gives the edge locations along with angle and creates Hough table if there are multiple intersections at borders. In this paper, a statistical way to represent the relation of repeating contours at each angle around the shape centroid is presented. The main contribution of this paper is to use the standard deviation of repeating contours. We calculate the angle between the shape centroid and each point on the contour. For each integer angle value, three features were extracted: the number of contour repetitions, the average distance of the points at that angle to the centroid, and the standard deviation of the points at the same angle. Thus, a 2D image was represented by a constant sized matrix, regardless of its size. In the case of similarity between two images, instead of merging features within a single expression, the algorithm picked the feature with the highest similarity rate for that comparison. We tested the proposed method on MPEG-7, Kimia99, ETH-80 datasets for a benchmark with the state-of-the-art. It outperformed most of the recent methods in terms of retrieval rate.', 'corpus_id': 234487606, 'score': 0}, {'doc_id': '232085715', 'title': 'Deep learning saliency maps do not accurately highlight diagnostically relevant regions for medical image interpretation', 'abstract': 'Deep learning has enabled automated medical image interpretation at a level often surpassing that of practicing medical experts. However, many clinical practices have cited a lack of model interpretability as reason to delay the use of ""black-box"" deep neural networks in clinical workflows. Saliency maps, which ""explain"" a model\'s decision by producing heat maps that highlight the areas of the medical image that influence model prediction, are often presented to clinicians as an aid in diagnostic decision-making. In this work, we demonstrate that the most commonly used saliency map generating method, Grad-CAM, results in low performance for 10 pathologies on chest X-rays. We examined under what clinical conditions saliency maps might be more dangerous to use compared to human experts, and found that Grad-CAM performs worse for pathologies that had multiple instances, were smaller in size, and had shapes that were more complex. Moreover, we showed that model confidence was positively correlated with Grad-CAM localization performance, suggesting that saliency maps were safer for clinicians to use as a decision aid when the model had made a positive prediction with high confidence. Our work demonstrates that several important limitations of interpretability techniques for medical imaging must be addressed before use in clinical workflows.', 'corpus_id': 232085715, 'score': 1}, {'doc_id': '231951523', 'title': 'Hierarchical Similarity Learning for Language-Based Product Image Retrieval', 'abstract': 'This paper aims for the language-based product image retrieval task. The majority of previous works have made significant progress by designing network structure, similarity measurement, and loss function. However, they typically perform vision-text matching at certain granularity regardless of the intrinsic multiple granularities of images. In this paper, we focus on the cross-modal similarity measurement, and propose a novel Hierarchical Similarity Learning (HSL) network. HSL first learns multi-level representations of input data by stacked encoders, and object-granularity similarity and image-granularity similarity are computed at each level. All the similarities are combined as the final hierarchical cross-modal similarity. Experiments on a large-scale product retrieval dataset demonstrate the effectiveness of our proposed method. Code and data are available at https://github.com/liufh1/hsl.', 'corpus_id': 231951523, 'score': 0}, {'doc_id': '234499683', 'title': 'Double U-Nets for Image Segmentation by Integrating the Region and Boundary Information', 'abstract': 'The existing CNN-based segmentation methods use the object regions alone as the labels to train their networks, and the potentially useful boundaries annotated by radiologists are not used directly during the training. Thus, we proposed a framework of double U-Nets to integrate object regions and boundaries for more accurate segmentation. The proposed network consisted of a down-sampling path followed by two symmetric up-sampling paths. The down-sampling path learned the low-level features of regions and boundaries, and two up-sampling paths learned the high-level features of regions and boundaries, respectively. The outputs from the down-sampling path were concatenated with the corresponding ones from two up-sampling paths by skip connections. The outputs of double U-Nets were the predicted probability images of object regions and boundaries, and they were integrated to calculate the dice loss with the corresponding labels. The proposed double U-Nets were evaluated on two datasets: 247 radiographs for the segmentation of lungs, hearts, and clavicles, and 284 radiographs for the segmentation of pelvises. Compared with the baseline U-Net, our double U-Nets improved the mean dices and reduced the 90% Hausdorff distances for the “difficult” objects (lower lungs, clavicles, and pelvises), and the integration of “difficult” object regions and boundaries can improve the segmentation results compared with the use of object regions alone. However, for the “easy” objects (entire lungs and hearts) or “very difficult” objects (pelvises in lateral and implanted images), the integration did not improve the segmentation performance.', 'corpus_id': 234499683, 'score': 0}]"
161	"{'doc_id': '219687659', 'title': 'Symbolic Logic meets Machine Learning: A Brief Survey in Infinite Domains', 'abstract': 'The tension between deduction and induction is perhaps the most fundamental issue in areas such as philosophy, cognition and artificial intelligence (AI). The deduction camp concerns itself with questions about the expressiveness of formal languages for capturing knowledge about the world, together with proof systems for reasoning from such knowledge bases. The learning camp attempts to generalize from examples about partial descriptions about the world. In AI, historically, these camps have loosely divided the development of the field, but advances in cross-over areas such as statistical relational learning, neuro-symbolic systems, and high-level control have illustrated that the dichotomy is not very constructive, and perhaps even ill-formed. In this article, we survey work that provides further evidence for the connections between logic and learning. Our narrative is structured in terms of three strands: logic versus learning, machine learning for logic, and logic for machine learning, but naturally, there is considerable overlap. We place an emphasis on the following ""sore"" point: there is a common misconception that logic is for discrete properties, whereas probability theory and machine learning, more generally, is for continuous properties. We report on results that challenge this view on the limitations of logic, and expose the role that logic can play for learning in infinite domains.', 'corpus_id': 219687659}"	5226	"[{'doc_id': '219573265', 'title': 'Closed Loop Neural-Symbolic Learning via Integrating Neural Perception, Grammar Parsing, and Symbolic Reasoning', 'abstract': 'The goal of neural-symbolic computation is to integrate the connectionist and symbolist paradigms. Prior methods learn the neural-symbolic models using reinforcement learning (RL) approaches, which ignore the error propagation in the symbolic reasoning module and thus converge slowly with sparse rewards. In this paper, we address these issues and close the loop of neural-symbolic learning by (1) introducing the \\textbf{grammar} model as a \\textit{symbolic prior} to bridge neural perception and symbolic reasoning, and (2) proposing a novel \\textbf{back-search} algorithm which mimics the top-down human-like learning procedure to propagate the error through the symbolic reasoning module efficiently. We further interpret the proposed learning framework as maximum likelihood estimation using Markov chain Monte Carlo sampling and the back-search algorithm as a Metropolis-Hastings sampler. The experiments are conducted on two weakly-supervised neural-symbolic tasks: (1) handwritten formula recognition on the newly introduced HWF dataset; (2) visual question answering on the CLEVR dataset. The results show that our approach significantly outperforms the RL methods in terms of performance, converging speed, and data efficiency. Our code and data are released at \\url{this https URL}.', 'corpus_id': 219573265, 'score': 1}, {'doc_id': '215786542', 'title': 'Learning Structured Embeddings of Knowledge Graphs with Adversarial Learning Framework', 'abstract': 'Many large-scale knowledge graphs are now available and ready to provide semantically structured information that is regarded as an important resource for question answering and decision support tasks. However, they are built on rigid symbolic frameworks which makes them hard to be used in other intelligent systems. We present a learning method using generative adversarial architecture designed to embed the entities and relations of the knowledge graphs into a continuous vector space. A generative network (GN) takes two elements of a (subject, predicate, object) triple as input and generates the vector representation of the missing element. A discriminative network (DN) scores a triple to distinguish a positive triple from those generated by GN. The training goal for GN is to deceive DN to make wrong classification. When arriving at a convergence, GN recovers the training data and can be used for knowledge graph completion, while DN is trained to be a good triple classifier. Unlike few previous studies based on generative adversarial architectures, our GN is able to generate unseen instances while they just use GN to better choose negative samples (already existed) for DN. Experiments demonstrate our method can improve classical relational learning models (e.g.TransE) with a significant margin on both the link prediction and triple classification tasks.', 'corpus_id': 215786542, 'score': 0}, {'doc_id': '220769010', 'title': 'A Survey on Graph Neural Networks for Knowledge Graph Completion', 'abstract': 'Knowledge Graphs are increasingly becoming popular for a variety of downstream tasks like Question Answering and Information Retrieval. However, the Knowledge Graphs are often incomplete, thus leading to poor performance. As a result, there has been a lot of interest in the task of Knowledge Base Completion. More recently, Graph Neural Networks have been used to capture structural information inherently stored in these Knowledge Graphs and have been shown to achieve SOTA performance across a variety of datasets. In this survey, we understand the various strengths and weaknesses of the proposed methodology and try to find new exciting research problems in this area that require further investigation.', 'corpus_id': 220769010, 'score': 1}, {'doc_id': '215745174', 'title': 'Pre-training Text Representations as Meta Learning', 'abstract': ""Pre-training text representations has recently been shown to significantly improve the state-of-the-art in many natural language processing tasks. The central goal of pre-training is to learn text representations that are useful for subsequent tasks. However, existing approaches are optimized by minimizing a proxy objective, such as the negative log likelihood of language modeling. In this work, we introduce a learning algorithm which directly optimizes model's ability to learn text representations for effective learning of downstream tasks. We show that there is an intrinsic connection between multi-task pre-training and model-agnostic meta-learning with a sequence of meta-train steps. The standard multi-task learning objective adopted in BERT is a special case of our learning algorithm where the depth of meta-train is zero. We study the problem in two settings: unsupervised pre-training and supervised pre-training with different pre-training objects to verify the generality of our approach.Experimental results show that our algorithm brings improvements and learns better initializations for a variety of downstream tasks."", 'corpus_id': 215745174, 'score': 1}, {'doc_id': '6180068', 'title': 'Lifted Relational Neural Networks', 'abstract': 'We propose a method combining relational-logic representations with neural network learning. A general lifted architecture, possibly reflecting some background domain knowledge, is described through relational rules which may be handcrafted or learned. The relational rule-set serves as a template for unfolding possibly deep neural networks whose structures also reflect the structures of given training or testing relational examples. Different networks corresponding to different examples share their weights, which co-evolve during training by stochastic gradient descent algorithm. The framework allows for hierarchical relational modeling constructs and learning of latent relational concepts through shared hidden layers weights corresponding to the rules. Discovery of notable relational concepts and experiments on 78 relational learning benchmarks demonstrate favorable performance of the method.', 'corpus_id': 6180068, 'score': 1}, {'doc_id': '221246405', 'title': 'Neural Logic Reasoning', 'abstract': 'Recent years have witnessed the success of deep neural networks in many research areas. The fundamental idea behind the design of most neural networks is to learn similarity patterns from data for prediction and inference, which lacks the ability of cognitive reasoning. However, the concrete ability of reasoning is critical to many theoretical and practical problems. On the other hand, traditional symbolic reasoning methods do well in making logical inference, but they are mostly hard rule-based reasoning, which limits their generalization ability to different tasks since difference tasks may require different rules. Both reasoning and generalization ability are important for prediction tasks such as recommender systems, where reasoning provides strong connection between user history and target items for accurate prediction, and generalization helps the model to draw a robust user portrait over noisy inputs. In this paper, we propose Logic-Integrated Neural Network (LINN) to integrate the power of deep learning and logic reasoning. LINN is a dynamic neural architecture that builds the computational graph according to input logical expressions. It learns basic logical operations such as AND, OR, NOT as neural modules, and conducts propositional logical reasoning through the network for inference. Experiments on theoretical task show that LINN achieves significant performance on solving logical equations and variables. Furthermore, we test our approach on the practical task of recommendation by formulating the task into a logical inference problem. Experiments show that LINN significantly outperforms state-of-the-art recommendation models in Top-K recommendation, which verifies the potential of LINN in practice.', 'corpus_id': 221246405, 'score': 1}, {'doc_id': '212737183', 'title': 'Toward Interpretable Machine Learning: Transparent Deep Neural Networks and Beyond', 'abstract': 'With the broader and highly successful usage of machine learning in industry and the sciences, there has been a growing demand for explainable AI. Interpretability and explanation methods for gaining a better understanding about the problem solving abilities and strategies of nonlinear Machine Learning such as Deep Learning (DL), LSTMs, and kernel methods are therefore receiving increased attention. In this work we aim to (1) provide a timely overview of this active emerging field and explain its theoretical foundations, (2) put interpretability algorithms to a test both from a theory and comparative evaluation perspective using extensive simulations, (3) outline best practice aspects i.e. how to best include interpretation methods into the standard usage of machine learning and (4) demonstrate successful usage of explainable AI in a representative selection of application scenarios. Finally, we discuss challenges and possible future directions of this exciting foundational field of machine learning.', 'corpus_id': 212737183, 'score': 0}, {'doc_id': '220042223', 'title': 'Benchmark and Best Practices for Biomedical Knowledge Graph Embeddings', 'abstract': 'Much of biomedical and healthcare data is encoded in discrete, symbolic form such as text and medical codes. There is a wealth of expert-curated biomedical domain knowledge stored in knowledge bases and ontologies, but the lack of reliable methods for learning knowledge representation has limited their usefulness in machine learning applications. While text-based representation learning has significantly improved in recent years through advances in natural language processing, attempts to learn biomedical concept embeddings so far have been lacking. A recent family of models called knowledge graph embeddings have shown promising results on general domain knowledge graphs, and we explore their capabilities in the biomedical domain. We train several state-of-the-art knowledge graph embedding models on the SNOMED-CT knowledge graph, provide a benchmark with comparison to existing methods and in-depth discussion on best practices, and make a case for the importance of leveraging the multi-relational nature of knowledge graphs for learning biomedical knowledge representation. The embeddings, code, and materials will be made available to the community.', 'corpus_id': 220042223, 'score': 1}, {'doc_id': '215744806', 'title': 'AMR Parsing via Graph-Sequence Iterative Inference', 'abstract': 'We propose a new end-to-end model that treats AMR parsing as a series of dual decisions on the input sequence and the incrementally constructed graph. At each time step, our model performs multiple rounds of attention, reasoning, and composition that aim to answer two critical questions: (1) which part of the input sequence to abstract; and (2) where in the output graph to construct the new concept. We show that the answers to these two questions are mutually causalities. We design a model based on iterative inference that helps achieve better answers in both perspectives, leading to greatly improved parsing accuracy. Our experimental results significantly outperform all previously reported Smatch scores by large margins. Remarkably, without the help of any large-scale pre-trained language model (e.g., BERT), our model already surpasses previous state-of-the-art using BERT. With the help of BERT, we can push the state-of-the-art results to 80.2% on LDC2017T10 (AMR 2.0) and 75.4% on LDC2014T12 (AMR 1.0).', 'corpus_id': 215744806, 'score': 0}, {'doc_id': '218470290', 'title': 'Knowledge Base Inference for Regular Expression Queries', 'abstract': 'Two common types of tasks on Knowledge Bases have been studied -- single link prediction (Knowledge Base Completion) and path query answering. However, our analysis of user queries on a real-world knowledge base reveals that a significant fraction of queries specify paths using regular expressions(regex). Such regex queries cannot be handled by any of the existing link prediction or path query answering models. In response, we present Regex Query Answering, the novel task of answering regex queries on incomplete KBs. We contribute two datasets for the task, including one where test queries are harvested from actual user querylogs. We train baseline neural models for our new task and propose novel ways to handle disjunction and Kleene plus regex operators.', 'corpus_id': 218470290, 'score': 0}]"
162	"{'doc_id': '5159462', 'title': 'Predictive State Representations: A New Theory for Modeling Dynamical Systems', 'abstract': ""Modeling dynamical systems, both for control purposes and to make predictions about their behavior, is ubiquitous in science and engineering. Predictive state representations (PSRs) are a recently introduced class of models for discrete-time dynamical systems. The key idea behind PSRs and the closely related OOMs (Jaeger's observable operator models) is to represent the state of the system as a set of predictions of observable outcomes of experiments one can do in the system. This makes PSRs rather different from history-based models such as nth-order Markov models and hidden-state-based models such as HMMs and POMDPs. We introduce an interesting construct, the system-dynamics matrix, and show how PSRs can be derived simply from it. We also use this construct to show formally that PSRs are more general than both nth-order Markov models and HMMs/POMDPs. Finally, we discuss the main difference between PSRs and OOMs and conclude with directions for future work."", 'corpus_id': 5159462}"	2774	[{'doc_id': '219956112', 'title': 'Bypassing Gradients Re-Projection with Episodic Memories in Online Continual Learning', 'abstract': 'The use of episodic memories in continual learning is an efficient way to prevent the phenomenon of catastrophic forgetting. In recent studies, several gradient-based approaches have been developed to make more efficient use of compact episodic memories, which constrain the gradients resulting from new samples with gradients from memorized samples. In this paper, we propose a method for decreasing the diversity of gradients through an auxiliary optimization objective that we call Discriminative Representation Loss, instead of directly re-projecting the gradients. Our methods show promising performance with relatively cheap computational cost on several benchmark experiments.', 'corpus_id': 219956112, 'score': 1}, {'doc_id': '233241236', 'title': 'Memory Capacity of Neural Turing Machines with Matrix Representation', 'abstract': 'It is well known that recurrent neural networks (RNNs) faced limitations in learning longterm dependencies that have been addressed by memory structures in long short-term memory (LSTM) networks. Matrix neural networks feature matrix representation which inherently preserves the spatial structure of data and has the potential to provide better memory structures when compared to canonical neural networks that use vector representation. Neural Turing machines (NTMs) are novel RNNs that implement notion of programmable computers with neural network controllers to feature algorithms that have copying, sorting, and associative recall tasks. In this paper, we study augmentation of memory capacity with matrix representation of RNNs and NTMs (MatNTMs). We investigate if matrix representation has a better memory capacity than the vector representations in conventional neural networks. We use a probabilistic model of the memory capacity using Fisher information and investigate how the memory capacity for matrix representation networks are limited under various constraints, and in general, without any constraints. In the case of memory capacity without any constraints, we found that the upper bound on memory capacity to be N for an N×N state matrix. The results from our experiments using synthetic algorithmic tasks show that MatNTMs have a better learning capacity when compared to its counterparts.', 'corpus_id': 233241236, 'score': 0}, {'doc_id': '44430682', 'title': 'A probabilistic interpretation of replicator-mutator dynamics', 'abstract': 'In this note, we investigate the relationship between probabilistic updating mechanisms and discrete-time replicator-mutator dynamics. We consider the recently shown connection between Bayesian updating and replicator dynamics and extend it to the replicator-mutator dynamics by considering prediction and filtering recursions in hidden Markov models (HMM). We show that it is possible to understand the evolution of the frequency vector of a population under the replicator-mutator equation as a posterior predictive inference procedure in an HMM. This view enables us to derive a natural dual version of the replicator-mutator equation, which corresponds to updating the filtering distribution. Finally, we conclude with the implications of the interpretation and with some comments related to the recent discussions about evolution and learning.', 'corpus_id': 44430682, 'score': 1}, {'doc_id': '233762187', 'title': 'Precis of A Bayesian account of learning algorithms and generalising representations in the brain', 'abstract': 'Without learning we would be limited to a set of preprogrammed behaviours. While that may be acceptable for flies1, it does not provide the basis for adaptive or intelligent behaviours familiar to humans. Learning, then, is one of the crucial components of brain operation. Learning, however, takes time. Thus, the key to adaptive behaviour is learning to systematically generalise; that is, have learned knowledge that can be flexibly recombined to understand any world in front of you. This thesis attempts to make inroads on two questions how can brain networks learn, and what are the principles behind representations of knowledge that allow generalisation. With the industrialisation of science, the twentieth century bore fruit in the form of an increasingly detailed understanding of neurons, synapses, neurotransmitters, resting potentials, action potentials, networks and so on (1–4). Though we have gained a great level of detail about many of these micro-processes as well as high-level understandings of intelligence thanks to philosophy, experimental psychology, and behavioural and cognitive neuroscience (5–9) a large gulf of understanding remains between these levels of granularity. This thesis focuses on spanning this gap by providing high-level computational frameworks that translate to low-level processes. Any high-level brain framework must have successful behaviour at its heart as that is the role of the brain. Analogously, neurons are central to low-level understanding as the basis of brain function is believed to be the transfer of information between neurons, mediated via weighted connections. Different weights lead to different functions. Thus, learning appropriate configurations of weights is the fundamental problem facing brains. There are two facets to this learning the first is how, and the second is what. The how are the learning algorithms that determine updates to these synaptic connections, and the what are the neural representations that reflect how the world works. In this vein, this thesis examines 1) the algorithmic implementation of learning in biological neural networks, and 2) a computational framework for the neural representations of task generalisation. Both these research directions are bound together by Bayesian thinking, and both of these pieces of work bridge the gap between highand lowlevel understanding, as well as between brains and machines.', 'corpus_id': 233762187, 'score': 0}, {'doc_id': '233391835', 'title': 'USING MULTIPLICATIVE DIFFERENTIAL NEURAL UNITS', 'abstract': 'Automated mathematical reasoning is a challenging problem that requires an agent to learn algebraic patterns that contain long-range dependencies. Two particular tasks that test this type of reasoning are (1) mathematical equation verification, which requires determining whether trigonometric and linear algebraic statements are valid identities or not, and (2) equation completion, which entails filling in a blank within an expression to make it true. Solving these tasks with deep learning requires that the neural model learn how to manipulate and compose various algebraic symbols, carrying this ability over to previously unseen expressions. Artificial neural networks, including recurrent networks and transformers, struggle to generalize on these kinds of difficult compositional problems, often exhibiting poor extrapolation performance. In contrast, recursive neural networks (recursiveNNs) are, theoretically, capable of achieving better extrapolation due to their tree-like design but are difficult to optimize as the depth of their underlying tree structure increases. To overcome this issue, we extend recursive-NNs to utilize multiplicative, higher-order synaptic connections and, furthermore, to learn to dynamically control and manipulate an external memory. We argue that this key modification gives the neural system the ability to capture powerful transition functions for each possible input. We demonstrate the effectiveness of our proposed higher-order, memoryaugmented recursive-NN models on two challenging mathematical equation tasks, showing improved extrapolation, stable performance, and faster convergence. Our models achieve a 1.53% average improvement over current state-of-the-art methods in equation verification and achieve a 2.22% Top-1 average accuracy and 2.96% Top-5 average accuracy for equation completion.', 'corpus_id': 233391835, 'score': 0}, {'doc_id': '14974685', 'title': 'The AHA! Experience: Creativity Through Emergent Binding in Neural Networks', 'abstract': 'Many kinds of creativity result from combination of mental representations. This paper provides a computational account of how creative thinking can arise from combining neural patterns into ones that are potentially novel and useful. We defend the hypothesis that such combinations arise from mechanisms that bind together neural activity by a process of convolution, a mathematical operation that interweaves structures. We describe computer simulations that show the feasibility of using convolution to produce emergent patterns of neural activity that can support cognitive and emotional processes underlying human creativity.', 'corpus_id': 14974685, 'score': 1}, {'doc_id': '233034499', 'title': 'Grounding Language Processing: The Added Value of Specifying Linguistic/Compositional Representations and Processes', 'abstract': 'Abundant empirical evidence suggests that visual perception and motor responses are involved in language comprehension (‘grounding’). However, when modeling the grounding of sentence comprehension on a word-by-word basis, linguistic representations and cognitive processes are rarely made fully explicit. This article reviews representational formalisms and associated (computational) models with a view to accommodating incremental and compositional grounding effects. Are different representation formats equally suitable and what mechanisms and representations do models assume to accommodate grounding effects? I argue that we must minimally specify compositional semantic representations, a set of incremental processes/mechanisms, and an explicit link from the assumed processes to measured behavior. Different representational formats can be contrasted in psycholinguistic modeling by holding the set of processes/mechanisms constant; contrasting different processes/mechanisms is possible by holding representations constant. Such psycholinguistic modeling could be applied across a wide range of experimental investigations and complement computational modeling.', 'corpus_id': 233034499, 'score': 0}, {'doc_id': '233241202', 'title': 'Syntactic Perturbations Reveal Representational Correlates of Hierarchical Phrase Structure in Pretrained Language Models', 'abstract': 'While vector-based language representations from pretrained language models have set a new standard for many NLP tasks, there is not yet a complete accounting of their inner workings. In particular, it is not entirely clear what aspects of sentence-level syntax are captured by these representations, nor how (if at all) they are built along the stacked layers of the network. In this paper, we aim to address such questions with a general class of interventional, input perturbation-based analyses of representations from pretrained language models. Importing from computational and cognitive neuroscience the notion of representational invariance, we perform a series of probes designed to test the sensitivity of these representations to several kinds of structure in sentences. Each probe involves swapping words in a sentence and comparing the representations from perturbed sentences against the original. We experiment with three different perturbations: (1) random permutations of n-grams of varying width, to test the scale at which a representation is sensitive to word position; (2) swapping of two spans which do or do not form a syntactic phrase, to test sensitivity to global phrase structure; and (3) swapping of two adjacent words which do or do not break apart a syntactic phrase, to test sensitivity to local phrase structure. Results from these probes collectively suggest that Transformers build sensitivity to larger parts of the sentence along their layers, and that hierarchical phrase structure plays a role in this process. More broadly, our results also indicate that structured input perturbations widens the scope of analyses that can be performed on often-opaque deep learning systems, and can serve as a complement to existing tools (such as supervised linear probes) for interpreting complex black-box models.', 'corpus_id': 233241202, 'score': 0}, {'doc_id': '53175158', 'title': 'Attentive Tensor Product Learning', 'abstract': 'This paper proposes a novel neural architecture — Attentive Tensor Product Learning (ATPL) — to represent grammatical structures of natural language in deep learning models. ATPL exploits Tensor Product Representations (TPR), a structured neural-symbolic model developed in cognitive science, to integrate deep learning with explicit natural language structures and rules. The key ideas of ATPL are: 1) unsupervised learning of role-unbinding vectors of words via the TPR-based deep neural network; 2) the use of attention modules to compute TPR; and 3) the integration of TPR with typical deep learning architectures including long short-term memory and feedforward neural networks. The novelty of our approach lies in its ability to extract the grammatical structure of a sentence by using role-unbinding vectors, which are obtained in an unsupervised manner. Our ATPL approach is applied to 1) image captioning, 2) part of speech (POS) tagging, and 3) constituency parsing of a natural language sentence. The experimental results demonstrate the effectiveness of the proposed approach in all these three natural language processing tasks.', 'corpus_id': 53175158, 'score': 1}, {'doc_id': '237420734', 'title': 'Learning with Holographic Reduced Representations', 'abstract': 'Holographic Reduced Representations (HRR) are a method for performing symbolic AI on top of real-valued vectors [1] by associating each vector with an abstract concept, and providing mathematical operations to manipulate vectors as if they were classic symbolic objects. This method has seen little use outside of older symbolic AI work and cognitive science. Our goal is to revisit this approach to understand if it is viable for enabling a hybrid neural-symbolic approach to learning as a differentiable component of a deep learning architecture. HRRs today are not effective in a differentiable solution due to numerical instability, a problem we solve by introducing a projection step that forces the vectors to exist in a well behaved point in space. In doing so we improve the concept retrieval efficacy of HRRs by over 100×. Using multi-label classification we demonstrate how to leverage the symbolic HRR properties to develop an output layer and loss function that is able to learn effectively, and allows us to investigate some of the pros and cons of an HRR neuro-symbolic learning approach.', 'corpus_id': 237420734, 'score': 1}]
163	{'doc_id': '214803107', 'title': 'Light3DPose: Real-time Multi-Person 3D Pose Estimation from Multiple Views', 'abstract': 'We present an approach to perform 3D pose estimation of multiple people from a few calibrated camera views. Our architecture, leveraging the recently proposed unprojection layer, aggregates feature-maps from a 2D pose estimator backbone into a comprehensive representation of the 3D scene. Such intermediate representation is then elaborated by a fully-convolutional volumetric network and a decoding stage to extract 3D skeletons with sub-voxel accuracy. Our method achieves state of the art MPJPE on the CMU Panoptic dataset using a few unseen views and obtains competitive results even with a single input view. We also assess the transfer learning capabilities of the model by testing it against the publicly available Shelf dataset obtaining good performance metrics. The proposed method is inherently efficient: as a pure bottom-up approach, it is computationally independent of the number of people in the scene. Furthermore, even though the computational burden of the 2D part scales linearly with the number of input views, the overall architecture is able to exploit a very lightweight 2D backbone which is orders of magnitude faster than the volumetric counterpart, resulting in fast inference time. The system can run at 6 FPS, processing up to 10 camera views on a single 1080Ti GPU.', 'corpus_id': 214803107}	3926	"[{'doc_id': '212747986', 'title': '3D Crowd Counting via Multi-View Fusion with 3D Gaussian Kernels', 'abstract': 'Crowd counting has been studied for decades and a lot of works have achieved good performance, especially the DNNs-based density map estimation methods. Most existing crowd counting works focus on single-view counting, while few works have studied multi-view counting for large and wide scenes, where multiple cameras are used. Recently, an end-to-end multi-view crowd counting method called multi-view multi-scale (MVMS) has been proposed, which fuses multiple camera views using a CNN to predict a 2D scene-level density map on the ground-plane. Unlike MVMS, we propose to solve the multi-view crowd counting task through 3D feature fusion with 3D scene-level density maps, instead of the 2D ground-plane ones. Compared to 2D fusion, the 3D fusion extracts more information of the people along z-dimension (height), which helps to solve the scale variations across multiple views. The 3D density maps still preserve the 2D density maps property that the sum is the count, while also providing 3D information about the crowd density. We also explore the projection consistency among the 3D prediction and the ground-truth in the 2D views to further enhance the counting performance. The proposed method is tested on 3 multi-view counting datasets and achieves better or comparable counting performance to the state-of-the-art.', 'corpus_id': 212747986, 'score': 0}, {'doc_id': '233864855', 'title': 'PoseAug: A Differentiable Pose Augmentation Framework for 3D Human Pose Estimation', 'abstract': 'Existing 3D human pose estimators suffer poor generalization performance to new datasets, largely due to the limited diversity of 2D-3D pose pairs in the training data. To address this problem, we present PoseAug, a new auto-augmentation framework that learns to augment the available training poses towards a greater diversity and thus improve generalization of the trained 2D-to-3D pose estimator. Specifically, PoseAug introduces a novel pose augmentor that learns to adjust various geometry factors (e.g., posture, body size, view point and position) of a pose through differentiable operations. With such differentiable capacity, the augmentor can be jointly optimized with the 3D pose estimator and take the estimation error as feedback to generate more diverse and harder poses in an online manner. Moreover, PoseAug introduces a novel part-aware Kinematic Chain Space for evaluating local joint-angle plausibility and develops a discriminative module accordingly to ensure the plausibility of the augmented poses. These elaborate designs enable PoseAug to generate more diverse yet plausible poses than existing offline augmentation methods, and thus yield better generalization of the pose estimator. PoseAug is generic and easy to be applied to various 3D pose estimators. Extensive experiments demonstrate that PoseAug brings clear improvements on both intra-scenario and cross-scenario datasets. Notably, it achieves 88.6% 3D PCK on MPI-INF-3DHP under cross-dataset evaluation setup, improving upon the previous best data augmentation based method by 9.1%. Code can be found at: this https URL.', 'corpus_id': 233864855, 'score': 0}, {'doc_id': '215745329', 'title': 'A Novel Pose Proposal Network and Refinement Pipeline for Better Object Pose Estimation', 'abstract': ""In this paper, we present a novel deep learning pipeline for 6D object pose estimation and refinement from RGB inputs. The first component of the pipeline leverages a region proposal framework to estimate multi-class single-shot 6D object poses directly from an RGB image and through a CNN-based encoder multi-decoders network. The second component, a multi-attentional pose refinement network (MARN), iteratively refines the estimated pose. MARN takes advantage of both visual and flow features to learn a relative transformation between an initially predicted pose and a target pose. MARN is further augmented by a spatial multi-attention block that emphasizes objects' discriminative feature parts. Experiments on three benchmarks for 6D pose estimation show that the proposed pipeline outperforms state-of-the-art RGB-based methods with competitive runtime performance."", 'corpus_id': 215745329, 'score': 0}, {'doc_id': '70350026', 'title': 'Self-Supervised Learning of 3D Human Pose Using Multi-View Geometry', 'abstract': 'Training accurate 3D human pose estimators requires large amount of 3D ground-truth data which is costly to collect. Various weakly or self supervised pose estimation methods have been proposed due to lack of 3D data. Nevertheless, these methods, in addition to 2D ground-truth poses, require either additional supervision in various forms (e.g. unpaired 3D ground truth data, a small subset of labels) or the camera parameters in multiview settings. To address these problems, we present EpipolarPose, a self-supervised learning method for 3D human pose estimation, which does not need any 3D ground-truth data or camera extrinsics. During training, EpipolarPose estimates 2D poses from multi-view images, and then, utilizes epipolar geometry to obtain a 3D pose and camera geometry which are subsequently used to train a 3D pose estimator. We demonstrate the effectiveness of our approach on standard benchmark datasets (i.e. Human3.6M and MPI-INF-3DHP) where we set the new state-of-the-art among weakly/self-supervised methods. Furthermore, we propose a new performance measure Pose Structure Score (PSS) which is a scale invariant, structure aware measure to evaluate the structural plausibility of a pose with respect to its ground truth. Code and pretrained models are available at https://github.com/mkocabas/EpipolarPose', 'corpus_id': 70350026, 'score': 1}, {'doc_id': '235593047', 'title': 'Part-aware Measurement for Robust Multi-View Multi-Human 3D Pose Estimation and Tracking', 'abstract': 'This paper introduces an approach for multi-human 3D pose estimation and tracking based on calibrated multi-view. The main challenge lies in finding the cross-view and temporal correspondences correctly even when several human pose estimations are noisy. Compare to previous solutions that construct 3D poses from multiple views, our approach takes advantage of temporal consistency to match the 2D poses estimated with previously constructed 3D skeletons in every view. Therefore cross-view and temporal associations are accomplished simultaneously. Since the performance suffers from mistaken association and noisy predictions, we design two strategies for aiming better correspondences and 3D reconstruction. Specifically, we propose a part-aware measurement for 2D-3D association and a filter that can cope with 2D outliers during reconstruction. Our approach is efficient and effective comparing to state-of-the-art methods; it achieves competitive results on two benchmarks: 96.8% on Campus and 97.4% on Shelf. Moreover, we extends the length of Campus evaluation frames to be more challenging and our proposal also reach well-performed result. The code will be available at https://git.io/JO4KE.', 'corpus_id': 235593047, 'score': 1}, {'doc_id': '221397221', 'title': 'LiftFormer: 3D Human Pose Estimation using attention models', 'abstract': ""Estimating the 3D position of human joints has become a widely researched topic in the last years. Special emphasis has gone into defining novel methods that extrapolate 2-dimensional data (keypoints) into 3D, namely predicting the root-relative coordinates of joints associated to human skeletons. The latest research trends have proven that the Transformer Encoder blocks aggregate temporal information significantly better than previous approaches. Thus, we propose the usage of these models to obtain more accurate 3D predictions by leveraging temporal information using attention mechanisms on ordered sequences human poses in videos. \nOur method consistently outperforms the previous best results from the literature when using both 2D keypoint predictors by 0.3 mm (44.8 MPJPE, 0.7% improvement) and ground truth inputs by 2mm (MPJPE: 31.9, 8.4% improvement) on Human3.6M. It also achieves state-of-the-art performance on the HumanEva-I dataset with 10.5 P-MPJPE (22.2% reduction). The number of parameters in our model is easily tunable and is smaller (9.5M) than current methodologies (16.95M and 11.25M) whilst still having better performance. Thus, our 3D lifting model's accuracy exceeds that of other end-to-end or SMPL approaches and is comparable to many multi-view methods."", 'corpus_id': 221397221, 'score': 1}, {'doc_id': '211572845', 'title': '4D Association Graph for Realtime Multi-Person Motion Capture Using Multiple Video Cameras', 'abstract': ""his paper contributes a novel realtime multi-person motion capture algorithm using multiview video inputs. Due to the heavy occlusions and closely interacting motions in each view, joint optimization on the multiview images and multiple temporal frames is indispensable, which brings up the essential challenge of realtime efficiency. To this end, for the first time, we unify per-view parsing, cross-view matching, and temporal tracking into a single optimization framework, i.e., a 4D association graph that each dimension (image space, viewpoint and time) can be treated equally and simultaneously. To solve the 4D association graph efficiently, we further contribute the idea of 4D limb bundle parsing based on heuristic searching, followed with limb bundle assembling by proposing a bundle Kruskal's algorithm. Our method enables a realtime motion capture system running at 30fps using 5 cameras on a 5-person scene. Benefiting from the unified parsing, matching and tracking constraints, our method is robust to noisy detection due to severe occlusions and close interacting motions, and achieves high-quality online pose reconstruction quality. The proposed method outperforms state-of-the-art methods quantitatively without using high-level appearance information."", 'corpus_id': 211572845, 'score': 1}, {'doc_id': '212736938', 'title': 'Weakly-Supervised 3D Human Pose Learning via Multi-View Images in the Wild', 'abstract': 'One major challenge for monocular 3D human pose estimation in-the-wild is the acquisition of training data that contains unconstrained images annotated with accurate 3D poses. In this paper, we address this challenge by proposing a weakly-supervised approach that does not require 3D annotations and learns to estimate 3D poses from unlabeled multi-view data, which can be acquired easily in in-the-wild environments. We propose a novel end-to-end learning framework that enables weakly-supervised training using multi-view consistency. Since multi-view consistency is prone to degenerated solutions, we adopt a 2.5D pose representation and propose a novel objective function that can only be minimized when the predictions of the trained model are consistent and plausible across all camera views. We evaluate our proposed approach on two large scale datasets (Human3.6M and MPII-INF-3DHP) where it achieves state-of-the-art performance among semi-/weakly-supervised methods.', 'corpus_id': 212736938, 'score': 1}]"
164	{'doc_id': '220438671', 'title': 'HCI in Games: Second International Conference, HCI-Games 2020, Held as Part of the 22nd HCI International Conference, HCII 2020, Copenhagen, Denmark, July 19–24, 2020, Proceedings', 'abstract': 'General game-playing artificial intelligence (AI) has recently seen important advances due to the various techniques known as ‘deep learning’. However, in terms of human-computer interaction, the advances conceal a major limitation: these algorithms do not incorporate any sense of what human players find meaningful in games. I argue that adaptive game AI will be enhanced by a generalised player model, because games are inherently human artefacts which require some encoding of the human perspective in order to respond naturally to individual players. The player model provides constraints on the adaptive AI, which allow it to encode aspects of what human players find meaningful. I propose that a general player model requires parameters for the subjective experience of play, including: player psychology, game structure, and actions of play. I argue that such a player model would enhance efficiency of per-game solutions, and also support study of game-playing by allowing (within-player) comparison between games, or (within-game) comparison between players (human and AI). Here we detail requirements for functional adaptive AI, arguing from first-principles drawn from games research literature, and propose a formal specification for a generalised player model based on our ‘Behavlets’ method for psychologically-derived player modelling.', 'corpus_id': 220438671}	4681	"[{'doc_id': '219955873', 'title': 'Data-Driven Game Development: Ethical Considerations', 'abstract': 'In recent years, the games industry has made a major move towards data-driven development, using data analytics and player modeling to inform design decisions. Data-driven techniques are beneficial as they allow for the study of player behavior at scale, making them very applicable to modern digital game development. However, with this move towards data driven decision-making comes a number of ethical concerns. Previous work in player modeling [45] as well as work in the fields of AI and machine learning [9, 53] have demonstrated several ways in which algorithmic decision-making can be flawed due to data or algorithmic bias or lack of data from specific groups. Further, black box algorithms create a trust problem due to lack of interpretability and transparency of the results or models developed based on the data, requiring blind faith in the results. In this position paper, we discuss several factors affecting the use of game data in the development cycle. In addition to issues raised by previous work, we also raise issues with algorithms marginalizing certain player groups and flaws in the resulting models due to their inability to reason about situational factors affecting players’ decisions. Further, we outline some work that seeks to address these problems and identify some open problems concerning ethics and game data science.', 'corpus_id': 219955873, 'score': 1}, {'doc_id': '215744827', 'title': 'Computers in Secondary Schools: Educational Games', 'abstract': 'This entry introduces educational games in secondary schools. Educational games include three main types of educational activities with a playful learning intention supported by digital technologies: educational serious games, educational gamification, and learning through game creation. Educational serious games are digital games that support learning objectives. Gamification is defined as the use of ""game design elements and game thinking in a non-gaming context"" (Deterding et al. 2011, p. 13). Educational gamification is not developed through a digital game but includes game elements for supporting the learning objectives. Learning through game creation is focused on the process of designing and creating a prototype of a game to support a learning process related to the game creation process or the knowledge mobilized through the game creation process. Four modalities of educational games in secondary education are introduced in this entry to describe educational games in secondary education: educational purpose of entertainment games, serious games, gamification, and game design.', 'corpus_id': 215744827, 'score': 0}, {'doc_id': '215814147', 'title': 'SportsXR - Immersive Analytics in Sports', 'abstract': 'We wish to thank Coach Kathy Delaney-Smith, Mike Roux, Mark Kaliris, and Lindsay Werner at Harvard Women’s Basketball, and Mike Sotsky and Casey Brinn at Harvard Men’s Basketball for their time and expertise. This research is supported in part by King Abdullah University of Science and Technology (KAUST) and the KAUST Office of Sponsored Research (OSR) award OSR-2015-CCF-2533-01.', 'corpus_id': 215814147, 'score': 0}, {'doc_id': '220439779', 'title': 'Design, User Experience, and Usability. Design for Contemporary Interactive Environments: 9th International Conference, DUXU 2020, Held as Part of the 22nd HCI International Conference, HCII 2020, Copenhagen, Denmark, July 19–24, 2020, Proceedings, Part II', 'abstract': 'Human-algorithm interaction emerges as the new frontier of studies involving interaction design and interface ergonomics. This paper aims to discuss the effectiveness and communicability of streaming media recommendation systems, based on machine learning, from users’ mental model point of view. We examined the content consumption practices on the Netflix platform, identifying some sensitive aspects of the interaction with recommendation algorithms. One-on-one semi-structured interviews were applied to a sample of students from three different universities in Rio de Janeiro, Brazil. We realised that interviewees had not correctly understood how the system works and have not formed an adequate mental model about tracked data and how it is processed to create personalised lists. Another issue concerns data privacy: Users have revealed a suspicion concerning algorithms and what might happen to usage data, not only in the Netflix platform but also in other services that use algorithm-based recommendation. Interviewees’ responses suggested that there may be communication failures, and UX designers should strive to make it visible how the system tracks and processes user interaction data.', 'corpus_id': 220439779, 'score': 1}, {'doc_id': '219721421', 'title': 'On the environment-destructive probabilistic trends: a perceptual and behavioral study on video game players', 'abstract': ""Currently, gaming is the world's favorite form of entertainment. Various studies have shown how games impact players' perceptions and behaviors, prompting opportunities for purposes beyond entertainment. This study uses Animal Crossing: New Horizons (ACNH), a real-time life-simulation game, as a unique case study of how video games can affect humans' environmental perceptions. A dataset of 584 observations from a survey of ACNH players and the Hamiltonian MCMC technique has enabled us to explore the relationship between in-game behaviors and perceptions. The findings indicate a probabilistic trend towards exploiting the in-game environment despite players' perceptions, suggesting that the simplification of commercial game design may overlook opportunities to engage players in pro-environmental activities."", 'corpus_id': 219721421, 'score': 1}, {'doc_id': '219955919', 'title': 'Reflection in Game-Based Learning: A Survey of Programming Games', 'abstract': 'Reflection is a critical aspect of the learning process. However, educational games tend to focus on supporting learning concepts rather than supporting reflection. While reflection occurs in educational games, the educational game design and research community can benefit from more knowledge of how to facilitate player reflection through game design. In this paper, we examine educational programming games and analyze how reflection is currently supported. We find that current approaches prioritize accuracy over the individual learning process and often only support reflection post-gameplay. Our analysis identifies common reflective features, and we develop a set of open areas for future work. We discuss these promising directions towards engaging the community in developing more mechanics for reflection in educational games.', 'corpus_id': 219955919, 'score': 0}, {'doc_id': '218719915', 'title': 'User Attention and Behaviour in Virtual Reality Art Encounter', 'abstract': 'With the proliferation of consumer virtual reality (VR) headsets and creative tools, content creators have started to experiment with new forms of interactive audience experience using immersive media. Understanding user attention and behaviours in virtual environment can greatly inform creative processes in VR. We developed an abstract VR painting and an experimentation system to study audience encounters through eye gaze and movement tracking. The data from a user experiment with 35 participants reveal a range of user activity patterns in art exploration. Deep learning models are used to study the connections between behavioural data and audience background. New integrated methods to visualise user attention as part of the artwork are also developed as a feedback loop to the content creator.', 'corpus_id': 218719915, 'score': 0}, {'doc_id': '220920168', 'title': 'Quixo Is Solved', 'abstract': 'Quixo is a two-player game played on a 5 × 5 grid where the players try to align five identical symbols. Specifics of the game require the usage of novel techniques. Using a combination of value iteration and backward induction, we propose the first complete analysis of the game. We describe memory-efficient data structures and algorithmic optimizations that make the game solvable within reasonable time and space constraints. Our main conclusion is that Quixo is a Draw game. The paper also contains the analysis of smaller boards and presents some interesting states extracted from our computations.', 'corpus_id': 220920168, 'score': 0}, {'doc_id': '222096072', 'title': 'HCI International 2020 - Late Breaking Papers: User Experience Design and Case Studies: 22nd HCI International Conference, HCII 2020, Copenhagen, Denmark, July 19–24, 2020, Proceedings', 'abstract': 'Heuristic evaluation (HE) is one of the most commonly used usability evaluation methods. In HE, 3–5 evaluators evaluate a certain system guided by a list of usability heuristics with the goal of detecting usability issues. Although HE is popular in the usability field, it heavily depends on the expertise of the evaluator, meaning that there is a large gap in HE performance between expert evaluators and novice evaluators. One of the factors contributing to this gap is the difficulty of thoroughly understanding usability heuristics. Usability heuristics are abstract and require simplification to be better understood by novice evaluators. In this work, our goal was to simplify Nielsen’s heuristics to make them easier to understand since they are one of the most popular sets of usability heuristics. We interviewed 15 usability experts with at least 4 years of experience in the field from both academia and industry and asked them to explain Nielsen’s heuristics in detail. We analyzed their responses, and we produced a modified list that is more detailed than Nielsen’s original list.', 'corpus_id': 222096072, 'score': 1}, {'doc_id': '219955784', 'title': '”And then they died”: Using Action Sequences for Data Driven, Context Aware Gameplay Analysis', 'abstract': 'Many successful games rely heavily on data analytics to understand players and inform design. Popular methodologies focus on machine learning and statistical analysis of aggregated data. While effective in extracting information regarding player action, much of the context regarding when and how those actions occurred is lost. Qualitative methods allow researchers to examine context and derive meaningful explanations about the goals and motivations behind player behavior, but are difficult to scale. In this paper, we build on previous work by combining two existing methodologies: Interactive Behavior Analytics (IBA) [2] and sequence analysis (SA), in order to create a novel, mixed methods, human-in-the-loop data analysis methodology that uses behavioral labels and visualizations to allow analysts to examine player behavior in a way that is context sensitive, scalable, and generalizable. We present the methodology along with a case study demonstrating how it can be used to analyze behavioral patterns of teamwork in the popular multiplayer game Defense of the Ancients 2 (DotA 2).', 'corpus_id': 219955784, 'score': 1}]"
165	{'doc_id': '97216579', 'title': 'A diabatic representation of the two lowest electronic states of Li3', 'abstract': 'Using the Multi-Reference Configuration Interaction method, the adiabatic potential energy surfaces of Li3 are computed. The two lowest electronic states are bound and exhibit a conical intersection. By fitting the calculated potential energy surfaces to the cubic E ⊗ ɛ Jahn-Teller model we extract the effective Jahn-Teller parameters corresponding to Li3. These are used to set up the transformation matrix which transforms from the adiabatic to a diabatic representation. This diabatization method gives a Hamiltonian for Li3 which is free from singular non-adiabatic couplings and should be accurate for large internuclear distances, and it thereby allows for bound dynamics in the vicinity of the conical intersection to be explored.', 'corpus_id': 97216579}	7500	[{'doc_id': '221802290', 'title': 'Collective emission of photons from dense, dipole-dipole interacting atomic ensembles', 'abstract': 'We study the collective radiation properties of cold, trapped ensembles of atoms. We consider the high-density regime with the mean interatomic distance being comparable to, or smaller than, the wavelength of the resonant optical radiation emitted by the atoms. We find that the emission rate of a photon from an excited atomic ensemble is strongly enhanced for an elongated cloud. We analyze collective single-excitation eigenstates of the atomic ensemble and find that the absorption-emission spectrum is broadened and shifted to lower frequencies as compared to the noninteracting (low-density) or single-atom spectrum. We also analyze the spatial and temporal profile of the emitted radiation. Finally, we explore how to efficiently excite the collective superradiant states of the atomic ensemble from a long-lived storage state in order to implement matter-light interfaces for quantum computation and communication applications.', 'corpus_id': 221802290, 'score': 0}, {'doc_id': '118411939', 'title': 'Three-Body Recombination in Cold Atomic Gases', 'abstract': 'Systems of three particles show a surprising feature in their bound state spectrum: a series of geometrically scaled states, known as Efimov states. These states have not yet been observed directly, but many recent experiments show indirect evidence of their existence via the so-called recombination process. The theories that predict the Efimov states also predicts either resonant enhancement of the recombination process or suppression by destructive interference, depending on the sign of the interaction between the particles. The theories predict universal features for the Efimov states, for instance that the geometric scaling factor is 22.7, meaning that one state is 22.7 times larger than its lower lying neighbour state. This thesis seeks to investigate non-universal effects by incorporating additional information about the physical interactions into the universal theories.', 'corpus_id': 118411939, 'score': 1}, {'doc_id': '219401859', 'title': 'Nondestructive dispersive imaging of rotationally excited ultracold molecules.', 'abstract': 'A barrier to realizing the potential of molecules for quantum information science applications is a lack of high-fidelity, single-molecule imaging techniques. Here, we present and theoretically analyze a general scheme for dispersive imaging of electronic ground-state molecules. Our technique relies on the intrinsic anisotropy of excited molecular rotational states to generate optical birefringence, which can be detected through polarization rotation of an off-resonant probe laser beam. Using 23Na87Rb and 87Rb133Cs as examples, we construct a formalism for choosing the molecular state to be imaged and the excited electronic states involved in off-resonant coupling. Our proposal establishes the relevant parameters for achieving degree-level polarization rotations for bulk molecular gases, thus enabling high-fidelity nondestructive imaging. We additionally outline requirements for the high-fidelity imaging of individually trapped molecules.', 'corpus_id': 219401859, 'score': 0}, {'doc_id': '230523882', 'title': 'Collisions between Ultracold Molecules and Atoms in a Magnetic Trap.', 'abstract': 'We prepare mixtures of ultracold CaF molecules and Rb atoms in a magnetic trap and study their inelastic collisions. When the atoms are prepared in the spin-stretched state and the molecules in the spin-stretched component of the first rotationally excited state, they collide inelastically with a rate coefficient k_{2}=(6.6±1.5)×10^{-11}\u2009\u2009cm^{3}/s at temperatures near 100\u2009\u2009μK. We attribute this to rotation-changing collisions. When the molecules are in the ground rotational state we see no inelastic loss and set an upper bound on the spin-relaxation rate coefficient of k_{2}<5.8×10^{-12}\u2009\u2009cm^{3}/s with 95%\xa0confidence. We compare these measurements to the results of a single-channel loss model based on quantum defect theory. The comparison suggests a short-range loss parameter close to unity for rotationally excited molecules, but below 0.04 for molecules in the rotational ground state.', 'corpus_id': 230523882, 'score': 0}, {'doc_id': '224704544', 'title': 'Sub-Poissonian atom-number distributions by means of Rydberg dressing and electromagnetically induced transparency', 'abstract': 'A method is proposed to produce atomic ensembles with sub-Poissonian atom number distributions. The method consists of removing the excess atoms using the interatomic interactions induced by Rydberg dressing. The selective removal of atoms occurs via spontaneous decay into untrapped states using an electromagnetically induced transparency scheme. Ensembles with the desired number of atoms can be produced almost deterministically. Numerical simulations predict a strong reduction of the atom number fluctuations, with the variance twenty times less than the Poisson noise level (the predicted Fano factor is F = 0.05). Strikingly, the method is suitable for both fermions and bosons. It solves the problem of the atom-number fluctuations in bosons, whose weak interactions have usually been an obstacle to controlling the number of atoms.', 'corpus_id': 224704544, 'score': 0}, {'doc_id': '2474215', 'title': 'Self-broadening and exciton line shifts in gases: Beyond the local-field approximation.', 'abstract': None, 'corpus_id': 2474215, 'score': 1}, {'doc_id': '221173101', 'title': 'Driven Quantum Cyclotron with One Electron or Positron', 'abstract': 'The first quantum calculation is presented for a harmonic detection oscillator coupled to a one-particle quantum cyclotron - one trapped electron or positron that occupies only its lowest cyclotron and spin states. The calculation is used to investigate new measurement methods that could circumvent and minimize the detector backaction that limited past measurements of these moments. New methods that allow new measurements are urgently needed because there is now an intriguing 2.4 standard deviation discrepancy between the most precise prediction of the standard model of particle physics, and the most accurate measurement of a property of an elementary particle.', 'corpus_id': 221173101, 'score': 0}, {'doc_id': '232417512', 'title': 'Quantum liquids and droplets with low-energy interactions in one dimension', 'abstract': 'We consider interacting one-dimensional bosons in the universal low-energy regime. The interactions consist of a combination of attractive and repulsive parts that can stabilize quantum gases, droplets and liquids. In particular, we study the role of effective three-body repulsion, in systems with weak attractive pairwise interactions. Its low-energy description is often argued to be equivalent to a model including only two-body interactions with non-zero range. Here, we show that, at zero temperature, the equations of state in both theories agree quantitatively at low densities for overall repulsion, in the gas phase, as can be inferred from the S-matrix formulation of statistical mechanics. However, this agreement is absent in the attractive regime, where universality only occurs in the long-distance properties of quantum droplets. We develop analytical tools to investigate the properties of the theory, and obtain astounding agreement with exact numerical calculations using the density-matrix renormalization group.', 'corpus_id': 232417512, 'score': 1}, {'doc_id': '119248981', 'title': 'Universal three-body recombination via resonant d-wave interactions', 'abstract': 'For a system of three identical bosons interacting via short-range forces, when two of the atoms are about to form a two-body s-wave dimer, there exists an infinite number of three-body bound states. This effect is the well-known Efimov effect. These three-body states (Efimov states) are found to be universal for ultracold atomic gases and the lowest Efimov state crosses the three-body break-up threshold when the s-wave two-body scattering length is $a \\approx -9.73 r_{\\rm vdW}$, $r_{\\rm vdW}$ being the van der Waals length. This article focuses on a generalized version of this Efimov scenario, where two of the atoms are about to form a two-body d-wave dimer, which leads to strong d-wave interactions. In a recent paper [B. Gao, Phys. Rev. A. {\\bf 62}, 050702(R) (2000)], Bo Gao has predicted that for broad resonances the d-wave dimer is always formed near $a \\approx 0.956 r_{\\rm vdW}$. Here we find that a single universal three-body state associated with the d-wave dimer is also formed near the three-body break-up threshold at $a \\approx 1.09 r_{\\rm vdW}$ and its signature can be found through enhancement of the three-body recombination. The three-body effective potential curves that are crucial for understanding the recombination dynamics are also calculated and analyzed. An improved method to calculate the couplings, effective potential curves, and recombination rate coefficients is presented.', 'corpus_id': 119248981, 'score': 1}, {'doc_id': '118620830', 'title': 'Quantum metasurfaces with atom arrays', 'abstract': 'Metasurfaces mould the flow of classical light waves by engineering subwavelength patterns from dielectric or metallic thin films. We introduce and analyse a method in which quantum operator-valued reflectivity can be used to control both the spatiotemporal and quantum properties of transmitted and reflected light. Such quantum metasurfaces are realized by entangling the macroscopic response of atomically thin atom arrays to light. We show that such a system allows for parallel quantum operations between atoms and photons as well as for the generation of highly entangled photonic states such as photonic Greenberger–Horne–Zeilinger and three-dimensional cluster states suitable for quantum information processing. We analyse the influence of imperfections as well as specific implementations based on atom arrays excited into Rydberg states. A kind of quantum metasurface made of an atom array is proposed, providing the possibility to control both spatiotemporal and quantum properties of transmitted and reflected light.', 'corpus_id': 118620830, 'score': 1}]
166	"{'doc_id': '212114261', 'title': 'What is ""intelligent"" in intelligent user interfaces?: a meta-analysis of 25 years of IUI', 'abstract': 'This reflection paper takes the 25th IUI conference milestone as an opportunity to analyse in detail the understanding of intelligence in the community: Despite the focus on intelligent UIs, it has remained elusive what exactly renders an interactive system or user interface ""intelligent"", also in the fields of HCI and AI at large. We follow a bottom-up approach to analyse the emergent meaning of intelligence in the IUI community: In particular, we apply text analysis to extract all occurrences of ""intelligent"" in all IUI proceedings. We manually review these with regard to three main questions: 1) What is deemed intelligent? 2) How (else) is it characterised? and 3) What capabilities are attributed to an intelligent entity? We discuss the community\'s emerging implicit perspective on characteristics of intelligence in intelligent user interfaces and conclude with ideas for stating one\'s own understanding of intelligence more explicitly.', 'corpus_id': 212114261}"	1148	"[{'doc_id': '86866942', 'title': 'Guidelines for Human-AI Interaction', 'abstract': 'Advances in artificial intelligence (AI) frame opportunities and challenges for user interface design. Principles for human-AI interaction have been discussed in the human-computer interaction community for over two decades, but more study and innovation are needed in light of advances in AI and the growing uses of AI technologies in human-facing applications. We propose 18 generally applicable design guidelines for human-AI interaction. These guidelines are validated through multiple rounds of evaluation including a user study with 49 design practitioners who tested the guidelines against 20 popular AI-infused products. The results verify the relevance of the guidelines over a spectrum of interaction scenarios and reveal gaps in our knowledge, highlighting opportunities for further research. Based on the evaluations, we believe the set of design guidelines can serve as a resource to practitioners working on the design of applications and features that harness AI technologies, and to researchers interested in the further development of human-AI interaction design principles.', 'corpus_id': 86866942, 'score': 1}, {'doc_id': '220713394', 'title': 'Learning User-Preferred Mappings for Intuitive Robot Control', 'abstract': 'When humans control drones, cars, and robots, we often have some preconceived notion of how our inputs should make the system behave. Existing approaches to teleoperation typically assume a one-size-fits-all approach, where the designers pre-define a mapping between human inputs and robot actions, and every user must adapt to this mapping over repeated interactions. Instead, we propose a personalized method for learning the human’s preferred or preconceived mapping from a few robot queries. Given a robot controller, we identify an alignment model that transforms the human’s inputs so that the controller’s output matches their expectations. We make this approach data-efficient by recognizing that human mappings have strong priors: we expect the input space to be proportional, reversable, and consistent. Incorporating these priors ensures that the robot learns an intuitive mapping from few examples. We test our learning approach in robot manipulation tasks inspired by assistive settings, where each user has different personal preferences and physical capabilities for teleoperating the robot arm. Our simulated and experimental results suggest that learning the mapping between inputs and robot actions improves objective and subjective performance when compared to manually defined alignments or learned alignments without intuitive priors. The supplementary video showing these user studies can be found at: https://youtu.be/rKHka0_48-Q', 'corpus_id': 220713394, 'score': 0}, {'doc_id': '220364234', 'title': 'Tree-Augmented Cross-Modal Encoding for Complex-Query Video Retrieval', 'abstract': 'The rapid growth of user-generated videos on the Internet has intensified the need for text-based video retrieval systems. Traditional methods mainly favor the concept-based paradigm on retrieval with simple queries, which are usually ineffective for complex queries that carry far more complex semantics. Recently, embedding-based paradigm has emerged as a popular approach. It aims to map the queries and videos into a shared embedding space where semantically-similar texts and videos are much closer to each other. Despite its simplicity, it forgoes the exploitation of the syntactic structure of text queries, making it suboptimal to model the complex queries. To facilitate video retrieval with complex queries, we propose a Tree-augmented Cross-modal Encoding method by jointly learning the linguistic structure of queries and the temporal representation of videos. Specifically, given a complex user query, we first recursively compose a latent semantic tree to structurally describe the text query. We then design a tree-augmented query encoder to derive structure-aware query representation and a temporal attentive video encoder to model the temporal characteristics of videos. Finally, both the query and videos are mapped into a joint embedding space for matching and ranking. In this approach, we have a better understanding and modeling of the complex queries, thereby achieving a better video retrieval performance. Extensive experiments on large scale video retrieval benchmark datasets demonstrate the effectiveness of our approach.', 'corpus_id': 220364234, 'score': 0}, {'doc_id': '220768971', 'title': 'Are Visual Explanations Useful? A Case Study in Model-in-the-Loop Prediction', 'abstract': 'We present a randomized controlled trial for a model-in-the-loop regression task, with the goal of measuring the extent to which (1) good explanations of model predictions increase human accuracy, and (2) faulty explanations decrease human trust in the model. We study explanations based on visual saliency in an image-based age prediction task for which humans and learned models are individually capable but not highly proficient and frequently disagree. Our experimental design separates model quality from explanation quality, and makes it possible to compare treatments involving a variety of explanations of varying levels of quality. We find that presenting model predictions improves human accuracy. However, visual explanations of various kinds fail to significantly alter human accuracy or trust in the model - regardless of whether explanations characterize an accurate model, an inaccurate one, or are generated randomly and independently of the input image. These findings suggest the need for greater evaluation of explanations in downstream decision making tasks, better design-based tools for presenting explanations to users, and better approaches for generating explanations.', 'corpus_id': 220768971, 'score': 1}, {'doc_id': '220380881', 'title': 'Modeling and Mitigating Human Annotation Errors to Design Efficient Stream Processing Systems with Human-in-the-loop Machine Learning', 'abstract': 'High-quality human annotations are necessary for creating effective machine learning-driven stream processing systems. We study hybrid stream processing systems based on a Human-In-The-Loop Machine Learning (HITL-ML) paradigm, in which one or many human annotators and an automatic classifier (trained at least partially by the human annotators) label an incoming stream of instances. This is typical of many near-real time social media analytics and web applications, including the annotation of social media posts during emergencies by digital volunteer groups. From a practical perspective, low-quality human annotations result in wrong labels for retraining automated classifiers and indirectly contribute to the creation of inaccurate classifiers. \nConsidering human annotation as a psychological process allows us to address these limitations. We show that human annotation quality is dependent on the ordering of instances shown to annotators, and can be improved by local changes in the instance sequence/ordering provided to the annotators, yielding a more accurate annotation of the stream. We design a theoretically-motivated human error framework for the human annotation task to study the effect of ordering instances (i.e., an ""annotation schedule""). Further, we propose an error-avoidance approach to the active learning (HITL-ML) paradigm for stream processing applications that is robust to these likely human errors when deciding a human annotation schedule. We validate the human error framework using crowdsourcing experiments and evaluate the proposed algorithm against standard baselines for active learning via extensive experimentation on classification tasks of filtering relevant social media posts during natural disasters.', 'corpus_id': 220380881, 'score': 1}, {'doc_id': '220425376', 'title': 'When Humans and Machines Make Joint Decisions: A Non-Symmetric Bandit Model', 'abstract': 'How can humans and machines learn to make joint decisions? This has become an important question in domains such as medicine, law and finance. We approach the question from a theoretical perspective and formalize our intuitions about human-machine decision making in a non-symmetric bandit model. In doing so, we follow the example of a doctor who is assisted by a computer program. We show that in our model, exploration is generally hard. In particular, unless one is willing to make assumptions about how human and machine interact, the machine cannot explore efficiently. We highlight one such assumption, policy space independence, which resolves the coordination problem and allows both players to explore independently. Our results shed light on the fundamental difficulties faced by the interaction of humans and machines. We also discuss practical implications for the design of algorithmic decision systems.', 'corpus_id': 220425376, 'score': 1}, {'doc_id': '8943607', 'title': 'Principles of mixed-initiative user interfaces', 'abstract': 'Recent debate has centered on the relative promise of focusinguser-interface research on developing new metaphors and tools thatenhance users abilities to directly manipulate objects versusdirecting effort toward developing interface agents that provideautomation. In this paper, we review principles that show promisefor allowing engineers to enhance human-computer interactionthrough an elegant coupling of automated services with directmanipulation. Key ideas will be highlighted in terms of the Lookoutsystem for scheduling and meeting management.', 'corpus_id': 8943607, 'score': 1}, {'doc_id': '220381402', 'title': 'From API to NLI: A New Interface for Library Reuse', 'abstract': 'Abstract Developers frequently reuse APIs from existing libraries to implement certain functionality. However, learning APIs is difficult due to their large scale and complexity. In this paper, we design an abstract framework NLI2Code to ease the reuse process. Under the framework, users can reuse library functionalities with a high-level, automatically-generated NLI (Natural Language Interface) instead of the detailed API elements. The framework consists of three components: a functional feature extractor to summarize the frequently-used library functions in natural language form, a code pattern miner to give a code template for each functional feature, and a synthesizer to complete code patterns into well-typed snippets. From the perspective of a user, a reuse task under NLI2Code starts from choosing a functional feature and our framework will guide the user to synthesize the desired solution. We instantiated the framework as a tool to reuse Java libraries. The evaluation shows our tool can generate a high-quality natural language interface and save half of the coding time for newcomers to solve real-world programming tasks.', 'corpus_id': 220381402, 'score': 0}, {'doc_id': '220381441', 'title': 'PinnerSage: Multi-Modal User Embedding Framework for Recommendations at Pinterest', 'abstract': ""Latent user representations are widely adopted in the tech industry for powering personalized recommender systems. Most prior work infers a single high dimensional embedding to represent a user, which is a good starting point but falls short in delivering a full understanding of the user's interests. In this work, we introduce PinnerSage, an end-to-end recommender system that represents each user via multi-modal embeddings and leverages this rich representation of users to provides high quality personalized recommendations. PinnerSage achieves this by clustering users' actions into conceptually coherent clusters with the help of a hierarchical clustering method (Ward) and summarizes the clusters via representative pins (Medoids) for efficiency and interpretability. PinnerSage is deployed in production at Pinterest and we outline the several design decisions that makes it run seamlessly at a very large scale. We conduct several offline and online A/B experiments to show that our method significantly outperforms single embedding methods."", 'corpus_id': 220381441, 'score': 0}]"
167	{'doc_id': '45091696', 'title': 'Secular Stagnation? The Effect of Aging on Economic Growth in the Age of Automation', 'abstract': 'Several recent theories emphasize the negative effects of an aging population on economic growth, either because of the lower labor force participation and productivity of older workers or because aging will create an excess of savings over desired investment, leading to secular stagnation. We show that there is no such negative relationship in the data. If anything, countries experiencing more rapid aging have grown more in recent decades. We suggest that this counterintuitive finding might reflect the more rapid adoption of automation technologies in countries undergoing more pronounced demographic changes, and provide evidence and theoretical underpinnings for this argument.', 'corpus_id': 45091696}	12947	[{'doc_id': '232221783', 'title': 'The Great Transition: Kuznets Facts for Family-Economists', 'abstract': 'The 20th century beheld a dramatic transformation of the family. Some Kuznets style facts regarding structural change in the family are presented. Over the course of the 20th century in the United States fertility declined, educational attainment waxed, housework fell, leisure increased, jobs shifted from blue to white collar, and marriage waned. These trends are also observed in the cross-country data. A model is developed, and then calibrated, to address the trends in the US data. The calibration procedure is closely connected to the underlying economic logic. Three drivers of the great transition are considered: neutral technological progress, skilled-biased technological change, and drops in the price of labor-saving household durables. \n \nYou can download the Kuznets facts here: https://www.ricardomarto.com/data/', 'corpus_id': 232221783, 'score': 0}, {'doc_id': '235420411', 'title': 'Technology as enabler of the automation of work? Current societal challenges for a future perspective of work / A tecnologia como facilitadora da automação do trabalho? Desafios sociais atuais para uma visão do futuro do trabalho', 'abstract': 'Due to the innovative possibilities of digital technologies, the issue of increasing automation is once again on the agenda – and not only in the industry, but also in other branches and sectors of contemporary societies. Although public and scientific discussions about automation seem to raise relevant questions of the “old” debate, such as the replacement of human labor by introducing new technologies, the authors focus here on the new contextual quality of these questions. The debate should rethink the relationship between technology and work with regard to quantitative and qualitative changes in work. In this article, our example will be the introduction of automation in industry, which has been reflected in the widely recognized study by Frey and Osborne in 2013. They estimated the expected impacts of future computerization on US labor market outcomes as very high, specifically regarding the number of jobs at risk. Surprisingly, this study was the starting point of an intensive international debate on the impact of technologies on the future of work and the role of technological change in working environments. Thus, according to the authors, “old” questions remain important, but they should be reinterpreted for “new” societal demands and expectations of future models of work. Keywords : automation, technical unemployment, transformation of work, new models of work *** Em virtude das possibilidades de inovacao colocadas pelas tecnologias digitais, vem crescendo o debate sobre o aumento da automacao – nao apenas na industria, mas tambem em outros ramos e setores das sociedades contemporâneas. Embora os debates publico e cientifico sobre automacao parecam focar questoes relevantes do “antigo” debate, como a substituicao do trabalho humano pela introducao de novas tecnologias, os autores concentram-se, aqui, no novo aspecto contextual dessas questoes. O debate precisa repensar a relacao entre tecnologia e trabalho no que concerne as mudancas quantitativas e qualitativas no trabalho. Neste artigo, nosso exemplo sera a introducao da automacao na industria, que se refletiu no estudo amplamente reconhecido de Frey e Osborne, de 2013. Eles estimaram impactos profundos da futura informatizacao sobre os desfechos do mercado de trabalho dos EUA, particularmente em relacao ao numero de empregos em risco. Surpreendentemente, este estudo foi o ponto de partida para um intenso debate internacional sobre o impacto das tecnologias no futuro do trabalho e o papel da mudanca tecnologica nos ambientes de trabalho. Assim, de acordo com os autores, embora as “velhas” questoes permanecam importantes, elas devem ser reinterpretadas considerando as “novas” demandas sociais e expectativas de modelos futuros de trabalho. Palavras-chave : automacao, desemprego tecnico, transformacao do trabalho, novos modelos de trabalho.', 'corpus_id': 235420411, 'score': 1}, {'doc_id': '231958242', 'title': 'Financial Frictions, Allocative Efficiency, and Unemployment: A Quantitative Analysis for Argentina∗', 'abstract': 'Argentina is characterized by low levels of private credit and persistent labor market rigidities. Furthermore, financial development remained stagnant in Argentina even during episodes of fast economic growth, in stark contrast with the experience of sustained growth accelerations around the world. The goals of the paper are twofold. Firstly, it is concerned with quantifying the productivity losses associated with such low levels of private credit penetration and characterizing its implications for different subsets of firms in the economy. The latter is important in light of various policy interventions aimed at mitigating the impact of low access to credit based on firm-size thresholds. Secondly, it studies the dynamics of hypothetical reforms to credit markets in a context of rigid labor markets, which seems to be the adequate scenario in which structural reforms will have to be implemented, given the stickiness that labor market regulations have shown to reform efforts in the past. It finds sizable productivity losses from financial frictions, in the order of 13%. At the micro level it finds that it is the youngest firms, whose average marginal return to capital is far above the riskfree rate in the economy, that are more prone to become financially constrained. Turning to reform scenarios, we investigate sudden reforms that are implemented abruptly and more plausible reform paths that gradually dismantle financial frictions. In the former, productivity and the investment rate rise sharply on impact, while it also does the rate of unemployment, going from 5 to almost 12%. In the latter, the rise of unemployment is more gradual and less sharp, peaking at 7%. On the flipside, the investment rate declines on impact, although the contraction is short-lived.', 'corpus_id': 231958242, 'score': 0}, {'doc_id': '231203954', 'title': 'The productivity paradox: policy lessons from MICROPROD', 'abstract': 'MICROPROD researchers have so far delivered 20 papers on four broad issues relevant for today’s policy debates: the measurement and effects of intangible capital on productivity; the impact of globalisation, international trade and the integration of global value chains (GVCs) on productivity; factor allocation and allocative efficiency; and finally the social consequences of the two structural shocks Europe has faced in the last two decades: globalisation and technological progress.', 'corpus_id': 231203954, 'score': 0}, {'doc_id': '233474507', 'title': 'ADBI Working Paper Series DEMOGRAPHIC TRANSITION FOR ECONOMIC DEVELOPMENT IN TAIPEI,CHINA: LITERATURE AND POLICY IMPLICATIONS', 'abstract': 'As an economy with a population of 23 million, Taipei,China is enjoying the demographic dividend of economic growth resulting from a shift in the population age structure, but an increasingly aged population could be bad for the economy. As an international comparison, its population is aging faster than that of most members of the Asian Development Bank and other advanced economies. Based on the literature, this paper contributes to the evaluation of the impact of aging on economic growth and volatility using relevant data and then concludes with the government policies relevant to the prospective aging problem. At the current stage, the situation is not as bad as expected. An aging workforce with positive productivity has no negative impact on economic growth. The old-age dependency ratio has a significantly negative effect on economic development, but appropriate foreign labor immigration and elderly long-term care policies can mitigate it. Higher education attainment still works to support economic growth in the long run. Besides, an increase in longevity first enhances and then erodes net foreign assets, and high old-age dependency ratios cause investments to respond strongly to technology shocks because individuals prefer to save more for retirement in aging societies. However, population aging has a minor influence on the dynamics of the macroeconomic variables. Nevertheless, the government should be cautious and allocate resources to help labor-intensive and low-skilled industries transform into more innovation-oriented and knowledge-intensive ones. Social welfare support must also become an important aspect of the social security framework.', 'corpus_id': 233474507, 'score': 1}, {'doc_id': '233990582', 'title': 'Automation Threat and Wage Bargaining', 'abstract': 'I analyze how the possibility of automating jobs impacts wages even in the absence of adoption of the automation technology. I build a multi-occupation search and bargaining model in which firms and workers bargain over wages and some occupations can be automated. A firm that can threaten to automate an occupation instead of hiring a worker has a higher outside option during the bargaining process. Thus, the possibility of automating improves the bargaining outcome of the firm and lowers the wage of the worker. Using data from the Current Population Survey and an index of automatability from the literature I show that, in line with the model, the threat of automation decreases workers’ wages, that this effect is more pronounced in labor markets where union intensity is higher, and that the return to experience in an occupation is negatively affected by the threat of automation. These results suggest that, even if only a small number of firms automate, automation technologies may still have a large effect on the labor market. ∗antoine.arnoud@yale.edu. I greatly thank my advisers Aleh Tsyvinski, Michael Peters, and Fabrizio Zilibotti for their continuous guidance and support throughout this project. For insightful discussions, I thank Costas Arkolakis, Bill Brainard, David Card, Ilse Lindenlaub, Alan Manning, Guy Michaels, Giuseppe Moscarini, Pascual Restrepo, John Roemer, Emmanuel Saez, Gabriel Zucman, participants at the Macroeconomics and Public Finance seminars at Yale. I also thank Taha Choukhmane, Louise Laage, and Haejin Lee. I acknowledge financial support from the Yale University MacMillan Center, the Cowles Foundation and the Washington Center for Equitable Growth .', 'corpus_id': 233990582, 'score': 1}, {'doc_id': '221521478', 'title': 'Industrialization Without Innovation', 'abstract': 'The introduction of labor-saving technologies in agriculture can release workers who find occupation in the manufacturing sector. The traditional view is that this structural transformation process leads to economic growth. However, if workers leaving agriculture are unskilled, the labor reallocation process reinforces comparative advantage in the least skill-intensive manufacturing industries. We embed this mechanism in a multi-sector endogenous growth model where only skill-intensive manufacturing industries innovate and generate knowledge spillovers. In this setup, the increase in the relative size of the unskilled-labor intensive industries reduces the incentives to innovate and slows down growth. We test the predictions of the model in the context of a large and exogenous increase in agricultural productivity in Brazil. We use social security data to develop a new measure of the labor input in innovation which is representative at any level of spatial aggregation. We find that regions adopting the new agricultural technology experienced a reallocation of unskilled workers away from agriculture into the least R&D-intensive manufacturing industries. The expansion of low-R&D industries attracted workers away from innovative occupations in high-R&D industries, slowing down local aggregate manufacturing productivity growth.', 'corpus_id': 221521478, 'score': 0}, {'doc_id': '86854488', 'title': 'Automation and Demographic Change', 'abstract': 'We analyze the effects of declining population growth on the adoption of automation technology. A standard theoretical framework of the accumulation of traditional physical capital and of automation capital predicts that countries with a lower population growth rate are the ones that innovate and/or adopt new automation technologies faster. We test the theoretical prediction by means of panel data for 60 countries over the time span from 1993 to 2013. Regression estimates provide empirical support for the theoretical prediction and suggest that a 1% increase in population growth is associated with approximately a 2% reduction in the growth rate of robot density. Our results are robust to the inclusion of standard control variables, the use of different estimation methods, the consideration of a dynamic framework with the lagged dependent variable as regressor, and changing the measurement of the stock of robots.', 'corpus_id': 86854488, 'score': 1}, {'doc_id': '232226833', 'title': 'How Does Automation Affect Economic Growth and Income Distribution in a Two-Class Economy?', 'abstract': 'This study uses a growth model with automation technology to consider two classes— workers and capitalists—and investigates how advances in automation technology affect economic growth and income distribution. In addition to the two production factors labor and traditional capital, we consider automation capital as the third production factor. We also introduce Pasinetti-type saving functions into the model to investigate how the difference between the capitalists’ and workers’ saving rates affect economic growth and income distribution. When the capitalists’ saving rate is higher than a threshold level, per capita output exhibits endogenous growth irrespective of the workers’ savings rate. In this case, the income gap between workers and capitalists widens over time. When the capitalists’ saving rate is less than the threshold level, two different long-run states occur depending on the workers’ saving rate: the capitalists’ own automation capital share approaches a constant, and it approaches zero. In both cases, the per capita output growth is zero and the income gap between the two classes becomes constant over time.', 'corpus_id': 232226833, 'score': 1}, {'doc_id': '233217845', 'title': 'Optimal Fiscal Policy in the Presence of Declining Labor Share∗', 'abstract': 'Numerous recent studies have documented that the labor’s share in national income, which has been quite stable until the early 1980’s, has been declining at a considerable rate since then. In this paper, we analyze the implications of this decline on the optimal capital and labor income taxation from the perspective of a government that needs to finance spending. Our main qualitative finding is that the optimal tax implications of the decline in the labor share depend on the mechanism responsible for it. In particular, if the labor share declines because of rising market power or other mechanisms that raise the share of profits in national income, then the decline in the labor share should optimally be accompanied with a rise in capital income taxes. If, on the other hand, the labor share declines because of a rise in capital share, then it has no bearing on optimal capital income taxation. In our baseline calibration, we find that the optimal tax rate on capital income rises about 5-10% from the early 1980’s to 2020 depending on the increase in profit share.', 'corpus_id': 233217845, 'score': 0}]
168	{'doc_id': '19556381', 'title': 'Oribatid Mite Communities in the Canopy of Montane Abies amabilis and Tsuga heterophylla Trees on Vancouver Island, British Columbia', 'abstract': 'Abstract To study the oribatid mite community inhabiting microhabitats in the canopy of montane Abies amabilis [(Douglas ex D. Don) Lindl.] and Tsuga heterophylla [(Raf.) Sarg] tree species across five elevational sites, we collected 180 branch tips and 180 foliose/crustose lichen samples over three time periods. Thirty-three species of oribatid mites were identified from the study area. Mite species richness and abundance was significantly affected by microhabitat, and this association was independent of sampling time. At the microhabitat scale, distinct species assemblages were associated with lichen and branch tip habitats, and to a lesser degree, tree species. Conifer specificity was most apparent in the closely related species of Jugatala, where Jugatala tuberosa Ewing was only found on branch tips from A. amabilis and Jugatala sp. was primarily found on branch tips from T. heterophylla. Microhabitat specificity was most pronounced in Dendrozetes sp. where most individuals were found on branch tips and Anachiperia geminus Lindo et al. that occurred primarily on lichens. Principal components analysis of oribatid mite community composition further showed a high degree of association with microhabitat and tree species. Habitat profiles are difficult to discern for many species because tree, microhabitat, and elevation preferences confound distribution patterns. Given the significant tree-microhabitat associations in species composition in this montane canopy study, we suggest that sampling multiple microhabitats across elevations to look for patterns in community structure offers opportunities to explicitly test organizing principles in community ecology.', 'corpus_id': 19556381}	16790	"[{'doc_id': '20027181', 'title': ""Does the Existence of Bird's Nest Ferns Enhance the Diversity of Oribatid (Acari: Oribatida) Communities in a Subtropical Forest?"", 'abstract': ""We examined the effects of the presence of bird's nest ferns on the species diversity of oribatid mites in the whole forest in terms of the three categories of species diversity (α-, β-, and γ-diversity) in a subtropical forest in south-western Japan. The species diversity (1\xa0−\xa0D) of oribatid communities in the ferns was significantly lower than those in bark of trees and the forest-floor litter and soil, and was similar to that in the branches. The oribatid faunas in the litter in and the roots of the fern were more similar to those in both the forest-floor litter and soil than to the faunas in the other arboreal habitats. However, the ferns can be colonized by endemic oribatid species specialized to such environments. The number of oribatid species estimated for a hypothetical stand with no ferns was about 180 species from 80 samples; this value did not differ significantly from that in another hypothetical stand with ferns (ca.\xa0190 species). Thus, the species richness of oribatid communities estimated for the whole forest (the γ-diversity) was not affected by the presence or absence of bird's nest ferns. The α- and β-diversities of oribatid communities on bird's nest ferns were lower than those in other habitats, and they might not dramatically raise the overall γ-diversity of invertebrate communities in the whole forest. The bird's nest ferns, however, can generate a unique habitat for specialized species, and this would help to maintain species diversities of invertebrates at the whole-forest scale in subtropical forests."", 'corpus_id': 20027181, 'score': 1}, {'doc_id': '233268296', 'title': 'Soil mites communities ( Acari : Oribatida , Mesostigmata ) in Kokorycz', 'abstract': 'Soil fauna is an important reservoir of biodiversity in forest ecosystems and plays an essential role in these ecosystems. The relationship between soil fauna groups from different trophic levels reflects well the conditions of the ecosystem, which is crucial especially for protected areas such as nature reserves. The aim of the study was to examine for the first time the soil mite (Oribatida and Mesostigmata) community (e.g. abundance, richness, and diversity) in the soil of the Kokorycz Nature Reserve located in the Lower Silesia Region.  In 2012, a  total of 50  soil  samples were collected using a  soil  corer  (10 cm2)  in  flood plain  forest  (40\xad120 y.o.), dominated by common oak and ash. Overall, 125 species (84 moss mites and 41 predators) were recorded in our study. The most abundant Oribatida were Conchogneta willmanni, Lauroppia fallax, Oppiella (O.) nova, and Rhinoppia subpectinata. Interestingly, ten oribatid  species were  recorded  from  the Lower  Silesia Region  for  the  first  time. Among Mesostigmata,  the most  common were Oodinychus ovalis, Paragamasus runcatellus, Paragamasus vagabundus, and Rhodacarus coronatus. Our study reported high soil fauna species diversity of this reserve and noted that the community is dominated by species typical for mature deciduous forests.', 'corpus_id': 233268296, 'score': 0}, {'doc_id': '233304528', 'title': 'Floristic Variation of Tree Communities In Island Forests of Pulau Tuba and Gunung Raya Forest Reserve, Langkawi', 'abstract': 'Island forests are among forest habitats that are vulnerable to natural and anthropogenic disturbances, whereby the disturbances would influence the survival of biological species of the ecosystems. Langkawi Archipelago contains many small island forests and rapid development of tourism industry within this archipelago might contribute impacts to the tree flora of the forest communities on the small islands. Hence, in this study the species richness and floristic variation pattern of tree communities of two selected island forests in the Langkawi Archipelago were explored, and data gathered are anticipated to be used for management of island forests in Langkawi. Tree survey was carried out in 10 study plots of 20m x 25m each, at island forests of Pulau Tuba Forest Reserve (PTB) and Gunung Raya Forest Reserve (GRFR), making the total of 20 study plots. All trees with diameter at breast height (dbh) of 5.0 cm and above were enumerated and tree species were identified. Species data were analyzed for diversity and richness using the Shannon and Margalef indices; whilst Detrended Correspondence Analysis (DCA) was used to determine floristic pattern. A total of 1062 trees were recorded from all study plots which comprised of 49 families, 134 genera and 213 tree species. The GRFR exhibited the highest species number of 135 tree species, followed by the PTB (106 tree species). Species accumulation curves showed that the curves were far from reaching the asymptote even when the whole dataset were combined. The DCA ordination diagram clearly grouped the study plots by their geological formation that indicated a gradient of species change in GRFR and PTB sites.', 'corpus_id': 233304528, 'score': 0}, {'doc_id': '4663586', 'title': 'Tropical nematode diversity: vertical stratification of nematode communities in a Costa Rican humid lowland rainforest', 'abstract': 'Comparisons of nematode communities among ecosystems have indicated that, unlike many organisms, nematode communities have less diversity in the tropics than in temperate ecosystems. There are, however, few studies of tropical nematode diversity on which to base conclusions of global patterns of diversity. This study reports an attempt to estimate nematode diversity in the lowland tropical rainforest of La Selva Biological Research Station in Costa Rica. We suggest one reason that previous estimates of tropical nematode diversity were low is because habitats above the mineral soil are seldom sampled. As much as 62% of the overall genetic diversity, measured by an 18S ribosomal barcode, existed in litter and understorey habitats and not in soil. A maximum‐likelihood tree of barcodes from 360 individual nematodes indicated most major terrestrial nematode lineages were represented in the samples. Estimated ‘species’ richness ranged from 464 to 502 within the four 40 × 40 m plots. Directed sampling of insects and their associated nematodes produced a second set of barcodes that were not recovered by habitat sampling, yet may constitute a major class of tropical nematode diversity. While the generation of novel nematode barcodes proved relatively easy, their identity remains obscure due to deficiencies in existing taxonomic databases. Specimens of Criconematina, a monophyletic group of soil‐dwelling plant‐parasitic nematodes were examined in detail to assess the steps necessary for associating barcodes with nominal species. Our results highlight the difficulties associated with studying poorly understood organisms in an understudied ecosystem using a destructive (i.e. barcode) sampling method.', 'corpus_id': 4663586, 'score': 1}, {'doc_id': '233544031', 'title': 'Does litter decomposition affect mite communities (Acari, Mesostigmata)? A five-year litterbag experiment with 14 tree species in mixed forest stands growing on a post-industrial area', 'abstract': 'Abstract Decomposition and topsoil microclimate, mainly humidity and soil temperature, affects the availability of nutrients, as well as the edaphon structure, including soil mites, springtails, nematodes, insects and oligochaetes. Soil arthropod decomposers are the food base for predators in the soil trophic chains, including mesostigmatid (gamasid) mites. The aim of our study was to describe the succession of mesostigmatid assemblages on decomposed litter of 14 tree species in mixed stands growing on a reclaimed post-mining site. We hypothesized that litter species would significantly affect the gamasid abundance, species richness and diversity. Moreover, we hypothesized that mesostigmatid abundance, species richness and diversity would be higher in deciduous litter compared to pioneer Scots pine litter. Additionally, along with the decomposition process, the gamasid diversity, species richness and abundance would be significantly affected by soil temperature and differ among collection dates, with the lowest values at the beginning and the end, and with the highest values in the middle of the study period. In December 2011, 1389 litterbags (mesh size\xa0=\xa01\xa0mm) containing leaf litter (initial dry mass\xa0=\xa08.004–10.772\xa0g) were placed on research plots and collected after 3, 6, 9, 13, 19, 25, 31, 37, 43 and 58\xa0months after the experiment started. We determined the percentage litter mass loss for each sample and mean soil temperature on each research plot. Soil microarthropods were extracted from litterbag samples using Berlese-Tullgren funnels, and mesostigmatid mites were selected using a stereomicroscope. The mites were identified to genus and/or species level including developmental stages, using a microscope with high magnification and acarological keys. In total, 19,296 gamasid mites were selected and classified into 52 taxa. The dominant species were Trachytes aegrota (C.L. Koch) (49.9% of all gamasid mites found), Veigaia nemorensis (C.L. Koch) (8.9%) and Gamasellodes bicolor (Berlese) (7.7%). We found that time of litterbag sampling did not affect abundance, species richness and diversity of gamasid mites. Moreover, litter species affected species richness and diversity of gamasid mite assemblages, however, abundance was affected when calculated per sample, but not when calculated per dry litter mass. Additionally, we found that mean soil temperature and percentage litter mass loss also significantly affected abundance and species richness, however, the impact on diversity was insignificant. Our results may help to better understand of the importance of soil fauna that has a decisive impact on soil-forming processes on degraded areas. Results could also help to choose the right tree species to improve revitalization of degraded areas by creating better conditions for edaphic fauna, including species not directly involved in the decomposition process.', 'corpus_id': 233544031, 'score': 0}, {'doc_id': '86279129', 'title': 'Seasonal changes in the nematode fauna in pine trees killed by the pinewood nematode, Bursaphelenchus xylophilus', 'abstract': 'The seasonal changes in the number of free-living nematodes and the pinewood nematode (PWN), Bursaphelenchus xylophilus, in 15-year-old Japanese black pine (Pinus thunbergii) trees killed by the PWN were determined. Fifteen species of nematodes were isolated, i.e. one species, each of Mononchida and Plectidae, two species, each of Monhisterida, Rhabditida and Tylenchida, three species of Bursaphelenchus including the PWN, and four species of Diplogasterida. The PWN and one species of Diplogasterida were the most prevalent nematodes isolated. The numbers of PWNs decreased from August to December, 2004, but then increased in February, 2005, then decreased again until June, 2005. During the experimental period the population changes of the Diplogasterida nematode mimicked those of the PWN, however, correlations between the numbers of the two nematodes varied considerably both among trees and seasons. Both the PWN and the Diplogasterida nematode were more abundant around the pupal chambers of the vector beetle, Monochamus alternatus, than elsewhere in the tree wood. Jpn. J. Nematol. 36 (2), 87-100 (2006).', 'corpus_id': 86279129, 'score': 1}, {'doc_id': '233866620', 'title': 'Ectomycorrhizal fungal communities associated with Larix gemelinii Rupr. in the Great Khingan Mountains, China', 'abstract': 'Larix gemelinii is an important tree species in the Great Khingan Mountains in Northeast China with a high economic and ecological value for its role in carbon sequestration and as a source of lumber and nuts. However, the ectomycorrhizal (EM) fungal diversity and community composition of this tree remain largely undefined. We examined EM fungal communities associated with L. gemelinii from three sites in the Great Khingan Mountains using Illumina Miseq to sequence the rDNA ITS2 region and evaluated the impact of spatial, soil, and climatic variables on the EM fungal community. A total of 122 EM fungal operational taxonomic units (OTUs) were identified from 21 pooled-root samples, and the dominant EM fungal lineages were /tricholoma, /tomentella-thelephora, /suillus-rhizopogon, and /piloderma. A high proportion of unique EM fungal OTUs were present; some abundant OTUs largely restricted to specific sites. EM fungal richness and community assembly were significantly correlated with spatial distance and climatic and soil variables, with mean annual temperature being the most important predictor for fungal richness and geographic distance as the largest determinant for community turnover. Our findings indicate that L. gemelinii has a rich and distinctive EM fungal community contributing to our understanding of the montane EM fungal community structure from the perspective of a single host plant that has not been previously reported.', 'corpus_id': 233866620, 'score': 0}, {'doc_id': '128784977', 'title': 'Structure and microclimate of forest canopies.', 'abstract': None, 'corpus_id': 128784977, 'score': 1}, {'doc_id': '126881787', 'title': 'Soil Mites (Oribatids) Climbing Trees', 'abstract': 'In general conception, habitats of oribatid mites are soils including mosses, litter, rotten woods and other plant detritus on the soil surface. As Trave (1963) and a few other acarologists took notice, however, some species of oribatid mites are often found on living trees and they all have been considered as species wandering between soils and trees. Thus, we have had no doubt that a tree fauna of oribatid mites, if exists, is quite dependent upon the soil fauna under the tree in question. Having investigated the Japanese oribatid fauna in soils as well as on trees, I came to the conclusion, however, that this conception seems to be wrong.', 'corpus_id': 126881787, 'score': 1}, {'doc_id': '234685719', 'title': 'Title: Communities of mites (Acari) in litter and soil under the invasive red oak (Quercus rubra L.) and native pedunculate oak (Q. robur L.) Author:', 'abstract': 'Because of thoughtless decisions or unintentional introduction, alien species disturb native ecosystems. red oak (Quercus rubra), among other alien woody plants, is still used to rehabilitate degraded land because of its better resistance to pollution and faster growth, as compared to native tree species. Soil mites, especially oribatida, are good bioindicators of ecosystem disturbance, so the main goal of this study was to explore the influence of invasive and native oaks on mite communities. forest stands dominated by 40-year-old Q. rubra or 35-year-old Q. robur were compared. over 2300 soil mites were extracted from 20 soil and 20 litter samples. mite densities in the communities were higher in red oak litter, which is probably a result of the thicker layer of shed leaves. changes in species composition of oribatid communities were observed in litter, in contrast to a lack of differences in soil. These observations are consistent with other researches on invasive woody plants. we expect that over time these changes will also be noticeable in the soil and will increase in litter.', 'corpus_id': 234685719, 'score': 0}]"
169	{'doc_id': '233438802', 'title': 'PREDICTION OF SOIL AVAILABLE PHOSPHOROUS CONTENT USING SPECTRA-RADIOMETER AND GIS IN SOUTHERN OF IRAQ', 'abstract': 'In this study, soil samples were collected from two locations: Samawa and Rumetha in southern Iraq. The samples from each location were split into two datasets: calibration set and validation set. A representative soil sample for each location was chosen for application of 5 levels of potassium phosphate fertilizer in 3 replications. Vis-NIR reflectance (350-2500 nm) and GIS-Kriging were used in combination with Partial Least Square (PLS) to predict soil available P. According to the results of this study, three wavelength regions were reported as a main sensitive bands for soil available P. The best prediction ability was achieved for Rumetha location at 1400-1600 nm with an R 2 of 0.85, lowest RMSE of 1.405, and lowest standard deviation of 1.577 and for Samawa location at 900-1000 nm with an R 2 of 0.81, RMSE of 2.666 and lowest standard deviation of 2.879. The capability of the Vis-NIRSbased and GIS-Kriging prediction models were evaluated by using cross-validation values Q 2 and R 2 between measured and predicted soil available P of each model. The selection principle parameters showed best prediction by NIRS-models with an R 2 of 0.79 for Rumetha and 0.75 for Samawa location. While the prediction ability of GIS-Kriging models were in worst with an Q 2 of 0.17 for Samawa location and reasonable with an Q 2 of 0.58 for Rumetha location. These empirically result is an evident of the superiority of NIRS-based models for prediction soil available P over the GIS-Kriging models.', 'corpus_id': 233438802}	10895	"[{'doc_id': '233868998', 'title': 'Forensic soil provenancing in an urban/suburban setting: A sequential multivariate approach', 'abstract': ""Compositional data from a soil survey over North Canberra, Australian Capital Territory, are used to develop and test an empirical soil provenancing method. Mineralogical data from Fourier transform infrared spectroscopy (FTIR) and magnetic susceptibility (MS), and geochemical data from X‐ray fluorescence (XRF; for total major oxides) and inductively coupled plasma‐mass spectrometry (ICP‐MS; for both total and aqua regia‐soluble trace elements) are performed on the survey's 268 topsoil samples (0–5 cm depth; 1 sample per km2). Principal components (PCs) are calculated after imputation of censored data and centered log‐ratio transformation. The sequential provenancing approach is underpinned by (i) the preparation of interpolated raster grids of the soil properties (including PCs); (ii) the explicit quantification and propagation of uncertainty; (iii) the intersection of the soil property rasters with the values of the evidentiary sample (± uncertainty); and (iv) the computation of cumulative provenance rasters (“heat maps”) for the various analytical techniques. The sequential provenancing method is tested on the North Canberra soil survey with three “blind” samples representing simulated evidentiary samples. Performance metrics of precision and accuracy indicate that the FTIR and MS (mineralogy), as well as XRF and total ICP‐MS (geochemistry) analytical methods, offer the most precise and accurate provenance predictions. Inclusion of PCs in provenancing adds marginally to the performance. Maximizing the number of analytes/analytical techniques is advantageous in soil provenancing. Despite acknowledged limitations and gaps, it is concluded that the empirical soil provenancing approach can play an important role in forensic and intelligence applications."", 'corpus_id': 233868998, 'score': 0}, {'doc_id': '233545831', 'title': 'Soil nutrient information extraction model based on transfer learning and near infrared spectroscopy', 'abstract': 'Abstract This study aims to combine the transfer learning algorithm and near-infrared spectroscopy technology to build a soil nutrient information extraction model. We determine the soil nutrients with near-infrared spectroscopy technology; moreover, the characteristic bands of soil nutrients were also determined for preprocessing the soil nutrient information. Then, combined with the transfer learning algorithm, soil nutrient information extraction model was constructed. Simulation results showed that the proposed model could improve the efficiency of soil nutrient information extraction, and also obtain high-confidence feature extraction results. This will solve the problem of low-accuracy and efficiency of soil nutrient information extraction through traditional models.', 'corpus_id': 233545831, 'score': 1}, {'doc_id': '235418049', 'title': 'Variation in the content and fluorescent composition of dissolved organic matter in soil water during rainfall-induced wetting and extract of dried soil.', 'abstract': 'Dissolved organic matter (DOM) is present in all soils, providing a readily available carbon source for microorganisms, which influences microbially mediated biogeochemical processes. Rainfall-induced wetting can alter the content and composition of soil DOM. However, conventional methods commonly used to extract DOM from soils involve air- or oven-drying followed by extraction with water, and the results vary considerably in terms of indexes used. Therefore, this study aimed to determine the variation in DOM content and composition of soil gravity water and capillary water during wetting, and establish a better method to obtain real soil DOM information. Following simulated rainfall (50 mm h-1, 0-24 h), gravity water and capillary water in fresh soil samples were separated using a high-speed refrigerated centrifuge. Additionally, DOM in dried soil samples was extracted using various soil/water ratios after drying by different methods. The DOM data obtained by conventional methods were compared with capillary water data. The results showed that DOM degradation occurred mainly in capillary water close to the surface of soil particles. Among the six fluorescent components of DOM identified, a tryptophan-like component (Ex/Em = 295/335 nm and 230/335 nm) was possibly derived from terrestrial plants, and a tyrosine-like component (Ex/Em = 265/305 nm) was likely derived from microbial secretion. Except for little variation in the fluorescence index, dissolved organic carbon concentrations in capillary water were double those in dried soil extracted by conventional methods. The humification index and spectral slope ratio of DOM extracted by conventional methods also markedly varied, and no clear patterns were observed for the variation in specific UV absorbance at 254 nm. These findings allow real information to be obtained regarding soil DOM during wetting, and better selection of the extraction method and indexes when studying soil DOM.', 'corpus_id': 235418049, 'score': 0}, {'doc_id': '235651515', 'title': 'Can Agricultural Management Induced Changes in Soil Organic Carbon Be Detected Using Mid-Infrared Spectroscopy?', 'abstract': 'A major limitation to building credible soil carbon sequestration programs is the cost of measuring soil carbon change. Diffuse reflectance spectroscopy (DRS) is considered a viable low-cost alternative to traditional laboratory analysis of soil organic carbon (SOC). While numerous studies have shown that DRS can produce accurate and precise estimates of SOC across landscapes, whether DRS can detect subtle management induced changes in SOC at a given site has not been resolved. Here, we leverage archived soil samples from seven long-term research trials in the U.S. to test this question using mid infrared (MIR) spectroscopy coupled with the USDA-NRCS Kellogg Soil Survey Laboratory MIR spectral library. Overall, MIR-based estimates of SOC%, with samples scanned on a secondary instrument, were excellent with the root mean square error ranging from 0.10 to 0.33% across the seven sites. In all but two instances, the same statistically significant (p < 0.10) management effect was found using both the lab-based SOC% and MIR estimated SOC% data. Despite some additional uncertainty, primarily in the form of bias, these results suggest that large existing MIR spectral libraries can be operationalized in other laboratories for successful carbon monitoring.', 'corpus_id': 235651515, 'score': 1}, {'doc_id': '233550254', 'title': 'Calibration methods for measuring the color of moist soils with digital cameras', 'abstract': 'Abstract The study of soil color is important for soil classification and as a diagnostic criterion reflecting a range of soil processes and soil properties. Field measurements of soil colors are affected by moisture content and other environmental and lighting condition. Here we evaluated calibration procedures to obtain colorimetrically corrected soil profile images on field moist soils using a gray card or histogram of reference soil images. In the laboratory, we further evaluated the colorimetric calibration methods on moist soil samples with varying moisture content and texture using an external scale (color scale with air-dry soil standards with known colors) or an internal scale (moist reference soil samples). The colors measured by a spectrometer were used as true colors of moist soil samples. The CIE L*a*b* color difference (ΔEab*) was used to evaluate the colorimetric accuracy between colors obtained from different devices. The results showed that the soil colors were greatly affected by soil moisture content. With the decrease in soil moisture, a significant increase in lightness (L*) and yellowness (b*) was observed. The field experiment showed that the colorimetrically accurate soil profile images at natural moist condition cannot be obtained by calibrating the digital image using a gray card or using the image of another camera. The laboratory experiments showed that colorimetrically accurate images can be obtained for sandy loam soil at a moisture content of less than 12% using the external standard. For soils with different texture, color characteristics, and moisture content (5–40%), colorimetrically accurate images of the samples can be obtained by using the internal scale. We conclude that the proposed calibration methods can be used to obtain colors of moist soils.', 'corpus_id': 233550254, 'score': 1}, {'doc_id': '234235322', 'title': 'An Evaluation of Some Machine Learning Algorithms as Tools for Predicting Soil Characteristics Based on Their Spectral Response in the Vis‑NIR Range', 'abstract': 'Using the Land Use and Coverage Frame Survey (LUCAS) database of Euro‐ pean soil surface layer properties, statistical and machine learning predictive models for several key soil characteristics (clay content, pH in CaCl2, concentra‐ tion of organic carbon, calcium carbonates and nitrogen and exchange cations capacity) were compared on the basis of processing their spectral responses in the visible (Vis) and near ‐infrared (NIR) parts. Standard methods of rela‐ tionship modeling were used: stepwise regression, partial least squares regres‐ sion and linear regression with input data obtained from principal components analysis. Using the inputs extracted by statistical algorithms various machine learning algorithms were used in the modeling. The usefulness of the models was analyzed by comparison with the values of the determination coefficients, the root mean square error and the distribution of residual values. The mean square error of estimation in the cross ‐validation procedure for the stack mod‐ el using the multilayer perceptron and the distributed random forest were as follows: for clay content – ca. 4.5%; for pH – ca. 0.35; for SOC – ca. 7.5 g/kg (0.75% by weight); for CaCO3 content – ca. 19 g/kg; for N content – ca. 0.50 g/kg; and for CEC – ca. 3.5 cmol(+)/kg.', 'corpus_id': 234235322, 'score': 1}, {'doc_id': '235307913', 'title': 'Estimating Soil Organic Matter Content Using Sentinel-2 Imagery by Machine Learning in Shanghai', 'abstract': 'Soil organic matter (SOM) plays an important role in the field of climate change and terrestrial ecosystems. SOM in large areas, especially in urban areas, is difficult to monitor and estimate by traditional methods. Urban land structure is complex, and soil is a mixture of organic and inorganic constituents with different physical and chemical properties. Previous studies showed that remote sensing techniques that provide diverse data in the visible-near-infrared (VNIR)-shortwave infrared (SWIR) spectral range, are promising in the prediction of SOM content on a large scale. Sentinel-2 covers the important spectral bands (VNIR-SWIR) for SOM prediction with a short revisit time. Thus, this article aimed to evaluate the capacity of Sentinel-2 for SOM prediction in an urban area (i.e., Shanghai). 103 bare soil samples filtrated from 398 soil samples at a depth of 20 cm were selected. Three methods, partial least square regression (PLSR), artificial neural network (ANN), and support vector machine (SVM), were applied. The root mean square error (RMSE) of modelling (mRMSE) and the coefficient of determination (<inline-formula> <tex-math notation=""LaTeX"">$R^{2}$ </tex-math></inline-formula>) of modelling <inline-formula> <tex-math notation=""LaTeX"">$(mR^{2})$ </tex-math></inline-formula> were used to reflect the accuracy of the model. The results show that PLSR has the poorest performance. ANN has the highest modelling accuracy (mRMSE = 7.387 g kg<inline-formula> <tex-math notation=""LaTeX"">$^{-1}$ </tex-math></inline-formula>, <inline-formula> <tex-math notation=""LaTeX"">$mR^{2}=0.446$ </tex-math></inline-formula>). The ANN prediction accuracy of RMSE (pRMSE) is 4.713 g kg<inline-formula> <tex-math notation=""LaTeX"">$^{-1}$ </tex-math></inline-formula> and the prediction accuracy of <inline-formula> <tex-math notation=""LaTeX"">$R^{2}~(pR^{2})$ </tex-math></inline-formula> is 0.723. For SVR, the pRMSE is 4.638 g kg<inline-formula> <tex-math notation=""LaTeX"">$^{-1}$ </tex-math></inline-formula>, and the <inline-formula> <tex-math notation=""LaTeX"">$pR^{2}$ </tex-math></inline-formula> is 0.732. The prediction accuracy of SVR is slightly higher than that of ANN. The spatial distribution of SOM demonstrates that the value obtained by ANN is the closest to the range of the bare soil samples, and ANN performs better in vegetation-covered areas. Therefore, Sentinel-2 can be used to estimate SOM content in urban areas, and ANN is a promising method for SOM estimation.', 'corpus_id': 235307913, 'score': 1}, {'doc_id': '234221223', 'title': 'Fast, simultaneous and contactless assessment of intact mango fruit by means of near infrared spectroscopy', 'abstract': 'This study aims to apply near infrared technology as a fast, simultaneous and non-destructive method for quality assessment on intact mango fruit in form of total soluble solids (TSS) and vitamin C. Absorbance spectra of 186 intact mango fruits with four different cultivars were acquired and recorded in wavelength ranging from 1000–2500 nm. Spectra data were enhanced and corrected using three different methods namely moving average smoothing (MAS), extended multiplicative scatter correction (EMSC) and standard normal variate (SNV). In addition, they were divided into two datasets namely calibration (n = 143) and prediction (n = 43) datasets consisting all four mango cultivars. The models used to predict TSS and vitamin C were developed using partial least square regression (PLSR). Prediction performance were quantified using correlation coefficient (r), root mean square error (RMSE), ratio prediction to deviation (RPD) and range to error ratio (RER) indexes. The results showed that the best prediction models for TSS and vitamin C were achieved when the models were constructed using EMSC correction approach with r = 0.86, RMSE = 1.67 Brix, RPD = 2.34 and RER = 9.72 for TSS. Meanwhile, for vitamin C, r = 0.86, RMSE = 6.84 mg·100g−1, RPD = 2.00 and RER = 8.87. From this study, it was concluded that near infrared technology combined with proper spectra enhancement method may be applied as a rapid, simultaneous and contactless method for quality assessment on intact mangoes.', 'corpus_id': 234221223, 'score': 0}, {'doc_id': '233581819', 'title': 'Transfer learning strategy for plastic pollution detection in soil: Calibration transfer from high-throughput HSI system to NIR sensor', 'abstract': 'Abstract Rapid detection tasks in soil environment are generally implemented by various spectrometers and chemometric models. To reduce costs for model construction, calibration transfer from laboratory spectral instruments to portable devices has recently received extensive attention. In different application cases of model transference, most conventional methods require extra time to tune hyperparameters and to select calibration transfer techniques. Based on the near-infrared (NIR) analytical technique, this work aimed at exploring a transfer learning strategy to detect plastic pollution levels in the soil by transferring the model from a high-throughput hyperspectral image (HSI) system to an ultra-portable NIR sensor. Transfer learning was explored to diagnose the proper calibration transfer algorithm and construct the transferable model. For transferable model construction, conventional calibration transfer algorithms (Direct Standardization (DS) or Repeatability file (Repfile)) served as a pre-processing step, and non-parametric transfer learning algorithm (Easy Transfer Learning (EasyTL)) was explored in the modeling step. Supporting vector machine (SVM) was carried out as a typical modeling algorithm for comparison. For transformation algorithms selection, a distance metric algorithm, maximum mean discrepancy (MMD), was performed on spectral feature matrices before and after DS or Repfile transformation. On three transfer tasks, the results indicated that the Repfile-EasyTL model was a promising solution with higher accuracy, much lower time costs, less parameters, and dependency on the increase of standard samples than other models (SVM, DS-SVM, Repfile-SVM, EasyTL, DS-EasyTL). Moreover, MMD distance presented the great potential to serve as an indicator to vote the optimal calibration transfer algorithm before the modeling step.', 'corpus_id': 233581819, 'score': 0}, {'doc_id': '212830043', 'title': 'Custom-engineered micro-habitats for characterizing rhizosphere interactions', 'abstract': 'The interactions amongst plants and microorganisms within the rhizosphere have a profound influence on global biogeochemical cycles, and a better understanding of these interactions will benefit society through improved climate change prediction, increased food security, and enhanced bioenergy production. However, the rhizosphere is one of the most complex and bio-diverse ecosystems on earth, making it difficult to parse apart specific interactions between species. This difficulty is compounded by the inability to directly visualize rhizosphere interactions through the soil. Additionally, conventional laboratory techniques do not offer real-time, high-resolution visualization or the proper environmental control to isolate and probe these interactions. A knowledge gap persists in how to design appropriate culturing platforms that allow researchers to collect spatially and temporally sensitive information about physical and chemical interactions in the rhizosphere. This dissertation addresses that gap by demonstrating the design and use of several customengineered micro-habitats in characterizing plant-microbe interactions. Specifically this thesis introduces novel protocols for culturing plants and microorganisms together in microfluidic platforms, pairing platforms to multi-modal imaging techniques with organelle scale resolution, and recreating the structural complexity of the rhizosphere in a microfluidic habitat. Not only does this thesis introduce novel engineered systems, but the work contained herein also goes beyond proof-of-concept experiments and demonstrates the ability of these platforms to generate hypotheses and answer outstanding biological questions.', 'corpus_id': 212830043, 'score': 0}]"
170	{'doc_id': '89443524', 'title': 'The Effects of Naturally Occurring Phenolic Compounds on Seed Germination', 'abstract': 'Caffeic acid, chlorogenic acid, coumarin, p - coumaric acid, ferulic acid, fumaric acid, gallic acid, hydro - cinnamic acid, p - hydroxybenzoic acid, juglone, and pyro - catechol were examined for effects on germination of nine crop and weed species: cotton (Gossypium hirsutum L.), cantaloupe (Cucumis melo L.), corn (Zea mays L.), sorghum (Sorghum bicolor (L.) Moench), hemp sesbania (Sesbania exaltata (Raf.) Cary), sicklepod (Cassia obtusifolia L.), velvet- leaf (Abutilon theophrasti Medic.), prickly sida (Sida spin osa L.), and redroot pigweed (Amaranthus retroflexus L.). Germi- nation tests with 10-3 and 10-5M solutions were conducted under controlled conditions in petri dishes at 25 C in the dark. At 10-3 M, coumarin, hydrocinnamic acid, juglone and pyro- catechol inhibited germination, but p-hydroxybenzaldehyde and p-hydroxybenzoic acid were not effective and others had intermediate effects. There was little effect by any com- pound at 10-5 M. Chlorogenic acid, p - hydroxybenzaldehyde, and pyrocatechol, each combined with coumarin, inhibited germination. The combination of coumarin plus p - hydroxy - benzaldehyde had an additive effect on hemp sesbania and prickly sida, inhibiting germination to a greater extent than either compound alone. The lack of inhibitory action at the higher concentration of some of these chemicals suggests they may not exhibit a high allelopathic potential. Additional index words. Allelopathy, secondary plant com- pounds.', 'corpus_id': 89443524}	10957	"[{'doc_id': '30213683', 'title': 'Nanofertilizers: New Products for the Industry?', 'abstract': ""Mineral fertilizers are key to food production, despite plant low nutrient uptake efficiencies and high losses. However, nanotechnology can both enhance crop productivity and reduce nutrient losses. This has raised interest in nanoscale and nanoenabled bulk fertilizers, hence the concept of nanofertilizers. Nevertheless, large-scale industrial production of nanofertilizers is yet to be realized. Here, we highlight the science-based evidence and outstanding concerns for motivating fertilizer industry production of nanofertilizers, including the notion of toxicity associated with nanoscale materials; scant nanofertilizer research with key crop nutrients; inadequacy of soil- or field-based studies with nanofertilizers; type of nanomaterials to produce as fertilizers; how to efficiently and effectively apply nanofertilizers at the field scale; and the economics of nanofertilizers. It is anticipated that the development and validation of nanofertilizers that are nondisruptive to existing bulk fertilizer production systems will motivate the industry's involvement in nanofertilizers."", 'corpus_id': 30213683, 'score': 1}, {'doc_id': '227121838', 'title': 'A review on hospital wastewater treatment: A special emphasis on occurrence and removal of pharmaceutically active compounds, resistant microorganisms, and SARS-CoV-2', 'abstract': '\n The hospital wastewater imposes a potent threat to the security of human health concerning its high vulnerability towards the outbreak of several diseases. Furthermore, the outbreak of COVID-19 pandemic demanded a global attention towards monitoring viruses and other infectious pathogens in hospital wastewater and their removal. Apart from that, the presence of various recalcitrant organics, pharmaceutically active compounds (PhACs), etc. imparts a complex pollution load to water resources and ecosystem. In this review, an insight into the occurrence, persistence and removal of drug-resistant microorganisms and infectious viruses as well as other micro-pollutants have been documented. The performance of various pilot/full-scale studies have been evaluated in terms of removal of biochemical oxygen demand (BOD), chemical oxygen demand (COD), total suspended solids (TSS), PhACs, pathogens, etc. It was found that many biological processes, such as membrane bioreactor, activated sludge process, constructed wetlands, etc. provided more than 80% removal of BOD, COD, TSS, etc. However, the removal of several recalcitrant organic pollutants are less responsive to those processes and demands the application of tertiary treatments, such as adsorption, ozone treatment, UV treatment, etc. Antibiotic-resistant microorganisms, viruses were found to be persistent even after the treatment of hospital wastewater, and high dose of chlorination or UV treatment was required to inactivate them. This article circumscribes the various emerging technologies, which have been used to treat PhACs and pathogens. The present review also emphasized the global concern of the presence of SARS-CoV-2 RNA in hospital wastewater and its removal by the existing treatment facilities.\n', 'corpus_id': 227121838, 'score': 0}, {'doc_id': '227070180', 'title': 'EMPIRICAL STUDY OF KEY SUCCESS FACTORS OF CLOSED SUPPLY CHAIN FOR VEGETABLES: THE CASE OF EXPORT FROM YUNAN TO THAILAND', 'abstract': 'This paper analyzes the key success factors of the closed supply chain for vegetables exported from Yunnan and Thailand by empirical study, with the aim of guaranteeing the quality and safety of vegetables. After the introduction, literature reviews with the key term of vegetable quality and safety are presented. Followed section 3 indicates data sources and methodologies from both quantitative and qualitative approaches. Then operation results and discussion are shown to determine the key success factors. The last section proposes some recommendations for member enterprises and related government departments.', 'corpus_id': 227070180, 'score': 0}, {'doc_id': '227122083', 'title': 'Global crop waste burning – micro-biochar; how a small community development organization learned experientially to address a huge problem one tiny field at a time', 'abstract': 'The world’s 2.5 billion poorest people - small farmers living at the far fringe of the developing world – and their billion or so slightly better off neighbors burn 10.5 billion metric tonnes (tonnes) of crop waste annually. Smoke from their fires reddens the sun, closes airports, shuts schools and governments – and kills millions of people (World Health Organization (WHO). who.int/health-topics/air-pollution#tab=tab_1). Their fires release 16.6 billion tonnes of CO 2 , and emit 9.8 billion tonnes CO 2 e, 1.1 billion tonnes of smog precursors and 66 million tonnes of PM2.5. (Akagi et al., Atmospheric Chem Physics 4039-4071, 2011; Environmental Protection Agency, epa.gov/ghgemissions/understanding-global-warming-potentials; Food and Agriculture Organization, FAOSTAT, http://www.fao.org/faostat/en/#data ) [See Attachments 1–3. For details of the Attachments, please see the section below entitled “Availability of data and materials.”]. No one yet has stopped the burning. Seminars, health warnings, bans, threats, jailings, shootings – nothing has worked, because not one has offered farmers a better alternative. This is the story of how Warm Heart, a small, community development NGO, learned enough about small farmers’ plight to collaborate with them to develop the technology, training and social organization to mobilize villages to form biochar social enterprises. These make it profitable for farmers to convert crop waste into biochar, reducing CO 2 e, smog precursor and PM2.5 emissions, improving health and generating new local income – in short, to address the big three SDGs (1, 2 and 3) from the bottom. Warm Heart, however, wanted more; it wanted a system so appealing that it would spread by imitation and not require outside intervention. Based on what it has learned, Warm Heart wants to teach others that the knowledge to stop the smoke and improve the quality of one’s life does not require outside experts and lots of money. It wants to teach that anyone can learn to create a more sustainable world by themselves. This article traces the experiential learning process by which Warm Heart and its partners achieved their goals and shares Warm Heart’s open source solution. It serves four purposes. The article closely explores an experiential learning process. It details the underlying logic, workings and consequences of crop waste burning in the developing world. It demonstrates the application of this knowledge to the development of a sustainable – even profitable – solution to this global problem that does not require costly outside intervention but can be undertaken by local communities and small NGOs anywhere. Finally, it models how local communities, small NGOs and social investors can turn this global problem into a profitable business opportunity.', 'corpus_id': 227122083, 'score': 0}, {'doc_id': '181477965', 'title': 'Biostimulant Application with a Tropical Plant Extract Enhances Corchorus olitorius Adaptation to Sub-Optimal Nutrient Regimens by Improving Physiological Parameters', 'abstract': 'The emerging role of plant biostimulants in enhancing nutrient efficiency is important for maintaining soil fertility under sub-optimal nutrient regimens. We aimed to elucidate the morpho-physiological and biochemical effects as well as mineral composition changes of greenhouse jute (Corchorus olitorius L.) treated with a commercial vegetal-derived biostimulant from a tropical plant extract (PE; Auxym®, Italpollina, Rivoli Veronese, Italy). Plants were sprayed in weekly intervals with a solution containing 2 mL·L−1 PE. Jute plants were supplied with three nutrient solution concentrations: full-, half-, and quarter-strength. Decreasing macronutrient concentrations in the nutrient solution (NS), especially at quarter-strength, triggered a decrease in several morphological (plant height, leaf number, and dry biomass) and physiological (net CO2 assimilation rate (ACO2) and SPAD (Soil Plant Analysis Development) index) parameters. PE application triggered specific ameliorative effects in terms of fresh yield at both half- and quarter-strength nutrient solution (15.5% and 29.5%, respectively). This was associated with an enhancement in ACO2, SPAD index, and especially the nutritional status (high nitrate, K, and Mg contents, and low Na content). The foliar application of PE, strongly increased chlorophyll b content, enhancing jute plant adaptation to fluctuating light and therefore the efficiency of photosynthesis, positively affecting starch, soluble proteins, and total amino acids content but only when jute plants were irrigated with full-strength NS, compared to the respective control treatment. At lower nutrient strength, PE reprogrammed the nitrogen distribution, allowing its remobilization from glutamate, which was quantitatively the major amino acid under lower nutrient strength, but not from chlorophylls, thus maintaining efficient photosynthesis. We confirmed that PE Auxym® acts in a balanced manner on the main metabolic pathways of the plant, regulating the uptake and transport of mineral nutrients and protein synthesis, increasing the accumulation of essential amino acids under full nutritive solutions, and re-distributing nitrogen from amino acids to allow leaf growth and expansion even under sub-optimal nutrient conditions. Overall, the use of natural plant biostimulants may be a potential solution in low-input conditions, where environmental constraints and restricted use of fertilizers may affect potential crop productivity.', 'corpus_id': 181477965, 'score': 1}, {'doc_id': '236233405', 'title': 'Effect of condensed tannins on growth performance, intestinal immune capacity and bacterial microbiomes of\n Lateolabrax japonicus', 'abstract': None, 'corpus_id': 236233405, 'score': 1}, {'doc_id': '7944243', 'title': 'Plant-borne flavonoids released into the rhizosphere: impact on soil bio-activities related to plant nutrition. A review', 'abstract': 'Plants produce and release in the surrounding soil, the so-called rhizosphere, a vast variety of secondary metabolites. Among them, flavonoids are the most studied, mainly for their role in the establishment of rhizobium–legume symbiosis; on the other hand, some studies highlight that they are also important in the plant strategies to acquire nutrients from the soil, for example, by acting on its chemistry. The scope of this review is to give a quick overview on the types and amounts of plant-released flavonoids in order to focus on their effects on soil activities that in turn can influence nutrient availability and so plant mineral nutrition; emphasis is given to the different nutrient cycles, soil enzyme, and soil bacteria activities, and their influence on soil macrofauna and roots of other plants. Finally, the possible outcome of the climate change on these processes is discussed.', 'corpus_id': 7944243, 'score': 1}, {'doc_id': '227081386', 'title': 'Nutritive Values and Importance of Tropical Green Leafy Vegetables in Human Diet – A Review', 'abstract': 'India, endowed with climatic conditions provides opportunities for growing an array of green leafy vegetable, which form second important category of vegetables. These green leafy vegetables are mostly rich in essential minerals, vitamins and dietary fibre and have therapeutic properties. With the wake of the novel coronavirus disease (COVID-19) pandemic, there is need to boost our immune systems to prevent from the rate of disease incidence (Padhee and Joanna Kane-Potaka, 2020). Consuming green leafy vegetables are the right and smart way to augment the meet out the nutritional requirements necessary for good health and wellbeing.', 'corpus_id': 227081386, 'score': 0}, {'doc_id': '3445984', 'title': 'How Plant Root Exudates Shape the Nitrogen Cycle.', 'abstract': 'Although the global nitrogen (N) cycle is largely driven by soil microbes, plant root exudates can profoundly modify soil microbial communities and influence their N transformations. A detailed understanding is now beginning to emerge regarding the control that root exudates exert over two major soil N processes - nitrification and N2 fixation. We discuss recent breakthroughs in this area, including the identification of root exudates as nitrification inhibitors and as signaling compounds facilitating N-acquisition symbioses. We indicate gaps in current knowledge, including questions of how root exudates affect newly discovered microbial players and N-cycle components. A better understanding of these processes is urgent given the widespread inefficiencies in agricultural N use and their links to N pollution and climate change.', 'corpus_id': 3445984, 'score': 1}, {'doc_id': '227126945', 'title': 'Mixing strategies combined with shape design to enhance productivity of a raceway pond', 'abstract': 'This paper focuses on mixing strategies and designing shape of the bottom topographies to enhance the growth of the microalgae in raceway ponds. A physical-biological coupled model is used to describe the growth of the algae. A simple model of a mixing device such as a paddle wheel is also considered. The complete process model was then included in an optimization problem associated with the maximization of the biomass production. The results show that non-trivial topographies can be coupled with some specific mixing strategies to improve the microalgal productivity.', 'corpus_id': 227126945, 'score': 0}]"
171	{'doc_id': '214272340', 'title': 'Context-Aware Personalized Web Search Using Navigation History', 'abstract': 'It is highly desirable that web search engines know users well and provide just what the user needs. Although great effort has been devoted to achieve this dream, the commonly used web search engines still provide a “one-fit-all” results. One of the barriers is lack of an accurate representation of user search context that supports personalised web search. This article presents a method to represent user search context and incorporate this representation to produce personalised web search results based on Google search results. The key contributions are twofold: a method to build contextual user profiles using their browsing behaviour and the semantic knowledge represented in a domain ontology; and an algorithm to re-rank the original search results using these contextual user profiles. The effectiveness of proposed new techniques were evaluated through comparisons of cases with and without these techniques respectively and a promising result of 35% precision improvement is achieved.', 'corpus_id': 214272340}	5160	"[{'doc_id': '215737055', 'title': 'CSRN: Collaborative Sequential Recommendation Networks for News Retrieval', 'abstract': 'Nowadays, news apps have taken over the popularity of paper-based media, providing a great opportunity for personalization. Recurrent Neural Network (RNN)-based sequential recommendation is a popular approach that utilizes users\' recent browsing history to predict future items. This approach is limited that it does not consider the societal influences of news consumption, i.e., users may follow popular topics that are constantly changing, while certain hot topics might be spreading only among specific groups of people. Such societal impact is difficult to predict given only users\' own reading histories. On the other hand, the traditional User-based Collaborative Filtering (UserCF) makes recommendations based on the interests of the ""neighbors"", which provides the possibility to supplement the weaknesses of RNN-based methods. However, conventional UserCF only uses a single similarity metric to model the relationships between users, which is too coarse-grained and thus limits the performance. In this paper, we propose a framework of deep neural networks to integrate the RNN-based sequential recommendations and the key ideas from UserCF, to develop Collaborative Sequential Recommendation Networks (CSRNs). Firstly, we build a directed co-reading network of users, to capture the fine-grained topic-specific similarities between users in a vector space. Then, the CSRN model encodes users with RNNs, and learns to attend to neighbors and summarize what news they are reading at the moment. Finally, news articles are recommended according to both the user\'s own state and the summarized state of the neighbors. Experiments on two public datasets show that the proposed model outperforms the state-of-the-art approaches significantly.', 'corpus_id': 215737055, 'score': 0}, {'doc_id': '220425421', 'title': 'Inductive Relational Matrix Completion', 'abstract': ""Data sparsity and cold-start issues emerge as two major bottlenecks for matrix completion in the context of user-item interaction matrix. We propose a novel method that can fundamentally address these issues. The main idea is to partition users into support users, which have many observed interactions (i.e., non-zero entries in the matrix), and query users, which have few observed entries. For support users, we learn their transductive preference embeddings using matrix factorization over their interactions (a relatively dense sub-matrix). For query users, we devise an inductive relational model that learns to estimate the underlying relations between the two groups of users. This allows us to attentively aggregate the preference embeddings of support users in order to compute inductive embeddings for query users. This new method can address the data sparsity issue by generalizing the behavior patterns of warm-start users to others and thus enables the model to also work effectively for cold-start users with no historical interaction. As theoretical insights, we show that a general version of our model does not sacrifice any expressive power on query users compared with transductive matrix factorization under mild conditions. Also, the generalization error on query users is bounded by the numbers of support users and query users' observed interactions. Moreover, extensive experiments on real-world datasets demonstrate that our model outperforms several state-of-the-art methods by achieving significant improvements on MAE and AUC for warm-start, few-shot (sparsity) and zero-shot (cold-start) recommendation."", 'corpus_id': 220425421, 'score': 0}, {'doc_id': '45555614', 'title': 'Exploring External Knowledge Base for Personalized Search in Collaborative Tagging Systems', 'abstract': 'Alongside the enormous volume of user-generated content posted to World Wide Web, there exists a thriving demand for search personalization services, especially those utilizing collaborative tagging data. To provide personalized services, a user model is usually required. We address the setting adopted by the majority of previous work, where a user model consists solely of the user’s past information. We construct an augmented user model from a number of tags and documents. These resources are further processed according to the user’s past information by exploring external knowledge base. A novel generative model is proposed for user model generation. This model leverages recent advances in neural language models such as Word Embeddings with latent semantic models such as Latent Dirichlet Allocation. We further present a new query expansion method to facilitate the desired personalized retrieval. Experiments conducted by utilizing real-world collaborative tagging data show that the methods proposed in the current paper outperform several non-personalized methods as well as existing personalized search methods by utilizing user models solely constructed from usage histories.', 'corpus_id': 45555614, 'score': 1}, {'doc_id': '218763237', 'title': 'CATN: Cross-Domain Recommendation for Cold-Start Users via Aspect Transfer Network', 'abstract': ""In a large recommender system, the products (or items) could be in many different categories or domains. Given two relevant domains (e.g., Book and Movie), users may have interactions with items in one domain but not in the other domain. To the latter, these users are considered as cold-start users. How to effectively transfer users' preferences based on their interactions from one domain to the other relevant domain, is the key issue in cross-domain recommendation. Inspired by the advances made in review-based recommendation, we propose to model user preference transfer at aspect-level derived from reviews. To this end, we propose a cross-domain recommendation framework via aspect transfer network for cold-start users (named CATN). CATN is devised to extract multiple aspects for each user and each item from their review documents, and learn aspect correlations across domains with an attention mechanism. In addition, we further exploit auxiliary reviews from like-minded users to enhance a user's aspect representations. Then, an end-to-end optimization framework is utilized to strengthen the robustness of our model. On real-world datasets, the proposed CATN outperforms SOTA models significantly in terms of rating prediction accuracy. Further analysis shows that our model is able to reveal user aspect connections across domains at a fine level of granularity, making the recommendation explainable."", 'corpus_id': 218763237, 'score': 0}, {'doc_id': '232415557', 'title': 'Topic-enhanced knowledge-aware retrieval model for diverse relevance estimation', 'abstract': 'Relevance measures the relation between query and document which contains several different dimensions, e.g., semantic similarity, topical relatedness, cognitive relevance (the relations in the aspect of knowledge), usefulness, timeliness, utility and so on. However, existing retrieval models mainly focus on semantic similarity and cognitive relevance while ignore other possible dimensions to model relevance. Topical relatedness, as an important dimension to measure relevance, is not well studied in existing neural information retrieval. In this paper, we propose a Topic Enhanced Knowledge-aware retrieval Model (TEKM) that jointly learns semantic similarity, knowledge relevance and topical relatedness to estimate relevance between query and document. We first construct a neural topic model to learn topical information and generate topic embeddings of a query. Then we combine the topic embeddings with a knowledge-aware retrieval model to estimate different dimensions of relevance. Specifically, we exploit kernel pooling to soft match topic embeddings with word and entity in a unified embedding space to generate fine-grained topical relatedness. The whole model is trained in an end-to-end manner. Experiments on a large-scale publicly available benchmark dataset show that TEKM outperforms existing retrieval models. Further analysis also shows how topic relatedness is modeled to improve traditional retrieval model with semantic similarity and knowledge relevance.', 'corpus_id': 232415557, 'score': 1}, {'doc_id': '220404390', 'title': 'Improving Conversational Recommender Systems via Knowledge Graph based Semantic Fusion', 'abstract': ""Conversational recommender systems (CRS) aim to recommend high-quality items to users through interactive conversations. Although several efforts have been made for CRS, two major issues still remain to be solved. First, the conversation data itself lacks of sufficient contextual information for accurately understanding users' preference. Second, there is a semantic gap between natural language expression and item-level user preference. To address these issues, we incorporate both word-oriented and entity-oriented knowledge graphs~(KG) to enhance the data representations in CRSs, and adopt Mutual Information Maximization to align the word-level and entity-level semantic spaces. Based on the aligned semantic representations, we further develop a KG-enhanced recommender component for making accurate recommendations, and a KG-enhanced dialog component that can generate informative keywords or entities in the response text. Extensive experiments have demonstrated the effectiveness of our approach in yielding better performance on both recommendation and conversation tasks."", 'corpus_id': 220404390, 'score': 0}, {'doc_id': '224803983', 'title': 'Learning To Retrieve: How to Train a Dense Retrieval Model Effectively and Efficiently', 'abstract': 'Ranking has always been one of the top concerns in information retrieval research. For decades, lexical matching signal has dominated the ad-hoc retrieval process, but it also has inherent defects, such as the vocabulary mismatch problem. Recently, Dense Retrieval (DR) technique has been proposed to alleviate these limitations by capturing the deep semantic relationship between queries and documents. The training of most existing Dense Retrieval models relies on sampling negative instances from the corpus to optimize a pairwise loss function. Through investigation, we find that this kind of training strategy is biased and fails to optimize full retrieval performance effectively and efficiently. To solve this problem, we propose a Learning To Retrieve (LTRe) training technique. LTRe constructs the document index beforehand. At each training iteration, it performs full retrieval without negative sampling and then updates the query representation model parameters. Through this process, it teaches the DR model how to retrieve relevant documents from the entire corpus instead of how to rerank a potentially biased sample of documents. Experiments in both passage retrieval and document retrieval tasks show that: 1) in terms of effectiveness, LTRe significantly outperforms all competitive sparse and dense baselines. It even gains better performance than the BM25-BERT cascade system under reasonable latency constraints. 2) in terms of training efficiency, compared with the previous state-of-the-art DR method, LTRe provides more than 170x speed-up in the training process. Training with a compressed index further saves computing resources with minor performance loss.', 'corpus_id': 224803983, 'score': 1}, {'doc_id': '219792931', 'title': 'Interactive Recommender System via Knowledge Graph-enhanced Reinforcement Learning', 'abstract': 'Interactive recommender system (IRS) has drawn huge attention because of its flexible recommendation strategy and the consideration of optimal long-term user experiences. To deal with the dynamic user preference and optimize accumulative utilities, researchers have introduced reinforcement learning (RL) into IRS. However, RL methods share a common issue of sample efficiency, i.e., huge amount of interaction data is required to train an effective recommendation policy, which is caused by the sparse user responses and the large action space consisting of a large number of candidate items. Moreover, it is infeasible to collect much data with explorative policies in online environments, which will probably harm user experience. In this work, we investigate the potential of leveraging knowledge graph (KG) in dealing with these issues of RL methods for IRS, which provides rich side information for recommendation decision making. Instead of learning RL policies from scratch, we make use of the prior knowledge of the item correlation learned from KG to (i) guide the candidate selection for better candidate item retrieval, (ii) enrich the representation of items and user states, and (iii) propagate user preferences among the correlated items over KG to deal with the sparsity of user feedback. Comprehensive experiments have been conducted on two real-world datasets, which demonstrate the superiority of our approach with significant improvements against state-of-the-arts.', 'corpus_id': 219792931, 'score': 0}, {'doc_id': '220250059', 'title': 'TFNet: Multi-Semantic Feature Interaction for CTR Prediction', 'abstract': 'The CTR (Click-Through Rate) prediction plays a central role in the domain of computational advertising and recommender systems. There exists several kinds of methods proposed in this field, such as Logistic Regression (LR), Factorization Machines (FM) and deep learning based methods like Wide&Deep, Neural Factorization Machines (NFM) and DeepFM. However, such approaches generally use the vector-product of each pair of features, which have ignored the different semantic spaces of the feature interactions. In this paper, we propose a novel Tensor-based Feature interaction Network (TFNet) model, which introduces an operating tensor to elaborate feature interactions via multi-slice matrices in multiple semantic spaces. Extensive offline and online experiments show that TFNet: 1) outperforms the competitive compared methods on the typical Criteo and Avazu datasets; 2) achieves large improvement of revenue and click rate in online A/B tests in the largest Chinese App recommender system, Tencent MyApp.', 'corpus_id': 220250059, 'score': 1}, {'doc_id': '220487143', 'title': 'BISON: BM25-weighted Self-Attention Framework for Multi-Fields Document Search', 'abstract': 'Recent breakthrough in natural language processing has advanced the information retrieval from keyword match to semantic vector search. To map query and documents into semantic vectors, self-attention models are being widely used. However, typical self-attention models, like Transformer, lack prior knowledge to distinguish the importance of different tokens, which has been proved to play a critical role in information retrieval tasks. In addition to this, when applying WordPiece tokenization, a rare word may be split into several different tokens. How to translate word-level prior knowledge into WordPiece tokens becomes a new challenge for the semantic representation generation. Moreover, web documents usually have multiple fields. Due to the heterogeneity of different fields, simple combination is not a good choice. In this paper, We propose a novel BM25-weighted Self-Attention framework (BISON) for web document search. By leveraging BM25 as prior weights, BISON learns weighted attention scores jointly with query matrix Q and key matrix K. We also present an efficient whole word weight sharing solution to mitigate prior knowledge discrepancy between words and WordPiece tokens. Furthermore, BISON effectively combines multiple fields by placing different fields into different segments. We demonstrate BISON is more efficient to capture the topical and semantic representation both in query and document. Intrinsic evaluation and experiments conducted on public data sets reveal BISON to be a general framework for document ranking task. It outperforms BERT and other modern models while retaining the same model complexity with BERT.', 'corpus_id': 220487143, 'score': 1}]"
172	{'doc_id': '227254543', 'title': 'How to Exploit the Transferability of Learned Image Compression to Conventional Codecs', 'abstract': 'Lossy image compression is often limited by the simplicity of the chosen loss measure. Recent research suggests that generative adversarial networks have the ability to overcome this limitation and serve as a multi-modal loss, especially for textures. Together with learned image compression, these two techniques can be used to great effect when relaxing the commonly employed tight measures of distortion. However, convolutional neural network based algorithms have a large computational footprint. Ideally, an existing conventional codec should stay in place, which would ensure faster adoption and adhering to a balanced computational envelope. \nAs a possible avenue to this goal, in this work, we propose and investigate how learned image coding can be used as a surrogate to optimize an image for encoding. The image is altered by a learned filter to optimise for a different performance measure or a particular task. Extending this idea with a generative adversarial network, we show how entire textures are replaced by ones that are less costly to encode but preserve sense of detail. \nOur approach can remodel a conventional codec to adjust for the MS-SSIM distortion with over 20% rate improvement without any decoding overhead. On task-aware image compression, we perform favourably against a similar but codec-specific approach.', 'corpus_id': 227254543}	4391	[{'doc_id': '229340375', 'title': 'DAQ: Distribution-Aware Quantization for Deep Image Super-Resolution Networks', 'abstract': 'Quantizing deep convolutional neural networks for image super-resolution substantially reduces their computational costs. However, existing works either suffer from a severe performance drop in ultra-low precision of 4 or lower bit-widths, or require a heavy fine-tuning process to recover the performance. To our knowledge, this vulnerability to low precisions relies on two statistical observations of feature map values. First, distribution of feature map values varies significantly per channel and per input image. Second, feature maps have outliers that can dominate the quantization error. Based on these observations, we propose a novel distribution-aware quantization scheme (DAQ) which facilitates accurate training-free quantization in ultra-low precision. A simple function of DAQ determines dynamic range of feature maps and weights with low computational burden. Furthermore, our method enables mixed-precision quantization by calculating the relative sensitivity of each channel, without any training process involved. Nonetheless, quantization-aware training is also applicable for auxiliary performance gain. Our new method outperforms recent training-free and even trainingbased quantization methods to the state-of-the-art image super-resolution networks in ultra-low precision.', 'corpus_id': 229340375, 'score': 1}, {'doc_id': '218551728', 'title': 'IKONOS: An intelligent tool to support diagnosis of Covid-19 by texture analysis of x-ray images', 'abstract': 'In late 2019, the SARS-Cov-2 spread worldwide. The virus has high rates of proliferation and causes severe respiratory symptoms, such as pneumonia. There is still no specific treatment and diagnosis for the disease. The standard diagnostic method for pneumonia is chest X-ray image. There are many advantages to using Covid-19 diagnostic X-rays: low cost, fast and widely available. We propose an intelligent system to support diagnosis by X-ray images.We tested Haralick and Zernike moments for feature extraction. Experiments with classic classifiers were done. Support vector machines stood out, reaching an average accuracy of 89:78%, average recall and sensitivity of 0:8979, and average precision and specificity of 0:8985 and 0:9963 respectively. The system is able to differentiate Covid-19 from viral and bacterial pneumonia, with low computational cost.', 'corpus_id': 218551728, 'score': 0}, {'doc_id': '218569100', 'title': 'The Digital Transformation Of Education In India During The Period Of Lockdown Due To Covid-19', 'abstract': 'This paper sought to an act of measuring the impact of COVID-19 pandemic in unleashing digital transformation in the education sector in India. In order to measure the impact, the study tracked the rate at which the virtual tools were used by various schools and institutions during the COVID-19 lockdown. Data were obtained from secondary sources, mainly newspaper articles, magazines and peer-reviewed journals. The findings are that, in India, during the lockdown, a variety of virtual tools were unleashed from primary education to higher and tertiary education where educational activities switched to online learning. These observations point to the fact that India, generally, has some pockets of excellence to drive the education sector to the next level, which has the potential to increase access. Access to education has always been a challenge due to a limited number of spaces available. Much as this pandemic has brought with it massive human suffering across the globe, there is an opportunity to assess successes and failures of deployed technologies, costs associated with them, and scaling these technologies to improve access. Index Terms – COVID-19, Digital Transformation, Education.', 'corpus_id': 218569100, 'score': 0}, {'doc_id': '218581433', 'title': 'Hierarchical Regression Network for Spectral Reconstruction from RGB Images', 'abstract': 'Capturing visual image with a hyperspectral camera has been successfully applied to many areas due to its narrowband imaging technology. Hyperspectral reconstruction from RGB images denotes a reverse process of hyperspectral imaging by discovering an inverse response function. Current works mainly map RGB images directly to corresponding spectrum but do not consider context information explicitly. Moreover, the use of encoder-decoder pair in current algorithms leads to loss of information. To address these problems, we propose a 4-level Hierarchical Regression Network (HRNet) with PixelShuffle layer as inter-level interaction. Furthermore, we adopt a residual dense block to remove artifacts of real world RGB images and a residual global block to build attention mechanism for enlarging perceptive field. We evaluate proposed HRNet with other architectures and techniques by participating in NTIRE 2020 Challenge on Spectral Reconstruction from RGB Images. The HRNet is the winning method of track 2 - real world images and ranks 3rd on track 1 - clean images.', 'corpus_id': 218581433, 'score': 0}, {'doc_id': '232304665', 'title': 'Semantic Perceptual Image Compression With a Laplacian Pyramid of Convolutional Networks', 'abstract': 'The existing image compression methods usually choose or optimize low-level representation manually. Actually, these methods struggle for the texture restoration at low bit rates. Recently, deep neural network (DNN)-based image compression methods have achieved impressive results. To achieve better perceptual quality, generative models are widely used, especially generative adversarial networks (GAN). However, training GAN is intractable, especially for high-resolution images, with the challenges of unconvincing reconstructions and unstable training. To overcome these problems, we propose a novel DNN-based image compression framework in this paper. The key point is decomposing an image into multi-scale sub-images using the proposed Laplacian pyramid based multi-scale networks. For each pyramid scale, we train a specific DNN to exploit the compressive representation. Meanwhile, each scale is optimized with different aspects, including pixel, semantics, distribution and entropy, for a good “rate-distortion-perception” trade-off. By independently optimizing each pyramid scale, we make each stage manageable and make each sub-image plausible. Experimental results demonstrate that our method achieves state-of-the-art performance, with advantages over existing methods in providing improved visual quality. Additionally, a better performance in the down-stream visual analysis tasks which are conducted on the reconstructed images, validates the excellent semantics-preserving ability of the proposed method.', 'corpus_id': 232304665, 'score': 1}, {'doc_id': '218571711', 'title': 'Décision kinésithérapique : Alexandre L. 57 ans : Kinésithérapie et Covid-19 en réanimation, de la phase aiguë à la réhabilitation', 'abstract': '\n Résumé\n \n Un homme de 57 ans est hospitalisé en réanimation pour un syndrome de détresse respiratoire aigüe (SDRA) lié à une infection à Covid-19. Après une première phase au cours de laquelle le patient est sédaté et curarisé, la kinésithérapie consiste à mobiliser passivement le patient, à participer au décubitus ventral; la kinésithérapie respiratoire n’étant pas forcément nécessaire. Dans un second temps, l’extubation est possible et plusieurs aspects sont développés\xa0: la kinésithérapie respiratoire, l’oxygénation, la déglutition et surtout la réhabilitation. Cependant, des atteintes du parenchyme pulmonaire abaissent de façon importante la saturation en oxygène au cours des exercices. L’oxygénation à haut débit et/ou la ventilation non-invasive (VNI) permettent d’optimiser la réhabilitation chez ce patient avec une réserve respiratoire encore précaire.\n Indice de factualité (i-FACT): 3.2\n \n \n Abstract\n \n A 57-year-old man is hospitalized in intensive care for an acute respiratory distress syndrome related to a Covid-19 infection. After a first phase during which the patient is sedated and nerve-blocked, physiotherapy consists in passively mobilizing the patient, participating in the prone position, respiratory physiotherapy is not necessary. In a second step, extubation is possible and several aspects are developed: respiratory physiotherapy, oxygenation, swallowing and rehabilitation. However, damage to the lung significantly decreases oxygen saturation during exercise. High-flow nasal oxygenation and / or non-invasive ventilation (NIV) can optimize rehabilitation in this patient with a still precarious respiratory function.\n Evidence index (EVID-i): 3.2\n \n', 'corpus_id': 218571711, 'score': 0}, {'doc_id': '233405268', 'title': 'GRACE: A Compressed Communication Framework for Distributed Machine Learning', 'abstract': 'Powerful computer clusters are used nowadays to train complex deep neural networks (DNN) on large datasets. Distributed training increasingly becomes communication bound. For this reason, many lossy compression techniques have been proposed to reduce the volume of transferred data. Unfortunately, it is difficult to argue about the behavior of compression methods, because existing work relies on inconsistent evaluation testbeds and largely ignores the performance impact of practical system configurations. In this paper, we present a comprehensive survey of the most influential compressed communication methods for DNN training, together with an intuitive classification (i.e., quantization, sparsification, hybrid and low-rank). Next, we propose GRACE, a unified framework and API that allows for consistent and easy implementation of compressed communication on popular machine learning toolkits. We instantiate GRACE on TensorFlow and PyTorch, and implement 16 such methods. Finally, we present a thorough quantitative evaluation with a variety of DNNs (convolutional and recurrent), datasets and system configurations. We show that the DNN architecture affects the relative performance among methods. Interestingly, depending on the underlying communication library and computational cost of compression / decompression, we demonstrate that some methods may be impractical. GRACE and the entire benchmarking suite are available as open-source.', 'corpus_id': 233405268, 'score': 0}, {'doc_id': '233004406', 'title': 'A Combined Deep Learning based End-to-End Video Coding Architecture for YUV Color Space', 'abstract': 'Most of the existing deep learning based end-to-end video coding (DLEC) architectures are designed specifically for RGB color format, yet the video coding standards, including H.264/AVC, H.265/HEVC and H.266/VVC developed over past few decades, have been designed primarily for YUV 4:2:0 format, where the chrominance (U and V) components are subsampled to achieve superior compression performances considering the human visual system. While a broad number of papers on DLEC compare these two distinct coding schemes in RGB domain, it is ideal to have a common evaluation framework in YUV 4:2:0 domain for a more fair comparison. This paper introduces a new DLEC architecture for video coding to effectively support YUV 4:2:0 and compares its performance against the HEVC standard under a common evaluation framework. The experimental results on YUV 4:2:0 video sequences show that the proposed architecture can outperform HEVC in intra-frame coding, however inter-frame coding is not as efficient on contrary to the RGB coding results reported in recent papers.', 'corpus_id': 233004406, 'score': 1}, {'doc_id': '228084046', 'title': 'Neural Rate Control for Video Encoding using Imitation Learning', 'abstract': 'In modern video encoders, rate control is a critical component and has been heavily engineered. It decides how many bits to spend to encode each frame, in order to optimize the rate-distortion trade-off over all video frames. This is a challenging constrained planning problem because of the complex dependency among decisions for different video frames and the bitrate constraint defined at the end of the episode. \nWe formulate the rate control problem as a Partially Observable Markov Decision Process (POMDP), and apply imitation learning to learn a neural rate control policy. We demonstrate that by learning from optimal video encoding trajectories obtained through evolution strategies, our learned policy achieves better encoding efficiency and has minimal constraint violation. In addition to imitating the optimal actions, we find that additional auxiliary losses, data augmentation/refinement and inference-time policy improvements are critical for learning a good rate control policy. We evaluate the learned policy against the rate control policy in libvpx, a widely adopted open source VP9 codec library, in the two-pass variable bitrate (VBR) mode. We show that over a diverse set of real-world videos, our learned policy achieves 8.5% median bitrate reduction without sacrificing video quality.', 'corpus_id': 228084046, 'score': 1}, {'doc_id': '232269858', 'title': 'A Subjective Study on Videos at Various Bit Depths', 'abstract': 'Bit depth adaptation, where the bit depth of a video sequence is reduced before transmission and up-sampled during display, can potentially reduce data rates with limited impact on perceptual quality. In this context, we conducted a subjective study on a UHD video database, BVI-BD, to explore the relationship between bit depth and visual quality. In this paper, three bit depth adaptation methods are investigated, including linear scaling, error diffusion, and a novel adaptive Gaussian filtering approach for up-sampling. The results from a subjective experiment indicate that above a critical bit depth, bit depth adaptation has no significant impact on perceptual quality, while reducing the amount information that is required to be transmitted. Below the critical bit depth, the more ‘advanced’ adaptation methods can be used to retain ‘good’ visual quality down to around 2 bits per color channel for the experimental setup - far lower than the common 8 bits per color channel. A selection of image quality metrics were benchmarked on the subjective data, and analysis indicates that a bespoke quality metric may be required to enable accurate bit depth adaptation.', 'corpus_id': 232269858, 'score': 1}]
173	{'doc_id': '235641922', 'title': 'Nitrogen content diagnosis of apple trees canopies using hyperspectral reflectance combined with PLS variable extraction and \nextreme learning machine', 'abstract': 'Nitrogen (N) is an important mineral element in apple production. Rapid estimation of apple tree N status is helpful for achieving precise N management. The objective of this work was to explore partial least squares (PLS) regression in dimensional reduction of spectral data and build the diagnostic model. The spectral reflectance data were collected from Fuji apple trees with 4 levels of N fertilizer treatment in the Loess Plateau in 2018 and 2019 using an ASD portable spectroradiometer, and leaf total N content was obtained at the same time. The raw spectra were pretreated using Savitzky-Golay (SG) smoothing and a combination of SG and first-order derivative (SG_FD) or second-order derivative (SG_SD). The samples were divided into a calibration dataset and a prediction dataset using SPXY. Based on 4 factors of PLS regression, including latent variables (LVs), X-loading, variable importance in projection (VIP) and regression coefficients (RC), the 6 methods (LVs, X-loading, VIP_01, VIP_02, RC_01 and RC_02) were derived and used for variable extraction, based on which PLS model and ELM model were established. The results indicated that the spectral data processed by SG_FD had the highest signal-to-noise ratio and was selected for subsequent analysis. The amounts of variables extracted by LVs, X-loading, VIP_01, VIP_02, RC_01 and RC_02 were 6, 11, 18, 305, 26 and 88, respectively. The method of extracting variables with an RC threshold based on the minimum RMSEP (RC_02) could effectively avoid the omission of effective information. The RC_02 method was recommended for related research which required accurate wavelength information as a variable. The variable extraction method based on LVs generated an ELM model with a simple structure. The prediction results showed that the ELM model outperformed the PLS model. The PLS(LVs)_ELM model was the best; R2P, RMSEP and RPD were 0.837, 2.393 and 2.220, respectively.', 'corpus_id': 235641922}	17836	"[{'doc_id': '234309916', 'title': 'Determination of Leaf Water Content with a Portable NIRS System Based on Deep Learning and Information Fusion Analysis', 'abstract': 'Highlights A portable NIRS system with local computing hardware was developed for leaf water content determination. The proposed convolutional neural network for regression showed a satisfactory performance. Decision fusion of multiple regression models achieved a higher precision than single models. All of the devices and machine intelligence algorithms were integrated into the system. Software was developed for system control and user interface. Abstract. Spectroscopy has been widely used as a valid non-destructive technique for the determination of crop physiological parameters. In this study, a portable near-infrared spectroscopy (NIRS) system was developed for rapid measurement of rape (Brassica campestris) leaf water content. An integrated spectrometer (900 to 1700 nm) was used to collect the spectra. A Wi-Fi module was adopted for driving the spectrometer and realizing data communication. The NVIDIA Jetson Nano developer kit was employed to handle the received spectra and perform computing tasks. Three embedded spectral analysis models, including support vector regression (SVR), partial least square regression (PLSR), and deep convolutional neural network for regression (CNN-R), and decision fusions of these methods were built and compared. The results demonstrated that the separate models produced satisfactory predictions. The proposed system achieved the highest precision based on the fusion of PLSR and CNN-R. The hardware devices and analytical algorithms were all integrated into the proposed portable system, and the tested samples were collected from an actual field environment, which shows great potential of the system for outdoor applications.', 'corpus_id': 234309916, 'score': 0}, {'doc_id': '233537051', 'title': 'Is this melon sweet? A quantitative classification for near-infrared spectroscopy', 'abstract': 'Abstract Melons are nutritious, healthy, and one of the most eatable summer fruits in South Asia, especially in Pakistan. A melon is delicious if it is sweet, however, the gauge of its sweetness depends on the individual taste buds. In this paper, a direct sweetness classifier is proposed as a quantitative measure, to predict the sweetness of melon as opposed to indirect measure of soluble solid content (SSC/°Brix) based thresholding for near-infrared (NIR) spectroscopy. To provide guidance for fruit sweetness classification, sensory test was conducted, and sweetness standards were established as; very sweet (with °Brix over 10), sweet (with °Brix between 7 and 10), and flat (with °Brix below 7) class. NIR spectral data obtained using F-750 produce quality meter (310–1100\xa0nm) was analyzed to build SSC prediction model and direct sweetness classification model. The best SSC model was obtained using multiple linear regression on second derivative of spectral data (for wavelength range 729–975\xa0nm) with correlation coefficient\xa0=\xa00.93, and root mean square error\xa0=\xa01.63 on test samples. Sweetness of test samples were obtained using °Brix thresholding with an accuracy of 55.45% for three classes. The best direct sweetness classifier was obtained using K nearest neighbor (KNN) on second derivative of spectral data (for wavelength range 729–975\xa0nm) with an accuracy of 70.3% for three classes on test samples. It was further observed that classification accuracy for sweet and flat melon can be improved by combining sweet and very sweet class samples into one ‘satisfactory’ class (with °Brix over 7). For °Brix thresholding-based classification the accuracy was improved to 80.2% and for KNN based direct sweetness classification the accuracy was improved to 88.12%. Extensive evaluation validates our argument that modeling a direct sweetness classifier is a better approach as compared to °Brix based thresholding for sweetness classification using NIR spectroscopy.', 'corpus_id': 233537051, 'score': 1}, {'doc_id': '233567950', 'title': 'Determination of main raw material source in bar soaps using mid-infrared spectroscopy combined with classification tools', 'abstract': 'Abstract The personal care industry is one of the fastest-growing markets in the world in terms of revenue and sales volume. In addition, this market has been going through some modifications to serve its consumers, a fact that has generated the development of a new line of products. Nowadays several products, including bar soaps, are commercialized as free of animal and synthetic materials. In this context, the development of methodologies to verify the source of the raw material (animal, synthetic, or vegetable) used in the production of soaps can be a valuable tool to supplement the quality control routines. Infrared spectroscopy combined with pattern recognition methods, i.e. Principal Component Analysis (PCA), Partial Least Squares Discriminatory Analysis (PLS-DA) and Data Driven Soft Independent Modeling of Class Analogy (DD-SIMCA), were applied to verify the main source of raw materials used in the soaps production. Analyzing the scores plot for the first two Principal Components of PCA it was possible to observe distinct clusters of soap samples grouped based on the main sources of raw material. Both supervised methods, PLS-DA and DD-SIMCA, were able to correctly classify all samples into their corresponding classes. The most influential variables for the discrimination of groups by PLS-DA were the spectral bands related to changes in the carbon chains and in the amount of carboxyl or glycerol groups in the chemical structure. While DD-SIMCA demonstrated similarity between soaps containing mainly animal and vegetal raw material. This study indicates the potential of chemometric tools in the development of robust predictive models that may be integrated to identify the raw materials used in bar soaps.', 'corpus_id': 233567950, 'score': 0}, {'doc_id': '235516920', 'title': 'Vis-NIR spectroscopy predicts threshold velocity of wind erosion in calcareous soils', 'abstract': 'Abstract Wind erosion potential can be assessed using the Threshold Friction Velocity (TFV) of the soil, which is not always easy to measure, especially on regional and global scales. To overcome this difficulty, the spectroscopy technique can provide a useful approach in estimating the TFV as an alternative for time-consuming wind tunnel studies in the field. In this study, we evaluated the potential of Vis-NIR spectroscopy in predicting the TFV and some TFV-related soil properties using Partial Least Square Regression (PLSR) and the Support Vector Regression (SVR). We also developed a Point Spectrotransfer Function (PSTF) using Multiple Linear Regression (MLR) to predict the TFV based on diagnostic wavelengths and compared it to the derived Pedotransfer Function (PTF). For this purpose, 300 in-situ wind tunnel tests were performed in the Fars Province, Iran and the spectral reflectance of soil samples were analysed using a spectrophotometer apparatus. The 10 best key wavelengths resulting from the correlation analysis between the TFV and the spectral reflectance were 750, 1342, 1446, 1578, 1746, 1939, 2072, 2162, 2217, and 2338\xa0nm which were mostly located in the short-wavelength infrared (SWIR) area. The derived PSTF performed better than the PTF for the TFV estimation (R2\xa0=\xa00.94, RMSE\xa0=\xa00.71). Results of the predictive models revealed that machine learning using the SVR had a significantly (P', 'corpus_id': 235516920, 'score': 0}, {'doc_id': '235450820', 'title': 'A best-practice guide to predicting plant traits from leaf-level hyperspectral data using partial least squares regression.', 'abstract': 'Partial least squares regression (PLSR) modelling is a statistical technique for correlating datasets, and involves the fitting of a linear regression between two matrices. One application of PLSR enables leaf traits to be estimated from hyperspectral optical reflectance data, facilitating rapid, high-throughput, non-destructive plant phenotyping. This technique is of interest and importance in a wide range of contexts including crop breeding and ecosystem monitoring. The lack of a consensus in the literature on how to perform PLSR means that interpreting model results can be challenging, applying existing models to novel datasets can be impossible, and unknown or undisclosed assumptions can lead to incorrect or spurious predictions. We address this lack of consensus by proposing best practices for using PLSR to predict plant traits from leaf-level hyperspectral data, including a discussion of when PLSR is applicable, and recommendations for data collection. We provide a tutorial to demonstrate how to develop a PLSR model, in the form of an R script accompanying this manuscript. This practical guide will assist all those interpreting and using PLSR models to predict leaf traits from spectral data, and advocates for a unified approach to using PLSR for predicting traits from spectra in the plant sciences.', 'corpus_id': 235450820, 'score': 0}, {'doc_id': '233545480', 'title': 'Development of NIR-HSI and chemometrics process analytical technology for drying of beef jerky', 'abstract': 'Abstract Beef jerky samples immersed in brine or water, both with and without ultrasound treatment were dried at 60\xa0°C for 30, 60, 90, 120, 150, 180, 210 and 240\xa0min. Drying behaviour was evaluated using ten drying kinetic models and hyperspectral imaging. All samples reached water activity values (Aw)\xa0 4, indicating their suitability for process control applications. The best performing MC prediction models for the non-ultrasound and ultrasound treated samples were developed using the ensemble Monte Carlo variable selection (EMCVS) on second derivative of log(1/R) spectra and EMCVS-selectivity ratio on linear detrended log (1/R) spectra, respectively. This study demonstrated the potential of NIR-HSI and chemometrics as a PAT tool for drying of beef jerky.', 'corpus_id': 233545480, 'score': 0}, {'doc_id': '235379979', 'title': 'Unsupervised Feature Selection Via Orthogonal Basis Clustering and Local Structure Preserving.', 'abstract': ""Due to the ``curse of dimensionality'' issue, how to discard redundant features and select informative features in high-dimensional data has become a critical problem, hence there are many research studies dedicated to solving this problem. Unsupervised feature selection technique, which does not require any prior category information to conduct with, has gained a prominent place in preprocessing high-dimensional data among all feature selection techniques, and it has been applied to many neural networks and learning systems related applications, e.g., pattern classification. In this article, we propose an efficient method for unsupervised feature selection via orthogonal basis clustering and reliable local structure preserving, which is referred to as OCLSP briefly. Our OCLSP method consists of an orthogonal basis clustering together with an adaptive graph regularization, which realizes the functionality of simultaneously achieving excellent cluster separation and preserving the local information of data. Besides, we exploit an efficient alternative optimization algorithm to solve the challenging optimization problem of our proposed OCLSP method, and we perform a theoretical analysis of its computational complexity and convergence. Eventually, we conduct comprehensive experiments on nine real-world datasets to test the validity of our proposed OCLSP method, and the experimental results demonstrate that our proposed OCLSP method outperforms many state-of-the-art unsupervised feature selection methods in terms of clustering accuracy and normalized mutual information, which indicates that our proposed OCLSP method has a strong ability in identifying more important features."", 'corpus_id': 235379979, 'score': 0}, {'doc_id': '233582814', 'title': 'Efficient cross-validatory algorithm for identifying dynamic nonlinear process models', 'abstract': 'Abstract Kernel partial least squares (KPLS) is an effective nonlinear modeling technique for control engineering applications, including model predictive control, process monitoring or general system diagnosis. It can deal with small sample sizes and variable sets that are noisy and highly correlated. Kernel partial least squares maps the input (or cause) variables to a feature space and carries out the task of producing an optimal prediction model for the process output (or effect) variables using the standard linear partial least squares (PLS) approach. Resulting from the typically large size of the feature space, the kernel partial least squares procedure can be computational intensive. In particular, if the optimal model structure is estimated using cross-validation, KPLS is not efficient in handling large data sets. This paper first modifies the conventional kernel partial least squares procedure in order to embed it within a leave-one-out cross-validation (LOOCV) framework. The proposed efficient kernel partial least squares (EKPLS) is able to reduce the computational complexity by an order of magnitude compared to the conventional approach, which is proven both analytically and through modeling applications to three industrial data sets.', 'corpus_id': 233582814, 'score': 0}, {'doc_id': '233555229', 'title': 'Evaluation and Prediction of Topsoil organic carbon using Machine learning and hybrid models at a Field-scale', 'abstract': 'Abstract Digital mapping of soil organic carbon (SOC) is crucial to evaluate its spatial variability and also to assess environmental factors controlling it at field scale. The current study was conducted to compare one statistical method include partial least squares regression (PLSR), four individual machine learning(ML) algorithms including random forest (RF), quantile regression forest (QRF), cubist (CB), fuzzy logic (SoLIM) along with two hybrid methods including the random forest-ordinary kriging (RF-OK) and quantile random regression forest-ordinary kriging (QRF-OK) to map SOC. A number of 146 soil samples (0–30\xa0cm) were collected in Khorramabad plain (680\xa0ha). For the quantitative evaluation of SOC spatial variability, two scenarios were considered to use remote sensing data (R) and a combination of geo-morphometric and remote sensing data (GR). The data randomly were split into 80% (117 points) for training and 20% (29 points) for validation. The model performances were evaluated by the statistical indices as the coefficient of determination (R2), root mean square error (RMSE). According to principal component analysis (PCA), nine covariates including transformed soil adjusted vegetation index (TSAVI), relative vegetation index (RVI), Band 10, Band 11,Digital Elevation Model (DEM), Standard height(Standard_he), Valley_Dep, terrain surface texture (texture), and terrain surface convexity (Convexity) were selected as the environmental predictors. Results showed that the hybrid model RF-OK (RMSE\xa0=\xa00.05and R2\xa0=\xa00.93) and SoLIM model (RMSE\xa0=\xa00.47and R2\xa0=\xa00.41) with scenario GR had the highest and lowest accurate respectively. TSAVI, DEM, and Band 10 were the most important predictors and explanation more than 50% of SOC spatial variability in the study area. Generally, using hybrid machine learning models in combination with geo-morphometric and remote sensing covariates make it possible to model and predict SOC with acceptable accurate in the field-scale croplands.', 'corpus_id': 233555229, 'score': 1}, {'doc_id': '234743466', 'title': 'Rapid Determination of Low Heavy Metal Concentrations in Grassland Soils around Mining Using Vis–NIR Spectroscopy: A Case Study of Inner Mongolia, China', 'abstract': 'Proximal sensing offers a novel means for determination of the heavy metal concentration in soil, facilitating low cost and rapid analysis over large areas. In this respect, spectral data and model variables play an important role. Thus far, no attempts have been made to estimate soil heavy metal content using continuum-removal (CR), different preprocessing and statistical methods, and different modeling variables. Considering the adsorption and retention of heavy metals in spectrally active constituents in soil, this study proposes a method for determining low heavy metal concentrations in soil using spectral bands associated with soil organic matter (SOM) and visible–near-infrared (Vis–NIR). To rapidly determine the concentration of heavy metals using hyperspectral data, partial least squares regression (PLSR), principal component regression (PCR), and support vector machine regression (SVMR) statistical methods and 16 preprocessing combinations were developed and explored to determine an optimal combination. The results showed that the multiplicative scatter correction and standard normal variate preprocessing methods evaluated with the second derivative spectral transformation method could accurately determine soil Cr and Ni concentrations. The root-mean-square error (RMSE) values of Vis–NIR model combinations with PLSR, PCR, and SVMR were 0.34, 3.42, and 2.15 for Cr, and 0.07, 1.78, and 1.14 for Ni, respectively. Soil Cr and Ni showed strong spectral responses to the Vis–NIR spectral band. The R2 value of the Vis–NIR-based PLSR model was higher than 0.99, and the RMSE value was 0.07–0.34, suggesting higher stability and accuracy. The results were more accurate for Ni than Cr, and PLSR showed the best performance, followed by SVMR and PCR. This perspective has critical implications for guiding quantitative biogeochemical analysis using proximal sensing data.', 'corpus_id': 234743466, 'score': 1}]"
174	{'doc_id': '44068919', 'title': 'Interdisciplinary Team Science and the Public: Steps Toward a Participatory Team Science', 'abstract': 'Interdisciplinary team science involves research collaboration among investigators from different disciplines who work interdependently to share leadership and responsibility. Although over the past several decades there has been an increase in knowledge produced by science teams, the public has not been meaningfully engaged in this process. We argue that contemporary changes in how science is understood and practiced offer an opportunity to reconsider engaging the public as active participants on teams and coin the term participatory team science to describe public engagement in team science. We discuss how public engagement can enhance knowledge within the team to address complex problems and suggest a different organizing framework for team science that aligns better with how teams operate and with participatory approaches to research. We also summarize work on public engagement in science, describe opportunities for various types of engagement, and provide an example of participatory team science carried out across research phases. We conclude by discussing implications of participatory team science for psychology, including changing the default when assembling an interdisciplinary science team by identifying meaningful roles for public engagement through participatory team science.', 'corpus_id': 44068919}	20842	"[{'doc_id': '204945575', 'title': 'Measuring quality and outcomes of research collaborations: An integrative review', 'abstract': 'Introduction: Although the science of team science is no longer a new field, the measurement of team science and its standardization remain in relatively early stages of development. To describe the current state of team science assessment, we conducted an integrative review of measures of research collaboration quality and outcomes. Methods: Collaboration measures were identified using both a literature review based on specific keywords and an environmental scan. Raters abstracted details about the measures using a standard tool. Measures related to collaborations with clinical care, education, and program delivery were excluded from this review. Results: We identified 44 measures of research collaboration quality, which included 35 measures with reliability and some form of statistical validity reported. Most scales focused on group dynamics. We identified 89 measures of research collaboration outcomes; 16 had reliability and 15 had a validity statistic. Outcome measures often only included simple counts of products; publications rarely defined how counts were delimited, obtained, or assessed for reliability. Most measures were tested in only one venue. Conclusions: Although models of collaboration have been developed, in general, strong, reliable, and valid measurements of such collaborations have not been conducted or accepted into practice. This limitation makes it difficult to compare the characteristics and impacts of research teams across studies or to identify the most important areas for intervention. To advance the science of team science, we provide recommendations regarding the development and psychometric testing of measures of collaboration quality and outcomes that can be replicated and broadly applied across studies.', 'corpus_id': 204945575, 'score': 1}, {'doc_id': '238214767', 'title': 'An integrated model for interdisciplinary graduate education: Computation and mathematics for biological networks', 'abstract': 'The current challenges at the forefront of data-enabled science and engineering require interdisciplinary solutions. Yet most traditional doctoral programs are not structured to support successful interdisciplinary research. Here we describe the design of and students’ experiences in the COMBINE (Computation and Mathematics for Biological Networks) interdisciplinary graduate program at the University of Maryland. COMBINE focuses on the development and application of network science methods to biological systems for students from three primary domains: life sciences, computational/engineering sciences, and mathematical/physical sciences. The program integrates three established models (T-shaped, pi-shaped and shield-shaped) for interdisciplinary training. The program components largely fall into three categories: (1) core coursework that provides content expertise, communication, and technical skills, (2) discipline-bridging elective courses in the two COMBINE domains that complement the student’s home domain, (3) broadening activities such as workshops, symposiums, and formal peer-mentoring groups. Beyond these components, the program builds community through both formal and informal networking and social events. In addition to the interactions with other program participants, students engage with faculty in several ways beyond the conventional adviser framework, such as the requirement to select a second out-of-field advisor, listening to guest speakers, and networking with faculty through workshops. We collected data through post-program surveys, interviews and focus groups with students, alumni and faculty advisors. Overall, COMBINE students and alumni reported feeling that the program components supported their growth in the three program objectives of Network Science & Interdisciplinarity, Communication, and Career Preparation, but also recommended ways to improve the program. The value of the program can be seen not only through the student reports, but also through the students’ research products in network science which include multiple publications and presentations. We believe that COMBINE offers an effective model for integrated interdisciplinary training that can be readily applied in other fields.', 'corpus_id': 238214767, 'score': 0}, {'doc_id': '225465902', 'title': 'Operationalization, implementation, and evaluation of Collaboration Planning: A pilot interventional study of nascent translational teams', 'abstract': 'Abstract Background: The University of Wisconsin Institute for Clinical and Translational Research hub supports multiple pilot award programs that engage cross-disciplinary Translational Teams. To support those teams, our Team Science group aims to offer a learning experience that is accessible, active, and actionable. We identified Collaboration Planning as a high-impact intervention to stimulate team-building activities that provide Translational Team members with the skills to lead and participate in high-impact teams. Methods: We adapted the published materials on Collaboration Planning to develop a 90-minute facilitated intervention with questions in 10 areas, presuming no previous knowledge of Science of Team Science (SciTS) or team-science best practices. Attendees received a short follow-up survey and submitted a written collaboration plan with their first quarterly progress report. Results: Thirty-nine participants from 13 pilot teams from a wide range of disciplines engaged in these sessions. We found that teams struggled to know who to invite, that some of our questions were confusing and too grounded in the language of SciTS, and groups lacked plans for managing their information and communications. We identified several areas for improvement including ensuring that the process is flexible to meet the needs of different teams, continuing to evolve the questions so they resonate with teams, and the need to provide resources for areas where teams needed additional guidance, including information and data management, authorship policies, and conflict management. Conclusions: With further development and testing, Collaboration Planning has the potential to support Translational Teams in developing strong team dynamics and team functioning.', 'corpus_id': 225465902, 'score': 1}, {'doc_id': '237101638', 'title': 'On the theory-practice gap in the environmental realm: perspectives from and for diverse environmental professionals', 'abstract': 'The theory-practice gap (TPG) is well known in the environmental realm, referring to disconnects between knowledge generated through scientific research and the needs, expectations, and practices of knowledge users for environmental decision-making and practice. While the presence of the TPG is well established, we have yet to fully implement mechanisms for overcoming its challenges. Thus, our goal is to characterize the TPG and identify practical recommendations for minimizing it. Here, a diverse group of experts in the environmental realm (spanning landscape planning, conservation science, environmental sociology, resource management, political science, and anthropology, among others) present our perspectives on the TPG. More specifically, we share an organized framework for understanding the TPG and suggest recommendations that can help make progress in one or more dimension(s). Conceptual topics discussed are the implications of the gap and its persistence. Organizational/institutional topics include the implications of the overabundance, inaccessibility, and uncertainty of scientific information, and a need for mainstreaming boundary spanning activities. Lastly, cultural topics include differences in culture and epistemologies across knowledge generators and users, shifting cultures through co-production, and changes in educational curricula. Recommendations for minimizing the TPG include conceptually recasting what is considered ‘success,’ institutional reform, enhanced information delivery, leveraging knowledge brokers and boundary organizations, leveraging ‘champions’ in policy, using co-production and/or integrative research, confronting the contemporary ‘fake news’ phenomenon, and rethinking researcher and practitioner training and development. By sharing our framework and recommendations, we provide insight, as well as a starting point for those looking to narrow the TPG and improve knowledge generator-user relationships.', 'corpus_id': 237101638, 'score': 0}, {'doc_id': '237638773', 'title': 'A dissemination strategy to promote relational coordination in the veterans health administration: a case study', 'abstract': 'Large healthcare institutions like the Veterans Health Administration (VA) continually seek best practices to improve clinical care. Relational coordination is an evidence-based organizational theory of communicating and relating to coordinate work and drive performance outcomes. Implementing relational coordination-guided practices can be difficult due to challenges with spreading information across large systems. Using social marketing theory and evidence-based dissemination strategies, we developed an evidence-based dissemination plan to educate and motivate researchers and operational staff to study and implement relational coordination in the VA. In this case study, we used the four Ps (product, price, place, promotion) of social marketing theory to develop a 2-phase dissemination strategy. In phase one, we created and distributed relational coordination information and invited VA staff to join the Relational Coordination Research Collaborative. In phase two, dissemination efforts targeted researchers ready to implement relational coordination within existing programs of research. Process and outcome measures included dissemination, engagement and adoption data and a post-project survey. Quantitative results were calculated using descriptive statistics. Survey text responses were analyzed using deductive content analysis and a structured categorization matrix. Phase one included social media dissemination, virtual and in-person presentations, as well as phone and email communication between project staff and the target audience. In total, 47 VA staff became members of the Relational Coordination Research Collaborative and 27 routinely participated in online research seminars. In phase 2, 13 researchers expressed interest in studying relational coordination and 5 projects were selected to participate. Multiple relational coordination-related trainings and publications originated from this program. Dissemination approaches that involved personalized, one-on-one efforts (e.g., phone or email) seemed to be more effective at disseminating relational coordination compared to social media or online presentations. Participants in phase 2 agreed that relational coordination should be adopted in the VA but indicated that cost would be a barrier. Results support the importance of evidence-based dissemination planning that address the unique costs and benefits of programs.', 'corpus_id': 237638773, 'score': 0}, {'doc_id': '237556051', 'title': 'Culture of Interdisciplinary Collaboration in Nursing Research Training.', 'abstract': 'BACKGROUND\nEstablishing and maintaining collaborative scientific environments that can cultivate and benefit from a full range of talents is essential for the quality and influence of science. Inclusion of research training and career development interventions to expose nursing PhD students, postdocs, and junior faculty to team science stands to prepare graduates to effectively engage with interdisciplinary colleagues to conduct cutting-edge nursing research and compete successfully for precious research resources. To be effective, nursing research workforce development programs need to recognize and share a culture of interdisciplinarity.\n\n\nOBJECTIVES\nThis project aims to develop, validate, and disseminate a theoretically grounded and methodologically rigorous tool for cultural consensus analysis of the culture of interdisciplinary collaboration in nursing research.\n\n\nMETHODS\nCulture can be defined as shared cognitive structures and consensus around culturally correct values, attitudes, and normative behaviors. This mixed-methods study employs cultural consensus analysis (CCA) to assess construct validity and empirically determine a set of underlying socially learned and shared notions about the cultural domain of interdisciplinary collaboration in nursing research. The study will include three phases: (a) qualitative data collection and analysis to define the cultural domain of interdisciplinary collaborations in nursing research; (b) validation of the CCA tool with the use the cultural knowledge statements; and (c) application of the CCA tool to assess cultural differences among nursing trainees, junior faculty, and training directors. The study participant pool consists of NIH-NINR awardees, including training directors of institutional training grants, pre- and postdoctoral trainees with individual fellowship training grants, and junior faculty with career development awards. Qualitative data will be analyzed to formulate cultural statements about the values and behaviors that promote interdisciplinary collaboration in nursing research. Subsequent survey data will be assessed using matrix algebra, principal component analysis, and the Stuart-Maxwell Marginal Homogeneity test.\n\n\nDISCUSSION\nThe development and validation of a CCA tool is a novel approach to assess, support, and systematically examine interdisciplinary collaboration and team science in nursing research and training. Yet, the investigation of culture needs to remain value neutral, refrain from being prescriptive, and be sensitive to the emergence and dominance of one ""right"" culture.', 'corpus_id': 237556051, 'score': 0}, {'doc_id': '61155796', 'title': 'The integration of the data scientist into the team: Implications and challenges', 'abstract': 'Modern biomedical research is complex and requires a cross section of experts collaborating using multi-, inter-, or transdisciplinary approaches to address scientific questions. Known as team science, such approaches have become so critical it has given rise to a new field – the science of team science. In biomedical research, data scientists often play a critical role in team-based collaborations. Integration of data scientists into research teams has multiple advantages to the clinical and translational investigator as well as to the data scientist. Clinical and translational investigators benefit from having an invested dedicated collaborator who can assume principal responsibility for essential data-related activities, while the data scientist can build a career developing tools that are relevant and data-driven. Participation in team science, however, can pose challenges. One particular challenge is the ability to appropriately evaluate the data scientist’s scholarly contributions, necessary for promotion. Only a minority of academic health centers have attempted to address this challenge. In order for team science to thrive on academic campuses, leaders of institutions need to hire data science faculty for the purpose of doing team science, with novel systems in place that incentivize the data scientist’s engagement in team science and that allow for appropriate evaluation of performance. Until such systems are adopted at the institutional level, the ability to conduct team science to address modern biomedical research with its increasingly complex data needs will be compromised. Fostering team science on campuses by putting supportive systems in place will benefit not only clinical and translational investigators as well as data scientists, but also the larger academic institution.', 'corpus_id': 61155796, 'score': 1}, {'doc_id': '4607386', 'title': 'Team science as interprofessional collaborative research practice: a systematic review of the science of team science literature', 'abstract': ""The National Institute of Health's concept of team science is a means of addressing complex clinical problems by applying conceptual and methodological approaches from multiple disciplines and health professions. The ultimate goal is the improved quality of care of patients with an emphasis on better population health outcomes. Collaborative research practice occurs when researchers from >1 health-related profession engage in scientific inquiry to jointly create and disseminate new knowledge to clinical and research health professionals in order to provide the highest quality of patient care to improve population health outcomes. Training of clinicians and researchers is necessary to produce clinically relevant evidence upon which to base patient care for disease management and empirically guided team-based patient care. In this study, we hypothesized that team science is an example of effective and impactful interprofessional collaborative research practice. To assess this hypothesis, we examined the contemporary literature on the science of team science (SciTS) produced in the past 10\u2005years (2005–2015) and related the SciTS to the overall field of interprofessional collaborative practice, of which collaborative research practice is a subset. A modified preferred reporting items for systematic reviews and meta-analyses (PRISMA) approach was employed to analyze the SciTS literature in light of the general question: Is team science an example of interprofessional collaborative research practice? After completing a systematic review of the SciTS literature, the posed hypothesis was accepted, concluding that team science is a dimension of interprofessional collaborative practice."", 'corpus_id': 4607386, 'score': 1}, {'doc_id': '237502460', 'title': 'Teamwork Effectiveness in Student’s Final Year Project', 'abstract': 'The goal of this paper is to deliberate issues pertaining to teamwork effectiveness among students in a university environment. Final year project is an important assessment to measure students’ teamwork skills in the university. Three main factors have been identified that contribute to teamwork effectiveness. The factors are namely interpersonal skills, interdependence and commitment to success. Interpersonal factors include social sensitivity and emotional engagement attributes. In this perspective, measures such as teamwork support, care, trust, honesty and respect towards team members are important key criteria. On the other hand, interdependence among others includes traits such as promoting each other to achieve common goals, bringing the best of each other and helping each other completing the task. Commitment to team success involves high obligation, high motivation, strong common goals and strong shared values and beliefs. In this paper, construct items for each factor are developed based on previous studies. The findings show that these factors are relevant toward the success of the student’s final year project. Lacking these skills may result in poor performance among team members and may lead to unfavorable outcome.', 'corpus_id': 237502460, 'score': 0}, {'doc_id': '43929344', 'title': 'The Science of Team Science: A Review of the Empirical Evidence and Research Gaps on Collaboration in Science', 'abstract': 'Collaborations among researchers and across disciplinary, organizational, and cultural boundaries are vital to address increasingly complex challenges and opportunities in science and society. In addition, unprecedented technological advances create new opportunities to capitalize on a broader range of expertise and information in scientific collaborations. Yet rapid increases in the demand for scientific collaborations have outpaced changes in the factors needed to support teams in science, such as institutional structures and policies, scientific culture, and funding opportunities. The Science of Team Science (SciTS) field arose with the goal of empirically addressing questions from funding agencies, administrators, and scientists regarding the value of team science (TS) and strategies for successfully leading, engaging in, facilitating, and supporting science teams. Closely related fields have rich histories studying teams, groups, organizations, and management and have built a body of evidence for effective teaming in contexts such as industry and the military. Yet few studies had focused on science teams. Unique contextual factors within the scientific enterprise create an imperative to study these teams in context, and provide opportunities to advance understanding of other complex forms of collaboration. This review summarizes the empirical findings from the SciTS literature, which center around five key themes: the value of TS, team composition and its influence on TS performance, formation of science teams, team processes central to effective team functioning, and institutional influences on TS. Cross-cutting issues are discussed in the context of new research opportunities to further advance SciTS evidence and better inform policies and practices for effective TS.', 'corpus_id': 43929344, 'score': 1}]"
175	{'doc_id': '3763258', 'title': 'Joint 3D Proposal Generation and Object Detection from View Aggregation', 'abstract': 'We present AVOD, an Aggregate View Object Detection network for autonomous driving scenarios. The proposed neural network architecture uses LIDAR point clouds and RGB images to generate features that are shared by two subnetworks: a region proposal network (RPN) and a second stage detector network. The proposed RPN uses a novel architecture capable of performing multimodal feature fusion on high resolution feature maps to generate reliable 3D object proposals for multiple object classes in road scenes. Using these proposals, the second stage detection network performs accurate oriented 3D bounding box regression and category classification to predict the extents, orientation, and classification of objects in 3D space. Our proposed architecture is shown to produce state of the art results on the KITTI 3D object detection benchmark [1] while running in real time with a low memory footprint, making it a suitable candidate for deployment on autonomous vehicles. Code is available at', 'corpus_id': 3763258}	2533	"[{'doc_id': '211818175', 'title': 'Closed-Loop Benchmarking of Stereo Visual-Inertial SLAM Systems: Understanding the Impact of Drift and Latency on Tracking Accuracy', 'abstract': 'Visual-inertial SLAM is essential for robot navigation in GPS-denied environments, e.g. indoor, underground. Conventionally, the performance of visual-inertial SLAM is evaluated with open-loop analysis, with a focus on the drift level of SLAM systems. In this paper, we raise the question on the importance of visual estimation latency in closed-loop navigation tasks, such as accurate trajectory tracking. To understand the impact of both drift and latency on visualinertial SLAM systems, a closed-loop benchmarking simulation is conducted, where a robot is commanded to follow a desired trajectory using the feedback from visual-inertial estimation. By extensively evaluating the trajectory tracking performance of representative state-of-the-art visual-inertial SLAM systems, we reveal the importance of latency reduction in visual estimation module of these systems. The findings suggest directions of future improvements for visual-inertial SLAM.', 'corpus_id': 211818175, 'score': 1}, {'doc_id': '212736991', 'title': 'An Experimental Evaluation of Robustness and Precision for Long-term LiDAR-based Localization in Highly Changing Environments', 'abstract': 'One of the hardest challenges to face in the development of a non GPS-based localization system for autonomous vehicles is the changes of the environment. LiDAR-based systems typically try to match the last measurements obtained with a previously recorded map of the area. If the existing map is not updated along time, there is a good chance that the measures will not match the environment well enough, causing the vehicle to lose track of its location. In this paper, we present and analyze experimental results regarding the robustness and precision of a map-matching based localization system over a certain period of time in the following three cases: (1) without any update of the initial map, (2) updating the map as the vehicle moves and (3) with map updates that take into account surrounding structures labeled as ""fixed"" which are treated differently. The environment of the tests is a busy parking area, which ensures drastic changes from one day to the next. The precision is obtained by comparing the positions computed using the map with the ones provided by a Real-Time Kinematic GPS system. The experimental results reveal a positioning error of about 6cm which remains stable even after 23 days when using fixed structures on the working area.', 'corpus_id': 212736991, 'score': 0}, {'doc_id': '14042524', 'title': 'On-Manifold Preintegration for Real-Time Visual--Inertial Odometry', 'abstract': 'Current approaches for visual--inertial odometry (VIO) are able to attain highly accurate state estimation via nonlinear optimization. However, real-time optimization quickly becomes infeasible as the trajectory grows over time; this problem is further emphasized by the fact that inertial measurements come at high rate, hence, leading to the fast growth of the number of variables in the optimization. In this paper, we address this issue by preintegrating inertial measurements between selected keyframes into single relative motion constraints. Our first contribution is a preintegration theory that properly addresses the manifold structure of the rotation group. We formally discuss the generative measurement model as well as the nature of the rotation noise and derive the expression for the maximum a posteriori state estimator. Our theoretical development enables the computation of all necessary Jacobians for the optimization and a posteriori bias correction in analytic form. The second contribution is to show that the preintegrated inertial measurement unit model can be seamlessly integrated into a visual--inertial pipeline under the unifying framework of factor graphs. This enables the application of incremental-smoothing algorithms and the use of a structureless model for visual measurements, which avoids optimizing over the 3-D points, further accelerating the computation. We perform an extensive evaluation of our monocular VIO pipeline on real and simulated datasets. The results confirm that our modeling effort leads to an accurate state estimation in real time, outperforming state-of-the-art approaches.', 'corpus_id': 14042524, 'score': 1}, {'doc_id': '221857395', 'title': 'MAFF-Net: Filter False Positive for 3D Vehicle Detection with Multi-modal Adaptive Feature Fusion', 'abstract': '3D vehicle detection based on multi-modal fusion is an important task of many applications such as autonomous driving. Although significant progress has been made, we still observe two aspects that need to be further improvement: First, the specific gain that camera images can bring to 3D detection is seldom explored by previous works. Second, many fusion algorithms run slowly, which is essential for applications with high real-time requirements(autonomous driving). To this end, we propose an end-to-end trainable single-stage multi-modal feature adaptive network in this paper, which uses image information to effectively reduce false positive of 3D detection and has a fast detection speed. A multi-modal adaptive feature fusion module based on channel attention mechanism is proposed to enable the network to adaptively use the feature of each modal. Based on the above mechanism, two fusion technologies are proposed to adapt to different usage scenarios: PointAttentionFusion is suitable for filtering simple false positive and faster; DenseAttentionFusion is suitable for filtering more difficult false positive and has better overall performance. Experimental results on the KITTI dataset demonstrate significant improvement in filtering false positive over the approach using only point cloud data. Furthermore, the proposed method can provide competitive results and has the fastest speed compared to the published state-of-the-art multi-modal methods in the KITTI benchmark.', 'corpus_id': 221857395, 'score': 1}, {'doc_id': '218502155', 'title': 'RadarSLAM: Radar based Large-Scale SLAM in All Weathers', 'abstract': 'Numerous Simultaneous Localization and Mapping (SLAM) algorithms have been presented in last decade using different sensor modalities. However, robust SLAM in extreme weather conditions is still an open research problem. In this paper, RadarSLAM, a full radar based graph SLAM system, is proposed for reliable localization and mapping in large-scale environments. It is composed of pose tracking, local mapping, loop closure detection and pose graph optimization, enhanced by novel feature matching and probabilistic point cloud generation on radar images. Extensive experiments are conducted on a public radar dataset and several self-collected radar sequences, demonstrating the state-of-the-art reliability and localization accuracy in various adverse weather conditions, such as dark night, dense fog and heavy snowfall.', 'corpus_id': 218502155, 'score': 0}, {'doc_id': '219259778', 'title': 'milliEgo: mmWave Aided Egomotion Estimation with Deep Sensor Fusion', 'abstract': 'Robust and accurate trajectory estimation of mobile agents such as people and robots is a key requirement for providing spatial awareness to emerging capabilities such as augmented reality or autonomous interaction. Although currently dominated by vision based techniques e.g., visual-inertial odometry, these suffer from challenges with scene illumination or featureless surfaces. As an alternative, we propose \\sysname, a novel deep-learning approach to robust egomotion estimation which exploits the capabilities of low-cost mmWave radar. Although mmWave radar has a fundamental advantage over monocular cameras of being metric i.e., providing absolute scale or depth, current single chip solutions have limited and sparse imaging resolution, making existing point-cloud registration techniques brittle. We propose a new architecture that is optimized for solving this underdetermined pose transformation problem. Secondly, to robustly fuse mmWave pose estimates with additional sensors, e.g. inertial or visual sensor we introduce a mixed attention approach to deep fusion. Through extensive experiments, we demonstrate how mmWave radar outperforms existing state-of-the-art odometry techniques. We also show that the neural architecture can be made highly efficient and suitable for real-time embedded applications.', 'corpus_id': 219259778, 'score': 1}, {'doc_id': '210928259', 'title': 'Online LiDAR-SLAM for Legged Robots with Robust Registration and Deep-Learned Loop Closure', 'abstract': 'In this paper, we present a 3D factor-graph LiDAR-SLAM system which incorporates a state-of-the-art deeply learned feature-based loop closure detector to enable a legged robot to localize and map in industrial environments. Point clouds are accumulated using an inertial-kinematic state estimator before being aligned using ICP registration. To close loops we use a loop proposal mechanism which matches individual segments between clouds. We trained a descriptor offline to match these segments. The efficiency of our method comes from carefully designing the network architecture to minimize the number of parameters such that this deep learning method can be deployed in real-time using only the CPU of a legged robot, a major contribution of this work. The set of odometry and loop closure factors are updated using pose graph optimization. Finally we present an efficient risk alignment prediction method which verifies the reliability of the registrations. Experimental results at an industrial facility demonstrated the robustness and flexibility of our system, including autonomous following paths derived from the SLAM map.', 'corpus_id': 210928259, 'score': 1}, {'doc_id': '212644553', 'title': 'SDVTracker: Real-Time Multi-Sensor Association and Tracking for Self-Driving Vehicles', 'abstract': 'Accurate motion state estimation of Vulnerable Road Users (VRUs), is a critical requirement for autonomous vehicles that navigate in urban environments. Due to their computational efficiency, many traditional autonomy systems perform multi-object tracking using Kalman Filters which frequently rely on hand-engineered association. However, such methods fail to generalize to crowded scenes and multi-sensor modalities, often resulting in poor state estimates which cascade to inaccurate predictions. We present a practical and lightweight tracking system, SDVTracker, that uses a deep learned model for association and state estimation in conjunction with an Interacting Multiple Model (IMM) filter. The proposed tracking method is fast, robust and generalizes across multiple sensor modalities and different VRU classes. In this paper, we detail a model that jointly optimizes both association and state estimation with a novel loss, an algorithm for determining ground-truth supervision, and a training procedure. We show this system significantly outperforms hand-engineered methods on a real-world urban driving dataset while running in less than 2.5 ms on CPU for a scene with 100 actors, making it suitable for self-driving applications where low latency and high accuracy is critical.', 'corpus_id': 212644553, 'score': 1}, {'doc_id': '221995520', 'title': 'Loop-box: Multi-Agent Direct SLAM Triggered by Single Loop Closure for Large-Scale Mapping', 'abstract': 'In this article, we present a multiagent framework for real-time large-scale 3-D reconstruction applications. In SLAM, researchers usually build and update a 3-D map after applying nonlinear pose graph optimization techniques. Moreover, many multiagent systems are prevalently using odometry information from additional sensors. These methods generally involve extensive computer vision algorithms and are tightly coupled with various sensors. We develop a generic method for the key challenging scenarios in multiagent 3-D mapping based on different camera systems. The proposed framework performs actively in terms of localizing each agent after the first loop closure between them. It is shown that the proposed system only uses monocular cameras to yield real-time multiagent large-scale localization and 3-D global mapping. Based on the initial matching, our system can calculate the optimal scale difference between multiple 3-D maps and then estimate an accurate relative pose transformation for large-scale global mapping.', 'corpus_id': 221995520, 'score': 0}, {'doc_id': '222379651', 'title': 'DynaSLAM II: Tightly-Coupled Multi-Object Tracking and SLAM', 'abstract': 'The assumption of scene rigidity is common in visual SLAM algorithms. However, it limits their applicability in populated real-world environments. Furthermore, most scenarios including autonomous driving, multi-robot collaboration and augmented/virtual reality, require explicit motion information of the surroundings to help with decision making and scene understanding. We present in this paper DynaSLAM II, a visual SLAM system for stereo and RGB-D camera configurations that tightly integrates the multi-object tracking capability. DynaSLAM II makes use of instance semantic segmentation and ORB features to track dynamic objects. The structures of the static scene and the dynamic objects are optimized jointly with the trajectories of both the camera and the moving agents within a novel bundle adjustment proposal. The 3D bounding boxes of the objects are also estimated and loosely optimized within a fixed temporal window. We demonstrate that tracking dynamic objects does not only provide rich clues for scene understanding but can be also beneficial for camera tracking.', 'corpus_id': 222379651, 'score': 0}]"
176	{'doc_id': '210835507', 'title': '3PXNet: Pruned-Permuted-Packed XNOR Networks for Edge Machine Learning', 'abstract': 'As the adoption of Neural Networks continues to proliferate different classes of applications and systems, edge devices have been left behind. Their strict energy and storage limitations make them unable to cope with the sizes of common network models. While many compression methods such as precision reduction and sparsity have been proposed to alleviate this, they don’t go quite far enough. To push size reduction to its absolute limits, we combine binarization with sparsity in Pruned-Permuted-Packed XNOR Networks (3PXNet), which can be efficiently implemented on even the smallest of embedded microcontrollers. 3PXNets can reduce model sizes by up to 38X and reduce runtime by up to 3X compared with already compact conventional binarized implementations with less than 3% accuracy reduction. We have created the first software implementation of sparse-binarized Neural Networks, released as open source library targeting edge devices. Our library is complete with training methodology and model generating scripts, making it easy and fast to deploy.', 'corpus_id': 210835507}	6078	[{'doc_id': '220496140', 'title': 'Learning to Learn Parameterized Classification Networks for Scalable Input Images', 'abstract': 'Convolutional Neural Networks (CNNs) do not have a predictable recognition behavior with respect to the input resolution change. This prevents the feasibility of deployment on different input image resolutions for a specific model. To achieve efficient and flexible image classification at runtime, we employ meta learners to generate convolutional weights of main networks for various input scales and maintain privatized Batch Normalization layers per scale. For improved training performance, we further utilize knowledge distillation on the fly over model predictions based on different input resolutions. The learned meta network could dynamically parameterize main networks to act on input images of arbitrary size with consistently better accuracy compared to individually trained models. Extensive experiments on the ImageNet demonstrate that our method achieves an improved accuracy-efficiency trade-off during the adaptive inference process. By switching executable input resolutions, our method could satisfy the requirement of fast adaption in different resource-constrained environments. Code and models are available at this https URL.', 'corpus_id': 220496140, 'score': 0}, {'doc_id': '220514221', 'title': 'Alpha-Net: Architecture, Models, and Applications', 'abstract': 'Deep learning network training is usually computationally expensive and intuitively complex. We present a novel network architecture for custom training and weight evaluations. We reformulate the layers as ResNet-similar blocks with certain inputs and outputs of their own, the blocks (called Alpha blocks) on their connection configuration form their own network, combined with our novel loss function and normalization function form the complete Alpha-Net architecture. We provided the empirical mathematical formulation of network loss function for more understanding of accuracy estimation and further optimizations. We implemented Alpha-Net with 4 different layer configurations to express the architecture behavior comprehensively. On a custom dataset based on ImageNet benchmark, we evaluate Alpha-Net v1, v2, v3, and v4 for image recognition to give the accuracy of 78.2%, 79.1%, 79.5%, and 78.3% respectively. The Alpha-Net v3 gives improved accuracy of approx. 3% over the last state-of-the-art network ResNet 50 on ImageNet benchmark. We also present an analysis of our dataset with 256, 512, and 1024 layers and different versions of the loss function. Input representation is also crucial for training as initial preprocessing will take only a handful of features to make training less complex than it needs to be. We also compared network behavior with different layer structures, different loss functions, and different normalization functions for better quantitative modeling of Alpha-Net.', 'corpus_id': 220514221, 'score': 0}, {'doc_id': '221068357', 'title': 'GraSeq: Graph and Sequence Fusion Learning for Molecular Property Prediction', 'abstract': 'With the recent advancement of deep learning, molecular representation learning -- automating the discovery of feature representation of molecular structure, has attracted significant attention from both chemists and machine learning researchers. Deep learning can facilitate a variety of downstream applications, including bio-property prediction, chemical reaction prediction, etc. Despite the fact that current SMILES string or molecular graph molecular representation learning algorithms (via sequence modeling and graph neural networks, respectively) have achieved promising results, there is no work to integrate the capabilities of both approaches in preserving molecular characteristics (e.g, atomic cluster, chemical bond) for further improvement. In this paper, we propose GraSeq, a joint graph and sequence representation learning model for molecular property prediction. Specifically, GraSeq makes a complementary combination of graph neural networks and recurrent neural networks for modeling two types of molecular inputs, respectively. In addition, it is trained by the multitask loss of unsupervised reconstruction and various downstream tasks, using limited size of labeled datasets. In a variety of chemical property prediction tests, we demonstrate that our GraSeq model achieves better performance than state-of-the-art approaches.', 'corpus_id': 221068357, 'score': 1}, {'doc_id': '231698931', 'title': 'Approximating Probability Distributions by ReLU Networks', 'abstract': 'How many neurons are needed to approximate a target probability distribution using a neural network with a given input distribution and approximation error? This paper examines this question for the case when the input distribution is uniform, and the target distribution belongs to the class of histogram distributions. We obtain a new upper bound on the number of required neurons, which is strictly better than previously existing upper bounds. The key ingredient in this improvement is an efficient construction of the neural nets representing piecewise linear functions. We also obtain a lower bound on the minimum number of neurons needed to approximate the histogram distributions.', 'corpus_id': 231698931, 'score': 1}, {'doc_id': '213729382', 'title': 'Dynamic Model Pruning with Feedback', 'abstract': 'Deep neural networks often have millions of parameters. This can hinder their deployment to low-end devices, not only due to high memory requirements but also because of increased latency at inference. We propose a novel model compression method that generates a sparse trained model without additional overhead: by allowing (i) dynamic allocation of the sparsity pattern and (ii) incorporating feedback signal to reactivate prematurely pruned weights we obtain a performant sparse model in one single training pass (retraining is not needed, but can further improve the performance). We evaluate the method on CIFAR-10 and ImageNet, and show that the obtained sparse models can reach the state-of-the-art performance of dense models and further that their performance surpasses all previously proposed pruning schemes (that come without feedback mechanisms).', 'corpus_id': 213729382, 'score': 0}, {'doc_id': '221836090', 'title': 'Evolutionary Architecture Search for Graph Neural Networks', 'abstract': 'Automated machine learning (AutoML) has seen a resurgence in interest with the boom of deep learning over the past decade. In particular, Neural Architecture Search (NAS) has seen significant attention throughout the AutoML research community, and has pushed forward the state-of-the-art in a number of neural models to address grid-like data such as texts and images. However, very litter work has been done about Graph Neural Networks (GNN) learning on unstructured network data. Given the huge number of choices and combinations of components such as aggregator and activation function, determining the suitable GNN structure for a specific problem normally necessitates tremendous expert knowledge and laborious trails. In addition, the slight variation of hyper parameters such as learning rate and dropout rate could dramatically hurt the learning capacity of GNN. In this paper, we propose a novel AutoML framework through the evolution of individual models in a large GNN architecture space involving both neural structures and learning parameters. Instead of optimizing only the model structures with fixed parameter settings as existing work, an alternating evolution process is performed between GNN structures and learning parameters to dynamically find the best fit of each other. To the best of our knowledge, this is the first work to introduce and evaluate evolutionary architecture search for GNN models. Experiments and validations demonstrate that evolutionary NAS is capable of matching existing state-of-the-art reinforcement learning approaches for both the semi-supervised transductive and inductive node representation learning and classification.', 'corpus_id': 221836090, 'score': 1}, {'doc_id': '232110866', 'title': 'Perceiver: General Perception with Iterative Attention', 'abstract': 'Biological systems understand the world by simultaneously processing high-dimensional inputs from modalities as diverse as vision, audition, touch, proprioception, etc. The perception models used in deep learning on the other hand are designed for individual modalities, often relying on domain-specific assumptions such as the local grid structures exploited by virtually all existing vision models. These priors introduce helpful inductive biases, but also lock models to individual modalities. In this paper we introduce the Perceiver – a model that builds upon Transformers and hence makes few architectural assumptions about the relationship between its inputs, but that also scales to hundreds of thousands of inputs, like ConvNets. The model leverages an asymmetric attention mechanism to iteratively distill inputs into a tight latent bottleneck, allowing it to scale to handle very large inputs. We show that this architecture performs competitively or beyond strong, specialized models on classification tasks across various modalities: images, point clouds, audio, video and video+audio. The Perceiver obtains performance comparable to ResNet-50 on ImageNet without convolutions and by directly attending to 50,000 pixels. It also surpasses state-of-the-art results for all modalities in AudioSet.', 'corpus_id': 232110866, 'score': 1}, {'doc_id': '219963786', 'title': 'NeuralScale: Efficient Scaling of Neurons for Resource-Constrained Deep Neural Networks', 'abstract': 'Deciding the amount of neurons during the design of a deep neural network to maximize performance is not intuitive. In this work, we attempt to search for the neuron (filter) configuration of a fixed network architecture that maximizes accuracy. Using iterative pruning methods as a proxy, we parametrize the change of the neuron (filter) number of each layer with respect to the change in parameters, allowing us to efficiently scale an architecture across arbitrary sizes. We also introduce architecture descent which iteratively refines the parametrized function used for model scaling. The combination of both proposed methods is coined as NeuralScale. To prove the efficiency of NeuralScale in terms of parameters, we show empirical simulations on VGG11, MobileNetV2 and ResNet18 using CIFAR10, CIFAR100 and TinyImageNet as benchmark datasets. Our results show an increase in accuracy of 3.04%, 8.56% and 3.41% for VGG11, MobileNetV2 and ResNet18 on CIFAR10, CIFAR100 and TinyImageNet respectively under a parameter-constrained setting (output neurons (filters) of default configuration with scaling factor of 0.25).', 'corpus_id': 219963786, 'score': 0}, {'doc_id': '220265908', 'title': 'Maximum Entropy Models for Fast Adaptation', 'abstract': 'Deep Neural Networks have shown great promise on a variety of downstream tasks; but their ability to adapt to new data and tasks remains a challenging problem. The ability of a model to perform few-shot adaptation to a novel task is important for the scalability and deployment of machine learning models. Recent work has shown that the learned features in a neural network follow a normal distribution [41], which thereby results in a strong prior on the downstream task. This implicit overfitting to data from training tasks limits the ability to generalize and adapt to unseen tasks at test time. This also highlights the importance of learning task-agnostic representations from data. In this paper, we propose a regularization scheme using a max-entropy prior on the learned features of a neural network; such that the extracted features make minimal assumptions about the training data. We evaluate our method on adaptation to unseen tasks by performing experiments in 4 distinct settings. We find that our method compares favourably against multiple strong baselines across all of these experiments.', 'corpus_id': 220265908, 'score': 1}, {'doc_id': '229678739', 'title': 'Logic Tensor Networks', 'abstract': 'Artificial Intelligence agents are required to learn from their surroundings and to reason about the knowledge that has been learned in order to make decisions. While state-of-the-art learning from data typically use sub-symbolic distributed representations, reasoning is normally useful at a higher level of abstraction with the use of a first-order logic language for knowledge representation. As a result, attempts at combining symbolic AI and neural computation into neural-symbolic systems have been on the increase. In this paper, we present Logic Tensor Networks (LTN), a neurosymbolic formalism and computational model that supports learning and reasoning through the introduction of a many-valued, end-to-end differentiable first-order logic called Real Logic as a representation language for deep learning. We show that LTN provides a uniform language for the specification and the computation of several AI tasks such as data clustering, multi-label classification, relational learning, query answering, semi-supervised learning, regression and embedding learning. We implement and illustrate each of the above tasks with a number of simple explanatory examples using TensorFlow 2.', 'corpus_id': 229678739, 'score': 1}]
177	{'doc_id': '235755202', 'title': 'Generalization Error Analysis of Neural networks with Gradient Based Regularization', 'abstract': 'We study gradient-based regularization methods for neural networks. We mainly focus on two regularizationmethods: the total variation and the Tikhonov regularization. Applying these methods is equivalent to using neural networks to solve some partial differential equations, mostly in high dimensions in practical applications. In this work, we introduce a general framework to analyze the generalization error of regularized networks. The error estimate relies on two assumptions on the approximation error and the quadrature error. Moreover, we conduct some experiments on the image classification tasks to show that gradient-based methods can significantly improve the generalization ability and adversarial robustness of neural networks. A graphical extension of the gradient-based methods are also considered in the experiments.', 'corpus_id': 235755202}	9851	"[{'doc_id': '229331851', 'title': 'On the eigenvector bias of Fourier feature networks: From regression to solving multi-scale PDEs with physics-informed neural networks', 'abstract': 'Physics-informed neural networks (PINNs) are demonstrating remarkable promise in integrating physical models with gappy and noisy observational data, but they still struggle in cases where the target functions to be approximated exhibit high-frequency or multi-scale features. In this work we investigate this limitation through the lens of Neural Tangent Kernel (NTK) theory and elucidate how PINNs are biased towards learning functions along the dominant eigen-directions of their limiting NTK. Using this observation, we construct novel architectures that employ spatio-temporal and multi-scale random Fourier features, and justify how such coordinate embedding layers can lead to robust and accurate PINN models. Numerical examples are presented for several challenging cases where conventional PINN models fail, including wave propagation and reaction-diffusion dynamics, illustrating how the proposed methods can be used to effectively tackle both forward and inverse problems involving partial differential equations with multi-scale behavior. All code an data accompanying this manuscript will be made publicly available at https://github.com/ PredictiveIntelligenceLab/MultiscalePINNs.', 'corpus_id': 229331851, 'score': 1}, {'doc_id': '227745095', 'title': 'Statistical Mechanics of Deep Linear Neural Networks: The Back-Propagating Renormalization Group', 'abstract': 'The success of deep learning in many real-world tasks has triggered an effort to theoretically understand the power and limitations of deep learning in training and generalization of complex tasks, so far with limited progress. In this work, we study the statistical mechanics of learning in Deep Linear Neural Networks (DLNNs) in which the input-output function of an individual unit is linear. Despite the linearity of the units, learning in DLNNs is highly nonlinear, hence studying its properties reveals some of the essential features of nonlinear Deep Neural Networks (DNNs). We solve exactly the network properties following supervised learning using an equilibrium Gibbs distribution in the weight space. To do this, we introduce the Back-Propagating Renormalization Group (BPRG) which allows for the incremental integration of the network weights layer by layer from the network output layer and progressing backward. This procedure allows us to evaluate important network properties such as its generalization error, the role of network width and depth, the impact of the size of the training set, and the effects of weight regularization and learning stochasticity. Furthermore, by performing partial integration of layers, BPRG allows us to compute the emergent properties of the neural representations across the different hidden layers. We have proposed a heuristic extension of the BPRG to nonlinear DNNs with rectified linear units (ReLU). Surprisingly, our numerical simulations reveal that despite the nonlinearity, the predictions of our theory are largely shared by ReLU networks with modest depth, in a wide regime of parameters. Our work is the first exact statistical mechanical study of learning in a family of Deep Neural Networks, and the first development of the Renormalization Group approach to the weight space of these systems.', 'corpus_id': 227745095, 'score': 0}, {'doc_id': '148571926', 'title': 'AutoAssist: A Framework to Accelerate Training of Deep Neural Networks', 'abstract': 'Deep neural networks have yielded superior performance in many applications; however, the gradient computation in a deep model with millions of instances lead to a lengthy training process even with modern GPU/TPU hardware acceleration. In this paper, we propose AutoAssist, a simple framework to accelerate training of a deep neural network. Typically, as the training procedure evolves, the amount of improvement in the current model by a stochastic gradient update on each instance varies dynamically. In AutoAssist, we utilize this fact and design a simple instance shrinking operation, which is used to filter out instances with relatively low marginal improvement to the current model; thus the computationally intensive gradient computations are performed on informative instances as much as possible. We prove that the proposed technique outperforms vanilla SGD with existing importance sampling approaches for linear SVM problems, and establish an O(1/k) convergence for strongly convex problems. In order to apply the proposed techniques to accelerate training of deep models, we propose to jointly train a very lightweight Assistant network in addition to the original deep network referred to as Boss. The Assistant network is designed to gauge the importance of a given instance with respect to the current Boss such that a shrinking operation can be applied in the batch generator. With careful design, we train the Boss and Assistant in a nonblocking and asynchronous fashion such that overhead is minimal. We demonstrate that AutoAssist reduces the number of epochs by 40% for training a ResNet to reach the same test accuracy on an image classification data set and saves 30% training time needed for a transformer model to yield the same BLEU scores on a translation dataset.', 'corpus_id': 148571926, 'score': 1}, {'doc_id': '22553892', 'title': 'A New Type of Neurons for Machine Learning', 'abstract': 'In machine learning, an artificial neural network is the mainstream approach. Such a network consists of many neurons. These neurons are of the same type characterized by the 2 features: (1) an inner product of an input vector and a matching weighting vector of trainable parameters and (2) a nonlinear excitation function. Here, we investigate the possibility of replacing the inner product with a quadratic function of the input vector, thereby upgrading the first-order neuron to the second-order neuron, empowering individual neurons and facilitating the optimization of neural networks. Also, numerical examples are provided to illustrate the feasibility and merits of the second-order neurons. Finally, further topics are discussed.', 'corpus_id': 22553892, 'score': 1}, {'doc_id': '227160944', 'title': 'Frequency Principle in deep learning: an overview', 'abstract': 'Understanding deep learning is increasingly emergent as it penetrates more and more industry and science. In recent years, a research line from Fourier analysis sheds lights into this “black box” magic by showing a Frequency Principle (F-Principle or spectral bias) of the training behavior of deep neural networks (DNNs) — DNNs often fit functions from low to high frequency during the training. The F-Principle is first demonstrated by an one-dimensional synthetic data followed by the verification in high-dimensional real datasets. A series of works subsequently enhance the validity of the F-Principle. The low-frequency implicit bias illustrates the strength of neural network at learning low-frequency function while suffering from learning high-frequency function. Such understanding inspires the design of DNN-based algorithms in practical problems, explains phenomena emerging in various scenarios, and further advances the study of deep learning from frequency perspective. Although incomplete, we provide an overview of F-Principle and propose some future research problems.', 'corpus_id': 227160944, 'score': 1}, {'doc_id': '227338773', 'title': 'Multivariate Density Estimation with Deep Neural Mixture Models', 'abstract': ""Albeit worryingly underrated in the recent literature on machine learning in general (and, on deep learning in particular), multivariate density estimation is a fundamental task in many applications, at least implicitly, and still an open issue. With a few exceptions, deep neural networks (DNNs) have seldom been applied to density estimation, mostly due to the unsupervised nature of the estimation task, and (especially) due to the need for constrained training algorithms that ended up realizing proper probabilistic models that satisfy Kolmogorov's axioms. Moreover, in spite of the well-known improvement in terms of modeling capabilities yielded by mixture models over plain single-density statistical estimators, no proper mixtures of multivariate DNN-based component densities have been investigated so far. The paper fills this gap by extending our previous work on Neural Mixture Densities (NMMs) to multivariate DNN mixtures. A maximum-likelihood (ML) algorithm for estimating Deep NMMs (DNMMs) is handed out, which satisfies numerically a combination of hard and soft constraints aimed at ensuring satisfaction of Kolmogorov's axioms. The class of probability density functions that can be modeled to any degree of precision via DNMMs is formally defined. A procedure for the automatic selection of the DNMM architecture, as well as of the hyperparameters for its ML training algorithm, is presented (exploiting the probabilistic nature of the DNMM). Experimental results on univariate and multivariate data are reported on, corroborating the effectiveness of the approach and its superiority to the most popular statistical estimation techniques."", 'corpus_id': 227338773, 'score': 0}, {'doc_id': '227342566', 'title': 'Estimating Vector Fields from Noisy Time Series', 'abstract': 'While there has been a surge of recent interest in learning differential equation models from time series, methods in this area typically cannot cope with highly noisy data. We break this problem into two parts: (i) approximating the unknown vector field (or right-hand side) of the differential equation, and (ii) dealing with noise. To deal with (i), we describe a neural network architecture consisting of tensor products of one-dimensional neural shape functions. For (ii), we propose an al-ternating minimization scheme that switches between vector field training and filtering steps, together with multiple trajectories of training data. We find that the neural shape function architecture retains the approximation properties of dense neural networks, enables effective computation of vector field error, and allows for graphical interpretability, all for data/systems in any finite dimension d. We also study the combination of either our neural shape function method or existing differential equation learning methods with alternating minimization and multiple trajectories. We find that retrofitting any learning method in this way boosts the method’s robustness to noise. While in their raw form the methods struggle with 1% Gaussian noise, after retrofitting, they learn accurate vector fields from data with 10% Gaussian noise.', 'corpus_id': 227342566, 'score': 0}, {'doc_id': '1890353', 'title': 'Introspection: Accelerating Neural Network Training By Learning Weight Evolution', 'abstract': 'Neural Networks are function approximators that have achieved state-of-the-art accuracy in numerous machine learning tasks. In spite of their great success in terms of accuracy, their large training time makes it difficult to use them for various tasks. In this paper, we explore the idea of learning weight evolution pattern from a simple network for accelerating training of novel neural networks. We use a neural network to learn the training pattern from MNIST classification and utilize it to accelerate training of neural networks used for CIFAR-10 and ImageNet classification. Our method has a low memory footprint and is computationally efficient. This method can also be used with other optimizers to give faster convergence. The results indicate a general trend in the weight evolution during training of neural networks.', 'corpus_id': 1890353, 'score': 1}]"
178	"{'doc_id': '12261165', 'title': 'Binary Spatter-Coding of Ordered K-Tuples', 'abstract': ""Information with structure is traditionally organized into records with fields. For example, a medical record consisting of name, sex, age, and weight might look like (Joe, male, 66, 77). What 77 stands for is determined by its location in the record, so that this is an example of local representation. The brain's wiring, and robustness under local damage, speak for the importance of distributed representations. The Holographic Reduced Representation (HRR) of Plate is a prime example based on real or complex vectors. This paper describes how spatter coding leads to binary HRRs, and how the fields of a record are encoded into a long binary word without fields and how they are extracted from such a word."", 'corpus_id': 12261165}"	1214	"[{'doc_id': '733980', 'title': 'Hyperdimensional Computing: An Introduction to Computing in Distributed Representation with High-Dimensional Random Vectors', 'abstract': 'The 1990s saw the emergence of cognitive models that depend on very high dimensionality and randomness. They include Holographic Reduced Representations, Spatter Code, Semantic Vectors, Latent Semantic Analysis, Context-Dependent Thinning, and Vector-Symbolic Architecture. They represent things in high-dimensional vectors that are manipulated by operations that produce new high-dimensional vectors in the style of traditional computing, in what is called here hyperdimensional computing on account of the very high dimensionality. The paper presents the main ideas behind these models, written as a tutorial essay in hopes of making the ideas accessible and even provocative. A sketch of how we have arrived at these models, with references and pointers to further reading, is given at the end. The thesis of the paper is that hyperdimensional representation has much to offer to students of cognitive science, theoretical neuroscience, computer science and engineering, and mathematics.', 'corpus_id': 733980, 'score': 1}, {'doc_id': '212658019', 'title': 'Text classification with word embedding regularization and soft similarity measure', 'abstract': ""Since the seminal work of Mikolov et al., word embeddings have become the preferred word representations for many natural language processing tasks. Document similarity measures extracted from word embeddings, such as the soft cosine measure (SCM) and the Word Mover's Distance (WMD), were reported to achieve state-of-the-art performance on semantic text similarity and text classification. \nDespite the strong performance of the WMD on text classification and semantic text similarity, its super-cubic average time complexity is impractical. The SCM has quadratic worst-case time complexity, but its performance on text classification has never been compared with the WMD. Recently, two word embedding regularization techniques were shown to reduce storage and memory costs, and to improve training speed, document processing speed, and task performance on word analogy, word similarity, and semantic text similarity. However, the effect of these techniques on text classification has not yet been studied. \nIn our work, we investigate the individual and joint effect of the two word embedding regularization techniques on the document processing speed and the task performance of the SCM and the WMD on text classification. For evaluation, we use the $k$NN classifier and six standard datasets: BBCSPORT, TWITTER, OHSUMED, REUTERS-21578, AMAZON, and 20NEWS. \nWe show 39% average $k$NN test error reduction with regularized word embeddings compared to non-regularized word embeddings. We describe a practical procedure for deriving such regularized embeddings through Cholesky factorization. We also show that the SCM with regularized word embeddings significantly outperforms the WMD on text classification and is over 10,000 times faster."", 'corpus_id': 212658019, 'score': 0}, {'doc_id': '210839072', 'title': 'Siamese Graph Neural Networks for Data Integration', 'abstract': 'Data integration has been studied extensively for decades and approached from different angles. However, this domain still remains largely rule-driven and lacks universal automation. Recent development in machine learning and in particular deep learning has opened the way to more general and more efficient solutions to data integration problems. In this work, we propose a general approach to modeling and integrating entities from structured data, such as relational databases, as well as unstructured sources, such as free text from news articles. Our approach is designed to explicitly model and leverage relations between entities, thereby using all available information and preserving as much context as possible. This is achieved by combining siamese and graph neural networks to propagate information between connected entities and support high scalability. We evaluate our method on the task of integrating data about business entities, and we demonstrate that it outperforms standard rule-based systems, as well as other deep learning approaches that do not use graph-based representations.', 'corpus_id': 210839072, 'score': 0}, {'doc_id': '211082582', 'title': 'Neuroevolution of Neural Network Architectures Using CoDeepNEAT and Keras', 'abstract': 'Machine learning is a huge field of study in computer science and statistics dedicated to the execution of computational tasks through algorithms that do not require explicit instructions but instead rely on learning patterns from data samples to automate inferences. A large portion of the work involved in a machine learning project is to define the best type of algorithm to solve a given problem. Neural networks - especially deep neural networks - are the predominant type of solution in the field. However, the networks themselves can produce very different results according to the architectural choices made for them. Finding the optimal network topology and configurations for a given problem is a challenge that requires domain knowledge and testing efforts due to a large number of parameters that need to be considered. The purpose of this work is to propose an adapted implementation of a well-established evolutionary technique from the neuroevolution field that manages to automate the tasks of topology and hyperparameter selection. It uses a popular and accessible machine learning framework - Keras - as the back-end, presenting results and proposed changes concerning the original algorithm. The implementation is available at GitHub (this https URL) with documentation and examples to reproduce the experiments performed for this work.', 'corpus_id': 211082582, 'score': 0}, {'doc_id': '211082837', 'title': 'fastai: A Layered API for Deep Learning', 'abstract': 'fastai is a deep learning library which provides practitioners with high-level components that can quickly and easily provide state-of-the-art results in standard deep learning domains, and provides researchers with low-level components that can be mixed and matched to build new approaches. It aims to do both things without substantial compromises in ease of use, flexibility, or performance. This is possible thanks to a carefully layered architecture, which expresses common underlying patterns of many deep learning and data processing techniques in terms of decoupled abstractions. These abstractions can be expressed concisely and clearly by leveraging the dynamism of the underlying Python language and the flexibility of the PyTorch library. fastai includes: a new type dispatch system for Python along with a semantic type hierarchy for tensors; a GPU-optimized computer vision library which can be extended in pure Python; an optimizer which refactors out the common functionality of modern optimizers into two basic pieces, allowing optimization algorithms to be implemented in 4-5 lines of code; a novel 2-way callback system that can access any part of the data, model, or optimizer and change it at any point during training; a new data block API; and much more. We have used this library to successfully create a complete deep learning course, which we were able to write more quickly than using previous approaches, and the code was more clear. The library is already in wide use in research, industry, and teaching. NB: This paper covers fastai v2, which is currently in pre-release at this http URL', 'corpus_id': 211082837, 'score': 0}, {'doc_id': '195855432', 'title': 'A neural representation of continuous space using fractional binding', 'abstract': 'We present a novel method for constructing neurally implemented spatial representations that we show to be useful for building models of spatial cognition. This method represents continuous (i.e., real-valued) spaces using neurons, and identifies a set of operations for manipulating these representations. Specifically, we use “fractional binding” to construct “spatial semantic pointers” (SSPs) that we use to generate and manipulate representations of spatial maps encoding the positions of objects. We show how these representations can be transformed to answer queries about the location and identities of objects, move the relative or global position of items, and answer queries about regions of space, among other things. We demonstrate that the neural implementation in spiking networks of SSPs have similar accuracy and capacity as the mathematical ideal.', 'corpus_id': 195855432, 'score': 1}, {'doc_id': '155109576', 'title': 'A Binary Learning Framework for Hyperdimensional Computing', 'abstract': 'Brain-inspired Hyperdimensional (HD) computing is a computing paradigm emulating a neuron’s activity in high-dimensional space. In practice, HD first encodes all data points to high-dimensional vectors, called hypervectors, and then performs the classification task in an efficient way using a well-defined set of operations. In order to provide acceptable classification accuracy, the current HD computing algorithms need to map data points to hypervectors with non-binary elements. However, working with non-binary vectors significantly increases the HD computation cost and the amount of memory requirement for both training and inference. In this paper, we propose BinHD, a novel learning framework which enables HD computing to be trained and tested using binary hypervectors. BinHD encodes data points to binary hypervectors and provides a framework which enables HD to perform the training task with significantly low resources and memory footprint. In inference, BinHD binarizes the model and simplifies the costly Cosine similarity used in existing HD computing algorithms to a hardware-friendly Hamming distance metric. In addition, for the first time, BinHD introduces the concept of learning rate in HD computing which gives an extra knob to the HD in order to control the training efficiency and accuracy. We accordingly design a digital hardware to accelerate BinHD computation. Our evaluations on four practical classification applications show that BinHD in training (inference) can achieve 12.4× and 6.3× (13.8× and 9.9×) energy efficiency and speedup as compared to the state-of-the-art HD computing algorithm while providing the similar classification accuracy.', 'corpus_id': 155109576, 'score': 1}, {'doc_id': '18702275', 'title': 'Reasoning with vectors: A continuous model for fast robust inference', 'abstract': 'This paper describes the use of continuous vector space models for reasoning with a formal knowledge base. The practical significance of these models is that they support fast, approximate but robust inference and hypothesis generation, which is complementary to the slow, exact, but sometimes brittle behavior of more traditional deduction engines such as theorem provers. The paper explains the way logical connectives can be used in semantic vector models, and summarizes the development of Predication-based Semantic Indexing, which involves the use of Vector Symbolic Architectures to represent the concepts and relationships from a knowledge base of subject-predicate-object triples. Experiments show that the use of continuous models for formal reasoning is not only possible, but already demonstrably effective for some recognized informatics tasks, and showing promise in other traditional problem areas. Examples described in this paper include: predicting new uses for existing drugs in biomedical informatics; removing unwanted meanings from search results in information retrieval and concept navigation; type-inference from attributes; comparing words based on their orthography; and representing tabular data, including modelling numerical values. The algorithms and techniques described in this paper are all publicly released and freely available in the Semantic Vectors open-source software package.', 'corpus_id': 18702275, 'score': 1}, {'doc_id': '216553168', 'title': ""Neuromorphic Nearest Neighbor Search Using Intel's Pohoiki Springs"", 'abstract': 'Neuromorphic computing applies insights from neuroscience to uncover innovations in computing technology. In the brain, billions of interconnected neurons perform rapid computations at extremely low energy levels by leveraging properties that are foreign to conventional computing systems, such as temporal spiking codes and finely parallelized processing units integrating both memory and computation. Here, we showcase the Pohoiki Springs neuromorphic system, a mesh of 768 interconnected Loihi chips that collectively implement 100 million spiking neurons in silicon. We demonstrate a scalable approximate k-nearest neighbor (k-NN) algorithm for searching large databases that exploits neuromorphic principles. Compared to state-of-the-art conventional CPU-based implementations, we achieve superior latency, index build time, and energy efficiency when evaluated on several standard datasets containing over 1 million high-dimensional patterns. Further, the system supports adding new data points to the indexed database online in O(1) time unlike all but brute force conventional k-NN implementations.', 'corpus_id': 216553168, 'score': 0}, {'doc_id': '49301394', 'title': 'Hierarchical Hyperdimensional Computing for Energy Efficient Classification', 'abstract': 'Brain-inspired Hyperdimensional (HD) computing emulates cognition tasks by computing with hypervectors rather than traditional numerical values. In HD, an encoder maps inputs to high dimensional vectors (hypervectors) and combines them to generate a model for each existing class. During inference, HD performs the task of reasoning by looking for similarities of the input hypervector and each pre-stored class hypervector However, there is not a unique encoding in HD which can perfectly map inputs to hypervectors. This results in low HD classification accuracy over complex tasks such as speech recognition. In this paper we propose MHD, a multi-encoder hierarchical classifier, which enables HD to take full advantages of multiple encoders without increasing the cost of classification. MHD consists of two HD stages: a main stage and a decider stage. The main stage makes use of multiple classifiers with different encoders to classify a wide range of input data. Each classifier in the main stage can trade between efficiency and accuracy by dynamically varying the hypervectors’ dimensions. The decider stage, located before the main stage, learns the difficulty of the input data and selects an encoder within the main stage that will provide the maximum accuracy, while also maximizing the efficiency of the classification task. We test the accuracy/efficiency of the proposed MHD on speech recognition application. Our evaluation shows that MHD can provide a 6.6× improvement in energy efficiency and a 6.3× speedup, as compared to baseline single level HD.', 'corpus_id': 49301394, 'score': 1}]"
179	{'doc_id': '214758813', 'title': 'Experimental Treatment with Favipiravir for COVID-19: An Open-Label Control Study☆', 'abstract': '\n Abstract\n \n An outbreak of severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) infection and its caused coronavirus disease 2019 (COVID-19) has been reported in China since December 2019. More than 16% of patients developed acute respiratory distress syndrome, and the fatality ratio was about 1%–2%. No specific treatment has been reported. Herein, we examine the effects of Favipiravir (FPV) versus Lopinavir (LPV)/ritonavir (RTV) for the treatment of COVID-19. Patients with laboratory-confirmed COVID-19 who received oral FPV (Day 1: 1600\xa0mg twice daily; Days 2–14: 600\xa0mg twice daily) plus interferon (IFN)-α by aerosol inhalation (5 million U twice daily) were included in the FPV arm of this study, whereas patients who were treated with LPV/RTV (Days 1–14: 400\xa0mg/100\xa0mg twice daily) plus IFN-α by aerosol inhalation (5 million U twice daily) were included in the control arm. Changes in chest computed tomography (CT), viral clearance, and drug safety were compared between the two groups. For the 35 patients enrolled in the FPV arm and the 45 patients in the control arm, all baseline characteristics were comparable between the two arms. A shorter viral clearance time was found for the FPV arm versus the control arm (median (interquartile range, IQR), 4 (2.5–9) d versus 11 (8–13) d, P\xa0<\xa00.001). The FPV arm also showed significant improvement in chest imaging compared with the control arm, with an improvement rate of 91.43% versus 62.22% (P\xa0=\xa00.004). After adjustment for potential confounders, the FPV arm also showed a significantly higher improvement rate in chest imaging. Multivariable Cox regression showed that FPV was independently associated with faster viral clearance. In addition, fewer adverse reactions were found in the FPV arm than in the control arm. In this open-label nonrandomized control study, FPV showed significantly better treatment effects on COVID-19 in terms of disease progression and viral clearance; if causal, these results should be important information for establishing standard treatment guidelines to combat the SARS-CoV-2 infection.\n \n', 'corpus_id': 214758813}	5596	[{'doc_id': '218571559', 'title': 'COVID-19: Clinical course and outcomes of 36 maintenance hemodialysis patients from a single center in Spain.', 'abstract': '\n \n SARS-CoV-2-pneumonia emerged in Wuhan, China in December 2019. Unfortunately, there is lack of evidence about the optimal management of novel coronavirus disease 2019 (COVID-19), even less in patients on maintenance hemodialysis (MHD) therapy than in the general population. In this retrospective observational single-center study we analyzed the clinical course and outcomes of all MHD patients hospitalized with COVID-19 from March 12th to April 10th, 2020 as confirmed by real time polymerase chain reaction. Baseline features, clinical course, laboratory data, and different therapies were compared between survivors and non-survivors to identify risk factors associated with mortality. Among the 36 patients, 11 (30.5%) died and 7 could be discharged within the observation period. Clinical and radiological evolution during the first week of admission were predictive of mortality. Among the 36 patients, 18 had worsening of their clinical status, as defined by severe hypoxia with oxygen therapy requirements greater than 4 Liters/minute and radiological worsening. Significantly 11 out of those 18 patients (61.1%) died. None of the classical cardiovascular risk factors in the general population were associated with higher mortality. However, a longer time on hemodialysis (hazard ratio 1.008(95% confidence interval 1.001-1.015) per year), increased LDH levels (1.006(1.001-1.011), and lower lymphocyte count (0.996 (0.992-1.000) one week after clinical onset were all significantly associated with higher mortality risk. Thus, the mortality among hospitalized hemodialysis patients diagnosed with COVID-19 is high. Lymphopenia and increased LDH levels were associated with poor prognosis.\n \n', 'corpus_id': 218571559, 'score': 0}, {'doc_id': '220602322', 'title': 'COVID-19 in people with diabetes: understanding the reasons for worse outcomes', 'abstract': '\n Summary\n \n Since the initial COVID-19 outbreak in China, much attention has focused on people with diabetes because of poor prognosis in those with the infection. Initial reports were mainly on people with type 2 diabetes, although recent surveys have shown that individuals with type 1 diabetes are also at risk of severe COVID-19. The reason for worse prognosis in people with diabetes is likely to be multifactorial, thus reflecting the syndromic nature of diabetes. Age, sex, ethnicity, comorbidities such as hypertension and cardiovascular disease, obesity, and a pro-inflammatory and pro-coagulative state all probably contribute to the risk of worse outcomes. Glucose-lowering agents and anti-viral treatments can modulate the risk, but limitations to their use and potential interactions with COVID-19 treatments should be carefully assessed. Finally, severe acute respiratory syndrome coronavirus 2 infection itself might represent a worsening factor for people with diabetes, as it can precipitate acute metabolic complications through direct negative effects on β-cell function. These effects on β-cell function might also cause diabetic ketoacidosis in individuals with diabetes, hyperglycaemia at hospital admission in individuals with unknown history of diabetes, and potentially new-onset diabetes.\n \n', 'corpus_id': 220602322, 'score': 0}, {'doc_id': '224817799', 'title': 'High versus standard doses of corticosteroids in severe COVID-19: a retrospective cohort study', 'abstract': 'Despite the increasing evidence of the benefit of corticosteroids for the treatment of moderate-severe coronavirus disease 2019 (COVID-19) patients, no data are available about the potential role of high doses of steroids for these patients. We evaluated the mortality, the risk of need for mechanical ventilation (MV), or death and the risk of developing a severe acute respiratory distress syndrome (ARDS) between high (HD) and standard doses (SD) among patients with a severe COVID-19. All consecutive confirmed COVID-19 patients admitted to a single center were selected, including those treated with steroids and an ARDS. Patients were allocated to the HD (≥\u2009250 mg/day of methylprednisolone) of corticosteroids or the SD (≤\u20091.5 mg/kg/day of methylprednisolone) at discretion of treating physician. Five hundred seventy-three patients were included: 428 (74.7%) men, with a median (IQR) age of 64 (54–73) years. In the HD group, a worse baseline respiratory situation was observed and male gender, older age, and comorbidities were significantly more common. After adjusting by baseline characteristics, HDs were associated with a higher mortality than SD (adjusted OR 2.46, 95% CI 1.59–3.81, p\u2009<\u20090.001) and with an increased risk of needing MV or death (adjusted OR 2.35, p\u2009=\u20090.001). Conversely, the risk of developing a severe ARDS was similar between groups. Interaction analysis showed that HD increased mortality exclusively in elderly patients. Our real-world experience advises against exceeding 1–1.5 mg/kg/day of corticosteroids for severe COVID-19 with an ARDS, especially in older subjects. This reinforces the rationale of modulating rather than suppressing immune responses in these patients. Electronic supplementary material The online version of this article (10.1007/s10096-020-04078-1) contains supplementary material, which is available to authorized users.', 'corpus_id': 224817799, 'score': 1}, {'doc_id': '229174254', 'title': 'Use of Intravenous Immunoglobulin (Prevagen or Octagam) for the Treatment of COVID-19: Retrospective Case Series', 'abstract': 'Treatment with immunomodulators, such as intravenous immunoglobulin (IVIG), may attenuate inflammatory responses observed in the severe stages of acute respiratory distress syndrome (ARDS) caused by coronavirus disease 19 (COVID-19). We retrospectively evaluated the clinical courses of 12 COVID-19 patients who received IVIG at various stages of their illness, including within the first 72 h of clinical presentation, after initiation of mechanical ventilation, and after prolonged ventilation and ICU stay. The patients included 9 men and 3 women with a median age of 50 years (range 23–74), median Charlson Comorbidity Score of 2 (range 0–7), and median Acute Physiology and Chronic Health Evaluation Score of 13 (range 5–33) at the time of IVIG. The IVIG total dose ranged from 0.5 to 2.0 g/kg (median 1.25 g/kg) distributed over 1–4 daily doses. The most common regimen received was 0.5 g/kg daily for 3 days. The median time to IVIG administration was 9 days (range 0–48 days) after admission. The median time from first IVIG dose administration to hospital discharge was 14 days (range 3–48). The 5 patients who received IVIG ≤4 days of admission demonstrated a significantly shorter length of hospital stay after treatment (median 7 days, range 3–14 days) than the 7 patients who received it >7 days after admission (median 33 days, range 8–48 days, p = 0.03, Mann-Whitney U test). These cases demonstrate that IVIG may improve the clinical state of patients with moderate to severe COVID-19 infection. Despite very high illness severity scores, all patients survived hospital discharge. No thrombotic events occurred and IVIG was well tolerated, despite most cases demonstrating very elevated D-dimer suggestive of active intravascular fibrinolysis. We believe that IVIG warrants immediate clinical trial evaluation in COVID-19 to confirm its role as a mainstay treatment of moderate to severe COVID-19 infection as a means to reduce hospital stay and utilization of ICU resources, including mechanical ventilation, and potentially reduce mortality.', 'corpus_id': 229174254, 'score': 1}, {'doc_id': '224823862', 'title': 'Efficacy of Tocilizumab in Patients Hospitalized with Covid-19', 'abstract': 'Abstract Background The efficacy of interleukin-6 receptor blockade in hospitalized patients with coronavirus disease 2019 (Covid-19) who are not receiving mechanical ventilation is unclear. Methods We performed a randomized, double-blind, placebo-controlled trial involving patients with confirmed severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) infection, hyperinflammatory states, and at least two of the following signs: fever (body temperature >38°C), pulmonary infiltrates, or the need for supplemental oxygen in order to maintain an oxygen saturation greater than 92%. Patients were randomly assigned in a 2:1 ratio to receive standard care plus a single dose of either tocilizumab (8 mg per kilogram of body weight) or placebo. The primary outcome was intubation or death, assessed in a time-to-event analysis. The secondary efficacy outcomes were clinical worsening and discontinuation of supplemental oxygen among patients who had been receiving it at baseline, both assessed in time-to-event analyses. Results We enrolled 243 patients; 141 (58%) were men, and 102 (42%) were women. The median age was 59.8 years (range, 21.7 to 85.4), and 45% of the patients were Hispanic or Latino. The hazard ratio for intubation or death in the tocilizumab group as compared with the placebo group was 0.83 (95% confidence interval [CI], 0.38 to 1.81; P=0.64), and the hazard ratio for disease worsening was 1.11 (95% CI, 0.59 to 2.10; P=0.73). At 14 days, 18.0% of the patients in the tocilizumab group and 14.9% of the patients in the placebo group had had worsening of disease. The median time to discontinuation of supplemental oxygen was 5.0 days (95% CI, 3.8 to 7.6) in the tocilizumab group and 4.9 days (95% CI, 3.8 to 7.8) in the placebo group (P=0.69). At 14 days, 24.6% of the patients in the tocilizumab group and 21.2% of the patients in the placebo group were still receiving supplemental oxygen. Patients who received tocilizumab had fewer serious infections than patients who received placebo. Conclusions Tocilizumab was not effective for preventing intubation or death in moderately ill hospitalized patients with Covid-19. Some benefit or harm cannot be ruled out, however, because the confidence intervals for efficacy comparisons were wide. (Funded by Genentech; ClinicalTrials.gov number, NCT04356937.)', 'corpus_id': 224823862, 'score': 1}, {'doc_id': '237375513', 'title': 'Efficacy and safety of baricitinib for the treatment of hospitalised adults with COVID-19 (COV-BARRIER): a randomised, double-blind, parallel-group, placebo-controlled phase 3 trial', 'abstract': '\n Background\n Baricitinib is an oral selective Janus kinase 1/2 inhibitor with known anti-inflammatory properties. This study evaluates the efficacy and safety of baricitinib in combination with standard of care for the treatment of hospitalised adults with COVID-19.\n \n Methods\n In this phase 3, double-blind, randomised, placebo-controlled trial, participants were enrolled from 101 centres across 12 countries in Asia, Europe, North America, and South America. Hospitalised adults with COVID-19 receiving standard of care were randomly assigned (1:1) to receive once-daily baricitinib (4 mg) or matched placebo for up to 14 days. Standard of care included systemic corticosteroids, such as dexamethasone, and antivirals, including remdesivir. The composite primary endpoint was the proportion who progressed to high-flow oxygen, non-invasive ventilation, invasive mechanical ventilation, or death by day 28, assessed in the intention-to-treat population. All-cause mortality by day 28 was a key secondary endpoint, and all-cause mortality by day 60 was an exploratory endpoint; both were assessed in the intention-to-treat population. Safety analyses were done in the safety population defined as all randomly allocated participants who received at least one dose of study drug and who were not lost to follow-up before the first post-baseline visit. This study is registered with ClinicalTrials.gov, NCT04421027.\n \n Findings\n Between June 11, 2020, and Jan 15, 2021, 1525 participants were randomly assigned to the baricitinib group (n=764) or the placebo group (n=761). 1204 (79·3%) of 1518 participants with available data were receiving systemic corticosteroids at baseline, of whom 1099 (91·3%) were on dexamethasone; 287 (18·9%) participants were receiving remdesivir. Overall, 27·8% of participants receiving baricitinib and 30·5% receiving placebo progressed to meet the primary endpoint (odds ratio 0·85 [95% CI 0·67 to 1·08], p=0·18), with an absolute risk difference of −2·7 percentage points (95% CI −7·3 to 1·9). The 28-day all-cause mortality was 8% (n=62) for baricitinib and 13% (n=100) for placebo (hazard ratio [HR] 0·57 [95% CI 0·41–0·78]; nominal p=0·0018), a 38·2% relative reduction in mortality; one additional death was prevented per 20 baricitinib-treated participants. The 60-day all-cause mortality was 10% (n=79) for baricitinib and 15% (n=116) for placebo (HR 0·62 [95% CI 0·47–0·83]; p=0·0050). The frequencies of serious adverse events (110 [15%] of 750 in the baricitinib group vs 135 [18%] of 752 in the placebo group), serious infections (64 [9%] vs 74 [10%]), and venous thromboembolic events (20 [3%] vs 19 [3%]) were similar between the two groups.\n \n Interpretation\n Although there was no significant reduction in the frequency of disease progression overall, treatment with baricitinib in addition to standard of care (including dexamethasone) had a similar safety profile to that of standard of care alone, and was associated with reduced mortality in hospitalised adults with COVID-19.\n \n Funding\n Eli Lilly and Company.\n \n Translations\n For the French, Japanese, Portuguese, Russian and Spanish translations of the abstract see Supplementary Materials section.\n', 'corpus_id': 237375513, 'score': 1}, {'doc_id': '220526615', 'title': 'Clinical Manifestations and Outcomes of Critically Ill Children and Adolescents with Coronavirus Disease 2019 in New York City', 'abstract': '\n               \n                  Objectives\n                  To describe the clinical manifestations and outcomes of critically ill children with coronavirus disease-19 (COVID-19) in New York City.\n               \n               \n                  Study design\n                  Retrospective observational study of children 1 month to 21 years admitted March 14 to May 2, 2020 to 9 New York City pediatric intensive care units (PICUs) with SARS-CoV-2 infection.\n               \n               \n                  Results\n                  Of 70 children admitted to PICUs: median age 15 [IQR 9, 19] years; 61.4% male; 38.6% Hispanic; 32.9% Black; 74.3% with comorbidities. Fever (72.9%) and cough (71.4%) were the common presenting symptoms. Twelve patients (17%) met severe sepsis criteria; 14 (20%) required vasopressor support; 21 (30%) developed acute respiratory distress syndrome (ARDS); 9 (12.9%) met acute kidney injury criteria; 1 (1.4%) required renal replacement therapy, and 2 (2.8%) had cardiac arrest. For treatment, 27 (38.6%) patients received hydroxychloroquine; 13 (18.6%) remdesivir; 23 (32.9%) corticosteroids; 3 (4.3%) tocilizumab; 1 (1.4%) anakinra; no patient was given immunoglobulin or convalescent plasma. Forty-nine (70%) patients required respiratory support: 14 (20.0%) non-invasive mechanical ventilation, 20 (28.6%) invasive mechanical ventilation (IMV), 7 (10%) prone position, 2 (2.8%) inhaled nitric oxide, and 1 (1.4%) extracorporeal membrane oxygenation. Nine (45%) of the 20 patients requiring IMV were extubated by day 14 with median IMV duration of 218 [IQR 79, 310.4] hours. Presence of ARDS was significantly associated with duration of PICU and hospital stay, and lower probability of PICU and hospital discharge at hospital day 14 (P < .05 for all).\n               \n               \n                  Conclusions\n                  Critically ill children with COVID-19 predominantly are adolescents, have comorbidities, and require some form of respiratory support. The presence of ARDS is significantly associated with prolonged PICU and hospital stay.\n               \n            ', 'corpus_id': 220526615, 'score': 0}, {'doc_id': '220437021', 'title': 'Intravenous high-dose vitamin C for the treatment of severe COVID-19: study protocol for a multicentre randomised controlled trial', 'abstract': 'Introduction The rapid worldwide spread of COVID-19 has caused a global health crisis. To date, symptomatic supportive care has been the most common treatment. It has been reported that the mechanism of COVID-19 is related to cytokine storms and subsequent immunogenic damage, especially damage to the endothelium and alveolar membrane. Vitamin C (VC), also known as L-ascorbic acid, has been shown to have antimicrobial and immunomodulatory properties. A high dose of intravenous VC (HIVC) was proven to block several key components of cytokine storms, and HIVC showed safety and varying degrees of efficacy in clinical trials conducted on patients with bacterial-induced sepsis and acute respiratory distress syndrome (ARDS). Therefore, we hypothesise that HIVC could be added to the treatment of ARDS and multiorgan dysfunction related to COVID-19. Methods and analysis The investigators designed a multicentre prospective randomised placebo-controlled trial that is planned to recruit 308 adults diagnosed with COVID-19 and transferred into the intensive care unit. Participants will randomly receive HIVC diluted in sterile water or placebo for 7 days once enrolled. Patients with a history of VC allergy, end-stage pulmonary disease, advanced malignancy or glucose-6-phosphate dehydrogenase deficiency will be excluded. The primary outcome is ventilation-free days within 28 observational days. This is one of the first clinical trials applying HIVC to treat COVID-19, and it will provide credible efficacy and safety data. We predict that HIVC could suppress cytokine storms caused by COVID-19, help improve pulmonary function and reduce the risk of ARDS of COVID-19. Ethics and dissemination The study protocol was approved by the Ethics Committee of Zhongnan Hospital of Wuhan University (identifiers: Clinical Ethical Approval No. 2020001). Findings of the trial will be disseminated through peer-reviewed journals and scientific conferences. Trial registration number NCT04264533.', 'corpus_id': 220437021, 'score': 0}, {'doc_id': '226246887', 'title': 'Tocilizumab in the treatment of rapidly evolving COVID-19 pneumonia and multifaceted critical illness: A retrospective case series', 'abstract': '\n Background\n COVID-19 associated critical illness characterized by rapidly evolving acute respiratory failure (ARF) can develop, especially on the grounds of hyperinflammation.\n \n Aim and methods\n A case-series of 61 patients admitted to our intensive care unit (ICU) between August 12 and September 12, 2020 with confirmed COVID-19 pneumonia and rapidly evolving ARF requiring oxygen support therapy and/or mechanical ventilation was retrospectively analyzed. We examined whether intravenous administration of tocilizumab, a monoclonal interleukin-6 receptor antibody, was associated with improved outcome. All patients received empiric antivirals, dexamethasone 6\u202fmg/day for 7 days, antibiotics, and prophylactic anticoagulation. Tocilizumab was administered at a dosage of 8\u202fmg/kg [two consecutive intravenous infusions 12\u202fh apart]. Outcome measures such as mortality on day-14, ICU length of stay, and rate of nosocomial acquired bacterial infections were also analyzed. Results: Patients were males (88.2%) aged 51 [interquartile range (IQR): 42.5–58.75)], with admission Acute Physiology and Chronic Health Evaluation (APACHE) 4 score of 53 (IQR: 37.75–72.5), and had more than one comorbidity (62.3%). On admission, twenty nine patients (47.5%) were mechanically ventilated, and thirty two patients (52.5%) were receiving oxygen therapy. No serious adverse effects due to tocilizumab therapy were recorded. However, twelve patients (19.6%) developed nosocomial acquired infections. ICU length of stay was 13 (IQR: 9–17) days, and mortality on day-14 was 24.6%. Six patients were shifted to other hospitals but were followed-up. The overall mortality on day-30 was 31.1%. Non-mechanically ventilated patients had higher survival rates compared to mechanically ventilated patients although results were not significant [hazards ratio\u202f=\u202f2.6 (95% confidence intervals: 0.9–7.7), p\u202f=\u202f0.08]. Tocilizumab did not affect the mortality of critically ill COVID-19 patients.\n \n Conclusion\n Tocilizumab could be an adjunct safe therapy in rapidly evolving COVID-19 pneumonia and associated critical illness.\n', 'corpus_id': 226246887, 'score': 1}]
180	{'doc_id': '198897678', 'title': 'Interpreting the Latent Space of GANs for Semantic Face Editing', 'abstract': 'Despite the recent advance of Generative Adversarial Networks (GANs) in high-fidelity image synthesis, there lacks enough understanding of how GANs are able to map a latent code sampled from a random distribution to a photo-realistic image. Previous work assumes the latent space learned by GANs follows a distributed representation but observes the vector arithmetic phenomenon. In this work, we propose a novel framework, called InterFaceGAN, for semantic face editing by interpreting the latent semantics learned by GANs. In this framework, we conduct a detailed study on how different semantics are encoded in the latent space of GANs for face synthesis. We find that the latent code of well-trained generative models actually learns a disentangled representation after linear transformations. We explore the disentanglement between various semantics and manage to decouple some entangled semantics with subspace projection, leading to more precise control of facial attributes. Besides manipulating gender, age, expression, and the presence of eyeglasses, we can even vary the face pose as well as fix the artifacts accidentally generated by GAN models. The proposed method is further applied to achieve real image manipulation when combined with GAN inversion methods or some encoder-involved models. Extensive results suggest that learning to synthesize faces spontaneously brings a disentangled and controllable facial attribute representation.', 'corpus_id': 198897678}	12537	"[{'doc_id': '231698400', 'title': 'cGANs for Cartoon to Real-life Images', 'abstract': 'Image-to-image translation is a learning task to establish visual mapping between an input and output image. The task has several variations differentiated based on the purpose of the translation, such as synthetic− →real translation [17][21], photo− →caricature translation [23] and many others. The problem has been tackled using different approaches, either through traditional computer vision methods [7], as well as deep learning approaches in recent trends. One approach currently deemed popular and effective is using conditional generative adversarial network, also known shortly as cGAN [15]. It is adapted to perform image-to-image translation tasks with typically two networks: a generator and a discriminator[10]. The generator attempts to generate a duplicated imitation of the input data distribution from a noise distribution while the discriminator classifies whether the generator’s input is fake, i.e. imitation from the generated distribution, or real, i.e. ground truth from the original distribution. Previous research has focused on specific purpose for cross-image translation. Efros et al. [4] proposed a simple model to synthesize an image based on stitches of input images, which represents a traditional approach directly from input image. Fergus et al. [5] addressed specific problems of blurry image into higher crispness, which provided insights on retaining the crispness of duplicated imitation. Additionally, through user studies and an automated mechanism to select images, Chen et al. [3] developed a model to optimize image selection process before cross-image translation. This project is based on an existing implementation of a network called Pix2Pix[10], a cGAN implementation based on U-Net architecture and convolution Markovian discriminator. The use of U-Net architecture [19], instead of the general encoder-decoder architecture is to improve the efficiency of processing input and output with higher resolution while facilitating the transfer of shared, low-level information directly across layers. The U-Net architecture enables cross-layer communication and therefore could more effi-', 'corpus_id': 231698400, 'score': 0}, {'doc_id': '235899107', 'title': 'StyleFusion: A Generative Model for Disentangling Spatial Segments', 'abstract': 'We present StyleFusion, a new mapping architecture for StyleGAN, which takes as input a number of latent codes and fuses them into a single style code. Inserting the resulting style code into a pre-trained StyleGAN generator results in a single harmonized image in which each semantic region is controlled by one of the input latent codes. Effectively, StyleFusion yields a disentangled representation of the image, providing fine-grained control over each region of the generated image. Moreover, to help facilitate global control over the generated image, a special input latent code is incorporated into the fused representation. StyleFusion operates in a hierarchical manner, where each level is tasked with learning to disentangle a pair of image regions (e.g., the car body and wheels). The resulting learned disentanglement allows one to modify both local, fine-grained semantics (e.g., facial features) as well as more global features (e.g., pose and background), providing improved flexibility in the synthesis process. As a natural extension, StyleFusion enables one to perform semantically-aware crossimage mixing of regions that are not necessarily aligned. Finally, we demonstrate how StyleFusion can be paired with existing editing techniques to more faithfully constrain the edit to the user’s region of interest. Code is available at: https://github.com/OmerKafri/StyleFusion.', 'corpus_id': 235899107, 'score': 1}, {'doc_id': '232185323', 'title': 'Level-aware Haze Image Synthesis by Self-Supervised Content-Style Disentanglement', 'abstract': 'The key procedure of haze image translation through adversarial training lies in the disentanglement between the feature only involved in haze synthesis, i.e.style feature, and the feature representing the invariant semantic content, i.e. content feature. Previous methods separate content feature apart by utilizing it to classify haze image during the training process. However, in this paper we recognize the incompleteness of the content-style disentanglement in such technical routine. The flawed style feature entangled with content information inevitably leads the ill-rendering of the haze images. To address, we propose a self-supervised style regression via stochastic linear interpolation to reduce the content information in style feature. The ablative experiments demonstrate the disentangling completeness and its superiority in level-aware haze image synthesis. Moreover, the generated haze data are applied in the testing generalization of vehicle detectors. Further study between haze-level and detection performance shows that haze has obvious impact on the generalization of the vehicle detectors and such performance degrading level is linearly correlated to the haze-level, which, in turn, validates the effectiveness of the proposed method.', 'corpus_id': 232185323, 'score': 0}, {'doc_id': '229924297', 'title': 'OSTeC: One-Shot Texture Completion', 'abstract': 'The last few years have witnessed the great success of non-linear generative models in synthesizing high-quality photorealistic face images. Many recent 3D facial texture reconstruction and pose manipulation from a single image approaches still rely on large and clean face datasets to train image-to-image Generative Adversarial Networks (GANs). Yet the collection of such a large scale highresolution 3D texture dataset is still very costly and difficult to maintain age/ethnicity balance. Moreover, regressionbased approaches suffer from generalization to the in-thewild conditions and are unable to fine-tune to a targetimage. In this work, we propose an unsupervised approach for one-shot 3D facial texture completion that does not require large-scale texture datasets, but rather harnesses the knowledge stored in 2D face generators. The proposed approach rotates an input image in 3D and fill-in the unseen regions by reconstructing the rotated image in a 2D face generator, based on the visible parts. Finally, we stitch the most visible textures at different angles in the UV imageplane. Further, we frontalize the target image by projecting the completed texture into the generator. The qualitative and quantitative experiments demonstrate that the completed UV textures and frontalized images are of high quality, resembles the original identity, can be used to train a texture GAN model for 3DMM fitting and improve poseinvariant face recognition.1', 'corpus_id': 229924297, 'score': 0}, {'doc_id': '222291148', 'title': 'Unsupervised Image-to-Image Translation via Pre-trained StyleGAN2 Network', 'abstract': 'Image-to-Image (I2I) translation is a heated topic in academia, and it also has been applied in real-world industry for tasks like image synthesis, super-resolution, and colorization. However, traditional I2I translation methods train data in two or more domains together. This requires lots of computation resources. Moreover, the results are of lower quality, and they contain many more artifacts. The training process could be unstable when the data in different domains are not balanced, and modal collapse is more likely to happen. We proposed a new I2I translation method that generates a new model in the target domain via a series of model transformations on a pre-trained StyleGAN2 model in the source domain. After that, we proposed an inversion method to achieve the conversion between an image and its latent vector. By feeding the latent vector into the generated model, we can perform I2I translation between the source domain and target domain. Both qualitative and quantitative evaluations were conducted to prove that the proposed method can achieve outstanding performance in terms of image quality, diversity and semantic similarity to the input and reference images compared to state-of-the-art works.', 'corpus_id': 222291148, 'score': 1}, {'doc_id': '227161905', 'title': 'StyleUV: Diverse and High-fidelity UV Map Generative Model', 'abstract': 'Reconstructing 3D human faces in the wild with the 3D Morphable Model (3DMM) has become popular in recent years. While most prior work focuses on estimating more robust and accurate geometry, relatively little attention has been paid to improving the quality of the texture model. Meanwhile, with the advent of Generative Adversarial Networks (GANs), there has been great progress in reconstructing realistic 2D images. Recent work demonstrates that GANs trained with abundant high-quality UV maps can produce high-fidelity textures superior to those produced by existing methods. However, acquiring such high-quality UV maps is difficult because they are expensive to acquire, requiring laborious processes to refine. In this work, we present a novel UV map generative model that learns to generate diverse and realistic synthetic UV maps without requiring high-quality UV maps for training. Our proposed framework can be trained solely with in-the-wild images (i.e., UV maps are not required) by leveraging a combination of GANs and a differentiable renderer. Both quantitative and qualitative evaluations demonstrate that our proposed texture model produces more diverse and higher fidelity textures compared to existing methods.', 'corpus_id': 227161905, 'score': 0}, {'doc_id': '236094869', 'title': 'Coarse-to-fine', 'abstract': 'Facial structure editing of portrait images is challenging given the facial variety, the lack of ground-truth, the necessity of jointly adjusting color and shape, and the requirement of no visual artifacts. In this paper, we investigate how to perform chin editing as a case study of editing facial structures. We present a novel method that can automatically remove the double chin effect in portrait images. Our core idea is to train a fine classification boundary in the latent space of the portrait images. This can be used to edit the chin appearance by manipulating the latent code of the input portrait image while preserving the original portrait features. To achieve such a fine separation boundary, we employ a carefully designed training stage based on latent codes of paired synthetic images with and without a double chin. In the testing stage, our method can automatically handle portrait images with only a refinement to subtle misalignment before and after double chin editing. Our model enables alteration to the neck region of the input portrait image while keeping other regions unchanged, and guarantees the rationality of neck structure and the consistency of facial characteristics. To the best of our knowledge, this presents the first effort towards an effective application for editing double chins. We validate the efficacy and efficiency of our approach through extensive experiments and user studies.', 'corpus_id': 236094869, 'score': 1}, {'doc_id': '232269768', 'title': 'Using latent space regression to analyze and leverage compositionality in GANs', 'abstract': 'In recent years, Generative Adversarial Networks have become ubiquitous in both research and public perception, but how GANs convert an unstructured latent code to a high quality output is still an open question. In this work, we investigate regression into the latent space as a probe to understand the compositional properties of GANs. We find that combining the regressor and a pretrained generator provides a strong image prior, allowing us to create composite images from a collage of random image parts at inference time while maintaining global consistency. To compare compositional properties across different generators, we measure the trade-offs between reconstruction of the unrealistic input and image quality of the regenerated samples. We find that the regression approach enables more localized editing of individual image parts compared to direct editing in the latent space, and we conduct experiments to quantify this independence effect. Our method is agnostic to the semantics of edits, and does not require labels or predefined concepts during training. Beyond image composition, our method extends to a number of related applications, such as image inpainting or example-based image editing, which we demonstrate on several GANs and datasets, and because it uses only a single forward pass, it can operate in real-time. Code is available on our project page: https://chail.github.io/latent-composition/. Compose ⊗ Image Composition Attribute Editing Multimodal Editing Image Completion ⊗ ⊗ ⊗', 'corpus_id': 232269768, 'score': 1}, {'doc_id': '229298022', 'title': 'Self-Supervised Sketch-to-Image Synthesis', 'abstract': ""Imagining a colored realistic image from an arbitrarily drawn sketch is one of the human capabilities that we eager machines to mimic. Unlike previous methods that either requires the sketch-image pairs or utilize low-quantity detected edges as sketches, we study the exemplar-based sketch-to-image (s2i) synthesis task in a self-supervised learning manner, eliminating the necessity of the paired sketch data. To this end, we first propose an unsupervised method to efficiently synthesize line-sketches for general RGB-only datasets. With the synthetic paired-data, we then present a self-supervised Auto-Encoder (AE) to decouple the content/style features from sketches and RGB-images, and synthesize images that are both content-faithful to the sketches and style-consistent to the RGB-images. While prior works employ either the cycle-consistence loss or dedicated attentional modules to enforce the content/style fidelity, we show AE's superior performance with pure self-supervisions. To further improve the synthesis quality in high resolution, we also leverage an adversarial network to refine the details of synthetic images. Extensive experiments on 1024*1024 resolution demonstrate a new state-of-art-art performance of the proposed model on CelebA-HQ and Wiki-Art datasets. Moreover, with the proposed sketch generator, the model shows a promising performance on style mixing and style transfer, which require synthesized images to be both style-consistent and semantically meaningful. Our code is available on this https URL, and please visit this https URL for an online demo of our model."", 'corpus_id': 229298022, 'score': 0}, {'doc_id': '236429028', 'title': 'Perceptually Validated Precise Local Editing for Facial Action Units with StyleGAN', 'abstract': 'The ability to edit facial expressions has a wide range of applications in computer graphics. The ideal facial expression editing algorithm needs to satisfy two important criteria. First, it should allow precise and targeted editing of individual facial actions. Second, it should generate high fidelity outputs without artifacts. We build a solution based on StyleGAN [18], which has been used extensively for semantic manipulation of faces. As we do so, we add to our understanding of how various semantic attributes are encoded in StyleGAN. In particular, we show that a naive strategy to perform editing in the latent space results in undesired coupling between certain action units, even if they are conceptually distinct. For example, although brow lowerer and lip tightener are distinct action units, they appear correlated in the training data. Hence, StyleGAN has difficulty in disentangling them. We allow disentangled editing of such action units by computing detached regions of influence for each action unit, and restrict editing to these regions. We validate the effectiveness of our local editing method through perception experiments conducted with 23 subjects. The results show that our method provides higher control over local editing and produces images with superior fidelity compared to the state-of-the-art methods.', 'corpus_id': 236429028, 'score': 1}]"
181	{'doc_id': '221516770', 'title': 'PAC Reinforcement Learning Algorithm for General-Sum Markov Games', 'abstract': 'This paper presents a theoretical framework for probably approximately correct (PAC) multi-agent reinforcement learning (MARL) algorithms for Markov games. The paper offers an extension to the well-known Nash Q-learning algorithm, using the idea of delayed Q-learning, in order to build a new PAC MARL algorithm for general-sum Markov games. In addition to guiding the design of a provably PAC MARL algorithm, the framework enables checking whether an arbitrary MARL algorithm is PAC. Comparative numerical results demonstrate performance and robustness.', 'corpus_id': 221516770}	1172	"[{'doc_id': '221802566', 'title': 'TotalBotWar: A New Pseudo Real-time Multi-action Game Challenge and Competition for AI', 'abstract': ""This paper presents TotalBotWar, a new pseudo real-time multi-action challenge for game AI, as well as some initial experiments that benchmark the framework with different agents. The game is based on the real-time battles of the popular TotalWar games series where players manage an army to defeat the opponent's one. In the proposed game, a turn consists of a set of orders to control the units. The number and specific orders that can be performed in a turn vary during the progression of the game. One interesting feature of the game is that if a particular unit does not receive an order in a turn, it will continue performing the action specified in a previous turn. The turn-wise branching factor becomes overwhelming for traditional algorithms and the partial observability of the game state makes the proposed game an interesting platform to test modern AI algorithms."", 'corpus_id': 221802566, 'score': 0}, {'doc_id': '223957174', 'title': 'Online non-convex optimization with imperfect feedback', 'abstract': ""We consider the problem of online learning with non-convex losses. In terms of feedback, we assume that the learner observes - or otherwise constructs - an inexact model for the loss function encountered at each stage, and we propose a mixed-strategy learning policy based on dual averaging. In this general context, we derive a series of tight regret minimization guarantees, both for the learner's static (external) regret, as well as the regret incurred against the best dynamic policy in hindsight. Subsequently, we apply this general template to the case where the learner only has access to the actual loss incurred at each stage of the process. This is achieved by means of a kernel-based estimator which generates an inexact model for each round's loss function using only the learner's realized losses as input."", 'corpus_id': 223957174, 'score': 0}, {'doc_id': '222066882', 'title': 'Relay Pursuit of an Evader by a Heterogeneous Group of Pursuers using Potential Games', 'abstract': 'In this paper, we propose a decentralized game-theoretic pursuit policy for a heterogeneous group of pursuers who individually attempt to, without any prescribed cooperative pursuit strategy, capture a single evader who strives to delay or avoid capture if possible. We assume that the pursuers are rational (self-interested) agents who are not necessarily connected via communication network. Our proposed pursuit policy is motivated from the semi-cooperative pursuit policy called relay pursuit [1] under which only the pursuer who can capture the evader faster than the others is active while the rest stay put. In contrast to the latter strategy, our proposed method does not rely on geometric tools. It relies instead on reducing the noncooperative pursuit-evasion game into a sequence of maximum weighted bipartite matching problems which seek to find the pursuer-evader assignments which will result in minimum time of capture. To find the optimal assignment in a decentralized manner, the graph matching problem at each time instant is formulated as a static potential game whose pure strategy Nash equilibria correspond to the optimal assignments. Such equilibria are found by iteratively executing a game-theoretic learning algorithm called Joint Strategy Fictitious Play (JSFP) under which every pursuer synchronously takes his best reply strategy (pursue or stay put), depending on the joint actions of other pursuers, until they reach a Nash equilibrium. We illustrate the performance of our method by means of extensive numerical simulations.', 'corpus_id': 222066882, 'score': 0}, {'doc_id': '221186809', 'title': 'Primal-dual evolutionary dynamics for constrained population games', 'abstract': 'Population games can be regarded as a tool to study the strategic interaction of a population of players. Although several attention has been given to such field, most of the available works have focused only on the unconstrained case. That is, the allowed equilibrium of the game is not constrained. To further extend the capabilities of population games, in this paper we propose a novel class of primal-dual evolutionary dynamics that allow the consideration of constraints that must be satisfied at the equilibrium of the game. Using duality theory and Lyapunov stability theory, we provide sufficient conditions to guarantee the asymptotic stability and feasibility of the equilibria set of the game under the considered constraints. Furthermore, we illustrate the application of the developed theory to some classical population games with the addition of constraints.', 'corpus_id': 221186809, 'score': 1}, {'doc_id': '221139700', 'title': 'On the Suboptimality of Negative Momentum for Minimax Optimization', 'abstract': 'Smooth game optimization has recently attracted great interest in machine learning as it generalizes the single-objective optimization paradigm. However, game dynamics is more complex due to the interaction between different players and is therefore fundamentally different from minimization, posing new challenges for algorithm design. Notably, it has been shown that negative momentum is preferred due to its ability of reducing oscillation in game dynamics. Nevertheless, existing analysis about negative momentum was restricted to simple bilinear games. In this paper, we extend the analysis of negative momentum to smooth and strongly-convex strongly-concave minimax games by taking the variational inequality formulation. By connecting momentum method with Chebyshev polynomials, we show that negative momentum accelerates convergence of game dynamics locally, though with a suboptimal rate. To the best of our knowledge, this is the \\emph{first work} that provides an explicit convergence rate for negative momentum in this setting.', 'corpus_id': 221139700, 'score': 1}, {'doc_id': '219635890', 'title': 'First-order methods for large-scale market equilibrium computation', 'abstract': 'Market equilibrium is a solution concept with many applications such as digital ad markets, fair division, and resource sharing. For many classes of utility functions, equilibria are captured by convex programs. We develop simple first-order methods that are suitable for solving these programs for large-scale markets. We focus on three practically-relevant utility classes: linear, quasilinear, and Leontief utilities. Using structural properties of a market equilibrium under each utility class, we show that the corresponding convex programs can be reformulated as optimization of a structured smooth convex function over a polyhedral set, for which projected gradient achieves linear convergence. To do so, we utilize recent linear convergence results under weakened strong-convexity conditions, and further refine the relevant constants, both in general and for our specific setups. We then show that proximal gradient (a generalization of projected gradient) with a practical version of linesearch achieves linear convergence under the Proximal-PL condition. For quasilinear utilities, we show that Mirror Descent applied to a specific convex program achieves sublinear last-iterate convergence and recovers the Proportional Response dynamics, an elegant and efficient algorithm for computing market equilibrium under linear utilities. Numerical experiments show that proportional response is highly efficient for computing an approximate solution, while projected gradient with linesearch can be much faster when higher accuracy is required.', 'corpus_id': 219635890, 'score': 1}, {'doc_id': '224815797', 'title': 'A Minimax Theorem for Nonconcave-Nonconvex Games or: How I Learned to Stop Worrying about Mixed-Nash and Love Neural Nets', 'abstract': 'Adversarial training, a special case of multi-objective optimization, is an increasingly prevalent machine learning technique: some of its most notable applications include GAN-based generative modeling and self-play techniques in reinforcement learning which have been applied to complex games such as Go or Poker. In practice, a \\emph{single} pair of networks is typically trained in order to find an approximate equilibrium of a highly nonconcave-nonconvex adversarial problem. However, while a classic result in game theory states such an equilibrium exists in concave-convex games, there is no analogous guarantee if the payoff is nonconcave-nonconvex. Our main contribution is to provide an approximate minimax theorem for a large class of games where the players pick neural networks including WGAN, StarCraft II, and Blotto Game. Our findings rely on the fact that despite being nonconcave-nonconvex with respect to the neural networks parameters, these games are concave-convex with respect to the actual models (e.g., functions or distributions) represented by these neural networks.', 'corpus_id': 224815797, 'score': 1}, {'doc_id': '226281387', 'title': 'Stability of Gradient Learning Dynamics in Continuous Games: Scalar Action Spaces', 'abstract': 'Learning processes in games explain how players grapple with one another in seeking an equilibrium. We study a natural model of learning based on individual gradients in two- player continuous games. In such games, the arguably natural notion of a local equilibrium is a differential Nash equilibrium. However, the set of locally exponentially stable equilibria of the learning dynamics do not necessarily coincide with the set of differential Nash equilibria of the corresponding game. To characterize this gap, we provide formal guarantees for the stability or instability of such fixed points by leveraging the spectrum of the linearized game dynamics. We provide a comprehensive understanding of scalar games and find that equilibria that are both stable and Nash are robust to variations in learning rates.', 'corpus_id': 226281387, 'score': 1}, {'doc_id': '221516493', 'title': 'Centralized and Distributed Deep Reinforcement Learning Methods for Downlink Sum-Rate Optimization', 'abstract': 'For a multi-cell, multi-user, cellular network downlink sum-rate maximization through power allocation is a nonconvex and NP-hard optimization problem. In this article, we present an effective approach to solving this problem through single- and multi-agent actor-critic deep reinforcement learning (DRL). Specifically, we use finite-horizon trust region optimization. Through extensive simulations, we show that we can simultaneously achieve higher spectral efficiency than state-of-the-art optimization algorithms like weighted minimum mean-squared error (WMMSE) and fractional programming (FP), while offering execution times more than two orders of magnitude faster than these approaches. Additionally, the proposed trust region methods demonstrate superior performance and convergence properties than the Advantage Actor-Critic (A2C) DRL algorithm. In contrast to prior approaches, the proposed decentralized DRL approaches allow for distributed optimization with limited CSI and controllable information exchange between BSs while offering competitive performance and reduced training times.', 'corpus_id': 221516493, 'score': 0}, {'doc_id': '221995421', 'title': 'Zero Knowledge Games', 'abstract': 'Zero-knowledge strategies as a form of inference and reasoning operate using the concept of zero-knowledge signaling, such that any imperfect recall or incomplete information can be attenuated for. The resulting effect of structuring a continuous game within a zero-knowledge strategy demonstrates the ability to infer, within acceptable probabilities, which approximate stage a player is in. This occurs only when an uninformed player attempts non-revealing strategies, resulting in a higher probability of failing to appear informed. Thus, an opposing player understanding their opponent is uninformed can choose a more optimal strategy. In cases where an informed player chooses a non-revealing strategy, introducing a hedge algebra as a doxastic heuristic informs feasibility levels of trust. A counter strategy employing such a hedge algebra facilitates optimal outcomes for both players, provided the trust is well placed. Given indefinite, finite sub-games leading to continued interactions based on trust, extensions to continuous games are feasible.', 'corpus_id': 221995421, 'score': 0}]"
182	{'doc_id': '868607', 'title': 'HUMAN AUDITORY PERCEPTION OF PULSED RADIOFREQUENCY ENERGY', 'abstract': 'Human auditory perception of pulses of radiofrequency (RF) energy is a well-established phenomenon that is dependent upon the energy in a single pulse and not on average power density. RF-induced sounds can be characterized as the perception of subtle sounds because, in general, a quiet environment is required for the sounds to be heard. The sound is similar to other common sounds such as a click, buzz, hiss, knock or chirp. Effective radiofrequencies range from 216 to 10,000 MHz, but an individual’s ability to hear RF-induced sounds is dependent upon high-frequency acoustic hearing in the kHz range. The fundamental frequency of RFinduced sounds is independent of the radiofrequency but dependent upon head dimensions. The detection of RF-induced sounds is similar to acoustic sound detection once the cochlea is stimulated; however, the site of conversion of RF energy to acoustic energy is peripheral to the cochlea. The thermoelastic expansion theory explains the RF hearing phenomenon. RF-induced sounds involve the perception, via bone conduction, of thermally generated sound transients, that is, audible sounds are produced by rapid thermal expansion resulting from only a 5 x 10 C temperature rise in tissue at the threshold level due to absorption of the energy in the RF pulse. The experimental weigh-of-evidence excludes direct stimulation of the central nervous system by RF pulses. The perception of RF-induced sounds near the threshold exposure level is considered to be a biological effect without an accompanying health effect. This conclusion is supported by a comparison of pressures induced in the body by RF pulses and by clinical ultrasound procedures.', 'corpus_id': 868607}	17640	"[{'doc_id': '234789579', 'title': 'Health Risks from Exposure to Electromagnetic Waves Radiation from 5G', 'abstract': 'This work shows a compilation of the results of different studies related with the affectations in living organisms produced by electromagnetic waves radiation based on their power and frequency ranges. Currently, the growing of the population throughout the planet makes necessary an analysis of the nature of these electromagnetic waves and their effects on living beings, specially in wireless communications field. The information to be transmitted requires that the next generation of mobile telephony (5G) uses bands with frequencies higher than those used by the current generation and previous generations for its operation; for this reason, it is necessary to establish frequency ranges that could be considered non-harmful for living beings. In the present work, a detailed study was done about electromagnetic waves, including the frequency bands that were previously and currently used in mobile telephony, and some of their effects on living organisms, with the aim of publicizing some of the possible consequences of the evolution of mobile telephony on', 'corpus_id': 234789579, 'score': 1}, {'doc_id': '235321316', 'title': 'Superhuman spatial hearing technology for ultrasonic frequencies', 'abstract': 'Ultrasonic sources are inaudible to humans, and while digital signal processing techniques are available to bring ultrasonic signals into the audible range, there are currently no systems which also simultaneously permit the listener to localise the sources through spatial hearing. Therefore, we describe a method whereby an in-situ listener with normal binaural hearing can localise ultrasonic sources in real-time; opening-up new applications, such as the monitoring of certain forms of wild life in their habitats and man-made systems. In this work, an array of ultrasonic microphones is mounted to headphones, and the spatial parameters of the ultrasonic sound-field are extracted. A pitch-shifted signal is then rendered to the headphones with spatial properties dictated by the estimated parameters. The processing provides the listener with the spatial cues that would normally occur if the acoustic wave produced by the source were to arrive at the listener having already been pitch-shifted. The results show that the localisation accuracy delivered by the proof-of-concept device implemented here is almost as good as with audible sources, as tested both in the laboratory and under conditions in the field.', 'corpus_id': 235321316, 'score': 0}, {'doc_id': '234484963', 'title': 'Assessing Agreement between Frequency-Specific Chirp Auditory Steady-State Response and Pure Tone Audiometry in Adults by Intraclass Correlation Coefficient.', 'abstract': 'INTRODUCTION\nChirp auditory steady-state response (ASSR) can be used to assess frequency-specific hearing thresholds. However, its reliability has not been confirmed yet. The purpose of this proposed study is to analyze the agreement of thresholds measured by chirp-ASSR and pure tone audiometry (PTA) to investigate the value of chirp-ASSR in hearing threshold evaluation.\n\n\nMETHODS\nParticipants with normal hearing (age: 18-66, 108 ears) and patients with sensorineural hearing loss (age: 22-82, 75 ears) were tested using PTA and chirp-ASSR at 0.5, 1, 2, and 4 kHz, respectively. Intraclass correlation coefficient (ICC) and Bland-Altman plot were introduced to analyze the agreement between the 2 methods.\n\n\nRESULTS\nOne-hundred eight participants underwent both chirp-ASSR and PTA to estimate their thresholds. The ICCs yielded by these 2 methods are 0.757, 0.893, 0.883, and 0.921 (p < 0.001) at 0.5, 1, 2, and 4 kHz carrier frequency, respectively. However, there is a significant difference between the 2 methods at 2 kHz: the mean value of the ASSR thresholds is 5.27 dB HL higher than the value of PTA thresholds. Additionally, the 95% limits of agreement range from -27.48 to 26.66 dB HL at 0.5 kHz, from -18.19 to 17.87 dB HL at 1 kHz, from -12.01 to 22.55 dB HL at 2 kHz, and from -21.29 to 19.17 dB HL at 4 kHz, which are large enough to affect clinical decision-making.\n\n\nCONCLUSION\nIn this study, we have confirmed good to excellent correlation between chirp-ASSR and PTA in threshold estimation for adults with and without hearing loss. The degree of correlations is higher for participants with hearing loss and for measurements at high frequencies. However, significant systematic difference and large limits of agreement between the 2 methods have been found. These findings show that chirp-ASSR can be treated as a supplementary method to PTA when evaluating the hearing level, but the 2 methods are not interchangeable due to their systematic difference and large limits of agreement.', 'corpus_id': 234484963, 'score': 0}, {'doc_id': '235908166', 'title': 'Time-temperature Thresholds and Safety Factors for Thermal Hazards from Radiofrequency Energy above 6 GHz', 'abstract': 'Abstract Two major sets of exposure limits for radiofrequency (RF) radiation, those of the International Commission on Nonionizing Radiation Protection (ICNIRP 2020) and the Institute of Electrical and Electronics Engineers (IEEE C95.1–2019), have recently been revised and updated with significant changes in limits above 6 GHz through the millimeter wave (mm-wave) band (30–300 GHz). This review compares available data on thermal damage and pain from exposure to RF energy above 6 GHz with corresponding data from infrared energy and other heat sources and estimates safety factors that are incorporated in the IEEE and ICNIRP RF exposure limits. The benchmarks for damage are the same as used in ICNIRP IR limits: minimal epithelial damage to cornea and first-degree burn (erythema in skin observable within 48 h after exposure). The data suggest that limiting thermal hazard to skin is cutaneous pain for exposure durations less than ≈20 min and thermal damage for longer exposures. Limitations on available data and thermal models are noted. However, data on RF and IR thermal damage and pain thresholds show that exposures far above current ICNIRP and IEEE limits would be required to produce thermally hazardous effects. This review focuses exclusively on thermal hazards from RF exposures above 6 GHz to skin and the cornea, which are the most exposed tissues in the considered frequency range.', 'corpus_id': 235908166, 'score': 1}, {'doc_id': '237552337', 'title': 'Health Hazardous of Specific Absorption Rate ( SAR ) of Mobile Phone Tower Waves Mushtaq', 'abstract': 'The physiological mechanism of mobile phone radiation related health effect is not well known. Mobile phone and their tower radiations affect human skin and blood. Those People who often talk on mobile phone handset or living near their tower have higher interaction with radiation. In this paper penetration of high frequency electromagnetic waves emitted from mobile phone tower into human skin and blood tissues was studied. The effect of specific absorption rate (SAR) was calculated at frequencies 800, 900, 1800 and 2450 MHz and effective radiated power from the mobile phone tower is taken as 20 Watts.', 'corpus_id': 237552337, 'score': 0}, {'doc_id': '233453744', 'title': 'A new computational approach on signal propagation inside the cochlea', 'abstract': 'Signal propagation inside the cochlea is analyzed from an approximate point of view. This concept has been established by enabling assumptions regarding the feasibility of modeling experiments on the structure of the cochlea in line with the expected results of mathematical models. This article sheds light on reviewing the hearing system from a mathematical, mechanical, electrical, and chemical point of view. This is not to determine which model is better, but to analyze the advancement of the work on cochlear modeling. This paper attempts to present which side of the ear and how the Fourier transformation actually performs its function (seperating complex waves at many frequencies of the sine waves). It provides several improved observations that integrate the effects of the observed studies in such a way that certain characteristics are captured in different types of liner & non-linear modeling in Cochlea, which is considered to be a frequency analyzer existing in the inner ear. Our proposed approach has been developed following advanced computational language-python.', 'corpus_id': 233453744, 'score': 0}, {'doc_id': '235219701', 'title': 'Millimeter (MM) wave and microwave frequency radiation produce deeply penetrating effects: the biology and the physics', 'abstract': 'Abstract Millimeter wave (MM-wave) electromagnetic fields (EMFs) are predicted to not produce penetrating effects in the body. The electric but not magnetic part of MM-EMFs are almost completely absorbed within the outer 1 mm of the body. Rodents are reported to have penetrating MM-wave impacts on the brain, the myocardium, liver, kidney and bone marrow. MM-waves produce electromagnetic sensitivity-like changes in rodent, frog and skate tissues. In humans, MM-waves have penetrating effects including impacts on the brain, producing EEG changes and other neurological/neuropsychiatric changes, increases in apparent electromagnetic hypersensitivity and produce changes on ulcers and cardiac activity. This review focuses on several issues required to understand penetrating effects of MM-waves and microwaves: 1. Electronically generated EMFs are coherent, producing much higher electrical and magnetic forces then do natural incoherent EMFs. 2. The fixed relationship between electrical and magnetic fields found in EMFs in a vacuum or highly permeable medium such as air, predicted by Maxwell’s equations, breaks down in other materials. Specifically, MM-wave electrical fields are almost completely absorbed in the outer 1 mm of the body due to the high dielectric constant of biological aqueous phases. However, the magnetic fields are very highly penetrating. 3. Time-varying magnetic fields have central roles in producing highly penetrating effects. The primary mechanism of EMF action is voltage-gated calcium channel (VGCC) activation with the EMFs acting via their forces on the voltage sensor, rather than by depolarization of the plasma membrane. Two distinct mechanisms, an indirect and a direct mechanism, are consistent with and predicted by the physics, to explain penetrating MM-wave VGCC activation via the voltage sensor. Time-varying coherent magnetic fields, as predicted by the Maxwell–Faraday version of Faraday’s law of induction, can put forces on ions dissolved in aqueous phases deep within the body, regenerating coherent electric fields which activate the VGCC voltage sensor. In addition, time-varying magnetic fields can directly put forces on the 20 charges in the VGCC voltage sensor. There are three very important findings here which are rarely recognized in the EMF scientific literature: coherence of electronically generated EMFs; the key role of time-varying magnetic fields in generating highly penetrating effects; the key role of both modulating and pure EMF pulses in greatly increasing very short term high level time-variation of magnetic and electric fields. It is probable that genuine safety guidelines must keep nanosecond timescale-variation of coherent electric and magnetic fields below some maximum level in order to produce genuine safety. These findings have important implications with regard to 5G radiation.', 'corpus_id': 235219701, 'score': 1}, {'doc_id': '233281520', 'title': 'Tinnitus: A Tingling Mystery to be Decrypted', 'abstract': 'Tinnitus is a hearing disorder that causes ringing, buzzing or hissing sensation to the patient’s auditory senses. It has become a very common complaint over the years affecting around 7-8% of the human population all over the world. The disorder causes the patients to feel irritable, annoyed, depressed, and distressed. As a result, it obstructs their sense of relaxation, enjoyment, and even their sleep thus forcing them to avoid any social gatherings. There has been a substantial amount of work that has been carried out pertinent to this disorder. This paper reviews existing research and work done regarding Tinnitus effects, causes, and diagnosis. The numerous ways in which Tinnitus could affect an individual have been depicted. From the plethora of probable causes of this disorder, the most conceivable ones are highlighted. Moreover, this paper documents and reviews the attempts at treating Tinnitus, relevant engineering breakthroughs, and the various ways in which Tinnitus noise is suppressed – such as Tinnitus Retraining Therapy, Neuromodulation, and Signal processing approach. The manuscripts highlight the pros and cons of these methods. Over 45 research articles and other reliable internet medical sources were reviewed and these pieces of work were contrasted. These findings should help in understanding both – the disorder, as well as the situation of the patients suffering from it. Through this manuscript, an attempt was made to spread awareness about the mysterious disorder.', 'corpus_id': 233281520, 'score': 0}, {'doc_id': '234340178', 'title': 'Is infrasound perceived by the auditory system through distortions', 'abstract': 'Several studies showed that the human auditory system is sensitive to infrasound. The present study investigated if non-linear processes within the ear generate distortion products, which may explain the auditory sensitivity to infrasound. Pure-tone infrasound stimuli were presented to the ears of 16 listeners with a low-distortion sound reproduction system via an ear insert. Simultaneously, the sound in the ear canal was recorded with a high-sensitivity probe microphone. In addition, detection thresholds in quiet for these stimuli were determined in all listeners. All of the listeners showed distortion products, which were detected by analyzing the sound recorded in their ear canals during infrasound stimulation, for at least one of the signal frequencies. The sound pressure levels of the distortion products were well below the reference thresholds at the corresponding frequencies. For each signal frequency, the listeners’ detection thresholds in cases with ear-generated distortion products did not differ significantly from those in cases without. Thus, the present data do not support the hypothesis that distortion products play a major role in the auditory perception of infrasound.', 'corpus_id': 234340178, 'score': 1}, {'doc_id': '235073892', 'title': 'Low-Level Radiofrequency Exposure Induces Vasoconstriction in Rats.', 'abstract': ""Recent studies have revealed that rodents' physiological responses to low-intensity radiofrequency (RF) electromagnetic fields\xa0were similar to thermoregulatory responses to cold conditions. The primary autonomic response to cold exposure is peripheral vasoconstriction that allows rodents to reduce heat loss and maintain a relatively constant internal body temperature. In the present study, we investigated the effects of 900\u2009MHz RF at a low level (SAR of 0.35\u2009W/kg) on tail skin temperature (Ttail ) in rats. We showed that rats exposed to RF had lower Ttail than control rats at ambient temperatures between 27 and 28 °C, suggesting that RF could induce a noticeable degree of vasoconstriction under mild-warm ambient temperatures. This difference in Ttail was suppressed after the intraperitoneal injection of a vasodilator, an α-adrenergic antagonist, confirming the hypothesis of the vasoconstriction in exposed rats. Moreover, like a response to cold stimuli, RF exposure led to increased plasma concentrations of important factors: noradrenaline (a neurotransmitter responsible for vasoconstriction and thermogenesis)\xa0and fatty acids (markers of activated thermogenesis). Taken together, these findings indicate that low-intensity RF levels triggered some key physiological events usually associated with responses to cold in rats. © 2021 Bioelectromagnetics Society."", 'corpus_id': 235073892, 'score': 1}]"
183	{'doc_id': '233296940', 'title': 'On the Faithfulness Measurements for Model Interpretations', 'abstract': 'Recent years have witnessed the emergence of a variety of post-hoc interpretations that aim to uncover how natural language processing (NLP) models make predictions. Despite the surge of new interpretations, it remains an open problem how to define and quantitatively measure the faithfulness of interpretations, i.e., to what extent they conform to the reasoning process behind the model. To tackle these issues, we start with three criteria: the removal-based criterion, the sensitivity of interpretations, and the stability of interpretations, that quantify different notions of faithfulness, and propose novel paradigms to systematically evaluate interpretations in NLP. Our results show that the performance of interpretations under different criteria of faithfulness could vary substantially. Motivated by the desideratum of these faithfulness notions, we introduce a new class of interpretation methods that adopt techniques from the adversarial robustness domain. Empirical results show that our proposed methods achieve top performance under all three criteria. Along with experiments and analysis on both the text classification and the dependency parsing tasks, we come to a more comprehensive understanding of the diverse set of interpretations.', 'corpus_id': 233296940}	7310	"[{'doc_id': '221739102', 'title': 'Are Interpretations Fairly Evaluated? A Definition Driven Pipeline for Post-Hoc Interpretability', 'abstract': 'Recent years have witnessed an increasing number of interpretation methods being developed for improving transparency of NLP models. Meanwhile, researchers also try to answer the question that whether the obtained interpretation is faithful in explaining mechanisms behind model prediction? Specifically, (Jain and Wallace, 2019) proposes that ""attention is not explanation"" by comparing attention interpretation with gradient alternatives. However, it raises a new question that can we safely pick one interpretation method as the ground-truth? If not, on what basis can we compare different interpretation methods? In this work, we propose that it is crucial to have a concrete definition of interpretation before we could evaluate faithfulness of an interpretation. The definition will affect both the algorithm to obtain interpretation and, more importantly, the metric used in evaluation. Through both theoretical and experimental analysis, we find that although interpretation methods perform differently under a certain evaluation metric, such a difference may not result from interpretation quality or faithfulness, but rather the inherent bias of the evaluation metric.', 'corpus_id': 221739102, 'score': 1}, {'doc_id': '216867610', 'title': 'How Do Decisions Emerge across Layers in Neural Models? Interpretation with Differentiable Masking', 'abstract': ""Attribution methods assess the contribution of inputs (e.g., words) to the model prediction. One way to do so is erasure: a subset of inputs is considered irrelevant if it can be removed without affecting the model prediction. Despite its conceptual simplicity, erasure is not commonly used in practice. First, the objective is generally intractable, and approximate search or leave-one-out estimates are typically used instead; both approximations may be inaccurate and remain very expensive with modern deep (e.g., BERT-based) NLP models. Second, the method is susceptible to the hindsight bias: the fact that a token can be dropped does not mean that the model `knows' it can be dropped. The resulting pruning is over-aggressive and does not reflect how the model arrives at the prediction. To deal with these two challenges, we introduce Differentiable Masking. DiffMask relies on learning sparse stochastic gates (i.e., masks) to completely mask-out subsets of the input while maintaining end-to-end differentiability. The decision to include or disregard an input token is made with a simple linear model based on intermediate hidden layers of the analyzed model. First, this makes the approach efficient at test time because we predict rather than search. Second, as with probing classifiers, this reveals what the network `knows' at the corresponding layers. This lets us not only plot attribution heatmaps but also analyze how decisions are formed across network layers. We use DiffMask to study BERT models on sentiment classification and question answering."", 'corpus_id': 216867610, 'score': 1}, {'doc_id': '19624082', 'title': 'Explaining Recurrent Neural Network Predictions in Sentiment Analysis', 'abstract': 'Recently, a technique called Layer-wise Relevance Propagation (LRP) was shown to deliver insightful explanations in the form of input space relevances for understanding feed-forward neural network classification decisions. In the present work, we extend the usage of LRP to recurrent neural networks. We propose a specific propagation rule applicable to multiplicative connections as they arise in recurrent network architectures such as LSTMs and GRUs. We apply our technique to a word-based bi-directional LSTM model on a five-class sentiment prediction task, and evaluate the resulting LRP relevances both qualitatively and quantitatively, obtaining better results than a gradient-based related method which was used in previous work.', 'corpus_id': 19624082, 'score': 1}, {'doc_id': '135466391', 'title': 'Evaluating Recurrent Neural Network Explanations', 'abstract': 'Recently, several methods have been proposed to explain the predictions of recurrent neural networks (RNNs), in particular of LSTMs. The goal of these methods is to understand the network’s decisions by assigning to each input variable, e.g., a word, a relevance indicating to which extent it contributed to a particular prediction. In previous works, some of these methods were not yet compared to one another, or were evaluated only qualitatively. We close this gap by systematically and quantitatively comparing these methods in different settings, namely (1) a toy arithmetic task which we use as a sanity check, (2) a five-class sentiment prediction of movie reviews, and besides (3) we explore the usefulness of word relevances to build sentence-level representations. Lastly, using the method that performed best in our experiments, we show how specific linguistic phenomena such as the negation in sentiment analysis reflect in terms of relevance patterns, and how the relevance visualization can help to understand the misclassification of individual samples.', 'corpus_id': 135466391, 'score': 1}, {'doc_id': '222133166', 'title': 'Learning from Context or Names? An Empirical Study on Neural Relation Extraction', 'abstract': 'Neural models have achieved remarkable success on relation extraction (RE) benchmarks. However, there is no clear understanding which type of information affects existing RE models to make decisions and how to further improve the performance of these models. To this end, we empirically study the effect of two main information sources in text: textual context and entity mentions (names). We find that (i) while context is the main source to support the predictions, RE models also heavily rely on the information from entity mentions, most of which is type information, and (ii) existing datasets may leak shallow heuristics via entity mentions and thus contribute to the high performance on RE benchmarks. Based on the analyses, we propose an entity-masked contrastive pre-training framework for RE to gain a deeper understanding on both textual context and type information while avoiding rote memorization of entities or use of superficial cues in mentions. We carry out extensive experiments to support our views, and show that our framework can improve the effectiveness and robustness of neural models in different RE scenarios. All the code and datasets are released at this https URL.', 'corpus_id': 222133166, 'score': 0}, {'doc_id': '226226804', 'title': 'Reasoning Over History: Context Aware Visual Dialog', 'abstract': 'While neural models have been shown to exhibit strong performance on single-turn visual question answering (VQA) tasks, extending VQA to a multi-turn, conversational setting remains a challenge. One way to address this challenge is to augment existing strong neural VQA models with the mechanisms that allow them to retain information from previous dialog turns. One strong VQA model is the MAC network, which decomposes a task into a series of attention-based reasoning steps. However, since the MAC network is designed for single-turn question answering, it is not capable of referring to past dialog turns. More specifically, it struggles with tasks that require reasoning over the dialog history, particularly coreference resolution. We extend the MAC network architecture with Context-aware Attention and Memory (CAM), which attends over control states in past dialog turns to determine the necessary reasoning operations for the current question. MAC nets with CAM achieve up to 98.25% accuracy on the CLEVR-Dialog dataset, beating the existing state-of-the-art by 30% (absolute). Our error analysis indicates that with CAM, the model’s performance particularly improved on questions that required coreference resolution.', 'corpus_id': 226226804, 'score': 0}, {'doc_id': '91184042', 'title': 'PAWS: Paraphrase Adversaries from Word Scrambling', 'abstract': 'Existing paraphrase identification datasets lack sentence pairs that have high lexical overlap without being paraphrases. Models trained on such data fail to distinguish pairs like flights from New York to Florida and flights from Florida to New York. This paper introduces PAWS (Paraphrase Adversaries from Word Scrambling), a new dataset with 108,463 well-formed paraphrase and non-paraphrase pairs with high lexical overlap. Challenging pairs are generated by controlled word swapping and back translation, followed by fluency and paraphrase judgments by human raters. State-of-the-art models trained on existing datasets have dismal performance on PAWS (<40% accuracy); however, including PAWS training data for these models improves their accuracy to 85% while maintaining performance on existing tasks. In contrast, models that do not capture non-local contextual information fail even with PAWS training examples. As such, PAWS provides an effective instrument for driving further progress on models that better exploit structure, context, and pairwise comparisons.', 'corpus_id': 91184042, 'score': 1}, {'doc_id': '222090330', 'title': 'Understanding tables with intermediate pre-training', 'abstract': 'Table entailment, the binary classification task of finding if a sentence is supported or refuted by the content of a table, requires parsing language and table structure as well as numerical and discrete reasoning. While there is extensive work on textual entailment, table entailment is less well studied. We adapt TAPAS (Herzig et al., 2020), a table-based BERT model, to recognize entailment. Motivated by the benefits of data augmentation, we create a balanced dataset of millions of automatically created training examples which are learned in an intermediate step prior to fine-tuning. This new data is not only useful for table entailment, but also for SQA (Iyyer et al., 2017), a sequential table QA task. To be able to use long examples as input of BERT models, we evaluate table pruning techniques as a pre-processing step to drastically improve the training and prediction efficiency at a moderate drop in accuracy. The different methods set the new state-of-the-art on the TabFact (Chen et al., 2020) and SQA datasets.', 'corpus_id': 222090330, 'score': 0}, {'doc_id': '222091022', 'title': 'ISAAQ - Mastering Textbook Questions with Pre-trained Transformers and Bottom-Up and Top-Down Attention', 'abstract': 'Textbook Question Answering is a complex task in the intersection of Machine Comprehension and Visual Question Answering that requires reasoning with multimodal information from text and diagrams. For the first time, this paper taps on the potential of transformer language models and bottom-up and top-down attention to tackle the language and visual understanding challenges this task entails. Rather than training a language-visual transformer from scratch we rely on pre-trained transformers, fine-tuning and ensembling. We add bottom-up and top-down attention to identify regions of interest corresponding to diagram constituents and their relationships, improving the selection of relevant visual information for each question and answer options. Our system ISAAQ reports unprecedented success in all TQA question types, with accuracies of 81.36%, 71.11% and 55.12% on true/false, text-only and diagram multiple choice questions. ISAAQ also demonstrates its broad applicability, obtaining state-of-the-art results in other demanding datasets.', 'corpus_id': 222091022, 'score': 0}, {'doc_id': '226237434', 'title': 'MACE: Model Agnostic Concept Extractor for Explaining Image Classification Networks', 'abstract': ""Deep convolutional networks have been quite successful at various image classification tasks. The current methods to explain the predictions of a pre-trained model rely on gradient information, often resulting in saliency maps that focus on the foreground object as a whole. However, humans typically reason by dissecting an image and pointing out the presence of smaller concepts. The final output is often an aggregation of the presence or absence of these smaller concepts. In this work, we propose MACE: a Model Agnostic Concept Extractor, which can explain the working of a convolutional network through smaller concepts. The MACE framework dissects the feature maps generated by a convolution network for an image to extract concept based prototypical explanations. Further, it estimates the relevance of the extracted concepts to the pre-trained model's predictions, a critical aspect required for explaining the individual class predictions, missing in existing approaches. We validate our framework using VGG16 and ResNet50 CNN architectures, and on datasets like Animals With Attributes 2 (AWA2) and Places365. Our experiments demonstrate that the concepts extracted by the MACE framework increase the human interpretability of the explanations, and are faithful to the underlying pre-trained black-box model."", 'corpus_id': 226237434, 'score': 0}]"
184	"{'doc_id': '219412549', 'title': 'The Socio-Economics of Roman Storage', 'abstract': ""In a pre-industrial world, storage could make or break farmers and empires alike. How did it shape the Roman empire?\xa0The Socio-Economics of Roman Storage\xa0cuts across the scales of farmer and state to trace the practical and moral reverberations of storage from villas in Italy to silos in Gaul, and from houses in Pompeii to warehouses in Ostia. Following on from the material turn, an abstract notion of 'surplus' makes way for an emphasis on storage's material transformations (e.g. wine fermenting; grain degrading; assemblages forming), which actively shuffle social relations and economic possibilities, and are a sensitive indicator of changing mentalities. This archaeological study tackles key topics, including the moral resonance of agricultural storage; storage as both a shared and a contested concern during and after conquest; the geography of knowledge in domestic settings; the supply of the metropolis of Rome; and the question of how empires scale up. It will be of interest to scholars and students of Roman archaeology and history, as well as anthropologists who study the links between the scales of farmer and state."", 'corpus_id': 219412549}"	16980	"[{'doc_id': '234100749', 'title': 'Open-Range Cattle Grazing and the Spread of Farming in Neolithic Central Europe', 'abstract': 'Bogucki P. 2013. Open-Range Cattle Grazing and the Spread of Farming in Neolithic Central Europe. In S. Kadrow and P. Włodarczak (eds.), Environment and subsistence – forty years after Janusz Kruk’s „Settlement studies...” (= Studien zur Archäologie in Ostmitteleuropa / Studia nad Pradziejami Europy Środkowej 11). Rzeszów, Bonn: Mitel & Verlag Dr. Rudolf Habelt GmbH, 1–14. This essay advances the proposition that the earliest farmers in central Europe practiced a form of livestock management in which they allowed their cattle to range freely in the forests and to seek good grazing resources with minimal human assistance. Open-range grazing is often used by settlers introducing domestic livestock to new habitats. It provides a plausible alternative to models involving open pastures close to settlements or transhumance between settled areas and distant pastures.', 'corpus_id': 234100749, 'score': 1}, {'doc_id': '160093104', 'title': 'Roman Granaries and Store Buildings', 'abstract': None, 'corpus_id': 160093104, 'score': 1}, {'doc_id': '100784176', 'title': 'Structure and Function of Storage Pit, Polota, for Long-Term Storageof Sorghum-A Case Study of Storage Pit in Dirashe Special Worenda,Ethiopia', 'abstract': '“Storage pits” with bag-like, tubular or flask-like shapes had been utilized for storing grains and nuts all around the world until several centuries BC. Storage pits were, however, mostly replaced with aboveground storehouse once the cultivation of rice, wheat and barley, which were not suitable for underground storage, became widespread. Nevertheless, such storage pits are now still being used locally in some rural villages in Ethiopia and Sudan. Storage pits can prevent losses due to weather, mice, sparrows, fire, water, and theft. However, storage pits are highly humid inside, which leads fungal and bacterial proliferation. Stored grains tend to severely deteriorate within several months, which is often before the next harvest season comes. However, the local people in the Dirashe area in Southern Ethiopia state that underground storehouses called polota with a flask-like shape can store sorghum for a maximum of 20 years. This study investigates the location, structure and storage function of polota in order to understand the reason why polota is capable of long-term storage while such storage pits highly humid inside. First, soil samples were collected from the locations where polota were constructed. The x-ray fluorescence analysis was conducted on these samples to analyze their chemical compositions. Then, the rates of iron (g)/aluminum (g), aluminum (g)/titanium (g), silicon (mol)/aluminum (g) were calculated. The result indicates that polota were built in the areas where basalt layers were chemically weathered. Based on the actual measurement of polota, all the polota are shaped like a flask, about 1.5 m in diameter and about 2 m in depth. The chemically weathered basalt makes it easier to work on while maintaining its dense composition, thus it can be easily formed into a flask-like shape. Also, the airtight characteristic can maintain the temperature and humidity inside stable. The measurement of hygro-thermal properties performed inside polota in which sorghum was stored shows the stable temperature at 31 Celsius and relative humidity at 92%. A low concentration of oxygen (O2), 2.7%, and high concentration of carbon dioxide (CO2), 1,60,000 ppm, were also measured inside polota. While polota is as high in humidity as other storage pits, thus not suitable for storage, it now revealed that a low O2 concentration prevents propagation of noxious insects and a high CO2 concentration induces a state of quiescence to the stored grains, inhibiting deterioration and enabling long-term storage.', 'corpus_id': 100784176, 'score': 1}, {'doc_id': '230539638', 'title': 'Archaeological Pitfalls of Storage', 'abstract': 'A number of researchers have argued that storage inherently generates surpluses and plays a critical role in the development of socioeconomic inequalities. This has led to archaeological attempts to track and quantify prehistoric storage practices. However, the models used have often been narrowly focused on traditional ecological assumptions, and the evidence used in interpretations has generally been in the form of pit storage or masonry structures. I contend that these approaches have produced flawed reconstructions of past storage and surplus conditions due to the widespread occurrence of storage techniques that leave little archaeological evidence. Indirect ways of monitoring surpluses and storage are probably far more accurate and reliable ways of dealing with these issues. I provide an example from the Near East of how this can be done by estimating the caloric requirements of feasts and prestige items. On the other hand, pit storage and the more archaeologically visible forms of storage can be used to address other interesting questions about past societies involving levels of security, inequalities, and exploitation.', 'corpus_id': 230539638, 'score': 1}, {'doc_id': '233835142', 'title': 'In praise of social distance in public spaces', 'abstract': ""In the wake of the novel coronavirus pandemic, many have voiced the concern that social distancing in public spaces could negate the ability to connect with others and thereby diminish the sense of social belonging The reason why many find social distancing so difficult, and so off-putting is that it runs counter to so much of what makes life, especially city life, worthwhile Here, Melcher argues that social distance does not threaten people's ability to connect socially;it is a critical requirement for social interactions within public spaces Public spaces work well precisely because they allow for social distance, not because they overcome or bridge it The most important connections that public spaces provide are those of a public and social nature rather than those of an intimate or personal nature"", 'corpus_id': 233835142, 'score': 0}, {'doc_id': '232186163', 'title': 'Value and politics: introduction to the special issue', 'abstract': 'The papers in this special section address the relation between value and politics ethnographically, across varied locations and spaces. In doing so, they necessarily address the complexity of the concept of value. Our title here is perhaps tautological because value is always about politics. One might argue that “value and politics” is just another way of getting at political economy, for flows of value provide the deeper structure in relation to which classes exist. Writers of introductions to collections about the anthropology of value struggle to clearly articulate common themes and to define what exactly is meant by value/s and its/their associated processes (cf. Otto and Willerslev 2013). Likewise, for the papers collected here, we cannot offer a common language that glosses the diverse articulations of value. We think this is because the anthropological theory of value, in its best version, offers a holistic framework capable of bringing together the integrated character inherent in the social division of labor. It goes to the core of social relationships which keep people entangled in a given social formation. The specific cases call for highlighting one or another aspect of value and politics, employing the language most relevant to each case. In thinking about value, it is crucial to continually remind ourselves that value is always about the relations of power that are integral to social life and social reproduction; it describes who produces and who consumes in a certain equilibrium (not to be confused with fairness). Marx makes a distinction between the “real value relation” and “value” (Turner 2008). By “real value relations,” he means the proportional allocation of social labor through which a social group sustains itself in the appropriation and transformation of the environment and its members. Value points to how the relational character of such activity becomes represented and understood, and both materially and symbolically organized, in a given society. Power relations produce slippage from real value relations, such', 'corpus_id': 232186163, 'score': 0}, {'doc_id': '232273846', 'title': 'Agroecology and feminist economics : New values for new times', 'abstract': 'T he contemporary crises we now face stem from the overexploitation of nature for the benefit of individual profit. Industrial food is an important component of this model. The fallout of this is all too familiar: soil deterioration, biodiversity loss, deforestation, indigenous and other peoples’ rights violations, precarious rural livelihoods, unsafe working conditions, climate change, the double-edged sword of obesity and malnutrition and strong concentration of power. The capitalist, patriarchal and colonialist system has divided the world into those who have and those who have not, those whose voices are heard and those who are silenced. As a result, women, indigenous as well as black and brown people (among others) have been pushed aside for centuries. The COVID-19 outbreak amplifies, deepens and uncovers these pre-existing tragedies, inequalities and injustices. In many places, new ways of being in the world are being developed. It is high time that we listen to (and learn from) other ways of doing things, other cosmovisions, other ways of organising society, other values precisely those that have been silenced. The world needs new values and new leadership in these shifting times. This is a crucial moment; the decisions we make now could lead us down a path of destruction, but could equally send us on a path towards regeneration. This issue of Farming Matters brings to the forefront how perspectives such as intersectional feminism and indigenous cosmologies coupled with agroecology have been transforming our economy and society. These insights offer pertinent lessons for the pursuit of deeper, much needed transformation.', 'corpus_id': 232273846, 'score': 0}, {'doc_id': '234177585', 'title': 'Los depósitos y las áreas de almacenamiento en la urbe Chimú: Chan Chan, Trujillo, Perú', 'abstract': 'Las áreas de almacenamiento son recurrentes en todos los palacios de Chan Chan, siendo los depósitos las estructuras de mayor importancia en el acúmulo de recursos. Mediante los resultados obtenidos en las investigaciones arqueológicas en los palacios Chayhuac, Uhle, Bandelier, Gran Chimú, Tschudi y Rivero, es posible conocer los diferentes tamaños y tipos de los depósitos. El tipo de almacenaje dependió de la naturaleza del objeto, usando o no recipientes para tal fin, pero posiblemente fue prioridad el almacenamiento de granos, como frejoles, maní y maíz, siendo este último parte fundamental de la dieta y la materia prima para la elaboración de la chicha. Los depósitos en la capital Chimú pudieron almacenar muchos objetos en su interior (el mayor tiene 27.09 m2 y 22.76 m3), es el palacio Uhle el que presente mayor capacidad como área de almacenamiento (1852.68 m2 y 1671.50 m3). Toda esta organización de recaudación estuvo organizada desde puntos de control denominados “audiencias” y su preponderancia dentro de los palacios estuvo asociada el acceso a los recursos durante la realización de las actividades políticas en la urbe, siendo posible reconocer las políticas administrativas que existió durante la construcción de los palacios Uhle y Bandelier.', 'corpus_id': 234177585, 'score': 1}, {'doc_id': '234708460', 'title': 'Test Excavations at Sites 41LK284 and 41LK294, FM 1042 at the Nueces River, Live Oak County, Texas', 'abstract': 'The planned extension of FM 1042 in Live Oak County includes a crossing at the Nueces River. Phase II archaeological testing of two sites (41LK284 and 41LK294) on the north and south terraces of the Nueces River was undertaken by TxDOT archaeologists, prior to construction, to determine eligibility for inclusion in the National Register of Historic Places(in accordance with 36 CFR Part 800) and State Landmark status. Portions of both sites are located within the right-of-way. Surveys conducted in 1988 and 1992 recorded a light scatter of mussel shell and chert flakes on the surface of both sites. Cultural debris found in the roadcut was reported at depths of up to 1 m at 41LK284 and up to .5 m at 41LK294. A possible Matamoros point fragment and an end-scraper were also recovered from the surface of 41LK294, suggesting a Late Prehistoric occupation. Results of testing indicate that the portions of both sites within the right-of-way are disturbed and do not meet the criteria for designation as a State Archaeological Landmark or for listing on the National Register of Historic Places.', 'corpus_id': 234708460, 'score': 0}, {'doc_id': '233225531', 'title': 'Finding Freedom: Exploratory Archaeological Investigations at the Free African American Site of Pandenarium (36ME253), 1854–1930s', 'abstract': ""In 1854, 63 free African Americans arrived at Pandenarium (36ME253) in northwestern Pennsylvania. Archaeological investigations at the site bring to light the narrative of the freed African Americans who settled there, often countering local stories of its history. Supplementing archaeological methods of inquiry, earlier historical research with analysis of light-detection and ranging (LiDAR) data, aerial photography, and historical mapping developed a more robust context in which to understand better the people of Pandenarium. Collection of ground-penetrating radar (GPR) data, shovel test-pit excavation, and test-unit excavations uncovered the remains of multiple generations living in a place of changing meanings and spatial layout. Landscape and ceramic analyses highlighted the lives of African Americans living at Pandenarium, helping us to move beyond earlier narratives built on secondhand accounts inferring the inhabitants’ inadequacy and race as key factors in the dissolution of the community. Resumen En el año 1854, 63 afroamericanos libres llegaron a Pandenarium (36ME253) en el noroeste de Pensilvania. Las investigaciones arqueológicas en el sitio sacan a la luz la narrativa de los afroamericanos liberados que se establecieron allí, a menudo contrarrestando los cuentos locales de su historia. Al complementar los métodos arqueológicos de investigación, la investigación histórica anterior, combinada con el análisis de datos de detección y alcance de la luz (LiDAR, por sus siglas en inglés), fotografía aérea y mapeo histórico, se desarrolló un contexto más sólido para comprender mejor a la gente de Pandenarium. Con la recopilación de datos de radar de penetración de suelos (GPR, por sus siglas en inglés), la excavación de pozos de prueba con pala y las excavaciones de unidades de prueba, se descubrieron los restos de varias generaciones que vivían en un lugar de disposición espacial y significados cambiantes. Los análisis de paisajes y cerámica revelaron más sobre las vidas de los afroamericanos que vivían en Pandenarium, ayudándonos a ir más allá de las narrativas anteriores construidas sobre relatos de segunda mano que infieren la insuficiencia y la raza de los habitantes como factores clave en la disolución de la comunidad. Résumé En 1854, 63 Africains-américains libres arrivèrent à Pandenarium (36ME253) dans le Nord-Ouest de la Pennsylvanie. Les fouilles archéologiques sur le site mettent en lumière le récit des Africains-américains libérés qui se sont installés ici, venant souvent contredire les anecdotes locales sur leur histoire. Les méthodes archéologiques complémentaires d'enquête, la recherche historique antérieure, en combinaison avec l'analyse des données de détection de la lumière et de mesure à distance (LiDAR, light-detection and ranging) et la photographie aérienne, ont conduit au développement d'un contexte plus solide dans lequel on peut mieux comprendre la population de Pandenarium. La collecte de données géoradar (GPR, ground-penetrating radar), le sondage à la pelle et les fouilles de périmètre-test ont mis à jour les vestiges de générations nombreuses ayant vécu sur un site dont les significations et l'aménagement de l'espace montrent une évolution. Les analyses de terrain et de céramiques ont mis en lumière l'existence des Africains-américains ayant vécu à Pandenarium, nous permettant ainsi de passer outre les narrations précédentes façonnées à partir de récits de seconde main suggérant que les insuffisances et la race des habitants sont des facteurs clés dans la dissolution de la communauté."", 'corpus_id': 233225531, 'score': 0}]"
185	{'doc_id': '233582108', 'title': 'Classification using semantic feature and machine learning: Land-use case application', 'abstract': 'Land cover classification has interested recent works especially for deforestation, urban are monitoring and agricultural land use. Traditional classification approaches have limited accuracy especially for non-heterogeneous land cover. Thus using machine may improve the classification accuracy. The presented paper deals with the land-use scene recognition on very high-resolution remote sensing imagery. We proposed a new framework based on semantic features, handcrafted features and machine learning classifiers decisions. The method starts by semantic feature extraction using a convolutional neural network. Handcraft features are also extracted based on color and multi-resolution characteristics. Then, the classification stage is processed by three learning machine algorithms. The final classification result performed by majority vote algorithm. The idea behind is to take advantages from semantic features and handcrafted features. The second scope is to use the decision fusion to enhance the classification result. Experimentation results show that the proposed method provides good accuracy and trustable tool for land use image identification.', 'corpus_id': 233582108}	11687	"[{'doc_id': '235681697', 'title': 'Onion Crop Monitoring with Multispectral Imagery using Deep Neural Network', 'abstract': 'The world’s growing population leads the government of Pakistan to increase the supply of food for the coming years in a well-organized manner. Feasible agriculture plays a vital role for sustain food production and preserves the environment from any unnecessary chemicals by the use of technology for good management. This research presents the design and development of a multi-spectral imaging system for precision agriculture tasks. This imaging system includes an RGB camera and Pi NoIR camera controlled by a raspberry pi in a drone. The images are captured by Unmanned Aerial Vehicle (UAV) and then send images to the Java application. Images are processed to sharp, resize by application. The Normalized Difference Vegetation Index (NDVI) is calculated to determine the crop health status based on real-time data. The Deep Learning (DL) technique is used to recognize the onion crop growth stage using the captured dataset. We express how to implement a progressive model for the deep neural network to recognize the onion crop growth stage. The performance accuracy of the system for batch size 16 is 96.10% and for batch size 32 is 93.80%. Keywords—UAV; deep neural networks; onion crop; NDVI; crop monitoring; VGG16', 'corpus_id': 235681697, 'score': 1}, {'doc_id': '233393745', 'title': 'ASPCNet: A Deep Adaptive Spatial Pattern Capsule Network for Hyperspectral Image Classification', 'abstract': 'Previous studies have shown the great potential of capsule networks for the spatial contextual feature extraction from hyperspectral images (HSIs). However, the sampling locations of the convolutional kernels of capsules are fixed and cannot be adaptively changed according to the inconsistent semantic information of HSIs. Based on this observation, this paper proposes an adaptive spatial pattern capsule network (ASPCNet) architecture by developing an adaptive spatial pattern (ASP) unit, that can rotate the sampling location of convolutional kernels on the basis of an enlarged receptive field. Note that this unit can learn more discriminative representations of HSIs with fewer parameters. Specifically, two cascaded ASP-based convolution operations (ASPConvs) are applied to input images to learn relatively high-level semantic features, transmitting hierarchical structures among capsules more accurately than the use of the most fundamental features. Furthermore, the semantic features are fed into ASP-based conv-capsule operations (ASPCaps) to explore the shapes of objects among the capsules in an adaptive manner, further exploring the potential of capsule networks. Finally, the class labels of image patches centered on test samples can be determined according to the fully connected capsule layer. Experiments on three public datasets demonstrate that ASPCNet can yield competitive performance with higher accuracies than state-of-the-art methods.', 'corpus_id': 233393745, 'score': 0}, {'doc_id': '235308101', 'title': 'Learn to Optimize Panchromatic Imagery for Accurate Building Extraction', 'abstract': 'Due to the limited training data, current data-driven algorithms, including deep convolutional networks (DCNs), are susceptible to training data that cannot be applied to new data directly. Unlike existing methods that are trying to improve model generation capability using limited data, we introduce a learning-based image translation method to generate data that share the same characteristics of target data. The low-resolution panchromatic satellite images are converted into high-resolution color images through interpolation and colorization with the proposed symmetric colorization network (SCN). Experiments on a very-high-resolution (VHR) dataset show that images generated by our SCN are with both quantitatively and qualitatively high color fidelity. Furthermore, we also demonstrate that high extraction accuracy is retained during the model transferring from aerial to satellite images. For pre-trained feature pyramid network (FPN), compared to the performance on raw panchromatic images, the interpolated and colorized images increase 305.7% of recall (0.929 vs. 0.229), 78.2% of overall accuracy (0.768 vs. 0.431), 132.5% of f1-score (0.851 vs. 0.366), and 230.8% of Jaccard index (0.741 vs. 0.224), respectively.', 'corpus_id': 235308101, 'score': 0}, {'doc_id': '234499407', 'title': 'Attention-Guided Label Refinement Network for Semantic Segmentation of Very High Resolution Aerial Orthoimages', 'abstract': 'The recent applications of fully convolutional networks (FCNs) have shown to improve the semantic segmentation of very high resolution (VHR) remote-sensing images because of the excellent feature representation and end-to-end pixel labeling capabilities. While many FCN-based methods concatenate features from multilevel encoding stages to refine the coarse labeling results, the semantic gap between features of different levels and the selection of representative features are often overlooked, leading to the generation of redundant information and unexpected classification results. In this article, we propose an attention-guided label refinement network (ALRNet) for improved semantic labeling of VHR images. ALRNet follows the paradigm of the encoder–decoder architecture, which progressively refines the coarse labeling maps of different scales by using the channelwise attention mechanism. A novel attention-guided feature fusion module based on the squeeze-and-excitation module is designed to fuse higher level and lower level features. In this way, the semantic gaps among features of different levels are declined, and the category discrimination of each pixel in the lower level features is strengthened, which is helpful for subsequent label refinement. ALRNet is tested on three public datasets, including two ISRPS 2-D labeling datasets and the Wuhan University aerial building dataset. Results demonstrated that ALRNet had shown promising segmentation performance in comparison with state-of-the-art deep learning networks. The source code of ALRNet is made publicly available for further studies.', 'corpus_id': 234499407, 'score': 0}, {'doc_id': '233435454', 'title': 'Enhanced-Random-Feature-Subspace-Based Ensemble CNN for the Imbalanced Hyperspectral Image Classification', 'abstract': 'Hyperspectral image (HSI) classification often faces the problem of multiclass imbalance, which is considered to be one of the major challenges in the field of remote sensing. In recent years, deep learning has been successfully applied to the HSI classification, a convolutional neural network (CNN) is one of the most representative of them. However, it is difficult to effectively improve the accuracy of minority classes under the problem of multiclass imbalance. In addition, ensemble learning has been successfully applied to solve multiclass imbalance, such as random forest (RF) This article proposes a novel enhanced-random-feature-subspace-based ensemble CNN algorithm for the multiclass imbalanced problem. The main idea is to perform random oversampling of training samples and multiple data enhancements based on random feature subspace, and then, construct an ensemble learning model combining random feature selection and CNN to the HSI classification. Experimental results on three public hyperspectral datasets show that the performance of the proposed method is better than the traditional CNN, RF, and deep learning ensemble methods.', 'corpus_id': 233435454, 'score': 0}, {'doc_id': '207800213', 'title': 'Harmonization of Landsat and Sentinel 2 for Crop Monitoring in Drought Prone Areas: Case Studies of Ninh Thuan (Vietnam) and Bekaa (Lebanon)', 'abstract': ""Proper satellite-based crop monitoring applications at the farm-level often require near-daily imagery at medium to high spatial resolution. The synthesizing of ongoing satellite missions by ESA (Sentinel 2) and NASA (Landsat7/8) provides this unprecedented opportunity at a global scale; nonetheless, this is rarely implemented because these procedures are data demanding and computationally intensive. This study developed a complete stream processing in the Google Earth Engine cloud platform to generate harmonized surface reflectance images of Landsat7,8 and Sentinel 2 missions. The harmonized images were generated for two agriculture schemes in Bekaa (Lebanon) and Ninh Thuan (Vietnam) during the period 2018-2019. We evaluated the performance of several pre-processing steps needed for the harmonization including image co-registration, brdf correction, topographic correction, and band adjustment. This study found that the miss-registration between Landsat 8 and Sentinel 2 images, varied from 10 meters in Ninh Thuan, Vietnam to 32 meters in Bekaa, Lebanon, and if not treated, posed a great impact on the quality of the harmonized dataset. Analysis of a pair overlapped L8-S2 images over the Bekaa region showed that after the harmonization, all band-to-band spatial correlations were greatly improved from (0.57, 0.64, 0.67, 0.75, 0.76, 0.75, 0.79) to (0.87, 0.91, 0.92, 0.94, 0.97, 0.97, 0.96) in bands (blue, green, red, nir,swir1,swir2, ndvi) respectively. We demonstrated that dense observation of the harmonized dataset can be very helpful for characterizing cropland in highly dynamic areas. We detected unimodal, bimodal and trimodal shapes in the temporal NDVI patterns (likely cycles of paddy rice) in Ninh Thuan province only during the year 2018. We fitted the temporal signatures of the NDVI time series using harmonic (Fourier) analysis. Derived phase (angle from the starting point to the cycle's peak) and amplitude (the cycle's height) were combined with max-NDVI to generate an R-G-B image. This image highlighted croplands as colored pixels (high phase and amplitude) and other types of land as grey/dark pixels (low phase/amplitude). Generated harmonized datasets that contain surface reflectance images (bands blue, green, red, nir, swir1, swir2, and ndvi at 30 meters) over the two studied sites are provided for public usage and testing."", 'corpus_id': 207800213, 'score': 1}, {'doc_id': '235383648', 'title': 'Improving Chlorophyll-A Estimation From Sentinel-2 (MSI) in the Barents Sea Using Machine Learning', 'abstract': 'This article addresses methodologies for remote sensing of ocean Chlorophyll-a (Chl-a), with emphasis on the Barents Sea. We aim at improving the monitoring capacity by integrating in situ Chl-a observations and optical remote sensing to locally train machine learning (ML) models. For this purpose, in situ measurements of Chl-a ranging from 0.014–10.81 mg/m$^{3}$, collected for the years 2016–2018, were used to train and validate models. To accurately estimate Chl-a, we propose to use additional information on pigment content within the productive column by matching the depth-integrated Chl-a concentrations with the satellite data. Using the optical images captured by the multispectral imager instrument on Sentinel-2 and the in situ measurements, a new spatial window-based match-up dataset creation method is proposed to increase the number of match-ups and hence improve the training of the ML models. The match-ups are then filtered to eliminate erroneous samples based on the spectral distribution of the remotely sensed reflectance. In addition, we design and implement a neural network model dubbed as the ocean color net (OCN), that has performed better than existing ML-based techniques, including the Gaussian process Regression (GPR), regionally tuned empirical techniques, including the ocean color (OC3) algorithm and the spectral band ratios, as well as the globally trained Case-2 regional/coast colour (C2RCC) processing chain model C2RCC-networks. The proposed OCN model achieved reduced mean absolute error compared to the GPR by 5.2%, C2RCC by 51.7%, OC3 by 22.6%, and spectral band ratios by 29%. Moreover, the proposed spatial window and depth-integrated match-up creation techniques improved the performance of the proposed OCN by 57%, GPR by 41.9%, OC3 by 5.3%, and spectral band ratio method by 24% in terms of RMSE compared to the conventional match-up selection approach.', 'corpus_id': 235383648, 'score': 1}, {'doc_id': '235125956', 'title': 'Multimodal remote sensing benchmark datasets for land cover classification with a shared and specific feature learning model', 'abstract': 'As remote sensing (RS) data obtained from different sensors become available largely and openly, multimodal data processing and analysis techniques have been garnering increasing interest in the RS and geoscience community. However, due to the gap between different modalities in terms of imaging sensors, resolutions, and contents, embedding their complementary information into a consistent, compact, accurate, and discriminative representation, to a great extent, remains challenging. To this end, we propose a shared and specific feature learning (S2FL) model. S2FL is capable of decomposing multimodal RS data into modality-shared and modality-specific components, enabling the information blending of multi-modalities more effectively, particularly for heterogeneous data sources. Moreover, to better assess multimodal baselines and the newly-proposed S2FL model, three multimodal RS benchmark datasets, i.e., Houston2013 – hyperspectral and multispectral data, Berlin – hyperspectral and synthetic aperture radar (SAR) data, Augsburg – hyperspectral, SAR, and digital surface model (DSM) data, are released and used for land cover classification. Extensive experiments conducted on the three datasets demonstrate the superiority and advancement of our S2FL model in the task of land cover classification in comparison with previously-proposed state-of-the-art baselines. Furthermore, the baseline codes and datasets used in this paper will be made available freely at https://github.com/danfenghong/ISPRS_S2FL.', 'corpus_id': 235125956, 'score': 1}, {'doc_id': '234501501', 'title': 'Bilinear Multimodal Discriminator for Adversarial Domain Adaptation with Privileged Information', 'abstract': 'Over the past few years, deep Convolutional Neural Networks have shown outstanding performance on semantic segmentation, which is an essential tool needed by self-driving cars to understand their environments. However, their training relies on large datasets with pixel-level ground truth annotations, which are costly and tedious to produce on real data, making application to new situations difficult. In this context, Unsupervised Domain Adaptation (UDA) from synthetic data is an approach of great interest since it leverages cost-free labeled synthetic datasets to help generalizing to unlabeled real ones. In this paper, we propose a new adversarial training strategy for UDA that uses additional privileged information on the synthetic domain during training to improve transfer to the real one. Our method introduces a multimodal discriminator for adversarial training, featuring a bilinear fusion between representations of segmentation and privileged information to exploit at best alignment between modalities. We evaluate our approach on real-world Cityscapes dataset, using synthetic labeled data with depth as privileged information from SYNTHIA dataset and show competitive results.', 'corpus_id': 234501501, 'score': 0}, {'doc_id': '235387738', 'title': 'UAV image crop classification based on deep learning with spatial and spectral features', 'abstract': 'Unmanned Aerial Vehicle image analysis technology has become an effective means to classify crops. However, the UAV image classification mostly adopts shallow learning algorithm, with few computing units. These methods have low recognition accuracy and poor generalization ability. Deep learning has powerful function expression and excellent feature learning abilities. The learned features have more essential character for data and have achieved remarkable results in image classification. Therefore, the study proposes a crop classification method based on Unmanned Aerial Vehicle image with deep learning and spatial and spectral feature fusion. The method uses deep convolutional neural network to classify Unmanned Aerial Vehicle images. Simplified AlexNet network structure, which reduces the number of network layers, was used to accelerate the convergence speed of the model while ensuring the accuracy of crop classification in practical applications. Then, the vegetation index and height features of the Unmanned Aerial Vehicle image were extracted. Feature combination and comparative analyses were carried out to find the most effective feature combination and improve the accuracy of crop classification through spatial and spectral feature fusion. In addition, a Sample Expansion Strategy was used to optimize the classification model and further improve the classification result to achieve a perfect performance in the crop classification of drone images. The experimental results showed that the deep learning method can effectively identify crop types in Unmanned Aerial Vehicle images, and the overall classification accuracy is raised from 86.07% to 92.76% when combining spatial and spectral feature fusion with Sample Expansion Strategy.', 'corpus_id': 235387738, 'score': 1}]"
186	{'doc_id': '212657775', 'title': 'FlowFusion: Dynamic Dense RGB-D SLAM Based on Optical Flow', 'abstract': 'Dynamic environments are challenging for visual SLAM since the moving objects occlude the static environment features and lead to wrong camera motion estimation. In this paper, we present a novel dense RGB-D SLAM solution that simultaneously accomplishes the dynamic/static segmentation and camera ego-motion estimation as well as the static background reconstructions. Our novelty is using optical flow residuals to highlight the dynamic semantics in the RGB-D point clouds and provide more accurate and efficient dynamic/static segmentation for camera tracking and background reconstruction. The dense reconstruction results on public datasets and real dynamic scenes indicate that the proposed approach achieved accurate and efficient performances in both dynamic and static environments compared to state-of-the-art approaches.', 'corpus_id': 212657775}	2533	"[{'doc_id': '125951547', 'title': 'Batch Measurement Error Covariance Estimation for Robust Localization', 'abstract': None, 'corpus_id': 125951547, 'score': 1}, {'doc_id': '212657562', 'title': 'Decentralized Visual-Inertial-UWB Fusion for Relative State Estimation of Aerial Swarm', 'abstract': 'The collaboration of unmanned aerial vehicles (UAVs) has become a popular research topic for its practicability in multiple scenarios. The collaboration of multiple UAVs, which is also known as aerial swarm is a highly complex system, which still lacks a state-of-art decentralized relative state estimation method. In this paper, we present a novel fully decentralized visual-inertial-UWB fusion framework for relative state estimation and demonstrate the practicability by performing extensive aerial swarm flight experiments. The comparison result with ground truth data from the motion capture system shows the centimeter-level precision which outperforms all the Ultra-WideBand (UWB) and even vision based method. The system is not limited by the field of view (FoV) of the camera or Global Positioning System (GPS), meanwhile on account of its estimation consistency, we believe that the proposed relative state estimation framework has the potential to be prevalently adopted by aerial swarm applications in different scenarios in multiple scales.', 'corpus_id': 212657562, 'score': 0}, {'doc_id': '211296290', 'title': 'Denoising IMU Gyroscopes With Deep Learning for Open-Loop Attitude Estimation', 'abstract': 'This article proposes a learning method for denoising gyroscopes of Inertial Measurement Units (IMUs) using ground truth data, and estimating in real time the orientation (attitude) of a robot in dead reckoning. The obtained algorithm outperforms the state-of-the-art on the (unseen) test sequences. The obtained performances are achieved, thanks to a well-chosen model, a proper loss function for orientation increments, and through the identification of key points when training with high-frequency inertial data. Our approach builds upon a neural network based on dilated convolutions, without requiring any recurrent neural network. We demonstrate how efficient our strategy is for 3D attitude estimation on the EuRoC and TUM-VI datasets. Interestingly, we observe our dead reckoning algorithm manages to beat top-ranked visual-inertial odometry systems in terms of attitude estimation although it does not use vision sensors. We believe this article offers new perspectives for visual-inertial localization and constitutes a step toward more efficient learning methods involving IMUs. Our open-source implementation is available at https://github.com/mbrossar/denoise-imu-gyro.', 'corpus_id': 211296290, 'score': 0}, {'doc_id': '220487020', 'title': 'Camera-Lidar Integration: Probabilistic sensor fusion for semantic mapping', 'abstract': 'An automated vehicle operating in an urban environment must be able to perceive and recognise object/obstacles in a three-dimensional world while navigating in a constantly changing environment. In order to plan and execute accurate sophisticated driving maneuvers, a high-level contextual understanding of the surroundings is essential. Due to the recent progress in image processing, it is now possible to obtain high definition semantic information in 2D from monocular cameras, though cameras cannot reliably provide the highly accurate 3D information provided by lasers. The fusion of these two sensor modalities can overcome the shortcomings of each individual sensor, though there are a number of important challenges that need to be addressed in a probabilistic manner. In this paper, we address the common, yet challenging, lidar/camera/semantic fusion problems which are seldom approached in a wholly probabilistic manner. Our approach is capable of using a multi-sensor platform to build a three-dimensional semantic voxelized map that considers the uncertainty of all of the processes involved. We present a probabilistic pipeline that incorporates uncertainties from the sensor readings (cameras, lidar, IMU and wheel encoders), compensation for the motion of the vehicle, and heuristic label probabilities for the semantic images. We also present a novel and efficient viewpoint validation algorithm to check for occlusions from the camera frames. A probabilistic projection is performed from the camera images to the lidar point cloud. Each labelled lidar scan then feeds into an octree map building algorithm that updates the class probabilities of the map voxels every time a new observation is available. We validate our approach using a set of qualitative and quantitative experimental tests on the USyd Dataset.', 'corpus_id': 220487020, 'score': 1}, {'doc_id': '15533148', 'title': 'Switchable constraints vs. max-mixture models vs. RRR - A comparison of three approaches to robust pose graph SLAM', 'abstract': ""SLAM algorithms that can infer a correct map despite the presence of outliers have recently attracted increasing attention. In the context of SLAM, outlier constraints are typically caused by a failed place recognition due to perceptional aliasing. If not handled correctly, they can have catastrophic effects on the inferred map. Since robust robotic mapping and SLAM are among the key requirements for autonomous long-term operation, inference methods that can cope with such data association failures are a hot topic in current research. Our paper compares three very recently published approaches to robust pose graph SLAM, namely switchable constraints, max-mixture models and the RRR algorithm. All three methods were developed as extensions to existing factor graph-based SLAM back-ends and aim at improving the overall system's robustness to false positive loop closure constraints. Due to the novelty of the three proposed algorithms, no direct comparison has been conducted so far."", 'corpus_id': 15533148, 'score': 1}, {'doc_id': '237339873', 'title': 'Robust Tightly Coupled Pose Measurement Based on Multi-Sensor Fusion in Mobile Robot System', 'abstract': 'Currently, simultaneous localization and mapping (SLAM) is one of the main research topics in the robotics field. Visual-inertia SLAM, which consists of a camera and an inertial measurement unit (IMU), can significantly improve robustness and enable scale weak-visibility, whereas monocular visual SLAM is scale-invisible. For ground mobile robots, the introduction of a wheel speed sensor can solve the scale weak-visibility problem and improve robustness under abnormal conditions. In this paper, a multi-sensor fusion SLAM algorithm using monocular vision, inertia, and wheel speed measurements is proposed. The sensor measurements are combined in a tightly coupled manner, and a nonlinear optimization method is used to maximize the posterior probability to solve the optimal state estimation. Loop detection and back-end optimization are added to help reduce or even eliminate the cumulative error of the estimated poses, thus ensuring global consistency of the trajectory and map. The outstanding contribution of this paper is that the wheel odometer pre-integration algorithm, which combines the chassis speed and IMU angular speed, can avoid the repeated integration caused by linearization point changes during iterative optimization; state initialization based on the wheel odometer and IMU enables a quick and reliable calculation of the initial state values required by the state estimator in both stationary and moving states. Comparative experiments were conducted in room-scale scenes, building scale scenes, and visual loss scenarios. The results showed that the proposed algorithm is highly accurate—2.2 m of cumulative error after moving 812 m (0.28%, loopback optimization disabled)—robust, and has an effective localization capability even in the event of sensor loss, including visual loss. The accuracy and robustness of the proposed method are superior to those of monocular visual inertia SLAM and traditional wheel odometers.', 'corpus_id': 237339873, 'score': 1}, {'doc_id': '221340621', 'title': 'Multi-View Fusion of Sensor Data for Improved Perception and Prediction in Autonomous Driving', 'abstract': ""We present an end-to-end method for object detection and trajectory prediction utilizing multi-view representations of LiDAR returns. Our method builds on a state-of-the-art Bird's-Eye View (BEV) network that fuses voxelized features from a sequence of historical LiDAR data as well as rasterized high-definition map to perform detection and prediction tasks. We extend the BEV network with additional LiDAR Range-View (RV) features that use the raw LiDAR information in its native, non-quantized representation. The RV feature map is projected into BEV and fused with the BEV features computed from LiDAR and high-definition map. The fused features are then further processed to output the final detections and trajectories, within a single end-to-end trainable network. In addition, using this framework the RV fusion of LiDAR and camera is performed in a straightforward and computational efficient manner. The proposed approach improves the state-of-the-art on proprietary large-scale real-world data collected by a fleet of self-driving vehicles, as well as on the public nuScenes data set."", 'corpus_id': 221340621, 'score': 1}, {'doc_id': '210714109', 'title': 'Spatiotemporal Camera-LiDAR Calibration: A Targetless and Structureless Approach', 'abstract': 'The demand for multimodal sensing systems for robotics is growing due to the increase in robustness, reliability and accuracy offered by these systems. These systems also need to be spatially and temporally co-registered to be effective. In this letter, we propose a targetless and structureless spatiotemporal camera-LiDAR calibration method. Our method combines a closed-form solution with a modified structureless bundle adjustment where the coarse-to-fine approach does not require an initial guess on the spatiotemporal parameters. Also, as 3D features (structure) are calculated from triangulation only, there is no need to have a calibration target or to match 2D features with the 3D point cloud which provides flexibility in the calibration process and sensor configuration. We demonstrate the accuracy and robustness of the proposed method through both simulation and real data experiments using multiple sensor payload configurations mounted to hand-held, aerial and legged robot systems. Also, qualitative results are given in the form of a colorized point cloud visualization.', 'corpus_id': 210714109, 'score': 0}, {'doc_id': '212628577', 'title': 'DA4AD: End-to-end Deep Attention Aware Features Aided Visual Localization for Autonomous Driving', 'abstract': 'We present a visual localization framework aided by novel deep attention aware features for autonomous driving that achieves centimeter level localization accuracy. Conventional approaches to the visual localization problem rely on handcrafted features or human-made objects on the road. They are known to be either prone to unstable matching caused by severe appearance or lighting changes, or too scarce to deliver constant and robust localization results in challenging scenarios. In this work, we seek to exploit the deep attention mechanism to search for salient, distinctive and stable features that are good for longterm matching in the scene through a novel end-to-end deep neural network. Furthermore, our learned feature descriptors are demonstrated to be competent to establish robust matches and therefore successfully estimate the optimal camera poses with high precision. We comprehensively validate the effectiveness of our method using a freshly collected dataset with high-quality ground truth trajectories and hardware synchronization between sensors. Results demonstrate that our method achieves a competitive localization accuracy when compared to the LiDAR-based localization solutions under various challenging circumstances, leading to a potential low-cost localization solution for autonomous driving.', 'corpus_id': 212628577, 'score': 0}, {'doc_id': '220936387', 'title': 'GP-SLAM+: real-time 3D lidar SLAM based on improved regionalized Gaussian process map reconstruction', 'abstract': 'This paper presents a 3D lidar SLAM system based on improved regionalized Gaussian process (GP) map reconstruction to provide both low-drift state estimation and mapping in real-time for robotics applications. We utilize spatial GP regression to model the environment. This tool enables us to recover surfaces including those in sparsely scanned areas and obtain uniform samples with uncertainty. Those properties facilitate robust data association and map updating in our scan-to-map registration scheme, especially when working with sparse range data. Compared with previous GP-SLAM, this work overcomes the prohibitive computational complexity of GP and redesigns the registration strategy to meet the accuracy requirements in 3D scenarios. For large-scale tasks, a two-thread framework is employed to suppress the drift further. Aerial and ground-based experiments demonstrate that our method allows robust odometry and precise mapping in real-time. It also outperforms the state-of-the-art lidar SLAM systems in our tests with light-weight sensors.', 'corpus_id': 220936387, 'score': 1}]"
187	{'doc_id': '227228213', 'title': 'Improved Semantic Role Labeling using Parameterized Neighborhood Memory Adaptation', 'abstract': 'Deep neural models achieve some of the best results for semantic role labeling. Inspired by instance-based learning that utilizes nearest neighbors to handle low-frequency context-specific training samples, we investigate the use of memory adaptation techniques in deep neural models. We propose a parameterized neighborhood memory adaptive (PNMA) method that uses a parameterized representation of the nearest neighbors of tokens in a memory of activations and makes predictions based on the most similar samples in the training data. We empirically show that PNMA consistently improves the SRL performance of the base model irrespective of types of word embeddings. Coupled with contextualized word embeddings derived from BERT, PNMA improves over existing models for both span and dependency semantic parsing datasets, especially on out-of-domain text, reaching F1 scores of 80.2, and 84.97 on CoNLL2005, and CoNLL2009 datasets, respectively.', 'corpus_id': 227228213}	4011	[{'doc_id': '227208888', 'title': 'Learning Relation Prototype from Unlabeled Texts for Long-tail Relation Extraction', 'abstract': 'Relation Extraction (RE) is a vital step to complete Knowledge Graph (KG) by extracting entity relations from texts.However, it usually suffers from the long-tail issue. The training data mainly concentrates on a few types of relations, leading to the lackof sufficient annotations for the remaining types of relations. In this paper, we propose a general approach to learn relation prototypesfrom unlabeled texts, to facilitate the long-tail relation extraction by transferring knowledge from the relation types with sufficient trainingdata. We learn relation prototypes as an implicit factor between entities, which reflects the meanings of relations as well as theirproximities for transfer learning. Specifically, we construct a co-occurrence graph from texts, and capture both first-order andsecond-order entity proximities for embedding learning. Based on this, we further optimize the distance from entity pairs tocorresponding prototypes, which can be easily adapted to almost arbitrary RE frameworks. Thus, the learning of infrequent or evenunseen relation types will benefit from semantically proximate relations through pairs of entities and large-scale textual information.We have conducted extensive experiments on two publicly available datasets: New York Times and Google Distant Supervision.Compared with eight state-of-the-art baselines, our proposed model achieves significant improvements (4.1% F1 on average). Furtherresults on long-tail relations demonstrate the effectiveness of the learned relation prototypes. We further conduct an ablation study toinvestigate the impacts of varying components, and apply it to four basic relation extraction models to verify the generalization ability.Finally, we analyze several example cases to give intuitive impressions as qualitative analysis. Our codes will be released later.', 'corpus_id': 227208888, 'score': 0}, {'doc_id': '227210469', 'title': 'Relation Clustering in Narrative Knowledge Graphs', 'abstract': 'When coping with literary texts such as novels or short stories, the extraction of structured information in the form of a knowledge graph might be hindered by the huge number of possible relations between the entities corresponding to the characters in the novel and the consequent hurdles in gathering supervised information about them. Such issue is addressed here as an unsupervised task empowered by transformers: relational sentences in the original text are embedded (with SBERT) and clustered in order to merge together semantically similar relations. All the sentences in the same cluster are finally summarized (with BART) and a descriptive label extracted from the summary. Preliminary tests show that such clustering might successfully detect similar relations, and provide a valuable preprocessing for semi-supervised approaches.', 'corpus_id': 227210469, 'score': 1}, {'doc_id': '212628456', 'title': 'Parsing Thai Social Data: A New Challenge for Thai NLP', 'abstract': 'Dependency parsing (DP) is a task that analyzes text for syntactic structure and relationship between words. DP is widely used to improve natural language processing (NLP) applications in many languages such as English. Previous works on DP are generally applicable to formally written languages. However, they do not apply to informal languages such as the ones used in social networks. Therefore, DP has to be researched and explored with such social network data. In this paper, we explore and identify a DP model that is suitable for Thai social network data. After that, we will identify the appropriate linguistic unit as an input. The result showed that, the transition based model called, improve Elkared dependency parser outperform the others at UAS of 81.42%.', 'corpus_id': 212628456, 'score': 0}, {'doc_id': '227231182', 'title': 'Extreme Model Compression for On-device Natural Language Understanding', 'abstract': 'In this paper, we propose and experiment with techniques for extreme compression of neural natural language understanding (NLU) models, making them suitable for execution on resource-constrained devices. We propose a task-aware, end-to-end compression approach that performs word-embedding compression jointly with NLU task learning. We show our results on a large-scale, commercial NLU system trained on a varied set of intents with huge vocabulary sizes. Our approach outperforms a range of baselines and achieves a compression rate of 97.4% with less than 3.7% degradation in predictive performance. Our analysis indicates that the signal from the downstream task is important for effective compression with minimal degradation in performance.', 'corpus_id': 227231182, 'score': 1}, {'doc_id': '229153267', 'title': 'GDPNet: Refining Latent Multi-View Graph for Relation Extraction', 'abstract': 'Relation Extraction (RE) is to predict the relation type of two entities that are mentioned in a piece of text, e.g., a sentence or a dialogue. When the given text is long, it is challenging to identify indicative words for the relation prediction. Recent advances on RE task are from BERT-based sequence modeling and graph-based modeling of relationships among the tokens in the sequence. In this paper, we propose to construct a latent multi-view graph to capture various possible relationships among tokens. We then refine this graph to select important words for relation prediction. Finally, the representation of the refined graph and the BERT-based sequence representation are concatenated for relation extraction. Specifically, in our proposed GDPNet (Gaussian Dynamic Time Warping Pooling Net), we utilize Gaussian Graph Generator (GGG) to generate edges of the multi-view graph. The graph is then refined by Dynamic Time Warping Pooling (DTWPool). On DialogRE and TACRED, we show that GDPNet achieves the best performance on dialogue-level RE, and comparable performance with the state-of-the-arts on sentence-level RE.', 'corpus_id': 229153267, 'score': 1}, {'doc_id': '10077985', 'title': 'Automatically Tagging Constructions of Causation and Their Slot-Fillers', 'abstract': 'This paper explores extending shallow semantic parsing beyond lexical-unit triggers, using causal relations as a test case. Semantic parsing becomes difficult in the face of the wide variety of linguistic realizations that causation can take on. We therefore base our approach on the concept of constructions from the linguistic paradigm known as Construction Grammar (CxG). In CxG, a construction is a form/function pairing that can rely on arbitrary linguistic and semantic features. Rather than codifying all aspects of each construction’s form, as some attempts to employ CxG in NLP have done, we propose methods that offload that problem to machine learning. We describe two supervised approaches for tagging causal constructions and their arguments. Both approaches combine automatically induced pattern-matching rules with statistical classifiers that learn the subtler parameters of the constructions. Our results show that these approaches are promising: they significantly outperform naïve baselines for both construction recognition and cause and effect head matches.', 'corpus_id': 10077985, 'score': 1}, {'doc_id': '227228154', 'title': 'Language-Driven Region Pointer Advancement for Controllable Image Captioning', 'abstract': 'Controllable Image Captioning is a recent sub-field in the multi-modal task of Image Captioning wherein constraints are placed on which regions in an image should be described in the generated natural language caption. This puts a stronger focus on producing more detailed descriptions, and opens the door for more end-user control over results. A vital component of the Controllable Image Captioning architecture is the mechanism that decides the timing of attending to each region through the advancement of a region pointer. In this paper, we propose a novel method for predicting the timing of region pointer advancement by treating the advancement step as a natural part of the language structure via a NEXT-token, motivated by a strong correlation to the sentence structure in the training data. We find that our timing agrees with the ground-truth timing in the Flickr30k Entities test data with a precision of 86.55% and a recall of 97.92%. Our model implementing this technique improves the state-of-the-art on standard captioning metrics while additionally demonstrating a considerably larger effective vocabulary size.', 'corpus_id': 227228154, 'score': 0}, {'doc_id': '227238703', 'title': 'Meta-Embeddings for Natural Language Inference and Semantic Similarity tasks', 'abstract': 'Word Representations form the core component for almost all advanced Natural Language Processing (NLP) applications such as text mining, question-answering, and text summarization, etc. Over the last two decades, immense research is conducted to come up with one single model to solve all major NLP tasks. The major problem currently is that there are a plethora of choices for different NLP tasks. Thus for NLP practitioners, the task of choosing the right model to be used itself becomes a challenge. Thus combining multiple pre-trained word embeddings and forming meta embeddings has become a viable approach to improve tackle NLP tasks. Meta embedding learning is a process of producing a single word embedding from a given set of pre-trained input word embeddings. In this paper, we propose to use Meta Embedding derived from few State-of-the-Art (SOTA) models to efficiently tackle mainstream NLP tasks like classification, semantic relatedness, and text similarity. We have compared both ensemble and dynamic variants to identify an efficient approach. The results obtained show that even the best State-of-the-Art models can be bettered. Thus showing us that meta-embeddings can be used for several NLP tasks by harnessing the power of several individual representations.', 'corpus_id': 227238703, 'score': 0}, {'doc_id': '227229069', 'title': 'Intrinsic Knowledge Evaluation on Chinese Language Models', 'abstract': 'Recent NLP tasks have benefited a lot from pre-trained language models (LM) since they are able to encode knowledge of various aspects. However, current LM evaluations focus on downstream performance, hence lack to comprehensively inspect in which aspect and to what extent have they encoded knowledge. This paper addresses both queries by proposing four tasks on syntactic, semantic, commonsense, and factual knowledge, aggregating to a total of $39,308$ questions covering both linguistic and world knowledge in Chinese. Throughout experiments, our probes and knowledge data prove to be a reliable benchmark for evaluating pre-trained Chinese LMs. Our work is publicly available at https://github.com/ZhiruoWang/ChnEval.', 'corpus_id': 227229069, 'score': 0}, {'doc_id': '232269696', 'title': 'GPT Understands, Too', 'abstract': 'While GPTs with traditional fine-tuning fail to achieve strong results on natural language understanding (NLU), we show that GPTs can be better than or comparable to similar-sized BERTs on NLU tasks with a novel method P-tuning— which employs trainable continuous prompt embeddings. On the knowledge probing (LAMA) benchmark, the best GPT recovers 64% (P@1) of world knowledge without any additional text provided during test time, which substantially improves the previous best by 20+ percentage points. On the SuperGlue benchmark, GPTs achieve comparable and sometimes better performance to similar-sized BERTs in supervised learning. Importantly, we find that P-tuning also improves BERTs’ performance in both few-shot and supervised settings while largely reducing the need for prompt engineering. Consequently, Ptuning outperforms the state-of-the-art approaches on the few-shot SuperGlue benchmark.', 'corpus_id': 232269696, 'score': 1}]
188	{'doc_id': '17966572', 'title': 'A CMOS silicon spin qubit', 'abstract': 'Silicon, the main constituent of microprocessor chips, is emerging as a promising material for the realization of future quantum processors. Leveraging its well-established complementary metal–oxide–semiconductor (CMOS) technology would be a clear asset to the development of scalable quantum computing architectures and to their co-integration with classical control hardware. Here we report a silicon quantum bit (qubit) device made with an industry-standard fabrication process. The device consists of a two-gate, p-type transistor with an undoped channel. At low temperature, the first gate defines a quantum dot encoding a hole spin qubit, the second one a quantum dot used for the qubit read-out. All electrical, two-axis control of the spin qubit is achieved by applying a phase-tunable microwave modulation to the first gate. The demonstrated qubit functionality in a basic transistor-like device constitutes a promising step towards the elaboration of scalable spin qubit geometries in a readily exploitable CMOS platform.', 'corpus_id': 17966572}	3897	"[{'doc_id': '4421793', 'title': 'A single-atom electron spin qubit in silicon', 'abstract': 'A single atom is the prototypical quantum system, and a natural candidate for a quantum bit, or qubit—the elementary unit of a quantum computer. Atoms have been successfully used to store and process quantum information in electromagnetic traps, as well as in diamond through the use of the nitrogen–vacancy-centre point defect. Solid-state electrical devices possess great potential to scale up such demonstrations from few-qubit control to larger-scale quantum processors. Coherent control of spin qubits has been achieved in lithographically defined double quantum dots in both GaAs (refs 3–5) and Si (ref. 6). However, it is a formidable challenge to combine the electrical measurement capabilities of engineered nanostructures with the benefits inherent in atomic spin qubits. Here we demonstrate the coherent manipulation of an individual electron spin qubit bound to a phosphorus donor atom in natural silicon, measured electrically via single-shot read-out. We use electron spin resonance to drive Rabi oscillations, and a Hahn echo pulse sequence reveals a spin coherence time exceeding 200\u2009µs. This time should be even longer in isotopically enriched 28Si samples. Combined with a device architecture that is compatible with modern integrated circuit technology, the electron spin of a single phosphorus atom in silicon should be an excellent platform on which to build a scalable quantum computer.', 'corpus_id': 4421793, 'score': 1}, {'doc_id': '211069511', 'title': 'Formation of quantum dots in GaN/AlGaN FETs', 'abstract': 'GaN and the heterostructures are attractive in condensed matter science and applications for electronic devices. We measure the electron transport in GaN/AlGaN field-effect transistors (FETs) at cryogenic temperature. We observe formation of quantum dots in the conduction channel near the depletion of the 2-dimensional electron gas (2DEG). Multiple quantum dots are formed in the disordered potential induced by impurities in the FET conduction channel. We also measure the gate insulator dependence of the transport properties. These results can be utilized for the development of quantum dot devices utilizing GaN/AlGaN heterostructures and evaluation of the impurities in GaN/AlGaN FET channels.', 'corpus_id': 211069511, 'score': 0}, {'doc_id': '216652795', 'title': 'Pandemic and Food Security: A View from the Global South', 'abstract': ""This paper offers a view of COVID-19's specific effects on the availability, access, utilization, and stability of food in the developing world It is indicated that the COVID-19 pandemic has amplified food security issues that have been endemic in the vast majority of the countries of the South in modern times"", 'corpus_id': 216652795, 'score': 0}, {'doc_id': '211069168', 'title': 'Design of a Single-Shot Electron detector with sub-electron sensitivity for electron flying qubit operation.', 'abstract': 'The recent realization of coherent single-electron sources in ballistic conductors let us envision performing time-resolved electronic interferometry experiments analogous to quantum optics experiments.One could eventually use propagating electronic excitations as flying qubits. However an important missing brick is the single-shot electron detection which would enable a complete quantum information operation with flying qubits. Here, we propose and discuss the design of a single charge detector able to achieve in-flight detection of electron flying qubits. Its sub-electron sensitivity would allow the detection of the fractionally charged flying anyons of the Fractional Quantum Hall Effect and would enable the detection of anyonic statistics using coincidence measurements.', 'corpus_id': 211069168, 'score': 1}, {'doc_id': '212725221', 'title': 'Spin orbit field in a physically defined p type MOS silicon double quantum dot', 'abstract': 'We experimentally and theoretically investigate the spin orbit (SO) field in a physically defined, p type metal oxide semiconductor double quantum dot in silicon. We measure the magnetic field dependence of the leakage current through the double dot in the Pauli spin blockade. A finite magnetic field lifts the blockade, with the lifting least effective when the external and SO fields are parallel. In this way, we find that the spin flip of a tunneling hole is due to a SO field pointing perpendicular to the double dot axis and almost fully out of the quantum well plane. We augment the measurements by a derivation of SO terms using group symmetric representations theory. It predicts that without in plane electric fields (a quantum well case), the SO field would be mostly within the plane, dominated by a sum of a Rashba and a Dresselhaus like term. We, therefore, interpret the observed SO field as originated in the electric fields with substantial in plane components.', 'corpus_id': 212725221, 'score': 1}, {'doc_id': '211043783', 'title': 'Synthesizing three-body interaction of spin chirality with superconducting qubits', 'abstract': 'Superconducting qubits provide a competitive platform for quantum simulation of complex dynamics that lies at the heart of quantum many-body systems, because of the flexibility and scalability afforded by the nature of microfabrication. However, in a multiqubit device, the physical form of couplings between qubits is either an electric (capacitor) or magnetic field (inductor), and the associated quadratic field energy determines that only two-body interaction in the Hamiltonian can be directly realized. Here we propose and experimentally synthesize the three-body spin-chirality interaction in a superconducting circuit based on Floquet engineering. By periodically modulating the resonant frequencies of the qubits connected with each other via capacitors, we can dynamically turn on and off qubit-qubit couplings, and further create chiral flows of the excitations in the three-qubit circular loop. Our result is a step toward engineering dynamical and many-body interactions in multiqubit superconducting devices, which potentially expands the degree of freedom in quantum simulation tasks.', 'corpus_id': 211043783, 'score': 0}, {'doc_id': '216651155', 'title': 'Recent advances in influenza vaccines', 'abstract': 'Seasonal influenza remains a major public health problem, responsible for hundreds of thousands of deaths every year, mostly of elderly people. Despite the wide availability of vaccines, there are multiple problems decreasing the effectiveness of vaccination programs. These include viral variability and hence the requirement to match strains by estimating which will become prevalent each season, problems associated with vaccine and adjuvant production, and the route of administration as well as the perceived lower vaccine efficiency in older adults. Clinical protection is still suboptimal for all of these reasons, and vaccine uptake remains too low in most countries. Efforts to improve the effectiveness of influenza vaccines include developing universal vaccines independent of the circulating strains in any particular season and stimulating cellular as well as humoral responses, especially in the elderly. This commentary assesses progress over the last 3 years towards achieving these aims. Since the beginning of 2020, an unprecedented international academic and industrial effort to develop effective vaccines against the new coronavirus SARS-CoV-2 has diverted attention away from influenza, but many of the lessons learned for the one will synergize with the other to mutual advantage. And, unlike the SARS-1 epidemic and, we hope, the SARS-CoV-2 pandemic, influenza will not be eliminated and thus efforts to improve influenza vaccines will remain of crucial importance.', 'corpus_id': 216651155, 'score': 0}, {'doc_id': '211097000', 'title': 'Paired electron motion in interacting chains of quantum dots', 'abstract': 'We study the motion of a pair of electrons along two separate parallel chains of quantum dots. The electrons that are released from the central dot of each chain tend to accompany and not avoid each other. The correlated electron motion involves entanglement of the wave functions which is generated in time upon release of the initial confinement. Observation of the simultaneous presence of electrons at the same side of the chain can provide fingerprint of the paired electron motion.', 'corpus_id': 211097000, 'score': 1}, {'doc_id': '216642772', 'title': 'Increased travel times to United States SARS-CoV-2 testing sites: a spatial modeling study', 'abstract': 'Importance: Access to testing is key to a successful response to the COVID-19 pandemic. Objective: To determine the geographic accessibility to SARS-CoV-2 testing sites in the United States, as quantified by travel time. Design: Cross-sectional analysis of SARS-CoV-2 testing sites as of April 7, 2020 in relation to travel time. Setting: United States COVID-19 pandemic. Participants: The United States, including the 48 contiguous states and the District of Columbia. Exposures: Population density, percent minority, percent uninsured, and median income by county from the 2018 American Community Survey demographic data. Main Outcome: SARS-CoV-2 testing sites identified in two national databases (Carbon Health and CodersAgainstCovid), geocoded by address. Median county 1 km2 gridded friction surface of travel times, as a measure of geographic accessibility to SARS-CoV-2 testing sites. Results: 6,236 unique SARS-CoV-2 testing sites in 3,108 United States counties were identified. Thirty percent of the U.S. population live in a county (N = 1,920) with a median travel time over 20 minutes. This was geographically heterogeneous; 86% of the Mountain division population versus 5% of the Middle Atlantic population lived in counties with median travel times over 20 min. Generalized Linear Models showed population density, percent minority, percent uninsured and median income were predictors of median travel time to testing sites. For example, higher percent uninsured was associated with longer travel time (B= 0.41 min/percent, 95% confidence interval 0.3-0.53, p = 1.2x10-12), adjusting for population density. Conclusions and Relevance: Geographic accessibility to SARS-Cov-2 testing sites is reduced in counties with lower population density and higher percent of minority and uninsured, which are also risk factors for worse healthcare access and outcomes. Geographic barriers to SARS-Cov-2 testing may exacerbate health inequalities and bias county-specific transmission estimates. Geographic accessibility should be considered when planning the location of future testing sites and interpreting epidemiological data.', 'corpus_id': 216642772, 'score': 0}, {'doc_id': '216068464', 'title': 'Parity readout of spin qubits in silicon quantum dots', 'abstract': 'Amanda Seedhouse, Tuomo Tanttu, Ross C. C. Leon, Ruichen Zhao, ∗ Kuan Yen Tan, † Bas Hensen, Fay E. Hudson, Kohei M. Itoh, Jun Yoneda, Chih Hwan Yang, Andrea Morello, Arne Laucht, Susan N. Coppersmith, Andre Saraiva, and Andrew S. Dzurak School of Electrical Engineering and Telecommunications, The University of New South Wales, Sydney, NSW 2052, Australia QCD Labs, QTF Centre of Excellence, Department of Applied Physics, Aalto University, 00076 AALTO, Finland School of Fundamental Science and Technology, Keio University, 3-14-1 Hiyoshi, Kohoku-ku, Yokohama 223-8522, Japan School of Physics, University of New South Wales, Sydney, NSW 2052, Australia (Dated: April 23, 2020)', 'corpus_id': 216068464, 'score': 1}]"
189	{'doc_id': '218595860', 'title': 'Solving high-dimensional Hamilton-Jacobi-Bellman PDEs using neural networks: perspectives from the theory of controlled diffusions and measures on path space', 'abstract': 'Optimal control of diffusion processes is intimately connected to the problem of solving certain Hamilton-Jacobi-Bellman equations. Building on recent machine learning inspired approaches towards high-dimensional PDEs, we investigate the potential of iterative diffusion optimisation techniques, in particular considering applications in importance sampling and rare event simulation. The choice of an appropriate loss function being a central element in the algorithmic design, we develop a principled framework based on divergences between path measures, encompassing various existing methods. Motivated by connections to forward-backward SDEs, we propose and study the novel log-variance divergence, showing favourable properties of corresponding Monte Carlo estimators. The promise of the developed approach is exemplified by a range of high-dimensional and metastable numerical examples.', 'corpus_id': 218595860}	5117	"[{'doc_id': '227013048', 'title': 'Visual Forecasting of Time Series with Image-to-Image Regression', 'abstract': 'Time series forecasting is essential for agents to make decisions in many domains. Existing models rely on classical statistical methods to predict future values based on previously observed numerical information. Yet, practitioners often rely on visualizations such as charts and plots to reason about their predictions. Inspired by the end-users, we re-imagine the topic by creating a framework to produce visual forecasts, similar to the way humans intuitively do. In this work, we take a novel approach by leveraging advances in deep learning to extend the field of time series forecasting to a visual setting. We do this by transforming the numerical analysis problem into the computer vision domain. Using visualizations of time series data as input, we train a convolutional autoencoder to produce corresponding visual forecasts. We examine various synthetic and real datasets with diverse degrees of complexity. Our experiments show that visual forecasting is effective for cyclic data but somewhat less for irregular data such as stock price. Importantly, we find the proposed visual forecasting method to outperform numerical baselines. We attribute the success of the visual forecasting approach to the fact that we convert the continuous numerical regression problem into a discrete domain with quantization of the continuous target signal into pixel space.', 'corpus_id': 227013048, 'score': 0}, {'doc_id': '227126670', 'title': 'Solving path dependent PDEs with LSTM networks and path signatures', 'abstract': 'Using a combination of recurrent neural networks and signature methods from the rough paths theory we design efficient algorithms for solving parametric families of path dependent partial differential equations (PPDEs) that arise in pricing and hedging of path-dependent derivatives or from use of non-Markovian model, such as rough volatility models in Jacquier and Oumgari, 2019. The solutions of PPDEs are functions of time, a continuous path (the asset price history) and model parameters. As the domain of the solution is infinite dimensional many recently developed deep learning techniques for solving PDEs do not apply. Similarly as in Vidales et al. 2018, we identify the objective function used to learn the PPDE by using martingale representation theorem. As a result we can de-bias and provide confidence intervals for then neural network-based algorithm. We validate our algorithm using classical models for pricing lookback and auto-callable options and report errors for approximating both prices and hedging strategies.', 'corpus_id': 227126670, 'score': 0}, {'doc_id': '227151560', 'title': 'Remaining Useful Life Estimation Under Uncertainty with Causal GraphNets', 'abstract': 'In this work, a novel approach for the construction and training of time series models is presented that deals with the problem of learning on large time series with non-equispaced observations, which at the same time may possess features of interest that span multiple scales. The proposed method is appropriate for constructing predictive models for non-stationary stochastic time series.The efficacy of the method is demonstrated on a simulated stochastic degradation dataset and on a real-world accelerated life testing dataset for ball-bearings. The proposed method, which is based on GraphNets, implicitly learns a model that describes the evolution of the system at the level of a state-vector rather than of a raw observation. The proposed approach is compared to a recurrent network with a temporal convolutional feature extractor head (RNN-tCNN) which forms a known viable alternative for the problem context considered. Finally, by taking advantage of recent advances in the computation of reparametrization gradients for learning probability distributions, a simple yet effective technique for representing prediction uncertainty as a Gamma distribution over remaining useful life predictions is employed.', 'corpus_id': 227151560, 'score': 0}, {'doc_id': '220713819', 'title': 'Changes to the extreme and erratic behaviour of cryptocurrencies during COVID-19', 'abstract': '\n This paper introduces new methods for analysing the extreme and erratic behaviour of time series to evaluate the impact of COVID-19 on cryptocurrency market dynamics. Across 51 cryptocurrencies, we examine extreme behaviour through a study of distribution extremities, and erratic behaviour through structural breaks. First, we analyse the structure of the market as a whole and observe a reduction in self-similarity as a result of COVID-19, particularly with respect to structural breaks in variance. Second, we compare and contrast these two behaviours, and identify individual anomalous cryptocurrencies. Tether (USDT) and TrueUSD (TUSD) are consistent outliers with respect to their returns, while Holo (HOT), NEXO (NEXO), Maker (MKR) and NEM (XEM) are frequently observed as anomalous with respect to both behaviours and time. Even among a market known as consistently volatile, this identifies individual cryptocurrencies that behave most irregularly in their extreme and erratic behaviour and shows these were more affected during the COVID-19 market crisis.\n', 'corpus_id': 220713819, 'score': 1}, {'doc_id': '226299995', 'title': 'Discrete solution pools and noise-contrastive estimation for predict-and-optimize', 'abstract': 'Numerous real-life decision-making processes involve solving a combinatorial optimization problem with uncertain input that can be estimated from historic data. There is a growing interest in decision-focused learning methods, where the loss function used for learning to predict the uncertain input uses the outcome of solving the combinatorial problem over a set of predictions. Different surrogate loss functions have been identified, often using a continuous approximation of the combinatorial problem. However, a key bottleneck is that to compute the loss, one has to solve the combinatorial optimisation problem for each training instance in each epoch, which is computationally expensive even in the case of continuous approximations. \nWe propose a different solver-agnostic method for decision-focused learning, namely by considering a pool of feasible solutions as a discrete approximation of the full combinatorial problem. Solving is now trivial through a single pass over the solution pool. We design several variants of a noise-contrastive loss over the solution pool, which we substantiate theoretically and empirically. Furthermore, we show that by dynamically re-solving only a fraction of the training instances each epoch, our method performs on par with the state of the art, whilst drastically reducing the time spent solving, hence increasing the feasibility of predict-and-optimize for larger problems.', 'corpus_id': 226299995, 'score': 0}, {'doc_id': '229156868', 'title': 'Lagrangian Reachtubes: The Next Generation', 'abstract': 'We introduce LRT-NG, a set of techniques and an associated toolset that computes a reachtube (an over-approximation of the set of reachable states over a given time horizon) of a nonlinear dynamical system. LRT-NG significantly advances the state-of-the-art Langrangian Reachability and its associated tool LRT. From a theoretical perspective, LRT-NG is superior to LRT in three ways. First, it uses for the first time an analytically computed metric for the propagated ball which is proven to minimize the ball’s volume. We emphasize that the metric computation is the centerpiece of all bloating-based techniques. Secondly, it computes the next reachset as the intersection of two balls: one based on the Cartesian metric and the other on the new metric. While the two metrics were previously considered opposing approaches, their joint use considerably tightens the reachtubes. Thirdly, it avoids the ""wrapping effect"" associated with the validated integration of the center of the reachset, by optimally absorbing the interval approximation in the radius of the next ball. From a tool-development perspective, LRT-NG is superior to LRT in two ways. First, it is a standalone tool that no longer relies on CAPD. This required the implementation of the Lohner method and a Runge-Kutta time-propagation method. Secondly, it has an improved interface, allowing the input model and initial conditions to be provided as external input files. Our experiments on a comprehensive set of benchmarks, including two Neural ODEs, demonstrates LRT-NG’s superior performance compared to LRT, CAPD, and Flow*.', 'corpus_id': 229156868, 'score': 1}, {'doc_id': '224814213', 'title': 'Logistic $Q$-Learning', 'abstract': 'We propose a new reinforcement learning algorithm derived from a regularized linear-programming formulation of optimal control in MDPs. The method is closely related to the classic Relative Entropy Policy Search (REPS) algorithm of Peters et al. (2010), with the key difference that our method introduces a Q-function that enables efficient exact model-free implementation. The main feature of our algorithm (called QREPS) is a convex loss function for policy evaluation that serves as a theoretically sound alternative to the widely used squared Bellman error. We provide a practical saddle-point optimization method for minimizing this loss function and provide an error-propagation analysis that relates the quality of the individual updates to the performance of the output policy. Finally, we demonstrate the effectiveness of our method on a range of benchmark problems.', 'corpus_id': 224814213, 'score': 0}, {'doc_id': '228810043', 'title': 'Forecasting the Returns of Cryptocurrency: A Model Averaging Approach', 'abstract': 'This paper aims to enrich the understanding and modelling strategies for cryptocurrency markets by investigating major cryptocurrencies’ returns determinants and forecast their returns. To handle model uncertainty when modelling cryptocurrencies, we conduct model selection for an autoregressive distributed lag (ARDL) model using several popular penalized least squares estimators to explain the cryptocurrencies’ returns. We further introduce a novel model averaging approach or the shrinkage Mallows model averaging (SMMA) estimator for forecasting. First, we find that the returns for most cryptocurrencies are sensitive to volatilities from major financial markets. The returns are also prone to the changes in gold prices and the Forex market’s current and lagged information. Then, when forecasting cryptocurrencies’ returns, we further find that an ARDL(p,q) model estimated by the SMMA estimator outperforms the competing estimators and models out-of-sample.', 'corpus_id': 228810043, 'score': 1}, {'doc_id': '221082095', 'title': 'SDE-Net: Equipping Deep Neural Networks with Uncertainty Estimates', 'abstract': 'Uncertainty quantification is a fundamental yet unsolved problem for deep learning. The Bayesian framework provides a principled way of uncertainty estimation but is often not scalable to modern deep neural nets (DNNs) that have a large number of parameters. Non-Bayesian methods are simple to implement but often conflate different sources of uncertainties and require huge computing resources. We propose a new method for quantifying uncertainties of DNNs from a dynamical system perspective. The core of our method is to view DNN transformations as state evolution of a stochastic dynamical system and introduce a Brownian motion term for capturing epistemic uncertainty. Based on this perspective, we propose a neural stochastic differential equation model (SDE-Net) which consists of (1) a drift net that controls the system to fit the predictive function; and (2) a diffusion net that captures epistemic uncertainty. We theoretically analyze the existence and uniqueness of the solution to SDE-Net. Our experiments demonstrate that the SDE-Net model can outperform existing uncertainty estimation methods across a series of tasks where uncertainty plays a fundamental role.', 'corpus_id': 221082095, 'score': 1}, {'doc_id': '233004471', 'title': 'A Survey on Semi-parametric Machine Learning Technique for Time Series Forecasting', 'abstract': 'Artificial Intelligence (AI) has recently shown its capabilities for almost every field of life. Machine Learning, which is a subset of AI, is a ‘HOT’ topic for researchers. Machine Learning outperforms other classical forecasting techniques in almost all-natural applications. It is a crucial part of modern research. As per this statement, Modern Machine Learning algorithms are hungry for big data. Due to the small datasets, the researchers may not prefer to use Machine Learning algorithms. To tackle this issue, the main purpose of this survey is to illustrate, demonstrate related studies for significance of a semi-parametric Machine Learning framework called Grey Machine Learning (GML). This kind of framework is Khwaja Mutahir Ahmad School of Information and Software Engineering, University of Electronic Science and Technology of China, 611731 Chengdu, Sichuan, P.R.China. E-mail: khwajamutahir311@gmail.com Gang He( ) School of Computer Science and Technology, Southwest University of Science and Technology, 621010 Mianyang, Sichuan, P.R.China. E-mail: ganghe@swust.edu.cn Wenxin Yu School of Computer Science and Technology, Southwest University of Science and Technology, 621010 Mianyang, Sichuan, P.R.China. E-mail: yuwenxin@swust.edu.cn Xiaochuan Xu School of Mathematics and Statistics, Sichuan University of Science and Engineering, 643000 Zigong, Sichuan, P.R.China. E-mail: contactayesha@qq.com Jay Kumar School of Computer Science and Engineering, University of Electronic Science and Technology of China, 611731 Chengdu, Sichuan, P.R.China. E-mail: jay@std.uestc.edu.cn Muhammad Asim Saleem School of Information and Software Engineering, University of Electronic Science and Technology of China, 611731 Chengdu, Sichuan, P.R.China. E-mail: asim.saleem1@hotmail.com ar X iv :2 10 4. 00 87 1v 1 [ cs .L G ] 2 A pr 2 02 1 2 Khwaja Mutahir Ahmad et al. capable of handling large datasets as well as small datasets for time series forecasting likely outcomes. This survey presents a comprehensive overview of the existing semi-parametric machine learning techniques for time series forecasting. In this paper, a primer survey on the GML framework is provided for researchers. To allow an in-depth understanding for the readers, a brief description of Machine Learning, as well as various forms of conventional grey forecasting models are discussed. Moreover, a brief description on the importance of GML framework is presented.', 'corpus_id': 233004471, 'score': 1}]"
190	{'doc_id': '227339390', 'title': 'Neurosymbolic AI for Situated Language Understanding', 'abstract': 'In recent years, data-intensive AI, particularly the domain of natural language processing and understanding, has seen significant progress driven by the advent of large datasets and deep neural networks that have sidelined more classic AI approaches to the field. These systems can apparently demonstrate sophisticated linguistic understanding or generation capabilities, but often fail to transfer their skills to situations they have not encountered before. We argue that computational situated grounding provides a solution to some of these learning challenges by creating situational representations that both serve as a formal model of the salient phenomena, and contain rich amounts of exploitable, task-appropriate data for training new, flexible computational models. Our model reincorporates some ideas of classic AI into a framework of neurosymbolic intelligence, using multimodal contextual modeling of interactive situations, events, and object properties. We discuss how situated grounding provides diverse data and multiple levels of modeling for a variety of AI learning challenges, including learning how to interact with object affordances, learning semantics for novel structures and configurations, and transferring such learned knowledge to new objects and situations.', 'corpus_id': 227339390}	11650	"[{'doc_id': '237251782', 'title': 'VALSE: A Task-Independent Benchmark for Vision and Language Models Centered on Linguistic Phenomena', 'abstract': 'We propose VALSE (Vision And Language 001 Structured Evaluation), a novel benchmark de002 signed for testing general-purpose pretrained 003 vision and language (V&L) models for spe004 cific visio-linguistic grounding capabilities. 005 Currently, V&L models are evaluated on tasks 006 such as visual question answering or visual rea007 soning, which do not address their fine-grained 008 linguistic capabilities. VALSE addresses this 009 gap by offering a suite of six tests targeting 010 specific linguistic phenomena. Solving these 011 tests requires models to ground these phenom012 ena in the visual modality, allowing more fine013 grained evaluations than hitherto possible. We 014 build VALSE using methods that support the 015 construction of reliable foils, and report re016 sults from evaluating five widely-used V&L 017 models. Our experiments suggest that current 018 models have considerable difficulty addressing 019 most phenomena. Hence, we expect VALSE 020 to serve as an important benchmark to mea021 sure future progress of pretrained V&L models 022 from a linguistic perspective, complementing 023 the canonical task-centred V&L evaluations. 024', 'corpus_id': 237251782, 'score': 1}, {'doc_id': '222124849', 'title': 'Syntax Representation in Word Embeddings and Neural Networks - A Survey', 'abstract': 'Neural networks trained on natural language processing tasks capture syntax even though it is not provided as a supervision signal. This indicates that syntactic analysis is essential to the understating of language in artificial intelligence systems. This overview paper covers approaches of evaluating the amount of syntactic information included in the representations of words for different neural network architectures. We mainly summarize re-search on English monolingual data on language modeling tasks and multilingual data for neural machine translation systems and multilingual language models. We describe which pre-trained models and representations of language are best suited for transfer to syntactic tasks.', 'corpus_id': 222124849, 'score': 0}, {'doc_id': '222291439', 'title': 'Self-play for Data Efficient Language Acquisition', 'abstract': 'When communicating, people behave consistently across conversational roles: People understand the words they say and are able to produce the words they hear. To date, artificial agents developed for language tasks have lacked such symmetry, meaning agents trained to produce language are unable to understand it and vice-versa. In this work, we exploit the symmetric nature of communication in order to improve both the efficiency and quality of language acquisition in learning agents. Specifically, we consider the setting in which an agent must learn to both understand and generate words in an existing language, but with the assumption that access to interaction with ""oracle"" speakers of the language is very limited. We show that using self-play as a substitute for direct supervision enables the agent to transfer its knowledge across roles (e.g. training as a listener but testing as a speaker) and make better inferences about the ground truth lexicon using only a handful of interactions with the oracle.', 'corpus_id': 222291439, 'score': 0}, {'doc_id': '237260024', 'title': 'Airbert: In-domain Pretraining for Vision-and-Language Navigation', 'abstract': 'Vision-and-language navigation (VLN) aims to enable embodied agents to navigate in realistic environments using natural language instructions. Given the scarcity of domain-specific training data and the high diversity of image and language inputs, the generalization of VLN agents to unseen environments remains challenging. Recent methods explore pretraining to improve generalization, however, the use of generic image-caption datasets or existing smallscale VLN environments is suboptimal and results in limited improvements. In this work, we introduce BnB1, a largescale and diverse in-domain VLN dataset. We first collect image-caption (IC) pairs from hundreds of thousands of listings from online rental marketplaces. Using IC pairs we next propose automatic strategies to generate millions of VLN path-instruction (PI) pairs. We further propose a shuffling loss that improves the learning of temporal order inside PI pairs. We use BnB to pretrain our Airbert2 model that can be adapted to discriminative and generative settings and show that it outperforms state of the art for Room-to-Room (R2R) navigation and Remote Referring Expression (REVERIE) benchmarks. Moreover, our in-domain pretraining significantly increases performance on a challenging few-shot VLN evaluation, where we train the model only on VLN instructions from a few houses.', 'corpus_id': 237260024, 'score': 1}, {'doc_id': '216035815', 'title': 'Experience Grounds Language', 'abstract': ""Successful linguistic communication relies on a shared experience of the world, and it is this shared experience that makes utterances meaningful. Despite the incredible effectiveness of language processing models trained on text alone, today's best systems still make mistakes that arise from a failure to relate language to the physical world it describes and to the social interactions it facilitates. \nNatural Language Processing is a diverse field, and progress throughout its development has come from new representational theories, modeling techniques, data collection paradigms, and tasks. We posit that the present success of representation learning approaches trained on large text corpora can be deeply enriched from the parallel tradition of research on the contextual and social nature of language. \nIn this article, we consider work on the contextual foundations of language: grounding, embodiment, and social interaction. We describe a brief history and possible progression of how contextual information can factor into our representations, with an eye towards how this integration can move the field forward and where it is currently being pioneered. We believe this framing will serve as a roadmap for truly contextual language understanding."", 'corpus_id': 216035815, 'score': 1}, {'doc_id': '222379195', 'title': 'Natural Language Rationales with Full-Stack Visual Reasoning: From Pixels to Semantic Frames to Commonsense Graphs', 'abstract': 'Natural language rationales could provide intuitive, higher-level explanations that are easily understandable by humans, complementing the more broadly studied lower-level explanations based on gradients or attention weights. We present the first study focused on generating natural language rationales across several complex visual reasoning tasks: visual commonsense reasoning, visual-textual entailment, and visual question answering. The key challenge of accurate rationalization is comprehensive image understanding at all levels: not just their explicit content at the pixel level, but their contextual contents at the semantic and pragmatic levels. We present RationaleˆVT Transformer, an integrated model that learns to generate free-text rationales by combining pretrained language models with object recognition, grounded visual semantic frames, and visual commonsense graphs. Our experiments show that free-text rationalization is a promising research direction to complement model interpretability for complex visual-textual reasoning tasks. In addition, we find that integration of richer semantic and pragmatic visual features improves visual fidelity of rationales.', 'corpus_id': 222379195, 'score': 1}, {'doc_id': '237592834', 'title': 'COVR: A test-bed for Visually Grounded Compositional Generalization with real images', 'abstract': 'While interest in models that generalize at test time to new compositions has risen in recent years, benchmarks in the visually-grounded domain have thus far been restricted to synthetic images. In this work, we propose COVR, a new test-bed for visually-grounded compositional generalization with real images. To create COVR, we use real images annotated with scene graphs, and propose an almost fully automatic procedure for generating question-answer pairs along with a set of context images. COVR focuses on questions that require complex reasoning, including higherorder operations such as quantification and aggregation. Due to the automatic generation process, COVR facilitates the creation of compositional splits, where models at test time need to generalize to new concepts and compositions in a zeroor few-shot setting. We construct compositional splits using COVR and demonstrate a myriad of cases where state-ofthe-art pre-trained language-and-vision models struggle to compositionally generalize.', 'corpus_id': 237592834, 'score': 1}, {'doc_id': '222124957', 'title': 'Which *BERT? A Survey Organizing Contextualized Encoders', 'abstract': 'Pretrained contextualized text encoders are now a staple of the NLP community. We present a survey on language representation learning with the aim of consolidating a series of shared lessons learned across a variety of recent efforts. While significant advancements continue at a rapid pace, we find that enough has now been discovered, in different directions, that we can begin to organize advances according to common themes. Through this organization, we highlight important considerations when interpreting recent contributions and choosing which model to use.', 'corpus_id': 222124957, 'score': 0}]"
191	{'doc_id': '212894792', 'title': 'Traditional methods of food grains preservation and storage in Nigeria and India', 'abstract': 'Abstract Efficient post-harvest handling storage can tremendously contribute to socio-economic empowerment in developing nations. Farmers use traditional storage containers for storing food grains for their own need. These storage structures are comparatively cheap, eco-friendly and impart high shelf life to the stored commodities. These traditional storage systems could be applied in modern storage areas with minor modification, could save food commodities that would be damaged by insects. Although chemical methods of management of stored produce pests are highly successful, they leave behind toxic residues. The traditional wisdom and methods of storage can protect commodities from insect infestation for substantially longer periods.', 'corpus_id': 212894792}	16980	[{'doc_id': '164600204', 'title': 'EVIDENCE FOR SIGNIFICANT SUBTERRANEAN STORAGE AT TWO HUNTER-GATHERER SITES: THE PRESENCE OF A MAST-BASED ECONOMY IN THE LATE ARCHAIC COASTAL AMERICAN SOUTHEAST', 'abstract': 'Excavations at two Late Archaic shell rings on St. Catherines Island, Georgia, revealed evidence of significant amounts of subterranean storage. Based on botanical evidence, ethonographic analogies, and interpretations of other Late Archaic sites, hickory nuts and acorns are the most likely resource being stored, and quantifying the capacity found at each ring highlights the prevalence and importance of mast storage. These findings are important because large-scale storage has rarely been proposed for Late Archaic coastal peoples and, therefore, its impact as a potential factor for social changes enacted during this time period, including increasing sedentism, formalization of intragroup relations, and regionalization of cultural identities, has yet to be explored. Excavaciones en dos anillos de concha correspondientes al Arcaico Tardío de la isla de St. Catherines, Georgia, obtuvieron significativas evidencias de almacenaje subterráneo. Con base en evidencia botánica, analogías etnográficas e interpretaciones de otros sitios del Arcaico Tardío, es probable que las nueces de nogal y las bellotas fueran los recursos almacenados. Cuantificando la capacidad registrada en cada anillo, se destaca la prevalencia e importancia del almacenamiento de bellotas. Estos hallazgos son importantes pues el almacenamiento a larga escala raramente ha sido propuesto para los pueblos costeros del Arcaico Tardío. Por lo tanto, aún no se ha explorado su impacto como factor potencial para cambios sociales promulgados durante este período, incluyendo el incremento del sedentarismo, la formulación de relaciones intragrupo y la regionalización de identidades culturales.', 'corpus_id': 164600204, 'score': 1}, {'doc_id': '233621621', 'title': 'Mapping Logistical Capitalism', 'abstract': 'This review article engages with a rich field of scholarship on logistics that has gathered momentum over the past decade, focusing on two new publications by Laleh Khalili and Martín Arboleda. It contextualizes how and why logistics is bound up with the militarization of contemporary political and social life. I argue that the later 20th century rise of logistics can be better understood as both a response to and symptom of capitalist crisis and I situate this scholarship on war and logistics in relationship to Giovanni Arrighi’s account of crisis and ‘unravelling hegemony’. I also show how logistics provides essential critical and visual resources that contribute to efforts to map global capitalism and to debates on totality and class composition in contemporary critical theory. Finally, contemporary events such as the ongoing Coronavirus crisis and the reemergence of Black Lives Matter are considered in light of this analysis with reference to the centrality of logistics to racial capitalism.', 'corpus_id': 233621621, 'score': 0}, {'doc_id': '234098954', 'title': 'Meat, Markets, and Monopolies: The Politics and Economics of Animal Agriculture', 'abstract': 'The COVID-19 crisis has focused renewed attention on the harms of the present-day meat industry. From debates about zoonotic transmission in factory farms and wild animal markets to outcries over high rates of illness and death among agricultural and food processing workers, the economics and ethics of intensified meat production and consumption are at the fulcrum of public discourse on the virus. Conversations about the more-thanhuman dimensions of COVID-19 highlight a core contradiction for contemporary carnivores: While most strongly disagree with animal cruelty and many treat their companion animals as full-fledged members of the nuclear family, they are collectively eating twice as much meat as they did in 1961. Almost all of this meat is raised in horrific conditions, ‘‘outside of every supposed norm of a life proper’’ (Derrida 2002:394). Moreover, the ‘‘distance and concealment’’ (Pachirat 2011:252) on which industrial animal agriculture has long depended is key to enabling this mass disavowal, a state of affairs that the editors of Global Meat: Social and Environmental Consequences of the Expanding Meat Industry seek to challenge with the publication of a collection of research on the formation and impact of this gargantuan industry. While the book was released before the virus came to light, it has much to offer readers newly interested in understanding the political economy of industrial meat production and its social and environmental consequences. Noting that ‘‘the expanse of this industry can be difficult to see precisely because of its wide reach’’ (p. 1), the editors seek to map its tentacles at different scales and in a variety of geographic locales. The text includes nine chapters, written largely by U.S.-based contributors, and grew out of a 2017 workshop hosted by the Georgia Institute of Technology, where one of the editors, Bill Winders, is located. Winders is the author of Grains (2016) and The Politics of Food Supply: U.S. Agricultural Policy in the World Economy (2009). His collaborator, Elizabeth Ransom, is a professor of International Affairs at Penn State University and coeditor of Rural America in a Globalizing World: Problems and Prospects for the 2010s (2014). The present book is the sixteenth in MIT Press’s Food, Health, and the Environment series and joins an explosion of writing on the problems and promises of contemporary food systems. Global Meat is divided into three parts, each of which begins with a synthesis of the arguments to follow. Part I contextualizes the global forces shaping this expanding industry, which realized a 500 percent increase in meat production between 1960 and 2016, a rate that far outpaces population growth. Here the focus is on the role of government subsidies in driving corporate concentration and on the size and power of the three biggest firms: Brazil-based JBS, China-based WH Group, and U.S.-based Tyson. A chapter on the vagaries of both intensively managed aquaculture and the fishing of wild stocks rounds out this section. The second part of the book explores how global forces are experienced at the local level through studies of pork production in China, small-scale cattle ranching in the Ecuadorian Amazon, and poultry plant workers in the southeastern United States. The third and final part explores the consequences of the global meat industry through Global Meat: Social and Environmental Consequences of the Expanding Meat Industry, edited by Bill Winders and Elizabeth Ransom. Cambridge, MA: MIT Press. 250 pp. $35.00 paper. ISBN: 9780262537735. 202 Review Essays', 'corpus_id': 234098954, 'score': 0}, {'doc_id': '132077146', 'title': 'Experimental storage of corn underground and its implications for Iron Age settlements', 'abstract': None, 'corpus_id': 132077146, 'score': 1}, {'doc_id': '233275114', 'title': 'Chert procurement in Corsica during the Neolithic: Inferring social territories in the Tyrrhenian islands', 'abstract': 'From the Neolithic, foreign siliceous materials were imported into Corsica as the island lacks local chert and obsidian. Such a context constitutes a real opportunity to investigate the relationship of the island with surrounding areas, in perspective with cultural evolutions. For 20 years, chert sourcing studies were carried out. We took into account 26 sites, dating from the Ancient Neolithic to the Final Neolithic. The work is based on non-destructive petro-archaeological observations of the artefacts. and on the survey and characterization of Sardinian sources (320 samples collected and 60 different lithotypes characterized). The study aims to better understand the place of chert among the lithic assemblages throughout time and identify the provenance of most of the chert material introduced into Corsica. It reveals procurement evolution in terms of preferred facies and of stages of introduction, depending on the chronology and geographical situation of the sites. The results confirm connections with Sardinia, among which Perfugas basin constitutes a major source of raw material for Corsica. It also shows relations with the Italian Peninsula for some Middle Neolithic sites in Northern Corsica. Comparisons with data from Sardinia show the affinities and differences between the two islands and open further avenues for research.', 'corpus_id': 233275114, 'score': 0}, {'doc_id': '229392298', 'title': 'Of birds and trees: Rethinking decoloniality through unsettlement as a pluriversal human condition', 'abstract': 'Unsettlement is our current shared pluriversal human condition which is experienced differently depending on our trajectories, privileges and disadvantages. The negative phase of globalization epitmoized in the Covid-19 crisis, threatens to\xa0 fold the world into a digital slavery where coloniality would finally stop to be seen as a\xa0 ‘problem’ of refugees, migrants and indigenous people or a fashionable term of the academic elite, to be faced directly by each and every. Previously decoloniality has focused mainly on the critique of the intersections of race and capitalism in the production of knowledge and subjectivities. It has seldom addressed the future or ventured outside the position of the colonial difference (exteriority). In the face of the global challenges including the defunct politics and the ultra-right populist turn, as well as the Anthropocene and technological colonization, fragmenting the human species, coloniality needs to be complemented with additional dimensions that would allow overcoming its stand-pointism and refusal to dialogue across the imperial difference with other critical positions. One of such dimensions is unsettlement which is discussed in the article as a promising concept in the agenda for refuturing. Can unsettlement generate new transversal relational solidarity beyond the bankrupt institutions and power structures? Can it launch new communities of change which would inevitably also change ourselves as humans? How would art and fiction react to these tectonic shifts and advance the shaping of the agendas of these communities of change?\xa0 The article briefly addresses two possible paths for artistic representations of the unsettlement\xa0\xa0 - the introspective one struggling with multiple identifications and re-weaving oneself and one’s world anew and a less realized promising way of the positive ontological design fictions and utopias/dystopias transcending modernity/coloniality to imagine an alternative other world. The unsettled ‘birds’ rather than rooted ‘trees’ are likely to be the main protagonists of these fictions helping us to learn that unsettlement can eventually bring a positive sense of the self and/in the world and a new political imagination to refuturing.', 'corpus_id': 229392298, 'score': 0}, {'doc_id': '133018103', 'title': 'L’experimentació sobre sitges tradicionals. Aportacions de l’arqueologia i de l’agronomia', 'abstract': None, 'corpus_id': 133018103, 'score': 1}, {'doc_id': '211568666', 'title': 'Traditional grain storage practices among Soligas of Karnataka, India', 'abstract': 'Indigenous grain storage practices are economically viable, eco-friendly and location specific. In recent times, they are fast disappearing due to advancement in grain technology. However, these indigenous grain storage practices are still in practice in tribal areas. This may be due to the failure of extension activities in reaching the new technologies in grain storage practices to the needy or higher application costs. Tribal communities of the country play an important role in nation’s food security. ‘Soligas’ — a forest dwelling tribe living in small settlements called Podus in the Biligirirangana Hills, Karnataka, India practice subsistence agriculture and store the grains by their own traditional methods. The Soligas use different types of traditional storage structures, namely thenemane, maize (Zea mays L.) cobs tied to overhead ropes, mud pots, bamboo basket, gunny bags and cloth bags for safe storage of food grains. They also adopt different grain protection measures, viz. frequent drying, red earth smearing, mixing lime powder, neem (Azadirachta indica A. Juss) leaf, use of lakkisoppu (Vitex negundo L.), mixing kaadugeru seeds (Semicarpus anacardium L.), mixing of dry chillies (Capsicum sp.), mixing ash of mattimara [Terminalia crenulata (Heyne) Roth], mixing ash and smearing of castor (Ricinus communis L.) oil to protect the grains from various stored grain insects. This study sought to evaluate the different structures and grain protection measures, and grain damage by stored grain insects (SGI) which varied from 30 to 70%. The results revealed that none of these storage structures were suitable for safe storage of grains, as they had one type of disadvantage or the other; not being airtight, nor moisture proof, not insect or rodent proof.', 'corpus_id': 211568666, 'score': 1}, {'doc_id': '161163498', 'title': 'L’experimentació sobre sitges tradicionals. Aportacions de l’arqueologia i de l’agronomia', 'abstract': None, 'corpus_id': 161163498, 'score': 1}, {'doc_id': '232273846', 'title': 'Agroecology and feminist economics : New values for new times', 'abstract': 'T he contemporary crises we now face stem from the overexploitation of nature for the benefit of individual profit. Industrial food is an important component of this model. The fallout of this is all too familiar: soil deterioration, biodiversity loss, deforestation, indigenous and other peoples’ rights violations, precarious rural livelihoods, unsafe working conditions, climate change, the double-edged sword of obesity and malnutrition and strong concentration of power. The capitalist, patriarchal and colonialist system has divided the world into those who have and those who have not, those whose voices are heard and those who are silenced. As a result, women, indigenous as well as black and brown people (among others) have been pushed aside for centuries. The COVID-19 outbreak amplifies, deepens and uncovers these pre-existing tragedies, inequalities and injustices. In many places, new ways of being in the world are being developed. It is high time that we listen to (and learn from) other ways of doing things, other cosmovisions, other ways of organising society, other values precisely those that have been silenced. The world needs new values and new leadership in these shifting times. This is a crucial moment; the decisions we make now could lead us down a path of destruction, but could equally send us on a path towards regeneration. This issue of Farming Matters brings to the forefront how perspectives such as intersectional feminism and indigenous cosmologies coupled with agroecology have been transforming our economy and society. These insights offer pertinent lessons for the pursuit of deeper, much needed transformation.', 'corpus_id': 232273846, 'score': 0}]
192	{'doc_id': '199442632', 'title': 'Learning to Generalize to Unseen Tasks with Bilevel Optimization', 'abstract': 'Recent metric-based meta-learning approaches, which learn a metric space that generalizes well over combinatorial number of different classification tasks sampled from a task distribution, have been shown to be effective for few-shot classification tasks of unseen classes. They are often trained with episodic training where they iteratively train a common metric space that reduces distance between the class representatives and instances belonging to each class, over large number of episodes with random classes. However, this training is limited in that while the main target is the generalization to the classification of unseen classes during training, there is no explicit consideration of generalization during meta-training phase. To tackle this issue, we propose a simple yet effective meta-learning framework for metricbased approaches, which we refer to as learning to generalize (L2G), that explicitly constrains the learning on a sampled classification task to reduce the classification error on a randomly sampled unseen classification task with a bilevel optimization scheme. This explicit learning aimed toward generalization allows the model to obtain a metric that separates well between unseen classes. We validate our L2G framework on mini-ImageNet and tiered-ImageNet datasets with two base meta-learning few-shot classification models, Prototypical Networks and Relation Networks. The results show that L2G significantly improves the performance of the two methods over episodic training. Further visualization shows that L2G obtains a metric space that clusters and separates unseen classes well.', 'corpus_id': 199442632}	14859	[{'doc_id': '235691634', 'title': 'Generalization on Unseen Domains via Inference-Time Label-Preserving Target Projections', 'abstract': 'Generalization of machine learning models trained on a set of source domains on unseen target domains with different statistics, is a challenging problem. While many approaches have been proposed to solve this problem, they only utilize source data during training but do not take advantage of the fact that a single target example is available at the time of inference. Motivated by this, we propose a method that effectively uses the target sample during inference beyond mere classification. Our method has three components (i) A label-preserving feature or metric transformation on source data such that the source samples are clustered in accordance with their class irrespective of their domain (ii) A generative model trained on the these features (iii) A label-preserving projection of the target point on the source-feature manifold during inference via solving an optimization problem on the input space of the generative model using the learned metric. Finally, the projected target is used in the classifier. Since the projected target feature comes from the source manifold and has the same label as the real target by design, the classifier is expected to perform better on it than the true target. We demonstrate that our method outperforms the state-of-the-art Domain Generalization methods on multiple datasets and tasks.', 'corpus_id': 235691634, 'score': 0}, {'doc_id': '236641318', 'title': 'A Channel Coding Benchmark for Meta-Learning', 'abstract': 'Meta-learning provides a popular and effective family of methods for data-efficient learning of new tasks. However, several important issues in meta-learning have proven hard to study thus far. For example, performance degrades in real-world settings where meta-learners must learn from a wide and potentially multi-modal distribution of training tasks; and when distribution shift exists between meta-train and meta-test task distributions. These issues are typically hard to study since the shape of task distributions, and shift between them are not straightforward to measure or control in standard benchmarks. We propose the channel coding problem as a benchmark for meta-learning. Channel coding is an important practical application where task distributions naturally arise, and fast adaptation to new tasks is practically valuable. We use this benchmark to study several aspects of meta-learning, including the impact of task distribution breadth and shift, which can be controlled in the coding problem. Going forward, this benchmark provides a tool for the community to study the capabilities and limitations of meta-learning, and to drive research on practically robust and effective meta-learners.', 'corpus_id': 236641318, 'score': 1}, {'doc_id': '125734953', 'title': 'Meta-Learning with Adaptive Layerwise Metric and Subspace', 'abstract': 'Recent advances in meta-learning demonstrate that deep representations combined with the gradient descent method have sufficient capacity to approximate any learning algorithm. A promising approach is the model-agnostic meta-learning (MAML) which embeds gradient descent into the meta-learner. It optimizes for the initial parameters of the learner to warm-start the gradient descent updates, such that new tasks can be solved using a small number of examples. In this paper we elaborate the gradient-based meta-learning, developing two new schemes. First, we present a feedforward neural network, referred to as T-net, where the linear transformation between two adjacent layers is decomposed as T W such that W is learned by task-specific learners and the transformation T, which is shared across tasks, is meta-learned to speed up the convergence of gradient updates for task-specific learners. Second, we present MT-net where gradient updates in the T-net are guided by a binary mask M that is meta-learned, restricting the updates to be performed in a subspace. Empirical results demonstrate that our method is less sensitive to the choice of initial learning rates than existing meta-learning methods, and achieves the state-of-the-art or comparable performance on few-shot classification and regression tasks.', 'corpus_id': 125734953, 'score': 1}, {'doc_id': '221187086', 'title': 'Does MAML really want feature reuse only?', 'abstract': 'Meta-learning, the effort to solve new tasks with only a few samples, has attracted great attention in recent years. Model Agnostic Meta-Learning (MAML) is one of the most representative gradient-based meta-learning algorithms. MAML learns new tasks with a few data samples with inner updates from a meta-initialization point and learns the meta-initialization parameters with outer updates. Recently, it has been hypothesized that feature reuse, which makes little change in efficient representations, is the dominant factor in the performance of meta-initialized model through MAML rather than rapid learning, which makes a big change in representations. In this work, we propose a novel meta-learning algorithm, coined as BOIL (Body Only update in Inner Loop), that updates only the body (extractor) of the model and freezes the head (classifier) of the model during inner loop updates. The BOIL algorithm thus heavily relies on rapid learning. Note that BOIL is the opposite direction to the hypothesis that feature reuse is more efficient than rapid learning. We validate the BOIL algorithm on various data sets and show significant performance improvement over MAML. The results imply that rapid learning in gradient-based meta-learning approaches is necessary.', 'corpus_id': 221187086, 'score': 1}, {'doc_id': '235826257', 'title': 'Parameterless Transductive Feature Re-representation for Few-Shot Learning', 'abstract': 'Recent literature in few-shot learning (FSL) has shown that transductive methods often outperform their inductive counterparts. However, most transductive solutions, particularly the meta-learning based ones, require inserting trainable parameters on top of some inductive baselines to facilitate transduction. In this paper, we propose a parameterless transductive feature re-representation framework that differs from all existing solutions from the following perspectives. (1) It is widely compatible with existing FSL methods, including meta-learning and fine tuning based models. (2) The framework is simple and introduces no extra training parameters when applied to any architecture. We conduct experiments on three benchmark datasets by applying the framework to both representative meta-learning baselines and state-ofthe-art FSL methods. Our framework consistently improves performances in all experiments and refreshes the state-of-the-art FSL results.', 'corpus_id': 235826257, 'score': 1}, {'doc_id': '235313639', 'title': 'Adversarially Adaptive Normalization for Single Domain Generalization', 'abstract': 'Single domain generalization aims to learn a model that performs well on many unseen domains with only one domain data for training. Existing works focus on studying the adversarial domain augmentation (ADA) to improve the model’s generalization capability. The impact on domain generalization of the statistics of normalization layers is still underinvestigated. In this paper, we propose a generic normalization approach, adaptive standardization and rescaling normalization (ASR-Norm), to complement the missing part in previous works. ASR-Norm learns both the standardization and rescaling statistics via neural networks. This new form of normalization can be viewed as a generic form of the traditional normalizations. When trained with ADA, the statistics in ASR-Norm are learned to be adaptive to the data coming from different domains, and hence improves the model generalization performance across domains, especially on the target domain with large discrepancy from the source domain. The experimental results show that ASR-Norm can bring consistent improvement to the state-of-the-art ADA approaches by 1.6%, 2.7%, and 6.3% averagely on the Digits, CIFAR-10-C, and PACS benchmarks, respectively. As a generic tool, the improvement introduced by ASR-Norm is agnostic to the choice of ADA methods.', 'corpus_id': 235313639, 'score': 0}, {'doc_id': '235658346', 'title': 'R-Drop: Regularized Dropout for Neural Networks', 'abstract': 'Dropout is a powerful and widely used technique to regularize the training of deep neural networks. In this paper, we introduce a simple regularization strategy upon dropout in model training, namely R-Drop, which forces the output distributions of different sub models generated by dropout to be consistent with each other. Specifically, for each training sample, R-Drop minimizes the bidirectional KL-divergence between the output distributions of two sub models sampled by dropout. Theoretical analysis reveals that R-Drop reduces the freedom of the model parameters and complements dropout. Experiments on 5 widely used deep learning tasks (18 datasets in total), including neural machine translation, abstractive summarization, language understanding, language modeling, and image classification, show that R-Drop is universally effective. In particular, it yields substantial improvements when applied to fine-tune large-scale pre-trained models, e.g., ViT, RoBERTa-large, and BART, and achieves state-of-the-art (SOTA) performances with the vanilla Transformer model on WMT14 English→German translation (30.91 BLEU) and WMT14 English→French translation (43.95 BLEU), even surpassing models trained with extra large-scale data and expert-designed advanced variants of Transformer models. Our code is available at GitHub1.', 'corpus_id': 235658346, 'score': 0}, {'doc_id': '235344366', 'title': 'Meta-learning with few-shot models Analysis Final Project', 'abstract': 'This project focuses on understanding the various elements of Meta-learning and few-shot models and the effectiveness of the different detailed implementation approaches. Using the default RobustQA project as a baseline, we explored the different implementations of the Meta-learning algorithm, LEOPARD[1], and evaluate the impact on performance of the prediction accuracy. We have also experimented with the eval-every parameter to understand how fast each implementation can learn when presented with the out of domain questions initially. We found that the multiple datasets implementation of the Leopard algorithm yields the best few-shot result. On the first evaluation at step O (after 1 batch of data for learning) this implementation already achieving a result of a EM score of 34.55 (on the validation set) compared to the 32 EM scores that the other implementation and the baseline are getting. However, after the model is trained for a longer time, we found that the baseline can actually achieve a better EM score overall with 42.202 on the test set. Although, the difference in the overall accuracy of the test set score are very small for different implementations, we found the more simple implementation yields better accuracy in the long run. Our key finding is that the design of a few-shot learning algorithm or model is actually a trade off between few-shot accuracy and the overall highest achievable accuracy. 1 Key Information to include ¢ Mentor: Rui Wang (ruil @stanford.edu) ¢ External Collaborators (if you have any): None ¢ Sharing project: None', 'corpus_id': 235344366, 'score': 0}, {'doc_id': '235422615', 'title': 'Knowledge Consolidation based Class Incremental Online Learning with Limited Data', 'abstract': 'We propose a novel approach for class incremental online learning in a limited data setting. This problem setting is challenging because of the following constraints: (1) Classes are given incrementally, which necessitates a class incremental learning approach; (2) Data for each class is given in an online fashion, i.e., each training example is seen only once during training; (3) Each class has very few training examples; and (4) We do not use or assume access to any replay/memory to store data from previous classes. Therefore, in this setting, we have to handle twofold problems of catastrophic forgetting and overfitting. In our approach, we learn robust representations that are generalizable across tasks without suffering from the problems of catastrophic forgetting and overfitting to accommodate future classes with limited samples. Our proposed method leverages the meta-learning framework with knowledge consolidation. The meta-learning framework helps the model for rapid learning when samples appear in an online fashion. Simultaneously, knowledge consolidation helps to learn a robust representation against forgetting under online updates to facilitate future learning. Our approach significantly outperforms other methods on several benchmarks.', 'corpus_id': 235422615, 'score': 0}, {'doc_id': '3503217', 'title': 'Continuous Adaptation via Meta-Learning in Nonstationary and Competitive Environments', 'abstract': 'Ability to continuously learn and adapt from limited experience in nonstationary environments is an important milestone on the path towards general intelligence. In this paper, we cast the problem of continuous adaptation into the learning-to-learn framework. We develop a simple gradient-based meta-learning algorithm suitable for adaptation in dynamically changing and adversarial scenarios. Additionally, we design a new multi-agent competitive environment, RoboSumo, and define iterated adaptation games for testing various aspects of continuous adaptation strategies. We demonstrate that meta-learning enables significantly more efficient adaptation than reactive baselines in the few-shot regime. Our experiments with a population of agents that learn and compete suggest that meta-learners are the fittest.', 'corpus_id': 3503217, 'score': 1}]
193	{'doc_id': '39012603', 'title': 'Profilin regulates F-actin network homeostasis by favoring formin over Arp2/3 complex.', 'abstract': 'Fission yeast cells use Arp2/3 complex and formin to assemble diverse filamentous actin (F-actin) networks within a common cytoplasm for endocytosis, division, and polarization. Although these homeostatic F-actin networks are usually investigated separately, competition for a limited pool of actin monomers (G-actin) helps to regulate their size and density. However, the mechanism by which G-actin is correctly distributed between rival F-actin networks is not clear. Using a combination of cell biological approaches and in vitro reconstitution of competition between actin assembly factors, we found that the small G-actin binding protein profilin directly inhibits Arp2/3 complex-mediated actin assembly. Profilin is therefore required for formin to compete effectively with excess Arp2/3 complex for limited G-actin and to assemble F-actin for contractile ring formation in dividing cells.', 'corpus_id': 39012603}	2212	"[{'doc_id': '211004037', 'title': 'Leader-cell-driven epithelial sheet fingering.', 'abstract': 'Collective cell migration is crucial in many biological processes such as wound healing, tissue morphogenesis, and tumor progression. The leading front of a collective migrating epithelial cell layer often destabilizes into multicellular finger-like protrusions, each of which is guided by a leader cell at the fingertip. Here, we develop a subcellular-element-based model of this fingering instability, which incorporates leader cells and other related properties of a monolayer of epithelial cells. Our model recovers multiple aspects of the dynamics, especially the traction force patterns and velocity fields, observed in experiments on MDCK cells. Our model predicts the necessity of the leader cell and its minimal functions for the formation and maintenance of a stable finger pattern. Meanwhile, our model allows for an analysis of the role of supra-cellular actin cable on the leading front, predicting that while this observed structure helps maintain the shape of the finger, it is not required in order to form a finger. In addition, we also study the driving instability in the context of continuum active fluid model, which justifies some of our assumptions in the computational approach. In particular, we show that in our model no finger protrusions would emerge in a phenotypically homogenous active fluid and hence the role of the leader cell and its followers are often critical.', 'corpus_id': 211004037, 'score': 0}, {'doc_id': '210991189', 'title': 'Icariin promotes osteogenic differentiation of BMSCs by upregulating BMAL1 expression via BMP signaling', 'abstract': 'Increasing research has demonstrated that expression of brain and muscle ARNT-like 1 (BMAL1) and other circadian clock genes can be regulated by drugs and toxicants. We previously demonstrated that icariin, extracted from Herba Epimedii, sromotes osteogenic differentiation. However, the mechanism underlying the association between icariin and BMAL1 in osteogenic differentiation of bone marrow-derived mesenchymal stem cells (BMSCs) remains unclear. The present study was designed with an aim to clarify the association between icariin and BMAL1 in osteogenic differentiation of BMSCs. The Cell Counting Kit-8 assay was used to evaluate cell proliferation. The expression of bone morphogenetic protein 2 (BMP2), RUNX family transcription factor 2 (RUNX2), alkaline phosphatase (ALP), osteocalcin (OC) and BMAL1 in BMSCs was evaluated by reverse transcription-quantitative PCR and western blotting. ALP and Alizarin red S (ARS) staining were also performed. Icariin promoted BMSC proliferation, and upregulated expression of osteogenic genes and BMAL1. In addition, expression of the osteogenic genes BMP2, RUNX2, ALP and OC were upregulated by BMAL1 overexpression. Furthermore, we confirmed that BMAL1 deficiency suppressed osteogenic differentiation in BMSCs. Finally, ARS staining of BMAL1−/− BMSCs revealed that BMAL1 was an essential intermediary in matrix mineralization during osteogenic differentiation. In conclusion, these results demonstrated that icariin promoted osteogenic differentiation through BMAL1-BMP2 signaling in BMSCs. The present study thus described a novel target of icariin that has potential applications in the treatment of osteogenic disorders.', 'corpus_id': 210991189, 'score': 0}, {'doc_id': '206659280', 'title': 'Vinculin forms a directionally asymmetric catch bond with F-actin', 'abstract': ""Making the right catch Tension reveals cryptic vinculin-binding sites on α-catenin and talin at cadherin-based cell-cell and integrin-based cell-matrix adhesions, respectively. The enrichment of vinculin at cellular adhesions is thus an indicator of load-induced reinforcement of the cytoskeletal linkage. Huang et al. used a single-molecule optical trap assay to measure the binding lifetimes of vinculin to single actin filaments under load. The vinculin-F-actin interaction formed a directional catch bond—one that is very weak at low force but that greatly increases in lifetime with increasing force. This explains vinculin's role as a reinforcing linker at both cell-cell and cell-matrix adhesions. Science, this issue p. 703 The cell adhesion protein vinculin forms a load- and direction-dependent bond with actin filaments. Vinculin is an actin-binding protein thought to reinforce cell-cell and cell-matrix adhesions. However, how mechanical load affects the vinculin–F-actin bond is unclear. Using a single-molecule optical trap assay, we found that vinculin forms a force-dependent catch bond with F-actin through its tail domain, but with lifetimes that depend strongly on the direction of the applied force. Force toward the pointed (–) end of the actin filament resulted in a bond that was maximally stable at 8 piconewtons, with a mean lifetime (12 seconds) 10 times as long as the mean lifetime when force was applied toward the barbed (+) end. A computational model of lamellipodial actin dynamics suggests that the directionality of the vinculin–F-actin bond could establish long-range order in the actin cytoskeleton. The directional and force-stabilized binding of vinculin to F-actin may be a mechanism by which adhesion complexes maintain front-rear asymmetry in migrating cells."", 'corpus_id': 206659280, 'score': 1}, {'doc_id': '39583135', 'title': 'Differential Transmission of Actin Motion Within Focal Adhesions', 'abstract': 'Cell migration requires the transmission of motion generated in the actin cytoskeleton to the extracellular environment through a complex assembly of proteins in focal adhesions. We developed correlational fluorescent speckle microscopy to measure the coupling of focal-adhesion proteins to actin filaments. Different classes of focal-adhesion structural and regulatory molecules exhibited varying degrees of correlated motions with actin filaments, indicating hierarchical transmission of actin motion through focal adhesions. Interactions between vinculin, talin, and actin filaments appear to constitute a slippage interface between the cytoskeleton and integrins, generating a molecular clutch that is regulated during the morphodynamic transitions of cell migration.', 'corpus_id': 39583135, 'score': 1}, {'doc_id': '211068886', 'title': 'The architecture of co-culture spheroids regulates tumor invasion within a 3D extracellular matrix.', 'abstract': 'Tumor invasion, the process by which tumor cells break away from their primary tumor and gain access to vascular systems, is an important step in cancer metastasis. Most current 3D tumor invasion assays consisted of single tumor cells embedded within an extracellular matrix (ECM). These assays taught us much of what we know today on how key biophysical (e.g. ECM stiffness) and biochemical (e.g. cytokine gradients) parameters within the tumor microenvironment guided and regulated tumor invasion. One limitation of the single tumor cell invasion assay was that it did not account for cell-cell adhesion within the tumor. In this article, we developed a micrometer scale 3D co-culture spheroid invasion assay that was compatible with microscopic imaging. Micrometer scale co-culture spheroids (1:1 ratio of metastatic breast cancer MDA-MB-231 and non-tumorigenic epithelial MCF-10A cells) were made using an array of microwells, and then were embedded within a collagen matrix in a microfluidic platform. Real time imaging of tumor spheroid invasion revealed that the spatial distribution of the two cell types within the tumor spheroid critically regulated tumor invasion. This work linked tumor architecture with tumor invasion and highlighted the importance of the biophysical cues within the bulk of the tumor in tumor invasion.', 'corpus_id': 211068886, 'score': 0}, {'doc_id': '42640963', 'title': 'Nonlinear Elasticity and an 8-nm Working Stroke of Single Myosin Molecules in Myofilaments', 'abstract': ""Measuring Single Myosins at Work In the past 15 years, the molecular mechanism of muscle contraction has been investigated at the single-molecule level; however, results have varied between laboratories because of the nonprocessive properties of skeletal myosin. Now, Kaya and Higuchi (p. 686) have measured the nonlinear elasticity and working stroke size of single skeletal myosins by combining optical trapping and fluorescence imaging with subnanometer accuracy. The data suggest that it is important to relate myosin's internal structural changes to physiological force generation and filament sliding. Single-molecule measurements refine our understanding of how muscle myosin works. Using optical trapping and fluorescence imaging techniques, we measured the step size and stiffness of single skeletal myosins interacting with actin filaments and arranged on myosin-rod cofilaments that approximate myosin mechanics during muscle contraction. Stiffness is dramatically lower for negatively compared to positively strained myosins, consistent with buckling of myosin’s subfragment 2 rod domain. Low stiffness minimizes drag of negatively strained myosins during contraction at loaded conditions. Myosin's elastic portion is stretched during active force generation, reducing apparent step size with increasing load, even though the working stroke is approximately constant at about 8 nanometers. Taking account of the nonlinear nature of myosin elasticity is essential to relate myosin’s internal structural changes to physiological force generation and filament sliding."", 'corpus_id': 42640963, 'score': 1}, {'doc_id': '4458133', 'title': 'Structure of the F-actin–tropomyosin complex', 'abstract': 'Filamentous actin (F-actin) is the major protein of muscle thin filaments, and actin microfilaments are the main component of the eukaryotic cytoskeleton. Mutations in different actin isoforms lead to early-onset autosomal dominant non-syndromic hearing loss, familial thoracic aortic aneurysms and dissections, and multiple variations of myopathies. In striated muscle fibres, the binding of myosin motors to actin filaments is mainly regulated by tropomyosin and troponin. Tropomyosin also binds to F-actin in smooth muscle and in non-muscle cells and stabilizes and regulates the filaments there in the absence of troponin. Although crystal structures for monomeric actin (G-actin) are available, a high-resolution structure of F-actin is still missing, hampering our understanding of how disease-causing mutations affect the function of thin muscle filaments and microfilaments. Here we report the three-dimensional structure of F-actin at a resolution of 3.7\xa0Å in complex with tropomyosin at a resolution of 6.5\xa0Å, determined by electron cryomicroscopy. The structure reveals that the D-loop is ordered and acts as a central region for hydrophobic and electrostatic interactions that stabilize the F-actin filament. We clearly identify map density corresponding to ADP and Mg2+ and explain the possible effect of prominent disease-causing mutants. A comparison of F-actin with G-actin reveals the conformational changes during filament formation and identifies the D-loop as their key mediator. We also confirm that negatively charged tropomyosin interacts with a positively charged groove on F-actin. Comparison of the position of tropomyosin in F-actin–tropomyosin with its position in our previously determined F-actin–tropomyosin–myosin structure reveals a myosin-induced transition of tropomyosin. Our results allow us to understand the role of individual mutations in the genesis of actin- and tropomyosin-related diseases and will serve as a strong foundation for the targeted development of drugs.', 'corpus_id': 4458133, 'score': 1}, {'doc_id': '211531426', 'title': 'Protein Trafficking in the Biosynthetic Pathway', 'abstract': 'In the absence of specific mechanisms to recognize and sequester cargo destined for transport and retrieval, communicating organelles would quickly lose their identity. The biosynthetic pathway, comprising of the endoplasmic reticulum, the Golgi complex, vesicular-tubular clusters (also known as ERGIC or pre-Golgi intermediates) and vesicular/tubular transport carriers has been a frequent target for the examination of such transport processes. In the biosynthetic pathway, proteins are directed to the endoplasmic reticulum by an N-terminal signal peptide. Within the lumen of the ER, proteins may be proteolytically processed, folded, glycosylated and assembled into their tertiary or quaternary structures. Improperly folded proteins are passed by retrograde transport through the ER translocon to be degraded by a proteasome. Correctly folded proteins are packaged into vesicles or tubules at ER exit sites and transported to the Golgi apparatus where further maturation occurs. Though many questions remain regarding mechanisms of protein retention, sorting, packaging, and recycling in the biosynthetic pathway, new discoveries through complementary methods have assisted in the clarification of a number of the steps involved. The Endoplasmic Reticulum Structure and Function The endoplasmic reticulum (ER), originally discovered by Porter and coworkers in 1945 (Palade 1975), is the first organelle in the biosynthetic pathway (Figure 1). The ER is a labyrinthine array of membrane-bounded tubules and cisternae which extend throughout the cell. The position and organization of the ER has been demonstrated to depend, at least in part, on microtubules (Lee et al., 1989, Lane and Allan 1998, Marsh et al., 2001). The functions of the ER include: protein synthesis, folding, assembly, and degradation; lipid biosynthesis and metabolism; detoxification; nucleus compartmentalization; ion gradient retention; and membrane transport (Rooney and Meldolesi 1996, Lippincott-Schwartz et al., 2000). ER membranes form a continuous interconnected system of rough (RER) and smooth (SER) regions, depending on whether ribosomes are associated with their cytoplasmic surfaces (Cole et al., 1996a, Dayel et al., 1999). RER is the site of cotranslational or posttranslational protein insertion, while SER is proposed to be the site of lipid biosynthesis and detoxification (Amar-Costesec et al., 1984, Lippincott-Schwartz et al., 2000). The RER represents the entry point for newly synthesized proteins destined for the biosynthetic and secretory pathways. The process of how the appropriate proteins are targeted to the ER, and thus, the true starting point of research on protein trafficking, is • MALDI-TOF Mass Spectrometry in Microbiology Edited by: M Kostrzewa, S Schubert (2016) www.caister.com/malditof • Aspergillus and Penicillium in the Post-genomic Era Edited by: RP Vries, IB Gelber, MR Andersen (2016) www.caister.com/aspergillus2 • The Bacteriocins: Current Knowledge and Future Prospects Edited by: RL Dorit, SM Roy, MA Riley (2016) www.caister.com/bacteriocins • Omics in Plant Disease Resistance Edited by: V Bhadauria (2016) www.caister.com/opdr • Acidophiles: Life in Extremely Acidic Environments Edited by: R Quatrini, DB Johnson (2016) www.caister.com/acidophiles • Climate Change and Microbial Ecology: Current Research and Future Trends Edited by: J Marxsen (2016) www.caister.com/climate • Biofilms in Bioremediation: Current Research and Emerging Technologies Edited by: G Lear (2016) www.caister.com/biorem • Microalgae: Current Research and Applications Edited by: MN Tsaloglou (2016) www.caister.com/microalgae • Gas Plasma Sterilization in Microbiology: Theory, Applications, Pitfalls and New Perspectives Edited by: H Shintani, A Sakudo (2016) www.caister.com/gasplasma • Virus Evolution: Current Research and Future Directions Edited by: SC Weaver, M Denison, M Roossinck, et al. (2016) www.caister.com/virusevol • Arboviruses: Molecular Biology, Evolution and Control Edited by: N Vasilakis, DJ Gubler (2016) www.caister.com/arbo • Shigella: Molecular and Cellular Biology Edited by: WD Picking, WL Picking (2016) www.caister.com/shigella • Aquatic Biofilms: Ecology, Water Quality and Wastewater Treatment Edited by: AM Romaní, H Guasch, MD Balaguer (2016) www.caister.com/aquaticbiofilms • Alphaviruses: Current Biology Edited by: S Mahalingam, L Herrero, B Herring (2016) www.caister.com/alpha • Thermophilic Microorganisms Edited by: F Li (2015) www.caister.com/thermophile • Flow Cytometry in Microbiology: Technology and Applications Edited by: MG Wilkinson (2015) www.caister.com/flow • Probiotics and Prebiotics: Current Research and Future Trends Edited by: K Venema, AP Carmo (2015) www.caister.com/probiotics • Epigenetics: Current Research and Emerging Trends Edited by: BP Chadwick (2015) www.caister.com/epigenetics2015 • Corynebacterium glutamicum: From Systems Biology to Biotechnological Applications Edited by: A Burkovski (2015) www.caister.com/cory2 • Advanced Vaccine Research Methods for the Decade of Vaccines Edited by: F Bagnoli, R Rappuoli (2015) www.caister.com/vaccines • Antifungals: From Genomics to Resistance and the Development of Novel Agents Edited by: AT Coste, P Vandeputte (2015) www.caister.com/antifungals • Bacteria-Plant Interactions: Advanced Research and Future Trends Edited by: J Murillo, BA Vinatzer, RW Jackson, et al. (2015) www.caister.com/bacteria-plant • Aeromonas Edited by: J Graf (2015) www.caister.com/aeromonas • Antibiotics: Current Innovations and Future Trends Edited by: S Sánchez, AL Demain (2015) www.caister.com/antibiotics • Leishmania: Current Biology and Control Edited by: S Adak, R Datta (2015) www.caister.com/leish2 • Acanthamoeba: Biology and Pathogenesis (2nd edition) Author: NA Khan (2015) www.caister.com/acanthamoeba2 • Microarrays: Current Technology, Innovations and Applications Edited by: Z He (2014) www.caister.com/microarrays2 • Metagenomics of the Microbial Nitrogen Cycle: Theory, Methods and Applications Edited by: D Marco (2014) www.caister.com/n2 Caister Academic Press is a leading academic publisher of advanced texts in microbiology, molecular biology and medical research. Full details of all our publications at caister.com Further Reading Order from caister.com/order explained by the signal hypothesis. This hypothesis postulates that an amino-terminal leader peptide serves as a signal that directs the protein to the ER and then is cleaved off by a signal peptidase in the ER membrane before the polypeptide chain is fully synthesized (Blobel and Dobberstein 1975, Walter and Johnson 1994). In mammalian cells, greater detail has been elucidated. The ER signal peptide on the nascent polypeptide chain is bound by the signal-recognition particle (SRP), which cycles between the ER membrane and the cytosol. The binding of the SRP to the signal peptide and ribosome induces a pause in translation and permits the binding of the SRP-ribosome complex to the SRP receptor, which is located adjacent to the translocon. The translocon is a complex molecular machine that regulates the movement of polypeptides in both directions through the ER bilayer while maintaining the membrane permeability barrier. Once the SRP-ribosome complex is bound to the SRP receptor, the nascent chain is inserted into the aqueous translocon pore and a GTP-dependent interaction of SRP with its receptor triggers the release of SRP from the ribosome and the SRP receptor from the translocon (Bacher et al., 1996). The nascent chain is prevented by Binding Protein (BiP) from passing through the luminal end of the pore until the chain reaches ~ 70 amino acids in length (Crowley et al., 1994). The delayed release of BiP and the final opening of the pore may constitute a safety mechanism to ensure that one end of the pore is not opened before the other end is completely sealed. The release of the BiP may be elicited after the unfolded nascent chain becomes long enough to bind to BiP, alter its conformation, and release BiP from the translocon. Cotranslational protein translocation then proceeds through the aqueous pore, which is now sealed at its cytoplasmic end by tight binding of the ribosome to the translocon. When the chain length totals ~150 residues, the signal peptidase cleaves off the leader peptide. After termination of translation, the ribosome is released and the pore contracts, sealed on its luminal side by BiP (Hamman et al., 1998, Johnson and van Waes 1999). For soluble proteins, cleavage of the signal peptide releases the polypeptide into the ER lumen. Other proteins remain anchored to the phospholipid bilayer by a covalently attatched glycosylphosphatidylinositol (GPI) membrane anchor. These proteins are initially anchored to the ER membrane by an internal stop-transfer membrane anchor sequence. However, a short sequence of amino acids in the exoplasmic domain adjacent to the membrane-spanning domain is recognized by an endoprotease that simultaneously cleaves off the stop-transfer membrane-anchor sequence and transfers the remainder of the protein to a pre-formed GPI anchor in the membrane (Kodukula et al., 1992). In contrast, single or multiple-pass integral membrane proteins remain in the ER membrane after cleavage by the signal peptidase due to the presence of uncleaved stop-transfer membrane-anchor sequences or internal signal anchor sequences. Because membrane proteins are always inserted from the cytosolic side of the ER in a programmed manner, all copies of the same polypeptide chain will have the same orientation in the lipid bilayer. This generates an asymmetrical ER membrane in which the protein domains exposed on one side differ from those exposed on the other. This asymmetry is maintained as proteins made in the ER are transported to other cell membranes (Lodish et al., 2000). Quality Control Once in the ER, proteins are covalently modified (N-glycosylation, oligosaccharide trimming, formation of disulfide bonds) and acqu', 'corpus_id': 211531426, 'score': 0}, {'doc_id': '211086643', 'title': 'Regulation of Nucleotide Metabolism and Germline Proliferation in Response to Nucleotide Imbalance and Genotoxic Stresses by EndoU Nuclease', 'abstract': 'SUMMARY Nucleotide deprivation and imbalance present detrimental conditions for animals and are thus expected to trigger cellular responses that direct protective changes in metabolic, developmental, and behavioral programs, albeit such mechanisms are vastly underexplored. Following our previous finding that Caenorhabditis elegans shut down germ cell proliferation in response to pyrimidine deprivation, we find in this study that endonuclease ENDU-2 regulates nucleotide metabolism and germ cell proliferation in response to nucleotide imbalance and other genotoxic stress, and that it affects mitotic chromosomal segregation in the intestine and lifespan. ENDU-2 expression is induced by nucleotide imbalance and genotoxic stress, and ENDU-2 exerts its function in the intestine, mostly by inhibiting the phosphorylation of CTPS-1 through repressing the PKA pathway and histone deacetylase HDA-1. Human EndoU also affects the response to genotoxic drugs. Our work reveals an unknown role of ENDU-2 in regulating nucleotide metabolism and animals’ response to genotoxic stress, which may link EndoU function to cancer treatment.', 'corpus_id': 211086643, 'score': 0}, {'doc_id': '6635407', 'title': 'Inter-Cellular Forces Orchestrate Contact Inhibition of Locomotion', 'abstract': 'Summary Contact inhibition of locomotion (CIL) is a multifaceted process that causes many cell types to repel each other upon collision. During development, this seemingly uncoordinated reaction is a critical driver of cellular dispersion within embryonic tissues. Here, we show that Drosophila hemocytes require a precisely orchestrated CIL response for their developmental dispersal. Hemocyte collision and subsequent repulsion involves a stereotyped sequence of kinematic stages that are modulated by global changes in cytoskeletal dynamics. Tracking actin retrograde flow within hemocytes in vivo reveals synchronous reorganization of colliding actin networks through engagement of an inter-cellular adhesion. This inter-cellular actin-clutch leads to a subsequent build-up in lamellar tension, triggering the development of a transient stress fiber, which orchestrates cellular repulsion. Our findings reveal that the physical coupling of the flowing actin networks during CIL acts as a mechanotransducer, allowing cells to haptically sense each other and coordinate their behaviors.', 'corpus_id': 6635407, 'score': 1}]"
194	{'doc_id': '39219588', 'title': 'On Bidirectional Transitions between Localist and Distributed Representations: The Case of Common Substrings Search Using Vector Symbolic Architecture', 'abstract': 'The contribution of this article is twofold. First, it presents an encoding approach for seamless bidirectional transitions between localist and distributed representation domains. Second, the appr ...', 'corpus_id': 39219588}	2774	[{'doc_id': '12066798', 'title': 'An Analysis of ACME : The Analogical Constraint Mapping Engine.', 'abstract': 'This paper discusses s ome pragmatic issues on the a nalogical constraint m apping engine (ACME), a widely u sed artificial neural network model for analogical matching employing Grossbergs IAC (interactive Activation and Competition) artificial neural network model. Our analysis beings with an investigation into the use of a Hopfield constraint satisfaction n etwork for image reconstruction. This domain providing us with a constraint satisfaction network based on the more usual Hopfield model, but which shares many characteristics with the ACME model. Based on this comparison we demonstrate how the c onvergence time increases exponentially with increasing n etwork size, a factor which h as s erious implications for the practical application o f this model t o all but t rivially small m etaphors. Finally, we briefly present a localist connectionist m odel for solution g eneration in on e domain, with linear scaleability over problem size.', 'corpus_id': 12066798, 'score': 1}, {'doc_id': '237149108', 'title': 'Near-channel classifier: symbiotic communication and classification in high-dimensional space', 'abstract': 'Brain-inspired high-dimensional (HD) computing represents and manipulates data using very long, random vectors with dimensionality in the thousands. This representation provides great robustness for various classification tasks where classifiers operate at low signal-to-noise ratio (SNR) conditions. Similarly, hyperdimensional modulation (HDM) leverages the robustness of complex-valued HD representations to reliably transmit information over a wireless channel, achieving a similar SNR gain compared to state-of-the-art codes. Here, we first propose methods to improve HDM in two ways: (1) reducing the complexity of encoding and decoding operations by generating, manipulating, and transmitting bipolar or integer vectors instead of complex vectors; (2) increasing the SNR gain by 0.2\xa0dB using a new soft-feedback decoder; it can also increase the additive superposition capacity of HD vectors up to 1.7 $$\\times$$ × in noise-free cases. Secondly, we propose to combine encoding/decoding aspects of communication with classification into a single framework by relying on multifaceted HD representations. This leads to a near-channel classification (NCC) approach that avoids transformations between different representations and the overhead of multiple layers of encoding/decoding, hence reducing latency and complexity of a wireless smart distributed system while providing robustness against noise and interference from other nodes. We provide a use-case for wearable hand gesture recognition with 5 classes from 64 EMG sensors, where the encoded vectors are transmitted to a remote node for either performing NCC, or reconstruction of the encoded data. In NCC mode, the original classification accuracy of 94% is maintained, even in the channel at SNR of 0\xa0dB, by transmitting 10,000-bit vectors. We remove the redundancy by reducing the vector dimensionality to 2048-bit that still exhibits a graceful degradation: less than 6% accuracy loss is occurred in the channel at −\xa05\xa0dB, and with the interference from 6 nodes that simultaneously transmit their encoded vectors. In the reconstruction mode, it improves the mean-squared error by up to 20\xa0dB, compared to standard decoding, when transmitting 2048-dimensional vectors.', 'corpus_id': 237149108, 'score': 1}, {'doc_id': '232289618', 'title': 'Hyperdimensional Computing and Spectral Learning', 'abstract': 'Hyperdimensional (HD) computing is an emerging paradigm for machine learning based on biologically plausible models of memory. HD computing represents data as random points with i.i.d. components in a high-dimensional, low-precision, space. These points are then used as input to learning algorithms. The i.i.d. and low-precision nature of HD representations accord naturally with highly parallel hardware like FPGAs and PIM architectures and HD computing has recently gained significant popularity in the hardware community [1]–[3]. Existing work on HD computing has focused primarily on learning linear functions over the encodings. However, much data of practical interest is nonlinear, meaning that linear methods may fail to capture important structure in the data. Our goal is to assess the utility of using nonlinear learning algorithms on HD representations of data. A complication arises because nonlinear models are generally more complex computationally than simple linear methods which are traditionally used with HD computing. In this work, we show how nonlinear learning on HD representations can be formulated as a sparse convex optimization problem using the Boolean Fourier transform. We additionally present results from a preliminary empirical study motivated by this formulation designed to assess the possible practical benefits of this approach.', 'corpus_id': 232289618, 'score': 1}, {'doc_id': '232325097', 'title': 'From convolutional neural networks to models of higher-level cognition (and back again).', 'abstract': 'The remarkable successes of convolutional neural networks (CNNs) in modern computer vision are by now well known, and they are increasingly being explored as computational models of the human visual system. In this paper, we ask whether CNNs might also provide a basis for modeling higher-level cognition, focusing on the core phenomena of similarity and categorization. The most important advance comes from the ability of CNNs to learn high-dimensional representations of complex naturalistic images, substantially extending the scope of traditional cognitive models that were previously only evaluated with simple artificial stimuli. In all cases, the most successful combinations arise when CNN representations are used with cognitive models that have the capacity to transform them to better fit human behavior. One consequence of these insights is a toolkit for the integration of cognitively motivated constraints back into CNN training paradigms in computer vision and machine learning, and we review cases where this leads to improved performance. A second consequence is a roadmap for how CNNs and cognitive models can be more fully integrated in the future, allowing for flexible end-to-end algorithms that can learn representations from data while still retaining the structured behavior characteristic of human cognition.', 'corpus_id': 232325097, 'score': 0}, {'doc_id': '233168647', 'title': 'Recognizing and Verifying Mathematical Equations using Multiplicative Differential Neural Units', 'abstract': 'Automated mathematical reasoning is a challenging problem that requires an agent to learn algebraic patterns that contain long-range dependencies. Two particular tasks that test this type of reasoning are (1) mathematical equation verification, which requires determining whether trigonometric and linear algebraic statements are valid identities or not, and (2) equation completion, which entails filling in a blank within an expression to make it true. Solving these tasks with deep learning requires that the neural model learn how to manipulate and compose various algebraic symbols, carrying this ability over to previously unseen expressions. Artificial neural networks, including recurrent networks and transformers, struggle to generalize on these kinds of difficult compositional problems, often exhibiting poor extrapolation performance. In contrast, recursive neural networks (recursive-NNs) are, theoretically, capable of achieving better extrapolation due to their tree-like design but are difficult to optimize as the depth of their underlying tree structure increases. To overcome this issue, we extend recursive-NNs to utilize multiplicative, higher-order synaptic connections and, furthermore, to learn to dynamically control and manipulate an external memory. We argue that this key modification gives the neural system the ability to capture powerful transition functions for each possible input. We demonstrate the effectiveness of our proposed higher-order, memory-augmented recursive-NN models on two challenging mathematical equation tasks, showing improved extrapolation, stable performance, and faster convergence. Our models achieve a 1.53% average improvement over current state-of-the-art methods in equation verification and achieve a 2.22% Top-1 average accuracy and 2.96% Top5 average accuracy for equation completion.', 'corpus_id': 233168647, 'score': 0}, {'doc_id': '233219674', 'title': 'Neuro-Symbolic VQA: A review from the perspective of AGI desiderata', 'abstract': 'An ultimate goal of the AI and ML fields is artificial general intelligence (AGI); although such systems remain science fiction, various models exhibit aspects of AGI. In this work, we look at neuro-symbolic (NS) approaches to visual question answering (VQA) from the perspective of AGI desiderata. We see how well these systems meet these desiderata, and how the desiderata often pull the scientist in opposing directions. It is my hope that through this work we can temper model evaluation on benchmarks with a discussion of the properties of these systems and their potential for future extension.', 'corpus_id': 233219674, 'score': 0}, {'doc_id': '233762187', 'title': 'Precis of A Bayesian account of learning algorithms and generalising representations in the brain', 'abstract': 'Without learning we would be limited to a set of preprogrammed behaviours. While that may be acceptable for flies1, it does not provide the basis for adaptive or intelligent behaviours familiar to humans. Learning, then, is one of the crucial components of brain operation. Learning, however, takes time. Thus, the key to adaptive behaviour is learning to systematically generalise; that is, have learned knowledge that can be flexibly recombined to understand any world in front of you. This thesis attempts to make inroads on two questions how can brain networks learn, and what are the principles behind representations of knowledge that allow generalisation. With the industrialisation of science, the twentieth century bore fruit in the form of an increasingly detailed understanding of neurons, synapses, neurotransmitters, resting potentials, action potentials, networks and so on (1–4). Though we have gained a great level of detail about many of these micro-processes as well as high-level understandings of intelligence thanks to philosophy, experimental psychology, and behavioural and cognitive neuroscience (5–9) a large gulf of understanding remains between these levels of granularity. This thesis focuses on spanning this gap by providing high-level computational frameworks that translate to low-level processes. Any high-level brain framework must have successful behaviour at its heart as that is the role of the brain. Analogously, neurons are central to low-level understanding as the basis of brain function is believed to be the transfer of information between neurons, mediated via weighted connections. Different weights lead to different functions. Thus, learning appropriate configurations of weights is the fundamental problem facing brains. There are two facets to this learning the first is how, and the second is what. The how are the learning algorithms that determine updates to these synaptic connections, and the what are the neural representations that reflect how the world works. In this vein, this thesis examines 1) the algorithmic implementation of learning in biological neural networks, and 2) a computational framework for the neural representations of task generalisation. Both these research directions are bound together by Bayesian thinking, and both of these pieces of work bridge the gap between highand lowlevel understanding, as well as between brains and machines.', 'corpus_id': 233762187, 'score': 0}, {'doc_id': '2352281', 'title': 'Holographic reduced representations', 'abstract': 'Associative memories are conventionally used to represent data with very simple structure: sets of pairs of vectors. This paper describes a method for representing more complex compositional structure in distributed representations. The method uses circular convolution to associate items, which are represented by vectors. Arbitrary variable bindings, short sequences of various lengths, simple frame-like structures, and reduced representations can be represented in a fixed width vector. These representations are items in their own right and can be used in constructing compositional structures. The noisy reconstructions extracted from convolution memories can be cleaned up by using a separate associative memory that has good reconstructive properties.', 'corpus_id': 2352281, 'score': 1}, {'doc_id': '203606064', 'title': 'Compensating Supervision Incompleteness with Prior Knowledge in Semantic Image Interpretation', 'abstract': 'Semantic Image Interpretation is the task of extracting a structured semantic description from images. This requires the detection of visual relationships: triples 〈subject, relation, object〉 describing a semantic relation between a subject and an object. A pure supervised approach to visual relationship detection requires a complete and balanced training set for all the possible combinations of 〈subject, relation, object〉. However, such training sets are not available and would require a prohibitive human effort. This implies the ability of predicting triples which do not appear in the training set. This problem is called zero-shot learning. State-of-the-art approaches to zero-shot learning exploit similarities among relationships in the training set or external linguistic knowledge. In this paper, we perform zero-shot learning by using Logic Tensor Networks, a novel Statistical Relational Learning framework that exploits both the similarities with other seen relationships and background knowledge, expressed with logical constraints between subjects, relations and objects. The experiments on the Visual Relationship Dataset show that the use of logical constraints outperforms the current methods. This implies that background knowledge can be used to alleviate the incompleteness of training sets.', 'corpus_id': 203606064, 'score': 1}, {'doc_id': '233241202', 'title': 'Syntactic Perturbations Reveal Representational Correlates of Hierarchical Phrase Structure in Pretrained Language Models', 'abstract': 'While vector-based language representations from pretrained language models have set a new standard for many NLP tasks, there is not yet a complete accounting of their inner workings. In particular, it is not entirely clear what aspects of sentence-level syntax are captured by these representations, nor how (if at all) they are built along the stacked layers of the network. In this paper, we aim to address such questions with a general class of interventional, input perturbation-based analyses of representations from pretrained language models. Importing from computational and cognitive neuroscience the notion of representational invariance, we perform a series of probes designed to test the sensitivity of these representations to several kinds of structure in sentences. Each probe involves swapping words in a sentence and comparing the representations from perturbed sentences against the original. We experiment with three different perturbations: (1) random permutations of n-grams of varying width, to test the scale at which a representation is sensitive to word position; (2) swapping of two spans which do or do not form a syntactic phrase, to test sensitivity to global phrase structure; and (3) swapping of two adjacent words which do or do not break apart a syntactic phrase, to test sensitivity to local phrase structure. Results from these probes collectively suggest that Transformers build sensitivity to larger parts of the sentence along their layers, and that hierarchical phrase structure plays a role in this process. More broadly, our results also indicate that structured input perturbations widens the scope of analyses that can be performed on often-opaque deep learning systems, and can serve as a complement to existing tools (such as supervised linear probes) for interpreting complex black-box models.', 'corpus_id': 233241202, 'score': 0}]
195	{'doc_id': '237452250', 'title': 'SORNet: Spatial Object-Centric Representations for Sequential Manipulation', 'abstract': 'Sequential manipulation tasks require a robot to perceive the state of an environment and plan a sequence of actions leading to a desired goal state, where the ability to reason about spatial relationships among object entities from raw sensor inputs is crucial. Prior works relying on explicit state estimation or endto-end learning struggle with novel objects. In this work, we propose SORNet (Spatial Object-Centric Representation Network), which extracts object-centric representations from RGB images conditioned on canonical views of the objects of interest. We show that the object embeddings learned by SORNet generalize zero-shot to unseen object entities on three spatial reasoning tasks: spatial relationship classification, skill precondition classification and relative direction regression, significantly outperforming baselines. Further, we present real-world robotic experiments demonstrating the usage of the learned object embeddings in task planning for sequential manipulation.', 'corpus_id': 237452250}	20509	[{'doc_id': '237304243', 'title': 'The Surprising Effectiveness of Visual Odometry Techniques for Embodied PointGoal Navigation', 'abstract': 'It is fundamental for personal robots to reliably navigate to a specified goal. To study this task, PointGoal navigation has been introduced in simulated Embodied AI environments. Recent advances solve this PointGoal navigation task with near-perfect accuracy (99.6% success) in photo-realistically simulated environments, assuming noiseless egocentric vision, noiseless actuation and most importantly, perfect localization. However, under realistic noise models for visual sensors and actuation, and without access to a “GPS and Compass sensor,” the 99.6%-success agents for PointGoal navigation only succeed with 0.3%.1 In this work, we demonstrate the surprising effectiveness of visual odometry for the task of PointGoal navigation in this realistic setting, i.e., with realistic noise models for perception and actuation and without access to GPS and Compass sensors. We show that integrating visual odometry techniques into navigation policies improves the state-of-the-art on the popular Habitat PointNav benchmark by a large margin, improving success from 64.5% to 71.7% while executing 6.4 times faster.', 'corpus_id': 237304243, 'score': 1}, {'doc_id': '236954782', 'title': 'NOVEL LAYOUTS USING ABSTRACT 2-D MAPS', 'abstract': 'Efficiently training agents with planning capabilities has long been one of the major challenges in decision-making. In this work, we focus on zero-shot navigation ability on a given abstract 2-D occupancy map, like human navigation by reading a paper map, by treating it as an image. To learn this ability, we need to efficiently train an agent on environments with a small proportion of training maps and share knowledge effectively across the environments. We hypothesize that model-based navigation can better adapt agent’s behaviors to a task, since it disentangles the variations in map layout and goal location and enables longer-term planning ability on novel locations compared to reactive policies. We propose to learn a hypermodel that can understand patterns from a limited number of abstract maps and goal locations, to maximize alignment between the hypermodel predictions and real trajectories to extract information from multi-task off-policy experiences, and to construct denser feedback for planners by n-step goal relabelling. We train our approach on DeepMind Lab environments with layouts from different maps, and demonstrate superior performance on zero-shot transfer to novel maps and goals.', 'corpus_id': 236954782, 'score': 0}, {'doc_id': '236922071', 'title': 'LEARNING EFFICIENT PLANNING-BASED REWARDS', 'abstract': 'Imitation learning from limited demonstrations is challenging. Most inverse reinforcement learning (IRL) methods are unable to perform as good as the demonstrator, especially in a high-dimensional environment, e.g, the Atari domain. To address this challenge, we propose a novel reward learning method, which streamlines a differential planning module with dynamics modeling. Our method learns useful planning computations with a meaningful reward function that focuses on the resulting region of an agent executing an action. Such a planning-based reward function leads to policies with better generalization ability. Empirical results with multiple network architectures and reward instances show that our method can outperform state-of-the-art IRL methods on multiple Atari games and continuous control tasks. Our method achieves performance that is averagely 1,139.1% of the demonstration.', 'corpus_id': 236922071, 'score': 0}, {'doc_id': '236924142', 'title': 'ON ( plate , table ) : 2 ON ( glass , table ) : 1 ON ( fork , table ) : 1 Ground-truth Goal VirtualHome-Social Task Demonstration', 'abstract': 'In this paper, we introduce Watch-And-Help (WAH), a challenge for testing social intelligence in agents. In WAH, an AI agent needs to help a human-like agent perform a complex household task efficiently. To succeed, the AI agent needs to i) understand the underlying goal of the task by watching a single demonstration of the human-like agent performing the same task (social perception), and ii) coordinate with the human-like agent to solve the task in an unseen environment as fast as possible (human-AI collaboration). For this challenge, we build VirtualHomeSocial, a multi-agent household environment, and provide a benchmark including both planning and learning based baselines. We evaluate the performance of AI agents with the human-like agent as well as with real humans using objective metrics and subjective user ratings. Experimental results demonstrate that the proposed challenge and virtual environment enable a systematic evaluation on the important aspects of machine social intelligence at scale.1', 'corpus_id': 236924142, 'score': 1}, {'doc_id': '237260024', 'title': 'Airbert: In-domain Pretraining for Vision-and-Language Navigation', 'abstract': 'Vision-and-language navigation (VLN) aims to enable embodied agents to navigate in realistic environments using natural language instructions. Given the scarcity of domain-specific training data and the high diversity of image and language inputs, the generalization of VLN agents to unseen environments remains challenging. Recent methods explore pretraining to improve generalization, however, the use of generic image-caption datasets or existing smallscale VLN environments is suboptimal and results in limited improvements. In this work, we introduce BnB1, a largescale and diverse in-domain VLN dataset. We first collect image-caption (IC) pairs from hundreds of thousands of listings from online rental marketplaces. Using IC pairs we next propose automatic strategies to generate millions of VLN path-instruction (PI) pairs. We further propose a shuffling loss that improves the learning of temporal order inside PI pairs. We use BnB to pretrain our Airbert2 model that can be adapted to discriminative and generative settings and show that it outperforms state of the art for Room-to-Room (R2R) navigation and Remote Referring Expression (REVERIE) benchmarks. Moreover, our in-domain pretraining significantly increases performance on a challenging few-shot VLN evaluation, where we train the model only on VLN instructions from a few houses.', 'corpus_id': 237260024, 'score': 1}, {'doc_id': '237397741', 'title': 'Discovering and Achieving Goals with World Models', 'abstract': 'How can an artificial agent learn to solve a wide range of tasks in a complex visual environment in the absence of external supervision? We decompose this question into two problems, global exploration of the environment and learning to reliably reach situations found during exploration. We introduce the Explore Achieve Network (ExaNet), a unified solution to these by learning a world model from the high-dimensional images and using it to train an explorer and an achiever policy from imagined trajectories. Unlike prior methods that explore by reaching previously visited states, our explorer plans to discover unseen surprising states through foresight, which are then used as diverse targets for the achiever. After the unsupervised phase, ExaNet solves tasks specified by goal images without any additional learning. We introduce a challenging benchmark spanning across four standard robotic manipulation and locomotion domains with a total of over 40 test tasks. Our agent substantially outperforms previous approaches to unsupervised goal reaching and achieves goals that require interacting with multiple objects in sequence. Finally, to demonstrate the scalability and generality of our approach, we train a single general agent across four distinct environments. For videos, see https://sites.google.com/view/ exanet/home.', 'corpus_id': 237397741, 'score': 1}, {'doc_id': '236772810', 'title': 'Learning Embeddings that Capture Spatial Semantics for Indoor Navigation', 'abstract': 'Incorporating domain-specific priors in search and navigation tasks has shown promising results in improving generalization and sample complexity over end-toend trained policies. In this work, we study how object embeddings that capture spatial semantic priors can guide search and navigation task in a structured environment. We know that humans can search for an object like a book, or a plate in an unseen house, based on spatial semantics of bigger objects detected. For example, a book is likely to be on a bookshelf or a table, whereas a plate is likely to be in a cupboard or dishwasher. We propose a method to incorporate such spatial semantic awareness in robots by leveraging pre-trained language models and multirelational knowledge bases as object embeddings. We demonstrate using these object embeddings to search a query object in an unseen indoor environment. We measure the performance of these embeddings in an indoor simulator (AI2Thor). We further evaluate different pre-trained embedding on Success Rate (SR) and Success weighted by Path Length (SPL). Code is available at: https://github.com/vidhiJain/SpatialEmbeddings', 'corpus_id': 236772810, 'score': 1}, {'doc_id': '237421139', 'title': 'Hierarchical Object-to-Zone Graph for Object Navigation', 'abstract': 'The goal of object navigation is to reach the expected objects according to visual information in the unseen environments. Previous works usually implement deep models to train an agent to predict actions in real-time. However, in the unseen environment, when the target object is not in egocentric view, the agent may not be able to make wise decisions due to the lack of guidance. In this paper, we propose a hierarchical object-to-zone (HOZ) graph to guide the agent in a coarse-to-fine manner, and an online-learning mechanism is also proposed to update HOZ according to the real-time observation in new environments. In particular, the HOZ graph is composed of scene nodes, zone nodes and object nodes. With the pre-learned HOZ graph, the real-time observation and the target goal, the agent can constantly plan an optimal path from zone to zone. In the estimated path, the next potential zone is regarded as subgoal, which is also fed into the deep reinforcement learning model for action prediction. Our methods are evaluated on the AI2-Thor simulator. In addition to widely used evaluation metrics SR and SPL, we also propose a new evaluation metric of SAE that focuses on the effective action rate. Experimental results demonstrate the effectiveness and efficiency of our proposed method. The code is available at https://github.com/sx-zhang/HOZ.git.', 'corpus_id': 237421139, 'score': 0}, {'doc_id': '236908377', 'title': 'ICLR 2021 1 ? ? ? ? ?', 'abstract': 'We introduce environment predictive coding, a self-supervised approach to learn environment-level representations for embodied agents. In contrast to prior work on self-supervised learning for images, we aim to jointly encode a series of images gathered by an agent as it moves about in 3D environments. We learn these representations via a zone prediction task, where we intelligently mask out portions of an agent’s trajectory and predict them from the unmasked portions, conditioned on the agent’s camera poses. By learning such representations on a collection of videos, we demonstrate successful transfer to multiple downstream navigationoriented tasks. Our experiments on the photorealistic 3D environments of Gibson and Matterport3D show that our method outperforms the state-of-the-art on challenging tasks with only a limited budget of experience.', 'corpus_id': 236908377, 'score': 0}, {'doc_id': '236924393', 'title': 'Learning to Design and Construct Bridge without Blueprint', 'abstract': 'Autonomous assembly has been a desired functionality of many intelligent robot systems. We study a new challenging assembly task, designing and constructing a bridge without a blueprint. In this task, the robot needs to first design a feasible bridge architecture for arbitrarily wide cliffs and then manipulate the blocks reliably to construct a stable bridge according to the proposed design. In this paper, we propose a bilevel approach to tackle this task. At the high level, the system learns a bridge blueprint policy in a physical simulator using deep reinforcement learning and curriculum learning. A policy is represented as an attention-based neural network with objectcentric input, which enables generalization to different number of blocks and cliff widths. For low-level control, we implement a motion-planning-based policy for real-robot motion control, which can be directly combined with a trained blueprint policy for real-world bridge construction without tuning. In our field study, our bi-level robot system demonstrates the capability of manipulating blocks to construct a diverse set of bridges with different architectures.', 'corpus_id': 236924393, 'score': 0}]
196	"{'doc_id': '211133074', 'title': 'Cross-sectional Stock Price Prediction using Deep Learning for Actual Investment Management', 'abstract': 'Stock price prediction has been an important research theme both academically and practically. Various methods to predict stock prices have been studied until now. The feature that explains the stock price by a cross-section analysis is called a ""factor"" in the field of finance. Many empirical studies in finance have identified which stocks having features in the cross-section relatively increase and which decrease in terms of price. Recently, stock price prediction methods using machine learning, especially deep learning, have been proposed since the relationship between these factors and stock prices is complex and non-linear. However, there are no practical examples for actual investment management. In this paper, therefore, we present a cross-sectional daily stock price prediction framework using deep learning for actual investment management. For example, we build a portfolio with information available at the time of market closing and invest at the time of market opening the next day. We perform empirical analysis in the Japanese stock market and confirm the profitability of our framework.', 'corpus_id': 211133074}"	3049	"[{'doc_id': '210911765', 'title': 'Forecasting Corn Yield With Machine Learning Ensembles', 'abstract': 'The emergence of new technologies to synthesize and analyze big data with high-performance computing has increased our capacity to more accurately predict crop yields. Recent research has shown that machine learning (ML) can provide reasonable predictions faster and with higher flexibility compared to simulation crop modeling. However, a single machine learning model can be outperformed by a “committee” of models (machine learning ensembles) that can reduce prediction bias, variance, or both and is able to better capture the underlying distribution of the data. Yet, there are many aspects to be investigated with regard to prediction accuracy, time of the prediction, and scale. The earlier the prediction during the growing season the better, but this has not been thoroughly investigated as previous studies considered all data available to predict yields. This paper provides a machine leaning based framework to forecast corn yields in three US Corn Belt states (Illinois, Indiana, and Iowa) considering complete and partial in-season weather knowledge. Several ensemble models are designed using blocked sequential procedure to generate out-of-bag predictions. The forecasts are made in county-level scale and aggregated for agricultural district and state level scales. Results show that the proposed optimized weighted ensemble and the average ensemble are the most precise models with RRMSE of 9.5%. Stacked LASSO makes the least biased predictions (MBE of 53 kg/ha), while other ensemble models also outperformed the base learners in terms of bias. On the contrary, although random k-fold cross-validation is replaced by blocked sequential procedure, it is shown that stacked ensembles perform not as good as weighted ensemble models for time series data sets as they require the data to be non-IID to perform favorably. Comparing our proposed model forecasts with the literature demonstrates the acceptable performance of forecasts made by our proposed ensemble model. Results from the scenario of having partial in-season weather knowledge reveals that decent yield forecasts with RRMSE of 9.2% can be made as early as June 1st. Moreover, it was shown that the proposed model performed better than individual models and benchmark ensembles at agricultural district and state-level scales as well as county-level scale. To find the marginal effect of each input feature on the forecasts made by the proposed ensemble model, a methodology is suggested that is the basis for finding feature importance for the ensemble model. The findings suggest that weather features corresponding to weather in weeks 18–24 (May 1st to June 1st) are the most important input features.', 'corpus_id': 210911765, 'score': 0}, {'doc_id': '211252428', 'title': 'Sector connectedness in the Chinese stock markets', 'abstract': 'Uncovering the risk transmitting path within economic sectors in China is crucial for understanding the stability of the Chinese economic system, especially under the current situation of the China-US trade conflicts. In this paper, we try to uncover the risk spreading channels by means of volatility spillovers within the Chinese sectors using stock market data. By applying the generalized variance decomposition framework based on the VAR model and the rolling window approach, a set of connectedness matrices is obtained to reveal the overall and dynamic spillovers within sectors. It is found that 17 sectors (mechanical equipment, electrical equipment, utilities, and so on) are risk transmitters and 11 sectors (national defence, bank, non-bank finance, and so on) are risk takers during the whole period. During the periods with the extreme risk events (the global financial crisis, the Chinese interbank liquidity crisis, the Chinese stock market plunge, and the China-US trade war), we observe that the connectedness measures significantly increase and the financial sectors play a buffer role in stabilizing the economic system. The robust tests suggest that our results are not sensitive to the changes of model parameters. Our results not only uncover the spillover effects within the Chinese sectors, but also highlight the deep understanding of the risk contagion patterns in the Chinese stock markets.', 'corpus_id': 211252428, 'score': 0}, {'doc_id': '13853164', 'title': 'Interdependence among agricultural commodity markets, macroeconomic factors, crude oil and commodity index', 'abstract': 'This paper examines the degree of interdependence between three agricultural commodity prices, crude oil price returns, macroeconomic variables and the S&P GSCI commodity returns index. We apply Aielli (2013) cDCC model using monthly data from 1982 to 2012 to estimate the dynamic correlations of the returns series and endogenously detect any structural instability of the dynamic correlations. Our results indicate that crude oil price returns present statistically significant dynamic correlations with all the macroeconomic variables in addition to the GSCI index. Additionally, we detect structural changes in these dynamic correlations mainly associated with the financial crisis of 2008. On the other hand, our results show that there exists no degree of interdependence between maize, soybeans and sugar with crude oil price returns and most of the macroeconomic variables. The exceptions are between soybeans with the U.S. exchange rate and sugar with global economic activity. Nevertheless, only the GSCI index presents significant dynamic correlations with these commodity price returns.', 'corpus_id': 13853164, 'score': 1}, {'doc_id': '156964279', 'title': 'Valuing Public Information in Agricultural Commodity Markets: WASDE Corn Reports', 'abstract': 'Monthly WASDE reports by USDA estimate current and future global supply-utilization balances for various commodities, including corn. Existing literature has shown that markets respond to WASDE releases (news effects) but has not quantified the value or distribution of benefits from those reports. We use Monte Carlo simulations of a quarterly model of the U.S. corn market to estimate the value of the WASDE forecast and its components. Our results show significant value to market participants from the WASDE reports, roughly $301 million or 0.55% of overall corn market value. The results also show significant value for each forecasted component of the reports: area ($145 million), yield ($188 million), production ($299 million), demand/stocks ($300 million) and exports ($320 million). The benefits of each component do not strictly sum when new information is added because substantial redistribution of benefits occurs, since specific information components help specific interest groups. The expected benefits or losses realized by consumers, producers or traders is often nearly as large as (and sometimes larger than) the net benefits to society from better information. In the base case benefits from WASDE information largely accrues to producers ($153 million) and consumers ($341 million). Traders lose $192 million, as they are presumed to buy at harvest, before valuable demand, stocks and export data is known. Farmers behave as traders when they choose to store, sell forward, or participate directly in futures markets. Thus, the net trader benefit or loss accrues partially to farmers as traders and partially to commercial agents. These results are sensitive to elasticity assumptions that capture both how agents behave in markets and how their welfare is measured.', 'corpus_id': 156964279, 'score': 1}, {'doc_id': '159447322', 'title': ""Can agricultural commodity prices predict Nigeria's inflation?"", 'abstract': ""Abstract This study examines the predictability of agricultural commodity prices in Nigeria's inflation forecast. It considers twelve major agricultural commodities and their predictability are evaluated singly and jointly for both food and headline inflation using various methodological approaches. Our preliminary analysis suggests that the predictor series exhibit some salient features such as persistence, endogeneity and conditional heteroscedasticity (high volatility) effects. Also, the parameters of the agricultural commodity-based inflation model tend to shift over short periods based on the results of the Bai and Perron (2003) test. Thus, we employ the Westerlund and Narayan (2015) estimator which accounts for these salient features. Also, we control for the time-varying parameters of the inflation model using the breaks obtained from the Bai and Perron (2003) test as well as the recursive (expanded) window approach. Our results show that agricultural commodities individually predict both headline and food inflation better than the random walk model which is the benchmark model for forecasting inflation in the literature. Thereafter, we construct a composite index for all the agricultural commodities using the principal components approach and the performance of the predictive model significantly improves relative to the individual agricultural commodities particularly when food inflation is considered. These results are consistent for both in-sample and out-of-sample forecasts and are robust to alternative measures of forecast performance, multiple forecast horizons and different data frequencies."", 'corpus_id': 159447322, 'score': 1}, {'doc_id': '211044062', 'title': 'Text-based crude oil price forecasting', 'abstract': ""Crude oil price forecasting has attracted substantial attention in the field of forecasting. Recently, the research on text-based crude oil price forecasting has advanced. To improve accuracy, some studies have added as many covariates as possible, such as textual and nontextual factors, to their models, leading to unnecessary human intervention and computational costs. Moreover, some methods are only designed for crude oil forecasting and cannot be well transferred to the forecasting of other similar futures commodities. In contrast, this article proposes a text-based forecasting framework for futures commodities that uses only future news headlines obtained from Investing.com to forecast crude oil prices. Two marketing indexes, the sentiment index and the topic intensity index, are extracted from these news headlines. Considering that the public's sentiment changes over time, the time factor is innovatively applied to the construction of the sentiment index. Taking the nature of the short news headlines into consideration, a short text topic model called SeaNMF is used to calculate the topic intensity of the futures market more accurately. Two methods, VAR and RFE, are used for lag order judgment and feature selection, respectively, at the model construction stage. The experimental results show that the Ada-text model outperforms the Adaboost.RT baseline model and the other benchmarks."", 'corpus_id': 211044062, 'score': 0}, {'doc_id': '214795257', 'title': 'Company classification using machine learning', 'abstract': 'The recent advancements in computational power and machine learning algorithms have led to vast improvements in manifold areas of research. Especially in finance, the application of machine learning enables both researchers and practitioners to gain new insights into financial data and well-studied areas such as company classification. In our paper, we demonstrate that unsupervised machine learning algorithms can be used to visualize and classify company data in an economically meaningful and effective way. In particular, we implement the data-driven dimension reduction and visualization tool t-distributed stochastic neighbor embedding (t-SNE) in combination with spectral clustering. The resulting company groups can then be utilized by experts in the field for empirical analysis and optimal decision making. By providing an exemplary out-of-sample study within a portfolio optimization framework, we show that the application of t-SNE and spectral clustering improves the overall portfolio performance. Therefore, we introduce our approach to the financial community as a valuable technique in the context of data analysis and company classification.', 'corpus_id': 214795257, 'score': 0}, {'doc_id': '8543580', 'title': 'Volatility linkages between energy and agricultural commodity prices', 'abstract': 'Abstract We investigate price and volatility risk originating in linkages between energy and agricultural commodity prices in Germany and study their dynamics over time. We propose an econometric approach to quantify the volatility and correlation risk structure, which has a large impact for investment and hedging strategies of market participants as well as for policy makers. Volatilities and their short and long run linkages are analyzed using an asymmetric dynamic conditional correlation GARCH model as well as a multivariate multiplicative volatility model. Our approach provides a flexible and accurate fitting procedure for volatility and correlation risk. We find that in the long run prices move together and preserve an equilibrium, while correlations are mostly positive with persistent market shocks. Our results reveal that concerns about biodiesel being the cause of high and volatile agricultural commodity prices are rather unjustified.', 'corpus_id': 8543580, 'score': 1}, {'doc_id': '157070007', 'title': 'Realized Volatility Forecasting of Agricultural Commodity Futures Using Long Memory and Regime Switching', 'abstract': 'We investigate the dynamic properties of the realized volatility of five agricultural commodity futures by employing the high-frequency data from Chinese markets and find that the realized volatility exhibits both long memory and regime switching. To capture these properties simultaneously, we utilize a Markov switching autoregressive fractionally integrated moving average (MS-ARFIMA) model to forecast the realized volatility by combining the long memory process with regime switching component, and compare its forecast performances with the competing models at various horizons. The full-sample estimation results show that the dynamics of the realized volatility of agricultural commodity futures are characterized by two levels of long memory: one associated with the low-volatility regime and the other with the high-volatility regime, and the probability to stay in the low-volatility regime is higher than that in the high-volatility regime. The out-of-sample volatility forecast results show that the combination of long memory with switching regimes improves the performance of realized volatility forecast, and the proposed model represents a superior out-of-sample realized volatility forecast to the competing models. Copyright © 2016 John Wiley & Sons, Ltd.', 'corpus_id': 157070007, 'score': 1}, {'doc_id': '212628659', 'title': 'S-APIR: News-based Business Sentiment Index', 'abstract': 'This paper describes our work on developing a new business sentiment index using daily newspaper articles. We adopt a recurrent neural network (RNN) with Gated Recurrent Units to predict the business sentiment of a given text. An RNN is initially trained on Economy Watchers Survey and then fine-tuned on news texts for domain adaptation. Also, a one-class support vector machine is applied to filter out texts deemed irrelevant to business sentiment. Moreover, we propose a simple approach to temporally analyzing how much and when any given factor influences the predicted business sentiment. The validity and utility of the proposed approaches are empirically demonstrated through a series of experiments on Nikkei Newspaper articles published from 2013 to 2018.', 'corpus_id': 212628659, 'score': 0}]"
197	{'doc_id': '233568599', 'title': 'Generalisation of tea moisture content models based on VNIR spectra subjected to fractional differential treatment', 'abstract': 'Model generalisation for the detection of tea moisture content was investigated across different leaf surface orientations and tea varieties in this study. The micromorphology of leaves plucked from three tea bushes was analysed, and differences between different surface orientations and varieties were observed. The VNIR spectra (350–2500\xa0nm) of the leaves were collected and analysed. Excellent prediction performance was obtained for moisture detection models based on spectra for the same leaf surface orientation and variety. By contrast, the prediction performance decreased severely if the test spectra were obtained for different leaf surface orientations and varieties. To solve this issue, differential treatments with fractional order between 0 and 2 were carried out on the spectra. The results showed that the prediction performance improved for generalisation between varieties and orientations, especially for orders of 0.4 or 0.6. The mechanism by which the fractional differential treatment mines the common information from the spectra with varying characteristics was elucidated by calculating the correlation coefficients between the moisture content and the spectra treated with different differential orders. The results of this study advance tea moisture detection based on VNIR spectra and the ability to generalise across spectra with different characteristics.', 'corpus_id': 233568599}	17836	"[{'doc_id': '235169748', 'title': 'Dimensionality reduction and unsupervised clustering for EELS-SI.', 'abstract': 'A novel combination of machine learning algorithms is proposed for the differentiation of distinct spectra in a large electron energy loss spectroscopy spectrum image (EELS-SI) dataset. For clustering of the EEL spectra including similar fine structures in an efficient space, linear and nonlinear dimensionality reduction methods are used to project the EEL spectra onto a low-dimensional space. Then, a density-based clustering algorithm is applied to distinguish the meaningful data clusters. By applying this strategy to various experimental EELS-SI datasets, differentiation of several groups of EEL spectra representing specific fine structures was achieved. It is possible to investigate particular fine structures by averaging all of the spectra in each cluster. Also, the spatial distributions of each cluster in the scanning regions can be observed, which enables investigation of the locations of different fine structures in materials. This method does not require any prior knowledge, i.e., it is a data-driven analysis; therefore, it can be applied to any hyperspectral image.', 'corpus_id': 235169748, 'score': 0}, {'doc_id': '235480408', 'title': 'Single-step genomic prediction of Eucalyptus dunnii using different identity-by-descent and identity-by-state relationship matrices.', 'abstract': 'Genomic selection based on the single-step genomic best linear unbiased prediction (ssGBLUP) approach is becoming an important tool in forest tree breeding. The quality of the variance components and the predictive ability of the estimated breeding values (GEBV) depends on how well marker-based genomic relationships describe the actual genetic relationships at unobserved causal loci. We investigated the performance of GEBV obtained when fitting models with genomic covariance matrices based on two identity-by-descent (IBD) and two identity-by-state (IBS) relationship measures. Multiple-trait multiple-site ssGBLUP models were fitted to diameter and stem straightness in five open-pollinated progeny trials of Eucalyptus dunnii, genotyped using the EUChip60K. We also fitted the conventional ABLUP model with a pedigree-based covariance matrix. Estimated relationships from the IBD estimators displayed consistently lower standard deviations than those from the IBS approaches. Although ssGBLUP based in IBS estimators resulted in higher trait-site heritabilities, the gain in accuracy of the relationships using IBD estimators has resulted in higher predictive ability and lower bias of GEBV, especially for low-heritability trait-site. ssGBLUP based on IBS and IBD approaches performed considerably better than the traditional ABLUP. In summary, our results advocate the use of the ssGBLUP approach jointly with the IBD relationship matrix in open-pollinated forest tree evaluation.', 'corpus_id': 235480408, 'score': 0}, {'doc_id': '235514951', 'title': 'Relationship of the optical properties with soluble solids content and moisture content of strawberry during ripening', 'abstract': ""Abstract To understand the relationship of optical properties with soluble solids content (SSC) and moisture content of strawberry during ripening, a single integrating sphere system was built to estimate the absorption coefficient (μa) and reduced scattering coefficient (μs') of strawberry in white, color turning and red ripening stages over the wavelength range of 550−850 nm and 950−1650 nm. The relationship between optical properties and SSC and moisture content was analyzed, and the determination models for SSC and moisture content were established by using partial least squares regression and support vector machine methods based on the spectra of μa, μs', and μa together with μs'. The results showed that the absorption peaks of strawberry were at 675, 975, 1197 and 1411 nm, and the μs' generally decreased with increased maturity of strawberry. The μa was positively correlated with SSC and negatively correlated with moisture content, while the μs'was positively correlated with moisture content and negatively with SSC. The best correlations of μa with SSC and moisture content were found at 1411 nm with the correlation coefficients of 0.72 and -0.74, respectively. The established support vector machine models based on the μa spectra in 950−1650 nm and 550−850 nm had the smallest root-mean-squares error of calibration set of 0.98 % and 0.89 % for SSC and moisture content, respectively. This study indicates that SSC and moisture content mainly affect the absorption property of strawberry, and μa has greater potential than μs' and μa together with μs' in determining the internal quality of strawberry."", 'corpus_id': 235514951, 'score': 1}, {'doc_id': '233538574', 'title': 'Using autoencoders to compress soil VNIR–SWIR spectra for more robust prediction of soil properties', 'abstract': 'Abstract In the past two decades, research efforts have focused on using near infrared diffuse reflectance spectroscopy (350–2500\xa0nm) as a rapid and cost-effective method for soil analysis. Given the multi-faceted importance of the soil ecosystem, and considering the increasing pressures exerted upon it due to climate change, degradation and urbanization, the advantages of soil spectroscopy are significant. Large soil spectral libraries have been developed to this end throughout the world. The soil matrix is however complex, posing a challenge in the determination of key soil properties from the spectra. To tackle this challenge, two methodologies are generally used: a) the use of spectral pre-processing techniques to transfer the spectra into a new space in which the association between spectrum and soil property is supposed to be more clear, and b) the use of more sophisticated machine learning models (e.g. deep learning). In this paper, we propose a novel methodology using stacked autoencoders to transform the initially recorded spectra in a new compressed (i.e. latent) space which can help the chemometric models enhance the accuracy of prediction. This is an unsupervised learning approach which only depends on the input data (i.e. the spectra). Following the significant results obtained in the literature using combinations of different spectra pre-processing techniques and the simultaneous prediction of multiple soil properties, the proposed methodology is extended to facilitate these approaches. We demonstrate this capacity by applying it in the mineral samples of the LUCAS 2009 topsoil database, and simultaneously predicting eight properties (the particle size distribution, pH, CEC, organic carbon, calcium carbonate, and total nitrogen) using an artificial neural network. Compared to standard pre-processing techniques and other transformations such as the principal components space, the proposed methodology using only one spectral source as input decreases the RMSE on average by 8.4% and by 3.5%, respectively. With respect to the current state-of-the-art and in particular a multi-input convolutional neural network which was recently proposed and outperformed the compared methodologies, the results of our multi-input methodology exhibit an average RMSE decrease of 9.9%. The interpretability aspect of the transformed feature space and the compressed spectra was also examined to identify how the compressed information encodes the input data and enables better associations between input and output.', 'corpus_id': 233538574, 'score': 1}, {'doc_id': '235220001', 'title': 'Using Spectral Reflectance to Estimate the Leaf Chlorophyll Content of Maize Inoculated With Arbuscular Mycorrhizal Fungi Under Water Stress', 'abstract': 'Leaf chlorophyll content is an important indicator of the growth and photosynthesis of maize under water stress. The promotion of maize physiological growth by (AMF) has been studied. However, studies of the effects of AMF on the leaf chlorophyll content of maize under water stress as observed through spectral information are rare. In this study, a pot experiment was carried out to spectrally estimate the leaf chlorophyll content of maize subjected to different durations (20, 35, and 55 days); degrees of water stress (75%, 55% and 35% water supply) and two inoculation treatments (inoculation with Funneliformis mosseae and no inoculation). Three machine learning algorithms, including the back propagation (BP) method, least square support vector machine (LSSVM) and random forest (RF) method, were used to estimate the leaf chlorophyll content of maize. The results showed that AMF increased the leaf chlorophyll content, net photosynthetic rate (A), stomatal conductance (gs), transpiration rate (E), and water use efficiency (WUE) of maize but decreased the intercellular carbon dioxide concentration (Ci) of maize and atmospheric vapor pressure deficit (VPD) regardless of the water stress duration and degree. The first-order differential spectral data can better reflect the correlation between leaf chlorophyll content and spectrum of inoculated maize when compared with original spectral data. The BP model performed bestin modeling the maize leaf chlorophyll content, yielding the largest R2-values and smallest root mean square error (RMSE) values, regardless of stress duration. These results provide a reliable basis for the effective monitoring of the leaf chlorophyll content of maize under water stress.', 'corpus_id': 235220001, 'score': 1}, {'doc_id': '234295646', 'title': 'Application of spectra pre-treatments on firmness assessment of intact sapodilla using vis-nir spectroscopy', 'abstract': 'This study aimed to obtain the best calibration model from various spectra pre-treatment methods to assess sapodilla fruit firmness using vis-nir spectroscopy. Before the spectra data measurement, samples were treated with storage of 0, 5 and 10 days at room temperature. Spectra data measurement was carried out using the NirVana AG410 visible and near infrared spectrometer from 312 to 1050 nm with interval of 3 nm. RAW spectra were pre-treated using the multiplicative scatter correction (MSC), standard normal variate (SNV), and Savitzky-Golay first derivative (dg1) with 9 points of smoothing. The calibration model was developed using PLS (partial least squares) method. Validation was done by K fold cross validation method. The results showed the MSC and SNV spectra were able to eliminate noises of RAW spectra, whereas in the dg1 spectra, noises were still visible. The best model was acquired by SNV spectra with R2 (coefficient of determination) of calibration and validation of 0.882 and 0.870, root mean square error of calibration (RMSEC) and root mean square error of cross validation (RMSECV) values of 2.92 and 3.08, and the ratio of performance to deviation (RPD) of 2.76. The result indicated the spectra pre-treatments were able to improve the accuracy of calibration model on assessment of sapodilla fruit firmness.', 'corpus_id': 234295646, 'score': 1}, {'doc_id': '234794173', 'title': 'Microplastic Spectral Classification Needs an Open Source Community: Open Specy to the Rescue!', 'abstract': ""Microplastic pollution research has suffered from inadequate data and tools for spectral (Raman and infrared) classification. Spectral matching tools often are not accurate for microplastics identification and are cost-prohibitive. Lack of accuracy stems from the diversity of microplastic pollutants, which are not represented in spectral libraries. Here, we propose a viable software solution: Open Specy. Open Specy is on the web (www.openspecy.org) and in an R package. Open Specy is free and allows users to view, process, identify, and share their spectra to a community library. Users can upload and process their spectra using smoothing (Savitzky-Golay filter) and polynomial baseline correction techniques (IModPolyFit). The processed spectrum can be downloaded to be used in other applications or identified using an onboard reference library and correlation-based matching criteria. Open Specy's data sharing and session log features ensure reproducible results. Open Specy houses a growing library of reference spectra, which increasingly represents the diversity of microplastics as a contaminant suite. We compared the functionality and accuracy of Open Specy for microplastic identification to commonly used spectral analysis software. We found that Open Specy was the only open source software and the only software with a community library, and Open Specy had comparable accuracy to popular software (OMNIC Picta and KnowItAll). Future developments will enhance spectral identification accuracy as the reference library and functionality grows through community-contributed spectra and community-developed code. Open Specy can also be used for applications beyond microplastic analysis. Open Specy's source code is open source (CC-BY-4.0, attribution only) (https://github.com/wincowgerDEV/OpenSpecy)."", 'corpus_id': 234794173, 'score': 0}, {'doc_id': '234195613', 'title': 'Research and application of SVM analysis model for spectral fingerprint of mixed gas', 'abstract': 'The infrared spectrum analysis method has the characteristics of non-contact, high-speed and no consumption of sample, There is some useless interference information that is not sensitive to the analysis results in the infrared spectrum data sample. Combining support vector machine with infrared spectrum analysis, the band of spectrum data that is sensitive to gas concentration is selected by experiment. The useless, insensitive and disturbing information is removed, and the useful information is retained. And then the infrared spectrum analysis model based on support vector machine is used to complete the output of gas concentration of mixed gas components. Based on Lambert’s theorem and using the known spectrum data samples as input in experiment, the sensitivity analysis of the mixed gas spectrum data samples is carried out by using the variable sliding window technology which is similar to the center wavelength and band-pass filter, and the start point and band width of the band are both variable. The band with small error is selected as the spectrum data fingerprint series to edit and splice, and then the mixed gas data sample with the fingerprint characteristics of the mixed gas component gas spectrum data is formed. On this basis, a support vector machine based infrared spectrum analysis model of mixed gases is established. The analytical model includes two processes: training and verification. The preconfigured mixed gas samples that are known concentration is fingerprint edited and processed after scanning by the spectrometer. Then the gas samples are formed the spectral data samples which can be considered as the input samples of the analytical model. The model is trained to determine the support vector and corresponding weight of the analytical model based on the spectral data samples; The fingerprint characteristic data samples of the unknown-concentration mixed gas spectrum are tested by the analytical model that has determined parameters. Then, the gas concentration of the mixed gas components is obtained. The aspects of spectral data’s fingerprint feature extraction and the selection of the model’s main parameters are researched, through experiment, the methods and processes of fingerprint feature editing and splicing extraction of spectral data are studied, and the influence of the model’s parameters on the analysis results is optimized. The experimental results show that under the condition of optimizing the model’s parameters, selecting the fingerprint features of the spectral data reasonably and constructing a new spectral data sample with the fingerprint features of the mixed gas component can help eliminate the measurement cross sensitivity caused by the cross overlap of the absorption spectra of the mixed gas component, and improve the calculation efficiency and the analysis accuracy, which has theoretical and practical value.', 'corpus_id': 234195613, 'score': 0}, {'doc_id': '233410271', 'title': 'Superlatives in Teaching General Chemistry.', 'abstract': 'What is the strongest Brønsted acid, the strongest base, the strongest oxidizing agent? If not understood in an absolute, once-and-forever sense, the answers to such questions may help at extending and reinforcing the meaning of simple concepts in first-year chemistry courses. Moreover, they serve the purpose of introducing research aspects and linking them to general chemistry.', 'corpus_id': 233410271, 'score': 0}, {'doc_id': '235553524', 'title': 'Soil salinity mapping using Landsat 8 OLI data and regression modeling in the Great Hungarian Plain', 'abstract': ""Salt's deposition in the subsoil is known as salinization. It is caused by natural processes such as mineral weathering or human-made activities such as irrigation with saline water. This environmental issue has grown more critical and is frequently occurring in the Hungarian Great Plain, adversely influencing agricultural productivity. This study aims to predict soil salinity in the Great Hungarian Plain, located in the east of Hungary, using Landsat 8 OLI data combined with four state-of-the-art regression models, i.e., Multiple Linear Regression, Partial Least Squares Regression, Ridge Regression, and Feedforward Artificial Neural Network. For this purpose, seventy-six soil samples were collected during a field survey conducted by the Research Institute for Soil Sciences and Agricultural Chemistry between the 15 of September and the 15 of October, 2016. We used the min–max accuracy, the root-mean-square error (RMSE), and the mean squared error (MSE) to evaluate and compare the four models' performance. The results showed that the ridge regression model performed the best in terms of prediction (MSEtraining\u2009=\u20090.006, MSEtest\u2009=\u20090.0007, RMSE\u2009=\u20090.081), with a min–max accuracy equal to 0.75. Hence, the application of regression modeling on spectral indices, principal component analysis, and land surface temperature derived from multispectral data is an efficient method for soil salinity assessment at local scales. The resulting map can provide an overview of salinity levels and evaluate the efficiency of land management strategies in irrigated areas. An increase in sampling density will be recommended to validate this approach on the regional scale."", 'corpus_id': 235553524, 'score': 1}]"
198	"{'doc_id': '151049146', 'title': 'INVENTORY FLOW TILL PRODUCTION CAPACITY', 'abstract': 'An efficient management of inventories means proper planning and usage of the control methods as Just in Time(JIT), Material requirements planning(MRP), Vendor Management Inventory(VMI) or Distribution resource planning(DRP). Are presented and analyzed in their interdependence, issues such as: delivery time, payment term, payment methods, payment instruments, delivery time, risk assuming in terms of delivery terms agreed and accepted, transport administration, minimum quantity delivered, stock buffer, planned quantities to be supplied according to the production and sales plan, monthly average consumption, product category (""jumper"" or not), the free market price, availability of one article on market, the importance of Bulletin analysis and Declaration of conformity , migration tests for packaging that come into direct contact with the product, management methods used, system management, storage capacity, mandatory and optional commercial documents, the frequency of inventory management, ambient or temperature controlled, humidity records, storage capacity, Quality standards followed, internal audit and external audit, physical and qualitative reception, issuance and tracking complaints, labelling, items identifying, samples management, expired items administration ,losses recording, key performance indicators and many other aspects referring to inventory flow till production capacity of food company.', 'corpus_id': 151049146}"	20633	"[{'doc_id': '237380176', 'title': 'Integration of Food and Nutrition Education Across the Secondary School Curriculum: Two Experiential Models as Two Case Studies', 'abstract': 'The aim of this paper is to present the implementation and evaluation of two recognised programs, one from Australia and one from Denmark, that endeavour to integrate and enhance food and nutrition education across the secondary school curriculum and whole school programs. This paper details descriptions of design, delivery mode, core components and evaluation of each program based on existing detailed reports and original research investigations. Resultantly, one program in Australia (Stephanie Alexander Kitchen Garden Program) and one program in Denmark (LOMA or LOkal MAd = local food) are reported as two case studies. The target group for both programs is secondary school students in Years 7–12; both programs are conducted within secondary schools and within school hours. Both interventions focus on developing secondary students’ food production and food preparation knowledge and skills. Their evaluation methods have consisted of pre- and post-intervention surveys, single case study, and focus groups with both students and teachers. Both programs have reported possible integration across secondary school subjects and modifications in students’ knowledge and skills in food and nutrition. These programs have focused on developing an experiential and localised learning model for food and nutrition education, which may also address food insecurity concerns among adolescents which has been shown to correlate with poor nutrient intake and consequential health complications. Their overall model can be adapted taking into account the social, economic, and environmental context of a secondary school.', 'corpus_id': 237380176, 'score': 0}, {'doc_id': '237803721', 'title': 'Supply Chain Responsiveness to a (Post)-Pandemic Grocery and Food Service E-Commerce Economy: An Exploratory Canadian Case Study', 'abstract': 'The focus of this study looks at the motivations and rationale from a national survey of over 7200 Canadians in November 2020 into why they use online services to purchase food. As a result of the global COVID-19 pandemic, food supply chains have been significantly altered. Consumers are purchasing foods with different dynamics, including when they buy in-person at groceries, at restaurants or at food service establishments. Elements of the food supply chain will be permanently altered post-pandemic. The study looks at a specific set of factors, captured in the survey, namely, consumer price sensitivity to the costs of online food purchasing, growing sustainability-related concerns over food packaging and waste, and product sensory experience related to how online purchasing changes from in-person food selection. The end goal, emerging from a case study, is insight into the strategies and preparedness with which CPGs, food services, and retailers can better manage the supply chain in their food product offerings in the post-pandemic era.', 'corpus_id': 237803721, 'score': 0}, {'doc_id': '202159503', 'title': 'Smart cook: making cooking easier with multimodal learning', 'abstract': ""Learning how to cook presents at least two significant challenges. First, it can be difficult for novices to find appropriate recipes based on the ingredients available in one's pantry and/or refrigerator. Second, it can be difficult to focus on cooking tasks and following a recipe at the same time. In this poster, we present the design process and implementation of a system that uses deep learning to address the first of these two problems. Our initial design work focuses on streamlining the process of entering and tracking potential ingredients on hand and determining appropriate recommendations for recipes that utilize these ingredients. Here, we present the current state of our project, explaining in particular our contributions to minimizing the overhead of tracking kitchen ingredients and converting this inventory information into effective recipe recommendations using a multimodal machine learning approach."", 'corpus_id': 202159503, 'score': 1}, {'doc_id': '237460165', 'title': 'The role of integrated marketing communication for ustainable development in food production', 'abstract': ""The decrease in food production output, the suspension of production, and the decrease in product demand have influenced the operation of producers and their communication with customers in 2020. This brings to the forefront the producer's role in the use of IMC for sustainable development in Latvia. The purpose of the survey of leading specialists at Latvian food producers was to find out their opinion on the trends of development and a sustainable use of IMC in business. The object of the research: IMC for sustainable development. The subject: IMC for sustainable marketing at Latvian food producers. The study uses monographic, quantitative, qualitative methods – interviews of leading specialists of producers. It represents a follow-up to the author's previous studies in the food retail industry where she researched food retail chains and conducted a survey of buyers. She developed a conceptual model of IMC for sustainable business development and found that each sector has peculiarities in product selling, service provision, etc., yet there are also common trends that apply to all industries. The author urges further market research, covering producers. The results show some trends: 1) the motivation to use IMC for sustainable development has grown due to the increased use of technologies; 2) extended periods of sedentarism have exacerbated the problem of overweight in society and given rise to demand for healthy ecological products, including natural ingredients in production; 3) the risk of employee illness and the reorganization of production has contributed to the use of digital marketing."", 'corpus_id': 237460165, 'score': 0}, {'doc_id': '167862185', 'title': ""Azteca Foods craves inventory management system : Azteca Foods spices up inventory tracking with an advanced automated system that doesn't cost too much dough"", 'abstract': None, 'corpus_id': 167862185, 'score': 1}, {'doc_id': '227181021', 'title': 'Food pantries select healthier foods after nutrition information is available on their food bank’s ordering platform', 'abstract': 'Abstract Objective: In the USA, community-based food pantries provide free groceries to people struggling with food insecurity. Many pantries obtain food from regional food banks using an online shopping platform. A food bank introduced a visible nutrition rank (i.e. green, yellow or red) onto its platform. The hypothesis was that pantry orders would increase for the healthiest options (green) and decrease for the least healthy options (red). Design: Interrupted time series (ITS) analysis of a natural experiment. Monthly data included nutrition ranks of available inventory and itemised records of all products ordered during the 15-month baseline period and 14-month intervention. Setting: A New England food bank. Participants: The twenty-five largest food pantries in the network based on pounds of food ordered. Results: Descriptive analyses of 63 922 pantry ordering records before and after the visible ranks identified an increase in the proportion of green items ordered (39·3–45·4 %) and a decrease in the proportion of red items ordered (10·5–5·1 %). ITS analyses controlling for monthly changes in inventory available and pantry variables indicated that average monthly orders of green items increased by 1286 pounds (P < 0·001) and red orders decreased by 631 pounds (P = 0·045). Among the largest changes were increases in orders of fresh produce, brown rice, low-fat dairy and low-fat meats and decreases in orders of sugary juice drinks, canned fruit with added sugar, higher fat dairy and higher fat meats. Conclusions: This promising practice can support system-wide efforts to promote healthier foods within the food banking network.', 'corpus_id': 227181021, 'score': 1}, {'doc_id': '73482319', 'title': 'A systematic review of food pantry-based interventions in the USA', 'abstract': 'Abstract Objective Food pantries play a critical role in combating food insecurity. The objective of the present work was to systematically review and synthesize scientific evidence regarding the effectiveness of food pantry-based interventions in the USA. Design Keyword/reference search was conducted in PubMed, Web of Science, Scopus, Cochrane Library and CINAHL for peer-reviewed articles published until May 2018 that met the following criteria. Setting: food pantry and/or food bank in the USA; study design: randomized controlled trial (RCT) or pre–post study; outcomes: diet-related outcomes (e.g. nutrition knowledge, food choice, food security, diet quality); study subjects: food pantry/bank clients. Results Fourteen articles evaluating twelve distinct interventions identified from the keyword/reference search met the eligibility criteria and were included in the review. Five were RCT and the remaining seven were pre–post studies. All studies found that food pantry-based interventions were effective in improving participants’ diet-related outcomes. In particular, the nutrition education interventions and the client-choice intervention enhanced participants’ nutrition knowledge, cooking skills, food security status and fresh produce intake. The food display intervention helped pantry clients select healthier food items. The diabetes management intervention reduced participants’ glycaemic level. Conclusions Food pantry-based interventions were found to be effective in improving participants’ diet-related outcomes. Interventions were modest in scale and usually short in follow-up duration. Future studies are warranted to address the challenges of conducting interventions in food pantries, such as shortage in personnel and resources, to ensure intervention sustainability and long-term effectiveness.', 'corpus_id': 73482319, 'score': 1}, {'doc_id': '235828625', 'title': 'Research on Innovative Business Plan. Smart Cattle Farming Using Artificial Intelligent Robotic Process Automation', 'abstract': 'Integrating livestock management with the required devices and sensors is now seen as a critical factor in the agricultural sector’s long-term success. The findings revealed that the agricultural business sector is open to implementing Information and Communication Technology (ICT) solutions, so the aim of this paper is to determine how advantageous it is for Romanian farmers to invest in a project that employs smart cattle farming methods that incorporate Artificial Intelligence (AI), Robotic Process Automation (RPA) and the Internet of Things (IOT). An unstructured interview was used to gather empirical evidence during a focus group meeting. Analyzing the selected primary performance metrics, it was projected that the farm’s profitability would increase by 19 percent, productivity would increase by 21 percent, and the farm’s environmental impact would decrease by 22 percent. Automation and remote work would help minimize the farm’s worker burden while also making control panels, decision-making files, and data analysis more available. In order for the domain to be as prosperous as possible, farmers must be made aware of the benefits of using these emerging technologies for closing the gap between farmers and Information Technology (IT) solution providers, and this can be accomplished through continuous training for both farmers and their technology vendors.', 'corpus_id': 235828625, 'score': 0}, {'doc_id': '235819168', 'title': 'The impact of trust, convenience, and risk perception on online frozen food purchase decision', 'abstract': ""The Covid 19 pandemic, which is still ongoing, necessitates that we be able to respond to new behaviors by introducing health precautions such as preserving cleanliness and keeping distance by avoiding crowds in order to limit the spread of Covid 19. This undoubtedly has an effect on people's lifestyles, particularly purchasing patterns. Some people also opt to shop only once a week in order to minimize their encounters with a large number of people. As a result, they choose frozen foods that can be processed for an extended period of time and prefer to buy online. When conducting online transactions, it is common to notice the risks that can arise. The aim of this analysis is to evaluate the impact of confidence, convenience, and risk perception on online purchases of frozen food items. This study employs quantitative approaches as well as survey analysis. The study data where gathered by providing questionnaires to 100 frozen food customers. SPSS 25.0 was used to process the obtained data. According to the findings of partial data processing, ease and risk expectations influence the decision to buy frozen food items online. However, confidence has no bearing on the decision to buy frozen food items online. Meanwhile, the effects of simultaneous data processing suggest that trust, convenience and risk perception have an impact on the decision to buy frozen food online."", 'corpus_id': 235819168, 'score': 0}, {'doc_id': '83218573', 'title': 'Tracking the nutrient profile of food bank inventory', 'abstract': None, 'corpus_id': 83218573, 'score': 1}]"
199	{'doc_id': '234321598', 'title': 'Recent advances in nano-encapsulation technologies for controlled release of biostimulants and antimicrobial agents', 'abstract': 'Abstract Natural antimicrobials and biostimulants hamper the negative impact of agrochemicals and their delivery needs a proper carrier system. For these bioactives, nano-encapsulation technologies are widely applied in the field of food and pharmaceuticals and rapidly taking center stage in agriculture. The present chapter highlights the nano-encapsulation technologies and their applications in the delivery of bioactive ingredients. Nanoencapsulation techniques such as those based on lipid-based delivery systems, electro-spinning, electro-spraying, and complex coacervation are described. Moreover, new-generation delivery systems like nanostructured lipid carriers, solid lipid nanoparticles, and biological nanocarriers are highlighted along with their functionalities for entrapping bioactive compounds. The release mechanisms of active ingredients from nanocapsules are explained at the end of the chapter. With a substantial improvement over conventional agricultural methodologies, these promising nanoencapsulation technologies have tremendous scope for the effective and sustained release of bioactives for the precision crop protection and production.', 'corpus_id': 234321598}	10957	[{'doc_id': '236319243', 'title': 'Seed Treatment with Biostimulants Extracted from Weeping Willow (Salix babylonica) Enhances Early Maize Growth', 'abstract': 'Biostimulants can be used as innovative and promising agents to address current needs of sustainable agriculture. Weeping willow tree (Salix babylonica) extracts are rich in many bioactive compounds, including, but not limited, to salicylates and phenolics. In this study, the potential of willow bark (WB) and willow leaf (WL) extracts is evaluated as plant-based biostimulants to improve the early growth of maize (Zea mays) under control and salinity stress conditions. In 3 days, seed treatment with salicylic acid and willow extract increased the shoot FW of maize seedlings 130% and 225%, respectively. The root area was, on average, enhanced by 43% with SA and 87% with willow extract applications. Moreover, these extracts increased the leaf protein concentration and reduced the negative effects of salinity during early growth. Reductions in lipid peroxidation and specific activities of antioxidative enzymes by seed treatments with willow extracts suggests a mitigation of salinity-induced oxidative stress. For most reported traits, WL applications were at least as effective as WB applications. Results indicate that aqueous extracts of weeping willow leaves, as well as bark, can be used as seed treatment agents with biostimulant activity to improve seedling growth and establishment under control and stress conditions.', 'corpus_id': 236319243, 'score': 1}, {'doc_id': '237959767', 'title': 'Role of Jasmonic Acid and Salicylic Acid Signaling in Secondary Metabolite Production', 'abstract': 'Secondary metabolite synthesis takes place from primary metabolites. Secondary metabolites have no role in the development of plant growth, reproduction, and physiological procedures but they are needed for competitive weapons against a wide range of plant pathogens. Natural products as bioactive compounds derived from plants are mainly produced by the secondary metabolism. These metabolites are known for their wide therapeutic values. Plants are utilized for the isolation of various bioactive compounds; so they are sometimes overexploited and are getting threatened. Therefore, this problem can be overcome by enhancement of secondary metabolite production with various in vitro culture procedures. Elicitation by different molecules upscales the secondary metabolites production in a number of plants with varied potential for bioactive metabolite accumulation. Jasmonic acid (JA) and salicylic acid (SA) are significant molecules involved in the regulation of plant growth, immunity to pathogens, and abiotic stresses. The present review categorizes synthesis and enhancement of various bioactive compounds by JA and SA as elicitation using in vitro cultures. We also discuss the inception of JA and SA signaling with a focus on gene expression in relation to secondary metabolite biosynthesis.', 'corpus_id': 237959767, 'score': 1}, {'doc_id': '227126945', 'title': 'Mixing strategies combined with shape design to enhance productivity of a raceway pond', 'abstract': 'This paper focuses on mixing strategies and designing shape of the bottom topographies to enhance the growth of the microalgae in raceway ponds. A physical-biological coupled model is used to describe the growth of the algae. A simple model of a mixing device such as a paddle wheel is also considered. The complete process model was then included in an optimization problem associated with the maximization of the biomass production. The results show that non-trivial topographies can be coupled with some specific mixing strategies to improve the microalgal productivity.', 'corpus_id': 227126945, 'score': 0}, {'doc_id': '227121838', 'title': 'A review on hospital wastewater treatment: A special emphasis on occurrence and removal of pharmaceutically active compounds, resistant microorganisms, and SARS-CoV-2', 'abstract': '\n The hospital wastewater imposes a potent threat to the security of human health concerning its high vulnerability towards the outbreak of several diseases. Furthermore, the outbreak of COVID-19 pandemic demanded a global attention towards monitoring viruses and other infectious pathogens in hospital wastewater and their removal. Apart from that, the presence of various recalcitrant organics, pharmaceutically active compounds (PhACs), etc. imparts a complex pollution load to water resources and ecosystem. In this review, an insight into the occurrence, persistence and removal of drug-resistant microorganisms and infectious viruses as well as other micro-pollutants have been documented. The performance of various pilot/full-scale studies have been evaluated in terms of removal of biochemical oxygen demand (BOD), chemical oxygen demand (COD), total suspended solids (TSS), PhACs, pathogens, etc. It was found that many biological processes, such as membrane bioreactor, activated sludge process, constructed wetlands, etc. provided more than 80% removal of BOD, COD, TSS, etc. However, the removal of several recalcitrant organic pollutants are less responsive to those processes and demands the application of tertiary treatments, such as adsorption, ozone treatment, UV treatment, etc. Antibiotic-resistant microorganisms, viruses were found to be persistent even after the treatment of hospital wastewater, and high dose of chlorination or UV treatment was required to inactivate them. This article circumscribes the various emerging technologies, which have been used to treat PhACs and pathogens. The present review also emphasized the global concern of the presence of SARS-CoV-2 RNA in hospital wastewater and its removal by the existing treatment facilities.\n', 'corpus_id': 227121838, 'score': 0}, {'doc_id': '227108077', 'title': 'Determining the Effects of Manganese Source and Level in Diets Containing High Levels of Copper on Growth Performance of Growing-Finishing Pigs', 'abstract': 'A total of 1,994 pigs (PIC; 337 × 1050; initially 88.2 lb) were used to determine the effect of manganese (Mn) source and level on finishing pig growth performance. This experiment was a follow-up to an Mn source by level study conducted last year. However, unlike last year’s study, in the present study all diets contained 150 ppm added Cu from Cu hydroxychloride (IBC; Micronutrients, Indianapolis, IN). Dietary treatments were arranged in a 2 × 3 factorial with main effects of Mn source (Mn hydroxychloride, IntelliBond M, Micronutrients, Indianapolis, IN; or Mn sulfate, MnSO4, Eurochem, Veracruz, Mexico), and increasing added Mn concentration (8, 16, and 32 ppm). The trace mineral premix was formulated without added Mn. There were 27 pigs per pen and 12 pens per treatment. Diets were corn-soybean meal-distillers dried grains with solubles-based and were fed in 4 phases. Overall, there was no (P > 0.10) Mn source × level interaction observed for average daily gain (ADG), average daily feed intake (ADFI), and feed efficiency (F/G). Pigs fed IBM had increased (P < 0.05) final body weight (BW), ADG, and ADFI compared to pigs fed MnSO4. Pigs fed 16 ppm of Mn tended (P = 0.088) to have reduced ADFI when compared to pigs fed 8 and 32 ppm of Mn. In conclusion, there appears to be little benefit in growth performance by feeding more than 8 ppm of added Mn. However, pigs fed IBM had improved growth performance compared with those fed MnSO4. This response is different than our previous study with identical Mn sources but without high levels of added Cu. Further research is needed to understand why we observed an Mn source difference to Mn hydroxychloride when fed in conjunction with pharmacological levels of Cu on pig growth performance.', 'corpus_id': 227108077, 'score': 0}, {'doc_id': '227132557', 'title': 'Panax ginseng C. A. Meyer as a potential therapeutic agent for organ fibrosis disease', 'abstract': 'Background Ginseng ( Panax ginseng C. A. Meyer), a representative Chinese herbal medicine, can improve the body’s antioxidant and anti-inflammatory capacity. Recently, scientists have shifted emphasis towards the initial stages of different malignant diseases—corresponding organ fibrosis and explored the essential role of P. ginseng in the treatment of fibrotic diseases. Main body In the first instance, the review generalizes the molecular mechanisms and common therapeutic methods of fibrosis. Next, due to the convenience and safety of individual medication, the research progress of ginseng extract and formulas in treating liver fibrosis, pulmonary fibrosis, myocardial fibrosis, and renal fibrosis has been systematically summarized. Finally, we describe active ingredients isolated from P. ginseng for their outstanding anti-fibrotic properties and further reveal the potential therapeutic prospect and limitations of P. ginseng in fibrotic diseases. Conclusions P. ginseng can be regarded as a valuable herbal medicine against fibrous tissue proliferation. Ginseng extract, derived formulas and monomers can inhibit the abundant deposition of extracellular matrix which caused by repeated damage and provide protection for fibrotic organs. Although the molecular mechanisms such as transforming growth factor β signal transduction have been confirmed, future studies should still focus on exploring the underlying mechanisms of P. ginseng in treating fibrotic disease including the therapeutic targets of synergistic action of multiple components in P. ginseng . Moreover, it is also necessary to carry out clinical trial to evaluate the feasibility of P. ginseng in combination with common fibrosis drugs.', 'corpus_id': 227132557, 'score': 0}, {'doc_id': '23911321', 'title': 'Degradation of Soil Humic Extract by Wood- and Soil-Associated Fungi, Bacteria, and Commercial Enzymes', 'abstract': 'A bstractAn alkaline humic extract (HE) of a black calcareous forest mull was exposed to 36 fungal and 9 eubacterial isolates in liquid standing culture. At 21 d in fungi, and 4 d in bacteria, the groups of wood-degrading basidiomycetes, terricolous basidiomycetes, ectomycorrhizal fungi, soil-borne microfungi, and eubacteria had reduced the absorbance (A340) of HE media by 57, 28, 19, 26 and 5%, respectively. Gel permeation chromatography revealed that the large humic acid molecules were more readily degraded than the smaller fulvic acid molecules and served as a sole source of carbon and energy. The more active HE degraders reduced the overall molecular weight of humic and fulvic acids by 0.25 to 0.47 kDa. They also reduced the chemical reactivity of HE to tetrazotized o-dianisidine, indicating the degradation of hydroxylated aromatic molecules (which are responsible for this reaction). Decreases in absorbance, molecular weight, and reactivity were caused by fungal manganese peroxidase, horseradish peroxidase, β-glucosidase, and abiotic oxidants such as H2O2 and Mn(III) acetate. It is concluded that fungi, some of which are propagated in contaminated soils to control xenobiotics, metabolize HE compounds enzymatically. They use enzymes which are also involved in the degradation of soil xenobiotics. Because of reductions in the molecular weight of HE, which is a potential carrier of heavy metal ions and xenobiotics, solubility and motility of humic substances in soil and surface waters are increased.', 'corpus_id': 23911321, 'score': 1}, {'doc_id': '235732845', 'title': 'Exogenous ethylene reduces growth via alterations in central metabolism and cell wall composition in tomato (Solanum lycopersicum).', 'abstract': 'Ethylene is a gaseous hormone with a well-established role in the regulation of plant growth and development. However, its role in the modulation of carbon assimilation and central metabolism remains unclear. Here, we investigated the morphophysiological and biochemical responses of tomato plants (Solanum lycopersicum) following the application of ethylene in the form of ethephon (CEPA - 2-chloroethylphosphonic acid), forcing the classical triple response phenotype. CEPA-treated plants were characterized by growth inhibition, as revealed by significant reductions in both shoot and root dry weights, coupled with a reduced number of leaves and lower specific leaf area. Growth inhibition was associated with a reduction in carbon assimilation due to both lower photosynthesis rates and stomatal conductance, coupled with impairments in carbohydrate turnover. Furthermore, exogenous ethylene led to the accumulation of cell wall compounds (i.e., cellulose and lignin) and phenolics, indicating that exposure to exogenous ethylene also led to changes in specialized metabolism. Collectively, our findings demonstrate that exogenous ethylene disrupts plant growth and leaf structure by affecting both central and specialized metabolism, especially that involved in carbohydrate turnover and cell wall biosynthesis, ultimately leading to metabolic responses that mimic stress situations.', 'corpus_id': 235732845, 'score': 1}, {'doc_id': '227070180', 'title': 'EMPIRICAL STUDY OF KEY SUCCESS FACTORS OF CLOSED SUPPLY CHAIN FOR VEGETABLES: THE CASE OF EXPORT FROM YUNAN TO THAILAND', 'abstract': 'This paper analyzes the key success factors of the closed supply chain for vegetables exported from Yunnan and Thailand by empirical study, with the aim of guaranteeing the quality and safety of vegetables. After the introduction, literature reviews with the key term of vegetable quality and safety are presented. Followed section 3 indicates data sources and methodologies from both quantitative and qualitative approaches. Then operation results and discussion are shown to determine the key success factors. The last section proposes some recommendations for member enterprises and related government departments.', 'corpus_id': 227070180, 'score': 0}, {'doc_id': '236588605', 'title': 'Antibacterial activity of tannins towards Pseudomonas syringae pv. tomato, and their potential as biostimulants on tomato plants', 'abstract': 'Copyright: © 2021 P. Canzoniere, S. Francesconi, S. Giovando, G. M. Balestra. This is an open access, peerreviewed article published by Firenze University Press (http://www.fupress. com/pm) and distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.', 'corpus_id': 236588605, 'score': 1}]
