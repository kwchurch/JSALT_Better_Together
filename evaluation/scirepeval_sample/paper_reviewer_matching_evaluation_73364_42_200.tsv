	doc_id	title	abstract	corpus_id
0	91a5c521e12b74d42f4941a0df2094dab41594c4	Statistical reconstruction for quantitative CT applications	This paper summarizes considerations in developing statistical reconstruction algorithms for polyenergetic X-ray CT. The algorithms are based on Poisson statistics and polyenergetic X-ray attenuation physics and object models. In single-kVp scans, object models enable estimates of the contributions of bone and soft tissue at every pixel, based on prior assumptions about the tissue properties. In dual-kVp scans, one can estimate water and bone images independently. Preliminary results with fan-beam data from two cone beam systems show better accuracy for iterative methods over FBP.	1255118
1	9fed436969a59921eb5a495c8cf682cd0436902a	Antenna Pattern Measurement of the Large-Size Radio Telescope by Correlation Techniques and GNSS	It is a challenge to measure the antenna pattern of a large-sized radio telescope with a high resolution because it hardly rotates. This letter presents a simple and efficient method to measure the large-sized radio antenna pattern based on correlation techniques and the Global Navigation Satellite System (GNSS). We estimate the normalized pattern function of the electric field by utilizing correlation properties of the navigation signal in the auxiliary antenna and the radio antenna. The method avoids obtaining a large number of accurate pseudo-code information. Compared to the traditional methods based on the energy detection, the proposed method extends the measurement range and improves the measurement accuracy of the antenna pattern. At the same time, the method also has a better performance in anti-variation of channel and anti-interference.	127421397
2	ad67ccee45b801b0138016e2f44a566344e77320	Semi-supervised Learning by Entropy Minimization	"We consider the semi-supervised learning problem, where a decision rule is to be learned from labeled and unlabeled data. In this framework, we motivate minimum entropy regularization, which enables to incorporate unlabeled data in the standard supervised learning. Our approach includes other approaches to the semi-supervised problem as particular or limiting cases. A series of experiments illustrates that the proposed solution benefits from unlabeled data. The method challenges mixture models when the data are sampled from the distribution class spanned by the generative model. The performances are definitely in favor of minimum entropy regularization when generative models are misspecified, and the weighting of unlabeled data provides robustness to the violation of the ""cluster assumption"". Finally, we also illustrate that the method can also be far superior to manifold learning in high dimension spaces."	7890982
3	4abf8640db962fcdcae4cb0cd2e63885eb7344dc	A VLSI architecture for radix-2/sup k/ Viterbi decoding with transpose algorithm	The paper presents a novel transpose path metric (TPM) algorithm to reduce the interconnection routing complexity for a radix-2/sup k/ Viterbi decoder. With simple local interconnections, the algorithm can provide a permutation function for state rearrangement in a transpose strategy. With features of modulation and regularity, this algorithm is very suitable for VLSI implementation; consequently, a larger memory length VA decoder can be constructed with several smaller memory length modules. Finally, a VLSI architecture for a 16-states radix-4 VA decoder using TPM has been developed.	60592177
4	7eab3c593e4fd28adfec3c0523b9510cea4838af	Design of steerable filters for the detection of micro-particles	This paper presents two contributions. We first introduce a continuous-domain version of Principal-Component Analysis (PCA) for designing steerable filters so that they best approximate a given set of image templates. We exploit the fact that steerability does not need to be enforced explicitly if one extends the set of templates by incorporating all their rotations. Our results extend previous work by Perona to multiple templates. We then apply our framework to the automatic detection and classification of micro-particles that carry biochemical probes for molecular diagnostics. Our continuous-domain PCA formalism is particularly well adapted in this context because the geometry of the carriers is known analytically. In addition, the steerable structure of our filters allows for a fast FFT-based recognition of the type of probe.	14926948
5	0191e322146aea376d627234ef85ce29f1912adb	A lightweight multiview tracked person descriptor for camera sensor networks	We present a simple multiple view 3D model for object tracking and identification in camera networks. Our model is composed of 8 distinct views in the interval [0, 7pi/4]. Each of the 8 parts describes the person's appearance from that particular viewpoint. The model contains both color and structure information about each view which are assembled into a single entity and is meant as a simple, lightweight object representation for use in camera sensor networks. It is versatile in that it can be gradually assembled on-line while a person is tracked. The model's ease of use and effectiveness for identification in surveillance video is demonstrated.	14893323
6	f0992f17bf94f67ca6125d4f425ec6ab4c73f3a3	Entropy Numbers for Convex Combinations and MLPs	This chapter contains sections titled: Introduction, Tools from Functional Analysis, Convex Combinations of Parametric Families, Convex Combinations of Kernels, Multilayer Networks, Discussion, Appendix: A Remark on Traditional Weight Decay, Appendix: Proofs	118355980
7	228f2d45700346466fd7dd94e3283c0d4e869379	Enron case study: Analysis of email behavior using EmailTime	This paper presents a case study with Enron email dataset to explore the behaviors of email users within different organizational positions. We defined email behavior as the email activity level of people regarding a series of measured metrics e.g. sent and received emails, numbers of email addresses, etc. These metrics were calculated through EmailTime, a visual analysis tool of email correspondence over the course of time. Results showed specific patterns in the email datasets of different organizational positions.	9264929
8	4315b147fed81f96d7467693ffbdac5173b15ddf	A new clustering evaluation function using Renyi's information potential	Clustering is an important unsupervised learning paradigm, but so far the traditional methodologies are mostly based on the minimization of the variance between the data and the cluster means. Here we propose a new evaluation function based on a previously developed information theoretic measure defined from Renyi's (1960) entropy. We show how to apply Renyi's entropy to clustering and analyze the resulting staircase nature of the performance function that can be expected during learning. We suggest simulated annealing as a possible optimization criterion.	9918352
9	9dbec260d4ff68b183feda51d585ac2f839b2b72	Discovering single classes in remote sensing images with active learning	When dealing with supervised target detection, the acquisition of labeled samples is one of the most critical phases: the samples must be yet representative of the class of interest, but must also be found among a vast majority of non-target examples. Moreover, the efficiency of the search is also an issue, since the samples labeled as background are not used by target detectors such as the support vector data description (SVDD). In this work we propose a competitive and effective approach to identify the most relevant training samples for one-class classification based on the use of an active learning strategy. The SVDD classifier is first trained with insufficient target examples. It is then used to detect the most informative samples to be labeled by a user through active learning techniques. By selecting unlabeled samples in a smart way and by adopting a diversity criterion, it is possible to obtain an accurate description of the class of interest with a relatively small number of training samples. The performance of the proposed method is illustrated in a change detection scenario and is validated by comparison with state-of-art active learning techniques originally developed for multiclass problems.	14435217
10	cdd759240d2bdae429ad0821cd5a2a01ac358a1b	Algebraic signatures for scalable distributed data structures	Signatures detect changes to data objects. Numerous schemes are in use, especially the cryptographically secure standards SHA-1. We propose a novel signature scheme which we call algebraic signatures. The scheme uses the Galois field calculations. Its major property is the sure detection of any changes up to a parameterized size. More precisely, we detect for sure any changes that do not exceed n-symbols for an n-symbol algebraic signature. This property is new for any known signature scheme. For larger changes, the collision probability is typically negligible, as for the other known schemes. We apply the algebraic signatures to the scalable distributed data structures (SDDS). We filter at the SDDS client node the updates that do not actually change the records. We also manage the concurrent updates to data stored in the SDDS RAM buckets at the server nodes. We further use the scheme for the fast disk backup of these buckets. We sign our objects with 4-byte signatures, instead of 20-byte standard SHA-1 signatures. Our algebraic calculus is then also about twice as fast.	9757504
11	1c1f1aff8bd0d552980a51e5e93abf0ca3b2d8e4	Corpus-Based Induction of Syntactic Structure : Models of Constituency and Dependency	The task of statistically inducing hierarchical syntactic structure over unannotated sentences of natural language has received a great deal of attention (Carroll and Charniak, 1992a; Pereira and Schabes, 1992; Brill, 1993; Stolcke and Omohundro, 1994). Researchers have explored this problem for a variety of reasons: to argue empirically against the poverty of the stimulus (Clark, 2001), to use induction systems as a first stage in constructing large treebanks (van Zaanen, 2000), to build better language models (Baker, 1979; Chen, 1995), and to examine psychological issues in language learning (Solan et al., 2003). An important distinction should be drawn between work primarily interested in the weak generative capacity of models, where modeling hierarchical structure is only useful insofar as it leads to improved models over observed structures (Baker, 1979; Chen, 1995), and work interested in the strong generative capacity of models, where the unobserved structure itself is evaluated (van Zaanen, 2000; Clark, 2001; Klein and Manning, 2002). This paper falls into the latter category; we will be inducing models of linguistic constituency and dependency with the goal of recovering linguistically plausible structures. We make no claims as to the congitive plausibility of the induction mechanisms we present here, however the ability of these systems to recover substantial linguistic patterns from surface yields alone does speak to the strength of support for these patterns in the data, and hence to undermine arguments based on “the poverty of the stimulus” (Chomsky, 1965). 2 Distributional Syntax Induction	10438770
12	1a29597ec321e940748fd2afb2905bd47ef2fc15	is achieved. Prior to stabilization, neural networks do not jump around between points in activation space. Stabiliza-tion is the process whereby a network first generates a de-terminate activation pattern, and thereby arrives at a point in activation space		115300137
13	b1161d6dfb99a68a5b88876b5bbea772f620cc9d	Learning the Kernel Matrix for Superresolution	This paper proposes the application of learned kernels in support vector regression to superresolution in the discrete cosine transform (DCT) domain. Though previous works involve kernel learning, their problem formulation is examined to reformulate the semi-definite programming problem of finding the optimal kernel matrix. For the particular application to superresolution, downsampling properties derived in the DCT domain are exploited to add structure to the learning algorithm. The advantage of the proposed method over other learning-based superresolution algorithms include specificity with regard to image content, structured consideration of energy compaction, and the added degrees of freedom that regression has over classification-based algorithms	15477997
14	fb8dac27694a14b9b1ff9ef1fe04387bc14468f1	Spectral Kernel Methods for Clustering	In this paper we introduce new algorithms for unsupervised learning based on the use of a kernel matrix. All the information required by such algorithms is contained in the eigenvectors of the matrix or of closely related matrices. We use two different but related cost functions, the Alignment and the 'cut cost'. The first one is discussed in a companion paper [3], the second one is based on graph theoretic concepts. Both functions measure the level of clustering of a labeled dataset, or the correlation between data clusters and labels. We state the problem of unsupervised learning as assigning labels so as to optimize these cost functions. We show how the optimal solution can be approximated by slightly relaxing the corresponding optimization problem, and how this corresponds to using eigenvector information. The resulting simple algorithms are tested on real world data with positive results.	10304437
15	aa62f369d20dc644eb5cf8b88599900f6cbe7bca	SNAPPER: Fashion Coordinate Image Retrieval System	Fashion is one of the most important factors in our lives. A lot of people enjoy their fashion. On the other hand, they are not confident about their fashion coordination. Especially, it is a very worrisome problem to purchase a new fashion item that suits to their clothing. In this paper, we propose an image-retrieval-based system for supporting a person's fashion coordination and purchase of new fashion items. This system provides users an efficient way to know and purchase the fashion items which suit to those in their closets. The user uploads the image of his or her clothing and specifies the part which he or she puts it on. Then, our system retrieves similar items and recommends fashion coordinate from our fashion coordinate database that has been collected from fashion websites.	12438506
16	c7ca425c1dc2704d6748c42597ab2e47b48c1799	Efficient stereo video encoding for mobile applications using the 3D+F codec	This paper presents a stereo video codec called 3D+F which has specifically been designed to meet the low complexity requirements imposed by mobile applications while trying to be competitive with the state-of-the-art coders, such as the multi view video coding (MVC) extension of the H.264/AVC standard, in terms of rate-distortion (RD). By exploiting the stereo geometry of stereoscopic image pairs it is possible to transcode a 3D movie into a 2D movie and a 2-bit labeling. The 2D movie can then be compressed using any video coder while the labeling needs to be compressed losslessly. Preliminary subjective testing shows that the resulting quality has potential to be very competitive.	12346051
17	23f7c32dd053978fb913ee2cf28f4c109fc22ac8	Detecting Forgery From Static-Scene Video Based on Inconsistency in Noise Level Functions	Recently developed video editing techniques have enabled us to create realistic synthesized videos. Therefore, using video data as evidence in places such as courts of law requires a method to detect forged videos. In this study, we developed an approach to detect suspicious regions in a video of a static scene on the basis of the noise characteristics. The image signal contains irradiance-dependent noise the variance of which is described by a noise level function (NLF) as a function of irradiance. We introduce a probabilistic model providing the inference of an NLF that controls the characteristics of the noise at each pixel. Forged pixels in the regions clipped from another video camera can be differentiated by using maximum a posteriori estimation for the noise model when the NLFs of the regions are inconsistent with the rest of the video. We demonstrate the effectiveness of our proposed method by adapting it to videos recorded indoors and outdoors. The proposed method enables us to highly accurately evaluate the per-pixel authenticity of the given video, which achieves denser estimation than prior work based on block-level validation. In addition, the proposed method can be applied to various kinds of videos such as those contaminated by large noise and recorded with any scan formats, which limits the applicability of the existing methods.	8139575
18	83e6b767cecdfbf99b4c9a346c17fd45f3849050	Study on the interactive relationship between specialized market and industrial clusters of textiles and clothing in China	The purpose of this paper is to explore how the textiles and clothing industry cluster and specialized market influence each other. This research observed and documented a case study of industry development in Changshu of Jiangsu province. Through the analysis, the researchers summarized the factors that account for significantly improvement of competiveness of clothing industry in Changshu. The research indicates that the relationship between industrial cluster and specialized market is interdependent and mutually promotive. As the industry is one pillar of China's economy, it's important to study this interactive mechanism for further growth of the industry in a global competition situation.	45511302
19	3133a9ec787dd2f2a50ed7c927e5d867482375ab	Parametric Distance Metric Learning with Label Information	Distance-based methods in pattern recognition and machine learning have to rely on a similarity or dissimilarity measure between patterns in the input space. For many applications, Euclidean distance in the input space is not a good choice and hence more complicated distance metrics have to be used. In this paper, we propose a parametric method for metric learning based on class label information. We first define a dissimilarity measure that can be proved to be metric. It has the favorable property that between-class dissimilarity is always larger than within-class dissimilarity. We then perform parametric learning to find a regression mapping from the input space to a feature space, such that the dissimilarity between patterns in the input space is approximated by the Euclidean distance between points in the feature space. Parametric learning is performed using the iterative majorization algorithm. Experimental results on real-world benchmark data sets show that this approach is promising.	119018298
20	4f4d107584a6ec1d79d59adeab32faae81c497e5	A model-based method with joint sparsity constraint for direct diffusion tensor estimation	Diffusion tensor imaging (DTI) has been widely used for nondestructive characterization of microstructures of myocardium or brain connectivity. It requires repeated acquisition with different diffusion gradients. The long acquisition time greatly limits the clinical application of DTI. In this paper, a novel method, named model-based method with joint sparsity constraint (MB-JSC), effectively incorporates the prior information on the joint sparsity of different diffusion-weighted images in direct estimation of the diffusion tensor from highly undersampled k-space data. Experimental results demonstrate that the proposed method is able to estimate the diffusion tensors more accurately than the existing method when a high net reduction factor is used.	2189946
21	9b670525e9fb64509c3fb98dd0d65ed63b14c708	Multiple-antenna differential lattice decoding	From a lattice viewpoint, Clarkson, Sweldens and Zheng significantly reduced the complexity of multiantenna differential decoding. Their approximate decoding algorithm, however, has not unleashed the full potential of lattice decoding. In this paper, we present several improved algorithms, generally referred to as differential lattice decoding (DLD), for multiantenna communication. We first analyze two distinct approximate DLD algorithms, and then develop an algorithm that exactly finds the closest lattice point in the Euclidean space. This exact DLD is subsequently augmented by local search to compensate for the remaining approximation. The small amount of extra complexity of the exact or augmented DLD is rewarded by a clear performance gain. We find that employing basis reduction is very effective to reduce the overall decoding complexity for high lattice dimensions. Moreover, the dimension of the lattice defined in this paper is independent of the number of receive antennas, which results in not only lower complexity, but also better performance for a multiantenna receiver.	10940850
22	18bb76f5c7a10c6f79b0b842cf626e8cf65eaefb	Ant stigmergy shop floor control architecture for intelligent product oriented manufacturing system	In this paper we consider an alternative approach to the problem of infusing local shop floor control decisions with more global performance information. The approach proposed is that of using artificial ant colonies to decentralize and autonomize the shop floor routing. The proposed ant stigmergy shop floor control (ASSFC) has two functional levels: a virtual shop floor level in which ant agents explore optimal processing route stochastically, and a physical shop floor in which intelligent work pieces (IWPs) deterministically exploit the best paths that have been detected by their corresponding ant agents on virtual shop floor. The simple indirect communication and the robust adaptability to disturbances make the ASSFC more suitable for intelligent product oriented manufacturing system than tradition shop floor control.	9484222
23	9a3ac96e1add4369abdf32a128a17a18be1c3ae0	Computer Vision for Human–Machine Interaction: Probabilistic Models of Verbal and Body Gestures	This chapter describes several probabilistic techniques for representing, recognizing, and generating spatiotemporal connguration sequences. We rst describe how such techniques can be used to visually track and recognize lip movements to augment a speech recognition system. We then demonstrate additional techniques that can be used to animate video footage of talking faces and synchronize it to diierent sentences of an audio track. Finally we outline alternative low-level representations that are needed to apply these techniques to articulated body gestures. 7.1 Introduction Gestures can be described as characteristic conngurations over time. While uttering a sentence we express very ne grained verbal gestures as complex lip conngurations over time, and while performing body actions, we generate articulated connguration sequences of jointed arm and leg segments. Such conngurations lie in constrained subspaces and diierent y Part of this report is published at ICCV'95 9]	15661420
24	a00685aaa3683d8e197fae1dd80219d8c5c7cf0a	Maximum-likelihood estimation for articulatory speech recognition using a stochastic target model		39756606
25	636dad2afa8e41f9aa4faf6434418cfe3cbaa3ff	The CRLB analysis for target localization behind wall and in free space	In through-the-wall target localization area, wall effect including absorption and refraction will arise, wall effect needs to be compensated. If wall parameters including thickness and dielectric constant are exactly known, the target can be compensated to its correct position. In this paper, based on the transmitting model of through-the-wall radar, we deduced the Cramer-Rao lower bound (CRLB) of target localization behind wall. Due to the wall effect, the target localization CRLB behind wall is better than that in free space. We also validated our conclusions using wavenumber domain analysis. Simulation results evaluated the effect of wall thickness and dielectric constant on CRLB qualitatively.	23435197
26	a02f70c8d58d1a407a087636e57c19b706fb4090	UN-MUSIC and UN-CLE: an application of generalized correlation analysis to the estimation of the direction of arrival of signals in unknown correlated noise	A new approach is proposed for the consistent estimation of the directions of arrival (DOA) of signals in an unknown spatially-correlated noise environment. The signal and noise model used is based on the assumption that the data are received by two arrays well separated so that their noise outputs are uncorrelated. The generalized correlation decomposition of the cross-correlation matrix between the two arrays is then introduced. Of particular interest is the canonical correlation decomposition. The analysis of the generalized correlation leads to various interesting geometric and asymptotic properties of the eigenspace structure. Two algorithms, UN-MUSIC and UN-CLE, are developed to estimate the DOA of signals in unknown spatially correlated noise based on the utilization of these properties. Computer simulations show that these methods are superior in performance compared to conventional methods. Furthermore, it is demonstrated that the new methods are equally effective even when only one sensor array is employed. >	28674631
27	dd3fffe3093915bbd1ffbf261b25947aa508b03d	Joint optimization of scalar and tree-structured quantization of wavelet image decompositions	Wavelet image decompositions generate a tree-structured set of coefficients, providing an hierarchical data-structure for representing images. While early wavelet-based algorithms for image compression concentrated on optimal quantization of wavelet coefficients, several recent researchers have proposed approaches which couple coefficient quantization (either scalar or vector-based) with various strategies for quantizing the tree itself. This paper proposes an image compression algorithm based on optimal bit rate allocation between scalar and tree-structured quantizers. A predictive approach to representing the pruned tree structure is presented, and the entropy of this representation is included in the optimal allocation problem. The algorithm couples Lagrangian optimization of scalar quantizers with a marginal analysis approach for optimizing the tree structure, and achieves excellent coding efficiency in the rate-distortion sense.<<ETX>>	15560563
28	94066dc12fe31e96af7557838159bde598cb4f10	Policy Invariance Under Reward Transformations: Theory and Application to Reward Shaping	This paper investigates conditions under which modi cations to the reward function of a Markov decision process preserve the op timal policy It is shown that besides the positive linear transformation familiar from utility theory one can add a reward for tran sitions between states that is expressible as the di erence in value of an arbitrary poten tial function applied to those states Further more this is shown to be a necessary con dition for invariance in the sense that any other transformation may yield suboptimal policies unless further assumptions are made about the underlying MDP These results shed light on the practice of reward shap ing a method used in reinforcement learn ing whereby additional training rewards are used to guide the learning agent In par ticular some well known bugs in reward shaping procedures are shown to arise from non potential based rewards and methods are given for constructing shaping potentials corresponding to distance based and subgoal based heuristics We show that such po tentials can lead to substantial reductions in learning time	5730166
29	a1cda8e30ce35445e4f51b47ab65b775f75c9f18	Normalized face image generation with perceptron generative adversarial networks	This paper presents a deep neural architecture for synthesizing the frontal and neutral facial expression image of a subject given a query face image with arbitrary expression. This is achieved by introducing a combination of feature space perceptual loss, pixel-level loss, adversarial loss, symmetry loss, and identity-preserving loss. We leverage both the frontal and neutral face distributions and pre-trained discriminative deep perceptron models to guide the identity-preserving inference of the normalized views from expressive profiles. Unlike previous generative methods that utilize their intermediate features for the recognition tasks, the resulting expression- and pose-disentangledface image has potential for several downstream applications, such as facial expression or face recognition, and attribute estimation. We show that our approach produces photorealistic and coherent results, which assist the deep metric learning-based facial expression recognition (FER) to achieve promising results on two well-known FER datasets.	3887036
30	d76b7236a47b273063284419b76c55da291ee381	On the Complexity of Learning for Spiking Neurons with Temporal Coding	Abstract Spiking neurons are models for the computational units in biological neural systems where information is considered to be encoded mainly in the temporal patterns of their activity. In a network of spiking neurons a new set of parameters becomes relevant which has no counterpart in traditional neural network models: the time that a pulse needs to travel through a connection between two neurons (also known as delay of a connection). It is known that these delays are tuned in biological neural systems through a variety of mechanisms. In this article we consider the arguably most simple model for a spiking neuron, which can also easily be implemented in pulsed VLSI. We investigate the Vapnik–Chervonenkis (VC) dimension of networks of spiking neurons, where the delays are viewed as programmable parameters and we prove tight bounds for this VC dimension. Thus, we get quantitative estimates for the diversity of functions that a network with fixed architecture can compute with different settings of its delays. In particular, it turns out that a network of spiking neurons with k adjustable delays is able to compute a much richer class of functions than a threshold circuit with k adjustable weights. The results also yield bounds for the number of training examples that an algorithm needs for tuning the delays of a network of spiking neurons. Results about the computational complexity of such algorithms are also given.	6543171
31	69bc641357e9ce27b86ebd80b1ea680abf9d2a6b	The expected performance curve: a new assessment measure for person authentication	Keywords: learning Reference EPFL-REPORT-82993 URL: http://publications.idiap.ch/downloads/reports/2003/rr03-84.pdf Record created on 2006-03-10, modified on 2017-05-10	15524058
32	7019669fc9b5dc8b0b0933aed8ee1e4596cb3383	Simultaneous seismic compression and denoising using a lapped transform coder	Compression and denoising are two of the most successful applications of wavelets to signals and natural images. Both techniques have also been successfully applied to seismic signals, but compression is not widely accepted yet, since it is often believed to harm seismic information.	2454952
33	3798125b5843b00f9b5d441588c905e1746fc6e0	Analysis on Computer Automatic Identification of Low-Over Current for Conductor Insulating Lagging		206583074
34	7801c9d098d4c300b978ccaffca6e3b9ca3d58d9	Dots and Incipients: Extended Features for Partial Fingerprint Matching	There are fundamental differences in the way fingerprints are compared by forensic examiners and current automatic systems. For example, while automatic systems focus mainly on the quantitative measures of fingerprint minutiae (ridge ending and bifurcation points), forensic examiners often analyze details of intrinsic ridge characteristics and relational information. This process, known as qualitative friction ridge analysis [1], includes examination of ridge shape, pores, dots, incipient ridges, etc. This explains the challenges that current automatic systems face in processing partial fingerprints, mostly seen in latents. The forensics and automatic fingerprint identification systems (AFIS) communities have been active in standardizing the definition of extended feature set, as well as quantifying the relevance and reliability of these features for automatic matching systems. CDEFFS (committee to define an extended feature set) has proposed a working draft on possible definitions and representations of extended features [2]. However, benefits of utilizing these extended features in automatic systems are not yet known. While fingerprint matching technology is quite mature for matching tenprints [3], matching partial fingerprints, especially latents, still needs a lot of improvement. We propose an algorithm to extract two major level 3 feature types, dots and incipients, based on local phase symmetry and demonstrate their effectiveness in partial print matching. Since dots and incipients can be easily encoded by forensic examiners, we believe the results of this research will have benefits to next generation identification (NGI) systems.	14894885
35	e98d0826cbb2651313b44931358dbdf1312f4cc2	Tissue motion estimation using dictionary learning: Application to cardiac amyloidosis	Cardiac strain estimation from ultrasound images is an efficient tool for the diagnosis of cardiac diseases. This study focuses on cardiac amyloidosis (CA), a pathology characterized by nonspecific early symptoms such as increased wall thickness. Recent clinical studies have demonstrated that patients with CA present an apex-to-base gradient longitudinal strain pattern, i.e., a normal strain in apex and abnormally lower values for base segments. Existing cardiac motion estimation (ME) methods belong to three categories: optical flow, speckle tracking and elastic registration. To overcome the ill-posedness of ME, they use local parametric models (e.g., affine) or global regularizations (e.g., B-splines). The objective of this study is twofold: i) to propose a novel ME method based on dictionary learning, ii) to evaluate this method on patients subjected to CA.	47472176
36	762e7417c793a64e064276f178f3ddd061659965	An Infeasible Start Predictor Corrector Method for Semi-deenite Linear Programming	In this paper we present an infeasible start path following predictor corrector method for semideenite linear programming problem. This method does not assume that the dual pair of semideenite programs have feasible solutions, and, in at most O(jlog((A;b;C))jn) iterations of the predictor corrector method, nds either an approximate solution to the dual pair or shows that there is no optimal solution with duality gap zero in a well deened bounded region. The nonexistence of optimal solutions is detected in a nite number of iterations. Here is a measure of non-optimality and infeasibility of the pair of solutions found, and is generally chosen to be small; (A; b; C) is a function of the data of the problem and is a measure of the size of the region searched, and is generally large. The method we present generalizes a method for linear programming developed by Mizuno. We give some preliminary computational experience for this method, and compare its performance (measured by the number of iterations) with that of the code SP of Vandenberghe and Boyd which is Supported by the grant number CCR{9321550 from the National Science Foundation. 0 based on a potential reduction strategy, and the code SDPA of Fujisawa and Kojima which is based on a path following and potential reduction strategy.	18953785
37	186133e5027fc8a32c003a8ef732d25472683836	Maximum mutual information based reduction strategies for cross-correlation based joint distributional modeling	In maximum-likelihood based speech recognition systems, it is important to accurately estimate the joint distribution of feature vectors given a particular acoustic model. In previous work, we showed we can boost the accuracy in this task by modeling the joint distribution of time-localized feature vectors along with the statistics relating those feature vectors to their surrounding context. In this work, we evaluate information preserving reduction strategies for those statistics. We claim that those statistics corresponding to spectro-temporal loci in speech with relatively large mutual information are most useful in estimating the information contained in the feature-vector joint distribution. Furthermore, we claim that such statistics are most likely to generalize. Using an EM algorithm to compute the mutual information between pairs of points in the time-frequency grid, we verify these hypotheses using both overlap plots and speech recognition word error results.	12303329
38	afdec637c898ada6443c961b254b58e433a5198a	Universal Steganalysis Based on Local Prediction Error in Wavelet Domain	A passive universal image steganalysis method is proposed that is shown to be of higher detection accuracy than existing truly blind steganalysis methods including Farid's and the WAM. This is achieved by improving some weaknesses of Farid's steganalysis scheme in feature extraction, that is, instead of deriving an over-determined equation system for each sub band of the wavelet decomposition, the sub bands are divided into overlapping blocks and an over-determined equation system is constructed for each block. To guarantee the existence of finite answers, the over-determined equation systems are solved in a way different from Farid's by using Moore-Penrose pseudo-inverse concept. Further improvement to the performance is achieved by adding diagonal directions and increasing the number of moments. The comparative evaluations confirm the superiority of the proposed method over prevalent blind steganalysis schemes.	7342580
39	8b5fbc3788d15e93276f161f75a1b3054dac07be	Median Frequency Changes Of Elbow Antagonist Muscle Pair During Sustained Constant-force Contraction		61779016
40	864f4bd585a6f368a60ea4aa083a1ebfabace7b3	Learning-based control of preception for mobility	To overcome the lack of flexibility and inadequacy in performance speed of perception systems for use in real-time tasks, the authors have applied integrated learning techniques to a perception system that is based on a selective sensing paradigm. The incorporation of multiple learning algorithms at different levels provides a great deal of flexibility and robustness when different perceptual task are performed. Using a selective sensing paradigm allows the system to eliminate a large amount of nonpertinent sensory data so that processing speed is greatly increased. Such a perception system is being implemented on an autonomous mobile agent. The methodology and a preliminary example of learning within the perception system are presented.<<ETX>>	15720662
41	7f27d96c4c963690d1e61ca419a6b2f26c5fb275	Scale-invariant region-based hierarchical imagematching	This paper presents an approach to scale-invariant image matching. Given two images, the goal is to find correspondences between similar subimages, e.g., representing similar objects, even when the objects are captured under large variations in scale. As in previous work: similarity is defined in terms of geometric, photometric and structural properties of regions, and images are represented by segmentation trees that capture region properties and their recursive embedding. Matching two regions thus amounts to matching their corresponding subtrees. Scale invariance is aimed at overcoming two challenges in matching two images of similar objects. First, the absolute values of many object image properties may change with scale. Second, some of the finest details visible in the high-zoom image may not be visible in the coarser scale image. We normalize the region properties associated with one of the subtrees to the corresponding properties of the root of the other subtree. This makes the scales of objects represented by the two subtrees equal, and also decouples this scale from that of the entire scene. We also weight contributions of subregions to the total similarity of their parent regions by the relative area the subregions occupy within the parents. This reduces the penalty for not being able to match fine-resolution details present within only one of the two regions, since the penalty will be down-weighted by the relatively small area of these details. Our experiments demonstrate invariance of the proposed algorithm to large changes in scale.	14323360
42	c8518a038a8bcbdf9d2f465fb6462d7f45e1e2bc	Point-Based Manifold Harmonics	This paper proposes an algorithm to build a set of orthogonal Point-Based Manifold Harmonic Bases (PB-MHB) for spectral analysis over point-sampled manifold surfaces. To ensure that PB-MHB are orthogonal to each other, it is necessary to have symmetrizable discrete Laplace-Beltrami Operator (LBO) over the surfaces. Existing converging discrete LBO for point clouds, as proposed by Belkin et al. [CHECK END OF SENTENCE], is not guaranteed to be symmetrizable. We build a new point-wisely discrete LBO over the point-sampled surface that is guaranteed to be symmetrizable, and prove its convergence. By solving the eigen problem related to the new operator, we define a set of orthogonal bases over the point cloud. Experiments show that the new operator is converging better than other symmetrizable discrete Laplacian operators (such as graph Laplacian) defined on point-sampled surfaces, and can provide orthogonal bases for further spectral geometric analysis and processing tasks.	457918
43	f8e0600e1f00b1bc089ac7e8700426f7e13f3835	Neural Network Adaptive L2 Robust Control for Induction Motors	A neural network adaptive L2 robust control method is proposed for induction motors. The proposed controllers are combined with a rotor flux observer. The disturbances caused by the uncertainties of stator and rotor resistances and load torque are compensated using radial basis function neural networks (RBFNNs). Based on backstepping, RBFNN, and HJI (Hamilton-Jaccobi-Issacs) inequality, the neural network adaptive L2 robust controllers are designed and the control system is proved to have L2-gain less than or equal to a specified positive constant gamma without solving the HJI inequality directly. The simulation results indicate that the proposed method is robust to the considered uncertainties of the induction motor parameters and has high dynamic performance.	21223251
44	4838c664bb5b8c90e803719b1b72d0a993cb52f6	A game-theoretic mode for EV discharging price and its application	With a large number of EV (Electric Vehicle) accessing to distribution network, this paper put to use game theory to balance grid company's gains and the consumers' costs. Reasonable subsidy and discharging price are the fundamental driving force for consumers to participate in V2G (Vehicle-to-grid). Based on different strategic decisions of grid company and consumers, the paper analyzes the their profits and the losses explains the relasionship between dischanging price and the numbers of EV paticipating to V2G. This research provides a reference for companies to formulate the subsidy and the discharging price.	21314044
45	1720bb9b06045dde0923d9868f5b36b836da4bd7	Design of two-dimensional FIR eigenfilters for sampling-structure conversion	By minimizing a quadratic measure of the error in the passband and stopband, a 2-D diamond-shaped finite impulse response, (FIR) eigenfilter is designed for sampling-structure conversion. Time- and frequency-domain constraints, are easily incorporated into this method, such that the design difficulty inherent to sample-conversion filters can be effectively solved. A design example and simulation with test pictures are presented to demonstrate the effectiveness of the approach. >	26486803
46	48efb5a7032fbd6609c4c7c9f8fbd5a928c390e6	Notice of RetractionAn improved predictor-corrector procedure for the dynamics of geometrically nonlinear shell structure	For geometrically nonlinear structure, the expressions of tangent stiffness matrix and internal forces of shell element are derived by CR (Co-rotational) theory, and the simplified formulation is performed on static analysis of a hinged cylindrical segment and validated by the results of experiment and references. Then, a predictor-corrector procedure with an approximately energy conservation is developed. With the application on nonlinear symmetric motion of a cylinder under two pinching forces, the improved procedure presents a better stability and accuracy than nonlinear Newmark algorithm.	17137733
47	32b9f54fb3226851583ccf46db330e8ed917b7ee	Representation and Self-Similarity of Shapes	Representing shapes in a compact and informative form is a significant problem for vision systems that must recognize or classify objects. We describe a compact representation model for two-dimensional (2D) shapes by investigating their self-similarities and constructing their shape axis trees (SA-trees). Our approach can be formulated as a variational one (or, equivalently, as MAP estimation of a Markov random field). We start with a 2D shape, its boundary contour, and two different parameterizations for the contour (one parameterization is oriented counterclockwise and the other clockwise). To measure its self-similarity, the two parameterizations are matched to derive the best set of one-to-one point-to-point correspondences along the contour. The cost functional used in the matching may vary and is determined by the adopted self-similarity criteria, e.g., cocircularity, distance variation, parallelism, and region homogeneity. The loci of middle points of the pairing contour points yield the shape axis and they can be grouped into a unique free tree structure, the SA-tree. By implicitly encoding the (local and global) shape information into an SA-tree, a variety of vision tasks, e.g., shape recognition, comparison, and retrieval, can be performed in a more robust and efficient way via various tree-based algorithms. A dynamic programming algorithm gives the optimal solution in O(N/sup 1/), where N is the size of the contour.	1579121
48	599d18ea4f5c13be3524f6fbbde968ab81529e8e	Fast Multi-Hypothesis Motion Compensated Filter for Video Denoising	Multi-hypothesis motion compensated filter (MHMCF) utilizes a number of hypotheses (temporal predictions) to estimate the current pixel which is corrupted with noise. While showing remarkable denoising results, MHMCF is computationally intensive as full search is employed in the expectation of finding good temporal predictions in the presence of noise. In the frame of MHMCF, a fast denoising algorithm FMHMCF is proposed in this paper. With edge preserved low pass prefiltering and noise-robust fast multihypothesis search, FMHMCF could find reliable hypotheses while checking very few search locations, so that the denoising process can be dramatically accelerated. Experimental results show that FM-HMCF can be 10 to 15 times faster than MHMCF, while achieving the same or even better denoising performance.	2084497
49	7abfaaf7c5bbf8dcc7ed31a395beb433d744f86f	Attack-resistant cooperation stimulation in autonomous ad hoc networks	In autonomous ad hoc networks, nodes usually belong to different authorities and pursue different goals. In order to maximize their own performance, nodes in such networks tend to be selfish, and are not willing to forward packets for the benefits of other nodes. Meanwhile, some nodes might behave maliciously and try to disrupt the network and waste other nodes' resources. In this paper, we present an attack-resilient cooperation stimulation (ARCS) system for autonomous ad hoc networks to stimulate cooperation among selfish nodes and defend against malicious attacks. In the ARCS system, the damage that can be caused by malicious nodes can be bounded, the cooperation among selfish nodes can be enforced, and the fairness among nodes can also be achieved. Both theoretical analysis and simulation results have confirmed the effectiveness of the ARCS system. Another key property of the ARCS system lies in that it is completely self-organizing and fully distributed, and does not require any tamper-proof hardware or central management points.	9142345
50	78723a76fbcbf48eafc76336d6d900fdc165d0ea	Mining IC test data to optimize VLSI testing	We describe an application of data mining and decision analysis to the problem of die-level functional test in integrated circuit manufacturing. Integrated circuits are fabricated on large wafers that can hold hundreds of individual chips (“die”). In current practice, large and expensive machines test each of these die to check that they are functioning properly (die-level functional test; DLFT), and then the wafers are cut up, and the good die are assembled into packages and connected to the package pins. Finally, the resulting packages are tested to ensure that the final product is functioning correctly. The purpose of die-level functional test is to avoid the expense of packaging bad die and to provide rapid feedback to the fabrication process by detecting die failures. The challenge for a decisiontheoretic approach is to reduce the amount of DLFT (and the associated costs) while still providing process feedback. We describe a decisiontheoretic approach to DLFT in which historical test data is mined to create a probabilistic model of patterns of die failure. This model is combined with greedy value-of-information computations to decide in real time which die to test next and when to stop testing. We report the results of several experiments that demonstrate the ability of this procedure to make good testing decisions, good stopping decisions, and to detect anomalous die. Based on experiments with historical test data from Hewlett Packard Company, the resulting system has the potential to	2053428
51	730b5bd540c7c76bf0dc7cfb6fd884fa6b697ca5	An experimental digital consumer recorder for MPEG-coded video signals	The concept and real-time implementation of an experimental home-use digital recorder is presented, capable of recording MPEG-compressed video signals. The system has small recording mechanics based on the DVC standard and it uses MPEG compression for trick-mode signals as well. >	33864190
52	7d445f90115d93c8a86df50509332b99aa7c82f1	On the Relationship Between Image and Motion Segmentation	In this paper we present a generative model for image sequences, which can be applied to motion segmentation and tracking, and to image sequence compression. The model consists of regions of relatively constant color that have a motion model explaining their motion in time. At each frame, the model can allow accretion and deletion of pixels. We also present an algorithm for maximizing the posterior probability of the image sequence model, based on the recently introduced Swendsen-Wang Cuts algorithm. We show how one can use multiple cues and model switching in a reversible manner to make better bottom-up proposals. The algorithm works on the 3d spatiotemporal pixel volume to reassign entire trajectories of constant color in very few steps, while maintaining detailed balance.	7539352
53	db55217e6b1002fadd1a428b7e38a4ecac5b3450	Design of a new type of broadband antenna with indoor distribution	Based on the principle of discone antenna, a new type of broadband omnidirectional antenna is presented. According to antenna loading technology, introducing the cone surface profile of the broken line structure and short-circuit load of ways which use three short-circuit branches improve the low-frequency electrical characteristics of the antenna and the bandwidth. A very good miniaturization is achieved which reduces the total size. The physical antenna is produced. The calculated and measured VSWR patterns seem to be in good agreement with each other. The VSWR is less than 1.5 at 800MHz-2600MHz. H plane has an ominidirectional radiation pattern. As to small size and low cost, it will have good market prospects.	36556025
54	f1df5f74477658df45814743f2e605656e8ac16e	See the Forest for the Trees: Joint Spatial and Temporal Recurrent Neural Networks for Video-Based Person Re-identification	Surveillance cameras have been widely used in different scenes. Accordingly, a demanding need is to recognize a person under different cameras, which is called person re-identification. This topic has gained increasing interests in computer vision recently. However, less attention has been paid to video-based approaches, compared with image-based ones. Two steps are usually involved in previous approaches, namely feature learning and metric learning. But most of the existing approaches only focus on either feature learning or metric learning. Meanwhile, many of them do not take full use of the temporal and spatial information. In this paper, we concentrate on video-based person re-identification and build an end-to-end deep neural network architecture to jointly learn features and metrics. The proposed method can automatically pick out the most discriminative frames in a given video by a temporal attention model. Moreover, it integrates the surrounding information at each location by a spatial recurrent model when measuring the similarity with another pedestrian video. That is, our method handles spatial and temporal information simultaneously in a unified manner. The carefully designed experiments on three public datasets show the effectiveness of each component of the proposed deep network, performing better in comparison with the state-of-the-art methods.	483922
55	b2597f8d137b86920156b73fc9d6e3e64d597438	Detection of groups in crowd considering their activity state	In this paper, we focus on the problem of group detection in crowd, which is a task of partitioning a set of pedestrians in a scene into small subsets called groups based on their trajectories. Most of previous methods use only a single model for representing a relationship between trajectories of pedestrians who belong to the same group. However, such relationship would vary depending on the activity state (e.g. walking together, approaching, splitting, and so on) of the group. In this paper, we propose a novel group detection method which can cope with a variation of groups' activity state. The proposed method constructs different models for each activity state in order to appropriately evaluate the relationship of pedestrians' trajectories. In addition, our method regards groups' activity state as hidden variables and estimates their probability distributions, which is used for integrating the constructed models. The proposed method outperforms existing methods in the experiment on the public dataset.	1903712
56	03ae4bb406203a06bd1c905c6932a8bc4a358fc4	On-line one-class support vector machines. An application to signal segmentation	We describe an efficient algorithm to sequentially update a density support estimate obtained using one-class support vector machines. The solution provided is an exact solution, which proves to be far more computationally attractive than a batch approach. This deterministic technique is applied to the problem of audio signal segmentation, with simulations demonstrating the computational performance gain on toy data sets, and the accuracy of the segmentation on audio signals.	7752890
57	69b5b79a7b3cc467400c90dcfad99473628827b3	Fashion design conception based on female physical attractiveness	The objective of the study was to provide a new perspective and entry point for creative fashion design. Through analyzing the historical evolution of female physical attractiveness, the interactive evolution of physical beauty and clothing popularity is summarized since ancient Greece to the 21st century. Three basic design techniques, involving bareness, reproduction and exaggeration, which are used in the expression of physical beauty, were put forward. The design process based on the ideal of female physical beauty was described, including decision making of the popular features of physical beauty, the choice of design techniques, and the application of principles and elements of design.	40943467
58	f0daad6d74949df9614369d77497bdd7cb433096	An energy-based framework for dense 3D registration of volumetric brain images	In this paper we describe a new method for medical image registration. The registration is formulated as a minimization problem involving robust estimators. We propose an efficient hierarchical optimization framework which is both multiresolution and multigrid. An anatomical segmentation of the cortex is introduced in the adaptive partitioning of the volume on which the multigrid minimization is based. This allows to limit the estimation to the areas of interest, to accelerate the algorithm, and to refine the estimation in specified areas. Furthermore we introduce a methodology to constrain the registration with landmarks such as anatomical structures. The performances of this method are objectively evaluated on simulated data and its benefits are demonstrated on a large database of real acquisitions.	18179147
59	c27542c7e94b6d54cf562895ba9553969c3c0217	SAR Image Despeckling Based on the Nonlocally Centralized Sparse Representation Model	In this paper, a novel SAR image despeckling method based on the nonlocal centralized sparse representation (NCSR) is presented. NCSR model is initially proposed for image restoration, exhibiting excellent denoising performance for nature images corrupted by additive Gaussian noise. However, SAR images, whose speckle noise is multiplicative and follows non-Gaussian distribution, are very different from the nature images. Considering the properties of speckle, SAR images are pre-processed, and then based on the generalized likelihood ratio (GLR) criteria, a new metric function is developed for acquiring a better estimates of the sparse coding coefficients of each corresponding noiseless SAR image patch. Experimental results show that the proposed method has a good capability of speckle smoothing in homogeneous region, as well as edge and texture preservation.	15589937
60	b4945d3acf1335696dd4514ab94b4eac28fc635d	SOM-based similarity index measure: quantifying interactions between multivariate structures	This work addresses the issue of quantifying asymmetric functional relationships between signals. We specifically consider a previously proposed similarity index that is conceptually powerful, yet computationally very expensive. The complexity increases with the square of the number of samples in the signals. In order to counter this difficulty, a self-organizing map that is trained to model the statistical distribution of the signals of interest is introduced in the similarity index evaluation procedure. The SOM based technique is equally accurate, but computationally less expensive compared to the conventional measure. These results are demonstrated by comparing the original and SOM-based similarity index approaches on synthetic chaotic signal and real EEG signal mixtures.	14120461
61	13ed445f604b853e31bcd17e496810a4260eb766	Split and merge data association filter for dense multi-target tracking	Bayesian target tracking methods consist in filtering successive measurements coming from a detector. In the presence of clutter or multiple targets, the filter must be coupled with an association procedure. The classical Bayesian multitarget tracking methods rely on the hypothesis that a target can generate at most one measurement per scan and that a measurement originates from at most one target. When tracking a high number of deformable sources, the previous assumptions are often not met that leads to the failure of the existing methods. Here, we propose an algorithm which allows to perform the tracking in the cases when a single target generates several measurements or several targets generate a single measurement. The novel idea presented in this paper is the introduction of a set that we call virtual measurement set which supersedes and extends the set of measurements. This set is chosen to optimally fit the set of the predicted measurements at each time step. This is done in two stages: i) a set of feasible joint association events is built from virtual measurements that are created by successively splitting and merging the real measurements; ii) the joint probability is maximized over all feasible joint association events. The method has been tested on microscopy image sequences which typically contains densely moving objects and gives satisfactory preliminary results.	9633276
62	95a758a9e246a4064747da3833d505334aaf71fd	2875 nm Lasing From Ho3+-Doped Fluoroindate Glass Fibers	Ho<sup>3+</sup>-doped fluoroindate glass fibers (FIGFs) based on InF<sub>3</sub>-GaF<sub>3</sub>-ZnF<sub>2</sub>-BaF<sub>2</sub>-SrF<sub>2</sub>-YF<sub>3</sub>-LiF-PbF<sub>2</sub> glasses were fabricated by using a rod-in-tube method. By using a 92-cm-long Ho<sup>3+</sup>-doped FIGF as the gain medium and a 1120-nm fiber laser as the pump source, lasing at 2875 nm was obtained for a threshold pump power of 278 mW. By further increasing the pump power to 1224 mW, the obtained maximum output power was about 54.5 mW. The corresponding slope efficiency was about 6%. To the best of our knowledge, this is the first time to report lasing at 2875 nm from Ho<sup>3+</sup>-doped FIGFs.	11578065
63	2eae4b79d36d9cf4fd1ec4de7d4c716e3fc970ea	Metrology in uncalibrated images given one vanishing point	In this paper, we describe how 3D Euclidean measurements can be made in a pair of uncalibrated images, when only minimal geometric information are available in the image planes. This minimal information consists of a line in a reference plane, and the vanishing point orthogonal to it. Given such limited information, we show that the length ratio of two objects perpendicular to the reference plane can be expressed as a function of the camera intrinsic parameters. Assuming that the camera intrinsic parameters remain invariant between two views, we perform Euclidean metric measurements directly in the perspective images.	14308505
64	fa642fd2c1eaa68fe94d23ac4bd96e9165eaf4aa	Super-Resolution From Unregistered and Totally Aliased Signals Using Subspace Methods	In many applications, the sampling frequency is limited by the physical characteristics of the components: the pixel pitch, the rate of the analog-to-digital (AID) converter, etc. A low- pass filter is usually applied before the sampling operation to avoid aliasing. However, when multiple copies are available, it is possible to use the information that is inherently present in the aliasing to reconstruct a higher resolution signal. If the different copies have unknown relative offsets, this is a nonlinear problem in the offsets and the signal coefficients. They are not easily separable in the set of equations describing the super-resolution problem. Thus, we perform joint registration and reconstruction from multiple unregistered sets of samples. We give a mathematical formulation for the problem when there are M sets of N samples of a signal that is described by L expansion coefficients. We prove that the solution of the registration and reconstruction problem is generically unique if MN ges L + M - 1. We describe two subspace-based methods to compute this solution. Their complexity is analyzed, and some heuristic methods are proposed. Finally, some numerical simulation results on one- and two-dimensional signals are given to show the performance of these methods.	14610404
65	3d4380d50f1203f9d60b5f4cdb8fcff8a7bf614e	Multi-generation-robust Coding with JPEG XS	The JPEG committee (formally, ISO SC29 WG1) is currently standardizing a lightweight mezzanine codec for video over IP transport under the name JPEG XS. A particular challenging design constraint of this codec is multi-generation robustness, that is the necessity to minimize the error built-up under multiple re-compression cycles. In this paper, we discuss the sources of such errors, how they are avoided in the JPEG XS design and compare the multi-generation robustness of JPEG XS with that of other codecs.	32986247
66	15defb6e1f1f3cf73baac4e526096d74a19fcc7a	Area efficient, high-speed VLSI design for EBCOT block coder in JPEG 2000	With the growth in multimedia technology, demand for highspeed real time image compression system has also increased. JPEG 2000 standard is developed to cater such application requirements. However, the sequential execution of the bit plane coder (BPC) used in this standard consumes more clock cycles. To improve the performance of the BPC, a new concurrent context modeling technique is proposed in this paper. To study number of context generated in each clock cycle, analysis is carried out on five ISO grayscale images with size 512 × 512. The study revealed that about 58% of time more than 4 contexts are generated in one clock. Therefore, a new concurrent context coding architecture is proposed in this paper. It is implemented on Startix FPGA and the hardware requirement is reduced significantly, compared to similar architectures. Moreover, number of clock cycles required to encode a bit plane is reduced by 10% and it is minimum 2.5 times faster than the similar designs in existence. This design operates at 164.47 MHz, which makes it compatible for encoding HDTV 1920 × 1080 4:2:2 at 39 frames per second.	15671008
67	bde4d3eea849a38c5f260ba421a2ef3808760998	NMF-based multiple pitch estimation using sparseness and inter-frame continuity constraints	This paper proposes NMF-based (non-negative matrix factorization) multiple pitch estimation algorithm. The approach of NMF-based multiple pitch estimation is to decompose input magnitude spectrogram into sum of basis spectra representing individual pitches. In decomposing music signals, the amplitude of basis spectra should have sparseness, and the shape of amplitude should be continuous between neighbor temporal frames. We introduce the constraint using matrix norm to enforce these characteristic at once and propose new NMF algorithm for spectral decomposition with this constraint. The evaluation of solo piano music shows this algorithm can implement more robust pitch estimation in the place which input spectrum has certain different shape from basis spectra or the shape of input spectrum has temporal change.	17390698
68	3a1399533d6833bdb787c189eb91f25229cd511d	Image enhancement using multiscale differential operators	Differential operators have been widely used for multiscale geometric descriptions of images. Efficient computation of these differential operators can be obtained by taking advantage of the spline techniques. We make use of a special class of these operators for image enhancement, with a particular application to chromosome image enhancement. These operators constitute a translation invariant wavelet transform well suited for the structural description of chromosome geometry. Based on the fact that the geometrical features like edges are correlated between different scales in the representation, a novel algorithm is designed to enhance the salient features of the image. Comparisons of this algorithm with other approaches are presented.	16816081
69	0a51fe3107d8d778326abe1287dbf3448991d6d7	Edge-sensitive image restoration using order-constrained least squares methods	In this paper we consider a novel technique for the restoration of noise corrupted images, using order-constrained least squares methods. The restoration process uses a moving cross-shaped filter window, within which two operations are combined. The first operation consists of simple hypothesis tests for the presence of an edge of some minimal height δ, crossing the center of the window. The second operation computes the window output as the order-constrained least squares fit of the windowed values (if an edge is deemed to be present), or simply the average (if no edge is present). The new technique is applied to some actual noise corrupted images, and the results are compared to the results of applying similarly configured median and averaging filters. Some computational considerations and comparisons are discussed at the end of the paper.	9041530
70	273ebdbefd2b0d9653491fed7bb7fb9c645a7171	Spatial/spectral endmember extraction by multidimensional morphological operations	Spectral mixture analysis provides an efficient mechanism for the interpretation and classification of remotely sensed multidimensional imagery. It aims to identify a set of reference signatures (also known as endmembers) that can be used to model the reflectance spectrum at each pixel of the original image. Thus, the modeling is carried out as a linear combination of a finite number of ground components. Although spectral mixture models have proved to be appropriate for the purpose of large hyperspectral dataset subpixel analysis, few methods are available in the literature for the extraction of appropriate endmembers in spectral unmixing. Most approaches have been designed from a spectroscopic viewpoint and, thus, tend to neglect the existing spatial correlation between pixels. This paper presents a new automated method that performs unsupervised pixel purity determination and endmember extraction from multidimensional datasets; this is achieved by using both spatial and spectral information in a combined manner. The method is based on mathematical morphology, a classic image processing technique that can be applied to the spectral domain while being able to keep its spatial characteristics. The proposed methodology is evaluated through a specifically designed framework that uses both simulated and real hyperspectral data.	16839562
71	000ca2337959a2e17572bf0c10864d69c961edf6	A method of upscaling ground measurements of Leaf Area Index based on Taylor Series expansion Model	The ground measurements of Leaf Area Index (LAI) are usually used to validate the LAI estimated using remote sensing observations. The main problem encountered in the validation is the scale mismatch between the sampling area of ground measurements and coarse-resolution image pixel, especially when those sampling area are not ideal homogenous. This study introduced a new approach of upscaling ground measurements to the coarse-resolution scale for the validation of LAI estimations. This upscaling method was based on the Taylor Series expansion Model (TSM). The high-resolution images were used to provide auxiliary information at the sub-pixel scale of coarse-resolution image pixel. The possible error associated with this method is derived from the neglection of the third- and higher-order TSM terms and the uncertainty of the empirical model. The upscaled ground measurements with upscaling error smaller than half of eigenaccuracy could be used for validation of LAI estimations with coarse resolution.	289954
72	5cd5c8118343eea996c51176e491a458355fbcd0	Intelligent Instruments: Discovering How to Turn Spectral Data into Information	The Optical Plume Anomaly Detection (OPAD) program at the NASA Marshall Space Flight Center is using plume spectroscopy for the diagnosis of the Space Shuttle Main Engines. A challenging part of this program is matching high resolution spectral data with a physicist's model of spectroscopy to produce estimates of metallic erosion through the plume. This paper describes the discovery process used in doing this. The physicist's model had to be debugged in order to discover the various instrument characteristics, discover critical elements of the data, and in general perform exploratory analysis to understand the instrument and the data it produces. This model gives us strong prior knowledge, however, this needs to be incorporated with care. We had to use a range of statistical techniques in our analysis, including one-dimensional super-resolution to determine the instrument response function. The paper concludes with a discussion of the role of discovery in building intelligent instruments that turn real-time data into useful information.	9917088
73	e31a7c5404e84eb20c0725e8464b780db3a7aeae	A Comprehensive Survey on Three-Dimensional Mesh Watermarking	Three-dimensional (3-D) meshes have been used more and more in industrial, medical and entertainment applications during the last decade. Many researchers, from both the academic and the industrial sectors, have become aware of their intellectual property protection and authentication problems arising with their increasing use. This paper gives a comprehensive survey on 3-D mesh watermarking, which is considered an effective solution to the above two emerging problems. Our survey covers an introduction to the relevant state of the art, an attack-centric investigation, and a list of existing problems and potential solutions. First, the particular difficulties encountered while applying watermarking on 3-D meshes are discussed. Then we give a presentation and an analysis of the existing algorithms by distinguishing them between fragile techniques and robust techniques. Since attacks play an important role in the design of 3-D mesh watermarking algorithms, we also provide an attack-centric viewpoint of this state of the art. Finally, some future working directions are pointed out especially on the ways of devising robust and blind algorithms and on some new probably promising watermarking feature spaces.	7199412
74	363f116e17e4d98773fffe5ede8b5a626b0d9453	Social risk and depression: Evidence from manual and automatic facial expression analysis	Investigated the relationship between change over time in severity of depression symptoms and facial expression. Depressed participants were followed over the course of treatment and video recorded during a series of clinical interviews. Facial expressions were analyzed from the video using both manual and automatic systems. Automatic and manual coding were highly consistent for FACS action units, and showed similar effects for change over time in depression severity. For both systems, when symptom severity was high, participants made more facial expressions associated with contempt, smiled less, and those smiles that occurred were more likely to be accompanied by facial actions associated with contempt. These results are consistent with the “social risk hypothesis” of depression. According to this hypothesis, when symptoms are severe, depressed participants withdraw from other people in order to protect themselves from anticipated rejection, scorn, and social exclusion. As their symptoms fade, participants send more signals indicating a willingness to affiliate. The finding that automatic facial expression analysis was both consistent with manual coding and produced the same pattern of depression effects suggests that automatic facial expression analysis may be ready for use in behavioral and clinical science.	10800354
75	84e71215e3aa95d78e7b3c667069a6e7e544d0ed	Complexity in nonlinear delayed feedback oscillator with silicon Mach-Zehnder modulator	We report the complex dynamics observed in a nonlinear delayed feedback oscillator with silicon Mach-Zehnder electro-optic modulator. Period-doubling bifurcation, period-three signal, virtual chimera states and chaotic signal are observed.	22035979
76	d1bc9a2b4b67e0fa9e231bcc49e226ba7cfffa6f	Graph-cut optimization of the ratio of functions and its application to image segmentation	Optimizing the ratio of two functions of binary variables is a common task in many image analysis applications. In general, such a ratio is not amenable to graph-cut based optimization. In this paper, we show that if the numerator and the denominator of a ratio are individually graph- representable functions, then their ratio can be optimized via graph-cut based technique. As an example of such a ratio function we choose Yezzi et al.'s energy function (A. Yezzi et al., 1999), minimization of which produces a binary labeling of an image. Through examples, we illustrate the advantage of working with graph-cut-based optimization for the aforementioned ratio in finding a global solution as opposed to the local solutions found by level set methods proposed in (A. Yezzi et al., 1999).	16253047
77	be7963f5e46301e3b21d09df528fe36bba68094a	Globally convergent ordered subsets algorithms: application to tomography	We present new algorithms for penalized-likelihood image reconstruction: modified BSREM (block sequential regularized expectation maximization) and relaxed OS-SPS (ordered subsets separable paraboloidal surrogates). Both of them are globally convergent to the unique solution, easily incorporate convex penalty functions, and are parallelizable-updating all voxels (or pixels) simultaneously. They belong to a class of relaxed ordered subsets algorithms. We modify the scaling function of the existing BSREM (De Pierro and Yamagishi, 2001) so that we can prove global convergence without previously imposed assumptions. We also introduce a diminishing relaxation parameter into the existing OS-SPS (Erdogan and Fessler, 1999) to achieve global convergence. We also modify the penalized-likelihood function to enable the algorithms to cover a zero-background-event case. Simulation results show that the algorithms are both globally convergent and fast.	13913828
78	ff8d1aa95f02122aa3b687a117d339f07a80d790	Analysis of the Correlation between Local Field Potentials and Neuronal Firing Rate in the Motor Cortex	Neuronal firing rate has been the signal of choice for invasive motor brain machine interfaces (BMI). The use of local field potentials (LFP) in BMI experiments may provide additional dendritic information about movement intent and may improve performance. Here we study the time-varying amplitude modulated relationship between local field potentials (LFP) and single unit activity (SUA) in the motor cortex. We record LFP and SUA in the primary motor cortex of rats trained to perform a lever pressing task, and evaluate the correlation between pairs of peri-event time histograms (PETH) and movement evoked local field potentials (mEP) at the same electrode. Three different correlation coefficients were calculated and compared between the neuronal PETH and the magnitude and power of the mEP. Correlation as high as 0.7 for some neurons occurred between the PETH and the mEP magnitude. As expected, the correlations between the single trial LFP and SUV are much lower due to the inherent variability of both signals	4717571
79	86a1383463dabd9577b69a626923669e52555bf2	Efficient Operator for Local Energy Measurements to Enhance an Image Signal	For real-time imaging with digital video cameras and high-quality display with TV systems, the obtained picture quality including visibility of details, local contrast, and absence of artifacts, is very important to ensure user quality acceptance. We present a multi-window real-time high-frequency enhancement scheme, in which gain is a non-linear function of the detail energy. Then we discuss the computation of commonly used local energy measurements and show that a selection of those measurements can be calculated efficiently. In our first contribution, we propose a new local energy measurement APS that can be calculated more efficiently than the existing metrics in a 2-D-separable fashion. In addition, we also show that the new APS measurement gives better performance than standard energy measurements. The second contribution is the use of local contrast and a modified contrast gain formula that can substantially improve the overall algorithm performance, especially when a high-level contrast enhancement is desired. Our algorithm trades off between added contrast and “halo” artifacts, resulting in a good balance between visibility of details and an acceptable level of artifacts. The new scheme can be successfully applied to cameras and TV systems to improve their visual quality.	7501149
80	6297baec9728d9477c1501df9597b7811efa5bf1	Which ranking metric is optimal? With applications in image retrieval and stereo matching	Euclidean metric is frequently used in computer vision, mostly ad-hoc without any justification. However we have found that other metrics like a double exponential or Cauchy metric provide better results, in accordance with the maximum likelihood approach. In this paper we experiment with different modeling functions for similarity noise and compute the accuracy of different methods using these modeling functions in two kinds of applications: content-based image retrieval from a large database and stereo matching. We provide a way to determine the modeling distribution which fits best the similarity noise distribution according to the ground truth. In the optimum case, when one has chosen the best modeling distribution, its corresponding metric will give the best ranking results for the ground truth provided.	2334942
81	86b5b41139a20db1656eed6a5801f90296c84b09	Performance Guaranteed Network Acceleration via High-Order Residual Quantization	Input binarization has shown to be an effective way for network acceleration. However, previous binarization scheme could be regarded as simple pixel-wise thresholding operations (i.e., order-one approximation) and suffers a big accuracy loss. In this paper, we propose a highorder binarization scheme, which achieves more accurate approximation while still possesses the advantage of binary operation. In particular, the proposed scheme recursively performs residual quantization and yields a series of binary input images with decreasing magnitude scales. Accordingly, we propose high-order binary filtering and gradient propagation operations for both forward and backward computations. Theoretical analysis shows approximation error guarantee property of proposed method. Extensive experimental results demonstrate that the proposed scheme yields great recognition accuracy while being accelerated.	20799266
82	260c8b8472a06b4eef7a1edcb702383913b0d26d	Edge of Chaos and Prediction of Computational Power for Neural Microcircuit Models		59969679
83	1c2a0d990f963ab44850e14551a2bccd5a91172b	Locally adaptive fuzzy pulmonary vessel segmentation in contrast enhanced CT data	Pulmonary vascular tree segmentation is the fundamental basis for different applications, such as the detection and visualization of pulmonary emboli (PE). Such an application requires an accurate and reliable segmentation of pulmonary vessels with varying diameters. We present a novel fuzzy approach to pulmonary vessel segmentation in contrast enhanced computed tomography (CT) data that considers a radius estimate of the current vessel to adapt the segmentation parameters. Hence, our method allows to capture even vessels with small diameters while suppressing leakage into surrounding structures in close proximity of vessels with large diameters. The method has been evaluated on different chest CT scans of patients referred for PE and demonstrates promising results. For quantitative validation, randomly selected sub-volumes that have been semi-automatically segmented by a medical expert have been used as reference to compare the locally adaptive method against the same method with global parameters.	30951222
84	52bcc466afbdbe0b40be656bd19354fc5f225ced	Stereoscopic video streaming with end-to-end modeling	In this work, we propose a stereoscopic video streaming system that uses rateless codes (Raptor codes) as the error protection scheme. Initially, we model the rate-distortion (RD) curve of video encoder and performance of channel codec. Then, we estimated the distortion on the stereoscopic video quality caused by packet losses. Finally, analytical models and estimated single packet loss distortions are used to minimize the end-to-end distortion and obtain optimal encoder bit rates and unequal error protection (UEP) rates. The simulation results clearly demonstrates the significant quality gain against the non-optimized schemes.	18997659
85	b4b37e3d5be1fc69d75d165510be33c7dbb5acb0	An MPEG-7 Extension for Describing Visual Impairments	Analysing the condition of audiovisual essence is an important step in audiovisual production and preservation. Standardised impairment description of audiovisual media is a pre-requisite for system interoperability between content digitisation, documentation, management, restoration, production and delivery systems.We analyse existing capabilities for describing impairment in audiovisual metadata standards. Because of its unique detailed spatiotemporally structured description capabilities we have selected MPEG-7 as the basis for visual impairment description. Following the approach for audio quality description, we define a general description scheme for visual impairments, which allows representing defect events and statistical quality measures. For certain defects, more specialised descriptors are proposed. In addition, we define a comprehensive classification scheme for visual impairments.	12161577
86	341da40024811f91723e755f9cd060e4128d8e6e	Joint 2-D DOA Estimation via Sparse L-shaped Array	In this paper, we address the problem of estimating the two-dimensional (2-D) directions of arrival (DOA) of multiple signals, by means of a sparse L-shaped array. The array consists of one uniform linear array (ULA) and one sparse linear array (SLA). The shift-invariance property of the ULA is used to estimate the elevation angles with low computational burden. The signal subspace is constructed by the cross-covariance matrix (CCM) of the received data without implementing eigendecomposition. The source waveforms are then obtained by the estimated elevation angles, which together with each sensor of the SLA, considered as a linear regression model, is used to estimate the azimuth angle by the modified total least squares (MTLS) technique. Our new algorithm yields correct parameter pairs without requiring the computationally expensive pairing operation, and therefore, has at least two advantages over the previous L-shaped array based algorithms: less computational load and better performance due to the use of SLA and CCM. Expressions for the asymptotic mean-squared error (MSE) of the 2-D DOA estimates are derived. Simulation results show that our method provides accurate and consistent 2-D DOA estimation results that could not be obtained by the existing methods with comparable computational complexity.	17041713
87	48022cf3985eaab3325ec52e5caeab021dd1f889	The relationship between instantaneous frequency and time-frequency representations	The relationship between instantaneous frequency estimation based on the derivative of the phase of the analytic signal and the first moment of the general time-frequency representation from Cohen's (1989) class is given in the continuous- and discrete-time domains. Application of the standard linear definition of first moment to discrete-time-frequency representations leads to biased instantaneous frequency estimators with high variance. To correct this problem, it is shown that periodic (circular) definitions of moments must be used to account for the periodization of the frequency variable due to sampling. >	15576163
88	73792934c7103c60487d22ae29bc83e375bf4984	A recurrent neural architecture mimicking cortical preattentive vision systems		13336579
89	5d52cb62b7441478d879ff631f8abe84616abc3f	Optimal Control of End-User Energy Storage	An increasing number of retail energy markets show price fluctuations, providing users with the opportunity to buy energy at lower than average prices. We propose to temporarily store this inexpensive energy in a battery, and use it to satisfy demand when energy prices are high, thus allowing users to exploit the price variations without having to shift their demand to the low-price periods. We study the battery control policy that yields the best performance, i.e., minimizes the total discounted costs. The optimal policy is shown to have a threshold structure, and we derive these thresholds in a few special cases. The cost savings obtained from energy storage are demonstrated through extensive numerical experiments, and we offer various directions for future research.	2946940
90	abce1d46d2fc4e03892bc52aa560cf8855fa38bb	Performance characterization of triple-junction GaAs Solar Cell with double layers AR-coating and Sub-Wavelength AR-coating	The reflective spectrum and the incident angle effects of double-layers AR-coating and Sub-Wavelength AR-coating on the triple-junction GaAs Solar Cell are characterized by short-circuit current (I<sub>sc</sub>), open-circuit voltage (V<sub>oc</sub>), and conversion efficiency (η).	13439966
91	585c9f94e81c4c4a57e60d5b9167322d7e62f796	Virtual View Reconstruction Using Temporal Information	The most significant problem in generating virtual views from a limited number of video camera views is handling areas that have become dis-occluded by shifting the virtual view away from the camera view. We propose using temporal information to address this problem, based on the notion that dis-occluded areas may have been seen by some camera in some previous frames. We formulate the problem as one of estimating the underlying state of the object in a stochastic dynamical system, given a sequence of observations. We apply the formulation to improving the visual quality of virtual views generated from a single “color plus depth” camera, and show that our algorithm achieves better results than depth image based rendering using standard inpainting.	2886743
92	3d800e0259af9fe10936f0c6911a8b33c879c147	Image restoration and edge extraction based on 2-D stochastic models	In this paper, we consider application of 2-D stochastic models discussed in [1] to develop noncausal FIR filters for restoration of images degraded by additive white noise. The semicausal model of [1] is used to design masks for edge extraction from the noisy images. The results presented here indicate that good restorations and robust edge detection are possible using relatively simple algorithms.	46088597
93	8158c952942ba3e654d303583f32a6cb36bc15e3	Elastic shape registration using an incremental free form deformation approach with the ICP algorithm	Our paper handles the elastic shape registration by combining the incremental free form deformation (IFFD) with the point-based registration technique using the sum of least squares method. The iterative closest point (ICP) algorithm is used as criteria to establish point correspondences in each level of the IFFD framework. The IFFD control lattice resolution is increased step by step to achieve a satisfactory deformation of the source shape to exactly match the target boundaries. Our point-based registration is based on least squares that measure the Euclidean distance between source and target boundaries in addition to the shape constrains. The minimization gives a closed form solution of the lattice control points positions. Promising results will be demonstrated for closed and open shapes and structures. The approach can also work for structures that contain multiple parts without any problems.	9855620
94	7c60737a9f1f65b277999c87f22de6291d6b3509	A 2-D fluid electron and 1-D particle ion hybrid model of ion noise in microwave tubes	Since there is always some ambient gas in electron beam devices, background ionization is ubiquitous. For long pulse times, the electrostatic potentials associated with this ionization can reach significant levels and give rise to such observed phenomena as phase noise in microwave tubes. Observations of noise in microwave tubes such as coupled-cavity traveling wave tubes (CC-TWTs) and klystrons have been discussed in the literature. A hybrid model has been developed in which the electron beam is treated as a 2D fluid using the beam envelope equation, and the ions generated by beam ionization are treated as discrete particles in 1D. The effect of secondary electrons is neglected in the present analysis. The ionization rate depends on the ambient gas pressure and species as well as on the electron beam current and energy. Based on this rate, ions are created and distributed on an axial grid on each time step. The ion charge is then mapped onto the grid, and Poisson's equation is then solved in 1D under the assumption that the transverse scale lengths are less than the betatron wavelength of the electron beam. The ion charge distribution is then used to integrate the beam envelope equation that updates the beam equilibrium. The ion motion is then integrated subject to the wall potential, the space-charge potential of the electron beam, and the self-consistent ion potential. This process is iterated over any desired pulse time.	110409959
95	7668e1e0667a96eecde30061d9b3c36d0f67643e	Saliency Detection for Stereoscopic Images Based on Depth Confidence Analysis and Multiple Cues Fusion	Stereoscopic perception is an important part of human visual system that allows the brain to perceive depth. However, depth information has not been well explored in existing saliency detection models. In this letter, a novel saliency detection method for stereoscopic images is proposed. First, we propose a measure to evaluate the reliability of depth map, and use it to reduce the influence of poor depth map on saliency detection. Then, the input image is represented as a graph, and the depth information is introduced into graph construction. After that, a new definition of compactness using color and depth cues is put forward to compute the compactness saliency map. In order to compensate the detection errors of compactness saliency when the salient regions have similar appearances with background, foreground saliency map is calculated based on depth-refined foreground seeds' selection (DRSS) mechanism and multiple cues contrast. Finally, these two saliency maps are integrated into a final saliency map through weighted-sum method according to their importance. Experiments on two publicly available stereo data sets demonstrate that the proposed method performs better than other ten state-of-the-art approaches.	11417966
96	6fdb8ebf1ddc4139201c6ce9b62491b0eea2e0e1	Use of automatically defined functions and architecture-altering operations in automated circuit synthesis with genetic programming	"This paper demonstrates the usefulness of automatically defined functions and architecture-altering operations in designing analog electrical circuits using genetic programming. 
 
A design for a lowpass filter is genetically evolved in which an automatically defined function is profitably reused in the 100% compliant circuit. The symmetric reuse of an evolved substructure directly enhances the performance of the circuit. Genetic programming rediscovered the classical ladder topology used in Butterworth and Chebychev filters as well as the more complex topology used in Cauer (elliptic) filters. 
 
A design for a double-passband filter is genetically evolved in which the architecture-altering operations discover a suitable program architecture dynamically during the run. Two automatically defined functions are profitably reused in the genetically evolved 100% complaint circuit."	13494270
97	043b28c2586e390d6295578243b71e67ecac9a36	Nuclei segmentation via sparsity constrained convolutional regression	Automated profiling of nuclear architecture, in histology sections, can potentially help predict the clinical outcomes. However, the task is challenging as a result of nuclear pleomorphism and cellular states (e.g, cell fate, cell cycle), which are compounded by the batch effect (e.g, variations in fixation and staining). Present methods, for nuclear segmentation, are based on human-designed features that may not effectively capture intrinsic nuclear architecture. In this paper, we propose a novel approach, called sparsity constrained convolutional regression (SCCR), for nuclei segmentation. Specifically, given raw image patches and the corresponding annotated binary masks, our algorithm jointly learns a bank of convolutional filters and a sparse linear regressor, where the former is used for feature extraction, and the latter aims to produce a likelihood for each pixel being nuclear region or background. During classification, the pixel label is simply determined by a thresholding operation applied on the likelihood map. The method has been evaluated using the benchmark dataset collected from The Cancer Genome Atlas (TCGA). Experimental results demonstrate that our method outperforms traditional nuclei segmentation algorithms and is able to achieve competitive performance compared to the state-of-the-art algorithm built upon human-designed features with biological prior knowledge.	13299252
98	5437edd1eef84fab2c791a7936b5bd574197616f	An interference cancellation scheme for the multiuser TRDMA uplink system	The concept of Time Reversal Division Multiple Access (TRDMA) has recently been proposed as a promising medium access technology for the multi-user wireless broadband communications. Compared with the existing multi-carrier technology like OFDM/OFDMA, the TRDMA provides a cost-effective single-carrier alternative technology to combat the inter-symbol interference (ISI) for broadband communications, and at the same time leverages the degrees of freedom in a large number of multi-paths to form a unique high-resolution spatial focusing effect. Previous work on TRDMA mainly focus on the multi-user downlink system. In this paper, we first introduce a TRDMA-based multi-user uplink architecture and then propose a 2-dimensional (2D) parallel interference cancellation scheme to further enhance the system performance. The TRDMA uplink architecture keeps the cost of end-users at a minimum level, and reuses the processing power at the base station (BS) that has already been made available for the downlink. The proposed 2D parallel interference cancellation scheme utilizes the tentative decisions of detected symbols to effectively cancel the interference in both the time domain (i.e. ISI) and the user domain (i.e. inter-user interference (IUI)), which significantly improve the bit-error-rate performance in the high signal-to-noise-ratio (SNR) regime. Simulation results are provided and compared with the basic TRDMA system without interference cancellation.	12508444
99	2e59865aa2ddcecaf9275abcad9b134558c686c2	Joint Learning of Single-Image and Cross-Image Representations for Person Re-identification	Person re-identification has been usually solved as either the matching of single-image representation (SIR) or the classification of cross-image representation (CIR). In this work, we exploit the connection between these two categories of methods, and propose a joint learning frame-work to unify SIR and CIR using convolutional neural network (CNN). Specifically, our deep architecture contains one shared sub-network together with two sub-networks that extract the SIRs of given images and the CIRs of given image pairs, respectively. The SIR sub-network is required to be computed once for each image (in both the probe and gallery sets), and the depth of the CIR sub-network is required to be minimal to reduce computational burden. Therefore, the two types of representation can be jointly optimized for pursuing better matching accuracy with moderate computational cost. Furthermore, the representations learned with pairwise comparison and triplet comparison objectives can be combined to improve matching performance. Experiments on the CUHK03, CUHK01 and VIPeR datasets show that the proposed method can achieve favorable accuracy while compared with state-of-the-arts.	2304733
100	5e3cc23b2a203b00c3fd9c674380028913121bba	Total Least-Squares Solution of Active Target Localization Using TDOA and FDOA Measurements in WSN	Active target localization is one of the study emphases in wireless sensor network (WSN). When time difference of arrival (TDOA) and frequency difference of arrival (FDOA) measurements are used to target localization, the measurements are nonlinearly related to the target location parameters. By the analysis of TDOA and FDOA linearized equations, when there exists sensor location error, active target localization using TDOA and FDOA measurements is a typical total least-squares (TLS) problem. The TLS solution for active target localization is presented in this paper. Its minimum square error ( MSE) is compared with Cramer-Rao lower bound (CRLB) through the simulation results.	3829992
101	1d895658c96d267d677027b00f677e2425372a97	Simple agent framework: an educational tool introducing the basics of AI programming	We describe a Java-implemented agent framework developed for an introductory undergraduate course in knowledge engineering. Although numerous agent frameworks have been proposed in the vast body of literature, none of these available agent frameworks are simple enough for use by first year undergraduate students of computer science. Hence, we set out to create our own framework that would fulfill all requirements, satisfy the aims of the course, the computing skills level of the intended group of students, and the size of the intended group of students. Besides the designed agent framework, which embodies the concepts of concurrency, multiagent systems, and persistency, the strategy for the best possible utilization of the developed tool for the goals of guiding and instructing the students in their learning of AI concepts and techniques, is also discussed. The results of the coursework suggest that the developed agent framework is highly suitable for the purposes of teaching students the AI basics including the knowledge representation schemes, rule-based reasoning and intelligent agents paradigm.	17014841
102	9079778efa4d1a46ab00360bdeb081a3f0961ab1	Pricing game and evolution dynamics for mobile video streaming	The recent developments of smart mobile phones and 3G networks enable users to enjoy video programs by subscribing to data plans. Due to phone-to-phone communication technologies and ubiquity of mobile phones, data-plan subscribers are able to redistribute the video content to non-subscribers. Such a redistribution mechanism is a potential competitor for the mobile service provider and is very difficult to trace given users' high mobility. The service provider has to set a reasonable price for the data plan to prevent such unauthorized re-distribution behavior to protect or maximize his/her own profit. In this paper, we investigate the evolutionarily stable ratio of mobile users who decide to subscribe to the data plan. Such an analysis can help the service provider preserve his/her profit under the threat of the redistribution networks and can improve the quality of service for end users.	6362476
103	39a7cb4df9637fedde85676ccfc83c0f930c620a	Denoising of medical images corrupted by Poisson noise	Medical images are often noisy owing to the physical mechanisms of the acquisition process. The great majority of the denoising algorithms assume additive white Gaussian noise. However, some of the most popular medical image modalities are degraded by some type of non-Gaussian noise. Among these types, we refer the Poisson noise, which is particularly suitable for modeling the counting processes associated to many imaging modalities such as PET, SPECT, and fluorescent confocal microscopy imaging. The aim of this work is to compare the effectiveness of several denoising algorithms in the presence of Poisson noise. We consider algorithms specifically designed for Poisson noise (wavelets, Platelets, and minimum descritpion length) and algorithms designed for Gaussian noise (edge preserving bilateral filtering, total variation, and non-local means). These algorithms are applied to piecewise smooth simulated and real data. Somehow unexpectedly, we conclude that total variation, designed for Gaussian noise, outperforms more elaborated state-of-the-art methods specifically designed for Poisson noise.	17092822
104	9d6998e7bce7b19b065de2d8f56ee8adc89b5967	Optimal Linear Assignment of a Binary Group Code to Integer Vectors for Source-Channel Coding	This paper considers the source-channel coding problem of optimally assigning the codewords of an [n, k] binary linear code to the ^-dimensional vectors produced by an IID, uniformly distributed, integer-valued source with alphabet {0,1,..., 2k/n-1} when overall distortion is measured by mean squared error (MSE). It finds explicit formulas for the optimal encoding rule, minimum MSE decoding rule, and resulting distortion. This extends to the vector case the 1974 analysis by Wolf and Redinbo [2] of the optimal assignment of binary linear codes to integers. As in [1], [2], the main tool is abstract Fourier analysis for functions defined on groups. The optimal linear assignment found for vectors can be interpreted as applying the optimal assignment found in [2] to the integer in {0,1,..., 2k-1} formed by multiplexing the binary representations of the nmiddot integers in the vector being encoded in such a way that more significants bits of each integer come before less significant bits of all integers.	17816291
105	68b66b637757421d4055e68b5c0d2f01f808cc13	Recent advances on metamaterials with applications in terminal and high gain array and reflector antennas	This paper presents recent advances in the design and applications of metamaterials: electromagnetic band gap, artificial magnetic conductor and LH surfaces. Miniaturised EBG surfaces are studied and implemented using closely-coupled double-layer topologies. Antenna characteristics are shown from a multi-frequency array (using non-uniform left handed (LH) superstrate), a compact mobile handset prototype (incorporating a thin flexible miniaturised EBG surface). Furthermore, the use in arrays and reflector antennas is discussed	26115705
106	94ef05c6e84f266e7c258e156c8026604dc433c3	The research on a novel PZT actuated precise tilt positioning system	The need for precise fast positioning technique draws attention on space inter-satellite optical communication. This paper presents a novel PZT actuated precise tilt positioning system (PTPS), which consists of the control module, the PZT driving module, the precise tilt positioning mechanism (PTPM), and the precise displacement measure module. It provides a fast and precise control in tilt movements. Firstly, the structure analysis and design of PTPM are accomplished. Then the control module is developed and the current/ voltage composite driving method is investigated in detail. Finally, following the experimental results which confirm the validity of the design and theoretical analysis of the PFPS.	8140826
107	264f344b1a197af72f25a4e50b7d96942eb93201	A reconfigurable seamless-transition DC-DC Converter With lossless current-sensing technique	A reconfigurable DC-DC converter that provides both step-down and step-up functions is proposed to ensure a stable output voltage over full voltage range of battery in portable devices. The converter can automatically adjust itself into different modes, so as to enhance system efficiency. Moreover, seamless transition is achieved to reduce output ripple and improve line regulation. Additionally, a novel current-sensing technique is applied to realize lossless and accurate current sensing. The controller IC is designed and fabricated in 1.5-um BCD process. Test results verified the design target.	37180779
108	58f3a8b7a3852a7278d741d91377721b8619f961	3D rotation estimation using discrete spherical harmonic oscillator transforms	This paper presents an approach to 3D rotation estimation using discrete spherical harmonic oscillator transforms (discrete SHOTs). Discrete SHOTs not only have simple and fast implementation methods but also are compatible with the existing angle estimation algorithms related to spherical harmonics. Discrete SHOTs of the rotated signal follow the same formulation to the Wigner-D matrix as spherical harmonics transforms. Thus, the spherical harmonics related algorithms could be utilized to discrete SHOTs without modification. Furthermore, compared to some existing methods, our approach with discrete SHOTs exhibits higher accuracy, higher precision and improved robustness to noise if the input signal is sampled uniformly on Cartesian grids. The phenomenon results from no interpolations in discrete SHOTs.	206741658
109	7d25cba5e81c75d7ec981c2fb9b88545f4596694	Data-driven online variational filtering in wireless sensor networks	In this paper, a data-driven extension of the variational algorithm is proposed. Based on a few selected sensors, target tracking is performed distributively without any information about the observation model. Tracking under such conditions is possible if one exploits the information collected from extra inter-sensor RSSI measurements. The target tracking problem is formulated as a kernel matrix completion problem. A probabilistic kernel regression is then proposed that yields a Gaussian likelihood function. The likelihood is used to derive an efficient and accelerated version of the variational filter without resorting to Monte Carlo integration. The proposed data-driven algorithm is, by construction, robust to observation model deviations and adapted to non-stationary environments.	2938927
110	3b15ba129378cb647d9b1386e56e61c1e844fde3	Two-dimensional perfect reconstruction FIR filter banks with triangular supports	We present a theory and design of two-dimensional (2-D) perfect reconstruction (PR) filter banks (FBs) (PRFBs) in which the supports of the analysis and synthesis filters consist of two triangulars. The two-triangular FB can be realized by designing an appropriate 2-D complex prototype whose passband support is a triangle that is a half of a parallelepiped-shaped passband support defined by the sampling matrix. Then a complex prototype filter is modulated by the DFT, and each analysis filter is derived by taking the real part of the modulated output. We show that the two-triangular FB satisfies the condition of permissibility. A necessary and sufficient condition for 2-D PRFBs is derived. Moreover, we present a design method of the 2-D PRFB that minimizes the cost function consisting of the frequency constraint and PR condition. Finally, a design example is presented to confirm the validity of the proposed method.	10381781
111	3bde91651dfe7956e40976cfe501e7fee98357ee	Morphological scale-space fingerprints and their use in object recognition in range images	We present the theory of multiscale dilation-erosion scale-space and the process of feature extraction via morphological scale-space fingerprints. We then discuss the reduced form of the fingerprints and state the scale-space causality theorem. These fingerprints are then applied to the recognition of multiple objects from range data. The proposed recognition system is invariant to translation, rotation, scale, and partial occlusion. We demonstrate results for the recognition of human faces in a scene, and the recognition of mountain features in a digital elevation map.<<ETX>>	14129343
112	c96da8563ae1cdb9eae838a0cf6f0a744a7fa822	OpenGL-Based Hybrid GO/PO Computation for RCS of Electrically Large Complex Objects	In this letter, a novel method is proposed to assess the simulation of scattered fields from electrically large complex metallic objects. The basic idea is to employ Open Graphics Library (OpenGL) to accelerate paths tracing for geometrical optics (GO) and physical optics (PO) hybrid method. The procedure of OpenGL-based paths tracing is divided into two steps, which are shadow faces removal and reflected rays searching. In addition, some OpenGL speedup schemes have been proposed to improve the efficiency of this algorithm. Based on this, the GO/PO hybrid method can yield a superior performance for scattering analysis of large complex targets, especially at high frequency. Simulation results obtained from this model are verified by comparison to FEKO-MLFMM results, which proves an excellent accuracy computation, and the last example for an airplane model illustrates that the method we present can be very fast even at electrically large complex objects.	44296393
113	0d82d1485082f41ba3f1bc1d6b8854f766056cf5	Optimal Randomized RANSAC	A randomized model verification strategy for RANSAC is presented. The proposed method finds, like RANSAC, a solution that is optimal with user-specified probability. The solution is found in time that is close to the shortest possible and superior to any deterministic verification strategy. A provably fastest model verification strategy is designed for the (theoretical) situation when the contamination of data by outliers is known. In this case, the algorithm is the fastest possible (on the average) of all randomized RANSAC algorithms guaranteeing a confidence in the solution. The derivation of the optimality property is based on Wald's theory of sequential decision making, in particular, a modified sequential probability ratio test (SPRT). Next, the R-RANSAC with SPRT algorithm is introduced. The algorithm removes the requirement for a priori knowledge of the fraction of outliers and estimates the quantity online. We show experimentally that on standard test data, the method has performance close to the theoretically optimal and is 2 to 10 times faster than standard RANSAC and is up to four times faster than previously published methods.	1969509
114	aab83c77117de17fd279a954abe65de841ac2d89	A new approach of recovering 3-D shape from structure-lighting	This paper presents a new approach to recover 3-D shape from only one simple structured-lighting image without knowing any information about the surface of the object and the structured-lighting system. Analysis of a structured-lighting image can estimate the calibration parameters of the system including the focal length of the camera and the position of the grating. Without any corresponding, the approach, has been applied to recover rather complicated 3-D shape in a varied environment.	61124680
115	8044e2d57c192ec85ef98990ab26688f0bbd77b8	Evaluation of kernel methods for speaker verification and identification	Support vector machines are evaluated on speaker verification and speaker identification tasks. We compare the polynomial kernel, the Fisher kernel, a likelihood ratio kernel and the pair hidden Markov model kernel with baseline systems based on a discriminative polynomial classifier and generative Gaussian mixture model classifiers. Simulations were carried out on the YOHO database and some promising results were obtained.	764191
116	f35de953bd65fb8c29c4f8d8db2eadfc724e0802	Computer engineering education reform based on CDIO	Through detailed analysis of Conceive, Design, Implement, and Operate (CDIO) engineering education mode, the education of computer engineering is innovated in combining basis theory, personal capability and collaboration of team and environment of society. In view of Capability Maturity Model (CMM), a model of computer engineering education reform Based on CDIO is developed, and the program optimizing method called Prepare, Assess, Summary, and Adjust (PASA) based on CDIO is put forward, furthermore, the basic tasks and demands of the method are also discussed in detail. When the CDIO engineering education model with the project as its carrier is build, the Flexible, Module, and Season (FMS) organization mode of teaching is actively promoted, and the requirements of the project implementation arrangements are met. From the method had been implemented since 2010, the system has got the approval of the teachers and students, which stimulates students' interest, and has a better effect in practice.	14203562
117	18ca8d501ddddd9cebd1e194fefd0d1d6130fb15	CPU-efficient free view synthesis based on depth layering	In this paper, a new approach for depth-image based rendering (DIBR) based on depth layering is proposed. The approach effectively avoids the non-uniform to uniform resampling stage, which is otherwise inherent for classical DIBR. In contrast, the new approach employs depth layering, which approximates the scene geometry by a multi-planar surface, given the depth is defined within a closed range. Such an approximation facilitates a fast reverse coordinate mapping from virtual to reference view where straightforward resampling on a uniform grid is performed. The proposed rendering approach ensures an automatic z-ordering and disocclusion detection, while being very efficient even for CPU-based implementations. It is also applicable for reference and virtual views with different resolutions and as such can serve depth upsampling, view panning and zooming applications. The experimental results demonstrate its real-time capability, while the quality is comparable with other view synthesis approaches but for lower computational cost.	19954711
118	55211e271259b4f6d5ac9c51c7a5993febe305d6	Direct computation of length-N DHT from three adjacent length-N/3 DHT coefficients	A fast direct method for obtaining the length-N discrete Hartley transform (DHT) coefficients from three adjacent length-N/3 DHT coefficients is presented. The proposed method reduces significantly the number of arithmetic operations compared to the traditional approach. Furthermore, it is easy to implement.	17130098
119	797a13caf95f2f23c888efbebb0ddb099b894397	Mathematical modeling of clutter: descriptive vs. generative models	In this article, we present two mathematical paradigms for clutter modeling. Both paradigms pose clutter modeling as a statistical inference problem, and pursue probabilistic models for characterizing observed training images. The two paradigms differ in the forms (or families) of models that they choose and in their philosophical assumptions on real world clutter patterns. The first paradigm studies descriptive models, such as Markov random field (MRF) models and the minimax entropy models (Zhu, Wu, and Mumford 1997). In this modeling paradigm, image features are first extracted from images, and statistics of these features are calculated. The latter define an image ensemble-called the Julesz ensemble which is an equivalence class where all images share the same feature statistics. For any large images from this ensemble, a local patch given its boundary condition is then Gibbs (or MRF) models. We shall review the recent conclusions about ensemble equivalence studied in (Wu, Zhu and Liu, 1999). The second paradigm studies generative model, such as the random collage model (Lee and Mumford, 1999). In contrast to a descriptive model, a generative model introduces hidden variables which are assumed to be the underlying causes producing the observed image. For example, trees and rock for clutter. The learning process makes inference about the hidden variables. We shall discuss a texton model for clutter and effective Markov chain Monte Carlo methods for stochastic inference. We shall also reveal the deep relationship between the two modeling paradigm.	120725672
120	a7227836cdefcbc382531e6cd4a7d09b15b7f04a	Iterative Image Coding with Overcomplete Curvelet Transform	This paper presents a new coding technique based on a overcomplete curvelet transform. Curvelets provide an essentially optimal representation for typical objects which are discontinuities along curves. However curvelets transform has high redundancy, the process of selecting the optimal set of coefficients to code is much more difficult because many different sets of transform coefficients can represent the same decoded image. The transform is optimized through an iterative projection process in the transform domain in order to minimize the quantization error in the image domain. So we select the optimal set of quantized coefficients based on iterative projection of signals between the image domain and transform domain with a non-linear process (a quantizer). The optimal set of quantized coefficients is coded by the simple entropy coder. The compression performance of the coding technique based on a overcomplete curvelet transform is compared with that of Wavelet Transform, both objectively and subjectively, and is found to offer advantages of up to about 0.7 dB for Lena image and to about 1.42 dB for Barbara image in PSNR and significant reduction in visibility of some types of coding artifacts.	14382703
121	eedfec33d7c1eddfd3c76fba008a53b9dee2eb98	A Cooperative Making Multi-agent Model on Railway Daily Dispatching Plan Based on Blackboard	Establishing the new complex cooperative making information system about daily dispatching plan is the focal problem in the field of railway dispatching informationization in China now, through integrating the core function modules of each existing special business making information by using the advanced technology, Building the corresponding cooperative making model about daily dispatching plan is helpful to successfully establishing such a complex information system. In lots of modeling technology, the modeling technology formed by combining the blackboard structure and multi agent technology has extensive application prospect. And it is suitable for building the cooperative making model about daily dispatching plan. In this paper, the multi agent model based on the blackboard structure is analyzed. Then the cooperative planning multi agent model of railway daily dispatching plan is built based on the description of the internal business logics of the as-is making process, and the realization process about the model formulating is described.	18327934
122	ddbc94241f4cc0188b2f3368bf45c3e0404c3393	Study on Simulation Technology of 3G Mobile Communication Network	The simulation technology of 3G mobile communication network is a very important part of network researching. It can increase researching efficiency and decrease developing cost of network. At first, in this paper, we have discussed producing method of some kinds of stochastic numbers, include heavy- tail distributing stochastic numbers, which often used in network simulation. We have also introduced structure of UMTS (Universal Mobile Telecommunication System). Finally, a 3G mobile communication network simulation model has been set up.	15102567
123	5ca2d17f2b7628c9d15b58afdf4a3bb0dd96e295	Induced Magnetoelectric Effect Driven by Magnetization in BaFe12O19- P(VDF-TrFE) Composites	Films of BaFe12O19/poly(vinylidenefluoride)-trifluorethylene composites with 5, 10, and 20 wt% barium ferrite content have been fabricated. BaFe12O19 microparticles have the shape of thin hexagonal platelets, the easy direction of magnetization remaining along the c-axis, which is perpendicular to the plates. This fact allows for ferrite particles orientation in-plane and out-of-plane within the composite films, as confirmed by measured hysteresis loops. While the in-plane-induced magnetoelectric (ME) effect is practically zero, these composite films show a good out-of-plane ME with maximum ME coupling coefficient changes of 3, 17, and 2 mV/cm.Oe for the 5, 10, and 20 wt% barium ferrite content films, respectively. We conclude that this ME behavior appears to be driven by the magnetization process arising when we applied the external magnetic field. We have also measured the linear and reversible ME effect for low applied bias field, when magnetization process is still reversible.	10720170
124	c01e7e7c4812d813b5a24f7935990552dd1f78ca	Designing Effective Inter-Pixel Information Flow for Natural Image Matting	We present a novel, purely affinity-based natural image matting algorithm. Our method relies on carefully defined pixel-to-pixel connections that enable effective use of information available in the image and the trimap. We control the information flow from the known-opacity regions into the unknown region, as well as within the unknown region itself, by utilizing multiple definitions of pixel affinities. This way we achieve significant improvements on matte quality near challenging regions of the foreground object. Among other forms of information flow, we introduce color-mixture flow, which builds upon local linear embedding and effectively encapsulates the relation between different pixel opacities. Our resulting novel linear system formulation can be solved in closed-form and is robust against several fundamental challenges in natural matting such as holes and remote intricate structures. While our method is primarily designed as a standalone natural matting tool, we show that it can also be used for regularizing mattes obtained by various sampling-based methods. Our evaluation using the public alpha matting benchmark suggests a significant performance improvement over the state-of-the-art.	24382270
125	5add60f701a5156e9d6bd7ded5d840ae355d8191	Effect of Six Fungicides against Botrytis Cinerea on Protected Cultivation Tomato	Six fungicides are selected and used to control Botrytis Cinerea on tomato under protected cultivation. The fungicides include Fludioxonil, BoscaLid, Pyrimethanil (ISO, BSI), Diethofencarb, Carbendazim, Procymidone and Thiophanate-methyl. These Chemicals were sprayed three times and once a week before or in the beginning of disease development. The results show that all the six fungicides have different negative effect on the growing of Botrytis Cinerea. Two of them, 50% BoscaLid (750 g/hm2) and 50% Fludioxonil (750 g/hm2), have significantly inhibited more than 77% of disease development in leaves and fruits resulting in almost 4 times increase of yield compared with control. Other two Chemicals, 40% Pyrimethanil (1550 g/hm2) and 60% Diethofencarb-Carbendazim (1550 g/hm2) also reduced more than 49.35% of disease development in leaves and fruits resulting in 3 time increase of yield compared with control. The last two fungicides 50% Procymidone (1800 g/hm2) and 70% Thiophanate-methyl (2580 g/hm2) have less effect on Botrytis Cinerea.	14647524
126	64c4bccb1f08c617a221be3eb1ce6872f24f2fad	Kernel Methods on Spike Train Space for Neuroscience: A Tutorial	Over the last decade, several positive-definite kernels have been proposed to treat spike trains as objects in Hilbert space. However, for the most part, such attempts still remain a mere curiosity for both computational neuroscientists and signal processing experts. This tutorial illustrates why kernel methods can, and have already started to, change the way spike trains are analyzed and processed. The presentation incorporates simple mathematical analogies and convincing practical examples in an attempt to show the yet unexplored potential of positive definite functions to quantify point processes. It also provides a detailed overview of the current state of the art and future challenges with the hope of engaging the readers in active participation.	13892814
127	8d8fa1c8d776da5fcdafe03d562af0c19455d590	Comparison of Spectral Estimation Methods for Rapidly Varying Currents Obtained by High-Frequency Radar	A comparative study of the periodogram method and high-resolution techniques (the autoregressive and multiple signal classification methods) for current mapping by a high-frequency (HF) surface wave radar is undertaken for the case of 66-s-long data. This analysis is extended from a previous study that used the commonly adopted 6–13-min coherent integration times. This reduction in the sample size will result in poor Doppler resolution and reduction in signal-to-noise ratio (SNR) for the conventional periodogram method. Two Bragg-peak identification methods for current estimation, the conventional centroid method and the symmetric-peak-sum (SPS) method, are examined in conjunction with each of the spectral estimation techniques. A weighted sum of the current estimates using the two Doppler shift identification methods is also recommended to provide a lower root mean square (RMS) difference. The weight is optimized using a genetic algorithm. Field data comparison with current measurements obtained from a current meter indicates that the high-resolution spectral estimation method is capable of providing the same RMS difference level for short and long time series, while the RMS difference for currents obtained from the periodogram method increases dramatically for short time series. Significant improvement in the current velocities retrieved from a short time series indicates the potential for measuring rapidly changing currents using the suggested technique.	37476753
128	72ef6513525777e4828798d1c73679a6d6e5aa00	Adaptation to switching force fields		61581121
129	6436dce0e39f15a1ca9269e6ca813dfebb0af3a2	The Author-Topic Model for Authors and Documents	We introduce the author-topic model, a generative model for documents that extends Latent Dirichlet Allocation (LDA; Blei, Ng, & Jordan, 2003) to include authorship information. Each author is associated with a multinomial distribution over topics and each topic is associated with a multinomial distribution over words. A document with multiple authors is modeled as a distribution over topics that is a mixture of the distributions associated with the authors. We apply the model to a collection of 1,700 NIPS conference papers and 160,000 CiteSeer abstracts. Exact inference is intractable for these datasets and we use Gibbs sampling to estimate the topic and author distributions. We compare the performance with two other generative models for documents, which are special cases of the author-topic model: LDA (a topic model) and a simple author model in which each author is associated with a distribution over words rather than a distribution over topics. We show topics recovered by the author-topic model, and demonstrate applications to computing similarity between authors and entropy of author output.	1997763
130	171b036024840246b11d31a97468e2c59bd1be52	Evolution equations for continuous-scale morphology	Several nonlinear partial differential equations that model the scale evolution associated with continuous-space multiscale morphological erosions, dilations, openings, and closings are discussed. These systems relate the infinitesimal evolution of the multiscale signal ensemble in scale space to a nonlinear operator acting on the space of signals. The type of this nonlinear operator is determined by the shape and dimensionality of the structuring element used by the morphological operators, generally taking the form of nonlinear algebraic functions of certain differential operators.<<ETX>>	122512440
131	f188ae376e7d95da93126c9127fd0238eb9eccd2	Design of Intelligent Thyristor Converter Based on dsPIC	With the development of large scale integrated circuit technology and computer technology, that using a microprocessor as the core microcomputer of excitation thyristor converter will become the main development of thyristor converters in the future. This paper introduces the features of dsPIC30F6011, and focuses on the structure and function of the intelligent detection and control modules in the intelligent thyristor converter, the intelligent detection and control modules are embedded in the intelligent thyristor converter. The experimental result has been given. The system integration rate of this intelligent thyristor converter is high, the hardware is simple, the operation is reliable, and this kind of intelligent thyristor converter has a certain economic value and practical significance.	17474539
132	78c427d4acfa7eee1efcb647ab140720425d733e	Binary orthogonal code design for MIMO radar systems	Multiple Input Multiple Output (MIMO) radar can effectively improve radar performance by transmitting specially designed orthogonal signals. A novel algorithm is presented to design binary orthogonal code for MIMO radar system. The proposed algorithm employs the Walsh function to maintain the orthogonal property of signals. The binary orthogonal code set is obtained by executing synchronal random exchange in column and random selection from rows to coding template, with using Genetic Algorithm (GA) for optimization. The simulation results show that the algorithm is feasible.	9912542
133	b95737c67e85f9c395ec13709a84fb2129a47616	Learning by Experience from Others — Social Learning and Imitation in Animals and Robots	A challenging current research direction is the design of intelligent software systems — ‘agents’ — that are able to autonomously solve certain tasks within their environment. Application areas of software agents can be found in robotics, as for example agents that control robots to rescue people in dangerous environments, and also in virtual worlds as electronic markets, where intelligent agents have to compete against other market participants, that pursue their own goals.	167164974
134	cab305a7fc8767a2a243dcbd6b51352a47eb46ea	Enhanced motion compensation algorithm based on second-order prediction	Several techniques based on the multiple reference frame scheme have been proposed to improve the motion prediction gain. Though these techniques yield higher prediction gain than the single reference frame scheme, they require tremendous computational complexity during the motion search procedure. Besides, blocking artifacts may be visible along the block boundaries, since each macroblock is predicted independently of its neighbors. To overcome these drawbacks, this paper proposes a novel motion compensation algorithm, based on the double reference frame (DRF), the double motion vector (DMV), and the searching position shifting (SPS) schemes. First, to reduce the motion vector bitrate and the computational complexity of motion search procedure, we constrain the number of reference frames to 2, and use only two motion vectors per block. Second, to alleviate the blocking artifacts and to get the better pel prediction, the searching position shifting scheme is introduced. Experimental results demonstrate that the proposed algorithm yields a 3-4 dB higher prediction gain than the single reference frame scheme. The subjective quality is also improved by alleviating the blocking artifacts.	34403195
135	4efc2b1601380a9989b716016e6df76fbc142ad2	A new heat flux coupled combustion model of sandwich propellant with complex gas reaction kinetics	This paper proposed a new heat flux coupled combustion model of sandwich propellant, a modified 37 species and 127 reactions kinetics mechanism was utilized to describe combustion flame in gas phase; a simple arrhenius pyrolyis law was used to calculate mass flux of oxidizer and binder constitute, initial species fraction released from burning surface was determined by experimental data, an “equivalent source term method” was applied to describe surface decomposition via pyrolysis relations; steady heat conduction equation was solved in inert solid phase. This model adopted full-coupled way of heat flux to determine temperature of burning surface and burning rate, then 2-D heat conduction behavior in solid phase was considered. Numerical studies have been conducted on 2-D AP(ammonium perchlorate)/HTPB(hydroxylterminated polybutadiene) sandwich models, and the flame structure, the temperature and temperature gradient along burning surface, and burning rate characteristics were discussed.	17176090
136	dced3527b7f14da2911cce01cc1dfcc925ab26e1	Proceedings of the Sixth Conference on Natural Language Learning		61244366
137	dd969f336f1c2957846b09ba0a4f812c9580ca42	A new technology for epitaxial II-VI compound semiconductor devices	ZnS and ZnSe are grown by metalorganic chemical-vapor deposition over GaAs and GaAlAs chemi-stop layers on a GaAs substrate. The III-V layers allow selective chemical removal of the substrate in order to expose the ZnS. The device allows investigation of the electrical and optical properties of epitaxial wide-gap II-VI compounds in the absence of any influence from the substrate material.	22198361
138	70919a28fa94a94aa90daba0c8f9a7890ee1a892	3D High-Efficiency Video Coding for Multi-View Video and Depth Data	This paper describes an extension of the high efficiency video coding (HEVC) standard for coding of multi-view video and depth data. In addition to the known concept of disparity-compensated prediction, inter-view motion parameter, and inter-view residual prediction for coding of the dependent video views are developed and integrated. Furthermore, for depth coding, new intra coding modes, a modified motion compensation and motion vector coding as well as the concept of motion parameter inheritance are part of the HEVC extension. A novel encoder control uses view synthesis optimization, which guarantees that high quality intermediate views can be generated based on the decoded data. The bitstream format supports the extraction of partial bitstreams, so that conventional 2D video, stereo video, and the full multi-view video plus depth format can be decoded from a single bitstream. Objective and subjective results are presented, demonstrating that the proposed approach provides 50% bit rate savings in comparison with HEVC simulcast and 20% in comparison with a straightforward multi-view extension of HEVC without the newly developed coding tools.	11226465
139	2d8e7549f6b72faabcd3b1fdb7cc77fead35eb24	Automated translation between RESTful/JSON and SPARQL messages for accessing semantic data	Linked Data is a powerful technology for storing and publishing structure of data. It is helpful for web applications because of its usefulness through semantic query data. However, using Linked Data is not easy for normal users who lack of knowledge about the structure of data or query's syntax of Linked Data. From that problem, we propose a translator component that is used for translating RESTful/JSON request messages to SPARQL command based on ontology - a metadata that describes the structure of data. Clients do not need to care about the structure of stored data or SPARQL language, a kind of query language used for querying linked data, when they insert a new instance or query all instances of any specific class. In addition, the translator component has the search function that can find a set of data from multiple classes based on finding the shortest paths between the target classes and target classes. This translator component will be applied for any dynamic ontological structure as well as automatically generate SPARQL command based on user request message.	22048700
140	ec72ff8586b96aa7959902200d1c3024683eb2a5	Cross-Layer Frame Discarding for Cellular Video Coding	In the case of delivering real-time video over the 3G cellular networks, burst frame losses may be inevitable and unpredictable, which may cause severe quality degradation. Based on cross-layer frame discarding (CLFD), this paper proposes an enhanced error-resilient video coding scheme for cellular video communication. By using unequal retransmission at the radio link (RL) layer, a base station can provide reliable transmission for the relatively important frames in one video sequence. Relying on the unequal protection at the RL layer, the encoder at the application (APP) layer can actively discard a certain number of frames according to the received acknowledgement messages. Thus, unpredictable burst frame losses during transmission can be transformed into selective frame discarding at the encoder. Experiments results show that the proposed scheme can enhance the error resilience of the cellular video communication significantly.	16029352
141	37d2fd63f1a86a7d54c8da777ff15c6e6c444636	Yarbus lives: a foveated exploration of how task influences saccadic eye movement	Qualitative analysis. Some images resulted in viewing patterns that were easily differentiated according to task (boat among flooded trees), other images less so (Gauguin's Mary). Some trials were very focused on the task at hand; others scanned more widely. Free viewing frequently resulted in longer viewing times than specific tasks. The new visualization, because it is closer to V1's representation than the traditional visualization, is a useful means to represent eye movement data.	143881850
142	6e4ddeb24d3a3ae255b99fcc9173fcfb5e3970cd	Construction of a linear unbiased diffeomorphic probabilistic liver atlas from CT images	The construction of probabilistic liver atlases has received little attention in the past. Existing methods are based on landmarks and are sensitive to their choices and placements. We propose an iterative landmark-free method based on dense volumes to construct linear unbiased diffeomorphic probabilistic atlases from liver CT images. The linear averaging of the transformed images is set as the common target space followed by pairwise diffeomorphic registrations to warp all images to the target using a recent-proposed efficient deformation approach during each iteration cycle. Iterative pairwise registrations are directly used to handle possible large deformations without the need for an extra step to remove global deformations such as the use of affine transformations in traditional methods. Compared with those approaches estimating the unbiased atlas and the transformations groupwise simultaneously, the current method is more efficient. The efficiency and the convergence of our method have been demonstrated experimentally by validation using 25 CT liver sets.	9693357
143	50111bae012de57ff8440daf0a972dc9a410002d	Hand-Crafted Features or Machine Learnt Features? Together They Improve RGB-D Object Recognition	RGB-D object recognition is an important research topic in computer version, and seeking a robust image representation is the most important sub problem for RGB-D object recognition. On the one hand, the recently emerging deep learning methods, which learns image representations automatically by capturing the data structure, have demonstrated the impressive performance for object recognition. On the other hand, the previously commonly used hand-crafted features also encodes the prior knowledge about the data. By realizing that the hand-crafted features and machine learnt features actually characterize the different aspects of image data, rather than only using one type of feature, we propose to jointly use the machine learnt features and hand-crafted features for RGB-D object recognition. Specifically, we use the Convolution Neural Networks (CNNs) to extract the machine learnt representation, and use Locality-constrained Linear Coding (LLC) based spatial pyramid matching for hand-crafted features. We evaluated our proposed approach on three publicly available RGB-D datasets. Experimental results show that our method achieves the best performance under all the cases, which demonstrates the effectiveness of our method.	34101838
144	bcaad51adc1666f8652d387d6df7603e85a0786f	N-dimensional zonal algorithms. The future of block based motion estimation?	The popularity of zonal based algorithms for block based motion estimation has been increasing due to their superior performance in both terms of reduced complexity and superior quality versus other preexisting algorithms. In our previous work we mainly focused on generalizing the different parameters used in these algorithms and finding the possibly most efficient implementation. In this paper a further generalization of these algorithms is presented, where instead we mainly consider the way zones can be designed and what should be the ultimate goal of such an algorithm. As a result we present a framework of algorithms which can have applications not only in video coding, but also in other video signal processing areas, such as computer vision, video analysis, salient stills etc. We do so by initially considering the dimensionality of video data and how it can be most efficiently analyzed and exploited in the context of zonal algorithms. A formulization of these algorithms is then presented, according to which different implementations for different applications can be selected. Simulation results, for the simple 3-D case using the predictive diamond search (PDS) algorithm, a low complexity zonal algorithm, demonstrate the efficacy of the proposed techniques while still having low complexity. Higher order implementations using more dimensions and more efficient zonal algorithms can also be considered.	14889109
145	1182782d2e61aab9c715884079fdc906af52ac50	Advances in Neural Information Processing Systems 25 (NIPS 2012)		65103251
146	17638775fccc1a228509bb2577516addd2dcb5cd	Coarse-to-fine video text detection	In this paper, we propose an effective coarse-to-fine algorithm to detect text in video. Firstly, in coarse-detection section, stroke filter is employed to detect all candidate stroke pixels, and then a fast region growing method is developed to connect these pixels into regions which are further separated into candidate text lines by projection operation. Secondly, in fine-detection section, correct text regions are selected from candidate ones by support vector machine (SVM) model and stroke features, and text regions in multi-resolution are integrated. Finally, the result is optimized significantly according to temporal correlation information. Experimental results show that our algorithm achieves real-time performance and is robust for the variation of language, font, size, color and noise of text caused by low frame resolution in video.	3488550
147	7d0a055984505e1415b93dfff8224d618af3f93c	3D subband coder for very low bit rates	A video coder based on 3D subband coding system (SBC) targetted for very low bit rate applications is developed and simulated. It employs QMF subband analysis to decompose an input sequence into 4 temporal bands. A low-complexity block-based motion detection scheme is then applied to all bands followed by motion classification and reduced motion search on the baseband which is H.261-like coded and the high temporal bands are vector-quantised (VQ) with a bank of codebooks (switched codebook VQ). Simulations are performed at 9.6 kbit/sand 14.4 kbit/s at 5 frame/s for QCIF format and results showed that it achieved better performance than the ITU-T short-term reference model SlM3 coder at similar bit rates. Subjective evaluation favours the 3D SBC coder because it does not suffer from 'blocking' artifacts.<<ETX>>	42374442
148	0476acda863db6c619a7bf0c1a0686168d6b72c4	Advanced refractory layout of preheater vessels	The clinker burning process in modern kiln lines, with multi staged preheating towers under the use of high rates of alternative fuels and raw materials, needs special attention regarding several possible wear phenomena of refractory and the steel structure itself. Analyses of refractory installations with long term running time experiences of our worldwide operators, guided us to new concepts and layouts for special areas of preheating towers. Focussed on steel shell corrosion we give examples about the corrosion mechanism, and adapted refractory concepts are presented to avoid steel shell corrosion. An outlook is given about an advanced possible solution where the energy saving aspect is taken into account.	37402095
149	6dae62d50637a11661eb39d514c29c555088e1d8	Median non-local means filtering for low SNR image denoising: Application to PET with anatomical knowledge	Denoising low signal-to-noise-ratio (SNR) images is a significant challenge since the intensity gradient due to noise elements may compete with or even exceed the intensity gradient due to features in the images. This situation can often be encountered in photon-limited medical imaging applications such as MLEM reconstructed Positron Emission Tomography (PET) images. In this study, we propose a median non-local means filter for denoising low-SNR images. The proposed method incorporates a median filtering operation indirectly in the non­local means (NLM) method, which gives more robust estimation of the weights used to average the pixels in the image. For the application of multi-modality imaging such as PET/CT, we further extended the method to incorporate anatomical side information which can be obtained from co-registered CT images without segmentation to preserve abrupt changes between organs on PET images and reduce the computational cost of weight calculations. We applied the proposed method (AMNLM) to a PET/CT simulation, a real physical phantom study and a clinical patient study with lung lesions. The results suggest that the proposed method outperforms the standard Gaussian filtering approach, anisotropic-median diffusion filtering (AMDF) and NLM in terms of visual assessment and trade-off between lesion contrast and noise.	24728631
150	39748b669ef21cf9395f86795eab95341b6b29a5	On the application of turbo codes to the robust transmission of compressed images	Compressed images transmitted over noisy channels are extremely sensitive to bit errors. This necessitates the application of error control channel coding to the compressed representation before transmission. This paper presents an image transmission system which takes advantage of the superior performance of turbo codes, an important new class of parallel concatenated codes. Several aspects of the application of turbo codes to image transmission are studied, including comparison to a previous image transmission system using convolutional codes. Experimental results for several channel signal-to-noise ratios show that, in the same SNR range, turbo codes achieve much better performance with less decoding complexity than convolutional codes and that similar performance can be achieved at much lower channel SNRs. Studies also show that the use of feedback from an outer Reed-Solomon code to aid turbo decoding results in further improvement.	704484
151	915d2d5a38a9c169cbc9bb0323f40e2742050c1f	Laplace distribution based CTU level rate control for HEVC	This paper proposes a coding tree unit (CTU) level rate control for HEVC based on the Laplace distribution modeling of the transformed residuals. Firstly, we give a study on the relationship model among the optimal quantization step, the Laplace parameter and the Lagrange multiplier. Based on the relationship model, the quantization parameter for each CTU can be dynamically adjusted according to the distribution of the transformed residual. Secondly, a CTU level rate control scheme is proposed to achieve accurate rate control as well as high coding performance. Experimental results show that the proposed rate control scheme achieves better coding performance than the state-of-the-art rate control schemes for HEVC in terms of both objective and subjective quality.	16297959
152	03440207f6956ea9c4f770939750ede04ffb2f76	Open Identity Management Framework for SaaS Ecosystem	As Software-as-a-Service (SaaS) becomes more and more popular, the identity management and federation among SaaS applications also become an important factor impacting the growth of SaaS ecosystem. Typically, there are three major functions to be enabled in identity federation: 1) Single Sign-On across different services. 2) Account provisioning to different services. 3) Secure backend service call between services. Current SaaS delivery platforms provide these functions in an ad-hoc way, which might limit the growth of SaaS ecosystem. To overcome the limitations, this paper proposes an open identity framework, which leverages open identity protocol such as OpenID and OAuth. Moreover, an OAuth broker is proposed to mediate backend service calls among SaaS applications. The framework can bring benefits to all the roles involved in the ecosystem in a non-intrusive and user-centric way. Open is a good design principle, and it is also the attitude and sprit of collaboration. We think that a SaaS ecosystem based on open technologies could make the composition of services easier and accelerate the on-boarding of service providers. Moreover, more customers might also be attracted by the openness of the ecosystem.	36907545
153	3446fa1956268b9ca4924a219a7d33b5e56316b9	A kernel logistic neural network based on restricted Boltzmann machine	A multi-class classification technique which combines kernel logistic neural network (KLNN) and restricted Boltzmann machine (RBM), called KLNN-RBM, is designed. The principal component analysis (PCA) is applied to determine the dimension of the kernel function. The initial weights and thresholds of this model are obtained by RBM. Then, the maximum likelihood estimate with a ridge regularization term and a new stochastic gradient descent method with a scaling factor are used to optimize the parameters in order to realize the multi-class classification. Some numerical simulations illustrate the validity of the proposed method.	18942080
154	71cf55dc8b387f2e820ccb5a44d6e624b0803d9c	Energy-efficient coordinated multi-point transmission for centralized power wireless private TD-LTE network	This paper proposes and evaluates the energy-efficient Coordinated Multi-Point (CoMP) transmission schemes for centralized power wireless private TD-LTE network to utilize the limited radio frequency resources to satisfy the explosively increasing data traffic demands in Smart Grid. Centralized architecture is introduced to implement the power wireless private TD-LTE network. Energy-efficient CoMP transmissions are modeled as optimization problems and solved by proposed methods. Simulation results are given to evaluate and compare the performance of transmission schemes.	19449593
155	d38c0cc757414b5af86903ebfe4070ebfd413a6b	New dictionary and fast atom searching method for matching pursuit representation of displaced frame difference	Matching pursuit decomposes a signal into a linear expansion of functions selected from a redundant dictionary, isolating the signal structures that are coherent with respect to a given dictionary. In this paper we focus on the Matching Pursuit representation of the displaced frame difference (dfd). In particular, we introduce a new dictionary for matching pursuit that efficiently exploits the signal structures of the dfd. We also propose a fast strategy to find the atoms exploiting the maximum of the absolute value of the error in the motion predicted image and the convergence of the MSE with the rotation of the atoms. Results show that the fast strategy is quite robust when compared to exhaustive search techniques and it improves the results of a suboptimal search strategy based on a genetic algorithm.	16499365
156	1a86568fdba2b85a9f0b69d563dd22aa5a8d3562	Perceptual Fidelity Aware Mean Squared Error	How to measure the perceptual quality of natural images is an important problem in low level vision. It is known that the Mean Squared Error (MSE) is not an effective index to describe the perceptual fidelity of images. Numerous perceptual fidelity indices have been developed, while the representatives include the Structural SIMilarity (SSIM) index and its variants. However, most of those perceptual measures are nonlinear, and they cannot be easily dopted as an objective function to minimize in various low level vision tasks. Can MSE be perceptual fidelity aware after some minor adaptation? In this paper we propose a simple framework to enhance the perceptual fidelity awareness of MSE by introducing an l2-norm structural error term to it. Such a Structural MSE (SMSE) can lead to very competitive image quality assessment (IQA) results. More surprisingly, we show that by using certain structure extractors, SMSE can be further turned into a Gaussian smoothed MSE (i.e., the Euclidean distance between the original and distorted images after Gaussian smooth filtering), which is much simpler to calculate but achieves rather better IQA performance than SSIM. The so called Perceptual-fidelity Aware MSE (PAMSE) can have great potentials in applications such as perceptual image coding and perceptual image restoration.	11447466
157	76b3bab18a9f98034c08404ccb4eaa5785c18d10	3D myocardial strain strain reconstruction from tagged MRI using a cylindrical B-spline model	In this paper, we present a new method for reconstructing 3D left ventricular myocardial strain from tagged magnetic resonance (MR) image data with a 3D B-spline deformation model. The B-spline model is based on a cylindrical coordinate system that more closely fits the morphology of the myocardium than previously proposed Cartesian B-spline models and does not require explicit regularization. Simulation results demonstrate that the reconstructed strains are robust to the center of the cylindrical coordinate system. We also validate our method by reconstructing strains from imaging studies of a normal human volunteer and a patient with an antero-septal myocardial infarction.	30103450
158	ea21dbebe6459a6a8dec184d83d44bbfb23373ec	Bivariate shrinkage with local variance estimation	The performance of image-denoising algorithms using wavelet transforms can be improved significantly by taking into account the statistical dependencies among wavelet coefficients as demonstrated by several algorithms presented in the literature. In two earlier papers by the authors, a simple bivariate shrinkage rule is described using a coefficient and its parent. The performance can also be improved using simple models by estimating model parameters in a local neighborhood. This letter presents a locally adaptive denoising algorithm using the bivariate shrinkage function. The algorithm is illustrated using both the orthogonal and dual tree complex wavelet transforms. Some comparisons with the best available results are given in order to illustrate the effectiveness of the proposed algorithm.	1920558
159	2ffbc7b12664fb343b4a576219320ea64a9e6c80	Bytecode-based software monitoring and trusted evolution programming framework	Point to reliability of distributed software in open complex environment, this paper proposes a bytecode-based software monitoring and trusted evolution programming framework, which injects monitoring ability and evolution ability into the software in construction and running phase. The framework is comprised of three module, business-logic and monitor-requirement description module (BMDM), monitoring capability infusing module (MCIM), and online evolution module (OEM), BMDM provides monitoring requirement expression language MRL+, MCIM compiles monitoring requirements monitoring requirement into aspect code, insert aspect code into source system by bytecode-based AOP technology, generate the target system with monitoring ability, and send monitoring information to OEM by event. OEM compares the state event with predefined evolution rules, when the predefined rules are triggered, perform the evolution actions, achieve real-time system's monitoring and correction. Finally, actual cases are analyzed to show that the framework can construct reliability software with monitoring ability and evolution ability efficiently.	13951950
160	bce0162b9205e376f3152076e07670598c0b1119	Fault detection of eccentricity by means of joint time-frequency analysis in PMSM under dynamic conditions	This paper presents a study of eccentricity fault detection in permanent magnet synchronous machines (PMSM), under dynamic conditions. The fault simulation was made by means of two-dimensional (2-D) finite element analysis (FEA). Joint time - frequency transforms, as Wigner Ville distribution (WVD) and Zao-Atlas-Marks distribution, were proposed for signal analysis. Simulations carried out were compared with experimental results.	18273901
161	f49ced0c8dd666da0729583a2485f9018d5df50e	Quality Improvement of Video Codec by Rate-Distortion Optimized Quantization	Conventional quantization methods consider only the distortion between original and reconstructed video as the cost of compression. Considering the time-varying nature of network bandwidth for multimedia services, we believe a video coding system can provide a better quality of experience if it takes the bit rate of the compressed bit stream into consideration as well when optimizing the quantization. In this paper we present a rate-distortion optimization approach to the quantization of video coding. This approach is able to balance between rate and distortion for quantization and enhance the overall quality of the entire coding system, with only a slight increase in computational overhead. We implement this method in H.264/AVC, and the extensive experimental data obtained under various test conditions show that the performance of the R-D optimized quantization is indeed better than the H.264 reference software.	24962756
162	bd842b7876cde81166c592877149aa266ae93b81	Information mining platforms: an infrastructure for KDD rapid deployment	Experience has shown that the data extraction, parsing, cleaning and analysis steps of a KDD problem account for a much larger expenditure of resources (time and money) than the statistical modeling or machine learning part. Couple this statement with the need for fast turn-around time in commercial applications, and the obvious conclusion is that it is impractical to start from scratch for each new KDD application. To deal with this situation, we propose the use of an information mining platform that amortizes several of the critical preand post-processing steps needed to apply KDD. Thus, new KDD applications can leverage the platform for efficiency and robustness.	18961237
163	dbab5f59ee4248b9698c31df9fc6f7eaa9dcc336	Online identification of hidden semiMarkov models	Hidden Markov models (HMM) are a powerful tool in signal modelling. In an HMM, the probability that signal leaves a state is constant, and hence the duration that signal stays in each state has an exponential distribution. However, this exponential density is not appropriate for a large class of physical signals. Hence, a more sophisticated model, called hidden semiMarkov models (HSMM), are used where the state durations are modelled in some form. This paper presents new signal model for hidden semiMarkov models. This model is based on state duration dependant transition probabilities, where the state duration densities are modelled with parametric distribution functions. An adaptive algorithm for online identification of HSMMs based on our signal model is presented. This algorithm is based on the 'recursive prediction error' technique, where the parameter estimates are updated adaptively in a direction that maximizes the likelihood of parameter estimates. From the numerical results it is shown that the proposed algorithms can successfully estimate the true value of parameters. These results also show that our algorithm can adaptively track the parameter's changes in time.	121939130
164	36708bbc473ec0b07752782de8a4d00f903eec3d	Gene functional classification from heterogeneous data	In our attempts to understand cellular function at the molecular level, we must be able to synthesize information from disparate types of genomic data. We consider the problem of inferring gene functional classifications from a heterogeneous data set consisting of DNA microarray expression measurements and phylogenetic profiles from whole-genome sequence comparisons. We demonstrate the application of the support vector machine (SVM) learning algorithm to this functional inference task. Our results suggest the importance of exploiting prior information about the heterogeneity of the data. In particular, we propose an SVM kernel function that is explicitly heterogeneous. We also show how to use knowledge about heterogeneity to aid in feature selection.	1044641
165	45f586ceddfc81abe0583337a6a2fc0b4b1f67a8	HoG based two-directional Dynamic Time Warping for handwritten word spotting	We present a Histogram of Oriented Gradient (HoG) based two-directional Dynamic Time Warping (DTW) matching method for handwritten word spotting. Firstly, we extract HoG descriptors from each cell in the normalized images. Then we connect the HoG descriptors in the same column and get a sequence of feature vectors. We do the same operation for the HoG descriptors in the same row. We then apply the two-directional DTW method to calculate the distance between the feature vectors sequences extracted from the query word and the candidate one. The experimental results show that the two-directional DTW is more robust to word deformation than the traditional DTW. And the local features such as HoG, LBP and SIFT combined with the two-directional DTW method outperform the method using the local feature descriptors directly. The HoG based two-directional DTW get the highest mean average precision on both the George Washington dataset and the CASIA-HWDB 2.1 dataset.	842061
166	7aac1045e6943b4a7978e260a3035662d5b3bf8d	Learning from one example through shared densities on transforms	"We define a process called congealing in which elements of a dataset (images) are brought into correspondence with each other jointly, producing a data-defined model. It is based upon minimizing the summed component-wise (pixel-wise) entropies over a continuous set of transforms on the data. One of the biproducts of this minimization is a set of transform, one associated with each original training sample. We then demonstrate a procedure for effectively bringing test data into correspondence with the data-defined model produced in the congealing process. Subsequently; we develop a probability density over the set of transforms that arose from the congealing process. We suggest that this density over transforms may be shared by many classes, and demonstrate how using this density as ""prior knowledge"" can be used to develop a classifier based on only a single training example for each class."	2699786
167	5557695c5cfb74c327a1cd6bbbcd0e8ef095a99e	PAC-Bayes & Margins	"We show two related things: 
 
(1) Given a classifier which consists of a weighted sum of features with a large margin, we can construct a stochastic classifier with negligibly larger training error rate. The stochastic classifier has a future error rate bound that depends on the margin distribution and is independent of the size of the base hypothesis class. 
 
(2) A new true error bound for classifiers with a margin which is simpler, functionally tighter, and more data-dependent than all previous bounds."	14991207
168	a0c40b07c68d68f8f795d9c59075d48042188b60	Characterization and modeling of frequency dispersion in RF LDMOS transistors	This paper characterises the effects of frequency dispersion in laterally diffused metal-oxide semiconductor (LDMOS) from pulsed-S-parameter measurements. Using the calculated high-frequency drain current from measured data, we demonstrate that length of the lightly doped drain extension is directly proportional to the amount of current collapse at high frequency. To capture the frequency dispersion in a nonlinear model, a frequency-domain mapping is proposed to augment quasi-static nonlinear electrothermal models. The importance of including the dispersiveness is demonstrated through comparing a standard quasi-static model, the newly augmented model, and on-wafer load-pull measurements. For a 5-mm, 500 μm unit-gate-width transistor, we demonstrate that the quasi-static model over predicts the drain efficiency during large signal drive at P3dB by 9%. The new augmented model is able to predict the efficiency within 2% of the measured value.	17286732
169	7d47c7d236f765a5de5d80acbc9cc1a54f664bee	A computational approach to boundary detection	A unified approach to boundary perception is presented. The model consists of a hierarchical system which extracts and groups salient features in the image at different spatial scales. In the first stage a Gabor wavelet decomposition provides a representation of the image which is orientation selective, has optimal localization properties, and provides a good model for early feature detection. Following this, local competitive interactions are introduced which help in reducing the effects of noise and illumination variations. Scale interactions help in localizing line ends and corners, and play an important role in boundary perception. The final stage groups similar features aiding in boundary completion. Experimental results on detecting edges, texture boundaries, and illusory contours are provided.<<ETX>>	20248294
170	7ff1d01985a4eafe1ecc268dfdc1cd283decb188	A current cycle feedback iterative learning control approach to AFM imaging	In this article, we proposed a novel current cycle feedback (CCF) iterative learning control (ILC) approach to achieve high-speed imaging on atomic force microscope (AFM). AFM-imaging requires precision positioning of the AFM probe relative to the sample in 3-D (x-y-z) dimension. It has been demonstrated that with advanced control techniques such as the inversion-based iterative-control (IIC) technique, precision positioning of the AFM probe in the lateral (x-y) direction can be successfully achieved. Additional challenges, however, must be overcome to achieve precision positioning of the AFM-probe in the vertical direction. The main contribution of this article is the development of the CCF-ILC approach to the AFM z-axis control. Particularly, the proposed CCF-ILC controller design utilizes the developed robust-inversion technique to minimize the model uncertainty effect on the feedforward control, and remove the causality constraints existing in other CCF-ILC approaches. Experimental results for AFM imaging are presented and discussed to illustrate the proposed method.	5722243
171	955968b66c1986b9f01e405f4c56bd69e9c8bcaa	Collaborative video streaming with Raptor network coding	We investigate the problem of collaborative video streaming with Raptor network coding over overlay networks. We exploit path and source diversity, as well as basic processing capabilities of network nodes to increase the overall throughput and improve the video quality at the clients. We consider an architecture where several streaming servers simultaneously deliver video information to a set of clients. The servers apply Raptor coding on the video packets for error resiliency, and the forwarding peer nodes further combine the Raptor coded video packets in order to increase the packet diversity in the network. We find the optimal source and channel rate allocation in such a collaborative streaming system. The resulting scheme efficiently exploits the available network resources for improved video quality. The experimental evaluation demonstrates that it typically outperforms Raptor video streaming systems that do not use network coding.	30559369
172	4db5b9bfcb7403e71827cf2d47e740f00a718560	A System and Vision Localization Method for the Opening of Railway Oil Tank Wagon Based on Shape Matching	The opening localization for railway oil tank wagon using machine vision technology is an effective way to load petroleum automatically. In this paper, we introduce a localization system for the opening of railway oil tank wagon, which consists of industrial camera, lens, LED lighter, industrial PC, and IO module. In addition, we also propose a shape matching localization method, which can overcome the adverse effects produced by illumination changes, various types of railway oil tank wagon, and various poses of camera. The shape template is extracted from a standard opening image using edge detection and shape fitting methods, which can help avoid the randomness of template selection. An optimized similarity measure is exploited to avoid the edge extraction, so that the threshold setting of edge extraction can be avoided. In addition, we also exploit scalable template to find various types opening and exploit image pyramid during shape matching procedure to gain a speedup. The indoor and outdoor experiments show that the localization system and the proposed method fully meet the requirements of practical industrial applications.	41123906
173	a383b9dda7ca4340fbe638ff14bef6c95875b770	The Consistency of Greedy Algorithms for Classification	We consider a class of algorithms for classification, which are based on sequential greedy minimization of a convex upper bound on the 0 - 1 loss function. A large class of recently popular algorithms falls within the scope of this approach, including many variants of Boosting algorithms. The basic question addressed in this paper relates to the statistical consistency of such approaches. We provide precise conditions which guarantee that sequential greedy procedures are consistent, and establish rates of convergence under the assumption that the Bayes decision boundary belongs to a certain class of smooth functions. The results are established using a form of regularization which constrains the search space at each iteration of the algorithm. In addition to providing general consistency results, we provide rates of convergence for smooth decision boundaries. A particularly interesting conclusion of our work is that Logistic function based Boosting provides faster rates of convergence than Boosting based on the exponential function used in AdaBoost.	7849224
174	96d8b5e1e8cefa549970b44416b45cbe577078ed	Comparison and fusion of multiresolution features for texture classification		13391506
175	c8eaef3cc05af48eaf3eaca4f4a2b6394bbc1cd4	A Cross-Resolution Leaky Prediction Scheme for In-Band Wavelet Video Coding With Spatial Scalability	In most existing in-band wavelet video coding schemes, over-complete wavelet transform is used for the motion-compensated temporal filtering (MCTF) of each spatial subband. It can overcome the shift-variance of critical sampling wavelet transform and improve the coding efficiency of the in-band scheme. However, a dilemma exists in the current implementations of in-band MCTF (IBMCTF), which is whether or not to exploit the spatial highpass subbands in motion compensation of the spatial lowpass subband. The absence of the spatial highpass subbands will result in significant quality loss in the reconstructed full-resolution video, whereas the presence of the spatial highpass subbands may bring serious mismatch error in the decoded low-resolution video since the corresponding highpass subbands may be unavailable at the decoder. In this paper, we first analyze the mismatch error propagation in decoding the low-resolution video. Based on our analysis, we then propose a frame-based cross-resolution leaky prediction scheme for IBMCTF. It can make a good tradeoff between alleviating the low-resolution mismatch and improving the full-resolution coding efficiency. Experimental results show that the proposed scheme can dramatically reduce the mismatch error by 0.3-2.5 dB for low resolution, while the performance loss is marginal for high resolution.	14123669
176	807153a5fa05c0bb425fb8c124a275bbdfc1f543	Change Detection and Estimation in Large Scale Sensor Networks: Linear Complexity Algorithms	Abstract : We propose algorithms for nonparametric sample-based spacial change detection and estimation in large scale sensor networks. We collect random samples containing the location of sensors and their local decisions, and assume that the local decisions can be stimulated or normal , reflecting the local strength of some stimulating agent. Then change in the location of the agent manifests itself by a change in the distribution of stimulated sensors. In this paper, we are aiming at developing a test that, given two collections of samples, can decide whether the distribution generating the samples has changed or not, and give an estimated changed area if a change is indeed detected. The focus of this paper is to reduce the complexity of the detection and estimation algorithm. We propose two fast algorithms with almost linear complexity and analyze their completeness, flexibility and robustness.	15885859
177	24232511c7cb88468b5e69f7638e1c722315f616	Image and video coding—emerging standards and beyond		61591794
178	c71a777674b33c0d0996a6d3863dee57a62edfc9	3-D reconstruction of real-world objects using extended voxels	In this paper we present a voxel-based 3-D reconstruction technique that computes a set of non-transparent object surface voxels from a given set of calibrated camera views. We show that the quality of the reconstruction strongly depends on the accuracy of the computed voxel projection in the image plane and discuss different approximations of the exact projection. The most simple and computationally least demanding approximation is obtained when projecting point voxels, i.e., voxels without spatial extent. However, correct occlusion handling is not possible for point voxel volumes leading to reconstruction artifacts. The most accurate projection is obtained by computing the exact outline of the projected voxels. This projection is computationally most demanding but allows correct occlusion handling during reconstruction. Experimental results that compare the reconstruction quality for point and exact voxel projection show that it is worthwhile computing the tract image plane footprint of the projected voxels.	6405110
179	e5d11efd03226d3754246bc2471f30de02770240	Local tampering detection in video sequences	Video sequences are often believed to provide stronger forensic evidence than still images, e.g., when used in lawsuits. However, a wide set of powerful and easy-to-use video authoring tools is today available to anyone. Therefore, it is possible for an attacker to maliciously forge a video sequence, e.g., by removing or inserting an object in a scene. These forms of manipulation can be performed with different techniques. For example, a portion of the original video may be replaced by either a still image repeated in time or, in more complex cases, by a video sequence. Moreover, the attacker might use as source data either a spatio-temporal region of the same video, or a region taken from an external sequence. In this paper we present the analysis of the footprints left when tampering with a video sequence, and propose a detection algorithm that allows a forensic analyst to reveal video forgeries and localize them in the spatio-temporal domain. With respect to the state-of-the-art, the proposed method is completely unsupervised and proves to be robust to compression. The algorithm is validated against a dataset of forged videos available online.	11285357
180	5060ed574091f9a8d105ee0c039a5e1c9c27375d	An efficient wavelet-based pattern matching scheme for ECG data compression	A novel coding scheme for ECG data compression is proposed in this paper. Following beat delineation, the periods of the beats are normalized by multi-rate processing. Amplitude normalization is performed afterwards, and discrete wavelet transform is applied to each normalized beat. Due to the period and amplitude normalization, the wavelet transform coefficients bear a high correlation across beats. To increase the compression ratio, a pattern matching unit is utilized. The difference between the actual period and the standard period, and amplitude scale factor are also retained for each beat. At the decoder, the inverse wavelet transform is computed from the reconstructed wavelet transform coefficients. The original amplitude and period of each beat are then recovered. The simulation results show that the performance of the proposed compression algorithm is quite satisfactory and achieves significant improvement in comparison with some recent techniques.	14189623
181	68a13b8d9105f97643249700dee20ea90c4e70e4	Non-Uniformly Tiled CMOS Image Sensors for Efficient On-Chip Image Compression	We present a complementary metal-oxide semiconductor (CMOS) image sensor with non-uniform pixel placement that enables a highly efficient calculation of the discrete cosine transform (DCT), which is the most mathematically intensive step of an image compression algorithm. This technique is based on the arithmetic Fourier transform (AFT), which has been shown to be five times more computationally efficient than DCT derivation methods commonly used. In this paper, the focus is on the basic theory and algorithm as well as the sensitivity of the method to image sensor fixed pattern noise (FPN). The architecture and circuits have been implemented in a conventional CMOS process. The method has been demonstrated in the current prototype and results that enable an assessment of the sensitivity to FPN have been obtained.	32322798
182	2e922e3bb4241c413c138ab4fe1ba37fc6af3583	An efficient minimum area spacing algorithm for noise reduction	With high clock frequencies, faster transistor rise/fall time, long signal wires, the use of wider wires and Cu material interconnects, the decreased spacing between adjacent wires, and the increased aspect ratio, the crosstalk noise is becoming an important design metric in digital circuits. For a risk-free layout solution of a chip, capacitive and inductive noises should be considered at various routing process stages. A formulation and efficient solution for the min-area spacing problem to satisfy maximum reliability in multiple coupled nets is provided. The noise model used can handle different wire widths, different spacing among wires, and different wire lengths. Experimental results show that the proposed framework redistributes the spacing among neighboring wires to achieve maximum reliability through the minimization of the maximum crosstalk noise in nets.	39276564
183	c374ba2da0ad37741f7b2977d5fa2fcc3b6a2def	An Incremental Boundary Element Method for the Variation-Aware Library-Building Procedure of Capacitance Extraction	The variation of process parameters makes the number of geometry patterns in capacitance extraction increase for magnitudes, which brings a heavy burden to the library-building procedure. An incremental BEM solver is proposed for the variation-aware library-building. This method reuses the coefficient matrix and solution of the original pattern for the sequent computations for varied patterns. Numerical results show the proposed method is five times faster than the method just repeats the computation for all patterns, while preserving high accuracy	17269556
184	c97f5878c4f2f630db0bff93e563c7de6a2da896	Artificial invariant subspace with potential functions for humanoid robot balancing	Existing trajectory planning based locomotion algorithms lack the analytic tools to fully comprehend energy based movements that would allow for full stability and mobility. Such drawbacks make humanoid robots' locomotion sensitive to external disturbances and compromise robots' agility in unstructured environment. In this work, we specifically focus on the push recovery problem for humanoid robots. We propose an approach to design a nonlinear controller that is robust to external disturbances. It allows the state of the rigid body dynamics to asymptotically converge to the subspace that meet the criteria of balancing, based on the properties of artificial invariant subspace and potential functions. Our algorithm is completely adaptive in real time without requiring trajectory planning in advance. We demonstrate the robustness of the proposed algorithm base on extensive push recovery experiments on the DARWIN-OP robot platform on flat terrains.	12076496
185	ea6e2f2f0012d7a1a98df6efc2c60f1f96ae8650	Challenges in cloud based ingest and encoding for high quality streaming media	The Netflix ingest and encoding pipeline is a cloud-based platform that generates video encodes for the Netflix streaming service. Due to the large throughput of the system, automated video quality assessment of the source videos and the generated encodes is essential in ensuring the quality of experience of viewers. This paper discusses the motivations for integrating video quality assessment in the production pipeline, outlines currently deployed solutions and presents the technical challenges in improving the system.	14006472
186	b990d02f0e5cc5defc680e21684c1b4526f115b1	Object tracking via online metric learning	By considering visual tracking as a similarity matching problem, we propose a self-supervised tracking method that incorporates adaptive metric learning and semi-supervised learning into the framework of object tracking. For object representation, the spatial-pyramid structure is applied by fusing both the shape and texture cues as descriptors. A metric learner is adaptively trained online to best distinguish the foreground object and background, and a new bi-linear graph is defined accordingly to propagate the label of each sample. Then high-confident samples are collected to self-update the model to handle large-scale issue. Experiments on the benchmark dataset and comparisons with the state-of-the-art methods validate the advantages of our algorithm.	2872286
187	c5d853dfc39400292a43cee8f5d8fd4fd49a65c2	Automatic Segmentation of Fuzzy Laser Lines with Sub-Pixel Accuracy from the Uneven Background During Robotic Arc Welding	Robust and automatic segmentation of the reflected laser lines from the arc light modified background is prerequisite for the subsequent measurement of the weld pool shape which is of great importance for monitoring robotic arc welding. In this paper, the two dimensional intensity distribution caused by the arc light in the captured image is modeled. Based on the model, we propose an efficient and effective method that comprises three parts: (1), spline enhancement to achieve sub-pixel accuracy, (2), a gradient detection filter to eliminate the uneven background, (3), an effective threshold selection method. Experimental results verified that the proposed method is significantly superior to other state of art methods in segmentation accuracy.	117584189
188	dcf30b0c2a6ab3533c7b125d1128d3cf447e6f36	Performance of linear reduced-rank multistage receivers for DS-CDMA in frequency-selective fading channels	The performance of a set of linear reduced-rank multistage filter banks is studied in the context of multiuser detection for direct-sequence (DS) code-division multiple-access (CDMA) systems. The set of filter banks under consideration is comprised of the minimum mean-square error (MMSE), the minimum output energy (MOE), the best linear unbiased estimator (BLUE), and the maximum-likelihood (ML) detector. Based on a common framework for the multistage implementations of the aforementioned filter banks, the signal-to-interference plus noise ratios (SINRs) and bit-error rates (BERs) of these reduced-rank filter banks are studied for multipath Rayleigh-fading channels. A generic BER formula is provided for coherent detection and noncoherent differential detection schemes constructed under this common framework. Analysis shows that all of these performance measures are characterized by a kernel matrix K/sub mmse/ whose trace forms the output SINR of the MMSE filter bank. Through investigating the recursive structure of K/sub mmse/, the output SINRs are proven to be monotonically increasing with the number of stages and upper-bounded by a number equal to the paths of the desired user's channel. The condition for asymptotically achieving this upper bound is also provided, which leads to the notion of effective user capacity of linear reduced-rank multiuser detection as well as serves as a test for the existence of a BER floor for coherent detection. In addition, the channel mismatch due to differential detection is also shown to yield a BER floor for noncoherent detection. Based on this analysis, a simple yet effective rule for choosing the number of stages is provided for both coherent and noncoherent linear multistage multiuser detection.	14657245
189	97e1a7f2738636ebb7a5fad08b0547ccb17d16d4	The cost effective structure for designing hybrid cloud based enterprise E-learning platform	The total initial cost of enterprise learning management system is very high on the infrastructure and software applications, so that enterprises start to adopt cloud computing to reduce expenditure on infrastructure, software and IT human resource. The paper provides a cost effective structure of hybrid cloud architecture that has been implemented the multi-tenant model for enterprise to support customization sharing among different virtualized applications in a tenant area. The schema-sharing and multi-tenant data storage architecture also supports the learning content delivery. The important basis for the design method of software development shall embed with the cost effective structure, such as using multithreads to compute background processing and unzip the content files to reduce brand width usage. Even the file sizes of learning content shall be zipped to the less small size and duplicate to different servers for cutting down the traffic flow of brand width. The detailed calculation of the all I/O computer processing and brand width become the crucial implementation way.	18184738
190	7f0cdefc70f18880d87c6a0cd055a34711719f85	IBM's LVCSR system for transcription of broadcast news used in the 1997 hub4 english evaluation	This paper describes IBM’s large vocabulary continuous speech recognition (LVCSR) system used in the 1997 Hub4 English evaluation. It focusses on extensions and improvements to the system used in the 1996 evaluation. The recognizer uses an additional 35 hours of training data over the one used in the 1996 Hub4 evaluation [8]. It includes a number of new features: optimal feature space for acoustic modeling (in training and/or testing), filler-word modeling, Bayesian Information Criterion (BIC) based segmentation and segment clustering, an improved implementation of iterative MLLR, variance adaptation, and 4-gram language models. Results using the 1996 and 1997 DARPA Hub4 evaluation data sets are presented.	11764647
191	b0d9df610989ae524929adea00a05966c08671c7	Optimal joint source-channel bit allocation for MPEG-4 fine granularity scalable video over OFDM system	In this paper, we propose an optimal bit allocation method for MPEG-4 fine granularity scalable (FGS) video over OFDM transmission. In the proposed method, we consider the joint optimal bit allocation of source and forward error correction (Reed-Solomon) codes of different rates over OFDM. Our simulation results show that our proposed approach achieves significant PSNR gain over the existing approach without channel error protection when the input and the output data rates are fixed.	29658648
192	398441c0dd86727579f15d601a0378f3ef132aec	A metadata model for online laboratories	Making online laboratories available to the wider public requires them to be retrievable and reusable. To this end, we define a metadata set providing all required information in a machine readable form. This article presents the metadata set currently under discussion by the Global Online Lab Consortium (GOLC) as well as the issues of defining widely acceptable controlled vocabularies to describe the scientific field of remote experimentation.	43829422
193	c72a44f3692c4124066939df48e0e0af6f3c3957	Shot type characterization in 2D and 3D video content	Due to the enormous increase of video and image content on the web in the last decades, automatic video annotation became a necessity. The successful annotation of video and image content facilitate a successful indexing and retrieval in search databases. In this work we study a variety of possible shot type characterizations that can be assigned in a single video frame or still image. Possible ways to propagate these characterizations to a video segment (or to an entire shot) are also discussed. A method for the detection of Over-the-Shoulder shots in 3D (stereo) video is also proposed.	2416561
194	b388b751e1a9a5110893f98055154d2654e0433c	A low complexity algorithm for proportional resource allocation in OFDMA systems	Orthogonal frequency division multiple access (OFDMA) basestations allow multiple users to transmit simultaneously on different subcarriers during the same symbol period. This paper considers basestation allocation of subcarriers and power to each user to maximize the sum of user data rates, subject to constraints on total power, bit error rate, and proportionality among user data rates. Previous allocation methods have been iterative nonlinear methods suitable for offline optimization. In the special high subchannel SNR case, an iterative root-finding method has linear-time complexity in the number of users and N log N complexity in the number of subchannels. We propose a non-iterative method that is made possible by our relaxation of strict user rate proportionality constraints. Compared to the root-finding method, the proposed method waives the restriction of high subchannel SNR, has significantly lower complexity, and in simulation, yields higher user data rates.	16908979
195	4fb1b9d14f5eca57a6702f32cf99375c2624a78c	Real-Time Constrained Task Scheduling in 3D Chip Multiprocessor to Reduce Peak Temperature	Chip multiprocessor technique has been implemented in embedded systems due to the tremendous computation requirements. Three dimension chip multiprocessor architecture has been studied recently for integrating more functionalities and providing higher performance. The high temperature on chip is a critical issue for the 3D architecture. In this paper, we propose an online thermal prediction model for 3D chip. Using this model, we present a task scheduling algorithm based on rotation scheduling to reduce the peak temperature on chip. We consider the data dependencies, especially the inter-iteration dependencies which are not well considered in most of the current thermal-aware task scheduling algorithms. Our simulation result shows that our algorithm can efficiently reduce the peak temperature up to 10C.	3706665
196	dec714928a06610355e4d44c9468d2be0af2f47c	Color documents on the Web with DjVu	"We present a new image compression technique called ""DjVu"" that is specifically geared towards the compression of scanned documents in color at high resolution. With DjVu, a magazine page in color at 300 dpi typically occupies between 40 KB and 80 KB, approximately 5 to 10 times better than JPEG for a similar level of readability. Using a combination of hidden Markov model techniques and MDL-driven heuristics, DjVu first classifies each pixel in the image as either foreground (text, drawings) or background (pictures, photos, paper texture). The pixel categories form a bitonal image which is compressed using a pattern matching technique that takes advantage of the similarities between character shapes. A progressive, wavelet-based compression technique, combined with a masking algorithm, is then used to compress the foreground and background images at lower resolutions while minimizing the number of bits spent on the pixels that are not visible in the foreground and background planes. Encoders, decoders, and real-time, memory efficient plug-ins for various web browsers are available for all the major platforms."	3238305
197	c2f839d8bef7dac03bb5329eec048a182ff8a587	Genetic algorithm based design of power system stabilizers	This paper presents a GA based optimization scheme for simultaneous coordination of multiple machines power system damping controllers. The proposed algorithm is applied to tuning of single and multiple power system stabilizers. Controller design is tested on the small and mid-sized power systems to prove its effectiveness. PSSs are designed using MATLAB control system toolbox and optimized using GAOT toolbox. All models and simulations are carried out using MATLAB and SSAT developed by Powertech Labs Inc, Canada.	41923518
198	75c62be6a3cdc5c3c6999a21caab8573b37d6aa7	Learning and Inference for Clause Identification	This paper presents an approach to partial parsing of natural language sentences that makes global inference on top of the outcome of hierarchically learned local classifiers. The best decomposition of a sentence into clauses is chosen using a dynamic programming based scheme that takes into account previously identified partial solutions. This inference scheme applies learning at several levels--when identifying potential clauses and when scoring partial solutions. The classifiers are trained in a hierarchical fashion, building on previous classifications. The method presented significantly outperforms the best methods known so far for clause identification.	7646508
199	72d10c66b4a688158bbaa8ff02ab20ca2bc398f6	The kernel self-organising map	We review a recently-developed method of performing k-means clustering in a high-dimensional feature space and extend it to give the resultant mapping topology-preserving properties. We show the results of the new algorithm on the standard data set, on random numbers drawn uniformly from [0,1)/sup 2/ and on the Olivetti database of faces. The new algorithm converges extremely quickly.	39029569
