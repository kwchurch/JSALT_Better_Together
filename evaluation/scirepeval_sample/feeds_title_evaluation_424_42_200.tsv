	query	doc_id	feed_id	abbreviations	candidates
0	Coding Algorithm Validation	a0cfa32e924e26ca9285373c97a7226b	17794	{}	"[{'doc_id': '233989500', 'title': 'Accuracy of soft tissue balancing in total knee arthroplasty using surgeon-defined assessment versus a gap-balancer or electronic sensor', 'abstract': 'Background Soft tissue balancing is essential for the success of total knee arthroplasty (TKA) and is mainly dependent on surgeon-defined assessment (SDA) or a gap-balancer (GB). However, an electronic sensor has been developed to objectively measure the gap pressure. This study aimed to evaluate the accuracy of soft tissue balancing using SDA and GB compared with a sensor. Methods Forty-eight patients undergoing TKA (60 knees) were prospectively enrolled. Soft tissue balancing was sequentially performed using SDA, a GB, and an electronic sensor. We compared the SDA, GB, and sensor data to calculate the sensitivity, specificity, and accuracy at 0°, 45°, 90°, and 120° flexion. Cumulative summation (CUSUM) analysis was performed to assess the surgeon’s performance during the sensor introductory phase. Results The sensitivity of SDA was 63.3%, 68.3%, 80.0%, and 80.0% at 0°, 45°, 90°, and 120°, respectively. The accuracy of the GB compared with sensor data was 76.7% and 71.7% at 0° and 90°, respectively. Cohen’s kappa coefficient for the accuracy of the GB was 0.406 at 0° (moderate agreement) and 0.227 at 90° (fair agreement). The CUSUM 0° line achieved good prior performance at case 45, CUSUM 90° and 120° showed a trend toward good prior performance, while CUSUM 45° reached poor prior performance at case 8. Conclusion SDA was a poor predictor of knee balance. GB improved the accuracy of soft tissue balancing, but was still less accurate than the sensor, particularly for unbalanced knees. SDA improved with ongoing use of the sensor, except at 45° flexion.', 'corpus_id': 233989500, 'score': 0}, {'doc_id': '234494104', 'title': 'Predictive Models for Clinical Outcomes in Total Knee Arthroplasty: A Systematic Analysis', 'abstract': 'Background Predictive modeling promises to improve our understanding of what variables influence patient satisfaction after total knee arthroplasty (TKA). The purpose of this article was to systematically review the relevant literature using predictive models of clinical outcomes after TKA. The aim was to identify the predictor strategies used for systematic data collection with the highest likelihood of success in predicting clinical outcomes. Methods A Preferred Reporting Items for Systematic Reviews and Meta-Analyses protocol systematic review was conducted using 3 databases (MEDLINE, EMBASE, and PubMed) to identify all clinical studies that had used predictive models or that assessed predictive features for outcomes after TKA between 1996 and 2020. The ROBINS-I tool was used to evaluate the quality of the studies and the risk of bias. Results A total of 75 studies were identified of which 48 met our inclusion criteria. Preoperative predictive factors strongly associated with postoperative clinical outcomes were knee pain, knee-specific Patient-Reported Outcome Measure (PROM) scores, and mental health scores. Demographic characteristics, pre-existing comorbidities, and knee alignment had an inconsistent association with outcomes. The outcome measures that correlated best with the predictive models were improvement of PROM scores, pain scores, and patient satisfaction. Conclusions Several algorithms, based on PROM improvement, patient satisfaction, or pain after TKA, have been developed to improve decision-making regarding both indications for surgery and surgical strategy. Functional features such as preoperative pain and PROM scores were highly predictive for clinical outcomes after TKA. Some variables such as demographics data or knee alignment were less strongly correlated with TKA outcomes. Level of evidence Systematic review – Level III.', 'corpus_id': 234494104, 'score': 1}, {'doc_id': '234787208', 'title': 'A new prediction model for patient satisfaction after total knee arthroplasty and the roles of different scoring systems: a retrospective cohort study', 'abstract': 'Background Although total knee arthroplasty (TKA) is an efficacious treatment for end-stage osteoarthritis, ~20% of patients are dissatisfied with the results. We determined which factors contribute to patient satisfaction and compared the various scoring systems before and after surgery. Methods In this retrospective cohort study, 545 patients were enrolled and evaluated preoperatively and 1 year postoperatively. Patient demographics, as well as scores for the Western Ontario and McMaster Universities Osteoarthritis Index (WOMAC), Short Form (SF)-12, and 1989 Knee Society Clinical Rating System (1989 KSS), were recorded preoperatively and postoperatively. The possible predictors were introduced into a prediction model. Scores for overall satisfaction and the 2011 Knee Society Score (2011 KSS) were also assessed after TKA to identify the accuracy and agreement of the systems. Results There were 134 male patients and 411 female patients, with an overall prevalence of satisfaction of 83.7% 1 year after surgery. A history of surgery ( p < 0.001) and the 1989 KSS and SF-12 were of the utmost importance in the prediction model, whereas the WOMAC score had a vital role postoperatively (change in WOMAC pain score, p < 0.001; change in WOMAC physical function score, p < 0.001; postoperative WOMAC pain score, p = 0.004). C-index of model was 0.898 > 0.70 (95% confidence interval (CI): 0.86-0.94). The Hosmer-Lemeshow test showed a p value of 0.586, and the AUC of external cohort was 0.953 (sensitivity=0.87, specificity=0.97). The agreement between the assessment of overall satisfaction and the 2011 KSS satisfaction assessment was general (Kappa=0.437 > 0.4, p < 0.001). Conclusion A history of surgery, the preoperative 1989 KSS, and the preoperative SF-12 influenced patient satisfaction after primary TKA. We recommend the WOMAC (particularly the pain subscale score) to reflect overall patient satisfaction postoperatively.', 'corpus_id': 234787208, 'score': 1}, {'doc_id': '234774031', 'title': 'Prognostic models for knee osteoarthritis: a protocol for systematic review, critical appraisal, and meta-analysis', 'abstract': 'Background Osteoarthritis is the most common degenerative joint disease. It is associated with significant socioeconomic burden and poor quality of life, mainly due to knee osteoarthritis (KOA), and related total knee arthroplasty (TKA). Since early detection method and disease-modifying drug is lacking, the key of KOA treatment is shifting to disease prevention and progression slowing. The prognostic prediction models are called for to guide clinical decision-making. The aim of our review is to identify and characterize reported multivariable prognostic models for KOA about three clinical concerns: (1) the risk of developing KOA in the general population, (2) the risk of receiving TKA in KOA patients, and (3) the outcome of TKA in KOA patients who plan to receive TKA. Methods The electronic datasets (PubMed, Embase, the Cochrane Library, Web of Science, Scopus, SportDiscus, and CINAHL) and gray literature sources (OpenGrey, British Library Inside, ProQuest Dissertations & Theses Global, and BIOSIS preview) will be searched from their inception onwards. Title and abstract screening and full-text review will be accomplished by two independent reviewers. The multivariable prognostic models that concern on (1) the risk of developing KOA in the general population, (2) the risk of receiving TKA in KOA patients, and (3) the outcome of TKA in KOA patients who plan to receive TKA will be included. Data extraction instrument and critical appraisal instrument will be developed before formal assessment and will be modified during a training phase in advance. Study reporting transparency, methodological quality, and risk of bias will be assessed according to the TRIPOD statement, CHARMS checklist, and PROBAST tool, respectively. Prognostic prediction models will be summarized qualitatively. Quantitative metrics on the predictive performance of these models will be synthesized with meta-analyses if appropriate. Discussion Our systematic review will collate evidence from prognostic prediction models that can be used through the whole process of KOA. The review may identify models which are capable of allowing personalized preventative and therapeutic interventions to be precisely targeted at those individuals who are at the highest risk. To accomplish the prediction models to cross the translational gaps between an exploratory research method and a valued addition to precision medicine workflows, research recommendations relating to model development, validation, or impact assessment will be made. Systematic review registration PROSPERO CRD42020203543', 'corpus_id': 234774031, 'score': 0}, {'doc_id': '233474300', 'title': 'From Outpatient Procedures? Patients More Likely to Have Optimal Immediate Recovery Outcomes of Same-Day Orthopedic Surgery: Are Spine', 'abstract': 'Background: Spinal surgery is associated with an inherently elevated risk profile, and thus far there has been limited discussion about how these outpatient spine patients are benefiting from these same-day procedures against other typical outpatient orthopedic surgeries. Methods: Orthopedic patients who received either inpatient or outpatient surgery were isolated in the American College of Surgeons National Surgical Quality of Improvement Program (2005–2016). Patients were stratified by type of orthopedic surgery received (spine, knee, ankle, shoulder, or hip). Mean comparisons and chi-squared tests assessed basic demographics. Perioperative complications were analyzed via regression analyses in regard to their principal inpatient or outpatient orthopedic surgery received. Results: This study included 729 480 surgical patients: 32.5% received spinal surgery, 36.5% knee, 24.1% hip, 4.9% shoulder, and 1.7%ankle. Of those who received a spinal procedure, 74.7% were inpatients (IN), and 25.3% were outpatients (OUT): knee: 96.1% IN, 3.9% OUT; hip:98.9% IN, 1.1% OUT; ankle: 29% IN, 71% OUT; and shoulder: 52.6% IN, 47.6% OUT. Hip patients were the oldest, and knee patients had the highest body mass index out of the orthopedic groups (P , .00). Spine IN patients experienced more complications than the other orthopedic groups and had the lowest OUT complications(both P , .05). This same trend of having higher IN complications than OUT complications was identified for hip, shoulder, and knee. However, ankle procedures had greater OUT procedure complications than IN (P , .05). After controlling for age, body mass index, and Charlson Comorbidity Index, IN procedures, such as knee, hip, spine, and shoulder, were significantly associated with experiencing postoperative complications. From 2006 to 2016, IN and OUT surgeries were significantly different among complications experienced for all of the orthopedic groups (P , .05) with complications decreasing for IN and OUT patients by 2016. Conclusions: Over the past decade, spine surgery has decreased in complications for IN and OUT procedures along with IN/OUT knee, ankle, hip, and shoulder procedures, reflecting greater tolerance for risk in an outpatient setting. Level of Evidence: 3. Clinical Relevance: Despite the increase in riskier spine procedures, complications have decreased over the years. Surgeons should aim to continue to decrease inpatient spine complications to the level of other orthopedic surgeries.', 'corpus_id': 233474300, 'score': 0}, {'doc_id': '233303808', 'title': 'Identifying factors predicting prolonged rehabilitation after simultaneous bilateral total knee arthroplasty: a retrospective observational study', 'abstract': 'Background Rehabilitation is an effective procedure for promoting functional recovery after simultaneous bilateral total knee arthroplasty (TKA); however, it has been cited as a significant economic burden of medical care. We hypothesized that preoperative factors, including age, sex, body mass index, living alone, the knee society function score (KSS), the American society of anesthesiologists (ASA) class, hemoglobin (Hb), albumin level, mean range of motion, and the Kellgren–Lawrence grade, would predict prolonged rehabilitation utilization. Methods In total, 191 patients undergoing simultaneous bilateral TKA in a single hospital were enrolled. The successful compliance group included patients who completed their rehabilitation program and could return to their residence within 3\u2009weeks after surgery ( n \u2009=\u2009132), whereas the delayed group included the remaining patients ( n \u2009=\u200959). Logistic regression analysis was performed using preoperative factors. A prediction scoring system was created using the regression coefficients from the logistic regression model. Results Logistic regression analysis revealed that age (β\u2009=\u2009−\u20090.0870; P \u2009<\u2009\xa00.01) and Hb (β\u2009=\u20090.34; P \u2009<\u2009\xa00.05) were significantly associated with prolonged rehabilitation programs, whereas body mass index, living alone, KSS score, and ASA class were not significantly associated with successful completion of rehabilitation programs; however, these factors contributed to the prediction scoring formula, which was defined as follows: Score = 10 - 0.09 × age - 0.09 × body mass index - ( 0.56 × living alone [ alone : 1 , others : 0 ] ) + 0.03 × KSS stairs + 0.34 × Hb - 1.1 × ASA class . $$ {\\displaystyle \\begin{array}{l}\\mathrm{Score}=10-\\left(0.09\\times \\mathrm{age}\\right)-\\left(0.09\\times \\mathrm{body}\\ \\mathrm{mass}\\ \\mathrm{index}\\right)-\\left(0.56\\times \\mathrm{living}\\ \\mathrm{alone}\\ \\right[\\mathrm{alone}:1,\\\\ {}\\mathrm{others}:0\\left]\\right)+\\left(0.03\\times \\mathrm{KSS}\\ \\mathrm{stairs}\\right)+\\left(0.34\\times \\mathrm{Hb}\\right)-\\left(1.1\\times \\mathrm{ASA}\\ \\mathrm{class}\\right).\\end{array}} $$ The C-statistic for the scoring system was 0.748 (95% confidence interval, 0.672–0.824). The positive and negative likelihood ratios were 2.228 (95% CI, 1.256–3.950) and 0.386 (95% CI, 0.263–0.566), respectively. These results showed an increase of 15–20% and a decrease of 20–25% in the risk of prolonged rehabilitation. The optimal cutoff point for balancing sensitivity and specificity was 3.5, with 66.6% sensitivity and 78.0% specificity. Conclusions Older age and lower preoperative Hb were significantly associated with prolonged rehabilitation programs. We defined a new scoring formula using preoperative patient factors to predict prolonged rehabilitation utilization in patients undergoing simultaneous bilateral TKA.', 'corpus_id': 233303808, 'score': 0}, {'doc_id': '235272231', 'title': 'The short-term effectiveness and safety of second-generation patellofemoral arthroplasty and total knee arthroplasty on isolated patellofemoral osteoarthritis: a systematic review and meta-analysis', 'abstract': 'We aimed to compare second-generation patellofemoral arthroplasty (2G PFA) with total knee arthroplasty (TKA) in treating isolated patellofemoral osteoarthritis (PFOA) by assessing the percentages of revisions, complications, and patient-reported outcome measures (PROMs). Studies that compared the outcomes of 2G PFA and TKA in the treatment of isolated PFOA were searched in electronic databases, including MEDLINE, Embase, and Web of Science. Two researchers independently identified eligible studies, extracted the data, and evaluated the quality of the literature. Pooled risk ratios (RRs) or weighted mean differences with 95% confidence intervals were calculated using either fixed or random effects models. Descriptive analysis was used when data could not be pooled. A total of six studies were included in the review. For the revision percentage and complications, there were no significant differences between 2G PFA and TKA (RR = 2.29, 95% CI 0.69–7.58, P = 0.17; RR = 0.56, 95% CI 0.23–1.40, P = 0.22, respectively). Second, the results demonstrated that the differences in the Oxford Knee Score (OKS) and the University of California, Los Angeles (UCLA) activity score between 2G PFA and TKA were not significant (WMD −4.68, 95% CI −16.32 to 6.97, p = 0.43; WMD 0.16, 95% CI −1.21 to 1.53, P = 0.82). The Knee Injury and Osteoarthritis Outcome Score (KOOS), the American Knee Society Score (AKSS), and the Western Ontario and McMaster Universities Osteoarthritis Index (WOMAC) were presented in a narrative form due to methodological heterogeneity. For isolated PFOA, 2G PFA demonstrated similar results to TKA with respect to the percentages of revisions, complications, and PROMs.', 'corpus_id': 235272231, 'score': 0}, {'doc_id': '3812196', 'title': 'Validation of an International Statistical Classification of Diseases and Related Health Problems 10th Revision Coding Algorithm for Hospital Encounters with Hypoglycemia.', 'abstract': ""OBJECTIVES\nTo determine the positive predictive value and sensitivity of an International Statistical Classification of Diseases and Related Health Problems, 10th Revision, coding algorithm for hospital encounters concerning hypoglycemia.\n\n\nMETHODS\nWe carried out 2 retrospective studies in Ontario, Canada. We examined medical records from 2002 through 2014, in which older adults (mean age, 76) were assigned at least 1 code for hypoglycemia (E15, E160, E161, E162, E1063, E1163, E1363, E1463). The positive predictive value of the algorithm was calculated using a gold-standard definition (blood glucose value <4\u2009mmol/L or physician diagnosis of hypoglycemia). To determine the algorithm's sensitivity, we used linked healthcare databases to identify older adults (mean age, 77) with laboratory plasma glucose values <4\u2009mmol/L during a hospital encounter that took place between 2003 and 2011. We assessed how frequently a code for hypoglycemia was present. We also examined the algorithm's performance in differing clinical settings (e.g. inpatient vs. emergency department, by hypoglycemia severity).\n\n\nRESULTS\nThe positive predictive value of the algorithm was 94.0% (95% confidence interval 89.3% to 97.0%), and its sensitivity was 12.7% (95% confidence interval 11.9% to 13.5%). It performed better in the emergency department and in cases of more severe hypoglycemia (plasma glucose values <3.5\u2009mmol/L compared with ≥3.5\u2009mmol/L).\n\n\nCONCLUSIONS\nOur hypoglycemia algorithm has a high positive predictive value but is limited in sensitivity. Although we can be confident that older adults who are assigned 1 of these codes truly had a hypoglycemia event, many episodes will not be captured by studies using administrative databases."", 'corpus_id': 3812196, 'score': 1}, {'doc_id': '221106365', 'title': 'Validation of an electronic coding algorithm to identify the primary indication of orthopedic surgeries from administrative data', 'abstract': 'Background Determining the primary indication of a surgical procedure can be useful in identifying patients undergoing elective surgery where shared decision-making is recommended. The purpose of this study was to develop and validate an algorithm to identify patients receiving the following combinations of surgical procedure and primary indication as part of a study to promote shared decision-making: (1) knee arthroplasty to treat knee osteoarthritis (KOA); (2) hip arthroplasty to treat hip osteoarthritis (HOA); (3) spinal surgery to treat lumbar spinal stenosis (SpS); and (4) spinal surgery to treat lumbar herniated disc (HD). Methods Consecutive surgical procedures performed by participating spine, hip, and knee surgeons at four sites within an integrated care network were included. Study staff reviewed electronic medical records to ascertain a “gold standard” determination of the procedure and primary indication status. Electronic algorithms consisting of ICD-10 and CPT codes for each combination of procedure and indication were then applied to records for each case. The primary measures of validity for the algorithms were the sensitivity and specificity relative to the gold standard review. Results Participating surgeons performed 790 procedures included in this study. The sensitivity of the algorithms in determining whether a surgical case represented one of the combinations of procedure and primary indication ranged from 0.70 (HD) to 0.92 (KOA). The specificity ranged from 0.94 (SpS) to 0.99 (HOA, KOA). Conclusion The electronic algorithm was able to identify all four procedure/primary indication combinations of interest with high specificity. Additionally, the sensitivity for the KOA cases was reasonably high. For HOA and the spine conditions, additional work is needed to improve the sensitivity of the algorithm to identify the primary indication for each case.', 'corpus_id': 221106365, 'score': 1}, {'doc_id': '235168000', 'title': 'Validation of Claims Data for the Identification of Intraoperative Transesophageal Echocardiography During Cardiac Surgery.', 'abstract': 'OBJECTIVE\nThe goal of this study was to assess the validity of Current Procedural Terminology (CPT) claims data for the identification of intraoperative transesophageal echocardiography (TEE) during cardiac surgery.\n\n\nDESIGN\nThis study was a retrospective, cohort analysis.\n\n\nSETTING\nThis study used data from electronic medical records (EMRs), in combination with CPT billing claims data, from two hospitals within the Penn Medicine Health System-Penn Presbyterian Medical Center and the Hospital of the University of Pennsylvania.\n\n\nPARTICIPANTS\nThe cohort consisted of adult patients, aged ≥18 years, undergoing open cardiac valve surgery (repair or replacement), coronary artery bypass graft surgery, or aortic surgery between April 1 and October 31, 2019.\n\n\nINTERVENTIONS\nAgreement between TEE identified using CPT billing code(s) (93312-8 with or without 93320-1 or 93325) and TEE identified by manual EMR review.\n\n\nMEASUREMENTS AND MAIN RESULTS\nAs identified by a reference standard (ie, EMR review) of the 873 cases that met inclusion criteria, 867 (99.31%) cases were performed with TEE and six cases were performed without TEE (<1%). Of the 867 cases performed with TEE, CPT code(s) correctly identified 866 cases, as indicated by having at least one of the CPT codes (93312-8 with or without 93320-1 or 93325). These CPT codes identified intraoperative TEE with a 99.88% sensitivity, 100.00% specificity, 100.00% positive predictive value, and 85.71% negative predictive value. When billing claims for TEE were restricted to the CPT code 93312 alone, the results were identical.\n\n\nCONCLUSIONS\nBilling claims using CPT code(s) identified true intraoperative TEE with a high sensitivity, specificity, excellent positive predictive value, and moderate negative predictive value. These results demonstrated that claims data are a valuable data source from which to study the effect of TEE in cardiac surgical patients.', 'corpus_id': 235168000, 'score': 1}]"
1	PhD project	7f706c390d562785045ed8dc58943598	19614	{}	[{'doc_id': '220126138', 'title': 'Investigating the Perceptual Validity of Evaluation Metrics for Automatic Piano Music Transcription', 'abstract': 'Automatic Music Transcription (AMT) is usually evaluated using low-level criteria, typically by counting the number of errors, with equal weighting. Yet, some errors (e.g. out-of-key notes) are more salient than others. In this study, we design an online listening test to gather judgements about AMT quality. These judgements take the form of pairwise comparisons of transcriptions of the same music by pairs of different AMT systems. We investigate how these judgements correlate with benchmark metrics, and find that although they match in many cases, agreement drops when comparing pairs with similar scores, or pairs of poor transcriptions. We show that onset-only notewise F-measure is the benchmark metric that correlates best with human judgement, all the more so with higher onset tolerance thresholds. We define a set of features related to various musical attributes, and use them to design a new metric that correlates significantly better with listeners’ quality judgements. We examine which musical aspects were important to raters by conducting an ablation study on the defined metric, highlighting the importance of the rhythmic dimension (tempo, meter). We make the collected data entirely available for further study, in particular to evaluate the perceptual relevance of new AMT metrics.', 'corpus_id': 220126138, 'score': 1}, {'doc_id': '236635109', 'title': 'On-Line Audio-to-Lyrics Alignment Based on a Reference Performance', 'abstract': 'Audio-to-lyrics alignment has become an increasingly active research task in MIR, supported by the emergence of several open-source datasets of audio recordings with word-level lyrics annotations. However, there are still a number of open problems, such as a lack of robustness in the face of severe duration mismatches between audio and lyrics representation; a certain degree of language-specificity caused by acoustic differences across languages; and the fact that most successful methods in the field are not suited to work in real-time. Real-time lyrics alignment (tracking) would have many useful applications, such as fully automated subtitle display in live concerts and opera. In this work, we describe the first real-timecapable audio-to-lyrics alignment pipeline that is able to robustly track the lyrics of different languages, without additional language information. The proposed model predicts, for each audio frame, a probability vector over (European) phoneme classes, using a very small temporal context, and aligns this vector with a phoneme posteriogram matrix computed beforehand from another recording of the same work, which serves as a reference and a proxy to the written-out lyrics. We evaluate our system’s tracking accuracy on the challenging genre of classical opera. Finally, robustness to out-of-training languages is demonstrated in an experiment on Jingju (Beijing opera).', 'corpus_id': 236635109, 'score': 0}, {'doc_id': '17996408', 'title': 'Digital audio restoration', 'abstract': 'This chapter is concerned with the application of modern signal processing techniques to the restoration of degraded audio signals. Although attention is focussed on gramophone recordings, film sound tracks and tape recordings, many of the techniques discussed have applications in other areas where degraded audio signals occur, such as speech transmission, telephony and hearing aids.', 'corpus_id': 17996408, 'score': 1}, {'doc_id': '236447646', 'title': 'Audio-to-Score Alignment Using Deep Automatic Music Transcription', 'abstract': 'Audio-to-score alignment (A2SA) is a multimodal task consisting in the alignment of audio signals to music scores. Recent literature confirms the benefits of Automatic Music Transcription (AMT) for A2SA at the frame-level. In this work, we aim to elaborate on the exploitation of AMT Deep Learning (DL) models for achieving alignment at the note-level. We propose a method which benefits from HMM-based score-toscore alignment and AMT, showing a remarkable advancement beyond the state-of-the-art. We design a systematic procedure to take advantage of large datasets which do not offer an aligned score. Finally, we perform a thorough comparison and extensive tests on multiple datasets.', 'corpus_id': 236447646, 'score': 1}, {'doc_id': '12081873', 'title': 'Equine leukoencephalomalacia (ELEM) due to fumonisins B1 and B2 in Argentina', 'abstract': 'In August 2007 an outbreak of neurological disease and sudden death in Arabian horses occurred in a farm located in Coronel Rosales County, Buenos Aires Province, Argentina. The animals were on a pasture of native grasses and supplemented ad libitum with corn kernels and wheat bran. Three horses were observed having acute neurologic signs including blindness, four leg ataxia, hyperexcitability, aimless walking and circling, followed by death in two of them. Four other horses were found dead overnight without a history of neurologic signs. The morbidity, mortality and lethality rates were 11.6%, 10% and 85.7%, respectively. Grossly, the brain showed focal areas of hemorrhage, brown-yellow discoloration and softening of the sub-cortical white matter. The microscopic brain lesions consisted of extensive areas of malacia within the white matter of the cerebral hemispheres, brainstem and cerebellum, characterized by rarefaction of the white matter with cavitations filled with proteinaceous edema, multifocal hemorrhages and mild infiltration by neutrophils, and rare eosinophils. Swollen glial cells with abundant eosinophilic cytoplasm, distinct cell borders, intracytoplasmic deeply eosinophilic globules and eccentric, hyperchromatic, occasionally pyknotic nucleus were present throughout the areas of rarefaction hemorrhage, edema and necrosis. The feed supplements contained 12,490µg/kg of fumonisin B1 and 5,251µg/ kg of fumonisin B2. This is the first reported outbreak of ELEM associated with consumption of feed supplements containing high concentrations of fumonisins in Argentina.', 'corpus_id': 12081873, 'score': 0}, {'doc_id': '187827709', 'title': 'Determining the Viability of Electric Vehicles on Nantucket', 'abstract': 'I', 'corpus_id': 187827709, 'score': 0}, {'doc_id': '226406805', 'title': 'Room Effect on Musicians’ Performance', 'abstract': 'This chapter reviews the basics of music and room acoustics perception, an overview of auralization methods for the investigation of music performance and a series of studies related to the impact of room acoustics on listeners and musicians. The acoustics of the performance environment play a major role for musicians, both during rehearsals and concerts. However, systematic investigations of music performance are challenging due to the variety of conditions that determine the artists’ performance. Set-ups that allow controlled studies with variable but well-defined acoustic conditions have been developed over the last decades with increasing naturalness and applicability. Current auralization methods allow the reproduction of measured or synthesized room acoustics in real-time, thus enabling the perceptual assessment of room acoustics in laboratory conditions, isolating acoustics from other potential impacting factors. Common methodologies, as well as advantages and limitations of such virtual environments for the study of music and room acoustics perception are discussed in the first section. The virtual environments enable studies that help to explain why and how room acoustics can affect the listener subjective impact of a musical performance and to what extent listeners can be classified depending on their individual taste. Recent studies have shown that musicians systematically adjust their musical performance and adapt to the room acoustical conditions. The most important findings from these studies are presented in the second section. Methods and results from recent investigations of the impact of room acoustics on music performance are discussed in the third section of this chapter.', 'corpus_id': 226406805, 'score': 1}, {'doc_id': '236504453', 'title': 'Announcements', 'abstract': 'The annual conference of the International Society for Music Information Retrieval (ISMIR) will take place 8– 12 November 2021. The conference will be held virtually and comprise research presentations and workshops related to music information retrieval (MIR). Topics of interest at ISMIR include music signal processing, symbolic music processing, music-related metadata, representations of music, music acoustics, computational music theory and musicology, machine learning for music, sound source separation, music transcription and annotation, music generation, evaluation methodologies, novel datasets, philosophical and methodological foundations of MIR, human–computer interaction, digital libraries and archives, music', 'corpus_id': 236504453, 'score': 0}, {'doc_id': '146079650', 'title': 'Transferring Piano Performance Control across Environments', 'abstract': 'Player pianos driven by computers are able to record and reproduce various performance control parameters, including pitch, timing, velocity and pedaling. However, the resulting sound of performance is not 100% reproducible in a new environment due to the difference in room acoustics and physical properties of the piano. Inspired by the Psychoacoustic studies which showed that human pianists adjust their controls in new environments for better performances, we have developed a system that automatically transfers performance control across environments in order to make the reproduced sound as similar as the original one. In specific, our work includes (1) a systematic measurement of the control-sound relationship of player pianos under different environments, and (2) a novel algorithm to adjust the control parameters through interpolating the measured control-sound functions. We evaluated the effectiveness of our method by conducting a listening test. Experimental results show that our algorithm outperforms the baseline significantly.', 'corpus_id': 146079650, 'score': 1}, {'doc_id': '236317694', 'title': 'Public Domain Guitar-Recordings in Studio Quality', 'abstract': 'Our contribution presents free guitar samples, their score, and documentation. While the original idea is to provide ecologically valid testing material for virtual auditory environments (VAEs), the use of the guitar samples is not restricted to this application and also permits various other, free, academic and non-academic applications. In particular, the endeavor is part of a project of the German Acoustical Society (DEGA) and its TC on Virtual Acoustics that was launched in spring 2016. The project includes establishing suitable models of an extensible database of audio material. It considers public domain licenses, thinkable compensation of and contracts with musicians, and establishing best practice models for free, quotable, technically and musically welldocumented, public domain audio content. More information about that project can be found in [1].', 'corpus_id': 236317694, 'score': 0}]
2	GaN HEMT Reliability	14cc92a99219c8b58a6f035f91705266	10405	{'HEMT': 'high electron mobility transistor'}	[{'doc_id': '221265989', 'title': 'Indium-Tin-Oxide Transistors with One Nanometer Thick Channel and Ferroelectric Gating.', 'abstract': 'In this work, we demonstrate high performance indium-tin-oxide (ITO) transistors with the channel thickness down to 1 nm and ferroelectric Hf0.5Zr0.5O2 as gate dielectric. On-current of 0.243 A/mm is achieved on sub-micron gate-length ITO transistors with a channel thickness of 1 nm, while it increases to as high as 1.06 A/mm when the channel thickness increases to 2 nm. A raised source/drain structure with a thickness of 10 nm is employed, contributing to a low contact resistance of 0.15 Ω⋅mm and a low contact resistivity of 1.1×10-7 Ω⋅cm2. The ITO transistor with a recessed channel and ferroelectric gating demonstrates several unique advantages over 2D semiconductor transistors and other thin film transistors, including large-area wafer-size nanometer thin film formation, low contact resistance and contact resistivity, atomic thin channel being immunity to short channel effects, large gate modulation of high carrier density by ferroelectric gating, high-quality gate dielectric and passivation formation, and a large bandgap for the low-power back-end-of-line (BEOL) CMOS application.', 'corpus_id': 221265989, 'score': 0}, {'doc_id': '221172915', 'title': 'Performance Enhancement of Thin-Film Tunneling FETs by Drain Material Engineering Using an Ultra-Thin SiGe', 'abstract': 'In this work, the drain structure of a p-type thin-film tunneling FET is engineered to get better performance. An ultra-thin SiGe along with Si is used in the drain of silicon-based TFET. Two structures are compared with conventional TFET, one, SiGe is located on the top of Si in the drain and another one in reverse. Simulations approve these structures can reduce sub-threshold swing, OFF-current several times, and increase the ON-OFF ratio. Band diagram for conduction and valance bands are investigated and band to band tunneling (BTBT) generation rate is used to find better performance. We find current flows at Si in the drain with the wider bandgap. Ge mole fraction of SiGe is varied and its effects on the performance of TFET are studied. The SiGe thickness for both structures is explored to obtain the best thickness for SiGe.', 'corpus_id': 221172915, 'score': 0}, {'doc_id': '224818443', 'title': 'Role of ALD Al2O3 Surface Passivation on the Performance of p-Type Cu2O Thin Film Transistors.', 'abstract': 'High-performance p-type oxide thin film transistors (TFTs) have great potential for many semiconductor applications. However, these devices typically suffer from low hole mobility and high off-state currents. We fabricated p-type TFTs with a phase-pure polycrystalline Cu2O semiconductor channel grown by atomic layer deposition (ALD). The TFT switching characteristics were improved by applying a thin ALD Al2O3 passivation layer on the Cu2O channel, followed by vacuum annealing at 300 °C. Detailed characterization by transmission electron microscopy-energy dispersive X-ray analysis and X-ray photoelectron spectroscopy shows that the surface of Cu2O is reduced following Al2O3 deposition and indicates the formation of a 1-2 nm thick CuAlO2 interfacial layer. This, together with field-effect passivation caused by the high negative fixed charge of the ALD Al2O3, leads to an improvement in the TFT performance by reducing the density of deep trap states as well as by reducing the accumulation of electrons in the semiconducting layer in the device off-state.', 'corpus_id': 224818443, 'score': 0}, {'doc_id': '13617792', 'title': 'Improved reliability of AlGaN/GaN-on-Si high electron mobility transistors (HEMTs) with high density silicon nitride passivation', 'abstract': 'Abstract We have systematically studied the effects of SixN1\xa0−\xa0x passivation density on the reliability of AlGaN/GaN high electron mobility transistors. Upon stressing, devices degrade in two stages, fast-mode degradation and followed by slow-mode degradation. Both degradations can be explained as different stages of pit formation at the gate-edge. Fast-mode degradation is caused by pre-existing oxygen at the SixN1\xa0−\xa0x/AlGaN interface. It is not significantly affected by the SixN1\xa0−\xa0x density. On the other hand, slow-mode degradation is associated with SixN1\xa0−\xa0x degradation. SixN1\xa0−\xa0x degrades through electric-field induced oxidation in discrete locations along the gate-edges. The size of these degraded locations ranged from 100 to 300\xa0nm from the gate edge. There are about 16 degraded locations per 100\xa0μm gate-width. In each degraded location, low density nano-globes are formed within the SixN1\xa0−\xa0x. Because of the low density of the degraded locations, oxygen can diffuse through these areas and oxidize the AlGaN/GaN to form pits. This slow-mode degradation can be minimized by using high density (ρ\xa0=\xa02.48\xa0g/cm3) Si36N64 as the passivation layer. For slow-mode degradation, the median time to failure of devices with high density passivation is found to increase up to 2× as compared to the low density (ρ\xa0=\xa02.25\xa0g/cm3) Si43N57 passivation. A model based on Johnson-Mehl-Avrami theory is proposed to explain the kinetics of pit formation.', 'corpus_id': 13617792, 'score': 1}, {'doc_id': '222173489', 'title': 'N-polar GaN/AlN resonant tunneling diodes', 'abstract': 'N-polar GaN/AlN resonant tunneling diodes are realized on single-crystal N-polar GaN bulk substrate by plasma-assisted molecular beam epitaxy growth. The room-temperature current-voltage characteristics reveal a negative differential conductance (NDC) region with a peak tunneling current of 6.8$\\pm$ 0.8 kA/cm$^2$ at a forward bias of ~8 V. Under reverse bias, the polarization-induced threshold voltage is measured at ~$-$4 V. These resonant and threshold voltages are well explained with the polarization field which is opposite to that of the metal-polar counterpart, confirming the N-polarity of the RTDs. When the device is biased in the NDC-region, electronic oscillations are generated in the external circuit, attesting to the robustness of the resonant tunneling phenomenon. In contrast to metal-polar RTDs, N-polar structures have the emitter on the top of the resonant tunneling cavity. As a consequence, this device architecture opens up the possibility of seamlessly interfacing$-$via resonant tunneling injection$-$a wide range of exotic materials with III-nitride semiconductors, providing a route to explore new device physics.', 'corpus_id': 222173489, 'score': 0}, {'doc_id': '99351612', 'title': 'Threading dislocation movement in AlGaN/GaN-on-Si high electron mobility transistors under high temperature reverse bias stressing', 'abstract': 'Dislocations are known to be associated with both physical and electrical degradation mechanisms of AlGaN/GaN-on-Si high electron mobility transistors (HEMTs). We have observed threading dislocation movement toward the gate-edges in AlGaN/GaN-on-Si HEMT under high reverse bias stressing. Stressed devices have higher threading dislocation densities (i.e. ∼5 × 109/cm2) at the gate-edges, as compared to unstressed devices (i.e. ∼2.5 × 109/cm2). Dislocation movement correlates well with high tensile stress (∼1.6 GPa) at the gate-edges, as seen from inverse piezoelectric calculations and x-ray synchrotron diffraction residual stress measurements. Based on Peierls stress calculation, we believe that threading dislocations move via glide in 〈112¯0〉/{11¯00} and 〈112¯0〉/{11¯01} slip systems. This result illustrates the importance of threading dislocation mobility in controlling the reliability of AlGaN/GaN-on-Si HEMTs.', 'corpus_id': 99351612, 'score': 1}, {'doc_id': '35477667', 'title': 'Role of two-dimensional electron gas (2DEG) in AlGaN/GaN high electron mobility transistor (HEMT) ON-state degradation', 'abstract': 'Abstract We have investigated the influence of the two-dimensional electron gas (2DEG) in AlGaN/GaN high electron mobility transistors (HEMTs) on their reliability under ON-state conditions. Devices stressed in the ON-state showed a faster decrease in the maximum drain current (I Dmax ) compared to identical devices stressed in the OFF-state with a comparable electric field and temperature. Scanning electron microscope (SEM) images of ON-state stressed devices showed pit formation at locations away from the gate-edge in the drain-gate access region. Cross-sectional transmission electron microscope (TEM) images also showed dark features at the AlGaN/SiN interface away from the gate edge. Electron energy loss spectroscopy (EELS) analysis of the dark features indicated the presence of gallium, aluminum and oxygen. These dark features correlate with pits observed in the SEM micrographs. It is proposed that in addition to causing joule heating, energetic electrons in the 2D electron gas contribute to device degradation by promoting electrochemical oxidation of the AlGaN.', 'corpus_id': 35477667, 'score': 1}, {'doc_id': '104562707', 'title': 'Influence of substrate nitridation on the threading dislocation density of GaN grown on 200 mm Si (111) substrate', 'abstract': 'Abstract High quality GaN was grown on 200\u202fmm Si (111) substrates by using AlN and 3 step-graded AlxGa1-xN as the buffer layer in a metalorganic chemical vapor deposition system. We have investigated the influence of NH3 pre-flow time on the threading dislocation density (TDD) of AlN, AlGaN buffer layers and GaN layers. It was observed that the compressive stress introduced into the buffer layer and GaN is dependent on the nitridation time. The lowest TDD for GaN obtained in our samples was ~1\u202f×\u202f109\u202fcm−2 for screw type and 3.2\u202f×\u202f109 cm−2 for edge type dislocations, as obtained from atomic force microscopy and further confirmed by high resolution X-ray diffraction analysis. The threading dislocations generated in the first buffer layer (AlN) during its nucleation are found to influence the TDD in the subsequent layers. Samples without an intentional nitridation step exhibit higher TDD compared to the samples with optimal nitridation time. Longer nitridation time also leads to poor crystalline quality likely because of amorphous SiNx formation at the interface.', 'corpus_id': 104562707, 'score': 1}, {'doc_id': '28605369', 'title': 'Origin of physical degradation in AlGaN/GaN on Si high electron mobility transistors under reverse bias stressing', 'abstract': 'We have investigated the role of threading dislocations in pit formation during stressing of AlGaN/GaN on Si high electron mobility transistors under high reverse bias. Upon stressing, the drain current saturation (ID-saturation) decreases over time. The amount of ID-saturation degradation correlates well with pit formation at the gate-edge, where the electric field is the highest. Using a transmission electron microscope weak-beam technique, it is found that pits tend to nucleate at threading dislocations that have a screw component, even when these dislocations are at locations away from the gate-edge. An explanation based on an electrochemical oxidation model is proposed.', 'corpus_id': 28605369, 'score': 1}, {'doc_id': '221266265', 'title': 'Gate-controlled field emission current from MoS$_2$ nanosheets', 'abstract': 'Monolayer molybdenum disulfide (MoS$_2$) nanosheets, obtained via chemical vapor deposition onto SiO$_2$/Si substrates, are exploited to fabricate field-effect transistors with n-type conduction, high on/off ratio, steep subthreshold slope and good mobility. The transistor channel conductance increases with the reducing air pressure due to oxygen and water desorption. Local field emission measurements from the edges of the MoS$_2$ nanosheets are performed in high vacuum using a tip-shaped anode. It is demonstrated that the voltage applied to the Si substrate back-gate modulates the field emission current. Such a finding, that we attribute to gate-bias lowering of the MoS$_2$ electron affinity, enables a new field-effect transistor based on field emission.', 'corpus_id': 221266265, 'score': 0}]
3	Science of magic	6525a73d632f862f69cff43bf2fc5446	10856	{}	"[{'doc_id': '222089449', 'title': 'Why People Perceive Messages Differently: The Theory of Cognitive Mapping', 'abstract': 'Aim/Purpose The paper introduces new concepts including cognitive mapping, cognitive message processing, and message resonance. Background This paper draws upon philosophy, psychology, physiology, communications, and introspection to develop the theory of cognitive mapping. Methodology Theory development. Contribution The theory offers new ways to conceptualize the informing process. Findings Cognitive mapping has a far-reaching explanatory power on message resonance. Recommendations for Researchers The theory of cognitive mapping offers a new conceptualization for those exploring the informing process that is ripe for exploration and theory testing. Future Research This paper forms a building block toward the development of a fuller model of the informing process.', 'corpus_id': 222089449, 'score': 0}, {'doc_id': '151042764', 'title': 'Is Magic a Serious Research Topic? Reflexions On Some French Students’ Remarks About Magic in Psychology', 'abstract': 'Is magic a serious research topic? This question seems a bit weird. Obviously, in psychology show magic is a very serious topic. It deals with psychological processes, and with social interactions. In the late 19th century and early 20th century, famous psychologists as Binet [1,2]; Jastrow [3]; Triplett [4] use magical performances to understand psychological functioning. In 1999 Lamont & Wiseman [5] published Magic in theory, a theoretical book analysing the art of magic. Between 1887 and 1999, only 12 empirical articles about perception of magic in adults were published; they were 55 between 2000 and 2016 [6]. Nowadays, this field is sometimes called “science of magic” [7,8] or, most specifically, “neuromagic” [9]. In psychology, magic can be studied as any other object, or can be a mean to study psychological processes [10]. To our knowledge, most studies deal with perception and cognitive psychology [11,12] and very few with social psychology [13,14].', 'corpus_id': 151042764, 'score': 1}, {'doc_id': '152214855', 'title': 'A Particular Kind of Wonder: The Experience of Magic past and Present', 'abstract': 'Wonder may be an important emotion, but the term wonder is remarkably ambiguous. For centuries, in psychological discourse, it has been defined as a variety of things. In an attempt to be more focused, and given the growing scientific interest in magic, this article describes a particular kind of wonder: the response to a magic trick. It first provides a historical perspective by considering continuity and change over time in this experience, and argues that, in certain respects, this particular kind of wonder has changed. It then describes in detail the experience of magic, considers the extent to which it might be considered acquired rather than innate, and how it relates to other emotions, such as surprise. In the process, it discusses the role of belief and offers some suggestions for future research. It concludes by noting the importance of context and meaning in shaping the nature of the experience, and argues for the value of both experimental and historical research in the attempt to understand such experiences.', 'corpus_id': 152214855, 'score': 1}, {'doc_id': '224802259', 'title': ""Humanity's Magic Number as 1.5?"", 'abstract': 'Introduction Preliminary checklist of recognition of 1.5 Potentially significant approximations to 1.5 in practice? Clarification of the magic square governing human civilization? Radical possibility of 1.5 resulting from from golden ratio design by committee In quest of a language of proportion as the language of appropriateness Adaptation of the Uncertainty Principle to the social sciences? Language of proportion implied by poetic justice Deflowering of civilization versus Flowering of civilization: an aesthetic contrast? Spiral of silence and the associated ""conspiracy"" of silence Mysterious challenge of doubling, replication and multiplication References', 'corpus_id': 224802259, 'score': 0}, {'doc_id': '13949442', 'title': 'Where Science and Magic Meet: The Illusion of a “Science of Magic”', 'abstract': 'Recent articles calling for a scientific study of magic have been the subject of widespread interest. This article considers the topic from a broader perspective and argues that to engage in a science of magic, in any meaningful sense, is misguided. It argues that those who have called for a scientific theory of magic have failed to explain either how or why such a theory might be constructed, that a shift of focus to a neuroscience of magic is simply unwarranted, and that a science of magic is itself an inherently unsound idea. It seeks to provide a more informed view of the relationship between science and magic and suggests a more appropriate way forward for scientists.', 'corpus_id': 13949442, 'score': 1}, {'doc_id': '221995493', 'title': 'How do Visualization Designers Think? Design Cognition as a Core Aspect of Visualization Psychology', 'abstract': 'There are numerous opportunities for engaging in research at the intersection of psychology and visualization. While most opportunities taken up by the VIS community will likely focus on the psychology of users, there are also opportunities for studying the psychology of designers. In this position paper, I argue the importance of studying design cognition as a necessary component of a holistic program of research on visualization psychology. I provide a brief overview of research on design cognition in other disciplines, and discuss opportunities for VIS to build an analogous research program. Doing so can lead to a stronger integration of research and design practice, can provide a better understanding of how to educate and train future designers, and will likely surface both challenges and opportunities for future research.', 'corpus_id': 221995493, 'score': 0}, {'doc_id': '222007362', 'title': 'Emotions in Technology Design: From Experience to Ethics', 'abstract': 'Emotions are a hot topic in design, human–computer interaction and any area of business these days. Their significance in areas inwhich peoplemake choices, decisions and engage in action has been undeniable for at least the last 40 years of psychology and consumer scholarship.What once was an extremely contested, fuzzy and (almost) easily scientifically avoidable area, is now at the centre of everyone’s interest. In an era of cognitive computing, artificial intelligence (so-called learning and thinking machines), and optimization, all attention is placed on what makes us human, and theways inwhich human thought actually operates. This emotional logic, intentionality and consciousness itself, drive not simply theways inwhich individuals process (cognitise) information, but also ways in which society and the built world are structured. Emotions play asmuch a role in shaping technology design, as they do in the way we experience it. This introduction presents a book that takes many angles towards concretely understanding what it is in design that makes people emotionally experience it in the ways that they do. It introduces the main themes and concepts of the book that include ethics, culture, measurement and design methods. It additionally demonstrates a broader understanding of technology in chapters that investigate graffiti, urban and art experience, filmic experience, architecture and cultural movements. It is hoped that combining this broader cultural-emotional insight into one package will enable readers to connect their design practice and research to the broader system of emotions, culture, ethics, lived experience and technology. R. Rousi (B) University of Jyväskylä and Gofore, Jyväskylä, Finland e-mail: rebekah.rousi@jyu.fi J. Leikas VTT, Espoo, Finland e-mail: jaana.leikas@vtt.fi P. Saariluoma University of Jyväskylä, Jyväskylä, Finland e-mail: pertti.saariluoma@jyu.fi © Springer Nature Switzerland AG 2020 R. Rousi et al. (eds.), Emotions in Technology Design: From Experience to Ethics, Human–Computer Interaction Series, https://doi.org/10.1007/978-3-030-53483-7_1 1', 'corpus_id': 222007362, 'score': 0}, {'doc_id': '226602116', 'title': 'Investigating students’ affective states toward laboratory and context-based chemistry', 'abstract': 'Observations of natural phenomena are made possible with the invention of scientific apparatus and instruments. The focus in science education, however, has primarily been on theories rather than what enables the development of such theories, and chemistry curricula reflect this tradition. Introducing students to the role of instruments in science, both in experimental and theoretical aspects, can improve students’ overall understanding of, and appreciation for scientific practices. In addition, students’ increased perception of how chemical concepts are developed and how scientific observations are made can advance their awareness of the nature of science, thereby improving scientific literacy. Integrating the idea that instruments hold a central role in scientific progression can be achieved in both laboratories and lectures, providing students with opportunities to connect concepts to history, scientific practices, and applications. This dissertation is comprised of a series of studies which explores the use of technology and context-based curricular approach to provide general chemistry students with more information about instruments and applications in chemistry. Based on constructivism and the theory of meaningful learning, the affective learning domain, such as attitudes and motivation, was assessed in both chemistry laboratory and lecture courses. An augmented reality tool designed to connect students to information about commonly used instruments in a general chemistry lab course, specifically a pH meter and conductivity meter, was developed, implemented, and its effects on student learning and attitudes were investigated. In addition, for a chemistry lecture course, a context-based curricular approach was taken to introduce students to chemical concepts related to real-life applications, as well as to the role of scientific instruments, and this effort was assessed.', 'corpus_id': 226602116, 'score': 0}, {'doc_id': '142592204', 'title': 'Magic in Theory', 'abstract': None, 'corpus_id': 142592204, 'score': 1}, {'doc_id': '141795421', 'title': 'Magic in Theory: An Introduction to the Theoretical and Psychological Elements of Conjuring', 'abstract': 'Magic, properly performed, is a complex and skilful art, and is capable of deceiving anyone. One of the reasons why so little information is available to non-magicians interested in the topic is that magicians are understandably relectant to expose conjuring methods. Magic is a secretive business. Psychologists have long recognised that they may have much to learn from the techniques used by magicians to fool their audiences. Parapsychologists are aware that many individuals claiming to be psychic use magic tricks to fabricate paranormal phenomena. Failure to detect such fraud can lead to serious consequences, including loss of funding and negative publicity. Greater theoretical understanding of conjuring and psychic fraud will raise awareness of how vulnerable observers can be. Parapsychologists, psychologists and magicians have all written about the strategems that lie behind successful conjuring. Each has approached the topic from different viewpoints. This book is an attempt to draw together these different theoretical approaches and present them in a way that is accessible to a non-technical readership. It is partly based on interviews conducted with present-day magicians, many of who are internationally recognized by the magical fraternity for their insight into conjuring psychology and theory.', 'corpus_id': 141795421, 'score': 1}]"
4	Central Force Problem - Relativistic	e35c04c4556bae47d28884fb02fa9c56	20157	{}	"[{'doc_id': '236634319', 'title': 'Special relativity on the light-front', 'abstract': 'P.A.M. Dirac in 1949 showed that it is possible to construct relativistic dynamic forms starting from the description of the initial state of a given relativistic system in any space-time surface whose distances between two points on this hypersurface has no causal connection. The dynamic evolution corresponds to such a system following a trajectory through this hypersurface. For example, the commonest hypersurface of time t = 0 is our three-dimensional (Euclidean) space. It is invariant by rotations and translations. However, in any transformation of inertial frame of references that involves “boosts”, the time coordinate is modified and, consequently, the hypersurface at t = 0. Other hypersurfaces may be invariant through some kind of “boost”; the hyperplane that is called null-plane is such a hyperplane, defined by x+ = t + z/c, in which c is the speed of light in vacuum, and plays the role of the “time” coordinate in the light-front. The null-plane defined in such a way guarantees that a “boost” in the z direction does not modify the null-plane. Our aim here is to study special relativity under such a transformation of frame of references and see the consequences thereof.', 'corpus_id': 236634319, 'score': 1}, {'doc_id': '75586077', 'title': 'Efficacy of 56% aluminum phosphide tablet in dispose of the plague epidemic areas', 'abstract': 'Objective To evaluate the efficacy of 56% aluminum phosphide tablet in dispose of the plague epidemic areas of Meriones unguiculatus.Methods 56% aluminum phosphide tablet was used to kill rodents and parasitic fleas in different areas.Results The density of field rodents declined in 5 days and reached the expected standard.The rate of eliminating rodents were above 90%.Conclusion 56% aluminum phosphide tablet have a good effect in dispose of the plague epidemic areas of Meriones unguiculatus.', 'corpus_id': 75586077, 'score': 0}, {'doc_id': '222225047', 'title': 'Structure of the Cordillera de la Sal: A key tectonic feature for the Oligocene-Neogene evolution of the Salar de Atacama basin, Central Andes of Northern Chile', 'abstract': 'The Salar de Atacama basin is the main topographic low in the Preandean Depression of the Central Andes of Northern Chile. The integration of seismic reflection and surface structural data along the basin allows constrain the Oligocene and Neogene tectonic activity of the Salar de Atacama. A key element to unravel the Neogene to recent history of the basin is found along the Cordillera de la Sal which comprises more than 3.000 m of continental sedimentary successions belonging to the San Pedro Formation. Detailed analysis of the seismic information shows that large depocenters involving distal alluvial facies and evaporitic members of the San Pedro Formation were accumulated in close relation with Oligocene extension. Extension was controlled by a first order normal fault located along the western flank of the basin. The rise of the Cordillera de la Sal ridge involved compression and sinistral strike slip in its south domain, in combination with salt diapirism in its north domain, this transition is related to a change in the depth of the detachment level from 4000 to 6000 m from south to north, this detachment is mainly associated with the evaporitic lower members of the San Pedro Formation.', 'corpus_id': 222225047, 'score': 0}, {'doc_id': '236907382', 'title': 'Mechanics Notes: Landau & Lifshitz', 'abstract': 'Dynamically, Lagrangians are equivalent up to a total time derivative of a function of coordinates and time. So the Lagrangians L(q, q̇, t) and L′(q, q̇, t) = L(q, q̇, t) + d f (q, t) / dt yield the same equations of motion, because the extra term of L′ gives a constant term in the action that vanishes upon minimization. Rescaling by a constant also has no effect upon the motion (arbitrary units).', 'corpus_id': 236907382, 'score': 0}, {'doc_id': '236975912', 'title': 'Dual Relativistic Quantum Mechanics I', 'abstract': 'It was shown in [1] that the ultra-violet divergence in quantum electrodynamics (QED) is caused by a violation of the time-energy uncertainly relationship, due to the implicit assumption of infinitesimal time information (Dyson’s conjecture). In [2] it was shown that Einstein’s special theory of relativity and Maxwell’s field theory have mathematically equivalent dual versions. The dual versions arise from an identity relating observer time to proper time as a contact transformation on configuration space, which leaves phase space invariant. The special theory has a dual version in the sense that, for any set of n particles, every observer has two unique sets of global variables (X, t) and (X, τ ) to describe the dynamics, where X is the (unique) canonical center of mass. In the (X, t) variables, time is relative and the speed of light is unique, while in the (X, τ ) variables, time is unique and the speed of light is relative with no upper bound. In the Maxwell case, the two sets of particle wave equations are not equivalent. The dual version contains an additional longitudinal (dissipative) radiation term that appears instantaneously with acceleration, leading to the prediction that radiation from a betatron (of any frequency) will not produce photoelectrons. A major outcome is the dual unification of Newtonian mechanics and classical electrodynamics with Einstein’s special theory of relativity, without a self-energy divergency, or need of the problematic Lorentz-Dirac equation or any assumptions about the size or structure of a particle. The purpose of this paper is to introduce and develop the dual theory of relativistic quantum mechanics. We obtain three distinct dual relativistic wave equations that reduce to the Schrödinger equation when minimal coupling is turned off. We show that the dual Dirac equation provides a new formula for the anomalous magnetic moment of a charged particle. We can obtain the exact value for the electron g-factor and phenomenological values for the muon and proton g-factors.', 'corpus_id': 236975912, 'score': 1}, {'doc_id': '236973551', 'title': 'A compactified (almost popular) description of the unified fundamental interaction based on the Maxwell electromagnetism', 'abstract': 'Why two particles electrically charged with the charges of opposite polarity attract each other and those charged with the charges of the same polarity are mutually repelent? Why is the gravity exclusively the attractive force? Why there is inertia of mass, but no inertia of electric charge? What is an intrinsic relationship between the electric and gravitational interaction? How can the structure of the atom be described within the theories of macroscosm as Maxwell theory of electromagnetism and/or general relativity? This five and several further fundamental questions are attempted to be answered by the authors of the most advanced physical theories. Here, we present an extract of the main results of our recent work [1] and [2], which addresses and answers these and several further questions in the case of the interaction between two stable (or accelerating from the rest), electrically charged, point-like particles. The work does not bring any principially new theory. It is a new, unitary representation of the well-known Maxwell theory of electromagnetism, completed with some formulas of general relativity. The reader likely asks, why he or she should preferably devote his or her attention just to this work published among tens, maybe hundreds, other works published every year, which also address the above mentioned questions? After the Dirac’s quantum theory, the unitary representation is only the second theory ever providing, in an independent way, the exact theoretical determination of the energy states in the spectrum of hydrogen atom (with the same precision as Dirac’s theory provides). Moreover, one can hardly find another theory which would answer all the asked questions at the same time, within a single framework. The new representation remarkably applies the Occam’s razor: to answer the questions, one can forget, in principle, the whole physics developed after Maxwell and Einstein, a major part of quantum physics including. The representation is, in fact, a continuation of the main-stream physics of the beginning of 20-th century. Of course, the value of the knowledge achieved in the post-Maxwell and post-Einstein era is not lost, a lot of mathematical procedures and partial concepts from the post eras is utilized also within the new approach.', 'corpus_id': 236973551, 'score': 0}, {'doc_id': '237439358', 'title': 'Influence of cosmological expansion in local experiments', 'abstract': 'Whether the cosmological expansion can influence the local dynamics, below the galaxy clusters scale, has been the subject of intense investigations in the past three decades. In this work, we consider McVittie and Kottler spacetimes, embedding a spherical object in a FLRW spacetime. We calculate the influence of the cosmological expansion on the frequency shift of a resonator and estimate its effect on the exchange of light signals between local observers. In passing, we also clarify some of the statements made in the literature. 1 ar X iv :2 10 9. 03 28 0v 1 [ gr -q c] 7 S ep 2 02 1', 'corpus_id': 237439358, 'score': 0}, {'doc_id': '236942570', 'title': 'An “ab initio” Model for Quantum Theory and Relativity', 'abstract': 'The paper introduces a theoretical model aimed to show how the relativity can be made consistent with the non reality and non locality of the quantum physics. The concepts of quantization and superposition of states, usually regarded as distinctive properties of the quantum world, can be extended also to the relativity.', 'corpus_id': 236942570, 'score': 1}, {'doc_id': '237142449', 'title': ""Relativistic Mechanics Theory for Electrons that Exhibits Spin, Zitterbewegung, Superposition and Produces Dirac's Wave Equation"", 'abstract': 'A neo-classical relativistic mechanics model is presented where the spin of an electron is a natural part of its world space-time path as a point particle. The fourth-order equation of motion corresponds to the same Lagrangian function in proper time as in special relativity except for an additional spin energy term. The dynamic variables give a complete description of the electron in the classical mechanics tradition (hence “neo-classical"") that explains its spin, Schrödinger’s zitterbewegung and the presence of a magnetic moment. The total motion can be decomposed into a sum of a local circular motion about a point and a global motion of this point, which is called here the spin center. The local spin motion corresponds to Schrödinger’s zitterbewegung and is a perpetual circular motion relative to a reference frame fixed at the spin center. This local motion produces magnetic and electric dipoles through the Lorentz force on the electron’s point charge. The global motion is sub-luminal and described by Newton’s second law in proper time, the time for a clock fixed at the spin center, while the total motion occurs at the speed of light c, consistent with the eigenvalues of Dirac’s velocity operators having magnitude c. A spin tensor is introduced that is the angular momentum of the electron’s total motion about its spin center. The fundamental equations of motion re-written in an equivalent form using this spin tensor are identical to those of the Barut-Zanghi theory, which is then used to express the equations of motion in an equivalent operator form applied to a state function of proper time satisfying a neo-classical Dirac-Schrödinger spinor equation. This state function produces the dynamic variables from the same operators as in Dirac’s theory for the electron but with deterministic superpositions. It leads to a neo-classical wave function that satisfies Dirac’s relaPreprint submitted to arXiv.org August 18, 2021 tivistic wave equation for the free electron by applying the Lorentz transformation to express proper time in the state function in terms of an observer’s space-time coordinates. In summary, the presented neo-classical theory provides a complete hidden-variable model for spin that leads to Dirac’s relativistic wave equation for the free electron and that explains the electron’s moment coupling to an electro-magnetic field, albeit with a magnetic moment that is one half of that in Dirac’s theory.', 'corpus_id': 237142449, 'score': 1}, {'doc_id': '236475384', 'title': 'Combined theory of Special Relativity and Quantum Mechanics', 'abstract': 'Lorentz transformation plays a key role in Special Relativity by relating the spacetime distance between events being observed in a pair of inertial frames of reference. Depending on the relative velocity of the inertial frames, the magnitude of Lorentz transformation varies between the limits 0 and 1. The upper limit 1 represents a case where the pair of inertial frames of reference are stationary relative to each other. The lower limit 0 represents the other extreme case where the relative velocity of the frames is at the speed of light c. Similar numerical limits, on the other hand, appear in Quantum Mechanics but in the context of the summation of the probability density distribution of a particle over a region of space. The upper limit 1 represents a case where the probability of finding a particle in a region of space is certain. The lower limit, represents the opposite case where the probability of finding a particle in a region of space is not likely. The range of the limits being between 0 and 1 in both theories is not a numerical coincidence. In this paper, a combined theory is introduced which relates the Lorentz transformation of Special Relativity to the wavefunction of Quantum Mechanics. The combined theory offers a new insight to the physical reality. For instance, it is found that the inherent quantum uncertainties in the spacetime coordinate of a quantum particle in vacuum constitute a timelike four-vector whose length A is invariant. It is also found that local acceleration, like velocity itself, has an upper limit; such that no physical object can undergo a local acceleration higher than c2/A. The latter, in turn, constrains the mass of the smallest possible black hole called Unit Black Hole (UBH) to Ac2/4G and its event horizon diameter to the invariant A. The diameter of the event horizon, the mass and the Hawking temperature of more massive black holes are subsequently quantized starting from those of the UBH.', 'corpus_id': 236475384, 'score': 1}]"
5	Modularity	ab6293fb6b2a5eb7d5097158b4f3944f	18444	{}	"[{'doc_id': '147050708', 'title': 'Intergenerational Transmission of Musical Education', 'abstract': 'There is an extensive literature documenting the fact that there is a positive correlation between parental education and that of their children. While most research has focused on the transmission of formal schooling, there are other aspects of education that may be considered. For instance, music training has been shown to have a positive correlation with other cognitive abilities, such as mathematics and linguistics. In this paper, we analyze the intergenerational transmission of musical education. We have collected data on musical, general arts and formal education on a representative sample of Asturias, a Northern Spanish region. We find that the intergenerational link goes from both parents to their children. Furthermore, mothersâ€™ musical training has a greater impact on males than that of the fathersâ€™. On the contrary, in the case of females, only the father-child link is significant.', 'corpus_id': 147050708, 'score': 0}, {'doc_id': '141460329', 'title': 'Similarity of Neural Network Representations Revisited', 'abstract': 'Recent work has sought to understand the behavior of neural networks by comparing representations between layers and between different trained models. We examine methods for comparing neural network representations based on canonical correlation analysis (CCA). We show that CCA belongs to a family of statistics for measuring multivariate similarity, but that neither CCA nor any other statistic that is invariant to invertible linear transformation can measure meaningful similarities between representations of higher dimension than the number of data points. We introduce a similarity index that measures the relationship between representational similarity matrices and does not suffer from this limitation. This similarity index is equivalent to centered kernel alignment (CKA) and is also closely connected to CCA. Unlike CCA, CKA can reliably identify correspondences between representations in networks trained from different initializations.', 'corpus_id': 141460329, 'score': 1}, {'doc_id': '235446621', 'title': 'Gradient-trained Weights in Wide Neural Networks Align Layerwise to Error-scaled Input Correlations', 'abstract': 'Recent works have examined how deep neural networks, which can solve a variety of difficult problems, incorporate the statistics of training data to achieve their success. However, existing results have been established only in limited settings. In this work, we derive the layerwise weight dynamics of infinite-width neural networks with nonlinear activations trained by gradient descent. We show theoretically that weight updates are aligned with input correlations from intermediate layers weighted by error, and demonstrate empirically that the result also holds in finite-width wide networks. The alignment result allows us to formulate backpropagation-free learning rules, named Align-zero and Align-ada, that theoretically achieve the same alignment as backpropagation. Finally, we test these learning rules on benchmark problems in feedforward and recurrent neural networks and demonstrate, in wide networks, comparable performance to backpropagation.', 'corpus_id': 235446621, 'score': 0}, {'doc_id': '234683763', 'title': 'Layer-Skipping Connections Improve the Effectiveness of Equilibrium Propagation on Layered Networks', 'abstract': ""Equilibrium propagation is a learning framework that marks a step forward in the search for a biologically-plausible implementation of deep learning, and could be implemented efficiently in neuromorphic hardware. Previous applications of this framework to layered networks encountered a vanishing gradient problem that has not yet been solved in a simple, biologically-plausible way. In this paper, we demonstrate that the vanishing gradient problem can be mitigated by replacing some of a layered network's connections with random layer-skipping connections in a manner inspired by small-world networks. This approach would be convenient to implement in neuromorphic hardware, and is biologically-plausible."", 'corpus_id': 234683763, 'score': 0}, {'doc_id': '235254451', 'title': 'Efficient and robust multi-task learning in the brain with modular task primitives', 'abstract': 'In a real-world setting biological agents do not have infinite resources to learn new things. It is thus useful to recycle previously acquired knowledge in a way that allows for faster, less resource-intensive acquisition of multiple new skills. Neural networks in the brain are likely not entirely re-trained with new tasks, but how they leverage existing computations to learn new tasks is not well understood. In this work, we study this question in artificial neural networks trained on commonly used neuroscience paradigms. Building on recent work from the multi-task learning literature, we propose two ingredients: (1) network modularity, and (2) learning task primitives. Together, these ingredients form inductive biases we call structural and functional, respectively. Using a corpus of nine different tasks, we show that a modular network endowed with task primitives allows for learning multiple tasks well while keeping parameter counts, and updates, low. We also show that the skills acquired with our approach are more robust to a broad range of perturbations compared to those acquired with other multi-task learning strategies. This work offers a new perspective on achieving efficient multi-task learning in the brain, and makes predictions for novel neuroscience experiments in which targeted perturbations are employed to explore solution spaces.', 'corpus_id': 235254451, 'score': 1}, {'doc_id': '3475574', 'title': 'Neural Modularity Helps Organisms Evolve to Learn New Skills without Forgetting Old Skills', 'abstract': 'A long-standing goal in artificial intelligence is creating agents that can learn a variety of different skills for different problems. In the artificial intelligence subfield of neural networks, a barrier to that goal is that when agents learn a new skill they typically do so by losing previously acquired skills, a problem called catastrophic forgetting. That occurs because, to learn the new task, neural learning algorithms change connections that encode previously acquired skills. How networks are organized critically affects their learning dynamics. In this paper, we test whether catastrophic forgetting can be reduced by evolving modular neural networks. Modularity intuitively should reduce learning interference between tasks by separating functionality into physically distinct modules in which learning can be selectively turned on or off. Modularity can further improve learning by having a reinforcement learning module separate from sensory processing modules, allowing learning to happen only in response to a positive or negative reward. In this paper, learning takes place via neuromodulation, which allows agents to selectively change the rate of learning for each neural connection based on environmental stimuli (e.g. to alter learning in specific locations based on the task at hand). To produce modularity, we evolve neural networks with a cost for neural connections. We show that this connection cost technique causes modularity, confirming a previous result, and that such sparsely connected, modular networks have higher overall performance because they learn new skills faster while retaining old skills more and because they have a separate reinforcement learning module. Our results suggest (1) that encouraging modularity in neural networks may help us overcome the long-standing barrier of networks that cannot learn new skills without forgetting old ones, and (2) that one benefit of the modularity ubiquitous in the brains of natural animals might be to alleviate the problem of catastrophic forgetting.', 'corpus_id': 3475574, 'score': 1}, {'doc_id': '235352868', 'title': 'Extreme sparsity gives rise to functional specialization', 'abstract': 'Modularity of neural networks – both biological and artificial – can be thought of either structurally or functionally, and the relationship between these is an open question. We show that enforcing structural modularity via sparse connectivity between two dense sub-networks which need to communicate to solve the task leads to functional specialization of the sub-networks, but only at extreme levels of sparsity. With even a moderate number of interconnections, the sub-networks become functionally entangled. Defining functional specialization is in itself a challenging problem without a universally agreed solution. To address this, we designed three different measures of specialization (based on weight masks, retraining and correlation) and found them to qualitatively agree. Our results have implications in both neuroscience and machine learning. For neuroscience, it shows that we cannot conclude that there is functional modularity simply by observing moderate levels of structural modularity: knowing the brain’s connectome is not sufficient for understanding how it breaks down into functional modules. For machine learning, using structure to promote functional modularity – which may be important for robustness and generalization – may require extremely narrow bottlenecks between modules.', 'corpus_id': 235352868, 'score': 1}, {'doc_id': '235458180', 'title': 'Deep Learning Through the Lens of Example Difficulty', 'abstract': 'Existing work on understanding deep learning often employs measures that compress all data-dependent information into a few numbers. In this work, we adopt a perspective based on the role of individual examples. We introduce a measure of the computational difficulty of making a prediction for a given input: the (effective) prediction depth. Our extensive investigation reveals surprising yet simple relationships between the prediction depth of a given input and the model’s uncertainty, confidence, accuracy and speed of learning for that data point. We further categorize difficult examples into three interpretable groups, demonstrate how these groups are processed differently inside deep models and showcase how this understanding allows us to improve prediction accuracy. Insights from our study lead to a coherent view of a number of separately reported phenomena in the literature: early layers generalize while later layers memorize; early layers converge faster and networks learn easy data and simple functions first.', 'corpus_id': 235458180, 'score': 0}, {'doc_id': '24568694', 'title': 'Air embolism during insertion and replacement of tunneled dialysis catheters: a retrospective investigation of the effect of aerostatic sheaths and over-the-wire exchange.', 'abstract': 'PURPOSE\nTo determine the impact of the introduction of aerostatic sheaths on air embolism (AE) events during tunneled dialysis catheter (TDC) insertion and to characterize such events occurring during over-the-wire exchange (OTWE).\n\n\nMATERIALS AND METHODS\nBetween July 2001 and April 2013, 5,789 TDCs were placed, including 3,963 de novo placements, 1,811 OTWEs, and 15 tract recanalizations. There were 15 AE events reported, and the medical records of these patients were reviewed. The effect of aerostatic sheaths, introduced in July 2005, was compared with the period before their introduction; the same TDC design was used throughout.\n\n\nRESULTS\nOf the 15 AE events, 10 occurred during de novo placement (10 of 3,963 placement; 0.25%), 4 occurred during OTWE (4 of 1,811 placements; 0.22%), and 1 occurred during tract recanalization. With regard to aerostatic sheaths in de novo TDC placement, 4 of 1,174 (0.34%) AE events occurred before aerostatic sheath introduction, and 6 of 2,789 (0.22%) AE events occurred after aerostatic sheath introduction. These rates did not differ statistically (P = .5).\n\n\nCONCLUSIONS\nUse of aerostatic sheaths trended toward reducing AE events during de novo TDC placement. This trend was not statistically significant, probably owing to the rarity of AE despite the large sample size. Air embolism occurs during OTWE at a rate similar to de novo placement with aerostatic sheaths as well as during tract recanalization.', 'corpus_id': 24568694, 'score': 0}, {'doc_id': '10570975', 'title': 'The evolutionary origins of modularity', 'abstract': 'A central biological question is how natural organisms are so evolvable (capable of quickly adapting to new environments). A key driver of evolvability is the widespread modularity of biological networks—their organization as functional, sparsely connected subunits—but there is no consensus regarding why modularity itself evolved. Although most hypotheses assume indirect selection for evolvability, here we demonstrate that the ubiquitous, direct selection pressure to reduce the cost of connections between network nodes causes the emergence of modular networks. Computational evolution experiments with selection pressures to maximize network performance and minimize connection costs yield networks that are significantly more modular and more evolvable than control experiments that only select for performance. These results will catalyse research in numerous disciplines, such as neuroscience and genetics, and enhance our ability to harness evolution for engineering purposes.', 'corpus_id': 10570975, 'score': 1}]"
6	IML	f8b1eeb2efa77296eae9fd804e833bd6	13813	{'IML': 'intermediolateral cell column'}	"[{'doc_id': '231786680', 'title': 'Fast Concept Mapping: The Emergence of Human Abilities in Artificial Neural Networks when Learning Embodied and Self-Supervised', 'abstract': 'Most artificial neural networks used for object detection and recognition are trained in a fully supervised setup. This is not only very resource consuming as it requires large data sets of labeled examples but also very different from how humans learn. We introduce a setup in which an artificial agent first learns in a simulated world through self-supervised exploration. Following this, the representations learned through interaction with the world can be used to associate semantic concepts such as different types of doors. To do this, we use a method we call fast concept mapping which uses correlated firing patterns of neurons to define and detect semantic concepts. This association works instantaneous with very few labeled examples, similar to what we observe in humans in a phenomenon called fast mapping. Strikingly, this method already identifies objects with as little as one labeled example which highlights the quality of the encoding learned self-supervised through embodiment using curiosity-driven exploration. It therefor presents a feasible strategy for learning concepts without much supervision and shows that through pure interaction with the world meaningful representations of an environment can be learned.', 'corpus_id': 231786680, 'score': 1}, {'doc_id': '227053964', 'title': 'Experimenting Touchless Gestural Interaction for a University Public Web-based Display', 'abstract': 'Interest in and development of touchless gestural interfaces has recently exploded, fueled by the diffusion of both commercial midair gesture platforms and public interactive displays. This paper focuses on an application based on Microsoft Kinect that allows students to browse a university website, hosted on a public display, through simple gestures. We present two empirical evaluations where we evaluated how users react to this new way of interaction. In addition to confirming the current lack of standards, our results provide some inspiration for the design of touchless interaction.', 'corpus_id': 227053964, 'score': 1}, {'doc_id': '227248042', 'title': 'Interactive Teaching for Conversational AI', 'abstract': 'Current conversational AI systems aim to understand a set of pre-designed requests and execute related actions, which limits them to evolve naturally and adapt based on human interactions. Motivated by how children learn their first language interacting with adults, this paper describes a new Teachable AI system that is capable of learning new language nuggets called concepts, directly from end users using live interactive teaching sessions. The proposed setup uses three models to: a) Identify gaps in understanding automatically during live conversational interactions, b) Learn the respective interpretations of such unknown concepts from live interactions with users, and c) Manage a classroom sub-dialogue specifically tailored for interactive teaching sessions. We propose state-of-the-art transformer based neural architectures of models, fine-tuned on top of pre-trained models, and show accuracy improvements on the respective components. We demonstrate that this method is very promising in leading way to build more adaptive and personalized language understanding models.', 'corpus_id': 227248042, 'score': 1}, {'doc_id': '11430990', 'title': 'Interactive machine learning', 'abstract': 'Perceptual user interfaces (PUIs) are an important part of ubiquitous computing. Creating such interfaces is difficult because of the image and signal processing knowledge required for creating classifiers. We propose an interactive machine-learning (IML) model that allows users to train, classify/view and correct the classifications. The concept and implementation details of IML are discussed and contrasted with classical machine learning models. Evaluations of two algorithms are also presented. We also briefly describe Image Processing with Crayons (Crayons), which is a tool for creating new camera-based interfaces using a simple painting metaphor. The Crayons tool embodies our notions of interactive machine learning', 'corpus_id': 11430990, 'score': 1}, {'doc_id': '202640576', 'title': 'Supporting Musical Practice Sessions Through HMD-Based Augmented Reality', 'abstract': 'Learning a musical instrument requires a lot of practice, which ideally, should be done every day. During practice sessions, students are on their own in the overwhelming majority of the time, but access to experts that support students ""just-in-time"" is limited. Therefore, students commonly do not receive any feedback during their practice sessions. Adequate feedback, especially for beginners, is highly important for three particular reasons: (1) preventing the acquirement of wrong motions, (2) avoiding frustration due to a steep learning curve, and (3) potential health problems that arise from straining muscles or joints harmfully. In this paper, we envision the usage of head-mounted displays as assistance modality to support musical instrument learning. We propose a modular concept for several assistance modes to help students during their practice sessions. Finally, we discuss hardware requirements and implementations to realize the proposed concepts.', 'corpus_id': 202640576, 'score': 0}, {'doc_id': '231802270', 'title': 'Improving Reinforcement Learning with Human Assistance: An Argument for Human Subject Studies with HIPPO Gym', 'abstract': 'Reinforcement learning (RL) is a popular machine learning paradigm for game playing, robotics control, and other sequential decision tasks. However, RL agents often have long learning times with high data requirements because they begin by acting randomly. In order to better learn in complex tasks, this article argues that an external teacher can often significantly help the RL agent learn. OpenAI Gym is a common framework for RL research, including a large number of standard environments and agents, making RL research significantly more accessible. This article introduces our new open-source RL framework, the Human Input Parsing Platform for Openai Gym (HIPPO Gym), and the design decisions that went into its creation. The goal of this platform is to facilitate human-RL research, again lowering the bar so that more researchers can quickly investigate different ways that human teachers could assist RL agents, including learning from demonstrations, learning from feedback, or curriculum learning.', 'corpus_id': 231802270, 'score': 1}, {'doc_id': '229923021', 'title': 'Visual Probing and Correction of Object Recognition Models with Interactive user feedback', 'abstract': 'With the advent of state-of-the-art machine learning and deep learning technologies, several industries are moving towards the field. Applications of such technologies are highly diverse ranging from natural language processing to computer vision. Object recognition is one such area in the computer vision domain. Although proven to perform with high accuracy, there are still areas where such models can be improved. This is in-fact highly important in real-world use cases like autonomous driving or cancer detection, that are highly sensitive and expect such technologies to have almost no uncertainties. In this paper, we attempt to visualise the uncertainties in object recognition models and propose a correction process via user feedback. We further demonstrate our approach on the data provided by the VAST 2020 Mini-Challenge 2.', 'corpus_id': 229923021, 'score': 0}, {'doc_id': '190481457', 'title': 'Mastering Music Instruments through Technology in Solo Learning Sessions', 'abstract': 'Mastering a musical instrument requires time-consuming practice even if students are guided by an expert. In the overwhelming majority of the time, the students practice by themselves and traditional teaching materials, such as videos or textbooks, lack interaction and guidance possibilities. Adequate feedback, however, is highly important to prevent the acquirement of wrong motions and to avoid potential health problems. In this paper, we envision musical instruments as smart objects to enhance solo learning sessions. We give an overview of existing approaches and setups and discuss them. Finally, we conclude with recommendations for designing smart and augmented musical instruments for learning purposes.', 'corpus_id': 190481457, 'score': 0}, {'doc_id': '227209826', 'title': 'Towards Movement Generation with Audio Features', 'abstract': 'Sound and movement are closely coupled, particularly in dance. Certain audio features have been found to affect the way we move to music. Is this relationship between sound and movement something which can be modelled using machine learning? This work presents initial experiments wherein high-level audio features calculated from a set of music pieces are included in a movement generation model trained on motion capture recordings of improvised dance. Our results indicate that the model learns to generate realistic dance movements which vary depending on the audio features.', 'corpus_id': 227209826, 'score': 0}, {'doc_id': '227209653', 'title': 'Interactive Machine Learning of Musical Gesture', 'abstract': 'This chapter presents an overview of Interactive Machine Learning (IML) techniques applied to the analysis and design of musical gestures. We go through the main challenges and needs related to capturing, analysing, and applying IML techniques to human bodily gestures with the purpose of performing with sound synthesis systems. We discuss how different algorithms may be used to accomplish different tasks, including interacting with complex synthesis techniques and exploring interaction possibilities by means of Reinforcement Learning (RL) in an interaction paradigm we developed called Assisted Interactive Machine Learning (AIML). We conclude the chapter with a description of how some of these techniques were employed by the authors for the development of four musical pieces, thus outlining the implications that IML have for musical practice.', 'corpus_id': 227209653, 'score': 0}]"
7	Alpine Fault	279e8e218c10a93642e48bcb3e39a9fc	12540	{}	"[{'doc_id': '227254542', 'title': 'Numerical computation of stress-permeability relationships of fracture networks in a shale rock', 'abstract': 'We present stress-sensitive permeability relationships for two-dimensional fracture networks in the Opalinus Clay from the Mont Terri underground rock laboratory. These relationships may be used as a proxy for fracture network permeability in numerical models that resolve large spatial scales and are used in a variety of GeoEnergy applications involving flow in shaly rocks. To obtain these relationships we present a numerical procedure that uses experimentally determined stress-permeability relationships to numerically compute the effective permeability of the network. The material discontinuities stemming from the fractures are treated by a simple contact-interaction algorithm that accounts for normal interaction between fracture walls, allowing us to calculate the permeability of a fracture network under different stress conditions. We apply the procedure to four fracture networks digitized from two galleries of the Mont Terri rock laboratory. These fracture networks are mapped from the damage zone of the Main Fault that intersects the Opalinus Clay. The networks show a maximum variation of four orders of magnitude when stress ranges from 1MPa to 20 MPa. Our numerical procedure not only establishes representative stress-permeability relationships for a fractured rock mass under stress, but also provides a proxy for fracture network permeability for simulation in fractured formations.', 'corpus_id': 227254542, 'score': 0}, {'doc_id': '222282997', 'title': 'Selection of Ground Motion Models for Probabilistic Seismic Hazard Analysis in Iran', 'abstract': 'Back-projection has proven useful to image large earthquake rupture processes. It utilizes array techniques to estimate the spatial and temporal evolution of earthquake rupture over time, and can help us identify interesting earthquake phenomena like supershear rupture. However, the method does not directly solve an inverse problem and has difficulty in quantifying epistemic uncertainties, which can be caused by seismic array configurations, structural heterogeneities in the Earth’s crust, unknown seismic phases, and variations in the focal mechanism. These uncertainties may cause erroneous interpretations of earthquake physics, which is particularly challenging to distinguish for complex earthquake rupture processes.', 'corpus_id': 222282997, 'score': 0}, {'doc_id': '134211005', 'title': 'Past large earthquakes on the Alpine Fault: paleoseismological progress and future directions', 'abstract': 'ABSTRACT Paleoseismology has been making an important contribution to understanding the Alpine Fault and the hazard it poses to society. However, evidence of past earthquakes comes from a wide variety of sources and publication of the evidence has been somewhat fragmented. Here, we review physical evidence for past large to great earthquakes on the Alpine Fault to summarise current understanding, illustrate progress and highlight future directions. Paleoseismic evidence has been derived from tree disturbance, landscape features and trenches across the fault. These records have been supplemented and extended back in time with sedimentary evidence of Alpine Fault earthquakes from fault-proximal lakes and wetlands. In this review, we update radiocarbon analyses using recent calibration curves and modern Bayesian statistical methods where necessary to enable comparison between on-fault, fault-proximal and off-fault earthquake records. Over recent decades, Alpine Fault paleoseismology has progressed from playing an important role in demonstrating that large surface-rupturing earthquakes occur, to enabling estimates of earthquake recurrence behaviour, shaking intensities, rupture extents, landscape response durations and likelihood of the next earthquake.', 'corpus_id': 134211005, 'score': 1}, {'doc_id': '134068443', 'title': 'Frictional properties and 3-D stress analysis of the southern Alpine Fault, New Zealand', 'abstract': ""Abstract New Zealand's Alpine Fault (AF) ruptures quasi-periodically in large-magnitude earthquakes. Paleoseismological evidence suggests that about half of all recognized AF earthquakes terminated at the boundary between the Central and South Westland sections of the fault. There, fault geometry and the polarity of uplift change. The South Westland AF exhibits oblique-normal fault motion on a structure oriented 052°/82°SE that, for at least 35\u202fkm along strike, contains saponite-rich principal slip zone gouges. New hydrothermal friction experiments reveal that the saponite fault gouge is frictionally weak, exhibiting friction coefficients between μ\u202f=\u202f0.12 and μ\u202f=\u202f0.16 for a range of temperatures (T\u202f=\u202f25–210\u202f°C) and effective normal stresses (σn'\u202f=\u202f31.2–93.6\u202fMPa). The saponite gouge is rate-strengthening in all velocity steps performed at velocities between 0.01 and 3.0\u202fμm/s, behavior conducive to aseismic creep. A three-dimensional stress analysis shows that the South Westland AF is favorably oriented with respect to the regional stress field for slip within the frictionally weak saponite fault gouge. Geometrically, the fault is severely misoriented for slip in any fault-forming materials with friction coefficients exceeding μ∼0.5. The combination of weak gouges prone to aseismic creep, strong asperities, and low resolved shear stress may impede earthquake rupture propagation along the South Westland Alpine Fault."", 'corpus_id': 134068443, 'score': 1}, {'doc_id': '132516649', 'title': 'A plate boundary earthquake record from a wetland adjacent to the Alpine fault in New Zealand refines hazard estimates', 'abstract': ""Abstract Discovery and investigation of millennial-scale geological records of past large earthquakes improve understanding of earthquake frequency, recurrence behaviour, and likelihood of future rupture of major active faults. Here we present a ∼2000 year-long, seven-event earthquake record from John O'Groats wetland adjacent to the Alpine fault in New Zealand, one of the most active strike-slip faults in the world. We linked this record with the 7000 year-long, 22-event earthquake record from Hokuri Creek (20 km along strike to the north) to refine estimates of earthquake frequency and recurrence behaviour for the South Westland section of the plate boundary fault. Eight cores from John O'Groats wetland revealed a sequence that alternated between organic-dominated and clastic-dominated sediment packages. Transitions from a thick organic unit to a thick clastic unit that were sharp, involved a significant change in depositional environment, and were basin-wide, were interpreted as evidence of past surface-rupturing earthquakes. Radiocarbon dates of short-lived organic fractions either side of these transitions were modelled to provide estimates for earthquake ages. Of the seven events recognised at the John O'Groats site, three post-date the most recent event at Hokuri Creek, two match events at Hokuri Creek, and two events at John O'Groats occurred in a long interval during which the Hokuri Creek site may not have been recording earthquakes clearly. The preferred John O'Groats–Hokuri Creek earthquake record consists of 27 events since ∼6000 BC for which we calculate a mean recurrence interval of 291 ± 23 years , shorter than previously estimated for the South Westland section of the fault and shorter than the current interseismic period. The revised 50-year conditional probability of a surface-rupturing earthquake on this fault section is 29%. The coefficient of variation is estimated at 0.41. We suggest the low recurrence variability is likely to be a feature of other strike-slip plate boundary faults similar to the Alpine fault."", 'corpus_id': 132516649, 'score': 1}, {'doc_id': '226965171', 'title': 'On the correlation of earthquake occurrence among major fault zones in the eastern margin of the Tibetan Plateau by Big Data Analysis', 'abstract': 'The subsequent series of responses to big events may exhibit a synchronicity of event number, frequency and energy release in different fault zones. This synchronicity is a reliable source for probing non-intuitive geological structures, assessing regional seismicity hazard map and even predicting the next big events. The synchronicity of main faults in the eastern margin of the Qinghai-Tibetan Plateau is still unknown to us. We propose to examine the correlation of earthquake occurrence among different fault zones to indicate this synchronicity, and to obtain a preliminary understanding of geodynamics processes and the unrecognized characteristics of deep evolution in the eastern margin of the Qinghai-Tibetan Plateau. We estimate temporal changes of completeness level, frequency seismicity, and intensity seismicity, referring respectively to Mc, Z, and E values, of 21 main fault zones, using a seismic catalogue from 1970 to 2015. Our results reveal that six fault zone pairs of fault zones exhibit relative high correlation (>0.6) by all three indicators, while four fault zone pairs are non-adjacent with close internal affinity offsetting the limit of spatial distance, such as the pair of Rongjing-mabian fault and Minjiang-huya fault. Most strikingly, some fault zone pairs showing typical high correlation (>0.8) of seismicity frequency or seismicity intensity, the faults surprisingly belong to neither the same seismic belt nor the same geological block, exhibiting a regional scale remote triggering pattern of earthquakes or structures. An embryonic pattern to predict the next possible events will also be presented. This correlation analysis discovers a previously unrecognized strong coupling relationship among main faults with high earthquake risk in the eastern margin of the Qinghai-Tibetan Plateau.', 'corpus_id': 226965171, 'score': 0}, {'doc_id': '134456106', 'title': 'Focal mechanisms and inter-event times of low-frequency earthquakes reveal quasi-continuous deformation and triggered slow slip on the deep Alpine Fault', 'abstract': 'Abstract Characterising the seismicity associated with slow deformation in the vicinity of the Alpine Fault may provide constraints on the stresses acting on a major transpressive margin prior to an anticipated great (≥M8) earthquake. Here, we use recently detected tremor and low-frequency earthquakes (LFEs) to examine how slow tectonic deformation is loading the Alpine Fault late in its typical ∼300-yr seismic cycle. We analyse a continuous seismic dataset recorded between 2009 and 2016 using a network of 10–13 short-period seismometers, the Southern Alps Microearthquake Borehole Array. Fourteen primary LFE templates are used in an iterative matched-filter and stacking routine, allowing the detection of similar signals corresponding to LFE families sharing common locations. This yields an 8-yr catalogue containing 10,000 LFEs that are combined for each of the 14 LFE families using phase-weighted stacking to produce signals with the highest possible signal-to-noise ratios. We show that LFEs occur almost continuously during the 8-yr study period and highlight two types of LFE distributions: (1) discrete behaviour with an inter-event time exceeding 2 min; (2) burst-like behaviour with an inter-event time below 2 min. We interpret the discrete events as small-scale frequent deformation on the deep extent of the Alpine Fault and LFE bursts (corresponding in most cases to known episodes of tremor or large regional earthquakes) as brief periods of increased slip activity indicative of slow slip. We compute improved non-linear earthquake locations using a 3-D velocity model. LFEs occur below the seismogenic zone at depths of 17–42 km, on or near the hypothesised deep extent of the Alpine Fault. The first estimates of LFE focal mechanisms associated with continental faulting, in conjunction with recurrence intervals, are consistent with quasi-continuous shear faulting on the deep extent of the Alpine Fault.', 'corpus_id': 134456106, 'score': 1}, {'doc_id': '226236714', 'title': 'Earthquake and Electrochemistry: Unraveling the Unpredictable', 'abstract': ""Earthquakes are measured using well defined seismic parameters such as seismic moment (Mo), moment magnitude (Mw), and released elastic energy(E). How this tremendous amount of energy is accumulated silently deep inside the earth's crust? The most obvious question in seismic research remains unanswered. We found an inherent and intriguing connection between the released energy in an earthquake and electrochemical potential induced in an ultra-thin metal oxide electrode immersed in an aqueous pH solution, which leads us to understand the origin of the energy accumulation process in an earthquake. A huge electrochemical potential is accumulated from numerous electrochemical cells formed in a unique layer structure of hydrated clay minerals (predominantly smectite), which resulted in a lightning-like discharge in the lithosphere (hypocenter). The subsequent thunder-like massive shockwave is produced, which initiates tectonic plate movement along a fault line, probably through acoustic fluidization (AF), and resulting seismic energy is transmitted as primary wave (P-wave), secondary wave (S-wave), and surface waves. The presence of electrical voltage in the hypocenter directly supports the seismic electric signal (SES), further strengthening the VAN method of earthquake prediction. Our finding is supported by a plethora of research and observation devoted to seismic science. This study will indeed find its significance if immediate action is implemented to monitor the evolution of electrochemical potential, seismic electrical signal (SES), and ionic activity in the fault zone at lithosphere as well as in the ionosphere for predicting an impending earthquake for saving human lives as early as possible."", 'corpus_id': 226236714, 'score': 0}, {'doc_id': '224821702', 'title': 'Crustal deformation rates in Kashmir valley and adjoining regions from continuous GPS measurements from 2008 to 2019', 'abstract': 'We present GPS velocities in Kashmir valley and adjoining regions from continuous Global Positioning System (cGPS) network during 2008 to 2019. Results indicate total arc normal shortening rates of\u2009~\u200914 mm/year across this transect of Himalaya that is comparable to the rates of\u2009~\u200910 to 20 mm/year reported else-where in the 2500 km Himalaya Arc. For the first time in Himalayas, arc-parallel extension rate of\u2009~\u20097 mm/year was recorded in the Kashmir valley, pointing to oblique deformation. Inverse modeling of the contemporary deformation rates in Kashmir valley indicate oblique slip of\u2009~\u200916 mm/year along the decollement with locking depth of\u2009~\u200915 km and width of\u2009~\u2009145 km. This result is consistent with the recorded micro-seismicity and low velocity layer at a depth of 12 to 16 km beneath the Kashmir valley obtained from collocated broadband seismic network. Geodetic strain rates are consistent with the dislocation model and micro-seismic activity, with high strain accumulation (~\u20097e−08 maximum compression) to the north of Kashmir valley and south of Zanskar ranges. Assuming the stored energy was fully released during 1555 earthquake, high geodetic strain rate since then and observed micro-seismicity point to probable future large earthquakes of Mw\u2009~\u20097.7 in Kashmir seismic gap.', 'corpus_id': 224821702, 'score': 0}, {'doc_id': '134795563', 'title': 'Evidence for a pre-Eocene proto-Alpine Fault through Zealandia', 'abstract': 'ABSTRACT For many decades, there has been speculation about when the Alpine Fault first became a major regional structure. This paper presents a summary of recently published data, especially U–Pb ages, that supports the existence of a Late Cretaceous to Paleogene proto-Alpine Fault in the form of a thermotectonic corridor between North and South Zealandia. The evidence takes the form of fault-controlled sedimentary basins, ductile shear zones, Alpine Schist regional metamorphism and igneous rocks. Much critical evidence has been removed or strongly modified by Neogene shortening and the total lateral displacement history of the proto-Alpine Fault cannot yet be unambiguously determined.', 'corpus_id': 134795563, 'score': 1}]"
8	Superconductivity	89f017883905211924d86ec36dacec3b	2840	{}	[{'doc_id': '214713905', 'title': 'Superconductivity in the nonsymmorphic line-nodal compound \nCaSb2', 'abstract': 'We found superconductivity in CaSb$_2$ with the transition temperature of 1.7 K by means of electrical-resistivity, magnetic-susceptibility, and specific-heat measurements. This material crystallizes in a nonsymmorphic structure and is predicted to have multiple Dirac nodal lines in the bulk electronic band structure protected by symmetry even in the presence of spin-orbit coupling. We discuss a possible topological superconductivity for the quasi-2-dimensional band originating mainly from one of the antimony sites.', 'corpus_id': 214713905, 'score': 0}, {'doc_id': '115389393', 'title': 'Stability of microwave-irradiated nonequilibrium superconductors', 'abstract': 'The stability of microwave-irradiated nonequilibrium superconductors is investigated. It is shown that even at rather low radiation intensities the superconductor becomes unstable to fluctuations of the order-parameter modulus. The connection between this instabilitity and the enhancement of the superconductivity by microwave radiation is discussed.', 'corpus_id': 115389393, 'score': 1}, {'doc_id': '214623397', 'title': 'Possible signatures of mixed-parity superconductivity in doped polar \nSrTiO3\n films', 'abstract': 'Superconductors that possess both broken spatial inversion symmetry and spin-orbit interactions exhibit a mix of spin singlet and triplet pairing. Here, we report on measurements of the superconducting properties of electron-doped, strained $\\mathrm{SrTi}{\\mathrm{O}}_{3}$ films. These films have an enhanced superconducting transition temperature and were previously shown to undergo a transition to a polar phase prior to becoming superconducting. We show that some films show signatures of an unusual superconducting state, such as an in-plane critical field that is higher than both the paramagnetic and orbital pair breaking limits. Moreover, nonreciprocal transport, which reflects the ratio of odd versus even pairing interactions, is observed. Together, these characteristics indicate that these films provide a tunable platform for investigations of unconventional superconductivity.', 'corpus_id': 214623397, 'score': 0}, {'doc_id': '215416007', 'title': 'Unconventional superconducting properties of noncentrosymmetric \nRe5.5Ta', 'abstract': 'Rhenium based noncentrosymmetric superconductors crystallizing in $\\alpha$-Mn structure have become potential candidates to exhibit an unconventional superconducting ground-state. Here we report a detailed investigation on the superconducting and normal state properties of Re$_{5.5}$Ta, that also has the $\\alpha$-Mn structure. Magnetization, specific heat, and transport measurements confirm the bulk superconducting transition \\textit{T}$_{C}$ at 8.0 K. Upper critical field value (H$_{C2}$(0)) calculated from magnetization, specific heat and AC transport measurements exceed the Pauli paramagnetic limit (14.7 T), indicating that the superconducting properties of Re$_{5.5}$Ta are probably unconventional in nature. However, low-temperature specific heat and transverse-field muon spin rotation measurements suggest a surprising nodeless isotropic superconducting gap, although with strong electron-phonon coupling.', 'corpus_id': 215416007, 'score': 0}, {'doc_id': '201288719', 'title': 'Superconductivity and Microwaves', 'abstract': 'Superconductivity conforms to a quantum, thermal, and electrodynamic set of physical phenomena of great interest by themselves. They have, also, the potential to be one clean energy source that technology is looking for. Superconductors do not allow static magnetic fields to penetrate them below a critical field, that is, Meissner effect. However, microwave magnetic fields do penetrate them already, and their energy is readily absorbed by the superconductor. High-temperature, perovskite superconductors do absorb microwave energy the most due to the presence of unpaired electron spins, fluxoid dynamics, and quasiparticle motion. We describe the fundamental physics of the interaction of the superconductors with microwaves. Experimental techniques to measure microwave absorption are presented. Experimental setups for absorption of energy are described in terms of the central quantity, Q. The measurements are analyzed in terms of irreversible energy exchange processes. The knowledge gained can inform the design of superconducting devices operating in microwave environments.', 'corpus_id': 201288719, 'score': 1}, {'doc_id': '117990522', 'title': 'INFLUENCE OF NONEQUILIBRIUM EXCITATIONS ON THE PROPERTIES OF SUPERCONDUCTING FILMS IN A HIGH-FREQUENCY FIELD.', 'abstract': None, 'corpus_id': 117990522, 'score': 1}, {'doc_id': '211258704', 'title': 'Observation of long-range quantum interferences of d-wave Andreev pairs in graphene', 'abstract': 'Recent experiments have shown that superconducting correlations can be induced in graphene by proximity with high-temperature (cuprate) superconductors. Here we demonstrate that such correlations can propagate hundreds of nm into large-scale grown graphene, thereby allowing for the unusual observation of long-range interferences between d-wave Andreev pairs. This phenomenon is shown in planar YBa2Cu3O7/graphene devices that behave as Fabry-Perot interferometers, and it manifests in a series of pronounced conductance oscillations analogous to the De Gennes-Saint James resonances originally predicted for ultrathin metals backed by superconductors. The interpretation of the experimental results is supported by numerical simulations based on an extended Blonder-Tinkham-Klapwijk model that qualitatively and quantitatively reproduces the observed behavior', 'corpus_id': 211258704, 'score': 0}, {'doc_id': '211677540', 'title': 'Topological transitions in superconductor nanomembranes in a magnetic field with submicron inhomogeneity under a strong transport current', 'abstract': 'Under a strong transport current, the induced voltage in superconductor nanomembranes in a magnetic field with submicron inhomogeneity shows a pulse on a certain interval of the magnetic field. It is a manifestation of a wide phase-slip domain. The topological transition accompanying the phase-slip effect consists in the occurrence of two new loops of weak superconducting currents connecting two regions of superconducting screening currents, which are disconnected in case of vortex-chain dynamics. In the middle of the phase-slip domain, rapid dynamics of the superconducting order parameter consists in decoupling of a spontaneously nucleated vortex-antivortex pair, subsequent motion of a vortex and an antivortex in the opposite directions followed by their annihilation with an antivortex and a vortex from the adjacent pairs. The submicron-scale inhomogeneity of the magnetic field can be achieved through a direct patterning of the magnetic field applied to a planar membrane or using advanced nanostructuring, such as roll-up technology, focused ion-beam deposition or coating carbon nanotubes by superconducting materials. If the applied magnetic field is orthogonal to the axis of a microtube, which carries transport current in the azimuthal direction, the phase-slip regime is characterized by the vortex-antivortex lifetime of 10E-15 s versus 10E-12 s for disconnected vortex dynamics in the half-tubes. The phase-slip dynamics determines the voltage-magnetic field and voltage-current characteristics in nanoarchitectures with multiple disconnected loops of superconducting screening currents.', 'corpus_id': 211677540, 'score': 0}, {'doc_id': '148571834', 'title': 'Cavity Higgs polaritons', 'abstract': 'Motivated by the dramatic success of realizing cavity exciton-polariton condensation in experiment we consider the formation of polaritons from cavity photons and the amplitude or Higgs mode of a superconductor. \nEnabled by the recently predicted and observed supercurrent-induced linear coupling between these excitations and light, we find that hybridization between Higgs excitations in a disordered quasi-2D superconductor and resonant cavity photons can occur, forming Higgs-polariton states. \nThis provides the potential for a new means to manipulate the superconducting state as well as potential for novel photonic cavity circuit elements.', 'corpus_id': 148571834, 'score': 1}, {'doc_id': '214727866', 'title': 'Superfluid density in disordered superconductors', 'abstract': 'Recent experiments on disordered superconductors find that the superfluid density $n_s(T)$ (or stiffness) decreases dramatically and characteristically with disorder. We describe here an approach which includes the effect of electron pair phase in a gauge invariant manner, and use it to explicitly obtain $n_s(T)$ in the presence of (static) disorder. We compare our results successfully with experiment.', 'corpus_id': 214727866, 'score': 1}]
9	Software developers productivity	06dcb9479e348a151f80a9b0c3d938da	4041	{}	"[{'doc_id': '214728187', 'title': '20-MAD: 20 Years of Issues and Commits of Mozilla and Apache Development', 'abstract': 'Data of long-lived and high profile projects is valuable for research on successful software engineering in the wild. Having a dataset with different linked software repositories of such projects, enables deeper diving investigations. This paper presents 20-MAD, a dataset linking the commit and issue data of Mozilla and Apache projects. It includes over 20 years of information about 765 projects, 3.4M commits, 2.3M issues, and 17.3M issue comments, and its compressed size is over 6 GB. The data contains all the typical information about source code commits (e.g., lines added and removed, message and commit time) and issues (status, severity, votes, and summary). The issue comments have been pre-processed for natural language processing and sentiment analysis. This includes emoticons and valence and arousal scores. Linking code repository and issue tracker information, allows studying individuals in two types of repositories and provide more accurate time zone information for issue trackers as well. To our knowledge, this the largest linked dataset in size and in project lifetime that is not based on GitHub.', 'corpus_id': 214728187, 'score': 1}, {'doc_id': '211126879', 'title': 'An Exploratory Study of Code Smells in Web Games', 'abstract': 'With the continuous growth of the internet market, games are becoming more and more popular worldwide. However, increased market competition for game demands developers to write more efficient games in terms of performance, security, and maintenance. The continuous evolution of software systems and its increasing complexity may result in bad design decisions. Researchers analyzed the cognitive, behavioral and social effects of games. Also, gameplay and game mechanics have been a research area to enhance game playing, but to the extent of our knowledge, there hardly exists any research work that studies the bad coding practices in game development. Hence, through our study, we try to analyze and identify the presence of bad coding practices called code smells that may cause quality issues in games. To accomplish this, we created a dataset of 361 web games written in JavaScript. On this dataset, we run a JavaScript code smell detection tool JSNose to find the occurrence and distribution of code smell in web games. Further, we did a manual study on 9 web games to find violation of existing game programming patterns. Our results show that existing tools are mostly language-specific and are not enough in the context of games as they were not able to detect the anti-patterns or bad coding practices that are game-specific, motivating the need of game-specific code smell detection tools.', 'corpus_id': 211126879, 'score': 0}, {'doc_id': '118629392', 'title': 'Uniqueness of conformal densities and the semiflow of Weyl chambers', 'abstract': 'Abstract Let X = G/K be a symmetric space of noncompact type and Γ a discrete “generic” subgroup of G with critical exponent δ(Γ). We show that, if Γ is of divergence type, then there is a unique Γ-invariant conformal density of dimension δ (Γ) (hence a Patterson-Sullivan density) on the set of regular elements of the geometric boundary of X. This problem is directly related to the recurrence of the semiflow of Weyl chambers on X/gG.', 'corpus_id': 118629392, 'score': 0}, {'doc_id': '9916105', 'title': 'Understanding the Motivations, Participation, and Performance of Open Source Software Developers: A Longitudinal Study of the Apache Projects', 'abstract': ""Understanding what motivates participation is a central theme in the research on open source software (OSS) development. Our study contributes by revealing how the different motivations of OSS developers are interrelated, how these motivations influence participation leading to performance, and how past performance influences subsequent motivations. Drawing on theories of intrinsic and extrinsic motivation, we develop a theoretical model relating the motivations, participation, and performance of OSS developers. We evaluate our model using survey and archival data collected from a longitudinal field study of software developers in the Apache projects. Our results reveal several important findings. First, we find that developers' motivations are not independent but rather are related in complex ways. Being paid to contribute to Apache projects is positively related to developers' status motivations but negatively related to their use-value motivations. Perhaps surprisingly, we find no evidence of diminished intrinsic motivation in the presence of extrinsic motivations; rather, status motivations enhance intrinsic motivations. Second, we find that different motivations have an impact on participation in different ways. Developers' paid participation and status motivations lead to above-average contribution levels, but use-value motivations lead to below-average contribution levels, and intrinsic motivations do not significantly impact average contribution levels. Third, we find that developers' contribution levels positively impact their performance rankings. Finally, our results suggest that past-performance rankings enhance developers' subsequent status motivations."", 'corpus_id': 9916105, 'score': 1}, {'doc_id': '214743535', 'title': 'Can We Use SE-specific Sentiment Analysis Tools in a Cross-Platform Setting?', 'abstract': ""In this paper, we address the problem of using sentiment analysis tools 'off-the-shelf', that is when a gold standard is not available for retraining. We evaluate the performance of four SE-specific tools in a cross-platform setting, i.e., on a test set collected from data sources different from the one used for training. We find that (i) the lexicon-based tools outperform the supervised approaches retrained in a cross-platform setting and (ii) retraining can be beneficial in within-platform settings in the presence of robust gold standard datasets, even using a minimal training set. Based on our empirical findings, we derive guidelines for reliable use of sentiment analysis tools in software engineering."", 'corpus_id': 214743535, 'score': 0}, {'doc_id': '212646398', 'title': 'Is this GitHub Project Maintained? Measuring the Level of Maintenance Activity of Open-Source Projects', 'abstract': 'Abstract Context GitHub hosts an impressive number of high-quality OSS projects. However, selecting “the right tool for the job” is a challenging task, because we do not have precise information about those high-quality projects. Objective In this paper, we propose a data-driven approach to measure the level of maintenance activity of GitHub projects. Our goal is to alert users about the risks of using unmaintained projects and possibly motivate other developers to assume the maintenance of such projects. Method We train machine learning models to define a metric to express the level of maintenance activity of GitHub projects. Next, we analyze the historical evolution of 2927 active projects in the time frame of one year. Results From 2927 active projects, 16% become unmaintained in the interval of one year. We also found that Objective-C projects tend to have lower maintenance activity than projects implemented in other languages. Finally, software tools—such as compilers and editors—have the highest maintenance activity over time. Conclusions A metric about the level of maintenance activity of GitHub projects can help developers to select open source projects.', 'corpus_id': 212646398, 'score': 0}, {'doc_id': '86675594', 'title': 'What Predicts Software Developers’ Productivity?', 'abstract': 'Organizations have a variety of options to help their software developers become their most productive selves, from modifying office layouts, to investing in better tools, to cleaning up the source code. But which options will have the biggest impact? Drawing from the literature in software engineering and industrial/organizational psychology to identify factors that correlate with productivity, we designed a survey that asked 622 developers across 3 companies about these productivity factors and about self-rated productivity. Our results suggest that the factors that most strongly correlate with self-rated productivity were non-technical factors, such as job enthusiasm, peer support for new ideas, and receiving useful feedback about job performance. Compared to other knowledge workers, our results also suggest that software developers’ self-rated productivity is more strongly related to task variety and ability to work remotely.', 'corpus_id': 86675594, 'score': 1}, {'doc_id': '214605595', 'title': 'Beyond the Code: Mining Self-Admitted Technical Debt in Issue Tracker Systems', 'abstract': 'Self-admitted technical debt (SATD) is a particular case of Technical Debt (TD) where developers explicitly acknowledge their sub-optimal implementation decisions. Previous studies mine SATD by searching for specific TD-related terms in source code comments. By contrast, in this paper we argue that developers can admit technical debt by other means, e.g., by creating issues in tracking systems and labelling them as referring to TD. We refer to this type of SATD as issue-based SATD or just SATD-I. We study a sample of 286 SATD-I instances collected from five open source projects, including Microsoft Visual Studio and GitLab Community Edition. We show that only 29% of the studied SATD-I instances can be tracked to source code comments. We also show that SATD-I issues take more time to be closed, compared to other issues, although they are not more complex in terms of code churn. Besides, in 45% of the studied issues TD was introduced to ship earlier, and in almost 60% it refers to DESIGN flaws. Finally, we report that most developers pay SATD-I to reduce its costs or interests (66%). Our findings suggest that there is space for designing novel tools to support technical debt management, particularly tools that encourage developers to create and label issues containing TD concerns.', 'corpus_id': 214605595, 'score': 1}, {'doc_id': '212675695', 'title': ""A survey on test practitioners' awareness of test smells"", 'abstract': ""Developing test code may be a time-consuming task that usually requires much effort and cost, especially when it is done manually. Besides, during this process, developers and testers are likely to adopt bad design choices, which may lead to the introduction of the so-called test smells in test code. Test smells are bad solutions to either implement or design test code. As the test code with test smells increases in size, these tests might become more complex, and as a consequence, much harder to understand and evolve correctly. Therefore, test smells may have a negative impact on the quality and maintenance of test code and may also harm the whole software testing activities. In this context, this study aims to understand whether test professionals non-intentionally insert test smells. We carried out an expert survey to analyze the usage frequency of a set of test smells. Sixty professionals from different companies participated in the survey. We selected 14 widely studied smells from the literature, which are also implemented in existing test smell detection tools. The yielded results indicate that experienced professionals introduce test smells during their daily programming tasks, even when they are using standardized practices from their companies, and not only for their personal assumptions. Another relevant evidence was that developers' professional experience can not be considered as a root-cause for the insertion of test smells in test code."", 'corpus_id': 212675695, 'score': 0}, {'doc_id': '216036033', 'title': 'Chat activity is a better predictor than chat sentiment on software developers productivity', 'abstract': ""Recent works have proposed that software developers' positive emotion has a positive impact on software developers' productivity. In this paper we investigate two data sources: developers chat messages (from Slack and Hipchat) and source code commits of a single co-located Agile team over 200 working days. Our regression analysis shows that the number of chat messages is the best predictor and predicts productivity measured both in the number of commits and lines of code with R2 of 0.33 and 0.27 respectively. We then add sentiment analysis variables until AIC of our model no longer improves and gets R2 values of 0.37 (commits) and 0.30 (lines of code). Thus, analyzing chat sentiment improves productivity prediction over chat activity alone but the difference is not massive. This work supports the idea that emotional state and productivity are linked in software development. We find that three positive sentiment metrics, but surprisingly also one negative sentiment metric is associated with higher productivity."", 'corpus_id': 216036033, 'score': 1}]"
10	Codegen	607be0b9475c0b77cbc25dff3b36e49b	17037	{}	"[{'doc_id': '233412265', 'title': 'Sylkan: Towards a Vulkan Compute Target Platform for SYCL', 'abstract': 'SYCL is a modern high-level C++ programming interface which excels at expressing data parallelism for heterogeneous hardware platforms in a programmer-friendly way, and is standardized by the Khronos Group. The latest version of the standard, SYCL 2020, removes the previous dependence of the specification and its implementations on an underlying OpenCL target, opening the door for compliant alternative implementations. In this paper, we discuss the opportunities and challenges of mapping SYCL to Vulkan, a low-level explicit programming model for GPUs. This includes an analysis of the potential semantic mismatch between each respective standard, as well as approaches to work around some of these issues. Additionally, we present a prototype research implementation of Sylkan, a SYCL compiler and runtime targeting Vulkan. In order to evaluate our prototype qualitatively and quantitatively, we chose a variety of functional tests as well as three performance benchmarks. For the functional tests, we discuss and categorize the failures of the current prototype, noting which semantic mismatch or missing implementation causes them. For the performance benchmarks, we compare execution times against a OpenCL-based SYCL implementation and a native Vulkan version of each benchmark, on two hardware platforms.', 'corpus_id': 233412265, 'score': 0}, {'doc_id': '85668703', 'title': 'Competition for acorns among wild boar ( Sus scrofa ) and small mammals in a Mediterranean woodland', 'abstract': 'Observations of the rooting activity of wild boar in a holly-oak grove showed that in March–April the decrease of acorns in the diet (31%) was lower than their availability (82%). Moreover the occurrence of deep rooting events remains high despite the low occurrence of grass roots in the diet. These observations suggested that wild boar may exploit hoards of acorns collected by small mammals living in the study area (mainly wood mice Apodemus sp.). In order to test this hypothesis two experimental trials were set up to: (1) investigate whether wild boar were able to locate acorns buried in the ground (range 0–30 cm) and (2) establish if mouse burrows were more likely to be excavated than locations without burrows. The results clearly show that wild boar actively search for buried acorns, mainly in March (59%vs 31% in April and nothing in May) and that burrows are excavated significantly more than locations without burrows (ratio 2:1, respectively). Moreover, locations with burrows are characterized by a decline of rooting activity as a function of the distance from their centre (P= 0.02), which is absent in the control locations (P= 0.74). Our results show that wild boar are able to partly compensate for a reduced above-ground availability of acorns by predating on hoards collected by small mammals. Since this occurs during a critical period for female wild boar when they are giving birth and lactating, this behaviour may strongly influence the population dynamics of both wild boar and small mammals.', 'corpus_id': 85668703, 'score': 0}, {'doc_id': '234762891', 'title': 'HeapSafe: Securing Unprotected Heaps in RISC-V', 'abstract': 'RISC-V is a promising open-source architecture primarily targeted for embedded systems. Programs compiled using the RISC-V toolchain can run bare-metal on the system, and, as such, can be vulnerable to several memory corruption vulnerabilities. In this work, we present HeapSafe, a lightweight hardware assisted heap-buffer protection scheme to mitigate heap overflow and use-after-free vulnerabilities in a RISC-V SoC. The proposed scheme tags pointers associated with heap buffers with metadata indices and enforces tag propagation for commonly used pointer operations. The HeapSafe hardware is decoupled from the core and is designed as a configurable coprocessor and is responsible for validating the heap buffer accesses. Benchmark results show a 1.5X performance overhead and 1.59% area overhead, while being 22% faster than a software protection. We further implemented a HeapSafe-nb, an asynchronous validation design, which improves performance by 27% over the synchronous HeapSafe.', 'corpus_id': 234762891, 'score': 0}, {'doc_id': '14009935', 'title': 'tcc: A Template-Based Compiler for ‘C', 'abstract': 'Dynamic code generation is an important technique for improving the performance of software by exploiting information known only at run time. ‘C (Tick C) is a superset of ANSI C that, unlike most prior systems, allows high-level, efficient, and machineindependent specification of dynamically generated code. ‘C provides facilities for dynamic code generation within the context of a statically typed, imperative language closely related to the language most widely used in systems development. This paper describes tcc, a compiler currently being written for ‘C. tcc has two objectives: (1) to deliver a complete, solid implementation of ‘C, and (2) to minimize the run-time costs of dynamic code generation. tcc implements dynamic code generation by emitting templates, segments of binary code which at run time can be combined and completed with the values of registers, stack offsets, and constants. tcc also allows some decisions about storage allocation and instruction selection to occur at run time. This provides flexibility in combining arbitrary pieces of dynamic code, while allowing run-time code generation to occur very efficiently.', 'corpus_id': 14009935, 'score': 1}, {'doc_id': '233992496', 'title': 'Typsysteme auf Bit-Ebene für Assembler-Sprachen', 'abstract': 'Type systems help us to write software in various ways: they act as safety nets and make it harder to accidentally write unsafe or incorrect programs. Types are also a form of documentation and a contract between different parties. Some type systems have advanced to a stage where they may even suggest source code to fill holes in an unfinished program. But the more advanced features of type systems are usually only found in high-level languages, while low-level languages like assembly are often untyped. In order to introduce type systems and all their benefits to lower level languages, we may thus wish to translate not only the instructions of a high-level language, but also its types. A type system for an assembly language, then, needs to be able to express the desired properties about types, preferably in a general fashion, rather than in a way that is specifically tailored to one specific source language. In this thesis, we are not concerned so much with the transformation of types, but with the following main research question: “How can a type system handle individual as well as grouped bits and verify that they are processed correctly?” First, we define a set of questions about basic features of a language, and how these features can be used to write software. Second, a prototype assembler for a subset of the Arm A64 instruction set is described. This assembler shall feature a simple type system in which the smallest unit is the bit. The goal is to give the user the tools to define types which are defined down to the smallest unit, giving maximum control. In the third step, this prototype is used to find answers to the questions, which were defined earlier. Finally, the main research question is answered by considering the results that were gained. The final conclusion is that a type system like the one proposed may indeed be implemented and allows the user to express different variations of common type system features. Some rather uncommon features and special purpose safety checks, which are usually not found in typed programming languages, can be implemented as well. The definition of programs with their types may require more effort than in other languages, and if appropriate care is not taken, the time spent on type checking may get out of hand.', 'corpus_id': 233992496, 'score': 1}, {'doc_id': '233747369', 'title': 'Arranging for Safety Checks with Hardware Traps', 'abstract': 'Many programming languages are untyped and perform all type checks at run-time. This requires that type information for objects is kept around for as long as the objects are live. One way of achieving this involves tagging all pointers with the type of the object to which they refer. The compiler inserts safety checks before all object references in order to provide safe semantics and error reporting. The run-time safety checks can however be expensive and compilers sometimes provide an option to use unsafe semantics. The resulting programs are faster and more compact, but there are some obvious drawbacks. This paper describes a technique whereby a compiler can generate code that arranges for the processor to perform safety checks. The technique is usable on commodity hardware.', 'corpus_id': 233747369, 'score': 0}, {'doc_id': '227208952', 'title': 'Copy-and-Patch Binary Code Generation', 'abstract': 'Runtime compilation of runtime-constructed code is becoming standard practice in libraries, DSLs, and database management systems. Since compilation is expensive, systems that are sensitive to compile times such as relational database query compilers compile only hot code and interprets the rest with a much slower interpreter. We present a code generation technique that lowers an AST to binary code by stitching together code from a large library of binary AST node implementations. We call the implementations stencils because they have holes where values must be inserted during code generation. We show how to construct such a stencil library and describe the copy-and-patch technique that generates optimized binary code. The result is a code generator with negligible cost: it produces code from an AST in less time than it takes to construct the AST. Compared to LLVM, compilation is two orders of magnitude faster than -O0 and three orders of magnitude faster than higher optimization levels. The generated code runs an order of magnitude faster than interpretation and runs even faster than LLVM -O0. Thus, copy-and-patch can effectively replace both interpreters and LLVM -O0, making code generation more effective in compile-time sensitive applications.', 'corpus_id': 227208952, 'score': 1}, {'doc_id': '14051969', 'title': 'The Project Maxwell assembler system', 'abstract': ""The Java™ programming language is primarily used for platform-independent programming. Yet it also offers many productivity, maintainability and performance benefits for platform-specific functions, such as the generation of machine code.We have created reliable assemblers for SPARC™, AMD64, IA32 and PowerPC which support all user mode and privileged instructions and with 64-bit mode support for all but the latter. These assemblers are generated as Java source code by our extensible assembler framework, which itself is written in the Java language. The assembler generator also produces javadoc comments that precisely specify the legal values for each operand.Our design is based on the Klein Assembler System written in Self. Assemblers are generated from a specification, as are table-driven disassemblers and unit tests. The specifications that drive the generators are expressed as Java language objects. Thus no extra parsers are needed and developers do not need to learn any new syntax to extend the framework for additional ISAs.Every generated assembler is tested against a preexisting assembler by comparing the output of both. Each instruction's test cases are derived from the cross product of its potential operand values. The majority of tests are positive (i.e., result in a legal instruction encoding). The framework also generates negative tests, which are expected to cause an error detection by an assembler. As with the Klein Assembler System, we have found bugs in the external assemblers as well as in ISA reference manuals.Our framework generates tens of millions of tests. For symbolic operands, our tests include all applicable predefined constants. For integral operands, the important boundary values, such as the respective minimum, maximum, 0, 1 and -1, are tested. Full testing can take hours to run but gives us a high degree of confidence regarding correctness."", 'corpus_id': 14051969, 'score': 1}, {'doc_id': '16996088', 'title': 'The Virtual Processor: Fast, Architecture-Neutral Dynamic Code Generation', 'abstract': 'Tools supporting dynamic code generation tend too be low-level (leaving much work to the client application) or too intimately related with the language/system in which they are used (making them unsuitable for casual reuse). Applications or virtual machines wanting to benefit from runtime code generation are therefore forced to implement much of the compilation chain for themselves even when they make use of the available tools. The VPU is an fast, high-level code generation utility that performs most of the complex tasks related to code generation, including register allocation, and which produces good-quality C ABI-compliant native code. In the simplest cases, adding VPU-based runtime code generation to an application requires just a few lines of additional code--and for a typical virtual machine, VPU-based just-in-time compilation requires only a few lines of code per virtual instruction.', 'corpus_id': 16996088, 'score': 1}, {'doc_id': '100765549', 'title': 'Evolution and effects of surface chemistry of activated coke during combined SO_2/NO_xremoval process', 'abstract': 'The evolution of surface chemistry of activated coke during combined SO2/NOx removal process can be used to elucidate the mechanism of flue gas purification process.The experiments on combined desulfurization/denitration of the flue gas were conducted in a micro reactor.The effect of trace SO2 left in the flue gas after desulfurization on removal of NO by the selective catalytic reduction(SCR),as well as the influence of denitration on followed desulfurization were investigated.The X-ray photoelectron spectroscopy(XPS) and the scanning electron microscope(SEM) were used to characterize the surface properties and morphology of activated coke before and after desulfurization/denitration.The results show that the depleting adsorbed NH3 consumed by reaction with SO2 is the main reason for decreasing NO conversion.The desulfurization efficiency of activated coke are strengthened because of the decreasing acidic surface functional groups resulting from the decrease of oxygen content,delocalization of the π-electron of carbon,and the introduction of N-containing compounds onto the surface of activated coke resulting in the enhancement of basic surface functional groups.', 'corpus_id': 100765549, 'score': 0}]"
11	trimer	06d0af197c7a8b9b309086b582ad852f	7500	{}	"[{'doc_id': '219708363', 'title': 'Optical Shielding of Destructive Chemical Reactions between Ultracold Ground-State NaRb Molecules.', 'abstract': 'We propose a method to suppress the chemical reactions between ultracold bosonic ground-state ^{23}Na^{87}Rb molecules based on optical shielding. By applying a laser with a frequency blue-detuned from the transition between the lowest rovibrational level of the electronic ground state X^{1}Σ^{+}(v_{X}=0,j_{X}=0), and the long-lived excited level b^{3}Π_{0}(v_{b}=0,j_{b}=1), the long-range dipole-dipole interaction between the colliding molecules can be engineered, leading to a dramatic suppression of reactive and photoinduced inelastic collisions, for both linear and circular laser polarizations. We demonstrate that the spontaneous emission from b^{3}Π_{0}(v_{b}=0,j_{b}=1) does not deteriorate the shielding process. This opens the possibility for a strong increase of the lifetime of cold molecule traps and for an efficient evaporative cooling. We also anticipate that the proposed mechanism is valid for alkali-metal diatomics with sufficiently large dipole-dipole interactions.', 'corpus_id': 219708363, 'score': 0}, {'doc_id': '119202372', 'title': 'Calculations of long-range three-body interactions for Li (2 2 S)-Li (2 2 S)-Li (2 2 P)', 'abstract': 'General formulas for calculating the several leading long-range interactions among three identical atoms where two atoms are in identical $S$ states and the other atom is in a $P$ state are obtained using perturbation theory for the energies up to second order. The first-order (dipolar) interactions depend on the geometrical configurations of the three atoms. In second order, additive and nonadditive dispersion interactions are obtained. The nonadditive interactions depend on the geometrical configurations in marked contrast to the case where all three atoms are in identical $S$ states, for which the nonadditive (also known as triple-dipole or as Axilrod-Muto-Teller) dispersion interactions appear at the third order. The formalism is demonstrated by the calculation of the coefficients for the $\\mathrm{Li}(2\\phantom{\\rule{0.16em}{0ex}}^{2}S)\\ensuremath{-}\\mathrm{Li}(2\\phantom{\\rule{0.16em}{0ex}}^{2}S)\\ensuremath{-}\\mathrm{Li}(2\\phantom{\\rule{0.16em}{0ex}}^{2}P)$ system using variationally generated atomic lithium wave functions in Hylleraas coordinates. The present dipolar coefficients and additive and nonadditive dispersion coefficients may be useful in constructing precise potential energy surfaces for this three lithium atom system.', 'corpus_id': 119202372, 'score': 1}, {'doc_id': '219401859', 'title': 'Nondestructive dispersive imaging of rotationally excited ultracold molecules.', 'abstract': 'A barrier to realizing the potential of molecules for quantum information science applications is a lack of high-fidelity, single-molecule imaging techniques. Here, we present and theoretically analyze a general scheme for dispersive imaging of electronic ground-state molecules. Our technique relies on the intrinsic anisotropy of excited molecular rotational states to generate optical birefringence, which can be detected through polarization rotation of an off-resonant probe laser beam. Using 23Na87Rb and 87Rb133Cs as examples, we construct a formalism for choosing the molecular state to be imaged and the excited electronic states involved in off-resonant coupling. Our proposal establishes the relevant parameters for achieving degree-level polarization rotations for bulk molecular gases, thus enabling high-fidelity nondestructive imaging. We additionally outline requirements for the high-fidelity imaging of individually trapped molecules.', 'corpus_id': 219401859, 'score': 0}, {'doc_id': '218889749', 'title': 'Transition Strength Measurements to Guide Magic Wavelength Selection in Optically Trapped Molecules.', 'abstract': 'Optical trapping of molecules with long coherence times is crucial for many protocols in quantum information and metrology. However, the factors that limit the lifetimes of the trapped molecules remain elusive and require improved understanding of the underlying molecular structure. Here we show that measurements of vibronic line strengths in weakly and deeply bound ^{88}Sr_{2} molecules, combined with ab\xa0initio calculations, allow for unambiguous identification of vibrational quantum numbers. This, in turn, enables the construction of refined excited potential energy curves, informing the selection of magic wavelengths that facilitate long vibrational coherence. We demonstrate Rabi oscillations between far-separated vibrational states that persist for nearly 100\xa0ms.', 'corpus_id': 218889749, 'score': 0}, {'doc_id': '216412528', 'title': 'Long-range interaction of \nLi(2S2)−Li(2S2)−Li+(1S1)', 'abstract': ""The long-range interactions among two- or three-atom systems are of considerable importance in the cold and ultracold research areas for many-body systems. For an ion and an atom, the long-range interaction potential is dominated by the induction (or polarization) potential resulting from the (classical) effect of the ion's electric field on the atom and the leading term of the induction potential is much stronger than the (quantum mechanical) dispersion (or van der Waals) interaction. The present paper focuses on the long-range interaction of the $\\mathrm{Li}(2\\phantom{\\rule{0.16em}{0ex}}^{2}S)\\ensuremath{-}\\mathrm{Li}(2\\phantom{\\rule{0.16em}{0ex}}^{2}S)\\ensuremath{-}{\\mathrm{Li}}^{+}(1\\phantom{\\rule{0.16em}{0ex}}^{1}S)$ system, to see what changes this induction effect (originating in the electric field of the ${\\mathrm{Li}}^{+}$ ion) yields in the long-range additive and nonadditive interactions of this three-body system. Using perturbation theory for energies, we evaluate the coefficients ${C}_{n}$ in the potential energy for the three well-separated constituents, where $n$ refers to the corresponding order in inverse powers of distance, obtaining the additive interaction coefficients ${C}_{4}, {C}_{6}, {C}_{7}, {C}_{8}, {C}_{9}$ and the nonadditive interaction coefficients ${C}_{7}, {C}_{9}$. The obtained coefficients ${C}_{n}$ are calculated with highly accurate variationally generated nonrelativistic wave functions in Hylleraas coordinates. Our calculations may be of interest for the study of three-body recombination and for constructing precise potential energy surfaces. We also provide precise evaluations of the long-range potentials for the two-body $\\mathrm{Li}(2\\phantom{\\rule{0.16em}{0ex}}^{2}S)\\ensuremath{-}{\\mathrm{Li}}^{+}(1\\phantom{\\rule{0.16em}{0ex}}^{1}S)$ system. For both the two-body and three-body cases, we provide results for the like-nuclei cases of $^{6}\\mathrm{Li}$ and $^{7}\\mathrm{Li}$."", 'corpus_id': 216412528, 'score': 1}, {'doc_id': '213126278', 'title': 'Destiny of optical lattices with strong intersite interactions', 'abstract': 'Optical lattices are considered loaded by atoms or molecules that can exhibit strong interactions between different lattice sites. The strength of these interactions can be sufficient for generating collective phonon excitations above the ground-state energy level. Varying the interaction strength makes it possible to create several equilibrium three-dimensional phases, including conducting optical lattices, insulating optical lattices, delocalized quantum crystals, and localized quantum crystals. Also, there can exist finite one- and two-dimensional lattices of chains and planes.', 'corpus_id': 213126278, 'score': 1}, {'doc_id': '220769022', 'title': 'Magnetic Dipolar Interaction between Hyperfine Clock States in a Planar Alkali Bose Gas.', 'abstract': 'In atomic systems, clock states feature a zero projection of the total angular momentum and thus a low sensitivity to magnetic fields. This makes them widely used for metrological applications like atomic fountains or gravimeters. Here, we show that a mixture of two such nonmagnetic states still displays magnetic dipole-dipole interactions comparable to the one expected for the other Zeeman states of the same atomic species. Using high-resolution spectroscopy of a planar gas of ^{87}Rb atoms with a controlled in plane shape, we explore the effective isotropic and extensive character of these interactions and demonstrate their tunability. Our measurements set strong constraints on the relative values of the s-wave scattering lengths a_{ij} involving the two clock states.', 'corpus_id': 220769022, 'score': 1}, {'doc_id': '220961423', 'title': 'Landau-Fermi liquids without quasiparticles', 'abstract': 'Landau-Fermi liquid theory is conventionally believed to hold whenever the interacting single-particle density of states develops a $\\delta$-like component at the Fermi surface, which is associated with quasiparticles. Here we show that a microscopic justification can be actually achieved under more general circumstances, even in case coherent quasiparticles are totally missing and the interacting single-particle density of states vanishes at the chemical potential as consequence of a pole singularity in the self-energy.', 'corpus_id': 220961423, 'score': 0}, {'doc_id': '58928262', 'title': 'Calculations of long-range three-body interactions for He($n_0\\,^{\\lambda}S$)-He($n_0\\,^{\\lambda}S$)-He($n_0^{\\prime}\\,^{\\lambda}L$)', 'abstract': ""We theoretically investigate long-range interactions between an excited $L$ state He atom and two identical $S$ state He atoms, for the cases of the three atoms all in spin singlet states or all in spin triplet states, denoted by He($n_0\\,^{\\lambda}S$)-He($n_0\\,^{\\lambda}S$)-He($n_0^{\\prime}\\,^{\\lambda}L$), with $n_0$ and $n_0'$ principal quantum numbers, $\\lambda=1$ or 3 the spin multiplicity, and $L$ the orbital angular momentum of a He atom. Using degenerate perturbation theory for the energies up to second-order, we evaluate the coefficients $C_3$ of the first order dipolar interactions and the coefficients $C_6$ and $C_8$ of the second order additive and nonadditive interactions. Both the dipolar and dispersion interaction coefficients, for these three-body degenerate systems, show dependences on the geometrical configurations of the three atoms. The nonadditive interactions start to appear in second-order. To demonstrate the results and for applications, the obtained coefficients $C_n$ are evaluated with highly accurate variationally-generated nonrelativistic wave functions in Hylleraas coordinates for He($1\\,^{1}S$)-He($1\\,^{1}S$)-He($2\\,^{1}S$), He${(1\\,^{1}S)}$-He${(1\\,^{1}S)}$-He${(2\\,^{1}P)}$, He${(2\\,^{1}S)}$-He${(2\\,^{1}S)}$-He${(2\\,^{1}P)}$, and He${(2\\,^{3}S)}$-He${(2\\,^{3}S)}$-He${(2\\,^{3}P)}$. The calculations are given for three like-nuclei for the cases of hypothetical infinite mass He nuclei, and of real finite mass $^4{}$He or $^3{}$He nuclei. The special cases of the three atoms in equilateral triangle configurations are explored in detail, and for the cases where one of the atoms is in a $P$ state, we also present results for the atoms in an isosceles right triangle configuration or in an equally spaced co-linear configuration. The results can be applied to construct potential energy surfaces for three helium atom systems."", 'corpus_id': 58928262, 'score': 1}, {'doc_id': '221191908', 'title': 'Low-Cost, Large-Scale Production of the Anti-viral Lectin Griffithsin', 'abstract': 'Griffithsin, a broad-spectrum antiviral lectin, has potential to prevent and treat numerous viruses including HIV, HCV, HSV, SARS-CoV, and SARS-CoV-2. For these indications, the annual demand for Griffithsin could reach billions of doses and affordability is paramount. We report the lab-scale validation of a bioprocess that supports production volumes of >20 tons per year at a cost of goods sold below $3,500/kg. Recombinant expression in engineered E. coli enables Griffithsin titers ∼2.5 g/L. A single rapid precipitation step provides > 90% yield with 2-, 3-, and 4-log reductions in host cell proteins, endotoxin, and nucleic acids, respectively. Two polishing chromatography steps remove residual contaminants leading to pure, active Griffithsin. Compared to a conventional one this process shows lower costs and improved economies of scale. These results support the potential of biologics in very large-scale, cost-sensitive applications such as antivirals, and highlight the importance of bioprocess innovations in enabling these applications.', 'corpus_id': 221191908, 'score': 0}]"
12	NLU	04808383df00e6501c912f8fae661f73	4011	{'NLU': 'natural language understanding'}	"[{'doc_id': '41792210', 'title': 'From Characters to Understanding Natural Language (C2NLU): Robust End-to-End Deep Learning for NLP (Dagstuhl Seminar 17042)', 'abstract': 'This report documents the program and the outcomes of Dagstuhl Seminar 17042 ""From Characters to Understanding Natural Language (C2NLU): Robust End-to-End Deep Learning for NLP"". The seminar brought together researchers from different fields, including natural language processing, computational linguistics, deep learning and general machine learning. 31 participants from 22 academic and industrial institutions discussed advantages and challenges of using characters, i.e., ""raw text"", as input for deep learning models instead of language-specific tokens. Eight talks provided overviews of different topics, approaches and challenges in current natural language processing research. In five working groups, the participants discussed current natural language processing/understanding topics in the context of character-based modeling, namely, morphology, machine translation, representation learning, end-to-end systems and dialogue. In most of the discussions, the need for a more detailed model analysis was pointed out. Especially for character-based input, it is important to analyze what a deep learning model is able to learn about language - about tokens, morphology or syntax in general. For an efficient and effective understanding of language, it might furthermore be beneficial to share representations learned from multiple objectives to enable the models to focus on their specific understanding task instead of needing to learn syntactic regularities of language first. Therefore, benefits and challenges of transfer learning were an important topic of the working groups as well as of the panel discussion and the final plenary discussion.', 'corpus_id': 41792210, 'score': 1}, {'doc_id': '226964491', 'title': 'Pre-training Text-to-Text Transformers for Concept-centric Common Sense', 'abstract': 'Pre-trained language models (PTLM) have achieved impressive results in a range of natural language understanding (NLU) and generation (NLG) tasks. However, current pre-training objectives such as masked token prediction (for BERT-style PTLMs) and masked span infilling (for T5-style PTLMs) do not explicitly model the relational commonsense knowledge about everyday concepts, which is crucial to many downstream tasks that need common sense to understand or generate. To augment PTLMs with concept-centric commonsense knowledge, in this paper, we propose both generative and contrastive objectives for learning common sense from the text, and use them as intermediate self-supervised learning tasks for incrementally pre-training PTLMs (before task-specific fine-tuning on downstream datasets). Furthermore, we develop a joint pre-training framework to unify generative and contrastive objectives so that they can mutually reinforce each other. Extensive experimental results show that our method, concept-aware language model (CALM), can pack more commonsense knowledge into the parameters of a pre-trained text-to-text transformer without relying on external knowledge graphs, yielding better performance on both NLU and NLG tasks. We show that while only incrementally pre-trained on a relatively small corpus for a few steps, CALM outperforms baseline methods by a consistent margin and even comparable with some larger PTLMs, which suggests that CALM can serve as a general, plug-and-play method for improving the commonsense reasoning ability of a PTLM.', 'corpus_id': 226964491, 'score': 1}, {'doc_id': '211818180', 'title': 'Natural Language Processing Advancements By Deep Learning: A Survey', 'abstract': 'Natural Language Processing (NLP) helps empower intelligent machines by enhancing a better understanding of the human language for linguistic-based human-computer communication. Recent developments in computational power and the advent of large amounts of linguistic data have heightened the need and demand for automating semantic analysis using data-driven approaches. The utilization of data-driven strategies is pervasive now due to the significant improvements demonstrated through the usage of deep learning methods in areas such as Computer Vision, Automatic Speech Recognition, and in particular, NLP. This survey categorizes and addresses the different aspects and applications of NLP that have benefited from deep learning. It covers core NLP tasks and applications and describes how deep learning methods and models advance these areas. We further analyze and compare different approaches and state-of-the-art models.', 'corpus_id': 211818180, 'score': 1}, {'doc_id': '227228154', 'title': 'Language-Driven Region Pointer Advancement for Controllable Image Captioning', 'abstract': 'Controllable Image Captioning is a recent sub-field in the multi-modal task of Image Captioning wherein constraints are placed on which regions in an image should be described in the generated natural language caption. This puts a stronger focus on producing more detailed descriptions, and opens the door for more end-user control over results. A vital component of the Controllable Image Captioning architecture is the mechanism that decides the timing of attending to each region through the advancement of a region pointer. In this paper, we propose a novel method for predicting the timing of region pointer advancement by treating the advancement step as a natural part of the language structure via a NEXT-token, motivated by a strong correlation to the sentence structure in the training data. We find that our timing agrees with the ground-truth timing in the Flickr30k Entities test data with a precision of 86.55% and a recall of 97.92%. Our model implementing this technique improves the state-of-the-art on standard captioning metrics while additionally demonstrating a considerably larger effective vocabulary size.', 'corpus_id': 227228154, 'score': 0}, {'doc_id': '28517752', 'title': 'A Data-Driven Approach to Infer Knowledge Base Representation for Natural Language Relations', 'abstract': 'This paper studies the problem of discovering the structured knowledge representation of binary natural language relations. The representation, known as the schema, generalizes the traditional path of predicates to support more complex semantics. We present a search algorithm to generate schemas over a knowledge base, and propose a data-driven learning approach to discover the most suitable representations to one relation. Evaluation results show that inferred schemas are able to represent precise semantics, and can be used to enrich manually crafted knowledge bases. 1', 'corpus_id': 28517752, 'score': 1}, {'doc_id': '227229069', 'title': 'Intrinsic Knowledge Evaluation on Chinese Language Models', 'abstract': 'Recent NLP tasks have benefited a lot from pre-trained language models (LM) since they are able to encode knowledge of various aspects. However, current LM evaluations focus on downstream performance, hence lack to comprehensively inspect in which aspect and to what extent have they encoded knowledge. This paper addresses both queries by proposing four tasks on syntactic, semantic, commonsense, and factual knowledge, aggregating to a total of $39,308$ questions covering both linguistic and world knowledge in Chinese. Throughout experiments, our probes and knowledge data prove to be a reliable benchmark for evaluating pre-trained Chinese LMs. Our work is publicly available at https://github.com/ZhiruoWang/ChnEval.', 'corpus_id': 227229069, 'score': 0}, {'doc_id': '216869018', 'title': 'Natural Language Premise Selection: Finding Supporting Statements for Mathematical Text', 'abstract': 'Mathematical text is written using a combination of words and mathematical expressions. This combination, along with a specific way of structuring sentences makes it challenging for state-of-art NLP tools to understand and reason on top of mathematical discourse. In this work, we propose a new NLP task, the natural premise selection, which is used to retrieve supporting definitions and supporting propositions that are useful for generating an informal mathematical proof for a particular statement. We also make available a dataset, NL-PS, which can be used to evaluate different approaches for the natural premise selection task. Using different baselines, we demonstrate the underlying interpretation challenges associated with the task.', 'corpus_id': 216869018, 'score': 1}, {'doc_id': '212725539', 'title': 'LSCP: Enhanced Large Scale Colloquial Persian Language Understanding', 'abstract': 'Language recognition has been significantly advanced in recent years by means of modern machine learning methods such as deep learning and benchmarks with rich annotations. However, research is still limited in low-resource formal languages. This consists of a significant gap in describing the colloquial language especially for low-resourced ones such as Persian. In order to target this gap for low resource languages, we propose a “Large Scale Colloquial Persian Dataset” (LSCP). LSCP is hierarchically organized in a semantic taxonomy that focuses on multi-task informal Persian language understanding as a comprehensive problem. This encompasses the recognition of multiple semantic aspects in the human-level sentences, which naturally captures from the real-world sentences. We believe that further investigations and processing, as well as the application of novel algorithms and methods, can strengthen enriching computerized understanding and processing of low resource languages. The proposed corpus consists of 120M sentences resulted from 27M tweets annotated with parsing tree, part-of-speech tags, sentiment polarity and translation in five different languages.', 'corpus_id': 212725539, 'score': 0}, {'doc_id': '227238703', 'title': 'Meta-Embeddings for Natural Language Inference and Semantic Similarity tasks', 'abstract': 'Word Representations form the core component for almost all advanced Natural Language Processing (NLP) applications such as text mining, question-answering, and text summarization, etc. Over the last two decades, immense research is conducted to come up with one single model to solve all major NLP tasks. The major problem currently is that there are a plethora of choices for different NLP tasks. Thus for NLP practitioners, the task of choosing the right model to be used itself becomes a challenge. Thus combining multiple pre-trained word embeddings and forming meta embeddings has become a viable approach to improve tackle NLP tasks. Meta embedding learning is a process of producing a single word embedding from a given set of pre-trained input word embeddings. In this paper, we propose to use Meta Embedding derived from few State-of-the-Art (SOTA) models to efficiently tackle mainstream NLP tasks like classification, semantic relatedness, and text similarity. We have compared both ensemble and dynamic variants to identify an efficient approach. The results obtained show that even the best State-of-the-Art models can be bettered. Thus showing us that meta-embeddings can be used for several NLP tasks by harnessing the power of several individual representations.', 'corpus_id': 227238703, 'score': 0}, {'doc_id': '212628456', 'title': 'Parsing Thai Social Data: A New Challenge for Thai NLP', 'abstract': 'Dependency parsing (DP) is a task that analyzes text for syntactic structure and relationship between words. DP is widely used to improve natural language processing (NLP) applications in many languages such as English. Previous works on DP are generally applicable to formally written languages. However, they do not apply to informal languages such as the ones used in social networks. Therefore, DP has to be researched and explored with such social network data. In this paper, we explore and identify a DP model that is suitable for Thai social network data. After that, we will identify the appropriate linguistic unit as an input. The result showed that, the transition based model called, improve Elkared dependency parser outperform the others at UAS of 81.42%.', 'corpus_id': 212628456, 'score': 0}]"
13	Song Synth	437ef07033e1ded0acb2458f6821ceac	16008	{}	[{'doc_id': '233366278', 'title': 'Top 10 Artificial Intelligence Algorithms in Computer Music Composition', 'abstract': 'Music composition is now appealing to both musicians and non-musicians equally. It branches into various musical tasks such as the generation of melody, accompaniment, or rhythm. This paper discusses the top ten artificial intelligence algorithms with applications in computer music composition from 2010 to 2020. We give an analysis of each algorithm and highlight its recent applications in music composition tasks, shedding the light on its strengths and weaknesses. Our study gives an insight on the most suitable algorithm for each musical task, such as rule-based systems for music theory representation, case-based reasoning for capturing previous musical experiences, Markov chains for melody generation, generative grammars for fast composition of musical pieces that comply to music rules, and linear programming for timbre synthesis. Additionally, there are biologically inspired algorithms such as: genetic algorithms, and algorithms used by artificial immune systems and artificial neural networks, including shallow neural networks, deep neural networks, and generative adversarial networks. These relatively new algorithms are currently heavily used in performing numerous music composition tasks.', 'corpus_id': 233366278, 'score': 1}, {'doc_id': '233284509', 'title': 'Applications of Computational Intelligence in Computer Music Composition', 'abstract': 'Engaging computers in composing musical pieces is a challenging and trending field of research. The musical tasks that can be performed or aided by computers’ computational powers, are numerous. This paper is concerned with applications of computational intelligence in music composition. Its main objective is to survey various computational intelligence techniques for performing miscellaneous music composition tasks. To achieve this objective, we first define each music composition task, then we discuss the recent applications of each, and the techniques adopted in them. We also highlight the most suitable techniques for performing each task. Our study shows that the most suitable techniques for human composers imitative systems are case-based reasoning and artificial neural networks. It is also shown that Markov models are more suitable for predicting musical notes based on the given previous notes. Genetic algorithms excel in chord progressions generation. Deep neural networks are clever at capturing temporal information of a musical piece. The state-of-the-art generative adversarial networks produce music as close as possible to real compositions. At the end of this study, we shed the light on many future research directions in the field of computer music composition.', 'corpus_id': 233284509, 'score': 1}, {'doc_id': '233296970', 'title': 'Visually Guided Sound Source Separation and Localization using Self-Supervised Motion Representations', 'abstract': 'The objective of this paper is to perform audio-visual sound source separation, i.e. to separate component audios from a mixture based on the videos of sound sources. Moreover, we aim to pinpoint the source location in the input video sequence. Recent works have shown impressive audio-visual separation results when using prior knowledge of the source type (e.g. human playing instrument) and pre-trained motion detectors (e.g. keypoints or optical flows). However, at the same time, the models are limited to a certain application domain. In this paper, we address these limitations and make the following contributions: i) we propose a two-stage architecture, called Appearance and Motion network (AMnet), where the stages specialise to appearance and motion cues, respectively. The entire system is trained in a self-supervised manner; ii) we introduce an Audio-Motion Embedding (AME) framework to explicitly represent the motions that related to sound; iii) we propose an audio-motion transformer architecture for audio and motion feature fusion; iv) we demonstrate state-of-the-art performance on two challenging datasets (MUSIC-21 and AVE) despite the fact that we do not use any pre-trained keypoint detectors or optical flow estimators. Project page: https://ly-zhu.github.io/self-supervisedmotion-representations.', 'corpus_id': 233296970, 'score': 0}, {'doc_id': '233255223', 'title': 'AUTOMATIC MUSIC PRODUCTION USING GENERA-', 'abstract': 'When talking about computer-based music generation, two are the main threads of research: the construction of autonomous music-making systems, and the design of computer-based environments to assist musicians. However, even though creating accompaniments for melodies is an essential part of every producer’s and songwriter’s work, little effort has been done in the field of automatic music arrangement in the audio domain. In this contribution, we propose a novel framework for automatic music accompaniment in the Mel-frequency domain. Using several songs converted into Mel-spectrograms – a two-dimensional time-frequency representation of audio signals – we were able to automatically generate original arrangements for both bass and voice lines. Treating music pieces as images (Mel-spectrograms) allowed us to reformulate our problem as an unpaired imageto-image translation problem, and to tackle it with CycleGAN, a well-established framework. Moreover, the choice to deploy raw audio and Mel-spectrograms enabled us to more effectively model long-range dependencies, to better represent how humans perceive music, and to potentially draw sounds for new arrangements from the vast collection of music recordings accumulated in the last century. Our approach was tested on two different downstream tasks: given a bass line creating credible and on-time drums, and given an acapella song arranging it to a full song. In absence of an objective way of evaluating the output of music generative systems, we also defined a possible metric for the proposed task, partially based on human (and expert) judgement.', 'corpus_id': 233255223, 'score': 1}, {'doc_id': '232372180', 'title': 'Chord Conditioned Melody Generation With Transformer Based Decoders', 'abstract': 'For successful artificial music composition, chords and melody must be aligned well. Yet, chord conditioned melody generation remains a challenging task mainly due to its multimodality. While few studies have focused on this task, they face difficulties in generating dynamic rhythm patterns aligned appropriately with a given chord progression. In this paper, we propose a chord conditioned melody Transformer, a K-POP melody generation model, which separately produces rhythm and pitch conditioned on a chord progression. The model is trained in two phases. A rhythm decoder (RD) is trained first, and subsequently a pitch decoder is trained by utilizing the pre-trained RD. Experimental results show that reusing RD at the pitch decoding stage and training with pitch varied rhythm data improve the performance. It was also observed that the samples produced by the model well reflected the key characteristics of dataset in terms of both pitch and rhythm related features, including chord tone ratio and rhythm distribution. Qualitative analysis reveals the model’s capability of generating various melodies in accordance with a given chord progression, as well as the presence of repetitions and variations within the generated melodies. With subjective human listening test, we come to a conclusion that the model was able to successfully produce new melodies that sound pleasant in terms of both rhythm and pitch (Source code available at https://github.com/ckycky3/CMT-pytorch).', 'corpus_id': 232372180, 'score': 1}, {'doc_id': '232185495', 'title': 'Multi-Format Contrastive Learning of Audio Representations', 'abstract': 'Recent advances suggest the advantage of multi-modal training in comparison with single-modal methods. In contrast to this view, in our work we find that similar gain can be obtained from training with different formats of a single modality. In particular, we investigate the use of the contrastive learning framework to learn audio representations by maximizing the agreement between the raw audio and its spectral representation. We find a significant gain using this multi-format strategy against the single-format counterparts. Moreover, on the downstream AudioSet and ESC-50 classification task, our audio-only approach achieves new state-ofthe-art results with a mean average precision of 0.376 and an accuracy of 90.5%, respectively.', 'corpus_id': 232185495, 'score': 0}, {'doc_id': '232372487', 'title': 'Enhancing Local Dependencies for Transformer-Based Text-to-Speech via Hybrid Lightweight Convolution', 'abstract': 'Owing to the powerful self-attention mechanism, the Transformer network has achieved considerable successes across many sequence modeling tasks and has become one of the most popular methods in text-to-speech (TTS). The vanilla self-attention excels in capturing long-range dependencies but suffers in modeling stable short-range dependencies that are quite important for speech synthesis where the local audio signals are highly correlated. To address this problem, we propose the hybrid lightweight convolution (HLC), which is responsible for fully exploiting local structures of a sequence, and combine it with the self-attention to improve the Transformer-based TTS. The experimental results show that our modified model obtains better performance in both objective and subjective evaluations. At the same time, we also demonstrate that a more compact TTS model may be built through the combination of self-attention and proposed hybrid lightweight convolution. Besides, this method is also potentially adaptable for other sequence modeling tasks.', 'corpus_id': 232372487, 'score': 0}, {'doc_id': '233147773', 'title': 'MELODY GENERATION WITH MARKOV MODELS, A RULE BASED APPROACH', 'abstract': 'Algorithmic composition is an important area of musical research in machine learning and artificial intelligence. We propose a low-complexity solution to melody generation that develops melodies based on expert rule-based models and learned melody generation from Markov Models. Recent advances in machine learning based techniques for algorithmic composition have created an excellent set of methods for generating artistic musical works. These methods, while intrinsically very different from rule based models for the same purpose, can produce similar outputs if both are executed well. This project compares and contrasts a rule-based model and a markov model for melody generation. Both models strive to compare to a dataset of folk songs parsed into MIDI files, outputting MIDI data for playback and comparison. We compare the model outputs both quantitatively and qualitatively, both in terms of loyalty to source material, and in terms of adherence to music theory rules and practices. Our comparative results confirm the comparability of our melody generation with two additional advantages being flexibility of melody generation based on user preference and user dataset selection to program in inherent biases.', 'corpus_id': 233147773, 'score': 1}, {'doc_id': '232046451', 'title': 'MixSpeech: Data Augmentation for Low-Resource Automatic Speech Recognition', 'abstract': 'In this paper, we propose MixSpeech, a simple yet effective data augmentation method based on mixup for automatic speech recognition (ASR). MixSpeech trains an ASR model by taking a weighted combination of two different speech features (e.g., mel-spectrograms or MFCC) as the input, and recognizing both text sequences, where the two recognition losses use the same combination weight. We apply MixSpeech on two popular end-to-end speech recognition models including LAS (Listen, Attend and Spell) and Transformer, and conduct experiments on several low-resource datasets including TIMIT, WSJ, and HKUST. Experimental results show that MixSpeech achieves better accuracy than the baseline models without data augmentation, and outperforms a strong data augmentation method SpecAugment on these recognition tasks. Specifically, MixSpeech outperforms SpecAugment with a relative PER improvement of 10.6% on TIMIT dataset, and achieves a strong WER of 4.7% on WSJ dataset.', 'corpus_id': 232046451, 'score': 0}, {'doc_id': '233033844', 'title': 'Weakly-supervised Audio-visual Sound Source Detection and Separation', 'abstract': 'Learning how to localize and separate individual object sounds in the audio channel of the video is a difficult task. Current state-of-the-art methods predict audio masks from artificially mixed spectrograms, known as Mix-and-Separate framework. We propose an audio-visual co-segmentation, where the network learns both what individual objects look and sound like, from videos labeled with only object labels. Unlike other recent visually-guided audio source separation frameworks, our architecture can be learned in an end-to-end manner and requires no additional supervision or bounding box proposals. Specifically, we introduce weakly-supervised object segmentation in the context of sound separation. We also formulate spectrogram mask prediction using a set of learned mask bases, which combine using coefficients conditioned on the output of object segmentation , a design that facilitates separation. Extensive experiments on the MUSIC dataset show that our proposed approach outperforms state-of-the-art methods on visually guided sound source separation and sound denoising.', 'corpus_id': 233033844, 'score': 0}]
14	predicting-function	158d4fb96ad3b4bcc5a7643ad54f5996	3993	{}	"[{'doc_id': '216036143', 'title': 'Self-Supervised Feature Extraction for 3D Axon Segmentation', 'abstract': 'Existing learning-based methods to automatically trace axons in 3D brain imagery often rely on manually annotated segmentation labels. Labeling is a labor-intensive process and is not scalable to whole-brain analysis, which is needed for improved understanding of brain function. We propose a self-supervised auxiliary task that utilizes the tube-like structure of axons to build a feature extractor from unlabeled data. The proposed auxiliary task constrains a 3D convolutional neural network (CNN) to predict the order of permuted slices in an input 3D volume. By solving this task, the 3D CNN is able to learn features without ground-truth labels that are useful for downstream segmentation with the 3D U-Net model. To the best of our knowledge, our model is the first to perform automated segmentation of axons imaged at subcellular resolution with the SHIELD technique. We demonstrate improved segmentation performance over the 3D U-Net model on both the SHIELD PVGPe dataset and the BigNeuron Project, single neuron Janelia dataset.', 'corpus_id': 216036143, 'score': 0}, {'doc_id': '214693237', 'title': 'Planning with Brain-inspired AI', 'abstract': 'This article surveys engineering and neuroscientific models of planning as a cognitive function, which is regarded as a typical function of fluid intelligence in the discussion of general intelligence. It aims to present existing planning models as references for realizing the planning function in brain-inspired AI or artificial general intelligence (AGI). It also proposes themes for the research and development of brain-inspired AI from the viewpoint of tasks and architecture.', 'corpus_id': 214693237, 'score': 0}, {'doc_id': '6102161', 'title': 'Cortical Folding Patterns and Predicting Cytoarchitecture', 'abstract': 'The human cerebral cortex is made up of a mosaic of structural areas, frequently referred to as Brodmann areas (BAs). Despite the widespread use of cortical folding patterns to perform ad hoc estimations of the locations of the BAs, little is understood regarding 1) how variable the position of a given BA is with respect to the folds, 2) whether the location of some BAs is more variable than others, and 3) whether the variability is related to the level of a BA in a putative cortical hierarchy. We use whole-brain histology of 10 postmortem human brains and surface-based analysis to test how well the folds predict the locations of the BAs. We show that higher order cortical areas exhibit more variability than primary and secondary areas and that the folds are much better predictors of the BAs than had been previously thought. These results further highlight the significance of cortical folding patterns and suggest a common mechanism for the development of the folds and the cytoarchitectonic fields.', 'corpus_id': 6102161, 'score': 1}, {'doc_id': '214723743', 'title': 'A new sulcal landmark identifying anatomical and functional gradients in human lateral prefrontal cortex', 'abstract': 'Understanding the relationship between anatomy and function in portions of human cortex that are expanded compared to other mammals such as lateral prefrontal cortex (LPFC) is of major interest in cognitive neuroscience. Implementing a multi-modal approach and the manual definition of nearly 800 cortical indentations, or sulci, in 72 hemispheres, we report a new sulcal landmark in human LPFC: the posterior middle frontal sulcus (pmfs). The pmfs is a shallow tertiary sulcus with three components that differ in their myelin content, resting state connectivity profiles, and engagement across meta-analyses of 83 cognitive tasks. These findings support a classic, largely unconsidered anatomical theory that tertiary sulci serve as landmarks in association cortices, as well as a modern cognitive neuroscience theory proposing a functional hierarchy in LPFC. As there is a growing need for computational tools that automatically define tertiary sulci throughout cortex, we share pmfs probabilistic sulcal maps with the field.', 'corpus_id': 214723743, 'score': 1}, {'doc_id': '211126796', 'title': 'What is the function of inter-hemispheric inhibition?.', 'abstract': ""It is widely supposed that following unilateral brain injury, there arises an asymmetry in inter-hemispheric inhibition which has an adverse influence upon motor control. I argue that this 'inter-hemispheric imbalance' model arises from a fundamental misunderstanding of the roles played by inter-hemispheric (callosal) projections in mammalian brains. Drawing upon a large body of empirical data, derived largely from animal models, and associated theoretical modeling, it is demonstrated that inter-hemispheric projections perform contrast enhancing and integrative functions via mechanisms such as surround/lateral inhibition. The principal functional unit of callosal influence comprises a facilitatory centre and a depressing peripheral zone, that together shape the influence of converging inputs to pyramidal neurons. Inter-hemispheric inhibition is an instance of a more general feature of mammalian neural systems, whereby inhibitory interneurons act not simply to prevent over-excitation but to sculpt the output of specific circuits. The narrowing of the excitatory focus that occurs through crossed surround inhibition is a highly conserved motif of transcallosal interactions in mammalian sensory and motor cortices. A case is presented that the notion of 'inter-hemispheric imbalance' has been sustained, and clinical interventions derived from this model promoted, by erroneous assumptions concerning that revealed by investigative techniques such as transcranial magnetic stimulation (TMS). The alternative perspective promoted by the present analysis, also permits the basis of positive (e.g. post stroke) associations between the structural integrity of transcallosal projections and motor capability to be better understood."", 'corpus_id': 211126796, 'score': 0}, {'doc_id': '3222772', 'title': 'Predicting the location of entorhinal cortex from MRI', 'abstract': ""Entorhinal cortex (EC) is a medial temporal lobe area critical to memory formation and spatial navigation that is among the earliest parts of the brain affected by Alzheimer's disease (AD). Accurate localization of EC would thus greatly facilitate early detection and diagnosis of AD. In this study, we used ultra-high resolution ex vivo MRI to directly visualize the architectonic features that define EC rostrocaudally and mediolaterally, then applied surface-based registration techniques to quantify the variability of EC with respect to cortical geometry, and made predictions of its location on in vivo scans. The results indicate that EC can be localized quite accurately based on cortical folding patterns, within 3 mm in vivo, a significant step forward in our ability to detect the earliest effects of AD when clinical intervention is most likely to be effective."", 'corpus_id': 3222772, 'score': 1}, {'doc_id': '14629765', 'title': 'Accurate prediction of V1 location from cortical folds in a surface coordinate system', 'abstract': ""Previous studies demonstrated substantial variability of the location of primary visual cortex (V1) in stereotaxic coordinates when linear volume-based registration is used to match volumetric image intensities [Amunts, K., Malikovic, A., Mohlberg, H., Schormann, T., and Zilles, K. (2000). Brodmann's areas 17 and 18 brought into stereotaxic space-where and how variable? Neuroimage, 11(1):66-84]. However, other qualitative reports of V1 location [Smith, G. (1904). The morphology of the occipital region of the cerebral hemisphere in man and the apes. Anatomischer Anzeiger, 24:436-451; Stensaas, S.S., Eddington, D.K., and Dobelle, W.H. (1974). The topography and variability of the primary visual cortex in man. J Neurosurg, 40(6):747-755; Rademacher, J., Caviness, V.S., Steinmetz, H., and Galaburda, A.M. (1993). Topographical variation of the human primary cortices: implications for neuroimaging, brain mapping, and neurobiology. Cereb Cortex, 3(4):313-329] suggested a consistent relationship between V1 and the surrounding cortical folds. Here, the relationship between folds and the location of V1 is quantified using surface-based analysis to generate a probabilistic atlas of human V1. High-resolution (about 200 microm) magnetic resonance imaging (MRI) at 7 T of ex vivo human cerebral hemispheres allowed identification of the full area via the stria of Gennari: a myeloarchitectonic feature specific to V1. Separate, whole-brain scans were acquired using MRI at 1.5 T to allow segmentation and mesh reconstruction of the cortical gray matter. For each individual, V1 was manually identified in the high-resolution volume and projected onto the cortical surface. Surface-based intersubject registration [Fischl, B., Sereno, M.I., Tootell, R.B., and Dale, A.M. (1999b). High-resolution intersubject averaging and a coordinate system for the cortical surface. Hum Brain Mapp, 8(4):272-84] was performed to align the primary cortical folds of individual hemispheres to those of a reference template representing the average folding pattern. An atlas of V1 location was constructed by computing the probability of V1 inclusion for each cortical location in the template space. This probabilistic atlas of V1 exhibits low prediction error compared to previous V1 probabilistic atlases built in volumetric coordinates. The increased predictability observed under surface-based registration suggests that the location of V1 is more accurately predicted by the cortical folds than by the shape of the brain embedded in the volume of the skull. In addition, the high quality of this atlas provides direct evidence that surface-based intersubject registration methods are superior to volume-based methods at superimposing functional areas of cortex and therefore are better suited to support multisubject averaging for functional imaging experiments targeting the cerebral cortex."", 'corpus_id': 14629765, 'score': 1}, {'doc_id': '212747887', 'title': 'Segmentation of brain tumor on magnetic resonance imaging using a convolutional architecture', 'abstract': 'The brain is a complex organ controlling cognitive process and physical functions. Tumors in the brain are accelerated cell growths affecting the normal function and processes in the brain. MRI scans provides detailed images of the body being one of the most common tests to diagnose brain tumors. The process of segmentation of brain tumors from magnetic resonance imaging can provide a valuable guide for diagnosis, treatment planning and prediction of results. Here we consider the problem brain tumor segmentation using a Deep learning architecture for use in tumor segmentation. Although the proposed architecture is simple and computationally easy to train, it is capable of reaching $IoU$ levels of 0.95.', 'corpus_id': 212747887, 'score': 0}, {'doc_id': '211204840', 'title': 'The Fluidity of Concept Representations in Human Brain Signals', 'abstract': 'Cognitive theories of human language processing often distinguish between concrete and abstract concepts. In this work, we analyze the discriminability of concrete and abstract concepts in fMRI data using a range of analysis methods. We find that the distinction can be decoded from the signal with an accuracy significantly above chance, but it is not found to be a relevant structuring factor in clustering and relational analyses. From our detailed comparison, we obtain the impression that human concept representations are more fluid than dichotomous categories can capture. We argue that fluid concept representations lead to more realistic models of human language processing because they better capture the ambiguity and underspecification present in natural language use.', 'corpus_id': 211204840, 'score': 0}, {'doc_id': '8366133', 'title': ""Predicting the location of human perirhinal cortex, Brodmann's area 35, from MRI"", 'abstract': ""The perirhinal cortex (Brodmann's area 35) is a multimodal area that is important for normal memory function. Specifically, perirhinal cortex is involved in the detection of novel objects and manifests neurofibrillary tangles in Alzheimer's disease very early in disease progression. We scanned ex vivo brain hemispheres at standard resolution (1 mm × 1 mm × 1 mm) to construct pial/white matter surfaces in FreeSurfer and scanned again at high resolution (120 μm × 120 μm × 120 μm) to determine cortical architectural boundaries. After labeling perirhinal area 35 in the high resolution images, we mapped the high resolution labels to the surface models to localize area 35 in fourteen cases. We validated the area boundaries determined using histological Nissl staining. To test the accuracy of the probabilistic mapping, we measured the Hausdorff distance between the predicted and true labels and found that the median Hausdorff distance was 4.0mm for the left hemispheres (n=7) and 3.2mm for the right hemispheres (n=7) across subjects. To show the utility of perirhinal localization, we mapped our labels to a subset of the Alzheimer's Disease Neuroimaging Initiative dataset and found decreased cortical thickness measures in mild cognitive impairment and Alzheimer's disease compared to controls in the predicted perirhinal area 35. Our ex vivo probabilistic mapping of the perirhinal cortex provides histologically validated, automated and accurate labeling of architectonic regions in the medial temporal lobe, and facilitates the analysis of atrophic changes in a large dataset for earlier detection and diagnosis."", 'corpus_id': 8366133, 'score': 1}]"
15	Just Targets	f2f769c22a3957814e0bcf15a57f9a56	4669	{}	"[{'doc_id': '218505804', 'title': 'Potential therapeutic targets and promising drugs for combating SARS‐CoV‐2', 'abstract': 'As of April 9, 2020, a novel coronavirus (SARS‐CoV‐2) had caused 89,931 deaths and 1,503,900 confirmed cases worldwide, which indicates an increasingly severe and uncontrollable situation. Initially, little was known about the virus. As research continues, we now know the genome structure, epidemiological and clinical characteristics, and pathogenic mechanisms of SARS‐CoV‐2. Based on this knowledge, potential targets involved in the processes of virus pathogenesis need to be identified, and the discovery or development of drugs based on these potential targets is the most pressing need. Here, we have summarized the potential therapeutic targets involved in virus pathogenesis and discuss the advances, possibilities, and significance of drugs based on these targets for treating SARS‐CoV‐2. This review will facilitate the identification of potential targets and provide clues for drug development that can be translated into clinical applications for combating SARS‐CoV‐2.', 'corpus_id': 218505804, 'score': 0}, {'doc_id': '3734821', 'title': 'AMPK as a Therapeutic Target for Treating Metabolic Diseases', 'abstract': 'The AMP-activated protein kinase (AMPK) is a central regulator of multiple metabolic pathways and may have therapeutic importance for treating obesity, insulin resistance, type 2 diabetes (T2D), non-alcoholic fatty liver disease (NAFLD), and cardiovascular disease (CVD). Given the ubiquitous expression of AMPK, it has been a challenge to evaluate which tissue types may be most beneficially poised for mediating the positive metabolic effects of AMPK-centered treatments. In this review we evaluate the metabolic phenotypes of transgenic mouse models in which AMPK expression and function have been manipulated, and the impact this has on controlling lipid metabolism, glucose homeostasis, and inflammation. This information may be useful for guiding the development of AMPK-targeted therapeutics to treat chronic metabolic diseases.', 'corpus_id': 3734821, 'score': 1}, {'doc_id': '218571737', 'title': 'Multi-omics study revealing tissue-dependent putative mechanisms of SARS-CoV-2 drug targets on viral infections and complex diseases', 'abstract': 'Drug target prioritisation for new targets and drug repurposing of existing drugs for COVID-19 treatment are urgently needed for the current pandemic. Here we pooled 353 candidate drug targets of COVID-19 from clinical trial registries and the literature and estimated their putative causal effects in 11 SARS-CoV-2 related tissues on 622 complex human diseases. By constructing a disease atlas of drug targets for COVID-19, we prioritise 726 target-disease associations as evidence of causality using robust Mendelian randomization (MR) and colocalization evidence (http://epigraphdb.org/covid-19/ctda/). Triangulating these MR findings with historic drug trial information and the druggable genome, we ranked and prioritised three genes DHODH, ITGB5 and JAK2 targeted by three marketed drugs (Leflunomide, Cilengitide and Baricitinib) which may have repurposing potential with careful risk assessment. This study evidences the value of our integrative approach in prioritising and repurposing drug targets, which will be particularly applicable when genetic association studies of COVID-19 are available in the near future.', 'corpus_id': 218571737, 'score': 0}, {'doc_id': '17347401', 'title': 'RDF-based schema mediation for database grid', 'abstract': 'In presence of grid where a huge amount of databases can be involved in sharing cycle, database tools and middlewares should be well suited for schema mediation and query processing in a semantically meaningful way. Dart is an implemented prototype system whose goal is to provide a semantic solution for database resource sharing capable of deployment in grid settings. This paper particularly concerns the problems of schema mediation in DartGrid. Our approach mainly involves the following notions: a) we use RDF/OWL to define the mediated ontologies for integration, b) we devise a set of rules for automatically converting the relational schema to RDF/OWL description called source data semantic, c) we define the source data semantic (source schema) as the view of shared ontologies (mediated schema), d) query is formulated and posed on the shared ontologies. A set of grid services is developed for the implementation of above functionalities.', 'corpus_id': 17347401, 'score': 0}, {'doc_id': '14101836', 'title': 'Open Targets: a platform for therapeutic target identification and validation', 'abstract': 'We have designed and developed a data integration and visualization platform that provides evidence about the association of known and potential drug targets with diseases. The platform is designed to support identification and prioritization of biological targets for follow-up. Each drug target is linked to a disease using integrated genome-wide data from a broad range of data sources. The platform provides either a target-centric workflow to identify diseases that may be associated with a specific target, or a disease-centric workflow to identify targets that may be associated with a specific disease. Users can easily transition between these target- and disease-centric workflows. The Open Targets Validation Platform is accessible at https://www.targetvalidation.org.', 'corpus_id': 14101836, 'score': 1}, {'doc_id': '218973162', 'title': 'Combating biothreat pathogens: ongoing efforts for countermeasure development and unique challenges', 'abstract': '\n \n Abstract\n \n Research to discover and develop antibacterial and antiviral drugs with potent activity against pathogens of biothreat concern presents unique methodological and process-driven challenges. Herein, we review laboratory approaches for finding new antibodies, antibiotics, and antiviral molecules for pathogens of biothreat concern. Using high-throughput screening techniques, molecules that directly inhibit a pathogen’s entry, replication, or growth can be identified. Alternatively, molecules that target host proteins can be interesting targets for development when countering biothreat pathogens, due to the modulation of the host immune response or targeting proteins that interfere with the pathways required by the pathogen for replication. Monoclonal and cocktail antibody therapies approved by the Food and Drug Administration for countering anthrax and under development for treatment of Ebola virus infection are discussed. A comprehensive tabular review of current in vitro, in vivo, pharmacokinetic and efficacy datasets has been presented for biothreat pathogens of greatest concern. Finally, clinical trials and animal rule or traditional drug approval pathways are also reviewed.\n Opinions; interpretations; conclusions; and recommendations are those of the authors and are not necessarily endorsed by the US Army.\n \n', 'corpus_id': 218973162, 'score': 0}, {'doc_id': '52328975', 'title': 'Therapeutic target database update 2018: enriched resource for facilitating bench-to-clinic research of targeted therapeutics', 'abstract': 'Abstract Extensive efforts have been directed at the discovery, investigation and clinical monitoring of targeted therapeutics. These efforts may be facilitated by the convenient access of the genetic, proteomic, interactive and other aspects of the therapeutic targets. Here, we describe an update of the Therapeutic target database (TTD) previously featured in NAR. This update includes: (i) 2000 drug resistance mutations in 83 targets and 104 target/drug regulatory genes, which are resistant to 228 drugs targeting 63 diseases (49 targets of 61 drugs with patient prevalence data); (ii) differential expression profiles of 758 targets in the disease-relevant drug-targeted tissue of 12 615 patients of 70 diseases; (iii) expression profiles of 629 targets in the non-targeted tissues of 2565 healthy individuals; (iv) 1008 target combinations of 1764 drugs and the 1604 target combination of 664 multi-target drugs; (v) additional 48 successful, 398 clinical trial and 21 research targets, 473 approved, 812 clinical trial and 1120 experimental drugs, and (vi) ICD-10-CM and ICD-9-CM codes for additional 482 targets and 262 drugs against 98 disease conditions. This update makes TTD more useful for facilitating the patient focused research, discovery and clinical investigations of the targeted therapeutics. TTD is accessible at http://bidd.nus.edu.sg/group/ttd/ttd.asp.', 'corpus_id': 52328975, 'score': 1}, {'doc_id': '215786352', 'title': 'Computational Drug Repositioning and Elucidation of Mechanism of Action of Compounds against SARS-CoV-2', 'abstract': 'The COVID-19 crisis called for rapid reaction from all the fields of biomedical research. Traditional drug development involves time consuming pipelines that conflict with the urgence of identifying effective therapies during a health and economic emergency. Drug repositioning, that is the discovery of new clinical applications for drugs already approved for different therapeutic contexts, could provide an effective shortcut to bring COVID-19 treatments to the bedside in a timely manner. Moreover, computational approaches can help accelerate the process even further. Here we present the application of computational drug repositioning tools based on transcriptomics data to identify drugs that are potentially able to counteract SARS-CoV-2 infection, and also to provide insights on their mode of action. We believe that mucolytics and HDAC inhibitors warrant further investigation. In addition, we found that the DNA Mismatch repair pathway is strongly modulated by drugs with experimental in vitro activity against SARS-CoV-2 infection. Both full results and methods are publicly available.', 'corpus_id': 215786352, 'score': 0}, {'doc_id': '33740170', 'title': 'The utility of target-based discovery', 'abstract': 'Even as biomedical science has advanced dramatically, concerns have arisen regarding the cost and efficiency of translating these advances into safe and efficacious new drugs. Two different strategies are part of the ongoing effort to advance drug discovery: target-based and phenotypic screening strategies [1]. Recombinant technology and genomics enabled targetbased drug discovery, using methods engineered with defined molecular targets. Genetics and molecular biology identify genes involved in disease, and recombinant technology enables their expression and testing against large compound libraries by high-throughput screening. With the sequencing of the human genome and continued genomic efforts, the number of molecular targets available for drug discovery has further increased. In phenotypic screening, drugs are identified without knowledge of or bias toward a specific molecular target, first identifying their pharmacological actions in cells, tissues, or animals. Antibiotics, for example have often been identified by their ability to kill bacteria or slow bacterial growth, without initial knowledge of the molecular targets involved.', 'corpus_id': 33740170, 'score': 1}, {'doc_id': '6180139', 'title': 'Opportunities and challenges in phenotypic drug discovery: an industry perspective', 'abstract': ""Phenotypic drug discovery (PDD) approaches do not rely on knowledge of the identity of a specific drug target or a hypothesis about its role in disease, in contrast to the target-based strategies that have been widely used in the pharmaceutical industry in the past three decades. However, in recent years, there has been a resurgence in interest in PDD approaches based on their potential to address the incompletely understood complexity of diseases and their promise of delivering first-in-class drugs, as well as major advances in the tools for cell-based phenotypic screening. Nevertheless, PDD approaches also have considerable challenges, such as hit validation and target deconvolution. This article focuses on the lessons learned by researchers engaged in PDD in the pharmaceutical industry and considers the impact of 'omics' knowledge in defining a cellular disease phenotype in the era of precision medicine, introducing the concept of a chain of translatability. We particularly aim to identify features and areas in which PDD can best deliver value to drug discovery portfolios and can contribute to the identification and the development of novel medicines, and to illustrate the challenges and uncertainties that are associated with PDD in order to help set realistic expectations with regard to its benefits and costs."", 'corpus_id': 6180139, 'score': 1}]"
16	sepsis	34e594fc5457473156e71dc8ae66f0ad	1154	{}	[{'doc_id': '12521635', 'title': 'A Modified Sequential Organ Failure Assessment Score for Critical Care Triage', 'abstract': 'ABSTRACT Objective: The Sequential Organ Failure Assessment (SOFA) score has been recommended for triage during a mass influx of critically ill patients, but it requires laboratory measurement of 4 parameters, which may be impractical with constrained resources. We hypothesized that a modified SOFA (MSOFA) score that requires only 1 laboratory measurement would predict patient outcome as effectively as the SOFA score. Methods: After a retrospective derivation in a prospective observational study in a 24-bed medical, surgical, and trauma intensive care unit, we determined serial SOFA and MSOFA scores on all patients admitted during the 2008 calendar year and compared the ability to predict mortality and the need for mechanical ventilation. Results: A total of 1770 patients (56% male patients) with a 30-day mortality of 10.5% were included in the study. Day 1 SOFA and MSOFA scores performed equally well at predicting mortality with an area under the receiver operating curve (AUC) of 0.83 (95% confidence interval 0.81-.85) and 0.84 (95% confidence interval 0.82-.85), respectively (P = .33 for comparison). Day 3 SOFA and MSOFA predicted mortality for the 828 patients remaining in the intensive care unit with an AUC of 0.78 and 0.79, respectively. Day 5 scores performed less well at predicting mortality. Day 1 SOFA and MSOFA predicted the need for mechanical ventilation on day 3, with an AUC of 0.83 and 0.82, respectively. Mortality for the highest category of SOFA and MSOFA score (>11 points) was 53% and 58%, respectively. Conclusions: The MSOFA predicts mortality as well as the SOFA and is easier to implement in resource-constrained settings, but using either score as a triage tool would exclude many patients who would otherwise survive. (Disaster Med Public Health Preparedness. 2010;4:277-284)', 'corpus_id': 12521635, 'score': 1}, {'doc_id': '206536998', 'title': 'New directions for sepsis and septic shock research.', 'abstract': 'BACKGROUND\nSeptic shock is a frequent complication in intensive care unit that can result in multiple organ failure and death. In addition, recent data suggested that severe sepsis and septic shock represent an economic burden. Therefore, septic shock is an important public health problem.\n\n\nMETHOD\nIn this review, we will focus on the recent evidences concerning the stages of septic shock, the complex macrocirculation and microcirculation relationship, and the importance of those evidences for future resuscitation goals and therapeutic strategies during late septic shock.\n\n\nRESULT\nRecently, two stages of septic shock are suggested. In early stage, hypovolemia is the main contributing factor. During this stage, macrocirculatory and microcirculatory changes run parallel, and fluid resuscitation seems to be effective in restoring the hemodynamic parameters. Late stage of septic shock is characterized by complex microcirculation and macrocirculation relationship.\n\n\nCONCLUSIONS\nAlthough early goal-directed therapy is a stepwise approach in the treatment of septic shock, tissue perfusion remains an important factor that contributes to septic shock outcome. Because appropriate monitoring of tissue perfusion is a matter of debt, the ideal therapeutic strategy remains a controversial issue that needs further investigations.', 'corpus_id': 206536998, 'score': 1}, {'doc_id': '166483091', 'title': 'Appraisal of Transport Policy and Strategic Plans in Canadian Urban Areas: A Survey of Planners and Policy-Makers in Government Agencies across Canada', 'abstract': 'This paper describes how, with the growing complexity of travel demand patterns as well as residential and firm location processes in Canadian urban areas, the need for integrated urban models is increasingly recognized to support transport policy appraisals. Note however, that the use of these models by planning organizations bears many challenges, notably related to the overwhelming amount of output and the complexities associated with processing and interpreting this information. This paper builds on earlier efforts aimed at proposing a framework for policy appraisal in the Greater Toronto Area (GTA) that involves a set of sustainable transport indicators derived from the results of integrated urban models thus assisting decision-making on sustainable transport plans. In an attempt to render the framework more “policy-sensitive”, a survey was initiated with planners and policy-makers both in the GTA and other Canadian cities. The survey aims at collecting information with respect to the current policy evaluation process of transport plans and its associated pitfalls as well as the desired state of policy appraisal and the need for more formal evaluation tools. Participants’ reactions towards the adoption of sustainable transport indicators in policy assessment are also solicited.', 'corpus_id': 166483091, 'score': 0}, {'doc_id': '216144995', 'title': 'ICU and ventilator mortality among critically ill adults with COVID-19', 'abstract': 'We report preliminary data from a cohort of adults admitted to COVID-designated intensive care units from March 6 through April 17, 2020 across an academic healthcare system. Among 217 critically ill patients, mortality for those who required mechanical ventilation was 29.7% (49/165), with 8.5% (14/165) of patients still on the ventilator at the time of this report. Overall mortality to date in this critically ill cohort is 25.8% (56/217), and 40.1% (87/217) patients have survived to hospital discharge. Despite multiple reports of mortality rates exceeding 50% among critically ill adults with COVID-19, particularly among those requiring mechanical ventilation, our early experience indicates that many patients survive their critical illness.', 'corpus_id': 216144995, 'score': 0}, {'doc_id': '22341051', 'title': 'Sepsis Pathophysiology, Chronic Critical Illness, and Persistent Inflammation-Immunosuppression and Catabolism Syndrome', 'abstract': 'Objectives: To provide an appraisal of the evolving paradigms in the pathophysiology of sepsis and propose the evolution of a new phenotype of critically ill patients, its potential underlying mechanism, and its implications for the future of sepsis management and research. Design: Literature search using PubMed, MEDLINE, EMBASE, and Google Scholar. Measurements and Main Results: Sepsis remains one of the most debilitating and expensive illnesses, and its prevalence is not declining. What is changing is our definition(s), its clinical course, and how we manage the septic patient. Once thought to be predominantly a syndrome of over exuberant inflammation, sepsis is now recognized as a syndrome of aberrant host protective immunity. Earlier recognition and compliance with treatment bundles has fortunately led to a decline in multiple organ failure and in-hospital mortality. Unfortunately, more and more sepsis patients, especially the aged, are suffering chronic critical illness, rarely fully recover, and often experience an indolent death. Patients with chronic critical illness often exhibit “a persistent inflammation-immunosuppression and catabolism syndrome,” and it is proposed here that this state of persisting inflammation, immunosuppression and catabolism contributes to many of these adverse clinical outcomes. The underlying cause of inflammation-immunosuppression and catabolism syndrome is currently unknown, but there is increasing evidence that altered myelopoiesis, reduced effector T-cell function, and expansion of immature myeloid-derived suppressor cells are all contributory. Conclusions: Although newer therapeutic interventions are targeting the inflammatory, the immunosuppressive, and the protein catabolic responses individually, successful treatment of the septic patient with chronic critical illness and persistent inflammation-immunosuppression and catabolism syndrome may require a more complementary approach.', 'corpus_id': 22341051, 'score': 1}, {'doc_id': '1788772', 'title': 'Microcirculatory dysfunction in sepsis: pathophysiology, clinical monitoring, and potential therapies.', 'abstract': 'Abnormal microvascular perfusion, including decreased functional capillary density and increased blood flow heterogeneity, is observed in early stages of the systemic inflammatory response to infection and appears to have prognostic significance in human sepsis. It is known that improvements in systemic hemodynamics are weakly correlated with the correction of microcirculatory parameters, despite an appropriate treatment of macrohemodynamic abnormalities. Furthermore, conventional hemodynamic monitoring systems available in clinical practice fail to detect microcirculatory parameter changes and responses to treatments, as they do not evaluate intrinsic events that occur in the microcirculation. Fortunately, some bedside diagnostic methods and therapeutic options are specifically directed to the assessment and treatment of microcirculatory changes. In the present review we discuss fundamental aspects of septic microcirculatory abnormalities, including pathophysiology, clinical monitoring, and potential therapies.', 'corpus_id': 1788772, 'score': 1}, {'doc_id': '215782423', 'title': 'Development and external validation of a prognostic multivariable model on admission for hospitalized patients with COVID-19', 'abstract': 'Background: COVID-19 pandemic has developed rapidly and the ability to stratify the most vulnerable patients is vital. However, routinely used severity scoring systems are often low on diagnosis, even in non-survivors. Therefore, clinical prediction models for mortality are urgently required. Methods: We developed and internally validated a multivariable logistic regression model to predict inpatient mortality in COVID-19 positive patients using data collected retrospectively from Tongji Hospital, Wuhan (299 patients). External validation was conducted using a retrospective cohort from Jinyintan Hospital, Wuhan (145 patients). Nine variables commonly measured in these acute settings were considered for model development, including age, biomarkers and comorbidities. Backwards stepwise selection and bootstrap resampling were used for model development and internal validation. We assessed discrimination via the C statistic, and calibration using calibration-in-the-large, calibration slopes and plots. Findings: The final model included age, lymphocyte count, lactate dehydrogenase and SpO2 as independent predictors of mortality. Discrimination of the model was excellent in both internal (c=0.89) and external (c=0.98) validation. Internal calibration was excellent (calibration slope=1). External validation showed some over-prediction of risk in low-risk individuals and under-prediction of risk in high-risk individuals prior to recalibration. Recalibration of the intercept and slope led to excellent performance of the model in independent data. Interpretation: COVID-19 is a new disease and behaves differently from common critical illnesses. This study provides a new prediction model to identify patients with lethal COVID-19. Its practical reliance on commonly available parameters should improve usage of limited healthcare resources and patient survival rate.', 'corpus_id': 215782423, 'score': 0}, {'doc_id': '215415301', 'title': 'OPTIMIZATION OF SEPSIS DIAGNOSIS AND TREATMENT IN YOUNG CHILDREN', 'abstract': 'Introduction. Early diagnosis of sepsis allows to make a diagnosis on time, correctly assess the condition of young children, and start timely treatment. The article analyzes the diagnostic potential of new early marker like procalcitonin. Material and methods. The research involved 82 children with the diagnosis of sepsis, severe sepsis, or septic shock, who were under observation in the Tashkent Medical Academy. The study population constituted all children from newborns to 3 years old patients. All patients were divided into documented bacterial (n=22) versus abacterial inflammation (n=60) infections in order to assess serum PCT Central Asian Journal of Medicine 1 Karimdjanov et al.: OPTIMIZATION OF SEPSIS DIAGNOSIS AND TREATMENT IN YOUNG CHILDREN Published by 2030 Uzbekistan Research Online, 2019 Central Asian Journal of Medicine uzjournals.edu.uz/tma 191 2020#1 concentrations with a cutoff value of >0.5 ng/mL. The traditionally widely used biomarkers of sepsis are cytokines, CRP and PCT. Results and discussion. In the process of sepsis monitoring, procalcitonin unlike other markers, reliably reflects the real dynamics of its severity, quickly and adequately changes depending on the effectiveness of therapy, predicts relapses of sepsis after remission, when the clinical signs of sepsis and procalcitonin levels normalize. With surgical pathology, injuries and burns in the absence of an infection, procalcitonin does not increase. Early diagnosis of sepsis is difficult issue in pediatric practice, whilst it can be vital for positive patient outcomes in sepsis management. Any delay in diagnosis and treatment may bring on multiple organ failure and can be hazardous with elevated mortality consequences. Early diagnosis and effective management of sepsis not only gives an opportunity for prompt antibiotic therapy and a potential decrease in mortality, it can also belittle the unnecessary use of antibiotics. Conclusion. A review of the results of international and domestic studies suggest that procalcitonin is an effective method for the early diagnosis and monitoring of systemic infections of young children.', 'corpus_id': 215415301, 'score': 0}, {'doc_id': '206909867', 'title': 'Sepsis: pathophysiology and clinical management', 'abstract': 'Sepsis, severe sepsis, and septic shock represent increasingly severe systemic inflammatory responses to infection. Sepsis is common in the aging population, and it disproportionately affects patients with cancer and underlying immunosuppression. In its most severe form, sepsis causes multiple organ dysfunction that can produce a state of chronic critical illness characterized by severe immune dysfunction and catabolism. Much has been learnt about the pathogenesis of sepsis at the molecular, cell, and intact organ level. Despite uncertainties in hemodynamic management and several treatments that have failed in clinical trials, investigational therapies increasingly target sepsis induced organ and immune dysfunction. Outcomes in sepsis have greatly improved overall, probably because of an enhanced focus on early diagnosis and fluid resuscitation, the rapid delivery of effective antibiotics, and other improvements in supportive care for critically ill patients. These improvements include lung protective ventilation, more judicious use of blood products, and strategies to reduce nosocomial infections.', 'corpus_id': 206909867, 'score': 1}, {'doc_id': '4467814', 'title': 'Procedural Aggressiveness in Veterans with Advanced Non-Small-Cell Lung Cancer at the End of Life.', 'abstract': 'BACKGROUND\nEvidence suggests that the aggressiveness of care in cancer patients at the end of life is increasing. We sought to evaluate the use of invasive procedures at the end of life in patients with advanced non-small-cell lung cancer (NSCLC).\n\n\nOBJECTIVE\nTo evaluate the utilization of invasive procedures at the end of life in Veterans with advanced NSCLC.\n\n\nDESIGN\nRetrospective cohort study of Veterans with newly diagnosed stage IV NSCLC who died between 2006 and 2012.\n\n\nSETTING/SUBJECTS\nSubjects were identified from the Veterans Affairs Central Cancer Registry.\n\n\nMEASUREMENTS\nAll Veterans Administration (VA) and Medicare fee-for-service healthcare utilization and expenditure data were assembled for all subjects. The primary outcome was the number of invasive procedures performed in the last month of life. We classified procedures into three categories: minimally invasive, life-sustaining, and major-operative procedures. Logistic regression analysis was used to evaluate factors associated with the receipt of invasive procedures.\n\n\nRESULTS\nNineteen thousand nine hundred thirty subjects were included. Three thousand (15.1%) subjects underwent 5523 invasive procedures during the last month of life. The majority of procedures (69.6%) were classified as minimally invasive. The receipt of procedures in the last month of life was associated with receipt of chemotherapy (odds ratio [OR] 3.68, 95% confidence interval [CI] 3.38-4.0) and ICU admission (OR 3.13, 95% CI 2.83-3.45) and was inversely associated with use of hospice services (OR 0.35, 95% CI 0.33-0.38).\n\n\nCONCLUSIONS\nInvasive procedures are commonly performed among Veterans with stage IV NSCLC during their last month of life and are associated with other measures of aggressive end-of-life care.', 'corpus_id': 4467814, 'score': 0}]
17	Mindfulness	44be2a3f9cfc36a438b08e20c04edab4	12955	{}	"[{'doc_id': '231639566', 'title': 'Real-Time Telehealth Treatment Team Consultation for Self-Injury by Individuals with Autism Spectrum Disorder', 'abstract': 'Objectives Self-injurious behavior (SIB) refers to any repeated self-directed, non-suicidal, behavior that may cause or has the potential to cause physical harm to the person’s body. Behavioral interventions provide the standard evidence-based treatments for SIB by people with autism spectrum disorder (ASD) and intellectual disabilities (ID). Translating the proven effectiveness of behavioral interventions to treatment of self-injury in community settings by clinicians and caregivers has not been totally successful. The aim of the present study was to advance translational research by providing real-time telehealth consultation to a treatment team at a community-based mental health agency that provided inpatient and outpatient services to individuals with ASD and ID. Method The participants of this single-case experimental study were three adolescents with ASD who had been referred for services because of their increasingly unmanageable SIB both at home and at school. The telehealth consultant provided real-time assistance to the treatment team within a translational model of care in the development and implementation of a behavior support plan and an informal mindfulness-based Soles of the Feet (SoF) program. Results Both visual and statistical analyses demonstrated reductions in the frequency of SIB for all three adolescents, with overall clinically significant reductions only with the SoF intervention. Conclusion The results of this translational study suggest that telehealth consultation might be a viable technological alternative in situations which preclude face-to-face consultation. Telehealth consultation could be one method of supporting people with behavioral difficulties during pandemics, such as COVID-19.', 'corpus_id': 231639566, 'score': 0}, {'doc_id': '2755919', 'title': 'Mindfulness: Theoretical Foundations and Evidence for its Salutary Effects', 'abstract': 'Interest in mindfulness and its enhancement has burgeoned in recent years. In this article, we discuss in detail the nature of mindfulness and its relation to other, established theories of attention and awareness in day-to-day life. We then examine theory and evidence for the role of mindfulness in curtailing negative functioning and enhancing positive outcomes in several important life domains, including mental health, physical health, behavioral regulation, and interpersonal relationships. The processes through which mindfulness is theorized to have its beneficial effects are then discussed, along with proposed directions for theoretical development and empirical research.', 'corpus_id': 2755919, 'score': 1}, {'doc_id': '231646379', 'title': 'Robert S. Wallerstein Lecture in Psychoanalysis and Psychotherapy', 'abstract': 'Each year, the UCSF Department of Psychiatry and Behavioral Sciences invites a distinguished scholar to speak on campus as part of the Robert S. Wallerstein Visiting Lectureship in Psychoanalysis and Psychotherapy. This lecture series is held in honor of the late Robert S. Wallerstein, MD, and focuses on showcasing psychoanalytic knowledge and clinical expertise that influence psychiatry, psychotherapy, and psychoanalysis.', 'corpus_id': 231646379, 'score': 0}, {'doc_id': '22194749', 'title': 'The varieties of contemplative experience: A mixed-methods study of meditation-related challenges in Western Buddhists', 'abstract': 'Buddhist-derived meditation practices are currently being employed as a popular form of health promotion. While meditation programs draw inspiration from Buddhist textual sources for the benefits of meditation, these sources also acknowledge a wide range of other effects beyond health-related outcomes. The Varieties of Contemplative Experience study investigates meditation-related experiences that are typically underreported, particularly experiences that are described as challenging, difficult, distressing, functionally impairing, and/or requiring additional support. A mixed-methods approach featured qualitative interviews with Western Buddhist meditation practitioners and experts in Theravāda, Zen, and Tibetan traditions. Interview questions probed meditation experiences and influencing factors, including interpretations and management strategies. A follow-up survey provided quantitative assessments of causality, impairment and other demographic and practice-related variables. The content-driven thematic analysis of interviews yielded a taxonomy of 59 meditation-related experiences across 7 domains: cognitive, perceptual, affective, somatic, conative, sense of self, and social. Even in cases where the phenomenology was similar across participants, interpretations of and responses to the experiences differed considerably. The associated valence ranged from very positive to very negative, and the associated level of distress and functional impairment ranged from minimal and transient to severe and enduring. In order to determine what factors may influence the valence, impact, and response to any given experience, the study also identified 26 categories of influencing factors across 4 domains: practitioner-level factors, practice-level factors, relationships, and health behaviors. By identifying a broader range of experiences associated with meditation, along with the factors that contribute to the presence and management of experiences reported as challenging, difficult, distressing or functionally impairing, this study aims to increase our understanding of the effects of contemplative practices and to provide resources for mediators, clinicians, meditation researchers, and meditation teachers.', 'corpus_id': 22194749, 'score': 1}, {'doc_id': '231646746', 'title': 'CORRELATION OF DYSPNEA WITH AGE AND SPO2 LEVELS IN COVID-19 AND EFFECTIVENESS OF NEUROPHYSIOLOGICAL FACILITATION IN THE MANAGEMENT OF DYSPNEA-A RANDOMIZED CLINICAL CONTROL TRAIL', 'abstract': 'Suraj kuma, Gowrishankar Pottur, Sangh Mitra, Ajay Kumar Kushwaha, Pramod Kuma, Shailendra Pal Singh 1 Department of Physiotherapy, Faculty of Paramedical sciences, Uttar Pradesh University of Medical Sciences, Saifai, Etawah, U.P, India 2 MBBS, DGO. Awantibai Women Hospital, Lucknow, Uttar Pradesh, India 3 Department of Physiotherapy, Uttar Pradesh University of Medical Sciences, Saifai, Etawah, U.P, India 4 Physiotherapist, Department of General Surgery, Uttar Pradesh University of Medical Sciences, Saifai, Etawah, U.P, India', 'corpus_id': 231646746, 'score': 0}, {'doc_id': '2670798', 'title': 'Mindfulness-Based Interventions in Context: Past, Present, and Future', 'abstract': 'studies from the Center for Mindfulness in Medicine, Health Care, and Society not reviewed by Baer but which raise a number of key questions about clinical applicability, study design, and mechanism of action, and (7) current opportunities for professional training and development in mindfulness and its clinical applications.', 'corpus_id': 2670798, 'score': 1}, {'doc_id': '12955342', 'title': 'Mindfulness : A proposed operational definition', 'abstract': 'There has been substantial interest in mindfulness as an approach to reduce cognitive vulnerability to stress and emotional distress in recent years. However, thus far mindfulness has not been defined operationally. This paper describes the results of recent meetings held to establish a consensus on mindfulness and to develop conjointly a testable operational definition. We propose a two-component model of mindfulness and specify each component in terms of specific behaviors, experiential manifestations, and implicated psychological processes. We then address issues regarding temporal stability and situational specificity and speculate on the conceptual and operational distinctiveness of mindfulness. We conclude this paper by discussing implications for instrument development and briefly describing our own approach to measurement.', 'corpus_id': 12955342, 'score': 1}, {'doc_id': '231652404', 'title': 'Editorial for special issue on neglect rehabilitation.', 'abstract': ""It is clear already that in current and future years more people will suffer from stroke, whether related to COVID-19 or not, and given its prevalence, many more people's lives will be affected by neglect. Here we hope to have contributed to its possible amelioration with highlights of the latest thinking on neglect diagnosis, prevalence and treatment."", 'corpus_id': 231652404, 'score': 0}, {'doc_id': '204811481', 'title': 'PROMISE: A Model of Insight and Equanimity as the Key Effects of Mindfulness Meditation', 'abstract': 'In a comprehensive meta-analysis on the effects of mindfulness meditation, Eberth and Sedlmeier (2012) identified a multitude of positive effects that covered a wide range of psychological variables, such as heightened mindfulness as measured through contemporary mindfulness scales, reduced negative emotions, increased positive emotions, changes in self-concept, enhanced attention, perception, and wellbeing, improved interpersonal abilities, and a reduction of negative personality traits. The present research aimed at developing and testing a comprehensive model explaining the wide range of mindfulness meditation effects and their temporal and causal relationships. In Study 1, interviews with meditators at different levels of experience were analyzed using a grounded theory procedure. The resulting model was triangulated and refined by concepts from both Western research and ancient Buddhist scriptures. The model developed highlights equanimity (reduction in emotional reactivity) and insight (alteration of cognitions) as the two key effects of mindfulness meditation that eventually lead to increased wellbeing. The model was pilot-tested with a large sample of meditators and non-meditators in Study 2. Data showed an acceptable fit with the model and indicated that meditators and non-meditators score significantly differently on the model’s core categories.', 'corpus_id': 204811481, 'score': 1}, {'doc_id': '231644467', 'title': 'Perceptions, Experiences, and Challenges of Physicians Involved in Dementia Care During the COVID-19 Lockdown in India: A Qualitative Study', 'abstract': 'Introduction: With 5.3 million people living with dementia in India and the pandemic wreaking havoc, dementia care has faced unique challenges during the outbreak, with reduced healthcare access, travel restriction, long-term lockdown and fear of hospitalization. We explored the experiences and barriers faced by the physicians involved in dementia care during the lockdown period. Methods: A qualitative approach was used with purposive sampling. After an initial pilot, 148 physicians were included in the study. They were virtually interviewed in-depth based on a pre-designed semi-structured questionnaire, in areas related to tele-consultations, attributes related to dementia care, challenges faced and way forward. Interviews were recorded, transcribed and thematically analyzed using Nvivo-10 software. Triangulation, peer debriefing and respondent validation were used to ensure rigor. Results: The overarching categories that emerged were “Tele-medicine as the future of dementia care in India,” “people living with dementia being uniquely susceptible to the pandemic with a triple burden of: age, ageism and lack of autonomy” and “markedly reduced healthcare access in this population with significant mental health burden of caregivers.” The experiences of the physicians were categorized into their challenges during the lockdown period and perceptions related to specific facets of dementia care during the crisis. The general physicians expressed special “unmet needs” of dementia-specific training and specialist collaboration. Most of the participants perceived ambiguity related to the newly released telepsychiatry guidelines. Conclusion: Resource constraints and pandemic burden are currently high. This study looks at the “voices” of those actively providing dementia care during the ongoing crisis and to the best of our knowledge, is the first one from India to do so. Concurring with their experiences, PwD and their families are exposed to multiple vulnerabilities during COVID-19, need tailored care, especially at the primary healthcare level which includes general physicians. These relevant “voices” are discussed in light of the new tele-psychiatry guidelines and further optimization of dementia care in an aging India.', 'corpus_id': 231644467, 'score': 0}]"
18	"proliferative kidney disease"" -lupus"	f3b8653ffa1f999f8f8176183cbe9191	12748	{}	"[{'doc_id': '226678154', 'title': 'Pathological effect of infectious bronchitis disease virus on broiler chicken trachea and kidney tissues', 'abstract': 'Aim: This study aimed to investigate the pathological effects of the infectious bronchitis virus (IBV) on chicken trachea and kidney tissues and also desired to diagnose the virus genome using a molecular tool. Materials and Methods: Twenty trachea and kidney samples collected from one broiler farm contain 10,000 chickens at Tikrit city. The chickens showed signs of gasping and mortality (20%) at early ages (20 days old), the presence of IBV investigated using conventional reverse transcriptase-polymerase chain reaction technique with routine histopathological study to tracheal and renal tissue. Results: Postmortem lesion showed severe respiratory inflammation with abscesses at tracheal bifurcation lead to airway blog. Molecular results showed two genotypes of IBV, one of them not included in primer designer research. The histological study showed different stages of inflammation, degeneration, and necrosis to the renal and tracheal tissues. Conclusion: The respiratory and renal pathological effect of the virus responsible for the symptoms appeared on the affected chicks that caused mortality, with a high probability of presence of a new viral genotype added to the untranslated region.', 'corpus_id': 226678154, 'score': 0}, {'doc_id': '87458907', 'title': 'First record of proliferative kidney disease in Iceland.', 'abstract': 'Proliferative kidney disease caused by the myxozoan parasite Tetracapsuloides bryosalmonae is reported for the first time in Iceland. Infections were confirmed in both arctic charr and brown trout but only arctic charr showed clinical signs. The last two decades, populations of arctic charr in several lakes in Iceland have greatly declined. Possible relation of this decline with increasing water temperature has been speculated. It is hypothesized that PKD may play a significant role in this decline. Studies on the distribution of PKD and its effect on wild populations of arctic charr and brown trout in Iceland are presently in progress.', 'corpus_id': 87458907, 'score': 1}, {'doc_id': '46782103', 'title': 'Temperature-related parasite infection dynamics: the case of proliferative kidney disease of brown trout', 'abstract': 'SUMMARY Climate change, in particular rising temperature, is suspected to be a major driver for the emergence of many wildlife diseases. Proliferative kidney disease of salmonids, caused by the myxozoan Tetracapsuloides bryosalmonae, was used to evaluate how temperature dependence of host–parasite interactions modulates disease emergence. Brown trout (Salmo trutta fario) kept at 12 and 15 °C, were experimentally infected with T. bryosalmonae. Parasite development in the fish host and release of spores were quantified simultaneously to unravel parasite transmission potential from the vertebrate to the invertebrate host. A change to a stable plateau in infection intensity of the kidney coincided with a threshold at which spore shedding commenced. This onset of parasite release was delayed at the low temperature in accordance with reaching this infection intensity threshold, but the amount of spores released was irrespective of temperature. The production of parasite transmission stages declined with time. In conclusion, elevated temperature modifies the parasite transmission opportunities by increasing the duration of transmission stage production, which may affect the spread and establishment of the parasite in a wider range of rivers.', 'corpus_id': 46782103, 'score': 1}, {'doc_id': '33038671', 'title': 'Life cycle complexity, environmental change and the emerging status of salmonid proliferative kidney disease', 'abstract': ""P>1. Proliferative kidney disease (PKD) is a disease of salmonid fish caused by the endoparasitic myxozoan, Tetracapsuloides bryosalmonae, which uses freshwater bryozoans as primary hosts. Clinical PKD is characterised by a temperature-dependent proliferative and inflammatory response to parasite stages in the kidney.;2. Evidence that PKD is an emerging disease includes outbreaks in new regions, declines in Swiss brown trout populations and the adoption of expensive practices by fish farms to reduce heavy losses. Disease-related mortality in wild fish populations is almost certainly underestimated because of e.g. oversight, scavenging by wild animals, misdiagnosis and fish stocking.;3. PKD prevalences are spatially and temporally variable, range from 0 to 90-100% and are typically highest in juvenile fish.;4. Laboratory and field studies demonstrate that (i) increasing temperatures enhance disease prevalence, severity and distribution and PKD-related mortality; (ii) eutrophication may promote outbreaks. Both bryozoans and T. bryosalmonae stages in bryozoans undergo temperature- and nutrient-driven proliferation.;5. Tetracapsuloides bryosalmonae is likely to achieve persistent infection of highly clonal bryozoan hosts through vertical transmission, low virulence and host condition-dependent cycling between covert and overt infections. Exploitation of fish hosts entails massive proliferation and spore production by stages that escape the immune response. Many aspects of the parasite's life cycle remain obscure. If infectious stages are produced in all hosts then the complex life cycle includes multiple transmission routes.;6. Patterns of disease outbreaks suggest that background, subclinical infections exist under normal environmental conditions. When conditions change, outbreaks may then occur in regions where infection was hitherto unsuspected.;7. Environmental change is likely to cause PKD outbreaks in more northerly regions as warmer temperatures promote disease development, enhance bryozoan biomass and increase spore production, but may also reduce the geographical range of this unique multihost-parasite system. Coevolutionary dynamics resulting from host-parasite interactions that maximise fitness in previous environments may pose problems for sustainability, particularly in view of extensive declines in salmonid populations and degradation of many freshwater habitats."", 'corpus_id': 33038671, 'score': 1}, {'doc_id': '225057174', 'title': 'Identification of silkworm hemocyte subsets and analysis of their response to BmNPV infection based on single-cell RNA sequencing', 'abstract': 'A wide range of hemocyte types exist in insects but a full definition of the different subclasses is not yet established. The current knowledge of the classification of silkworm hemocytes mainly comes from morphology rather than specific markers, so our understanding of the detailed classification, hemocyte lineage and functions of silkworm hemocytes is very incomplete. Bombyx mori nucleopolyhedrovirus (BmNPV) is a representative member of the baculoviruses, which are a major pathogens that specifically infects silkworms and cause serious loss in sericulture industry. Here, we performed single-cell RNA sequencing (scRNA-seq) of silkworm hemocytes in BmNPV and mock-infected larvae to comprehensively identify silkworm hemocyte subsets and determined specific molecular and cellular characteristics in each hemocyte subset before and after viral infection. A total of 19 cell clusters and their potential marker genes were identified in silkworm hemocytes. Among these hemocyte clusters, clusters 0, 1, 2, 5 and 9 might be granulocytes (GR); clusters 14 and 17 were predicted as plasmatocytes (PL); cluster 18 was tentatively identified as spherulocytes (SP); and clusters 7 and 11 could possibly correspond to oenocytoids (OE). In addition, all of the hemocyte clusters were infected by BmNPV and some infected cells carried high viral-load in silkworm larvae at 3 day post infection (dpi). Interestingly, BmNPV infection can cause severe and diverse changes in gene expression in hemocytes. Cells belonging to the infection group mainly located at the early stage of the pseudotime trajectories. Furthermore, we found that BmNPV infection suppresses the immune response in the major hemocyte types. In summary, our scRNA-seq analysis revealed the diversity of silkworm hemocytes and provided a rich resource of gene expression profiles for a systems-level understanding of their functions in the uninfected condition and as a response to BmNPV.', 'corpus_id': 225057174, 'score': 0}, {'doc_id': '226958901', 'title': 'Leprosy in wild chimpanzees', 'abstract': 'Humans are considered the main host for Mycobacterium leprae, the aetiologic agent of leprosy, but spill-over to other mammals such as nine-banded armadillos and red squirrels occurs. Although naturally acquired leprosy has also been described in captive nonhuman primates, the exact origins of infection remain unclear. Here, we report on leprosy-like lesions in two wild populations of western chimpanzees (Pan troglodytes verus) in the Cantanhez National Park, Guinea-Bissau, and the Taï National Park, Côte d’Ivoire, West Africa. Longitudinal monitoring of both populations revealed the progression of disease symptoms compatible with advanced leprosy. Screening of faecal and necropsy samples confirmed the presence of M. leprae as the causative agent at each site and phylogenomic comparisons with other strains from humans and other animals show that the chimpanzee strains belong to different and rare genotypes (4N/O and 2F). The independent evolutionary origin of M. leprae in two geographically distant populations of wild chimpanzees, with no prolonged direct contact with humans, suggests multiple introductions of M. leprae from an unknown animal or environmental source.', 'corpus_id': 226958901, 'score': 0}, {'doc_id': '227157696', 'title': 'Fatal Interstitial Pneumonia Associated with Bovine Coronavirus in Cows from Southern Italy', 'abstract': 'An outbreak of winter dysentery, complicated by severe respiratory syndrome, occurred in January 2020 in a high production dairy cow herd located in a hilly area of the Calabria region. Of the 52 animals belonging to the farm, 5 (9.6%) died with severe respiratory distress, death occurring 3–4 days after the appearance of the respiratory signs (caught and gasping breath). Microbiological analysis revealed absence of pathogenic bacteria whilst Real-time PCR identified the presence of RNA from Bovine Coronavirus (BCoV) in several organs: lungs, small intestine (jejunum), mediastinal lymph nodes, liver and placenta. BCoV was therefore hypothesized to play a role in the lethal pulmonary infection. Like the other CoVs, BCoV is able to cause different syndromes. Its role in calf diarrhea and in mild respiratory disease is well known: we report instead the involvement of this virus in a severe and fatal respiratory disorder, with symptoms and disease evolution resembling those of Severe Acute Respiratory Syndromes (SARS).', 'corpus_id': 227157696, 'score': 0}, {'doc_id': '226258233', 'title': 'Application of Humanized Zebrafish Model in the Suppression of SARS-CoV-2 Spike Protein Induced Pathology by Tri-Herbal Medicine Coronil via Cytokine Modulation', 'abstract': 'Zebrafish has been a reliable model system for studying human viral pathologies. SARS-CoV-2 viral infection has become a global chaos, affecting millions of people. There is an urgent need to contain the pandemic and develop reliable therapies. We report the use of a humanized zebrafish model, xeno-transplanted with human lung epithelial cells, A549, for studying the protective effects of a tri-herbal medicine Coronil. At human relevant doses of 12 and 58 µg/kg, Coronil inhibited SARS-CoV-2 spike protein, induced humanized zebrafish mortality, and rescued from behavioral fever. Morphological and cellular abnormalities along with granulocyte and macrophage accumulation in the swim bladder were restored to normal. Skin hemorrhage, renal cell degeneration, and necrosis were also significantly attenuated by Coronil treatment. Ultra-high-performance liquid chromatography (UHPLC) analysis identified ursolic acid, betulinic acid, withanone, withaferine A, withanoside IV–V, cordifolioside A, magnoflorine, rosmarinic acid, and palmatine as phyto-metabolites present in Coronil. In A549 cells, Coronil attenuated the IL-1β induced IL-6 and TNF-α cytokine secretions, and decreased TNF-α induced NF-κB/AP-1 transcriptional activity. Taken together, we show the disease modifying immunomodulatory properties of Coronil, at human equivalent doses, in rescuing the pathological features induced by the SARS-CoV-2 spike protein, suggesting its potential use in SARS-CoV-2 infectivity.', 'corpus_id': 226258233, 'score': 0}, {'doc_id': '84279991', 'title': 'Ultrastructure of a haplosporean-like organism: the possible causative agent of proliferative kidney disease in rainbow trout.', 'abstract': 'The protozoan parasite which has been suggested as the causative agent of proliferative kidney disease in rainbow trout has previously been described as an‘amoeba’. However, this ultra-structural study of the organism (named‘PKX’) has demonstrated many similarities to a group of invertebrate parasites, the genus Marleilia (currently classified in the Haplosporea). The suggested affinity of PKX‘ to Marteilia is based on the presence of internal cleavage,‘haplosporosomes’, and an amorphous cell wall. Other cytoplasmic inclusions also show characteristics of the genus. Since a spore stage has not been recognised in ‘PKX’, a definitive taxonomic statement cannot yet be made.', 'corpus_id': 84279991, 'score': 1}, {'doc_id': '25952744', 'title': 'Proliferative kidney disease in rainbow trout: time- and temperature-related renal pathology and parasite distribution.', 'abstract': 'Proliferative kidney disease is a parasitic infection of salmonid fishes caused by Tetracapsuloides bryosalmonae. The main target organ of the parasite in the fish is the kidney. To investigate the influence of water temperature on the disease in fish, rainbow trout Oncorhynchus mykiss infected with T bryosalmonae were kept at 12 degrees C and 18 degrees C. The number of parasites, the type and degree of lesions in the kidney and the mortality rate was evaluated from infection until full development of disease. While mortality stayed low at 12 degrees C, it reached 77% at 18 degrees C. At 12 degrees C, pathological lesions were dominated by a multifocal proliferative and granulomatous interstitial nephritis. This was accompanied by low numbers of T. bryosalmonae, mainly located in the interstitial lesions. With progression of the disease, small numbers of parasites appeared in the excretory tubuli, and parasite DNA was detected in the urine. Parasite degeneration in the interstitium was observed at late stages of the disease. At 18 degrees C, pathological lesions in kidneys were more severe and more widely distributed, and accompanied by significantly higher parasite numbers. Distribution of parasites in the renal compartments, onset of parasite degeneration and time course of appearance of parasite DNA in urine were not clearly different from the 12 degrees C group. These findings indicate that higher mortality at 18 degrees C compared to 12 degrees C is associated with an enhanced severity of renal pathology and increased parasite numbers.', 'corpus_id': 25952744, 'score': 1}]"
19	epiCata	68ba55fc850fcadcc1af73289e756dfe	19008	{}	"[{'doc_id': '233004592', 'title': 'Weekly Bayesian Modelling Strategy to Predict Deaths by COVID-19: a Model and Case Study for the State of Santa Catarina, Brazil', 'abstract': 'Background: The novel coronavirus pandemic has affected Brazil\'s Santa Catarina State (SC) severely. At the time of writing (24 March 2021), over 764,000 cases and over 9,800 deaths by COVID-19 have been confirmed, hospitals were fully occupied with local news reporting at least 397 people in the waiting list for an ICU bed. In an attempt to better inform local policy making, we applied an existing Bayesian algorithm to model the spread of the pandemic in the seven geographic macro-regions of the state. Here we propose changes to extend the model and improve its forecasting capabilities. \nMethods: Our four proposed variations of the original method allow accessing data of daily reported infections and take into account under-reporting of cases more explicitly. Two of the proposed versions also attempt to model the delay in test reporting. We simulated weekly forecasting of deaths from the period from 31/05/2020 until 31/01/2021. First week data were used as a cold-start to the algorithm, after which weekly calibrations of the model were able to converge in fewer iterations. Google Mobility data were used as covariates to the model, as well as to estimate of the susceptible population at each simulated run. \nFindings: The changes made the model significantly less reactive and more rapid in adapting to scenarios after a peak in deaths is observed. Assuming that the cases are under-reported greatly benefited the model in its stability, and modelling retroactively-added data (due to the ""hot"" nature of the data used) had a negligible impact in performance. \nInterpretation: Although not as reliable as death statistics, case statistics, when modelled in conjunction with an overestimate parameter, provide a good alternative for improving the forecasting of models, especially in long-range predictions and after the peak of an infection wave.', 'corpus_id': 233004592, 'score': 1}, {'doc_id': '220334247', 'title': 'Sub-epidemic model forecasts for COVID-19 pandemic spread in the USA and European hotspots, February-May 2020', 'abstract': 'Mathematical models have been widely used to understand the dynamics of the ongoing coronavirus disease 2019 (COVID-19) pandemic as well as to predict future trends and assess intervention strategies. The asynchronicity of infection patterns during this pandemic illustrates the need for models that can capture dynamics beyond a single-peak trajectory to forecast the worldwide spread and for the spread within nations and within other sub-regions at various geographic scales. Here, we demonstrate a five-parameter sub-epidemic wave modeling framework that provides a simple characterization of unfolding trajectories of COVID-19 epidemics that are progressing across the world at different spatial scales. We calibrate the model to daily reported COVID-19 incidence data to generate six sequential weekly forecasts for five European countries and five hotspot states within the United States. The sub-epidemic approach captures the rise to an initial peak followed by a wide range of post-peak behavior, ranging from a typical decline to a steady incidence level to repeated small waves for sub-epidemic outbreaks. We show that the sub-epidemic model outperforms a three-parameter Richards model, in terms of calibration and forecasting performance, and yields excellent short- and intermediate-term forecasts that are not attainable with other single-peak transmission models of similar complexity. Overall, this approach predicts that a relaxation of social distancing measures would result in continuing sub-epidemics and ongoing endemic transmission. We illustrate how this view of the epidemic could help data scientists and policymakers better understand and predict the underlying transmission dynamics of COVID-19, as early detection of potential sub-epidemics can inform model-based decisions for tighter distancing controls.', 'corpus_id': 220334247, 'score': 1}, {'doc_id': '100753101', 'title': ""Assessment on the Uncertainty for Ammonia Nitrogen in Water by Nessler's Reagent Spectrophotometry"", 'abstract': ""Ammonia nitrogen in water quality was anaylzed by spectrophotometric determination using Nessler's Reagent spectrophotometry. Measurement uncertainty of this method to effect the results by repeatability standard solution,capacity containers,working curve,and so on,was studied."", 'corpus_id': 100753101, 'score': 0}, {'doc_id': '235355434', 'title': 'A comparison of five epidemiological models for transmission of SARS-CoV-2 in India', 'abstract': 'Background Many popular disease transmission models have helped nations respond to the COVID-19 pandemic by informing decisions about pandemic planning, resource allocation, implementation of social distancing measures, lockdowns, and other non-pharmaceutical interventions. We study how five epidemiological models forecast and assess the course of the pandemic in India: a baseline curve-fitting model, an extended SIR (eSIR) model, two extended SEIR (SAPHIRE and SEIR-fansy) models, and a semi-mechanistic Bayesian hierarchical model (ICM). Methods Using COVID-19 case-recovery-death count data reported in India from March 15 to October 15 to train the models, we generate predictions from each of the five models from October 16 to December 31. To compare prediction accuracy with respect to reported cumulative and active case counts and reported cumulative death counts, we compute the symmetric mean absolute prediction error (SMAPE) for each of the five models. For reported cumulative cases and deaths, we compute Pearson’s and Lin’s correlation coefficients to investigate how well the projected and observed reported counts agree. We also present underreporting factors when available, and comment on uncertainty of projections from each model. Results For active case counts, SMAPE values are 35.14% (SEIR-fansy) and 37.96% (eSIR). For cumulative case counts, SMAPE values are 6.89% (baseline), 6.59% (eSIR), 2.25% (SAPHIRE) and 2.29% (SEIR-fansy). For cumulative death counts, the SMAPE values are 4.74% (SEIR-fansy), 8.94% (eSIR) and 0.77% (ICM). Three models (SAPHIRE, SEIR-fansy and ICM) return total (sum of reported and unreported) cumulative case counts as well. We compute underreporting factors as of October 31 and note that for cumulative cases, the SEIR-fansy model yields an underreporting factor of 7.25 and ICM model yields 4.54 for the same quantity. For total (sum of reported and unreported) cumulative deaths the SEIR-fansy model reports an underreporting factor of 2.97. On October 31, we observe 8.18 million cumulative reported cases, while the projections (in millions) from the baseline model are 8.71 (95% credible interval: 8.63–8.80), while eSIR yields 8.35 (7.19–9.60), SAPHIRE returns 8.17 (7.90–8.52) and SEIR-fansy projects 8.51 (8.18–8.85) million cases. Cumulative case projections from the eSIR model have the highest uncertainty in terms of width of 95% credible intervals, followed by those from SAPHIRE, the baseline model and finally SEIR-fansy. Conclusions In this comparative paper, we describe five different models used to study the transmission dynamics of the SARS-Cov-2 virus in India. While simulation studies are the only gold standard way to compare the accuracy of the models, here we were uniquely poised to compare the projected case-counts against observed data on a test period. The largest variability across models is observed in predicting the “total” number of infections including reported and unreported cases (on which we have no validation data). The degree of under-reporting has been a major concern in India and is characterized in this report. Overall, the SEIR-fansy model appeared to be a good choice with publicly available R-package and desired flexibility plus accuracy.', 'corpus_id': 235355434, 'score': 0}, {'doc_id': '1585871', 'title': 'Model predictive control for the self-optimized operation in wastewater treatment plants: Analysis of dynamic issues', 'abstract': 'Abstract This paper describes a procedure to find the best controlled variables in an economic sense for the activated sludge process in a wastewater treatment plant, despite the large load disturbances. A novel dynamic analysis of the closed loop control of these variables has been performed, considering a nonlinear model predictive controller (NMPC) and a particular distributed NMPC-PI control structure where the PI is devoted to control the process active constraints and the NMPC the self-optimizing variables. The well-known self-optimizing control methodology has been applied, considering the most important measurements of the process. This methodology provides the optimum combination of measurements to keep constant with minimum economic loss. In order to avoid nonfeasible dynamic operation, a preselection of the measurements has been performed, based on the nonlinear model of the process and evaluating the possibility of keeping their values constant in the presence of typical disturbances.', 'corpus_id': 1585871, 'score': 0}, {'doc_id': '44123454', 'title': 'Modelling the skip-and-resurgence of Japanese encephalitis epidemics in Hong Kong', 'abstract': '\n Abstract\n \n Japanese encephalitis virus (JEV) is a zoonotic mosquito-borne virus, persisting in pigs, Ardeid birds and Culex mosquitoes. It is endemic to China and Southeastern Asia. The case-fatality ratio (CFR) or the rate of permanent psychiatric sequelae is 30% among symptomatic patients. There were no reported local JEV human cases between 2006 and 2010 in Hong Kong, but it was followed by a resurgence of cases from 2011 to 2017. The mechanism behind this “skip-and-resurgence” patterns is unclear.\n This work aims to reveal the mechanism behind the “skip-and-resurgence” patterns using mathematical modelling and likelihood-based inference techniques. We found that pig-to-pig transmission increases the size of JEV epidemics but is unlikely to maintain the same level of transmission among pigs. The disappearance of JEV human cases in 2006–2010 could be explained by a sudden reduction of the population of farm pigs as a result of the implementation of the voluntary “pig-rearing licence surrendering” policy. The resurgence could be explained by of a new strain in 2011, which increased the transmissibility of the virus or the spill-over ratio from reservoir to host or both.\n \n', 'corpus_id': 44123454, 'score': 1}, {'doc_id': '235440951', 'title': 'Modelling the impact of travel restrictions on COVID-19 cases in Newfoundland and Labrador', 'abstract': 'In many jurisdictions, public health authorities have implemented travel restrictions to reduce coronavirus disease 2019 (COVID-19) spread. Policies that restrict travel within countries have been implemented, but the impact of these restrictions is not well known. On 4 May 2020, Newfoundland and Labrador (NL) implemented travel restrictions such that non-residents required exemptions to enter the province. We fit a stochastic epidemic model to data describing the number of active COVID-19 cases in NL from 14 March to 26 June. We predicted possible outbreaks over nine weeks, with and without the travel restrictions, and for contact rates 40–70% of pre-pandemic levels. Our results suggest that the travel restrictions reduced the mean number of clinical COVID-19 cases in NL by 92%. Furthermore, without the travel restrictions there is a substantial risk of very large outbreaks. Using epidemic modelling, we show how the NL COVID-19 outbreak could have unfolded had the travel restrictions not been implemented. Both physical distancing and travel restrictions affect the local dynamics of the epidemic. Our modelling shows that the travel restrictions are a plausible reason for the few reported COVID-19 cases in NL after 4 May.', 'corpus_id': 235440951, 'score': 0}, {'doc_id': '201125081', 'title': 'A novel sub-epidemic modeling framework for short-term forecasting epidemic waves', 'abstract': 'BackgroundSimple phenomenological growth models can be useful for estimating transmission parameters and forecasting epidemic trajectories. However, most existing phenomenological growth models only support single-peak outbreak dynamics whereas real epidemics often display more complex transmission trajectories.MethodsWe develop and apply a novel sub-epidemic modeling framework that supports a diversity of epidemic trajectories including stable incidence patterns with sustained or damped oscillations to better understand and forecast epidemic outbreaks. We describe how to forecast an epidemic based on the premise that the observed coarse-scale incidence can be decomposed into overlapping sub-epidemics at finer scales. We evaluate our modeling framework using three outbreak datasets: Severe Acute Respiratory Syndrome (SARS) in Singapore, plague in Madagascar, and the ongoing Ebola outbreak in the Democratic Republic of Congo (DRC) and four performance metrics.ResultsThe sub-epidemic wave model outperforms simpler growth models in short-term forecasts based on performance metrics that account for the uncertainty of the predictions namely the mean interval score (MIS) and the coverage of the 95% prediction interval. For example, we demonstrate how the sub-epidemic wave model successfully captures the 2-peak pattern of the SARS outbreak in Singapore. Moreover, in short-term sequential forecasts, the sub-epidemic model was able to forecast the second surge in case incidence for this outbreak, which was not possible using the simple growth models. Furthermore, our findings support the view that the national incidence curve of the Ebola epidemic in DRC follows a stable incidence pattern with periodic behavior that can be decomposed into overlapping sub-epidemics.ConclusionsOur findings highlight how overlapping sub-epidemics can capture complex epidemic dynamics, including oscillatory behavior in the trajectory of the epidemic wave. This observation has significant implications for interpreting apparent noise in incidence data where the oscillations could be dismissed as a result of overdispersion, rather than an intrinsic part of the epidemic dynamics. Unless the oscillations are appropriately modeled, they could also give a false positive, or negative, impression of the impact from public health interventions. These preliminary results using sub-epidemic models can help guide future efforts to better understand the heterogenous spatial and social factors shaping sub-epidemic patterns for other infectious diseases.', 'corpus_id': 201125081, 'score': 1}, {'doc_id': '235249145', 'title': 'Projecting the criticality of COVID-19 transmission in India using GIS and machine learning methods', 'abstract': '\n There is a new public health catastrophe forbidding the world. With the advent and spread of 2019 novel coronavirus (2019-nCoV). Learning from the experiences of various countries and the World Health Organization (WHO) guidelines, social distancing, use of sanitizers, thermal screening, quarantining, and provision of lockdown in the cities being the effective measure that can contain the spread of the pandemic. Though complete lockdown helps in containing the spread, it generates complexity by breaking the economic activity chain. Besides, laborers, farmers, and workers may lose their daily earnings. Owing to these detrimental effects, the government has to open the lockdown strategically. Prediction of the COVID-19 spread and analyzing when the cases would stop increasing helps in developing a strategy. An attempt is made in this paper to predict the time after which the number of new cases stops rising, considering the strong implementation of lockdown conditions using three different techniques such as Decision Tree, Support Vector Machine, and Gaussian Process Regression algorithm are used to project the number of cases. Thus, the projections are used in identifying inflection points, which would help in planning the easing of lockdown in a few of the areas strategically. The criticality in a region is evaluated using the criticality index (CI), which is proposed by authors in one of the past of research works. This research work is made available in a dashboard to enable the decision-makers to combat the pandemic.\n', 'corpus_id': 235249145, 'score': 0}, {'doc_id': '235635730', 'title': 'Characterizing two outbreak waves of COVID-19 in Spain using phenomenological epidemic modelling', 'abstract': 'Since the first case reported of SARS-CoV-2 the end of December 2019 in China, the number of cases quickly climbed following an exponential growth trend, demonstrating that a global pandemic is possible. As of December 3, 2020, the total number of cases reported are around 65,527,000 contagions worldwide, and 1,524,000 deaths affecting 218 countries and territories. In this scenario, Spain is one of the countries that has suffered in a hard way, the ongoing epidemic caused by the novel coronavirus SARS-CoV-2, namely COVID-19 disease. In this paper, we present the utilization of phenomenological epidemic models to characterize the two first outbreak waves of COVID-19 in Spain. The study is driven using a two-step phenomenological epidemic approach. First, we use a simple generalized growth model to fit the main parameters at the early epidemic phase; later, we apply our previous finding over a logistic growth model to that characterize both waves completely. The results show that even in the absence of accurate data series, it is possible to characterize the curves of case incidence, and construct a short-term forecast of 60 days in the near time horizon, in relation to the expected total duration of the pandemic.', 'corpus_id': 235635730, 'score': 1}]"
20	Chlamy multiple fission model	d3ad35d36b6c4ee1c0351baaa90bd654	17132	{}	[{'doc_id': '233328847', 'title': 'Asynchronous nuclear cycles in multinucleated Plasmodium falciparum enable rapid proliferation', 'abstract': 'Malaria-causing parasites proliferate within erythrocytes through schizogony, forming multinucleated stages before cellularization. Nuclear multiplication does not follow a strict geometric 2n progression and each proliferative cycle produces a heterogeneous number of progeny. Here, by tracking nuclei and DNA replication, we show that individual nuclei replicate their DNA at different times, despite residing in a shared cytoplasm. Extrapolating from experimental data using mathematical modeling, we demonstrate that a limiting factor must exist that slows down the nuclear multiplication rate. Indeed, our data show that temporally overlapping DNA replication events were significantly slower than partially or non-overlapping events. Our findings suggest an evolutionary pressure that selects for asynchronous DNA replication, balancing available resources with rapid pathogen proliferation.', 'corpus_id': 233328847, 'score': 1}, {'doc_id': '233472094', 'title': 'Threshold accumulation of a constitutive protein explains E. coli cell-division behavior in nutrient upshifts', 'abstract': 'Significance The mechanism leading to cell division in the bacterium Escherichia coli is unknown, but we know that it results in adding a roughly constant size every cell cycle, regardless of size at birth. While most available studies try to infer information on cell division from steadily dividing cells in constant nutrient conditions, this study leverages a high-resolution device to monitor single-cell growth division upon nutrient changes. Comparing these data with different mathematical models, we are able to discriminate among fundamentally different mechanisms of cell-division control, and we show that the data support a model where an unregulated protein accumulates to a threshold and triggers division. Despite a boost of recent progress in dynamic single-cell measurements and analyses in Escherichia coli, we still lack a mechanistic understanding of the determinants of the decision to divide. Specifically, the debate is open regarding the processes linking growth and chromosome replication to division and on the molecular origin of the observed “adder correlations,” whereby cells divide, adding roughly a constant volume independent of their initial volume. In order to gain insight into these questions, we interrogate dynamic size-growth behavior of single cells across nutrient upshifts with a high-precision microfluidic device. We find that the division rate changes quickly after nutrients change, much before growth rate goes to a steady state, and in a way that adder correlations are robustly conserved. Comparison of these data to simple mathematical models falsifies proposed mechanisms, where replication–segregation or septum completions are the limiting step for cell division. Instead, we show that the accumulation of a putative constitutively expressed “P-sector divisor” protein explains the behavior during the shift.', 'corpus_id': 233472094, 'score': 0}, {'doc_id': '234599845', 'title': 'Cell size regulation in microorganisms', 'abstract': 'Various rod-shaped bacteria such as the canonical gram negative Escherichia coli or the wellstudied gram positive Bacillus subtilis divide symmetrically after they approximately double their volume. Their size at division is not constant, but is typically distributed over a narrow range. Here, we propose an analytically tractable model for cell size control, and calculate the cell size and interdivision time distributions. We suggest ways of extracting the model parameters from experimental data. Existing data for E. coli supports partial size control, and a particular explanation: a cell attempts to add a constant volume from the time of initiation of DNA replication to the next initiation event. This hypothesis explains how bacteria control their tight size distributions and accounts for the experimentally observed correlations between parents and daughters as well as the exponential dependence of size on growth rate.', 'corpus_id': 234599845, 'score': 0}, {'doc_id': '233245996', 'title': 'Emergent Spatiotemporal Population Dynamics with Cell-Length Control of Synthetic Microbial Consortia', 'abstract': 'Increased complexity of engineered microbial biocircuits highlights the need for distributed cell functionality due to concomitant increases of metabolic and regulatory burdens imposed on single-strain topologies. Distributed systems, however, introduce additional challenges since consortium composition and spatiotemporal dynamics of constituent strains must be robustly controlled to achieve desired circuit behaviors. Here, we address these challenges with a modeling-based investigation of emergent spatiotemporal population dynamics that result from cell-length control of monolayer, two-strain bacterial consortia. We demonstrate that with dynamic control of a strain’s division length, nematic cell alignment in close-packed monolayers can be destabilized. We found this destabilization conferred an emergent, competitive advantage on smaller-length strains—but by mechanisms that differed depending on the spatial patterns of the population. We used complementary modeling approaches to elucidate underlying mechanisms: an agent-based model to simulate detailed mechanical and signaling interactions between the competing strains and a reductive, stochastic lattice model to represent cell-cell interactions with a single rotational parameter. Our modeling suggests that spatial strain-fraction oscillations can be generated when cell-length control is coupled to quorum-sensing signaling in negative feedback topologies. Our research employs novel methods of population control and points the way to programming strain fraction dynamics in consortial synthetic biology. Engineered microbial collectives are more versatile and robust than single strain populations. However, the function of such collectives is sensitive to their spatiotemporal organization. Here, we demonstrate control of the spatiotemporal composition of synthetic microbial consortia by dynamically modulating the average cell length of constituent strains. Such modulation confers an emergent “mechanical fitness” advantage upon the shorter length strain. We used both a biophysically realistic agent-based model to test the impact of cell shape on spatiotemporal dynamics and a conceptually simpler stochastic lattice model to explain the essential mechanisms driving the dynamics.', 'corpus_id': 233245996, 'score': 0}, {'doc_id': '233206086', 'title': 'The Modular Circuitry of Apicomplexan Cell Division Plasticity', 'abstract': 'The close-knit group of apicomplexan parasites displays a wide variety of cell division modes, which differ between parasites as well as between different life stages within a single parasite species. The beginning and endpoint of the asexual replication cycles is a ‘zoite’ harboring the defining apical organelles required for host cell invasion. However, the number of zoites produced per division round varies dramatically and can unfold in several different ways. This plasticity of the cell division cycle originates from a combination of hard-wired developmental programs modulated by environmental triggers. Although the environmental triggers and sensors differ between species and developmental stages, widely conserved secondary messengers mediate the signal transduction pathways. These environmental and genetic input integrate in division-mode specific chromosome organization and chromatin modifications that set the stage for each division mode. Cell cycle progression is conveyed by a smorgasbord of positively and negatively acting transcription factors, often acting in concert with epigenetic reader complexes, that can vary dramatically between species as well as division modes. A unique set of cell cycle regulators with spatially distinct localization patterns insert discrete check points which permit individual control and can uncouple general cell cycle progression from nuclear amplification. Clusters of expressed genes are grouped into four functional modules seen in all division modes: 1. mother cytoskeleton disassembly; 2. DNA replication and segregation (D&S); 3. karyokinesis; 4. zoite assembly. A plug-and-play strategy results in the variety of extant division modes. The timing of mother cytoskeleton disassembly is hard-wired at the species level for asexual division modes: it is either the first step, or it is the last step. In the former scenario zoite assembly occurs at the plasma membrane (external budding), and in the latter scenario zoites are assembled in the cytoplasm (internal budding). The number of times each other module is repeated can vary regardless of this first decision, and defines the modes of cell division: schizogony, binary fission, endodyogeny, endopolygeny.', 'corpus_id': 233206086, 'score': 0}, {'doc_id': '195733125', 'title': 'Mathematical Modeling of the Size-structured Growth of Microalgae Dividing by Multiple Fission', 'abstract': 'A novel mathematical model to simulate the size-structured growth of microalgal strains dividing by multiple fission is proposed. The model is validated by comparison with literature experimental data. Then, the implications of such division mode on crucial aspects of microalgae cultivation and processing, such as time for steady state achievement, biomass productivity and flocculant dosage, were assessed through proper numerical simulations. The model well captured the experimental results and thus represents a suitable tool for the simulation of the growth of microalgae strains dividing by multiple fission.', 'corpus_id': 195733125, 'score': 1}, {'doc_id': '198914347', 'title': 'Growth and the cell cycle in green algae dividing by multiple fission', 'abstract': 'Most cells divide into two daughter cells; however, some green algae can have different division patterns in which a single mother cell can sometimes give rise to up to thousands of daughter cells. Although such cell cycle patterns can be very complex, they are governed by the same general concepts as the most common binary fission. Moreover, cell cycle progression appears to be connected with size, since cells need to ensure that their size after division will not drop below the limit required for survival. Although the exact mechanism that lets cells measure cell size remains largely unknown, there have been several prominent hypotheses that try to explain it.', 'corpus_id': 198914347, 'score': 1}, {'doc_id': '190874494', 'title': 'A single light-responsive sizer can control multiple-fission cycles in Chlamydomonas', 'abstract': 'Proliferating cells need to coordinate cell division and growth to maintain size homeostasis. Any systematic deviation from a balance between growth and division results in progressive changes of cell size over subsequent generations. While most eukaryotic cells execute binary division after a mass doubling, the photosynthetic green alga Chlamydomonas can grow more than eight-fold during daytime before undergoing rapid cycles of DNA replication, mitosis and cell division at night, which produce up to 16 daughter cells. Here, we propose a mechanistic model for multiple fission and size control in Chlamydomonas. The model comprises a light-sensitive and size-dependent biochemical toggle switch that acts as a sizer and guards transitions into and exit from a phase of cell-division cycle oscillations. We show that this simple ‘sizer-oscillator’ arrangement reproduces the experimentally observed features of multiple-fission cycles and the response of Chlamydomonas cells to different light-dark regimes. Our model also makes testable predictions about the dynamical properties of the biochemical network that controls these features and about the network’s makeup. Collectively, these results provide a new perspective on the concept of a ‘commitment point’ during the growth of Chlamydomonas cells and hint at an intriguing continuity of cell-size control in different eukaryotic lineages. Graphical abstract G1-sizer and S/M-oscillator can give rise to multiple-fission cycles in Chlamydomonas Light-responsive bistable switch may guard transition between G1 and S/M-cycles Illumination increases S/M-entry threshold, causing multiple-fission cycles Dark shift lowers S/M-entry threshold, allowing small cells to commit to fewer divisions', 'corpus_id': 190874494, 'score': 1}, {'doc_id': '233874168', 'title': 'How Many Is Enough? - Challenges of Multinucleated Cell Division in Malaria Parasites', 'abstract': 'Regulating the number of progeny generated by replicative cell cycles is critical for any organism to best adapt to its environment. Classically, the decision whether to divide further is made after cell division is completed by cytokinesis and can be triggered by intrinsic or extrinsic factors. Contrarily, cell cycles of some species, such as the malaria-causing parasites, go through multinucleated cell stages. Hence, their number of progeny is determined prior to the completion of cell division. This should fundamentally affect how the process is regulated and raises questions about advantages and challenges of multinucleation in eukaryotes. Throughout their life cycle Plasmodium spp. parasites undergo four phases of extensive proliferation, which differ over three orders of magnitude in the amount of daughter cells that are produced by a single progenitor. Even during the asexual blood stage proliferation parasites can produce very variable numbers of progeny within one replicative cycle. Here, we review the few factors that have been shown to affect those numbers. We further provide a comparative quantification of merozoite numbers in several P. knowlesi and P. falciparum parasite strains, and we discuss the general processes that may regulate progeny number in the context of host-parasite interactions. Finally, we provide a perspective of the critical knowledge gaps hindering our understanding of the molecular mechanisms underlying this exciting and atypical mode of parasite multiplication.', 'corpus_id': 233874168, 'score': 1}, {'doc_id': '233460971', 'title': 'The Consequences of Budding versus Binary Fission on Adaptation and Aging in Primitive Multicellularity', 'abstract': 'Early multicellular organisms must gain adaptations to outcompete their unicellular ancestors, as well as other multicellular lineages. The tempo and mode of multicellular adaptation is influenced by many factors including the traits of individual cells. We consider how a fundamental aspect of cells, whether they reproduce via binary fission or budding, can affect the rate of adaptation in primitive multicellularity. We use mathematical models to study the spread of beneficial, growth rate mutations in unicellular populations and populations of multicellular filaments reproducing via binary fission or budding. Comparing populations once they reach carrying capacity, we find that the spread of mutations in multicellular budding populations is qualitatively distinct from the other populations and in general slower. Since budding and binary fission distribute age-accumulated damage differently, we consider the effects of cellular senescence. When growth rate decreases with cell age, we find that beneficial mutations can spread significantly faster in a multicellular budding population than its corresponding unicellular population or a population reproducing via binary fission. Our results demonstrate that basic aspects of the cell cycle can give rise to different rates of adaptation in multicellular organisms.', 'corpus_id': 233460971, 'score': 0}]
21	Whiteness	f760f5e1e7491f2f7bfb4ba242c39153	13605	{}	"[{'doc_id': '231834773', 'title': 'Colonial Patronage: Evolutions in the Critique of Sartre’s “Orphée noir”', 'abstract': 'One of the most interesting and controversial episodes in the history of the Négritude literary and philosophical movement came when two white, French authors prefaced the texts of two of the movement’s most significant authors. Jean-Paul Sartre’s “Orphée noir” is one of these texts in question, and it served as the preface for Léopold Sédar Senghor’s Anthologie de le nouvelle poésie nègre et malgache de langue française. In one sense, one might characterize Sartre as a friend to the Négritude movement, exposing it to the francophone mainstream and thereby helping it gain traction in Western academia. Viewed a different way, however, and Sartre was intruding into a dialogue in a way he did not truly understand and limiting the movement he sought to help by defining it within his own definition of Blackness. In this project, I propose to investigate the larger implications and perspectives surrounding Sartre’s essay in order to extract the most important criticisms against it as well as the most optimistic takes on what good can be salvaged from his work.', 'corpus_id': 231834773, 'score': 1}, {'doc_id': '191605977', 'title': 'Whiteness and “The Canon”', 'abstract': ""It is a great tragedy that all things in this society, including history, pedagogy, and the pursuit of knowledge, must struggle under the asphyxiating sludge of race, which is the legacy of the myth of whiteness. I grew up in a different society where color has no meaning outside the painter's palette, and my early training was such that I developed a healthy admiration for the work of artists as varied as Francisco de Goya, Gustave Courbet, Kathe Kollwitz, Charles White, Jacob Lawrence, and Ben Shahn, without consciousness of their color.3 I also had considerable exposure to a history of world art without prejudice toward the contributions ofmy own culture to that heritage. I knew one language of art history; a race coding of that language would have appeared ludicrous. However, having since taught in the academy on three continents, I must testify to the cogency of the issues that you raise. A cursory look at the curricula of art history programs across the United States quickly reveals a methodical blindness to all that is not rooted in Western civilization, which is as troubling as it is enduring. This predilection is as evident in course plans and program leanings as it is in faculty hiring practices and student recruitment. My experience was not entirely different in Britain. There is truth, therefore, to the notion of a race-specific pedagogical system and environment so suffused with “absences of vital presences” as to alienate those who may not find themselves or their backgrounds reflected.4"", 'corpus_id': 191605977, 'score': 1}, {'doc_id': '231830644', 'title': 'Racial Security: The Unobserved Threat in IR', 'abstract': 'Since the development of the modern state in the seventeenth century, race has directly influenced human understanding of peace and order. Racist perceptions of anarchy associated non-Western and non-white communities with savagery. In turn, a racist cycle of global knowledge upon which the international community is based has developed. Additionally, imperialistic reign over the following centuries dictated the inability for society to challenge racist ideals and norms assumed by the powers that be. Unfortunately, the concepts of race and security have run counter-intuitively in respect to human development. Using the supporting research to construct a preliminary definition of Racial Security and its implications, this essay aims to show how race has compromised the theoretical understanding of international relations in its applicability to the fields of security and strategy.', 'corpus_id': 231830644, 'score': 0}, {'doc_id': '231742773', 'title': 'BLACK LIVES MATTER IN THE UNITED STATES OF AMERICA', 'abstract': ""This paper examines affiliation with the Black Lives Matter (BLM) movement using the constructivism theory. The main finding presented in the paper is that the discrimination experienced by African Americans in the United States in the past two decades. The BLM movement's history was a response to the death of two black teenagers, Trayvon Martin and Michael Brown, who were both unarmed and shot and killed. The most famous one happened this year, the death of George Floyd for the brutal police action by pressing the victim's neck with his leg until Floyd died. The second key finding is that BLM organizations generated more to frame the movement as a struggle for individual rights. Still, many youths assume that this movement is just a trend on social media. Finally, social media's influence where the spread of news, content, videos is the important point of the black lives matter movement in the US. Keyword: BlackLivesMatter, Social Media, Constructivism, Discrimination"", 'corpus_id': 231742773, 'score': 0}, {'doc_id': '190340996', 'title': 'What White Looks Like: African-American Philosophers on the Whiteness Question', 'abstract': ""Contributors Introduction: Fragments of a Social Ontology of Whiteness, George Yancy 1. Racial Exploitation and the Wages of Whiteness, Charles W. Mills 2. The Bad Faith of Whiteness, Robert E. Birt 3. The Impairment of Empathy in Goodwill Whites for African Americans, Janine Jones 4. Deligitimizing the Normativity of 'Whiteness': A Critical Africana Philosophical Study of the Metaphoricity of 'Whiteness', Clevis Headley 5. A Foucauldian (Genealogical) Reading Of Whiteness: The Production Of The Black Body/Self And The Racial Deformation. Of Pecola Breedlove In Toni Morrison's The Bluest Eye, George Yancy 6. Whiteness Visible: Enlightenment Racism and the Structure of Racialized Consciousness, Arnold Farr 7. Rehabilitate Racial Whiteness?, Lucius T. Outlaw 8. Critical Reflections on Three Popular Tropes in the Study of Whiteness, Lewis Gordon 9. Whiteness and Africana Phenomenology, Paget Henry 10. On the Nature of Whiteness and the Ontology of Race: Toward a Dialectical Materialist Analysis, John H. McClendon 11. Silence and Sympathy: Dewey's Whiteness, Paul C. Taylor 12. Whiteness and Feminism: Deja vu Discourses, What's Next?, Blanche Radford Curry 13. The Academic Addict: Mainlining White Supremacy (WS), Joy James"", 'corpus_id': 190340996, 'score': 1}, {'doc_id': '229935062', 'title': 'On Holding Various Truths to (Not) Be Self-Evident: Leading During the Dual Pandemics of 2020 as a Racialized Body', 'abstract': 'In this critical autoethnography, the author counter/narrates how she has navigated the dual pandemics of COVID-19 and structural racism predicated on over four centuries of racial oppression that reached a zenith on May 25, 2020, when George Floyd was murdered by a White police officer. As an Asian American woman dean of education at a White-dominated regional university, she weaves in between the past and present to discuss the multilayered intersections between the lives and livelihoods of Asian Americans and Black Americans by theorizing contemporary meanings of the historic slogan from the 1960s: “Yellow Peril Supports Black Power.” She reflects on a few critical moments during the dual pandemics where she has navigated predominantly White spaces that have attempted to center tired-old platitudes around justice for George Floyd despite the daily persistence of blatantly and subtly racist practices against individuals who are Black, Indigenous, and People of Color.', 'corpus_id': 229935062, 'score': 0}, {'doc_id': '230620229', 'title': 'World War II and American Racial Politics: Public Opinion, the Presidency, and Civil Rights Advocacy. By Steven White. New York: Cambridge University Press, 2019. 216p. $99.99 cloth.', 'abstract': 'how forms of civic engagement other than voting are subject to the noncognitive skills thesis, because the overall goal is to make better democratic citizens. Second, there is a tendency in the book (perhaps unintentionally) to treat young people monolithically. Formative characteristics and experiences vary across racial, ethnic, and even generational groups that likely contribute to the development of noncognitive skills. These factors should be more systematically considered. Finally, an important remaining question is how much electoral reforms can counter the necessity for improved noncognitive skills (or vice versa) in closing the youth voting gap. The book argues for investment in both, but in the real world where tradeoffs exist, it is important to have a better understanding of the potential relative success in outcomes. Yet anyone interested in increasing youth civic engagement should heed the call to explore the role of noncognitive skills in the participatory process, with Making Young Voters serving as a vital roadmap in the investigation.', 'corpus_id': 230620229, 'score': 0}, {'doc_id': '230577791', 'title': 'These Stories Must Be Told: Preliminary Observations by a Black Scholar Practitioner on Silences in the Archive', 'abstract': 'As a scholar practitioner, a trained philosophical theologian, Methodist clergywoman, and social enterprise founder who is conducting oral histories as part of my doctoral internship in the IUPUI Arts and Humanities Institute, my scholarly lens and methodological skills are being defined as I interrogate the COVID-19 archive. In this article I attempt to offer some preliminary reflections on my oral history curation focused on how Black and brown artists and activists, primarily based in Indianapolis, IN, frame their lived experiences of death, dying, mourning, and bereavement in the wake of COVID-19 utilizing critical archival practices: those practices that take seriously the methods of critical race theory, critical gender theory, Womanist, mujerista, and feminist methodologies, to name a few. The COVID-19 archive is a collection of oral histories, stories and artifacts depicting the times in which we are living, through the lenses of storytellers grappling with the pandemics of systemic racism, COVID-19, distrust in government, and various relics representing the idea of the United States of America in 2020, as such, I conclude with a brief exploration of how art emerges as both an outlet for creators and a mode of illumination for consumers.', 'corpus_id': 230577791, 'score': 0}, {'doc_id': '171473076', 'title': 'Look, A White!: Philosophical Essays on Whiteness', 'abstract': None, 'corpus_id': 171473076, 'score': 1}, {'doc_id': '151937310', 'title': 'Black Bodies, White Gazes: The Continuing Significance of Race in America', 'abstract': '149 Black Bodies, White Gazes: The Continuing Significance of Race George Yancy Rowman & Littlefield Publishers, Inc. ISBN: 978-0-7425-5298-2 In the text, Black Bodies White Gazes: the Continuing Significance of Race, George Yancy provides a powerful narrative concerning what he identifies in his introduction as the historical ontology of blackness as this applies to the fluid presencing of human embodiment within the context of anti-black racism. More importantly, Yancy explores the way in which whiteness becomes constructed as a “’transcendental norm,” which is never “raced”, but whose “privilege’ is always implicated in the problematic presencing of the black body. However, he also argues that the historical objectification of blackness is never sufficient to completely silence those potentialities for black embodiment which seek to construct an alternative meaning(s) for blackness that is not beholden to this transcendental normative process. As such, the social presencing of the Black body is never totally captured by the ongoing historical project of anti-black racism and is therefore always capable of constructing an alternative meaning for blackness that is free from the objectifying restrictions imposed upon it.', 'corpus_id': 151937310, 'score': 1}]"
22	research in effects of intercultural differences when teaching ESL, EFL	095cad742f3c81fd3b3614135f4383ec	9095	{'ESL': 'eslicarbazepine acetate', 'EFL': 'expiratory flow limitation'}	"[{'doc_id': '221691590', 'title': 'The Development of Financial Management Textbook Base on Problem-Based Learning With Blended Learning Approach', 'abstract': 'The now days education is hoped to be a learning process which the students as the central of learning activity. This way of teaching is aimed to increase the comprehension of the students toward the material independently. In other way, the application of the technology in teaching process is needed in order to make the teaching process more comprehensive. This point of view emerges the implementation of blended learning which combine offline and online to lead the students to the broader knowledge comprehension. The objective of this research is to find the different of the problems-based learning finance management textbooks implementation and proper when the books are used in teaching process with blended learning approach. The method of this research is RnD (Research and Development) while the subjects of this research are 12 students. The stages of this research are: 1) The previous researches, 2) problems identification, 3) Data collecting and textbooks designing, 4) textbooks validation by experts, 5) Textbooks revision, 6) Small Scale trial, 7) ongoing textbooks revision, 8) textbooks trial, 9) the textbook final revision. The data analysis technique in this research is textbooks validation data, and static comparative analysis. The analysis is helped by SPSS software. The result of the research produces a valid problem-based learning finance management textbooks to apply in lecturing. The significant increase of studying result is found before and after the finance management textbooks. The comparative statistic found the significant different in problem-based learning finance management textbooks taught by using blended leaning approach.', 'corpus_id': 221691590, 'score': 0}, {'doc_id': '221728668', 'title': ""The Impact of E-Learning Strategy on Students ' Academic Achievement"", 'abstract': ""This study examines the effect of E-learning during COVID-19 pandemic on the students’ Academic Achievement at Al-Quds Open University. The study has randomly selected 382 students’ GPA from the University's official records. It is mainly based on Statistical Package for Social Sciences program (SPSS.20) to make Paired Samples T-test and to study the hypotheses. The study has revealed that there are statistically significant differences in the students’ Academic Achievements during the implementation of the E-learning strategy in COVID-19 pandemic. This study shows that in general the GPA of students has increased about 2.188 points; but in particular the GPA of male students is affected more than female’s by just a slight difference of 1,198 point. On the other hand, looking at the Program of Study at the University; the Community Service is affected most with an increase by 3,276 points, Then Business Administration, Accounting and Finance are respectively affected more with having 2.6 points more on students’ GPA. However, the greatest effect on GPA is largely noticed on the students whose GPA is low in which the increase is about 6.568 points. The study results shows the importance of the implementation of E-learning strategy in higher education institutions so as to improve the students’ Academic Achievements. In addition, it sheds the light on the necessity of taking into consideration the specific features of some learning programs such as the Arabic Language and the Social Studies."", 'corpus_id': 221728668, 'score': 1}, {'doc_id': '210115504', 'title': 'EUROPEAN ACADEMIC RESEARCH, VOL', 'abstract': 'This study is conducted to investigate about the gaps and differences that exist between the target language and learners’ culture in the teaching of English Language through Oxford Modern English. At the same time, this research has also investigated the impacts of these differences on English language teaching, learning and cultural development of the ESL learner. The design of the research is mixed as data is collected from three text books through purposive sampling then arranged in tabulated form and percentage is presented in graphs by following the quantitative method. On the other hand, interpretation is given qualitatively according to the own observation of the researcher and data is categorized by applying the model of Byram et al. (1994). Therefore, this is a content based mixed method study. This study is completed under the main stream of academic discourse analysis by applying the model of Byram et al. (1994) to describe the gaps between the contents of these ELT Textbooks and the learners’ society on sociological, religious and cultural level. Findings show that there are many cultural differences and a dire need is to review, revise or replace these foreign cultural contents in ELT/EFL text books written by foreign authors who are totally unaware about the learners’ culture but using it with the amalgamation of target culture that is creating problems for the learners in the acquisition of this language. This discourse analysis is locating the gaps to overcome and achieve the objectives of ELT. Nilofar, Munazza Shaheen, Asifa Nosheen, Marium Bashir, Shumila AnjumRepresentation of Target Culture in Oxford Modern English Textbooks: Discourse Analysis EUROPEAN ACADEMIC RESEARCH Vol. VI, Issue 9 / December 2018 5264', 'corpus_id': 210115504, 'score': 1}, {'doc_id': '221860819', 'title': 'AN ACTIVE LEARNING APPROACH FOR A DESIGN THINKING COURSE', 'abstract': 'Design research has always tried to shed light on the way designers think, opening a variety of discourses on this topic, both in the professional and in the academic fields. On the other side, in Politecnico di Milano design students are exposed to “lear', 'corpus_id': 221860819, 'score': 1}, {'doc_id': '221152230', 'title': 'Measuring teaching quality, designing tests, and transforming feedback targeting various education actors', 'abstract': 'Education systems across the world face various challenges which may be caused or increased by global mobility, economic and political crises and more recently, pandemics such as Covid-19. Eventually, dealing with some challenges becomes part of educators’ daily business but still requires flexibility whilst maintaining focus on core issues like educational quality. This issue of EAEA focuses on educational quality regarding (1) measuring teaching quality across countries and regions, (2) designing tests and test formats and (3) providing and transforming feedback to teachers and migrant students for learning and change.', 'corpus_id': 221152230, 'score': 1}, {'doc_id': '227160516', 'title': 'Going beyond traditional approaches on industrial engineering education', 'abstract': 'This Research-to-Practice full paper refers to academic perspectives on educational innovation for industrial engineering education. Two common views prevail in educational innovation that turn into different results. One view refers to the use of pedagogical approaches to improve in-classroom students’ learning. This is an operational perspective about teaching activities, instructional facilitation and the use of academic resources. The second view refers to educational value creation for students, educational partners, society and to improve the academic positioning of universities. However, both views complement each other and can articulate a holistic approach on educational innovation. To proceed in this direction, this work unfolds in three parts. First, a literature review illustrates the differences between the two complementary views. Second, a conceptual framework is provided to connect the two perspectives and guide further educational innovation efforts. Third, a descriptive and exploratory application case is offered to exemplify the framework at the MIT Supply Chain and Logistics Excellence (SCALE) Latin America Network for industrial engineering education. This work contributes to educational practice with a tool to reflect upon innovation efforts, identify instances and align initiatives with intended educational purposes.', 'corpus_id': 227160516, 'score': 0}, {'doc_id': '227125675', 'title': 'Educating Civic-Minded Engineers: A Qualitative Study of First-Year Engineering Students', 'abstract': 'This Work-in-Progress Research paper introduces a qualitative study about engineering students’ perceptions and experiences with civic engagement. This study extends the scholarship on civic engagement in higher education into a context that has received relatively little attention: the engineering profession. As professionals, engineers have a responsibility to serve public welfare and community interests. Engineers can contribute their skills to serve community needs and improve the livelihoods of community members. Postsecondary engineering education provides a valuable opportunity to help students develop an appreciation for the civic aspects of the profession. As a result, it is important to understand how engineering programs influence students’ civic-minded dispositions. Using a social cognitive approach, this study seeks to understand how civic mindedness manifests in engineering students’ pre-curricular, curricular, and co-curricular experiences and how these experiences shape their perceptions of civics within the engineering profession. This paper outlines the theoretical approach and research design of this study, including data collection methods and validity considerations. The paper concludes with a summary of future work.', 'corpus_id': 227125675, 'score': 0}, {'doc_id': '227138977', 'title': 'Teaching Engineering Ethics With Drama', 'abstract': 'In this Work in Progress, we present a new, simple, and fun way of incorporating drama into engineering ethics education with students producing, performing, and watching drama. The method differs from more established drama-based pedagogies in engineering ethics education, such as role-plays. We argue that this method can contribute to learning by stimulating moral imagination, empathy and sympathy, which balances the otherwise cognitive focus in engineering ethics courses. The component has been tested in practice three times in 2019-2020 in the Engineering Ethics course at Uppsala University. Student feedback is used to illustrate how the method has been perceived by students.', 'corpus_id': 227138977, 'score': 0}, {'doc_id': '220883655', 'title': 'Learning and Teaching Online During Covid-19: Experiences of Student Teachers in an Early Childhood Education Practicum', 'abstract': 'Online learning is an educational process which takes place over the Internet as a form of distance education. Distance education became ubiquitous as a result of the COVID-19 pandemic during 2020. Because of these circumstances, online teaching and learning had an indispensable role in early childhood education programs, even though debates continue on whether or not it is beneficial for young children to be exposed extensively to Information and Communication Technology (ICT). This descriptive study demonstrates how a preservice teacher education course in early childhood education was redesigned to provide student teachers with opportunities to learn and teach online. It reports experiences and reflections from a practicum course offered in the Spring Semester of 2020, in the USA. It describes three phases of the online student teachers’ experiences–Preparation, Implementation, and Reflection. Tasks accomplished in each phase are reported. Online teaching experiences provided these preservice teachers with opportunities to interact with children, as well as to encourage reflection on how best to promote young children’s development and learning with online communication tools.', 'corpus_id': 220883655, 'score': 0}, {'doc_id': '221972246', 'title': 'Alina Schartner, Tony J. Young: Intercultural transitions in higher education: international student adjustment and adaptation', 'abstract': 'COVID-19 has shed a light on what it is like to be international students and the issues that they face in higher education (Cheung, 2020). Although there has already been a burgeoning literature on international students’ experiences, research in this area is largely fragmented and under-theorised, since different research fields (e.g. cultural studies, applied linguistics, social psychology, and educational research) have adopted different approaches to investigate international student mobility. Schartner and Young integrates a number of theoretical and methodological approaches to address this issue by presenting a conceptual model and empirical findings on the multifaceted experiences of mobile students, based on a research project in British higher education. The book comprises three parts. Part 1 addresses the theoretical and methodological approaches of the research project presented in the subject matter, Part 2 focuses on the empirical data informing the new model of international student experiences, and Part 3 explores the implications and applications of the findings. Throughout Chap. 1, the authors contextualise their study within the current political and social circumstances, providing a rationale for how ecumenical approaches to interculturality best fulfil their research purposes. The introductory chapter also clarifies the meaning of keywords, such as ‘adjustment’ and ‘adaptation’, which have been used ambiguously by earlier researchers. Chapter 2 outlines previous research on the transitional experiences of international students. It explores the multifaceted nature of the issue, which requires academic, psychological, and sociocultural adjustment/adaptation. The chapter also summarises earlier findings on the major factors including linguistic and sociocultural resources that contribute to the successful transition of mobile students. The authors turn to a methodological discussion in Chap. 3 and emphasise how a mixed-method design can (1) address the dichotomy between positivist and interpretivist approaches to intercultural research and (2) synthesise both adjustment and adaptation, the process and outcome of transition. Chapters 4–8 provide empirical data, both quantitative and qualitative, on the adjustment and adaptation experiences of international postgraduate students, which were', 'corpus_id': 221972246, 'score': 1}]"
23	biomedical text mining and NLP	b51d6e915f1367a252209957b6eaf0c2	6268	{'NLP': 'natural language processing'}	"[{'doc_id': '59602626', 'title': 'Integrating shortest dependency path and sentence sequence into a deep learning framework for relation extraction in clinical text', 'abstract': 'BackgroundExtracting relations between important clinical entities is critical but very challenging for natural language processing (NLP) in the medical domain. Researchers have applied deep learning-based approaches to clinical relation extraction; but most of them consider sentence sequence only, without modeling syntactic structures. The aim of this study was to utilize a deep neural network to capture the syntactic features and further improve the performances of relation extraction in clinical notes.MethodsWe propose a novel neural approach to model shortest dependency path (SDP) between target entities together with the sentence sequence for clinical relation extraction. Our neural network architecture consists of three modules: (1) sentence sequence representation module using bidirectional long short-term memory network (Bi-LSTM) to capture the features in the sentence sequence; (2) SDP representation module implementing the convolutional neural network (CNN) and Bi-LSTM network to capture the syntactic context for target entities using SDP information; and (3) classification module utilizing a fully-connected layer with Softmax function to classify the relation type between target entities.ResultsUsing the 2010 i2b2/VA relation extraction dataset, we compared our approach with other baseline methods. Our experimental results show that the proposed approach achieved significant improvements over comparable existing methods, demonstrating the effectiveness of utilizing syntactic structures in deep learning-based relation extraction. The F-measure of our method reaches 74.34% which is 2.5% higher than the method without using syntactic features.ConclusionsWe propose a new neural network architecture by modeling SDP along with sentence sequence to extract multi-relations from clinical text. Our experimental results show that the proposed approach significantly improve the performances on clinical notes, demonstrating the effectiveness of syntactic structures in deep learning-based relation extraction.', 'corpus_id': 59602626, 'score': 1}, {'doc_id': '212725611', 'title': 'Stanza: A Python Natural Language Processing Toolkit for Many Human Languages', 'abstract': 'We introduce Stanza, an open-source Python natural language processing toolkit supporting 66 human languages. Compared to existing widely used toolkits, Stanza features a language-agnostic fully neural pipeline for text analysis, including tokenization, multi-word token expansion, lemmatization, part-of-speech and morphological feature tagging, dependency parsing, and named entity recognition. We have trained Stanza on a total of 112 datasets, including the Universal Dependencies treebanks and other multilingual corpora, and show that the same neural architecture generalizes well and achieves competitive performance on all languages tested. Additionally, Stanza includes a native Python interface to the widely used Java Stanford CoreNLP software, which further extends its functionality to cover other tasks such as coreference resolution and relation extraction. Source code, documentation, and pretrained models for 66 languages are available at https://stanfordnlp.github.io/stanza/.', 'corpus_id': 212725611, 'score': 1}, {'doc_id': '218571287', 'title': 'Literature Triage on Genomic Variation Publications by Knowledge-enhanced Multi-channel CNN', 'abstract': 'Background: To investigate the correlation between genomic variation and certain diseases or phenotypes, the fundamental task is to screen out the concerning publications from massive literature, which is called literature triage. Some knowledge bases, including UniProtKB/Swiss-Prot and NHGRI-EBI GWAS Catalog are created for collecting concerning publications. These publications are manually curated by experts, which is time-consuming. Moreover, the manual curation of information from literature is not scalable due to the rapidly increasing amount of publications. In order to cut down the cost of literature triage, machine-learning models were adopted to automatically identify biomedical publications. Methods: Comparing to previous studies utilizing machine-learning models for literature triage, we adopt a multi-channel convolutional network to utilize rich textual information and meanwhile bridge the semantic gaps from different corpora. In addition, knowledge embeddings learned from UMLS is also used to provide extra medical knowledge beyond textual features in the process of triage. Results: We demonstrate that our model outperforms the state-of-the-art models over 5 datasets with the help of knowledge embedding and multiple channels. Our model improves the accuracy of biomedical literature triage results. Conclusions: Multiple channels and knowledge embeddings enhance the performance of the CNN model in the task of biomedical literature triage. Keywords: Literature Triage; Knowledge Embedding; Multi-channel Convolutional Network', 'corpus_id': 218571287, 'score': 0}, {'doc_id': '218470122', 'title': 'SciREX: A Challenge Dataset for Document-Level Information Extraction', 'abstract': 'Extracting information from full documents is an important problem in many domains, but most previous work focus on identifying relationships within a sentence or a paragraph. It is challenging to create a large-scale information extraction (IE) dataset at the document level since it requires an understanding of the whole document to annotate entities and their document-level relationships that usually span beyond sentences or even sections. In this paper, we introduce SciREX, a document level IE dataset that encompasses multiple IE tasks, including salient entity identification and document level N-ary relation identification from scientific articles. We annotate our dataset by integrating automatic and human annotations, leveraging existing scientific knowledge resources. We develop a neural model as a strong baseline that extends previous state-of-the-art IE models to document-level IE. Analyzing the model performance shows a significant gap between human performance and current baselines, inviting the community to use our dataset as a challenge to develop document-level IE models. Our data and code are publicly available at https://github.com/allenai/SciREX .', 'corpus_id': 218470122, 'score': 1}, {'doc_id': '220267240', 'title': 'Danish Clinical Event Extraction Developing a clinical event extraction system for electronic health records using deep learning and active learning', 'abstract': 'Danish electronic health records contain valuable information that is not fully utilized today. Given the unstructured nature of clinical texts, natural language processing techniques are required to extract relevant entities from these records. This study develops a deep learning based system capable of recognizing clinical events in Danish electronic health records. A total of 2.090 breast cancer-related sentences are annotated, comprising 42.753 words and 5.809 events. We fine-tune a pretrained multilingual BERT model on this data, reaching an F1-score of 87.5% on recognizing event spans. An active learning system is implemented for sentence querying to explore how the cost of annotation can be reduced. We implement two active learning strategies, uncertainty sampling and diversity sampling. We find that both strategies lead to a higher F1-score than random sampling when trained on the same amount of sentences. Specifically, we see an F1-score increase of 1.3% and 2.6% compared to random sampling, corresponding to an error reduction of 7.2% and 14.5%. However, in terms of the amount of annotated words, we find random sampling to be less costly than both active learning strategies. We test our fine-tuned BERT model on sentences related to pneumonia in order to establish its cross-domain capabilities. We observe a 5.7 percentage point drop in F1-score, leading to a score of 81.8%. While the benefits of active learning in terms of annotation cost remain unclear and require more research, these findings do prove the feasibility of a Danish clinical event extraction system.', 'corpus_id': 220267240, 'score': 0}, {'doc_id': '980182', 'title': 'BioCause: Annotating and analysing causality in the biomedical domain', 'abstract': 'BackgroundBiomedical corpora annotated with event-level information represent an important resource for domain-specific information extraction (IE) systems. However, bio-event annotation alone cannot cater for all the needs of biologists. Unlike work on relation and event extraction, most of which focusses on specific events and named entities, we aim to build a comprehensive resource, covering all statements of causal association present in discourse. Causality lies at the heart of biomedical knowledge, such as diagnosis, pathology or systems biology, and, thus, automatic causality recognition can greatly reduce the human workload by suggesting possible causal connections and aiding in the curation of pathway models. A biomedical text corpus annotated with such relations is, hence, crucial for developing and evaluating biomedical text mining.ResultsWe have defined an annotation scheme for enriching biomedical domain corpora with causality relations. This schema has subsequently been used to annotate 851 causal relations to form BioCause, a collection of 19 open-access full-text biomedical journal articles belonging to the subdomain of infectious diseases. These documents have been pre-annotated with named entity and event information in the context of previous shared tasks. We report an inter-annotator agreement rate of over 60% for triggers and of over 80% for arguments using an exact match constraint. These increase significantly using a relaxed match setting. Moreover, we analyse and describe the causality relations in BioCause from various points of view. This information can then be leveraged for the training of automatic causality detection systems.ConclusionAugmenting named entity and event annotations with information about causal discourse relations could benefit the development of more sophisticated IE systems. These will further influence the development of multiple tasks, such as enabling textual inference to detect entailments, discovering new facts and providing new hypotheses for experimental work.', 'corpus_id': 980182, 'score': 1}, {'doc_id': '122907719', 'title': 'Implementation of Non-Adiabatic Geometric Quantum Gate in Quantum Circuit', 'abstract': 'A scheme is investigated to realize non-adiabatic geometric quantum computation by directly varying the external parameters of quantum circuit. More intriguingly, we illustrate in detail how to control the parameters of the system to remove the dynamical phase. The scheme depends completely on manipulating both the circuit parameters and the magnetic field parameters, which does not require any rotation operation as done in the conventional geometric gates or the unconventional geometric gates.', 'corpus_id': 122907719, 'score': 0}, {'doc_id': '218628819', 'title': 'PERLEX: A Bilingual Persian-English Gold Dataset for Relation Extraction', 'abstract': 'Relation extraction is the task of extracting semantic relations between entities in a sentence. It is an essential part of some natural language processing tasks such as information extraction, knowledge extraction, and knowledge base population. The main motivations of this research stem from a lack of a dataset for relation extraction in the Persian language as well as the necessity of extracting knowledge from the growing big-data in the Persian language for different applications. In this paper, we present ""PERLEX"" as the first Persian dataset for relation extraction, which is an expert-translated version of the ""Semeval-2010-Task-8"" dataset. Moreover, this paper addresses Persian relation extraction utilizing state-of-the-art language-agnostic algorithms. We employ six different models for relation extraction on the proposed bilingual dataset, including a non-neural model (as the baseline), three neural models, and two deep learning models fed by multilingual-BERT contextual word representations. The experiments result in the maximum f-score 77.66% (provided by BERTEM-MTB method) as the state-of-the-art of relation extraction in the Persian language.', 'corpus_id': 218628819, 'score': 0}, {'doc_id': '211056463', 'title': 'BioRel: A Large-Scale Dataset for Biomedical Relation Extraction', 'abstract': 'Valuable biomedical knowledge usually exists in the form of electronic publications and literature, which is growing at an enormous rate. Relation extraction plays a critical role in discovering such knowledge and transform them into structural form. Previous relation extraction datasets in biomedical domain are mainly human-annotated, whose scales are usually limited due to their labor-intensive and time-consuming nature. In this paper, we present BioRel, a large-scale dataset constructed by using Unified Medical Language System (UMLS) as knowledge base and Medline as corpus. Entities in sentences of Medline are identified and linked to UMLS by Metamap. Relation label for each sentence is recognized using distant supervision. We adapt both state-of-the-art deep learning and statistical machine learning methods as baseline models and conduct comprehensive experiments on BioRel. Experimental results show that BioRel is suitable for training and evaluating relation extraction models for both deep learning and statistical methods by providing both reasonable baseline performance and many remaining challenges.', 'corpus_id': 211056463, 'score': 1}, {'doc_id': '218502187', 'title': 'FarsBase-KBP: A Knowledge Base Population System for the Persian Knowledge Graph', 'abstract': 'While most of the knowledge bases already support the English language, there is only one knowledge base for the Persian language, known as FarsBase, which is automatically created via semi-structured web information. Unlike English knowledge bases such as Wikidata, which have tremendous community support, the population of a knowledge base like FarsBase must rely on automatically extracted knowledge. Knowledge base population can let FarsBase keep growing in size, as the system continues working. In this paper, we present a knowledge base population system for the Persian language, which extracts knowledge from unlabeled raw text, crawled from the Web. The proposed system consists of a set of state-of-the-art modules such as an entity linking module as well as information and relation extraction modules designed for FarsBase. Moreover, a canonicalization system is introduced to link extracted relations to FarsBase properties. Then, the system uses knowledge fusion techniques with minimal intervention of human experts to integrate and filter the proper knowledge instances, extracted by each module. To evaluate the performance of the presented knowledge base population system, we present the first gold dataset for benchmarking knowledge base population in the Persian language, which consisting of 22015 FarsBase triples and verified by human experts. The evaluation results demonstrate the efficiency of the proposed system.', 'corpus_id': 218502187, 'score': 0}]"
24	DOI	b9ace5636941d59fe57af842507b8f82	9333	{'DOI': '1-(2,5-dimethoxy-4-iodophenyl)-2-aminopropane'}	"[{'doc_id': '221201647', 'title': 'Differential signaling signatures evoked by DOI versus lisuride stimulation of the 5-HT2A receptor.', 'abstract': 'The 5-HT2A receptor is a target for hallucinogenic and non-hallucinogenic ligands that evoke unique behavioral, electrophysiological and molecular consequences. Here, we explored the differential effects of distinct 5-HT2A receptor ligands on signaling pathways downstream to the 5-HT2A receptor. The hallucinogenic 5-HT2A receptor agonist DOI evoked an enhanced signaling response compared to the non-hallucinogenic 5-HT2A receptor agonist lisuride in human/rat 5-HT2AR-EGFP receptor expressing HEK293\xa0cell lines and cortical neuronal cultures. We noted higher levels of phospho-PLC, pPKC, pERK, pCaMKII, pCREB, as well as higher levels of IP3 and DAG production following 5-HT2A receptor stimulation with DOI. Our study reveals distinct signaling signatures, differing in magnitude and kinetics at the 5-HT2A receptor in response to DOI versus lisuride.', 'corpus_id': 221201647, 'score': 1}, {'doc_id': '54276126', 'title': 'P.1.g.022 Hallucinogenic and non-hallucinogenic 5-HT2A receptor agonists induce distinct patterns of G protein coupling in postmortem human brain', 'abstract': None, 'corpus_id': 54276126, 'score': 1}, {'doc_id': '221298949', 'title': 'Growth factors and SARS-CoV-2', 'abstract': 'Phosphoproteomics analysis reveals that blocking growth factor receptor signaling inhibits SARS-CoV-replication. Phosphoproteomics analysis reveals that blocking growth factor receptor signaling inhibits SARS-CoV-2 replication.', 'corpus_id': 221298949, 'score': 0}, {'doc_id': '222167821', 'title': 'SARS-CoV-2 spike protein co-opts VEGF-A/neuropilin-1 receptor signaling to induce analgesia', 'abstract': ""Supplemental Digital Content is Available in the Text. Severe acute respiratory syndrome coronavirus 2's spike protein promotes analgesia by interfering with vascular endothelial growth factor-A/NRP1 pathway, which may affect disease transmission dynamics."", 'corpus_id': 222167821, 'score': 0}, {'doc_id': '221140553', 'title': 'Subcellular hot spots of GPCR signaling promote vascular inflammation', 'abstract': '\n Abstract\n \n G-coupled protein receptors (GPCRs) comprise the largest class of druggable targets. Signaling by GPCRs is initiated from subcellular hot spots including the plasma membrane, signalosomes and endosomes to contribute to vascular inflammation. GPCR-G protein signaling at the plasma membrane causes endothelial barrier disruption and also cross-talks with growth factor receptors to promote proinflammatory signaling. A second surge of GPCR signaling is initiated by cytoplasmic NFκB activation mediated by β-arrestins and CARMA-Bcl10-MALT1 signalosomes. Once internalized, ubiquitinated GPCRs initiate signaling from endosomes via assembly of the transforming growth factor-β-activated kinase binding protein-1 (TAB1)-TAB2-p38 MAPK complex to promote vascular inflammation. Understanding the complexities of GPCR signaling is critical for development of new strategies to treat vascular inflammation such as that associated with COVID-19.\n \n', 'corpus_id': 221140553, 'score': 0}, {'doc_id': '13057908', 'title': 'Tolerance and Cross-Tolerance to Head Twitch Behavior Elicited by Phenethylamine- and Tryptamine-Derived Hallucinogens in Mice', 'abstract': 'The serotonin 5-hydroxytryptamine 2A (5-HT2A) receptor is a potential therapeutic target to a host of neuropsychiatric conditions, but agonist actions at this site are linked to abuse-related hallucinogenic effects that may limit therapeutic efficacy of chronic drug administration. Tolerance to some effects of hallucinogens has been observed in humans and laboratory animals, but the understanding of tolerance and cross-tolerance between distinct structural classes of hallucinogens is limited. Here, we used the drug-elicited head twitch response (HTR) in mice to assess the development of tolerance and cross-tolerance with two phenethylamine-derived [DOI (2,5-dimethoxy-4-iodoamphetamine) and 2C-T-7 (2,5-dimethoxy-4-propylthiophenethylamine)] and two tryptamine-derived [DPT (N,N-dipropyltryptamine) and DIPT (N,N-diisopropyltryptamine)] drugs with agonist affinity for 5-HT2A receptors. Tolerance developed to HTR elicited by daily DOI or 2C-T-7, but not to HTR elicited by DPT or DIPT. DOI-elicited tolerance was not surmountable with dose, and a similar insurmountable cross-tolerance was evident when DOI-tolerant mice were tested with various doses of 2C-T-7 or DPT. These studies suggest that the use of phenethylamine-derived hallucinogens as therapeutic agents may be limited not only by their abuse potential, but also by the rapid development of tolerance that would likely be maintained even if a patient were switched to a different 5-HT2A agonist medication from a distinct structural class. However, these experiments also imply that tryptamine-derived hallucinogens might have a reduced potential for tolerance development, compared with phenethylamine-derived 5-HT2A agonists, and might therefore be more suitable for chronic administration in a therapeutic context.', 'corpus_id': 13057908, 'score': 1}, {'doc_id': '220366007', 'title': 'Receptor-independent membrane mediated pathways of serotonin action', 'abstract': 'Serotonin is a neurotransmitter as well as a somatic signaling molecule, and the serotonergic system is a major target for psychotropic drugs. Serotonin, together with a few related neurotransmitters, has recently been found to exhibit an unexpectedly high lipid membrane affinity1–3. It has been conjectured that extrasynaptic serotonin can diffuse in the lipid membrane to efficiently reach remote receptors (and receptors with buried ligand-binding sites)4, providing a mechanism for the diffuse ‘volume’ neurotransmission that serotonin is capable of5–10. Here we show that membrane binding by serotonin can directly modulate membrane properties and cellular function, independent of its receptor-mediated actions. Atomic force microscopy shows that serotonin binding makes artificial lipid bilayers softer. It induces nucleation of liquid disordered domains inside the raft-like liquid-ordered domains in a ternary bilayer displaying phase separation. Solid-state NMR spectroscopy corroborates this data, revealing a rather homogeneous decrease in the order parameter of the lipid chains in the presence of serotonin. In the RN46A immortalized serotonergic neuronal cell line, extracellular serotonin enhances transferrin receptor endocytosis, an action exerted even in the presence of both broad-spectrum serotonin receptor and transporter inhibitors. Similarly, it increases the binding and internalization of Islet Amyloid Polypeptide (IAPP) oligomers, suggesting a connection between serotonin, which is co-secreted with IAPP by pancreatic beta cells, and the cellular effects of IAPP. Our results uncover a hitherto unknown serotonin-bilayer interaction that can potentiate key cellular processes in a receptor-independent fashion. Therefore, some pathways of serotonergic action may escape potent pharmaceutical agents designed for serotonin transporters or receptors. Conversely, bio-orthogonal serotonin-mimetics may provide a new class of cell-membrane modulators.', 'corpus_id': 220366007, 'score': 1}, {'doc_id': '221589141', 'title': 'Alfaxalone activates Human Pregnane-X Receptors with greater efficacy than Allopregnanolone: an in-vitro study with implications for neuroprotection during anesthesia', 'abstract': 'Background Alfaxalone is a fast acting intravenous anesthetic with high therapeutic index. It is an analogue of the naturally-occurring neurosteroid, allopregnanolone which has been implicated in causing neuroprotection, neurogenesis and preservation of cognition, through activation of pregnane X receptors in the central nervous system. This study investigated whether alfaxalone can activate human pregnane X receptors (h-PXR) as effectively as allopregnanolone. Methods Allopregnanolone and alfaxalone were dissolved in dimethyl sulfoxide to make allopregnanolone and alfaxalone treatment solutions (serial 3-fold dilution concentration range, 50,000 – 206 nM). Activation of h-PXR by these ligand solutions compared with vehicle control was measured by an in-vitro method using human embryonic kidney cells (HEK293) expressing h-PXR hybridised and linked to the firefly luciferase gene. Ligand binding with and activation of h-PXR in those cells caused downstream changes in luciferase activity and light emission. That activity was measured as relative light units using a plate-reading luminometer, thus quantifying the changes in h-PXR activity caused by the ligand applied to the HEK293 cells. Ligand log concentration response curves were constructed to compare efficacy and potency of allopregnanolone and alfaxalone. Results Allopregnanolone and alfaxalone both activated the h-PXR to cause dose-related light emission by the linked firefly luciferase. Control solutions (0.1% dimethyl sulfoxide) produced low level light emissions. Equimolar concentrations of alfaxalone were more efficacious in activation of h-PXR: 50,000 nM, p = 0.0019; 16,700 nM, p = 0.0472; 5,600 nM, p = 0.0031 [Brown-Forsythe and Welch ANOVA]. Conclusions Alfaxalone activates human-pregnane X receptors with greater efficacy compared with the endogenous ligand allopregnanolone. These results suggest that alfaxalone sedation and anesthesia may be accompanied by beneficial effects normally caused by the physiological effects of allopregnanolone, namely neuroprotection, neurogenesis, and preservation of cognition.', 'corpus_id': 221589141, 'score': 0}, {'doc_id': '206649856', 'title': 'Hallucinogens and Serotonin 5-HT2A Receptor-Mediated Signaling Pathways.', 'abstract': 'The neuropsychological effects of naturally occurring psychoactive chemicals have been recognized for millennia. Hallucinogens, which include naturally occurring chemicals such as mescaline and psilocybin, as well as synthetic compounds, such as lysergic acid diethylamide (LSD), induce profound alterations of human consciousness, emotion, and cognition. The discovery of the hallucinogenic effects of LSD and the observations that LSD and the endogenous ligand serotonin share chemical and pharmacological profiles led to the suggestion that biogenic amines like serotonin were involved in the psychosis of mental disorders such as schizophrenia. Although they bind other G protein-coupled receptor (GPCR) subtypes, studies indicate that several effects of hallucinogens involve agonist activity at the serotonin 5-HT2A receptor. In this chapter, we review recent advances in understanding hallucinogen drug action through characterization of structure, neuroanatomical location, and function of the 5-HT2A receptor.', 'corpus_id': 206649856, 'score': 1}, {'doc_id': '221140232', 'title': 'Angiotensin AT1 and AT2 receptor heteromer expression in the hemilesioned rat model of Parkinson’s disease that increases with levodopa-induced dyskinesia', 'abstract': 'The renin-angiotensin system (RAS) is altered in Parkinson’s disease (PD), a disease due to substantia nigra neurodegeneration and whose dopamine-replacement therapy, using the precursor levodopa, leads to dyskinesias as the main side effect. Angiotensin AT1 and AT2 receptors, mainly known for their role in regulating water homeostasis and blood pressure and able to form heterodimers (AT1/2Hets), are present in the central nervous system. We assessed the functionality and expression of AT1/2Hets in Parkinson disease (PD). Immunocytochemistry was used to analyze the colocalization between angiotensin receptors; bioluminescence resonance energy transfer was used to detect AT1/2Hets. Calcium and cAMP determination, MAPK activation, and label-free assays were performed to characterize signaling in homologous and heterologous systems. Proximity ligation assays were used to quantify receptor expression in mouse primary cultures and in rat striatal sections. We confirmed that AT1 and AT2 receptors form AT1/2Hets that are expressed in cells of the central nervous system. AT1/2Hets are novel functional units with particular signaling properties. Importantly, the coactivation of the two receptors in the heteromer reduces the signaling output of angiotensin. Remarkably, AT1/2Hets that are expressed in both striatal neurons and microglia make possible that candesartan, the antagonist of AT1, increases the effect of AT2 receptor agonists. In addition, the level of striatal expression increased in the unilateral 6-OH-dopamine lesioned rat PD model and was markedly higher in parkinsonian-like animals that did not become dyskinetic upon levodopa chronic administration if compared with expression in those that became dyskinetic. The results indicate that boosting the action of neuroprotective AT2 receptors using an AT1 receptor antagonist constitutes a promising therapeutic strategy in PD.', 'corpus_id': 221140232, 'score': 0}]"
25	Respiration - Oscillations	82035f9a48a50146e43233a9082df4da	14775	{}	[{'doc_id': '3008175', 'title': 'Selective entrainment of gamma subbands by different slow network oscillations', 'abstract': 'Significance Theta-gamma coupling has been largely documented in hippocampal and neocortical areas and hypothesized to constitute a network mechanism for information processing. However, we identify here another global slow rhythm at near-theta frequency that also couples to gamma. By simultaneously recording respiration, we could distinguish actual theta oscillations from a respiration-entrained rhythm (RR) in the local field potential whose peak frequency may overlap with theta. We demonstrate a robust specificity for the coupling of different gamma subbands to either theta or RR depending on brain state and region. The results suggest that the brain uses different frequency channels for transferring different types of information. Theta oscillations (4–12 Hz) are thought to provide a common temporal reference for the exchange of information among distant brain networks. On the other hand, faster gamma-frequency oscillations (30–160 Hz) nested within theta cycles are believed to underlie local information processing. Whether oscillatory coupling between global and local oscillations, as showcased by theta-gamma coupling, is a general coding mechanism remains unknown. Here, we investigated two different patterns of oscillatory network activity, theta and respiration-induced network rhythms, in four brain regions of freely moving mice: olfactory bulb (OB), prelimbic cortex (PLC), parietal cortex (PAC), and dorsal hippocampus [cornu ammonis 1 (CA1)]. We report differential state- and region-specific coupling between the slow large-scale rhythms and superimposed fast oscillations. During awake immobility, all four regions displayed a respiration-entrained rhythm (RR) with decreasing power from OB to CA1, which coupled exclusively to the 80- to 120-Hz gamma subband (γ2). During exploration, when theta activity was prevailing, OB and PLC still showed exclusive coupling of RR with γ2 and no theta-gamma coupling, whereas PAC and CA1 switched to selective coupling of theta with 40- to 80-Hz (γ1) and 120- to 160-Hz (γ3) gamma subbands. Our data illustrate a strong, specific interaction between neuronal activity patterns and respiration. Moreover, our results suggest that the coupling between slow and fast oscillations is a general brain mechanism not limited to the theta rhythm.', 'corpus_id': 3008175, 'score': 1}, {'doc_id': '233383068', 'title': 'Thalamocortical excitability modulation guides human perception under uncertainty', 'abstract': 'Knowledge about the relevance of environmental features can guide stimulus processing. However, it remains unclear how processing is adjusted when feature relevance is uncertain. We hypothesized that (a) heightened uncertainty would shift cortical networks from a rhythmic, selective processing-oriented state toward an asynchronous (“excited”) state that boosts sensitivity to all stimulus features, and that (b) the thalamus provides a subcortical nexus for such uncertainty-related shifts. Here, we had young adults attend to varying numbers of task-relevant features during EEG and fMRI acquisition to test these hypotheses. Behavioral modeling and electrophysiological signatures revealed that greater uncertainty lowered the rate of evidence accumulation for individual stimulus features, shifted the cortex from a rhythmic to an asynchronous/excited regime, and heightened neuromodulatory arousal. Crucially, this unified constellation of within-person effects was dominantly reflected in the uncertainty-driven upregulation of thalamic activity. We argue that neuromodulatory processes involving the thalamus play a central role in how the brain modulates neural excitability in the face of momentary uncertainty.', 'corpus_id': 233383068, 'score': 0}, {'doc_id': '233449793', 'title': 'Beta bursting in the retrosplenial cortex is a neurophysiological correlate of environmental novelty which is disrupted in a mouse model of Alzheimer’s disease', 'abstract': 'The retrosplenial cortex (RSC) plays a significant role in spatial learning and memory, and is functionally disrupted in the early stages of Alzheimer’s disease. In order to investigate neurophysiological correlates of spatial learning and memory in this region we employed in vivo electrophysiology in awake, behaving mice, comparing neural activity between wild-type and J20 mice, a mouse model of Alzheimer’s disease-associated amyloidopathy. To determine the response of the RSC to environmental novelty local field potentials were recorded while mice explored novel and familiar recording arenas. In familiar environments we detected short, phasic bursts of beta (20-30 Hz) oscillations (beta bursts) which arose at a low but steady rate. Exposure to a novel environment rapidly initiated a dramatic increase in the rate, size and duration of beta bursts. Additionally, theta-beta cross-frequency coupling was significantly higher during novelty, and spiking of neurons in the RSC was significantly enhanced during beta bursts. Finally, aberrant beta bursting was seen in J20 mice, including increased beta bursting during novelty and familiarity, yet a loss of coupling between beta bursts and spiking activity. These findings, support the concept that beta bursting may be responsible for the activation and reactivation of neuronal ensembles underpinning the formation and maintenance of cortical representations, and that disruptions to this activity in J20 mice may underlie cognitive impairments seen in these animals.', 'corpus_id': 233449793, 'score': 0}, {'doc_id': '6650980', 'title': 'Organization of prefrontal network activity by respiration-related oscillations', 'abstract': 'The medial prefrontal cortex (mPFC) integrates information from cortical and sub-cortical areas and contributes to the planning and initiation of behaviour. A potential mechanism for signal integration in the mPFC lies in the synchronization of neuronal discharges by theta (6–12\u2009Hz) activity patterns. Here we show, using in vivo local field potential (LFP) and single-unit recordings from awake mice, that prominent oscillations in the sub-theta frequency band (1–5\u2009Hz) emerge during awake immobility in the mPFC. These oscillation patterns are distinct from but phase-locked to hippocampal theta activity and occur synchronized with nasal respiration (hence termed prefrontal respiration rhythm [PRR]). PRR activity modulates the amplitude of prefrontal gamma rhythms with greater efficacy than theta oscillations. Furthermore, single-unit discharges of putative pyramidal cells and GABAergic interneurons are entrained by prefrontal PRR and nasal respiration. Our data thus suggest that PRR activity contributes to information processing in the prefrontal neuronal network.', 'corpus_id': 6650980, 'score': 1}, {'doc_id': '4485658', 'title': 'Respiration-Entrained Brain Rhythms Are Global but Often Overlooked', 'abstract': 'We revisit recent evidence showing that nasal respiration entrains oscillations at the same frequency as breathing in several regions of the rodent brain. Moreover, respiration modulates the amplitude of a specific gamma sub-band (70-120Hz), most prominently in frontal regions. Since rodents often breathe at delta and theta frequencies, we caution that previous studies on delta and theta power and their cross-regional synchrony, as well as on delta-gamma and theta-gamma coupling, may have detected the respiration-entrained rhythm and respiration-gamma coupling. We argue that the simultaneous tracking of respiration along with electrophysiological recordings is necessary to properly identify brain oscillations. We hypothesize that respiration-entrained oscillations aid long-range communication in the brain.', 'corpus_id': 4485658, 'score': 1}, {'doc_id': '27777215', 'title': 'Nasal Respiration Entrains Human Limbic Oscillations and Modulates Cognitive Function', 'abstract': 'The need to breathe links the mammalian olfactory system inextricably to the respiratory rhythms that draw air through the nose. In rodents and other small animals, slow oscillations of local field potential activity are driven at the rate of breathing (∼2–12 Hz) in olfactory bulb and cortex, and faster oscillatory bursts are coupled to specific phases of the respiratory cycle. These dynamic rhythms are thought to regulate cortical excitability and coordinate network interactions, helping to shape olfactory coding, memory, and behavior. However, while respiratory oscillations are a ubiquitous hallmark of olfactory system function in animals, direct evidence for such patterns is lacking in humans. In this study, we acquired intracranial EEG data from rare patients (Ps) with medically refractory epilepsy, enabling us to test the hypothesis that cortical oscillatory activity would be entrained to the human respiratory cycle, albeit at the much slower rhythm of ∼0.16–0.33 Hz. Our results reveal that natural breathing synchronizes electrical activity in human piriform (olfactory) cortex, as well as in limbic-related brain areas, including amygdala and hippocampus. Notably, oscillatory power peaked during inspiration and dissipated when breathing was diverted from nose to mouth. Parallel behavioral experiments showed that breathing phase enhances fear discrimination and memory retrieval. Our findings provide a unique framework for understanding the pivotal role of nasal breathing in coordinating neuronal oscillations to support stimulus processing and behavior. SIGNIFICANCE STATEMENT Animal studies have long shown that olfactory oscillatory activity emerges in line with the natural rhythm of breathing, even in the absence of an odor stimulus. Whether the breathing cycle induces cortical oscillations in the human brain is poorly understood. In this study, we collected intracranial EEG data from rare patients with medically intractable epilepsy, and found evidence for respiratory entrainment of local field potential activity in human piriform cortex, amygdala, and hippocampus. These effects diminished when breathing was diverted to the mouth, highlighting the importance of nasal airflow for generating respiratory oscillations. Finally, behavioral data in healthy subjects suggest that breathing phase systematically influences cognitive tasks related to amygdala and hippocampal functions.', 'corpus_id': 27777215, 'score': 1}, {'doc_id': '232272044', 'title': 'Laminar Profile of Auditory Steady-State Response in the Auditory Cortex of Awake Mice', 'abstract': 'Objective Auditory steady-state response (ASSR) is a gamma oscillation evoked by periodic auditory stimuli, which is commonly used in clinical electroencephalographic examination to evaluate the neurological functions. Though it has been suggested that auditory cortex is the origin of ASSR, how the laminar architecture of the neocortex contributes to the ASSR recorded from the brain surface remains unclear. Methods We used a 16-channel silicon probe to record the local field potential and the single-unit spike activity in the different layers of the auditory cortex of unanesthetized mice. Click-trains with a repetition rate at 40-Hz were present as sound stimuli to evoke ASSR. Results We found that the LFPs of all cortical layers showed a stable ASSR synchronizing to the 40-Hz click stimuli, while the ASSR was strongest in the granular (thalamorecipient) layer. Furthermore, time-frequency analyses also revealed the strongest coherence between the signals recorded from the granular layer and pial surface. Conclusion Our results reveal that the 40-Hz ASSR primarily shows the evoked gamma oscillation of thalamorecipient layers in the neocortex, and that the ASSR may be a biomarker to detect the cognitive deficits associated with impaired thalamo-cortical connection.', 'corpus_id': 232272044, 'score': 0}, {'doc_id': '233175685', 'title': 'Neocortical rhythm entrainment by parvalbumin-positive interneurons across cortical layers', 'abstract': 'Neocortical interneurons provide local inhibition responsible for organizing neuronal activity into brain oscillations that subserve several functions such as memory, attention and neuronal communication. However, little is known about the contribution of interneurons to the entrainment of neocortical oscillations across cortical layers. Here, using layer-specific optogenetic stimulations with micro-Light-Emitting-Diode (μLED) arrays, directed toward parvalbumin-expressing (PV) interneurons in non-anesthetized awake mice, we find that supragranular layer stimulations of PV neurons were most efficient at entraining supragranular layer neurons and local field potential (LFP) oscillations at gamma frequencies (γ: 25 - 80 Hz), whereas infragranular layer stimulation of PV neurons better entrained delta (δ: 2 - 5 Hz) and theta (θ: 6 - 10 Hz) frequency LFP oscillations. We found that PV neurons tightly control the transmission of multiple rhythms to the network across cortical layers in an orientation-selective manner. Intrinsic resonant properties of neurons could underlie such layer-specific properties of rhythm entrainment.', 'corpus_id': 233175685, 'score': 0}, {'doc_id': '16220287', 'title': 'Breathing as a Fundamental Rhythm of Brain Function', 'abstract': 'Ongoing fluctuations of neuronal activity have long been considered intrinsic noise that introduces unavoidable and unwanted variability into neuronal processing, which the brain eliminates by averaging across population activity (Georgopoulos et al., 1986; Lee et al., 1988; Shadlen and Newsome, 1994; Maynard et al., 1999). It is now understood, that the seemingly random fluctuations of cortical activity form highly structured patterns, including oscillations at various frequencies, that modulate evoked neuronal responses (Arieli et al., 1996; Poulet and Petersen, 2008; He, 2013) and affect sensory perception (Linkenkaer-Hansen et al., 2004; Boly et al., 2007; Sadaghiani et al., 2009; Vinnik et al., 2012; Palva et al., 2013). Ongoing cortical activity is driven by proprioceptive and interoceptive inputs. In addition, it is partially intrinsically generated in which case it may be related to mental processes (Fox and Raichle, 2007; Deco et al., 2011). Here we argue that respiration, via multiple sensory pathways, contributes a rhythmic component to the ongoing cortical activity. We suggest that this rhythmic activity modulates the temporal organization of cortical neurodynamics, thereby linking higher cortical functions to the process of breathing.', 'corpus_id': 16220287, 'score': 1}, {'doc_id': '234348738', 'title': 'Theta phase mediates deliberate action switching in human Supplementary Motor Areas', 'abstract': 'The ability to deliberately overwrite ongoing automatic actions is a necessary feature of adaptive behavior. It has been proposed that the supplementary motor areas (SMAs) operate as a controller that orchestrates the switching between automatic and deliberate processes by inhibiting ongoing behaviors and so facilitating the execution of alternative ones. In addition, previous studies support the involvement of SMAs theta waves (4-9 Hz) in cognitive control. However, the exact role of such oscillatory dynamics and their contribution to the control of action are not fully understood. To investigate the mechanisms by which the SMAs support direct control of deliberate behavior, we recorded intracranial electroencephalography (iEEG) activity in humans performing a motor sequence task. Subjects had to perform a “change of plans” motor task requiring habitual movements to be overwritten at unpredictable moments. We found that SMAs were exclusively active during trials that demand action reprogramming in response to the unexpected cue but were silent during automatic action execution. Importantly, SMAs activity was characterized by a distinct temporal pattern, expressed in a stereotypical phase alignment of theta oscillations. More specifically, single trial motor performance was correlated with the trial contribution to the global inter-trial phase coherence, with higher coherence associated with faster trials. In addition, theta phase modulated the amplitude of gamma oscillations, with higher cross-frequency coupling in faster trials. Our results suggest that within frontal cortical networks, theta oscillations could encode a control signal that promotes the execution of deliberate actions.', 'corpus_id': 234348738, 'score': 0}]
26	Hi-Dim Viz	12ad1b192becbe023ceb856131796d43	5543	{}	"[{'doc_id': '86660238', 'title': 'Dimensionality-Reduction Algorithms for Progressive Visual Analytics', 'abstract': 'Visual analysis of high dimensional data is a challenging process. Direct visualizations work well for a few dimensions but do not scale to the hundreds or thousands of dimensions that have become increasingly common in current data analytics problems. Visual analytics is the science of analytical reasoning facilitated by interactive visual interfaces, and it has been proven as an effective tool for high dimensional data analysis. In visual analytics systems, several visualizations are jointly analyzed in order to discover patterns in the data. One of the fundamental tools that has been integrated in visual analytics, is nonlinear dimensionality-reduction; a tool for the indirect visualization aimed at the discovery and analysis of non-linear patterns in the high-dimensional data. However, the computational complexity of non-linear dimensionality-reduction techniques does not allow direct employment in interactive systems. This limitation makes the analytic process a time-consuming task that can take hours, days or even weeks to be performed. In this thesis, we present novel algorithmic solutions that enable integration of non-linear dimensionality-reduction techniques in visual analytics systems. Our proposed algorithms are, not only much faster than existing solutions, but provide richer insights into the data at hand. This result, is achieved by introducing new data processing and optimization techniques and by embracing the recently introduced concept of Progressive Visual Analytics; a computational paradigm that enables the interactivity of complex analytics techniques by means of visualization as well as interaction with intermediate results. Moreover, we present several applications that are designed to provide unprecedented analytical capabilities in several domains. These applications are powered by the algorithms introduced in this dissertation and led to several discoveries in areas ranging from the biomedical research field, to social-network data analysis and machine-learning models interpretability.', 'corpus_id': 86660238, 'score': 1}, {'doc_id': '214802306', 'title': 'High-Dimensional Data Set Simplification by Laplace-Beltrami Operator', 'abstract': 'With the development of the Internet and other digital technologies, the speed of data generation has become considerably faster than the speed of data processing. Because big data typically contain massive redundant information, it is possible to significantly simplify a big data set while maintaining the key information it contains. In this paper, we develop a big data simplification method based on the eigenvalues and eigenfunctions of the Laplace-Beltrami operator (LBO). Specifically, given a data set that can be considered as an unorganized data point set in high-dimensional space, a discrete LBO defined on the big data set is constructed and its eigenvalues and eigenvectors are calculated. Then, the local extremum and the saddle points of the eigenfunctions are proposed to be the feature points of a data set in high-dimensional space, constituting a simplified data set. Moreover, we develop feature point detection methods for the functions defined on an unorganized data point set in high-dimensional space, and devise metrics for measuring the fidelity of the simplified data set to the original set. Finally, examples and applications are demonstrated to validate the efficiency and effectiveness of the proposed methods, demonstrating that data set simplification is a method for processing a maximum-sized data set using a limited data processing capability.', 'corpus_id': 214802306, 'score': 1}, {'doc_id': '218674158', 'title': 'A Picture for the Words! Textual Visualization in Big Data Analytics', 'abstract': 'Data Visualization has become an important aspect of big data analytics and has grown in sophistication and variety. We specifically identify the need for an analytical framework for data visualization with textual information. Data visualization is a powerful mechanism to represent data, but the usage of specific graphical representations needs to be better understood and classified to validate appropriate representation in the contexts of textual data and avoid distorted depictions of underlying textual data. We identify prominent textual data visualization approaches and discuss their characteristics. We discuss the use of multiple graph types in textual data visualization, including the use of quantity, sense, trend and context textual data visualization. We create an explanatory classification framework to position textual data visualization in a unique way so as to provide insights and assist in appropriate method or graphical representation classification.', 'corpus_id': 218674158, 'score': 0}, {'doc_id': '15651095', 'title': 'Visualizing High‐Dimensional Structures by Dimension Ordering and Filtering using Subspace Analysis', 'abstract': 'High‐dimensional data visualization is receiving increasing interest because of the growing abundance of high‐dimensional datasets. To understand such datasets, visualization of the structures present in the data, such as clusters, can be an invaluable tool. Structures may be present in the full high‐dimensional space, as well as in its subspaces. Two widely used methods to visualize high‐dimensional data are the scatter plot matrix (SPM) and the parallel coordinate plot (PCP). SPM allows a quick overview of the structures present in pairwise combinations of dimensions. On the other hand, PCP has the potential to visualize not only bi‐dimensional structures but also higher dimensional ones. A problem with SPM is that it suffers from crowding and clutter which makes interpretation hard. Approaches to reduce clutter are available in the literature, based on changing the order of the dimensions. However, usually this reordering has a high computational complexity. For effective visualization of high‐dimensional structures, also PCP requires a proper ordering of the dimensions.', 'corpus_id': 15651095, 'score': 1}, {'doc_id': '58605055', 'title': 'HyperTools: A Python toolbox for visualizing and manipulating high-dimensional data', 'abstract': 'Data visualizations can reveal trends and patterns that are not otherwise obvious from the raw data or summary statistics. While visualizing low-dimensional data is relatively straightforward (for example, plotting the change in a variable over time as (x,y) coordinates on a graph), it is not always obvious how to visualize high-dimensional datasets in a similarly intuitive way. Here we present HypeTools, a Python toolbox for visualizing and manipulating large, high-dimensional datasets. Our primary approach is to use dimensionality reduction techniques (Pearson, 1901; Tipping & Bishop, 1999) to embed high-dimensional datasets in a lower-dimensional space, and plot the data using a simple (yet powerful) API with many options for data manipulation [e.g. hyperalignment (Haxby et al., 2011), clustering, normalizing, etc.] and plot styling. The toolbox is designed around the notion of data trajectories and point clouds. Just as the position of an object moving through space can be visualized as a 3D trajectory, HyperTools uses dimensionality reduction algorithms to create similar 2D and 3D trajectories for time series of high-dimensional observations. The trajectories may be plotted as interactive static plots or visualized as animations. These same dimensionality reduction and alignment algorithms can also reveal structure in static datasets (e.g. collections of observations or attributes). We present several examples showcasing how using our toolbox to explore data through trajectories and low-dimensional embeddings can reveal deep insights into datasets across a wide variety of domains.', 'corpus_id': 58605055, 'score': 1}, {'doc_id': '218949248', 'title': 'BrePartition: Optimized High-Dimensional kNN Search with Bregman Distances', 'abstract': 'Bregman distances (also known as Bregman divergences) are widely used in machine learning, speech recognition and signal processing, and kNN searches with Bregman distances have become increasingly important with the rapid advances of multimedia applications. Data in multimedia applications such as images and videos are commonly transformed into space of hundreds of dimensions. Such high-dimensional space has posed significant challenges for existing kNN search algorithms with Bregman distances, which could only handle data of medium dimensionality (typically less than 100). This paper addresses the urgent problem of high-dimensional kNN search with Bregman distances. We propose a novel partition-filter-refinement framework. Specifically, we propose an optimized dimensionality partitioning scheme to solve several non-trivial issues. First, an effective bound from each partitioned subspace to obtain exact kNN results is derived. Second, we conduct an in-depth analysis of the optimized number of partitions and devise an effective strategy for partitioning. Third, we design an efficient integrated index structure for all the subspaces together to accelerate the search processing. Moreover, we extend our exact solution to an approximate version by a trade-off between the accuracy and efficiency. Experimental results on four real-world datasets and two synthetic datasets show the clear advantage of our method in comparison to state-of-the-art algorithms.', 'corpus_id': 218949248, 'score': 0}, {'doc_id': '219177100', 'title': 'Clustering-informed Cinematic Astrophysical Data Visualization with Application to the Moon-forming Terrestrial Synestia', 'abstract': 'Scientific visualization tools are currently not optimized to create cinematic, production-quality representations of numerical data for the purpose of science communication. In our pipeline \\texttt{Estra}, we outline a step-by-step process from a raw simulation into a finished render as a way to teach non-experts in the field of visualization how to achieve production-quality outputs on their own. We demonstrate feasibility of using the visual effects software Houdini for cinematic astrophysical data visualization, informed by machine learning clustering algorithms. To demonstrate the capabilities of this pipeline, we used a post-impact, thermally-equilibrated Moon-forming synestia from \\cite{Lock18}. Our approach aims to identify ""physically interpretable"" clusters, where clusters identified in an appropriate phase space (e.g. here we use a temperature-entropy phase-space) correspond to physically meaningful structures within the simulation data. Clustering results can then be used to highlight these structures by informing the color-mapping process in a simplified Houdini software shading network, where dissimilar phase-space clusters are mapped to different color values for easier visual identification. Cluster information can also be used in 3D position space, via Houdini\'s Scene View, to aid in physical cluster finding, simulation prototyping, and data exploration. Our clustering-based renders are compared to those created by the Advanced Visualization Lab (AVL) team for the full dome show ""Imagine the Moon"" as proof of concept. With \\texttt{Estra}, scientists have a tool to create their own production-quality, data-driven visualizations.', 'corpus_id': 219177100, 'score': 0}, {'doc_id': '218889503', 'title': 'HiVision: Rapid Visualization of Large-Scale Spatial Vector Data', 'abstract': 'Rapid visualization of large-scale spatial vector data is a long-standing challenge in Geographic Information Science. In existing methods, the computation overheads grow rapidly with data volumes, leading to the incapability of providing real-time visualization for large-scale spatial vector data, even with parallel acceleration technologies. To fill the gap, we present HiVision, a display-driven visualization model for large-scale spatial vector data. Different from traditional data-driven methods, the computing units in HiVision are pixels rather than spatial objects to achieve real-time performance, and efficient spatial-index-based strategies are introduced to estimate the topological relationships between pixels and spatial objects. HiVision can maintain exceedingly good performance regardless of the data volume due to the stable pixel number for display. In addition, an optimized parallel computing architecture is proposed in HiVision to ensure the ability of real-time visualization. Experiments show that our approach outperforms traditional methods in rendering speed and visual effects while dealing with large-scale spatial vector data, and can provide interactive visualization of datasets with billion-scale points/segments/edges in real-time with flexible rendering styles. The HiVision code is open-sourced at this https URL with an online demonstration.', 'corpus_id': 218889503, 'score': 0}, {'doc_id': '15167460', 'title': 'Visualizing Large-scale and High-dimensional Data', 'abstract': 'We study the problem of visualizing large-scale and high-dimensional data in a low-dimensional (typically 2D or 3D) space. Much success has been reported recently by techniques that first compute a similarity structure of the data points and then project them into a low-dimensional space with the structure preserved. These two steps suffer from considerable computational costs, preventing the state-of-the-art methods such as the t-SNE from scaling to large-scale and high-dimensional data (e.g., millions of data points and hundreds of dimensions). We propose the LargeVis, a technique that first constructs an accurately approximated K-nearest neighbor graph from the data and then layouts the graph in the low-dimensional space. Comparing to t-SNE, LargeVis significantly reduces the computational cost of the graph construction step and employs a principled probabilistic model for the visualization step, the objective of which can be effectively optimized through asynchronous stochastic gradient descent with a linear time complexity. The whole procedure thus easily scales to millions of high-dimensional data points. Experimental results on real-world data sets demonstrate that the LargeVis outperforms the state-of-the-art methods in both efficiency and effectiveness. The hyper-parameters of LargeVis are also much more stable over different data sets.', 'corpus_id': 15167460, 'score': 1}, {'doc_id': '218487412', 'title': 'Stochastic Neighbor Embedding of Multimodal Relational Data for Image-Text Simultaneous Visualization', 'abstract': 'Multimodal relational data analysis has become of increasing importance in recent years, for exploring across different domains of data, such as images and their text tags obtained from social networking services (e.g., Flickr). A variety of data analysis methods have been developed for visualization; to give an example, t-Stochastic Neighbor Embedding (t-SNE) computes low-dimensional feature vectors so that their similarities keep those of the observed data vectors. However, t-SNE is designed only for a single domain of data but not for multimodal data; this paper aims at visualizing multimodal relational data consisting of data vectors in multiple domains with relations across these vectors. By extending t-SNE, we herein propose Multimodal Relational Stochastic Neighbor Embedding (MR-SNE), that (1) first computes augmented relations, where we observe the relations across domains and compute those within each of domains via the observed data vectors, and (2) jointly embeds the augmented relations to a low-dimensional space. Through visualization of Flickr and Animal with Attributes 2 datasets, proposed MR-SNE is compared with other graph embedding-based approaches; MR-SNE demonstrates the promising performance.', 'corpus_id': 218487412, 'score': 0}]"
27	Computational Cognition	0098e5cf525c5705cfad65e631b4cbfc	8172	{}	"[{'doc_id': '15911191', 'title': 'Beyond Memorability: Visualization Recognition and Recall', 'abstract': ""In this paper we move beyond memorability and investigate how visualizations are recognized and recalled. For this study we labeled a dataset of 393 visualizations and analyzed the eye movements of 33 participants as well as thousands of participant-generated text descriptions of the visualizations. This allowed us to determine what components of a visualization attract people's attention, and what information is encoded into memory. Our findings quantitatively support many conventional qualitative design guidelines, including that (1) titles and supporting text should convey the message of a visualization, (2) if used appropriately, pictograms do not interfere with understanding and can improve recognition, and (3) redundancy helps effectively communicate the message. Importantly, we show that visualizations memorable “at-a-glance” are also capable of effectively conveying the message of the visualization. Thus, a memorable visualization is often also an effective one."", 'corpus_id': 15911191, 'score': 1}, {'doc_id': '115153204', 'title': 'The perceptual neural trace of memorable unseen scenes', 'abstract': 'Some scenes are more memorable than others: they cement in minds with consistencies across observers and time scales. While memory mechanisms are traditionally associated with the end stages of perception, recent behavioral studies suggest that the features driving these memorability effects are extracted early on, and in an automatic fashion. This raises the question: is the neural signal of memorability detectable during early perceptual encoding phases of visual processing? Using the high temporal resolution of magnetoencephalography (MEG), during a rapid serial visual presentation (RSVP) task, we traced the neural temporal signature of memorability across the brain. We found an early and prolonged memorability related signal under a challenging ultra-rapid viewing condition, across a network of regions in both dorsal and ventral streams. This enhanced encoding could be the key to successful storage and recognition.', 'corpus_id': 115153204, 'score': 1}, {'doc_id': '93619627', 'title': 'Hipólito de Eurípides: una visión política de la alteridad', 'abstract': 'Resumen es: La relacion entre pretendientes, nobles y servidores resulta muy compleja en Odisea. Dentro del expandido mundo domestico que el poema presenta no se ha...', 'corpus_id': 93619627, 'score': 0}, {'doc_id': '144787413', 'title': 'Use of Video Modeling to Teach Vocational Skills to Adolescents and Young Adults with Autism Spectrum Disorders', 'abstract': 'As part of a collaborative project between a University Center for Excellence in Developmental Disabilities and a local private business, we examined the effects of video modeling to teach vocational skills to four adolescents and young adults with Autism Spectrum Disorders. Video modeling was used to teach the participants to wear a WalkAround® mascot and entertain customers in a retail setting. Observations were conducted before and after participants watched a video model of the skills performed in both scripted and naturalistic scenes. All participants learned to use the targeted skills after watching the video model and all reported that they enjoyed the work. Implications and vocational applications are discussed.', 'corpus_id': 144787413, 'score': 0}, {'doc_id': '220363729', 'title': 'BézierSketch: A generative model for scalable vector sketches', 'abstract': 'The study of neural generative models of human sketches is a fascinating contemporary modeling problem due to the links between sketch image generation and the human drawing process. The landmark SketchRNN provided breakthrough by sequentially generating sketches as a sequence of waypoints. However this leads to low-resolution image generation, and failure to model long sketches. In this paper we present BezierSketch, a novel generative model for fully vector sketches that are automatically scalable and high-resolution. To this end, we first introduce a novel inverse graphics approach to stroke embedding that trains an encoder to embed each stroke to its best fit Bezier curve. This enables us to treat sketches as short sequences of paramaterized strokes and thus train a recurrent sketch generator with greater capacity for longer sketches, while producing scalable high-resolution results. We report qualitative and quantitative results on the Quick, Draw! benchmark.', 'corpus_id': 220363729, 'score': 0}, {'doc_id': '219966723', 'title': 'Image Sentiment Transfer', 'abstract': 'In this work, we introduce an important but still unexplored research task -- image sentiment transfer. Compared with other related tasks that have been well-studied, such as image-to-image translation and image style transfer, transferring the sentiment of an image is more challenging. Given an input image, the rule to transfer the sentiment of each contained object can be completely different, making existing approaches that perform global image transfer by a single reference image inadequate to achieve satisfactory performance. In this paper, we propose an effective and flexible framework that performs image sentiment transfer at the object level. It first detects the objects and extracts their pixel-level masks, and then performs object-level sentiment transfer guided by multiple reference images for the corresponding objects. For the core object-level sentiment transfer, we propose a novel Sentiment-aware GAN (SentiGAN). Both global image-level and local object-level supervisions are imposed to train SentiGAN. More importantly, an effective content disentanglement loss cooperating with a content alignment step is applied to better disentangle the residual sentiment-related information of the input image. Extensive quantitative and qualitative experiments are performed on the object-oriented VSO dataset we create, demonstrating the effectiveness of the proposed framework.', 'corpus_id': 219966723, 'score': 0}, {'doc_id': '18742872', 'title': 'Patients with schizophrenia are biased toward low spatial frequency to decode facial expression at a glance', 'abstract': 'Whereas patients with schizophrenia exhibit early visual processing impairments, their capacity at integrating visual information at various spatial scales, from low to high spatial frequencies, remains untested. This question is particularly acute given that, in ecological conditions of viewing, spatial frequency bands are naturally integrated to form a coherent percept. Here, 19 patients with schizophrenia and 16 healthy controls performed a rapid emotion recognition task with hybrid faces. Because these stimuli displayed in a single image two different facial expressions, in low (LSF) and high (HSF) spatial frequencies, the selected emotion probes which spatial scale is preferentially perceived. In a control experiment participants performed the same task with either low or high spatial frequency filtered faces. Results show that patients have a strong bias towards LSF with hybrid faces compared to healthy controls. However, both patients and healthy controls performed better with HSF filtered faces than with LSF filtered faces in the control experiment, demonstrating that the bias found with hybrid stimuli in patients was not due to an inability to process HSF. Whereas previous works found a LSF contrast deficit in schizophrenia, our results suggest a deficit in the normal time course of concurrently perceiving LSF and HSF. This early visual processing impairment is likely to contribute to the difficulties of patients with schizophrenia with facial processing and therefore social interaction.', 'corpus_id': 18742872, 'score': 1}, {'doc_id': '220496304', 'title': 'Applying recent advances in Visual Question Answering to Record Linkage', 'abstract': 'Multi-modal Record Linkage is the process of matching multi-modal records from multiple sources that represent the same entity. This field has not been explored in research and we propose two solutions based on Deep Learning architectures that are inspired by recent work in Visual Question Answering. The neural networks we propose use two different fusion modules, the Recurrent Neural Network + Convolutional Neural Network fusion module and the Stacked Attention Network fusion module, that jointly combine the visual and the textual data of the records. The output of these fusion models is the input of a Siamese Neural Network that computes the similarity of the records. Using data from the Avito Duplicate Advertisements Detection dataset, we train these solutions and from the experiments, we concluded that the Recurrent Neural Network + Convolutional Neural Network fusion module outperforms a simple model that uses hand-crafted features. We also find that the Recurrent Neural Network + Convolutional Neural Network fusion module classifies dissimilar advertisements as similar more frequently if their average description is bigger than 40 words. We conclude that the reason for this is that the longer advertisements have a different distribution then the shorter advertisements who are more prevalent in the dataset. In the end, we also conclude that further research needs to be done with the Stacked Attention Network, to further explore the effects of the visual data on the performance of the fusion modules.', 'corpus_id': 220496304, 'score': 0}, {'doc_id': '51963310', 'title': 'Memorable words are monogamous: The role of synonymy and homonymy in word recognition memory', 'abstract': 'What makes a word memorable? Prior research has identified numerous factors: word frequency, concreteness, imageability, and valence have all been shown to affect recognition performance. One important dimension that has not received much attention is the nature of the relationship between words and meanings. Under the hypothesis that words are encoded primarily by their meanings, and not by their surface forms, this relationship should be central to determining word memorability. In particular, rational analysis suggests that people will more easily remember words that convey a large amount of information about their intended meaning and that have few alternatives – that is, memorable words will be those with few possible meanings and synonyms. To test this hypothesis, we ran two large-scale recognition memory experiments (each with 2,222 words, 600+ participants). Memory performance was overall high, on par with memory for pictures in a similar paradigm. Critically, however, not all words were remembered equally well. Consistent with our proposal, the best recognized words had few meanings and few synonyms. Indeed, the most memorable words had a one-to-one relationship with their meanings. Estimates of memorability derived from this rational account explain a large amount of the variance in word memorability.', 'corpus_id': 51963310, 'score': 1}, {'doc_id': '8612834', 'title': 'High-level aftereffects to global scene properties.', 'abstract': ""Adaptation is ubiquitous in the human visual system, allowing recalibration to the statistical regularities of its input. Previous work has shown that global scene properties such as openness and mean depth are informative dimensions of natural scene variation useful for human and machine scene categorization (Greene & Oliva, 2009b; Oliva & Torralba, 2001). A visual system that rapidly categorizes scenes using such statistical regularities should be continuously updated, and therefore is prone to adaptation along these dimensions. Using a rapid serial visual presentation paradigm, we show aftereffects to several global scene properties (magnitude 8-21%). In addition, aftereffects were preserved when the test image was presented 10 degrees away from the adapted location, suggesting that the origin of these aftereffects is not solely due to low-level adaptation. We show systematic modulation of observers' basic-level scene categorization performances after adapting to a global property, suggesting a strong representational role of global properties in rapid scene categorization."", 'corpus_id': 8612834, 'score': 1}]"
28	Memory Management	fd3cbd78bed8cf1ca02da7abadef5f9b	15429	{}	[{'doc_id': '3006125', 'title': 'A type theory for memory allocation and data layout', 'abstract': 'Ordered type theory is an extension of linear type theory in which variables in the context may be neither dropped nor re-ordered. This restriction gives rise to a natural notion of adjacency. We show that a language based on ordered types can use this property to give an exact account of the layout of data in memory. The fuse constructor from ordered logic describes adjacency of values in memory, and the mobility modal describes pointers into the heap. We choose a particular allocation model based on a common implementation scheme for copying garbage collection and show how this permits us to separate out the allocation and initialization of memory locations in such a way as to account for optimizations such as the coalescing of multiple calls to the allocator.', 'corpus_id': 3006125, 'score': 1}, {'doc_id': '232478760', 'title': 'Idris 2: Quantitative Type Theory in Practice', 'abstract': 'Dependent types allow us to express precisely what a function is intended to do. Recent work on Quantitative Type Theory (QTT) extends dependent type systems with linearity, also allowing precision in expressing when a function can run. This is promising, because it suggests the ability to design and reason about resource usage protocols, such as we might find in distributed and concurrent programming, where the state of a communication channel changes throughout program execution. As yet, however, there has not been a full-scale programming language with which to experiment with these ideas. Idris 2 is a new version of the dependently typed language Idris, with a new core language based on QTT, supporting linear and dependent types. In this paper, we introduce Idris 2, and describe how QTT has influenced its design. We give examples of the benefits of QTT in practice including: expressing which data is erased at run time, at the type level; and, resource tracking in the type system leading to type-safe concurrent programming with session types. 2012 ACM Subject Classification Software and its engineering → Functional languages', 'corpus_id': 232478760, 'score': 0}, {'doc_id': '232068782', 'title': 'AwkwardForth: accelerating Uproot with an internal DSL', 'abstract': 'File formats for generic data structures, such as ROOT, Avro, and Parquet, pose a problem for deserialization: it must be fast, but its code depends on the type of the data structure, not known at compile-time. Just-in-time compilation can satisfy both constraints, but we propose a more portable solution: specialized virtual machines. AwkwardForth is a Forth-driven virtual machine for deserializing data into Awkward Arrays. As a language, it is not intended for humans to write, but it loosens the coupling between Uproot and Awkward Array. AwkwardForth programs for deserializing record-oriented formats (ROOT and Avro) are about as fast as C++ ROOT and 10–80× faster than fastavro. Columnar formats (simple TTrees, RNTuple, and Parquet) only require specialization to interpret metadata and are therefore faster with precompiled code.', 'corpus_id': 232068782, 'score': 0}, {'doc_id': '232177318', 'title': 'Functional Reactive Programming with nothing but Promises Implementing Push/Pull FRP using JavaScript Promises', 'abstract': 'Functional Reactive Programming (FRP) is a model of reactive programming defined by having a well-defined semantics given by time-indexed values. Promises are one-shot communication channels which allow asynchronous programs to be written in a synchronous style. In this paper, we show how timed promise lists, a timestamped linked list structure using promises rather than pointers, can be used to implement FRP. This idea originated with Elliott’s Push/Pull FRP, and we show that it can be expressed idiomatically in a strict functional language with promises, JavaScript. We identify a potential space leak with JavaScript’s built-in promises and propose an alternative implementation that avoids the leak.', 'corpus_id': 232177318, 'score': 0}, {'doc_id': '14382250', 'title': 'Safe Programming with Pointers Through Stateful Views', 'abstract': 'The need for direct memory manipulation through pointers is essential in many applications. However, it is also commonly understood that the use (or probably misuse) of pointers is often a rich source of program errors. Therefore, approaches that can effectively enforce safe use of pointers in programming are highly sought after. ATS is a programming language with a type system rooted in a recently developed framework Applied Type System, and a novel and desirable feature in ATS lies in its support for safe programming with pointers through a novel notion of stateful views. In particular, even pointer arithmetic is allowed in ATS and guaranteed to be safe by the type system of ATS. In this paper, we give an overview of this feature in ATS, presenting some interesting examples based on a prototype implementation of ATS to demonstrate the practicality of safe programming with pointer through stateful views.', 'corpus_id': 14382250, 'score': 1}, {'doc_id': '199010343', 'title': 'Efficient Deconstruction with Typed Pointer Reversal (abstract)', 'abstract': 'The resource-management model of C++ and Rust relies on compiler-generated destructors called predictably and reliably. In current implementations, the generated destructor consumes stack space proportionally to the depth of the structure it destructs. We describe a way to derive destructors for algebraic data types that consume a constant amount of stack and heap. We discuss applicability to C++ and Rust, and also some implication for anyone wishing to extend an ML-style language with first-class resources.', 'corpus_id': 199010343, 'score': 1}, {'doc_id': '232221083', 'title': 'Retrofitting effect handlers onto OCaml', 'abstract': 'Effect handlers have been gathering momentum as a mechanism for modular programming with user-defined effects. Effect handlers allow for non-local control flow mechanisms such as generators, async/await, lightweight threads and coroutines to be composably expressed. We present a design and evaluate a full-fledged efficient implementation of effect handlers for OCaml, an industrial-strength multi-paradigm programming language. Our implementation strives to maintain the backwards compatibility and performance profile of existing OCaml code. Retrofitting effect handlers onto OCaml is challenging since OCaml does not currently have any non-local control flow mechanisms other than exceptions. Our implementation of effect handlers for OCaml: (i) imposes a mean 1% overhead on a comprehensive macro benchmark suite that does not use effect handlers; (ii) remains compatible with program analysis tools that inspect the stack; and (iii) is efficient for new code that makes use of effect handlers.', 'corpus_id': 232221083, 'score': 0}, {'doc_id': '231986346', 'title': 'Crowbar: Behavioral Symbolic Execution for Deductive Verification of Active Objects', 'abstract': 'We present the Crowbar tool, a deductive verification system for the ABS language. ABS models distributed systems with the Active Object concurrency model. Crowbar implements behavioral symbolic execution: each method is symbolically executed, but specification and prior static analyses influence the shape of the symbolic execution tree. User interaction is realized through guided counterexamples, which present failed proof branches in terms of the input program. Crowbar has a clear interface to implement new specification languages and verification calculi in the Behavioral Program Logic and has been applied for the biggest verification case study of Active Objects.', 'corpus_id': 231986346, 'score': 0}, {'doc_id': '201668763', 'title': 'Kindly bent to free us', 'abstract': 'Systems programming often requires the manipulation of resources like file handles, network connections, or dynamically allocated memory. Programmers need to follow certain protocols to handle these resources correctly. Violating these protocols causes bugs ranging from type mismatches over data races to use-after-free errors and memory leaks. These bugs often lead to security vulnerabilities. While statically typed programming languages guarantee type soundness and memory safety by design, most of them do not address issues arising from improper handling of resources. An important step towards handling resources is the adoption of linear and affine types that enforce single-threaded resource usage. However, the few languages supporting such types require heavy type annotations. We present Affe, an extension of ML that manages linearity and affinity properties using kinds and constrained types. In addition Affe supports the exclusive and shared borrowing of affine resources, inspired by features of Rust. Moreover, Affe retains the defining features of the ML family: it is an impure, strict, functional expression language with complete principal type inference and type abstraction. does not require any linearity annotations in expressions and supports common functional programming idioms.', 'corpus_id': 201668763, 'score': 1}, {'doc_id': '53093884', 'title': 'To Memory Safety through Proofs', 'abstract': 'We present a type system capable of guaranteeing the memory safety of programs that may involve (sophisticated) pointer manipulation such as pointer arithmetic. With its root in a recently developed framework Applied Type System (ATS), the type system imposes a level of abstraction on program states through a novel notion of recursive stateful views and then relies on a form of linear logic to reason about such stateful views. We consider the design and then the formalization of the type system to constitute the primary contribution of the paper. In addition, we also mention a running implementation of the type system and then give some examples in support of the practicality of programming with recursive stateful views.', 'corpus_id': 53093884, 'score': 1}]
29	NeRF	72637c748df19db7671b53af503f32c8	12739	{}	[{'doc_id': '84110481', 'title': 'Chromosome Segregation Controlled by External Mechanical Impulse in a Mammalian Cell', 'abstract': 'During cellular development and growth, cells experience various impacts from physical environments, such as the exertion of the external force. It is not known, however, whether and how the external force affects the mechano-chemical processes in cell division. Here we found that a mechanical impulse externally applied to mitotic HeLa cells alters the balance of forces within the mitotic spindle. We identified two distinct mitotic responses to the external force that either facilitate or delay anaphase onset, depending on the direction of force and the extent of cell compression. External mechanical impulse that physically increases tension within the mitotic spindle accelerates anaphase onset and this is attributed to the facilitation of physical cleavage of sister chromatid cohesion. On the other hand, a decrease in tension hinders chromosome segregation and is coupled with the activation of the spindle assembly checkpoint, which impedes the degradation of mitotic proteins, e.g., cyclin B. We propose that the external mechanical impulse, which modulates tension on sister chromatids, can control metaphase progression through different mechano-chemical cellular reactions.A part of this work was performed in collaboration with Yasuhiko Terada in Waseda University, and with Kenta Kuwana, Tetsuo Kan and Isao Shimoyama in the University of Tokyo. This work was supported in part by Grant-in-Aid for Scientific Research on Priority Areas (to T.I.), Grants-in-Aid for Specially Promoted Research and Scientific Research (S) (to S.I.) from the Ministry of Education, Culture, Sports, Science and Technology of Japan.', 'corpus_id': 84110481, 'score': 0}, {'doc_id': '229181005', 'title': 'Object-Centric Neural Scene Rendering', 'abstract': 'We present a method for composing photorealistic scenes from captured images of objects. Our work builds upon neural radiance fields (NeRFs), which implicitly model the volumetric density and directionally-emitted radiance of a scene. While NeRFs synthesize realistic pictures, they only model static scenes and are closely tied to specific imaging conditions. This property makes NeRFs hard to generalize to new scenarios, including new lighting or new arrangements of objects. Instead of learning a scene radiance field as a NeRF does, we propose to learn object-centric neural scattering functions (OSFs), a representation that models per-object light transport implicitly using a lighting- and view-dependent neural network. This enables rendering scenes even when objects or lights move, without retraining. Combined with a volumetric path tracing procedure, our framework is capable of rendering both intra- and inter-object light transport effects including occlusions, specularities, shadows, and indirect illumination. We evaluate our approach on scene composition and show that it generalizes to novel illumination conditions, producing photorealistic, physically accurate renderings of multi-object scenes.', 'corpus_id': 229181005, 'score': 1}, {'doc_id': '231741196', 'title': 'Self-Supervised Equivariant Scene Synthesis from Video', 'abstract': 'We propose a self-supervised framework to learn scene representations from video that are automatically delineated into background, characters, and their animations. Our method capitalizes on moving characters being equivariant with respect to their transformation across frames and the background being constant with respect to that same transformation. After training, we can manipulate image encodings in real time to create unseen combinations of the delineated components. As far as we know, we are the first method to perform unsupervised extraction and synthesis of interpretable background, character, and animation. We demonstrate results on three datasets: Moving MNIST with backgrounds, 2D video game sprites, and Fashion Modeling.', 'corpus_id': 231741196, 'score': 0}, {'doc_id': '227208781', 'title': 'Neural Scene Flow Fields for Space-Time View Synthesis of Dynamic Scenes', 'abstract': 'We present a method to perform novel view and time synthesis of dynamic scenes, requiring only a monocular video with known camera poses as input. To do this, we introduce Neural Scene Flow Fields, a new representation that models the dynamic scene as a time-variant continuous function of appearance, geometry, and 3D scene motion. Our representation is optimized through a neural network to fit the observed input views. We show that our representation can be used for complex dynamic scenes, including thin structures, view-dependent effects, and natural degrees of motion. We conduct a number of experiments that demonstrate our approach significantly outperforms recent monocular view synthesis methods, and show qualitative results of space-time view synthesis on a variety of real-world videos.', 'corpus_id': 227208781, 'score': 1}, {'doc_id': '220825626', 'title': 'Neural Re-rendering of Humans from a Single Image', 'abstract': 'Human re-rendering from a single image is a starkly underconstrained problem, and state-of-the-art algorithms often exhibit undesired artefacts, such as over-smoothing, unrealistic distortions of the body parts and garments, or implausible changes of the texture. To address these challenges, we propose a new method for neural re-rendering of a human under a novel user-defined pose and viewpoint, given one input image. Our algorithm represents body pose and shape as a parametric mesh which can be reconstructed from a single image and easily reposed. Instead of a colour-based UV texture map, our approach further employs a learned high-dimensional UV feature map to encode appearance. This rich implicit representation captures detailed appearance variation across poses, viewpoints, person identities and clothing styles better than learned colour texture maps. The body model with the rendered feature maps is fed through a neural image translation network that creates the final rendered colour image. The above components are combined in an end-to-end-trained neural network architecture that takes as input a source person image and images of the parametric body model in the source pose and desired target pose. Experimental evaluation demonstrates that our approach produces higher-quality single image re-rendering results than existing methods.', 'corpus_id': 220825626, 'score': 0}, {'doc_id': '19749192', 'title': 'Mutations in the E2 glycoprotein of Venezuelan equine encephalitis virus confer heparan sulfate interaction, low morbidity, and rapid clearance from blood of mice.', 'abstract': 'The arbovirus, Venezuelan equine encephalitis virus (VEE), causes disease in humans and equines during periodic outbreaks. A murine model, which closely mimics the encephalitic form of the disease, was used to study mechanisms of attenuation. Molecularly cloned VEE viruses were used: a virulent, epizootic, parental virus and eight site-specific glycoprotein mutants derived from the parental virus. Four of these mutants were selected in vitro for rapid binding and penetration, resulting in positive charge changes in the E2 glycoprotein from glutamic acid or threonine to lysine (N. L. Davis, N. Powell, G. F. Greenwald, L. V. Willis, B. J. Johnson, J. F. Smith, and R. E. Johnston, Virology 183, 20-31, 1991). Tissue culture adaptation also selected for the ability to bind heparan sulfate as evidenced by inhibition of plaque formation by heparin, decreased infectivity for CHO cells deficient for heparan sulfate, and tight binding to heparin-agarose beads. In contrast, the parental virus and three other mutants did not use heparan sulfate as a receptor. All eight mutants were partially or completely attenuated with respect to mortality in adult mice after a subcutaneous inoculation, and the five mutants that interacted with heparan sulfate in vitro had low morbidity (0-50%). These same five mutants were cleared rapidly from the blood after an intravenous inoculation. In contrast, the parental virus and the other three mutants were cleared very slowly. In summary, the five VEE viruses that contain tissue-culture-selected mutations interacted with cell surface heparan sulfate, and this interaction correlated with low morbidity and rapid clearance from the blood. We propose that one mechanism of attenuation is rapid viral clearance in vivo due to binding of the virus to ubiquitous heparan sulfate.', 'corpus_id': 19749192, 'score': 0}, {'doc_id': '227342468', 'title': 'Dynamic Neural Radiance Fields for Monocular 4D Facial Avatar Reconstruction', 'abstract': 'We present dynamic neural radiance fields for modeling the appearance and dynamics of a human face. Digitally modeling and reconstructing a talking human is a key building-block for a variety of applications. Especially, for telepresence applications in AR or VR, a faithful reproduction of the appearance including novel viewpoints or head-poses is required. In contrast to state-of-the-art approaches that model the geometry and material properties explicitly, or are purely image-based, we introduce an implicit representation of the head based on scene representation networks. To handle the dynamics of the face, we combine our scene representation network with a low-dimensional morphable model which provides explicit control over pose and expressions. We use volumetric rendering to generate images from this hybrid representation and demonstrate that such a dynamic neural scene representation can be learned from monocular input data only, without the need of a specialized capture setup. In our experiments, we show that this learned volumetric representation allows for photo-realistic image generation that surpasses the quality of state-of-the-art video-based reenactment methods.', 'corpus_id': 227342468, 'score': 1}, {'doc_id': '227227965', 'title': 'D-NeRF: Neural Radiance Fields for Dynamic Scenes', 'abstract': 'Neural rendering techniques combining machine learning with geometric reasoning have arisen as one of the most promising approaches for synthesizing novel views of a scene from a sparse set of images. Among these, stands out the Neural radiance fields (NeRF), which trains a deep network to map 5D input coordinates (representing spatial location and viewing direction) into a volume density and view-dependent emitted radiance. However, despite achieving an unprecedented level of photorealism on the generated images, NeRF is only applicable to static scenes, where the same spatial location can be queried from different images. In this paper we introduce D-NeRF, a method that extends neural radiance fields to a dynamic domain, allowing to reconstruct and render novel images of objects under rigid and non-rigid motions from a \\emph{single} camera moving around the scene. For this purpose we consider time as an additional input to the system, and split the learning process in two main stages: one that encodes the scene into a canonical space and another that maps this canonical representation into the deformed scene at a particular time. Both mappings are simultaneously learned using fully-connected networks. Once the networks are trained, D-NeRF can render novel images, controlling both the camera view and the time variable, and thus, the object movement. We demonstrate the effectiveness of our approach on scenes with objects under rigid, articulated and non-rigid motions. Code, model weights and the dynamic scenes dataset will be released.', 'corpus_id': 227227965, 'score': 1}, {'doc_id': '228083626', 'title': 'Portrait Neural Radiance Fields from a Single Image', 'abstract': 'We present a method for estimating Neural Radiance Fields (NeRF) from a single headshot portrait. While NeRF has demonstrated high-quality view synthesis, it requires multiple images of static scenes and thus impractical for casual captures and moving subjects. In this work, we propose to pretrain the weights of a multilayer perceptron (MLP), which implicitly models the volumetric density and colors, with a meta-learning framework using a light stage portrait dataset. To improve the generalization to unseen faces, we train the MLP in the canonical coordinate space approximated by 3D face morphable models. We quantitatively evaluate the method using controlled captures and demonstrate the generalization to real portrait images, showing favorable results against state-of-the-arts.', 'corpus_id': 228083626, 'score': 1}, {'doc_id': '231749866', 'title': 'Deep Online Fused Video Stabilization', 'abstract': 'We present a deep neural network (DNN) that uses both sensor data (gyroscope) and image content (optical flow) to stabilize videos through unsupervised learning. The network fuses optical flow with real/virtual camera pose histories into a joint motion representation. Next, the LSTM block infers the new virtual camera pose, and this virtual pose is used to generate a warping grid that stabilizes the frame. Novel relative motion representation as well as a multi-stage training process are presented to optimize our model without any supervision. To the best of our knowledge, this is the first DNN solution that adopts both sensor data and image for stabilization. We validate the proposed framework through ablation studies and demonstrated the proposed method outperforms the state-of-art alternative solutions via quantitative evaluations and a user study. Check out our video results and dataset at our website.', 'corpus_id': 231749866, 'score': 0}]
30	Mental models in HRI	0e8670f94351048159d19c71fba6c732	13594	{'HRI': 'human-robot interaction'}	"[{'doc_id': '227014686', 'title': 'Explainable AI for System Failures: Generating Explanations that Improve Human Assistance in Fault Recovery', 'abstract': ""With the growing capabilities of intelligent systems, the integration of artificial intelligence (AI) and robots in everyday life is increasing. However, when interacting in such complex human environments, the failure of intelligent systems, such as robots, can be inevitable, requiring recovery assistance from users. In this work, we develop automated, natural language explanations for failures encountered during an AI agents' plan execution. These explanations are developed with a focus of helping non-expert users understand different point of failures to better provide recovery assistance. Specifically, we introduce a context-based information type for explanations that can both help non-expert users understand the underlying cause of a system failure, and select proper failure recoveries. Additionally, we extend an existing sequence-to-sequence methodology to automatically generate our context-based explanations. By doing so, we are able develop a model that can generalize context-based explanations over both different failure types and failure scenarios."", 'corpus_id': 227014686, 'score': 1}, {'doc_id': '229349386', 'title': 'Are We On The Same Page? Hierarchical Explanation Generation for Planning Tasks in Human-Robot Teaming using Reinforcement Learning', 'abstract': 'Providing explanations is considered an imperative ability for an AI agent in a human-robot teaming framework. The right explanation provides the rationale behind an AI agent’s decision making. However, to maintain the human teammate’s cognitive demand to comprehend the provided explanations, prior works have focused on providing explanations in a specific order or intertwining the explanation generation with plan execution. These approaches, however, do not consider the degree of details they share throughout the provided explanations. In this work, we argue that the explanations, especially the complex ones, should be abstracted to be aligned with the level of details the teammate desires to maintain the cognitive load of the recipient. The challenge here is to learn a hierarchical model of explanations and details the agent requires to yield the explanations as an objective. Moreover, the agent needs to follow a high-level plan in a task domain such that the agent can transfer learned teammate preferences to a scenario where lower-level control policies are different, while the high-level plan remains the same. Results confirmed our hypothesis that the process of understanding an explanation was a dynamic hierarchical process. The human preference that reflected this aspect corresponded exactly to creating and employing abstraction for knowledge assimilation hidden deeper in our cognitive process. We showed that hierarchical explanations achieved better task performance and behavior interpretability while reduced cognitive load. These results shed light on designing explainable agents utilizing reinforcement learning and planning across various domains.', 'corpus_id': 229349386, 'score': 0}, {'doc_id': '15512333', 'title': 'Tell me more?: the effects of mental model soundness on personalizing an intelligent agent', 'abstract': ""What does a user need to know to productively work with an intelligent agent? Intelligent agents and recommender systems are gaining widespread use, potentially creating a need for end users to understand how these systems operate in order to fix their agent's personalized behavior. This paper explores the effects of mental model soundness on such personalization by providing structural knowledge of a music recommender system in an empirical study. Our findings show that participants were able to quickly build sound mental models of the recommender system's reasoning, and that participants who most improved their mental models during the study were significantly more likely to make the recommender operate to their satisfaction. These results suggest that by helping end users understand a system's reasoning, intelligent agents may elicit more and better feedback, thus more closely aligning their output with each user's intentions."", 'corpus_id': 15512333, 'score': 1}, {'doc_id': '140250959', 'title': 'Method and apparatus for realizing binding and communication between user terminal and Internet of things device', 'abstract': 'The invention discloses a method for realizing binding and communication between a user terminal and an Internet of things device. The method comprises the following steps: a user terminal obtaining an instruction for searching for a device supporting an agreed protocol; searching a local area network to obtain an physical identification code of the device supporting the agreed protocol; requesting an Internet of things platform to carry out registration to obtain a device account and a device password for registering the Internet of things platform; sending the device account and the device password to the corresponding device for application when the user registers the Intent of things platform; and sending a device binding request comprising the device account and a user account used by a local computer for registering the Internet of things platform to the Internet of things platform so as to enable the user terminal registering the Internet of things platform by use of the user account to communicate with the device registering the Internet of things platform by use of the device account via the Internet of things platform. The method provides a foundation for realizing communication between the user terminal and the device. Besides, the invention further provides an apparatus for realizing binding between a user terminal and an Internet of things device and a method and apparatus for realizing communication between the two.', 'corpus_id': 140250959, 'score': 0}, {'doc_id': '231796001', 'title': 'Supporting an Online Investigation of User Interaction with an XAIP Agent', 'abstract': 'Human interaction relies on a wide range of signals, including non-verbal cues. In order to develop effective Explainable Planning (XAIP) agents it is important that we understand the range and utility of these communication channels. Our intention is to develop an interactive agent, whose behaviour is conditioned on the affective measures of the user (i.e., explicitly incorporating the user’s affective state within the planning model). Accurate prediction of user affective state relies on real-time analysis of various predictors, which can require specialist equipment and calibration. However, the worldwide COVID-19 lockdown has meant that many intended lab-based experiments have now been moved online, making such real-time analysis impractical. As a result, we have developed a website to support a data gathering experiment, including a video stream (for facial expression analysis) with access to mouse positions and task performance, providing rich observations of the users as they interact with the agent and its plan. Underlying this system is the agent’s behaviour strategy, which must be computed in advance and captured efficiently. This paper describes the built system and the challenges we faced getting it ready for deployment.', 'corpus_id': 231796001, 'score': 0}, {'doc_id': '231839673', 'title': '""I Don\'t Think So"": Disagreement-Based Policy Summaries for Comparing Agents', 'abstract': 'With Artificial Intelligence on the rise, human interaction with autonomous agents becomes more frequent. Effective human-agent collaboration requires that the human understands the agent’s behavior, as failing to do so may lead to reduced productiveness, misuse, frustration and even danger. Agent strategy summarization methods are used to describe the strategy of an agent to its destined user through demonstration. The summary’s purpose is to maximize the user’s understanding of the agent’s aptitude by showcasing its behaviour in a set of world states, chosen by some importance criteria. While shown to be useful, we show that these methods are limited in supporting the task of comparing agent behavior, as they independently generate a summary for each agent. In this paper, we propose a novel method for generating contrastive summaries that highlight the differences between agent’s policies by identifying and ranking states in which the agents disagree on the best course of action. We conduct a user study in which participants face an agent selection task. Our results show that the novel disagreement-based summaries lead to improved user performance compared to summaries generated using HIGHLIGHTS, a previous strategy summarization algorithm.', 'corpus_id': 231839673, 'score': 0}, {'doc_id': '14287365', 'title': 'An Embodied Cognition Approach to Mindreading Skills for Socially Intelligent Robots', 'abstract': ""Future applications for personal robots motivate research into developing robots that are intelligent in their interactions with people. Toward this goal, in this paper we present an integrated socio-cognitive architecture to endow an anthropomorphic robot with the ability to infer mental states such as beliefs, intents, and desires from the observable behavior of its human partner. The design of our architecture is informed by recent findings from neuroscience and embodies cognition that reveals how living systems leverage their physical and cognitive embodiment through simulation-theoretic mechanisms to infer the mental states of others. We assess the robot's mindreading skills on a suite of benchmark tasks where the robot interacts with a human partner in a cooperative scenario and a learning scenario. In addition, we have conducted human subjects experiments using the same task scenarios to assess human performance on these tasks and to compare the robot's performance with that of people. In the process, our human subject studies also reveal some interesting insights into human behavior."", 'corpus_id': 14287365, 'score': 1}, {'doc_id': '85502531', 'title': ""People's Explanations of Robot Behavior Subtly Reveal Mental State Inferences"", 'abstract': ""It has long been assumed that when people observe robots they intuitively ascribe mind and intentionality to them, just as they do to humans. However, much of this evidence relies on experimenter-provided questions or self-reported judgments. We propose a new way of investigating people's mental state ascriptions to robots by carefully studying explanations of robot behavior. Since people's explanations of human behavior are deeply grounded in assumptions of mind and intentional agency, explanations of robot behavior can reveal whether such assumptions similarly apply to robots. We designed stimulus behaviors that were representative of a variety of robots in diverse contexts and ensured that people saw the behaviors as equally intentional, desirable, and surprising across both human and robot agents. We provided 121 participants with verbal descriptions of these behaviors and asked them to explain in their own words why the agent (human or robot) had performed them. To systematically analyze the verbal data, we used a theoretically grounded classification method to identify core explanation types. We found that people use the same conceptual toolbox of behavior explanations for both human and robot agents, robustly indicating inferences of intentionality and mind. But people applied specific explanatory tools at somewhat different rates and in somewhat different ways for robots, revealing specific expectations people hold when explaining robot behaviors."", 'corpus_id': 85502531, 'score': 1}, {'doc_id': '210927566', 'title': ""Effects of a Social Robot's Self-Explanations on How Humans Understand and Evaluate Its Behavior"", 'abstract': ""Social robots interacting with users in real-life environments will often show surprising or even undesirable behavior. In this paper we investigate whether a robot's ability to self-explain its behavior affects the users' perception and assessment of this behavior. We propose an explanation model based on humans' folk-psychological concepts and test different explanation strategies in specifically designed HRI scenarios with robot behaviors perceived as intentional, but differently surprising or desirable. All types of explanation strategies increased the understandability and desirability of the behaviors. While merely stating an action had similar effects as giving a reason for it (an intention or need), combining both in a causal explanation helped the robot to better justify its behavior and to increase its understandability and desirability to a larger extent."", 'corpus_id': 210927566, 'score': 1}, {'doc_id': '225213456', 'title': 'A Survey of Mental Modeling Techniques in Human–Robot Teaming', 'abstract': 'As robots become increasingly prevalent and capable, the complexity of roles and responsibilities assigned to them as well as our expectations for them will increase in kind. For these autonomous systems to operate safely and efficiently in human-populated environments, they will need to cooperate and coordinate with human teammates. Mental models provide a formal mechanism for achieving fluent and effective teamwork during human–robot interaction by enabling awareness between teammates and allowing for coordinated action. Much recent research in human–robot interaction has made use of standardized and formalized mental modeling techniques to great effect, allowing for a wider breadth of scenarios in which a robotic agent can act as an effective and trustworthy teammate. This paper provides a structured overview of mental model theory and methodology as applied to human–robot teaming. Also discussed are evaluation methods and metrics for various aspects of mental modeling during human–robot interaction, as well as recent emerging applications and open challenges in the field.', 'corpus_id': 225213456, 'score': 1}]"
31	Contextual IR	0204be651a2aa0eab2d9aea4df1310a6	5160	{}	"[{'doc_id': '218685017', 'title': 'Controllable Multi-Interest Framework for Recommendation', 'abstract': ""Recently, neural networks have been widely used in e-commerce recommender systems, owing to the rapid development of deep learning. We formalize the recommender system as a sequential recommendation problem, intending to predict the next items that the user might be interacted with. Recent works usually give an overall embedding from a user's behavior sequence. However, a unified user embedding cannot reflect the user's multiple interests during a period. In this paper, we propose a novel controllable multi-interest framework for the sequential recommendation, called ComiRec. Our multi-interest module captures multiple interests from user behavior sequences, which can be exploited for retrieving candidate items from the large-scale item pool. These items are then fed into an aggregation module to obtain the overall recommendation. The aggregation module leverages a controllable factor to balance the recommendation accuracy and diversity. We conduct experiments for the sequential recommendation on two real-world datasets, Amazon and Taobao. Experimental results demonstrate that our framework achieves significant improvements over state-of-the-art models. Our framework has also been successfully deployed on the offline Alibaba distributed cloud platform."", 'corpus_id': 218685017, 'score': 0}, {'doc_id': '216022609', 'title': 'Evaluation in Contextual Information Retrieval', 'abstract': 'Context such as the user’s search history, demographics, devices, and surroundings, has become prevalent in various domains of information seeking and retrieval such as mobile search, task-based se...', 'corpus_id': 216022609, 'score': 1}, {'doc_id': '8312085', 'title': 'Exploring Query Auto-Completion and Click Logs for Contextual-Aware Web Search and Query Suggestion', 'abstract': ""Contextual data plays an important role in modeling search engine users' behaviors on both query auto-completion (QAC) log and normal query (click) log. User's recent search history on each log has been widely studied individually as the context to benefit the modeling of users' behaviors on that log. However, there is no existing work that explores or incorporates both logs together for contextual data. As QAC and click logs actually record users' sequential behaviors while interacting with a search engine, the available context of a user's current behavior based on the same type of log can be strengthened from the user's recent search history shown on the other type of log. Our paper proposes to model users' behaviors on both QAC and click logs simultaneously by utilizing both logs as the contextual data of each other. The key idea is to capture the correlation between users' behavior patterns on both logs. We model such correlation through a novel probabilistic model based on the Latent Dirichlet allocation (LDA) model. The learned users' behavior patterns on both logs are utilized to address not only the application of query auto-completion on QAC logs, but also the click prediction and relevance ranking of web documents on click logs. Experiments on real-world logs demonstrate the effectiveness of the proposed model on both applications."", 'corpus_id': 8312085, 'score': 1}, {'doc_id': '52179704', 'title': 'Evaluation in Contextual Information Retrieval', 'abstract': 'Context such as the user’s search history, demographics, devices, and surroundings, has become prevalent in various domains of information seeking and retrieval such as mobile search, task-based search, and social search. While evaluation is central and has a long history in information retrieval, it faces the big challenge of designing an appropriate methodology that embeds the context into evaluation settings. In this article, we present a unified summary of a wide range of main and recent progress in contextual information retrieval evaluation that leverages diverse context dimensions and uses different principles, methodologies, and levels of measurements. More specifically, this survey article aims to fill two main gaps in the literature: First, it provides a critical summary and comparison of existing contextual information retrieval evaluation methodologies and metrics according to a simple stratification model; second, it points out the impact of context dynamicity and data privacy on the evaluation design. Finally, we recommend promising research directions for future investigations.', 'corpus_id': 52179704, 'score': 1}, {'doc_id': '215989695', 'title': 'Modeling Queries with Contextual Snippets for Information Retrieval', 'abstract': 'Query expansion under the pseudo-relevance feedback (PRF) framework has been extensively studied in information retrieval. However, most expansion methods are mainly based on the statistics of sing...', 'corpus_id': 215989695, 'score': 1}, {'doc_id': '86380746', 'title': 'Toward Contextual Information Retrieval: A Review And Trends', 'abstract': 'Abstract With the growth of electronic data and the expansion of the World Wide Web (WWW), many classic existing retrieval models and systems ignore information about the actual user and search context. Due to the constraints imposed by this fact, context has received more attention in the information retrieval (IR) literature and its interactions over the past decade. In this paper, we emphasize on the importance and implications of context in information retrieval and how can it affect the retrieval systems to operate and behave more intelligently; we highlight some emerging trends of context; we present variety of practical uses of context along with its taxonomies and levels; we discuss how can we model context by these systems along with proposing some practical recommendations to enhance this research area for future research.', 'corpus_id': 86380746, 'score': 1}, {'doc_id': '218889328', 'title': 'How to Grow a (Product) Tree: Personalized Category Suggestions for eCommerce Type-Ahead', 'abstract': 'In an attempt to balance precision and recall in the search page, leading digital shops have been effectively nudging users into select category facets as early as in the type-ahead suggestions. In this work, we present SessionPath, a novel neural network model that improves facet suggestions on two counts: first, the model is able to leverage session embeddings to provide scalable personalization; second, SessionPath predicts facets by explicitly producing a probability distribution at each node in the taxonomy path. We benchmark SessionPath on two partnering shops against count-based and neural models, and show how business requirements and model behavior can be combined in a principled way.', 'corpus_id': 218889328, 'score': 0}, {'doc_id': '215737055', 'title': 'CSRN: Collaborative Sequential Recommendation Networks for News Retrieval', 'abstract': 'Nowadays, news apps have taken over the popularity of paper-based media, providing a great opportunity for personalization. Recurrent Neural Network (RNN)-based sequential recommendation is a popular approach that utilizes users\' recent browsing history to predict future items. This approach is limited that it does not consider the societal influences of news consumption, i.e., users may follow popular topics that are constantly changing, while certain hot topics might be spreading only among specific groups of people. Such societal impact is difficult to predict given only users\' own reading histories. On the other hand, the traditional User-based Collaborative Filtering (UserCF) makes recommendations based on the interests of the ""neighbors"", which provides the possibility to supplement the weaknesses of RNN-based methods. However, conventional UserCF only uses a single similarity metric to model the relationships between users, which is too coarse-grained and thus limits the performance. In this paper, we propose a framework of deep neural networks to integrate the RNN-based sequential recommendations and the key ideas from UserCF, to develop Collaborative Sequential Recommendation Networks (CSRNs). Firstly, we build a directed co-reading network of users, to capture the fine-grained topic-specific similarities between users in a vector space. Then, the CSRN model encodes users with RNNs, and learns to attend to neighbors and summarize what news they are reading at the moment. Finally, news articles are recommended according to both the user\'s own state and the summarized state of the neighbors. Experiments on two public datasets show that the proposed model outperforms the state-of-the-art approaches significantly.', 'corpus_id': 215737055, 'score': 0}, {'doc_id': '218900643', 'title': 'How to Retrain Recommender System?: A Sequential Meta-Learning Method', 'abstract': 'Practical recommender systems need be periodically retrained to refresh the model with new interaction data. To pursue high model fidelity, it is usually desirable to retrain the model on both historical and new data, since it can account for both long-term and short-term user preference. However, a full model retraining could be very time-consuming and memory-costly, especially when the scale of historical data is large. In this work, we study the model retraining mechanism for recommender systems, a topic of high practical values but has been relatively little explored in the research community. Our first belief is that retraining the model on historical data is unnecessary, since the model has been trained on it before. Nevertheless, normal training on new data only may easily cause overfitting and forgetting issues, since the new data is of a smaller scale and contains fewer information on long-term user preference. To address this dilemma, we propose a new training method, aiming to abandon the historical data during retraining through learning to transfer the past training experience.Specifically, we design a neural network-based transfer component, which transforms the old model to a new model that is tailored for future recommendations. To learn the transfer component well, we optimize the ""future performance\'\' -- i.e., the recommendation accuracy evaluated in the next time period. Our Sequential Meta-Learning(SML) method offers a general training paradigm that is applicable to any differentiable model. We demonstrate SML on matrix factorization and conduct experiments on two real-world datasets. Empirical results show that SML not only achieves significant speed-up, but also outperforms the full model retraining in recommendation accuracy, validating the effectiveness of our proposals. We release our codes at: https://github.com/zyang1580/SML.', 'corpus_id': 218900643, 'score': 0}, {'doc_id': '218684696', 'title': 'Table Search Using a Deep Contextualized Language Model', 'abstract': 'Pretrained contextualized language models such as BERT have achieved impressive results on various natural language processing benchmarks. Benefiting from multiple pretraining tasks and large scale training corpora, pretrained models can capture complex syntactic word relations. In this paper, we use the deep contextualized language model BERT for the task of ad hoc table retrieval. We investigate how to encode table content considering the table structure and input length limit of BERT. We also propose an approach that incorporates features from prior literature on table retrieval and jointly trains them with BERT. In experiments on public datasets, we show that our best approach can outperform the previous state-of-the-art method and BERT baselines with a large margin under different evaluation metrics.', 'corpus_id': 218684696, 'score': 0}]"
32	Uncertainty	9df64ea44035467d4b3b1d3137556ccb	6774	{}	"[{'doc_id': '9963515', 'title': 'Toward Robustness against Label Noise in Training Deep Discriminative Neural Networks', 'abstract': 'Collecting large training datasets, annotated with high-quality labels, is costly and time-consuming. This paper proposes a novel framework for training deep convolutional neural networks from noisy labeled datasets that can be obtained cheaply. The problem is formulated using an undirected graphical model that represents the relationship between noisy and clean labels, trained in a semi-supervised setting. In our formulation, the inference over latent clean labels is tractable and is regularized during training using auxiliary sources of information. The proposed model is applied to the image labeling problem and is shown to be effective in labeling unseen images as well as reducing label noise in training on CIFAR-10 and MS COCO datasets.', 'corpus_id': 9963515, 'score': 1}, {'doc_id': '160705', 'title': 'Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning', 'abstract': ""Deep learning tools have gained tremendous attention in applied machine learning. However such tools for regression and classification do not capture model uncertainty. In comparison, Bayesian models offer a mathematically grounded framework to reason about model uncertainty, but usually come with a prohibitive computational cost. In this paper we develop a new theoretical framework casting dropout training in deep neural networks (NNs) as approximate Bayesian inference in deep Gaussian processes. A direct result of this theory gives us tools to model uncertainty with dropout NNs -- extracting information from existing models that has been thrown away so far. This mitigates the problem of representing uncertainty in deep learning without sacrificing either computational complexity or test accuracy. We perform an extensive study of the properties of dropout's uncertainty. Various network architectures and non-linearities are assessed on tasks of regression and classification, using MNIST as an example. We show a considerable improvement in predictive log-likelihood and RMSE compared to existing state-of-the-art methods, and finish by using dropout's uncertainty in deep reinforcement learning."", 'corpus_id': 160705, 'score': 1}, {'doc_id': '221819362', 'title': 'Selectivity correction with online machine learning', 'abstract': ""Computer systems are full of heuristic rules which drive the decisions they make. These rules of thumb are designed to work well on average, but ignore specific information about the available context, and are thus sub-optimal. The emerging field of machine learning for systems attempts to learn decision rules with machine learning algorithms. In the database community, many recent proposals have been made to improve selectivity estimation with batch machine learning methods. Such methods are all batch methods which require retraining and cannot handle concept drift, such as workload changes and schema modifications. We present online machine learning as an alternative approach. Online models learn on the fly and do not require storing data, they are more lightweight than batch models, and finally may adapt to concept drift. As an experiment, we teach models to improve the selectivity estimates made by PostgreSQL's cost model. Our experiments make the case that simple online models are able to compete with a recently proposed deep learning method."", 'corpus_id': 221819362, 'score': 0}, {'doc_id': '59608823', 'title': 'A Simple Baseline for Bayesian Uncertainty in Deep Learning', 'abstract': 'We propose SWA-Gaussian (SWAG), a simple, scalable, and general purpose approach for uncertainty representation and calibration in deep learning. Stochastic Weight Averaging (SWA), which computes the first moment of stochastic gradient descent (SGD) iterates with a modified learning rate schedule, has recently been shown to improve generalization in deep learning. With SWAG, we fit a Gaussian using the SWA solution as the first moment and a low rank plus diagonal covariance also derived from the SGD iterates, forming an approximate posterior distribution over neural network weights; we then sample from this Gaussian distribution to perform Bayesian model averaging. We empirically find that SWAG approximates the shape of the true posterior, in accordance with results describing the stationary distribution of SGD iterates. Moreover, we demonstrate that SWAG performs well on a wide variety of tasks, including out of sample detection, calibration, and transfer learning, in comparison to many popular alternatives including MC dropout, KFAC Laplace, SGLD, and temperature scaling.', 'corpus_id': 59608823, 'score': 1}, {'doc_id': '6054025', 'title': 'Classification in the Presence of Label Noise: A Survey', 'abstract': 'Label noise is an important issue in classification, with many potential negative consequences. For example, the accuracy of predictions may decrease, whereas the complexity of inferred models and the number of necessary training samples may increase. Many works in the literature have been devoted to the study of label noise and the development of techniques to deal with label noise. However, the field lacks a comprehensive survey on the different types of label noise, their consequences and the algorithms that consider label noise. This paper proposes to fill this gap. First, the definitions and sources of label noise are considered and a taxonomy of the types of label noise is proposed. Second, the potential consequences of label noise are discussed. Third, label noise-robust, label noise cleansing, and label noise-tolerant algorithms are reviewed. For each category of approaches, a short discussion is proposed to help the practitioner to choose the most suitable technique in its own particular field of application. Eventually, the design of experiments is also discussed, what may interest the researchers who would like to test their own algorithms. In this paper, label noise consists of mislabeled instances: no additional information is assumed to be available like e.g., confidences on labels.', 'corpus_id': 6054025, 'score': 1}, {'doc_id': '221836242', 'title': 'Deep N-ary Error Correcting Output Codes', 'abstract': 'Ensemble learning consistently improves the performance of multi-class classification through aggregating a series of base classifiers. To this end, data-independent ensemble methods like Error Correcting Output Codes (ECOC) attract increasing attention due to its easiness of implementation and parallelization. Specifically, traditional ECOCs and its general extension N-ary ECOC decompose the original multi-class classification problem into a series of independent simpler classification subproblems. Unfortunately, integrating ECOCs, especially N-ary ECOC with deep neural networks, termed as deep N-ary ECOC, is not straightforward and yet fully exploited in the literature, due to the high expense of training base learners. To facilitate the training of N-ary ECOC with deep learning base learners, we further propose three different variants of parameter sharing architectures for deep N-ary ECOC. To verify the generalization ability of deep N-ary ECOC, we conduct experiments by varying the backbone with different deep neural network architectures for both image and text classification tasks. Furthermore, extensive ablation studies on deep N-ary ECOC show its superior performance over other deep data-independent ensemble methods.', 'corpus_id': 221836242, 'score': 0}, {'doc_id': '201070534', 'title': 'Symmetric Cross Entropy for Robust Learning With Noisy Labels', 'abstract': 'Training accurate deep neural networks (DNNs) in the presence of noisy labels is an important and challenging task. Though a number of approaches have been proposed for learning with noisy labels, many open issues remain. In this paper, we show that DNN learning with Cross Entropy (CE) exhibits overfitting to noisy labels on some classes (``easy"" classes), but more surprisingly, it also suffers from significant under learning on some other classes (``hard"" classes). Intuitively, CE requires an extra term to facilitate learning of hard classes, and more importantly, this term should be noise tolerant, so as to avoid overfitting to noisy labels. Inspired by the symmetric KL-divergence, we propose the approach of Symmetric cross entropy Learning (SL), boosting CE symmetrically with a noise robust counterpart Reverse Cross Entropy (RCE). Our proposed SL approach simultaneously addresses both the under learning and overfitting problem of CE in the presence of noisy labels. We provide a theoretical analysis of SL and also empirically show, on a range of benchmark and real-world datasets, that SL outperforms state-of-the-art methods. We also show that SL can be easily incorporated into existing methods in order to further enhance their performance.', 'corpus_id': 201070534, 'score': 1}, {'doc_id': '234367560', 'title': 'Learning from Noisy Labels with Complementary Loss Functions', 'abstract': 'Recent researches reveal that deep neural networks are sensitive to label noises hence leading to poor generalization performance in some tasks. Although different robust loss functions have been proposed to remedy this issue, they suffer from an underfitting problem, thus are not sufficient to learn accurate models. On the other hand, the commonly used Cross Entropy (CE) loss, which shows high performance in standard supervised learning (with clean supervision), is nonrobust to label noise. In this paper, we propose a general framework to learn robust deep neural networks with complementary loss functions. In our framework, CE and robust loss play complementary roles in a joint learning objective as per their learning sufficiency and robustness properties respectively. Specifically, we find that by exploiting the memorization effect of neural networks, we can easily filter out a proportion of hard samples and generate reliable pseudo labels for easy samples, and thus reduce the label noise to a quite low level. Then, we simply learn with CE on pseudo supervision and robust loss on original noisy supervision. In this procedure, CE can guarantee the sufficiency of optimization while the robust loss can be regarded as the supplement. Experimental results on benchmark classification datasets indicate that the proposed method helps achieve robust and sufficient deep neural network training simultaneously.', 'corpus_id': 234367560, 'score': 0}, {'doc_id': '234334052', 'title': 'Generalized Jensen-Shannon Divergence Loss for Learning with Noisy Labels', 'abstract': 'Prior works have found it beneficial to combine provably noise-robust loss functions e.g., mean absolute error (MAE) with standard categorical loss function e.g. cross entropy (CE) to improve their learnability. Here, we propose to use Jensen-Shannon divergence as a noise-robust loss function and show that it interestingly interpolate between CE and MAE with a controllable mixing parameter. Furthermore, we make a crucial observation that CE exhibit lower consistency around noisy data points. Based on this observation, we adopt a generalized version of the JensenShannon divergence for multiple distributions to encourage consistency around data points. Using this loss function, we show state-of-the-art results on both synthetic (CIFAR), and real-world (WebVision) noise with varying noise rates.', 'corpus_id': 234334052, 'score': 0}, {'doc_id': '233394171', 'title': 'Recalibration of Aleatoric and Epistemic Regression Uncertainty in Medical Imaging', 'abstract': 'The consideration of predictive uncertainty in medical imaging with deep learning is of utmost importance. We apply estimation of both aleatoric and epistemic uncertainty by variational Bayesian inference with Monte Carlo dropout to regression tasks and show that predictive uncertainty is systematically underestimated. We apply σ scaling with a single scalar value; a simple, yet effective calibration method for both types of uncertainty. The performance of our approach is evaluated on a variety of common medical regression data sets using different state-of-the-art convolutional network architectures. In our experiments, σ scaling is able to reliably recalibrate predictive uncertainty. It is easy to implement and maintains the accuracy. Well-calibrated uncertainty in regression allows robust rejection of unreliable predictions or detection of out-of-distribution samples. Our source code is available at: github.com/mlaves/well-calibrated-regression-uncertainty', 'corpus_id': 233394171, 'score': 0}]"
33	VSA	81401612b034b13750c8f6fda533ec2d	2774	{'VSA': 'vasospastic angina'}	[{'doc_id': '233241236', 'title': 'Memory Capacity of Neural Turing Machines with Matrix Representation', 'abstract': 'It is well known that recurrent neural networks (RNNs) faced limitations in learning longterm dependencies that have been addressed by memory structures in long short-term memory (LSTM) networks. Matrix neural networks feature matrix representation which inherently preserves the spatial structure of data and has the potential to provide better memory structures when compared to canonical neural networks that use vector representation. Neural Turing machines (NTMs) are novel RNNs that implement notion of programmable computers with neural network controllers to feature algorithms that have copying, sorting, and associative recall tasks. In this paper, we study augmentation of memory capacity with matrix representation of RNNs and NTMs (MatNTMs). We investigate if matrix representation has a better memory capacity than the vector representations in conventional neural networks. We use a probabilistic model of the memory capacity using Fisher information and investigate how the memory capacity for matrix representation networks are limited under various constraints, and in general, without any constraints. In the case of memory capacity without any constraints, we found that the upper bound on memory capacity to be N for an N×N state matrix. The results from our experiments using synthetic algorithmic tasks show that MatNTMs have a better learning capacity when compared to its counterparts.', 'corpus_id': 233241236, 'score': 0}, {'doc_id': '203620389', 'title': 'High-Dimensional Vector Spaces as the Architecture of Cognition', 'abstract': None, 'corpus_id': 203620389, 'score': 1}, {'doc_id': '233762187', 'title': 'Precis of A Bayesian account of learning algorithms and generalising representations in the brain', 'abstract': 'Without learning we would be limited to a set of preprogrammed behaviours. While that may be acceptable for flies1, it does not provide the basis for adaptive or intelligent behaviours familiar to humans. Learning, then, is one of the crucial components of brain operation. Learning, however, takes time. Thus, the key to adaptive behaviour is learning to systematically generalise; that is, have learned knowledge that can be flexibly recombined to understand any world in front of you. This thesis attempts to make inroads on two questions how can brain networks learn, and what are the principles behind representations of knowledge that allow generalisation. With the industrialisation of science, the twentieth century bore fruit in the form of an increasingly detailed understanding of neurons, synapses, neurotransmitters, resting potentials, action potentials, networks and so on (1–4). Though we have gained a great level of detail about many of these micro-processes as well as high-level understandings of intelligence thanks to philosophy, experimental psychology, and behavioural and cognitive neuroscience (5–9) a large gulf of understanding remains between these levels of granularity. This thesis focuses on spanning this gap by providing high-level computational frameworks that translate to low-level processes. Any high-level brain framework must have successful behaviour at its heart as that is the role of the brain. Analogously, neurons are central to low-level understanding as the basis of brain function is believed to be the transfer of information between neurons, mediated via weighted connections. Different weights lead to different functions. Thus, learning appropriate configurations of weights is the fundamental problem facing brains. There are two facets to this learning the first is how, and the second is what. The how are the learning algorithms that determine updates to these synaptic connections, and the what are the neural representations that reflect how the world works. In this vein, this thesis examines 1) the algorithmic implementation of learning in biological neural networks, and 2) a computational framework for the neural representations of task generalisation. Both these research directions are bound together by Bayesian thinking, and both of these pieces of work bridge the gap between highand lowlevel understanding, as well as between brains and machines.', 'corpus_id': 233762187, 'score': 0}, {'doc_id': '232325097', 'title': 'From convolutional neural networks to models of higher-level cognition (and back again).', 'abstract': 'The remarkable successes of convolutional neural networks (CNNs) in modern computer vision are by now well known, and they are increasingly being explored as computational models of the human visual system. In this paper, we ask whether CNNs might also provide a basis for modeling higher-level cognition, focusing on the core phenomena of similarity and categorization. The most important advance comes from the ability of CNNs to learn high-dimensional representations of complex naturalistic images, substantially extending the scope of traditional cognitive models that were previously only evaluated with simple artificial stimuli. In all cases, the most successful combinations arise when CNN representations are used with cognitive models that have the capacity to transform them to better fit human behavior. One consequence of these insights is a toolkit for the integration of cognitively motivated constraints back into CNN training paradigms in computer vision and machine learning, and we review cases where this leads to improved performance. A second consequence is a roadmap for how CNNs and cognitive models can be more fully integrated in the future, allowing for flexible end-to-end algorithms that can learn representations from data while still retaining the structured behavior characteristic of human cognition.', 'corpus_id': 232325097, 'score': 0}, {'doc_id': '233168647', 'title': 'Recognizing and Verifying Mathematical Equations using Multiplicative Differential Neural Units', 'abstract': 'Automated mathematical reasoning is a challenging problem that requires an agent to learn algebraic patterns that contain long-range dependencies. Two particular tasks that test this type of reasoning are (1) mathematical equation verification, which requires determining whether trigonometric and linear algebraic statements are valid identities or not, and (2) equation completion, which entails filling in a blank within an expression to make it true. Solving these tasks with deep learning requires that the neural model learn how to manipulate and compose various algebraic symbols, carrying this ability over to previously unseen expressions. Artificial neural networks, including recurrent networks and transformers, struggle to generalize on these kinds of difficult compositional problems, often exhibiting poor extrapolation performance. In contrast, recursive neural networks (recursive-NNs) are, theoretically, capable of achieving better extrapolation due to their tree-like design but are difficult to optimize as the depth of their underlying tree structure increases. To overcome this issue, we extend recursive-NNs to utilize multiplicative, higher-order synaptic connections and, furthermore, to learn to dynamically control and manipulate an external memory. We argue that this key modification gives the neural system the ability to capture powerful transition functions for each possible input. We demonstrate the effectiveness of our proposed higher-order, memory-augmented recursive-NN models on two challenging mathematical equation tasks, showing improved extrapolation, stable performance, and faster convergence. Our models achieve a 1.53% average improvement over current state-of-the-art methods in equation verification and achieve a 2.22% Top-1 average accuracy and 2.96% Top5 average accuracy for equation completion.', 'corpus_id': 233168647, 'score': 0}, {'doc_id': '86842861', 'title': 'One-shot Learning for Seizure Detection and Identification of Epileptogenic Brain Regions from Long-time Human iEEG Recording with End-to-end Binary Operations', 'abstract': 'This thesis presents an efficient algorithm by combining symbolic dynamics and brain-inspired hyperdimensional (HD) computing for both seizure onset detection and identification of ictogenic (= seizure generating) brain regions from intracranial electroencephalography (iEEG). Moreover, the simplicity of the algorithm eases its implementation on an embedded AI computing device platform (e.g. the NVIDIA Jetson TX2 Module) for long term operation. The proposed algorithm provides: (1) a unified method for both learning and classification tasks with end-to-end binary operations; (2) one-shot learning from seizure examples; (3) linear computational scalability to any number of electrodes; (4) generation of transparent codes with interpretable features; (5) a simple embedded implementation which is fast and energy efficient. The algorithm first transforms iEEG time series from each electrode into symbolic local binary pattern codes from which a distributed representation of the brain state of interest is constructed across all the electrodes and over time in a hyperdimensional space. Such holographic representation is used to quickly learn from seizures, detect their onset, and identify the spatial brain regions that generated them. Moreover, HD computing is characterized by one-shot or anyway fast learning, making it a prime candidate for utilization in such a domain with a typical low quantity of training data. I assess the performance of the proposed algorithm on two different dataset: (1) the first contains 99 short-time iEEG recordings from 16 drug-resistant epilepsy patients being implanted with 36 to 100 electrodes; (2) the second is composed by 18 long-time recordings from 18 drug-resistant epilepsy patients: a total of 2656 interictal hours and 120 seizures are contained in the recordings. All the patients come from the epilepsy surgery program of the Inselspital Bern. On the first dataset, for the majority of the patients (10 out of 16), our algorithm quickly learns from one or two seizures and perfectly (100%) generalizes on novel seizures using k-fold cross-validation. For the remaining six patients, the algorithm requires three to six seizures for learning. Our algorithm surpasses the state-of-the-art including deep learning algorithms by achieving higher specificity (94.84% vs. 94.77%) and macroaveraging accuracy (95.42% vs. 94.96%), and 74× lower memory footprint, but slightly higher average latency in detection (15.9 s vs. 14.7 s). On the second dataset,the algorithm learns from one or two seizures and achieves 0.0 false detection rate for all the patients. The state-of-the-art achieves again lower latency in detection (12.8 s vs. 17.3 s), but higher false detection rate (0.31 f/h) Moreover, the algorithm can reliably identify (with a p-value < 0.01) the relevant electrodes covering an ictogenic brain region at two levels of granularity: cerebral hemispheres and lobes. Finally, the algorithm shows 15× gain in execution time and 18× gain in energy consumption with respect to the state-of-the-art competitors, when implemented on the NVIDIA TX2 platform.', 'corpus_id': 86842861, 'score': 1}, {'doc_id': '233391835', 'title': 'USING MULTIPLICATIVE DIFFERENTIAL NEURAL UNITS', 'abstract': 'Automated mathematical reasoning is a challenging problem that requires an agent to learn algebraic patterns that contain long-range dependencies. Two particular tasks that test this type of reasoning are (1) mathematical equation verification, which requires determining whether trigonometric and linear algebraic statements are valid identities or not, and (2) equation completion, which entails filling in a blank within an expression to make it true. Solving these tasks with deep learning requires that the neural model learn how to manipulate and compose various algebraic symbols, carrying this ability over to previously unseen expressions. Artificial neural networks, including recurrent networks and transformers, struggle to generalize on these kinds of difficult compositional problems, often exhibiting poor extrapolation performance. In contrast, recursive neural networks (recursiveNNs) are, theoretically, capable of achieving better extrapolation due to their tree-like design but are difficult to optimize as the depth of their underlying tree structure increases. To overcome this issue, we extend recursive-NNs to utilize multiplicative, higher-order synaptic connections and, furthermore, to learn to dynamically control and manipulate an external memory. We argue that this key modification gives the neural system the ability to capture powerful transition functions for each possible input. We demonstrate the effectiveness of our proposed higher-order, memoryaugmented recursive-NN models on two challenging mathematical equation tasks, showing improved extrapolation, stable performance, and faster convergence. Our models achieve a 1.53% average improvement over current state-of-the-art methods in equation verification and achieve a 2.22% Top-1 average accuracy and 2.96% Top-5 average accuracy for equation completion.', 'corpus_id': 233391835, 'score': 0}, {'doc_id': '57365377', 'title': 'Efficient Biosignal Processing Using Hyperdimensional Computing: Network Templates for Combined Learning and Classification of ExG Signals', 'abstract': 'Recognizing the very size of the brain’s circuits, hyperdimensional (HD) computing can model neural activity patterns with points in a HD space, that is, with HD vectors. Key examined properties of HD computing include: a versatile set of arithmetic operations on HD vectors, generality, scalability, analyzability, one-shot learning, and energy efficiency. These make it a prime candidate for efficient biosignal processing where signals are noisy and nonstationary, training data sets are not huge, individual variability is significant, and energy-efficiency constraints are tight. Purely based on native HD computing operators, we describe a combined method for multiclass learning and classification of various ExG biosignals such as electromyography (EMG), electroencephalography (EEG), and electrocorticography (ECoG). We develop a full set of HD network templates that comprehensively encode body potentials and brain neural activity recorded from different electrodes into a single HD vector without requiring domain expert knowledge or ad hoc electrode selection process. Such encoded HD vector is processed as a single unit for fast one-shot learning, and robust classification. It can be interpreted to identify the most useful features as well. Compared to state-of-the-art counterparts, HD computing enables online, incremental, and fast learning as it demands less than a third as much training data as well as less preprocessing.', 'corpus_id': 57365377, 'score': 1}, {'doc_id': '150046998', 'title': 'A framework for computational models of human memory', 'abstract': 'We present analysis of existing memory models, examining how models represent knowledge, structure memory, learn, make decisions, and predict reaction times. On the basis of this analysis, we propose a theoretical framework that characterizes memory modelling in terms of six key decisions: (1) choice of knowledge representation scheme, (2) choice of data structure, (3) choice of associative architecture, (4) choice of learning rule, (5) choice of time variant process, and (6) choice of response decision criteria. This framework is both descriptive and proscriptive: we intend to both describe the state of the literature and outline what we believe is the most fruitful space of possibilities for the development of future memory models.', 'corpus_id': 150046998, 'score': 1}, {'doc_id': '198314968', 'title': 'Hyperdimensional Computing with Local Binary Patterns: One-shot Learning for Seizure Onset Detection and Identification of Ictogenic Brain Regions from Short-time iEEG Recordings', 'abstract': 'OBJECTIVE \n \nWe develop a fast learning algorithm combining symbolic dynamics and brain-inspired hyperdimensional computing for both seizure onset detection and identification of ictogenic (seizure generating) brain regions from intracranial electroencephalography (iEEG). \n \nMETHODS \n \nOur algorithm first transforms iEEG time series from each electrode into symbolic local binary pattern codes from which a holographic distributed representation of the brain state of interest is constructed across all the electrodes and over time in a hyperdimensional space. The representation is used to quickly learn from few seizures, detect their onset, and identify the spatial brain regions that generated them. \n \nRESULTS \n \nWe assess our algorithm on our dataset that contains 99 short-time iEEG recordings from 16 drug-resistant epilepsy patients being implanted with 36 to 100 electrodes. For the majority of the patients (10 out of 16), our algorithm quickly learns from one or two seizures and perfectly (100%) generalizes on novel seizures using k-fold cross-validation. For the remaining six patients, the algorithm requires three to six seizures for learning. Our algorithm surpasses the state-of-the-art including deep learning algorithms by achieving higher specificity (94.84% vs. 94.77%) and macroaveraging accuracy (95.42% vs. 94.96%), and 74x lower memory footprint, but slightly higher average latency in detection (15.9 s vs. 14.7 s). Moreover, the algorithm can reliably identify (with a p-value < 0.01) the relevant electrodes covering an ictogenic brain region at two levels of granularity: cerebral hemispheres and lobes. \n \nCONCLUSION AND SIGNIFICANCE \n \nOur algorithm provides: (1) a unified method for both learning and classification tasks with end-to-end binary operations; (2) one-shot learning from seizure examples; (3) linear computational scalability for increasing number of electrodes; (4) generation of transparent codes that enables post-translational supports for clinical decision making.', 'corpus_id': 198314968, 'score': 1}]
34	Causal text 	1228621de9773edd0dc9848717efda52	20566	{}	"[{'doc_id': '237420562', 'title': 'Transformer Models for Text Coherence Assessment', 'abstract': 'Coherence is an important aspect of text quality and is crucial for ensuring its readability. It is an essential desirable for outputs from text generation systems like summarization, question answering, machine translation, question generation, table-to-text, etc. An automated coherence scoring model is also helpful in essay scoring or providing writing feedback. A large body of previous work has leveraged entity-based methods, syntactic patterns, discourse relations and more recently traditional deep learning architectures for text coherence assessment. We hypothesize that coherence assessment is a cognitively complex task which requires deeper models and can benefit from other related tasks. Accordingly, in this paper, we propose four different Transformerbased architectures for the task: vanilla Transformer, hierarchical Transformer, multi-task learning-based model, and a model with factbased input representation. Our experiments with popular benchmark datasets across multiple domains on four different coherence assessment tasks demonstrate that our models achieve state-of-the-art results outperforming existing models by a good margin.', 'corpus_id': 237420562, 'score': 0}, {'doc_id': '237386009', 'title': 'Causal Inference in Natural Language Processing: Estimation, Prediction, Interpretation and Beyond', 'abstract': 'A fundamental goal of scientific research is to learn about causal relationships. However, despite its critical role in the life and social sciences, causality has not had the same importance in Natural Language Processing (NLP), which has traditionally placed more emphasis on predictive tasks. This distinction is beginning to fade, with an emerging area of interdisciplinary research at the convergence of causal inference and language processing. Still, research on causality in NLP remains scattered across domains without unified definitions, benchmark datasets and clear articulations of the remaining challenges. In this survey, we consolidate research across academic areas and situate it in the broader NLP landscape. We introduce the statistical challenge of estimating causal effects, encompassing settings where text is used as an outcome, treatment, or as a means to address confounding. In addition, we explore potential uses of causal inference to improve the performance, robustness, fairness, and interpretability of NLP models. We thus provide a unified overview of causal inference for the computational linguistics community. 1 ∗All authors equally contributed to this paper. Author names are organized alphabetically in two clusters: First students and post-docs and then faculty members. The email address of the corresponding (first) author is: feder@campus.technion.ac.il. An online repository containing existing research on causal inference and language processing is available here: https://github.com/causaltext/', 'corpus_id': 237386009, 'score': 1}, {'doc_id': '237635429', 'title': 'Proximal mediation analysis', 'abstract': 'A common concern when trying to draw causal inferences from observational data is that the measured covariates are insufficiently rich to account for all sources of confounding. In practice, many of the covariates may only be proxies of the latent confounding mechanism. Recent work has shown that in certain settings where the standard ‘no unmeasured confounding’ assumption fails, proxy variables can be leveraged to identify causal effects. Results currently exist for the total causal effect of an intervention, but little consideration has been given to learning about the direct or indirect pathways of the effect through a mediator variable. In this work, we describe three separate proximal identification results for natural direct and indirect effects in the presence of unmeasured confounding. We then develop a semiparametric framework for inference on natural (in)direct effects, which leads us to locally efficient, multiply robust estimators.', 'corpus_id': 237635429, 'score': 1}, {'doc_id': '237363313', 'title': 'DoWhy: Addressing Challenges in Expressing and Validating Causal Assumptions', 'abstract': 'Estimation of causal effects involves crucial assumptions about the data-generating process, such as directionality of effect, presence of instrumental variables or mediators, and whether all relevant confounders are observed. Violation of any of these assumptions leads to significant error in the effect estimate. However, unlike crossvalidation for predictive models, there is no global validator method for a causal estimate. As a result, expressing different causal assumptions formally and validating them (to the extent possible) becomes critical for any analysis. We present DoWhy, a framework that allows explicit declaration of assumptions through a causal graph and provides multiple validation tests to check a subset of these assumptions. Our experience with DoWhy highlights a number of open questions for future research: developing new ways beyond causal graphs to express assumptions, the role of causal discovery in learning relevant parts of the graph, and developing validation tests that can better detect errors, both for average and conditional treatment effects. DoWhy is available at https: //github.com/microsoft/dowhy.', 'corpus_id': 237363313, 'score': 1}, {'doc_id': '237490892', 'title': 'Uncovering Main Causalities for Long-tailed Information Extraction', 'abstract': 'Information Extraction (IE) aims to extract structural information from unstructured texts. In practice, long-tailed distributions caused by the selection bias of a dataset, may lead to incorrect correlations, also known as spurious correlations, between entities and labels in the conventional likelihood models. This motivates us to propose counterfactual IE (CFIE), a novel framework that aims to uncover the main causalities behind data in the view of causal inference. Specifically, 1) we first introduce a unified structural causal model (SCM) for various IE tasks, describing the relationships among variables; 2) with our SCM, we then generate counterfactuals based on an explicit language structure to better calculate the direct causal effect during the inference stage; 3) we further propose a novel debiasing approach to yield more robust predictions. Experiments on three IE tasks across five public datasets show the effectiveness of our CFIE model in mitigating the spurious correlation issues.', 'corpus_id': 237490892, 'score': 0}, {'doc_id': '51372401', 'title': 'What Is Going on with Stakeholder Theory in Project Management Literature? A Symbiotic Relationship for Sustainability', 'abstract': 'Nowadays the advance towards sustainability poses a global challenge for modern society as well as for companies. Professionals and academics continually redefine business processes and design management mechanisms in a more appropriate way in order to allow companies to balance economic activity with the environmental and social impact that they generate. Under this complex and dynamic scenario, creating a product, providing a service, or achieving a given result requires a different interpretation of the efficiency paradigm and an adequate socio-environmental intelligence. In the context of project management, sustainability-related knowledge, skills, and suitable tools are necessary to face this challenge. Moreover, its close relationship with stakeholder theory presents an alternative to approach that purpose. This article attempts a systematic review of the literature on stakeholder theory in project management during the past nine years, with the aim of providing a comprehensive view of this relationship, revealing its impact and influence on sustainability, and finding new research paths. We highlight the potential benefits derived from this relationship, either as an instrument for the promotion of corporate social responsibility and inclusive policies, as a means for the generation of shared value and technological innovation, or as a key factor in the strategy and business management of a given project.', 'corpus_id': 51372401, 'score': 0}, {'doc_id': '220793347', 'title': 'Adapting Text Embeddings for Causal Inference', 'abstract': ""Does adding a theorem to a paper affect its chance of acceptance? Does labeling a post with the author's gender affect the post popularity? This paper develops a method to estimate such causal effects from observational text data, adjusting for confounding features of the text such as the subject or writing quality. We assume that the text suffices for causal adjustment but that, in practice, it is prohibitively high-dimensional. To address this challenge, we develop causally sufficient embeddings, low-dimensional document representations that preserve sufficient information for causal identification and allow for efficient estimation of causal effects. Causally sufficient embeddings combine two ideas. The first is supervised dimensionality reduction: causal adjustment requires only the aspects of text that are predictive of both the treatment and outcome. The second is efficient language modeling: representations of text are designed to dispose of linguistically irrelevant information, and this information is also causally irrelevant. Our method adapts language models (specifically, word embeddings and topic models) to learn document embeddings that are able to predict both treatment and outcome. We study causally sufficient embeddings with semi-synthetic datasets and find that they improve causal estimation over related embedding methods. We illustrate the methods by answering the two motivating questions---the effect of a theorem on paper acceptance and the effect of a gender label on post popularity. Code and data available at this https URL}{this http URL"", 'corpus_id': 220793347, 'score': 1}, {'doc_id': '236980129', 'title': 'Causal Inference from Network Data', 'abstract': 'This tutorial presents state-of-the-art research on causal inference from network data in the presence of interference. We start by motivating research in this area with real-world applications, such as measuring influence in social networks and market experimentation. We discuss the challenges of applying existing causal inference techniques designed for independent and identically distributed (i.i.d.) data to relational data, some of the solutions that currently exist and the gaps and opportunities for future research. We present existing network experiment designs for measuring different possible effects of interest. Then we focus on causal inference from observational data, its representation, identification, and estimation. We conclude with research on causal discovery in networks.', 'corpus_id': 236980129, 'score': 0}, {'doc_id': '237353672', 'title': 'Pulling Up by the Causal Bootstraps: Causal Data Augmentation for Pre-training Debiasing', 'abstract': 'Machine learning models achieve state-of-the-art performance on many supervised learning tasks. However, prior evidence suggests that these models may learn to rely on “shortcut” biases or spurious correlations (intuitively, correlations that do not hold in the test as they hold in train) for good predictive performance. Such models cannot be trusted in deployment environments to provide accurate predictions. While viewing the problem from a causal lens is known to be useful, the seamless integration of causation techniques into machine learning pipelines remains cumbersome and expensive. In this work, we study and extend a causal pre-training debiasing technique called causal bootstrapping (CB) under five practical confounded-data generation-acquisition scenarios (with known and unknown confounding). Under these settings, we systematically investigate the effect of confounding bias on deep learning model performance, demonstrating their propensity to rely on shortcut biases when these biases are not properly accounted for. We demonstrate that such a causal pre-training technique can significantly outperform existing base practices to mitigate confounding bias on real-world domain generalization benchmarking tasks. This systematic investigation underlines the importance of accounting for the underlying data-generating mechanisms and fortifying data-preprocessing pipelines with a causal framework to develop methods robust to confounding biases.', 'corpus_id': 237353672, 'score': 0}, {'doc_id': '237248742', 'title': 'Sensitivity Analyses for Incorporating Machine Learning Predictions into Causal Estimates', 'abstract': 'Causal inference methods can yield insights into causation from observational datasets. When some necessary variables are unavailable for a causal analysis, machine learning systems may be able to infer those variables based on unstructured data such as images and text. However, if these inferred variables are to be incorporated into causal analyses, the error rate of the underlying classifier should affect the uncertainty in the causal conclusions. Past work has framed classifier accuracy as measurement error to incorporate predictions into consistently-estimated causal effects. However, this estimator is sensitive to small errors in its estimation of the classifier’s error rate, leading to erratic outputs that are uninterpretable for any given analysis. In this paper we introduce three sensitivity analyses that capture the uncertainty from using a machine learning classifier in causal estimation, and show that our methods enable more robust analyses.', 'corpus_id': 237248742, 'score': 1}]"
35	Epistemic uncertainty in safety risk frameworks	0ea8a399f93bc1d265cbdad6096d54e8	13478	{}	"[{'doc_id': '161092946', 'title': 'Socialização, violencia e prostituição', 'abstract': 'Nossa pesquisa esta vinculada ao Grupo de Pesquisa Violencia, Imaginario e Educacao (Violar), da Faculdade de Educacao da Universidade Estadual de Campinas – SP (UNICAMP), e tem como objeto de estudo a relacao entre a memoria da violencia no processo de socializacao de criancas e \nadolescentes e o exercicio da prostituicao como modo de vida. O ponto de partida de nossas reflexoes foram as narrativas de tres mulheres, cujas condicoes de ingresso precoce na prostituicao (antes dos 18 anos) e cuja trajetoria infanto-juvenil, marcada por varias modalidades de violencia \n(fisica, simbolica, sexual etc.), correspondem as caracteristicas que definimos para constituir nosso universo investigativo. Nosso intuito foi apresentar de forma textual a experiencia vivida e os argumentos orais dos proprios sujeitos. Para isso, ancoramo-nos em autores como Walter Benjamin, Eclea Bosi, Jose Carlos Sebe Bom Meihy e Maurice Halbwachs, buscando compreender os significados da violencia nas memorias das jovens. Apresentamos, por isso diferentes situacoes de miseria material e moral, as quais retratam inumeras transgressoes que contradizem nossa nocao de civilizacao. Nao obstante acreditamos que o desvelamento desta “condicao humana” pode contribuir com a producao do conhecimento tanto quanto a investigacao de outros universos do mundo social. \n \nAbstract', 'corpus_id': 161092946, 'score': 0}, {'doc_id': '186341627', 'title': 'Strength of Knowledge Assessment for Risk Informed Decision Making', 'abstract': 'Risk Informed Decision Making (RIDM) is based on risk metrics obtained from a Probabilistic Risk Assessment (PRA). For plants exposed to multiple hazards, Multi-Hazards Risk Aggregation (MHRA) is necessary to inform decisions. In practice, this is often done by a simple arithmetic summation over the different risk contributors, without taking into account that the state of knowledge of the risk models of the different hazards can be quite different. In this paper, we provide a hierarchical framework to assess the strength of knowledge that PRA models are based upon. The framework is organized in three attributes characterizing the knowledge which a PRA model is based upon (assumptions, data, phenomenological understanding). These attributes are further broken down into sub-attributes and, finally, “leaf” attributes that can be evaluated. The PRA models of two hazards groups for Nuclear Power Plants (NPPs) are considered and the strength of knowledge behind each model is assessed using the developed framework.', 'corpus_id': 186341627, 'score': 1}, {'doc_id': '214494009', 'title': 'A New Framework To Idenitfy And Assess Hidden Assumptions In The Background Knowledge Of A Risk Assessment', 'abstract': 'Abstract A risk assessment has a more or less subjective nature, as the analyst needs to make assumptions, analyse data, use models, and so on, to produce risk-related knowledge of the phenomena of interest. This background knowledge that forms the foundation of a risk assessment can be more or less strong, implying that it needs to be taken into consideration when describing and communicating risks. To meet this challenge, different methods have been developed to evaluate and inform the decision-maker about the strength of the background knowledge. For all these methods to be fully informative, the content of the background knowledge needs to be of good quality, covering, for example, all the relevant assumptions. To identify all the relevant assumptions, however, is not a trivial task, and the risk of missing assumptions increases with the complexity of the situation of interest. Hidden assumptions, which are not considered or identified, may induce false confidence in the risk assessment, its results and recommendations. This paper suggests a framework, using a systems approach, to identify and assess the background knowledge, as a means to reduce the risk of missing critical knowledge and obtain a more complete background knowledge, on which risk can be assessed.', 'corpus_id': 214494009, 'score': 1}, {'doc_id': '42509889', 'title': 'Strengthening quantitative risk assessments by systematic treatment of uncertain assumptions', 'abstract': 'Abstract The results of quantitative risk assessments (QRA) are conditional on the background knowledge on which the assessments are based, including phenomenological understanding, models, data and expert statements used, as well as assumptions made. Risk indices established in the risk assessment, such as individual risk numbers and f–N curves, may have a more or less solid foundation, depending for example on the validity of assumptions made. Poor models, lack of data or simplistic assumptions are examples of potential sources of uncertainty “hidden in the background knowledge” of a risk assessment. These uncertainties need to be reflected in the risk assessment. Recently, a method for treating uncertain assumptions in a QRA was suggested. The method is based on the different settings faced when making assumptions in risk assessments, considering beliefs about assumption deviation, sensitivity of the risk index to changes in the assumption, and the overall strength of knowledge involved. In the present paper we apply, test and adjust the method using a risk assessment of a lifting operation related to the oil and gas industry as a case. We find that an adjusted version of the method provides systematic guidance on how to treat uncertainties in a QRA.', 'corpus_id': 42509889, 'score': 1}, {'doc_id': '229227118', 'title': 'Integrating risk assessment with aviation cybersecurity framework', 'abstract': 'Risk assessment of aviation systems identifies the elements of the aviation system that we need to protect It differentiates those areas that have only an economic impact from those areas that also have a safety impact The timeliness required for responding to threats is critical and becomes a part of the risk assessment model The risk framework for Aviation safety and security has changed in the current post Covid-19 environment Quantitative techniques and probabilistic models help in developing composite risk score factors with many attributes It forms an extended part of Aviation Cybersecurity Framework In this paper we analyze various components of the risk extended Aviation Cybersecurity Framework in the post Covid-19 environment It forms the basis of risk mitigation and confidence building for aviation in the current environment © 2020, American Institute of Aeronautics and Astronautics Inc, AIAA All rights reserved', 'corpus_id': 229227118, 'score': 0}, {'doc_id': '198484063', 'title': 'Dealing with epistemic uncertainty in risk-informed decision making for dam safety management', 'abstract': 'Abstract In recent years, the application of risk analysis to inform dam safety governance has increased significantly. In this framework, considering explicitly and independently both natural and epistemic uncertainty in quantitative risk models allows to understand the sources of uncertainty in risk results and to estimate the effect of actions, tests, and surveys to reduce epistemic uncertainty. In this paper, Indexes of Coincidence are proposed to analyze the effect of epistemic uncertainty in the prioritization of investments based on risk results, which is the key issue in this paper. These indexes allow consideration of the convenience of conducting additional uncertainty reduction actions. These metrics have been applied to the prioritization of risk reduction measures for four concrete gravity dams in Spain. Results allow for a better understanding of how epistemic uncertainty of geotechnical resistance parameters influence risk-informed decision making. The proposed indexes are also useful for probabilistic risk analyses of other civil engineering structures with high epistemic uncertainty environments, since they analyze whether existing uncertainty could have an impact on decision making, outlining the need for extra studies, surveys and tests.', 'corpus_id': 198484063, 'score': 1}, {'doc_id': '231766007', 'title': 'Multifactorial Rare Diseases: Can Uncertainty Analysis Bring Added Value to the Search for Risk Factors and Etiopathogenesis?', 'abstract': 'Uncertainty analysis is the process of identifying limitations in knowledge and evaluating their implications for scientific conclusions. Uncertainty analysis is a stable component of risk assessment and is increasingly used in decision making on complex health issues. Uncertainties should be identified in a structured way and prioritized according to their likely impact on the outcome of scientific conclusions. Uncertainty is inherent to the rare diseases (RD) area, where research and healthcare have to cope with knowledge gaps due to the rarity of the conditions; yet a systematic approach toward uncertainties is not usually undertaken. The uncertainty issue is particularly relevant to multifactorial RD, whose etiopathogenesis involves environmental factors and genetic predisposition. Three case studies are presented: the newly recognized acute multisystem inflammatory syndrome in children and adolescents associated with SARS-CoV-2 infection; the assessment of risk factors for neural tube defects; and the genotype–phenotype correlation in familial Mediterranean fever. Each case study proposes the initial identification of the main epistemic and sampling uncertainties and their impacts. Uncertainty analysis in RD may present aspects similar to those encountered when conducting risk assessment in data-poor scenarios; therefore, approaches such as expert knowledge elicitation may be considered. The RD community has a main strength in managing uncertainty, as it proactively develops stakeholder involvement, data sharing and open science. The open science approaches can be profitably integrated by structured uncertainty analysis, especially when dealing with multifactorial RD involving environmental and genetic risk factors.', 'corpus_id': 231766007, 'score': 0}, {'doc_id': '110767940', 'title': '인공신경망을 이용한 판스프링의 피로설계', 'abstract': 'The vehicle suspension system is directly influenced to ride and handling. Therefore, the major components of the vehicle suspension system should have enough fatigue strength during its lifetime to protect passenger from the traffic accident. Spring is one of major suspension part of vehicle. Thus, in this paper, a fatigue design method for leaf spring based on geometric parameter was proposed. At first, it makes the fatigue test of leaf spring. On the base of these data, evaluation of numerical stress analysis for leaf spring assembly were performed. And next, after studying numerically on geometrical parameters of leaf spring assembly, investigated on the development of a fatigue life prediction method using the theory of neural network. An economical prediction method of fatigue design criterion for leaf spring assembly using the theory of artificial neural network was developed and certified its ability. Without performing a lot of additional fatigue test for a long time, fatigue design criterion for a new leaf spring assembly having different geometry can be predicted on the base of the already obtained fatigue data.', 'corpus_id': 110767940, 'score': 0}, {'doc_id': '229500901', 'title': 'Accommodation decision-making during the COVID-19 pandemic: Complexity insights from Greece', 'abstract': 'Abstract With the remaining ambiguity around COVID-19 effective treatment, the decision-making process for 2020 tourists remains fraught with complexity. Drawing from a sample of 385 permanent Athenian residents, the study explores the decision-making attributes driving their accommodation purchasing preferences in times of increased uncertainty. The complex dynamics are investigated using fuzzy-set Qualitative Comparative Analysis. A complementary analysis evaluates the size effect of the examined conditions using Necessary Condition Analysis. In total, four solutions are generated concerning: (i) health and safety; (ii) the price-quality nexus; (iii) risk aspects; and (iv) quality related health and safety. The study contributes towards the initiation of the theoretical discourse on the foundations of the exploration of tourists’ accommodation choice triggers and dilemmas in times of pandemics. The results inform market intelligence with regard to accommodation-related customer priorities, perceptions and intentions during the pandemic which lay several important managerial implications for the accommodation industry.', 'corpus_id': 229500901, 'score': 0}, {'doc_id': '33236711', 'title': 'Incorporating assumption deviation risk in quantitative risk assessments: A semi-quantitative approach', 'abstract': ""Quantitative risk assessments (QRAs) of complex engineering systems are based on numerous assumptions and expert judgments, as there is limited information available for supporting the analysis. In addition to sensitivity analyses, the concept of assumption deviation risk has been suggested as a means for explicitly considering the risk related to inaccuracies and deviations in the assumptions, which can significantly impact the results of the QRAs. However, challenges remain for its practical implementation, considering the number of assumptions and magnitude of deviations to be considered. This paper presents an approach for integrating an assumption deviation risk analysis as part of QRAs. The approach begins with identifying the safety objectives for which the QRA aims to support, and then identifies critical assumptions with respect to ensuring the objectives are met. Key issues addressed include the deviations required to violate the safety objectives, the uncertainties related to the occurrence of such events, and the strength of knowledge supporting the assessments. Three levels of assumptions are considered, which include assumptions related to the system's structural and operational characteristics, the effectiveness of the established barriers, as well as the consequence analysis process. The approach is illustrated for the case of an offshore installation."", 'corpus_id': 33236711, 'score': 1}]"
36	RWE	b6a96f1a4a8db65a5db3eea1287618ce	17827	{'RWE': 'real-world evidence'}	"[{'doc_id': '234489208', 'title': 'Chemotherapy-induced neutropenia and treatment efficacy in advanced non-small-cell lung cancer: a pooled analysis of 6 randomized trials', 'abstract': 'Background Chemotherapy-induced neutropenia (CIN) has been demonstrated to be a prognostic factor in several cancer conditions. We previously found a significant prognostic value of CIN on overall survival (OS), in a pooled dataset of patients with advanced non-small-cell lung cancer (NSCLC) receiving first line chemotherapy from 1996 to 2001. However, the prognostic role of CIN in NSCLC is still debated. Methods We performed a post hoc analysis pooling data prospectively collected in six randomized phase 3 trials in NSCLC conducted from 2002 to 2016. Patients who never started chemotherapy and those for whom toxicity data were missing were excluded. Neutropenia was categorized on the basis of worst grade during chemotherapy: absent (grade 0), mild (grade 1–2), or severe (grade 3–4). The primary endpoint was OS. Multivariable Cox model was applied for statistical analyses. In the primary analysis, a minimum time (landmark) at 180\u2009days from randomization was applied in order to minimize the time-dependent bias. Results Overall, 1529 patients, who received chemotherapy, were eligible; 572 of them (who received 6\u2009cycles of treatment) represented the landmark population. Severe CIN was reported in 143 (25.0%) patients and mild CIN in 135 (23.6%). At multivariable OS analysis, CIN was significantly predictive of prognosis although its prognostic value was entirely driven by severe CIN (hazard ratio [HR] of death 0.71; 95%CI: 0.53–0.95) while it was not evident with mild CIN (HR 1.21; 95%CI: 0.92–1.58). Consistent results were observed in the out-of-landmark group (including 957 patients), where both severe and mild CIN were significantly associated with a reduced risk of death. Conclusion The pooled analysis of six large trials of NSCLC treatment shows that CIN occurrence is significantly associated with a longer overall survival, particularly in patients developing severe CIN, confirming our previous findings.', 'corpus_id': 234489208, 'score': 0}, {'doc_id': '235360451', 'title': 'Effectiveness of neoadjuvant chemotherapy on the survival outcomes of patients with resectable non-small-cell lung cancer: A meta-analysis of randomized controlled trials.', 'abstract': 'PURPOSE\nTo determine the effectiveness of neoadjuvant chemotherapy (NACT) versus primary surgery on survival outcomes for resectable non-small-cell lung cancer (NSCLC) using an approach based on a meta-analysis.\n\n\nMETHODS\nThe PubMed, EmBase, Cochrane library, and CNKI databases were systematically browsed to identify randomized controlled trials (RCTs) which met a set of predetermined inclusion criteria throughout January 2020. Hazard ratios (HRs) were applied for the pooled overall survival (OS) and progression-free survival (PFS) values, and the pooled survival rates at 1-year and 3-year were used as the relative risk (RR). All the pooled effect estimates with 95% confidence intervals (CIs) were calculated using the random-effects model.\n\n\nRESULTS\nNineteen RCTs contained a total of 4372 NSCLC at I-III stages was selected for final meta-analysis. We noted NACT was significantly associated with an improvement in OS (HR: 0.87; 95%CI: 0.81-0.94; P\xa0<\xa00.001) and PFS (HR: 0.86; 95%CI: 0.78-0.96; P\xa0=\xa00.005). Moreover, the survival rate at 1-year (RR: 1.07; 95%CI: 1.02-1.12; P\xa0=\xa00.007) and 3-year (RR: 1.16; 95%CI: 1.06-1.27; P\xa0=\xa00.001) in the NACT group was significantly higher than the survival rate for the primary surgery group. Finally, the treatment effects of NACT versus primary surgery on survival outcomes might be different when stratified by the mean age of patients and the tumor stages.\n\n\nCONCLUSIONS\nNACT could improve survival outcomes for patients with resectable NSCLC, suggesting its suitable future applicability for clinical practice. However, large-scale RCT should be conducted to assess the chemotherapy regimen on the prognosis of resectable NSCLC.', 'corpus_id': 235360451, 'score': 0}, {'doc_id': '3813029', 'title': 'Real-world evidence in the treatment of ovarian cancer', 'abstract': ""Introduction\n'Real-world evidence (RWE)' refers to information on the utilization and outcome of new therapies and technologies in clinical practice. RWE may include single institution cohort studies, population-based health services studies, or (inter)national data on survival and mortality. This paper reviews RWE on the impact of treatment in ovarian cancer.\n\n\nMaterials and methods\nA literature review of publications addressing population level survival outcomes of new surgical and systemic treatment interventions in ovarian cancer was undertaken. In addition, literature and international cancer registry trends in ovarian cancer survival, mortality and incidence rates were compiled. These latter were utilized to make inferences on the relative impact of new treatments as well as changing incidence rates on observed mortality trends.\n\n\nResults\nThe last four decades have seen new systemic and surgical treatments introduced into practice for ovarian cancer based on randomized trial evidence. However, there has been little published on population level uptake and survival outcomes of those interventions. Exceptions were population studies on intraperitoneal chemotherapy and neoadjuvant chemotherapy. One paper demonstrated modest uptake of intraperitoneal chemotherapy and evidence of improved survival. Cancer registry statistics revealed falling incidence rates (∼1%-2% per year) for ovarian cancer across Europe, North America and elsewhere over the last three to four decades. Mortality rates also declined by ∼1%-2% per year over this period. Population 5-year relative survival estimates also improved over this period [from 33.7% in 1975 to 46.2% in 2008 (SEER data)].\n\n\nConclusions\nThere are few RWE studies of specific treatments in ovarian cancer. Trends in relative survival and population mortality have shown improvements. Mortality changes can be explained in part by reductions in ovarian cancer incidence rates (speculated to be due to use of oral contraceptives and reduction in postmenopausal hormone use). However, it is plausible that at least some of the mortality reduction is related to improved survival of patients with the introduction of effective new treatments."", 'corpus_id': 3813029, 'score': 1}, {'doc_id': '234471422', 'title': 'Drug development in the era of precision medicine in oncology-Clues beyond randomization.', 'abstract': 'The primary objective of any treatment in oncology is the improve patients’ overall survival (OS) and/or quality of life (QoL). Patients with solid tumors may often be cured thanks to local treatments including surgery and radiotherapy when then are free of distant metastases. In this setting, anticancer drugs may improve cure rates when combined to local treatments. In the recurrent and/ or metastatic setting, drugs represent the main treatment option, while surgery and radiotherapy might still be used in a palliative intent in most cases. With the exception of germline tumors and lymphoma, drugs have a limited ability to cure patients in this setting, and patients most often need to receive sequential treatments for life. In the ancestral paradigm of drug development in oncology, drugs used to be developed per cancer type following a well-established path that well suited chemotherapeutic agents for which antitumor activity largely depended on cancer types in preclinical models (Table 1). The first evaluation of new drugs in patients in phase I clinical trials was usually performed in patients who had exhausted standard of care, and not in healthy volunteers given the toxic nature of the drugs and their narrow therapeutic index. Dose escalation used to be open to patients with any type of cancer however, in order not to miss a serendipitous antitumor activity in unexpected cancer types. Phase I trials were not randomized, and enabled to establish the schedule and the recommended phase II dose that was the highest dose to be considered safe for further evaluation. Based on antitumor activity observed in preclinical models and during phase I trials, preliminary drug efficacy was then assessed in a specific cancer type and setting (usually in the recurrent and/or metastatic setting) in single-arm or randomized phase II clinical trials. Surrogate endpoints such as the objective response rate (ORR) or progression-free survival (PFS) were commonly used to have a rapid read-out of the efficacy. These surrogate endpoints were assessed by following over time the tumor burden, formerly the sum of the product of the two diameters of the target lesions (WHO criteria) (1), and more recently the sum of the largest diameter of up to five target lesions (RECIST) (2). Randomized phase III clinical trials were performed in a similar patient population in order to demonstrate an improvement in OS and/or QoL of the new treatment over the current standard of care. Randomization is the gold-standard approach for market access in order to avoid selection biases. Recently, seamless drug development led to increasingly replace phase II clinical trials with large expansion cohorts performed during phase I trials, in order to accelerate drug development (3). This paradigm based on the randomization for drug approval nicely fitted to drugs developed in specific but common cancer types, but is less suited to rare cancer types. A better understanding of cancer biology led to the development of molecularly targeted agents (MTAs) that were specifically designed to modulate a molecular pathway in the tumor cells or their microenvironment, and Editorial', 'corpus_id': 234471422, 'score': 1}, {'doc_id': '233487087', 'title': 'Trends in the prescription of systemic anticancer therapy and mortality among patients with advanced non-small cell lung cancer: a real-world retrospective observational cohort study from the I-O optimise initiative', 'abstract': 'Objectives To assess how a decade of developments in systematic anticancer therapy (SACT) for advanced non-small cell lung cancer (NSCLC) affected overall survival (OS) in a large UK University Hospital. Design Real-world retrospective observational cohort study using existing data recorded in electronic medical records. Setting A large National Health Service (NHS) university teaching hospital serving 800 000 people living in a diverse metropolitan area of the UK. Participants 2119 adults diagnosed with advanced NSCLC (tumour, node, metastasis stage IIIB or IV) between 2007 and 2017 at Leeds Teaching Hospitals NHS Trust. Main outcomes and measures OS following diagnosis and the analysis of factors associated with receiving SACT. Results Median OS for all participants was 2.9 months, increasing for the SACT-treated subcohort from 8.4 months (2007–2012) to 9.1 months (2013–2017) (p=0.02); 1-year OS increased from 33% to 39% over the same period for the SACT-treated group. Median OS for the untreated subcohort was 1.6 months in both time periods. Overall, 30.6% (648/2119) patients received SACT; treatment rates increased from 28.6% (338/1181) in 2007–2012 to 33.0% (310/938) in 2013–2017 (p=0.03). Age and performance status were independent predictors for SACT treatment; advanced age and higher performance status were associated with lower SACT treatment rates. Conclusion Although developments in SACT during 2007–2017 correspond to some changes in survival for treated patients with advanced NSCLC, treatment rates remain low and the prognosis for all patients remains poor.', 'corpus_id': 233487087, 'score': 1}, {'doc_id': '103141199', 'title': 'Nanoscale size effect of octahedral nickel catalyst towards ammonia decomposition reaction', 'abstract': 'Abstract Detailed density functional theory (DFT) calculations have been carried out to study the ammonia (NH3) decomposition activity and catalytic mechanism for octahedral nickel nanoclusters. Four different size nickel nanoclusters, Ni19, Ni44, Ni85 and Ni146, were applied to investigate the nanoscale size effect. The results reveal that these four nickel nanoclusters adopt the same NH3 decomposition mechanism. The predicted catalytic activities of these nickel nanoclusters decrease in the sequence of Ni44\xa0>\xa0Ni146\xa0≈\xa0Ni85\xa0>\xa0Ni19, based on the analysis of adsorption energies of NH3 decomposition intermediates and landscapes of NH3 decomposition pathways. Besides, the reaction heat of Ni146 nanocluster is only 0.65\xa0eV in the process of N2 desorption, showing the nature of easy desorption out of the catalyst surface. Moreover, molecular dynamics calculations show that large-size nanoclusters performed better on thermal stability. Combined with dynamics analysis, we found that Ni146 nanocluster possesses not only high thermodynamic stability but also catalytic activity. Electronic structural analysis show that negatively Mulliken charged sites were more favorable for adsorbing N and act as active centers.', 'corpus_id': 103141199, 'score': 0}, {'doc_id': '233405261', 'title': 'Using Electronic Health Records to Derive Control Arms for Early Phase SingleArm Lung Cancer Trials: ProofofConcept in Randomized Controlled Trials', 'abstract': 'Oncology drug development increasingly relies on single-arm clinical trials. External controls (ECs) derived from electronic health record (EHR) databases may provide additional context. Patients from a US-based oncology EHR database were aligned with patients from randomized controlled trials (RCTs) and trial-specific eligibility criteria were applied to the EHR dataset. Overall survival (OS) in the EC-derived control arm was compared with OS in the RCT experimental arm. The primary outcome was OS, defined as time from randomization or treatment initiation (EHR) to death. Cox regression models were used to obtain effect estimates using EHR data. EC-derived hazard ratio estimates aligned closely with those from the corresponding RCT with one exception. Comparing log HRs among all RCT and EC results gave a Pearson correlation coefficient of 0.86. Properly selected control arms from contemporaneous EHR data could be used to put single-arm trials of OS in advanced non-small cell lung cancer into context.', 'corpus_id': 233405261, 'score': 1}, {'doc_id': '235205365', 'title': 'Management of advanced ovarian cancer in Spain: an expert Delphi consensus', 'abstract': 'Background To determine the state of current practice and to reach a consensus on recommendations for the management of advanced ovarian cancer using a Delphi survey with a group of Spanish gynecologists and medical oncologists specially dedicated to gynecological tumors. Methods The questionnaire was developed by the byline authors. All questions but one were answered using a 9-item Likert-like scale with three types of answers: frequency, relevance and agreement. We performed two rounds between December 2018 and July 2019. A consensus was considered reached when at least 75% of the answers were located within three consecutive points of the Likert scale. Results In the first round, 32 oncologists and gynecologists were invited to participate, and 31 (96.9%) completed the online questionnaire. In the second round, 27 (87.1%) completed the online questionnaire. The results for the questions on first-line management of advanced disease, treatment of patients with recurrent disease for whom platinum might be the best option, and treatment of patients with recurrent disease for whom platinum might not be the best option are presented. Conclusions This survey shows a snapshot of current recommendations by this selected group of physicians. Although the majority of the agreements and recommendations are aligned with the recently published ESMO-ESGO consensus, there are some discrepancies that can be explained by differences in the interpretation of certain clinical trials, reimbursement or accessibility issues.', 'corpus_id': 235205365, 'score': 0}, {'doc_id': '233988691', 'title': 'Retrospective analysis of real-world treatment patterns and clinical outcomes in patients with advanced non-small cell lung cancer starting first-line systemic therapy in the United Kingdom', 'abstract': 'Background The treatment landscape for advanced non-small cell lung cancer (aNSCLC) has evolved rapidly since immuno-oncology (IO) therapies were introduced. This study used recent data to assess real-world treatment patterns and clinical outcomes in aNSCLC in the United Kingdom. Methods Electronic prescribing records of treatment-naive patients starting first-line (1\u2009L) treatment for aNSCLC between June 2016 and March 2018 (follow-up until December 2018) in the United Kingdom were assessed retrospectively. Patient characteristics and treatment patterns were analyzed descriptively. Outcomes assessed included overall survival (OS), time to treatment discontinuation, time to next treatment, and real-world tumor response. Results In all, 1003 patients were evaluated (median age, 68\u2009years [range, 28–93\u2009years]; 53.9% male). Use of 1\u2009L IO monotherapy (0–25.9%) and targeted therapy (11.8–15.9%) increased during the study period, but chemotherapy remained the most common 1\u2009L treatment at all time points (88.2–58.2%). Median OS was 9.5\u2009months (95% CI, 8.8–10.7\u2009months) for all patients, 8.1\u2009months (95% CI, 7.4–8.9\u2009months) with chemotherapy, 14.0\u2009months (95% CI, 10.7–20.6\u2009months) with IO monotherapy, and 20.2\u2009months (95% CI, 16.0–30.5\u2009months) with targeted therapy. In the 28.6% of patients who received second-line treatment, IO monotherapy was the most common drug class (used in 51.6%). Conclusions Although use of 1\u2009L IO monotherapy for aNSCLC increased in the United Kingdom during the study period, most patients received 1\u2009L chemotherapy. An OS benefit for first-line IO monotherapy vs chemotherapy was observed but was numerically smaller than that reported in clinical trials. Targeted therapy was associated with the longest OS, highlighting the need for improved treatment options for tumors lacking targetable mutations.', 'corpus_id': 233988691, 'score': 1}, {'doc_id': '235355345', 'title': 'ICI plus chemotherapy prolonged survival over ICI alone in patients with previously treated advanced NSCLC', 'abstract': 'Immune checkpoint inhibitors (ICI) monotherapy was standard of care in second-line treatment of patients with advance non-small cell lung cancer (NSCLC). This study aims to investigate the efficacy of ICI plus chemotherapy in patients with previously treated advanced NSCLC. An investigator-initiated trial (IIT) aiming to evaluate the efficacy and safety of ICI in combination with chemotherapy as second line and beyond for patients with advanced NSCLC was undergone at Shanghai Pulmonary Hospital (ChiCTR1900026203). Patients who received ICI monotherapy as second or later line setting during the same period were also collected as a comparator. From April 2018 to June 2019, 31 patients were included into this IIT study, simultaneously 51 patients treated with ICI monotherapy were selected as a comparator. ICI plus chemotherapy showed a significantly higher ORR (35.5% vs. 15.7%, p=0.039), prolonged PFS (median: 5.6 vs. 2.5 months, p = 0.013) and OS (median: NE vs. 12.6 months, p = 0.038) compared with ICI alone. In the subgroup of negative PD-L1 expression (9 patients in combination group and 12 patients in monotherapy group), ICI plus chemotherapy also had a favorable ORR (44.4% vs. 8.3%, p = 0.119), longer PFS (median: 6.5 vs 3.0 months, p < 0.05) and OS (median: NE vs. 8.2 months, p = 0.117). Meanwhile, the addition of chemotherapy did not increase immune-related adverse events. ICI plus chemotherapy showed superior ORR, PFS and OS than ICI alone patients with previous treated advanced NSCLC. These findings warrant further investigation.', 'corpus_id': 235355345, 'score': 0}]"
37	Human-robot interaction failures	82d04b2e3bb2303dc70069ea7f2da17b	13590	{}	"[{'doc_id': '222419859', 'title': 'A Tale of Two Suggestions: Action and Diagnosis Recommendations for Responding to Robot Failure', 'abstract': 'Robots operating without close human supervision might need to rely on a remote call center of operators for assistance in the event of a failure. In this work, we investigate the effects of providing decision support through diagnosis suggestions, as feedback, and action recommendations, as feedforward, to the human operators. We conduct a 10-condition user study involving 200 participants on Amazon Mechanical Turk to evaluate the effects of providing noisy and noise-free diagnosis suggestions and/or action recommendations to operators. We find that although action recommendations (feedforward) have a greater effect on successful error resolution than diagnosis information (feedback), the feedback likely helps ameliorate the deleterious effects of noise. Therefore, we find that error recovery interfaces should display both diagnosis and action recommendations for maximum effectiveness.', 'corpus_id': 222419859, 'score': 0}, {'doc_id': '67871340', 'title': 'Miscommunication Detection and Recovery in Situated Human–Robot Dialogue', 'abstract': 'Even without speech recognition errors, robots may face difficulties interpreting natural-language instructions. We present a method for robustly handling miscommunication between people and robots in task-oriented spoken dialogue. This capability is implemented in TeamTalk, a conversational interface to robots that supports detection and recovery from the situated grounding problems of referential ambiguity and impossible actions. We introduce a representation that detects these problems and a nearest-neighbor learning algorithm that selects recovery strategies for a virtual robot. When the robot encounters a grounding problem, it looks back on its interaction history to consider how it resolved similar situations. The learning method is trained initially on crowdsourced data but is then supplemented by interactions from a longitudinal user study in which six participants performed navigation tasks with the robot. We compare results collected using a general model to user-specific models and find that user-specific models perform best on measures of dialogue efficiency, while the general model yields the highest agreement with human judges. Our overall contribution is a novel approach to detecting and recovering from miscommunication in dialogue by including situated context, namely, information from a robot’s path planner and surroundings.', 'corpus_id': 67871340, 'score': 1}, {'doc_id': '20410650', 'title': 'What Went Wrong and Why? Diagnosing Situated Interaction Failures in the Wild', 'abstract': 'Effective situated interaction hinges on the well-coordinated operation of a set of competencies, including computer vision, speech recognition, and natural language, as well as higher-level inferences about turn taking and engagement. Systems often rely on a set of hand-coded and machine-learned components organized into several sensing and decision-making pipelines. Given their complexity and inter-dependencies, developing and debugging such systems can be challenging. “In-the-wild” deployments outside of controlled lab conditions bring further challenges due to unanticipated phenomena, including unexpected interactions such as playful engagements. We present a methodology for assessing performance, identifying problems, and diagnosing the root causes and influences of different types of failures on the overall performance of a situated interaction system functioning in the wild. We apply the methodology to a dataset of interactions collected with a robot deployed in a public space inside an office building. The analyses identify and characterize multiple types of failures, their causes, and their relationship to overall performance. We employ models that predict overall interaction quality from various combinations of failures. Finally, we discuss lessons learned with such a diagnostic methodology for improving situated systems deployed in the wild.', 'corpus_id': 20410650, 'score': 1}, {'doc_id': '207916599', 'title': 'A communication paradigm for human-robot interaction during robot failure scenarios', 'abstract': 'Abstract When autonomous robot systems experience failures, communication about the failure to both the people responsible for the robot and the people who happen to be nearby is critically important. New robot users, as well as bystanders, might not be familiar enough with a system to tell whether a robot is working properly and experienced robot operators might not notice signs of trouble due to being out of the loop. Thus, an important feature for robots will be the ability to communicate failure to humans when failures occur. In this chapter, we describe a study conducted with a smartphone-based feedback system and we designed to explore push and pull forms of communication. We found the communication methods improved participants’ understanding of robots’ state, increased their confidence in interacting with the robots, and allowed them to remotely monitor and control the robots. In keeping with the theme of this book, shared context between humans and machines improves performance.', 'corpus_id': 207916599, 'score': 1}, {'doc_id': '231572956', 'title': ""Aligning Robot's Behaviours and Users' Perceptions Through Participatory Prototyping"", 'abstract': 'Robots are increasingly being deployed in public spaces. However, the general population rarely has the opportunity to nominate what they would prefer or expect a robot to do in these contexts. Since most people have little or no experience interacting with a robot, it is not surprising that robots deployed in the real world may fail to gain acceptance or engage their intended users. To address this issue, we examine users’ understanding of robots in public spaces and their expectations of appropriate uses of robots in these spaces. Furthermore, we investigate how these perceptions and expectations change as users engage and interact with a robot. To support this goal, we conducted a participatory design workshop in which participants were actively involved in the prototyping and testing of a robot’s behaviours in simulation and on the physical robot. Our work highlights how social and interaction contexts influence users’ perception of robots in public spaces and how users’ design and understanding of what are appropriate robot behaviors shifts as they observe the enactment of their designs.', 'corpus_id': 231572956, 'score': 0}, {'doc_id': '228063902', 'title': 'Proactive Interaction Framework for Intelligent Social Receptionist Robots', 'abstract': 'Proactive human-robot interaction (HRI) allows the receptionist robots to actively greet people and offer services based on vision, which has been found to improve acceptability and customer satisfaction. Existing approaches are either based on multi-stage decision processes or based on end-to-end decision models. However, the rule-based approaches require sedulous expert efforts and only handle minimal pre-defined scenarios. On the other hand, existing works with end-to-end models are limited to very general greetings or few behavior patterns (typically less than 10). To address those challenges, we propose a new end-to-end framework, the TransFormer with Visual Tokens for Human-Robot Interaction (TFVT-HRI)1. The proposed framework extracts visual tokens of relative objects from an RGB camera first. To ensure the correct interpretation of the scenario, a transformer decision model is then employed to process the visual tokens, which is augmented with the temporal and spatial information. It predicts the appropriate action to take in each scenario and identifies the right target. Our data is collected from an in-service receptionist robot in an office building, which is then annotated by experts for appropriate proactive behavior. The action set includes 1000+ diverse patterns by combining language, emoji expression, and body motions. We compare our model with other SOTA end-to-end models on both offline test sets and online user experiments in realistic office building environments to validate this framework. It is demonstrated that the decision model achieves SOTA performance in action triggering and selection, resulting in more humanness and intelligence when compared with the previous reactive reception policies.', 'corpus_id': 228063902, 'score': 0}, {'doc_id': '49212476', 'title': 'Understanding and Resolving Failures in Human-Robot Interaction: Literature Review and Model Development', 'abstract': ""While substantial effort has been invested in making robots more reliable, experience demonstrates that robots operating in unstructured environments are often challenged by frequent failures. Despite this, robots have not yet reached a level of design that allows effective management of faulty or unexpected behavior by untrained users. To understand why this may be the case, an in-depth literature review was done to explore when people perceive and resolve robot failures, how robots communicate failure, how failures influence people's perceptions and feelings toward robots, and how these effects can be mitigated. Fifty-two studies were identified relating to communicating failures and their causes, the influence of failures on human-robot interaction (HRI), and mitigating failures. Since little research has been done on these topics within the HRI community, insights from the fields of human computer interaction (HCI), human factors engineering, cognitive engineering and experimental psychology are presented and discussed. Based on the literature, we developed a model of information processing for robotic failures (Robot Failure Human Information Processing, RF-HIP), that guides the discussion of our findings. The model describes the way people perceive, process, and act on failures in human robot interaction. The model includes three main parts: (1) communicating failures, (2) perception and comprehension of failures, and (3) solving failures. Each part contains several stages, all influenced by contextual considerations and mitigation strategies. Several gaps in the literature have become evident as a result of this evaluation. More focus has been given to technical failures than interaction failures. Few studies focused on human errors, on communicating failures, or the cognitive, psychological, and social determinants that impact the design of mitigation strategies. By providing the stages of human information processing, RF-HIP can be used as a tool to promote the development of user-centered failure-handling strategies for HRIs."", 'corpus_id': 49212476, 'score': 1}, {'doc_id': '21678075', 'title': 'Comparing Error-Handling Strategies in Human-Human and Human-Robot Dialogues', 'abstract': 'In this paper error-handling strategies are evaluated as they can be found in human-human and humanrobot dialogues. We compare human-human communication with unequal dialogue partners, such as foreigners, to human-robot communication in order to see how errors are indicated and finally repaired by the dialogue participants in both interaction types. The strategies used in humanhuman communication are ported to human-robot dialogues. The foreigner is taken as an example for a dialogue participant with less grammar and vocabulary coverage and can be seen as similar to the robot in this way. We finally compare the indicators for errors and the error repair strategies in humanhuman vs. human-robot dialogues.', 'corpus_id': 21678075, 'score': 1}, {'doc_id': '231592561', 'title': 'I Can See it in Your Eyes: Gaze towards a Robot as an Implicit Cue of Uncanniness and Task Performance in Long-term Interactions', 'abstract': 'Over the past years, extensive research has been dedicated to developing robust platforms and data-driven dialogue models to support long-term human-robot interactions. However, little is known about how people’s perception of robots and engagement with them develop over time and how these can be accurately assessed through implicit and continuous measurement techniques. In this paper, we investigate this by involving participants in three interaction sessions with multiple days of zero exposure in between. Each session consists of a joint task with a robot as well as two short social chats with it before and after the task. We measure participants’ gaze patterns with a wearable eye-tracker and gauge their perception of the robot and engagement with it and the joint task using questionnaires. Results disclose that aversion of gaze in a social chat is an indicator of a robot’s uncanniness and that the more people gaze at the robot in a joint task, the worse they perform. In contrast with most HRI literature, our results show that gaze towards an object of shared attention, rather than gaze towards a robotic partner, is the most meaningful predictor of engagement in a joint task. Furthermore, the analyses of long-term gaze patterns disclose that people’s mutual gaze in a social chat develops congruently with their perceptions of the robot over time. These are key findings for the HRI community as they entail that gaze behavior can be used as an implicit measure of people’s perception of robots in a social chat and of their engagement and task performance in a', 'corpus_id': 231592561, 'score': 0}, {'doc_id': '226964367', 'title': 'Conversational agents for learning foreign languages - a survey', 'abstract': ""Conversational practice, while crucial for all language learners, can be challenging to get enough of and very expensive. Chatbots are computer programs developed to engage in conversations with humans. They are designed as software avatars with limited, but growing conversational capability. The most natural and potentially powerful application of chatbots is in line with their fundamental nature - language practice. However, their role and outcomes within (in)formal language learning are currently tangential at best. Existing research in the area has generally focused on chatbots' comprehensibility and the motivation they inspire in their users. In this paper, we provide an overview of the chatbots for learning languages, critically analyze existing approaches, and discuss the major challenges for future work."", 'corpus_id': 226964367, 'score': 0}]"
38	Protein stability	5bef84f54cdb7e0d85711772df2acd67	15364	{}	[{'doc_id': '232335411', 'title': 'Water Contribution to the Protein Folding and Its Relevance in Protein Design and Protein Aggregation', 'abstract': '\u200bWater plays a fundamental role in protein stability. However, the effect of the properties of water on the behaviour of proteins is only partially understood. Several theories have been proposed to give insight into the mechanisms of cold and pressure denaturation, or the limits of temperature and pressure above which no protein has a stable, functional state, or how unfolding and aggregation are related. Here we review our results based on a theoretical approach that can rationalise the water contribution to protein solutions’ free energy. We show, using Monte Carlo simulations, how we can rationalise experimental data with our recent results. We discuss how our findings can help develop new strategies for the design of novel synthetic biopolymers or possible approaches for mitigating neurodegenerative pathologies. Giancarlo Franzese (\u200b \u200b) · Joan Àguila Rojas Secció de Física Estadística i Interdisciplinària–Departament de Física de la Matèria Condensada, Facultat de Física, Universitat de Barcelona, Martí i Franquès 1, Barcelona 08028, Spain Institute of Nanoscience and Nanotechnology (IN2UB), Universitat de Barcelona, Martí i Franquès 1, Barcelona 08028, Spain E-mail: gfranzese@ub.edu', 'corpus_id': 232335411, 'score': 0}, {'doc_id': '7085227', 'title': 'A novel approach for analyzing the structure of DNA modified by Benzo[a]pyrene diol epoxide at single-molecule resolution.', 'abstract': 'Benzo[a]pyrene diol epoxide (BPDE) has been shown to bind specifically to the exocyclic amino group of deoxyguanosine in duplex DNA. Interestingly, this metabolite exhibits stereoselectivity in its tumorigenic and mutagenic effects. It is thought that local DNA conformation is altered at the site of the adduct, resulting in aberrant biological processes, and that in certain sequence contexts BPDE-DNA adducts induce bends in the DNA. In the work presented here, we compared DNA structural alterations of BPDE-modified DNA and unmodified DNA via tapping mode atomic force microscopy (AFM). DNA fragments 366 base pairs (bp) in length were generated by PCR from the duplicated multiple-cloning site of pBEND2 inserted into pGEM-3Zf(-), and either mock-modified or treated with BPDE to give modification levels between 1 and 5% of the nucleotides. Control or BPDE-modified DNA was adsorbed to mica and visualized in air by AFM. The contour lengths and end-to-end lengths of individual molecules were measured. The ratio of end-to-end distance to contour length was significantly smaller for modified DNA molecules than for the unmodified DNA preparation, although the frequency distributions of the contour lengths were similar for the two preparations. This suggests BPDE-DNA adducts cause significant bending of DNA molecules, confirming previous conclusions based on more indirect measurements. The average induced bend angle for BPDE-DNA adducts is estimated to be at least 30 degrees.', 'corpus_id': 7085227, 'score': 0}, {'doc_id': '232098349', 'title': 'Macromolecular crowding effects on the kinetics of opposing reactions catalyzed by alcohol dehydrogenase', 'abstract': 'In order to better understand how the complex, densely packed, heterogeneous milieu of a cell influences enzyme kinetics, we exposed opposing reactions catalyzed by yeast alcohol dehydrogenase (YADH) to both synthetic and protein crowders ranging from 10 to 550 kDa. The results reveal that the effects from macromolecular crowding depend on the direction of the reaction. The presence of the synthetic polymers, Ficoll and dextran, decrease Vmax and Km for ethanol oxidation. In contrast, these crowders have little effect or even increase these kinetic parameters for acetaldehyde reduction. This increase in Vmax is likely due to excluded volume effects, which are partially counteracted by viscosity hindering release of the NAD+ product. Macromolecular crowding is further complicated by the presence of a depletion layer in solutions of dextran larger than YADH, which diminishes the hindrance from viscosity. The disparate effects from 25 g/L dextran or glucose compared to 25 g/L Ficoll or sucrose reveals that soft interactions must also be considered. Data from binary mixtures of glucose, dextran, and Ficoll support this “tuning” of opposing factors. While macromolecular crowding was originally proposed to influence proteins mainly through excluded volume effects, this work compliments the growing body of evidence revealing that other factors, such as preferential hydration, chemical interactions, and the presence of a depletion layer also contribute to the overall effect of crowding.', 'corpus_id': 232098349, 'score': 0}, {'doc_id': '8982559', 'title': 'Protein stability: a crystallographer’s perspective', 'abstract': 'An understanding of protein stability is essential for optimizing the expression, purification and crystallization of proteins. In this review, discussion will focus on factors affecting protein stability on a somewhat practical level, particularly from the view of a protein crystallographer.', 'corpus_id': 8982559, 'score': 1}, {'doc_id': '10972802', 'title': 'Protein crowder charge and protein stability.', 'abstract': 'Macromolecular crowding effects arise from steric repulsions and weak, nonspecific, chemical interactions. Steric repulsions stabilize globular proteins, but the effect of chemical interactions depends on their nature. Repulsive interactions such as those between similarly charged species should reinforce the effect of steric repulsions, increasing the equilibrium thermodynamic stability of a test protein. Attractive chemical interactions, on the other hand, counteract the effect of hard-core repulsions, decreasing stability. We tested these ideas by using the anionic proteins from Escherichia coli as crowding agents and assessing the stability of the anionic test protein chymotrypsin inhibitor 2 at pH 7.0. The anionic protein crowders destabilize the test protein despite the similarity of their net charges. Thus, weak, nonspecific, attractive interactions between proteins can overcome the charge-charge repulsion and counterbalance the stabilizing effect of steric repulsion.', 'corpus_id': 10972802, 'score': 1}, {'doc_id': '30412443', 'title': 'Thermal stability of lysozyme as a function of ion concentration: A reappraisal of the relationship between the Hofmeister series and protein stability', 'abstract': 'Anion and cation effects on the structural stability of lysozyme were investigated using differential scanning calorimetry. At low concentrations (<5 mM) anions and cations alter the stability of lysozyme but they do not follow the Hofmeister (or inverse Hofmeister) series. At higher concentrations protein stabilization follows the well‐established Hofmeister series. Our hypothesis is that there are three mechanisms at work. At low concentrations the anions interact with charged side chains where the presence of the ion can alter the structural stability of the protein. At higher concentrations the low charge density anions perchlorate and iodide interact weakly with the protein. Their presence however reduces the Gibbs free energy required to hydrate the core of the protein that is exposed during unfolding therefore destabilizing the structure. At higher concentrations the high charge density anions phosphate and sulfate compete for water with the protein as it unfolds increasing the Gibbs free energy required to hydrate the newly exposed core of the protein therefore stabilizing the structure.', 'corpus_id': 30412443, 'score': 1}, {'doc_id': '232323985', 'title': 'Free-energy changes of bacteriorhodopsin point mutants measured by single-molecule force spectroscopy', 'abstract': 'Significance Membrane proteins are of great biological and biomedical interest; they constitute 30% of encoded proteins and are the targets of ∼50% of drugs. Measurements of membrane protein energetics by mutating individual amino acids provide information that cannot be obtained from structural studies alone. Traditional thermodynamic assays employ chemical denaturants and nonnative micelles that imperfectly reflect the biologically relevant conditions of the native lipid bilayer. Additionally, chemical denaturation is not thermodynamically reversible for many proteins, limiting its scope. Here, we present a different approach that is based on the mechanical unfolding of individual membrane proteins. Our method yields precise energetic characterization of membrane protein point mutants embedded in their native lipid bilayer without introducing confounding denaturant interactions or requiring global reversibility. Single amino acid mutations provide quantitative insight into the energetics that underlie the dynamics and folding of membrane proteins. Chemical denaturation is the most widely used assay and yields the change in unfolding free energy (ΔΔG). It has been applied to >80 different residues of bacteriorhodopsin (bR), a model membrane protein. However, such experiments have several key limitations: 1) a nonnative lipid environment, 2) a denatured state with significant secondary structure, 3) error introduced by extrapolation to zero denaturant, and 4) the requirement of globally reversible refolding. We overcame these limitations by reversibly unfolding local regions of an individual protein with mechanical force using an atomic-force-microscope assay optimized for 2 μs time resolution and 1 pN force stability. In this assay, bR was unfolded from its native bilayer into a well-defined, stretched state. To measure ΔΔG, we introduced two alanine point mutations into an 8-amino-acid region at the C-terminal end of bR’s G helix. For each, we reversibly unfolded and refolded this region hundreds of times while the rest of the protein remained folded. Our single-molecule–derived ΔΔG for mutant L223A (−2.3 ± 0.6 kcal/mol) quantitatively agreed with past chemical denaturation results while our ΔΔG for mutant V217A was 2.2-fold larger (−2.4 ± 0.6 kcal/mol). We attribute the latter result, in part, to contact between Val217 and a natively bound squalene lipid, highlighting the contribution of membrane protein–lipid contacts not present in chemical denaturation assays. More generally, we established a platform for determining ΔΔG for a fully folded membrane protein embedded in its native bilayer.', 'corpus_id': 232323985, 'score': 0}, {'doc_id': '232409599', 'title': 'Quantitative Analysis of Protein Unfolded State Energetics: Experimental and Computational Studies Demonstrate That Non-Native Side-Chain Interactions Stabilize Local Native Backbone Structure.', 'abstract': 'Proteins fold on relatively smooth free energy landscapes which are biased toward the native state, but even simple topologies which fold rapidly can experience roughness on their free energy landscape. The details of these interactions are difficult to elucidate experimentally. Closely related to the problem of deciphering the details of the free energy landscape is the problem of defining the interactions in the denatured state ensemble (DSE) which is populated under native conditions, that is, under conditions where the native state is stable. The DSE of many proteins deviates from random coil models, but quantifying and defining the energetics of the transiently populated interactions in this ensemble is extremely challenging. Characterization of the DSE of proteins which fold to compact structures is also relevant to studies of intrinsically disordered proteins (IDPs) since interactions in the dynamic ensemble populated by IDPs can modulate their behavior. Here we show how experimental thermodynamic and pKa measurements can be combined with computational thermodynamic integration to quantify interactions in the DSE. We show that non-native side chain interactions can stabilize native backbone structure in the DSE and demonstrate that that even rapidly folding proteins can form energetically significant non-native interactions in their DSE. As an example, we characterize a non-native salt bridge that stabilizes local native backbone structure in the DSE of a widely studied fast-folding protein, the villin headpiece helical domain. The combined computational experimental approach is applicable to other protein unfolded states and provides insight that is impossible to obtain with either method alone.', 'corpus_id': 232409599, 'score': 0}, {'doc_id': '23507790', 'title': 'Recent Advances in the Applications of Ionic Liquids in Protein Stability and Activity: A Review', 'abstract': 'Room temperatures ionic liquids are considered as miraculous solvents for biological system. Due to their inimitable properties and large variety of applications, they have been widely used in enzyme catalysis and protein stability and separation. The related information present in the current review is helpful to the researchers working in the field of biotechnology and biochemistry to design or choose an ionic liquid that can serve as a noble and selective solvent for any particular enzymatic reaction, protein preservation and other protein based applications. We have extensively analyzed the methods used for studying the protein–IL interaction which is useful in providing information about structural and conformational dynamics of protein. This can be helpful to develop and understanding about the effect of ionic liquids on stability and activity of proteins. In addition, the affect of physico-chemical properties of ionic liquids, viz. hydrogen bond capacity and hydrophobicity on protein stability are discussed.', 'corpus_id': 23507790, 'score': 1}, {'doc_id': '207555963', 'title': 'Cosolvent effects on protein stability.', 'abstract': 'Proteins are marginally stable, and the folding/unfolding equilibrium of proteins in aqueous solution can easily be altered by the addition of small organic molecules known as cosolvents. Cosolvents that shift the equilibrium toward the unfolded ensemble are termed denaturants, whereas those that favor the folded ensemble are known as protecting osmolytes. Urea is a widely used denaturant in protein folding studies, and the molecular mechanism of its action has been vigorously debated in the literature. Here we review recent experimental as well as computational studies that show an emerging consensus in this problem. Urea has been shown to denature proteins through a direct mechanism, by interacting favorably with the peptide backbone as well as the amino acid side chains. In contrast, the molecular mechanism by which the naturally occurring protecting osmolyte trimethylamine N-oxide (TMAO) stabilizes proteins is not clear. Recent studies have established the strong interaction of TMAO with water. Detailed molecular simulations, when used with force fields that incorporate these interactions, can provide insight into this problem. We present the development of a model for TMAO that is consistent with experimental observations and that provides physical insight into the role of cosolvent-cosolvent interaction in determining its preferential interaction with proteins.', 'corpus_id': 207555963, 'score': 1}]
39	Forecasting	0de29ddbb6fc136b8faab710b343c452	20671	{}	"[{'doc_id': '233181527', 'title': 'Predicting Inflation with Neural Networks', 'abstract': 'This paper applies neural network models to forecast inflation. The use of a particular recurrent neural network, the long-short term memory model, or LSTM, that summarizes macroeconomic information into common components is a major contribution of the paper. Results from an exercise with US data indicate that the estimated neural nets usually present better forecasting performance than standard benchmarks, especially at long horizons. The LSTM in particular is found to outperform the traditional feed-forward network at long horizons, suggesting an advantage of the recurrent model in capturing the long-term trend of inflation. This finding can be rationalized by the so called long memory of the LSTM that incorporates relatively old information in the forecast as long as accuracy is improved, while economizing in the number of estimated parameters. Interestingly, the neural nets containing macroeconomic information capture well the features of inflation during and after the Great Recession, possibly indicating a role for nonlinearities and macro information in this episode. The estimated common components used in the forecast seem able to capture the business cycle dynamics, as well as information on prices. JEL Classification: C45, C53, C55, E31', 'corpus_id': 233181527, 'score': 1}, {'doc_id': '237620913', 'title': 'An LSTM and GRU based trading strategy adapted to the Moroccan market', 'abstract': 'Forecasting stock prices is an extremely challenging job considering the high volatility and the number of variables that influence it (political, economical, social, etc.). Predicting the closing price provides useful information and helps the investor make the right decision. The use of deep learning and more precisely of recurrent neural networks (RNNs) in stock market forecasting is an increasingly common practice in the literature. Long Short Term Memory (LSTM) and Gated Recurrent Unit (GRU) architectures are among the most widely used types of RNNs, given their suitability for sequential data. In this paper, we propose a trading strategy designed for the Moroccan stock market, based on two deep learning models: LSTM and GRU to predict the closing price in the short and medium term respectively. Decision rules for buying and selling stocks are implemented based on the forecasting given by the two models, then over four 3-year periods, we simulate transactions using these decision rules with different settings for each stock. The returns obtained will be used to estimate an expected return. We only hold stocks that outperform a benchmark index (expected return > threshold). The random search is then used to choose one of the available parameters and the performance of the portfolio built from the selected stocks will be tested over a further period. The repetition of this process with a variation of portfolio size makes it possible to select the best possible combination of stock each with the optimized parameter for the decision rules. The proposed strategy produces very promising results and outperforms the performance of indices used as benchmarks in the local market. Indeed, the annualized return of our strategy proposed during the test period is 27.13%, while it is 0.43% for Moroccan all share Indice (MASI), 15.24% for the distributor sector indices, and 19.94% for the pharmaceutical industry indices. Noted that brokerage fees are estimated and subtracted for each transaction. which makes the performance found even more realistic.', 'corpus_id': 237620913, 'score': 0}, {'doc_id': '238025844', 'title': 'Time Series Forecasting in Retail Sales Using LSTM and Prophet', 'abstract': ""Data science highlights fields of study and research such as time series, which, although widely explored in the past, gain new perspectives in the context of this discipline. This chapter presents two approaches to time series forecasting, long short-term memory (LSTM), a special kind of recurrent neural network (RNN), and Prophet, an open-source library developed by Facebook for time series forecasting. With a focus on developing forecasting processes by data mining or machine learning experts, LSTM uses gating mechanisms to deal with long-term dependencies, reducing the short-term memory effect inherent to the traditional RNN. On the other hand, Prophet encapsulates statistical and computational complexity to allow broad use of time series forecasting, prioritizing the expert's business knowledge through exploration and experimentation. Both approaches were applied to a retail time series. This case study comprises daily and half-hourly forecasts, and the performance of both methods was measured using the standard metrics."", 'corpus_id': 238025844, 'score': 1}, {'doc_id': '237988228', 'title': 'Prediction Intervals for Macroeconomic Variables Using LSTM Based LUBE Method', 'abstract': 'This paper considers Long Short Term Memory (LSTM) based Lower Upper Bound Estimation (LUBE) method to generate prediction intervals (PIs) for quantifying uncertainty in macro-economic variables. LSTMs praised for their ability to learn complex sequential information, and to handle the vanishing gradient problem. As a result, they became an essential algorithm to solve time series forecasting problems. We use LSTM units instead of Artificial Neurons in LUBE method to generate PIs and the same loss function as LUBE with GD [1]. This modified architecture of LUBE method with LSTM units outperformed the original LUBE method with Gradient descent (GD) training to generate PIs for three macro-economic variables namely, Consumer Price Index (CPI) headline inflation, CPI Food and Beverages inflation, and CPI Fuel and Light inflation series both in terms of prediction interval average width (PIAW) and coverage probability (PICP). The results are encouraging, and this modified LUBE method can be applied to generate PIs for other financial and non-financial time series.', 'corpus_id': 237988228, 'score': 1}, {'doc_id': '237101710', 'title': 'Application of a hybrid ARIMA-LSTM model based on the SPEI for drought forecasting', 'abstract': 'Drought forecasting can effectively reduce the risk of drought. We proposed a hybrid model based on deep learning methods that integrates an autoregressive integrated moving average (ARIMA) model and a long short-term memory (LSTM) model to improve the accuracy of short-term drought prediction. Taking China as an example, this paper compares and analyzes the prediction accuracy of six drought prediction models, namely, ARIMA, support vector regression (SVR), LSTM, ARIMA-SVR, least square-SVR (LS-SVR), and ARIMA-LSTM, for standardized precipitation evapotranspiration index (SPEI). The performance of all the models was compared using measures of persistence, such as the Nash-Sutcliffe efficiency (NSE). The results show that all three hybrid models (ARIMA-SVR, LS-SVR, and ARIMA-LSTM) had higher prediction accuracy than the single model, for a given lead time, at different scales. The NSEs of the hybrid models for the predicted SPEI1 are 0.043, 0.168, and 0.368, respectively, and the NSEs of SPEI24 is 0.781, 0.543, and 0.93, respectively. This finding indicates that when the lead time remains unchanged, the hybrid model has high prediction accuracy for SPEI on long time scales and low prediction accuracy for SPEI on short time scales, and the prediction accuracy of the model with a 1-month lead time is higher than that of the model with a 2-month lead time. In addition, the ARIMA-LSTM model has the highest prediction accuracy at the 6-, 12-, and 24-month scales, indicating that the model is more suitable for the forecasting of long-term drought in China.', 'corpus_id': 237101710, 'score': 1}, {'doc_id': '237587898', 'title': 'Comparative Analysis of Recurrent Neural Networks in Stock Price Prediction for Different Frequency Domains', 'abstract': 'Investors in the stock market have always been in search of novel and unique techniques so that they can successfully predict stock price movement and make a big profit. However, investors continue to look for improved and new techniques to beat the market instead of old and traditional ones. Therefore, researchers are continuously working to build novel techniques to supply the demand of investors. Different types of recurrent neural networks (RNN) are used in time series analyses, especially in stock price prediction. However, since not all stocks’ prices follow the same trend, a single model cannot be used to predict the movement of all types of stock’s price. Therefore, in this research we conducted a comparative analysis of three commonly used RNNs—simple RNN, Long Short Term Memory (LSTM), and Gated Recurrent Unit (GRU)—and analyzed their efficiency for stocks having different stock trends and various price ranges and for different time frequencies. We considered three companies’ datasets from 30 June 2000 to 21 July 2020. The stocks follow different trends of price movements, with price ranges of $30, $50, and $290 during this period. We also analyzed the performance for one-day, three-day, and five-day time intervals. We compared the performance of RNN, LSTM, and GRU in terms of R2 value, MAE, MAPE, and RMSE metrics. The results show that simple RNN is outperformed by LSTM and GRU because RNN is susceptible to vanishing gradient problems, while the other two models are not. Moreover, GRU produces lesser errors comparing to LSTM. It is also evident from the results that as the time intervals get smaller, the models produce lower errors and higher reliability.', 'corpus_id': 237587898, 'score': 0}, {'doc_id': '237312538', 'title': 'Stock Prediction and Forecasting using Stacked LSTM-Recurrent Neural Network Model', 'abstract': 'Accurately forecasting stock market returns is challenging due to the volatile and non-linear nature of financial capital markets. Artificial intelligence and increased computing power have ushered in a new era in which programmable methods of predicting market prices have proven to be more accurate. A successful prediction of a stock’s future price will result in a substantial profit. In this paper, we have proposed a deep learning-based model to make prediction more reliable and simpler. The paper focuses on the use of Long Short Term Memory algorithm (LSTM), which is an advanced form of Recurrent Neural Network. We checked the accuracy of our model using stacked Long Short Term Memory and forecasted the future close prices of stock data through a backtesting method by using multi-layer LSTM networks. After performing the experiment, we were able to forecast the forthcoming 10 days closing price of the given stock. Keywords— Recurrent Neural Network, Long-Short Term Memory, Deep Learning, Stock Market, Technical Analysis, Prediction, Forecasting.', 'corpus_id': 237312538, 'score': 0}, {'doc_id': '236943289', 'title': 'A Neural Network Ensemble Approach for GDP Forecasting', 'abstract': 'We propose an ensemble learning methodology to forecast the future US GDP growth release. Our approach combines a Recurrent Neural Network (RNN) with a Dynamic Factor model accounting for time-variation in the mean with a Generalized Autoregressive Score (DFM-GAS). We show how this combination improves forecasts in the aftermath of the 2008-09 global financial crisis, as well as in the latest COVID-19 recession, by reducing the root mean squared error for the short-term forecast horizon. Thus, we evaluate the predictive ability of each component of the ensemble by considering variations in the mean, as the latter are potentially caused by recessions affecting the economy. For our scope, we employ a set of predictors encompassing a wide range of variables measured at different time frequencies. Thus, we provide dynamic coefficients for predictors after an interpretable machine learning routine to assess how the model reflects the evolution of the business cycle.', 'corpus_id': 236943289, 'score': 1}, {'doc_id': '86241702', 'title': '$sup 90$Sr uptake in a Chlamydomonas marina strain', 'abstract': 'A marine alga strain (Chlamydomonas) was cultivated for 10 days in from 0.4 to 8 m mu Ci /sup 90/Sr, after which cell density and radioactivity levels were determined. At concentrations of up to 2 m mu Ci/ml, cell population growth was similar to that observed in the controls. As values were increased, there was a gradual change in morphology, color, and transparency, until the capacity for development was entirely lost. There was a linear relationship between culture and cell isotope concentration. The calculated angular coefficient coincided with the concentration factor deduced from the experimental data. This fuctor was constant in the ranged 0.4-2 m mu Ci/ml, after which it lost its validity. (auth)', 'corpus_id': 86241702, 'score': 0}, {'doc_id': '237295991', 'title': 'Dengue Prediction Using Deep Learning With Long Short-Term Memory', 'abstract': 'Dengue is a severe infectious disease on the rise in Malaysia, and there is a demand for artificial intelligence to support the health system. However, the application of deep learning, specifically Long Short-Term Memory (LSTM) time series forecasting, has not been explored by many in dengue prediction studies. However, considering the availability of daily weather data being collected, the ability of LSTM to capture long term dependencies can be leveraged in forecasting dengue cases. Therefore, this study investigates the performance and viability of LSTM time series forecasting on predicting dengue cases. An LSTM model is developed and evaluated to be compared to a Support Vector Regression (SVR) model by utilising the availability of a dengue dataset consisting of weather and climate data. The results indicated LSTM time series forecasting performed better than SVR, with R2 and MAE scoring 0.75 and 8.76. In short, LSTM has shown better performance and, in addition, capturing trends in the rise and fall of dengue cases. Altogether, this research could contribute to the fight against the increase of dengue cases without relying on forecasted weather data but instead, history.', 'corpus_id': 237295991, 'score': 0}]"
40	Human-AI Teams ML	84281ddcc1b299526a2a175d54c3b916	5637	{}	"[{'doc_id': '219260564', 'title': 'Consistent Estimators for Learning to Defer to an Expert', 'abstract': ""Learning algorithms are often used in conjunction with expert decision makers in practical scenarios, however this fact is largely ignored when designing these algorithms. In this paper we explore how to learn predictors that can either predict or choose to defer the decision to a downstream expert. Given only samples of the expert's decisions, we give a procedure based on learning a classifier and a rejector and analyze it theoretically. Our approach is based on a novel reduction to cost sensitive learning where we give a consistent surrogate loss for cost sensitive learning that generalizes the cross entropy loss. We show the effectiveness of our approach on a variety of experimental tasks."", 'corpus_id': 219260564, 'score': 1}, {'doc_id': '216562809', 'title': 'Optimizing AI for Teamwork', 'abstract': ""In many high-stakes domains such as criminal justice, finance, and healthcare, AI systems may recommend actions to a human expert responsible for final decisions, a context known as AI-advised decision making. When AI practitioners deploy the most accurate system in these domains, they implicitly assume that the system will function alone in the world. We argue that the most accurate AI team-mate is not necessarily the em best teammate; for example, predictable performance is worth a slight sacrifice in AI accuracy. So, we propose training AI systems in a human-centered manner and directly optimizing for team performance. We study this proposal for a specific type of human-AI team, where the human overseer chooses to accept the AI recommendation or solve the task themselves. To optimize the team performance we maximize the team's expected utility, expressed in terms of quality of the final decision, cost of verifying, and individual accuracies. Our experiments with linear and non-linear models on real-world, high-stakes datasets show that the improvements in utility while being small and varying across datasets and parameters (such as cost of mistake), are real and consistent with our definition of team utility. We discuss the shortcoming of current optimization approaches beyond well-studied loss functions such as log-loss, and encourage future work on human-centered optimization problems motivated by human-AI collaborations."", 'corpus_id': 216562809, 'score': 1}, {'doc_id': '227334816', 'title': 'Learning Interpretable Concept-Based Models with Human Feedback', 'abstract': ""Machine learning models that first learn a representation of a domain in terms of human-understandable concepts, then use it to make predictions, have been proposed to facilitate interpretation and interaction with models trained on high-dimensional data. However these methods have important limitations: the way they define concepts are not inherently interpretable, and they assume that concept labels either exist for individual instances or can easily be acquired from users. These limitations are particularly acute for high-dimensional tabular features. We propose an approach for learning a set of transparent concept definitions in high-dimensional tabular data that relies on users labeling concept features instead of individual instances. Our method produces concepts that both align with users' intuitive sense of what a concept means, and facilitate prediction of the downstream label by a transparent machine learning model. This ensures that the full model is transparent and intuitive, and as predictive as possible given this constraint. We demonstrate with simulated user feedback on real prediction problems, including one in a clinical domain, that this kind of direct feedback is much more efficient at learning solutions that align with ground truth concept definitions than alternative transparent approaches that rely on labeling instances or other existing interaction mechanisms, while maintaining similar predictive performance."", 'corpus_id': 227334816, 'score': 0}, {'doc_id': '3415066', 'title': 'Predict Responsibly: Increasing Fairness by Learning To Defer', 'abstract': ""Machine learning systems, which are often used for high-stakes decisions, suffer from two mutually reinforcing problems: unfairness and opaqueness. Many popular models, although generally accurate, cannot express uncertainty about their predictions. Even in regimes where a model is inaccurate, users may trust the model's predictions too fully, and allow its biases to reinforce the user's own. \nIn this work, we explore models that learn to defer. In our scheme, a model learns to classify accurately and fairly, but also to defer if necessary, passing judgment to a downstream decision-maker such as a human user. We further propose a learning algorithm which accounts for potential biases held by decision-makers later in a pipeline. Experiments on real-world datasets demonstrate that learning to defer can make a model not only more accurate but also less biased. Even when operated by highly biased users, we show that deferring models can still greatly improve the fairness of the entire pipeline."", 'corpus_id': 3415066, 'score': 1}, {'doc_id': '227344215', 'title': 'Bringing Cognitive Augmentation to Web Browsing Accessibility', 'abstract': 'In this paper we explore the opportunities brought by cognitive augmentation to provide a more natural and accessible web browsing experience. We explore these opportunities through \\textit{conversational web browsing}, an emerging interaction paradigm for the Web that enables blind and visually impaired users (BVIP), as well as regular users, to access the contents and features of websites through conversational agents. Informed by the literature, our previous work and prototyping exercises, we derive a conceptual framework for supporting BVIP conversational web browsing needs, to then focus on the challenges of automatically providing this support, describing our early work and prototype that leverage heuristics that consider structural and content features only.', 'corpus_id': 227344215, 'score': 0}, {'doc_id': '229210866', 'title': 'Learning Prediction Intervals for Model Performance', 'abstract': 'Understanding model performance on unlabeled data is a fundamental challenge of developing, deploying, and maintaining AI systems. Model performance is typically evaluated using test sets or periodic manual quality assessments, both of which require laborious manual data labeling. Automated performance prediction techniques aim to mitigate this burden, but potential inaccuracy and a lack of trust in their predictions has prevented their widespread adoption. We address this core problem of performance prediction uncertainty with a method to compute prediction intervals for model performance. Our methodology uses transfer learning to train an uncertainty model to estimate the uncertainty of model performance predictions. We evaluate our approach across a wide range of drift conditions and show substantial improvement over competitive baselines. We believe this result makes prediction intervals, and performance prediction in general, significantly more practical for real-world use.', 'corpus_id': 229210866, 'score': 0}, {'doc_id': '166386064', 'title': 'MTAS Salary & Fringe Benefit Survey (2003-2004)', 'abstract': 'The MTAS publications provided on this website are archival documents intended for informational purposes only and should not be considered as authoritative. The content contained in these publications may be outdated, and the laws referenced therein may have changed or may not be applicable to your city or circumstances. For current information, please visit the MTAS website at: mtas.tennessee.edu.', 'corpus_id': 166386064, 'score': 0}, {'doc_id': '20407781', 'title': 'Tackling centrosome biology through gene targeting in chicken B cells.', 'abstract': 'The centrosome proteome comprises hundreds of proteins whose function at the organelle and in the cellular context is unknown. Loss-of-function studies present a powerful tool to probe the roles of these individual constituents and hence improve our insight into key questions of centrosome biology such as how centrosomes are built, how they duplicate, and which cellular processes they partake in. In cultured cells ribonucleic acid (RNA) interference remains the most widely used method to achieve protein depletion, but due to the remarkable stability of many centrosome components depletion is often incomplete. In such instances genome editing provides a viable alternative. The exceptionally high homologous recombination rate of chicken DT40 cells makes this lymphocytic cell line ideal for genetic manipulation. Here we describe methods for the design and generation of knockouts and in situ tagging of genes in these cells. Furthermore, we report an optimized technique that allows isolation of centrosomes from DT40 cells for use in in vitro functional assays and proteomic analysis. Gene editing by CRISPR-Cas9 technology is fast replacing RNA interference as a method of choice for loss-of-function studies, but the combination of the fast cell cycle, the robustness in culture and ease of gene targeting, will continue to make DT40 cells a useful model system for studies of vertebrate protein function.', 'corpus_id': 20407781, 'score': 0}, {'doc_id': '8943607', 'title': 'Principles of mixed-initiative user interfaces', 'abstract': 'Recent debate has centered on the relative promise of focusinguser-interface research on developing new metaphors and tools thatenhance users abilities to directly manipulate objects versusdirecting effort toward developing interface agents that provideautomation. In this paper, we review principles that show promisefor allowing engineers to enhance human-computer interactionthrough an elegant coupling of automated services with directmanipulation. Key ideas will be highlighted in terms of the Lookoutsystem for scheduling and meeting management.', 'corpus_id': 8943607, 'score': 1}, {'doc_id': '218486980', 'title': 'Learning to Complement Humans', 'abstract': 'A rising vision for AI in the open world centers on the development of systems that can complement humans for perceptual, diagnostic, and reasoning tasks. To date, systems aimed at complementing the skills of people have employed models trained to be as accurate as possible in isolation. We demonstrate how an end-to-end learning strategy can be harnessed to optimize the combined performance of human-machine teams by considering the distinct abilities of people and machines. The goal is to focus machine learning on problem instances that are difficult for humans, while recognizing instances that are difficult for the machine and seeking human input on them. We demonstrate in two real-world domains (scientific discovery and medical diagnosis) that human-machine teams built via these methods outperform the individual performance of machines and people. We then analyze conditions under which this complementarity is strongest, and which training methods amplify it. Taken together, our work provides the first systematic investigation of how machine learning systems can be trained to complement human reasoning.', 'corpus_id': 218486980, 'score': 1}]"
41	demographic_predictions	f56eab2de96f12e616cc70530d536c97	20715	{}	"[{'doc_id': '2531956', 'title': 'Projection of Populations by Level of Educational Attainment, Age and Sex for 120 Countries for 2005-2050', 'abstract': 'Using demographic multi-state, cohort-component methods, we produce projections for 120 countries (covering 93% of the world population in 2005) by five-year age groups, sex, and four levels of educational attainment for the years 2005-2050. Taking into account differentials in fertility and mortality by education level, we present the first systematic global educational attainment projections according to four widely differing education scenarios. The results show the possible range of future educational attainment trends around the world, thereby contributing to long-term economic and social planning at the national and international levels, and to the assessment of the feasibility of international education goals.', 'corpus_id': 2531956, 'score': 1}, {'doc_id': '13719520', 'title': 'The human core of the shared socioeconomic pathways: Population scenarios by age, sex and level of education for all countries to 2100', 'abstract': 'Highlights • We convert the general SSP storylines into demographic scenarios for 195 countries.• Human populations are cross-classified by age, gender and level of education.• Future fertility and hence population growth will depend on female education.• In the median assumptions scenario (SSP2) world population will peak around 2070.• By 2100 world population ranges from 6.9 (SSP1) to 12.6 billion (SSP3).', 'corpus_id': 13719520, 'score': 1}, {'doc_id': '220516456', 'title': 'Fertility, mortality, migration, and population scenarios for 195 countries and territories from 2017 to 2100: a forecasting analysis for the Global Burden of Disease Study', 'abstract': ""Summary Background Understanding potential patterns in future population levels is crucial for anticipating and planning for changing age structures, resource and health-care needs, and environmental and economic landscapes. Future fertility patterns are a key input to estimation of future population size, but they are surrounded by substantial uncertainty and diverging methodologies of estimation and forecasting, leading to important differences in global population projections. Changing population size and age structure might have profound economic, social, and geopolitical impacts in many countries. In this study, we developed novel methods for forecasting mortality, fertility, migration, and population. We also assessed potential economic and geopolitical effects of future demographic shifts. Methods We modelled future population in reference and alternative scenarios as a function of fertility, migration, and mortality rates. We developed statistical models for completed cohort fertility at age 50 years (CCF50). Completed cohort fertility is much more stable over time than the period measure of the total fertility rate (TFR). We modelled CCF50 as a time-series random walk function of educational attainment and contraceptive met need. Age-specific fertility rates were modelled as a function of CCF50 and covariates. We modelled age-specific mortality to 2100 using underlying mortality, a risk factor scalar, and an autoregressive integrated moving average (ARIMA) model. Net migration was modelled as a function of the Socio-demographic Index, crude population growth rate, and deaths from war and natural disasters; and use of an ARIMA model. The model framework was used to develop a reference scenario and alternative scenarios based on the pace of change in educational attainment and contraceptive met need. We estimated the size of gross domestic product for each country and territory in the reference scenario. Forecast uncertainty intervals (UIs) incorporated uncertainty propagated from past data inputs, model estimation, and forecast data distributions. Findings The global TFR in the reference scenario was forecasted to be 1·66 (95% UI 1·33–2·08) in 2100. In the reference scenario, the global population was projected to peak in 2064 at 9·73 billion (8·84–10·9) people and decline to 8·79 billion (6·83–11·8) in 2100. The reference projections for the five largest countries in 2100 were India (1·09 billion [0·72–1·71], Nigeria (791 million [594–1056]), China (732 million [456–1499]), the USA (336 million [248–456]), and Pakistan (248 million [151–427]). Findings also suggest a shifting age structure in many parts of the world, with 2·37 billion (1·91–2·87) individuals older than 65 years and 1·70 billion (1·11–2·81) individuals younger than 20 years, forecasted globally in 2100. By 2050, 151 countries were forecasted to have a TFR lower than the replacement level (TFR <2·1), and 183 were forecasted to have a TFR lower than replacement by 2100. 23 countries in the reference scenario, including Japan, Thailand, and Spain, were forecasted to have population declines greater than 50% from 2017 to 2100; China's population was forecasted to decline by 48·0% (−6·1 to 68·4). China was forecasted to become the largest economy by 2035 but in the reference scenario, the USA was forecasted to once again become the largest economy in 2098. Our alternative scenarios suggest that meeting the Sustainable Development Goals targets for education and contraceptive met need would result in a global population of 6·29 billion (4·82–8·73) in 2100 and a population of 6·88 billion (5·27–9·51) when assuming 99th percentile rates of change in these drivers. Interpretation Our findings suggest that continued trends in female educational attainment and access to contraception will hasten declines in fertility and slow population growth. A sustained TFR lower than the replacement level in many countries, including China and India, would have economic, social, environmental, and geopolitical consequences. Policy options to adapt to continued low fertility, while sustaining and enhancing female reproductive health, will be crucial in the years to come. Funding Bill & Melinda Gates Foundation."", 'corpus_id': 220516456, 'score': 1}, {'doc_id': '237638223', 'title': 'Time Trends and Predictions of Suicide Mortality for People Aged 70 Years and Over From 1990 to 2030 Based on the Global Burden of Disease Study 2017', 'abstract': 'Background: High suicide rate in the elderly is an important global public health problem but has not received the attention it deserves. This study aimed to examine time trends of suicide mortality for people aged 70 years and over by sex, age, and location from 1990 to 2017, and to provide predictions up to 2030. Methods: Using data from the Global Burden of Disease study 2017, we presented elderly suicide mortality changes and compared the patterns for the elderly with that for all ages. We estimated associations between socio-demographic index (SDI) and suicide mortality rates using a restricted cubic spline smoother, and predicted suicide mortality rates up to 2030. Results: In 2017, 118,813 people aged 70 years and over died from suicide, indicating a mortality rate of 27.5 per 100,000, with the highest rates in Eastern Sub-Saharan Africa, Western Sub-Saharan Africa, and Central Sub-Saharan Africa, and for countries and territories, the highest were in South Korea, Zimbabwe, Lesotho, Mozambique, and Senegal. Between 1990 and 2017, suicide mortality rate for the elderly aged 70 years and over decreased globally (percentage change −29.1%), and the largest decreases occurred in East Asia, Southern Latin America, and Western Europe. Nationally, the largest decrease was found in Chile, followed by Czech Republic, Hungary, Turkey, and Philippines. For most countries, the elderly mortality rate was higher than the age-standardized rate, with the largest percentage differences in China and countries in Sub-Saharan Africa. The elderly suicide mortality rate decreased as SDI increased, except for a slight rebound at mid to high SDI. According to projections, 10 out of 195 countries were expected to meet the SDGs indicator of a third reduction by 2030. Conclusions: Variability in suicide mortality rates for the elderly aged 70 years and over by sex, age, region, country, and SDI can guide preventive policies, but causes of the variability need further study. Comprehensive strategies should be adopted to reduce suicide rates and close the gap to the 2030 SDGs.', 'corpus_id': 237638223, 'score': 0}, {'doc_id': '237403396', 'title': 'Economic impact of tuberculosis mortality in 120 countries and the cost of not achieving the Sustainable Development Goals tuberculosis targets: a full-income analysis', 'abstract': '\n Background\n The tuberculosis targets for the UN Sustainable Development Goals (SDGs) call for a 90% reduction in tuberculosis deaths by 2030, compared with 2015, but meeting this target now seems highly improbable. To assess the economic impact of not meeting the target until 2045, we estimated full-income losses in 120 countries, including those due to excess deaths resulting from COVID-19-related disruptions to tuberculosis services, for the period 2020–50.\n \n Methods\n Annual mortality risk changes at each age in each year from 2020 to 2050 were estimated for 120 countries. This risk change was then converted to full-income risk by calculating a population-level mortality risk change and multiplying it by the value of a statistical life-year in each country and year. As a comparator, we assumed that current rates of tuberculosis continue to decline through the period of analysis. We calculated the full-income losses, and mean life expectancy losses per person, at birth and at age 35 years, under scenarios in which the SDG targets are met in 2030 and in 2045. We defined the cost of inaction as the difference in full-income losses and tuberculosis mortality between these two scenarios.\n \n Findings\n From 2020 to 2050, based on the current annual decrease in tuberculosis deaths of 2%, 31·8 million tuberculosis deaths (95% uncertainty interval 25·2 million–39·5 million) are estimated to occur, corresponding to an economic loss of US$17·5 trillion (14·9 trillion–20·4 trillion). If the SDG tuberculosis mortality target is met in 2030, 23·8 million tuberculosis deaths (18·9 million–29·5 million) and $13·1 trillion (11·2 trillion–15·3 trillion) in economic losses can be avoided. If the target is met in 2045, 18·1 million tuberculosis deaths (14·3 million–22·4 million) and $10·2 trillion (8·7 trillion–11·8 trillion) can be avoided. The cost of inaction of not meeting the SDG tuberculosis mortality target until 2045 (vs 2030) is, therefore, 5·7 million tuberculosis deaths (5·1 million–8·1 million) and $3·0 trillion (2·5 trillion–3·5 trillion) in economic losses. COVID-19-related disruptions add $290·3 billion (260·2 billion–570·1 billion) to this cost.\n \n Interpretation\n Failure to achieve the SDG tuberculosis mortality target by 2030 will lead to profound economic and health losses. The effects of delay will be greatest in sub-Saharan Africa. Affected countries, donor nations, and the private sector should redouble efforts to finance tuberculosis programmes and research because the economic dividend of such strategies is likely to be substantial.\n \n Funding\n None.\n', 'corpus_id': 237403396, 'score': 0}, {'doc_id': '238214742', 'title': 'Projecting populations for major Pacific Island countries with and without COVID-19: pro-active insights for population policy', 'abstract': 'The paper projects aggregate populations of six Pacific Island countries in both pre- and post-COVID19 scenarios using a Cohort Component Method for the period 2020–2060. It uses baseline indicators resembling China and Italy’s experiences and finds that Pacific countries could experience a fatality rate between 5 and 20% due to the pandemic. It also finds that most Pacific Island countries would experience higher fatalities in the older age groups, consistent with what is being witnessed in other countries around the world. The analysis also shows that while the risk escalates for people over 50 years onward in all other sample countries, in Fiji, those in the age range of 60 years or more are at higher risk. The findings also indicate that for all countries, the fatality rate for 80 years and older is about 50%. The population projections show that Fiji will be most impacted, while others will experience around 2% initial population decline. The convergence to baseline is found to be slow (except for Tonga) in most Pacific countries. Consequently, the paper suggests a cautious approach in dealing with the current crisis.', 'corpus_id': 238214742, 'score': 0}, {'doc_id': '237144999', 'title': 'Methods for Small Area Population Forecasts: State-of-the-Art and Research Needs', 'abstract': 'Small area population forecasts are widely used by government and business for a variety of planning, research and policy purposes, and often influence major investment decisions. Yet, the toolbox of small area population forecasting methods and techniques is modest relative to that for national and large subnational regional forecasting. In this paper, we assess the current state of small area population forecasting, and suggest areas for further research. The paper provides a review of the literature on small area population forecasting methods published over the period 2001–2020. The key themes covered by the review are extrapolative and comparative methods, simplified cohort-component methods, model averaging and combining, incorporating socioeconomic variables and spatial relationships, ‘downscaling’ and disaggregation approaches, linking population with housing, estimating and projecting small area component input data, microsimulation, machine learning, and forecast uncertainty. Several avenues for further research are then suggested, including more work on model averaging and combining, developing new forecasting methods for situations which current models cannot handle, quantifying uncertainty, exploring methodologies such as machine learning and spatial statistics, creating user-friendly tools for practitioners, and understanding more about how forecasts are used. Supplementary Information The online version contains supplementary material available at 10.1007/s11113-021-09671-6.', 'corpus_id': 237144999, 'score': 0}, {'doc_id': '157602234', 'title': 'Population predictions for the world’s largest cities in the 21st century', 'abstract': 'We project populations to 2100 for the world’s larger cities. Three socioeconomic scenarios with various levels of sustainability and global cooperation are evaluated, and individual “best fit” projections made for each city using global urbanization forecasts. In 2010, 757 million people resided in the 101 largest cities – 11 per cent of the world’s population. By the end of the century, world population is projected to range from 6.9 billion to 13.1 billion, with 15 per cent to 23 per cent of people residing in the 101 largest cities (1.6 billion to 2.3 billion). The disparate effects of socioeconomic pathways on regional distribution of the world’s 101 largest cities in the 21st century are examined by changes in population rank for 2010, 2025, 2050, 2075 and 2100. Socioeconomic pathways are assessed based on their influence on the world’s largest cities. Two aspects of the projections raise concerns about reliability: the unlikely degree of growth of cities suggested for Africa and the growth of cities in coastal settings (and likely global immigration). Trends and the effect of sustainable development on regional distribution of large cities throughout the 21st century are discussed.', 'corpus_id': 157602234, 'score': 1}, {'doc_id': '43681236', 'title': 'World population stabilization unlikely this century', 'abstract': 'The United Nations (UN) recently released population projections based on data until 2012 and a Bayesian probabilistic methodology. Analysis of these data reveals that, contrary to previous literature, the world population is unlikely to stop growing this century. There is an 80% probability that world population, now 7.2 billion people, will increase to between 9.6 billion and 12.3 billion in 2100. This uncertainty is much smaller than the range from the traditional UN high and low variants. Much of the increase is expected to happen in Africa, in part due to higher fertility rates and a recent slowdown in the pace of fertility decline. Also, the ratio of working-age people to older people is likely to decline substantially in all countries, even those that currently have young populations. The 21st century is unlikely to see the end of global population growth. [Also see Perspective by Smeeding] Global population growth continuing The United Nations released new population projections for all countries in July 2014. Gerland et al. analyzed the data and describe the probabilistic population projections for the entire world as well as individual regions and countries (see the Perspective by Smeeding). World population is likely to continue growing for the rest of the century, with at least a 3.5-fold increase in the population of Africa. Furthermore, the ratio of working-age people to older people is almost certain to decline substantially in all countries, not just currently developed ones. Science, this issue p. 234; see also p. 163', 'corpus_id': 43681236, 'score': 1}, {'doc_id': '237067579', 'title': 'The Human Development Index In Canada: Ranking the Provinces and Territories Internationally, 2000-2015: An Update', 'abstract': 'We develop internationally comparable estimates of the Human Development Index (HDI) for the Canadian provinces and territories over the 2000-2014 period. The HDI is a composite index composed of three dimensions (life expectancy, education and income) measured by four indicators (life expectancy at birth, average years of education, expected years of schooling and GNI per capita). We first replicate the Canadian estimates from the most recent Human Development Report (HDR) using data from Statistics Canada. Next, we generate estimates for the provinces and territories following the same methodology and using the same Canadian data sources. We make these estimates internationally comparable by scaling each province or territory’s estimate to Canada’s in the most recent HDR. This allows the provinces and territories to be ranked in the most recent HDR international rankings for all four component variables as well as the overall HDI. The highest HDI score in 2014 among the provinces and territories belongs to Alberta, which would be fourth in the international rankings, while the lowest ranking region is Nunavut, which would be in 46th place. Overall, our report highlights the diverse human development experiences of Canadians that are concealed by Canada’s overall HDI.', 'corpus_id': 237067579, 'score': 0}]"
42	EEAP	7c0c11a398817c0a06d9c2b46cd6e414	1822	{'EEAP': 'Environmental Effects Assessment Panel'}	"[{'doc_id': '211258887', 'title': 'A knowledge-based model of civilization under climate change', 'abstract': 'Civilization produces knowledge, which acts as the driving force of its development. A macro-model of civilization that accounts for the effect of knowledge production on population, energy consumption and environmental conditions is developed. The model includes dynamic equations for world population, amount of knowledge circulating in civilization, the share of fossil fuels in total energy consumption, atmospheric CO2 concentration, and global mean surface temperature. Energy dissipation in knowledge production and direct loss of knowledge are taken into account. The model is calibrated using historical data for each variable. About 90 scenarios were calculated. It was shown that there are two control parameters - sensitivity of the population to temperature rise and coefficient of knowledge loss - which determine the future of civilization. In the two-dimensional space of these parameters, there is an area of sustainable development and an area of loss of stability. Calculations show that civilization is located just on the critical curve separating these areas, that is, at the edge of stability. A small deviation can ultimately lead either to a steady state of 10+ billion people or to the complete extinction of civilization. There are no intermediate steady states.', 'corpus_id': 211258887, 'score': 1}, {'doc_id': '210712559', 'title': 'Icing believed to be most likely cause of Dec . 27 Bek Air crash', 'abstract': 'NUR-SULTAN – The government approved a new draft Ecology Code during the Dec. 24 government meeting chaired by Prime Minister Askar Mamin. The Ministry of Ecology, Geology and Natural Resources developed the draft considering the experience of the Organisation for Economic Co-operation and Development (OECD) countries and opinions from the Kazakh public and business community. The ministry included many of the more than 2,000 suggestions received during the public discussion of the bill. The code has seven principles. The ‘polluter pays and fixes’ principle will control large company emissions with fines and responsibility for environmental damage. The fines will gradually double, quadruple and eventually, increase eight-fold. The second principle will reduce the number of first category enterprises obliged to undergo environmental impact assessment (EIA) to 2,600. The businesses account for 80 percent of emissions. The existing code, which has been covering almost all enterprises (nearly 19,000), proved itself “ineffective and impractical,” said Minister of Ecology, Geology and Natural Resources Magzum Mirzagaliyev while presenting the draft. Continued on Page A4 Icing believed to be most likely cause of Dec. 27 Bek Air crash', 'corpus_id': 210712559, 'score': 0}, {'doc_id': '211818183', 'title': 'Influence Of Climate Change On The Corn Yield In Ontario And Its Impact On Corn Farms Income At The 2068 Horizon', 'abstract': 'Our study aims at quantifying the impact of climate change on corn farming in Ontario under several warming scenarios at the 2068 horizon. It is articulated around a discrete-time dynamic model of corn farm income with an annual time-step, corresponding to one agricultural cycle from planting to harvest. At each period, we compute the income given the corn yield, which is highly dependent on weather variables. We also provide a reproducible forecast of the yearly distribution of corn yield for 10 cities in Ontario. The price of corn futures at harvest time is taken into account and we fit our model by using 49 years of historical data. We then conduct out-of-sample Monte-Carlo simulations to obtain the farm income forecasts under a given climate change scenario.', 'corpus_id': 211818183, 'score': 1}, {'doc_id': '212633620', 'title': 'Predicting the ecological outcomes of global consumption', 'abstract': 'Mapping pathways to achieving the sustainable development goals requires understanding and predicting how social, economic and political factors impact biodiversity. Trends in demography, economic growth, regional alliances and consumption behaviours can have profound effects on the environment by driving resource use and production. While these distant socio-economic drivers impact species and ecosystems at global scales, for example by driving greenhouse gas emissions and climate change, the most prevalent human impacts on biodiversity manifest through habitat loss and land use change decisions at finer scales. We provide the first integrated ecological-economic analysis pathway capable of supporting both national policy design challenges and global scale assessment of biodiversity risks posed by socio-economic drivers such as population growth, consumption and trade. To achieve this, we provide state-of-the-art integration of economic, land use, and biodiversity modelling, and illustrate its application using two case studies. We evaluate the national-level implications of change in trading conditions under a multi-lateral free trade agreement for the bird biodiversity of Vietnam. We review the implications for land-use and biodiversity under coupled socio-economic (Shared Socioeconomic Pathways) and climate (Resource Concentration Pathways) scenarios for Australia. Our study provides a roadmap for setting up high dimensional integrated analyses foe evaluating global priorities for protecting nature and livelihoods in vulnerable areas with the greatest conflicts for economic, social and environmental opportunities.', 'corpus_id': 212633620, 'score': 0}, {'doc_id': '210714019', 'title': 'Shifting velocity of temperature extremes under climate change', 'abstract': 'Rapid changes in climatic conditions threaten both socioeconomic and ecological systems, as these might not be able to adapt or to migrate at the same pace as that of global warming. In particular, an increase of weather and climate extremes can lead to increased stress on human and natural systems, and a tendency for serious adverse effects. We rely on the EURO-CORDEX simulations and focus on the the screen-level daily mean temperature (T2m). We compare the shifting velocities of the cold and hot extremes with these of the associated central trends, i.e., the arithmetical mean or median. We define the extremes relative to the T2m distribution as it evolves with time over the period of 1951--2100. We find that temperature extremes shift at a similar velocity compared to that of the central trends. Accordingly, the T2m probability distribution shifts mostly as a whole, as the tails of the distribution increase together with the central trends. Exceptions occur however in specific regions and for the clustering of warm days, which shifts slower than all other extremes investigated in this study.', 'corpus_id': 210714019, 'score': 0}, {'doc_id': '212478863', 'title': 'Climate Changes and Human Infectious Diseases', 'abstract': 'Because of being partially attributable to the varying effects of climate change, spatio-temporal scale, and different types of hostpathogen systems, the debate on the potential human health impacts remains polarizing, in spite of a growing progress in determining climate change effects on human infectious diseases. While regions geographically experiencing higher temperature anomalies have been provided more study attention, the Earth’ s most vulnerable regions to climate variability and extreme events unfortunately have been less studied. Agreements on the response of human infectious diseases to climate change tend to converge from local to global scales. Then the number of mechanistic studies are slowly growing, with abundance of findings of rapidly growing of statistical methods, for examples, using seasonal auto-regressive integrated moving average (SARIMA) model with local weather conditions in forecasting hand-foot-mouth disease, using generalized estimating equation models/multivariate random-effects meta-regression analyses in quantifying the city-specific climate change-malaria associations, etc. The authors extracted the published evidence and demonstrated that over the past three years (2016 to early 2018), the negative or uncertain reports on responses of human infectious diseases to climate change has been increasing. Research gaps and trends found in this study should be addressed in the near future, including impact of climate change on respiratory infectious diseases and human infectious diseases other than vector-borne diseases.', 'corpus_id': 212478863, 'score': 1}, {'doc_id': '211259294', 'title': 'Application of ERA5 and MENA simulations to predict offshore wind energy potential', 'abstract': 'This study explores wind energy resources in different locations through the Gulf of Oman and also their future variability due climate change impacts. In this regard, EC-EARTH near surface wind outputs obtained from CORDEX-MENA simulations are used for historical and future projection of the energy. The ERA5 wind data are employed to assess suitability of the climate model. Moreover, the ERA5 wave data over the study area are applied to compute sea surface roughness as an important variable for converting near surface wind speeds to those of wind speed at turbine hub-height. Considering the power distribution, bathymetry and distance from the coats, some spots as tentative energy hotspots to provide detailed assessment of directional and temporal variability and also to investigate climate change impact studies. RCP8.5 as a common climatic scenario is used to project and extract future variation of the energy in the selected sites. The results of this study demonstrate that the selected locations have a suitable potential for wind power turbine plan and constructions.', 'corpus_id': 211259294, 'score': 0}, {'doc_id': '20973046', 'title': 'Population and climate change.', 'abstract': ""To review, the four broad dimensions of any complex human problem, including climate change, are the human population, economics, culture, and environment. These dimensions interact with one another in all directions and on many time-scales. From 2010 to 2050, the human population is likely to grow bigger, more slowly, older, and more urban. It is projected that by 2050 more than 2.6 billion people (almost 94% of global urban growth) will be added to the urban population in today's developing countries. That works out to 1.26 million additional urban people in today's developing countries every week from 2010 to 2050. Humans alter the climate by emitting greenhouse gases, by altering planetary albedo, and by altering atmospheric components. Between 1900 and 2000, humans' emissions of carbon into the atmosphere increased fifteenfold, while the numbers of people increased less than fourfold. Population growth alone, with constant rates of emissions per person, could not account for the increase in the carbon emissions to the atmosphere. The world economy grew sixteenfold in the twentieth century, accompanied by enormous increases in the burning of gas, oil, and coal. In the last quarter of the twentieth century, population grew much faster in developing countries than in high-income countries, and, compared with population growth, the growth of carbon emissions to the atmosphere was even faster in developing countries than in high-income countries. The ratio of emissions-to-population growth rates was 2.8 in developing countries compared with 1.6 in high-income countries. Emissions of CO2 and other greenhouse gases are influenced by the sizes and density of settlements, the sizes of households, and the ages of householders. Between 2010 and 2050, these demographic factors are anticipated to change substantially. Therefore demography will play a substantial role in the dynamics of climate changes. Climate changes affect many aspects of the living environment, including human settlements, food production, and diseases. These changes will affect poor people more severely than rich, and poor nations more severely than rich. Yet not enough is known to predict quantitatively many details that will matter enormously to future people and other species. Three kinds of responses are related to demographic issues that affect climate changes: universal secondary education, voluntary contraception and maternal health services, and smarter urban design and construction. These responses may prevent, reduce, or ameliorate the impacts of climate changes. They are as relevant to rich countries as to poor, though in ways that are as different as are rich countries and poor. They are desirable in their own right because they improve the lives of the people they affect directly; and they are desirable for their beneficial effects on the larger society and globe. They are effective responses to the twin challenges of reducing poverty and reducing greenhouse gas emissions."", 'corpus_id': 20973046, 'score': 1}, {'doc_id': '210911544', 'title': 'Social Cost of Carbon: What Do the Numbers Really Mean?', 'abstract': 'The Social Cost of Carbon (SCC) is estimated by integrated assessment models and is widely used by government agencies to value the climate impacts of rulemakings, however, the core discussion around SCC so far was focused on validity of obtained numerical estimates and related uncertainties while largely neglecting a deeper discussion of the SCC applicability limits stemming from the calculation method. This work provides a conceptual mathematical background and the economic interpretation that is behind the SCC calculation in the three widely used integrated assessment models. Policy makers need to be aware of the difference between the commonly implied and the actual meaning of SCC that substantially limits its applicability in practice. The presented results call for a critical revision of the SCC concept and the SCC calculation methods in integrated assessment models.', 'corpus_id': 210911544, 'score': 0}, {'doc_id': '212657769', 'title': 'A Climate Insidium with a Price on Warming', 'abstract': 'In this paper, I introduce a new emissions trading system (ETS) design to address the problems with existing ETSs and carbon taxes. First, existing ETS designs inhibit emissions but do not constrain warming to any set level. Existing ETSs have the indirect objective of reducing emissions instead of directly reducing warming. Even a global mechanism using an existing ETS cannot guarantee a particular warming path. Part 1: A Price on Warming addresses this. My proposed market trades contracts tied to temperature in a double-sided auction of emissions permits and sequestration contracts. Unlike existing ETSs, the mechanism has a consistent timescale and metric tied to warming, with explicit limits on global temperature in every period into the far future. Every auction finds prices for emissions into the far future. Second, if a jurisdiction does not require firms to manage their emissions, the firms have little incentive to do so. Part 2: A Climate Insidium addresses this. My design incentivizes firms to participate even if their jurisdictions do not join. With sanctions from member jurisdictions and participating firms, the design has bottom-up incentives for joining, and the incentives rise over time under realistic conditions, potentially resulting in a rush to join. Third, existing designs have high transaction costs for implementation, requiring international treaties to begin. Part 3: A Faster Path Forward addresses this. I propose a path without national or international action to begin. A coalition can implement these rules, creating political force to accelerate participation. Full implementation still requires national agreements. This design appears to be closer to ""first best"", with a lower cost of climate mitigation, than any in the literature, while increasing the certainty of avoiding catastrophic global warming. It might also provide a faster pathway to implementation.', 'corpus_id': 212657769, 'score': 0}]"
43	Commonsense QA	8cfa4082f1f7c0c0c9d314ff5f63c6a6	1307	{}	"[{'doc_id': '215238985', 'title': 'Enhancing Review Comprehension with Domain-Specific Commonsense', 'abstract': 'Review comprehension has played an increasingly important role in improving the quality of online services and products and commonsense knowledge can further enhance review comprehension. However, existing general-purpose commonsense knowledge bases lack sufficient coverage and precision to meaningfully improve the comprehension of domain-specific reviews. In this paper, we introduce xSense, an effective system for review comprehension using domain-specific commonsense knowledge bases (xSense KBs). We show that xSense KBs can be constructed inexpensively and present a knowledge distillation method that enables us to use xSense KBs along with BERT to boost the performance of various review comprehension tasks. We evaluate xSense over three review comprehension tasks: aspect extraction, aspect sentiment classification, and question answering. We find that xSense outperforms the state-of-the-art models for the first two tasks and improves the baseline BERT QA model significantly, demonstrating the usefulness of incorporating commonsense into review comprehension pipelines. To facilitate future research and applications, we publicly release three domain-specific knowledge bases and a domain-specific question answering benchmark along with this paper.', 'corpus_id': 215238985, 'score': 0}, {'doc_id': '214611797', 'title': 'Pairwise Multi-Class Document Classification for Semantic Relations between Wikipedia Articles', 'abstract': 'Many digital libraries recommend literature to their users considering the similarity between a query document and their repository. However, they often fail to distinguish what is the relationship that makes two documents alike. In this paper, we model the problem of finding the relationship between two documents as a pairwise document classification task. To find the semantic relation between documents, we apply a series of techniques, such as GloVe, Paragraph Vectors, BERT, and XLNet under different configurations (e.g., sequence length, vector concatenation scheme), including a Siamese architecture for the Transformer-based systems. We perform our experiments on a newly proposed dataset of 32,168 Wikipedia article pairs and Wikidata properties that define the semantic document relations. Our results show vanilla BERT as the best performing system with an F1-score of 0.93, which we manually examine to better understand its applicability to other domains. Our findings suggest that classifying semantic relations between documents is a solvable task and motivates the development of a recommender system based on the evaluated techniques. The discussions in this paper serve as first steps in the exploration of documents through SPARQL-like queries such that one could find documents that are similar in one aspect but dissimilar in another.', 'corpus_id': 214611797, 'score': 0}, {'doc_id': '218487288', 'title': 'Connecting the Dots: A Knowledgeable Path Generator for Commonsense Question Answering', 'abstract': 'Commonsense question answering (QA) requires background knowledge which is not explicitly stated in a given context. Prior works use commonsense knowledge graphs (KGs) to obtain this knowledge for reasoning. However, relying entirely on these KGs may not suffice, considering their limited coverage and the contextual dependence of their knowledge. In this paper, we augment a general commonsense QA framework with a knowledgeable path generator. By extrapolating over existing paths in a KG with a state-of-the-art language model, our generator learns to connect a pair of entities in text with a dynamic, and potentially novel, multi-hop relational path. Such paths can provide structured evidence for solving commonsense questions without fine-tuning the path generator. Experiments on two datasets show the superiority of our method over previous works which fully rely on knowledge from KGs (with up to 6% improvement in accuracy), across various amounts of training data. Further evaluation suggests that the generated paths are typically interpretable, novel, and relevant to the task.', 'corpus_id': 218487288, 'score': 1}, {'doc_id': '211482559', 'title': 'COMMONGEN: Towards Generative Commonsense Reasoning via A Constrained Text Generation Challenge', 'abstract': 'Given a set of common concepts like “{apple (noun), pick (verb), tree (noun)}”, humans find it easy to write a sentence describing a grammatical and logically coherent scenario that covers these concepts, for example: “a boy picks an apple from a tree”. The process of generating these sentences requires humans to use commonsense knowledge. We denote this ability as generative commonsense reasoning. Recent work in commonsense reasoning has focused mainly on discriminating the most plausible scenes from distractors via natural language understanding (NLU) settings such as multi-choice question answering. However, generative commonsense reasoning is a relatively unexplored research area, primarily due to the lack of a specialized benchmark dataset. In this paper, we present a constrained natural language generation (NLG) dataset, named COMMONGEN, to explicitly challenge machines in generative commonsense reasoning. It consists of 30k concept-sets with humanwritten sentences as references. Crowdworkers were also asked to write the rationales (i.e. the commonsense facts) used for generating the sentences in the development and test sets. We conduct experiments on a variety of generation models with both automatic and human evaluation. Experimental results show that there is still a large gap between the current state-of-the-art pre-trained model, UniLM, and human performance. 1', 'corpus_id': 211482559, 'score': 1}, {'doc_id': '215745286', 'title': 'Unsupervised Commonsense Question Answering with Self-Talk', 'abstract': 'Natural language understanding involves reading between the lines with implicit background knowledge. Current systems either rely on pre-trained language models as the sole implicit source of world knowledge, or resort to external knowledge bases (KBs) to incorporate additional relevant knowledge. We propose an unsupervised framework based on \\emph{self-talk} as a novel alternative to multiple-choice commonsense tasks. Inspired by inquiry-based discovery learning (Bruner, 1961), our approach inquires language models with a number of information seeking questions such as ""$\\textit{what is the definition of ...}$"" to discover additional background knowledge. Empirical results demonstrate that the self-talk procedure substantially improves the performance of zero-shot language model baselines on four out of six commonsense benchmarks, and competes with models that obtain knowledge from external KBs. While our approach improves performance on several benchmarks, the self-talk induced knowledge even when leading to correct answers is not always seen as useful by human judges, raising interesting questions about the inner-workings of pre-trained language models for commonsense reasoning.', 'corpus_id': 215745286, 'score': 1}, {'doc_id': '214611567', 'title': 'Fast Cross-domain Data Augmentation through Neural Sentence Editing', 'abstract': 'Data augmentation promises to alleviate data scarcity. This is most important in cases where the initial data is in short supply. This is, for existing methods, also where augmenting is the most difficult, as learning the full data distribution is impossible. For natural language, sentence editing offers a solution - relying on small but meaningful changes to the original ones. Learning which changes are meaningful also requires large amounts of training data. We thus aim to learn this in a source domain where data is abundant and apply it in a different, target domain, where data is scarce - cross-domain augmentation. \nWe create the Edit-transformer, a Transformer-based sentence editor that is significantly faster than the state of the art and also works cross-domain. We argue that, due to its structure, the Edit-transformer is better suited for cross-domain environments than its edit-based predecessors. We show this performance gap on the Yelp-Wikipedia domain pairs. Finally, we show that due to this cross-domain performance advantage, the Edit-transformer leads to meaningful performance gains in several downstream tasks.', 'corpus_id': 214611567, 'score': 0}, {'doc_id': '214611659', 'title': 'E2EET: From Pipeline to End-to-end Entity Typing via Transformer-Based Embeddings', 'abstract': 'Entity Typing (ET) is the process of identifying the semantic types of every entity within a corpus. In contrast to Named Entity Recognition, where each token in a sentence is labelled with zero or one class label, ET involves labelling each entity mention with one or more class labels. Existing entity typing models, which operate at the mention level, are limited by two key factors: they do not make use of recently-proposed context-dependent embeddings, and are trained on fixed context windows. They are therefore sensitive to window size selection and are unable to incorporate the context of the entire document. In light of these drawbacks we propose to incorporate context using transformer-based embeddings for a mention-level model, and an end-to-end model using a Bi-GRU to remove the dependency on window size. An extensive ablative study demonstrates the effectiveness of contextualised embeddings for mention-level models and the competitiveness of our end-to-end model for entity typing.', 'corpus_id': 214611659, 'score': 0}, {'doc_id': '202537311', 'title': 'Commonsense Knowledge + BERT for Level 2 Reading Comprehension Ability Test', 'abstract': 'Commonsense knowledge plays an important role when we read. The performance of BERT on SQuAD dataset shows that the accuracy of BERT can be better than human users. However, it does not mean that computers can surpass the human being in reading comprehension. CommonsenseQA is a large-scale dataset which is designed based on commonsense knowledge. BERT only achieved an accuracy of 55.9% on it. The result shows that computers cannot apply commonsense knowledge like human beings to answer questions. Comprehension Ability Test (CAT) divided the reading comprehension ability at four levels. We can achieve human like comprehension ability level by level. BERT has performed well at level 1 which does not require common knowledge. In this research, we propose a system which aims to allow computers to read articles and answer related questions with commonsense knowledge like a human being for CAT level 2. This system consists of three parts. Firstly, we built a commonsense knowledge graph; and then automatically constructed the commonsense knowledge question dataset according to it. Finally, BERT is combined with the commonsense knowledge to achieve the reading comprehension ability at CAT level 2. Experiments show that it can pass the CAT as long as the required common knowledge is included in the knowledge base.', 'corpus_id': 202537311, 'score': 1}, {'doc_id': '53296520', 'title': 'CommonsenseQA: A Question Answering Challenge Targeting Commonsense Knowledge', 'abstract': 'When answering a question, people often draw upon their rich world knowledge in addition to the particular context. Recent work has focused primarily on answering questions given some relevant document or context, and required very little general background. To investigate question answering with prior knowledge, we present CommonsenseQA: a challenging new dataset for commonsense question answering. To capture common sense beyond associations, we extract from ConceptNet (Speer et al., 2017) multiple target concepts that have the same semantic relation to a single source concept. Crowd-workers are asked to author multiple-choice questions that mention the source concept and discriminate in turn between each of the target concepts. This encourages workers to create questions with complex semantics that often require prior knowledge. We create 12,247 questions through this procedure and demonstrate the difficulty of our task with a large number of strong baselines. Our best baseline is based on BERT-large (Devlin et al., 2018) and obtains 56% accuracy, well below human performance, which is 89%.', 'corpus_id': 53296520, 'score': 1}, {'doc_id': '214612170', 'title': 'Linguistically Driven Graph Capsule Network for Visual Question Reasoning', 'abstract': 'Recently, studies of visual question answering have explored various architectures of end-to-end networks and achieved promising results on both natural and synthetic datasets, which require explicitly compositional reasoning. However, it has been argued that these black-box approaches lack interpretability of results, and thus cannot perform well on generalization tasks due to overfitting the dataset bias. In this work, we aim to combine the benefits of both sides and overcome their limitations to achieve an end-to-end interpretable structural reasoning for general images without the requirement of layout annotations. Inspired by the property of a capsule network that can carve a tree structure inside a regular convolutional neural network (CNN), we propose a hierarchical compositional reasoning model called the ""Linguistically driven Graph Capsule Network"", where the compositional process is guided by the linguistic parse tree. Specifically, we bind each capsule in the lowest layer to bridge the linguistic embedding of a single word in the original question with visual evidence and then route them to the same capsule if they are siblings in the parse tree. This compositional process is achieved by performing inference on a linguistically driven conditional random field (CRF) and is performed across multiple graph capsule layers, which results in a compositional reasoning process inside a CNN. Experiments on the CLEVR dataset, CLEVR compositional generation test, and FigureQA dataset demonstrate the effectiveness and composition generalization ability of our end-to-end model.', 'corpus_id': 214612170, 'score': 0}]"
44	animal wellness	9921d8ac2bf1487ee08f78673f5029a5	16386	{}	"[{'doc_id': '226232135', 'title': 'Predicting dairy cattle heat stress using machine learning techniques.', 'abstract': ""The objectives of the study were to use a heat stress scoring system to evaluate the severity of heat stress on dairy cows using different heat abatement techniques. The scoring system ranged from 1 to 4, where 1 = no heat stress; 2 = mild heat stress; 3 = severe heat stress; and 4 = moribund. The accuracy of the scoring system was then predicted using 3 machine learning techniques: logistic regression, Gaussian naïve Bayes, and random forest. To predict the accuracy of the scoring system, these techniques used factors including temperature-humidity index, respiration rate, lying time, lying bouts, total steps, drooling, open-mouth breathing, panting, location in shade or sprinklers, somatic cell score, reticulorumen temperature, hygiene body condition score, milk yield, and milk fat and protein percent. Three different treatments, namely, portable shade structure, portable polyvinyl chloride pipe sprinkler system, or control with no heat abatement, were considered, where each treatment was replicated 3 times with 3 second-trimester lactating cows. Results indicate that random forest outperformed the other 2 methods, with respect to both accuracy and precision, in predicting the sprinkler group's score. Both logistic regression and random forest were consistent in predicting scores for control, shade, and combined groups. The mean probability of predicting non-heat-stressed cows was highest for cows in the sprinkler group. Finally, the logistic regression method worked best for predicting heat-stressed cows in control, shade, and combined. The insights gained from these results could aid dairy producers to detect heat stress before it becomes severe, which could decrease the negative effects of heat stress, such as milk loss."", 'corpus_id': 226232135, 'score': 1}, {'doc_id': '233744052', 'title': 'Delineation of temperature-humidity index (THI) as indicator of heat stress in riverine buffaloes (Bubalus bubalis) of a sub-tropical Indian region.', 'abstract': 'The erstwhile developed temperature-humidity index (THI) has been popularly used to indicate heat stress in dairy cattle and often in buffaloes. However, scientific literature suggests differences in thermotolerance and physiological responses to heat stress between cattle and buffalo. Therefore, THI range used to indicate degree of heat stress (mild, moderate, and severe) in cattle should be recalibrated for indicating heat stress in buffaloes. The present study was carried out to delineate THI range to indicate onset and severity of heat stress in buffaloes based on physiological, biochemical, and expression profiling of heat shock response (HSR) genes in animals at different THI. The result indicated early onset of heat stress in buffaloes as compared to cattle. Physiological and biochemical parameters indicated onset of mild signs of heat stress in buffaloes at THI 68-69. Significant deviation in these parameters was again observed at THI range 73-76. At THI 77-80, the physiological and biochemical responses of animals were further intensified indicating extreme alteration in homeostasis. The in vivo expression profiling of HSR genes indicated that members of Hsp70 gene family are expressed in a temporal pattern over different THIs, whereas expressions of Hsf genes were evident during intense heat stress. Overall, the study established that amplitude of heat shock response and THI range for indicating severity of thermal stress for buffaloes are not in unison to cattle. The study also suggests skin temperature of the poll region could be used as non-invasive tool for monitoring heat stress in dairy buffaloes.', 'corpus_id': 233744052, 'score': 1}, {'doc_id': '225276626', 'title': 'Thermal imaging combined with predictive machine learning based model for the development of thermal stress level classifiers', 'abstract': 'Abstract Thermal stress in dairy cows has been studied to improve production efficiency and animal welfare. Several authors have verified the potential of infrared thermography (IRT) as noninvasive tool for monitoring the surface temperature of animals. In this study, machine learning-based models for the individual assessment of thermal stress levels in dairy cows (Holstein) were established and evaluated considering both weather and animal factors. An experiment was performed with 26 lactating cows, which were monitored during summer and winter (40 nonconsecutive days in total; three times a day) to acquire the weather and physiological data including the rectal temperature (RT), respiration rate (RR), and body surface temperature (BST), measured by IRT in different body area. The data were analyzed with the Pearson correlation coefficient to determine the ideal body part (front, ocular area, rib, and flank regions) for computational modeling. In addition, a data analysis with linear regression was performed to enhance the parameters choices. The models based on artificial neural networks (ANNs) were developed based on the defined weather and physiological variables. The ANN-based models for predicting the RR (ANN-RR) and RT (ANN-RT) were established with Perceptron, feedforward, and multi-layered architectures. The model responses were used as classifiers for the thermal stress levels (comfort, alert, danger, and emergency). The classification efficiency was assessed with metrics extracted from the confusion matrices (accuracy) and compared with the results of traditional classification methods for thermal stress levels: the Temperature–Humidity Index (THI) and Black Globe-Humidity Index (BGHI). The ANN models provided better predictions of the RR and RT with coefficient of determination (R2) of 0.74 (ANN-RR) and 0.71 (ANN-RT) than the linear regression models. With regard to thermal stress levels, the ANN-based models demonstrated a good predictive ability compared with the BGHI and THI classifications. The best thermal stress accuracy predictions with ANN-RR and ANN-RT were 83% and 84%, respectively; the best accuracies of the BGHI- and THI-based classifiers were 68% and 55%, respectively. In addition, the ANN-based classifier enables an individual assessment of the thermal stress levels of animals.', 'corpus_id': 225276626, 'score': 1}, {'doc_id': '233278750', 'title': 'Spatiotemporal variations on infrared temperature as a thermal comfort indicator for cattle under agroforestry systems.', 'abstract': ""With the expanding use of thermal assessment techniques in beef cattle, infrared thermography has become a promising tool for assessing the environment for animal thermal comfort. Goals of this study were: (1) to evaluate cattle thermal comfort in agroforestry systems with different shade availability (2) to verify the spatiotemporal variations of infrared temperature inside agroforestry systems, and; (3) to test infrared thermography as a potential tool to assess animal thermal comfort indices in agroforestry systems. A trial was carried out between June 2015 and February 2016, covering Central-Brazil's dry winter and rainy summer seasons, respectively. The experimental area of Embrapa Beef Cattle is located in Campo Grande (Mato Grosso do Sul), coordinates 20°24'53″ S, 54°42'26″ W and 558\xa0m altitude. The 12\xa0ha plot has two agroforestry systems varying shade availability. Traditional Black Globe Temperature and Humidity Index, Heat Load Index and Radiation Thermal Load were determined, from measurements using digital thermo-hygrometers, with datalogger. Surface temperature and humidity of tree canopies and pasture were determined using an infrared thermographic camera. Results show spatiotemporal variations in infrared temperature. This means that the environment inside agroforestry systems is not homogeneously comfortable for cattle, and the system with the lowest shade availability has the greatest heat accumulation area. Weak to strong associations were identified between infrared variables and thermal comfort indices (0.08\xa0=\xa0r\xa0≤\xa00.75). Positive relationships were also obtained and equally well explained by the Black Globe Temperature and Humidity Index and Heat Load Index (0.55\xa0=\xa0R2\xa0≤\xa00.94). We conclude that infrared thermography can be used as a tool to assess thermal comfort indices in agroforestry systems and to determine onset of animal thermal stress from environment and heat body accumulation."", 'corpus_id': 233278750, 'score': 1}, {'doc_id': '233424273', 'title': 'Thermoregulatory and Feeding Behavior under Different Management and Heat Stress Conditions in Heifer Water Buffalo (Bubalus bubalis) in the Tropics', 'abstract': 'Simple Summary Silvopastoral systems can modulate the thermoregulatory behavior of buffaloes decreasing the heat stress and improving the animal welfare in the tropics. The objective of this study was to compare the behavior of heifer buffaloes in a silvopastoral systems with Leucahena leucocephala trees and a conventional system without trees under two heat stress condition (intense heat stress and moderate heat stress) in Cuba. The results show that despite intense heat stress conditions, the animals spent more time feeding in the silvopastoral system than in the conventional system. Besides, the silvopastoral system reduced the use of water in the wallowing areas. We conclude that pastures with trees increase fodder offer while improve grazing behavior and animal welfare for buffalo farming in tropical conditions. Abstract In the wake of climate change and global warming, the production systems of water buffaloes (Bubalus bubalis) are receiving increasing attention in the tropics, where the silvopastoral systems can improve animal welfare and production conditions. The objective of this study was to characterize the behavior of heifer buffaloes in a silvopastoral system (SPS) with Leucaena leucocephala (600 trees/ha) and in a conventional system (CVS), under intense heat stress and moderate heat stress in Cuba. We observed nine animals, with an average weight of 167.9 kg at the beginning of the study, during the daylight period, from 6:00 to 18:00 h, at 10 min intervals, for 12 days. Activities recorded were grazing, ingestion of tree leaves, rumination, water intake, walking, lying, standing, sheltering in the shade of trees, and wallowing. Sheltering in the shade of trees and wallowing were collectively considered as thermoregulatory behavior (TB). TB was different in both systems and conditions of heat stress (p < 0.05), with 4.06 in CVS and 3.81 h in SPS in the intense heat stress period, while it was 2.91 and 1.08 h for SPS and CVS, respectively, during the moderate heat stress period. The wallowing activity showed statistically significant differences (p < 0.05) in the intense heat stress season with 1.18 and 2.35 h for SPS and CVS, respectively. Time spent on feeding behavior was highest in the SPS system (p < 0.05). Longer times of thermoregulatory and feeding behavior indicate the importance of trees in animal welfare for this species in tropical conditions, thus supporting avoided deforestation and the replanting of trees in existing production systems and landscapes.', 'corpus_id': 233424273, 'score': 0}, {'doc_id': '233740622', 'title': 'Random Forest Modelling of Milk Yield of Dairy Cows under Heat Stress Conditions', 'abstract': 'Simple Summary Sustainability is a necessary goal for animal-derived products due to the mounting pressure on the livestock sector to meet the growing demand of an increasing population with rising incomes and the need to reduce the exploitation of resources and environmental impact, while safeguarding animal welfare. We found that by considering a precision livestock farming approach to feeding, advanced numerical methods could represent a reliable and viable tool for the evaluation of future productive scenarios of dairy cows in the presence of changing climate conditions. We believe that the model proposed here could help to develop and improve decision support for farmers to increase both milk yield and animal welfare and, on the other hand, to reduce the resources needed, hence increasing sustainability of the dairy sector. Abstract Precision Livestock Farming (PLF) relies on several technological approaches to acquire, in the most efficient way, precise and real-time data concerning production and welfare of individual animals. In this regard, in the dairy sector, PLF devices are being increasingly adopted, automatic milking systems (AMSs) are becoming increasingly widespread, and monitoring systems for animals and environmental conditions are becoming common tools in herd management. As a consequence, a great amount of daily recorded data concerning individual animals are available for the farmers and they could be used effectively for the calibration of numerical models to be used for the prediction of future animal production trends. On the other hand, the machine learning approaches in PLF are nowadays considered an extremely promising solution in the research field of livestock farms and the application of these techniques in the dairy cattle farming would increase sustainability and efficiency of the sector. The study aims to define, train, and test a model developed through machine learning techniques, adopting a Random Forest algorithm, having the main goal to assess the trend in daily milk yield of a single cow in relation to environmental conditions. The model has been calibrated and tested on the data collected on 91 lactating cows of a dairy farm, located in northern Italy, and equipped with an AMS and thermo-hygrometric sensors during the years 2016–2017. In the statistical model, having seven predictor features, the daily milk yield is evaluated as a function of the position of the day in the lactation curve and the indoor barn conditions expressed in terms of daily average of the temperature-humidity index (THI) in the same day and its value in each of the five previous days. In this way, extreme hot conditions inducing heat stress effects can be considered in the yield predictions by the model. The average relative prediction error of the milk yield of each cow is about 18% of daily production, and only 2% of the total milk production.', 'corpus_id': 233740622, 'score': 0}, {'doc_id': '233388460', 'title': 'Recent Advances on Early Detection of Heat Strain in Dairy Cows Using Animal-Based Indicators: A Review', 'abstract': 'Simple Summary In the dairy industry, heat stress and its induced heat strain result in huge economic loss every year. To better manage heat strain in dairy cows, it is more sensible to advance the detection by using more sensitive indicators so that cooling measures can be implemented in time. With the development of sensor technologies and wireless transmission technologies, body surface temperature and respiration rate can be measured automatically through wearable devices. Lots of efforts have been made recently to develop meaningful thresholds on both physiological and environmental sides. However, the existing thresholds should be used carefully considering the differences in experimental conditions and animal information. Further studies are required to evaluate and customize thresholds based on different influencing factors. A comprehensive early detection system for heat strain based on both animal- and environment-based indicators is expected. Abstract In pursuit of precision livestock farming, the real-time measurement for heat strain-related data has been more and more valued. Efforts have been made recently to use more sensitive physiological indicators with the hope to better inform decision-making in heat abatement in dairy farms. To get an insight into the early detection of heat strain in dairy cows, the present review focuses on the recent efforts developing early detection methods of heat strain in dairy cows based on body temperatures and respiratory dynamics. For every candidate animal-based indicator, state-of-the-art measurement methods and existing thresholds were summarized. Body surface temperature and respiration rate were concluded to be the best early indicators of heat strain due to their high feasibility of measurement and sensitivity to heat stress. Future studies should customize heat strain thresholds according to different internal and external factors that have an impact on the sensitivity to heat stress. Wearable devices are most promising to achieve real-time measurement in practical dairy farms. Combined with internet of things technologies, a comprehensive strategy based on both animal- and environment-based indicators is expected to increase the precision of early detection of heat strain in dairy cows.', 'corpus_id': 233388460, 'score': 1}, {'doc_id': '233668437', 'title': 'Heat stress effects on somatic cell score of Holstein cattle in tropical environment', 'abstract': 'ABSTRACT Considering the importance of dairy farming and negative effects of heat stress, the objective of this study was to investigate the effect of heat stress via temperature-humidity index (THI) and diurnal temperature variation (DTV) for somatic cell score (SCS) of Holstein dairy cattle, using random regression models. Data were a total of 52,012 test-day records for SCS of 9,765 first parity Holstein cows from Brazil, collected from 1997 to 2013, along with weather records (THI and DTV) from 18 weather stations. Least square linear regression models were used to determine THI and DTV thresholds for SCS increase caused by heat stress. In addition to the standard model (SM; without bioclimatic variables), THI and DTV were combined in various ways and tested for different days, totaling 21 models. Thresholds of THI and DTV for SCS increase was 70 (0.09 score unit/THI) and 9 (0.03 score unit/DTV), respectively. The model that included THI and DTV as fixed effects, considering the two days average, presented better fit (AIC, BIC and -2logL). Estimated breeding values (EBVs) and reliability of EBVs improved when using this model. Changes on SCS may be an early indicator of heat stress in Holstein cattle reared in tropical conditions. The sires are re-ranked when bioclimatic variables are included in the model. More significant reclassifications of sires were observed when we increased selection pressure. This study provides strong evidence of a genotype by environment interaction on SCS. Genetic evaluation using average of two days of THI and DTV as fixed effects, improves EBVs and reliability of EBVs.', 'corpus_id': 233668437, 'score': 0}, {'doc_id': '233282236', 'title': 'Effects of heat stress on milk yield of primiparous Holstein cows at regional scale using large data bases', 'abstract': 'With the prospects of global warming, heat stress, the depressive (summer) heat effect on milk yield, has become a high priority research problem in temperate zones. The effect of summer present day heat and lag heat effects on milk yield of first lactation grazing Holstein cows was assessed through the temperature and humidity index (THI). Additionally, THI thresholds were calculated. Daily air temperature and humidity data from three locations for six summer seasons (DecemberMarch in years 2001 – 2006) were used. Data of 35500 monthly test days from 8875 cows in 54 farms within the influence zones of the respective meteorological stations were analyzed. Mixed linear models were adjusted, considering the animal as random effect and location, farm, days in milking, age at calving, year of calving and THI as fixed effects. Four measures per animal were taken into account and modelled as repeated measures. A significant depressing heat effect on milk yield was found for the present day (THI) and also for one-day and two-days before (THI1 and THI2). Significant interactions between THI and days in milk, farm and year were found. The lag heat effects explained more variability on milk yield than the heat effect for the present day. Threshold THI-values were different depending on the considered day: 75, 75 and 72 were estimated for THI, THI1 and THI2, respectively. Heat stress caused a decrease in milk yield of 1.3%, 1.9%, and 0.9% of average daily production (per THI unit increase above threshold), depending on the THImeasure used.', 'corpus_id': 233282236, 'score': 0}, {'doc_id': '233419647', 'title': 'The Effect of Heat Stress on Milk Yield, Milk Fat Rate and Rectal Temperature in Holstein-Friesian Dairy Cattle', 'abstract': ""Heat stress is an environmental factor that negatively affects the morphological and physiological properties of dairy cattle. The aim of this study is to investigate the relationship between heat stress and milk yield, milk fat ratio and body temperature in Holstein-Friesian dairy cattle. Data Southeastern Anatolia Region of Turkey's Siirt province was obtained from a special Kurtalan Farm. Milk yield and content of 13 head Holstein-Friesian dairy cattle were recorded in March, April, May, June, July, August and September. In addition, temperature and humidity records were recorded in the farm and in the parlor to be used for calculating the temperature humidity index value. In the analysis of data, correlation and regression methods were used in SAS 9.4 program. As a result, positive correlation (P <0.001), milk yield and milk fat ratio and negative correlation (P <0.05, P <0.01) were detected between heat stress and body temperature. In addition, a significant negative relationship was observed between rectal temperature and milk yield (P <0.01)."", 'corpus_id': 233419647, 'score': 0}]"
45	Automatic fraud	b6e79ae51c60ee3e96a01d865d7eb046	4658	{}	"[{'doc_id': '211678046', 'title': 'Estimating a Null Model of Scientific Image Reuse to Support Research Integrity Investigations', 'abstract': 'When there is a suspicious figure reuse case in science, research integrity investigators often find it difficult to rebut authors claiming that ""it happened by chance"". In other words, when there is a ""collision"" of image features, it is difficult to justify whether it appears rarely or not. In this article, we provide a method to predict the rarity of an image feature by statistically estimating the chance of it randomly occurring across all scientific imagery. Our method is based on high-dimensional density estimation of ORB features using 7+ million images in the PubMed Open Access Subset dataset. We show that this method can lead to meaningful feedback during research integrity investigations by providing a null hypothesis for scientific image reuse and thus a p-value during deliberations. We apply the model to a sample of increasingly complex imagery and confirm that it produces decreasingly smaller p-values as expected. We discuss applications to research integrity investigations as well as future work.', 'corpus_id': 211678046, 'score': 1}, {'doc_id': '40418706', 'title': '[Mechanism of oxymatrine in preventing hepatic fibrosis formation in patients with chronic hepatitis B].', 'abstract': 'OBJECTIVE\nTo explore the mechanism of oxymatrine in preventing hepatic fibrosis formation in patients with chronic hepatitis B (CHB).\n\n\nMETHODS\nA total of 80 CHB patients receiving routine therapies for liver protection and support were divided into two groups. Oxymatrine at the daily dose of 150 mg was injected intravenously in the therapeutic group (n=40), and gluthion (1.2 g daily) was injected in the control group (n=40) for 8 weeks. The liver functions, indexes of hepatic fibrosis and the levels of transforming growth factor-beta1 (TGF-beta1), tumor necrosis factor-alpha (TNF-alpha) and interleukin-10 (IL-10) were measured in these patients before and after the therapy.\n\n\nRESULTS\nLiver functions was obviously improved after therapy in both groups, showing no significant difference between them (P>0.05). The indexes of hepatic fibrosis such as HA, LN, PCIII and C-IV were significantly lower in the therapeutic group than in the control group (P<0.01). The serum levels of TGF-beta1 and TNF-alpha decreased while IL-10 increased significantly after the treatment in the therapeutic group (P<0.05).\n\n\nCONCLUSION\nThe effect of oxymatrine against hepatic fibrosis is mediated by lowering the levels of TGF-beta1 and TNF-alpha and increasing the level of IL-10 in CHB patients.', 'corpus_id': 40418706, 'score': 0}, {'doc_id': '90208192', 'title': 'Bioscience-scale automated detection of figure element reuse', 'abstract': 'Scientists reuse figure elements sometimes appropriately, e.g. when comparing methods, and sometimes inappropriately, e.g. when presenting an old experiment as a new control. To understand such reuse, automatically detecting it would be important. Here we present an analysis of figure element reuse on a large dataset comprising 760 thousand open access articles and 2 million figures. Our algorithm detects figure region reuse, while being robust to rotation, cropping, resizing, and contrast changes, and estimates which of the reuses have biological meaning. Then a three-person panel analyzes how problematic these biological reuses are using contextual information such as captions and full texts. Based on the panel reviews, we estimate that 9% of the biological reuses would be unanimously perceived as at least suspicious. We further estimate that 0.6% of all articles would be unanimously perceived as fraudulent, with inappropriate reuses occurring 43% across articles, 28% within article, and 29% within a figure. Our tool rapidly detects image reuse at scale, promising to be useful to a broad range of people that campaign for scientific integrity. We suggest that a great deal of scientific fraud will be, sooner or later, detectable by automatic methods.', 'corpus_id': 90208192, 'score': 1}, {'doc_id': '218486858', 'title': 'Code replicability in computer graphics', 'abstract': 'Being able to duplicate published research results is an important process of conducting research whether to build upon these findings or to compare with them. This process is called ""replicability"" when using the original authors\' artifacts (e.g., code), or ""reproducibility"" otherwise (e.g., re-implementing algorithms). Reproducibility and replicability of research results have gained a lot of interest recently with assessment studies being led in various fields, and they are often seen as a trigger for better result diffusion and transparency. In this work, we assess replicability in Computer Graphics, by evaluating whether the code is available and whether it works properly. As a proxy for this field we compiled, ran and analyzed 151 codes out of 374 papers from 2014, 2016 and 2018 SIGGRAPH conferences. This analysis shows a clear increase in the number of papers with available and operational research codes with a dependency on the subfields, and indicates a correlation between code replicability and citation count. We further provide an interactive tool to explore our results and evaluation data.', 'corpus_id': 218486858, 'score': 0}, {'doc_id': '3423810', 'title': 'Testing Hypotheses on Risk Factors for Scientific Misconduct via Matched-Control Analysis of Papers Containing Problematic Image Duplications', 'abstract': 'It is commonly hypothesized that scientists are more likely to engage in data falsification and fabrication when they are subject to pressures to publish, when they are not restrained by forms of social control, when they work in countries lacking policies to tackle scientific misconduct, and when they are male. Evidence to test these hypotheses, however, is inconclusive due to the difficulties of obtaining unbiased data. Here we report a pre-registered test of these four hypotheses, conducted on papers that were identified in a previous study as containing problematic image duplications through a systematic screening of the journal PLoS ONE. Image duplications were classified into three categories based on their complexity, with category 1 being most likely to reflect unintentional error and category 3 being most likely to reflect intentional fabrication. We tested multiple parameters connected to the hypotheses above with a matched-control paradigm, by collecting two controls for each paper containing duplications. Category 1 duplications were mostly not associated with any of the parameters tested, as was predicted based on the assumption that these duplications were mostly not due to misconduct. Categories 2 and 3, however, exhibited numerous statistically significant associations. Results of univariable and multivariable analyses support the hypotheses that academic culture, peer control, cash-based publication incentives and national misconduct policies might affect scientific integrity. No clear support was found for the “pressures to publish” hypothesis. Female authors were found to be equally likely to publish duplicated images compared to males. Country-level parameters generally exhibited stronger effects than individual-level parameters, because developing countries were significantly more likely to produce problematic image duplications. This suggests that promoting good research practices in all countries should be a priority for the international research integrity agenda.', 'corpus_id': 3423810, 'score': 1}, {'doc_id': '31331482', 'title': 'Image Manipulation as Research Misconduct', 'abstract': 'A growing number of research misconduct cases handled by the Office of Research Integrity involve image manipulations. Manipulations may include simple image enhancements, misrepresenting an image as something different from what it is, and altering specific features of an image. Through a study of specific cases, the misconduct findings associated with image manipulation, detection methods and those likely to identify such manipulations, are discussed. This article explores sanctions imposed against guilty researchers and the factors that resulted in no misconduct finding although relevant images clearly were flawed. Although new detection tools are available for universities and journals to detect questionable images, this article explores why these tools have not been embraced.', 'corpus_id': 31331482, 'score': 1}, {'doc_id': '211258765', 'title': 'Citations Systematically Misrepresent the Quality and Impact of Research Articles: Survey and Experimental Evidence from Thousands of Citers', 'abstract': ""Citations are ubiquitous in evaluating research, but how exactly they relate to what they are thought to measure (quality and intellectual impact) is unclear. We investigate the relationships between citations, quality, and impact using a survey with an embedded experiment in which 12,670 authors in 15 academic fields describe about 25K specific referencing decisions. Results suggest that citation counts, when equated with quality and impact, are biased in opposite directions. First, experimentally exposing papers' actual citation counts during the survey causes respondents to perceive all but the top 10% cited papers as of lower quality. Because perceptions of quality are a key factor in citing decisions, citation counts are likely to endogenously cause more citing of top papers and equating them with quality overestimates the actual quality of those papers. Conversely, 54% of references had either zero or minor influence on authors who cite them, but references to highly cited papers were about 200% more likely to denote substantial impact. Equating citations with impact thus underestimates the impact of highly cited papers. Real citation practices thus reveal that citations are biased measures of quality and impact."", 'corpus_id': 211258765, 'score': 0}, {'doc_id': '223211397', 'title': '也论银行、股票市场与经济增长——基于1992～2012年中国数据的实证分析', 'abstract': '本文对银行、股票市场与经济增长三者之间的关系进行了再检验，通过建立联立方程并采用GMM方法对1992-2012年的季度数据进行系统、动态的实证研究，得出与高辉（2011）研究明显不同的结论：金融深化对经济增长存在u型的非线性关系，股票市场对经济增长有显著的负向作用；股票市场与银行的发展不存在挤出效应；本文也使用robust以及方差协方差矩阵等方法进行了验证，结果都显示GMM估计具有稳健性。', 'corpus_id': 223211397, 'score': 0}, {'doc_id': '51711439', 'title': 'Analysis and Correction of Inappropriate Image Duplication: the Molecular and Cellular Biology Experience', 'abstract': 'We analyzed 960 papers published in Molecular and Cellular Biology (MCB) from 2009 to 2016 and found 59 (6.1%) to contain inappropriately duplicated images. The 59 instances of inappropriate image duplication led to 41 corrections, 5 retractions, and 13 instances in which no action was taken. ABSTRACT We analyzed 960 papers published in Molecular and Cellular Biology (MCB) from 2009 to 2016 and found 59 (6.1%) to contain inappropriately duplicated images. The 59 instances of inappropriate image duplication led to 41 corrections, 5 retractions, and 13 instances in which no action was taken. Our experience suggests that the majority of inappropriate image duplications result from errors during figure preparation that can be remedied by correction. Nevertheless, ∼10% of papers with inappropriate image duplications in MCB were retracted (∼0.5% of total). If this proportion is representative, then as many as 35,000 papers in the literature are candidates for retraction due to inappropriate image duplication. The resolution of inappropriate image duplication concerns after publication required an average of 6 h of journal staff time per published paper. MCB instituted a pilot program to screen images of accepted papers prior to publication that identified 12 manuscripts (14.5% out of 83) with image concerns in 2 months. The screening and correction of papers before publication required an average of 30 min of staff time per problematic paper. Image screening can identify papers with problematic images prior to publication, reduces postpublication problems, and requires less staff time than the correction of problems after publication.', 'corpus_id': 51711439, 'score': 1}, {'doc_id': '215754144', 'title': 'Towards Robust Classification with Image Quality Assessment', 'abstract': 'Recent studies have shown that deep convolutional neural networks (DCNN) are vulnerable to adversarial examples and sensitive to perceptual quality as well as the acquisition condition of images. These findings raise a big concern for the adoption of DCNN-based applications for critical tasks. In the literature, various defense strategies have been introduced to increase the robustness of DCNN, including re-training an entire model with benign noise injection, adversarial examples, or adding extra layers. In this paper, we investigate the connection between adversarial manipulation and image quality, subsequently propose a protective mechanism that doesnt require re-training a DCNN. Our method combines image quality assessment with knowledge distillation to detect input images that would trigger a DCCN to produce egregiously wrong results. Using the ResNet model trained on ImageNet as an example, we demonstrate that the detector can effectively identify poor quality and adversarial images.', 'corpus_id': 215754144, 'score': 0}]"
46	Calibration	f6147c9c9b908be9b52803698be57ad2	19320	{}	[{'doc_id': '236772324', 'title': 'Soft Calibration Objectives for Neural Networks', 'abstract': 'Optimal decision making requires that classifiers produce uncertainty estimates consistent with their empirical accuracy. However, deep neural networks are often underor over-confident in their predictions. Consequently, methods have been developed to improve the calibration of their predictive uncertainty both during training and post-hoc. In this work, we propose differentiable losses to improve calibration based on a soft (continuous) version of the binning operation underlying popular calibration-error estimators. When incorporated into training, these soft calibration losses achieve state-of-the-art single-model ECE across multiple datasets with less than 1% decrease in accuracy. For instance, we observe an 82% reduction in ECE (70% relative to the post-hoc rescaled ECE) in exchange for a 0.7% relative decrease in accuracy relative to the cross entropy baseline on CIFAR-100. When incorporated post-training, the soft-binning-based calibration error objective improves upon temperature scaling, a popular recalibration method. Overall, experiments across losses and datasets demonstrate that using calibrationsensitive procedures yield better uncertainty estimates under dataset shift than the standard practice of using a cross entropy loss and post-hoc recalibration methods.2', 'corpus_id': 236772324, 'score': 1}, {'doc_id': '636054', 'title': 'Binary Classifier Calibration Using an Ensemble of Near Isotonic Regression Models', 'abstract': 'Learning accurate probabilistic models from data is crucial in many practical tasks in data mining. In this paper we present a new non-parametric calibration method called Ensemble of Near Isotonic Regression (ENIR). The method can be considered as an extension of BBQ, a recently proposed calibration method, as well as the commonly used calibration method based on isotonic regression (IsoRegC). ENIR is designed to address the key limitation of IsoRegC which is the monotonicity assumption of the predictions. Similar to BBQ, the method post-processes the output of a binary classifier to obtain calibrated probabilities. Thus it can be used with many existing classification models to generate accurate probabilistic predictions. We demonstrate the performance of ENIR on synthetic and real datasets for commonly applied binary classification models. Experimental results show that the method outperforms several common binary classifier calibration methods. In particular on the real data, ENIR commonly performs statistically significantly better than the other methods, and never worse. It is able to improve the calibration power of classifiers, while retaining their discrimination power. The method is also computationally tractable for large scale datasets, as it is O(N log N) time, where N is the number of samples.', 'corpus_id': 636054, 'score': 1}, {'doc_id': '237563214', 'title': 'Improving Regression Uncertainty Estimation Under Statistical Change', 'abstract': 'While deep neural networks are highly performant and successful in a wide range of real-world problems, estimating their predictive uncertainty remains a challenging task. To address this challenge, we propose and implement a loss function for regression uncertainty estimation based on the Bayesian Validation Metric (BVM) framework while using ensemble learning. A series of experiments on indistribution data show that the proposed method is competitive with existing state-of-the-art methods. In addition, experiments on out-of-distribution data show that the proposed method is robust to statistical change and exhibits superior predictive capability.', 'corpus_id': 237563214, 'score': 0}, {'doc_id': '236087245', 'title': 'Top-label calibration', 'abstract': 'We study the problem of post-hoc calibration for multiclass classification, with an emphasis on histogram binning. Multiple works have focused on calibration with respect to the confidence of just the predicted class (or ‘top-label’). We find that the popular notion of confidence calibration [Guo et al., 2017] is not sufficiently strong—there exist predictors that are not calibrated in any meaningful way but are perfectly confidence calibrated. We propose a closely related (but subtly different) notion, top-label calibration, that accurately captures the intuition and simplicity of confidence calibration, but addresses its drawbacks. We formalize a histogram binning (HB) algorithm that reduces top-label multiclass calibration to the binary case, prove that it has clean theoretical guarantees without distributional assumptions, and perform a methodical study of its practical performance. Some prediction tasks require stricter notions of multiclass calibration such as class-wise or canonical calibration. We formalize appropriate HB algorithms corresponding to each of these goals. In experiments with deep neural nets, we find that our principled versions of HB are often better than temperature scaling, for both top-label and class-wise calibration. Code for this work will be made publicly available at https://github.com/aigen/df-posthoc-calibration.', 'corpus_id': 236087245, 'score': 1}, {'doc_id': '237353103', 'title': 'Noisy Labels for Weakly Supervised Gamma Hadron Classification', 'abstract': 'Gamma hadron classification, a central machine learning task in gamma ray astronomy, is conventionally tackled with supervised learning. However, the supervised approach requires annotated training data to be produced in sophisticated and costly simulations. We propose to instead solve gamma hadron classification with a noisy label approach that only uses unlabeled data recorded by the real telescope. To this end, we employ the significance of detection as a learning criterion which addresses this form of weak supervision. We show that models which are based on the significance of detection deliver state-of-the-art results, despite being exclusively trained with noisy labels; put differently, our models do not require the costly simulated ground-truth labels that astronomers otherwise employ for classifier training. Our weakly supervised models exhibit competitive performances also on imbalanced data sets that stem from a variety of other application domains. In contrast to existing work on class-conditional label noise, we assume that only one of the class-wise noise rates is known.', 'corpus_id': 237353103, 'score': 0}, {'doc_id': '237581148', 'title': 'Uncertainty Toolbox: an Open-Source Library for Assessing, Visualizing, and Improving Uncertainty Quantification', 'abstract': 'As machine learning (ML) systems are increasingly deployed on an array of high-stakes tasks, there is a growing need to robustly quantify their predictive uncertainties. Uncertainty quantification (UQ) in machine learning generally refers to the task of quantifying the confidence of a given prediction, and this measure of confidence can be especially crucial in a variety of downstream applications, including Bayesian optimization (Jones et al., 1998; Shahriari et al., 2015), model-based reinforcement learning (Malik et al., 2019; Yu et al., 2020), and in high-stakes prediction settings where errors incur large costs (Wexler, 2017; Rudin, 2019).', 'corpus_id': 237581148, 'score': 0}, {'doc_id': '237489696', 'title': 'Adaptation of CNN Classifiers to Prior Shift', 'abstract': 'In many classification tasks, the test set’s relative class frequencies (class priors probabilities) differ from the relative class frequencies at training time. Such a phenomenon, called label shift or prior shift, can negatively affect the classifier’s performance. Considering a probabilistic classifier approximating posterior probabilities, the predictions can be adapted to the label shift by re-weighting with a ratio of the test set and training set priors. Labels in the test set are usually unknown, therefore the prior ratio has to be estimated in an unsupervised manner. This thesis reviews existing methods for adapting probabilistic classifiers to label shift and for estimating test priors in an unlabeled test set. Moreover, we propose novel algorithms to address the problems of estimating new priors and prior ratio. The methods are designed to handle a known issue in confusion matrix-based methods, where inconsistent estimates of decision probabilities and confusion matrices lead to negative values in estimated priors. Experimental evaluation shows that our method improves the stability of prior estimation and the adapted classifier’s accuracy compared to the baseline confusion matrix-based methods and achieves state-of-the-art performance in prior shift adaptation.', 'corpus_id': 237489696, 'score': 0}, {'doc_id': '211259368', 'title': 'Better Classifier Calibration for Small Datasets', 'abstract': 'Classifier calibration does not always go hand in hand with the classifier’s ability to separate the classes. There are applications where good classifier calibration, i.e., the ability to produce accurate probability estimates, is more important than class separation. When the amount of data for training is limited, the traditional approach to improve calibration starts to crumble. In this article, we show how generating more data for calibration is able to improve calibration algorithm performance in many cases where a classifier is not naturally producing well-calibrated outputs and the traditional approach fails. The proposed approach adds computational cost but considering that the main use case is with small datasets this extra computational cost stays insignificant and is comparable to other methods in prediction time. From the tested classifiers, the largest improvement was detected with the random forest and naive Bayes classifiers. Therefore, the proposed approach can be recommended at least for those classifiers when the amount of data available for training is limited and good calibration is essential.', 'corpus_id': 211259368, 'score': 1}, {'doc_id': '4590722', 'title': 'Binary classifier calibration using an ensemble of piecewise linear regression models', 'abstract': 'In this paper, we present a new nonparametric calibration method called ensemble of near-isotonic regression (ENIR). The method can be considered as an extension of BBQ (Naeini et al., in: Proceedings of twenty-ninth AAAI conference on artificial intelligence, 2015b), a recently proposed calibration method, as well as the commonly used calibration method based on isotonic regression (IsoRegC) (Zadrozny and Elkan, in: Proceedings of the ACM SIGKDD international conference on knowledge discovery and data mining 2002). ENIR is designed to address the key limitation of IsoRegC which is the monotonicity assumption of the predictions. Similar to BBQ, the method post-processes the output of a binary classifier to obtain calibrated probabilities. Thus, it can be used with many existing classification models to generate accurate probabilistic predictions. We demonstrate the performance of ENIR on synthetic and real datasets for commonly applied binary classification models. Experimental results show that the method outperforms several common binary classifier calibration methods. In particular, on the real data, we evaluated ENIR commonly performs statistically significantly better than the other methods, and never worse. It is able to improve the calibration power of classifiers, while retaining their discrimination power. The method is also computationally tractable for large-scale datasets, as it is $$O(N \\log N)$$O(NlogN) time, where N is the number of samples.', 'corpus_id': 4590722, 'score': 1}, {'doc_id': '237593007', 'title': 'Quantifying Model Predictive Uncertainty with Perturbation Theory', 'abstract': 'We propose a framework for predictive uncertainty quantification of a neural network that replaces the conventional Bayesian notion of weight probability density function (PDF) with a physics based potential field representation of the model weights in a Gaussian reproducing kernel Hilbert space (RKHS) embedding. This allows us to use perturbation theory from quantum physics to formulate a moment decomposition problem over the model weight-output relationship. The extracted moments reveal successive degrees of regularization of the weight potential field around the local neighborhood of the model output. Such localized moments represent well the PDF tails and provide significantly greater accuracy of the model’s predictive uncertainty than the central moments characterized by Bayesian and ensemble methods or their variants. We show that this consequently leads to a better ability to detect false model predictions of test data that has undergone a covariate shift away from the training PDF learned by the model. We evaluate our approach against baseline uncertainty quantification methods on several benchmark datasets that are corrupted using common distortion techniques. Our approach provides fast model predictive uncertainty estimates with much greater precision and calibration. Deep neural network (DNN) models have become the predominant choice for pattern representation in a wide variety of machine learning applications due to their remarkable performance advantages in the presence of large amount data (LeCun et al., 2015). The increased adoption of DNNs in safety critical and high stake problems such as medical diagnosis, chemical plant control, defense sysFigure 1: Proposed approach: Moments extracted from the local interaction of the model output with the RKHS potential field of the weights quantify the output uncertainty. tems and autonomous driving has led to growing concerns within the research community on the performance trustworthiness of such models (Kendall & Gal, 2017; Lundberg & Lee, 2017). This becomes particularly imperative in situations involving data distributional shifts or the presence of out of distribution data (OOD) during testing towards which the model may lack robustness due to poor choice of training parameters or lack of sufficiently labeled training data, especially since machine learning algorithms do not have extensive prior information like humans to deal with such situations (Amodei et al., 2016). An important way through which trust in the performance of machine learning algorithms (particularly DNNs) can be established is through accurate techniques of predictive uncertainty quantification of models that allow practitioners to determine how much they should rely on model predictions. Although there have been several categories of methods developed in the recent years, the Bayesian approach ar X iv :2 10 9. 10 88 8v 1 [ cs .L G ] 2 2 Se p 20 21 (MacKay, 1992; Neal, 2012; Bishop, 1995) had for long been regarded as the gold standard for natural representation of uncertainty in neural networks. However, it has been realized that they are unable to scale to modern applications and often fail to capture the true data distribution in practice (Lakshminarayanan et al., 2017). Another fundamental limitation of Bayesian techniques is that they are only capable of selecting measurements of central tendency from the posterior. This limits their sensitivity in quantifying local uncertainty information which is especially prevalent in modern applications where models can capture very complex patterns with significant local variations. We propose an approach for model uncertainty quantification that relies on the density and local fit criteria (Leonard et al., 1992) which in this context means that a model prediction y is reliable/certain only if the model has been trained to make predictions in the local vicinity of y (i.e. the model is locally regularized around y). To quantify the model space according to this criteria, one requires a rich localized representation of the model predictive PDF p(y |w) which deems a prediction y reliable only if δ= p(y +∆y |w)−p(y |w) ≈ 0 where∆y is a small perturbation around y . Towards this end, we utilize an uncertainty decomposition framework called the quantum information potential field (QIPF) (Singh & Principe (2020); Singh & Principe (2021)) through which we represent the model weight space as a potential field by embedding the weights in a Gaussian reproducing kernel Hilbert space (RKHS). Such a representation (depicted in Fig. 1) allows us to principally use the notion of perturbation theory in quantum physics to quantify the local gradients of the model’s predictive PDF space in terms of multiple uncertainty moments, thereby giving a high resolution description of δ. In essence, these moments successively quantify the degree of regularization of the weights in the local neighborhood of the model output. The QIPF framework, which was first introduced by (Singh & Principe (2020); Singh & Principe (2021)), has shown promising (albeit very preliminary) results in model uncertainty quantification for regression problems (Singh & Principe, 2021) and in particular applications of time series analysis (Singh & Principe, 2020). It enjoys the following advantages over other methods: • The QIPF utilizes the Gaussian RKHS whose mathematical properties, specifically the kernel trick (Smola & Schölkopf, 1998) and the kernel mean embedding theory (Muandet et al., 2017), makes it a universal injective estimator of data PDF and without making any underlying assumptions. • Through its physics based moment decomposition formulation, it is able to provide a multi-scale description of the local PDF dynamics that focuses on the tail regions of the PDF thereby providing a very accurate description of uncertainty. • It is also significantly simpler to compute and more scalable than Bayesian approaches. It is non-intrusive to the training process of the model and enables a single-shot estimation of model uncertainty at each test instance thereby offering practical advantages over ensemble and Monte Carlo approaches. Our main contributions in this paper are three-fold: • We use perturbation theory to provide a more concrete description of the QIPF framework and provide a new insight into how a potential field viewpoint of the model weight space can be used to describe the degree of regularization/certainty of trained weights in the local neighborhood of the output in terms of multiple moments. • We specifically evaluate the performance of the framework in an important and unsolved problem of model uncertainty quantification in situations involving covariate shift in test data. • We analyze the performance of the QIPF against baselines of multiple UQ approaches using diverse models trained on benchmark datasets. We use both accuracy as well as calibration metrics to evaluate performance.', 'corpus_id': 237593007, 'score': 0}]
47	Operating Systems	aab338915e32b60e92c8de3a23aa5c5d	19860	{}	"[{'doc_id': '236501153', 'title': 'Avocado: A Secure In-Memory Distributed Storage System', 'abstract': 'We introduce Avocado, a secure in-memory distributed storage system that provides strong security, fault-tolerance, consistency (linearizability) and performance for untrusted cloud environments. Avocado achieves these properties based on TEEs, which, however, are primarily designed for securing limited physical memory (enclave) within a single-node system. Avocado overcomes this limitation by extending the trust of a secure single-node enclave to the distributed environment over an untrusted network, while ensuring that replicas are kept consistent and fault-tolerant in a malicious environment. To achieve these goals, we design and implementAvocado underpinning on the cross-layer contributions involving the network stack, the replication protocol, scalable trust establishment, and memory management. Avocado is practical: In comparison to BFT, Avocado provides con dentiality with fewer replicas and is signi cantly faster—4.5× to 65× for YCSB read and write heavy workloads, respectively.', 'corpus_id': 236501153, 'score': 0}, {'doc_id': '14612067', 'title': 'The IX Operating System', 'abstract': 'The conventional wisdom is that aggressive networking requirements, such as high packet rates for small messages and μs-scale tail latency, are best addressed outside the kernel, in a user-level networking stack. We present ix, a dataplane operating system that provides high I/O performance and high resource efficiency while maintaining the protection and isolation benefits of existing kernels. ix uses hardware virtualization to separate management and scheduling functions of the kernel (control plane) from network processing (dataplane). The dataplane architecture builds upon a native, zero-copy API and optimizes for both bandwidth and latency by dedicating hardware threads and networking queues to dataplane instances, processing bounded batches of packets to completion, and eliminating coherence traffic and multicore synchronization. The control plane dynamically adjusts core allocations and voltage/frequency settings to meet service-level objectives. We demonstrate that ix outperforms Linux and a user-space network stack significantly in both throughput and end-to-end latency. Moreover, ix improves the throughput of a widely deployed, key-value store by up to 6.4× and reduces tail latency by more than 2× . With three varying load patterns, the control plane saves 46%--54% of processor energy, and it allows background jobs to run at 35%--47% of their standalone throughput.', 'corpus_id': 14612067, 'score': 1}, {'doc_id': '236924339', 'title': 'Two-Chains: High Performance Framework for Function Injection and Execution', 'abstract': 'Some important problems, such as semantic graph analysis, require large-scale irregular applications composed of many coordinating tasks that operate on a shared data set so big it has to be stored on many physical devices. In these cases, it may be more efficient to dynamically choose where code runs as the applications progresses. Many programming environments provide task migration or remote function calls, but they have sharp trade-offs between flexible composition, portability, performance, and code complexity.We developed Two-Chains, a high performance framework inspired by active message communication semantics. We use the GNU Binutils, the ELF binary format, and the RDMA network protocol to provide ultra-low granularity distributed function composition at runtime in user space at HPC performance levels using C libraries. Our framework allows the direct injection of function binaries and data to a remote machine cache using the RDMA network. It interoperates seamlessly with existing C libraries using standard dynamic linking and load symbol resolution. We analyze function delivery and execution on cache stashing-enabled hardware and show that stashing decreases latency, increases message rates, and improves noise tolerance. This demonstrates one way this method is suited to increasingly network-oriented hardware architectures.', 'corpus_id': 236924339, 'score': 0}, {'doc_id': '4943407', 'title': 'Solros: a data-centric operating system architecture for heterogeneous computing', 'abstract': 'We propose Solros---a new operating system architecture for heterogeneous systems that comprises fast host processors, slow but massively parallel co-processors, and fast I/O devices. A general consensus to fully drive such a hardware system is to have a tight integration among processors and I/O devices. Thus, in the Solros architecture, a co-processor OS (data-plane OS) delegates its services, specifically I/O stacks, to the host OS (control-plane OS). Our observation for such a design is that global coordination with system-wide knowledge (e.g., PCIe topology, a load of each co-processor) and the best use of heterogeneous processors is critical to achieving high performance. Hence, we fully harness these specialized processors by delegating complex I/O stacks on fast host processors, which leads to an efficient global coordination at the level of the control-plane OS. We developed Solros with Xeon Phi co-processors and implemented three core OS services: transport, file system, and network services. Our experimental results show significant performance improvement compared with the stock Xeon Phi running the Linux kernel. For example, Solros improves the throughput of file system and network operations by 19x and 7x, respectively. Moreover, it improves the performance of two realistic applications: 19x for text indexing and 2x for image search.', 'corpus_id': 4943407, 'score': 1}, {'doc_id': '220686636', 'title': 'DBOS: A Proposal for a Data-Centric Operating System', 'abstract': ""Current operating systems are complex systems that were designed before today's computing environments. This makes it difficult for them to meet the scalability, heterogeneity, availability, and security challenges in current cloud and parallel computing environments. To address these problems, we propose a radically new OS design based on data-centric architecture: all operating system state should be represented uniformly as database tables, and operations on this state should be made via queries from otherwise stateless tasks. This design makes it easy to scale and evolve the OS without whole-system refactoring, inspect and debug system state, upgrade components without downtime, manage decisions using machine learning, and implement sophisticated security features. We discuss how a database OS (DBOS) can improve the programmability and performance of many of today's most important applications and propose a plan for the development of a DBOS proof of concept."", 'corpus_id': 220686636, 'score': 1}, {'doc_id': '237212908', 'title': 'Persistent Memory: A Survey of Programming Support and Implementations', 'abstract': 'The recent rise of byte-addressable non-volatile memory technologies is blurring the dichotomy between memory and storage. In particular, they allow programmers to have direct access to persistent data instead of relying on traditional interfaces such as file and database systems. However, they also bring new challenges, as a failure may render the program in an unrecoverable and inconsistent state. Consequently, a lot of effort has been put by both industry and academia into making the task of programming with such memories easier while, at the same time, efficient from the runtime perspective. This survey summarizes such body of research, from the abstractions to the implementation level. As persistent memory is starting to appear commercially, the state-of-the-art research condensed here will help investigators to quickly stay up to date while also motivating others to pursue research in the field.', 'corpus_id': 237212908, 'score': 0}, {'doc_id': '153310994', 'title': 'CyPhOS - A Component-Based Cache-Aware Multi-core Operating System', 'abstract': 'Off-the-shelf multi-core processors provide a cost-efficient alternative to expensive special purpose processors at the cost of complex time predictability due to shared resources like buses, caches and the memory itself. This paper presents an operating system concept that takes control over the shared cache to minimize contention, by creating a component-based operating system, that is structured in small data chunks to allow better control over data and code movement in and out of the cache. An evaluation of the operating system shows that the system is able to reduce the difference between the ACET and observed WCET of a synthetic memory load test by 93% for ARM and 98% for Intel systems. Some noteworthy improvements were also achieved for the TACLe benchmarks.', 'corpus_id': 153310994, 'score': 1}, {'doc_id': '226260363', 'title': 'Theseus: an Experiment in Operating System Structure and State Management', 'abstract': 'This paper describes an operating system (OS) called Theseus. Theseus is the result of multi-year experimentation to redesign and improve OS modularity by reducing the states one component holds for another, and to leverage a safe programming language, namely Rust, to shift as many OS responsibilities as possible to the compiler. Theseus embodies two primary contributions. First, an OS structure in which many tiny components with clearly-defined, runtime-persistent bounds interact without holding states for each other. Second, an intralingual approach that realizes the OS itself using language-level mechanisms such that the compiler can enforce invariants about OS semantics. Theseus’s structure, intralingual design, and state management realize live evolution and fault recovery for core OS components in ways beyond that of existing works.', 'corpus_id': 226260363, 'score': 1}, {'doc_id': '235731835', 'title': 'Recent Advancements In Distributed System Communications', 'abstract': 'Overheads in Operating System kernel network stacks and sockets have been hindering OSes from managing networking operations efficiently for years. Moreover, when building Remote Procedure Calls over TCP, certain TCP features do not match the needs of RPCs, imposing additional overheads. These issues degrade the performance of distributed systems, which rely on fast communications between machines to be able to serve a large number of client requests with low latency and high throughput. The purpose of this literature survey is to look into recent proposals in research literature that aim to overcome these issues. The survey investigates research literature published between 2010-2020, in order to include important advancements during the most recent decade at the time of writing. The proposals found in papers have been categorized into hardware-based and software-based approaches. The former require specialized hardware to offer high communications performance. The latter are implemented in software and don’t rely on specialized hardware or require only certain hardware features. Furthermore, the proposals where also classified according to whether they implement kernel bypass, to avoid using the Operating System kernel network stack, or not. The hardware-based approaches examined here are RDMA, programmable Network Interface Controllers (NIC) and System-on-a-Chip (SoC), while the software-based approaches include optimized socket implementations and RPC frameworks, as well as user space networking.', 'corpus_id': 235731835, 'score': 0}, {'doc_id': '237099383', 'title': 'RapidVMI: Fast and multi-core aware active virtual machine introspection', 'abstract': 'Virtual machine introspection (VMI) is a technique for the external monitoring of virtual machines. Through previous work, it became apparent that VMI can contribute to the security of distributed systems and cloud architectures by facilitating stealthy intrusion detection, malware analysis, and digital forensics. The main shortcomings of active VMI-based approaches such as program tracing or process injection in production environments result from the side effects of writing to virtual address spaces and the parallel execution of shared main memory on multiple processor cores. In this paper, we present RapidVMI, a framework for active virtual machine introspection that enables fine-grained, multi-core aware VMI-based memory access on virtual address spaces. It was built to overcome the outlined shortcomings of existing VMI solutions and facilitate the development of introspection applications as if they run in the monitored virtual machine itself. Furthermore, we demonstrate that hypervisor support for this concept improves introspection performance in prevalent virtual machine tracing applications considerably up to 98 times.', 'corpus_id': 237099383, 'score': 0}]"
48	Provable security in blockchains	de798601b3b7c0dd9fb745c45f3c8e94	551	{}	"[{'doc_id': '52976145', 'title': 'Provably Secure Covert Communication on Blockchain', 'abstract': 'Blockchain is a public open ledger that provides data integrity in a distributed manner. It is the underlying technology of cryptocurrencies and an increasing number of related applications, such as smart contracts. The open nature of blockchain together with strong integrity guarantees on the stored data makes it a compelling platform for covert communication. In this paper, we suggest a method of securely embedding covert messages into a blockchain. We formulate a simplified ideal blockchain model based on existing implementations and devise a protocol that enables two parties to covertly communicate through the blockchain following that model. We also formulate a rigorous definition for the security and covertness of such a protocol based on computational indistinguishability. Finally, we show that our method satisfies this definition in the random oracle model for the underlying cryptographic hash function.', 'corpus_id': 52976145, 'score': 1}, {'doc_id': '16970691', 'title': 'Snow White: Provably Secure Proofs of Stake', 'abstract': 'Decentralized cryptocurrencies have pushed deployments of distributed consensus to more stringent environments than ever before. Most existing protocols rely on proofs-of-work which require expensive computational puzzles to enforce, imprecisely speaking, “one vote per unit of computation”. The enormous amount of energy wasted by these protocols has been a topic of central debate, and well-known cryptocurrencies have announced it a top priority to alternative paradigms. Among the proposed alternative solutions, proofs-of-stake protocols have been of particular interest, where roughly speaking, the idea is to enforce “one vote per unit of stake”. Although the community have rushed to propose numerous candidates for proofs-of-stake, no existing protocol has offered formal proofs of security, which we believe to be a critical, indispensible ingredient of a distributed consensus protocol, particularly one that is to underly a high-value cryptocurrency system. In this work, we seek to address the following basic questions: • What kind of functionalities and robustness requirements should a consensus candidate offer to be suitable in a proof-of-stake application? • Can we design a provably secure protocol that satisfies these requirements? To the best of our knowledge, we are the first to formally articulate a set of requirements for consensus candidates for proofs-of-stake. We argue that any consensus protocol satisfying these properties can be used for proofs-of-stake, as long as money does not switch hands too quickly. Moreover, we provide the first consensus candidate that provably satisfies the desired robustness properties.', 'corpus_id': 16970691, 'score': 1}, {'doc_id': '221655583', 'title': 'Stochastic Modeling Approaches for Analyzing Blockchain: A Survey', 'abstract': 'Blockchain technology has been attracting much attention from both academia and industry. It brings many benefits to various applications like Internet of Things. However, there are critical issues to be addressed before its widespread deployment, such as transaction efficiency, bandwidth bottleneck, and security. Techniques are being explored to tackle these issues. Stochastic modeling, as one of these techniques, has been applied to analyze a variety of blockchain characteristics, but there is a lack of a comprehensive survey on it. In this survey, we aim to fill the gap and review the stochastic models proposed to address common issues in blockchain. Firstly, this paper provides the basic knowledge of blockchain technology and stochastic models. Then, according to different objects, the stochastic models for blockchain analysis are divided into network-oriented and application-oriented (mainly refer to cryptocurrency). The network-oriented stochastic models are further classified into two categories, namely, performance and security. About the application-oriented stochastic models, the widest adoption mainly concentrates on the price prediction of cryptocurrency. Moreover, we provide analysis and comparison in detail on every taxonomy and discuss the strengths and weaknesses of the related works to serve guides for further researches. Finally, challenges and future research directions are given to apply stochastic modeling approaches to study blockchain. By analyzing and classifying the existing researches, we hope that our survey can provide suggestions for the researchers who are interested in blockchain and good at using stochastic models as a tool to address problems.', 'corpus_id': 221655583, 'score': 0}, {'doc_id': '43951901', 'title': 'Cluster ensembles: A survey of approaches with recent extensions and applications', 'abstract': 'Abstract Cluster ensembles have been shown to be better than any standard clustering algorithm at improving accuracy and robustness across different data collections. This meta-learning formalism also helps users to overcome the dilemma of selecting an appropriate technique and the corresponding parameters, given a set of data to be investigated. Almost two decades after the first publication of a kind, the method has proven effective for many problem domains, especially microarray data analysis and its down-streaming applications. Recently, it has been greatly extended both in terms of theoretical modelling and deployment to problem solving. The survey attempts to match this emerging attention with the provision of fundamental basis and theoretical details of state-of-the-art methods found in the present literature. It yields the ranges of ensemble generation strategies, summarization and representation of ensemble members, as well as the topic of consensus clustering. This review also includes different applications and extensions of cluster ensemble, with several research issues and challenges being highlighted.', 'corpus_id': 43951901, 'score': 0}, {'doc_id': '210164616', 'title': 'Formal specification of a security framework for smart contracts', 'abstract': 'As smart contracts are growing in size and complexity, it becomes harder and harder to ensure their correctness and security. Due to the lack of isolation mechanisms a single mistake or vulnerability in the code can bring the whole system down, and due to this smart contract upgrades can be especially dangerous. Traditional ways to ensure the security of a smart contract, including DSLs, auditing and static analysis, are used before the code is deployed to the blockchain, and thus offer no protection after the deployment. After each upgrade the whole code need to be verified again, which is a difficult and time-consuming process that is prone to errors. To address these issues a security protocol and framework for smart contracts called Cap9 was developed. It provides developers the ability to perform upgrades in a secure and robust manner, and improves isolation and transparency through the use of a low level capability-based security model. We have used Isabelle/HOL to develop a formal specification of the Cap9 framework and prove its consistency. The paper presents a refinement-based approach that we used to create the specification, as well as discussion of some encountered difficulties during this process.', 'corpus_id': 210164616, 'score': 1}, {'doc_id': '226290041', 'title': 'Tokoin: A Coin-Based Accountable Access Control Scheme for Internet of Things', 'abstract': 'With the prevalence of Internet of Things (IoT) applications, IoT devices interact closely with our surrounding environments, bringing us unparalleled smartness and convenience. However, the development of secure IoT solutions is getting a long way lagged behind, making us exposed to common unauthorized accesses that may bring malicious attacks and unprecedented danger to our daily life. Overprivilege attack, a widely reported phenomenon in IoT that accesses unauthorized or excessive resources, is notoriously hard to prevent, trace and mitigate. To tackle this challenge, we propose Tokoin-Based Access Control (TBAC), an accountable access control model enabled by blockchain and Trusted Execution Environment (TEE) technologies, to offer fine-graininess, strong auditability, and access procedure control for IoT. TBAC materializes the virtual access power into a definite-amount and secure cryptographic coin termed ""tokoin"" (token+coin), and manages it using atomic and accountable state-transition functions in a blockchain. We also realize access procedure control by mandating every tokoin a fine-grained access policy defining who is allowed to do what at when in where by how. The tokoin is peer-to-peer transferable, and can be modified only by the resource owner when necessary. We fully implement TBAC with well-studied cryptographic primitives and blockchain platforms and present a readily available APP for regular users. We also present a case study to demonstrate how TBAC is employed to enable autonomous in-home cargo delivery while guaranteeing the access policy compliance and home owner\'s physical security by regulating the physical behaviors of the deliveryman.', 'corpus_id': 226290041, 'score': 0}, {'doc_id': '221970442', 'title': 'Taxonomy of Centralization in Public Blockchain Systems: A Systematic Literature Review', 'abstract': 'Bitcoin introduced delegation of control over a monetary system from a select few to all who participate in that system. This delegation is known as the decentralization of controlling power and is a powerful security mechanism for the ecosystem. After the introduction of Bitcoin, the field of cryptocurrency has seen widespread attention from industry and academia, so much so that the original novel contribution of Bitcoin i.e. decentralization, may be overlooked, due to decentralizations assumed fundamental existence for the functioning of such cryptoassets. However recent studies have observed a trend of increased centralization in cryptocurrencies such as Bitcoin and Ethereum. As this increased centralization has an impact the security of the blockchain, it is crucial that it is measured, towards adequate control. This research derives an initial taxonomy of centralization present in decentralized blockchains through rigorous synthesis using a systematic literature review. This is followed by iterative refinement through expert interviews. We systematically analyzed 89 research papers published between 2009 and 2019. Our study contributes to the existing body of knowledge by highlighting the multiple definitions and measurements of centralization in the literature. We identify different aspects of centralization and propose an encompassing taxonomy of centralization concerns. This taxonomy is based on empirically observable and measurable characteristics. It consists of 13 aspects of centralization classified over six architectural layers Governance Network Consensus Incentive Operational and Application. We also discuss how the implications of centralization can vary depending on the aspects studied. We believe that this review and taxonomy provides a comprehensive overview of centralization in decentralized blockchains involving various conceptualizations and measures.', 'corpus_id': 221970442, 'score': 0}, {'doc_id': '16406453', 'title': 'Towards emotional awareness in software development teams', 'abstract': ""Emotions play an important role in determining work results and how team members collaborate within a project. When working in large, distributed teams, members can lose awareness of the emotional state of the project. We propose an approach to improve emotional awareness in software development teams by means of quantitative emotion summaries. Our approach automatically extracts and summarizes emotions expressed in collaboration artifacts by combining probabilistic topic modeling with lexical sentiment analysis techniques. We applied the approach to 1000 collaboration artifacts produced by three development teams in a three month period. Interviews with the teams' project leaders suggest that the proposed emotion summaries have a good correlation with the emotional state of the project, and could be useful for improving emotional awareness. However, the interviews also indicate that the current state of the summaries is not detailed enough and further improvements are needed."", 'corpus_id': 16406453, 'score': 0}, {'doc_id': '8189550', 'title': 'On the Security and Performance of Proof of Work Blockchains', 'abstract': ""Proof of Work (PoW) powered blockchains currently account for more than 90% of the total market capitalization of existing digital cryptocurrencies. Although the security provisions of Bitcoin have been thoroughly analysed, the security guarantees of variant (forked) PoW blockchains (which were instantiated with different parameters) have not received much attention in the literature. This opens the question whether existing security analysis of Bitcoin's PoW applies to other implementations which have been instantiated with different consensus and/or network parameters. In this paper, we introduce a novel quantitative framework to analyse the security and performance implications of various consensus and network parameters of PoW blockchains. Based on our framework, we devise optimal adversarial strategies for double-spending and selfish mining while taking into account real world constraints such as network propagation, different block sizes, block generation intervals, information propagation mechanism, and the impact of eclipse attacks. Our framework therefore allows us to capture existing PoW-based deployments as well as PoW blockchain variants that are instantiated with different parameters, and to objectively compare the tradeoffs between their performance and security provisions."", 'corpus_id': 8189550, 'score': 1}, {'doc_id': '8696548', 'title': 'Ouroboros: A Provably Secure Proof-of-Stake Blockchain Protocol', 'abstract': 'We present “Ouroboros”, the first blockchain protocol based on proof of stake with rigorous security guarantees. We establish security properties for the protocol comparable to those achieved by the bitcoin blockchain protocol. As the protocol provides a “proof of stake” blockchain discipline, it offers qualitative efficiency advantages over blockchains based on proof of physical resources (e.g., proof of work). We also present a novel reward mechanism for incentivizing Proof of Stake protocols and we prove that, given this mechanism, honest behavior is an approximate Nash equilibrium, thus neutralizing attacks such as selfish mining.', 'corpus_id': 8696548, 'score': 1}]"
49	Silicon Spin Qubit Research	67fe1485b43de72518ad875c63005990	3897	{}	[{'doc_id': '212747726', 'title': 'Simulated coherent electron shuttling in silicon quantum dots', 'abstract': 'Shuttling of single electrons in gate-defined silicon quantum dots is numerically simulated. A minimal gate geometry without explicit tunnel barrier gates is introduced, and used to define a chain of accumulation mode quantum dots, each controlled by a single gate voltage. One-dimensional potentials are derived from a three-dimensional electrostatic model, and used to construct an effective Hamiltonian for efficient simulation. Control pulse sequences are designed by maintaining a fixed adiabaticity, so that different shuttling conditions can be systematically compared. We first use these tools to optimize the device geometry for maximum transport velocity, considering only orbital states and neglecting valley and spin degrees of freedom. Taking realistic geometrical constraints into account, charge shuttling speeds up to ~ 300 m/s preserve adiabaticity. Coherent spin transport is simulated by including spin-orbit and valley terms in an effective Hamiltonian, shuttling one member of a singlet pair and tracking the entanglement fidelity. With realistic device and material parameters, shuttle speeds in the range 10-100 m/s with high spin entanglement fidelities are obtained when the tunneling energy exceeds the Zeeman energy. High fidelity also requires the inter-dot valley phase difference to be below a threshold determined by the ratio of tunneling and Zeeman energies, so that spin-valley-orbit mixing is weak. In this regime, we find that the primary source of infidelity is a coherent spin rotation that is correctable, in principle. The results pertain to proposals for large-scale spin qubit processors in isotopically purified silicon that rely on coherent shuttling of spins to rapidly distribute quantum information between computational nodes.', 'corpus_id': 212747726, 'score': 1}, {'doc_id': '211082615', 'title': 'Towards a realistic GaAs-spin qubit device for a classical error-corrected quantum memory', 'abstract': 'Based on numerically-optimized real-device gates and parameters we study the performance of the phase-flip (repetition) code on a linear array of Gallium Arsenide (GaAs) quantum dots hosting singlet-triplet qubits. We first examine the expected performance of the code using simple error models of circuit-level and phenomenological noise, reporting, for example, a circuit-level depolarizing noise threshold of approximately 3%. We then perform density-matrix simulations using a maximum-likelihood and minimum-weight matching decoder to study the effect of real-device dephasing, read-out error, quasi-static as well as fast gate noise. Considering the trade-off between qubit read-out error and dephasing time (T2) over measurement time, we identify a sub-threshold region for the phase-flip code which lies within experimental reach.', 'corpus_id': 211082615, 'score': 0}, {'doc_id': '205264280', 'title': 'A coherent spin–photon interface in silicon', 'abstract': 'Electron spins in silicon quantum dots are attractive systems for quantum computing owing to their long coherence times and the promise of rapid scaling of the number of dots in a system using semiconductor fabrication techniques. Although nearest-neighbour exchange coupling of two spins has been demonstrated, the interaction of spins via microwave-frequency photons could enable long-distance spin–spin coupling and connections between arbitrary pairs of qubits (‘all-to-all’ connectivity) in a spin-based quantum processor. Realizing coherent spin–photon coupling is challenging because of the small magnetic-dipole moment of a single spin, which limits magnetic-dipole coupling rates to less than 1 kilohertz. Here we demonstrate strong coupling between a single spin in silicon and a single microwave-frequency photon, with spin–photon coupling rates of more than 10 megahertz. The mechanism that enables the coherent spin–photon interactions is based on spin–charge hybridization in the presence of a magnetic-field gradient. In addition to spin–photon coupling, we demonstrate coherent control and dispersive readout of a single spin. These results open up a direct path to entangling single spins using microwave-frequency photons.', 'corpus_id': 205264280, 'score': 1}, {'doc_id': '212747703', 'title': 'A Phononic Bus for Coherent Interfaces Between a Superconducting Quantum Processor, Spin Memory, and Photonic Quantum Networks', 'abstract': 'We introduce a method for high-fidelity quantum state transduction between a superconducting microwave qubit and the ground state spin system of a solid-state artificial atom, mediated via an acoustic bus connected by piezoelectric transducers. Applied to present-day experimental parameters for superconducting circuit qubits and diamond silicon vacancy centers in an optimized phononic cavity, we estimate quantum state transduction with fidelity exceeding 99\\% at a MHz-scale bandwidth. By combining the complementary strengths of superconducting circuit quantum computing and artificial atoms, the hybrid architecture provides high-fidelity qubit gates with long-lived quantum memory, high-fidelity measurement, large qubit number, reconfigurable qubit connectivity, and high-fidelity state and gate teleportation through optical quantum networks.', 'corpus_id': 212747703, 'score': 1}, {'doc_id': '211532504', 'title': 'Gate-Defined Accumulation-Mode Quantum Dots in Monolayer and Bilayer \nWSe2', 'abstract': 'We report the fabrication and characterization of gate-defined hole quantum dots in monolayer and bilayer WSe$_2$. The devices were operated with gates above and below the WSe$_2$ layer to accumulate a hole gas, which for some devices was then selectively depleted to define the dot. Temperature dependence of conductance in the Coulomb blockade regime is consistent with transport through a single level, and excited state transport through the dots was observed at temperatures up to 10 K. For adjacent charge states of a bilayer WSe$_2$ dot, magnetic field dependence of excited state energies was used to estimate $g$-factors between 0.8 and 2.4 for different states. These devices provide a platform to evaluate valley-spin states in monolayer and bilayer WSe$_2$ for application as qubits.', 'corpus_id': 211532504, 'score': 0}, {'doc_id': '212725627', 'title': 'Progress toward a capacitively mediated CNOT between two charge qubits in Si/SiGe', 'abstract': 'Fast operations, an easily tunable Hamiltonian, and a straightforward two-qubit interaction make charge qubits a useful tool for benchmarking device performance and exploring two-qubit dynamics. Here, we tune a linear chain of four Si/SiGe quantum dots to host two double dot charge qubits. Using the capacitance between the double dots to mediate a strong two-qubit interaction, we simultaneously drive coherent transitions to generate correlations between the qubits. We then sequentially pulse the qubits to drive one qubit conditionally on the state of the other. We find that a conditional π -rotation can be driven in just 74\u2009ps with a modest fidelity demonstrating the possibility of two-qubit operations with a 13.5\u2009GHz clockspeed.', 'corpus_id': 212725627, 'score': 1}, {'doc_id': '211043855', 'title': 'Integrated optical multi-ion quantum logic.', 'abstract': 'Practical and useful quantum information processing requires substantial improvements with respect to current systems, both in the\xa0error rates of basic operations and in scale. The fundamental qualities of individual trapped-ion1 qubits are promising for long-term systems2, but the optics involved in their precise control are a barrier to scaling3. Planar-fabricated optics integrated within ion-trap devices can make such systems simultaneously more robust and parallelizable, as suggested by previous work with single ions4. Here we use scalable optics co-fabricated with a surface-electrode ion trap to achieve high-fidelity multi-ion quantum logic gates, which are often the limiting elements in building up the precise, large-scale entanglement that is\xa0essential to quantum computation. Light is efficiently delivered to a trap chip in a cryogenic environment via direct fibre coupling on multiple channels, eliminating the need for beam alignment into vacuum systems and cryostats and lending robustness to vibrations and beam-pointing drifts. This allows us to perform ground-state laser cooling of ion motion and to implement gates generating two-ion entangled states with fidelities greater than 99.3(2) per cent. This work demonstrates hardware that reduces noise and drifts in sensitive quantum logic, and simultaneously offers a route to practical parallelization for high-fidelity quantum processors5. Similar devices may also find applications in atom- and ion-based quantum sensing and timekeeping6.', 'corpus_id': 211043855, 'score': 0}, {'doc_id': '211031878', 'title': 'Precise high-fidelity electron–nuclear spin entangling gates in NV centers via hybrid dynamical decoupling sequences', 'abstract': 'Color centers in solids, such as the nitrogen-vacancy center in diamond, offer well-protected and well-controlled localized electron spins that can be employed in various quantum technologies. Moreover, the long coherence time of the surrounding spinful nuclei can enable a robust quantum register controlled through the color center. We design pulse sequence protocols that drive the electron spin to generate robust entangling gates with these nuclear memory qubits. We find that compared to using Carr-Purcell-Meiboom-Gill (CPMG) alone, Uhrig decoupling sequence and hybrid protocols composed of CPMG and Uhrig sequences improve these entangling gates in terms of fidelity, spin control range, and spin selectivity. We provide analytical expressions for the sequence protocols and also show numerically the efficacy of our method on nitrogen-vacancy centers in diamond. Our results are broadly applicable to color centers weakly coupled to a small number of nuclear spin qubits.', 'corpus_id': 211031878, 'score': 0}, {'doc_id': '211069176', 'title': 'A Mechanically Tunable Quantum Dot in a Graphene Break Junction', 'abstract': 'Graphene quantum dots (QDs) are intensively studied as platforms for the next generation of quantum electronic devices. Fine tuning of the transport properties in monolayer graphene QDs, in particular with respect to the independent modulation of the tunnel barrier transparencies, remains challenging and is typically addressed using electrostatic gating. We investigate charge transport in back-gated graphene mechanical break junctions and reveal Coulomb blockade physics characteristic of a single, high-quality QD when a nanogap is opened in a graphene constriction. By mechanically controlling the distance across the newly formed graphene nanogap, we achieve reversible tunability of the tunnel coupling to the drain electrode by 5 orders of magnitude, while keeping the source-QD tunnel coupling constant. The break junction device can therefore become a powerful platform to study the physical parameters that are crucial to the development of future graphene-based devices, including energy converters and quantum calorimeters.', 'corpus_id': 211069176, 'score': 0}, {'doc_id': '211677321', 'title': 'Split-Gate Cavity Coupler for Silicon Circuit Quantum Electrodynamics', 'abstract': 'Coherent charge-photon and spin-photon coupling has recently been achieved in silicon double quantum dots (DQD). Here we demonstrate a versatile split-gate cavity-coupler that allows more than one DQD to be coupled to the same microwave cavity. Measurements of the cavity transmission as a function of level detuning yield a charge cavity coupling rate $g_c/2\\pi$ = 58 MHz, charge decoherence rate $\\gamma_c/2\\pi$ = 36 MHz, and cavity decay rate $\\kappa/2\\pi$ = 1.2 MHz. The charge cavity coupling rate is in good agreement with device simulations. Our coupling technique can be extended to enable simultaneous coupling of multiple DQDs to the same cavity mode, opening the door to long-range coupling of semiconductor qubits using microwave frequency photons.', 'corpus_id': 211677321, 'score': 1}]
50	Clinical Abstractive Summarization	e8fb290ed9ec25285485f50a1cf38c21	8732	{}	"[{'doc_id': '221293259', 'title': 'A Baseline Analysis for Podcast Abstractive Summarization', 'abstract': ""Podcast summary, an important factor affecting end-users' listening decisions, has often been considered a critical feature in podcast recommendation systems, as well as many downstream applications. Existing abstractive summarization approaches are mainly built on fine-tuned models on professionally edited texts such as CNN and DailyMail news. Different from news, podcasts are often longer, more colloquial and conversational, and noisier with contents on commercials and sponsorship, which makes automatic podcast summarization extremely challenging. This paper presents a baseline analysis of podcast summarization using the Spotify Podcast Dataset provided by TREC 2020. It aims to help researchers understand current state-of-the-art pre-trained models and hence build a foundation for creating better models."", 'corpus_id': 221293259, 'score': 0}, {'doc_id': '220633461', 'title': 'SummPip: Unsupervised Multi-Document Summarization with Sentence Graph Compression', 'abstract': 'Obtaining training data for multi-document Summarization (MDS) is time consuming and resource-intensive, so recent neural models can only be trained for limited domains. In this paper, we propose SummPip: an unsupervised method for multi-document summarization, in which we convert the original documents to a sentence graph, taking both linguistic and deep representation into account, then apply spectral clustering to obtain multiple clusters of sentences, and finally compress each cluster to generate the final summary. Experiments on Multi-News and DUC-2004 datasets show that our method is competitive to previous unsupervised methods and is even comparable to the neural supervised approaches. In addition, human evaluation shows our system produces consistent and complete summaries compared to human written ones.', 'corpus_id': 220633461, 'score': 0}, {'doc_id': '221319573', 'title': 'Generating (Factual?) Narrative Summaries of RCTs: Experiments with Neural Multi-Document Summarization', 'abstract': 'We consider the problem of automatically generating a narrative biomedical evidence summary from multiple trial reports. We evaluate modern neural models for abstractive summarization of relevant article abstracts from systematic reviews previously conducted by members of the Cochrane collaboration, using the authors conclusions section of the review abstract as our target. We enlist medical professionals to evaluate generated summaries, and we find that modern summarization systems yield consistently fluent and relevant synopses, but that they are not always factual. We propose new approaches that capitalize on domain-specific models to inform summarization, e.g., by explicitly demarcating snippets of inputs that convey key findings, and emphasizing the reports of large and high-quality trials. We find that these strategies modestly improve the factual accuracy of generated summaries. Finally, we propose a new method for automatically evaluating the factuality of generated narrative evidence syntheses using models that infer the directionality of reported findings.', 'corpus_id': 221319573, 'score': 1}, {'doc_id': '221703127', 'title': 'Attention-Aware Inference for Neural Abstractive Summarization', 'abstract': ""Inspired by Google's Neural Machine Translation (NMT) \\cite{Wu2016Google} that models the one-to-one alignment in translation tasks with an optimal uniform attention distribution during the inference, this study proposes an attention-aware inference algorithm for Neural Abstractive Summarization (NAS) to regulate generated summaries to attend to source paragraphs/sentences with the optimal coverage. Unlike NMT, the attention-aware inference of NAS requires the prediction of the optimal attention distribution. Therefore, an attention-prediction model is constructed to learn the dependency between attention weights and sources. To apply the attention-aware inference on multi-document summarization, a Hierarchical Transformer (HT) is developed to accept lengthy inputs at the same time project cross-document information. Experiments on WikiSum \\cite{liu2018generating} suggest that the proposed HT already outperforms other strong Transformer-based baselines. By refining the regular beam search with the attention-aware inference, significant improvements on the quality of summaries could be further observed. Last but not the least, the attention-aware inference could be adopted to single-document summarization with straightforward modifications according to the model architecture."", 'corpus_id': 221703127, 'score': 1}, {'doc_id': '214802728', 'title': 'Boosting Factual Correctness of Abstractive Summarization', 'abstract': 'A commonly observed problem with abstractive summarization is the distortion or fabrication of factual information in the article. This inconsistency between summary and original text has led to various concerns over its applicability. In this paper, we firstly propose a Fact-Aware Summarization model, FASum, which extracts factual relations from the article and integrates this knowledge into the decoding process via neural graph computation. Then, we propose a Factual Corrector model, FC, that can modify abstractive summaries generated by any model to improve factual correctness. Empirical results show that FASum generates summaries with significantly higher factual correctness compared with state-of-the-art abstractive summarization systems, both under an independently trained factual correctness evaluator and human evaluation. And FC improves the factual correctness of summaries generated by various models via only modifying several entity tokens.', 'corpus_id': 214802728, 'score': 1}, {'doc_id': '220686889', 'title': 'Massive Multi-Document Summarization of Product Reviews with Weak Supervision', 'abstract': 'Product reviews summarization is a type of Multi-Document Summarization (MDS) task in which the summarized document sets are often far larger than in traditional MDS (up to tens of thousands of reviews). We highlight this difference and coin the term ""Massive Multi-Document Summarization"" (MMDS) to denote an MDS task that involves hundreds of documents or more. Prior work on product reviews summarization considered small samples of the reviews, mainly due to the difficulty of handling massive document sets. We show that summarizing small samples can result in loss of important information and provide misleading evaluation results. We propose a schema for summarizing a massive set of reviews on top of a standard summarization algorithm. Since writing large volumes of reference summaries needed for advanced neural network models is impractical, our solution relies on weak supervision. Finally, we propose an evaluation scheme that is based on multiple crowdsourced reference summaries and aims to capture the massive review collection. We show that an initial implementation of our schema significantly improves over several baselines in ROUGE scores, and exhibits strong coherence in a manual linguistic quality assessment.', 'corpus_id': 220686889, 'score': 0}, {'doc_id': '221702889', 'title': 'Unsupervised Abstractive Dialogue Summarization for Tete-a-Tetes', 'abstract': 'High-quality dialogue-summary paired data is expensive to produce and domain-sensitive, making abstractive dialogue summarization a challenging task. In this work, we propose the first unsupervised abstractive dialogue summarization model for tete-a-tetes (SuTaT). Unlike standard text summarization, a dialogue summarization method should consider the multi-speaker scenario where the speakers have different roles, goals, and language styles. In a tete-a-tete, such as a customer-agent conversation, SuTaT aims to summarize for each speaker by modeling the customer utterances and the agent utterances separately while retaining their correlations. SuTaT consists of a conditional generative module and two unsupervised summarization modules. The conditional generative module contains two encoders and two decoders in a variational autoencoder framework where the dependencies between two latent spaces are captured. With the same encoders and decoders, two unsupervised summarization modules equipped with sentence-level self-attention mechanisms generate summaries without using any annotations. Experimental results show that SuTaT is superior on unsupervised dialogue summarization for both automatic and human evaluations, and is capable of dialogue classification and single-turn conversation generation.', 'corpus_id': 221702889, 'score': 0}, {'doc_id': '218470160', 'title': 'Attend to Medical Ontologies: Content Selection for Clinical Abstractive Summarization', 'abstract': 'Sequence-to-sequence (seq2seq) network is a well-established model for text summarization task. It can learn to produce readable content; however, it falls short in effectively identifying key regions of the source. In this paper, we approach the content selection problem for clinical abstractive summarization by augmenting salient ontological terms into the summarizer. Our experiments on two publicly available clinical data sets (107,372 reports of MIMIC-CXR, and 3,366 reports of OpenI) show that our model statistically significantly boosts state-of-the-art results in terms of ROUGE metrics (with improvements: 2.9% RG-1, 2.5% RG-2, 1.9% RG-L), in the healthcare domain where any range of improvement impacts patients’ welfare.', 'corpus_id': 218470160, 'score': 1}, {'doc_id': '221319734', 'title': 'Extractive Summarizer for Scholarly Articles', 'abstract': 'We introduce an extractive method that will summarize long scientific papers. Our model uses presentation slides provided by the authors of the papers as the gold summary standard to label the sentences. The sentences are ranked based on their novelty and their importance as estimated by deep neural networks. Our window-based extractive labeling of sentences results in the improvement of at least 4 ROUGE1-Recall points.', 'corpus_id': 221319734, 'score': 0}, {'doc_id': '214713466', 'title': 'Abstractive Summarization with Combination of Pre-trained Sequence-to-Sequence and Saliency Models', 'abstract': 'Pre-trained sequence-to-sequence (seq-to-seq) models have significantly improved the accuracy of several language generation tasks, including abstractive summarization. Although the fluency of abstractive summarization has been greatly improved by fine-tuning these models, it is not clear whether they can also identify the important parts of the source text to be included in the summary. In this study, we investigated the effectiveness of combining saliency models that identify the important parts of the source text with the pre-trained seq-to-seq models through extensive experiments. We also proposed a new combination model consisting of a saliency model that extracts a token sequence from a source text and a seq-to-seq model that takes the sequence as an additional input text. Experimental results showed that most of the combination models outperformed a simple fine-tuned seq-to-seq model on both the CNN/DM and XSum datasets even if the seq-to-seq model is pre-trained on large-scale corpora. Moreover, for the CNN/DM dataset, the proposed combination model exceeded the previous best-performed model by 1.33 points on ROUGE-L.', 'corpus_id': 214713466, 'score': 1}]"
51	Embodied Navigation	a6ca8b9b83b3ae2f9737e690a35daa69	20509	{}	[{'doc_id': '236954782', 'title': 'NOVEL LAYOUTS USING ABSTRACT 2-D MAPS', 'abstract': 'Efficiently training agents with planning capabilities has long been one of the major challenges in decision-making. In this work, we focus on zero-shot navigation ability on a given abstract 2-D occupancy map, like human navigation by reading a paper map, by treating it as an image. To learn this ability, we need to efficiently train an agent on environments with a small proportion of training maps and share knowledge effectively across the environments. We hypothesize that model-based navigation can better adapt agent’s behaviors to a task, since it disentangles the variations in map layout and goal location and enables longer-term planning ability on novel locations compared to reactive policies. We propose to learn a hypermodel that can understand patterns from a limited number of abstract maps and goal locations, to maximize alignment between the hypermodel predictions and real trajectories to extract information from multi-task off-policy experiences, and to construct denser feedback for planners by n-step goal relabelling. We train our approach on DeepMind Lab environments with layouts from different maps, and demonstrate superior performance on zero-shot transfer to novel maps and goals.', 'corpus_id': 236954782, 'score': 0}, {'doc_id': '236924393', 'title': 'Learning to Design and Construct Bridge without Blueprint', 'abstract': 'Autonomous assembly has been a desired functionality of many intelligent robot systems. We study a new challenging assembly task, designing and constructing a bridge without a blueprint. In this task, the robot needs to first design a feasible bridge architecture for arbitrarily wide cliffs and then manipulate the blocks reliably to construct a stable bridge according to the proposed design. In this paper, we propose a bilevel approach to tackle this task. At the high level, the system learns a bridge blueprint policy in a physical simulator using deep reinforcement learning and curriculum learning. A policy is represented as an attention-based neural network with objectcentric input, which enables generalization to different number of blocks and cliff widths. For low-level control, we implement a motion-planning-based policy for real-robot motion control, which can be directly combined with a trained blueprint policy for real-world bridge construction without tuning. In our field study, our bi-level robot system demonstrates the capability of manipulating blocks to construct a diverse set of bridges with different architectures.', 'corpus_id': 236924393, 'score': 0}, {'doc_id': '236975859', 'title': 'Embodied BERT: A Transformer Model for Embodied, Language-guided Visual Task Completion', 'abstract': 'Language-guided robots performing home and office tasks must navigate in and interact with the world. Grounding language instructions against visual observations and actions to take in an environment is an open challenge. We present Embodied BERT (EmBERT), a transformer-based model which can attend to high-dimensional, multi-modal inputs across long temporal horizons for languageconditioned task completion.1 Additionally, we bridge the gap between successful object-centric navigation models used for non-interactive agents and the languageguided visual task completion benchmark, ALFRED, by introducing object navigation targets for EmBERT training. We achieve competitive performance on the ALFRED benchmark, and EmBERT marks the first transformer-based model to successfully handle the long-horizon, dense, multi-modal histories of ALFRED, and the first ALFRED model to utilize object-centric navigation targets.', 'corpus_id': 236975859, 'score': 1}, {'doc_id': '236922071', 'title': 'LEARNING EFFICIENT PLANNING-BASED REWARDS', 'abstract': 'Imitation learning from limited demonstrations is challenging. Most inverse reinforcement learning (IRL) methods are unable to perform as good as the demonstrator, especially in a high-dimensional environment, e.g, the Atari domain. To address this challenge, we propose a novel reward learning method, which streamlines a differential planning module with dynamics modeling. Our method learns useful planning computations with a meaningful reward function that focuses on the resulting region of an agent executing an action. Such a planning-based reward function leads to policies with better generalization ability. Empirical results with multiple network architectures and reward instances show that our method can outperform state-of-the-art IRL methods on multiple Atari games and continuous control tasks. Our method achieves performance that is averagely 1,139.1% of the demonstration.', 'corpus_id': 236922071, 'score': 0}, {'doc_id': '236908377', 'title': 'ICLR 2021 1 ? ? ? ? ?', 'abstract': 'We introduce environment predictive coding, a self-supervised approach to learn environment-level representations for embodied agents. In contrast to prior work on self-supervised learning for images, we aim to jointly encode a series of images gathered by an agent as it moves about in 3D environments. We learn these representations via a zone prediction task, where we intelligently mask out portions of an agent’s trajectory and predict them from the unmasked portions, conditioned on the agent’s camera poses. By learning such representations on a collection of videos, we demonstrate successful transfer to multiple downstream navigationoriented tasks. Our experiments on the photorealistic 3D environments of Gibson and Matterport3D show that our method outperforms the state-of-the-art on challenging tasks with only a limited budget of experience.', 'corpus_id': 236908377, 'score': 0}, {'doc_id': '235631955', 'title': 'Megaverse: Simulating Embodied Agents at One Million Experiences per Second', 'abstract': 'We present Megaverse, a new 3D simulation platform for reinforcement learning and embodied AI research. The efficient design of our engine enables physics-based simulation with highdimensional egocentric observations at more than 1,000,000 actions per second on a single 8-GPU node. Megaverse is up to 70x faster than DeepMind Lab in fully-shaded 3D scenes with interactive objects. We achieve this high simulation performance by leveraging batched simulation, thereby taking full advantage of the massive parallelism of modern GPUs. We use Megaverse to build a new benchmark that consists of several single-agent and multi-agent tasks covering Intel Labs University of Southern California Georgia Institute of Technology Stanford University. Correspondence to: Aleksei Petrenko <petrenko@usc.edu>. Proceedings of the 38 th International Conference on Machine Learning, PMLR 139, 2021. Copyright 2021 by the author(s). a variety of cognitive challenges. We evaluate model-free RL on this benchmark to provide baselines and facilitate future research. The source code is available at www.megaverse.info', 'corpus_id': 235631955, 'score': 1}, {'doc_id': '237421139', 'title': 'Hierarchical Object-to-Zone Graph for Object Navigation', 'abstract': 'The goal of object navigation is to reach the expected objects according to visual information in the unseen environments. Previous works usually implement deep models to train an agent to predict actions in real-time. However, in the unseen environment, when the target object is not in egocentric view, the agent may not be able to make wise decisions due to the lack of guidance. In this paper, we propose a hierarchical object-to-zone (HOZ) graph to guide the agent in a coarse-to-fine manner, and an online-learning mechanism is also proposed to update HOZ according to the real-time observation in new environments. In particular, the HOZ graph is composed of scene nodes, zone nodes and object nodes. With the pre-learned HOZ graph, the real-time observation and the target goal, the agent can constantly plan an optimal path from zone to zone. In the estimated path, the next potential zone is regarded as subgoal, which is also fed into the deep reinforcement learning model for action prediction. Our methods are evaluated on the AI2-Thor simulator. In addition to widely used evaluation metrics SR and SPL, we also propose a new evaluation metric of SAE that focuses on the effective action rate. Experimental results demonstrate the effectiveness and efficiency of our proposed method. The code is available at https://github.com/sx-zhang/HOZ.git.', 'corpus_id': 237421139, 'score': 0}, {'doc_id': '236956444', 'title': 'Semantic Tracklets: An Object-Centric Representation for Visual Multi-Agent Reinforcement Learning', 'abstract': 'Solving complex real-world tasks, e.g., autonomous fleet control, often involves a coordinated team of multiple agents which learn strategies from visual inputs via reinforcement learning. Many existing multi-agent reinforcement learning (MARL) algorithms however don’t scale to environments where agents operate on visual inputs. To address this issue, algorithmically, recent works have focused on non-stationarity and exploration. In contrast, we study whether scalability can also be achieved via a disentangled representation. For this, we explicitly construct an objectcentric intermediate representation to characterize the states of an environment, which we refer to as ‘semantic tracklets.’ We evaluate ‘semantic tracklets’ on the visual multi-agent particle environment (VMPE) and on the challenging visual multiagent GFootball environment. ‘Semantic tracklets’ consistently outperform baselines on VMPE, and achieve a +2.4 higher score difference than baselines on GFootball. Notably, this method is the first to successfully learn a strategy for five players in the GFootball environment using only visual data. For more, please see our project page: https://ioujenliu.github. io/SemanticTracklets', 'corpus_id': 236956444, 'score': 1}, {'doc_id': '235829678', 'title': 'Teaching Agents how to Map: Spatial Reasoning for Multi-Object Navigation', 'abstract': 'In the context of visual navigation, the capacity to map a novel environment is necessary for an agent to exploit its observation history in the considered place and efficiently reach known goals. This ability can be associated with spatial reasoning, where an agent is able to perceive spatial relationships and regularities, and discover object characteristics. In classical Reinforcement Learning (RL) setups, this capacity is learned from reward alone. We introduce supplementary supervision in the form of auxiliary tasks designed to favor the emergence of spatial perception capabilities in agents trained for a goal-reaching downstream objective. We show that learning to estimate metrics quantifying the spatial relationships between an agent at a given location and a goal to reach has a high positive impact in Multi-Object Navigation settings. Our method significantly improves the performance of different baseline agents, that either build an explicit or implicit representation of the environment, even matching the performance of incomparable oracle agents taking ground-truth maps as input.', 'corpus_id': 235829678, 'score': 1}, {'doc_id': '236942047', 'title': 'No RL, No Simulation: Learning to Navigate without Navigating', 'abstract': 'Most prior methods for learning navigation policies require access to simulation environments, as they need online policy interaction and rely on ground-truth maps for rewards. However, building simulators is expensive (requires manual effort for each and every scene) and creates challenges in transferring learned policies to robotic platforms in the real-world, due to the sim-to-real domain gap. In this paper, we pose a simple question: Do we really need active interaction, ground-truth maps or even reinforcement-learning (RL) in order to solve the image-goal navigation task? We propose a self-supervised approach to learn to navigate from only passive videos of roaming. Our approach, No RL, No Simulator (NRNS), is simple and scalable, yet highly effective. NRNS outperforms RL-based formulations by a significant margin. We present NRNS as a strong baseline for any future imagebased navigation tasks that use RL or Simulation.', 'corpus_id': 236942047, 'score': 1}]
52	Discourse Processing	a688e00113e1bc6cb586671cf7e8946e	11901	{}	[{'doc_id': '9713133', 'title': 'Shallow Discourse Parsing with Conditional Random Fields', 'abstract': 'Parsing discourse is a challenging natural language processing task. In this paper we take a data driven approach to identify arguments of explicit discourse connectives. In contrast to previous work we do not make any assumptions on the span of arguments and consider parsing as a token-level sequence labeling task. We design the argument segmentation task as a cascade of decisions based on conditional random fields (CRFs). We train the CRFs on lexical, syntactic and semantic features extracted from the Penn Discourse Treebank and evaluate feature combinations on the commonly used test split. We show that the best combination of features includes syntactic and semantic features. The comparative error analysis investigates the performance variability over connective types and argument positions.', 'corpus_id': 9713133, 'score': 1}, {'doc_id': '209516094', 'title': 'ÆTHEL: Automatically Extracted Typelogical Derivations for Dutch', 'abstract': 'We present ÆTHEL, a semantic compositionality dataset for written Dutch. ÆTHEL consists of two parts. First, it contains a lexicon of supertags for about 900 000 words in context. The supertags correspond to types of the simply typed linear lambda-calculus, enhanced with dependency decorations that capture grammatical roles supplementary to function-argument structures. On the basis of these types, ÆTHEL further provides 72 192 validated derivations, presented in four formats: natural-deduction and sequent-style proofs, linear logic proofnets and the associated programs (lambda terms) for meaning composition. ÆTHEL’s types and derivations are obtained by means of an extraction algorithm applied to the syntactic analyses of LASSY Small, the gold standard corpus of written Dutch. We discuss the extraction algorithm and show how ‘virtual elements’ in the original LASSY annotation of unbounded dependencies and coordination phenomena give rise to higher-order types. We suggest some example usecases highlighting the benefits of a type-driven approach at the syntax semantics interface. The following resources are open-sourced with ÆTHEL: the lexical mappings between words and types, a subset of the dataset consisting of 7 924 semantic parses, and the Python code that implements the extraction algorithm.', 'corpus_id': 209516094, 'score': 0}, {'doc_id': '71907', 'title': 'Identifying Argumentative Discourse Structures in Persuasive Essays', 'abstract': 'In this paper, we present a novel approach for identifying argumentative discourse structures in persuasive essays. The structure of argumentation consists of several components (i.e. claims and premises) that are connected with argumentative relations. We consider this task in two consecutive steps. First, we identify the components of arguments using multiclass classification. Second, we classify a pair of argument components as either support or non-support for identifying the structure of argumentative discourse. For both tasks, we evaluate several classifiers and propose novel feature sets including structural, lexical, syntactic and contextual features. In our experiments, we obtain a macro F1-score of 0.726 for identifying argument components and 0.722 for argumentative relations.', 'corpus_id': 71907, 'score': 1}, {'doc_id': '63110191', 'title': 'Препараты гиалуроновой кислоты при остеоартрозе: возможности импортозамещения', 'abstract': 'The paper discusses the possibilities of administering hyaluronic acid in osteoarthritis.\xa0 It is noted that the therapeutic efficacy of this drug class has been demonstrated in experimental and clinical studies and confirmed by the practical activities of specialists dealing with issues of arthrology. Along with the use of foreign hyaluronic acid solutions, Russian rheumatologists\xa0 can prescribe high-quality Russian analogues that include INTRAJECT®. The authors pres ent their own clinical observations that confirm that the drug may be administered.\xa0 The paper is illustrated with photos and radiographs.', 'corpus_id': 63110191, 'score': 0}, {'doc_id': '16489279', 'title': 'The DCU Discourse Parser for Connective, Argument Identification and Explicit Sense Classification', 'abstract': 'This paper describes our submission to the CoNLL-2015 shared task on discourse parsing. We factor the pipeline into subcomponents which are then used to form the final sequential architecture. Focusing on achieving good performance when inferring explicit discourse relations, we apply maximum entropy and recurrent neural networks to different sub-tasks such as connective identification, argument extraction, and sense classification. The our final system achieves 16.51%, 12.73% and 11.15% overall F1 scores on the dev, WSJ and blind test sets, respectively.', 'corpus_id': 16489279, 'score': 1}, {'doc_id': '1788414', 'title': 'Argumentation mining: the detection, classification and structure of arguments in text', 'abstract': 'Argumentation is the process by which arguments are constructed and handled. Argumentation constitutes a major component of human intelligence. The ability to engage in argumentation is essential for humans to understand new problems, to perform scientific reasoning, to express, to clarify and to defend their opinions in their daily lives. Argumentation mining aims to detect the arguments presented in a text document, the relations between them and the internal structure of each individual argument. In this paper we analyse the main research questions when dealing with argumentation mining and the different methods we have studied and developed in order to successfully confront the challenges of argumentation mining in legal texts.', 'corpus_id': 1788414, 'score': 1}, {'doc_id': '211296688', 'title': 'Parsing Early Modern English for Linguistic Search', 'abstract': 'We investigate the question of whether advances in NLP over the last few years make it possible to vastly increase the size of data usable for research in historical syntax. This brings together many of the usual tools in NLP - word embeddings, tagging, and parsing - in the service of linguistic queries over automatically annotated corpora. We train a part-of-speech (POS) tagger and parser on a corpus of historical English, using ELMo embeddings trained over a billion words of similar text. The evaluation is based on the standard metrics, as well as on the accuracy of the query searches using the parsed data.', 'corpus_id': 211296688, 'score': 0}, {'doc_id': '219969378', 'title': 'Labeling Explicit Discourse Relations using Pre-trained Language Models', 'abstract': 'Labeling explicit discourse relations is one of the most challenging sub-tasks of the shallow discourse parsing where the goal is to identify the discourse connectives and the boundaries of their arguments. The state-of-the-art models achieve slightly above 45% of F-score by using hand-crafted features. The current paper investigates the efficacy of the pre-trained language models in this task. We find that the pre-trained language models, when finetuned, are powerful enough to replace the linguistic features. We evaluate our model on PDTB 2.0 and report the state-of-the-art results in the extraction of the full relation. This is the first time when a model outperforms the knowledge intensive models without employing any linguistic features.', 'corpus_id': 219969378, 'score': 1}, {'doc_id': '212628456', 'title': 'Parsing Thai Social Data: A New Challenge for Thai NLP', 'abstract': 'Dependency parsing (DP) is a task that analyzes text for syntactic structure and relationship between words. DP is widely used to improve natural language processing (NLP) applications in many languages such as English. Previous works on DP are generally applicable to formally written languages. However, they do not apply to informal languages such as the ones used in social networks. Therefore, DP has to be researched and explored with such social network data. In this paper, we explore and identify a DP model that is suitable for Thai social network data. After that, we will identify the appropriate linguistic unit as an input. The result showed that, the transition based model called, improve Elkared dependency parser outperform the others at UAS of 81.42%.', 'corpus_id': 212628456, 'score': 0}, {'doc_id': '212725806', 'title': 'CompLex — A New Corpus for Lexical Complexity Prediction from Likert Scale Data', 'abstract': 'Predicting which words are considered hard to understand for a given target population is a vital step in many NLP applications such astext simplification. This task is commonly referred to as Complex Word Identification (CWI). With a few exceptions, previous studieshave approached the task as a binary classification task in which systems predict a complexity value (complex vs. non-complex) fora set of target words in a text. This choice is motivated by the fact that all CWI datasets compiled so far have been annotated using abinary annotation scheme. Our paper addresses this limitation by presenting the first English dataset for continuous lexical complexityprediction. We use a 5-point Likert scale scheme to annotate complex words in texts from three sources/domains: the Bible, Europarl,and biomedical texts. This resulted in a corpus of 9,476 sentences each annotated by around 7 annotators.', 'corpus_id': 212725806, 'score': 0}]
53	numbers in language	0aa95b01733d221b2b4c8b5f2f3e5ffa	707	{}	"[{'doc_id': '215548225', 'title': 'Injecting Numerical Reasoning Skills into Language Models', 'abstract': 'Large pre-trained language models (LMs) are known to encode substantial amounts of linguistic information. However, high-level reasoning skills, such as numerical reasoning, are difficult to learn from a language-modeling objective only. Consequently, existing models for numerical reasoning have used specialized architectures with limited flexibility. In this work, we show that numerical reasoning is amenable to automatic data generation, and thus one can inject this skill into pre-trained LMs, by generating large amounts of data, and training in a multi-task setup. We show that pre-training our model, GenBERT, on this data, dramatically improves performance on DROP (49.3 –> 72.3 F1), reaching performance that matches state-of-the-art models of comparable size, while using a simple and general-purpose encoder-decoder architecture. Moreover, GenBERT generalizes well to math word problem datasets, while maintaining high performance on standard RC tasks. Our approach provides a general recipe for injecting skills into large pre-trained LMs, whenever the skill is amenable to automatic data augmentation.', 'corpus_id': 215548225, 'score': 1}, {'doc_id': '209531622', 'title': 'Learning Numeral Embeddings', 'abstract': 'Word embedding is an essential building block for deep learning methods for natural language processing. Although word embedding has been extensively studied over the years, the problem of how to effectively embed numerals, a special subset of words, is still underexplored. Existing word embedding methods do not learn numeral embeddings well because there are an infinite number of numerals and their individual appearances in training corpora are highly scarce. In this paper, we propose two novel numeral embedding methods that can handle the out-of-vocabulary (OOV) problem for numerals. We first induce a finite set of prototype numerals using either a self-organizing map or a Gaussian mixture model. We then represent the embedding of a numeral as a weighted average of the prototype number embeddings. Numeral embeddings represented in this manner can be plugged into existing word embedding learning approaches such as skip-gram for training. We evaluated our methods and showed its effectiveness on four intrinsic and extrinsic tasks: word similarity, embedding numeracy, numeral prediction, and sequence labeling.', 'corpus_id': 209531622, 'score': 1}, {'doc_id': '210702847', 'title': 'A Continuous Space Neural Language Model for Bengali Language', 'abstract': 'Language models are generally employed to estimate the probability distribution of various linguistic units, making them one of the fundamental parts of natural language processing. Applications of language models include a wide spectrum of tasks such as text summarization, translation and classification. For a low resource language like Bengali, the research in this area so far can be considered to be narrow at the very least, with some traditional count based models being proposed. This paper attempts to address the issue and proposes a continuous-space neural language model, or more specifically an ASGD weight- dropped LSTM language model, along with techniques to efficiently train it for Bengali Language. The performance analysis with some currently existing count based models illustrated in this paper also shows that the proposed architecture outperforms its counterparts by achieving an inference perplexity as low as 51.2 on the held out data set for Bengali.', 'corpus_id': 210702847, 'score': 0}, {'doc_id': '212634265', 'title': 'Sentence Analogies: Exploring Linguistic Relationships and Regularities in Sentence Embeddings', 'abstract': 'While important properties of word vector representations have been studied extensively, far less is known about the properties of sentence vector representations. Word vectors are often evaluated by assessing to what degree they exhibit regularities with regard to relationships of the sort considered in word analogies. In this paper, we investigate to what extent commonly used sentence vector representation spaces as well reflect certain kinds of regularities. We propose a number of schemes to induce evaluation data, based on lexical analogy data as well as semantic relationships between sentences. Our experiments consider a wide range of sentence embedding methods, including ones based on BERT-style contextual embeddings. We find that different models differ substantially in their ability to reflect such regularities.', 'corpus_id': 212634265, 'score': 0}, {'doc_id': '216080851', 'title': 'Syntactic Structure from Deep Learning', 'abstract': 'Modern deep neural networks achieve impressive performance in engineering applications that require extensive linguistic skills, such as machine translation. This success has sparked interest in probing whether these models are inducing human-like grammatical knowledge from the raw data they are exposed to, and, consequently, whether they can shed new light on long-standing debates concerning the innate structure necessary for language acquisition. In this article, we survey representative studies of the syntactic abilities of deep networks, and discuss the broader implications that this work has for theoretical linguistics.', 'corpus_id': 216080851, 'score': 0}, {'doc_id': '214742998', 'title': 'Give your Text Representation Models some Love: the Case for Basque', 'abstract': 'Word embeddings and pre-trained language models allow to build rich representations of text and have enabled improvements across most NLP tasks. Unfortunately they are very expensive to train, and many small companies and research groups tend to use models that have been pre-trained and made available by third parties, rather than building their own. This is suboptimal as, for many languages, the models have been trained on smaller (or lower quality) corpora. In addition, monolingual pre-trained models for non-English languages are not always available. At best, models for those languages are included in multilingual versions, where each language shares the quota of substrings and parameters with the rest of the languages. This is particularly true for smaller languages such as Basque. In this paper we show that a number of monolingual models (FastText word embeddings, FLAIR and BERT language models) trained with larger Basque corpora produce much better results than publicly available versions in downstream NLP tasks, including topic classification, sentiment classification, PoS tagging and NER. This work sets a new state-of-the-art in those tasks for Basque. All benchmarks and models used in this work are publicly available.', 'corpus_id': 214742998, 'score': 0}, {'doc_id': '209202876', 'title': 'Just Add Functions: A Neural-Symbolic Language Model', 'abstract': ""Neural network language models (NNLMs) have achieved ever-improving accuracy due to more sophisticated architectures and increasing amounts of training data. However, the inductive bias of these models (formed by the distributional hypothesis of language), while ideally suited to modeling most running text, results in key limitations for today's models. In particular, the models often struggle to learn certain spatial, temporal, or quantitative relationships, which are commonplace in text and are second-nature for human readers. Yet, in many cases, these relationships can be encoded with simple mathematical or logical expressions. How can we augment today's neural models with such encodings?In this paper, we propose a general methodology to enhance the inductive bias of NNLMs by incorporating simple functions into a neural architecture to form a hierarchical neural-symbolic language model (NSLM). These functions explicitly encode symbolic deterministic relationships to form probability distributions over words. We explore the effectiveness of this approach on numbers and geographic locations, and show that NSLMs significantly reduce perplexity in small-corpus language modeling, and that the performance improvement persists for rare tokens even on much larger corpora. The approach is simple and general, and we discuss how it can be applied to other word classes beyond numbers and geography."", 'corpus_id': 209202876, 'score': 1}, {'doc_id': '202583694', 'title': 'Do NLP Models Know Numbers? Probing Numeracy in Embeddings', 'abstract': 'The ability to understand and work with numbers (numeracy) is critical for many complex reasoning tasks. Currently, most NLP models treat numbers in text in the same way as other tokens---they embed them as distributed vectors. Is this enough to capture numeracy? We begin by investigating the numerical reasoning capabilities of a state-of-the-art question answering model on the DROP dataset. We find this model excels on questions that require numerical reasoning, i.e., it already captures numeracy. To understand how this capability emerges, we probe token embedding methods (e.g., BERT, GloVe) on synthetic list maximum, number decoding, and addition tasks. A surprising degree of numeracy is naturally present in standard embeddings. For example, GloVe and word2vec accurately encode magnitude for numbers up to 1,000. Furthermore, character-level embeddings are even more precise---ELMo captures numeracy the best for all pre-trained methods---but BERT, which uses sub-word units, is less exact.', 'corpus_id': 202583694, 'score': 1}, {'doc_id': '216080804', 'title': 'DuReaderrobust: A Chinese Dataset Towards Evaluating the Robustness of Machine Reading Comprehension Models', 'abstract': 'Machine Reading Comprehension (MRC) is a crucial and challenging task in natural language processing. Although several MRC models obtains human parity performance on several datasets, we find that these models are still far from robust. To comprehensively evaluate the robustness of MRC models, we create a Chinese dataset, namely DuReader_{robust}. It is designed to challenge MRC models from the following aspects: (1) over-sensitivity, (2) over-stability and (3) generalization. Most of previous work studies these problems by altering the inputs to unnatural texts. By contrast, the advantage of DuReader_{robust} is that its questions and documents are natural texts. It presents the robustness challenges when applying MRC models to real-world applications. The experimental results show that MRC models based on the pre-trained language models perform much worse than human does on the robustness test set, although they perform as well as human on in-domain test set. Additionally, we analyze the behavior of existing models on the robustness test set, which might give suggestions for future model development. The dataset and codes are available at \\url{this https URL}', 'corpus_id': 216080804, 'score': 0}, {'doc_id': '29150573', 'title': 'Numeracy for Language Models: Evaluating and Improving their Ability to Predict Numbers', 'abstract': 'Numeracy is the ability to understand and work with numbers. It is a necessary skill for composing and understanding documents in clinical, scientific, and other technical domains. In this paper, we explore different strategies for modelling numerals with language models, such as memorisation and digit-by-digit composition, and propose a novel neural architecture that uses a continuous probability density function to model numerals from an open vocabulary. Our evaluation on clinical and scientific datasets shows that using hierarchical models to distinguish numerals from words improves a perplexity metric on the subset of numerals by 2 and 4 orders of magnitude, respectively, over non-hierarchical models. A combination of strategies can further improve perplexity. Our continuous probability density function model reduces mean absolute percentage errors by 18% and 54% in comparison to the second best strategy for each dataset, respectively.', 'corpus_id': 29150573, 'score': 1}]"
54	phewas research feed	8345da7483758c541566f955aa81bd42	4672	{}	"[{'doc_id': '212675176', 'title': 'Efficient testing and effect size estimation for set-based genetic association inference via semiparametric multilevel mixture modeling: Application to a genome-wide association study of coronary artery disease', 'abstract': 'In genetic association studies, rare variants with extremely small allele frequency play a crucial role in complex traits, and the set-based testing methods that jointly assess the effects of groups of single nucleotide polymorphisms (SNPs) were developed to improve powers for the association tests. However, the powers of these tests are still severely limited due to the extremely small allele frequency, and precise estimations for the effect sizes of individual SNPs are substantially impossible. In this article, we provide an efficient set-based inference framework that addresses the two important issues simultaneously based on a Bayesian semiparametric multilevel mixture model. We propose to use the multilevel hierarchical model that incorporate the variations in set-specific effects and variant-specific effects, and to apply the optimal discovery procedure (ODP) that achieves the largest overall power in multiple significance testing. In addition, we provide Bayesian optimal ""set-based"" estimator of the empirical distribution of effect sizes. Efficiency of the proposed methods is demonstrated through application to a genome-wide association study of coronary artery disease (CAD), and through simulation studies. These results suggested there could be a lot of rare variants with large effect sizes for CAD, and the number of significant sets detected by the ODP was much greater than those by existing methods.', 'corpus_id': 212675176, 'score': 1}, {'doc_id': '216588694', 'title': 'Regional differences in reported Covid-19 cases show genetic correlations with higher socio-economic status and better health, potentially confounding studies on the genetics of disease susceptibility', 'abstract': 'Background: In March 2020, England showed a rapid increase in Covid-19 cases. Susceptibility for infectious diseases like Covid-19 is likely to be partly genetic. Mapping the genetic susceptibility for Covid-19 outcomes may reveal biological mechanisms that could potentially aid in drug or vaccine developments. However, as the disease spreads unevenly across the country, regional allele frequency differences could become spuriously associated with disease prevalence. Methods: A regional genome-wide association study (RGWAS) was conducted in 396,042 individuals from England to investigate the association between 1.2 million genetic variants and regional differences in daily reported Covid-19 cases from March 1st to April 18th 2020. Results: The polygenic signal increases during the first weeks of March, peaking at March 13th with the measured genetic variants explaining ~3% of the variance, including two genome-wide significant loci. The explained variance starts to drop at the end of March and reaches almost zero on April 18th. The majority of this temporary polygenic signal is due to genes associated with higher educational attainment and better health. Conclusions: The temporary positive relationship between Covid-19 cases and regional socio-economic status (SES) at the beginning of the Covid-19 outbreak may reflect 1) a higher degree of international travelers, 2) more social contacts, and/or 3) better testing capacities in higher SES regions. These signals are in the opposite direction of expected disease risk increasing effects, which has the potential to cancel out signals of interest. Genetic association studies should be aware of the timing and location of cases as this can introduce interfering polygenic signals that reflect regional differences in genes associated with behavior.', 'corpus_id': 216588694, 'score': 0}, {'doc_id': '216128436', 'title': 'Lack of association between genetic variants at ACE2 and TMPRSS2 genes involved in SARS-CoV-2 infection and human quantitative phenotypes', 'abstract': ""Coronavirus disease 2019 (COVID-19) shows a wide variation in expression and severity of symptoms, from very mild or no symptomes, to flu-like symptoms, and in more severe cases, to pneumonia, acute respiratory distress syndrome and even death. Large differences in outcome have also been observed between males and females. The causes for this variability are likely to be multifactorial, and to include genetics. The SARS-CoV-2 virus responsible for the infection uses the human receptor angiotensin converting enzyme 2 (ACE2) for cell invasion, and the serine protease TMPRSS2 for S protein priming. Genetic variation in these two genes may thus modulate an individual's genetic predisposition to infection and virus clearance. While genetic data on COVID-19 patients is being gathered, we carried out a phenome-wide association scan (PheWAS) to investigate the role of these genes in other human phenotypes in the general population. We examined 178 quantitative phenotypes including cytokines and cardio-metabolic biomarkers, as well as 58 medications in 36,339 volunteers from the Lifelines population biobank, in relation to 1,273 genetic variants located in or near ACE2 and TMPRSS2. While none reached our threshold for significance, we observed a suggestive association of polymorphisms within the ACE2 gene with (1) the use of ARBs combination therapies (p=5.7x10-4), an association that is significantly stronger in females (pdiff=0.01), and (2) with the use of non-steroid anti-inflammatory and antirheumatic products (p=5.5x10-4). While these associations need to be confirmed in larger sample sizes, they suggest that these variants play a role in diseases such as hypertension and chronic inflammation that are often observed in the more severe COVID-19 cases. Further investigation of these genetic variants in the context of COVID-19 is thus promising for better understanding of disease variability. Full results are available at https://covid19research.nl."", 'corpus_id': 216128436, 'score': 0}, {'doc_id': '211205002', 'title': 'A Bayes factor approach with informative prior for rare genetic variant analysis from next generation sequencing data.', 'abstract': 'The discovery of rare genetic variants through Next Generation Sequencing is a very challenging issue in the field of human genetics. We propose a novel region-based statistical approach based on a Bayes Factor (BF) to assess evidence of association between a set of rare variants (RVs) located on the same genomic region and a disease outcome in the context of case-control design. Marginal likelihoods are computed under the null and alternative hypotheses assuming a binomial distribution for the RV count in the region and a beta or mixture of Dirac and beta prior distribution for the probability of RV. We derive the theoretical null distribution of the BF under our prior setting and show that a Bayesian control of the False Discovery Rate (BFDR) can be obtained for genome-wide inference. Informative priors are introduced using prior evidence of association from a Kolmogorov-Smirnov test statistic. We use our simulation program, sim1000G, to generate RV data similar to the 1,000 genomes sequencing project. Our simulation studies showed that the new BF statistic outperforms standard methods (SKAT, SKAT-O, Burden test) in case-control studies with moderate sample sizes and is equivalent to them under large sample size scenarios. Our real data application to a lung cancer case-control study found enrichment for RVs in known and novel cancer genes. It also suggests that using the BF with informative prior improves the overall gene discovery compared to the BF with non-informative\xa0prior. This article is protected by copyright. All rights reserved.', 'corpus_id': 211205002, 'score': 1}, {'doc_id': '8138497', 'title': 'Systematic comparison of phenome-wide association study of electronic medical record data and genome-wide association study data', 'abstract': 'Candidate gene and genome-wide association studies (GWAS) have identified genetic variants that modulate risk for human disease; many of these associations require further study to replicate the results. Here we report the first large-scale application of the phenome-wide association study (PheWAS) paradigm within electronic medical records (EMRs), an unbiased approach to replication and discovery that interrogates relationships between targeted genotypes and multiple phenotypes. We scanned for associations between 3,144 single-nucleotide polymorphisms (previously implicated by GWAS as mediators of human traits) and 1,358 EMR-derived phenotypes in 13,835 individuals of European ancestry. This PheWAS replicated 66% (51/77) of sufficiently powered prior GWAS associations and revealed 63 potentially pleiotropic associations with P < 4.6 × 10−6 (false discovery rate < 0.1); the strongest of these novel associations were replicated in an independent cohort (n = 7,406). These findings validate PheWAS as a tool to allow unbiased interrogation across multiple phenotypes in EMR-based cohorts and to enhance analysis of the genomic basis of human disease.', 'corpus_id': 8138497, 'score': 1}, {'doc_id': '212628398', 'title': 'Gene-Environment Interaction: A Variable Selection Perspective.', 'abstract': 'Gene-environment interactions have important implications for elucidating the genetic basis of complex diseases beyond the joint function of multiple genetic factors and their interactions (or epistasis). In the past, G\xa0×\xa0E interactions have been mainly conducted within the framework of genetic association studies. The high dimensionality of G\xa0×\xa0E interactions, due to the complicated form of environmental effects and the presence of a large number of genetic factors including gene expressions and SNPs, has motivated the recent development of penalized variable selection methods for dissecting G\xa0×\xa0E interactions, which has been ignored in the majority of published reviews on genetic interaction studies. In this article, we first survey existing studies on both gene-environment and gene-gene interactions. Then, after a brief introduction to the variable selection methods, we review penalization and relevant variable selection methods in marginal and joint paradigms, respectively, under a variety of conceptual models. Discussions on strengths and limitations, as well as computational aspects of the variable selection methods tailored for G\xa0×\xa0E studies, have also been provided.', 'corpus_id': 212628398, 'score': 1}, {'doc_id': '216071787', 'title': 'Impacts of genomic networks governed by human-specific regulatory sequences and genetic loci harboring fixed human-specific neuro-regulatory single nucleotide mutations on phenotypic traits of Modern Humans', 'abstract': 'Recent advances in identification and characterization of human-specific regulatory DNA sequences set the stage for the assessment of their global impact on physiology and pathology of Modern Humans. Gene set enrichment analyses (GSEA) of 8,405 genes linked with 35,074 human-specific neuro-regulatory single-nucleotide changes (hsSNCs) revealed a staggering breadth of significant associations with morphological structures, physiological processes, and pathological conditions of Modern Humans. Significantly enriched traits include more than 1,000 anatomically-distinct regions of the adult human brain, many different types of cells and tissues, more than 200 common human disorders and more than 1,000 records of rare diseases. Thousands of genes connected with neuro-regulatory hsSNCs have been identified, which represent essential genetic elements of the autosomal inheritance and offspring survival phenotypes. A total of 1,494 hsSNC- linked genes are associated with either autosomal dominant or recessive inheritance and 2,273 hsSNC-linked genes have been associated with premature death, embryonic lethality, as well as pre-, peri-, neo-, and post-natal lethality phenotypes of both complete and incomplete penetrance. Differential GSEA implemented on hsSNC-linked loci and associated genes identify 7,990 genes linked to evolutionary distinct classes of human-specific regulatory sequences (HSRS), expression of a majority of which (5,389 genes; 67%) is regulated by stem cell-associated retroviral sequences (SCARS). Interrogations of the MGI database revealed readily available mouse models tailored for precise experimental definitions of functional effects of hsSNCs and SCARS on genes causally affecting thousands of mammalian phenotypes and implicated in hundreds of common and rare human disorders. These observations suggest that a preponderance of human-specific traits evolved under a combinatorial regulatory control of HSRS and neuro-regulatory loci harboring hsSNCs that are fixed in humans, distinct from other primates, and located in differentially-accessible chromatin regions during brain development.', 'corpus_id': 216071787, 'score': 0}, {'doc_id': '215768703', 'title': 'Mendelian randomization and causal networks for systematic analysis of omics.', 'abstract': 'Mendelian randomization implemented through instrumental variable analysis is frequently discussed in causality and recently the number of applications on real data is increasing. However, there are very few discussions to address modern biomedical questions such as the integration of large scale omics in causality. While in the age of large omics, we face several hundred or thousands of components with little knowledge about the underlying structures, the focus of the field is on small scales and mostly with known structures. The availability of large omic data accentuates the need for techniques to identify interconnectivity among the omic components and reveal the principles that govern the relationships. This study extends instrumental variable techniques to identify causal networks in large scales and assess the assumptions. Large-scale causal networks are complex and further analyses are required to uncover mechanisms by which the components are related within and between omics and linked to disease endpoints. This study will review these utilities of causal networks for mechanistic understanding.', 'corpus_id': 215768703, 'score': 1}, {'doc_id': '216553354', 'title': 'Addressing Ancestry Disparities in Genomic Medicine: A Geographic-aware Algorithm', 'abstract': 'With declining sequencing costs a promising and affordable tool is emerging in cancer diagnostics: genomics. By using association studies, genomic variants that predispose patients to specific cancers can be identified, while by using tumor genomics cancer types can be characterized for targeted treatment. However, a severe disparity is rapidly emerging in this new area of precision cancer diagnosis and treatment planning, one which separates a few genetically well-characterized populations (predominantly European) from all other global populations. Here we discuss the problem of population-specific genetic associations, which is driving this disparity, and present a novel solution--coordinate-based local ancestry--for helping to address it. We demonstrate our boosting-based method on whole genome data from divergent groups across Africa and in the process observe signals that may stem from the transcontinental Bantu-expansion.', 'corpus_id': 216553354, 'score': 0}, {'doc_id': '215782092', 'title': 'ACE inhibition and cardiometabolic risk factors, lung ACE2 and TMPRSS2 gene expression, and plasma ACE2 levels: a Mendelian randomization study', 'abstract': 'Objectives: To use human genetic variants that proxy angiotensin-converting enzyme (ACE) inhibitor drug effects and cardiovascular risk factors to provide insight into how these exposures affect lung ACE2 and TMPRSS2 gene expression and circulating ACE2 levels. Design: Two-sample Mendelian randomization (MR) analysis. Setting: Summary-level genetic association data. Participants: Participants were predominantly of European ancestry. Variants that proxy ACE inhibitor drug effects and cardiometabolic risk factors (body mass index, chronic obstructive pulmonary disease, lifetime smoking index, low-density lipoprotein cholesterol, systolic blood pressure and type 2 diabetes mellitus) were selected from publicly available genome-wide association study data (sample sizes ranging from 188,577 to 898,130 participants). Genetic association estimates for lung expression of ACE2 and TMPRSS2 were obtained from the Gene-Tissue Expression (GTEx) project (515 participants) and the Lung eQTL Consortium (1,038 participants). Genetic association estimates for circulating plasma ACE2 levels were obtained from the INTERVAL study (4,947 participants). Main outcomes and measures: Lung ACE2 and TMPRSS2 expression and plasma ACE2 levels. Results: There were no association of genetically proxied ACE inhibition with any of the outcomes considered here. There was evidence of a positive association of genetic liability to type 2 diabetes mellitus with lung ACE2 gene expression in GTEx (p = 4x10-4) and with circulating plasma ACE2 levels in INTERVAL (p = 0.03), but not with lung ACE2 expression in the Lung eQTL Consortium study (p = 0.68). There were no associations between genetically predicted levels of the other cardiometabolic traits with the outcomes. Conclusions: This study does not provide evidence to support that ACE inhibitor antihypertensive drugs affect lung ACE2 and TMPRSS2 expression or plasma ACE2 levels. In the current COVID-19 pandemic, our findings do not support a change in ACE inhibitor medication use without clinical justification.', 'corpus_id': 215782092, 'score': 0}]"
55	De-Identification of EHRs	36d2fd09a81ddc40fa199c24a6d6a753	16783	{}	[{'doc_id': '232414659', 'title': 'De-identifying Spanish medical texts - named entity recognition applied to radiology reports', 'abstract': 'Background Medical texts such as radiology reports or electronic health records are a powerful source of data for researchers. Anonymization methods must be developed to de-identify documents containing personal information from both patients and medical staff. Although currently there are several anonymization strategies for the English language, they are also language-dependent. Here, we introduce a named entity recognition strategy for Spanish medical texts, translatable to other languages. Results We tested 4 neural networks on our radiology reports dataset, achieving a recall of 97.18% of the identifying entities. Alongside, we developed a randomization algorithm to substitute the detected entities with new ones from the same category, making it virtually impossible to differentiate real data from synthetic data. The three best architectures were tested with the MEDDOCAN challenge dataset of electronic health records as an external test, achieving a recall of 69.18%. Conclusions The strategy proposed, combining named entity recognition tasks with randomization of entities, is suitable for Spanish radiology reports. It does not require a big training corpus, thus it could be easily extended to other languages and medical texts, such as electronic health records.', 'corpus_id': 232414659, 'score': 1}, {'doc_id': '232352634', 'title': 'Benchmarking Modern Named Entity Recognition Techniques for Free-text Health Record De-identification', 'abstract': 'Electronic Health Records (EHRs) have become the primary form of medical data-keeping across the United States. Federal law restricts the sharing of any EHR data that contains protected health information (PHI). De-identification, the process of identifying and removing all PHI, is crucial for making EHR data publicly available for scientific research. This project explores several deep learning-based named entity recognition (NER) methods to determine which method(s) perform better on the de-identification task. We trained and tested our models on the i2b2 training dataset, and qualitatively assessed their performance using EHR data collected from a local hospital. We found that 1) BiLSTM-CRF represents the best-performing encoder/decoder combination, 2) character-embeddings and CRFs tend to improve precision at the price of recall, and 3) transformers alone under-perform as context encoders. Future work focused on structuring medical text may improve the extraction of semantic and syntactic information for the purposes of EHR deidentification.', 'corpus_id': 232352634, 'score': 1}, {'doc_id': '218974523', 'title': 'A Semi-supervised Approach for De-identification of Swedish Clinical Text', 'abstract': 'An abundance of electronic health records (EHR) is produced every day within healthcare. The records possess valuable information for research and future improvement of healthcare. Multiple efforts have been done to protect the integrity of patients while making electronic health records usable for research by removing personally identifiable information in patient records. Supervised machine learning approaches for de-identification of EHRs need annotated data for training, annotations that are costly in time and human resources. The annotation costs for clinical text is even more costly as the process must be carried out in a protected environment with a limited number of annotators who must have signed confidentiality agreements. In this paper is therefore, a semi-supervised method proposed, for automatically creating high-quality training data. The study shows that the method can be used to improve recall from 84.75% to 89.20% without sacrificing precision to the same extent, dropping from 95.73% to 94.20%. The model’s recall is arguably more important for de-identification than precision.', 'corpus_id': 218974523, 'score': 1}, {'doc_id': '233744760', 'title': 'A Hybrid Model for Named Entity Recognition on Chinese Electronic Medical Records', 'abstract': 'Electronic medical records (EMRs) contain valuable information about the patients, such as clinical symptoms, diagnostic results, and medications. Named entity recognition (NER) aims to recognize entities from unstructured text, which is the initial step toward the semantic understanding of the EMRs. Extracting medical information from Chinese EMRs could be a more complicated task because of the difference between English and Chinese. Some researchers have noticed the importance of Chinese NER and used the recurrent neural network or convolutional neural network (CNN) to deal with this task. However, it is interesting to know whether the performance could be improved if the advantages of the RNN and CNN can be both utilized. Moreover, RoBERTa-WWM, as a pre-training model, can generate the embeddings with word-level features, which is more suitable for Chinese NER compared with Word2Vec. In this article, we propose a hybrid model. This model first obtains the entities identified by bidirectional long short-term memory and CNN, respectively, and then uses two hybrid strategies to output the final results relying on these entities. We also conduct experiments on raw medical records from real hospitals. This dataset is provided by the China Conference on Knowledge Graph and Semantic Computing in 2019 (CCKS 2019). Results demonstrate that the hybrid model can improve performance significantly.', 'corpus_id': 233744760, 'score': 0}, {'doc_id': '233421290', 'title': 'Ensemble of deep masked language models for effective named entity recognition in multi-domain corpora', 'abstract': 'The health and life science domains are well-known for their wealth of entities. These entities are presented as free text in large corpora, such as biomedical scientific and electronic health records. To enable the secondary use of these corpora and unlock their value, named entity recognition (NER) methods are proposed. Inspired by the success of deep masked language models, we present an ensemble approach for NER using these models. Results show statistically significant improvement of the ensemble models over baselines based on individual models in multiple domains - chemical, clinical and wet lab - and languages - English and French. The ensemble model achieves an overall performance of 79.2% macro F1-score, a 4.6 percentage point increase upon the baseline in multiple domains and languages. These results suggests that ensembles are a more effective strategy for tackling NER. We further perform a detailed analysis of their performance based on a set of entity properties.', 'corpus_id': 233421290, 'score': 0}, {'doc_id': '233742517', 'title': 'Extracting Drug Names and Associated Attributes From Discharge Summaries: Text Mining Study', 'abstract': 'Background Drug prescriptions are often recorded in free-text clinical narratives; making this information available in a structured form is important to support many health-related tasks. Although several natural language processing (NLP) methods have been proposed to extract such information, many challenges remain. Objective This study evaluates the feasibility of using NLP and deep learning approaches for extracting and linking drug names and associated attributes identified in clinical free-text notes and presents an extensive error analysis of different methods. This study initiated with the participation in the 2018 National NLP Clinical Challenges (n2c2) shared task on adverse drug events and medication extraction. Methods The proposed system (DrugEx) consists of a named entity recognizer (NER) to identify drugs and associated attributes and a relation extraction (RE) method to identify the relations between them. For NER, we explored deep learning-based approaches (ie, bidirectional long-short term memory with conditional random fields [BiLSTM-CRFs]) with various embeddings (ie, word embedding, character embedding [CE], and semantic-feature embedding) to investigate how different embeddings influence the performance. A rule-based method was implemented for RE and compared with a context-aware long-short term memory (LSTM) model. The methods were trained and evaluated using the 2018 n2c2 shared task data. Results The experiments showed that the best model (BiLSTM-CRFs with pretrained word embeddings [PWE] and CE) achieved lenient micro F-scores of 0.921 for NER, 0.927 for RE, and 0.855 for the end-to-end system. NER, which relies on the pretrained word and semantic embeddings, performed better on most individual entity types, but NER with PWE and CE had the highest classification efficiency among the proposed approaches. Extracting relations using the rule-based method achieved higher accuracy than the context-aware LSTM for most relations. Interestingly, the LSTM model performed notably better in the reason-drug relations, the most challenging relation type. Conclusions The proposed end-to-end system achieved encouraging results and demonstrated the feasibility of using deep learning methods to extract medication information from free-text data.', 'corpus_id': 233742517, 'score': 0}, {'doc_id': '232283498', 'title': 'A BERT-BiGRU-CRF Model for Entity Recognition of Chinese Electronic Medical Records', 'abstract': 'Because of difficulty processing the electronic medical record data of patients with cerebrovascular disease, there is little mature recognition technology capable of identifying the named entity of cerebrovascular disease. Excellent research results have been achieved in the field of named entity recognition (NER), but there are several problems in the pre processing of Chinese named entities that have multiple meanings, of which neglecting the combination of contextual information is one. +erefore, to extract five categories of key entity information for diseases, symptoms, body parts, medical examinations, and treatment in electronic medical records, this paper proposes the use of a BERT-BiGRU-CRF named entity recognition method, which is applied to the field of cerebrovascular diseases. +e BERT layer first converts the electronic medical record text into a low-dimensional vector, then uses this vector as the input to the BiGRU layer to capture contextual features, and finally uses conditional random fields (CRFs) to capture the dependency between adjacent tags. +e experimental results show that the F1 score of the model reaches 90.38%.', 'corpus_id': 232283498, 'score': 0}, {'doc_id': '232369347', 'title': 'AI-NLM exploration of the Acronym Identification Shared Task at SDU@AAAI-21', 'abstract': 'National Library of Medicine has developed systems for recognition of named entities in biomedical and clinical text. The systems are primarily leveraging the Unified Medical Language System (UMLS) to recognize the terms and link them to the terminology part of the UMLS (Metathesaurus.) Biomedical and clinical texts are rife with acronyms and abbreviations. Acronym identification and disambiguation play, therefore, an important role in processing of the text using the UMLS-based approaches. To test the existing rule-based approaches developed at NLM and to explore the state-ofthe-art DL approaches, we participated in the SDU Acronym Identification shared task. Not surprisingly, our existing rulebased approach achieved high precision (over 96%), but had very low recall, whereas, the LSTM and BERT-based approaches had almost equal recall and precision and achieved F1 scores in the low 90s.', 'corpus_id': 232369347, 'score': 0}, {'doc_id': '221494492', 'title': 'Survey on RNN and CRF models for de-identification of medical free text', 'abstract': 'The increasing reliance on electronic health record (EHR) in areas such as medical research should be addressed by using ample safeguards for patient privacy. These records often tend to be big data, and given that a significant portion is stored as free (unstructured) text, we decided to examine relevant work on automated free text de-identification with recurrent neural network (RNN) and conditional random field (CRF) approaches. Both methods involve machine learning and are widely used for the removal of protected health information (PHI) from free text. The outcome of our survey work produced several informative findings. Firstly, RNN models, particularly long short-term memory (LSTM) algorithms, generally outperformed CRF models and also other systems, namely rule-based algorithms. Secondly, hybrid or ensemble systems containing joint LSTM-CRF models showed no advantage over individual LSTM and CRF models. Thirdly, overfitting may be an issue when customized de-identification datasets are used during model training. Finally, statistical validation of performance scores and diversity during experimentation were largely ignored. In our comprehensive survey, we also identify major research gaps that should be considered for future work.', 'corpus_id': 221494492, 'score': 1}, {'doc_id': '174778554', 'title': 'De-identification of French medical narratives', 'abstract': 'In this work, a rule-based method for the de-identification of French free-text medical data using natural language processing (NLP) tools is presented.', 'corpus_id': 174778554, 'score': 1}]
56	biomedical-measurement	cefeff6506e5cb1de70ee9c4622410db	14536	{}	"[{'doc_id': '227166262', 'title': 'A Three-Dimensional Deep Convolutional Neural Network for Automatic Segmentation and Diameter Measurement of Type B Aortic Dissection', 'abstract': 'Objective To provide an automatic method for segmentation and diameter measurement of type B aortic dissection (TBAD). Materials and Methods Aortic computed tomography angiographic images from 139 patients with TBAD were consecutively collected. We implemented a deep learning method based on a three-dimensional (3D) deep convolutional neural (CNN) network, which realizes automatic segmentation and measurement of the entire aorta (EA), true lumen (TL), and false lumen (FL). The accuracy, stability, and measurement time were compared between deep learning and manual methods. The intra- and inter-observer reproducibility of the manual method was also evaluated. Results The mean dice coefficient scores were 0.958, 0.961, and 0.932 for EA, TL, and FL, respectively. There was a linear relationship between the reference standard and measurement by the manual and deep learning method (r = 0.964 and 0.991, respectively). The average measurement error of the deep learning method was less than that of the manual method (EA, 1.64% vs. 4.13%; TL, 2.46% vs. 11.67%; FL, 2.50% vs. 8.02%). Bland-Altman plots revealed that the deviations of the diameters between the deep learning method and the reference standard were −0.042 mm (−3.412 to 3.330 mm), −0.376 mm (−3.328 to 2.577 mm), and 0.026 mm (−3.040 to 3.092 mm) for EA, TL, and FL, respectively. For the manual method, the corresponding deviations were −0.166 mm (−1.419 to 1.086 mm), −0.050 mm (−0.970 to 1.070 mm), and −0.085 mm (−1.010 to 0.084 mm). Intra- and inter-observer differences were found in measurements with the manual method, but not with the deep learning method. The measurement time with the deep learning method was markedly shorter than with the manual method (21.7 ± 1.1 vs. 82.5 ± 16.1 minutes, p < 0.001). Conclusion The performance of efficient segmentation and diameter measurement of TBADs based on the 3D deep CNN was both accurate and stable. This method is promising for evaluating aortic morphology automatically and alleviating the workload of radiologists in the near future.', 'corpus_id': 227166262, 'score': 1}, {'doc_id': '231986585', 'title': 'A Deep Learning-based Method to Extract Lumen and Media-Adventitia in Intravascular Ultrasound Images', 'abstract': 'Intravascular ultrasound (IVUS) imaging allows direct visualization of the coronary vessel wall and is suitable for the assessment of atherosclerosis and the degree of stenosis. Accurate segmentation and measurements of lumen and medianadventitia (MA) from IVUS are essential for such a successful clinical evaluation. However, current segmentation relies on manual operations, which is time-consuming and user-dependent. In this paper, we aim to develop a deep learning-based method using an encoder-decoder deep architecture to automatically extract both lumen and MA border. Our method named IVUS-U-Net++ is an extension of the well-known UNet++ model. More specifically, a feature pyramid network was added to the U-Net++ model, enabling the utilization of feature maps at different scales. As a result, the accuracy of the probability map and subsequent segmentation have been improved We collected 1746 IVUS images from 18 patients in this study. The whole dataset was split into a training dataset (1572 images) for the 10-fold cross-validation and a test dataset (174 images) for evaluating the performance of models. Our IVUS-U-Net++ segmentation model achieved a Jaccard measure (JM) of 0.9412, a Hausdorff distance (HD) of 0.0639 mm for the lumen border, and a JM of 0.9509, an HD of 0.0867 mm for the MA border, respectively. Moreover, the Pearson correlation and Bland-Altman analyses were performed to evaluate the correlations of 12 clinical parameters measured from our segmentation results and the ground truth, and automatic measurements agreed well with those from the ground truth (all Ps<0.01). In conclusion, our preliminary results demonstrate that the proposed IVUS-U-Net++ model has great promise for clinical use.', 'corpus_id': 231986585, 'score': 0}, {'doc_id': '218686895', 'title': 'Deep learning enables genetic analysis of the human thoracic aorta', 'abstract': 'The aorta is the largest blood vessel in the body, and enlargement or aneurysm of the aorta can predispose to dissection, an important cause of sudden death. While rare syndromes have been identified that predispose to aortic aneurysm, the common genetic basis for the size of the aorta remains largely unknown. By leveraging a deep learning architecture that was originally developed to recognize natural images, we trained a model to evaluate the dimensions of the ascending and descending thoracic aorta in cardiac magnetic resonance imaging. After manual annotation of just 116 samples, we applied this model to 3,840,140 images from the UK Biobank. We then conducted a genome-wide association study in 33,420 individuals, revealing 68 loci associated with ascending and 35 with descending thoracic aortic diameter, of which 10 loci overlapped. Integration of common variation with transcriptome-wide analyses, rare-variant burden tests, and single nucleus RNA sequencing prioritized SVIL, a gene highly expressed in vascular smooth muscle, that was significantly associated with the diameter of the ascending and descending aorta. A polygenic score for ascending aortic diameter was associated with a diagnosis of thoracic aortic aneurysm in the remaining 391,251 UK Biobank participants who did not undergo imaging (HR = 1.44 per standard deviation; P = 3.7·10−12). Defining the genetic basis of the diameter of the aorta may enable the identification of asymptomatic individuals at risk for aneurysm or dissection and facilitate the prioritization of potential therapeutic targets for the prevention or treatment of aortic aneurysm. Finally, our results illustrate the potential for rapidly defining novel quantitative traits derived from a deep learning model, an approach that can be more broadly applied to biomedical imaging data.', 'corpus_id': 218686895, 'score': 1}, {'doc_id': '73460553', 'title': 'Reconstruction of coronary circulation networks: A review of methods', 'abstract': 'Building anatomically accurate models of the coronary vascular system enables potentially deeper understandings of coronary circulation. To achieve this, (a) images at different levels of vascular network—arteries, arterioles, capillaries, venules, and veins—need to be obtained through suitable imaging modalities; and (b) from images, morphological and topological information needs to be extracted using image processing techniques. While there are several modalities that enable the imaging of large vessels, microcirculation imaging—capturing vessels having diameter lesser than 100 μm—has to date been typically confined to small regions of the heart. This spatially limited microcirculatory information has often been used within cardiac models, with the potentially erroneous assumption that it is representative of the whole organ. However, with the recent advancements in imaging and image processing, it is rapidly becoming feasible to acquire, process, and quantify microcirculation data at the scale of whole organ. In this review, we summarize the progress toward this goal followed through a presentation of the current state‐of‐the‐art imaging and image processing techniques in the context of coronary microcirculation extraction, prominently but not exclusively, from small animals.', 'corpus_id': 73460553, 'score': 1}, {'doc_id': '233263564', 'title': 'Deep learning-based techniques for the automatic segmentation of organs in thoracic computed tomography images: A Comparative study', 'abstract': 'Medical images have become the important part in medical diagnosis and treatment. These images play a significant role in medical field because the doctors are highly interested in exploring the anatomy of the human body. The medical images captured with various modalities like (PET, CT, SPECT, MRI, etc.) have different variability based on the intensity level. The segmentation of organs in medical images is the most crucial image-related application. Organs segmentation in the medical images help the doctors in planning the treatment in lesser time and with higher efficiency. Results of manual segmentation vary from experts to experts and it is very time taking task. Automatic segmentation is the solution to the problem as it gives precise results. Several techniques had been addressed in the literature for the segmentation of thoracic organs (heart, aorta, trachea, esophagus) automatically in the medical images. Out of those deep learning-based techniques outperformed in the automatic segmentation of organs by giving precise accuracy. Using deep learning models for segmentation purposes improve the segmentation results in various clinical applications. In this paper various deep learning-based techniques for automatic segmentation had been discussed. Also, the three authors’ results are compared based on the parameters such as Dice Coefficient (DC) and Hausdorff Metric (HM).', 'corpus_id': 233263564, 'score': 0}, {'doc_id': '232073100', 'title': 'Deep Learning Architectures and Techniques for Multi-organ Segmentation', 'abstract': 'Deep learning architectures used for automatic multi-organ segmentation in the medical field have gained increased attention in the last years as the results and achievements outweighed the older techniques. Due to improvements in the computer hardware and the development of specialized network designs, deep learning segmentation presents exciting developments and opportunities also for future research. Therefore, we have compiled a review of the most interesting deep learning architectures applicable to medical multi-organ segmentation. We have summarized over 50 contributions, most of which are more recent than 3 years. The papers were grouped into three categories based on the architecture: “Convolutional Neural Networks” (CNNs), “Fully Convolutional Neural Networks” (FCNs) and hybrid architectures that combine more designs - including “Generative Adversarial Networks” (GANs) or “Recurrent Neural Networks” (RNNs). Afterwards we present the most used multi-organ datasets, and we finalize by making a general discussion of current shortcomings and future potential research paths.', 'corpus_id': 232073100, 'score': 0}, {'doc_id': '211068999', 'title': 'A Deep Learning Approach to Automate High-Resolution Blood Vessel Reconstruction on Computerized Tomography Images With or Without the Use of Contrast Agent', 'abstract': 'Existing methods to reconstruct vascular structures from a computed tomography (CT) angiogram rely on injection of intravenous contrast to enhance the radio-density within the vessel lumen. However, pathological changes can be present in the blood lumen, vessel wall or a combination of both that prevent accurate reconstruction. In the example of aortic aneurysmal disease, a blood clot or thrombus adherent to the aortic wall within the expanding aneurysmal sac is present in 70-80% of cases. These deformations prevent the automatic extraction of vital clinically relevant information by current methods. In this study, we implemented a modified U-Net architecture with attention-gating to establish a high-throughput and automated segmentation pipeline of pathological blood vessels in CT images acquired with or without the use of a contrast agent. Twenty-six patients with paired non-contrast and contrast-enhanced CT images within the ongoing Oxford Abdominal Aortic Aneurysm (OxAAA) study were randomly selected, manually annotated and used for model training and evaluation (13/13). Data augmentation methods were implemented to diversify the training data set in a ratio of 10:1. The performance of our Attention-based U-Net in extracting both the inner lumen and the outer wall of the aortic aneurysm from CT angiograms (CTA) was compared against a generic 3-D U-Net and displayed superior results. Subsequent implementation of this network architecture within the aortic segmentation pipeline from both contrast-enhanced CTA and non-contrast CT images has allowed for accurate and efficient extraction of the entire aortic volume. This extracted volume can be used to standardize current methods of aneurysmal disease management and sets the foundation for subsequent complex geometric and morphological analysis. Furthermore, the proposed pipeline can be extended to other vascular pathologies.', 'corpus_id': 211068999, 'score': 1}, {'doc_id': '232173251', 'title': 'Fully-automated global and segmental strain analysis of DENSE cardiovascular magnetic resonance using deep learning for segmentation and phase unwrapping', 'abstract': 'Cardiovascular magnetic resonance (CMR) cine displacement encoding with stimulated echoes (DENSE) measures heart motion by encoding myocardial displacement into the signal phase, facilitating high accuracy and reproducibility of global and segmental myocardial strain and providing benefits in clinical performance. While conventional methods for strain analysis of DENSE images are faster than those for myocardial tagging, they still require manual user assistance. The present study developed and evaluated deep learning methods for fully-automatic DENSE strain analysis. Convolutional neural networks (CNNs) were developed and trained to (a) identify the left-ventricular (LV) epicardial and endocardial borders, (b) identify the anterior right-ventricular (RV)-LV insertion point, and (c) perform phase unwrapping. Subsequent conventional automatic steps were employed to compute strain. The networks were trained using 12,415 short-axis DENSE images from 45 healthy subjects and 19 heart disease patients and were tested using 10,510 images from 25 healthy subjects and 19 patients. Each individual CNN was evaluated, and the end-to-end fully-automatic deep learning pipeline was compared to conventional user-assisted DENSE analysis using linear correlation and Bland Altman analysis of circumferential strain. LV myocardial segmentation U-Nets achieved a DICE similarity coefficient of 0.87\u2009±\u20090.04, a Hausdorff distance of 2.7\u2009±\u20091.0 pixels, and a mean surface distance of 0.41\u2009±\u20090.29 pixels in comparison with manual LV myocardial segmentation by an expert. The anterior RV-LV insertion point was detected within 1.38\u2009±\u20090.9 pixels compared to manually annotated data. The phase-unwrapping U-Net had similar or lower mean squared error vs. ground-truth data compared to the conventional path-following method for images with typical signal-to-noise ratio (SNR) or low SNR (p\u2009<\u20090.05), respectively. Bland–Altman analyses showed biases of 0.00\u2009±\u20090.03 and limits of agreement of − 0.04 to 0.05 or better for deep learning-based fully-automatic global and segmental end-systolic circumferential strain vs. conventional user-assisted methods. Deep learning enables fully-automatic global and segmental circumferential strain analysis of DENSE CMR providing excellent agreement with conventional user-assisted methods. Deep learning-based automatic strain analysis may facilitate greater clinical use of DENSE for the quantification of global and segmental strain in patients with cardiac disease.', 'corpus_id': 232173251, 'score': 0}, {'doc_id': '231971077', 'title': 'A computationally efficient approach to segmentation of the aorta and coronary arteries using deep learning', 'abstract': 'A fully automatic two-dimensional Unet model is proposed to segment aorta and coronary arteries in computed tomography images. Two models are trained to segment two regions of interest, (1) the aorta and the coronary arteries or (2) the coronary arteries alone. Our method achieves 91.20% and 88.80% dice similarity coefficient accuracy on regions of interest 1 and 2 respectively. Compared with a semi-automatic segmentation method, our model performs better when segmenting the coronary arteries alone. The performance of the proposed method is comparable to existing published two-dimensional or three-dimensional deep learning models. Furthermore, the algorithmic and graphical processing unit memory efficiencies are maintained such that the model can be deployed within hospital computer networks where graphical processing units are typically not available.', 'corpus_id': 231971077, 'score': 0}, {'doc_id': '212799954', 'title': 'Machine deep learning accurately detects endoleak after endovascular abdominal aortic aneurysm repair', 'abstract': ""Objective The objective of this study was to develop a machine deep learning algorithm for endoleak detection and measurement of aneurysm diameter, area, and volume from computed tomography angiography (CTA). Methods Digital Imaging and Communications in Medicine files representing three-phase postoperative CTA images (N = 334) of 191 unique patients undergoing endovascular aneurysm repair for infrarenal abdominal aortic aneurysm (AAA) with a variety of commercial devices were used to train a deep learning pipeline across four tasks. The RetinaNet object-detection convolutional neural network (CNN) architecture was trained to predict bounding boxes around the axial CTA slices that were then stitched together in two dimensions into a smaller region containing the aneurysm. Multiclass endoleak detection and segmentation of the AAA, endograft, and endoleak were performed on this smaller region. Segmentations on a single randomly selected contrast from each scan included 33 full and 68 partial segmentations for endograft and AAA and 99 full segmentations for endoleak. A modified version of ResNet-50 CNN was used to detect endoleak on individual axial slices. A three-dimensional U-Net CNN model was trained on the task of dense three-dimensional segmentation and used to measure diameter and volume with a specially designed loss function. We made use of fivefold cross-validation to evaluate model performance for each step, splitting training and testing data at each fold, such that multiple scans from the same patient were preserved with the same fold. Algorithm predictions for endoleak were compared with the radiology report and with a subset of CTA images independently read by two vascular specialists. Results The localization portion of the network accurately predicted a region of interest containing the AAA in 99% of cases. The best model of binary endoleak detection obtained an area under the receiver operating characteristic curve of 0.94 ± 0.03 with an optimized accuracy of 0.89 ± 0.03 on a balanced data set. An introduced postprocessing algorithm for determining maximum diameter was used on both the predicted AAA segmentation and ground truth segmentation, predicting on average an absolute diameter error of 2.3 ± 2.0 mm by 1.4 ± 1.7 mm for each measurement, respectively. The algorithm measured AAA and endograft volume accurately (Dice coefficient, 0.95 ± 0.2) with an absolute volume error of 10.1 ± 9.1 mL. The algorithm measured endoleak volume less accurately, with a Dice score of 0.53 ± 0.21 and an average absolute volume error of 1.2 ± 1.9 mL. Conclusions This machine learning algorithm shows promise in augmenting a human's ability to interpret postoperative CTA images and may help improve surveillance after endovascular aneurysm repair. External validation on larger data sets and prospective study are required before the algorithm can be clinically applicable."", 'corpus_id': 212799954, 'score': 1}]"
57	RLSys	041ca0162a64caa5c0bf63ba90dc66f8	16648	{}	[{'doc_id': '232222444', 'title': 'Large Batch Simulation for Deep Reinforcement Learning', 'abstract': 'We accelerate deep reinforcement learning-based training in visually complex 3D environments by two orders of magnitude over prior work, realizing end-to-end training speeds of over 19,000 frames of experience per second on a single GPU and up to 72,000 frames per second on a single eight-GPU machine. The key idea of our approach is to design a 3D renderer and embodied navigation simulator around the principle of “batch simulation”: accepting and executing large batches of requests simultaneously. Beyond exposing large amounts of work at once, batch simulation allows implementations to amortize in-memory storage of scene assets, rendering work, data loading, and synchronization costs across many simulation requests, dramatically improving the number of simulated agents per GPU and overall simulation throughput. To balance DNN inference and training costs with faster simulation, we also build a computationally efficient policy DNN that maintains high task performance, and modify training algorithms to maintain sample efficiency when training with large mini-batches. By combining batch simulation and DNN performance optimizations, we demonstrate that PointGoal navigation agents can be trained in complex 3D environments on a single GPU in 1.5 days to 97% of the accuracy of agents trained on a prior state-of-the-art system using a 64-GPU cluster over three days. We provide open-source reference implementations of our batch 3D renderer and simulator to facilitate incorporation of these ideas into RL systems.', 'corpus_id': 232222444, 'score': 1}, {'doc_id': '234097493', 'title': 'Reward prediction for representation learning and reward shaping', 'abstract': 'One of the fundamental challenges in reinforcement learning (RL) is the one of data efficiency: modern algorithms require a very large number of training samples, especially compared to humans, for solving environments with high-dimensional observations. The severity of this problem is increased when the reward signal is sparse. In this work, we propose learning a state representation in a self-supervised manner for reward prediction. The reward predictor learns to estimate either a raw or a smoothed version of the true reward signal in environment with a single, terminating, goal state. We augment the training of out-of-the-box RL agents by shaping the reward using our reward predictor during policy learning. Using our representation for preprocessing high-dimensional observations, as well as using the predictor for reward shaping, is shown to significantly enhance Actor Critic using Kronecker-factored Trust Region and Proximal Policy Optimization in single-goal environments with visual inputs.', 'corpus_id': 234097493, 'score': 0}, {'doc_id': '229373012', 'title': 'Towards a Distributed Framework for Multi-Agent Reinforcement Learning Research', 'abstract': 'Some of the most important publications in deep reinforcement learning over the last few years have been fueled by access to massive amounts of computation through large scale distributed systems. The success of these approaches in achieving human-expert level performance on several complex video-game environments has motivated further exploration into the limits of these approaches as computation increases. In this paper, we present a distributed RL training framework designed for super computing infrastructures such as the MIT SuperCloud. We review a collection of challenging learning environments-such as Google Research Football, StarCraft II, and Multi-Agent Mujoco- which are at the frontier of reinforcement learning research. We provide results on these environments that illustrate the current state of the field on these problems. Finally, we also quantify and discuss the computational requirements needed for performing RL research by enumerating all experiments performed on these environments.', 'corpus_id': 229373012, 'score': 1}, {'doc_id': '232068936', 'title': 'Coverage as a Principle for Discovering Transferable Behavior in Reinforcement Learning', 'abstract': 'Designing agents that acquire knowledge autonomously and use it to solve new tasks efficiently is an important challenge in reinforcement learning, and unsupervised learning provides a useful paradigm for autonomous acquisition of task-agnostic knowledge. In supervised settings, representations discovered through unsupervised pre-training offer important benefits when transferred to downstream tasks. Given the nature of the reinforcement learning problem, we argue that representation alone is not enough for efficient transfer in challenging domains and explore how to transfer knowledge through behavior. The behavior of pre-trained policies may be used for solving the task at hand (exploitation), as well as for collecting useful data to solve the problem (exploration). We argue that policies pre-trained to maximize coverage will produce behavior that is useful for both strategies. When using these policies for both exploitation and exploration, our agents discover better solutions. The largest gains are generally observed in domains requiring structured exploration, including settings where the behavior of the pre-trained policies is misaligned with the downstream task.', 'corpus_id': 232068936, 'score': 0}, {'doc_id': '207870308', 'title': 'Decentralized Distributed PPO: Solving PointGoal Navigation', 'abstract': 'We present Decentralized Distributed Proximal Policy Optimization (DD-PPO), a method for distributed reinforcement learning in resource-intensive simulated environments. DD-PPO is distributed (uses multiple machines), decentralized (lacks a centralized server), and synchronous (no computation is ever stale), making it conceptually simple and easy to implement. In our experiments on training virtual robots to navigate in Habitat-Sim, DD-PPO exhibits near-linear scaling -- achieving a speedup of 107x on 128 GPUs over a serial implementation. We leverage this scaling to train an agent for 2.5 Billion steps of experience (the equivalent of 80 years of human experience) -- over 6 months of GPU-time training in under 3 days of wall-clock time with 64 GPUs. This massive-scale training not only sets the state of art on Habitat Autonomous Navigation Challenge 2019, but essentially solves the task --near-perfect autonomous navigation in an unseen environment without access to a map, directly from an RGB-D camera and a GPS+Compass sensor. Fortuitously, error vs computation exhibits a power-law-like distribution; thus, 90% of peak performance is obtained relatively early (at 100 million steps) and relatively cheaply (under 1 day with 8 GPUs). Finally, we show that the scene understanding and navigation policies learned can be transferred to other navigation tasks -- the analog of ImageNet pre-training + task-specific fine-tuning for embodied AI. Our model outperforms ImageNet pre-trained CNNs on these transfer tasks and can serve as a universal resource (all models and code are publicly available).', 'corpus_id': 207870308, 'score': 1}, {'doc_id': '233204444', 'title': 'ACERAC: Efficient reinforcement learning in fine time discretization', 'abstract': 'We propose a framework for reinforcement learning (RL) in fine time discretization and a learning algorithm in this framework. One of the main goals of RL is to provide a way for physical machines to learn optimal behavior instead of being programmed. However, the machines are usually controlled in fine time discretization. The most common RL methods apply independent random elements to each action, which is not suitable in that setting. It is not feasible because it causes the controlled system to jerk, and does not ensure sufficient exploration since a single action is not long enough to create a significant experience that could be translated into policy improvement. In the RL framework introduced in this paper, policies are considered that produce actions based on states and random elements autocorrelated in subsequent time instants. The RL algorithm introduced here approximately optimizes such a policy. The efficiency of this algorithm is verified against three other RL methods (PPO, SAC, ACER) in four simulated learning control problems (Ant, HalfCheetah, Hopper, and Walker2D) in diverse time discretization. The algorithm introduced here outperforms the competitors in most cases considered.', 'corpus_id': 233204444, 'score': 1}, {'doc_id': '233231295', 'title': 'Decomposed Soft Actor-Critic Method for Cooperative Multi-Agent Reinforcement Learning', 'abstract': 'Deep reinforcement learning methods have shown great performance on many challenging cooperative multi-agent tasks. Two main promising research directions are multi-agent value function decomposition and multi-agent policy gradients. In this paper, we propose a new decomposed multi-agent soft actor-critic (mSAC) method, which incorporates the idea of the multi-agent value function decomposition and soft policy iteration framework effectively and is a combination of novel and existing techniques, including decomposed Q value network architecture, decentralized probabilistic policy, and counterfactual advantage function (optional). Theoretically, mSAC supports efficient offpolicy learning and addresses credit assignment problem partially in both discrete and continuous action spaces. Tested on StarCraft II micromanagement cooperative multiagent benchmark, we empirically investigate the performance of mSAC against its variants and analyze the effects of the different components. Experimental results demonstrate that mSAC significantly outperforms policybased approach COMA, and achieves competitive results with SOTA value-based approach Qmix on most tasks in terms of asymptotic perfomance metric. In addition, mSAC achieves pretty good results on large action space tasks, such as 2c vs 64zg and MMM2.', 'corpus_id': 233231295, 'score': 0}, {'doc_id': '232307382', 'title': 'Softmax with Regularization: Better Value Estimation in Multi-Agent Reinforcement Learning', 'abstract': 'Overestimation in Q-learning is an important problem that has been extensively studied in single-agent reinforcement learning, but has received comparatively little attention in the multi-agent setting. In this work, we empirically demonstrate that QMIX, a popular Q-learning algorithm for cooperative multi-agent reinforcement learning (MARL), suffers from a particularly severe overestimation problem which is not mitigated by existing approaches. We rectify this by designing a novel regularization-based update scheme that penalizes large joint action-values deviating from a baseline and demonstrate its effectiveness in stabilizing learning. We additionally propose to employ a softmax operator, which we efficiently approximate in the multi-agent setting, to further reduce the potential overestimation bias. We demonstrate that our Softmax with Regularization (SR) method, when applied to QMIX, accomplishes its goal of avoiding severe overestimation and significantly improves performance in a variety of cooperative multi-agent tasks. To demonstrate the versatility of our method, we apply it to other Q-learning based MARL algorithms and achieve similar performance gains. Finally, we show that our method provides a consistent performance improvement on a set of challenging StarCraft II micromanagement tasks.', 'corpus_id': 232307382, 'score': 0}, {'doc_id': '232417219', 'title': 'Shaping Advice in Deep Multi-Agent Reinforcement Learning', 'abstract': 'Multi-agent reinforcement learning involves multiple agents interacting with each other and a shared environment to complete tasks. When rewards provided by the environment are sparse, agents may not receive immediate feedback on the quality of actions that they take, thereby affecting learning of policies. In this paper, we propose a method called Shaping Advice in deep Multi-agent reinforcement learning (SAM) to augment the reward signal from the environment with an additional reward termed shaping advice. The shaping advice is given by a difference of potential functions at consecutive time-steps. Each potential function is a function of observations and actions of the agents. The shaping advice needs to be specified only once at the start of training, and can be easily provided by non-experts. We show through theoretical analyses and experimental validation that the shaping advice provided by SAM does not distract agents from completing tasks specified by the environment reward. Theoretically, we prove that the convergence of policy gradients and value functions when using SAM implies the convergence of these quantities in the same environment in the absence of SAM. Experimentally, we evaluate SAM on three tasks in the multi-agent Particle World environment that have sparse rewards. We observe that using SAM results in agents learning policies to complete tasks faster, and obtain higher rewards than: i) using sparse rewards alone; ii) a state-of-the-art reward redistribution method.', 'corpus_id': 232417219, 'score': 0}, {'doc_id': '53084610', 'title': 'GPU-Accelerated Robotic Simulation for Distributed Reinforcement Learning', 'abstract': 'Most Deep Reinforcement Learning (Deep RL) algorithms require a prohibitively large number of training samples for learning complex tasks. Many recent works on speeding up Deep RL have focused on distributed training and simulation. While distributed training is often done on the GPU, simulation is not. In this work, we propose using GPU-accelerated RL simulations as an alternative to CPU ones. Using NVIDIA Flex, a GPU-based physics engine, we show promising speed-ups of learning various continuous-control, locomotion tasks. With one GPU and CPU core, we are able to train the Humanoid running task in less than 20 minutes, using 10-1000x fewer CPU cores than previous works. We also demonstrate the scalability of our simulator to multi-GPU settings to train more challenging locomotion tasks.', 'corpus_id': 53084610, 'score': 1}]
58	russian text style transfer	765e6a6c6537259d648e79f6a97c79c0	19584	{}	[{'doc_id': '235683062', 'title': 'XLM-E: Cross-lingual Language Model Pre-training via ELECTRA', 'abstract': 'In this paper, we introduce ELECTRA-style tasks (Clark et al., 2020b) to cross-lingual language model pre-training. Specifically, we present two pre-training tasks, namely multilingual replaced token detection, and translation replaced token detection. Besides, we pretrain the model, named as XLM-E, on both multilingual and parallel corpora. Our model outperforms the baseline models on various cross-lingual understanding tasks with much less computation cost. Moreover, analysis shows that XLM-E tends to obtain better cross-lingual transferability.', 'corpus_id': 235683062, 'score': 0}, {'doc_id': '236486197', 'title': 'PIE: A Parallel Idiomatic Expression Corpus for Idiomatic Sentence Generation and Paraphrasing', 'abstract': 'Idiomatic expressions (IE) play an important role in natural language, and have long been a “pain in the neck” for NLP systems. Despite this, text generation tasks related to IEs remain largely under-explored. In this paper, we propose two new tasks of idiomatic sentence generation and paraphrasing to fill this research gap. We introduce a curated dataset of 823 IEs, and a parallel corpus with sentences containing them and the same sentences where the IEs were replaced by their literal paraphrases as the primary resource for our tasks. We benchmark existing deep learning models, which have state-of-the-art performance on related tasks using automated and manual evaluation with our dataset to inspire further research on our proposed tasks. By establishing baseline models, we pave the way for more comprehensive and accurate modeling of IEs, both for generation and paraphrasing.', 'corpus_id': 236486197, 'score': 1}, {'doc_id': '234777967', 'title': 'Methods for Detoxification of Texts for the Russian Language', 'abstract': 'We introduce the first study of automatic detoxification of Russian texts to combat offensive language. Such a kind of textual style transfer can be used, for instance, for processing toxic content in social media. While much work has been done for the English language in this field, it has never been solved for the Russian language yet. We test two types of models – unsupervised approach based on BERT architecture that performs local corrections and supervised approach based on pretrained language GPT-2 model – and compare them with several baselines. In addition, we describe evaluation setup providing training datasets and metrics for automatic evaluation. The results show that the tested approaches can be successfully used for detoxification, although there is room for improvement.', 'corpus_id': 234777967, 'score': 1}, {'doc_id': '236881871', 'title': 'A template for the arxiv style', 'abstract': 'Automatic summarization techniques aim to shorten and generalize information given in the text while preserving its core message and the most relevant ideas. This task can be approached and treated with a variety of methods, however, not many attempts have been to produce solutions specifically for the Russian language despite existing localizations of the state-of-the-art models. In this paper, we aim to showcase ruGPT3 ability to summarize texts, fine-tuning it on the corpora of Russian news with their corresponding human-generated summaries. Additionally, we employ hyperparameter tuning so that the model’s output becomes less random and more tied to the original text. We evaluate the resulting texts with a set of metrics, showing that our solution can surpass the state-of-the-art model’s performance without additional changes in architecture or loss function. Despite being able to produce sensible summaries, our model still suffers from a number of flaws, namely, it is prone to altering Named Entities present in the original text (such as surnames, places, dates, etc.), deviating from facts stated in the given document, and repeating the information in the summary.', 'corpus_id': 236881871, 'score': 1}, {'doc_id': '235742704', 'title': 'Transfer Learning for Improving Results on Russian Sentiment Datasets', 'abstract': 'In this study, we test transfer learning approach on Russian sentiment benchmark datasets using additional train sample created with distant supervision technique. We compare several variants of combining additional data with benchmark train samples. The best results were achieved using three-step approach of sequential training on general, thematic and original train samples. For most datasets, the results were improved by more than 3% to the current state-of-the-art methods. The BERT-NLI model treating sentiment classification problem as a natural language inference task reached the human level of sentiment analysis on one of the datasets.', 'corpus_id': 235742704, 'score': 1}, {'doc_id': '235792352', 'title': 'Improving Neural Text Style Transfer by Introducing Loss Function Sequentiality', 'abstract': 'Text style transfer is an important issue for conversational agents as it may adapt utterance production to specific dialogue situations. It consists in introducing a given style within a sentence while preserving its semantics. Within this scope, different strategies have been proposed that either rely on parallel data or take advantage of non-supervised techniques. In this paper, we follow the latter approach and show that the sequential introduction of different loss functions into the learning process can boost the performance of a standard model. We also evidence that combining different style classifiers that either focus on global or local textual information improves sentence generation. Experiments on the Yelp dataset show that our methodology strongly competes with the current state-of-the-art models across style accuracy, grammatical correctness, and content preservation.', 'corpus_id': 235792352, 'score': 0}, {'doc_id': '235658460', 'title': 'Current Landscape of the Russian Sentiment Corpora', 'abstract': 'Currently, there are more than a dozen Russian-language corpora for sentiment analysis, differing in the source of the texts, domain, size, number and ratio of sentiment classes, and annotation method. This work examines publicly available Russian-language corpora, presents their qualitative and quantitative characteristics, which make it possible to get an idea of the current landscape of the corpora for sentiment analysis. The ranking of corpora by annotation quality is proposed, which can be useful when choosing corpora for training and testing. The influence of the training dataset on the performance of sentiment analysis is investigated based on the use of the deep neural network model BERT. The experiments with review corpora allow us to conclude that on average the quality of models increases with an increase in the number of training corpora. For the first time, quality scores were obtained for the corpus of reviews of ROMIP seminars based on the BERT model. Also, the study proposes the task of the building a universal model for sentiment analysis.', 'corpus_id': 235658460, 'score': 0}, {'doc_id': '236459894', 'title': 'GX at SemEval-2021 Task 2: BERT with Lemma Information for MCL-WiC Task', 'abstract': 'This paper presents the GX system for the Multilingual and Cross-lingual Word-in-Context Disambiguation (MCL-WiC) task. The purpose of the MCL-WiC task is to tackle the challenge of capturing the polysemous nature of words without relying on a fixed sense inventory in a multilingual and cross-lingual setting. To solve the problems, we use context-specific word embeddings from BERT to eliminate the ambiguity between words in different contexts. For languages without an available training corpus, such as Chinese, we use neuron machine translation model to translate the English data released by the organizers to obtain available pseudo-data. In this paper, we apply our system to the English and Chinese multilingual setting and the experimental results show that our method has certain advantages.', 'corpus_id': 236459894, 'score': 0}, {'doc_id': '236478373', 'title': 'New Dataset and Strong Baselines for the Grammatical Error Correction of Russian', 'abstract': 'Motivated by recent advancements in grammatical error correction in English and existing issues in the field, we describe a new resource, an annotated learner corpus of Russian, extracted from the Lang-8 language learning website. This new dataset is benchmarked against two grammatical error correction models that use state-of-the-art neural architectures. Results are provided on the newlycreated corpus and are compared against performance on another, existing resource. We also evaluate the contribution of the Lang-8 training data to the grammatical error correction of Russian and perform type-based analysis of the models. The expert annotations are available for research purposes.', 'corpus_id': 236478373, 'score': 1}, {'doc_id': '235592812', 'title': 'A Comprehensive Exploration of Pre-training Language Models', 'abstract': 'Recently, the development of pre-trained language models\nhas brought natural language processing (NLP) tasks to the new state-of-the-art. In this paper we explore the efficiency of various pre-trained\nlanguage models. We pre-train a list of transformer-based models with\nthe same amount of text and the same training steps. The experimental results shows that the most improvement upon the origin BERT is\nadding the RNN-layer to capture more contextual information for the\ntransformer-encoder layers.', 'corpus_id': 235592812, 'score': 0}]
59	AD speech feed 1	5bf4e98b829c321ba8cac4201eb2617f	13097	{}	"[{'doc_id': '227229040', 'title': ""Preclinical Stage Alzheimer's Disease Detection Using Magnetic Resonance Image Scans"", 'abstract': ""Alzheimer's disease is one of the diseases that mostly affects older people without being a part of aging. The most common symptoms include problems with communicating and abstract thinking, as well as disorientation. It is important to detect Alzheimer's disease in early stages so that cognitive functioning would be improved by medication and training. In this paper, we propose two attention model networks for detecting Alzheimer's disease from MRI images to help early detection efforts at the preclinical stage. We also compare the performance of these two attention network models with a baseline model. Recently available OASIS-3 Longitudinal Neuroimaging, Clinical, and Cognitive Dataset is used to train, evaluate and compare our models. The novelty of this research resides in the fact that we aim to detect Alzheimer's disease when all the parameters, physical assessments, and clinical data state that the patient is healthy and showing no symptoms"", 'corpus_id': 227229040, 'score': 0}, {'doc_id': '227949359', 'title': 'Predicting the outcome of non-pharmacological treatment for patients with dementia-related mild cognitive impairment', 'abstract': ""Dementia is a progressive cognitive syndrome, with few effective pharmacological treatments that can slow its progress. Hence, non-pharmacological treatments (NPTs) play an important role in improving patient symptoms and quality of life. Designing the optimal personalised NPT strategy relies on objectively and quantitatively predicting the treatment outcome. Magnetoencephalography (MEG) findings can reflect the cognitive status of patients with dementia, and thus potentially predict NPT outcome. In the present study, 16 participants with cognitive impairment underwent NPT for several months. Their cognitive performance was evaluated based on the Mini-Mental State Examination and the Alzheimer's Disease Assessment Scale - Cognitive at the beginning and end of the NPT period, while resting-state brain activity was evaluated using MEG during the NPT period. Our results showed that the spectral properties of MEG signals predicted the changes in cognitive performance scores. High frequency oscillatory intensity at the right superior frontal gyrus medial segment, opercular part of the inferior frontal gyrus, triangular part of the inferior frontal gyrus, post central gyrus, and angular gyrus predicted the changes in cognitive performance scores. Thus, resting-state brain activity may be a powerful tool in designing personalised NPT."", 'corpus_id': 227949359, 'score': 0}, {'doc_id': '145820931', 'title': ""Connected Speech Deficit as an Early Hallmark of CSF-defined Alzheimer's Disease and Correlation with Cerebral Hypoperfusion Pattern."", 'abstract': ""BACKGROUND\nDiagnosis of prodromal Alzheimer's disease (AD) still represents a burning issue and there is growing interest for detection of early and non-invasive biomarkers. Although progressive episodic memory impairment is the typical predominant feature of AD, communicative difficulties can be already present at the early stages of the disease.\n\n\nOBJECTIVE\nThis study investigated narrative discourse production deficit as a hallmark of CSF- defined prodromal AD and its correlation with cerebral hypoperfusion pattern.\n\n\nMETHOD\nNarrative assessment with a multilevel procedure for discourse analysis was conducted on 28 subjects with Mild Cognitive impairment (15 MCI due to AD; 13 MCI non-AD) and 28 healthy controls. The diagnostic workup included CSF AD biomarkers. Cerebral hypoperfusion pattern was identified by SPECT image processing.\n\n\nRESULTS\nThe results showed that the discourse analysis of global coherence and lexical informativeness indexes allowed to identify MCI due to AD from MCI non-AD and healthy subjects. These findings allow to hypothesize that loss of narrative efficacy could be a possible early clinical hallmark of the Alzheimer's disease. Furthermore, a significant correlation of global coherence and lexical informativeness reduction with the SPECT hypoperfusion was found in the dorsal aspect of the anterior part of the left inferior frontal gyrus, supporting the hypothesis that this area has a significant role in communicative efficacy and in particular in semantic selection executive control.\n\n\nCONCLUSION\nThis study therefore contributes to understanding of neural networks for language processing and their involvement in prodromal Alzheimer's disease. It also suggests an easy and sensitive tool for clinical practice that can help identifying individuals with prodromal Alzheimer's disease."", 'corpus_id': 145820931, 'score': 1}, {'doc_id': '227247744', 'title': ""MRI Images Analysis Method for Early Stage Alzheimer's Disease Detection"", 'abstract': ""Alzheimer's disease is a neurogenerative disease that alters memories, cognitive functions leading to death. Early diagnosis of the disease, by detection of the preliminary stage, called Mild Cognitive Impairment (MCI), remains a challenging issue. In this respect, we introduce, in this paper, a powerful classification architecture that implements the pre-trained network AlexNet to automatically extract the most prominent features from Magnetic Resonance Imaging (MRI) images in order to detect the Alzheimer's disease at the MCI stage. The proposed method is evaluated using a big database from OASIS Database Brain. Various sections of the brain: frontal, sagittal and axial were used. The proposed method achieved 96.83% accuracy by using 420 subjects: 210 Normal and 210 MRI"", 'corpus_id': 227247744, 'score': 0}, {'doc_id': '231418919', 'title': ""Deep Convolutional Neural Network based Classification of Alzheimer's Disease using MRI Data"", 'abstract': ""Alzheimer's disease (AD) is a progressive and incurable neurodegenerative disease which destroys brain cells and causes loss to patient's memory. Early detection can prevent the patient from further damage to the brain cells and hence avoid permanent memory loss. In the past few years, various automatic tools and techniques have been proposed for the diagnosis of AD. Several methods focus on fast, accurate, and early detection of the disease to minimize the loss to a patient's mental health. Although machine learning and deep learning techniques have significantly improved medical imaging systems for AD by providing diagnostic performance close to the human level. But the main problem faced during multi-class classification is the presence of highly correlated features in the brain structure. In this paper, we have proposed a smart and accurate way of diagnosing AD based on a two-dimensional deep convolutional neural network (2D-DCNN) using an imbalanced three-dimensional MRI dataset. Experimental results on Alzheimer's Disease Neuroimaging Initiative magnetic resonance imaging (MRI) dataset confirms that the proposed 2D-DCNN model is superior in terms of accuracy, efficiency, and robustness. The model classifies MRI into three categories: AD, mild cognitive impairment, and normal control; and has achieved 99.89% classification accuracy with imbalanced classes. The proposed model exhibits noticeable improvement in accuracy as compared to state-of-the-art methods."", 'corpus_id': 231418919, 'score': 0}, {'doc_id': '227012703', 'title': ""3D Grid-Attention Networks for Interpretable Age and Alzheimer's Disease Prediction from Structural MRI."", 'abstract': ""We propose an interpretable 3D Grid-Attention deep neural network that can accurately predict a person's age and whether they have Alzheimer's disease (AD) from a structural brain MRI scan. Building on a 3D convolutional neural network, we added two attention modules at different layers of abstraction, so that features learned are spatially related to the global features for the task. The attention layers allow the network to focus on brain regions relevant to the task, while masking out irrelevant or noisy regions. In evaluations based on 4,561 3-Tesla T1-weighted MRI scans from 4 phases of the Alzheimer's Disease Neuroimaging Initiative (ADNI), salience maps for age and AD prediction partially overlapped, but lower-level features overlapped more than higher-level features. The brain age prediction network also distinguished AD and healthy control groups better than another state-of-the-art method. The resulting visual analyses can distinguish interpretable feature patterns that are important for predicting clinical diagnosis. Future work is needed to test performance across scanners and populations."", 'corpus_id': 227012703, 'score': 0}, {'doc_id': '224803761', 'title': 'FLAME: A computerized neuropsychological composite for trials in early dementia', 'abstract': ""Abstract Introduction Sensitive neuropsychological tests are needed to improve power for clinical trials in early Alzheimer's disease (AD). Methods To develop a neuropsychological composite (FLAME – Factors of Longitudinal Attention, Memory and Executive Function), we assessed, 10,714 participants over the age of 50 from PROTECT with validated computerized assessments for 2 years. A factorial analysis was completed to identify the key cognitive factors in all participants, and further analyses examined sensitivity to change in people with stage 2/3 early Alzheimer's disease (AD) according to the US Food and Drug Administration (FDA) framework. Results The FLAME composite score (speed of attention, accuracy of attention, memory, and executive function) distinguished between normal cognition and stage 2/3 early AD at baseline, and was sensitive to cognitive and global/functional decline over 2 years, with the potential to improve power for clinical trials. Discussion FLAME is sensitive to change, providing a straightforward approach to reduce sample size for RCTs in early AD. Conclusion FLAME is a useful computerized neuropsychology composite with utility for clinical trials focusing on cognition."", 'corpus_id': 224803761, 'score': 1}, {'doc_id': '227011947', 'title': ""Combining Prosodic, Voice Quality and Lexical Features to Automatically Detect Alzheimer's Disease"", 'abstract': ""Alzheimer's Disease (AD) is nowadays the most common form of dementia, and its automatic detection can help to identify symptoms at early stages, so that preventive actions can be carried out. Moreover, non-intrusive techniques based on spoken data are crucial for the development of AD automatic detection systems. In this light, this paper is presented as a contribution to the ADReSS Challenge, aiming at improving AD automatic detection from spontaneous speech. To this end, recordings from 108 participants, which are age-, gender-, and AD condition-balanced, have been used as training set to perform two different tasks: classification into AD/non-AD conditions, and regression over the Mini-Mental State Examination (MMSE) scores. Both tasks have been performed extracting 28 features from speech -- based on prosody and voice quality -- and 51 features from the transcriptions -- based on lexical and turn-taking information. Our results achieved up to 87.5 % of classification accuracy using a Random Forest classifier, and 4.54 of RMSE using a linear regression with stochastic gradient descent over the provided test set. This shows promising results in the automatic detection of Alzheimer's Disease through speech and lexical features."", 'corpus_id': 227011947, 'score': 1}, {'doc_id': '231662473', 'title': ""Automatic Prediction of Cognitive and Functional Decline Can Significantly Decrease the Number of Subjects Required for Clinical Trials in Early Alzheimer's Disease."", 'abstract': ""BACKGROUND\nWhile both cognitive and magnetic resonance imaging (MRI) data has been used to predict progression in Alzheimer's disease, heterogeneity between patients makes it challenging to predict the rate of cognitive and functional decline for individual subjects.\n\n\nOBJECTIVE\nTo investigate prognostic power of MRI-based biomarkers of medial temporal lobe atrophy and macroscopic tissue change to predict cognitive decline in individual patients in clinical trials of early Alzheimer's disease.\n\n\nMETHODS\nData used in this study included 312 patients with mild cognitive impairment from the ADNI dataset with baseline MRI, cerebrospinal fluid amyloid-β, cognitive test scores, and a minimum of two-year follow-up information available. We built a prognostic model using baseline cognitive scores and MRI-based features to determine which subjects remain stable and which functionally decline over 2 and 3-year follow-up periods.\n\n\nRESULTS\nCombining both sets of features yields 77%accuracy (81%sensitivity and 75%specificity) to predict cognitive decline at 2 years (74%accuracy at 3 years with 75%sensitivity and 73%specificity). When used to select trial participants, this tool yields a 3.8-fold decrease in the required sample size for a 2-year study (2.8-fold decrease for a 3-year study) for a hypothesized 25%treatment effect to reduce cognitive decline.\n\n\nCONCLUSION\nWhen used in clinical trials for cohort enrichment, this tool could accelerate development of new treatments by significantly increasing statistical power to detect differences in cognitive decline between arms. In addition, detection of future decline can help clinicians improve patient management strategies that will slow or delay symptom progression."", 'corpus_id': 231662473, 'score': 1}, {'doc_id': '229152327', 'title': 'Classification of ALS patients based on acoustic analysis of sustained vowel phonations', 'abstract': 'Abstract Amyotrophic lateral sclerosis (ALS) is incurable neurological disorder with rapidly progressive course. Common early symptoms of ALS are difficulty in swallowing and speech. However, early acoustic manifestation of speech and voice symptoms is very variable, that making their detection very challenging, both by human specialists and automatic systems. This study presents an approach to voice assessment for automatic system that separates healthy people from patients with ALS. In particular, this work focus on analysing of sustain phonation of vowels /a/ and/i/ to perform automatic classification of ALS patients. A wide range of acoustic features such as MFCC, formants, jitter, shimmer, vibrato, PPE, GNE, HNR, etc. were analysed. We also proposed a new set of acoustic features for characterizing harmonic structure of the vowels. Calculation of these features is based on pitch synchronized voice analysis. A linear discriminant analysis (LDA) was used to classify the phonation produced by patients with ALS and those by healthy individuals. Several algorithms of feature selection were tested to find optimal feature subset for LDA model. The study’s experiments show that the most successful LDA model based on 32 features picked out by LASSO feature selection algorithm attains 99.7% accuracy with 99.3% sensitivity and 99.9% specificity. Among the classifiers with a small number of features, we can highlight LDA model with 5 features, which has 89.0% accuracy (87.5% sensitivity and 90.4% specificity).', 'corpus_id': 229152327, 'score': 1}]"
60	Event Prediction - Biotech Specific	9d230e69db65b5703d4fcdb6b49c384d	19823	{}	"[{'doc_id': '169188862', 'title': 'The Impact of Pre-merger Disclosure and Acquisition Experience on Mergers and Acquisitions', 'abstract': 'This thesis focuses on the effect of factors in the pre-acquisition planning stage on M&A outcomes. With three empirical chapters, the main findings provide compelling evidence that pre-acquisition factors are related to M&A fundamentals such as the motivation and incentives to carry out M&A transaction, and play significant roles in deal’s negotiation and post-acquisition integration. \n \nThe first empirical chapter of this thesis investigates whether the voluntary disclosure at pre-acquisition issuance activities, i.e. the intended ‘use of proceeds’, has influence on subsequent M&A outcomes. The results show that firms disclosing acquisition intention at debt/equity issuance significantly raise more funds but fail to allocate capital efficiently on value-increasing M&A transactions. This evidence is consistent with the capital need theory. \n \nThe second empirical chapter examines the wealth effect of mega corporate takeover and explores whether rich acquisition experience facilitate acquirers to generate shareholder value in mega-deals. The findings show that acquirer’s acquisition experience is positively related to mega-deals completion likelihood, stock performance in short- and long-run, and operating performance in the long-run following mega-mergers. The evidence indicates that acquirers are able to learn though experience and develop skills to deal with the complexity of mega-mergers. \n \nThe final empirical chapter provides evidence on the relationship between target CEOs’ acquisition experience and takeover gains for target shareholders. The results show that target shareholders are likely to receive lower bid premiums and earn lower abnormal stock returns around deal announcement when they have a CEO with more acquisition experience. Additionally, target CEOs’ acquisition experience is positively related to stock payment. Our evidence suggests that more experienced target CEOs tend to bargain for more personal benefits related to the voting influence in the combined firm instead of helping their shareholders to gain bargaining advantage in the negotiation.', 'corpus_id': 169188862, 'score': 0}, {'doc_id': '202711433', 'title': 'Prediction of Drug Approval After Phase I Clinical Trials in Oncology: RESOLVED2.', 'abstract': 'PURPOSE\nDrug development in oncology currently is facing a conjunction of an increasing number of antineoplastic agents (ANAs) candidate for phase I clinical trials (P1CTs) and an important attrition rate for final approval. We aimed to develop a machine learning algorithm (RESOLVED2) to predict drug development outcome, which could support early go/no-go decisions after P1CTs by better selection of drugs suitable for further development.\n\n\nMETHODS\nPubMed abstracts of P1CTs reporting on ANAs were used together with pharmacologic data from the DrugBank5.0 database to model time to US Food and Drug Administration (FDA) approval (FDA approval-free survival) since the first P1CT publication. The RESOLVED2 model was trained with machine learning methods. Its performance was evaluated on an independent test set with weighted concordance index (IPCW).\n\n\nRESULTS\nWe identified 462 ANAs from PubMed that matched with DrugBank5.0 (P1CT publication dates 1972 to 2017). Among 1,411 variables, 28 were used by RESOLVED2 to model the FDA approval-free survival, with an IPCW of 0.89 on the independent test set. RESOLVED2 outperformed a model that was based on efficacy/toxicity (IPCW, 0.69). In the test set at 6 years of follow-up, 73% (95% CI, 49% to 86%) of drugs predicted to be approved were approved, whereas 92% (95% CI, 87% to 98%) of drugs predicted to be nonapproved were still not approved (log-rank P < .001). A predicted approved drug was 16 times more likely to be approved than a predicted nonapproved drug (hazard ratio, 16.4; 95% CI, 8.40 to 32.2).\n\n\nCONCLUSION\nAs soon as P1CT completion, RESOLVED2 can predict accurately the time to FDA approval. We provide the proof of concept that drug development outcome can be predicted by machine learning strategies.', 'corpus_id': 202711433, 'score': 1}, {'doc_id': '469006', 'title': 'FDA actions against misleading or unsubstantiated economic and quality-of-life promotional claims: an analysis of warning letters and notices of violation.', 'abstract': 'OBJECTIVES\nThe objective of this study was to understand the types of economic and quality-of-life promotional claims the FDA considers false or misleading.\n\n\nMETHODS\nPublicly available FDA letters (n = 569) sent to pharmaceutical companies from 1997 through 2001 for inappropriate promotional claims were reviewed. A standard data collection form was developed, including six categories for economic violations and three for QOL violations. For QOL, only letters with explicit violations for false or misleading claims using the words ""quality of life"" or patient ""well-being"" were considered. Other information collected included type of regulatory letter and media in which violations were found.\n\n\nRESULTS\nTwenty-eight (4.9%) letters cited false and/or misleading economic claims. The most common economic violation was ""unsupported comparative claim of effectiveness, safety, or interchangeability"" (n = 14). Twenty-eight (4.9%) letters cited QOL violations, of which four contained both economic and QOL violations. The most common QOL violation was ""lack of substantial evidence for QOL claims"" (n = 15). None of the FDA letters used the term ""patient reported outcomes."" Violations were found most frequently in brochure and Web site-based promotions.\n\n\nCONCLUSIONS\nThe body of evidence that is emerging illustrates how the FDA is regulating promotional material containing misleading or unsubstantiated economic and QOL claims. However, knowing what constitutes an appropriate claim remains challenging because there are no formal guidelines describing what constitutes a violation, nor what level of substantiating evidence is required. More guidance may be needed to ensure appropriate use of these claims in drug promotions.', 'corpus_id': 469006, 'score': 1}, {'doc_id': '220439467', 'title': 'Predicting Regulatory Product Approvals Using a Proposed Quantitative Version of FDA’s Benefit–Risk Framework to Calculate Net-Benefit Score and Benefit–Risk Ratio', 'abstract': 'Background Approval of regulated medical products in the USA is based upon a rigorous review of the benefits and risks as performed by the US Food and Drug Administration (FDA) staff of scientists and is summarized in a descriptive and qualitative format called the FDA’s Benefit–Risk Framework (BRF). This present method highlights the key factors in regulatory decision-making, but does not clearly define the reason for its final approval. Method This study proposes a quantitative version of FDA’s BRF to calculate a Net-Benefit Score and a Benefit–Risk Ratio as a method to define a single-value summary of the tradeoffs between benefits and risks and allow comparisons among other products. In this retrospective review of five years of new molecular entities and new biologic (N\u2009=\u2009185 products) regulatory decision-making, this proposed scoring system codifies and quantitates the information about a product’s benefits, risks, and risk management information in a format that may predict why regulated medical products are approved in the USA. Results Simple calculation of codified benefits, risks, and risk mitigations with numerical limits is proposed to provide a repeatable process and transparency for documenting the net-benefit of regulatory product approval. Conclusion Use of a strict process of collecting, codifying, and analyzing public information to determine a Net-Benefit score and a Benefit–Risk Ratio is possible to anticipate regulatory product approval.', 'corpus_id': 220439467, 'score': 1}, {'doc_id': '135144064', 'title': 'I briozoi della Baia di Terra Nova (Mare di Ross, Antartide) e le implicazioni nelle ricerche climatiche', 'abstract': ""Il clima della penisola antartica sta cambiando rapidamente e ci si attende che l'oceano meridionale sia vulnerabile ai cambiamenti di export di carbonati indotti dal riscaldamento di origine antropica e agli effetti dell'acidificazione. La diversita e ricchezza dell'Antartico e la straordinaria capacita di adattamento delle sue specie lo rendono un ambiente ideale per studi di tipo adattativo. Tra gli organismi antartici calcificanti, i briozoi sono particolarmente interessanti grazie al loro ruolo come bioindicatori e promotori di biodiversita. I briozoi sono abbondanti e ampiamente distribuiti nel Mare di Ross, in cui si registra la piu altra produttivita di tutto l’oceano meridionale, e hanno un potenziale riconosciuto come organismi target per studi sul cambiamento climatico oltre che organismi chiave per l’accumulo di ‘Blue Carbon’. Negli ultimi 30 anni, il PNRA ha condotto diverse Campagne nel Mare di Ross e studi su diversi gruppi tassonomici nella Baia di Terra Nova, tra cui briozoi, ma sebbene siano state pubblicate le liste relative alle prime campagne oceanografiche condotte dal 1987 al 1995, non sono disponibili dati relativi a \ncampagne successive effettuate nella Baia di Terra Nova (BTN). Il presente RT aggiorna la lista della fauna a briozoi della Baia di Terra Nova confrontando i dati della letteratura con campioni raccolti da spedizioni successive e facenti parte della collezione del Museo \nNazionale dell’Antartide (MNA, Sezione di Genova) e del CNR-ISMAR di Bologna. La lista aggiornata riporta 133 taxa di cui 34 nuove segnalazioni per la BTN rispetto alle prime tre campagne italiane condotte nell’area e segnalazioni di specie fossili dagli affioramenti di Cape Russell. Da questa lista, aggiornata anche a livello di nomenclatura tassonomica, sono emerse tra le specie fortemente calcificate (102) alcune specie-chiave con un ruolo come biocostruttori e bioindicatori, sia incrostanti (56) che eretto-rigide (45), che potrebbero essere utilizzate in futuri studi sperimentali volti ad indagare il potenziale \nadattativo di alcune specie calcificanti e il loro ruolo come proxy del cambiamento climatico."", 'corpus_id': 135144064, 'score': 0}, {'doc_id': '155391945', 'title': 'Relative Idiosyncratic Volatility and the Timing of Corporate Insider Trading', 'abstract': ""This paper investigates whether corporate insiders trade when asymmetric information is high, using data on U.S. corporate insider transactions between 1986 and 2012. We generalize the literature focusing on insider trading around the announcement of different categories of corporate events. The key innovation of this paper is our asymmetric information proxy relivol, which measures deviations of idiosyncratic volatility from a firm's normal level. Our findings suggest that relivol positively predicts insider purchases, indicating that insiders buy shares when their informational advantage is high. However, insiders appear to sell less when relivol is high, which is consistent with existing evidence on sales being driven by alternative, non-information-related trading motives such as liquidity or diversification needs. Furthermore, we find that profits are significantly higher when insiders buy during periods of high relivol but not when they sell shares."", 'corpus_id': 155391945, 'score': 0}, {'doc_id': '29525416', 'title': 'Predicting success in regulatory approval from Phase I results', 'abstract': 'AbstractPurpose\nDrug development in oncology is resource intensive and has a high failure rate. In this exploratory analysis, we aimed to identify the characteristics and outcomes of published Phase I studies associated with future Food and Drug Administration (FDA) approval.MethodsPhase I studies of approved and non-approved anticancer agents between 2000 and 2013 were retrospectively examined. Fisher’s exact and chi-squared tests were used to compare the potential predictive measures.ResultsPhase I studies of 88 anticancer agents (54 approved and 34 non-approved by the FDA), treating a total of 4,423 subjects, were examined. The median number of patients in Phase I trials of approved and non-approved agents was 44.5 and 32, respectively. A total of 423 subjects (86 reporting studies) had a complete responses, and 342 subjects (80 reporting studies) had a partial responses (PR). A higher number of PR (P\xa0<\xa00.001), PR rate (P\xa0=\xa00.003) and longer PR duration (P\xa0=\xa00.001) were predictive of regulatory success.ConclusionsThese preliminary findings indicate that objective responses in Phase I trials may have predictive value for later regulatory approval.', 'corpus_id': 29525416, 'score': 1}, {'doc_id': '154676052', 'title': 'The PIN anomaly around M&A announcements', 'abstract': 'The probability of information-based trading (PIN) introduced by Easley and O’Hara (1987) has been increasingly used in empirical research in finance. We investigate its behavior around a sample of merger and acquisition announcements that took place on Euronext Paris between 1995 and 2000. The behavior of the PIN seems to be in contradiction with clear evidence of information leakages in our sample during the pre-event period. We investigate the reasons for its unusual behavior and raise some concerns about its use as an information-based trading indicator, at least around major corporate events.', 'corpus_id': 154676052, 'score': 0}, {'doc_id': '69221177', 'title': 'Dynamic Talent Flow Analysis with Deep Sequence Prediction Modeling', 'abstract': 'Talent flow analysis is a process for analyzing and modeling the flows of employees into and out of targeted organizations, regions, or industries. A clear understanding of talent flows is critical for many applications, such as human resource planning, brain drain monitoring, and future workforce forecasting. However, existing studies on talent flow analysis are either qualitative or limited by coarse level quantitative modeling. To this end, in this paper, we provide a fine-grained data-driven approach to model the dynamics and evolving nature of talent flows by leveraging the rich information available in job transition networks. Specifically, we first investigate how to enrich the sparse talent flow data by exploiting the correlations between the stock price movement and the talent flows of public companies. Then, we formalize the talent flow modeling problem as to predict the increments of the edge weights in the dynamic job transition network. In this way, the problem is transformed into a multi-step time series forecasting problem. A deep sequence prediction model is developed based on the recurrent neural network model, which consumes multiple input sources derived from dynamic job transition networks. Finally, experimental results on real-world data show that the proposed model outperforms other benchmark models in terms of prediction accuracy. The results also indicate that the proposed model can provide reasonable performance even if the historical talent flow data are not completely available.', 'corpus_id': 69221177, 'score': 1}, {'doc_id': '51665899', 'title': 'Transient voltages on bonded cable sheaths', 'abstract': 'Discussion of a paper by Herman Halperin, J. E. Clem, and K. W. Miller published in the January 1935 issue, pages 73–82, and presented for oral discussion at the cables session of the winter convention, New York, N. Y., January 24,1935.', 'corpus_id': 51665899, 'score': 0}]"
61	Narrative and Emotion	809688b5e2b8c558037c3ae1bc1b87f9	8092	{}	"[{'doc_id': '210440127', 'title': 'Stories That Open and Stories That Close: Theoretical and Clinical Narratives in Psychoanalysis', 'abstract': 'ABSTRACT Stories, narratives, actions over time reveal character, quirks, personal standards, human relational connections, social values, and even universal meanings! Stories do all this and then some, and good ones often cover many of these bases simultaneously. Good stories embody webs of experience that readers and listeners—and therapeutic partners—register on different levels and in different modes often at the same time. Good stories also shake our conventional assumptions and ways of thinking; they may shatter some adamantine shibboleths and open our minds and imaginations to new and future possibilities. These are the good stories, ones that are open to nuance and complexity and change, ones we try to create and cultivate in our analytic relationships. However, there are other stories, not such good ones, fixed and immovable stories, stuck in time and stultifying to growth and development. These are closed stories, and in this article I will explore varieties of therapeutic narratives that close down thinking and emotions, and I’ll speculate on their origins and remediation.', 'corpus_id': 210440127, 'score': 1}, {'doc_id': '220363729', 'title': 'BézierSketch: A generative model for scalable vector sketches', 'abstract': 'The study of neural generative models of human sketches is a fascinating contemporary modeling problem due to the links between sketch image generation and the human drawing process. The landmark SketchRNN provided breakthrough by sequentially generating sketches as a sequence of waypoints. However this leads to low-resolution image generation, and failure to model long sketches. In this paper we present BezierSketch, a novel generative model for fully vector sketches that are automatically scalable and high-resolution. To this end, we first introduce a novel inverse graphics approach to stroke embedding that trains an encoder to embed each stroke to its best fit Bezier curve. This enables us to treat sketches as short sequences of paramaterized strokes and thus train a recurrent sketch generator with greater capacity for longer sketches, while producing scalable high-resolution results. We report qualitative and quantitative results on the Quick, Draw! benchmark.', 'corpus_id': 220363729, 'score': 0}, {'doc_id': '220602886', 'title': 'Perceptions of radiography students toward problem-based learning almost two decades after its introduction at Makerere University, Uganda', 'abstract': ""\n               Abstract\n               \n                  Introduction\n                  Problem-based learning (PBL) has been reported to be a valuable student-centred learning approach across the globe. In PBL students first encounter a problem, which triggers discussion, followed by student-centred inquiry. Makerere University College of Health Sciences has been using PBL for radiography students since 2002. Over the years, the learning landscape may have changed, including the significant disruption of learning by the coronavirus disease 2019 global pandemic. The study aimed at exploring the perceptions of undergraduate radiography students about the PBL curriculum at Makerere University almost two decades after its introduction.\n               \n               \n                  Methods\n                  This exploratory qualitative study involved 18 radiography students sampled purposively, from whom data were gathered using\xa0focus group discussions. Thematic analysis was subsequently used.\n               \n               \n                  Results\n                  Three key themes emerged from the data: (1) quality of teaching, (2) curriculum efficiency, and (3) curriculum expectations and rating. All students were generally positive about the curriculum. Most agreed that the curriculum was efficient to a greater extent and had met their expectations and desired objectives. Students, however, faced challenges; for example, with limited learning resources during the learning process.\n               \n               \n                  Conclusion\n                  This study highlights the significant role of PBL in enhancing student's problem-solving, critical thinking, literature search, and, most of all, their practical skills. Prioritization of teaching based on practical relevance and learning objectives is of great importance. The radiography students believed that their curriculum program was generally beneficial to them; however, it was affected by limited resources and limited availability of teaching personnel, which needs to be addressed.\n               \n            "", 'corpus_id': 220602886, 'score': 0}, {'doc_id': '14055899', 'title': 'The Secrets of Storytelling : Why We Love a Good Yarn', 'abstract': 'When Brad Pitt tells Eric Bana in the 2004 film Troy that “there are no pacts between lions and men,” he is not reciting a clever line from the pen of a Hollywood screenwriter. He is speaking Achillesʼ words in English as Homer wrote them in Greek more than 2,000 years ago in the Iliad. The tale of the Trojan War has captivated generations of audiences while evolving from its origins as an oral epic to written versions and, finally, to several film adaptations. The power of this story to transcend time, language and culture is clear even today, evidenced by Troyʼs robust success around the world.', 'corpus_id': 14055899, 'score': 1}, {'doc_id': '51575171', 'title': 'Oral tradition and communication', 'abstract': ""Oral tradition has become a domain of great interest to scholars of different disciplines of knowledge such as literature, psychology, anthropology, and philosophy. It has a huge scope for the discipline of communication too. This article presents an appraisal of oral tradition as a means of communication from one generation to another. While doing so, it deals with following issues: Can history be narrated based on oral traditions just as it is done with ‘written documents'? Are the oral traditions only the sources of historiography or do they have other implications too? It also discusses whether oral traditions can be taken as valid historical sources, and, if not, whether there are means for testing its reliability. DOI: 10.3126/bodhi.v3i1.2813 Bodhi Vol.3(1) 2009 p.61-68"", 'corpus_id': 51575171, 'score': 1}, {'doc_id': '142341155', 'title': ""Toward an Emancipatory Psychoanalysis: Brandchaft's Intersubjective Vision"", 'abstract': ""Encountering Brandchaft. Toward an Emancipatory Psychoanalysis. Reconsiderations of Psychoanalytic Listening. Theoretical Considerations. A Case of Intractable Depression. Bonds that Shackle, Ties that Free. Whose Self Is It Anyway? Codetermination and Change in Psychoanalysis. To Free the Spirit from its Cell. The Self and its Objects in Developmental Trauma. Obsessional Disorders: A Developmental Systems Perspective. Systems of Pathological Accommodation in Psychoanalysis. Reflections on the Unconscious. Brandchaft's Intersubjective Vision."", 'corpus_id': 142341155, 'score': 1}, {'doc_id': '221352639', 'title': 'The politics of precarity', 'abstract': 'The adequacy of any theory of radical democracy requires that it thematize the social conditions within which an emancipatory politics might be enacted. Paul Apostolidis’s The Fight for Time offers a sustained reflection on how democratic politics is both frustrated and facilitated by widespread and increasing precarity. However, as this Critical Exchange demonstrates, the nature of precarity, the', 'corpus_id': 221352639, 'score': 0}, {'doc_id': '219632257', 'title': 'Diabolical dilemmas of COVID-19: An empirical study into Dutch society’s trade-offs between health impacts and other effects of the lockdown', 'abstract': 'We report and interpret preferences of a sample of the Dutch adult population for different strategies to end the so-called ‘intelligent lockdown’ which their government had put in place in response to the COVID-19 pandemic. Using a discrete choice experiment, we invited participants to make a series of choices between policy scenarios aimed at relaxing the lockdown, which were specified not in terms of their nature (e.g. whether or not to allow schools to re-open) but in terms of their effects along seven dimensions. These included health-related effects, but also impacts on the economy, education, and personal income. From the observed choices, we were able to infer the implicit trade-offs made by the Dutch between these policy effects. For example, we find that the average citizen, in order to avoid one fatality directly or indirectly related to COVID-19, is willing to accept a lasting lag in the educational performance of 18 children, or a lasting (>3 years) and substantial (>15%) reduction in net income of 77 households. We explore heterogeneity across individuals in terms of these trade-offs by means of latent class analysis. Our results suggest that most citizens are willing to trade-off health-related and other effects of the lockdown, implying a consequentialist ethical perspective. Somewhat surprisingly, we find that the elderly, known to be at relatively high risk of being affected by the virus, are relatively reluctant to sacrifice economic pain and educational disadvantages for the younger generation, to avoid fatalities. We also identify a so-called taboo trade-off aversion amongst a substantial share of our sample, being an aversion to accept morally problematic policies that simultaneously imply higher fatality numbers and lower taxes. We explain various ways in which our results can be of value to policy makers in the context of the COVID-19 and future pandemics.', 'corpus_id': 219632257, 'score': 0}, {'doc_id': '199484922', 'title': 'Temporality as Bergsonian Critique in the Advertising and Visual Art of Bertram Brooker', 'abstract': 'This article explores time concepts derived from Henri Bergson as adapted by Canadian marketing theorist and visual artist Bertram Brooker (1888–1955) in articles and textbooks published during the 1920s and early 1930s. Inspired by Bergson’s critique of the Western metaphysical tradition, Brooker proposed innovative, participatory advertising strategies based on the French philosopher’s non-rational conception of time and the co-evolution of bodies and media. The author argues that the Toronto artist-advertiser’s descriptions of radio as offering the possibility of an interactive and synesthetic alternative to conventional print-based forms of advertising indirectly influenced Harold Innis’ redemptive gloss on the latent dialogism of radio. A critique of Brooker’s and Innis’ respective articulations of “oral” media as foreshadowing the contemporary economy of televisual “flow” is also posited. KEYWORdS Advertising; Toronto School/Transformation history; Media theory; Orality/Oral culture; Philosophy RÉSUMÉ Cet article explore les concepts temporels du philosophe français Henri Bergson ainsi qu’ils étaient adaptés par le théoricien du marketing et artiste Bertram Brooker (18881955) dans ses articles et manuels des années 1920 et début des années 1930. La critique de la tradition métaphysique occidental affirmé par Bergson en a inspiré Brooker, qui a proposé des stratégies de publicité participatives fondées sur sa conception non-rationnel de temps aussi que sa conception de la co-évolution des corps et des médias. Cet article propose que la représentation de la radio en tant qu’un objet interactif et synésthetique soutenu par l’artistedirecteur de publicité a influencé indirectement l’interprétation rédemptrice de Harold Innis au sujet du dialogisme de la radio. Une critique des déclarations de Brooker et Innis sur les médias « orale » en tant que préfigurant de l’économie contemporaine de « flux » télévisuel est aussi offert. MOTS CLÉS Publicité; École de Toronto/Histoire de la transformation; Théorie des médias; Oralité/Culture orale; Philosophie Introduction The multidisciplinary production of the Canadian artist, author, and advertising executive bertram brooker (1888–1955) is marked by a persistent obsession with time. Previous commentators have detected the influence of Henri bergson on brooker’s advertising writings and visual art (see Lauder, 2006, 2010, 2012; Luff, 1991; Zemans, Canadian Journal of Communication Vol 39 (2014) 469–496 ©2014 Canadian Journal of Communication Corporation Adam Lauder was the first W. P. Scott Chair for Research in E-Librarianship at York University, 4700 Keele Street, Toronto, ON M3J 1P3. Email: alauder@yorku.ca . 470 Canadian Journal of Communication, Vol 39 (3) 1989). The artist-advertiser’s celebration of “becoming” and “flux” in his marketing texts of the 1920s, in particular, is visualized by his abstract canvases, the first to be exhibited in a solo exhibition in Canada, and innovative graphic designs. His work of that decade is thereby aligned with the earlier bergsonian modernisms of Futurist and Vorticist artists (see Antliff, 1993). Yet the present article constitutes the first scholarly paper to frame brooker’s engagement with bergson’s non-rational temporality as a critical project. brooker’s harnessing of bergsonian temporality to articulate a sophisticated critique of modernity was all the more unusual given his status as one of the pre-eminent innovators in North American advertising circles in the 1920s. Not that his application of bergsonian concepts to problems in advertising was not, in this context at least, itself remarkably out of the ordinary. Perhaps only the futurist graphic designs executed by Fortunato depero (1892–1960) for mass-market publications such as Movie Makers, Vanity Fair, and Vogue, during his sojourn in New York from 1928 to 1930, constitute a comparable appropriation of bergson in North American popular culture. but whereas depero’s designs celebrated the flux of the machine age, brooker urged fellow admen to adopt bergson’s insights as part of a thorough re-evaluation of modernist values and beliefs embraced by the advertising profession, including progress, efficiency, and rationalization. The critical orientation of brooker’s appropriation of bergson is closer to aspects of the post–World War I cultural criticism of Canadian-born artist and author Wyndham Lewis (1882–1957), which drew on bergson’s critique of the spatial models informing classical metaphysics and modern science even while reversing the terms of the French philosopher’s arguments to articulate a renovation of spatial perception. brooker was already familiar with Lewis’ monumental Time and Western Man in the year of its publication (brooker, 1927, pp. 6–7). Gregory betts (2013) has recently proposed parallels between brooker’s “vortex of art, media, and advertising” and Lewis’ discourse on technology and mass culture (p. 217). Lauder (2012) earlier noted that Time and Western Man influenced Harold Innis’ discourse on time and modernity, with which brooker’s critical project likewise shares much in common (see also Watson, 2006). The resonance between articulations of time found in the work of brooker, Lewis, and later Innis speaks to the common currency of bergsonian rhetoric in what Paul Tiessen (1993) has termed the “pre-McLuhan body Canadian media theory” of the 1920s through the 1940s. Similarly, Janine Marchessault (2005) and darroch and Marchessault (2009) have argued that bergson’s thought played a formative—albeit largely overlooked—role in the development of Toronto School communication theory. Through his astonishingly early and critical engagement with bergsonian time concepts across a broad spectrum of discourses and media, brooker occupies a key position in this configuration. Yet this temporal dimension of the artist-advertiser’s work, and its critical orientation, have largely eluded appraisal until now. Through frequent contributions to the high-profile American trade papers Printers’ Ink and Advertising and Selling, and, from 1924 through 1927, as editor and publisher of Canada’s premier advertising journal, Marketing and Business Management (to which he also made frequent contributions as an author from 1921 until at least 1931), brooker established an international reputation as an outspoken critic of then-dominant behaviourist and quantitative approaches to marketing (see Johnston, 2001). In place of the statistical instruments and mechanistic models promoted by peers, brooker urged fellow marketers to take the literary production of Chekhov, dickens, and Shakespeare as their model. “dickens,” he wrote, “analyzed the consumer demand of his day and adjusted his production accordingly. ‘People like deep-dyed villains,’ he said to himself, and straightaway produced Quilp” (Marketing, 1921, p. 332). brooker’s unapologetically literary approach to advertising drew fierce criticism from leading American advertisers of the day: his pointed exchanges with Earnest Elmo Calkins, Charles S. Knapp, and William E. Cameron in the pages of Printers’ Ink substantiate the claim of one observer in 1951 that the Canadian artist-advertiser’s intuitive approach to copy “was strongly felt in international advertising” (betts, 2005, p. 231). While the literary and aesthetic bias of brooker’s writings on advertising topics has received broad acknowledgment, the extent to which his critique of American models also drew on the philosophy of bergson has yet to receive adequate scholarly attention. Selections of brooker’s articles from Marketing and Printers’ Ink were collected and revised in two volumes published by McGraw-Hill under the nom de plume Richard Surrey (one of several pseudonyms employed by the chameleonic artist-advertiser): Layout Technique in Advertising (Surrey, 1929b) and Copy Technique in Advertising (Surrey, 1930a). Richard Cavell (2002) has characterized these textbooks as comprising “an artistic credo” (p. 15). However, it was betts (2005) who first recognized that Layout Technique is primarily concerned with spatial concerns of the type studied by Cavell, whereas Copy Technique “organized its arguments around conceptions of time” (p. 247).This conceptual division of labour foreshadows the space/time dualism that structures the late communications writings of Harold Innis. Adam Lauder (2012) has explored the possibility that brooker may have served as an indirect influence on Innis. Yet, though Lauder briefly discussed the shared commitment to cultural continuity and the “oral tradition” disclosed by the writings of brooker and later Innis, the bulk of his analysis is devoted to an investigation of the possibility that the artist-advertiser’s visualizations (in the form of innovative charts and maps) may have contributed to the political economist’s early interest in the formative influence of geography on the development of a “staples” economy in Canada as well as his subsequent theorization of (neo-)colonial “monopolies of space.” Through original readings of articles published in Marketing and Printers’ Ink during the 1920s and early 1930s and other primary sources, this article proposes that brooker’s commercial writings and visual art alike mounted a critique of modernist space that cleared a path for Innis’ late “plea for time.” Paradoxically, the humanistic rhetoric of time deployed by both figures reveals striking parallels with the contemporary televisual paradigm critiqued, among others, by Richard dienst (1994). building on this recognition, the present article also traces some unintended consequences of the defence of time and the “oral tradition” articulated by brooker and later Innis. Brooker and Bergson: A literature review The first to observe a bergsonian inflection in brooker’s art was Joyce Zemans (1989). However, Zemans’ perception of “bergsonian flow” (p. 30) in canvases by the artistLauder Advertising and Visual Art of Bertram', 'corpus_id': 199484922, 'score': 1}, {'doc_id': '220535649', 'title': 'A manifesto for planning after the coronavirus: Towards planning of care', 'abstract': 'The COVID-19 crisis upended the status quo of our everyday life. The rising discourse in the midst of this pandemic is ‘human guilt’ (e.g., ‘we are the virus!’), reviving the dark side of neo-Malthusian environmentalist ideology. While the pandemic should be considered a wake-up call for us to drastically rethink our relationship with nature, planning discipline cannot resign itself from its power and responsibility to make a difference in human and nonhuman lives. So, here I ask: How can we carefully reposition ‘human intervention’ in the aftermath of this ‘human guilt’, without nullifying the hopeful spirit and our belief in the power of planning? Inspired by Tronto/ Lawson’s geographies of care and Dewey-an pragmatism, this essay calls for the rise of ‘planning of care’. Planning of care not only recognises humans’ interdependency on one another, but also acknowledges cities’ on-going, dialectic relationship with their natural surroundings.', 'corpus_id': 220535649, 'score': 0}]"
62	meta learning	7d06819623f5d38ad26c4367a6055d44	14859	{}	[{'doc_id': '235658346', 'title': 'R-Drop: Regularized Dropout for Neural Networks', 'abstract': 'Dropout is a powerful and widely used technique to regularize the training of deep neural networks. In this paper, we introduce a simple regularization strategy upon dropout in model training, namely R-Drop, which forces the output distributions of different sub models generated by dropout to be consistent with each other. Specifically, for each training sample, R-Drop minimizes the bidirectional KL-divergence between the output distributions of two sub models sampled by dropout. Theoretical analysis reveals that R-Drop reduces the freedom of the model parameters and complements dropout. Experiments on 5 widely used deep learning tasks (18 datasets in total), including neural machine translation, abstractive summarization, language understanding, language modeling, and image classification, show that R-Drop is universally effective. In particular, it yields substantial improvements when applied to fine-tune large-scale pre-trained models, e.g., ViT, RoBERTa-large, and BART, and achieves state-of-the-art (SOTA) performances with the vanilla Transformer model on WMT14 English→German translation (30.91 BLEU) and WMT14 English→French translation (43.95 BLEU), even surpassing models trained with extra large-scale data and expert-designed advanced variants of Transformer models. Our code is available at GitHub1.', 'corpus_id': 235658346, 'score': 0}, {'doc_id': '228082357', 'title': 'Meta-Regularization by Enforcing Mutual-Exclusiveness', 'abstract': 'Meta-learning models have two objectives. First, they need to be able to make predictions over a range of task distributions while utilizing only a small amount of training data. Second, they also need to adapt to new novel unseen tasks at meta-test time again by using only a small amount of training data from that task. It is the second objective where meta-learning models fail for non-mutually exclusive tasks due to task overfitting. Given that guaranteeing mutually exclusive tasks is often difficult, there is a significant need for regularization methods that can help reduce the impact of task-memorization in meta-learning. For example, in the case of N-way, K-shot classification problems, tasks becomes non-mutually exclusive when the labels associated with each task is fixed. Under this design, the model will simply memorize the class labels of all the training tasks, and thus will fail to recognize a new task (class) at meta-test time. A direct observable consequence of this memorization is that the meta-learning model simply ignores the task-specific training data in favor of directly classifying based on the test-data input. In our work, we propose a regularization technique for meta-learning models that gives the model designer more control over the information flow during meta-training. Our method consists of a regularization function that is constructed by maximizing the distance between tasksummary statistics, h(D i ), in the case of black-box models and task specific network parameters (φi) in the case of optimization based models during meta-training. Maximizing the proposed regularization function reduces task-overfitting because both h(D i ) and (φi) are computed using task training data (D tri i ), and thus encourages the model to use D i during meta-training. Our proposed regularization function shows an accuracy boost of ∼ 36% on the Omniglot dataset for 5-way, 1-shot classification using black-box method and for 20-way, 1-shot classification problem using optimization-based method. 1', 'corpus_id': 228082357, 'score': 1}, {'doc_id': '235490524', 'title': 'Iterative Network Pruning with Uncertainty Regularization for Lifelong Sentiment Classification', 'abstract': 'Lifelong learning capabilities are crucial for sentiment classifiers to process continuous streams of opinioned information on the Web. However, performing lifelong learning is non-trivial for deep neural networks as continually training of incrementally available information inevitably results in catastrophic forgetting or interference. In this paper, we propose a novel i terative network p runing with uncertainty r egularization method for l ifelong s entiment classification (IPRLS), which leverages the principles of network pruning and weight regularization. By performing network pruning with uncertainty regularization in an iterative manner, IPRLS can adapt a single BERT model to work with continuously arriving data from multiple domains while avoiding catastrophic forgetting and interference. Specifically, we leverage an iterative pruning method to remove redundant parameters in large deep networks so that the freed-up space can then be employed to learn new tasks, tackling the catastrophic forgetting problem. Instead of keeping the old-tasks fixed when learning new tasks, we also use an uncertainty regularization based on the Bayesian online learning framework to constrain the update of old tasks weights in BERT, which enables positive backward transfer, i.e. learning new tasks improves performance on past tasks while protecting old knowledge from being lost. In addition, we propose a task-specific low-dimensional residual function in parallel to each layer of BERT, which makes IPRLS less prone to losing the knowledge saved in the base BERT network when learning a new task. Extensive experiments on 16 popular review corpora demonstrate that the proposed IPRLS method significantly outperforms the strong baselines for lifelong sentiment classification. For reproducibility, we submit the code and data at: \\urlhttps://github.com/siat-nlp/IPRLS .', 'corpus_id': 235490524, 'score': 0}, {'doc_id': '235344366', 'title': 'Meta-learning with few-shot models Analysis Final Project', 'abstract': 'This project focuses on understanding the various elements of Meta-learning and few-shot models and the effectiveness of the different detailed implementation approaches. Using the default RobustQA project as a baseline, we explored the different implementations of the Meta-learning algorithm, LEOPARD[1], and evaluate the impact on performance of the prediction accuracy. We have also experimented with the eval-every parameter to understand how fast each implementation can learn when presented with the out of domain questions initially. We found that the multiple datasets implementation of the Leopard algorithm yields the best few-shot result. On the first evaluation at step O (after 1 batch of data for learning) this implementation already achieving a result of a EM score of 34.55 (on the validation set) compared to the 32 EM scores that the other implementation and the baseline are getting. However, after the model is trained for a longer time, we found that the baseline can actually achieve a better EM score overall with 42.202 on the test set. Although, the difference in the overall accuracy of the test set score are very small for different implementations, we found the more simple implementation yields better accuracy in the long run. Our key finding is that the design of a few-shot learning algorithm or model is actually a trade off between few-shot accuracy and the overall highest achievable accuracy. 1 Key Information to include ¢ Mentor: Rui Wang (ruil @stanford.edu) ¢ External Collaborators (if you have any): None ¢ Sharing project: None', 'corpus_id': 235344366, 'score': 0}, {'doc_id': '235313639', 'title': 'Adversarially Adaptive Normalization for Single Domain Generalization', 'abstract': 'Single domain generalization aims to learn a model that performs well on many unseen domains with only one domain data for training. Existing works focus on studying the adversarial domain augmentation (ADA) to improve the model’s generalization capability. The impact on domain generalization of the statistics of normalization layers is still underinvestigated. In this paper, we propose a generic normalization approach, adaptive standardization and rescaling normalization (ASR-Norm), to complement the missing part in previous works. ASR-Norm learns both the standardization and rescaling statistics via neural networks. This new form of normalization can be viewed as a generic form of the traditional normalizations. When trained with ADA, the statistics in ASR-Norm are learned to be adaptive to the data coming from different domains, and hence improves the model generalization performance across domains, especially on the target domain with large discrepancy from the source domain. The experimental results show that ASR-Norm can bring consistent improvement to the state-of-the-art ADA approaches by 1.6%, 2.7%, and 6.3% averagely on the Digits, CIFAR-10-C, and PACS benchmarks, respectively. As a generic tool, the improvement introduced by ASR-Norm is agnostic to the choice of ADA methods.', 'corpus_id': 235313639, 'score': 0}, {'doc_id': '235422615', 'title': 'Knowledge Consolidation based Class Incremental Online Learning with Limited Data', 'abstract': 'We propose a novel approach for class incremental online learning in a limited data setting. This problem setting is challenging because of the following constraints: (1) Classes are given incrementally, which necessitates a class incremental learning approach; (2) Data for each class is given in an online fashion, i.e., each training example is seen only once during training; (3) Each class has very few training examples; and (4) We do not use or assume access to any replay/memory to store data from previous classes. Therefore, in this setting, we have to handle twofold problems of catastrophic forgetting and overfitting. In our approach, we learn robust representations that are generalizable across tasks without suffering from the problems of catastrophic forgetting and overfitting to accommodate future classes with limited samples. Our proposed method leverages the meta-learning framework with knowledge consolidation. The meta-learning framework helps the model for rapid learning when samples appear in an online fashion. Simultaneously, knowledge consolidation helps to learn a robust representation against forgetting under online updates to facilitate future learning. Our approach significantly outperforms other methods on several benchmarks.', 'corpus_id': 235422615, 'score': 0}, {'doc_id': '49868626', 'title': 'Meta-Learning with Latent Embedding Optimization', 'abstract': 'Gradient-based meta-learning techniques are both widely applicable and proficient at solving challenging few-shot learning and fast adaptation problems. However, they have practical difficulties when operating on high-dimensional parameter spaces in extreme low-data regimes. We show that it is possible to bypass these limitations by learning a data-dependent latent generative representation of model parameters, and performing gradient-based meta-learning in this low-dimensional latent space. The resulting approach, latent embedding optimization (LEO), decouples the gradient-based adaptation procedure from the underlying high-dimensional space of model parameters. Our evaluation shows that LEO can achieve state-of-the-art performance on the competitive miniImageNet and tieredImageNet few-shot classification tasks. Further analysis indicates LEO is able to capture uncertainty in the data, and can perform adaptation more effectively by optimizing in latent space.', 'corpus_id': 49868626, 'score': 1}, {'doc_id': '6719686', 'title': 'Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks', 'abstract': 'We propose an algorithm for meta-learning that is model-agnostic, in the sense that it is compatible with any model trained with gradient descent and applicable to a variety of different learning problems, including classification, regression, and reinforcement learning. The goal of meta-learning is to train a model on a variety of learning tasks, such that it can solve new learning tasks using only a small number of training samples. In our approach, the parameters of the model are explicitly trained such that a small number of gradient steps with a small amount of training data from a new task will produce good generalization performance on that task. In effect, our method trains the model to be easy to fine-tune. We demonstrate that this approach leads to state-of-the-art performance on two few-shot image classification benchmarks, produces good results on few-shot regression, and accelerates fine-tuning for policy gradient reinforcement learning with neural network policies.', 'corpus_id': 6719686, 'score': 1}, {'doc_id': '232135069', 'title': 'Clusterability in Neural Networks', 'abstract': 'The learned weights of a neural network have often been considered devoid of scrutable internal structure. In this paper, however, we look for structure in the form of clusterability: how well a network can be divided into groups of neurons with strong internal connectivity but weak external connectivity. We find that a trained neural network is typically more clusterable than randomly initialized networks, and often clusterable relative to random networks with the same distribution of weights. We also exhibit novel methods to promote clusterability in neural network training, and find that in multi-layer perceptrons they lead to more clusterable networks with little reduction in accuracy. Understanding and controlling the clusterability of neural networks will hopefully render their inner workings more interpretable to engineers by facilitating partitioning into meaningful clusters. Modularity is a common property of biological and engineered systems (Clune, Mouret, and Lipson 2013; Baldwin and Clark 2000; Booch et al. 2007). Reasons for modularity include adaptability and the ability to handle different situations with common sub-problems. It is also desirable from a perspective of transparency: modular systems allow those analyzing the system to inspect the function of individual modules and combine their understanding of each into an understanding of the entire system. In this work, we study a graph-theoretic analog to modularity: the extent to which a network can be partitioned into sets of neurons where each set is strongly internally connected, but only weakly connected to other sets. This definition refers only to the learned weights of the network, not to the data distribution, nor to the distributions of outputs or activations of the model. More specifically, we use a spectral clustering algorithm (Shi and Malik 2000) to decompose trained networks into clusters, and measure the goodness of this decomposition. Since any degree of non-uniformity of weights can induce clusterability, we also measure the relative clusterability of a network compared to networks with the same set of weights in each layer but shuffled randomly, in order to determine whether any clusterability is simply due to each layer’s distribution of weights. We conduct an empirical investigation into the clusterability of multi-layer perceptrons (MLPs) and convolutional neural networks (CNNs) trained on MNIST, Fashion* Primary authors. Figure 1: A pruned neural network, split into clusters. MNIST, and CIFAR-10 (LeCun et al. 1998; Xiao, Rasul, and Vollgraf 2017; Krizhevsky 2009), using weight pruning and other regularization methods. We also test if clusterability can be induced by training on datasets that benefit from some degree of parallelism to classify. In addition, we test the clusterability of neural networks trained by other researchers in the VGG, ResNet, and Inception families (Simonyan and Zisserman 2015; He et al. 2016a; Szegedy et al. 2016) for ImageNet classification (Deng et al. 2009). Finally, we investigate two ways of training neural networks specifically to promote clusterability: regularizing for clusterability and clusterable initialization. Our main contributions are: • Presenting a definition of absolute and relative clusterability of a neural network (section 1). • Showing that trained neural networks are often more absolutely clusterable than randomly initialized networks (sections 2 and 3). • Showing that neural networks trained with dropout and/or weight pruning are typically relatively clusterable, and often more clusterable than all 50 shuffled networks to which we compare them (sections 2 and 3). • Showing that large neural networks trained for state-ofthe-art ImageNet classification are reliably more clusterable than 99 out of 100 shuffled networks, and that a midsized VGG trained with dropout and weight pruning on CIFAR-10 classification reliably produces networks more clusterable than all 50 of their shuffles (section 3). ar X iv :2 10 3. 03 38 6v 1 [ cs .N E ] 4 M ar 2 02 1 • Demonstrating novel methods of promoting clusterability in MLPs in a way that is compatible with standard neural network architectures and training procedures, with little loss in accuracy (section 4). Code is available at https://github.com/dfilan/ clusterability in neural networks. 1 Clustering Neural Networks', 'corpus_id': 232135069, 'score': 1}, {'doc_id': '201651077', 'title': 'On the Convergence Theory of Gradient-Based Model-Agnostic Meta-Learning Algorithms', 'abstract': 'We study the convergence of a class of gradient-based Model-Agnostic Meta-Learning (MAML) methods and characterize their overall complexity as well as their best achievable accuracy in terms of gradient norm for nonconvex loss functions. We start with the MAML method and its first-order approximation (FO-MAML) and highlight the challenges that emerge in their analysis. By overcoming these challenges not only we provide the first theoretical guarantees for MAML and FO-MAML in nonconvex settings, but also we answer some of the unanswered questions for the implementation of these algorithms including how to choose their learning rate and the batch size for both tasks and datasets corresponding to tasks. In particular, we show that MAML can find an $\\epsilon$-first-order stationary point ($\\epsilon$-FOSP) for any positive $\\epsilon$ after at most $\\mathcal{O}(1/\\epsilon^2)$ iterations at the expense of requiring second-order information. We also show that FO-MAML which ignores the second-order information required in the update of MAML cannot achieve any small desired level of accuracy, i.e., FO-MAML cannot find an $\\epsilon$-FOSP for any $\\epsilon>0$. We further propose a new variant of the MAML algorithm called Hessian-free MAML which preserves all theoretical guarantees of MAML, without requiring access to second-order information.', 'corpus_id': 201651077, 'score': 1}]
63	joint neural	9c3b958853365f08bb5d2eb3c19aa8ed	19684	{}	[{'doc_id': '236459903', 'title': 'Recursive Tree-Structured Self-Attention for Answer Sentence Selection', 'abstract': 'Syntactic structure is an important component of natural language text. Recent topperforming models in Answer Sentence Selection (AS2) use self-attention and transfer learning, but not syntactic structure. Tree structures have shown strong performance in tasks with sentence pair input like semantic relatedness. We investigate whether tree structures can boost performance in AS2. We introduce the Tree Aggregation Transformer: a novel recursive, tree-structured self-attention model for AS2. The recursive nature of our model is able to represent all levels of syntactic parse trees with only one additional self-attention layer. Without transfer learning, we establish a new state of the art on the popular TrecQA and WikiQA benchmark datasets. Additionally, we evaluate our method on four Community Question Answering datasets, and find that tree-structured representations have limitations with noisy user-generated text. We conduct probing experiments to evaluate how our models leverage tree structures across datasets. Our findings show that the ability of treestructured models to successfully absorb syntactic information is strongly correlated with a higher performance in AS2.', 'corpus_id': 236459903, 'score': 0}, {'doc_id': '226283686', 'title': 'An Investigation of Potential Function Designs for Neural CRF', 'abstract': 'The neural linear-chain CRF model is one of the most widely-used approach to sequence labeling. In this paper, we investigate a series of increasingly expressive potential functions for neural CRF models, which not only integrate the emission and transition functions, but also explicitly take the representations of the contextual words as input. Our extensive experiments show that the decomposed quadrilinear potential function based on the vector representations of two neighboring labels and two neighboring words consistently achieves the best performance.', 'corpus_id': 226283686, 'score': 1}, {'doc_id': '236486152', 'title': 'Incorporating EDS Graph for AMR Parsing', 'abstract': 'AMR (Abstract Meaning Representation) and EDS (Elementary Dependency Structures) are two popular meaning representations in NLP/NLU. AMR is more abstract and conceptual, while EDS is more low level, closer to the lexical structures of the given sentences. It is thus not surprising that EDS parsing is easier than AMR parsing. In this work, we consider using information from EDS parsing to help improve the performance of AMR parsing. We adopt a transition-based parser and propose to add EDS graphs as additional semantic features using a graph encoder composed of LSTM layer and GCN layer. Our experimental results show that the additional information from EDS parsing indeed gives a boost to the performance of the base AMR parser used in our experiments.', 'corpus_id': 236486152, 'score': 0}, {'doc_id': '28320509', 'title': 'Conditional Random Field and Deep Feature Learning for Hyperspectral Image Classification', 'abstract': 'Image classification is considered to be one of the critical tasks in hyperspectral remote sensing image processing. Recently, a convolutional neural network (CNN) has established itself as a powerful model in classification by demonstrating excellent performances. The use of a graphical model such as a conditional random field (CRF) contributes further in capturing contextual information and thus improving the classification performance. In this paper, we propose a method to classify hyperspectral images by considering both spectral and spatial information via a combined framework consisting of CNN and CRF. We use multiple spectral band groups to learn deep features using CNN, and then formulate deep CRF with CNN-based unary and pairwise potential functions to effectively extract the semantic correlations between patches consisting of 3-D data cubes. Furthermore, we introduce a deep deconvolution network that improves the final classification performance. We also introduced a new data set and experimented our proposed method on it along with several widely adopted benchmark data sets to evaluate the effectiveness of our method. By comparing our results with those from several state-of-the-art models, we show the promising potential of our method.', 'corpus_id': 28320509, 'score': 1}, {'doc_id': '2448400', 'title': 'Construction and architecture of a dedicated compile server', 'abstract': 'A compilation server has been b uilt upon a microkernel, emplo ying a lightweight dedicated remote procedure call protocol, a reduced but f ast local file system and input/output libraries which exploit these characteristics. Theperformance of the compilation server remote procedure call protocol has been measured and it can out perform NFS by a f actor of five. The architecture of the serv er and clients also contribute to increased per formance and ease of maintenance. Introduction Traditionally a department network is constructed around a large filestore which serves a number of clients. The filestore is normally directly manipulated by a dedicated fileserver host which obe ys remote procedure calls issued by the client. The performance of such a configuration is limited by the network throughput, the fileserver and client response time. In a teaching department where laboratory work is being undertaken on many clients, great demands are briefly placed on a fileserver and clients for the duration of the timetabled laboratory. There is a continual push to maximize laboratory resources which is countered by technology obsolescence. Typically a workstation has a realistic life span of three years. It is often the case that after this period the older machines are percei ved as being of less use. Althoughthe new machines might ha ve improved resources, the older machines, while unable to deli ver the required performance of ne w applications might be capable of functioning as a dedicated server. As Pike reported, the workstation reaches redundanc y quickly as it is too slo w for fast compilation and too e xpensi ve just to be used as an X terminal. The approach tak en by Plan 9 and Chorus 1 is to rely more on specialist system and application serv ers. Users interact with ine xpensi ve graphic terminals in the case of Plan 9 and X terminals with Chorus. This method is pragmatic and utilizes appropriate hardware for a particular task. The approach presented here is to examine a specialist application server on a network supporting two operating systems. This paper will report on the benefits of a compilation server, the design decisions tak en and some preliminary performance results of', 'corpus_id': 2448400, 'score': 0}, {'doc_id': '102350997', 'title': 'Unsupervised Recurrent Neural Network Grammars', 'abstract': 'Recurrent neural network grammars (RNNG) are generative models of language which jointly model syntax and surface structure by incrementally generating a syntax tree and sentence in a top-down, left-to-right order. Supervised RNNGs achieve strong language modeling and parsing performance, but require an annotated corpus of parse trees. In this work, we experiment with unsupervised learning of RNNGs. Since directly marginalizing over the space of latent trees is intractable, we instead apply amortized variational inference. To maximize the evidence lower bound, we develop an inference network parameterized as a neural CRF constituency parser. On language modeling, unsupervised RNNGs perform as well their supervised counterparts on benchmarks in English and Chinese. On constituency grammar induction, they are competitive with recent neural language models that induce tree structures from words through attention mechanisms.', 'corpus_id': 102350997, 'score': 1}, {'doc_id': '236459941', 'title': 'Structural Guidance for Transformer Language Models', 'abstract': 'Transformer-based language models pretrained on large amounts of text data have proven remarkably successful in learning generic transferable linguistic representations. Here we study whether structural guidance leads to more human-like systematic linguistic generalization in Transformer language models without resorting to pre-training on very large amounts of data. We explore two general ideas. The “Generative Parsing” idea jointly models the incremental parse and word sequence as part of the same sequence modeling task. The “Structural Scaffold” idea guides the language model’s representation via additional structure loss that separately predicts the incremental constituency parse. We train the proposed models along with a vanilla Transformer language model baseline on a 14 million-token and a 46 million-token subset of the BLLIP dataset, and evaluate models’ syntactic generalization performances on SG Test Suites and sized BLiMP. Experiment results across two benchmarks suggest converging evidence that generative structural supervisions can induce more robust and humanlike linguistic generalization in Transformer language models without the need for data intensive pre-training.', 'corpus_id': 236459941, 'score': 0}, {'doc_id': '543551', 'title': 'Neural CRF Parsing', 'abstract': 'This paper describes a parsing model that combines the exact dynamic programming of CRF parsing with the rich nonlinear featurization of neural net approaches. Our model is structurally a CRF that factors over anchored rule productions, but instead of linear potential functions based on sparse features, we use nonlinear potentials computed via a feedforward neural network. Because potentials are still local to anchored rules, structured inference (CKY) is unchanged from the sparse case. Computing gradients during learning involves backpropagating an error signal formed from standard CRF sufficient statistics (expected rule counts). Using only dense features, our neural CRF already exceeds a strong baseline CRF model (Hall et al., 2014). In combination with sparse features, our system achieves 91.1 F1 on section 23 of the Penn Treebank, and more generally outperforms the best prior single parser results on a range of languages.', 'corpus_id': 543551, 'score': 1}, {'doc_id': '236460293', 'title': 'Discriminative Reranking for Neural Machine Translation', 'abstract': 'Reranking models enable the integration of rich features to select a better output hypothesis within an n-best list or lattice. These models have a long history in NLP, and we revisit discriminative reranking for modern neural machine translation models by training a large transformer architecture. This takes as input both the source sentence as well as a list of hypotheses to output a ranked list. The reranker is trained to predict the observed distribution of a desired metric, e.g. BLEU, over the n-best list. Since such a discriminator contains hundreds of millions of parameters, we improve its generalization using pre-training and data augmentation techniques. Experiments on four WMT directions show that our discriminative reranking approach is effective and complementary to existing generative reranking approaches, yielding improvements of up to 4 BLEU over the beam search output.', 'corpus_id': 236460293, 'score': 0}, {'doc_id': '235727618', 'title': 'R2D2: Recursive Transformer based on Differentiable Tree for Interpretable Hierarchical Language Modeling', 'abstract': 'Human language understanding operates at multiple levels of granularity (e.g., words, phrases, and sentences) with increasing levels of abstraction that can be hierarchically combined. However, existing deep models with stacked layers do not explicitly model any sort of hierarchical process. In this paper, we propose a recursive Transformer model based on differentiable CKY style binary trees to emulate this composition process, and we extend the bidirectional language model pre-training objective to this architecture, attempting to predict each word given its left and right abstraction nodes. To scale up our approach, we also introduce an efficient pruning and growing algorithm to reduce the time complexity and enable encoding in linear time. Experimental results on language modeling and unsupervised parsing show the effectiveness of our approach.', 'corpus_id': 235727618, 'score': 1}]
64	Sensor-Fusion	27f277dba965e748bb9223a759032415	2533	{}	"[{'doc_id': '210943036', 'title': 'ImVoteNet: Boosting 3D Object Detection in Point Clouds With Image Votes', 'abstract': '3D object detection has seen quick progress thanks to advances in deep learning on point clouds. A few recent works have even shown state-of-the-art performance with just point clouds input (e.g. VoteNet). However, point cloud data have inherent limitations. They are sparse, lack color information and often suffer from sensor noise. Images, on the other hand, have high resolution and rich texture. Thus they can complement the 3D geometry provided by point clouds. Yet how to effectively use image information to assist point cloud based detection is still an open question. In this work, we build on top of VoteNet and propose a 3D detection architecture called ImVoteNet specialized for RGB-D scenes. ImVoteNet is based on fusing 2D votes in images and 3D votes in point clouds. Compared to prior work on multi-modal detection, we explicitly extract both geometric and semantic features from the 2D images. We leverage camera parameters to lift these features to 3D. To improve the synergy of 2D-3D feature fusion, we also propose a multi-tower training scheme. We validate our model on the challenging SUN RGB-D dataset, advancing state-of-the-art results by 5.7 mAP. We also provide rich ablation studies to analyze the contribution of each design choice.', 'corpus_id': 210943036, 'score': 1}, {'doc_id': '210702638', 'title': 'Indoor Layout Estimation by 2D LiDAR and Camera Fusion', 'abstract': 'This paper presents an algorithm for indoor layout estimation and reconstruction through the fusion of a sequence of captured images and LiDAR data sets. In the proposed system, a movable platform collects both intensity images and 2D LiDAR information. Pose estimation and semantic segmentation is computed jointly by aligning the LiDAR points to line segments from the images. For indoor scenes with walls orthogonal to floor, the alignment problem is decoupled into top-down view projection and a 2D similarity transformation estimation and solved by the recursive random sample consensus (R-RANSAC) algorithm. Hypotheses can be generated, evaluated and optimized by integrating new scans as the platform moves throughout the environment. The proposed method avoids the need of extensive prior training or a cuboid layout assumption, which is more effective and practical compared to most previous indoor layout estimation methods. Multi-sensor fusion allows the capability of providing accurate depth estimation and high resolution visual information.', 'corpus_id': 210702638, 'score': 1}, {'doc_id': '156051565', 'title': 'Multi-Task Multi-Sensor Fusion for 3D Object Detection', 'abstract': ""In this paper we propose to exploit multiple related tasks for accurate multi-sensor 3D object detection. Towards this goal we present an end-to-end learnable architecture that reasons about 2D and 3D object detection as well as ground estimation and depth completion. Our experiments show that all these tasks are complementary and help the network learn better representations by fusing information at various levels. Importantly, our approach leads the KITTI benchmark on 2D, 3D and bird's eye view object detection, while being real-time."", 'corpus_id': 156051565, 'score': 1}, {'doc_id': '52211898', 'title': 'Deep Continuous Fusion for Multi-sensor 3D Object Detection', 'abstract': 'In this paper, we propose a novel 3D object detector that can exploit both LIDAR as well as cameras to perform very accurate localization. Towards this goal, we design an end-to-end learnable architecture that exploits continuous convolutions to fuse image and LIDAR feature maps at different levels of resolution. Our proposed continuous fusion layer encode both discrete-state image features as well as continuous geometric information. This enables us to design a novel, reliable and efficient end-to-end learnable 3D object detector based on multiple sensors. Our experimental evaluation on both KITTI as well as a large scale 3D object detection benchmark shows significant improvements over the state of the art.', 'corpus_id': 52211898, 'score': 1}, {'doc_id': '208118518', 'title': 'RGB-D Odometry and SLAM', 'abstract': 'The emergence of modern RGB-D sensors had a significant impact in many application fields, including robotics, augmented reality (AR), and 3D scanning. They are low-cost, low-power, and low-size alternatives to traditional range sensors such as LiDAR. Moreover, unlike RGB cameras, RGB-D sensors provide the additional depth information that removes the need of frame-by-frame triangulation for 3D scene reconstruction. These merits have made them very popular in mobile robotics and AR, where it is of great interest to estimate ego-motion and 3D scene structure. Such spatial understanding can enable robots to navigate autonomously without collisions and allow users to insert virtual entities consistent with the image stream. In this chapter, we review common formulations of odometry and Simultaneous Localization and Mapping (known by its acronym SLAM) using RGB-D stream input. The two topics are closely related, as the former aims to track the incremental camera motion with respect to a local map of the scene, and the latter to jointly estimate the camera trajectory and the global map with consistency. In both cases, the standard approaches minimize a cost function using nonlinear optimization techniques. This chapter consists of three main parts: In the first part, we introduce the basic concept of odometry and SLAM and motivate the use of RGB-D sensors. We also give mathematical preliminaries relevant to most odometry and SLAM algorithms. In the second part, we detail the three main components of SLAM systems: camera pose tracking, scene mapping, and loop closing. For each component, we describe different approaches proposed in the literature. In the final part, we provide a brief discussion on advanced research topics with the references to the state of the art.', 'corpus_id': 208118518, 'score': 0}, {'doc_id': '210714109', 'title': 'Spatiotemporal Camera-LiDAR Calibration: A Targetless and Structureless Approach', 'abstract': 'The demand for multimodal sensing systems for robotics is growing due to the increase in robustness, reliability and accuracy offered by these systems. These systems also need to be spatially and temporally co-registered to be effective. In this letter, we propose a targetless and structureless spatiotemporal camera-LiDAR calibration method. Our method combines a closed-form solution with a modified structureless bundle adjustment where the coarse-to-fine approach does not require an initial guess on the spatiotemporal parameters. Also, as 3D features (structure) are calculated from triangulation only, there is no need to have a calibration target or to match 2D features with the 3D point cloud which provides flexibility in the calibration process and sensor configuration. We demonstrate the accuracy and robustness of the proposed method through both simulation and real data experiments using multiple sensor payload configurations mounted to hand-held, aerial and legged robot systems. Also, qualitative results are given in the form of a colorized point cloud visualization.', 'corpus_id': 210714109, 'score': 0}, {'doc_id': '211677612', 'title': '3D Point Cloud Processing and Learning for Autonomous Driving', 'abstract': 'We present a review of 3D point cloud processing and learning for autonomous driving. As one of the most important sensors in autonomous vehicles, light detection and ranging (LiDAR) sensors collect 3D point clouds that precisely record the external surfaces of objects and scenes. The tools for 3D point cloud processing and learning are critical to the map creation, localization, and perception modules in an autonomous vehicle. While much attention has been paid to data collected from cameras, such as images and videos, an increasing number of researchers have recognized the importance and significance of LiDAR in autonomous driving and have proposed processing and learning algorithms to exploit 3D point clouds. We review the recent progress in this research area and summarize what has been tried and what is needed for practical and safe autonomous vehicles. We also offer perspectives on open issues that are needed to be solved in the future.', 'corpus_id': 211677612, 'score': 0}, {'doc_id': '212675370', 'title': 'Confidence Guided Stereo 3D Object Detection with Split Depth Estimation', 'abstract': 'Accurate and reliable 3D object detection is vital to safe autonomous driving. Despite recent developments, the performance gap between stereo-based methods and LiDAR-based methods is still considerable. Accurate depth estimation is crucial to the performance of stereo-based 3D object detection methods, particularly for those pixels associated with objects in the foreground. Moreover, stereo-based methods suffer from high variance in the depth estimation accuracy, which is often not considered in the object detection pipeline. To tackle these two issues, we propose CG-Stereo, a confidence-guided stereo 3D object detection pipeline that uses separate decoders for foreground and background pixels during depth estimation, and leverages the confidence estimation from the depth estimation network as a soft attention mechanism in the 3D object detector. Our approach outperforms all state-of-the-art stereo-based 3D detectors on the KITTI benchmark.', 'corpus_id': 212675370, 'score': 0}, {'doc_id': '211677466', 'title': 'D3VO: Deep Depth, Deep Pose and Deep Uncertainty for Monocular Visual Odometry', 'abstract': 'We propose D3VO as a novel framework for monocular visual odometry that exploits deep networks on three levels -- deep depth, pose and uncertainty estimation. We first propose a novel self-supervised monocular depth estimation network trained on stereo videos without any external supervision. In particular, it aligns the training image pairs into similar lighting condition with predictive brightness transformation parameters. Besides, we model the photometric uncertainties of pixels on the input images, which improves the depth estimation accuracy and provides a learned weighting function for the photometric residuals in direct (feature-less) visual odometry. Evaluation results show that the proposed network outperforms state-of-the-art self-supervised depth estimation networks. D3VO tightly incorporates the predicted depth, pose and uncertainty into a direct visual odometry method to boost both the front-end tracking as well as the back-end non-linear optimization. We evaluate D3VO in terms of monocular visual odometry on both the KITTI odometry benchmark and the EuRoC MAV dataset. The results show that D3VO outperforms state-of-the-art traditional monocular VO methods by a large margin. It also achieves comparable results to state-of-the-art stereo/LiDAR odometry on KITTI and to the state-of-the-art visual-inertial odometry on EuRoC MAV, while using only a single camera.', 'corpus_id': 211677466, 'score': 0}, {'doc_id': '131777068', 'title': 'Sensor Fusion for Joint 3D Object Detection and Semantic Segmentation', 'abstract': 'In this paper, we present an extension to LaserNet, an efficient and state-of-the-art LiDAR based 3D object detector. We propose a method for fusing image data with the LiDAR data and show that this sensor fusion method improves the detection performance of the model especially at long ranges. The addition of image data is straightforward and does not require image labels. Furthermore, we expand the capabilities of the model to perform 3D semantic segmentation in addition to 3D object detection. On a large benchmark dataset, we demonstrate our approach achieves state-of-the-art performance on both object detection and semantic segmentation while maintaining a low runtime.', 'corpus_id': 131777068, 'score': 1}]"
65	Automating discovery	da7a6206e442f7fc2998bb24f7a95c8e	8072	{}	"[{'doc_id': '24832185', 'title': 'Automating drug discovery', 'abstract': 'Small-molecule drug discovery can be viewed as a challenging multidimensional problem in which various characteristics of compounds — including efficacy, pharmacokinetics and safety — need to be optimized in parallel to provide drug candidates. Recent advances in areas such as microfluidics-assisted chemical synthesis and biological testing, as well as artificial intelligence systems that improve a design hypothesis through feedback analysis, are now providing a basis for the introduction of greater automation into aspects of this process. This could potentially accelerate time frames for compound discovery and optimization and enable more effective searches of chemical space. However, such approaches also raise considerable conceptual, technical and organizational challenges, as well as scepticism about the current hype around them. This article aims to identify the approaches and technologies that could be implemented robustly by medicinal chemists in the near future and to critically analyse the opportunities and challenges for their more widespread application.', 'corpus_id': 24832185, 'score': 1}, {'doc_id': '219708401', 'title': 'Mining Personalized Climate Preferences for Assistant Driving', 'abstract': ""Both assistant driving and self-driving have attracted a great amount of attention in the last few years. However, the majority of research efforts focus on safe driving; few research has been conducted on in-vehicle climate control, or assistant driving based on travellers' personal habits or preferences. In this paper, we propose a novel approach for climate control, driver behavior recognition and driving recommendation for better fitting drivers' preferences in their daily driving. The algorithm consists three components: (1) A in-vehicle sensing and context feature enriching compnent with a Internet of Things (IoT) platform for collecting related environment, vehicle-running, and traffic parameters that affect drivers' behaviors. (2) A non-intrusive intelligent driver behaviour and vehicle status detection component, which can automatically label vehicle's status (open windows, turn on air condition, etc.), based on results of applying further feature extraction and machine learning algorithms. (3) A personalized driver habits learning and preference recommendation component for more healthy and comfortable experiences. A prototype using a client-server architecture with an iOS app and an air-quality monitoring sensor has been developed for collecting heterogeneous data and testing our algorithms. Real-world experiments on driving data of 11,370 km (320 hours) by different drivers in multiple cities worldwide have been conducted, which demonstrate the effective and accuracy of our approach."", 'corpus_id': 219708401, 'score': 0}, {'doc_id': '220962215', 'title': 'Quantum-accessible reinforcement learning beyond strictly epochal environments', 'abstract': 'In recent years, quantum-enhanced machine learning has emerged as a particularly fruitful application of quantum algorithms, covering aspects of supervised, unsupervised and reinforcement learning. Reinforcement learning offers numerous options of how quantum theory can be applied, and is arguably the least explored, from a quantum perspective. Here, an agent explores an environment and tries to find a behavior optimizing some figure of merit. Some of the first approaches investigated settings where this exploration can be sped-up, by considering quantum analogs of classical environments, which can then be queried in superposition. If the environments have a strict periodic structure in time (i.e. are strictly episodic), such environments can be effectively converted to conventional oracles encountered in quantum information. However, in general environments, we obtain scenarios that generalize standard oracle tasks. In this work, we consider one such generalization, where the environment is not strictly episodic, which is mapped to an oracle identification setting with a changing oracle. We analyze this case and show that standard amplitude-amplification techniques can, with minor modifications, still be applied to achieve quadratic speed-ups. In addition, we prove that an algorithm based on Grover iterations is optimal for oracle identification even if the oracle changes over time in a way that the “rewarded space” is monotonically increasing. This result constitutes one of the first generalizations of quantum-accessible reinforcement learning.', 'corpus_id': 220962215, 'score': 0}, {'doc_id': '219966508', 'title': 'Advantages of biologically-inspired adaptive neural activation in RNNs during learning', 'abstract': 'Dynamic adaptation in single-neuron response plays a fundamental role in neural coding in biological neural networks. Yet, most neural activation functions used in artificial networks are fixed and mostly considered as an inconsequential architecture choice. In this paper, we investigate nonlinear activation function adaptation over the large time scale of learning, and outline its impact on sequential processing in recurrent neural networks. We introduce a novel parametric family of nonlinear activation functions, inspired by input-frequency response curves of biological neurons, which allows interpolation between well-known activation functions such as ReLU and sigmoid. Using simple numerical experiments and tools from dynamical systems and information theory, we study the role of neural activation features in learning dynamics. We find that activation adaptation provides distinct task-specific solutions and in some cases, improves both learning speed and performance. Importantly, we find that optimal activation features emerging from our parametric family are considerably different from typical functions used in the literature, suggesting that exploiting the gap between these usual configurations can help learning. Finally, we outline situations where neural activation adaptation alone may help mitigate changes in input statistics in a given task, suggesting mechanisms for transfer learning optimization.', 'corpus_id': 219966508, 'score': 0}, {'doc_id': '220633102', 'title': 'Implementation feedback of the IVOA Provenance data model', 'abstract': 'The IVOA Provenance Data model defines entities, agents and activities as container classes to describe the provenance of datasets, with the executed tasks and responsibilities attached to agents. It also provides a set of classes to describe the activities type and their configuration template, as well as the configuration applied effectively during the execution of a task. Here we highlight lessons learned in the implementation of the CDS ProvHiPS service distributing provenance metadata for the HST HiPS data collections, and for the HST archive original images used to produce the HiPS tiles. ProvHiPS is based on the ProvTAP protocol, the emerging TAP standard for distributing provenance metadata. ProvTAP queries may rapidly become very complex. Various graph representation strategies, including ad hoc solutions, triplestore and SQL CTE have been considered and are discussed shortly.', 'corpus_id': 220633102, 'score': 0}, {'doc_id': '211250301', 'title': 'Towards an ontology for automatic scientific discovery', 'abstract': 'While some attempts have been made to automate the scientific discovery process in specific domains, these approaches have limited support for formal representation and reasoning about observations and phenomena. This research aims to create a generic formal ontology to support an intelligent agent for observation induced knowledge discovery.', 'corpus_id': 211250301, 'score': 1}, {'doc_id': '121687096', 'title': 'Summary of photon‐beam stabilization at the Photon Factory', 'abstract': 'The Photon Factory (PF) storage ring is a dedicated synchrotron radiation source. The stability of photon beams is the most important subject regarding such sources, since accurate positioning is especially required for sharply pointing undulator beams. A modification to increase the brightness by reducing the emittance of the circulating beams was carried out in 1987 at the PF ring. Though significant upgrades in the brightness have been obtained, many difficulties have simultaneously arisen, such as an increase of beam instabilities and photon‐beam drifts. Stabilizing efforts have been carried out and great progress has been made: building distortion due to thermal stress, reduced to 1/6 by thermally insulating the roof.', 'corpus_id': 121687096, 'score': 0}, {'doc_id': '111346604', 'title': 'Review Robot Scientists for autonomous scientific discovery', 'abstract': 'We review the main components of autonomous scientific discovery, and how they lead to the concept of a Robot Scientist. This is a system which uses techniques from artificial intelligence to automate all aspects of the scientific discovery process: it generates hypotheses from a computer model of the domain, designs experiments to test these hypotheses, runs the physical experiments using robotic systems, analyses and interprets the resulting data, and repeats the cycle. We describe our two prototype Robot Scientists: Adam and Eve. Adam has recently proven the potential of such systems by identifying twelve genes responsible for catalysing specific reactions in the metabolic pathways of the yeast Saccharomyces cerevisiae. This work has been formally recorded in great detail using logic. We argue that the reporting of science needs to become fully formalised and that Robot Scientists can help achieve this. This will make scientific information more reproducible and reusable, and promote the integration of computers in scientific reasoning. We believe the greater automation of both the physical and intellectual aspects of scientific investigations to be essential to the future of science. Greater automation improves the accuracy and reliability of experiments, increases the pace of discovery and, in common with conventional laboratory automation, removes tedious and repetitive tasks from the human scientist. Review Towards the full automation of scientific discovery', 'corpus_id': 111346604, 'score': 1}, {'doc_id': '62190591', 'title': 'Towards automating the discovery of certain innovative design principles through a clustering-based optimization technique', 'abstract': 'In this article, a methodology is proposed for automatically extracting innovative design principles which make a system or process (subject to conflicting objectives) optimal using its Pareto-optimal dataset. Such ‘higher knowledge’ would not only help designers to execute the system better, but also enable them to predict how changes in one variable would affect other variables if the system has to retain its optimal behaviour. This in turn would help solve other similar systems with different parameter settings easily without the need to perform a fresh optimization task. The proposed methodology uses a clustering-based optimization technique and is capable of discovering hidden functional relationships between the variables, objective and constraint functions and any other function that the designer wishes to include as a ‘basis function’. A number of engineering design problems are considered for which the mathematical structure of these explicit relationships exists and has been revealed by a previous study. A comparison with the multivariate adaptive regression splines (MARS) approach reveals the practicality of the proposed approach due to its ability to find meaningful design principles. The success of this procedure for automated innovization is highly encouraging and indicates its suitability for further development in tackling more complex design scenarios.', 'corpus_id': 62190591, 'score': 1}, {'doc_id': '2765050', 'title': 'Robot-discoverer: Artiicial Intelligent Agent Who Searches for Knowledge', 'abstract': ""The paper is concerned with autonomous intelligent robots who discover knowledge about their environment. First, we compare human and robotic discovery and we clarify the notion of robotic agent and the meaning of autonomous pursuit of knowledge by a robotic system. Then we describe the basic components of machine discoverers, distinguishing 1 a general purpose discovery mechanism, applicable in many domains, and 2 various ways of linking that algorithm with the physical world through robot's sensors and manipulators. We discuss the ways in which diierent concrete robotic discoverers explore and represent their environment, including the exploration of ooce environment with a mobile robot, experiments made by robot arms, and a robot-scientist that makes simple chemistry experiments."", 'corpus_id': 2765050, 'score': 1}]"
66	Filtering	9a588db8471730dbfebac65cd5467ad8	13982	{}	[{'doc_id': '220066978', 'title': 'OpusFilter: A Configurable Parallel Corpus Filtering Toolbox', 'abstract': 'This paper introduces OpusFilter, a flexible and modular toolbox for filtering parallel corpora. It implements a number of components based on heuristic filters, language identification libraries, character-based language models, and word alignment tools, and it can easily be extended with custom filters. Bitext segments can be ranked according to their quality or domain match using single features or a logistic regression model that can be trained without manually labeled training data. We demonstrate the effectiveness of OpusFilter on the example of a Finnish-English news translation task based on noisy web-crawled training data. Applying our tool leads to improved translation quality while significantly reducing the size of the training data, also clearly outperforming an alternative ranking given in the crawled data set. Furthermore, we show the ability of OpusFilter to perform data selection for domain adaptation.', 'corpus_id': 220066978, 'score': 1}, {'doc_id': '221097895', 'title': 'Bifixer and Bicleaner: two open-source tools to clean your parallel data', 'abstract': 'This paper shows the utility of two open-source tools designed for parallel data cleaning: Bifixer and Bicleaner. Already used to clean highly noisy parallel content from crawled multilingual websites, we evaluate their performance in a different scenario: cleaning publicly available corpora commonly used to train machine translation systems. We choose four English–Portuguese corpora which we plan to use internally to compute paraphrases at a later stage. We clean the four corpora using both tools, which are described in detail, and analyse the effect of some of the cleaning steps on them. We then compare machine translation training times and quality before and after cleaning these corpora, showing a positive impact particularly for the noisiest ones.', 'corpus_id': 221097895, 'score': 1}, {'doc_id': '227736819', 'title': 'Globetrotter: Unsupervised Multilingual Translation from Visual Alignment', 'abstract': 'Multi-language machine translation without parallel corpora is challenging because there is no explicit supervision between languages. Existing unsupervised methods typically rely on topological properties of the language representations. We introduce a framework that instead uses the visual modality to align multiple languages, using images as the bridge between them. We estimate the cross-modal alignment between language and images, and use this estimate to guide the learning of cross-lingual representations. Our language representations are trained jointly in one model with a single stage. Experiments with fifty-two languages show that our method outperforms baselines on unsupervised word-level and sentence-level translation using retrieval.', 'corpus_id': 227736819, 'score': 0}, {'doc_id': '229366017', 'title': 'Findings of the WMT 2020 Shared Task on Parallel Corpus Filtering and Alignment', 'abstract': 'Following two preceding WMT Shared Task on Parallel Corpus Filtering (Koehn et al., 2018, 2019), we posed again the challenge of assigning sentence-level quality scores for very noisy corpora of sentence pairs crawled from the web, with the goal of sub-selecting the highest-quality data to be used to train ma-chine translation systems. This year, the task tackled the low resource condition of Pashto–English and Khmer–English and also included the challenge of sentence alignment from document pairs.', 'corpus_id': 229366017, 'score': 1}, {'doc_id': '231718973', 'title': 'Cross-Lingual Named Entity Recognition Using Parallel Corpus: A New Approach Using XLM-RoBERTa Alignment', 'abstract': 'We propose a novel approach for cross-lingual Named Entity Recognition (NER) zero-shot transfer using parallel corpora. We built an entity alignment model on top of XLMRoBERTa to project the entities detected on the English part of the parallel data to the target language sentences, whose accuracy surpasses all previous unsupervised models. With the alignment model we can get pseudolabeled NER data set in the target language to train task-specific model. Unlike using translation methods, this approach benefits from natural fluency and nuances in target-language original corpus. We also propose a modified loss function similar to focal loss but assigns weights in the opposite direction to further improve the model training on noisy pseudolabeled data set. We evaluated this proposed approach over 4 target languages on benchmark data sets and got competitive F1 scores compared to most recent SOTA models. We also gave extra discussions about the impact of parallel corpus size and domain on the final transfer performance.', 'corpus_id': 231718973, 'score': 0}, {'doc_id': '195316269', 'title': 'Low-Resource Corpus Filtering Using Multilingual Sentence Embeddings', 'abstract': 'In this paper, we describe our submission to the WMT19 low-resource parallel corpus filtering shared task. Our main approach is based on the LASER toolkit (Language-Agnostic SEntence Representations), which uses an encoder-decoder architecture trained on a parallel corpus to obtain multilingual sentence representations. We then use the representations directly to score and filter the noisy parallel sentences without additionally training a scoring function. We contrast our approach to other promising methods and show that LASER yields strong results. Finally, we produce an ensemble of different scoring methods and obtain additional gains. Our submission achieved the best overall performance for both the Nepali-English and Sinhala-English 1M tasks by a margin of 1.3 and 1.4 BLEU respectively, as compared to the second best systems. Moreover, our experiments show that this technique is promising for low and even no-resource scenarios.', 'corpus_id': 195316269, 'score': 1}, {'doc_id': '230435667', 'title': 'Decoding Time Lexical Domain Adaptationfor Neural Machine Translation', 'abstract': 'Machine translation systems are vulnerable to domain mismatch, especially when the task is low-resource. In this setting, out of domain translations are often of poor quality and prone to hallucinations, due to the translation model preferring to predict common words it has seen during training, as opposed to the more uncommon ones from a different domain. We present two simple methods for improving translation quality in this particular setting: First, we use lexical shortlisting in order to restrict the neural network predictions by IBM model computed alignments. Second, we perform nbest list reordering by reranking all translations based on the amount they overlap with each other. Our methods are computationally simpler and faster than alternative approaches, and show a moderate success on low-resource settings with explicit out of domain test sets. However, our methods lose their effectiveness when the domain mismatch is too great, or in high resource setting.', 'corpus_id': 230435667, 'score': 0}, {'doc_id': '231925060', 'title': 'Crowdsourcing Parallel Corpus for English-Oromo Neural Machine Translation using Community Engagement Platform', 'abstract': 'Even though Afaan Oromo is the most widely spoken language in the Cushitic family by more than fifty million people in the Horn and East Africa, it is surprisingly resource-scarce from a technological point of view. The increasing amount of various useful documents written in English language brings to investigate the machine that can translate those documents and make it easily accessible for local language. The paper deals with implementing a translation of English to Afaan Oromo and vice versa using Neural Machine Translation. But the implementation is not very well explored due to the limited amount and diversity of the corpus. However, using a bilingual corpus of just over 40k sentence pairs we have collected, this study showed a promising result. About a quarter of this corpus is collected via Community Engagement Platform (CEP) that was implemented to enrich the parallel corpus through crowdsourcing translations.', 'corpus_id': 231925060, 'score': 0}, {'doc_id': '228375792', 'title': 'Document-aligned Japanese-English Conversation Parallel Corpus', 'abstract': 'Sentence-level (SL) machine translation (MT) has reached acceptable quality for many high-resourced languages, but not document-level (DL) MT, which is difficult to 1) train with little amount of DL data; and 2) evaluate, as the main methods and data sets focus on SL evaluation. To address the first issue, we present a document-aligned Japanese-English conversation corpus, including balanced, high-quality business conversation data for tuning and testing. As for the second issue, we manually identify the main areas where SL MT fails to produce adequate translations in lack of context. We then create an evaluation set where these phenomena are annotated to alleviate automatic evaluation of DL systems. We train MT models using our corpus to demonstrate how using context leads to improvements.', 'corpus_id': 228375792, 'score': 0}, {'doc_id': '53246085', 'title': 'An Unsupervised System for Parallel Corpus Filtering', 'abstract': 'In this paper we describe LMU Munich’s submission for the WMT 2018 Parallel Corpus Filtering shared task which addresses the problem of cleaning noisy parallel corpora. The task of mining and cleaning parallel sentences is important for improving the quality of machine translation systems, especially for low-resource languages. We tackle this problem in a fully unsupervised fashion relying on bilingual word embeddings created without any bilingual signal. After pre-filtering noisy data we rank sentence pairs by calculating bilingual sentence-level similarities and then remove redundant data by employing monolingual similarity as well. Our unsupervised system achieved good performance during the official evaluation of the shared task, scoring only a few BLEU points behind the best systems, while not requiring any parallel training data.', 'corpus_id': 53246085, 'score': 1}]
67	Hyponatremia and IL-6	e9ca84c614fe9728dc610c702fb70b30	11750	{}	"[{'doc_id': '5301424', 'title': 'Incidence of Exercise-Associated Hyponatremia and Its Association With Nonosmotic Stimuli of Arginine Vasopressin in the GNW100s Ultra-endurance Marathon', 'abstract': 'Objectives:(1) To examine the incidence of exercise-associated hyponatremia (EAH) during and after an ultramarathon and (2) to evaluate hypothesized nonosmotic stimuli [interleukin-6 (IL-6), hypoglycemia, ambient temperature] with arginine vasopressin (AVP) concentrations in hyponatremic versus normonatremic runners. Design:Prospective cohort study. Setting:The Great North Walk 100s ultramarathons. Participants:Fifteen runners participated in either 103.7- or 173.7-km ultramarathons. Main Outcome Measures:Serum sodium concentration ([Na+]) and AVP concentration. Secondary outcome measures included IL-6, blood glucose, ambient temperature, weight change, fluid consumption, and use of nonsteroidal anti-inflammatory drugs (NSAIDs). Results:Postrace EAH incidence was 4 of 15 runners, whereas EAH incidence at any point during the race was in 10 of 15 runners. A significant positive correlation was noted between AVP and IL-6 (r = 0.31, P < 0.05) but not between AVP and blood glucose (r = 0.09, nonsignificant) or ambient temperature (r = −0.12, NS). Subgroup analysis revealed that the correlation between AVP and IL-6 was significant in hyponatremic (r = 0.37, P < 0.05) but not normonatremic runners (r = 0.31, NS). Hyponatremic runners lost less weight than normonatremic runners (2.5 vs 3.7 kg, P < 0.05, respectively) despite similar fluid consumption. Seven of 10 hyponatremic runners consumed NSAIDs versus 0 of 5 normonatremic runners. Conclusions:Exercise-associated hyponatremia incidence mid-race is higher than postrace, suggesting that 40% of runners are able to self-correct low serum [Na+] status during an ultramarathon. Interleukin-6 seems to be the main nonosmotic stimulus associated with AVP in hyponatremic runners. Nonsteroidal anti-inflammatory ingestion is more common in hyponatremic versus normonatremic runners. Clinical Relevance:Exercise-associated hyponatremia associated with nonosmotic AVP secretion may be more common during ultramarathon races without discriminatory clinical symptomatology.', 'corpus_id': 5301424, 'score': 1}, {'doc_id': '232036438', 'title': 'Severe symptomatic hyponatremia due to cerebral salt wasting syndrome in a patient with traumatic head injury and Dandy-Walker malformation of the brain', 'abstract': 'Cerebral salt wasting (CSW) is an uncommon cause of hyponatremia characterized by extracellular volume depletion, high urine sodium concentration and osmolality, and low serum uric acid concentration in association with central nervous system (CNS) disease. Distinguishing CSW from the syndrome of inappropriate secretion of antidiuretic hormone (SIADH), a much more common form of hyponatremia in this setting, can be challenging because both present with identical laboratory features. However, treatment of CSW and SIADH differs, making a correct diagnosis important. Here we present a case of CSW in a 75-year-old man in whom severe hyponatremia and volume depletion were discovered in the setting of traumatic head injury and Dandy-Walker malformation of the brain, a rare congenital brain malformation. Treatment with intravenous normal saline and later oral salt supplementation and fludrocortisone was successful.', 'corpus_id': 232036438, 'score': 0}, {'doc_id': '231841752', 'title': 'Beneficial effects of dantrolene in the treatment of rhabdomyolysis as a potential late complication associated with COVID-19: a case report', 'abstract': 'Background Patients with severe COVID-19 have disorders of the respiratory, cardiovascular, coagulation, skeletal muscle, and central nervous systems. These systemic failures may be associated with cytokine release syndrome, characterized by hyperpyrexia, thrombocytopenia, hyperferritinemia, and the elevation of other inflammatory markers. Rhabdomyolysis with high fever is a complication that is rarely found in COVID-19. The exact relations of these clinical conditions in patients with COVID-19 remain unknown. Case presentation We present the case of a 36-year-old man with severe COVID-19 complicated by rhabdomyolysis and high fever. After admission, his condition continued to deteriorate, with a high body temperature. On day 9, the patient had elevated creatine kinase and myoglobin levels consistent with rhabdomyolysis (26,046 U/L and 3668\xa0ng/mL, respectively). In addition to viral therapy, he was immediately treated with hydration. However, the patient had persistent fever and elevated creatine kinase levels. The patient was diagnosed with malignant hyperthermia as a late complication of COVID-19, although he had no hereditary predisposition to malignant hyperthermia or neuroleptic malignant syndrome. The administration of dantrolene with muscle relaxation and anti-inflammatory function showed potential efficacy for rhabdomyolysis, high fever, and increased plasma inflammatory markers. Conclusions Malignant hyperthermia is triggered by not only anesthetic agents but also viral infections. A possible mechanism of malignant hyperthermia is hypersensitivity of calcium release from the sarcoplasmic reticulum. These include mutations in or\xa0the activation of the skeletal muscle ryanodine receptor calcium release channel. Dantrolene is a ryanodine receptor antagonist and is used as an anti-inflammatory agent. The administration of dantrolene showed potential efficacy for rhabdomyolysis, high body temperature due to inflammation, and increased inflammatory markers. The underlying mechanism of the association of rhabdomyolysis and high fever in COVID-19 might be similar to the pathogenesis of malignant hyperthermia.', 'corpus_id': 231841752, 'score': 0}, {'doc_id': '26643156', 'title': 'The role of interleukin 6 in the pathogenesis of hyponatremia associated with Guillain-Barré syndrome.', 'abstract': 'Nefrologia 2012;32(1):114-32 Furthermore, Mastorakos et al. reported that plasma antidiuretic hormone levels were elevated after IL-6 injection in cancer patients, suggesting that IL-6 activated the magnocellular ADHsecreting neurons and that it might be involved in SIADH. Activation of the subfornical organ and the organum vasculosum of the lamina terminalis by IL-6 could eventually lead to thirst and increased vasopressin secretion by neurons from the supraoptic nucleus and paraventricular nucleus. The combination of antidiuresis and increased water intake may result in hyponatremia.', 'corpus_id': 26643156, 'score': 1}, {'doc_id': '231882238', 'title': 'Ibrutinib-induced acute kidney injury via interstitial nephritis', 'abstract': ""Abstract The introduction of Bruton's tyrosine kinase inhibitor ibrutinib has made a significant progress in the treatment of chronic lymphocytic leukemia and other B-cell malignancies. Due to the reduction of cytokine release, it is effective in chronic graft-versus-host disease, and its use has also been suggested in autoimmune diseases and in prevention of COVID-19-associated lung damage. Despite this effect on the immune response, we report a severe hypersensitivity reaction in a 76-year-old male patient diagnosed with prolymphocytic leukemia. Four weeks after the ibrutinib start, non-oliguric acute kidney injury with proteinuria and microscopic hematuria developed and that was accompanied by lower limb purpuras and paresthesia. Renal biopsy revealed acute interstitial nephritis. Employing 1\u2009mg/kg methylprednisolone administration, serum creatinine decreased from 365\u2009μmol/L to 125\u2009μmol/L at 11\u2009days and the proteinuria-hematuria as well as the purpura, paresthesia resolved. Three months later at stabile eGFR of 56\u2009ml/min/1.73 m2 methylprednisolone was withdrawn and a rituximab-venetoclax treatment was initiated without side effects. We conclude that despite the beneficial effect on cytokines response in Th1 direction, ibrutinib can cause acute interstitial nephritis. Early detection, discontinuation of ibrutinib, glucocorticoid administration may help to better preserve renal function, thereby lowering the risk of potential subsequent kidney injury."", 'corpus_id': 231882238, 'score': 0}, {'doc_id': '219540036', 'title': 'Hyponatremia: A possible immuno‐neuroendocrine interface with COVID‐19 in a kidney transplant recipient', 'abstract': 'There is fast‐emerging, cumulative clinical data on coronavirus disease 2019 (COVID‐19) in kidney transplant recipients. Although respiratory tract symptoms are often the initial presentation among kidney transplant recipients who contract COVID‐19, other clinical features which may indicate underlying SARS‐CoV‐2‐related inflammation, such as gastrointestinal symptoms, are not uncommon. Hyponatremia can develop and may reflect underlying inflammation. Interferon‐6 is an important pro‐inflammatory cytokine involved in the pathogenesis of severe COVID‐19 complications and may play a role in the inappropriately higher secretion of antidiuretic hormone leading to hyponatremia. This pathway is the so‐called immuno‐neuroendocrine interface. Hyponatremia in COVID‐19 has been reported in a few case series of non‐kidney transplant patients and only one reported kidney transplant recipient. However, the clinical course and prognostic value of hyponatremia in this population are not described in detail. We report a kidney transplant recipient who was infected with COVID‐19 and exhibited severe hyponatremia secondary to the syndrome of inappropriate antidiuretic hormone secretion. Hyponatremia is one of the clinical presentations of COVID‐19, although less common, and may occur more frequently in kidney transplant recipients. Thus, the possible underlying immuno‐neuroendocrine relationship related to the inflammatory process of COVID‐19 leading to hyponatremia and its prognostic value are reviewed.', 'corpus_id': 219540036, 'score': 1}, {'doc_id': '224823075', 'title': 'Hemoadsorption cartridge and coronavirus disease 2019 infections: A case report and brief literature review.', 'abstract': 'The cytokine storm has been frequently reported to occur in patients with severe coronavirus disease 2019 (COVID-19). Data from the literature suggest that elevated levels of inflammatory mediators, such as interleukin (IL)-6, IL-8, and tumor necrosis factor, indicate a severe course or the fatality of the disease. Several therapeutic options have been employed to treat critically ill patients, including hemoadsorption of inflammatory mediators. We here present a case of severe acute respiratory syndrome caused by COVID-19 and acute renal failure. The patient was admitted to our intensive care unit and treated with mechanical ventilation, renal replacement therapy, and hemoadsorption to reduce the cytokine release syndrome, which plays a fundamental role in the clinical presentation of COVID-19 patients. We also discuss the potential advantages of reducing cytokine plasma levels using a hemoadsorption cartridge.', 'corpus_id': 224823075, 'score': 0}, {'doc_id': '15428182', 'title': 'Hyponatremia and Inflammation: The Emerging Role of Interleukin-6 in Osmoregulation', 'abstract': 'Although hyponatremia is a recognized complication of several inflammatory diseases, its pathophysiology in this setting has remained elusive until recently. A growing body of evidence now points to an important role for interleukin-6 in the non-osmotic release of vasopressin. Here, we review this evidence by exploring the immuno-neuroendocrine pathways connecting interleukin-6 with vasopressin. The importance of these connections extends to several clinical scenarios of hyponatremia and inflammation, including hospital-acquired hyponatremia, postoperative hyponatremia, exercise-associated hyponatremia, and hyponatremia in the elderly. Besides insights in pathophysiology, the recognition of the propensity for antidiuresis during inflammation is also important with regard to monitoring patients and selecting the appropriate intravenous fluid regimen, for which recommendations are provided.', 'corpus_id': 15428182, 'score': 1}, {'doc_id': '231962146', 'title': 'Role of vitamin D in treating COVID-19-associated coagulopathy: problems and perspectives', 'abstract': 'Aggressive inflammatory response leading to hypercoagulability has been found to be associated with disease severity in COVID-19 patients and portends bad treatment outcome. A state of acute disseminated intravascular coagulation (DIC), along with pulmonary embolism and/or deep vein thrombosis, has been observed in critically ill ICU patients. Autopsy reports of COVID-19 patients demonstrated microthrombi in lungs and in other organs, as well as marked inflammatory changes, characteristic clinicopathological features that exacerbate disease severity. Vitamin D supplementation was recommended by many clinicians across the globe to improve clinical symptoms of COVID-19 patients, mainly because of its immunomodulatory roles on immune cells. Furthermore, vitamin D and its associated molecules are also known to directly or indirectly regulate various thrombotic pathways. We propose that vitamin D supplementation not only attenuates the risk of Acute Respiratory Disease Syndrome (ARDS) but it also may have a role in reducing coagulation abnormalities in critically ill COVID-19 patients. The overarching goal of this review is to discuss the effects of vitamin D on coagulation pathways and other intertwined processes leading to thrombosis. Many clinical trials are currently investigating the efficacy of vitamin D supplementation in reducing the risk of COVID-19 infection. However, randomized placebo control clinical trials are also necessary to ascertain the effect of vitamin D supplementation on reducing the risk of coagulopathy in COVID-19 patients.', 'corpus_id': 231962146, 'score': 0}, {'doc_id': '204974346', 'title': 'Correlation of IL-6 secretion and hyponatremia with the use of CD19+ chimeric antigen receptor T-cells\u2029.', 'abstract': 'BACKGROUND\nVarious studies have demonstrated that interleukin-6 (IL-6) activates the central magnocellular arginine vasopressin (AVP)-secreting neurons in the brain to produce non-osmotic, non-volume-mediated increases in AVP. The most common toxicity of CD19+ chimeric antigen receptor (CAR) T-cells is cytokine release syndrome, which is related to increased levels of IL-6. This study will evaluate the correlation of IL-6 levels with hyponatremia in patients receiving CD19+ CAR T-cells.\n\n\nMATERIALS AND METHODS\nThis is a single-center retrospective analysis of adult patients who received CD19+ CAR T-cells for the treatment of relapsed/refractory acute lymphoblastic leukemia (ALL).\n\n\nRESULTS\nHyponatremia, defined as a serum sodium (Na) ≤ 135\xa0mEq/L, occurred in 31 (61%) patients. A change in Na >\xa07\xa0mEq occurred in 32 (63%) patients, and the median lowest Na was 133\xa0mEq/L (interquartile range (IQR): 131\xa0-\xa0136)). There was an inverse linear relationship between IL-6 levels and lowest Na (p\xa0=\xa00.001). Overall, per 10-fold increase in IL-6, Na decreased by an average of 2.68\xa0mEq/L.\n\n\nCONCLUSION\nHyponatremia is common in patients who received CD19+ CAR T-cells. There is an inverse linear relationship between IL-6 levels and nadir Na (p\xa0=\xa00.001). Further studies will be needed to confirm a causative relationship between IL-6 levels and hyponatremia following CD19+ CAR T-cell infusion.\u2029.', 'corpus_id': 204974346, 'score': 1}]"
68	Retrosynthesis 	43df97cb6ae64e9d2d71cbeaa5718329	3230	{}	[{'doc_id': '214055777', 'title': 'Incubation Period and Other Epidemiological Characteristics of 2019 Novel Coronavirus Infections with Right Truncation: A Statistical Analysis of Publicly Available Case Data', 'abstract': 'The geographic spread of 2019 novel coronavirus (COVID-19) infections from the epicenter of Wuhan, China, has provided an opportunity to study the natural history of the recently emerged virus. Using publicly available event-date data from the ongoing epidemic, the present study investigated the incubation period and other time intervals that govern the epidemiological dynamics of COVID-19 infections. Our results show that the incubation period falls within the range of 2-14 days with 95% confidence and has a mean of around 5 days when approximated using the best-fit lognormal distribution. The mean time from illness onset to hospital admission (for treatment and/or isolation) was estimated at 3-4 days without truncation and at 5-9 days when right truncated. Based on the 95th percentile estimate of the incubation period, we recommend that the length of quarantine should be at least 14 days. The median time delay of 13 days from illness onset to death (17 days with right truncation) should be considered when estimating the COVID-19 case fatality risk.', 'corpus_id': 214055777, 'score': 0}, {'doc_id': '211476589', 'title': 'Understanding of COVID‐19 based on current evidence', 'abstract': 'Since December 2019, a series of unexplained pneumonia cases have been reported in Wuhan, China. On 12 January 2020, the World Health Organization (WHO) temporarily named this new virus as the 2019 novel coronavirus (2019‐nCoV). On 11 February 2020, the WHO officially named the disease caused by the 2019‐nCoV as coronavirus disease (COVID‐19). The COVID‐19 epidemic is spreading all over the world, especially in China. Based on the published evidence, we systematically discuss the characteristics of COVID‐19 in the hope of providing a reference for future studies and help for the prevention and control of the COVID‐19 epidemic.', 'corpus_id': 211476589, 'score': 0}, {'doc_id': '46865246', 'title': 'Neural-Symbolic Machine Learning for Retrosynthesis and Reaction Prediction.', 'abstract': 'Reaction prediction and retrosynthesis are the cornerstones of organic chemistry. Rule-based expert systems have been the most widespread approach to computationally solve these two related challenges to date. However, reaction rules often fail because they ignore the molecular context, which leads to reactivity conflicts. Herein, we report that deep neural networks can learn to resolve reactivity conflicts and to prioritize the most suitable transformation rules. We show that by training our model on 3.5\u2005million reactions taken from the collective published knowledge of the entire discipline of chemistry, our model exhibits a top10-accuracy of 95\u2009% in retrosynthesis and 97\u2009% for reaction prediction on a validation set of almost 1\u2005million reactions.', 'corpus_id': 46865246, 'score': 1}, {'doc_id': '202768445', 'title': 'Retrosynthesis Prediction with Conditional Graph Logic Network', 'abstract': 'Retrosynthesis is one of the fundamental problems in organic chemistry. The task is to identify reactants that can be used to synthesize a specified product molecule. Recently, computer-aided retrosynthesis is finding renewed interest from both chemistry and computer science communities. Most existing approaches rely on template-based models that define subgraph matching rules, but whether or not a chemical reaction can proceed is not defined by hard decision rules. In this work, we propose a new approach to this task using the Conditional Graph Logic Network, a conditional graphical model built upon graph neural networks that learns when rules from reaction templates should be applied, implicitly considering whether the resulting reaction would be both chemically feasible and strategic. We also propose an efficient hierarchical sampling to alleviate the computation cost. While achieving a significant improvement of 8.2% over current state-of-the-art methods on the benchmark dataset, our model also offers interpretations for the prediction.', 'corpus_id': 202768445, 'score': 1}, {'doc_id': '212707837', 'title': 'Coronavirus disease 2019: What we know?', 'abstract': 'In late December 2019, a cluster of unexplained pneumonia cases has been reported in Wuhan, China. A few days later, the causative agent of this mysterious pneumonia was identified as a novel coronavirus. This causative virus has been temporarily named as severe acute respiratory syndrome coronavirus 2 and the relevant infected disease has been named as coronavirus disease 2019 (COVID‐19) by the World Health Organization, respectively. The COVID‐19 epidemic is spreading in China and all over the world now. The purpose of this review is primarily to review the pathogen, clinical features, diagnosis, and treatment of COVID‐19, but also to comment briefly on the epidemiology and pathology based on the current evidence.', 'corpus_id': 212707837, 'score': 0}, {'doc_id': '211230955', 'title': 'Incubation Period and Other Epidemiological Characteristics of 2019 Novel Coronavirus Infections with Right Truncation: A Statistical Analysis of Publicly Available Case Data', 'abstract': 'The geographic spread of 2019 novel coronavirus (COVID-19) infections from the epicenter of Wuhan, China, has provided an opportunity to study the natural history of the recently emerged virus. Using publicly available event-date data from the ongoing epidemic, the present study investigated the incubation period and other time intervals that govern the epidemiological dynamics of COVID-19 infections. Our results show that the incubation period falls within the range of 2–14 days with 95% confidence and has a mean of around 5 days when approximated using the best-fit lognormal distribution. The mean time from illness onset to hospital admission (for treatment and/or isolation) was estimated at 3–4 days without truncation and at 5–9 days when right truncated. Based on the 95th percentile estimate of the incubation period, we recommend that the length of quarantine should be at least 14 days. The median time delay of 13 days from illness onset to death (17 days with right truncation) should be considered when estimating the COVID-19 case fatality risk.', 'corpus_id': 211230955, 'score': 0}, {'doc_id': '11001549', 'title': 'Computer-Assisted Retrosynthesis Based on Molecular Similarity', 'abstract': 'We demonstrate molecular similarity to be a surprisingly effective metric for proposing and ranking one-step retrosynthetic disconnections based on analogy to precedent reactions. The developed approach mimics the retrosynthetic strategy defined implicitly by a corpus of known reactions without the need to encode any chemical knowledge. Using 40\u202f000 reactions from the patent literature as a knowledge base, the recorded reactants are among the top 10 proposed precursors in 74.1% of 5000 test reactions, providing strong quantitative support for our methodology. Extension of the one-step strategy to multistep pathway planning is demonstrated and discussed for two exemplary drug products.', 'corpus_id': 11001549, 'score': 1}, {'doc_id': '215415359', 'title': 'Predicting COVID-19 Using Hybrid AI Model', 'abstract': 'Background: The coronavirus disease 2019 (COVID-19) breaking out in late December 2019 is gradually being controlled in China, but it is still spreading in other countries and regions worldwide. It is urgent to conduct prediction research on the development and spread of the epidemic. \n \nMethods: A hybrid AI model is proposed for COVID-19 prediction. First, by analyzing the change in the infectious capacity of virus carriers within a few days after infection, an improved SI (ISI) model is proposed. Second, considering the effects of prevention and control measures and the increase of the public’s prevention awareness, the natural language processing (NLP) module and the long short-term memory (LSTM) network are embedded into the ISI model to build the hybrid AI model for COVID-19 prediction. \n \nFindings: Compared with traditional epidemic models, the proposed ISI model finds that the new confirmed cases of COVID-19 are mainly infected by the cases from previous 3 to 8 days, and the average infection time is about 5.5 days. With introduction of NLP and LSTM into the hybrid AI model, the mean absolute percentage errors (MAPE) of the prediction results are 0.52%, 0.38%, 0.05%, 0.86% for the next 6 days in Wuhan, Beijing, Shanghai and nationwide respectively, which proves our model is more in line with the actual epidemic development trend. \n \nInterpretation: The infectious capacity of virus carriers varies at different stages. Additionally, both the prevention and control measures and the public’s awareness of the epidemic have great impact on the transmission of COVID-19. \n \nFunding Statement: This study was supported by the fund from the National Key Research and Development Program of China (Grant No. 2016YFB1000900). \n \nDeclaration of Interests: The authors declare that they have no competing interests.', 'corpus_id': 215415359, 'score': 0}, {'doc_id': '213016152', 'title': 'Serial interval of novel coronavirus (2019-nCoV) infections', 'abstract': 'Objective: To estimate the serial interval of novel coronavirus (COVID-19) from information on 28 infector-infectee pairs. Methods: We collected dates of illness onset for primary cases (infectors) and secondary cases (infectees) from published research articles and case investigation reports. We subjectively ranked the credibility of the data and performed analyses on both the full dataset (n=28) and a subset of pairs with highest certainty in reporting (n=18). In addition, we adjusting for right truncation of the data as the epidemic is still in its growth phase. Results: Accounting for right truncation and analyzing all pairs, we estimated the median serial interval at 4.0 days (95% credible interval [CrI]: 3.1, 4.9). Limiting our data to only the most certain pairs, the median serial interval was estimated at 4.6 days (95% CrI: 3.5, 5.9). Conclusions: The serial interval of COVID-19 is shorter than its median incubation period. This suggests that a substantial proportion of secondary transmission may occur prior to illness onset. The COVID-19 serial interval is also shorter than the serial interval of severe acute respiratory syndrome (SARS), indicating that calculations made using the SARS serial interval may introduce bias.', 'corpus_id': 213016152, 'score': 0}, {'doc_id': '214636811', 'title': 'Title : Understanding of COVID-19 based on current evidence Running head : Current understanding of COVID-19', 'abstract': 'Since December 2019, a series of unexplained pneumonia cases has been reported in Wuhan, China. On January 12, 2020, the World Health Organization (WHO) temporarily named this new virus as the 2019 novel coronavirus (2019-nCoV). On February 11, 2020, the WHO officially named the disease caused by the 2019-nCoV as Corona Virus Disease (COVID-19). The COVID-19 epidemic is spreading all over the world, especially in China. Based on the published evidence, we systematically discuss the characteristics of COVID-19 in the hope of providing a reference for future studies and help for the prevention and control of the COVID-19 epidemic.', 'corpus_id': 214636811, 'score': 0}]
69	Data augmentation + Semantic Parsing	77f2fb35cb3aabefe5eec561d3bec85f	13343	{}	"[{'doc_id': '231709501', 'title': 'El Volumen Louder Por Favor: Code-switching in Task-oriented Semantic Parsing', 'abstract': 'Being able to parse code-switched (CS) utterances, such as Spanish+English or Hindi+English, is essential to democratize task-oriented semantic parsing systems for certain locales. In this work, we focus on Spanglish (Spanish+English) and release a dataset, CSTOP, containing 5800 CS utterances alongside their semantic parses. We examine the CS generalizability of various Cross-lingual (XL) models and exhibit the advantage of pre-trained XL language models when data for only one language is present. As such, we focus on improving the pre-trained models for the case when only English corpus alongside either zero or a few CS training instances are available. We propose two data augmentation methods for the zero-shot and the few-shot settings: fine-tune using translate-and-align and augment using a generation model followed by match-and-filter. Combining the few-shot setting with the above improvements decreases the initial 30-point accuracy gap between the zero-shot and the full-data settings by two thirds.', 'corpus_id': 231709501, 'score': 0}, {'doc_id': '229923938', 'title': 'Optimizing Deeper Transformers on Small Datasets: An Application on Text-to-SQL Semantic Parsing', 'abstract': 'Due to the common belief that training deep transformers from scratch requires large datasets, people usually only use shallow and simple additional layers on top of pre-trained models during fine-tuning on small datasets. We provide evidence that this does not always need to be the case: with proper initialization and training techniques, the benefits of very deep transformers are shown to carry over to hard structural prediction tasks, even using small datasets. In particular, we successfully train 48 layers of transformers for a semantic parsing task. These comprise 24 fine-tuned transformer layers from pre-trained RoBERTa and 24 relation-aware transformer layers trained from scratch. With fewer training steps and no task-specific pretraining, we obtain the state of the art performance on the challenging cross-domain Textto-SQL semantic parsing benchmark Spider. We achieve this by deriving a novel Data dependent Transformer Fixed-update initialization scheme (DT-Fixup), inspired by the prior T-Fixup work (Huang et al., 2020). Further error analysis demonstrates that increasing the depth of the transformer model can help improve generalization on the cases requiring reasoning and structural understanding1.', 'corpus_id': 229923938, 'score': 1}, {'doc_id': '40990080', 'title': ""Junior hospital doctors' views on their training in the UK."", 'abstract': ""To ascertain the views of senior house officers and registrars on the educational and training component of their posts, a questionnaire was sent to all full-time doctors working in training posts in general and/or geriatric medicine at three district general and three teaching hospitals. Completed questionnaires were received from 64 (61%) of 105 doctors who were contacted. Most had a careers counsellor or tutor, although less than two-thirds thought they had benefited from this arrangement. The majority of doctors attended at least two medical tutorials or meetings per week; most wanted to attend more but were unable to because of other work commitments. Supervision by more senior staff on the ward was deemed by most to be satisfactory, but less so in out-patient clinics. Overall, one-third of doctors thought that training was inadequate and three-quarters wanted a greater amount of formal education. The majority of junior doctors' time was spent on routine work and most considered :training' constituted less than 10% of their working time. Doctors in training require more sessions designated as educational, with protected time to attend these."", 'corpus_id': 40990080, 'score': 0}, {'doc_id': '216144451', 'title': 'G-DAug: Generative Data Augmentation for Commonsense Reasoning', 'abstract': 'Recent advances in commonsense reasoning depend on large-scale human-annotated training sets to achieve peak performance. However, manual curation of training sets is expensive and has been shown to introduce annotation artifacts that neural models can readily exploit and overfit to. We propose a novel generative data augmentation technique, G-DAUGˆC, that aims to achieve more accurate and robust learning in a low-resource setting. Our approach generates synthetic examples using pretrained language models and selects the most informative and diverse set of examples for data augmentation. On experiments with multiple commonsense reasoning benchmarks, G-DAUGˆC consistently outperforms existing data augmentation methods based on back-translation, establishing a new state-of-the-art on WinoGrande, CODAH, and CommonsenseQA, as well as enhances out-of-distribution generalization, proving to be robust against adversaries or perturbations. Our analysis demonstrates that G-DAUGˆC produces a diverse set of fluent training examples, and that its selection and training approaches are important for performance.', 'corpus_id': 216144451, 'score': 1}, {'doc_id': '43457908', 'title': 'Staphylococcus aureus bacteraemia of unknown primary source: where do we stand?', 'abstract': 'There is no generally held definition of Staphylococcus aureus bacteraemia (SAB) of unknown source. For this paper, we consider it to occur when one or more positive blood cultures obtained from a patient grows S. aureus and the origin of the bacteraemia is uncertain after history, physical examination, chest radiography and any further investigations provoked by clinical findings. The incidence of SAB appears to be rising, particularly community-acquired (CA), but also hospital- or healthcare-acquired (HA). Major drivers appear to be intravenous drug use and increasing use of indwelling intravascular devices. There is an increasing prevalence of meticillin-resistant S. aureus (MRSA), both CA and HA. There is increasing hospital acquisition of MRSA that is phenotypically like CA strains, and there is increasing community-based treatment of HA infection. Metastatic infection is a risk of SAB. Infective endocarditis (IE) is a longstanding dreaded concern of SAB. Transoesophageal echocardiography appears to be a superior modality of recognising IE in the context of SAB and can guide the duration of therapy. Prosthetic joints and heart valves are at particular risk of haematogenous seeding from SAB. Implications of the rise of CA-MRSA in terms of metastatic infection warrant further study.', 'corpus_id': 43457908, 'score': 0}, {'doc_id': '231692915', 'title': 'Distilling Large Language Models into Tiny and Effective Students using pQRNN', 'abstract': 'Large pre-trained multilingual models like mBERT, XLM-R achieve state of the art results on language understanding tasks. However, they are not well suited for latency critical applications on both servers and edge devices. It’s important to reduce the memory and compute resources required by these models. To this end, we propose pQRNN, a projectionbased embedding-free neural encoder that is tiny and effective for natural language processing tasks. Without pre-training, pQRNNs significantly outperform LSTM models with pre-trained embeddings despite being 140x smaller. With the same number of parameters, they outperform transformer baselines thereby showcasing their parameter efficiency. Additionally, we show that pQRNNs are effective student architectures for distilling large pretrained language models. We perform careful ablations which study the effect of pQRNN parameters, data augmentation, and distillation settings. On MTOP, a challenging multilingual semantic parsing dataset, pQRNN students achieve 95.9% of the performance of an mBERT teacher while being 350x smaller. On mATIS, a popular parsing task, pQRNN students on average are able to get to 97.1% of the teacher while again being 350x smaller. Our strong results suggest that our approach is great for latency-sensitive applications while being able to leverage large mBERT-like models.', 'corpus_id': 231692915, 'score': 0}, {'doc_id': '231602921', 'title': 'Structured Prediction as Translation between Augmented Natural Languages', 'abstract': 'We propose a new framework, Translation between Augmented Natural Languages (TANL), to solve many structured prediction language tasks including joint entity and relation extraction, nested named entity recognition, relation classification, semantic role labeling, event extraction, coreference resolution, and dialogue state tracking. Instead of tackling the problem by training task-specific discriminative classifiers, we frame it as a translation task between augmented natural languages, from which the task-relevant information can be easily extracted. Our approach can match or outperform task-specific models on all tasks, and in particular, achieves new state-of-the-art results on joint entity and relation extraction (CoNLL04, ADE, NYT, and ACE2005 datasets), relation classification (FewRel and TACRED), and semantic role labeling (CoNLL-2005 and CoNLL2012). We accomplish this while using the same architecture and hyperparameters for all tasks and even when training a single model to solve all tasks at the same time (multi-task learning). Finally, we show that our framework can also significantly improve the performance in a low-resource regime, thanks to better use of label semantics.', 'corpus_id': 231602921, 'score': 0}, {'doc_id': '229331741', 'title': 'Learning Contextual Representations for Semantic Parsing with Generation-Augmented Pre-Training', 'abstract': 'Most recently, there has been significant interest in learning contextual representations for various NLP tasks, by leveraging large scale text corpora to train large neural language models with self-supervised learning objectives, such as Masked Language Model (MLM). However, based on a pilot study, we observe three issues of existing general-purpose language models when they are applied to text-to-SQL semantic parsers: fail to detect column mentions in the utterances, fail to infer column mentions from cell values, and fail to compose complex SQL queries. To mitigate these issues, we present a model pre-training framework, GenerationAugmented Pre-training (GAP), that jointly learns representations of natural language utterances and table schemas by leveraging generation models to generate pre-train data. GAP MODEL is trained on 2M utterance-schema pairs and 30K utterance-schema-SQL triples, whose utterances are produced by generative models. Based on experimental results, neural semantic parsers that leverage GAP MODEL as a representation encoder obtain new state-of-the-art results on both SPIDER and CRITERIA-TO-SQL benchmarks.', 'corpus_id': 229331741, 'score': 1}, {'doc_id': '231749880', 'title': 'Neural Data Augmentation via Example Extrapolation', 'abstract': 'In many applications of machine learning, certain categories of examples may be underrepresented in the training data, causing systems to underperform on such “few-shot” cases at test time. A common remedy is to perform data augmentation, such as by duplicating underrepresented examples, or heuristically synthesizing new examples. But these remedies often fail to cover the full diversity and complexity of real examples. We propose a data augmentation approach that performs neural Example Extrapolation (Ex2). Given a handful of exemplars sampled from some distribution, Ex2 synthesizes new examples that also belong to the same distribution. The Ex2 model is learned by simulating the example generation procedure on data-rich slices of the data, and it is applied to underrepresented, few-shot slices. We apply Ex2 to a range of language understanding tasks and significantly improve over state-of-the-art methods on multiple few-shot learning benchmarks, including for relation extraction (FewRel) and intent classification + slot filling (SNIPS).', 'corpus_id': 231749880, 'score': 1}, {'doc_id': '231749482', 'title': 'On Robustness of Neural Semantic Parsers', 'abstract': 'Semantic parsing maps natural language (NL) utterances into logical forms (LFs), which underpins many advanced NLP problems. Semantic parsers gain performance boosts with deep neural networks, but inherit vulnerabilities against adversarial examples. In this paper, we provide the first empirical study on the robustness of semantic parsers in the presence of adversarial attacks. Formally, adversaries of semantic parsing are considered to be the perturbed utterance-LF pairs, whose utterances have exactly the same meanings as the original ones. A scalable methodology is proposed to construct robustness test sets based on existing benchmark corpora. Our results answered five research questions in measuring the sate-of-the-art parsers’ performance on robustness test sets, and evaluating the effect of data augmentation.', 'corpus_id': 231749482, 'score': 1}]"
70	deep learning soil spectroscopy	65992edf7d9de181a86856d2524b9ff4	14954	{}	[{'doc_id': '216376924', 'title': 'Simultaneous prediction of soil properties from VNIR-SWIR spectra using a localized multi-channel 1-D convolutional neural network', 'abstract': 'Abstract The use of visible near-infrared and shortwave-infrared (VNIR-SWIR) diffuse reflectance spectroscopy for the estimation of soil properties is increasingly maturing with large-scale soil spectral libraries (SSLs) of laboratory spectra developed across the globe. Such an SSL is the publicly available LUCAS topsoil database with approximately 20,000 soil samples encompassing 23 countries of the European Union. A wide variety of machine learning tools have been applied to the LUCAS SSL to predict some of the soil samples’ physicochemical properties with different degrees of accuracy. In this paper, we developed and examined the use of a novel one-dimensional convolutional neural network (CNN) to simultaneously predict ten physicochemical properties of the LUCAS SSL. Leveraging on the use of multiple-input channels it uses as model inputs the absorbance spectra along with some pre-processed spectra developed using standard techniques. Moreover, it exploits the use of local spectral neighborhoods to perform an adaptive error-correction mechanism. This novel localized multi-channel 1-D CNN was applied to all the available physicochemical properties of the LUCAS SSL and was statistically compared with the current state-of-the-art where it was shown to statistically outperform its counterparts, as well as with other CNNs where it exhibited the best performance. In particular, for the mineral soil samples, the RMSE for the Clay content was 4.80% ( R 2 0.86), for soil organic carbon the RMSE was 10.96\xa0g\xa0kg−1 ( R 2 0.86), while for total nitrogen the RMSE was 0.66\xa0g\xa0kg−1 ( R 2 0.83).', 'corpus_id': 216376924, 'score': 1}, {'doc_id': '233255212', 'title': 'Predicting soil carbon by efficiently using variation in a mid-IR soil spectral library', 'abstract': 'Abstract. Traditional laboratory methods of acquiring soil information remain important for assessing key soil properties, soil functions and ecosystem services over space and time. Infrared spectroscopic modelling can link and massively scale up these methods for many soil characteristics in a cost-effective and timely manner. In Switzerland, only 10\u2009% to 15\u2009% of agricultural soils have been mapped sufficiently to serve spatial decision support systems, presenting an urgent need for rapid quantitative soil characterization. The current Swiss soil spectral library (SSL; n\u2009=\u20094374) in the mid-infrared range includes soil samples from the Biodiversity Monitoring Program (BDM), arranged in a regularly spaced grid across Switzerland, and temporally-resolved data from the Swiss Soil Monitoring Network (NABO). Given the relatively low representation of organic soils and their organo-mineral diversity in the SSL, we aimed to develop both an efficient calibration sampling scheme and accurate modelling strategy to estimate soil carbon (SC) contents of heterogeneous samples between 0\u2009m to 2\u2009m depth from 26 locations within two drained peatland regions (HAFL dataset; n\u2009=\u2009116). The focus was on minimizing the need for new reference analyses by efficiently mining the spectral information of SSL instances and their target-feature representations. We used partial least square regressions (PLSR) together with a 5 times repeated, grouped by location, 10-fold cross validation (CV) to predict SC ranging from 1\u2009% to 52\u2009% in the local HAFL dataset. We compared the validation performance of different calibration schemes involving local models (1), models using the entire SSL spiked with local samples (2) and 15 subsets of local and SSL samples using the RS-LOCAL algorithm (3). Using local and RS-LOCAL calibrations with at least 5 local samples, we achieved similar validation results for predictions of SC up to 52\u2009% (R2\u2009=\u20090.94–0.96, bias\u2009=\u2009−0.6–1.5, RMSE = 2.6\u2009% to 3.5\u2009% total carbon). However, calibrations of representative SSL and local samples using RS-LOCAL only required 5 local samples for very accurate models (RMSE\u2009=\u20092.9\u2009% total carbon), while local calibrations required 50 samples for similarly accurate results (RMSE', 'corpus_id': 233255212, 'score': 1}, {'doc_id': '231951571', 'title': 'Deep Learning Approaches for Forecasting Strawberry Yields and Prices Using Satellite Images and Station-Based Soil Parameters', 'abstract': 'Computational tools for forecasting yields and prices for fresh produce have been based on traditional machine learning approaches or time series modeling. We propose here an alternate approach based on deep learning algorithms for forecasting strawberry yields and prices in Santa Barbara county, California. Building the proposed forecasting model comprises three stages: first, the station-based ensemble model (ATT-CNN-LSTMSeriesNet_Ens) with its compound deep learning components, SeriesNet with Gated Recurrent Unit (GRU) and Convolutional Neural Network LSTM with Attention layer (AttCNN-LSTM), are trained and tested using the station-based soil temperature and moisture data of Santa Barbara as input and the corresponding strawberry yields or prices as output. Secondly, the remote sensing ensemble model (SIM_CNN-LSTM_Ens), which is an ensemble model of Convolutional Neural Network LSTM (CNN-LSTM) models, is trained and tested using satellite images of the same county as input mapped to the same yields and prices as output. These two ensembles forecast strawberry yields and prices with minimal forecasting errors and highest model correlation for five weeks ahead forecasts. Finally, the forecasts of these two models are ensembled to have a final forecasted value for yields and prices by introducing a voting ensemble. Based on an aggregated performance measure (AGM), it is found that this voting ensemble not only enhances the forecasting performance by 5% compared to its best performing component model but also outperforms the Deep Learning (DL) ensemble model found in literature by 33% for forecasting yields and 21% for forecasting prices.', 'corpus_id': 231951571, 'score': 0}, {'doc_id': '232270161', 'title': 'Spatio-temporal Crop Classification On Volumetric Data', 'abstract': 'Large-area crop classification using multi-spectral imagery is a widely studied problem for several decades and is generally addressed using classical Random Forest classifier. Recently, deep convolutional neural networks (DCNN) have been proposed. However, these methods only achieved results comparable with Random Forest. In this work, we present a novel CNN based architecture for large-area crop classification. Our methodology combines both spatio-temporal analysis via 3D CNN as well as temporal analysis via 1D CNN. We evaluated the efficacy of our approach on Yolo and Imperial county benchmark datasets. Our combined strategy outperforms both classical as well as recent DCNN based methods in terms of classification accuracy by 2% while maintaining a minimum number of parameters and the lowest inference time.', 'corpus_id': 232270161, 'score': 0}, {'doc_id': '231728679', 'title': 'Low Dimensional Convolutional Neural Network For Solar Flares GOES Time Series Classification', 'abstract': 'Space weather phenomena such as solar flares, have massive destructive power when reaches certain amount of magnitude. Such high magnitude solar flare event can interfere space-earth radio communications and neutralize space-earth electronics equipment. In the current study, we explorer the deep learning approach to build a solar flare forecasting model and examine its limitations along with the ability of features extraction, based on the available time-series data. For that purpose, we present a multi-layer 1D Convolutional Neural Network (CNN) to forecast solar flare events probability occurrence of M and X classes at 1,3,6,12,24,48,72,96 hours time frame. In order to train and evaluate the performance of the model, we utilised the available Geostationary Operational Environmental Satellite (GOES) X-ray time series data, ranged between July 1998 and January 2019, covering almost entirely the solar cycles 23 and 24. The forecasting model were trained and evaluated in two different scenarios (1) random selection and (2) chronological selection, which were compare afterward. Moreover we compare our results to those considered as state-of-the-art flare forecasting models, both with similar approaches and different ones.The majority of the results indicates that (1) chronological selection obtain a degradation factor of 3% versus the random selection for the M class model and elevation factor of 2% for the X class model. (2) When consider utilizing only X-ray time-series data, the suggested model achieve high score results compare to other studies. (3) The suggested model combined with solely X-ray time-series fails to distinguish between M class magnitude and X class magnitude solar flare events. All source code are available at https://github.com/vladlanda', 'corpus_id': 231728679, 'score': 0}, {'doc_id': '228847291', 'title': 'The influence of training sample size on the accuracy of deep learning models for the prediction of soil properties with near-infrared spectroscopy data', 'abstract': 'Abstract. The number of samples used in the calibration data set affects the quality of the generated predictive models using visible, near and shortwave infrared (VIS–NIR–SWIR) spectroscopy for soil attributes. Recently, the convolutional neural network (CNN) has been regarded as a highly accurate model for predicting soil properties on a large database. However, it has not yet been ascertained how large the sample size should be for CNN model to be effective. This paper investigates the effect of the training sample size on the accuracy of deep learning and machine learning models. It aims at providing an estimate of how many calibration samples are needed to improve the model performance of soil properties predictions with CNN as compared to conventional machine learning models. In addition, this paper also looks at a way to interpret the CNN models, which are commonly labelled as a black box. It is hypothesised that the performance of machine learning models will increase with an increasing number of training samples, but it will plateau when it reaches a certain number, while the performance of CNN will keep improving. The performances of two machine learning models (partial least squares regression – PLSR; Cubist) are compared against the CNN model. A VIS–NIR–SWIR spectra library from Brazil, containing 4251 unique sites with averages of two to three samples per depth (a total of 12\u2009044 samples), was divided into calibration (3188 sites) and validation (1063 sites) sets. A subset of the calibration data set was then created to represent a smaller calibration data set ranging from 125, 300, 500, 1000, 1500, 2000, 2500 and 2700 unique sites, which is equivalent to a sample size of approximately 350, 840, 1400, 2800, 4200, 5600, 7000 and 7650. All three models (PLSR, Cubist and CNN) were generated for each sample size of the unique sites for the prediction of five different soil properties, i.e. cation exchange capacity, organic carbon, sand, silt and clay content. These calibration subset sampling processes and modelling were repeated 10 times to provide a better representation of the model performances. Learning curves showed that the accuracy increased with an increasing number of training samples. At a lower number of samples ( \u20091000), PLSR and Cubist performed better than CNN. The performance of CNN outweighed the PLSR and Cubist model at a sample size of 1500 and 1800, respectively. It can be recommended that deep learning is most efficient for spectra modelling for sample sizes above 2000. The accuracy of the PLSR and Cubist model seems to reach a plateau above sample sizes of 4200 and 5000, respectively, while the accuracy of CNN has not plateaued. A sensitivity analysis of the CNN model demonstrated its ability to determine important wavelengths region that affected the predictions of various soil attributes.', 'corpus_id': 228847291, 'score': 1}, {'doc_id': '52314629', 'title': 'Transfer Learning for Soil Spectroscopy Based on Convolutional Neural Networks and Its Application in Soil Clay Content Mapping Using Hyperspectral Imagery', 'abstract': 'Soil spectra are often measured in the laboratory, and there is an increasing number of large-scale soil spectral libraries establishing across the world. However, calibration models developed from soil libraries are difficult to apply to spectral data acquired from the field or space. Transfer learning has the potential to bridge the gap and make the calibration model transferrable from one sensor to another. The objective of this study is to explore the potential of transfer learning for soil spectroscopy and its performance on soil clay content estimation using hyperspectral data. First, a one-dimensional convolutional neural network (1D-CNN) is used on Land Use/Land Cover Area Frame Survey (LUCAS) mineral soils. To evaluate whether the pre-trained 1D-CNN model was transferrable, LUCAS organic soils were used to fine-tune and validate the model. The fine-tuned model achieved a good accuracy (coefficient of determination (R2) = 0.756, root-mean-square error (RMSE) = 7.07 and ratio of percent deviation (RPD) = 2.26) for the estimation of clay content. Spectral index, as suggested as a simple transferrable feature, was also explored on LUCAS data, but did not performed well on the estimation of clay content. Then, the pre-trained 1D-CNN model was further fine-tuned by field samples collect in the study area with spectra extracted from HyMap imagery, achieved an accuracy of R2 = 0.601, RMSE = 8.62 and RPD = 1.54. Finally, the soil clay map was generated with the fine-tuned 1D-CNN model and hyperspectral data.', 'corpus_id': 52314629, 'score': 1}, {'doc_id': '232233184', 'title': 'A review of machine learning in processing remote sensing data for mineral exploration', 'abstract': 'As a primary step in mineral exploration, a variety of features are mapped such as lithological units, alteration types, structures, and minerals. These features are extracted to aid decision-making in targeting ore deposits. Different types of remote sensing data including satellite optical and radar, airborne, and drone-based data make it possible to overcome problems associated with mapping these important parameters on the field. The rapid increase in the volume of remote sensing data obtained from different platforms has allowed scientists to develop advanced, innovative, and powerful data processing methodologies. Machine learning methods can help in processing a wide range of remote sensing data and in determining the relationship between the reflectance continuum and features of interest. Moreover, these methods are robust in processing spectral and ground truth measurements ∗Corresponding author Email address: e.farahbakhsh@aut.ac.ir (Ehsan Farahbakhsh) School of Mining Engineering, College of Engineering, University of Tehran, Tehran, Iran Department of Mining Engineering, Amirkabir University of Technology (Tehran Polytechnic), Tehran, Iran EarthByte Group, School of Geosciences, University of Sydney, Australia School of Mathematics and Statistics, University of New South Wales, Sydney, NSW, 2052, Australia Preprint submitted to Elsevier March 16, 2021 ar X iv :2 10 3. 07 67 8v 1 [ cs .L G ] 1 3 M ar 2 02 1 against noise and uncertainties. In recent years, many studies have been carried out by supplementing geological surveys with remote sensing data, and this area is now considered a hotspot in geoscience research. This paper reviews the implementation and adaptation of some popular and recently established machine learning methods for remote sensing data processing and investigates their applications for exploring different ore deposits. Lastly, the challenges and future directions in this critical interdisciplinary field are discussed.', 'corpus_id': 232233184, 'score': 0}, {'doc_id': '233265664', 'title': 'Convolutional Neural Network approach for the prediction of Soil texture properties', 'abstract': 'Background/Objectives: The main objective is to achieve improved performance of soil properties prediction for hyperspectral data. In this work, convolutional neural network is trained to understand the pattern of hyperspectral data by spatial interpolation. Methods/Statistical analysis: The proposed methodology is used to predict six soil properties- Organic Carbon content (OC), Cation Exchange Capacity (CEC), Nitrogen Content (N), pH level in water, Clay particle and Sand Particle. Soil texture which defines the relative content of soil particles is determined by the percentage of clay, sand and silt in the soil. The input to the Convolutional Neural Network (CNN) is the Hyperspectral data in the form of multiple arrays. The statistical evaluation of model performance is evaluated using root-mean-square error and r square. Findings: In this research, deep learning approach is used to capture the pattern hidden in the soil. Deep learning is a kind of neural network which can model complex relationship for representing non-linearity for a scalable data. The main challenge is predicting a soil type, as it involves complex structural characteristics and soil features. Novelty/Improvements: The performance of soil texture prediction is improved by automatic feature learning capability in the proposed CNN model. The average rmse value obtained in proposed method for all the six soil texture properties is 5.68%. \nKeywords: Soil texture; convolutional neural network; hyperspectral data; deep learning', 'corpus_id': 233265664, 'score': 1}, {'doc_id': '73299117', 'title': 'Obstrução intestinal por melanoma metastático: Relato de caso', 'abstract': 'Melanoma is a highly malignant tumor and represents 3% of all cancers. Malignant melanoma is the most common metastatic tumor involving the gastrointestinal tract, the small bowel being the localization most frequently involved. Its clinical presentation is inespecific and an obstructive acute abdomen syndrome could be its first manifestation. The article is about a case of a 36 years-old patient, which had been submitted to resection of melanoma in the dorsal region in 2009. The patient developed acute intestinal obstruction and laparotomy showed intestinal intussusceptions caused by metastatic melanoma at 50 cm from the angle of Treitz. The specimen was sent for pathological, which confirmed the presence of metastatic melanoma in the small bowel. The treatment of metastatic melanoma involves systemic treatment and treatment of complications. Intestinal metastasis must be considered in any patient with gastrointestinal symptoms and previous history of melanoma and the surgical treatment should always be considered because it can occasionally provide long-term survival. Key-words: Melanoma. Metastasis. Intestinal obstruction. Intussesception. Surgery.', 'corpus_id': 73299117, 'score': 0}]
71	Multi-View 3D Pose Estimation	c2a4021e16744026d26dd1ebf236e015	3926	{}	"[{'doc_id': '212747986', 'title': '3D Crowd Counting via Multi-View Fusion with 3D Gaussian Kernels', 'abstract': 'Crowd counting has been studied for decades and a lot of works have achieved good performance, especially the DNNs-based density map estimation methods. Most existing crowd counting works focus on single-view counting, while few works have studied multi-view counting for large and wide scenes, where multiple cameras are used. Recently, an end-to-end multi-view crowd counting method called multi-view multi-scale (MVMS) has been proposed, which fuses multiple camera views using a CNN to predict a 2D scene-level density map on the ground-plane. Unlike MVMS, we propose to solve the multi-view crowd counting task through 3D feature fusion with 3D scene-level density maps, instead of the 2D ground-plane ones. Compared to 2D fusion, the 3D fusion extracts more information of the people along z-dimension (height), which helps to solve the scale variations across multiple views. The 3D density maps still preserve the 2D density maps property that the sum is the count, while also providing 3D information about the crowd density. We also explore the projection consistency among the 3D prediction and the ground-truth in the 2D views to further enhance the counting performance. The proposed method is tested on 3 multi-view counting datasets and achieves better or comparable counting performance to the state-of-the-art.', 'corpus_id': 212747986, 'score': 0}, {'doc_id': '215745329', 'title': 'A Novel Pose Proposal Network and Refinement Pipeline for Better Object Pose Estimation', 'abstract': ""In this paper, we present a novel deep learning pipeline for 6D object pose estimation and refinement from RGB inputs. The first component of the pipeline leverages a region proposal framework to estimate multi-class single-shot 6D object poses directly from an RGB image and through a CNN-based encoder multi-decoders network. The second component, a multi-attentional pose refinement network (MARN), iteratively refines the estimated pose. MARN takes advantage of both visual and flow features to learn a relative transformation between an initially predicted pose and a target pose. MARN is further augmented by a spatial multi-attention block that emphasizes objects' discriminative feature parts. Experiments on three benchmarks for 6D pose estimation show that the proposed pipeline outperforms state-of-the-art RGB-based methods with competitive runtime performance."", 'corpus_id': 215745329, 'score': 0}, {'doc_id': '70350026', 'title': 'Self-Supervised Learning of 3D Human Pose Using Multi-View Geometry', 'abstract': 'Training accurate 3D human pose estimators requires large amount of 3D ground-truth data which is costly to collect. Various weakly or self supervised pose estimation methods have been proposed due to lack of 3D data. Nevertheless, these methods, in addition to 2D ground-truth poses, require either additional supervision in various forms (e.g. unpaired 3D ground truth data, a small subset of labels) or the camera parameters in multiview settings. To address these problems, we present EpipolarPose, a self-supervised learning method for 3D human pose estimation, which does not need any 3D ground-truth data or camera extrinsics. During training, EpipolarPose estimates 2D poses from multi-view images, and then, utilizes epipolar geometry to obtain a 3D pose and camera geometry which are subsequently used to train a 3D pose estimator. We demonstrate the effectiveness of our approach on standard benchmark datasets (i.e. Human3.6M and MPI-INF-3DHP) where we set the new state-of-the-art among weakly/self-supervised methods. Furthermore, we propose a new performance measure Pose Structure Score (PSS) which is a scale invariant, structure aware measure to evaluate the structural plausibility of a pose with respect to its ground truth. Code and pretrained models are available at https://github.com/mkocabas/EpipolarPose', 'corpus_id': 70350026, 'score': 1}, {'doc_id': '212736938', 'title': 'Weakly-Supervised 3D Human Pose Learning via Multi-View Images in the Wild', 'abstract': 'One major challenge for monocular 3D human pose estimation in-the-wild is the acquisition of training data that contains unconstrained images annotated with accurate 3D poses. In this paper, we address this challenge by proposing a weakly-supervised approach that does not require 3D annotations and learns to estimate 3D poses from unlabeled multi-view data, which can be acquired easily in in-the-wild environments. We propose a novel end-to-end learning framework that enables weakly-supervised training using multi-view consistency. Since multi-view consistency is prone to degenerated solutions, we adopt a 2.5D pose representation and propose a novel objective function that can only be minimized when the predictions of the trained model are consistent and plausible across all camera views. We evaluate our proposed approach on two large scale datasets (Human3.6M and MPII-INF-3DHP) where it achieves state-of-the-art performance among semi-/weakly-supervised methods.', 'corpus_id': 212736938, 'score': 1}, {'doc_id': '214714313', 'title': 'MetaFuse: A Pre-trained Fusion Model for Human Pose Estimation', 'abstract': 'Cross view feature fusion is the key to address the occlusion problem in human pose estimation. The current fusion methods need to train a separate model for every pair of cameras making them difficult to scale. In this work, we introduce MetaFuse, a pre-trained fusion model learned from a large number of cameras in the Panoptic dataset. The model can be efficiently adapted or finetuned for a new pair of cameras using a small number of labeled images. The strong adaptation power of MetaFuse is due in large part to the proposed factorization of the original fusion model into two parts—(1) a generic fusion model shared by all cameras, and (2) lightweight camera-dependent transformations. Furthermore, the generic model is learned from many cameras by a meta-learning style algorithm to maximize its adaptation capability to various camera poses. We observe in experiments that MetaFuse finetuned on the public datasets outperforms the state-of-the-arts by a large margin which validates its value in practice.', 'corpus_id': 214714313, 'score': 1}, {'doc_id': '153312868', 'title': 'Learnable Triangulation of Human Pose', 'abstract': 'We present two novel solutions for multi-view 3D human pose estimation based on new learnable triangulation methods that combine 3D information from multiple 2D views. The first (baseline) solution is a basic differentiable algebraic triangulation with an addition of confidence weights estimated from the input images. The second, more complex, solution is based on volumetric aggregation of 2D feature maps from the 2D backbone followed by refinement via 3D convolutions that produce final 3D joint heatmaps. Crucially, both of the approaches are end-to-end differentiable, which allows us to directly optimize the target metric. We demonstrate transferability of the solutions across datasets and considerably improve the multi-view state of the art on the Human3.6M dataset.', 'corpus_id': 153312868, 'score': 1}, {'doc_id': '233864855', 'title': 'PoseAug: A Differentiable Pose Augmentation Framework for 3D Human Pose Estimation', 'abstract': 'Existing 3D human pose estimators suffer poor generalization performance to new datasets, largely due to the limited diversity of 2D-3D pose pairs in the training data. To address this problem, we present PoseAug, a new auto-augmentation framework that learns to augment the available training poses towards a greater diversity and thus improve generalization of the trained 2D-to-3D pose estimator. Specifically, PoseAug introduces a novel pose augmentor that learns to adjust various geometry factors (e.g., posture, body size, view point and position) of a pose through differentiable operations. With such differentiable capacity, the augmentor can be jointly optimized with the 3D pose estimator and take the estimation error as feedback to generate more diverse and harder poses in an online manner. Moreover, PoseAug introduces a novel part-aware Kinematic Chain Space for evaluating local joint-angle plausibility and develops a discriminative module accordingly to ensure the plausibility of the augmented poses. These elaborate designs enable PoseAug to generate more diverse yet plausible poses than existing offline augmentation methods, and thus yield better generalization of the pose estimator. PoseAug is generic and easy to be applied to various 3D pose estimators. Extensive experiments demonstrate that PoseAug brings clear improvements on both intra-scenario and cross-scenario datasets. Notably, it achieves 88.6% 3D PCK on MPI-INF-3DHP under cross-dataset evaluation setup, improving upon the previous best data augmentation based method by 9.1%. Code can be found at: this https URL.', 'corpus_id': 233864855, 'score': 0}, {'doc_id': '84666114', 'title': 'Suppressor of Cytokine Signaling-1 Regulates the Sensitivity of Pancreatic β Cells to Tumor Necrosis Factor*', 'abstract': 'Suppressor of cytokine signaling-1 (SOCS-1) is a negative regulator of the Jak-STAT (signal transducer and activator of transcription cytokine) signaling pathway but may also regulate other pathways. At least in vitro, SOCS-1 inhibits the action of multiple cytokines. By studying the effects of SOCS-1 deficiency, we investigated whether SOCS-1 is involved in preventing cytokine-induced death of pancreatic islet cells, a potential mechanism of insulin deficiency in autoimmune diabetes. Tumor necrosis factor (TNF) + interferon-γ (IFNγ) was more potent at inducing cell death in SOCS-1−/− islets than in wild type. Individually, these cytokines did not induce cell death. The titration of the two cytokines suggested that this increased cell death was because of hypersensitivity to TNF. Interleukin-1 + IFNγ induced the same level of cell death in SOCS-1−/− and wild-type islets, suggesting that the sensitivity of islets to IFNγ or interleukin-1-mediated cytotoxicity is not affected by SOCS-1 deficiency. Additionally, SOCS-1−/− β cells were responsive to lower concentrations of TNF measured by class I major histocompatibility complex up-regulation. The TNF + IFNγ damage of islets was mediated by inducible nitric-oxide synthase (iNOS), and increased iNOS expression and nitric oxide production were found in SOCS-1−/− islets following cytokine treatment. A further analysis revealed that SOCS-1 deficiency results in augmented TNF signaling via the p38 mitogen-activated protein kinase pathway but not NFκB or c-Jun N-terminal kinase pathways. Increased p38 signaling may be responsible for the increased iNOS expression in SOCS-1−/− islets. Therefore, these findings provide evidence that physiological levels of SOCS-1 negatively regulate TNF signaling.', 'corpus_id': 84666114, 'score': 0}, {'doc_id': '214803107', 'title': 'Light3DPose: Real-time Multi-Person 3D Pose Estimation from Multiple Views', 'abstract': 'We present an approach to perform 3D pose estimation of multiple people from a few calibrated camera views. Our architecture, leveraging the recently proposed unprojection layer, aggregates feature-maps from a 2D pose estimator backbone into a comprehensive representation of the 3D scene. Such intermediate representation is then elaborated by a fully-convolutional volumetric network and a decoding stage to extract 3D skeletons with sub-voxel accuracy. Our method achieves state of the art MPJPE on the CMU Panoptic dataset using a few unseen views and obtains competitive results even with a single input view. We also assess the transfer learning capabilities of the model by testing it against the publicly available Shelf dataset obtaining good performance metrics. The proposed method is inherently efficient: as a pure bottom-up approach, it is computationally independent of the number of people in the scene. Furthermore, even though the computational burden of the 2D part scales linearly with the number of input views, the overall architecture is able to exploit a very lightweight 2D backbone which is orders of magnitude faster than the volumetric counterpart, resulting in fast inference time. The system can run at 6 FPS, processing up to 10 camera views on a single 1080Ti GPU.', 'corpus_id': 214803107, 'score': 1}, {'doc_id': '177503032', 'title': 'بررسی ضریب تغییرات مکانی و زمانی هدرروی خاک تحت جریان متمرکز در شیارهای زراعت گندم دیم', 'abstract': 'در دهه\u200cهای گذشته نقش فرسایش خاک مورد توجه بسیاری از پژوهشگران بوده، به\u200cطوری\xad\u200cکه آگاهی از عامل\u200cهای مؤثر در آن و تغییرات گسترده این عوامل از نیازهای اولیه مدیریت سرزمین است. هدف از این پژوهش، بررسی ضریب تغییرات زمانی و مکانی متغیرهای فرسایشی و عامل\u200cهای هیدرولیکی در شیارهای حاصل از شخم در زراعت دیم گندم در دامنه\u200cهای با شیب 20 درصد است. در این رابطه، ابتدا شخم رایج منطقه از بالا به\xad\u200cسمت پایین شیب انجام و شیارهای 10، 20 و 30 متری ایجاد شد. پس از اعمال دبی\u200cهای 10، 15 و 20 لیتر بر دقیقه به\u200c\xadمدت 10 دقیقه، متغیرهای هیدرولیکی و غلظت\u200cهای رسوب در سه فاصله زمانی و مکانی مساوی اندازه\u200cگیری و نرخ جدایش و انتقال ذرات محاسبه شد. به\xad\u200cمنظور بررسی ضریب تغییرات عامل\u200cهای یاد شده نیز شاخص\u200c خطای نسبی محاسبه شد. نتایج نشان داد که خطای نسبی نرخ جدایش، انتقال و غلظت رسوب اختلاف معنی\u200cداری در سطح یک درصد را با عامل\u200cهای هیدرولیکی دارند. این اختلاف بین قدرت جریان خطای نسبی نرخ جدایش و انتقال در سطح پنج درصد معنی\u200cدار شد. بررسی زمانی هدرروی خاک نیز نشان داد که ضریب تغییرات در بازه زمانی انتهایی و ابتدایی بیشتر از بازه میانی است، ولی بین این بازه\u200cها تفاوت معنی\u200cدار مشاهده نشد. علت آن \xadرا می\xad\u200cتوان به\xad\u200cوجود رسوبات فراهم در مراحل اولیه و زیرخوردگی و ریزش دیواره\u200cها در مراحل پایانی نسبت داد. بررسی مکانی مقدار ضریب تغییرات نرخ جدایش نشان داد که این عامل در مقطع ابتدایی شیارها از مقاطع بعدی در سطح یک درصد بوده و بیشتر است، در حالی\xad\u200cکه بین مقاطع دوم و سوم اختلاف معنی\u200cداری وجود نداشت.', 'corpus_id': 177503032, 'score': 0}]"
72	Transformer	bbca27187a8eb6f1e22cf6b21598fbe4	11003	{}	[{'doc_id': '52967399', 'title': 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding', 'abstract': 'We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).', 'corpus_id': 52967399, 'score': 1}, {'doc_id': '227053761', 'title': 'Neuro-Symbolic Representations for Video Captioning: A Case for Leveraging Inductive Biases for Vision and Language', 'abstract': 'Neuro-symbolic representations have proved effective in learning structure information in vision and language. In this paper, we propose a new model architecture for learning multi-modal neuro-symbolic representations for video captioning. Our approach uses a dictionary learning-based method of learning relations between videos and their paired text descriptions. We refer to these relations as relative roles and leverage them to make each token role-aware using attention. This results in a more structured and interpretable architecture that incorporates modality-specific inductive biases for the captioning task. Intuitively, the model is able to learn spatial, temporal, and cross-modal relations in a given pair of video and text. The disentanglement achieved by our proposal gives the model more capacity to capture multi-modal structures which result in captions with higher quality for videos. Our experiments on two established video captioning datasets verifies the effectiveness of the proposed approach based on automatic metrics. We further conduct a human evaluation to measure the grounding and relevance of the generated captions and observe consistent improvement for the proposed model. The codes and trained models can be found at this https URL', 'corpus_id': 227053761, 'score': 0}, {'doc_id': '221971009', 'title': 'SPARTA: Efficient Open-Domain Question Answering via Sparse Transformer Matching Retrieval', 'abstract': 'We introduce SPARTA, a novel neural retrieval method that shows great promise in performance, generalization, and interpretability for open-domain question answering. Unlike many neural ranking methods that use dense vector nearest neighbor search, SPARTA learns a sparse representation that can be efficiently implemented as an Inverted Index. The resulting representation enables scalable neural retrieval that does not require expensive approximate vector search and leads to better performance than its dense counterpart. We validated our approaches on 4 open-domain question answering (OpenQA) tasks and 11 retrieval question answering (ReQA) tasks. SPARTA achieves new state-of-the-art results across a variety of open-domain question answering tasks in both English and Chinese datasets, including open SQuAD, CMRC and etc. Analysis also confirms that the proposed method creates human interpretable representation and allows flexible control over the trade-off between performance and efficiency.', 'corpus_id': 221971009, 'score': 1}, {'doc_id': '226246229', 'title': 'Indic-Transformers: An Analysis of Transformer Language Models for Indian Languages', 'abstract': 'Language models based on the Transformer architecture have achieved state-of-the-art performance on a wide range of NLP tasks such as text classification, question-answering, and token classification. However, this performance is usually tested and reported on high-resource languages, like English, French, Spanish, and German. Indian languages, on the other hand, are underrepresented in such benchmarks. Despite some Indian languages being included in training multilingual Transformer models, they have not been the primary focus of such work. In order to evaluate the performance on Indian languages specifically, we analyze these language models through extensive experiments on multiple downstream tasks in Hindi, Bengali, and Telugu language. Here, we compare the efficacy of fine-tuning model parameters of pre-trained models against that of training a language model from scratch. Moreover, we empirically argue against the strict dependency between the dataset size and model performance, but rather encourage task-specific model and method selection. We achieve state-of-the-art performance on Hindi and Bengali languages for text classification task. Finally, we present effective strategies for handling the modeling of Indian languages and we release our model checkpoints for the community : https://huggingface.co/neuralspace-reverie.', 'corpus_id': 226246229, 'score': 0}, {'doc_id': '226227533', 'title': 'COOT: Cooperative Hierarchical Transformer for Video-Text Representation Learning', 'abstract': 'Many real-world video-text tasks involve different levels of granularity, such as frames and words, clip and sentences or videos and paragraphs, each with distinct semantics. In this paper, we propose a Cooperative hierarchical Transformer (COOT) to leverage this hierarchy information and model the interactions between different levels of granularity and different modalities. The method consists of three major components: an attention-aware feature aggregation layer, which leverages the local temporal context (intra-level, e.g., within a clip), a contextual transformer to learn the interactions between low-level and high-level semantics (inter-level, e.g. clip-video, sentence-paragraph), and a cross-modal cycle-consistency loss to connect video and text. The resulting method compares favorably to the state of the art on several benchmarks while having few parameters. All code is available open-source at this https URL', 'corpus_id': 226227533, 'score': 0}, {'doc_id': '59604492', 'title': 'End-to-End Open-Domain Question Answering with BERTserini', 'abstract': 'We demonstrate an end-to-end question answering system that integrates BERT with the open-source Anserini information retrieval toolkit. In contrast to most question answering and reading comprehension models today, which operate over small amounts of input text, our system integrates best practices from IR with a BERT-based reader to identify answers from a large corpus of Wikipedia articles in an end-to-end fashion. We report large improvements over previous results on a standard benchmark test collection, showing that fine-tuning pretrained BERT with SQuAD is sufficient to achieve high accuracy in identifying answer spans.', 'corpus_id': 59604492, 'score': 1}, {'doc_id': '226965686', 'title': 'JNLP Team: Deep Learning for Legal Processing in COLIEE 2020', 'abstract': 'We propose deep learning based methods for automatic systems of legal retrieval and legal question-answering in COLIEE 2020. These systems are all characterized by being pre-trained on large amounts of data before being finetuned for the specified tasks. This approach helps to overcome the data scarcity and achieve good performance, thus can be useful for tackling related problems in information retrieval, and decision support in the legal domain. Besides, the approach can be explored to deal with other domain specific problems.', 'corpus_id': 226965686, 'score': 0}, {'doc_id': '215737187', 'title': 'Dense Passage Retrieval for Open-Domain Question Answering', 'abstract': 'Open-domain question answering relies on efficient passage retrieval to select candidate contexts, where traditional sparse vector space models, such as TF-IDF or BM25, are the de facto method. In this work, we show that retrieval can be practically implemented using dense representations alone, where embeddings are learned from a small number of questions and passages by a simple dual-encoder framework. When evaluated on a wide range of open-domain QA datasets, our dense retriever outperforms a strong Lucene-BM25 system largely by 9%-19% absolute in terms of top-20 passage retrieval accuracy, and helps our end-to-end QA system establish new state-of-the-art on multiple open-domain QA benchmarks.', 'corpus_id': 215737187, 'score': 1}, {'doc_id': '227151337', 'title': 'SEA: Sentence Encoder Assembly for Video Retrieval by Textual Queries', 'abstract': 'Retrieving unlabeled videos by textual queries, known as Ad-hoc Video Search (AVS), is a core theme in multimedia data management and retrieval. The success of AVS counts on cross-modal representation learning that encodes both query sentences and videos into common spaces for semantic similarity computation. Inspired by the initial success of previously few works in combining multiple sentence encoders, this paper takes a step forward by developing a new and general method for effectively exploiting diverse sentence encoders. The novelty of the proposed method, which we term Sentence Encoder Assembly (SEA), is two-fold. First, different from prior art that use only a single common space, SEA supports text-video matching in multiple encoder-specific common spaces. Such a property prevents the matching from being dominated by a specific encoder that produces an encoding vector much longer than other encoders. Second, in order to explore complementarities among the individual common spaces, we propose multi-space multi-loss learning. As extensive experiments on four benchmarks (MSR-VTT, TRECVID AVS 2016-2019, TGIF and MSVD) show, SEA surpasses the state-of-the-art. In addition, SEA is extremely ease to implement. All this makes SEA an appealing solution for AVS and promising for continuously advancing the task by harvesting new sentence encoders.', 'corpus_id': 227151337, 'score': 0}, {'doc_id': '211204736', 'title': 'REALM: Retrieval-Augmented Language Model Pre-Training', 'abstract': 'Language model pre-training has been shown to capture a surprising amount of world knowledge, crucial for NLP tasks such as question answering. However, this knowledge is stored implicitly in the parameters of a neural network, requiring ever-larger networks to cover more facts. \nTo capture knowledge in a more modular and interpretable way, we augment language model pre-training with a latent knowledge retriever, which allows the model to retrieve and attend over documents from a large corpus such as Wikipedia, used during pre-training, fine-tuning and inference. For the first time, we show how to pre-train such a knowledge retriever in an unsupervised manner, using masked language modeling as the learning signal and backpropagating through a retrieval step that considers millions of documents. \nWe demonstrate the effectiveness of Retrieval-Augmented Language Model pre-training (REALM) by fine-tuning on the challenging task of Open-domain Question Answering (Open-QA). We compare against state-of-the-art models for both explicit and implicit knowledge storage on three popular Open-QA benchmarks, and find that we outperform all previous methods by a significant margin (4-16% absolute accuracy), while also providing qualitative benefits such as interpretability and modularity.', 'corpus_id': 211204736, 'score': 0}]
73	Causal Knowledge	feb923f660bae105ec38d6e4deef4777	18397	{}	"[{'doc_id': '235655134', 'title': 'Open-Ended Generative Commonsense Question Answering with Knowledge Graph-enhanced Language Models', 'abstract': 'Natural Language Processing (NLP) has seen the development of large-scale pre-trained language models, which are trained on vast amounts of language data on unsupervised tasks (called pretraining. The model parameters obtained through pre-training are later fine-tuned on specific tasks. Famous examples of such models include BERT (Devlin et al., 2018), and more recently GPT-3 (Brown et al., 2020). Such a pretraining-finetuning paradigm has seen a lot of success in pushing the ability of NLP models to understand language, as we expect the unsupervised pre-training tasks allow the language models to learn of the syntax and semantics of language, as well as commonsense knowledge. Nevertheless, all of such knowledge is only contained implicitly in the model’s parameters. Would it be possible to combine external structured knowledge with the implicit knowledge of these language models? We would like to explore possibilities to build a model that combines the power of large-scale pretrained language models with information from commonsense knowledge graphs (KG) such as ConceptNet (Liu and Singh, 2004; Speer et al., 2016). A knowledge graph consists of nodes that correspond to real-world concepts, and are connected by directed edges that represent the relation between two concepts. We focus on open-ended question answering, which is a language question and answer task that requires commonsense knowledge about the world. Specifically, we focus on a generative type of this task, where the answers must be spontaneously generated by a model, as opposed to a multi-choice question. We work on a dataset called ProtoQA (Boratko et al., 2020). This dataset contains questions regarding “prototypical situations"", such as “Name something that people usually do before they Figure 1: Planned Architecture of our model.', 'corpus_id': 235655134, 'score': 0}, {'doc_id': '235266051', 'title': 'CIDER: Commonsense Inference for Dialogue Explanation and Reasoning', 'abstract': 'Commonsense inference to understand and explain human language is a fundamental research problem in natural language processing. Explaining human conversations poses a great challenge as it requires contextual understanding, planning, inference, and several aspects of reasoning including causal, temporal, and commonsense reasoning. In this work, we introduce CIDER – a manually curated dataset that contains dyadic dialogue explanations in the form of implicit and explicit knowledge triplets inferred using contextual commonsense inference. Extracting such rich explanations from conversations can be conducive to improving several downstream applications. The annotated triplets are categorized by the type of commonsense knowledge present (e.g., causal, conditional, temporal). We set up three different tasks conditioned on the annotated dataset: Dialogue-level Natural Language Inference, Span Extraction, and Multi-choice Span Selection. Baseline results obtained with transformer-based models reveal that the tasks are difficult, paving the way for promising future research. The dataset and the baseline implementations are publicly available at https://github.com/declare-lab/CIDER.', 'corpus_id': 235266051, 'score': 1}, {'doc_id': '235311044', 'title': 'Balanced COPA: Countering Superficial Cues in Causal Reasoning', 'abstract': 'Pretrained language models such as ELMo [15], BERT [1], RoBERTa [12] and ALBERT [9] have led to improved performance in benchmarks of natural language understanding, in tasks such as natural language inference [NLI, 11], argumentation [14], and commonsense reasoning [10, 18]. However, recent work has identified superficial cues in benchmark datasets which are predictive of the correct answer, such as unbalanced token distributions and lexical overlap. Once these cues are neutralized, models perform poorly, suggesting that their good performance is an instance of the Clever Hans effect1 [16]: Models trained on datasets with superficial cues learn heuristics for exploiting these cues, but do not develop any deeper understanding of the task. While superficial cues have been identified in, among others, datasets for NLI [5, 13], machine reading comprehension [20], and argumentation [14], one of the main benchmarks for commonsense reasoning, namely the Choice of Plausible Alternatives [COPA, 17], has not been analyzed so far. Here we present an analysis of superficial cues in COPA. Given a premise, such as The man broke his toe, COPA requires choosing the more plausible, causally related alternative, in this case either: because He got a hole in his sock (wrong) or because He dropped a hammer on his foot (correct). Our analysis reveals that COPA contains superfcial cues (§2) and that finetuned BERT [1] performs well (83.9 percent accuracy) on easy instances containing superficial cues, but worse (71.9 percent) on hard instances without such simple cues (§4.3). To prevent models from exploiting superficial cues in COPA, we introduce Balanced COPA (B-COPA). BCOPA contains one additional, mirrored instance for each original training instance. This mirrored instance uses the same alternatives as the corresponding original instance,', 'corpus_id': 235311044, 'score': 1}, {'doc_id': '235679389', 'title': 'Multi-Sense: Commonsense Enrichment through Multi-Task Learning', 'abstract': 'A recent discovery showed that large pre-trained language models (LM) are capable of encoding knowledge into their parameters, as a result, serving as powerful zero-shot/few-shot learners of knowledge-dependent tasks. However, the commonsense knowledge of such models is relatively under-explored despite their importance on various natural language tasks. In this paper, we propose Multi-Sense model that is enriched with commonsense by multi-task learning (MTL) on commonsense question-answering and natural language inference (NLI) tasks. We hypothesize that supervision from tasks that require commonsense reasoning ability will implicitly help strengthen the commonsense representation within the model parameters. Empirical results demonstrate that the multi-task commonsense enrichment step is helpful in downstream tasks (i.e., reading comprehension, fact-checking). In addition, we demonstrate the strength of Multi-Sense in low resource settings by conducting a few-shot learning analysis.', 'corpus_id': 235679389, 'score': 0}, {'doc_id': '235485240', 'title': 'SPBERT: Pre-training BERT on SPARQL Queries for End-to-end Question Answering over Knowledge Graphs', 'abstract': 'We aim to create an unprecedented attempt to build an endto-end Question Answering (QA) over Knowledge Graphs (KGs), which can construct SPARQL queries from natural language questions and generate a verbalized answer to its queries. Hence, we introduce SPBERT, a Transformer-based language model pre-trained on massive SPARQL query logs. By incorporating masked language modelling objective and word structural objective, SPBERT can learn general-purpose representations in both natural language and SPARQL query language and make the most of the sequential order of words that are crucial for structured language like SPARQL. In this paper, we investigate how SPBERT and encoder-decoder architecture can be adapted for Knowledge-based QA corpora. We conduct exhaustive experiments on two auxiliary tasks, including SPARQL Query Construction and Answer Verbalization Generation. Results show that SPBERT obtains promising performance and achieves state-of-the-art results on several of these tasks.', 'corpus_id': 235485240, 'score': 0}, {'doc_id': '234470046', 'title': 'Encoding Explanatory Knowledge for Zero-shot Science Question Answering', 'abstract': 'This paper describes N-XKT (Neural encoding based on eXplanatory Knowledge Transfer), a novel method for the automatic transfer of explanatory knowledge through neural encoding mechanisms. We demonstrate that NXKT is able to improve accuracy and generalization on science Question Answering (QA). Specifically, by leveraging facts from background explanatory knowledge corpora, the NXKT model shows a clear improvement on zero-shot QA. Furthermore, we show that NXKT can be fine-tuned on a target QA dataset, enabling faster convergence and more accurate results. A systematic analysis is conducted to quantitatively analyze the performance of the N-XKT model and the impact of different categories of knowledge on the zero-shot generalization task.', 'corpus_id': 234470046, 'score': 1}, {'doc_id': '63564022', 'title': 'Predicting Endpoint of Goal-Directed Motion in Modern Desktop Interfaces using Motion Kinematics', 'abstract': 'Researchers who study pointing facilitation have identified the ability to identify–during motion–the likely target of a user’s pointing gesture, as a necessary precursor to pointing facilitation in modern computer interfaces. To address this need, we develop and analyze how an understanding of the underlying characteristics of motion can enhance our ability to predict the target or endpoint of a goal-directed movement in graphical user interfaces. Using established laws of motion and an analysis of users’ kinematic profiles, we demonstrate that the initial 90% of motion is primarly balistic and submovements are limited to the last 10% of gesture movement. Through experimentation, we demonstrate that target constraint and the intended use of a target has either a minimal effect on the motion profile or affects the last 10% of motion. Therefore, we demonstrate that any technique that models the intial 90% of gesture motion will not be affected by target constraint or intended use. Given, these results, we develop a technique to model the initial ballistic motion to predict user endpoint by adopting principles from the minimum jerk principle. Based on this principle, we derive an equation to model the initial ballistic phase of movement in order to predict movement distance and direction. We demonstrate through experimentation that we can successfully model pointing motion to identify a region of likely targets on the computer display. Next, we characterize the effects of target size and target distance on prediction accuracy. We demonstrate that there exists a linear relationship between prediction accuracy and target distance and that this relationship can be leveraged to create a probabilistic model for each target on the computer display. We then demonstrate how these probabilities could be used to enable pointing facilitation in modern computer interfaces. Finally, we demonstrate that the results from our evaluation of our technique are supported by the current motor control literature. In addition, we show that our technique provides optimal accuracy for any optimal accuracy when prediction of motion endpoint is performed using only the ballistic components of motion and before 90% of motion distance.', 'corpus_id': 63564022, 'score': 0}, {'doc_id': '235097200', 'title': 'Enhancing Multiple-Choice Question Answering with Causal Knowledge', 'abstract': 'The task of causal question answering aims to reason about causes and effects over a provided real or hypothetical premise. Recent approaches have converged on using transformer-based language models to solve question answering tasks. However, pretrained language models often struggle when external knowledge is not present in the premise or when additional context is required to answer the question. To the best of our knowledge, no prior work has explored the efficacy of augmenting pretrained language models with external causal knowledge for multiple-choice causal question answering. In this paper, we present novel strategies for the representation of causal knowledge. Our empirical results demonstrate the efficacy of augmenting pretrained models with external causal knowledge. We show improved performance on the COPA (Choice of Plausible Alternatives) and WIQA (What If Reasoning Over Procedural Text) benchmark tasks. On the WIQA benchmark, our approach is competitive with the state-of-the-art and exceeds it within the evaluation subcategories of In-Paragraph and Out-of-Paragraph perturbations.', 'corpus_id': 235097200, 'score': 1}, {'doc_id': '235247888', 'title': 'Relational Gating for ""What If"" Reasoning', 'abstract': 'This paper addresses the challenge of learning to do procedural reasoning over text to answer ""What if..."" questions. We propose a novel relational gating network that learns to filter the key entities and relationships and learns contextual and cross representations of both procedure and question for finding the answer. Our relational gating network contains an entity gating module, relation gating module, and contextual interaction module. These modules help in solving the ""What if..."" reasoning problem. We show that modeling pairwise relationships helps to capture higher-order relations and find the line of reasoning for causes and effects in the procedural descriptions. Our proposed approach achieves the state-of-the-art results on the WIQA dataset.', 'corpus_id': 235247888, 'score': 1}, {'doc_id': '51759755', 'title': 'Kidins220/ ARMS association with Nav channels', 'abstract': 'Kidins220/ ARMS acts as a signaling platform at the plasma membrane and is implicated in a multitude of neuronal functions, including the control of neuronal activity. Here, we used the Kidins220 mouse model to study the effects of Kidins220 ablation on neuronal excitability. Multi-electrode array recordings showed reduced evoked spiking activity in Kidins220 hippocampal networks, which was compatible with the increased excitability of GABAergic neurons determined by current-clamp recordings. Spike waveform analysis further indicated an increased sodium conductance in this neuronal subpopulation. Kidins220 association with brain voltage-gated sodium channels was shown by co-immunoprecipitation experiments and Na current recordings in transfected HEK293 cells, which revealed dramatic alterations of kinetics and voltagedependence. Finally, an in silico interneuronal model incorporating the Kidins220-induced Na current alterations reproduced the firing phenotype observed in Kidins220 neurons. These results identify Kidins220 as a novel modulator of Nav channel activity, broadening our understanding of the molecular mechanisms regulating network excitability. The scaffold protein Kidins220/ ARMS (1,2), hereafter referred to as Kidins220, acts as a signaling platform implicated in many cellular functions through a plethora of interactions with membrane receptors, cytosolic signaling components and cytoskeletal proteins (3). In the nervous system, where the protein is preferentially expressed, Kidins220 covers such diverse functions as the control of neuronal survival pathways, neurite outgrowth and maturation, and finally neuronal activity. Cumulative evidence from several studies supported the idea that neuronal activity in the hippocampus is reciprocally connected to Kidins220 protein levels, which appear to affect excitatory and inhibitory circuits in opposite ways (4-7). This connection http://www.jbc.org/cgi/doi/10.1074/jbc.M115.654699 The latest version is at JBC Papers in Press. Published on June 2, 2015 as Manuscript M115.654699 Copyright 2015 by The American Society for Biochemistry and Molecular Biology, Inc. by gest on O cber 2, 2017 hp://w w w .jb.org/ D ow nladed from Kidins220/ ARMS association with Nav channels 2 holds true also in the reverse direction, as sustained neuronal activity reduces the amount of Kidins220 protein via transcriptional downregulation and calpain-dependent protein cleavage (6,8). The molecular mechanisms by which Kidins220 affects neuronal activity are only partially understood. In some instances, similarly to other multi-domain scaffold proteins, such as PSD-95 (9) or Zonula Occludens (10), an association of Kidins220 with membrane channels appears to be involved, as demonstrated by immunoprecipitation assays for AMPA and NMDA receptors (5,8). In the case of AMPA receptors, Kidins220 expression negatively regulated the basal synaptic strength of glutamatergic hippocampal synapses by affecting the phosphorylation state and cell surface expression of the GluA1 subunit (5). In other instances, the effects on neuronal activity are most likely related to the close association of Kidins220 with neurotrophin receptors. In fact, Kidins220 has been shown to interact with the three members of the Trk family and with the p75 neurotrophin receptor (11,12). Moreover, Kidins220 is tyrosinephosphorylated by Trk receptors upon activation by extracellular neurotrophins (2). Neurotrophins, and in particular brain-derived neurotrophic factor (BDNF), are known to participate in synaptic plasticity (13), and in some cases an involvement of Kidins220 in the synaptic effects of neurotrophins has been demonstrated. Kidins220 knock-down in hippocampal neurons interfered with the BDNF-evoked enhancement of inhibitory neurotransmission (7). Long-term potentiation in mouse hippocampal slices, for which a direct implication of the BDNF-TrkB system has been shown (14), was enhanced in Kidins220 mice having reduced protein levels (6). Finally, the potentiation of excitatory post-synaptic currents in hippocampal neurons induced by acute BDNF application (15) was impaired in Kidins220 mice (16). The proper functioning of brain circuits relies on the ability of the neural networks to maintain the correct balance between excitatory and inhibitory activity. Neuronal excitability is determined by a complex network of biological processes, which result from the interplay of extracellular signals, membrane receptors and channels, and intracellular signalling cascades. Being responsible for AP onset at the axonal initial segment, Nav channels are fundamental players in all kinds of neuronal communication. They are multimeric complexes of α and β subunits that exist in several isoforms, creating a large repertoire of channels with different biophysical properties (17). The specific channel localization is determined by the interaction of α and β subunits with a set of adhesion molecules, as well as cytoskeletal and extracellular matrix proteins (17,18). Nav channel activity is modulated by the coordinated activation of several signalling pathways, being targets of phosphorylation by protein kinase A (19) and by Fyn kinase acting downstream of the BDNF/TrkB complex (20). Despite the vast amount of data available in the literature, however, the network of signalling events controlling Nav channel localization and activity is still far from being completely understood. Here, we examined the effects of Kidins220 ablation on the excitability of cultured hippocampal neurons. Our results revealed increased excitability of GABAergic neurons in Kidins220 null mice and provided evidence for a functional association of Kidins220 with Nav channels in the brain. EXPERIMENTAL PROCEDURES Reagents – All chemicals, biochemical reagents and drugs were from Sigma Aldrich (Milan, Italy), unless otherwise specified. Tissue culture reagents and media were from Life Technologies (Milan, Italy) or Sigma Aldrich. Generation of Kidins220 mice and primary hippocampal cultures – The generation of the Kidins220 strain was described in a previous study (16). All embryos used in this study were obtained from crosses of Kidins220 mice in the C57BL/6 background. Mice were mated overnight and separated in the morning. The development of the embryos was timed from the detection of a vaginal plug, which was considered day 0.5. Hippocampi were dissected from wild-type and Kidins220 E18.5 embryo littermates obtained from crossing Kidins220 mice. Briefly, hippocampi were dissected in ice-cold PBS, incubated with trypsin (0.125%) for 15 min at 37°C, and mechanically dissociated. Autaptic cultures were prepared as previously described (21,22). Briefly, dissociated neurons were plated at very low density (20 cells mm) on microdots (40300 mm diameter) obtained by spraying poly-Dlysine (0.14 mg ml) on dishes pre-treated with by gest on O cber 2, 2017 hp://w w w .jb.org/ D ow nladed from Kidins220/ ARMS association with Nav channels 3 0.15% agarose. Neurons were plated in Neurobasal medium containing 10% horse serum, 2 mM glutamine and antibiotics (plating medium). After 3 h, the medium was removed and replaced with Neurobasal containing 2% B27 supplement, 2 mM glutamine and antibiotics (maintenance medium). All experiments were carried out in accordance with the guidelines established by the European Communities Council (Directive 2010/63/EU of September 22, 2010) and were approved by the Italian Ministry of Health. Cell culture and transient transfection – HEK293 and COS cells were cultured in DMEM supplemented with 10% fetal bovine serum, glutamine (2 mM) and antibiotics, in a humidified 5% CO2 atmosphere at 37°C. Plasmids coding for HA-Kidins220 (kind gift of Dr. G. Schiavo, UCL, London, UK) and Nav1.2 (kind gift of Dr. W.A. Catterall, University of Washington, Seattle, USA) were transiently co-transfected into cultured cells using Lipofectamine 2000 (Life Technologies), following standard protocols. Immunoprecipitation and immunoblot analysis – Cultured cells were washed once in ice-cold PBS and lysed in RIPA buffer (50 mM Tris-HCl pH 7.4, 150 mM NaCl, 2 mM EDTA, 1% NP40, 0.1% SDS) plus protease inhibitors (complete EDTAfree protease inhibitors, Roche Diagnostics GmbH, Mannheim, Germany) for 1 h at 4°C under constant agitation. Mouse brain tissue was extracted in the same buffer using a teflon dounce homogenizer (Wheaton, Millville, NJ, USA) and incubated as before. After centrifugation at 16000g for 15 min at 4°C, protein concentration was quantified by using a Bradford Protein Assay (BioRad, Hercules, CA, USA). Following protein extraction, protein lysates were precleared using 25 μl Protein A or G-Sepharose Fast Flow (GE Healthcare, Milan, Italy) for 2 h at 4°C. Precleared lysates were incubated 4 h with 1 μg of the appropriate antibody at 4°C, then immunocomplexes were isolated by adding Protein A or G-Sepharose overnight at 4°C. SDS-PAGE and Western blotting were performed by using BOLT 4–12% Bis-Tris Plus precast gels (Life Technologies). Nitrocellulose membranes were incubated with the appropriate primary antibodies, successively with fluorescently conjugated secondary antibodies and revealed by a Typhoon TRIO+ Variable Mode Imager (GE Healthcare). The following antibodies were used: polyclonal anti-Kidins220 (#ab34790, Abcam, Cambridge, UK), monoclonal anti-HA High Affinity (11867423001, Roche Diagnostics GmbH), monoclonal anti-sodium channel (pan) (#S8809, Sigma Aldrich), polyclonal anti-Nav1.2 (#ASC002, Alomone Labs Ltd., Jerusalem, Israel), polyclonal anti-GFP (#A11122, Life Technologies), monoclonal anti-GFP (#ab1218, Abcam). When used on transfected HEK293 or COS cells, in our hands, the pan-Nav antibody did not recognize Nav1.2 in Western blots, while it recognized Nav1.2 very well in immunocytochemical assays (Fig. 3C,D). This could possibly be due to an alteration of the panNav specific epitope in the HEK-expressed Nav1.2 subunit upon SDS', 'corpus_id': 51759755, 'score': 0}]"
74	Crash Prediction	0c2bbca6de321e06aa209f92a8f1c8be	20162	{}	"[{'doc_id': '233753065', 'title': 'Macroscopic Equity Markets Model: Towards Predicting Financial Crashes', 'abstract': 'This research examines the structural properties of the macroscopic model introduced in [AlShelahi and Saigal, 2018]. We present a theoretical analysis of the behavior of the macroscopic variables. In particular, we show that the model exhibits shock-like solutions, providing a new narrative for financial shocks. To solve the system of stochastic nonlinear partial differential equations adaptively, an integrative algorithm is devised and tested on abnormal and normal trading days. The results suggest that abnormalities can be identified before crashes. Our findings warrant further investigation into the macroscopic structure of equity markets.', 'corpus_id': 233753065, 'score': 1}, {'doc_id': '237285849', 'title': 'A SURVEY ON STOCK PRICE PREDICTION USING MACHINE LEARNING', 'abstract': 'Stock returns are very fluctuating in nature. They rely upon various factors like previous stock prices, current market trends, financial news, etc. To feature their annual income, people have now started watching stock investments as a remunerative option. There are many tools available to investors using technical analysis to form decisions. With expert guidance and intelligent planning, we will almost double our annual income through stock returns. These days, social media has become a mirror. It reflects people’s thoughts and opinions on any particular event or news. Sentiments of the general public associated with an organization can have an upshot on its stock prices. This paper surveys various machine learning techniques and algorithms employed to boost the accuracy of stock price prediction. Keywords— Stock prices, Technical analysis, Stock Returns, Social Media, Sentiments', 'corpus_id': 237285849, 'score': 0}, {'doc_id': '237252600', 'title': 'On the Construction of a Positive Sentiment Index for COVID-19: Evidence from G20 Stock Markets', 'abstract': 'The present study investigates the degree of market responses through the scope of investors’ sentiment during the COVID-19 pandemic across G20 markets, by constructing a novel positive search volume index for COVID-19 (COVID19+). Our key findings, obtained using a Panel-GARCH model, indicate that an increased COVID19+ index suggests that investors decrease their COVID-19 related crisis sentiment by escalating their Google searches for positively associated COVID-19 related keywords. Specifically, we explore the predictive power of the newly constructed index on stock returns and volatility. According to our findings, investor sentiment positively (negatively) predicts the stock return (volatility) during the COVID-19. This is the first study of its kind assessing global sentiment by proposing a novel proxy and its impacts on the G20 equity market.', 'corpus_id': 237252600, 'score': 0}, {'doc_id': '221283286', 'title': 'Predicting Stock Market Trends Using Machine Learning and Deep Learning Algorithms Via Continuous and Binary Data; a Comparative Analysis', 'abstract': 'The nature of stock market movement has always been ambiguous for investors because of various influential factors. This study aims to significantly reduce the risk of trend prediction with machine learning and deep learning algorithms. Four stock market groups, namely diversified financials, petroleum, non-metallic minerals and basic metals from Tehran stock exchange, are chosen for experimental evaluations. This study compares nine machine learning models (Decision Tree, Random Forest, Adaptive Boosting (Adaboost), eXtreme Gradient Boosting (XGBoost), Support Vector Classifier (SVC), Naïve Bayes, K-Nearest Neighbors (KNN), Logistic Regression and Artificial Neural Network (ANN)) and two powerful deep learning methods (Recurrent Neural Network (RNN) and Long short-term memory (LSTM). Ten technical indicators from ten years of historical data are our input values, and two ways are supposed for employing them. Firstly, calculating the indicators by stock trading values as continuous data, and secondly converting indicators to binary data before using. Each prediction model is evaluated by three metrics based on the input ways. The evaluation results indicate that for the continuous data, RNN and LSTM outperform other prediction models with a considerable difference. Also, results show that in the binary data evaluation, those deep learning methods are the best; however, the difference becomes less because of the noticeable improvement of models’ performance in the second way.', 'corpus_id': 221283286, 'score': 1}, {'doc_id': '236687029', 'title': 'Do the Stock Market Indices Follow a Random Walk?', 'abstract': 'This chapter aims to test the hypothesis of an efficient market, in its weak form, in the stock markets of Brazil, China, South Korea, USA, Spain, Italy, in the period from December 2, 2020 to May 12, 2020. The results show that the market efficiency hypothesis is rejected in all markets. In corroboration the DFA exponents show long memories, which put in question the market efficiency, in its weak form, suggesting that the stock markets analyzed show some predictability. In conclusion, investors should avoid investing in stock markets, at least while this pandemic lasts, and invest in less risky markets in order to mitigate risk and improve the efficiency of their portfolios.', 'corpus_id': 236687029, 'score': 0}, {'doc_id': '150705099', 'title': 'Why Stock Markets Crash: Critical Events in Complex Financial Systems', 'abstract': 'Why Stock Markets Crash: Critical Events in Complex Financial Systems, by Didier Sornette, 2003, Princeton, NJ: Princeton University Press Consider the following events: a pressure tank within a rocket propulsion system fails during a launch; tectonic plates shift, causing the first significant earthquake in a locale for several decades; a stock market experiences a crash after a prolonged run-up in price levels. The commonality here is that all of these events are ultimately characterized by a ""rupture"" in the underlying system, following a buildup of ""pressure"" over a period of time. The recognition of certain engineering and geologic events as analogous in this way to financial market crashes was the impetus for the interesting and enjoyable new book Why Stock Markets Crash: Critical Events in Complex Financial Systems, by Didier Sornette. The major thesis of this book is that a stock market crash is not the result of short-term exogenous events, but rather involves a long-term endogenous buildup, with exogenous events acting merely as triggers. In particular, Sornette examines financial crashes within the framework of the ""spontaneous emergence of extreme events in self-organizing systems,"" noting that ""extreme events are characteristic of many... \'complex systems.\'"" The author employs mathematical tools-specifically, log-periodic power laws-to study the prebubble or precrash buildup in a financial system to its critical point. Efforts by nonfinancial people to analyze and explain financial phenomena using quantitative techniques from the hard and engineering sciences can be of tremendous use and interest to those of us in the financial community-provided that the mathematical techniques are applied by an author with an exposure to and understanding of the financial instruments, processes, and markets that are being analyzed. The author of Why Stock Markets Crash has done an admirable job of understanding and appreciating the financial world and its nuances. Didier Sornette is a professor of geophysics at UCLA, as well as a research director at the National Center of Scientific Research in France. He specializes in the prediction of catastrophic events within a complex system framework. In this book, as well as in a portion of his hundreds of journal articles, he takes his previous work in the physical and geological sciences and exports his mathematical modeling and prediction skills to the financial markets. In the first chapter, Sornette places historical extreme financial events-in particular, market crashes-in a complex, self-organizing system framework. This is followed by two chapters devoted, respectively, to the basic concepts and characteristics of financial markets, and to some statistical analyses demonstrating that financial crashes are essentially outliers. …', 'corpus_id': 150705099, 'score': 1}, {'doc_id': '215829436', 'title': 'Stock market prediction using machine learning classifiers and social media, news', 'abstract': 'Accurate stock market prediction is of great interest to investors; however, stock markets are driven by volatile factors such as microblogs and news that make it hard to predict stock market index based on merely the historical data. The enormous stock market volatility emphasizes the need to effectively assess the role of external factors in stock prediction. Stock markets can be predicted using machine learning algorithms on information contained in social media and financial news, as this data can change investors’ behavior. In this paper, we use algorithms on social media and financial news data to discover the impact of this data on stock market prediction accuracy for ten subsequent days. For improving performance and quality of predictions, feature selection and spam tweets reduction are performed on the data sets. Moreover, we perform experiments to find such stock markets that are difficult to predict and those that are more influenced by social media and financial news. We compare results of different algorithms to find a consistent classifier. Finally, for achieving maximum prediction accuracy, deep learning is used and some classifiers are ensembled. Our experimental results show that highest prediction accuracies of 80.53% and 75.16% are achieved using social media and financial news, respectively. We also show that New York and Red Hat stock markets are hard to predict, New York and IBM stocks are more influenced by social media, while London and Microsoft stocks by financial news. Random forest classifier is found to be consistent and highest accuracy of 83.22% is achieved by its ensemble.', 'corpus_id': 215829436, 'score': 1}, {'doc_id': '201107067', 'title': 'The Emergence of Critical Stocks in Market Crash', 'abstract': 'In complex systems like financial market, risk tolerance of individuals is crucial for system resilience. The single-security price limit, designed as risk tolerance to protect investors by avoiding sharp price fluctuation, is blamed for feeding market panic in times of crash. The relationship between the critical market confidence which stabilizes the whole system and the price limit is therefore an important aspect of system resilience. Using a simplified dynamic model on networks of investors and stocks, an unexpected linear association between price limit and critical market confidence is theoretically derived and empirically verified in this paper. Our results highlight the importance of relatively “small” but critical stocks that drive the system to collapse by passing the failure from periphery to core. These small stocks, largely originating from homogeneous investment strategies across the market, has unintentionally suppressed system resilience with the exclusive increment of individual risk tolerance. Imposing random investment requirements to mitigate herding behavior can thus improve the market resilience.', 'corpus_id': 201107067, 'score': 1}, {'doc_id': '237447213', 'title': 'Stock Price Prediction Based on LSTM Deep Learning Model', 'abstract': 'Predicting the stock market is either the easiest or the toughest task in the field of computations. There are many factors related to prediction, physical factors vs. physiological, rational and irrational , capitalist sentiment, market , etc. All these aspects combine to make stock costs volatile and are extremely tough to predict with high accuracy. The prices of a stock market depend very much on demand and supply. High demand stocks will increase in price while heavy selling stocks will decrease. Fluctuations in stock prices affect investor perception and thus there is a need to predict future share prices and to predict stock market prices to make more acquaint and precise investment decisions. We examine data analysis in this domain as a game-changer. This paper proposes that historical value bears the impact of all other market events and can be used to predict future movement. Machine Learning techniques can detect paradigms and insights that can be used to construct surprisingly correct predictions. We propose the LSTM (Long Short Term Memory) model to examine the future price of a stock. This paper is to predict stock market prices to make more acquaint and precise investment decisions.', 'corpus_id': 237447213, 'score': 0}, {'doc_id': '236159272', 'title': 'A Long Short-Term Memory Network Stock Price Prediction with Leading Indicators.', 'abstract': 'The accuracy of the prediction of stock price fluctuations is crucial for investors, and it helps investors manage funds better when formulating trading strategies. Using forecasting tools to get a predicted value that is closer to the actual value from a given financial data set has always been a major goal of financial researchers and a problem. In recent years, people have paid particular attention to stocks, and gradually used various tools to predict stock prices. There is more than one factor that affects financial trends, and people need to consider it from all aspects, so research on stock price fluctuations has also become extremely difficult. This paper mainly studies the impact of leading indicators on the stock market. The framework used in this article is proposed based on long short-term memory (LSTM). In this study, leading indicators that affect stock market volatility are added, and the proposed framework is thus named as a stock tending prediction framework based on LSTM with leading indicators (LSTMLI). This study uses stock markets in the United States and Taiwan, respectively, with historical data, futures, and options as data sets to predict stock prices in these two markets. We measure the predictive performance of LSTMLI relative to other neural network models, and the impact of leading indicators on stock prices is studied. Besides, when using LSTMLI to predict the rise and fall of stock prices in the article, the conventional regression method is not used, but the classification method is used, which can give a qualitative output based on the data set. The experimental results show that the LSTMLI model using the classification method can effectively reduce the prediction error. Also, the data set with leading indicators is better than the prediction results of the single historical data using the LSTMLI model.', 'corpus_id': 236159272, 'score': 0}]"
75	Investing	3ba5906c43ffde7fc50626c0dd79d562	5117	{}	"[{'doc_id': '212725640', 'title': 'Deep Deterministic Portfolio Optimization', 'abstract': 'Can deep reinforcement learning algorithms be exploited as solvers for optimal trading strategies? The aim of this work is to test reinforcement learning algorithms on conceptually simple, but mathematically non-trivial, trading environments. The environments are chosen such that an optimal or close-to-optimal trading strategy is known. We study the deep deterministic policy gradient algorithm and show that such a reinforcement learning agent can successfully recover the essential features of the optimal trading strategies and achieve close-to-optimal rewards.', 'corpus_id': 212725640, 'score': 1}, {'doc_id': '227151560', 'title': 'Remaining Useful Life Estimation Under Uncertainty with Causal GraphNets', 'abstract': 'In this work, a novel approach for the construction and training of time series models is presented that deals with the problem of learning on large time series with non-equispaced observations, which at the same time may possess features of interest that span multiple scales. The proposed method is appropriate for constructing predictive models for non-stationary stochastic time series.The efficacy of the method is demonstrated on a simulated stochastic degradation dataset and on a real-world accelerated life testing dataset for ball-bearings. The proposed method, which is based on GraphNets, implicitly learns a model that describes the evolution of the system at the level of a state-vector rather than of a raw observation. The proposed approach is compared to a recurrent network with a temporal convolutional feature extractor head (RNN-tCNN) which forms a known viable alternative for the problem context considered. Finally, by taking advantage of recent advances in the computation of reparametrization gradients for learning probability distributions, a simple yet effective technique for representing prediction uncertainty as a Gamma distribution over remaining useful life predictions is employed.', 'corpus_id': 227151560, 'score': 0}, {'doc_id': '218684766', 'title': 'Forward Utilities and Mean-Field Games Under Relative Performance Concerns', 'abstract': 'We introduce the concept of mean field games for agents using Forward utilities of CARA type to study a family of portfolio management problems under relative performance concerns. Under asset specialization of the fund managers, we solve the forward-utility finite player game and the forward-utility mean-field game. We study best response and equilibrium strategies in the single common stock asset and the asset specialization with common noise. As an application, we draw on the core features of the forward utility paradigm and discuss a problem of time-consistent mean-field dynamic model selection in sequential time-horizons.', 'corpus_id': 218684766, 'score': 1}, {'doc_id': '125681903', 'title': 'A new approach for worst-case regret portfolio optimization problem', 'abstract': 'This paper considers the worst-case regret portfolio optimization problem when the distributions of the asset returns are uncertain. In general, the solution to this problem is NP hard and approximation methods that minimise the difference between the maximum return and the sum of each portfolio return are often proposed. Applying the duality of semi-infinite programming, the worst-case regret portfolio optimization problem with uncertain distributions can be equivalently reformulated to a linear optimization problem, and the established solution approaches for linear optimization can then be applied. An example of a portfolio optimization problem is provided to show the efficiency of our method and the results demonstrate that our method can satisfy the portfolio risk diversification property under the uncertain distributions of the returns.', 'corpus_id': 125681903, 'score': 1}, {'doc_id': '221112580', 'title': 'Convergence of Deep Fictitious Play for Stochastic Differential Games', 'abstract': ""Stochastic differential games have been used extensively to model agents' competitions in Finance, for instance, in P2P lending platforms from the Fintech industry, the banking system for systemic risk, and insurance markets. The recently proposed machine learning algorithm, deep fictitious play, provides a novel efficient tool for finding Markovian Nash equilibrium of large $N$-player asymmetric stochastic differential games [J. Han and R. Hu, Mathematical and Scientific Machine Learning Conference, 2020]. By incorporating the idea of fictitious play, the algorithm decouples the game into $N$ sub-optimization problems, and identifies each player's optimal strategy with the deep backward stochastic differential equation (BSDE) method parallelly and repeatedly. In this paper, under appropriate conditions, we prove the convergence of deep fictitious play (DFP) to the true Nash equilibrium. We can also show that the strategy based on DFP forms an $\\epsilon$-Nash equilibrium. We generalize the algorithm by proposing a new approach to decouple the games, and present numerical results of large population games showing the empirical convergence of the algorithm beyond the technical assumptions in the theorems."", 'corpus_id': 221112580, 'score': 1}, {'doc_id': '226299995', 'title': 'Discrete solution pools and noise-contrastive estimation for predict-and-optimize', 'abstract': 'Numerous real-life decision-making processes involve solving a combinatorial optimization problem with uncertain input that can be estimated from historic data. There is a growing interest in decision-focused learning methods, where the loss function used for learning to predict the uncertain input uses the outcome of solving the combinatorial problem over a set of predictions. Different surrogate loss functions have been identified, often using a continuous approximation of the combinatorial problem. However, a key bottleneck is that to compute the loss, one has to solve the combinatorial optimisation problem for each training instance in each epoch, which is computationally expensive even in the case of continuous approximations. \nWe propose a different solver-agnostic method for decision-focused learning, namely by considering a pool of feasible solutions as a discrete approximation of the full combinatorial problem. Solving is now trivial through a single pass over the solution pool. We design several variants of a noise-contrastive loss over the solution pool, which we substantiate theoretically and empirically. Furthermore, we show that by dynamically re-solving only a fraction of the training instances each epoch, our method performs on par with the state of the art, whilst drastically reducing the time spent solving, hence increasing the feasibility of predict-and-optimize for larger problems.', 'corpus_id': 226299995, 'score': 0}, {'doc_id': '227013048', 'title': 'Visual Forecasting of Time Series with Image-to-Image Regression', 'abstract': 'Time series forecasting is essential for agents to make decisions in many domains. Existing models rely on classical statistical methods to predict future values based on previously observed numerical information. Yet, practitioners often rely on visualizations such as charts and plots to reason about their predictions. Inspired by the end-users, we re-imagine the topic by creating a framework to produce visual forecasts, similar to the way humans intuitively do. In this work, we take a novel approach by leveraging advances in deep learning to extend the field of time series forecasting to a visual setting. We do this by transforming the numerical analysis problem into the computer vision domain. Using visualizations of time series data as input, we train a convolutional autoencoder to produce corresponding visual forecasts. We examine various synthetic and real datasets with diverse degrees of complexity. Our experiments show that visual forecasting is effective for cyclic data but somewhat less for irregular data such as stock price. Importantly, we find the proposed visual forecasting method to outperform numerical baselines. We attribute the success of the visual forecasting approach to the fact that we convert the continuous numerical regression problem into a discrete domain with quantization of the continuous target signal into pixel space.', 'corpus_id': 227013048, 'score': 0}, {'doc_id': '224814213', 'title': 'Logistic $Q$-Learning', 'abstract': 'We propose a new reinforcement learning algorithm derived from a regularized linear-programming formulation of optimal control in MDPs. The method is closely related to the classic Relative Entropy Policy Search (REPS) algorithm of Peters et al. (2010), with the key difference that our method introduces a Q-function that enables efficient exact model-free implementation. The main feature of our algorithm (called QREPS) is a convex loss function for policy evaluation that serves as a theoretically sound alternative to the widely used squared Bellman error. We provide a practical saddle-point optimization method for minimizing this loss function and provide an error-propagation analysis that relates the quality of the individual updates to the performance of the output policy. Finally, we demonstrate the effectiveness of our method on a range of benchmark problems.', 'corpus_id': 224814213, 'score': 0}, {'doc_id': '221266581', 'title': 'Kernel-Based Graph Learning From Smooth Signals: A Functional Viewpoint', 'abstract': 'The problem of graph learning concerns the construction of an explicit topological structure revealing the relationship between nodes representing data entities, which plays an increasingly important role in the success of many graph-based representations and algorithms in the field of machine learning and graph signal processing. In this paper, we propose a novel graph learning framework that incorporates prior information along node and observation side, and in particular the covariates that help to explain the dependency structures in graph signals. To this end, we consider graph signals as functions in the reproducing kernel Hilbert space associated with a Kronecker product kernel, and integrate functional learning with smoothness-promoting graph learning to learn a graph representing the relationship between nodes. The functional learning increases the robustness of graph learning against missing and incomplete information in the graph signals. In addition, we develop a novel graph-based regularisation method which, when combined with the Kronecker product kernel, enables our model to capture both the dependency explained by the graph and the dependency due to graph signals observed under different but related circumstances, e.g. different points in time. The latter means the graph signals are free from the i.i.d. assumptions required by the classical graph learning models. Experiments on both synthetic and real-world data show that our methods outperform the state-of-the-art models in learning a meaningful graph topology from graph signals, in particular with heavy noise, missing values, and multiple dependency.', 'corpus_id': 221266581, 'score': 1}, {'doc_id': '227126670', 'title': 'Solving path dependent PDEs with LSTM networks and path signatures', 'abstract': 'Using a combination of recurrent neural networks and signature methods from the rough paths theory we design efficient algorithms for solving parametric families of path dependent partial differential equations (PPDEs) that arise in pricing and hedging of path-dependent derivatives or from use of non-Markovian model, such as rough volatility models in Jacquier and Oumgari, 2019. The solutions of PPDEs are functions of time, a continuous path (the asset price history) and model parameters. As the domain of the solution is infinite dimensional many recently developed deep learning techniques for solving PDEs do not apply. Similarly as in Vidales et al. 2018, we identify the objective function used to learn the PPDE by using martingale representation theorem. As a result we can de-bias and provide confidence intervals for then neural network-based algorithm. We validate our algorithm using classical models for pricing lookback and auto-callable options and report errors for approximating both prices and hedging strategies.', 'corpus_id': 227126670, 'score': 0}]"
76	Kant	42053bf131abdcd37dda6a8e4777b7bb	18159	{}	"[{'doc_id': '233454682', 'title': 'Synthetic a Priori in the Second Critique', 'abstract': 'The Critique of Pure Reason has two primary segmentations: Transcendental Doctrine of Elements and Transcendental Doctrine of Method. The letter section is comparatively smaller and in Norman Kemp Smith‟s translation it comes to some 96 pages 1 . This segment is divided into four chapters. Out of these four chapters first and second chapters have four and three sections respectively. The last two chapters, i. e. third and fourth, do not have sub-divisions. In the second chapter (entitled „The Canon of Pure Reason‟), Section 2 (under the rubric „The Ideal of the Highest Good, as a Determining Ground of the Ultimate End of Pure Reason‟), Kant sums up his interest in reason in three brief questions: 1. What can I know? 2. What I ought to do? 3. What may I hope? Out of these three questions, the second one introduces his task in the field of moral philosophy. The first question, as Kant himself said, is purely speculative, the second one is purely practical and the third one is a combination of speculative and practical. Though the second question falls within the purview of pure reason still it is not a transcendental question, rather a moral issue. 3 In view of this Kant did not discuss the issue in the 1 st', 'corpus_id': 233454682, 'score': 1}, {'doc_id': '235222956', 'title': 'REMARKS ON THE RIDDLE-CHARACTER OF ART AND METAPHYSICAL EXPERIENCE // Observaciones sobre el carácter enigmático del arte y la experiencia metafísica', 'abstract': 'This essay attempts through extended textual exegesis to clarify two fundamental concepts in late Adorno’s metaphysical and philosophical-aesthetic writings respectively: “metaphysical experience” and the “riddle-character” of art. The first part of the essay explores how in Negative Dialectics Adorno reworks certain Kantian themes to provide an account of metaphysical experience as the exercise of the mind’s capacity for thinking beyond the immanently given that brings happiness despite failing to attain the absolute. The second part of the essay interprets concepts from Aesthetic Theory and related writings, including the internal/external, performative/cognitive perspectives on the artwork, and its similarity to language, to show how the riddle-character of art elicits an experience similar to metaphysical experience, in that it demonstrates the ability of mind to reach beyond its cognitive selflimitations into a response-dependent objectivity with utopian implications.', 'corpus_id': 235222956, 'score': 0}, {'doc_id': '235064885', 'title': 'Co to znaczy być transcendentalnym idealistą i empirycznym realistą jednocześnie? O statusie rzeczy w filozofii transcendentalnej.', 'abstract': 'The aim of the article is to present an interpretation of the Kantian concept of transcendental idealism, which would make it possible to understand the status of things from the perspective of transcendental philosophy. The main claim of the article is that Kant’s standpoint can be situated beyond metaphysical realism and idealism, or, to use contemporary terms, beyond representationalism and constructivism. The standpoint in question can thus be regarded as an inspiration to reject the Cartesian dualism of substance and to propose a new philosophical perspective. In addition, the author claims that the understanding of transcendental philosophy presented in the article has provided a basis for the new, pluralist and relationalist ontology advanced by Nicolai Hartmann. She also suggests considering this ontology as another, after neo-Kantianism, stage in the development of German transcendental philosophy.', 'corpus_id': 235064885, 'score': 0}, {'doc_id': '234352431', 'title': 'The Nationality of Sublimity: Kant and Burke on the Intuition and Representation of Infinity', 'abstract': 'There can be little que~tion that both Kant\'s and Burke\'s theories of the sublime directly affected the way contemporaries saw and reprp.sented nature.1 What has not been sufficiently emphasized. however, is the distinctive natiunality of the sublime the extent to which Kant changed Burke\'s ideas from an identifiably German cultural perspective, and how German landscape painters inspired by Kant opened up radically new possibilities for the depiction of the sublime by executing canvases that explicitly (if not consdously) contradicted Burke\'s definition. My argument has three steps: in the first. I examine Kant\'s revision of Burke by comparing their ideas on what I see as the paradigm of the sublime. the intuition of infinity. In the second. I turn to the strong contrast in the (physical rather than mental) representations of infinity that result from their respective theories. Finally, I will suggest how the interaction of science with theories of landscape stimulated the unique depictions of the sublime that we find in German landscape painting c. )800. depictions that are significantly different in appearance and conception from their British and French2 counterparts. By putting Kant\'s theories in their cultural context. I seek to make the additional point that our ~veryday distinction between ~\'theory"" and ""practice"" needs to be retqought. Kant\'s Critique of Judgement is normally held to be ""theoretical."" to have little to do ""practically"" with the arts. Landscape painting is thought to have little to do with ""theory:\' My claim. however, is that because Kant\'s theory of the sublime so informed German l:mdscape painting. and because the', 'corpus_id': 234352431, 'score': 0}, {'doc_id': '235362855', 'title': 'Three Kantian Accounts of Concept Formation', 'abstract': 'Abstract This article has two aims. First, I offer a philological analysis of a key passage from Kant’s Logic: § 6. § 6 is widely regarded as the locus classicus for Kant’s theory of concept formation. However, I show that the part of this section that is most cited and discussed by scholars should not be attributed to Kant, as it is not corroborated by any of his Reflexionen. Second, I attempt to identify Jäsche’s source for this unsupported passage. Ultimately, I conclude that the unsupported passage in § 6 was based on a set of student notes that was similar to the Wiener and Warschauer logic notebooks. However, I also argue that it would be a mistake to regard this passage as the final or definitive statement of Kant’s views about concept formation.', 'corpus_id': 235362855, 'score': 1}, {'doc_id': '235188825', 'title': 'Orr and Kant: An analysis of the intellectual encounter behind ‘The Christian worldview’', 'abstract': ""Abstract Today, Christianity is often described as a ‘worldview’, especially among Reformed evangelicals in the USA. In this article I return to the 1890 lectures where Scottish theologian James Orr adapted the concept of Weltanschauung for Christian purposes. Although it was coined by Immanuel Kant in 1790, and primarily used in subsequent decades to theorise cultural difference and evaluate aesthetic expression, Orr nevertheless claims that the idea of a worldview is ‘as old as the dawn of reflection’ and thus appropriate to articulating Christianity. I examine Orr's engagement with the Kantian and emerging historicist context, paying particular attention to his epistemological and aesthetic citations and showing how Orr both adopts and departs from the characteristic features of the Kantian subject. I conclude by assessing the philosophical and theological costs of this project that, among other things, positions Christianity for perpetual culture war within secular societies similarly shaped by the post-Kantian subject."", 'corpus_id': 235188825, 'score': 0}, {'doc_id': '141821157', 'title': 'Kant and the Capacity to Judge: Sensibility and Discursivity in the Transcendental Analytic of the Critique of Pure Reason', 'abstract': 'Acknowledgments Ch. 1 Synthesis and Judgment Ch. 2 The ""Threefold Synthesis"" and the Mathematical Model Ch. 3 The Transition to Judgment Ch. 4 Logical Definitions of Judgment Ch. 5 How Discursive Understanding Comes to the Sensible Given: Comparison of Representations and Judgment Ch. 6 Concepts of Comparison, Forms of Judgment, Concept Formation Ch. 7 Judgments of Perception and Judgments of Experience Ch. 8 Synthesis Speciosa and Forms of Sensibility Ch. 9 The Primacy of Quantitative Syntheses Ch. 10 The Real as Appearance: Imagination and Sensation Ch. 11 The Constitution of Experience Conclusion: The Capacity to Judge and ""Ontology as Immanent Thinking"" Bibliography Index Index of Citations of Kant\'s Works', 'corpus_id': 141821157, 'score': 1}, {'doc_id': '235352249', 'title': 'Kant and the End of Wonder', 'abstract': 'In the conclusion to his Critique of Practical Reason (1788), Kant famously writes: “Two things fill the mind with ever new and increasing wonder [Bewunderung]1 and reverence, the more often and more steadily one reflects on them: the starry heavens above me and the moral law within me. I do not need to search for them and merely conjecture them as though they were veiled in obscurity or in the transcendent region beyond my horizon; I see them before me and connect them immediately with the consciousness of my existence” (Critique of Practical Reason, 5: 1612). In accordance with this revealing passage from the end of Kant’s Critique of Practical Reason, we can see virtually the whole of Kant’s philosophy as flowing from his wonder at either the starry heavens above or the moral law within. Not only does his philosophy begin in wonder, however, Kant also ends his discussions of the starry heavens and the moral law with a kind of wonder. Unlike philosophers who begin in', 'corpus_id': 235352249, 'score': 0}, {'doc_id': '235362944', 'title': 'Infinite Regress: Wolff’s Cosmology and the Background of Kant’s Antinomies', 'abstract': 'Abstract Wolff’s relation to Leibniz and Kant’s relation to both are notoriously vexed questions. First, this paper argues that Wolff’s most serious departure from Leibniz consists in his (so far overlooked) rejection of the latter’s infinitism. Second, it contends that the controversies that surrounded Wolff’s early acceptance of infinite causal regress and prompted his conversion to finitism played a prominent role in shaping the theses of Kant’s Antinomies. Whereas Leibniz and the early Wolff considered infinite regress to provide support for the contingency of the world and the existence of God, Wolff’s enemies condemned it as Spinozistic. After Wolff, the claim that an infinite chain of causes is impossible became the standard view among both Wolffian and anti-Wolffian metaphysicians.', 'corpus_id': 235362944, 'score': 1}, {'doc_id': '235362896', 'title': 'Kant on the Formation of Empirical Concepts', 'abstract': 'Abstract According to Kant’s lectures on logic, the formation of empirical concepts consists in the logical acts of comparison, reflection, and abstraction. This paper defends the tenability of Kant’s account by solving two prominent difficulties identified by commentators. Firstly, I justify Kant’s chronological presentation of the three acts by clarifying two meanings of ‘comparison’ in his writings: while comparison-1 refers to apprehension in relation to apperception and precedes reflection, comparison-2 refers to a twofold operation comprising both comparison-1 and reflection, such that its completion presupposes reflection. Secondly, to unravel an alleged ‘circularity’ in Kant’s account, I propose multiple interactions between comparison-1, which can be entirely arbitrary, and reflection, which examines the compared representations according to the imagination’s free agreement with the understanding, namely, a lawfulness without law. By means of such interactions, we experiment back and forth and lawfully generate an empirical concept without relying on conceptual guidance.', 'corpus_id': 235362896, 'score': 1}]"
77	Neural Cryptanalysis	eb1235a4a4350895f13c40b0ce3fe222	12165	{}	[{'doc_id': '226306899', 'title': 'EM-X-DL: Efficient Cross-Device Deep Learning Side-Channel Attack with Noisy EM Signatures', 'abstract': '\n This work presents a\n Cross-device Deep-Learning based Electromagnetic (EM-X-DL) side-channel analysis (SCA)\n on AES-128, in the presence of a significantly lower\n signal-to-noise ratio (SNR)\n compared to previous works. Using a novel algorithm to intelligently select multiple training devices and proper choice of hyperparameters, the proposed 256-class\n deep neural network (DNN)\n can be trained efficiently utilizing pre-processing techniques like PCA, LDA, and FFT on measurements from the target encryption engine running on an 8-bit Atmel microcontroller. In this way, EM-X-DL achieves >90% single-trace attack accuracy. Finally, an efficient end-to-end SCA leakage detection and attack framework using EM-X-DL demonstrates high confidence of an attacker with <20 averaged EM traces.\n', 'corpus_id': 226306899, 'score': 0}, {'doc_id': '223590495', 'title': 'TranSCA: Cross-Family Profiled Side-Channel Attacks using Transfer Learning on Deep Neural Networks', 'abstract': 'Side-channel analysis (SCA) utilizing the power consumption of a device has proved to be an efficient technique for recovering secret keys exploiting the implementation vulnerability of mathematically secure cryptographic algorithms. Recently, Deep Learning-based profiled SCA (DL-SCA) has gained popularity, where an adversary trains a deep learning model using profiled traces obtained from a dummy device (a device that is similar to the target device) and uses the trained model to retrieve the secret key from the target device. However, for efficient key recovery from the target device, training of such a model requires a large number of profiled traces from the dummy device and extensive training time. In this paper, we propose TranSCA, a new DL-SCA strategy that tries to address the issue. TranSCA works in three steps – an adversary (1) performs a one-time training of a base model using profiled traces from any device, (2) fine-tunes the parameters of the base model using significantly less profiled traces from a dummy device with the aid of transfer learning strategy in lesser time than training from scratch, and (3) uses the fine-tuned model to attack the target device. We validate TranSCA on simulated power traces created to represent different FPGA families. Experimental results show that the transfer learning strategy makes it possible to attack a new device from the knowledge of another device even if the new device belongs to a different family. Also, TranSCA requires very few power traces from the dummy device compared to when applying DL-SCA without any previous knowledge.', 'corpus_id': 223590495, 'score': 0}, {'doc_id': '222291557', 'title': 'Improved Fault Analysis on SIMECK Ciphers', 'abstract': 'The advances of the Internet of Things (IoT) have had a fundamental impact and influence in sharping our rich living experiences. However, since IoT devices are usually resource-constrained, lightweight block ciphers have played a major role in serving as a building block for secure IoT protocols. In CHES 2015, SIMECK, a family of block ciphers, was designed for resource-constrained IoT devices. Since its publication, there have been many analyses on its security. In this paper, under the one bit-flip model, we propose a new efficient fault analysis attack on SIMECK ciphers. Compared to those previously reported attacks, our attack can recover the full master key by injecting faults into only a single round of all SIMECK family members. This property is crucial, as it is infeasible for an attacker to inject faults into different rounds of a SIMECK implementation on IoT devices in the real world. Specifically, our attack is characterized by exercising a deep analysis of differential trail between the correct and faulty immediate ciphertexts. Extensive simulation evaluations are conducted, and the results demonstrate the effectiveness and correctness of our proposed attack.', 'corpus_id': 222291557, 'score': 0}, {'doc_id': '229181076', 'title': 'Quantum key recovery attack on SIMON32/64', 'abstract': 'The quantum security of lightweight block ciphers is receiving more and more attention. However, the existing quantum attacks on lightweight block ciphers only focused on the quantum exhaustive search, while the quantum attacks combined with classical cryptanalysis methods haven’t been well studied. In this paper, we study quantum key recovery attack on SIMON32/64 using Quantum Amplitude Amplification algorithm in Q1 model. At first, we reanalyze the quantum circuit complexity of quantum exhaustive search on SIMON32/64. We estimate the Clifford gates count more accurately and reduce the T gate count. Also, the T-depth and full depth is reduced due to our minor modifications. Then, using four differentials given by Biryukov in FSE 2014 as our distinguisher, we give our quantum key recovery attack on 19-round SIMON32/64. We treat the two phases of key recovery attack as two QAA instances separately, and the first QAA instance consists of four sub-QAA instances. Then, we design the quantum circuit of these two QAA instances and estimate their corresponding quantum circuit complexity. We conclude that the quantum circuit of our quantum key recovery attack is lower than quantum exhaustive search. Our work firstly studies the quantum dedicated attack on SIMON32/64. And this is the first work to study the complexity of quantum dedicated attacks from the perspective of quantum circuit complexity, which is a more fine-grained analysis of quantum dedicated attacks’ complexity.', 'corpus_id': 229181076, 'score': 0}, {'doc_id': '211268704', 'title': 'Recent Advances of Neural Attacks against Block Ciphers', 'abstract': 'Neural cryptanalysis is the utilization of deep learning to attack cryptographic primitives. As computing power increases, deploying neural cryptanalysis becomes a more feasible option to attack more complex ciphers. After reviewing all recent neural cryptanalysis publications on the security of block ciphers including the detailed outcome of the attacks, we find that the types of neural cryptanalysis on block ciphers can be classified into key recovery, cipher emulation, and identification attacks. We evaluate whether the publications have used correct methodologies to analyze the attack results or not, discuss limitations of current neural cryptanalysis results, and suggest future direction of development of this field.', 'corpus_id': 211268704, 'score': 1}, {'doc_id': '227339100', 'title': 'Hybrid Chaotic Method for Medical Images Ciphering', 'abstract': 'Healthcare is an essential application of e-services, where for diagnostic testing, medical imaging acquiring, processing, analysis, storage, and protection are used. Image ciphering during storage and transmission over the networks used has seen implemented using many types of ciphering algorithms for security purpose. Current cyphering algorithms are classified into two types: traditional classical cryptography using standard algorithms (DES, AES, IDEA, RC5, RSA, ...) and chaos cryptography using continuous (Chau, Rossler, Lorenz, ...) or discreet (Logistics, Henon, ...) algorithms. The traditional algorithms have struggled to combat image data as compared to regular textual data. Whereas, the chaotic algorithms are more efficient for image ciphering. The Significance characteristics of chaos are its extreme sensitivity to initial conditions and algorithm parameters. In this paper, medical image security based on hybrid/mixed chaotic algorithms is proposed. The proposed method is implemented using MATLAB. Where the image of the Retina of the Eye to detect Blood Vessels is ciphered. The Pseudo-Random Numbers Generators (PRNGs) from the different chaotic algorithms are implemented, and their statistical properties are evaluated using the National Institute of Standards and Technology NIST and other statistical test-suits. Then, these algorithms are used to secure the data, where the statistical properties of the cipher-text are also tested. We propose two PRNGs to increase the complexity of the PRNGs and to allow many of the NIST statistical tests to be passed: one based on two-hybrid mixed chaotic logistic maps and one based on two-hybrid mixed chaotic Henon maps, where each chaotic algorithm runs side-by-side and starts with random initial conditions and parameters (encryption keys). The resulting hybrid PRNGs passed many of the NIST statistical test suits.', 'corpus_id': 227339100, 'score': 0}, {'doc_id': '207596791', 'title': 'Research on Plaintext Restoration of AES Based on Neural Network', 'abstract': 'Known plaintext attack is a common attack method in cryptographic attack. For ciphertext, only known part of the plaintext but unknown key, how to restore the rest of the plaintext is an important part of the known plaintext attack. This paper uses backpropagation neural networks to perform cryptanalysis on AES in an attempt to restore plaintext. The results show that the neural network can restore the entire byte with a probability of more than 40%, restoring more than half of the plaintext bytes with a probability of more than 63% and restoring more than half of the bytes above 89%.', 'corpus_id': 207596791, 'score': 1}, {'doc_id': '216414084', 'title': 'Optimized deep neural network for cryptanalysis of DES', 'abstract': None, 'corpus_id': 216414084, 'score': 1}, {'doc_id': '221662969', 'title': 'Deep Learning-Based Cryptanalysis of Lightweight Block Ciphers', 'abstract': 'Most of the traditional cryptanalytic technologies often require a great amount of time, known plaintexts, and memory. This paper proposes a generic cryptanalysis model based on deep learning (DL), where the model tries to find the key of block ciphers from known plaintext-ciphertext pairs. We show the feasibility of the DL-based cryptanalysis by attacking on lightweight block ciphers such as simplified DES, Simon, and Speck. The results show that the DL-based cryptanalysis can successfully recover the key bits when the keyspace is restricted to 64 ASCII characters. The traditional cryptanalysis is generally performed without the keyspace restriction, but only reduced-round variants of Simon and Speck are successfully attacked. Although a text-based key is applied, the proposed DL-based cryptanalysis can successfully break the full rounds of Simon32/64 and Speck32/64. The results indicate that the DL technology can be a useful tool for the cryptanalysis of block ciphers when the keyspace is restricted.', 'corpus_id': 221662969, 'score': 1}, {'doc_id': '204790807', 'title': 'Neural Cryptanalysis: Metrics, Methodology, and Applications in CPS Ciphers', 'abstract': 'Many real-world cyber-physical systems (CPS) use proprietary cipher algorithms. In this work, we describe an easy-to-use black-box security evaluation approach to measure the strength of proprietary ciphers without having to know the algorithms. We quantify the strength of a cipher by measuring how difficult it is for a neural network to mimic the cipher algorithm. We define new metrics (e.g., cipher match rate, training data complexity and training time complexity) that are computed from neural networks to quantitatively represent the cipher strength. This measurement approach allows us to directly compare the security of ciphers. Our experimental demonstration utilizes fully connected neural networks with multiple parallel binary classifiers at the output layer. The results show that when compared with round-reduced DES, the security strength of Hitag2 (a popular stream cipher used in the keyless entry of modern cars) is weaker than 3-round DES.', 'corpus_id': 204790807, 'score': 1}]
78	ODQA	159ec28c4d209b14f51c4539653dcbfe	18792	{}	"[{'doc_id': '235097600', 'title': 'TextGraphs 2021 Shared Task on Multi-Hop Inference for Explanation Regeneration', 'abstract': 'The Shared Task on Multi-Hop Inference for Explanation Regeneration asks participants to compose large multi-hop explanations to questions by assembling large chains of facts from a supporting knowledge base. While previous editions of this shared task aimed to evaluate explanatory completeness – finding a set of facts that form a complete inference chain, without gaps, to arrive from question to correct answer, this 2021 instantiation concentrates on the subtask of determining relevance in large multi-hop explanations. To this end, this edition of the shared task makes use of a large set of approximately 250k manual explanatory relevancy ratings that augment the 2020 shared task data. In this summary paper, we describe the details of the explanation regeneration task, the evaluation data, and the participating systems. Additionally, we perform a detailed analysis of participating systems, evaluating various aspects involved in the multi-hop inference process. The best performing system achieved an NDCG of 0.82 on this challenging task, substantially increasing performance over baseline methods by 32%, while also leaving significant room for future improvement.', 'corpus_id': 235097600, 'score': 0}, {'doc_id': '125545', 'title': 'Task-Oriented Query Reformulation with Reinforcement Learning', 'abstract': 'Search engines play an important role in our everyday lives by assisting us in finding the information we need. When we input a complex query, however, results are often far from satisfactory. In this work, we introduce a query reformulation system based on a neural network that rewrites a query to maximize the number of relevant documents returned. We train this neural network with reinforcement learning. The actions correspond to selecting terms to build a reformulated query, and the reward is the document recall. We evaluate our approach on three datasets against strong baselines and show a relative improvement of 5-20% in terms of recall. Furthermore, we present a simple method to estimate a conservative upper-bound performance of a model in a particular environment and verify that there is still large room for improvements.', 'corpus_id': 125545, 'score': 1}, {'doc_id': '235212203', 'title': 'Improve Query Focused Abstractive Summarization by Incorporating Answer Relevance', 'abstract': 'Query focused summarization (QFS) models aim to generate summaries from source documents that can answer the given query. Most previous work on QFS only considers the query relevance criterion when producing the summary. However, studying the effect of answer relevance in the summary generating process is also important. In this paper, we propose QFS-BART, a model that incorporates the explicit answer relevance of the source documents given the query via a question answering model, to generate coherent and answerrelated summaries. Furthermore, our model can take advantage of large pre-trained models which improve the summarization performance significantly. Empirical results on the Debatepedia dataset show that the proposed model achieves the new state-of-the-art performance.1', 'corpus_id': 235212203, 'score': 0}, {'doc_id': '235641228', 'title': 'Representing Long Documents with Contextualized Passage Embeddings', 'abstract': 'In this study we investigated a method for processing a large document collection with many long documents. The goal was to improve the processing runtime and memory requirements for document level tasks. We propose to represent the document collection by a database of passage embeddings instead by the document’s tokens. For our approach the passage embeddings should be task-agnostic, such they can be pretrained, precomputed and then be reused for many tasks. We propose a pretraining method for passage embeddings with (i) a passage encoder (PE) coupled with (ii) a bidirectional document encoder (BDE) over the passage embeddings. BDE can be finetuned for different downstream tasks to quickly learn a new model for a new task. In experiments, we found that PE+BDE is competitive with token-level or sentence-level models and sometimes even better. For plagiarism detection, for example, we improve over a Longformer-based model by +14 accuracy points. For passage classification on the contract understanding task PE+BDE reaches an AUPR 60.3 while a token-level information extraction approach using RoBERTalarge obtained an AUPR of 48.2 .', 'corpus_id': 235641228, 'score': 0}, {'doc_id': '6740934', 'title': 'Win-win search: dual-agent stochastic game in session search', 'abstract': 'Session search is a complex search task that involves multiple search iterations triggered by query reformulations. We observe a Markov chain in session search: user\'s judgment of retrieved documents in the previous search iteration affects user\'s actions in the next iteration. We thus propose to model session search as a dual-agent stochastic game: the user agent and the search engine agent work together to jointly maximize their long term rewards. The framework, which we term ""win-win search"", is based on Partially Observable Markov Decision Process. We mathematically model dynamics in session search, including decision states, query changes, clicks, and rewards, as a cooperative game between the user and the search engine. The experiments on TREC 2012 and 2013 Session datasets show a statistically significant improvement over the state-of-the-art interactive search and session search algorithms.', 'corpus_id': 6740934, 'score': 1}, {'doc_id': '173990818', 'title': 'Latent Retrieval for Weakly Supervised Open Domain Question Answering', 'abstract': 'Recent work on open domain question answering (QA) assumes strong supervision of the supporting evidence and/or assumes a blackbox information retrieval (IR) system to retrieve evidence candidates. We argue that both are suboptimal, since gold evidence is not always available, and QA is fundamentally different from IR. We show for the first time that it is possible to jointly learn the retriever and reader from question-answer string pairs and without any IR system. In this setting, evidence retrieval from all of Wikipedia is treated as a latent variable. Since this is impractical to learn from scratch, we pre-train the retriever with an Inverse Cloze Task. We evaluate on open versions of five QA datasets. On datasets where the questioner already knows the answer, a traditional IR system such as BM25 is sufficient. On datasets where a user is genuinely seeking an answer, we show that learned retrieval is crucial, outperforming BM25 by up to 19 points in exact match.', 'corpus_id': 173990818, 'score': 1}, {'doc_id': '53081945', 'title': 'NPRF: A Neural Pseudo Relevance Feedback Framework for Ad-hoc Information Retrieval', 'abstract': 'Pseudo relevance feedback (PRF) is commonly used to boost the performance of traditional information retrieval (IR) models by using top-ranked documents to identify and weight new query terms, thereby reducing the effect of query-document vocabulary mismatches. While neural retrieval models have recently demonstrated strong results for ad-hoc retrieval, combining them with PRF is not straightforward due to incompatibilities between existing PRF approaches and neural architectures. To bridge this gap, we propose an end-to-end neural PRF framework that can be used with existing neural IR models by embedding different neural models as building blocks. Extensive experiments on two standard test collections confirm the effectiveness of the proposed NPRF framework in improving the performance of two state-of-the-art neural IR models.', 'corpus_id': 53081945, 'score': 1}, {'doc_id': '235792421', 'title': 'ReadsRE: Retrieval-Augmented Distantly Supervised Relation Extraction', 'abstract': 'Distant supervision (DS) has been widely used to automatically construct (noisy) labeled data for relation extraction (RE). To address the noisy label problem, most models have adopted the multi-instance learning paradigm by representing entity pairs as a bag of sentences. However, this strategy depends on multiple assumptions (e.g., all sentences in a bag share the same relation), which may be invalid in real-world applications. Besides, it cannot work well on long-tail entity pairs which have few supporting sentences in the dataset. In this work, we propose a new paradigm named retrieval-augmented distantly supervised relation extraction (ReadsRE), which can incorporate large-scale open-domain knowledge (e.g., Wikipedia) into the retrieval step. ReadsRE seamlessly integrates a neural retriever and a relation predictor in an end-to-end framework. We demonstrate the effectiveness of ReadsRE on the well-known NYT10 dataset. The experimental results verify that ReadsRE can effectively retrieve meaningful sentences (i.e., denoise), and relieve the problem of long-tail entity pairs in the original dataset through incorporating external open-domain corpus. Through comparisons, we show ReadsRE outperforms other baselines for this task.', 'corpus_id': 235792421, 'score': 0}, {'doc_id': '225062257', 'title': 'Retrieve, Rerank, Read, then Iterate: Answering Open-Domain Questions of Arbitrary Complexity from Text', 'abstract': 'Current approaches to open-domain question answering often make crucial assumptions that prevent them from generalizing to real-world settings, including the access to parameterized retrieval systems well-tuned for the task, access to structured metadata like knowledge bases and web links, or a priori knowledge of the complexity of questions to be answered (e.g., single-hop or multi-hop). To address these limitations, we propose a unified system to answer open-domain questions of arbitrary complexity directly from text that works with off-the-shelf retrieval systems on arbitrary text collections. We employ a single multi-task model to perform all the necessary subtasks---retrieving supporting facts, reranking them, and predicting the answer from all retrieved documents---in an iterative fashion. To emulate a more realistic setting, we also constructed a new unified benchmark by collecting about 200 multi-hop questions that require three Wikipedia pages to answer, and combining them with existing datasets. We show that our model not only outperforms state-of-the-art systems on several existing benchmarks that exclusively feature single-hop or multi-hop open-domain questions, but also achieves strong performance on the new benchmark.', 'corpus_id': 225062257, 'score': 1}, {'doc_id': '235271356', 'title': 'RuBQ 2.0: An Innovated Russian Question Answering Dataset', 'abstract': 'The paper describes the second version of RuBQ, a Russian dataset for knowledge base question answering (KBQA) over Wikidata. Whereas the first version builds on Q&A pairs harvested online, the extension is based on questions obtained through search engine query suggestion services. The questions underwent crowdsourced and in-house annotation in a quite different fashion compared to the first edition. The dataset doubled in size: RuBQ 2.0 contains 2,910 questions along with the answers and SPARQL queries. The dataset also incorporates answer-bearing paragraphs from Wikipedia for the majority of questions. The dataset is suitable for the evaluation of KBQA, machine reading comprehension (MRC), hybrid questions answering, as well as semantic parsing. We provide the analysis of the dataset and report several KBQA and MRC baseline results. The dataset is freely available under the', 'corpus_id': 235271356, 'score': 0}]"
79	NeuroCardio	dceb0c1a2d13924557f97b5d16e6fe37	20578	{}	"[{'doc_id': '237406696', 'title': 'Brain Circuits Involved in the Development of Chronic Musculoskeletal Pain: Evidence From Non-invasive Brain Stimulation', 'abstract': 'It has been well-documented that the brain changes in states of chronic pain. Less is known about changes in the brain that predict the transition from acute to chronic pain. Evidence from neuroimaging studies suggests a shift from brain regions involved in nociceptive processing to corticostriatal brain regions that are instrumental in the processing of reward and emotional learning in the transition to the chronic state. In addition, dysfunction in descending pain modulatory circuits encompassing the periaqueductal gray and the rostral anterior cingulate cortex may also be a key risk factor for pain chronicity. Although longitudinal imaging studies have revealed potential predictors of pain chronicity, their causal role has not yet been determined. Here we review evidence from studies that involve non-invasive brain stimulation to elucidate to what extent they may help to elucidate the brain circuits involved in pain chronicity. Especially, we focus on studies using non-invasive brain stimulation techniques [e.g., transcranial magnetic stimulation (TMS), particularly its repetitive form (rTMS), transcranial alternating current stimulation (tACS), and transcranial direct current stimulation (tDCS)] in the context of musculoskeletal pain chronicity. We focus on the role of the motor cortex because of its known contribution to sensory components of pain via thalamic inhibition, and the role of the dorsolateral prefrontal cortex because of its role on cognitive and affective processing of pain. We will also discuss findings from studies using experimentally induced prolonged pain and studies implicating the DLPFC, which may shed light on the earliest transition phase to chronicity. We propose that combined brain stimulation and imaging studies might further advance mechanistic models of the chronicity process and involved brain circuits. Implications and challenges for translating the research on mechanistic models of the development of chronic pain to clinical practice will also be addressed.', 'corpus_id': 237406696, 'score': 0}, {'doc_id': '149445874', 'title': ""The brain's resonance of breathing - decelerated breathing synchronizes heart rate and slow cortical potentials."", 'abstract': 'Numerous methods for enhancing consciousness and well-being emphasize the role of breathing. We have for the first time investigated the link between body rhythms and slow cortical brain dynamics during paced breathing. Physiological data from 37 participants are presented, who conducted paced breathing sessions with respiration rates (RR) from 6 to 14 seconds/cycle for 7 min each task. Measures of respiration, heart rate variability (HRV), and 64 channels EEG as well as subjective ratings were recorded and compared with each other. Both, the respiratory sinus arrhythmia of the HRV and the slow cortical potentials (SCPs) of the EEG correlated with the respiration cycle. The highest correlations were observed at a RR of 10 seconds/cycle especially for the SCPs. A strong positive voltage deflection during inhalation is followed by a negative variation during exhalation. This decelerated breathing rhythm matches the frequency of the baroreceptor sensitivity, leading to synchronization between breath, HRV, baroreceptors and the brain. Subjectively, participants rated this RR as the most relaxing one. This study demonstrates the importance of the speed of breathing on the brain dynamics which might help in understanding the role of the breath for mental health. .', 'corpus_id': 149445874, 'score': 1}, {'doc_id': '1700773', 'title': 'Vascular Dynamics Aid a Coupled Neurovascular Network Learn Sparse Independent Features: A Computational Model', 'abstract': 'Cerebral vascular dynamics are generally thought to be controlled by neural activity in a unidirectional fashion. However, both computational modeling and experimental evidence point to the feedback effects of vascular dynamics on neural activity. Vascular feedback in the form of glucose and oxygen controls neuronal ATP, either directly or via the agency of astrocytes, which in turn modulates neural firing. Recently, a detailed model of the neuron-astrocyte-vessel system has shown how vasomotion can modulate neural firing. Similarly, arguing from known cerebrovascular physiology, an approach known as “hemoneural hypothesis” postulates functional modulation of neural activity by vascular feedback. To instantiate this perspective, we present a computational model in which a network of “vascular units” supplies energy to a neural network. The complex dynamics of the vascular network, modeled by a network of oscillators, turns neurons ON and OFF randomly. The informational consequence of such dynamics is explored in the context of an auto-encoder network. In the proposed model, each vascular unit supplies energy to a subset of hidden neurons of an autoencoder network, which constitutes its “projective field.” Neurons that receive adequate energy in a given trial have reduced threshold, and thus are prone to fire. Dynamics of the vascular network are governed by changes in the reconstruction error of the auto-encoder network, interpreted as the neuronal demand. Vascular feedback causes random inactivation of a subset of hidden neurons in every trial. We observe that, under conditions of desynchronized vascular dynamics, the output reconstruction error is low and the feature vectors learnt are sparse and independent. Our earlier modeling study highlighted the link between desynchronized vascular dynamics and efficient energy delivery in skeletal muscle. We now show that desynchronized vascular dynamics leads to efficient training in an auto-encoder neural network.', 'corpus_id': 1700773, 'score': 1}, {'doc_id': '237156116', 'title': 'Neural Responses During Emotion Transitions and Emotion Regulation', 'abstract': 'Why are some people more susceptible to interference from previous emotional stimuli? Neural mechanisms underlying emotion regulation are typically studied with one-off positive or negative stimuli. Less is known about how they operate during dynamic emotional experiences, which more closely resemble how emotions occur in real life. Therefore, we investigated the interaction among temporal context, stimulus content, and regulatory strategy. Image sequences included either neutral to negative emotion or negative to neutral emotion. Participants were instructed to either passively watch the emotional stimuli or apply cognitive reappraisal during the image sequences presentation. Participants also reported their habitual use of cognitive reappraisal in their daily lives on a standard scale. We measured functional connectivity (FC) with electroencephalography (EEG) source localization. A three-way interaction suggested that, in addition to momentary emotional content and regulatory effort, the temporal context of stimuli impacts the FC between the ventromedial prefrontal cortex (vmPFC) and the ventral anterior cingulate cortex (ACC) in both alpha and beta frequency bands. In the reappraisal condition—but not the passive watch conditions—, individual differences in habitual reappraisal were manifested in the FC of vmPFC-ACC in alpha band. Emotion transitions may be more demanding because prefrontal-posterior FC in the beta band decreased during emotion transitions regardless of emotional content or regulation efforts. Flexible emotion regulation enables the recruiting of neural activities in response to the content of dynamic, ever-changing experiences encountered in daily life. Studying brain responses to dynamic emotional stimuli may shed light on individual differences in adaptation and psychological health. It also provides a more ecologically valid assessment of emotion regulation.', 'corpus_id': 237156116, 'score': 0}, {'doc_id': '237299518', 'title': 'A neuro-cardiac self-regulation therapy to improve autonomic and neural function after SCI: a randomized controlled trial protocol', 'abstract': 'Background Spinal cord injury (SCI) is associated with autonomic imbalance and significant secondary conditions, including cardiac and brain dysfunction that adversely impact health and wellbeing. This study will investigate the effectiveness (intention-to-treat) of a neuro-cardiac self-regulation therapy to improve autonomic and neural/brain activity in adults with SCI living in the community. Methods A two-arm parallel, randomised controlled trial in which adults with SCI living in the community post-rehabilitation will be randomly assigned to a treatment or control group. The treatment group ( N \u2009=\u200960) aged 18–70\u2009years with a chronic traumatic or non-traumatic SCI, will receive intervention sessions once per week for 10\u2009weeks, designed to regulate autonomic activity using computer-based feedback of heart rate variability and controlled breathing (called HRV-F). Comprehensive neurophysiological and psychological assessment will occur at baseline, immediate post-treatment, and 6 and 12-months post-treatment. Primary outcome measures include electrocardiography/heart rate variability (to assess autonomic nervous system function) and transcranial doppler sonography (to assess cerebral blood circulation in basal cerebral arteries). Secondary outcomes measures include continuous blood pressure, electroencephalography, functional near-infrared spectroscopy, respiration/breath rate, electrooculography, cognitive capacity, psychological status, pain, fatigue, sleep and quality of life. Controls ( N \u2009=\u200960) will receive usual community care, reading material and a brief telephone call once per week for 10\u2009weeks and be similarly assessed over the same time period as the HRV-F group. Linear mixed model analysis with repeated measures will determine effectiveness of HRV-F and latent class mixture modelling used to determine trajectories for primary and selected secondary outcomes of interest. Discussion Treatments for improving autonomic function after SCI are limited. It is therefore important to establish whether a neuro-cardiac self-regulation therapy can result in improved autonomic functioning post-SCI, as well as whether HRV-F is associated with better outcomes for secondary conditions such as cardiovascular health, cognitive capacity and mental health. Trial registration The study has been prospectively registered with the Australian and New Zealand Clinical Trial Registry ( ACTRN12621000870853 .aspx). Date of Registration: 6th July 2021. Trial Sponsor: The University of Sydney, NSW 2006. Protocol version: 22/07/2021.', 'corpus_id': 237299518, 'score': 0}, {'doc_id': '237807715', 'title': 'Piezo2, A Pressure Sensitive Channel Is Expressed in Select Neurons of the Mouse Brain: a Putative Mechanism for Synchronizing Neural Networks by Transducing Intracranial Pressure Pulses', 'abstract': '\n Piezo2 expression in mouse brain was examined using an anti-PIEZO2 antibody (Ab) generated against a C-terminal fragment of the human PIEZO2 protein. As a positive control for Ab staining of mouse neurons, the Ab stained a majority of mouse dorsal root ganglion (DRG) neurons, consistent with recent in situ hybridization and single cell RNA sequencing studies of Piezo2 expression. As a negative control and test for specificity, the Ab failed to stain human erythrocytes, which selectively express PIEZO1. In brain slices isolated from the same mice as the DRG, the Ab displayed high selectivity in staining specific neuron types, including pyramidal neurons in the neocortex and hippocampus, Purkinje cells in the cerebellar cortex and mitral cells in the olfactory bulb. Given the demonstrated role of Piezo2 channels in peripheral neurons as a low-threshold pressure sensor (i.e., ≤ 5 mm Hg) critical for the regulation of breathing and blood pressure, its expression in select brain neurons has interesting implications. In particular, we hypothesize that Piezo2 provides select brain neurons with an intrinsic resonance enabling their entrainment by the normal intracranial pressure (ICP) pulses (~ 5 mm Hg) associated with breathing and cardiac cycles. This mechanism could serve to increase the robustness of respiration-entrained oscillations previously reported across widely distributed neuronal networks in both rodent and human brains. This idea of a “global brain rhythm” has previously been thought to arise from the effect of nasal airflow activating mechanosensitive neurons within the olfactory epithelium, which then synaptically entrain mitral cells within the olfactory bulb and through their projections, neural networks in other brain regions, including the hippocampus and neocortex. Our proposed, non-synaptic, intrinsic mechanism in which Piezo2 tracks the “metronome-like” ICP pulses would have the advantage that spatially separated brain networks could also be synchronized by a physical force that is rapidly transmitted throughout the brain.', 'corpus_id': 237807715, 'score': 1}, {'doc_id': '235663508', 'title': 'The Influence of Heart Rate Variability Biofeedback on Cardiac Regulation and Functional Brain Connectivity', 'abstract': 'Background Heart rate variability (HRV) biofeedback has a beneficial impact on perceived stress and emotion regulation. However, its impact on brain function is still unclear. In this study, we aimed to investigate the effect of an 8-week HRV-biofeedback intervention on functional brain connectivity in healthy subjects. Methods HRV biofeedback was carried out in five sessions per week, including four at home and one in our lab. A control group played jump‘n’run games instead of the training. Functional magnetic resonance imaging was conducted before and after the intervention in both groups. To compute resting state functional connectivity (RSFC), we defined regions of interest in the ventral medial prefrontal cortex (VMPFC) and a total of 260 independent anatomical regions for network-based analysis. Changes of RSFC of the VMPFC to other brain regions were compared between groups. Temporal changes of HRV during the resting state recording were correlated to dynamic functional connectivity of the VMPFC. Results First, we corroborated the role of the VMPFC in cardiac autonomic regulation. We found that temporal changes of HRV were correlated to dynamic changes of prefrontal connectivity, especially to the middle cingulate cortex, the left insula, supplementary motor area, dorsal and ventral lateral prefrontal regions. The biofeedback group showed a drop in heart rate by 5.2 beats/min and an increased SDNN as a measure of HRV by 8.6 ms (18%) after the intervention. Functional connectivity of the VMPFC increased mainly to the insula, the amygdala, the middle cingulate cortex, and lateral prefrontal regions after biofeedback intervention when compared to changes in the control group. Network-based statistic showed that biofeedback had an influence on a broad functional network of brain regions. Conclusion Our results show that increased heart rate variability induced by HRV-biofeedback is accompanied by changes in functional brain connectivity during resting state.', 'corpus_id': 235663508, 'score': 1}, {'doc_id': '226291226', 'title': 'Impact of Altered Breathing Patterns on Interaction of EEG and Heart Rate Variability', 'abstract': 'Background: Altered pattern of respiration has been shown to affect both the cardiac as well as cortical activity, which is the basis of central–autonomic dual interaction concept. On the other hand, effect of this association between altered breathing with slow cortical activity, that is, electroencephalography (EEG) theta waves (associated with learning and relaxed alertness) on the cardiac autonomic balance is largely unclear. Objective: The study aims to understand this interaction in response to altered respiratory patterns, for example, voluntary apnea, bradypnea, and tachypnea in terms of EEG and heart rate variability (HRV) correlates in normal healthy subjects. Methods: This study was conducted on 32 adult male subjects. EEG from F3, F4, P3, P4, O1 and O2 cortical areas and Lead II electrocardiography for HRV analysis was continuously recorded during aforesaid respiratory interventions. Power spectral analysis of EEG for theta waves and HRV measures, that is, RMSSD, pNN50, HF, LF, and LF/HF was calculated as % change taking resting value as 100%. Results: Apnea caused decrease in theta power, whereas an increase in LF/HF was observed in HRV. Bradypnea on the other hand, did not elicit any significant change in power of theta waves. However, decreased RMSSD and pNN50 were observed in HRV. Tachypnea led to increase in theta power with HRV depicting significantly decreased RMSSD and pNN50. Besides, significant correlation between EEG and HRV measures was found during tachypnea, which shifted toward posterior cortical sites as compared to resting condition. Conclusion: Various altered respiratory patterns caused either depressed parasympathetic or increased sympathetic output, whereas increased theta power along with posterior shift of correlation between theta power and HRV measures observed during post tachypnea might be due to involvement of global brain areas due to respiration-coupled neuronal activity. Thus, a definite link between cortical activity and autonomic output in relation to altered respiratory patterns may be suggested.', 'corpus_id': 226291226, 'score': 1}, {'doc_id': '238232466', 'title': 'Altered Hypothalamic Functional Connectivity Following Total Sleep Deprivation in Young Adult Males', 'abstract': 'Background: Sleep deprivation can markedly influence vigilant attention that is essential to complex cognitive processes. The hypothalamus plays a critical role in arousal and attention regulation. However, the functional involvement of the hypothalamus in attentional impairments after total sleep deprivation (TSD) remains unclear. The purpose of this study is to investigate the alterations in hypothalamic functional connectivity and its association with the attentional performance following TSD. Methods: Thirty healthy adult males were recruited in the study. Participants underwent two resting-state functional magnetic resonance imaging (rs-fMRI) scans, once in rested wakefulness (RW) and once after 36 h of TSD. Seed-based functional connectivity analysis was performed using rs-fMRI for the left and right hypothalamus. Vigilant attention was measured using a psychomotor vigilance test (PVT). Furthermore, Pearson correlation analysis was conducted to investigate the relationship between altered hypothalamic functional connectivity and PVT performance after TSD. Results: After TSD, enhanced functional connectivity was observed between the left hypothalamus and bilateral thalamus, bilateral anterior cingulate cortex, right amygdala, and right insula, while reduced functional connectivity was observed between the left hypothalamus and bilateral middle frontal gyrus (AlphaSim corrected, P < 0.01). However, significant correlation between altered hypothalamic functional connectivity and PVT performance was not observed after Bonferroni correction (P > 0.05). Conclusion: Our results suggest that TSD can lead to disrupted hypothalamic circuits, which may provide new insight into neural mechanisms of attention impairments following sleep deprivation.', 'corpus_id': 238232466, 'score': 0}, {'doc_id': '237003229', 'title': 'Using Fractional Amplitude of Low-Frequency Fluctuations and Functional Connectivity in Patients With Post-stroke Cognitive Impairment for a Simulated Stimulation Program', 'abstract': 'Stroke causes alterations in local spontaneous neuronal activity and related networks functional connectivity. We hypothesized that these changes occur in patients with post-stroke cognitive impairment (PSCI). Fractional amplitude of low-frequency fluctuations (fALFF) was calculated in 36 patients with cognitive impairment, including 16 patients with hemorrhagic stroke (hPSCI group), 20 patients with ischemic stroke (iPSCI group). Twenty healthy volunteers closely matched to the patient groups with respect to age and gender were selected as the healthy control group (HC group). Regions with significant alteration were regarded as regions of interest (ROIs) using the one-way analysis of variance, and then the seed-based functional connectivity (FC) with other regions in the brain was analyzed. Pearson correlation analyses were performed to investigate the correlation between functional indexes and cognitive performance in patients with PSCI. Our results showed that fALFF values of bilateral posterior cingulate cortex (PCC)/precuneus and bilateral anterior cingulate cortex in the hPSCI group were lower than those in the HC group. Compared with the HC group, fALFF values were lower in the superior frontal gyrus and basal ganglia in the iPSCI group. Correlation analysis showed that the fALFF value of left PCC was positively correlated with MMSE scores and MoCA scores in hPSCI. Besides, the reduction of seed-based FC values was reported, especially in regions of the default-mode network (DMN) and the salience network (SN). Abnormalities of spontaneous brain activity and functional connectivity are observed in PSCI patients. The decreased fALFF and FC values in DMN of patients with hemorrhagic and SN of patients with ischemic stroke may be the pathological mechanism of cognitive impairment. Besides, we showed how to use fALFF values and functional connectivity maps to specify a target map on the cortical surface for repetitive transcranial magnetic stimulation (rTMS).', 'corpus_id': 237003229, 'score': 0}]"
80	Ind/Conv Gain/Loss Mammals	87c08fb3c81b45d8e3192b3b3c025d5c	16011	{}	[{'doc_id': '232283165', 'title': 'Ecological correlates of gene family size: the draft genome of the redheaded', 'abstract': None, 'corpus_id': 232283165, 'score': 0}, {'doc_id': '232431462', 'title': 'Assessing evidence for adaptive evolution in two hearing-related genes important for high-frequency hearing in echolocating mammals', 'abstract': 'Abstract High-frequency hearing is particularly important for echolocating bats and toothed whales. Previously, studies of the hearing-related genes Prestin, KCNQ4, and TMC1 documented that adaptive evolution of high-frequency hearing has taken place in echolocating bats and toothed whales. In this study, we present two additional candidate hearing-related genes, Shh and SK2, that may also have contributed to the evolution of echolocation in mammals. Shh is a member of the vertebrate Hedgehog gene family and is required in the specification of the mammalian cochlea. SK2 is expressed in both inner and outer hair cells, and it plays an important role in the auditory system. The coding region sequences of Shh and SK2 were obtained from a wide range of mammals with and without echolocating ability. The topologies of phylogenetic trees constructed using Shh and SK2 were different; however, multiple molecular evolutionary analyses showed that those two genes experienced different selective pressures in echolocating bats and toothed whales compared to nonecholocating mammals. In addition, several nominally significant positively selected sites were detected in the nonfunctional domain of the SK2 gene, indicating that different selective pressures were acting on different parts of the SK2 gene. This study has expanded our knowledge of the adaptive evolution of high-frequency hearing in echolocating mammals.', 'corpus_id': 232431462, 'score': 1}, {'doc_id': '91792171', 'title': 'Detecting signatures of convergent evolution genome-wide', 'abstract': None, 'corpus_id': 91792171, 'score': 1}, {'doc_id': '233015553', 'title': 'Molluscan mitochondrial genomes break the rules', 'abstract': 'The first animal mitochondrial genomes to be sequenced were of several vertebrates and model organisms, and the consistency of genomic features found has led to a ‘textbook description’. However, a more broad phylogenetic sampling of complete animal mitochondrial genomes has found many cases where these features do not exist, and the phylum Mollusca is especially replete with these exceptions. The characterization of full mollusc mitogenomes required considerable effort involving challenging molecular biology, but has created an enormous catalogue of surprising deviations from that textbook description, including wide variation in size, radical genome rearrangements, gene duplications and losses, the introduction of novel genes, and a complex system of inheritance dubbed ‘doubly uniparental inheritance’. Here, we review the extraordinary variation in architecture, molecular functioning and intergenerational transmission of molluscan mitochondrial genomes. Such features represent a great potential for the discovery of biological history, processes and functions that are novel for animal mitochondrial genomes. This provides a model system for studying the evolution and the manifold roles that mitochondria play in organismal physiology, and many ways that the study of mitochondrial genomes are useful for phylogeny and population biology. This article is part of the Theo Murphy meeting issue ‘Molluscan genomics: broad insights and future directions for a neglected phylum’.', 'corpus_id': 233015553, 'score': 0}, {'doc_id': '232308283', 'title': 'Shifts in morphology, gene expression, and selection underlie web loss in Hawaiian Tetragnatha spiders', 'abstract': 'Background A striking aspect of evolution is that it often converges on similar trajectories. Evolutionary convergence can occur in deep time or over short time scales, and is associated with the imposition of similar selective pressures. Repeated convergent events provide a framework to infer the genetic basis of adaptive traits. The current study examines the genetic basis of secondary web loss within web-building spiders (Araneoidea). Specifically, we use a lineage of spiders in the genus Tetragnatha (Tetragnathidae) that has diverged into two clades associated with the relatively recent (5\xa0mya) colonization of, and subsequent adaptive radiation within, the Hawaiian Islands. One clade has adopted a cursorial lifestyle, and the other has retained the ancestral behavior of capturing prey with sticky orb webs. We explore how these behavioral phenotypes are reflected in the morphology of the spinning apparatus and internal silk glands, and the expression of silk genes. Several sister families to the Tetragnathidae have undergone similar web loss, so we also ask whether convergent patterns of selection can be detected in these lineages. Results The cursorial clade has lost spigots associated with the sticky spiral of the orb web. This appears to have been accompanied by loss of silk glands themselves. We generated phylogenies of silk proteins (spidroins), which showed that the transcriptomes of cursorial Tetragnatha contain all major spidroins except for flagelliform. We also found an uncharacterized spidroin that has higher expression in cursorial species. We found evidence for convergent selection acting on this spidroin, as well as genes involved in protein metabolism, in the cursorial Tetragnatha and divergent cursorial lineages in the families Malkaridae and Mimetidae. Conclusions Our results provide strong evidence that independent web loss events and the associated adoption of a cursorial lifestyle are based on similar genetic mechanisms. Many genes we identified as having evolved convergently are associated with protein synthesis, degradation, and processing, which are processes that play important roles in silk production. This study demonstrates, in the case of independent evolution of web loss, that similar selective pressures act on many of the same genes to produce the same phenotypes and behaviors.', 'corpus_id': 232308283, 'score': 0}, {'doc_id': '232060649', 'title': 'Sociality sculpts similar patterns of molecular evolution in two independently evolved lineages of eusocial bees', 'abstract': 'While it is well known that the genome can affect social behavior, recent models posit that social lifestyles can, in turn, influence genome evolution. Here, we perform the most phylogenetically comprehensive comparative analysis of 16 bee genomes to date: incorporating two published and four new carpenter bee genomes (Apidae: Xylocopinae) for a first-ever genomic comparison with a monophyletic clade containing solitary through advanced eusocial taxa. We find that eusocial lineages have undergone more gene family expansions, feature more signatures of positive selection, and have higher counts of taxonomically restricted genes than solitary and weakly social lineages. Transcriptomic data reveal that caste-affiliated genes are deeply-conserved; gene regulatory and functional elements are more closely tied to social phenotype than phylogenetic lineage; and regulatory complexity increases steadily with social complexity. Overall, our study provides robust empirical evidence that social evolution can act as a major and surprisingly consistent driver of macroevolutionary genomic change. Shell et al. compare the molecular evolution of bee genomes across species with varying social behaviours. Their findings indicate that eusocial lineages tend to have more gene family expansion than weakly social lineages. Transcriptomic data suggests that social phenotype is a consistent driver of macroevolutionary genomic change.', 'corpus_id': 232060649, 'score': 0}, {'doc_id': '233328739', 'title': 'Convergent selection on juvenile hormone signaling is associated with the evolution of eusociality in bees', 'abstract': 'Life’s most dramatic innovations, from the emergence of self-replicating molecules to highly-integrated societies, often involve increases in biological complexity. Some groups traverse different levels of complexity, providing a framework to identify key factors shaping these evolutionary transitions. Halictid bees span the transition from individual to group reproduction, with repeated gains and losses of eusociality. We generated chromosome-length genome assemblies for 17 species and searched for genes that both experienced positive selection when eusociality arose and relaxed selection when eusociality was secondarily lost. Loci exhibiting these complementary evolutionary signatures are predicted to carry costs outweighed by their importance for traits in eusocial lineages. Strikingly, these loci included two proteins that bind and transport juvenile hormone (JH) – a key regulator of insect development and reproduction. Though changes in JH abundance are frequently associated with polymorphisms, the mechanisms coupling JH to novel phenotypes are not well understood. Our results suggest novel links between JH and eusociality arose in halictids by altering transport and availability of JH in a tissue-specific manner, including in the brain. Through genomic comparisons of species encompassing both the emergence and breakdown of eusociality, we provide insights into the mechanisms targeted by selection to shape a key evolutionary transition.', 'corpus_id': 233328739, 'score': 0}, {'doc_id': '232125726', 'title': 'Embryonic evidence uncovers convergent origins of laryngeal echolocation in bats', 'abstract': 'Bats are the second-most speciose group of mammals, comprising 20% of species diversity today. Their global explosion, representing one of the greatest adaptive radiations in mammalian history, is largely attributed to their ability of laryngeal echolocation and powered flight, which enabled them to conquer the night sky, a vast and hitherto unoccupied ecological niche. While there is consensus that powered flight evolved only once in the lineage, whether laryngeal echolocation has a single origin in bats or evolved multiple times independently remains disputed. Here, we present developmental evidence in support of laryngeal echolocation having multiple origins in bats. This is consistent with a non-echolocating bat ancestor and independent gain of echolocation in Yinpterochiroptera and Yangochiroptera, as well as the gain of primitive echolocation in the bat ancestor, followed by convergent evolution of laryngeal echolocation in Yinpterochiroptera and Yangochiroptera, with loss of primitive echolocation in pteropodids. Our comparative embryological investigations found that there is no developmental difference in the hearing apparatus between non-laryngeal echolocating bats (pteropodids) and terrestrial non-bat mammals. In contrast, the echolocation system is developed heterotopically and heterochronically in the two phylogenetically distant laryngeal echolocating bats (rhinolophoids and yangochiropterans), providing the first embryological evidence that the echolocation system evolved independently in these bats.', 'corpus_id': 232125726, 'score': 1}, {'doc_id': '232224270', 'title': 'Complementary evolution of coding and noncoding sequence underlies mammalian hairlessness', 'abstract': 'Body hair is a defining mammalian characteristic, but several mammals, such as whales, naked mole-rats, and humans, have notably less hair than others. To find the genetic basis of reduced hair quantity, we used our evolutionary-rates-based method, RERconverge, to identify coding and noncoding sequences that evolve at significantly different rates in so-called hairless mammals compared to hairy mammals. Using RERconverge, we performed an unbiased, genome-wide scan over 62 mammal species using 19,149 genes and 343,598 conserved noncoding regions to find genetic elements that evolve at significantly different rates in hairless mammals compared to hairy mammals. We show that these rate shifts resulted from relaxation of evolutionary constraint on hair-related sequences in hairless species. In addition to detecting known and potential novel hair-related genes, we also discovered hundreds of putative hair-related regulatory elements. Computational investigation revealed that genes and their associated noncoding regions show different evolutionary patterns and influence different aspects of hair growth and development. Many genes under accelerated evolution are associated with the structure of the hair shaft itself, while evolutionary rate shifts in noncoding regions also included the dermal papilla and matrix regions of the hair follicle that contribute to hair growth and cycling. Genes that were top-ranked for coding sequence acceleration included known hair and skin genes KRT2, KRT35, PKP1, and PTPRM that surprisingly showed no signals of evolutionary rate shifts in nearby noncoding regions. Conversely, accelerated noncoding regions are most strongly enriched near regulatory hair-related genes and microRNAs, such as mir205, ELF3, and FOXC1, that themselves do not show rate shifts in their protein-coding sequences. Such dichotomy highlights the interplay between the evolution of protein sequence and regulatory sequence to contribute to the emergence of a convergent phenotype.', 'corpus_id': 232224270, 'score': 1}, {'doc_id': '232224405', 'title': 'Refining Convergent Rate Analysis with Topology in Mammalian Longevity and Marine Transitions', 'abstract': 'The quest to map the genetic foundations of phenotypes has been empowered by the modern diversity, quality, and availability of genomic resources. Despite these expanding resources, the abundance of variation within lineages makes the association of genetic change to specific phenotypes improbable. Drawing such connections requires an a priori means of isolating the associated changes from background genomic variation. Evolution may provide these means via convergence; i.e., the shared variation that may result from replicate evolutionary experiments across independent trait occurrences. To leverage these opportunities, we developed TRACCER: Topologically Ranked Analysis of Convergence via Comparative Evolutionary Rates. As compared to current methods, this software empowers rate convergence analysis by factoring in topological relationships, because variation between phylogenetically proximate trait changes is more likely to be facilitating the trait. Pairwise comparisons are performed not with singular branches, but in reference to their most recent common ancestors. This ensures that comparisons represent identical genetic contexts and timeframes while obviating the problematic requirement of assigning ancestral states. We applied TRACCER to two case studies: marine mammal transitions, an unambiguous trait which has independently evolved three times, as well as the evolution of mammalian longevity, a less delineated trait but with more instances to compare. TRACCER, by factoring in topology, identifies highly significant, convergent genetic signals in these test cases, with important incongruities and statistical resolution when compared to existing convergence approaches. These improvements in sensitivity and specificity generate refined targets for downstream analysis of convergent evolution and identification of genotype-phenotype relationships.', 'corpus_id': 232224405, 'score': 1}]
81	smart Home	c62b4a3072c02c4a228c41e26e86e783	7014	{}	"[{'doc_id': '219300570', 'title': 'Impact of Covid-19 on consumer behavior: Will the old habits return or die?', 'abstract': '\n Abstract\n \n The COVID-19 pandemic and the lockdown and social distancing mandates have disrupted the consumer habits of buying as well as shopping. Consumers are learning to improvise and learn new habits. For example, consumers cannot go to the store, so the store comes to home. While consumers go back to old habits, it is likely that they will be modified by new regulations and procedures in the way consumers shop and buy products and services. New habits will also emerge by technology advances, changing demographics and innovative ways consumers have learned to cope with blurring the work, leisure, and education boundaries.\n \n', 'corpus_id': 219300570, 'score': 0}, {'doc_id': '7126677', 'title': 'Principles of Smart Home Control', 'abstract': 'Seeking to be sensitive to users, smart home researchers have focused on the concept of control. They attempt to allow users to gain control over their lives by framing the problem as one of end-user programming. But families are not users as we typically conceive them, and a large body of ethnographic research shows how their activities and routines do not map well to programming tasks. End-user programming ultimately provides control of devices. But families want more control of their lives. In this paper, we explore this disconnect. Using grounded contextual fieldwork with dual-income families, we describe the control that families want, and suggest seven design principles that will help end-user programming systems deliver that control.', 'corpus_id': 7126677, 'score': 1}, {'doc_id': '14480787', 'title': 'Home automation in the wild: challenges and opportunities', 'abstract': 'Visions of smart homes have long caught the attention of researchers and considerable effort has been put toward enabling home automation. However, these technologies have not been widely adopted despite being available for over three decades. To gain insight into this state of affairs, we conducted semi-structured home visits to 14 households with home automation. The long term experience, both positive and negative, of the households we interviewed illustrates four barriers that need to be addressed before home automation becomes amenable to broader adoption. These barriers are high cost of ownership, inflexibility, poor manageability, and difficulty achieving security. Our findings also provide several directions for further research, which include eliminating the need for structural changes for installing home automation, providing users with simple security primitives that they can confidently configure, and enabling composition of home devices.', 'corpus_id': 14480787, 'score': 1}, {'doc_id': '219259981', 'title': 'A review of smartphones based indoor positioning: challenges and applications', 'abstract': 'The continual proliferation of mobile devices has encouraged much effort in using the smartphones for indoor positioning. This article is dedicated to review the most recent and interesting smartphones based indoor navigation systems, ranging from electromagnetic to inertia to visible light ones, with an emphasis on their unique challenges and potential real-world applications. A taxonomy of smartphones sensors will be introduced, which serves as the basis to categorise different positioning systems for reviewing. A set of criteria to be used for the evaluation purpose will be devised. For each sensor category, the most recent, interesting and practical systems will be examined, with detailed discussion on the open research questions for the academics, and the practicality for the potential clients.', 'corpus_id': 219259981, 'score': 0}, {'doc_id': '67781466', 'title': 'Benefits and risks of smart home technologies', 'abstract': 'Smart homes are a priority area of strategic energy planning and national policy. The market adoption of smart home technologies (SHTs) relies on prospective users perceiving clear benefits with acceptable levels of risk. This paper characterises the perceived benefits and risks of SHTs from multiple perspectives.', 'corpus_id': 67781466, 'score': 1}, {'doc_id': '221881991', 'title': 'Smart cities as a platform for technological and social innovation in productivity, sustainability, and livability: A conceptual framework', 'abstract': '\n Despite a great deal of attention paid to smart cities, the conceptual framework for understanding them has been partial at best. This chapter establishes a holistic framework to define and evaluate smart cities through three core objectives that any city wants to improve—productivity, sustainability, and livability. Although smartness includes a wide range of aspects within a city, it should tackle the complexity of urban challenges internally and externally generated. Thus, adaptive capacity is becoming more and more important, requiring timely innovation. The chapter asserts cities are and should be a platform for technological and social innovation to enhance these three urban cores. Creating smart cities via innovation is not a one-way process, but reciprocal. Innovation can create smart built environments, and, in turn, smart cities engender innovation. There are many successful evidences and documented examples of both technology-oriented initiatives and social innovation strategies worldwide. However, there is limited understanding of the combined view on technological innovation or social innovation that can contribute to meeting urban challenges. Furthermore, how the urban future might benefit from interdependency and interactions of the elements in these two concepts has not been fully explored. The research will set an agenda for measurement of cities’ performance in productivity, sustainability, and livability from both technological and social innovation perspectives.\n', 'corpus_id': 221881991, 'score': 0}, {'doc_id': '167784161', 'title': 'Who uses smart home technologies? Representations of users by the smart home industry', 'abstract': 'Through ambient intelligence and automated control systems, smart homes have been presented as a key means by which households can optimize their use of energy-consuming appliances in order to save energy and money. Whilst the adoption of smart home technologies and their appropriation within everyday domestic lives is critical to the overall success of smart homes, to date visions of smart homes have been strongly driven by technology push and have not been based on a clear understanding of user-centric benefits, nor have users been engaged with in any clear or systematic way. There is thus an important need to understand how smart home users are being represented and understood within these technology-driven visions. The paper presents the results of a content analysis of industry-produced smart home marketing materials that focussed on representations of the technology itself, its users, and of technology-user interactions. The content analysis was based on a coding template derived from a systematic review of the academic literature on smart homes and their users. Key findings from the content analysis include: • differences in opinion around whether user practices are predominantly stable, routine and predictable or involve substantial variability and unpredictability; • consensus around the modular development of smart homes within existing homes through additional and integrated (rather than replacement) technologies; • a widespread lack of attention to within household interactions and the possibility of multiple users with divergent technology preferences; • an implicit assumption that user decision-making is mainly rational, centred on information-processing; • strong consensus on the design of user interfaces as mobile, familiar, intuitive, and visible devices; • ambiguity regarding the potential tension between control and empowerment as opposed to automation. The paper concludes that industry visions of smart homes are more convergent than academic research suggests, particularly around issues of user decisions and interaction, trust and confidentiality, and control and automation.', 'corpus_id': 167784161, 'score': 1}, {'doc_id': '220407868', 'title': 'Mothers, childcare duties, and remote working under COVID-19 lockdown in Italy: Cultivating communities of care', 'abstract': 'Drawing on a virtual ethnography, we explore how the increase in remote working has created unequal domestic rearrangements of parenting duties with respect to gender relations during the COVID-19 lockdown in Italy. We also discuss the resources that mothers have mobilized to create a network of social support in the organization of care.', 'corpus_id': 220407868, 'score': 0}, {'doc_id': '2394201', 'title': 'Sabbath day home automation: ""it\'s like mixing technology and religion""', 'abstract': ""We present a qualitative study of 20 American Orthodox Jewish families' use of home automation for religious purposes. These lead users offer insight into real-life, long-term experience with home automation technologies. We discuss how automation was seen by participants to contribute to spiritual experience and how participants oriented to the use of automation as a religious custom. We also discuss the relationship of home automation to family life. We draw design implications for the broader population, including surrender of control as a design resource, home technologies that support long-term goals and lifestyle choices, and respite from technology."", 'corpus_id': 2394201, 'score': 1}, {'doc_id': '219691177', 'title': 'Using online technologies to improve diversity and inclusion in cognitive interviews with young people', 'abstract': 'Background We aimed to assess the feasibility of using multiple technologies to recruit and conduct cognitive interviews among young people across the United States to test items measuring sexual and reproductive empowerment. We sought to understand whether these methods could achieve a diverse sample of participants. With more researchers turning to approaches that maintain social distancing in the context of COVID-19, it has become more pressing to refine these remote research methods. Methods We used several online sites to recruit for and conduct cognitive testing of survey items. To recruit potential participants we advertised the study on the free online bulletin board, Craigslist, and the free online social network, Reddit. Interested participants completed an online Qualtrics screening form. To maximize diversity, we purposefully selected individuals to invite for participation. We used the video meeting platform, Zoom, to conduct the cognitive interviews. The interviewer opened a document with the items to be tested, shared the screen with the participant, and gave them control of the mouse and keyboard. After the participant self-administered the survey, the\xa0interviewer asked about interpretation and comprehension. After completion of the interviews we sent participants a follow-up survey about their impressions of the research methods and technologies used. We describe the processes, the advantages and disadvantages, and offer recommendations for researchers. Results We recruited and interviewed 30 young people from a range of regions, gender identities, sexual orientations, ages, education, and experiences with sexual activity. These methods allowed us to recruit a purposefully selected diverse sample in terms of race/ethnicity and region. It also may have offered potential participants a feeling of safety and anonymity leading to greater participation from gay, lesbian, and transgender people who would not have agreed to participate in-person. Conducting the interviews using video chat may also have facilitated the inclusion of individuals who would not volunteer for in-person meetings. Disadvantages of video interviewing included participant challenges to finding a private space for the interview and problems\xa0with electronic devices. Conclusions Online technologies can be used to achieve a diverse sample of research participants, contributing to research findings that better respond to young people’s unique identities and situations.', 'corpus_id': 219691177, 'score': 0}]"
82	Active Learning	6dc00516e64a4b92f00d5dbe1615cafe	3494	{}	"[{'doc_id': '211020634', 'title': 'Iterative Data Programming for Expanding Text Classification Corpora', 'abstract': 'Real-world text classification tasks often require many labeled training examples that are expensive to obtain. Recent advancements in machine teaching, specifically the data programming paradigm, facilitate the creation of training data sets quickly via a general framework for building weak models, also known as labeling functions, and denoising them through ensemble learning techniques. We present a fast, simple data programming method for augmenting text data sets by generating neighborhood-based weak models with minimal supervision. Furthermore, our method employs an iterative procedure to identify sparsely distributed examples from large volumes of unlabeled data. The iterative data programming techniques improve newer weak models as more labeled data is confirmed with human-in-loop. We show empirical results on sentence classification tasks, including those from a task of improving intent recognition in conversational agents.', 'corpus_id': 211020634, 'score': 0}, {'doc_id': '2754649', 'title': 'Active Discriminative Text Representation Learning', 'abstract': ""We propose a new active learning (AL) method for text classification with convolutional neural networks (CNNs). In AL, one selects the instances to be manually labeled with the aim of maximizing model performance with minimal effort. Neural models capitalize on word embeddings as representations (features), tuning these to the task at hand. We argue that AL strategies for multi-layered neural models should focus on selecting instances that most affect the embedding space (i.e., induce discriminative word representations). This is in contrast to traditional AL approaches (e.g., entropy-based uncertainty sampling), which specify higher level objectives. We propose a simple approach for sentence classification that selects instances containing words whose embeddings are likely to be updated with the greatest magnitude, thereby rapidly learning discriminative, task-specific embeddings. We extend this approach to document classification by jointly considering: (1) the expected changes to the constituent word representations; and (2) the model's current overall uncertainty regarding the instance. The relative emphasis placed on these criteria is governed by a stochastic process that favors selecting instances likely to improve representations at the outset of learning, and then shifts toward general uncertainty sampling as AL progresses. Empirical results show that our method outperforms baseline AL approaches on both sentence and document classification tasks. We also show that, as expected, the method quickly learns discriminative word embeddings. To the best of our knowledge, this is the first work on AL addressing neural models for text classification."", 'corpus_id': 2754649, 'score': 1}, {'doc_id': '214667381', 'title': 'Instance Credibility Inference for Few-Shot Learning', 'abstract': 'Few-shot learning (FSL) aims to recognize new objects with extremely limited training data for each category. Previous efforts are made by either leveraging meta-learning paradigm or novel principles in data augmentation to alleviate this extremely data-scarce problem. In contrast, this paper presents a simple statistical approach, dubbed Instance Credibility Inference (ICI) to exploit the distribution support of unlabeled instances for few-shot learning. Specifically, we first train a linear classifier with the labeled few-shot examples and use it to infer the pseudo-labels for the unlabeled data. To measure the credibility of each pseudo-labeled instance, we then propose to solve another linear regression hypothesis by increasing the sparsity of the incidental parameters and rank the pseudo-labeled instances with their sparsity degree. We select the most trustworthy pseudo-labeled instances alongside the labeled examples to re-train the linear classifier. This process is iterated until all the unlabeled samples are included in the expanded training set, i.e. the pseudo-label is converged for unlabeled data pool. Extensive experiments under two few-shot settings show that our simple approach can establish new state-of-the-arts on four widely used few-shot learning benchmark datasets including miniImageNet, tieredImageNet, CIFAR-FS, and CUB. Our code is available at: https://github.com/Yikai-Wang/ICI-FSL', 'corpus_id': 214667381, 'score': 0}, {'doc_id': '211133088', 'title': 'Reinforced active learning for image segmentation', 'abstract': 'Learning-based approaches for semantic segmentation have two inherent challenges. First, acquiring pixel-wise labels is expensive and time-consuming. Second, realistic segmentation datasets are highly unbalanced: some categories are much more abundant than others, biasing the performance to the most represented ones. In this paper, we are interested in focusing human labelling effort on a small subset of a larger pool of data, minimizing this effort while maximizing performance of a segmentation model on a hold-out set. We present a new active learning strategy for semantic segmentation based on deep reinforcement learning (RL). An agent learns a policy to select a subset of small informative image regions -- opposed to entire images -- to be labeled, from a pool of unlabeled data. The region selection decision is made based on predictions and uncertainties of the segmentation model being trained. Our method proposes a new modification of the deep Q-network (DQN) formulation for active learning, adapting it to the large-scale nature of semantic segmentation problems. We test the proof of concept in CamVid and provide results in the large-scale dataset Cityscapes. On Cityscapes, our deep RL region-based DQN approach requires roughly 30% less additional labeled data than our most competitive baseline to reach the same performance. Moreover, we find that our method asks for more labels of under-represented categories compared to the baselines, improving their performance and helping to mitigate class imbalance.', 'corpus_id': 211133088, 'score': 0}, {'doc_id': '157059801', 'title': 'Teaching a black-box learner', 'abstract': 'One widely-studied model of teaching (Goldman & Kearns, 1995; Shinohara & Miyano, 1991; Anthony et al., 1992) calls for a teacher to provide the minimal set of labeled examples that uniquely specifies a target concept. The assumption is that the teacher knows the learner’s hypothesis class, which is often not true of real-life teaching scenarios. We consider the problem of teaching a learner whose representation and hypothesis class are unknown: that is, the learner is a black box. We find that a teacher who does not interact with the learner can do no better than providing random examples. However, by interacting with the black-box learner, a teacher can efficiently find a set of teaching examples that is a provably good approximation to the optimal set. As an illustration, we show how this scheme can be used to shrink training sets for any family of classifiers: that is, to find an approximatelyminimal subset of training instances that yields the same classifier as the entire set.', 'corpus_id': 157059801, 'score': 1}, {'doc_id': '214727875', 'title': 'Neural Networks Are More Productive Teachers Than Human Raters: Active Mixup for Data-Efficient Knowledge Distillation From a Blackbox Model', 'abstract': 'We study how to train a student deep neural network for visual recognition by distilling knowledge from a blackbox teacher model in a data-efficient manner. Progress on this problem can significantly reduce the dependence on large-scale datasets for learning high-performing visual recognition models. There are two major challenges. One is that the number of queries into the teacher model should be minimized to save computational and/or financial costs. The other is that the number of images used for the knowledge distillation should be small; otherwise, it violates our expectation of reducing the dependence on large-scale datasets. To tackle these challenges, we propose an approach that blends mixup and active learning. The former effectively augments the few unlabeled images by a big pool of synthetic images sampled from the convex hull of the original images, and the latter actively chooses from the pool hard examples for the student neural network and query their labels from the teacher model. We validate our approach with extensive experiments.', 'corpus_id': 214727875, 'score': 0}, {'doc_id': '324600', 'title': 'Active Learning Literature Survey', 'abstract': 'The key idea behind active learning is that a machine learning algorithm can achieve greater accuracy with fewer labeled training instances if it is allowed to choose the data from which is learns. An active learner may ask queries in the form of unlabeled instances to be labeled by an oracle (e.g., a human annotator). Active learning is well-motivated in many modern machine learning problems, where unlabeled data may be abundant but labels are difficult, time-consuming, or expensive to obtain. This report provides a general introduction to active learning and a survey of the literature. This includes a discussion of the scenarios in which queries can be formulated, and an overview of the query strategy frameworks proposed in the literature to date. An analysis of the empirical and theoretical evidence for active learning, a summary of several problem setting variants, and a discussion of related topics in machine learning research are also presented.', 'corpus_id': 324600, 'score': 1}, {'doc_id': '215548017', 'title': 'Scalable Active Learning for Object Detection', 'abstract': 'Deep Neural Networks trained in a fully supervised fashion are the dominant technology in perception-based autonomous driving systems. While collecting large amounts of unlabeled data is already a major undertaking, only a subset of it can be labeled by humans due to the effort needed for high-quality annotation. Therefore, finding the right data to label has become a key challenge. Active learning is a powerful technique to improve data efficiency for supervised learning methods, as it aims at selecting the smallest possible training set to reach a required performance. We have built a scalable production system for active learning in the domain of autonomous driving. In this paper, we describe the resulting high-level design, sketch some of the challenges and their solutions, present our current results at scale, and briefly describe the open problems and future directions.', 'corpus_id': 215548017, 'score': 0}, {'doc_id': '7806109', 'title': 'Support Vector Machine Active Learning with Applications to Text Classification', 'abstract': 'Support vector machines have met with significant success in numerous real-world learning tasks. However, like most machine learning algorithms, they are generally applied using a randomly selected training set classified in advance. In many settings, we also have the option of using pool-based active learning. Instead of using a randomly selected training set, the learner has access to a pool of unlabeled instances and can request the labels for some number of them. We introduce a new algorithm for performing active learning with support vector machines, i.e., an algorithm for choosing which instances to request next. We provide a theoretical motivation for the algorithm using the notion of a version space. We present experimental results showing that employing our active learning method can significantly reduce the need for labeled training instances in both the standard inductive and transductive settings.', 'corpus_id': 7806109, 'score': 1}, {'doc_id': '49741470', 'title': 'How transferable are the datasets collected by active learners?', 'abstract': 'Active learning is a widely-used training strategy for maximizing predictive performance subject to a fixed annotation budget. Between rounds of training, an active learner iteratively selects examples for annotation, typically based on some measure of the model\'s uncertainty, coupling the acquired dataset with the underlying model. However, owing to the high cost of annotation and the rapid pace of model development, labeled datasets may remain valuable long after a particular model is surpassed by new technology. In this paper, we investigate the transferability of datasets collected with an acquisition model A to a distinct successor model S. We seek to characterize whether the benefits of active learning persist when A and S are different models. To this end, we consider two standard NLP tasks and associated datasets: text classification and sequence tagging. We find that training S on a dataset actively acquired with a (different) model A typically yields worse performance than when S is trained with ""native"" data (i.e., acquired actively using S), and often performs worse than training on i.i.d. sampled data. These findings have implications for the use of active learning in practice,suggesting that it is better suited to cases where models are updated no more frequently than labeled data.', 'corpus_id': 49741470, 'score': 1}]"
83	AI and Economic growth	a3b54f5b14e135f425132955c204bbc3	12947	{}	"[{'doc_id': '7468879', 'title': 'Robots and Jobs: Evidence from US Labor Markets', 'abstract': 'As robots and other computer-assisted technologies take over tasks previously performed by labor, there is increasing concern about the future of jobs and wages. We analyze the effect of the increase in industrial robot usage between 1990 and 2007 on US local labor markets. Using a model in which robots compete against human labor in the production of different tasks, we show that robots may reduce employment and wages, and that the local labor market effects of robots can be estimated by regressing the change in employment and wages on the exposure to robots in each local labor market—defined from the national penetration of robots into each industry and the local distribution of employment across industries. Using this approach, we estimate large and robust negative effects of robots on employment and wages across commuting zones. We bolster this evidence by showing that the commuting zones most exposed to robots in the post-1990 era do not exhibit any differential trends before 1990. The impact of robots is distinct from the impact of imports from China and Mexico, the decline of routine jobs, offshoring, other types of IT capital, and the total capital stock (in fact, exposure to robots is only weakly correlated with these other variables). According to our estimates, one more robot per thousand workers reduces the employment to population ratio by about 0.18-0.34 percentage points and wages by 0.25-0.5 percent.', 'corpus_id': 7468879, 'score': 1}, {'doc_id': '151194632', 'title': 'Skills, Tasks and Technologies: Implications for Employment and Earnings', 'abstract': 'A central organizing framework of the voluminous recent literature studying changes in the returns to skills and the evolution of earnings inequality is what we refer to as the canonical model, which elegantly and powerfully operationalizes the supply and demand for skills by assuming two distinct skill groups that perform two different and imperfectly substitutable tasks or produce two imperfectly substitutable goods. Technology is assumed to take a factor-augmenting form, which, by complementing either high or low skill workers, can generate skill biased demand shifts. In this paper, we argue that despite its notable successes, the canonical model is largely silent on a number of central empirical developments of the last three decades, including: (1) significant declines in real wages of low skill workers, particularly low skill males; (2) non-monotone changes in wages at different parts of the earnings distribution during different decades; (3) broad-based increases in employment in high skill and low skill occupations relative to middle skilled occupations (i.e., job ""polarization""); (4) rapid diffusion of new technologies that directly substitute capital for labor in tasks previously performed by moderately skilled workers; and (5) expanding offshoring in opportunities, enabled by technology, which allow foreign labor to substitute for domestic workers specific tasks. Motivated by these patterns, we argue that it is valuable to consider a richer framework for analyzing how recent changes in the earnings and employment distribution in the United States and other advanced economies are shaped by the interactions among worker skills, job tasks, evolving technologies, and shifting trading opportunities. We propose a tractable task-based model in which the assignment of skills to tasks is endogenous and technical change may involve the substitution of machines for certain tasks previously performed by labor. We further consider how the evolution of technology in this task-based setting may be endogenized. We show how such a framework can be used to interpret several central recent trends, and we also suggest further directions for empirical exploration.', 'corpus_id': 151194632, 'score': 1}, {'doc_id': '232147300', 'title': ""Does automation erode governments' tax basis? An empirical assessment of tax revenues in Europe"", 'abstract': 'Decomposing taxes by source (labor, capital, sales), we analyze the impact of automation (1) on tax revenues, (2) the structure of taxation, and (3) identify channels of impact in 19 EU countries during 1995-2016. Robots and Information and Communication Technologies (ICT) are different technologies designed to automate manual (robots) or cognitive tasks (ICT). Until 2007, robot diffusion led to decreasing factor and tax income, and a shift from taxes on capital to goods. ICTs changed the structure of taxation from capital to labor. We find decreasing employment, but increasing wages and labor income. After 2008, robots have no effect but we find an ICT-induced increase in capital income, a rise of services, but no effect on taxation. Automation goes through different phases with different economic impacts which affect the amount and structure of taxes. Whether automation erodes taxation depends (a) on the technology type, (b) the stage of diffusion and (c) local conditions.', 'corpus_id': 232147300, 'score': 0}, {'doc_id': '232966563', 'title': 'WORKING PAPER SERIES WAGE INEQUALITY AND FIRM GROWTH', 'abstract': 'We examine how within-firm skill premia–wage differentials associated with jobs involving different skill requirements–vary both across firms and over time. Our firm-level results mirror patterns found in aggregate wage trends, except that we find them with regard to increases in firm size. In particular, we find that wage differentials between highand either mediumor low-skill jobs increase with firm size, while those between mediumand low-skill jobs are either invariant to firm size or, if anything, slightly decreasing. We find the same pattern within firms over time, suggesting that rising wage inequality–even nuanced patterns, such as divergent trends in upperand lower-tail inequality–may be related to firm growth. We explore two possible channels: i) wages associated with “routine” job tasks are relatively lower in larger firms due to a higher degree of automation in these firms, and ii) larger firms pay relatively lower entry-level managerial wages in return for providing better career opportunities. Lastly, we document a strong and positive relation between within-country variation in firm growth and rising wage inequality for a broad set of developed countries. In fact, our results suggest that part of what may be perceived as a global trend toward more wage inequality may be driven by an increase in employment by the largest firms in the economy. Holger M. Mueller Stern School of Business New York University 44 West Fourth Street Suite 9-190 New York, NY 10012-1126 and NBER hmueller@stern.nyu.edu Paige P. Ouimet Kenan-Flagler Business School University of North Carolina at Chapel Hill Paige_Ouimet@kenan-flagler.unc.edu Elena Simintzi University of British Columbia Sauder School of Business 2053 Main Mall Vancouver V6T 1Z2 Canada elena.simintzi@sauder.ubc.ca', 'corpus_id': 232966563, 'score': 0}, {'doc_id': '18383943', 'title': 'The Race between Man and Machine: Implications of Technology for Growth, Factor Shares, and Employment', 'abstract': 'We examine the concerns that new technologies will render labor redundant in a framework in which tasks previously performed by labor can be automated and new versions of existing tasks, in which labor has a comparative advantage, can be created. In a static version where capital is fixed and technology is exogenous, automation reduces employment and the labor share, and may even reduce wages, while the creation of new tasks has the opposite effects. Our full model endogenizes capital accumulation and the direction of research towards automation and the creation of new tasks. If the long-run rental rate of capital relative to the wage is sufficiently low, the long-run equilibrium involves automation of all tasks. Otherwise, there exists a stable balanced growth path in which the two types of innovations go hand-in-hand. Stability is a consequence of the fact that automation reduces the cost of producing using labor, and thus discourages further automation and encourages the creation of new tasks. In an extension with heterogeneous skills, we show that inequality increases during transitions driven both by faster automation and introduction of new tasks, and characterize the conditions under which inequality is increasing or stable in the long run.', 'corpus_id': 18383943, 'score': 1}, {'doc_id': '231203954', 'title': 'The productivity paradox: policy lessons from MICROPROD', 'abstract': 'MICROPROD researchers have so far delivered 20 papers on four broad issues relevant for today’s policy debates: the measurement and effects of intangible capital on productivity; the impact of globalisation, international trade and the integration of global value chains (GVCs) on productivity; factor allocation and allocative efficiency; and finally the social consequences of the two structural shocks Europe has faced in the last two decades: globalisation and technological progress.', 'corpus_id': 231203954, 'score': 0}, {'doc_id': '231993412', 'title': 'www.econstor.eu Task content and job losses in the Great Lockdown', 'abstract': 'I examine the short-term labor market effects of the Great Lockdown in the United States. I analyze job losses by task content (Acemoglu & Autor 2011), and show that they follow underlying trends; jobs with a high non-routine content are especially well-protected, even if they are not teleworkable. The importance of the task content, particularly for non-routine cognitive analytical tasks, is strong even after controlling for age, gender, race, education, sector and location (and hence for differential demand and supply shocks). Jobs subject to higher structural turnover rates are much more likely to be terminated, suggesting that easier-to-replace employees were at a particular disadvantage, even within sectors; at the same time, there is evidence of labor hoarding for more valuable matches. Individuals in low-skilled jobs fared comparatively better in industries with a high share of highskilled workers. JEL Codes: D22, E32, J23, J24, M51.', 'corpus_id': 231993412, 'score': 0}, {'doc_id': '231958242', 'title': 'Financial Frictions, Allocative Efficiency, and Unemployment: A Quantitative Analysis for Argentina∗', 'abstract': 'Argentina is characterized by low levels of private credit and persistent labor market rigidities. Furthermore, financial development remained stagnant in Argentina even during episodes of fast economic growth, in stark contrast with the experience of sustained growth accelerations around the world. The goals of the paper are twofold. Firstly, it is concerned with quantifying the productivity losses associated with such low levels of private credit penetration and characterizing its implications for different subsets of firms in the economy. The latter is important in light of various policy interventions aimed at mitigating the impact of low access to credit based on firm-size thresholds. Secondly, it studies the dynamics of hypothetical reforms to credit markets in a context of rigid labor markets, which seems to be the adequate scenario in which structural reforms will have to be implemented, given the stickiness that labor market regulations have shown to reform efforts in the past. It finds sizable productivity losses from financial frictions, in the order of 13%. At the micro level it finds that it is the youngest firms, whose average marginal return to capital is far above the riskfree rate in the economy, that are more prone to become financially constrained. Turning to reform scenarios, we investigate sudden reforms that are implemented abruptly and more plausible reform paths that gradually dismantle financial frictions. In the former, productivity and the investment rate rise sharply on impact, while it also does the rate of unemployment, going from 5 to almost 12%. In the latter, the rise of unemployment is more gradual and less sharp, peaking at 7%. On the flipside, the investment rate declines on impact, although the contraction is short-lived.', 'corpus_id': 231958242, 'score': 0}, {'doc_id': '45091696', 'title': 'Secular Stagnation? The Effect of Aging on Economic Growth in the Age of Automation', 'abstract': 'Several recent theories emphasize the negative effects of an aging population on economic growth, either because of the lower labor force participation and productivity of older workers or because aging will create an excess of savings over desired investment, leading to secular stagnation. We show that there is no such negative relationship in the data. If anything, countries experiencing more rapid aging have grown more in recent decades. We suggest that this counterintuitive finding might reflect the more rapid adoption of automation technologies in countries undergoing more pronounced demographic changes, and provide evidence and theoretical underpinnings for this argument.', 'corpus_id': 45091696, 'score': 1}, {'doc_id': '84834077', 'title': 'Automation and New Tasks: How Technology Displaces and Reinstates Labor', 'abstract': 'We present a framework for understanding the effects of automation and other types of technological changes on labor demand, and use it to interpret changes in US employment over the recent past. At the center of our framework is the allocation of tasks to capital and labor—the task content of production. Automation, which enables capital to replace labor in tasks it was previously engaged in, shifts the task content of production against labor because of a displacement effect. As a result, automation always reduces the labor share in value added and may reduce labor demand even as it raises productivity. The effects of automation are counterbalanced by the creation of new tasks in which labor has a comparative advantage. The introduction of new tasks changes the task content of production in favor of labor because of a reinstatement effect, and always raises the labor share and labor demand. We show how the role of changes in the task content of production—due to automation and new tasks—can be inferred from industry-level data. Our empirical decomposition suggests that the slower growth of employment over the last three decades is accounted for by an acceleration in the displacement effect, especially in manufacturing, a weaker reinstatement effect, and slower growth of productivity than in previous decades.', 'corpus_id': 84834077, 'score': 1}]"
84	Rituals	330cc3df850257c9dd23a0f7c25abc58	14379	{}	"[{'doc_id': '195776159', 'title': 'Adaptive Partitioning Design and Analysis for Emulation of a Complex Computer Code', 'abstract': 'Computer models are used as replacements for physical experiments in a large variety of applications. Nevertheless, direct use of the computer model for the ultimate scientific objective is often limited by the complexity and cost of the model. Historically, Gaussian process regression has proven to be the almost ubiquitous choice for a fast statistical emulator for such a computer model, due to its flexible form and analytical expressions for measures of predictive uncertainty. However, even this statistical emulator can be computationally intractable for large designs, due to computing time increasing with the cube of the design size. Multiple methods have been proposed for addressing this problem. We discuss several of them, and compare their predictive and computational performance in several scenarios. \nWe then propose solving this problem using an adaptive partitioning emulator (APE). The new approach is motivated by the idea that most computer models are only complex in particular regions of the input space. By taking a data-adaptive approach to the development of a design, and choosing to partition the space in the regions of highest variability, we obtain a higher density of points in these regions and hence accurate prediction.', 'corpus_id': 195776159, 'score': 0}, {'doc_id': '147313721', 'title': 'The Evolution and Ontogeny of Ritual', 'abstract': 'Convergent developments across social scientific disciplines provide evidence that ritual is a psychologically prepared, culturally inherited, behavioral trademark of our species. We draw on evidence from the anthropological and evolutionary science literatures to provide a psychological account of the social functions of ritual in group behavior. Solving the adaptive problems associated with group living requires psychological mechanisms for identifying group members, ensuring their commitment to the group, facilitating cooperation with their coalition, and maintaining group cohesion. We also examine evidence that the threat of social exclusion and loss of status motivates engagement in ritual throughout development and provides an account of the ontogeny of ritual cognition. The intersection of these lines of inquiry promises to provide new avenues for theory and research on the evolution and ontogeny of social group cognition. \n \n \nKeywords: \n \nritual; \ncultural evolution; \ncultural transmission; \ncooperation; \nsocial group cognition', 'corpus_id': 147313721, 'score': 1}, {'doc_id': '232327967', 'title': 'The Effects of Social Perception on Moral Judgment', 'abstract': 'When people express a moral judgment, others make inferences about their personality, such as whether they are warm or competent. People may use this interpersonal process to present themselves in a way that is socially acceptable in the current circumstances. Across four studies, we investigated this hypothesis in Chinese culture and showed that college student participants tended to associate others’ deontological moral judgments with warmth and utilitarian moral judgments with competence (Study 1, Mage = 21.1, SD = 2.45; Study 2, Mage = 20.53, SD = 1.87). In addition, participants made more deontological judgments after preparing to be interviewed for a job requiring them to be in a warm social role, and more utilitarian judgments after preparing for a job requiring them to be in a competent social role (Study 3, Mage = 19.5, SD = 1.63). This effect held true in moral dilemmas involving different degrees of hypothetical personal involvement, and appeared to be mediated by the perception of others’ expectations (Study 4, Mage = 19.92, SD = 1.97). The results suggest an important role for social cognition as an influence on moral judgments in Chinese culture.', 'corpus_id': 232327967, 'score': 0}, {'doc_id': '231991610', 'title': 'The Evolved Psychology of Psychedelic Set and Setting: Inferences Regarding the Roles of Shamanism and Entheogenic Ecopsychology', 'abstract': 'This review illustrates the relevance of shamanism and its evolution under effects of psilocybin as a framework for identifying evolved aspects of psychedelic set and setting. Effects of 5HT2 psychedelics on serotonin, stress adaptation, visual systems and personality illustrate adaptive mechanisms through which psychedelics could have enhanced hominin evolution as an environmental factor influencing selection for features of our evolved psychology. Evolutionary psychology perspectives on ritual, shamanism and psychedelics provides bases for inferences regarding psychedelics’ likely roles in hominin evolution as exogenous neurotransmitter sources through their effects in selection for innate dispositions for psychedelic set and setting. Psychedelics stimulate ancient brain structures and innate modular thought modules, especially self-awareness, other awareness, “mind reading,” spatial and visual intelligences. The integration of these innate modules are also core features of shamanism. Cross-cultural research illustrates shamanism is an empirical phenomenon of foraging societies, with its ancient basis in collective hominid displays, ritual alterations of consciousness, and endogenous healing responses. Shamanic practices employed psychedelics and manipulated extrapharmacological effects through stimulation of serotonin and dopamine systems and augmenting processes of the reptilian and paleomammalian brains. Differences between chimpanzee maximal displays and shamanic rituals reveal a zone of proximal development in hominin evolution. The evolution of the mimetic capacity for enactment, dance, music, and imitation provided central capacities underlying shamanic performances. Other chimp-human differences in ritualized behaviors are directly related to psychedelic effects and their integration of innate modular thought processes. Psychedelics and other ritual alterations of consciousness stimulate these and other innate responses such as soul flight and death-and-rebirth experiences. These findings provided bases for making inferences regarding foundations of our evolved set, setting and psychology. Shamanic setting is eminently communal with singing, drumming, dancing and dramatic displays. Innate modular thought structures are prominent features of the set of shamanism, exemplified in animism, animal identities, perceptions of spirits, and psychological incorporation of spirit others. A shamanic-informed psychedelic therapy includes: a preparatory set with practices such as sexual abstinence, fasting and dream incubation; a set derived from innate modular cognitive capacities and their integration expressed in a relational animistic worldview; a focus on internal imagery manifesting a presentational intelligence; and spirit relations involving incorporation of animals as personal powers. Psychedelic research and treatment can adopt this shamanic biogenetic paradigm to optimize set, setting and ritual frameworks to enhance psychedelic effects.', 'corpus_id': 231991610, 'score': 0}, {'doc_id': '146898991', 'title': 'The Social Functions of Group Rituals', 'abstract': 'Convergent developments across social scientific disciplines provide evidence that ritual is a psychologically prepared, culturally inherited, behavioral trademark of our species. We draw on evidence from the anthropological and evolutionary-science literatures to offer a psychological account of the social functions of ritual for group behavior. Solving the adaptive problems associated with group living requires psychological mechanisms for identifying group members, ensuring their commitment to the group, facilitating cooperation with coalitions, and maintaining group cohesion. The intersection of these lines of inquiry yields new avenues for theory and research on the evolution and ontogeny of social group cognition.', 'corpus_id': 146898991, 'score': 1}, {'doc_id': '2779137', 'title': 'The Psychology of Rituals: An Integrative Review and Process-Based Framework', 'abstract': 'Traditionally, ritual has been studied from broad sociocultural perspectives, with little consideration of the psychological processes at play. Recently, however, psychologists have begun turning their attention to the study of ritual, uncovering the causal mechanisms driving this universal aspect of human behavior. With growing interest in the psychology of ritual, this article provides an organizing framework to understand recent empirical work from social psychology, cognitive science, anthropology, behavioral economics, and neuroscience. Our framework focuses on three primary regulatory functions of rituals: regulation of (a) emotions, (b) performance goal states, and (c) social connection. We examine the possible mechanisms underlying each function by considering the bottom-up processes that emerge from the physical features of rituals and top-down processes that emerge from the psychological meaning of rituals. Our framework, by appreciating the value of psychological theory, generates novel predictions and enriches our understanding of ritual and human behavior more broadly.', 'corpus_id': 2779137, 'score': 1}, {'doc_id': '232278143', 'title': 'Commentary on Alessandroni and Rodríguez', 'abstract': 'The current target article provides a robust investigation of the “cultural character” of cognitive development. This investigation has both theoretical and empirical/ methodological aspects. Methodologically, the authors argue for a unit of analysis concerning the development of object knowledge that includes other agents engaged in communication with the infant (i.e., that includes the sociocultural aspects of the infants’ developmental environment). We agree with such a position and further illustrate its utility in our own analysis of the phenomenon of overimitation. With respect to the underlying theory, we agree with the arguments against strictly cognitivist frameworks (including those with a more recent “embodied” flavor), as well as the fundamental importance ascribed to sociality and culture. However, for some aspects of the pragmatics of the object paradigm we would suggest narrowing the scope about the necessity of culture for development while in other respects we would like to suggest possible elaborations or extensions. Perhaps most fundamentally, we suggest that the physical versus cultural split that frames the target article discussion is not as metaphysically fundamental as seems to be presupposed.', 'corpus_id': 232278143, 'score': 0}, {'doc_id': '3900485', 'title': 'Understanding and sharing intentions: The origins of cultural cognition', 'abstract': ""We propose that the crucial difference between human cognition and that of other species is the ability to participate with others in collaborative activities with shared goals and intentions: shared intentionality. Participation in such activities requires not only especially powerful forms of intention reading and cultural learning, but also a unique motivation to share psychological states with others and unique forms of cognitive representation for doing so. The result of participating in these activities is species-unique forms of cultural cognition and evolution, enabling everything from the creation and use of linguistic symbols to the construction of social norms and individual beliefs to the establishment of social institutions. In support of this proposal we argue and present evidence that great apes (and some children with autism) understand the basics of intentional action, but they still do not participate in activities involving joint intentions and attention (shared intentionality). Human children's skills of shared intentionality develop gradually during the first 14 months of life as two ontogenetic pathways intertwine: (1) the general ape line of understanding others as animate, goal-directed, and intentional agents; and (2) a species-unique motivation to share emotions, experience, and activities with other persons. The developmental outcome is children's ability to construct dialogic cognitive representations, which enable them to participate in earnest in the collectivity that is human cognition."", 'corpus_id': 3900485, 'score': 1}, {'doc_id': '232047414', 'title': 'The amoral atheist? A cross-national examination of cultural, motivational, and cognitive antecedents of disbelief, and their implications for morality', 'abstract': 'There is a widespread cross-cultural stereotype suggesting that atheists are untrustworthy and lack a moral compass. Is there any truth to this notion? Building on theory about the cultural, (de)motivational, and cognitive antecedents of disbelief, the present research investigated whether there are reliable similarities as well as differences between believers and disbelievers in the moral values and principles they endorse. Four studies examined how religious disbelief (vs. belief) relates to endorsement of various moral values and principles in a predominately religious (vs. irreligious) country (the U.S. vs. Sweden). Two U.S. M-Turk studies (Studies 1A and 1B, N = 429) and two large cross-national studies (Studies 2–3, N = 4,193), consistently show that disbelievers (vs. believers) are less inclined to endorse moral values that serve group cohesion (the binding moral foundations). By contrast, only minor differences between believers and disbelievers were found in endorsement of other moral values (individualizing moral foundations, epistemic rationality). It is also demonstrated that presumed cultural and demotivational antecedents of disbelief (limited exposure to credibility-enhancing displays, low existential threat) are associated with disbelief. Furthermore, these factors are associated with weaker endorsement of the binding moral foundations in both countries (Study 2). Most of these findings were replicated in Study 3, and results also show that disbelievers (vs. believers) have a more consequentialist view of morality in both countries. A consequentialist view of morality was also associated with another presumed antecedent of disbelief—analytic cognitive style.', 'corpus_id': 232047414, 'score': 0}, {'doc_id': '24586183', 'title': 'Evaluating ritual efficacy: Evidence from the supernatural', 'abstract': 'Rituals pose a cognitive paradox: although widely used to treat problems, rituals are causally opaque (i.e., they lack a causal explanation for their effects). How is the efficacy of ritual action evaluated in the absence of causal information? To examine this question using ecologically valid content, three studies (N=162) were conducted in Brazil, a cultural context in which rituals called simpatias are used to treat a great variety of problems ranging from asthma to infidelity. Using content from existing simpatias, experimental simpatias were designed to manipulate the kinds of information that influences perceptions of efficacy. A fourth study (N=68) with identical stimuli was conducted with a US sample to assess the generalizability of the findings across two different cultural contexts. The results provide evidence that information reflecting intuitive causal principles (i.e., repetition of procedures, number of procedural steps) and transcendental influence (i.e., presence of religious icons) affects how people evaluate ritual efficacy.', 'corpus_id': 24586183, 'score': 1}]"
85	Mars Colony	f08c2e910f0767f32e88b0cb6c1dfa11	1114	{}	"[{'doc_id': '212742268', 'title': 'How China is planning to go to Mars amid the coronavirus outbreak', 'abstract': 'The launch is on track for July, as Europe and Russia announce a two-year delay in their journey to the red planet. “The launch is so important politically that they will make it happen,” says US-based planetary geologist Raymond Arvidson.', 'corpus_id': 212742268, 'score': 1}, {'doc_id': '211572463', 'title': 'Achieving the required mobility in the solar system through direct fusion drive', 'abstract': 'Abstract To develop a spacefaring civilization, humankind must develop technologies which enable safe, affordable and repeatable mobility through the solar system. One such technology is nuclear fusion propulsion which is at present under study mostly as a breakthrough toward the first interstellar probes. The aim of the present paper is to show that direct fusion drive is even more important in human planetary exploration and constitutes the natural solution to the problem of exploring and colonizing the solar system.', 'corpus_id': 211572463, 'score': 1}, {'doc_id': '212644500', 'title': 'A New Approach for Macroscopic Analysis in order to improve the Technical and Economic Impacts of Urban Interchanges on Traffic Networks -- A Case Study', 'abstract': 'Perusing three important elements (economic, safety and traffic) is the overall objective of decision evaluation across all transport projects. In this study, we investigate the feasibility of development of city interchanges, and road connections for network users. In order to achieve this goal, a series of smaller goals are required including determining benefits, costs of implementing new highway interchanges, quantifying the effective parameters, quantifying the increase in fuel consumption, quantifying the reduction in travel time and growth in travel speeds. In this study, geometric advancement of Hakim highway, and Yadegar-e-Emam highway was investigated just Macro from cloverleaf intersection with a low capacity to three-level directional intersection and enhanced cloverleaf. For this purpose, the simulation was done by EMME/2 software. The results of the method of net present value (NPV) were evaluated economically, and the benefit and cost of each one was stated precisely in different years (%28 improvement). The sensitivity analysis indicated that the cost of fuel, cost of travel time, cost of accidents and cost of pollutants have the highest impact factor in this assessment respectively.', 'corpus_id': 212644500, 'score': 0}, {'doc_id': '211678182', 'title': 'Cleaner Production in Optimized Multivariate Networks: Operations Management through a Roll of Dice', 'abstract': 'The importance of supply chain management in analyzing and later catalyzing economic expectations while simultaneously prioritizing cleaner production aspects is a vital component of modern finance. Such predictions, though, are often known to be less than accurate due to the ubiquitous uncertainty plaguing most business decisions. Starting from a multi-dimensional cost function defining the sustainability of the supply chain (SC) kernel, this article outlines a 4-component SC module - environmental, demand, economic, and social uncertainties - each ranked according to its individual weight. Our mathematical model then assesses the viability of a sustainable business by first ranking the potentially stochastic variables in order of their subjective importance, and then optimizing the cost kernel, defined from a utility function. The model will then identify conditions (as equations) validating the sustainability of a business venture. The ranking is initially obtained from an Analytical Hierarchical Process; the resultant weighted cost function is then optimized to analyze the impact of market uncertainty based on our supply chain model. Model predictions are then ratified against SME data to emphasize the importance of cleaner production in business strategies.', 'corpus_id': 211678182, 'score': 0}, {'doc_id': '210839032', 'title': 'The Habitable Exoplanet Observatory (HabEx) Mission Concept Study Final Report', 'abstract': 'The Habitable Exoplanet Observatory, or HabEx, has been designed to be the Great Observatory of the 2030s. For the first time in human history, technologies have matured sufficiently to enable an affordable space-based telescope mission capable of discovering and characterizing Earthlike planets orbiting nearby bright sunlike stars in order to search for signs of habitability and biosignatures. Such a mission can also be equipped with instrumentation that will enable broad and exciting general astrophysics and planetary science not possible from current or planned facilities. HabEx is a space telescope with unique imaging and multi-object spectroscopic capabilities at wavelengths ranging from ultraviolet (UV) to near-IR. These capabilities allow for a broad suite of compelling science that cuts across the entire NASA astrophysics portfolio. HabEx has three primary science goals: (1) Seek out nearby worlds and explore their habitability; (2) Map out nearby planetary systems and understand the diversity of the worlds they contain; (3) Enable new explorations of astrophysical systems from our own solar system to external galaxies by extending our reach in the UV through near-IR. This Great Observatory science will be selected through a competed GO program, and will account for about 50% of the HabEx primary mission. The preferred HabEx architecture is a 4m, monolithic, off-axis telescope that is diffraction-limited at 0.4 microns and is in an L2 orbit. HabEx employs two starlight suppression systems: a coronagraph and a starshade, each with their own dedicated instrument.', 'corpus_id': 210839032, 'score': 1}, {'doc_id': '19398128', 'title': 'Water extraction on Mars for an expanding human colony.', 'abstract': ""In-situ water extraction is necessary for an extended human presence on Mars. This study looks at the water requirements of an expanding human colony on Mars and the general systems needed to supply that water from the martian atmosphere and regolith. The proposed combination of systems in order to supply the necessary water includes a system similar to Honeybee Robotics' Mobile In-Situ Water Extractor (MISWE) that uses convection, a system similar to MISWE but that directs microwave energy down a borehole, a greenhouse or hothouse type system, and a system similar to the Mars Atmospheric Resource Recovery System (MARRS). It is demonstrated that a large water extraction system that can take advantage of large deposits of water ice at site specific locations is necessary to keep up with the demands of a growing colony."", 'corpus_id': 19398128, 'score': 1}, {'doc_id': '212633620', 'title': 'Predicting the ecological outcomes of global consumption', 'abstract': 'Mapping pathways to achieving the sustainable development goals requires understanding and predicting how social, economic and political factors impact biodiversity. Trends in demography, economic growth, regional alliances and consumption behaviours can have profound effects on the environment by driving resource use and production. While these distant socio-economic drivers impact species and ecosystems at global scales, for example by driving greenhouse gas emissions and climate change, the most prevalent human impacts on biodiversity manifest through habitat loss and land use change decisions at finer scales. We provide the first integrated ecological-economic analysis pathway capable of supporting both national policy design challenges and global scale assessment of biodiversity risks posed by socio-economic drivers such as population growth, consumption and trade. To achieve this, we provide state-of-the-art integration of economic, land use, and biodiversity modelling, and illustrate its application using two case studies. We evaluate the national-level implications of change in trading conditions under a multi-lateral free trade agreement for the bird biodiversity of Vietnam. We review the implications for land-use and biodiversity under coupled socio-economic (Shared Socioeconomic Pathways) and climate (Resource Concentration Pathways) scenarios for Australia. Our study provides a roadmap for setting up high dimensional integrated analyses foe evaluating global priorities for protecting nature and livelihoods in vulnerable areas with the greatest conflicts for economic, social and environmental opportunities.', 'corpus_id': 212633620, 'score': 0}, {'doc_id': '131754984', 'title': 'An integrated economics model for ISRU in support of a Mars colony - initial status report', 'abstract': 'The aim of this effort is to develop an integrated set of risk-based financial and technical models to evaluate multiple Off-Earth Mining (OEM) scenarios. This quantitative, scenarioand simulation-based tool will help identify combinations of market variables, technical parameters, and policy levers that will enable the expansion of the global economy into the solar system and return economic benefits. Human ventures in space are entering a new phase in which missions formerly driven by government agencies are now being replaced by those led by commercial enterprises – in launch, satellite deployment, resupply of the International Space Station, and space tourism. In the not-too-distant future, commercial opportunities will also include the mining of asteroids, the Moon, and Mars. This investigation will examine the role of OEM in a growing space economy. (In this investigation, the term ‘mining’ is taken to embrace minerals, ice/water, and other in situ resources.) OEM can be the engine that drives the space economy, so it would be useful to understand what OEM market conditions and technology requirements are needed for that economy to prosper. These specific elements will be studied in the wider context of creating an economy that could ultimately support a sustainable Mars Colony. Such a colony will need in situ resources not only for its own survival, but to prosper and grow, it must create viable business ventures, essentially by fulfilling the demand for in situ resources from and on Mars. This investigation will focus on understanding the role and economic prospect for OEM associated with the Human Colonization of Mars (HCM).', 'corpus_id': 131754984, 'score': 1}, {'doc_id': '211204788', 'title': 'Economic Viability and Infrastructure Requirements for the Electrification of Highway Traffic', 'abstract': 'Battery electric vehicles are rapidly entering the market. Their success offers great opportunities for the decarbonization of the transport sector, but also pose new challenges to energy infrastructures. Public charging stations must be built and power grids may become congested. In this article, we analyze the optimal layout and operation of charging systems along highways using a high-resolution optimization model. We discuss the economic viability and identify potential roadblocks impeding a rapid build-up of electric mobility. We find that congestion of regional distribution grids becomes a serious issue already for a moderate market penetration of electric vehicles. While peak loads can be handled by battery electric storage systems, the grid connection fundamentally limits the total amount of cars that can be served per day. Our results further highlight the interdependency of different sectors and the importance of regional infrastructures during the transformation to a sustainable energy system. Given the long time period needed for the planning and realization of infrastructure measures, rapid decisions are imperative.', 'corpus_id': 211204788, 'score': 0}, {'doc_id': '211126741', 'title': 'Site-dependent levelized cost assessment for fully renewable Power-to-Methane systems', 'abstract': 'The generation of synthetic natural gas from renewable electricity enables long-term energy storage and provides clean fuels for transportation. In this article, we analyze fully renewable Power-to-Methane systems using a high-resolution energy system optimization model applied to two regions within Europe. The optimum system layout and operation depend on the availability of natural resources, which vary between locations and years. We find that much more wind than solar power is used, while the use of an intermediate battery electric storage system has little effects. The resulting levelized costs of methane vary between 0.24 and 0.30 Euro/kWh and the economic optimal utilization rate between 63% and 78%. We further discuss how the economic competitiveness of Power-to-Methane systems can be improved by the technical developments and by the use of co-products, such as oxygen and curtailed electricity. A sensitivity analysis reveals that the interest rate has the highest influence on levelized costs, followed by the investment costs for wind and electrolyzer stack.', 'corpus_id': 211126741, 'score': 0}]"
86	FantasticLR	5a8419bda8bd1ace98dd50230c5377c4	13843	{}	"[{'doc_id': '231986276', 'title': 'A Probabilistically Motivated Learning Rate Adaptation for Stochastic Optimization', 'abstract': 'Machine learning practitioners invest significant manual and computational resources in finding suitable learning rates for optimization algorithms. We provide a probabilistic motivation, in terms of Gaussian inference, for popular stochastic firstorder methods. As an important special case, it recovers the Polyak step with a general metric. The inference allows us to relate the learning rate to a dimensionless quantity that can be automatically adapted during training by a control algorithm. The resulting meta-algorithm is shown to adapt learning rates in a robust manner across a large range of initial values when applied to deep learning benchmark problems.', 'corpus_id': 231986276, 'score': 1}, {'doc_id': '1634298', 'title': 'Wireless Application Protocol (WAP) and Mobile Wireless Access', 'abstract': 'Abstract It is projected that by year-end 2002 we will have 400 million Internet subscribers and 600 million mobile phone users. As a result of this expansion, there will be a growing demand for wireless data services with a corresponding demand for quick access to information from any location — hence the watchwords of “any place, any where, any time.” While WAP does provide access to the Internet, a “killer” application has not yet made an appearance.', 'corpus_id': 1634298, 'score': 0}, {'doc_id': '232073314', 'title': 'A Theoretical analysis of early learning and memorization in a linear model', 'abstract': 'In this section, we formalize and substantiate the claims of Theorem 1. Theorem 1 has three parts, which we address in the following sections. First, in Section A.2, we show that the classifier makes progress during the early-learning phase: over the first T iterations, the gradient is well correlated with v and the accuracy on mislabeled examples increases. However, as noted in the main text, this early progress halts because the gradient terms corresponding to correctly labeled examples begin to disappear. We prove this rigorously in Section A.3, which shows that the overall magnitude of the gradient terms corresponding to correctly labeled examples shrinks over the first T iterations. Finally, in Section A.4, we prove the claimed asymptotic behavior: as t ! 1, gradient descent perfectly memorizes the noisy labels.', 'corpus_id': 232073314, 'score': 0}, {'doc_id': '144836264', 'title': 'Goals and perceived ability: Impact on student valuing, self-regulation, and persistence.', 'abstract': 'Abstract We examined the motivational patterns and self-regulatory activities of 119 students in introductory statistics. Toward the end of the course subjects were given a questionnaire which assessed perceived ability, goal orientation (learning and performance), valuing of statistics (intrinsic and extrinsic), and the extent to which subjects used self-regulatory activities such as goal-setting, self-monitoring, and task-appropriate cognitive strategies. Predictions from Dweck′s goal orientation theory were tested. The findings were generally consistent with the theoretical predictions; however, the predicted interaction of dominant goal orientation and perceived ability failed to emerge.', 'corpus_id': 144836264, 'score': 0}, {'doc_id': '204008515', 'title': 'On the adequacy of untuned warmup for adaptive optimization', 'abstract': 'Adaptive optimization algorithms such as Adam (Kingma & Ba, 2014) are widely used in deep learning. The stability of such algorithms is often improved with a warmup schedule for the learning rate. Motivated by the difficulty of choosing and tuning warmup schedules, Liu et al. (2019) propose automatic variance rectification of Adam\'s adaptive learning rate, claiming that this rectified approach (""RAdam"") surpasses the vanilla Adam algorithm and reduces the need for expensive tuning of Adam with warmup. In this work, we point out various shortcomings of this analysis. We then provide an alternative explanation for the necessity of warmup based on the magnitude of the update term, which is of greater relevance to training stability. Finally, we provide some ""rule-of-thumb"" warmup schedules, and we demonstrate that simple untuned warmup of Adam performs more-or-less identically to RAdam in typical practical settings. We conclude by suggesting that practitioners stick to linear warmup with Adam, with a sensible default being linear warmup over $2 / (1 - \\beta_2)$ training iterations.', 'corpus_id': 204008515, 'score': 1}, {'doc_id': '195874215', 'title': 'Towards Explaining the Regularization Effect of Initial Large Learning Rate in Training Neural Networks', 'abstract': ""Stochastic gradient descent with a large initial learning rate is a widely adopted method for training modern neural net architectures. Although a small initial learning rate allows for faster training and better test performance initially, the large learning rate achieves better generalization soon after the learning rate is annealed. Towards explaining this phenomenon, we devise a setting in which we can prove that a two layer network trained with large initial learning rate and annealing provably generalizes better than the same network trained with a small learning rate from the start. The key insight in our analysis is that the order of learning different types of patterns is crucial: because the small learning rate model first memorizes low noise, hard-to-fit patterns, it generalizes worse on higher noise, easier-to-fit patterns than its large learning rate counterpart. This concept translates to a larger-scale setting: we demonstrate that one can add a small patch to CIFAR-10 images that is immediately memorizable by a model with small initial learning rate, but ignored by the model with large learning rate until after annealing. Our experiments show that this causes the small learning rate model's accuracy on unmodified images to suffer, as it relies too much on the patch early on."", 'corpus_id': 195874215, 'score': 1}, {'doc_id': '233284212', 'title': 'FACTORIZED NEURAL LAYERS', 'abstract': 'Factorized layers—operations parameterized by products of two or more matrices—occur in a variety of deep learning contexts, including compressed model training, certain types of knowledge distillation, and multi-head selfattention architectures. We study how to initialize and regularize deep nets containing such layers, examining two simple, understudied schemes, spectral initialization and Frobenius decay, for improving their performance. The guiding insight is to design optimization routines for these networks that are as close as possible to that of their well-tuned, non-decomposed counterparts; we back this intuition with an analysis of how the initialization and regularization schemes impact training with gradient descent, drawing on modern attempts to understand the interplay of weight-decay and batch-normalization. Empirically, we highlight the benefits of spectral initialization and Frobenius decay across a variety of settings. In model compression, we show that they enable low-rank methods to significantly outperform both unstructured sparsity and tensor methods on the task of training low-memory residual networks; analogs of the schemes also improve the performance of tensor decomposition techniques. For knowledge distillation, Frobenius decay enables a simple, overcomplete baseline that yields a compact model from over-parameterized training without requiring retraining with or pruning a teacher network. Finally, we show how both schemes applied to multi-head attention lead to improved performance on both translation and unsupervised pre-training.', 'corpus_id': 233284212, 'score': 0}, {'doc_id': '232320578', 'title': 'How to decay your learning rate', 'abstract': 'Complex learning rate schedules have become an integral part of deep learning. We find empirically that common fine-tuned schedules decay the learning rate after the weight norm bounces. This leads to the proposal of ABEL: an automatic scheduler which decays the learning rate by keeping track of the weight norm. ABEL’s performance matches that of tuned schedules and is more robust with respect to its parameters. Through extensive experiments in vision, NLP, and RL, we show that if the weight norm does not bounce, we can simplify schedules even further with no loss in performance. In such cases, a complex schedule has similar performance to a constant learning rate with a decay at the end of training.', 'corpus_id': 232320578, 'score': 1}, {'doc_id': '232119481', 'title': 'Trade-offs of Local SGD at Scale: An Empirical Study', 'abstract': 'As datasets and models become increasingly large, distributed training has become a necessary component to allow deep neural networks to train in reasonable amounts of time. However, distributed training can have substantial communication overhead that hinders its scalability. One strategy for reducing this overhead is to perform multiple unsynchronized SGD steps independently on each worker between synchronization steps, a technique known as local SGD. We conduct a comprehensive empirical study of local SGD and related methods on a large scale image classification task. We find that performing local SGD comes at a price: lower communication costs (and thereby faster training) are accompanied by lower accuracy. This finding is in contrast from the smaller-scale experiments in prior work, suggesting that local SGD encounters challenges at scale. We further show that incorporating the slow momentum framework of Wang et al. (2020) consistently improves accuracy without requiring additional communication, hinting at future directions for potentially escaping this trade-off.', 'corpus_id': 232119481, 'score': 0}, {'doc_id': '211259030', 'title': 'The Early Phase of Neural Network Training', 'abstract': 'Recent studies have shown that many important aspects of neural network learning take place within the very earliest iterations or epochs of training. For example, sparse, trainable sub-networks emerge (Frankle et al., 2019), gradient descent moves into a small subspace (Gur-Ari et al., 2018), and the network undergoes a critical period (Achille et al., 2019). Here we examine the changes that deep neural networks undergo during this early phase of training. We perform extensive measurements of the network state and its updates during these early iterations of training, and leverage the framework of Frankle et al. (2019) to quantitatively probe the weight distribution and its reliance on various aspects of the dataset. We find that, within this framework, deep networks are not robust to reinitializing with random weights while maintaining signs, and that weight distributions are highly non-independent even after only a few hundred iterations. Despite this, pre-training with blurred inputs or an auxiliary self-supervised task can approximate the changes in supervised networks, suggesting that these changes are label-agnostic, though labels significantly accelerate this process. Together, these results help to elucidate the network changes occurring during this pivotal initial period of learning.', 'corpus_id': 211259030, 'score': 1}]"
87	tokenize	77e9ef3680c5518785ef0121d3884c3d	3840	{}	"[{'doc_id': '216144648', 'title': 'Multiple Segmentations of Thai Sentences for Neural Machine Translation', 'abstract': 'Thai is a low-resource language, so it is often the case that data is not available in sufficient quantities to train an Neural Machine Translation (NMT) model which perform to a high level of quality. In addition, the Thai script does not use white spaces to delimit the boundaries between words, which adds more complexity when building sequence to sequence models. In this work, we explore how to augment a set of English–Thai parallel data by replicating sentence-pairs with different word segmentation methods on Thai, as training data for NMT model training. Using different merge operations of Byte Pair Encoding, different segmentations of Thai sentences can be obtained. The experiments show that combining these datasets, performance is improved for NMT models trained with a dataset that has been split using a supervised splitting tool.', 'corpus_id': 216144648, 'score': 0}, {'doc_id': '13753208', 'title': 'Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates', 'abstract': 'Subword units are an effective way to alleviate the open vocabulary problems in neural machine translation (NMT). While sentences are usually converted into unique subword sequences, subword segmentation is potentially ambiguous and multiple segmentations are possible even with the same vocabulary. The question addressed in this paper is whether it is possible to harness the segmentation ambiguity as a noise to improve the robustness of NMT. We present a simple regularization method, subword regularization, which trains the model with multiple subword segmentations probabilistically sampled during training. In addition, for better subword sampling, we propose a new subword segmentation algorithm based on a unigram language model. We experiment with multiple corpora and report consistent improvements especially on low resource and out-of-domain settings.', 'corpus_id': 13753208, 'score': 1}, {'doc_id': '3643309', 'title': 'A Closer Look at Skip-gram Modelling', 'abstract': 'Data sparsity is a large problem in natural language processing that refers to the fact that language is a system of rare events, so varied and complex, that even using an extremely large corpus, we can never accurately model all possible strings of words. This paper examines the use of skip-grams (a technique where by n-grams are still stored to model language, but they allow for tokens to be skipped) to overcome the data sparsity problem. We analyze this by computing all possible skip-grams in a training corpus and measure how many adjacent (standard) n-grams these cover in test documents. We examine skip-gram modelling using one to four skips with various amount of training data and test against similar documents as well as documents generated from a machine translation system. In this paper we also determine the amount of extra training data required to achieve skip-gram coverage using standard adjacent tri-grams.', 'corpus_id': 3643309, 'score': 1}, {'doc_id': '216642136', 'title': 'Syntax-aware Data Augmentation for Neural Machine Translation', 'abstract': 'Data augmentation is an effective performance enhancement in neural machine translation (NMT) by generating additional bilingual data. In this paper, we propose a novel data augmentation enhancement strategy for neural machine translation. Different from existing data augmentation methods which simply choose words with the same probability across different sentences for modification, we set sentence-specific probability for word selection by considering their roles in sentence. We use dependency parse tree of input sentence as an effective clue to determine selecting probability for every words in each sentence. Our proposed method is evaluated on WMT14 English-to-German dataset and IWSLT14 German-to-English dataset. The result of extensive experiments show our proposed syntax-aware data augmentation method may effectively boost existing sentence-independent methods for significant translation performance improvement.', 'corpus_id': 216642136, 'score': 0}, {'doc_id': '52051958', 'title': 'SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing', 'abstract': 'This paper describes SentencePiece, a language-independent subword tokenizer and detokenizer designed for Neural-based text processing, including Neural Machine Translation. It provides open-source C++ and Python implementations for subword units. While existing subword segmentation tools assume that the input is pre-tokenized into word sequences, SentencePiece can train subword models directly from raw sentences, which allows us to make a purely end-to-end and language independent system. We perform a validation experiment of NMT on English-Japanese machine translation, and find that it is possible to achieve comparable accuracy to direct subword training from raw sentences. We also compare the performance of subword training and segmentation with various configurations. SentencePiece is available under the Apache 2 license at https://github.com/google/sentencepiece.', 'corpus_id': 52051958, 'score': 1}, {'doc_id': '214742998', 'title': 'Give your Text Representation Models some Love: the Case for Basque', 'abstract': 'Word embeddings and pre-trained language models allow to build rich representations of text and have enabled improvements across most NLP tasks. Unfortunately they are very expensive to train, and many small companies and research groups tend to use models that have been pre-trained and made available by third parties, rather than building their own. This is suboptimal as, for many languages, the models have been trained on smaller (or lower quality) corpora. In addition, monolingual pre-trained models for non-English languages are not always available. At best, models for those languages are included in multilingual versions, where each language shares the quota of substrings and parameters with the rest of the languages. This is particularly true for smaller languages such as Basque. In this paper we show that a number of monolingual models (FastText word embeddings, FLAIR and BERT language models) trained with larger Basque corpora produce much better results than publicly available versions in downstream NLP tasks, including topic classification, sentiment classification, PoS tagging and NER. This work sets a new state-of-the-art in those tasks for Basque. All benchmarks and models used in this work are publicly available.', 'corpus_id': 214742998, 'score': 0}, {'doc_id': '202583325', 'title': 'K-BERT: Enabling Language Representation with Knowledge Graph', 'abstract': 'Pre-trained language representation models, such as BERT, capture a general language representation from large-scale corpora, but lack domain-specific knowledge. When reading a domain text, experts make inferences with relevant knowledge. For machines to achieve this capability, we propose a knowledge-enabled language representation model (K-BERT) with knowledge graphs (KGs), in which triples are injected into the sentences as domain knowledge. However, too much knowledge incorporation may divert the sentence from its correct meaning, which is called knowledge noise (KN) issue. To overcome KN, K-BERT introduces soft-position and visible matrix to limit the impact of knowledge. K-BERT can easily inject domain knowledge into the models by equipped with a KG without pre-training by-self because it is capable of loading model parameters from the pre-trained BERT. Our investigation reveals promising results in twelve NLP tasks. Especially in domain-specific tasks (including finance, law, and medicine), K-BERT significantly outperforms BERT, which demonstrates that K-BERT is an excellent choice for solving the knowledge-driven problems that require experts.', 'corpus_id': 202583325, 'score': 1}, {'doc_id': '214713496', 'title': 'Learning Contextualized Sentence Representations for Document-Level Neural Machine Translation', 'abstract': 'Document-level machine translation incorporates inter-sentential dependencies into the translation of a source sentence. In this paper, we propose a new framework to model cross-sentence dependencies by training neural machine translation (NMT) to predict both the target translation and surrounding sentences of a source sentence. By enforcing the NMT model to predict source context, we want the model to learn ""contextualized"" source sentence representations that capture document-level dependencies on the source side. We further propose two different methods to learn and integrate such contextualized sentence embeddings into NMT: a joint training method that jointly trains an NMT model with the source context prediction model and a pre-training & fine-tuning method that pretrains the source context prediction model on a large-scale monolingual document corpus and then fine-tunes it with the NMT model. Experiments on Chinese-English and English-German translation show that both methods can substantially improve the translation quality over a strong document-level Transformer baseline.', 'corpus_id': 214713496, 'score': 0}, {'doc_id': '211133077', 'title': 'Incorporating BERT into Neural Machine Translation', 'abstract': 'The recently proposed BERT has shown great power on a variety of natural language understanding tasks, such as text classification, reading comprehension, etc. However, how to effectively apply BERT to neural machine translation (NMT) lacks enough exploration. While BERT is more commonly used as fine-tuning instead of contextual embedding for downstream language understanding tasks, in NMT, our preliminary exploration of using BERT as contextual embedding is better than using for fine-tuning. This motivates us to think how to better leverage BERT for NMT along this direction. We propose a new algorithm named BERT-fused model, in which we first use BERT to extract representations for an input sequence, and then the representations are fused with each layer of the encoder and decoder of the NMT model through attention mechanisms. We conduct experiments on supervised (including sentence-level and document-level translations), semi-supervised and unsupervised machine translation, and achieve state-of-the-art results on seven benchmark datasets. Our code is available at \\url{this https URL}.', 'corpus_id': 211133077, 'score': 0}, {'doc_id': '204949631', 'title': 'BPE-Dropout: Simple and Effective Subword Regularization', 'abstract': 'Subword segmentation is widely used to address the open vocabulary problem in machine translation. The dominant approach to subword segmentation is Byte Pair Encoding (BPE), which keeps the most frequent words intact while splitting the rare ones into multiple tokens. While multiple segmentations are possible even with the same vocabulary, BPE splits words into unique sequences; this may prevent a model from better learning the compositionality of words and being robust to segmentation errors. So far, the only way to overcome this BPE imperfection, its deterministic nature, was to create another subword segmentation algorithm (Kudo, 2018). In contrast, we show that BPE itself incorporates the ability to produce multiple segmentations of the same word. We introduce BPE-dropout - simple and effective subword regularization method based on and compatible with conventional BPE. It stochastically corrupts the segmentation procedure of BPE, which leads to producing multiple segmentations within the same fixed BPE framework. Using BPE-dropout during training and the standard BPE during inference improves translation quality up to 2.3 BLEU compared to BPE and up to 0.9 BLEU compared to the previous subword regularization.', 'corpus_id': 204949631, 'score': 1}]"
88	Zero Shot Learning	4c0064ec68df074be5693d8db4edd8af	7832	{}	"[{'doc_id': '220647579', 'title': 'Leveraging Seen and Unseen Semantic Relationships for Generative Zero-Shot Learning', 'abstract': 'Zero-shot learning (ZSL) addresses the unseen class recognition problem by leveraging semantic information to transfer knowledge from seen classes to unseen classes. Generative models synthesize the unseen visual features and convert ZSL into a classical supervised learning problem. These generative models are trained using the seen classes and are expected to implicitly transfer the knowledge from seen to unseen classes. However, their performance is stymied by overfitting, which leads to substandard performance on Generalized Zero-Shot learning (GZSL). To address this concern, we propose the novel LsrGAN, a generative model that Leverages the Semantic Relationship between seen and unseen categories and explicitly performs knowledge transfer by incorporating a novel Semantic Regularized Loss (SR-Loss). The SR-loss guides the LsrGAN to generate visual features that mirror the semantic relationships between seen and unseen classes. Experiments on seven benchmark datasets, including the challenging Wikipedia text-based CUB and NABirds splits, and Attribute-based AWA, CUB, and SUN, demonstrates the superiority of the LsrGAN compared to previous state-of-the-art approaches under both ZSL and GZSL. Code is available at https: // github. com/ Maunil/ LsrGAN', 'corpus_id': 220647579, 'score': 1}, {'doc_id': '221083344', 'title': 'Webly Supervised Semantic Embeddings for Large Scale Zero-Shot Learning', 'abstract': 'Zero-shot learning (ZSL) makes object recognition in images possible in absence of visual training data for a part of the classes from a dataset. When the number of classes is large, classes are usually represented by semantic class prototypes learned automatically from unannotated text collections. This typically leads to much lower performances than with manually designed semantic prototypes such as attributes. While most ZSL works focus on the visual aspect and reuse standard semantic prototypes learned from generic text collections, we focus on the problem of semantic class prototype design for large scale ZSL. More specifically, we investigate the use of noisy textual metadata associated to photos as text collections, as we hypothesize they are likely to provide more plausible semantic embeddings for visual classes if exploited appropriately. We thus make use of a source-based voting strategy to improve the robustness of semantic prototypes. Evaluation on the large scale ImageNet dataset shows a significant improvement in ZSL performances over two strong baselines, and over usual semantic embeddings used in previous works. We show that this improvement is obtained for several embedding methods, leading to state of the art results when one uses automatically created visual and text features.', 'corpus_id': 221083344, 'score': 1}, {'doc_id': '219283878', 'title': 'The surgical management for isolated scaphotrapeziotrapezoid (STT) osteoarthritis : a systematic review of the literature.', 'abstract': 'We performed a systematic review to find out the safety and efficacy of various procedures for isolated scaphotrapeziotrapezoid osteoarthritis. Eleven articles were included. The most common procedure was arthroplasty with pyrocarbon implant (28%), followed by resection of distal pole of scaphoid with proximal trapezium and trapezoid resection (18%). The other procedures included trapeziectomy with ligament reconstruction and tendon interposition (LRTI) (14%), arthroscopic resection of distal scaphoid (11%), trapezium and trapezoid resection with LRTI (10%) and arthrodesis (10%). Complications were noted in 18 (15%) patients. The most common complication (7.5%) was asymptomatic dorsal intercalated segmental instability (DISI) followed by dislocation of the pyrocarbon implant (3%). Fusion resulted in decreased range of motion and grip strength. The distal scaphoid resection was related to high rate of DISI. Although the pyrocarbon implant has a higher dislocation rate which requires revision surgery, this complication is avoidable with good surgical technique. Arthroplasty with pyrocarbon implant may be the first choice in younger patients.', 'corpus_id': 219283878, 'score': 0}, {'doc_id': '220525447', 'title': 'Deep learning models for representing out-of-vocabulary words', 'abstract': 'Communication has become increasingly dynamic with the popularization of social networks and applications that allow people to express themselves and communicate instantly. In this scenario, distributed representation models have their quality impacted by new words that appear frequently or that are derived from spelling errors. These words that are unknown by the models, known as out-of-vocabulary (OOV) words, need to be properly handled to not degrade the quality of the natural language processing (NLP) applications, which depend on the appropriate vector representation of the texts. To better understand this problem and finding the best techniques to handle OOV words, in this study, we present a comprehensive performance evaluation of deep learning models for representing OOV words. We performed an intrinsic evaluation using a benchmark dataset and an extrinsic evaluation using different NLP tasks: text categorization, named entity recognition, and part-of-speech tagging. Although the results indicated that the best technique for handling OOV words is different for each task, Comick, a deep learning method that infers the embedding based on the context and the morphological structure of the OOV word, obtained promising results.', 'corpus_id': 220525447, 'score': 1}, {'doc_id': '221370530', 'title': 'All About Knowledge Graphs for Actions', 'abstract': 'Current action recognition systems require large amounts of training data for recognizing an action. Recent works have explored the paradigm of zero-shot and few-shot learning to learn classifiers for unseen categories or categories with few labels. Following similar paradigms in object recognition, these approaches utilize external sources of knowledge (eg. knowledge graphs from language domains). However, unlike objects, it is unclear what is the best knowledge representation for actions. In this paper, we intend to gain a better understanding of knowledge graphs (KGs) that can be utilized for zero-shot and few-shot action recognition. In particular, we study three different construction mechanisms for KGs: action embeddings, action-object embeddings, visual embeddings. We present extensive analysis of the impact of different KGs in different experimental setups. Finally, to enable a systematic study of zero-shot and few-shot approaches, we propose an improved evaluation paradigm based on UCF101, HMDB51, and Charades datasets for knowledge transfer from models trained on Kinetics.', 'corpus_id': 221370530, 'score': 0}, {'doc_id': '220871211', 'title': 'Multi-label Zero-shot Classification by Learning to Transfer from External Knowledge', 'abstract': 'Multi-label zero-shot classification aims to predict multiple unseen class labels for an input image. It is more challenging than its single-label counterpart. On one hand, the unconstrained number of labels assigned to each image makes the model more easily overfit to those seen classes. On the other hand, there is a large semantic gap between seen and unseen classes in the existing multi-label classification datasets. To address these difficult issues, this paper introduces a novel multi-label zero-shot classification framework by learning to transfer from external knowledge. We observe that ImageNet is commonly used to pretrain the feature extractor and has a large and fine-grained label space. This motivates us to exploit it as external knowledge to bridge the seen and unseen classes and promote generalization. Specifically, we construct a knowledge graph including not only classes from the target dataset but also those from ImageNet. Since ImageNet labels are not available in the target dataset, we propose a novel PosVAE module to infer their initial states in the extended knowledge graph. Then we design a relational graph convolutional network (RGCN) to propagate information among classes and achieve knowledge transfer. Experimental results on two benchmark datasets demonstrate the effectiveness of the proposed approach.', 'corpus_id': 220871211, 'score': 1}, {'doc_id': '7244042', 'title': 'Train Once, Test Anywhere: Zero-Shot Learning for Text Classification', 'abstract': ""Zero-shot Learners are models capable of predicting unseen classes. In this work, we propose a Zero-shot Learning approach for text categorization. Our method involves training model on a large corpus of sentences to learn the relationship between a sentence and embedding of sentence's tags. Learning such relationship makes the model generalize to unseen sentences, tags, and even new datasets provided they can be put into same embedding space. The model learns to predict whether a given sentence is related to a tag or not; unlike other classifiers that learn to classify the sentence as one of the possible classes. We propose three different neural networks for the task and report their accuracy on the test set of the dataset used for training them as well as two other standard datasets for which no retraining was done. We show that our models generalize well across new unseen classes in both cases. Although the models do not achieve the accuracy level of the state of the art supervised models, yet it evidently is a step forward towards general intelligence in natural language processing."", 'corpus_id': 7244042, 'score': 1}, {'doc_id': '220446064', 'title': 'IITK at the FinSim Task: Hypernym Detection in Financial Domain via Context-Free and Contextualized Word Embeddings', 'abstract': 'In this paper, we present our approaches for the FinSim 2020 shared task on ""Learning Semantic Representations for the Financial Domain"". The goal of this task is to classify financial terms into the most relevant hypernym (or top-level) concept in an external ontology. We leverage both context-dependent and context-independent word embeddings in our analysis. Our systems deploy Word2vec embeddings trained from scratch on the corpus (Financial Prospectus in English) along with pre-trained BERT embeddings. We divide the test dataset into two subsets based on a domain rule. For one subset, we use unsupervised distance measures to classify the term. For the second subset, we use simple supervised classifiers like Naive Bayes, on top of the embeddings, to arrive at a final prediction. Finally, we combine both the results. Our system ranks 1st based on both the metrics, i.e., mean rank and accuracy.', 'corpus_id': 220446064, 'score': 0}, {'doc_id': '15222691', 'title': 'The dynamic modulation of GABA(A) receptor trafficking and its role in regulating the plasticity of inhibitory synapses.', 'abstract': 'Inhibition in the adult mammalian central nervous system (CNS) is mediated by γ-aminobutyric acid (GABA). The fast inhibitory actions of GABA are mediated by GABA type A receptors (GABA(A)Rs); they mediate both phasic and tonic inhibition in the brain and are the principle sites of action for anticonvulsant, anxiolytic, and sedative-hypnotic agents that include benzodiazepines, barbiturates, neurosteroids, and some general anesthetics. GABA(A)Rs are heteropentameric ligand-gated ion channels that are found concentrated at inhibitory postsynaptic sites where they mediate phasic inhibition and at extrasynaptic sites where they mediate tonic inhibition. The efficacy of inhibition and thus neuronal excitability is critically dependent on the accumulation of specific GABA(A)R subtypes at inhibitory synapses. Here we evaluate how neurons control the number of GABA(A)Rs on the neuronal plasma membrane together with their selective stabilization at synaptic sites. We then go on to examine the impact that these processes have on the strength of synaptic inhibition and behavior.', 'corpus_id': 15222691, 'score': 0}, {'doc_id': '220347289', 'title': 'Bayesian multilingual topic model for zero-shot cross-lingual topic identification', 'abstract': 'This paper presents a Bayesian multilingual topic model for learning language-independent document embeddings. Our model learns to represent the documents in the form of Gaussian distributions, thereby encoding the uncertainty in its covariance. We propagate the learned uncertainties through linear classifiers for zero-shot cross-lingual topic identification. Our experiments on 5 language Europarl and Reuters (MLDoc) corpora show that the proposed model outperforms multi-lingual word embedding and BiLSTM sentence encoder based systems with significant margins in the majority of the transfer directions. Moreover, our system trained under a single day on a single GPU with much lower amounts of data performs competitively as compared to the state-of-the-art universal BiLSTM sentence encoder trained on 93 languages. Our experimental analysis shows that the amount of parallel data improves the overall performance of embeddings. Nonetheless, exploiting the uncertainties is always beneficial.', 'corpus_id': 220347289, 'score': 0}]"
89	Endoscope reprocessing	f5664dec13a1bcb3da0c812ba0e6c06a	5694	{}	[{'doc_id': '221310838', 'title': 'Did granny know best? Evaluating the antibacterial, antifungal and antiviral efficacy of acetic acid for home care procedures', 'abstract': 'Background Acetic acid has been used to clean and disinfect surfaces in the household for many decades. The antimicrobial efficacy of cleaning procedures can be considered particularly important for young, old, pregnant, immunocompromised people, but may also concern other groups, particularly with regards to the COVID-19 pandemics. This study aimed to show that acetic acid exhibit an antibacterial and antifungal activity when used for cleaning purposes and is able to destroy certain viruses. Furthermore, a disinfecting effect of laundry in a simulated washing cycle has been investigated. Results At a concentration of 10% and in presence of 1.5% citric acid, acetic acid showed a reduction of >\u20095-log steps according to the specifications of DIN EN 1040 and DIN EN 1275 for the following microorganisms: P. aeruginosa , E. coli , S. aureus , L. monocytogenes , K. pneumoniae , E. hirae and A. brasiliensis . For MRSA a logarithmic reduction of 3.19 was obtained. Tests on surfaces according to DIN EN 13697 showed a complete reduction (>\u20095-log steps) for P. aeruginosa , E. coli , S. aureus , E. hirae , A. brasiliensis and C. albicans at an acetic acid concentration of already 5%. Virucidal efficacy tests according to DIN EN 14476 and DIN EN 16777 showed a reduction of ≥4-log-steps against the Modified Vaccinia virus Ankara (MVA) for acetic acid concentrations of 5% or higher. The results suggest that acetic acid does not have a disinfecting effect on microorganisms in a dosage that is commonly used for cleaning. However, this can be achieved by increasing the concentration of acetic acid used, especially when combined with citric acid. Conclusions Our results show a disinfecting effect of acetic acid in a concentration of 10% and in presence of 1.5% citric acid against a variety of microorganisms. A virucidal effect against enveloped viruses could also be proven. Furthermore, the results showed a considerable antimicrobial effect of acetic acid when used in domestic laundry procedures.', 'corpus_id': 221310838, 'score': 0}, {'doc_id': '221034718', 'title': 'SpyGlass application for duodenoscope working channel inspection: Impact on the microbiological surveillance', 'abstract': 'BACKGROUND Patient-ready duodenoscopes were designed with an assumed contamination rate of less than 0.4%; however, it has been reported that 5.4% of clinically used duodenoscopes remain contaminated with viable high-concern organisms despite following the manufacturer’s instructions. Visual inspection of working channels has been proposed as a quality control measure for endoscope reprocessing. There are few studies related to this issue. AIM To investigate the types, severity rate, and locations of abnormal visual inspection findings inside patient-ready duodenoscopes and their microbiological significance. METHODS Visual inspections of channels were performed in 19 patient-ready duodenoscopes using the SpyGlass visualization system in two endoscopy units of tertiary care teaching hospitals (Tri-Service General Hospital and National Taiwan University Hospital) in Taiwan. Inspections were recorded and reviewed to evaluate the presence of channel scratches, buckling, stains, debris, and fluids. These findings were used to analyze the relevance of microbiological surveillance. RESULTS Seventy-two abnormal visual inspection findings in the 19 duodenoscopes were found, including scratches (n = 10, 52.6%), buckling (n = 15, 78.9%), stains (n = 14, 73.7%), debris (n = 14, 73.7%), and fluids (n = 6, 31.6%). Duodenoscopes > 12 mo old had a significantly higher number of abnormal visual inspection findings than those ≤ 12 mo old (46 findings vs 26 findings, P < 0.001). Multivariable regression analyses demonstrated that the bending section had a significantly higher risk of being scratched, buckled, and stained, and accumulating debris than the insertion tube. Debris and fluids showed a significant positive correlation with microbiological contamination (P < 0.05). There was no significant positive Spearman’s correlation coefficient between negative bacterial cultures and debris, between that and fluids, and the concomitance of debris and fluids. This result demonstrated that the presence of fluid and debris was associated with positive cultures, but not negative cultures. Further multivariate analysis demonstrated that fluids, but not debris, is an independent factor for bacterial culture positivity. CONCLUSION In patient-ready duodenoscopes, scratches, buckling, stains, debris, and fluids inside the working channel are common, which increase the microbiological contamination susceptibility. The SpyGlass visualization system may be recommended to identify suboptimal reprocessing.', 'corpus_id': 221034718, 'score': 1}, {'doc_id': '221124748', 'title': 'Alternative Methods of Sterilization in Dental Practices Against COVID-19', 'abstract': 'SARS-CoV-2, and several other microorganisms, may be present in nasopharyngeal and salivary secretions in patients treated in dental practices, so an appropriate clinical behavior is required in order to avoid the dangerous spread of infections. COVID-19 could also be spread when patients touches a contaminated surface with infected droplets and then touch their nose, mouth, or eyes. It is time to consider a dental practice quite similar to a hospital surgery room, where particular attention should be addressed to problems related to the spreading of infections due to air and surface contamination. The effectiveness of conventional cleaning and disinfection procedures may be limited by several factors; first of all, human operator dependence seems to be the weak aspect of all procedures. The improvement of these conventional methods requires the modification of human behavior, which is difficult to achieve and sustain. As alternative sterilization methods, there are some that do not depend on the operator, because they are based on devices that perform the entire procedure on their own, with minimal human intervention. In conclusion, continued efforts to improve the traditional manual disinfection of surfaces are needed, so dentists should consider combining the use of proper disinfectants and no-touch decontamination technologies to improve sterilization procedures.', 'corpus_id': 221124748, 'score': 0}, {'doc_id': '220907227', 'title': 'Microbial contamination of powered air purifying respirators (PAPR) used during the COVID-19 pandemic: an in situ microbiological study', 'abstract': 'OBJECTIVE To determine whether internal components of powered air purifying respirators (PAPR) used during the Corona virus 2019 disease (COVID-19) pandemic are contaminated with bacteria, fungi and/or any viral material. DESIGN In situ microbiological study. SETTING Single NHS Trust, UK. OUTCOME MEASURES Growth of any bacteria or fungi, or positive polymerase chain reaction results for common respiratory viruses and severe acute respiratory syndrome coronavirus-2 (SARS-CoV-2) RESULTS 25 PAPR hoods were swabbed; ten (40%) returned positive results. Bacterial growth was detected on six hoods (bacillus simplex, kocuria rhizophilia, bacillus weihenstephensis, microcccus luteus and staphylococcus epidermidis); five of the hoods were positive for fungal growth (non-sporulating environmental mould, NSEM); all sampled hoods tested negative for both SARS-CoV-2 and an expanded panel of respiratory viruses. There was wide variation in the storage of cleaned hoods. CONCLUSION Despite following recommended cleaning procedures, bacteria and fungi can remain on the internal components of PAPR hoods, at levels significant enough to be swabbed and cultured. PAPR hoods have the potential to cross-infect wearers and patients and are used primarily by clinicians who fail to fit disposable FFP3 respirators; the female sex and non-Caucasian people are less likely to fit FFP3 respirators. The hoods tested cannot be adequately cleaned for use in high risk healthcare environments, PAPR hoods and tubes can act as fomites, and there are evident shortcomings in their provision.', 'corpus_id': 220907227, 'score': 0}, {'doc_id': '220334092', 'title': 'Reducing Anesthesia Workstation Contamination', 'abstract': 'Healthcare-associated infections are a source of morbidity and mortality in the United States and have been shown to be more preventable than current incidence. Anesthesia providers may be a source of and vector for some of these infections. Nurse anesthetists provide direct individual care for numerous patients daily, managing airways and invasive devices that contaminate hands with secretions which then may be transferred to the anesthesia workstation. Due to its complexity, the anesthesia machine is difficult to thoroughly clean and may become a reservoir for contaminants. The purpose of this paper will be to examine new interventions being explored to reduce the contamination of the anesthesia workstation. These interventions will include hand hygiene interventions, ultraviolet (UV) radiation for workstation disinfection, and anesthesia workstation barrier devices. Analysis of which interventions are the most effective may help to guide the direction of interventions to help reduce anesthesia machine contamination. REDUCING ANESTHESIA CONTAMINATION 3 Reducing Anesthesia Workstation Contamination Introduction The Centers for Disease Control and Prevention (CDC) estimated in 2018 that healthcareassociated infections (HAIs) affected at least one in 31 hospitalized patients (CDC, 2018). Similarly, according to the World Health Organization (WHO), the incidence of HAI in 2002 was at 4.5%, affecting 1.7 million patients and causing 99,000 deaths with an estimated financial impact of $6.5 billion in 2004 (WHO, 2009). The cost of HAIs in financial terms, patient mortality, and loss of quality of life cannot be underestimated. A meta-analysis by Umscheid et al. in 2011 of data from the National Nosocomial Infections Surveillance System (NNIS), the National Hospital Discharge Survey, and the American Hospital Association identified 1,737,125 total infections in 2002, with 98,987 deaths. Central line-associated blood stream infections (CLABSI) and ventilator associated pneumonia (VAP) accounted for two-thirds of deaths and had a five-fold increase in mortality compared to other HAIs. Costs per HAI ranged from a low of $5,600 with surgical site infections (SSIs) to a high of $110,800 for CLABSIs; with potential savings ranging from $115 million from catheter-associated urinary tract infections (CAUTIs) to $18.2 billion from CLABSIs. From evidence-based practice studies examined, it was estimated that certain percentages of various HAIs are preventable. CLABSI and CAUTI could potentially be reduced by 65-70%, while VAP and SSIs could reasonably be prevented in 55% of cases. (Umscheid et al., 2011). Another study, a multistate survey including 183 hospitals and 11,290 patients by Magill et al. in 2014 found a 4% incidence of HAIs and an 11.5% rate of mortality from HAI. Infections related to invasive devices such as VAP, CAUTI, and CLABSI accounted for 25.6% of HAIs, with SSIs accounting for another 21.8%. Median time until presentation of HAI was six days, REDUCING ANESTHESIA CONTAMINATION 4 and present on admission HAIs were 19.4% of the total, with 67.3% of these being SSIs (Magill et al., 2014). These factors may help obscure the role of anesthesia in contributing to infection. As providers constantly in direct contact with patients, nurse anesthetists are in a prime position to either be a significant vector for HAIs or to help find ways to solve this costly problem. These challenges illuminate the important task of anesthesia providers becoming involved in finding unique solutions to anesthesia’s role in propagating HAIs. Solutions that work in much of the hospital may not be effective for the operating room (OR) setting. In that vein, this paper will investigate the scope and nature of the problem of anesthesia workstation contamination as well as several avenues of solutions proposed to help reduce contamination. These include novel methods of hand hygiene customized for anesthesia providers, use of UV radiation to help decontaminate the complex permanent parts of the anesthesia workstation, and a novel cover system to help prevent any contamination of the anesthesia machine. Methods Articles for inclusion in this literature review were found by searching PubMed and CINAHL. Studies were chosen from within the last five years, with exceptions made for studies with great impact on the state of the literature that provided a basis for future study.', 'corpus_id': 220334092, 'score': 0}, {'doc_id': '221178141', 'title': 'The Research Gap in Non-tuberculous Mycobacterium (NTM) and Reusable Medical Devices', 'abstract': 'Patient infections with Non-tuberculous Mycobacterium (NTM) have been attributed to some reusable medical devices (1, 2), such as heater cooler devices (3–5), dental unit waterlines (6), bronchoscopes (7), and automated endoscope reprocessors (8, 9). Such incidents can be related to insufficient reprocessing or growth of resistant organisms. For example, NTM infections can arise from patient exposure to contaminated water from established biofilms in water systems, and in some cases, aerosolization of the contaminated water (10). These medical devices are regulated by the U.S. Food and Drug Administration (FDA) and the Agency seeks to better understand the mechanism bywhich devices can transmit NTM. SomeNTM-specific challenges include potentially years-long incubation period to clinical infection, subsequent difficulty identifying the bacteria to the species-level and extended duration of treatment (11). While general awareness around clinically relevant, rapidand slow-growing NTM and infections appears to have increased (11), a literature review of Mycobacterium research reveals the trend of testing rapid-growing Mycobacterium spp. as surrogates and extrapolating data to their slow-growing counterparts (12–14). However, with more than 150 known NTM species (15), questions about the applicability of surrogate data have been voiced for decades (11, 13). Comparative analyses between slowand rapid-growing NTM tend to be limited and in some cases, have demonstrated notable variation, as well as intra-species differences (13, 14, 16, 17). This raises questions about the applicability of using one species of NTM (herein referred to as surrogate NTM) in place of another for medical device testing. The topic of appropriate surrogates can be considered in many ways. However, in the context of this manuscript we will discuss the applicability of NTM surrogates for testing intermediateand high-level disinfection1 of, and aerosolization from, reusable medical devices in the context of specific NTM outbreaks. Note that this manuscript does not address or suggests any changes in wellestablished disinfection validation methods that are routinely used for medical device testing.', 'corpus_id': 221178141, 'score': 1}, {'doc_id': '215405822', 'title': 'MANAGEMENT OF ENDOSCOPIC ACCESSORIES, VALVES, AND WATER AND IRRIGATION BOTTLES IN THE GASTROENTEROLOGY SETTING.', 'abstract': 'Disclaimer The Society of Gastroenterology Nurses and Associates, Inc. (SGNA) presents this guideline for use in developing institutional policies, procedures, and/or protocols. Information contained in this guideline is based on current published data and current practice at the time of publication. The Society of Gastroenterology Nurses and Associates, Inc. assume no responsibility for the practices or recommendations of any member or other practitioner, or for the policies and practices of any practice setting. Nurses and associates function within the limits of state licensure, state nurse practice act, and/or institutional policy. The Society of Gastroenterology Nurses and Associates, Inc. does not endorse or recommend any commercial products, processes, or services. A commercial product, process, or service is recognized as being consumed by or used on patients.', 'corpus_id': 215405822, 'score': 1}, {'doc_id': '221137076', 'title': 'Does double high-level disinfection for duodenoscopes add any value? A mini-systematic review and meta-analysis', 'abstract': 'Duodenoscope-emerging infection especially drug-resistant bacteria is considered a major concern nowadays. Different approaches were attempted to overcome this problem, like double high-level disinfection procedure. We performed a systematic review and meta-analysis to evaluate risk difference for positive cultures from duodenoscopes between double high-level disinfection (dHLD) and single (standard) high-level disinfection (sHLD). A thorough literature search (in October and November 2019) for studies comparing dHLD and sHLD for duodenoscopes was performed by 3 researchers in the Web of Science, Scopus, PubMed, and Cochran databases. The search terms were “duodenoscope,” “ERCP endoscope,” “disinfection,” “sterilization,” and “reprocessing,” and only randomized clinical trials with the English language were accepted. Four trials were identified studying dHLD, and only 2 clinical trials comparing dHLD with standard sHLD were found reporting 6193 duodenoscope cultures. Overall sHLD cultures were 2972, and dHLD cultures were 3221; overall positive cultures were 140 in sHLD and 161 in dHLD. The results of a meta-analysis using the random-effect model showed no significant risk difference (RD) between the 2 procedures for duodenoscope positive cultures (p = 0.53, RD 0.003, 95% CI “− 0.007–0.013”). Double HLD offered no significant difference over single HLD for duodenoscope disinfection. An alternative strategy to overcome duodenoscope-transmitted infection is a big issue to be resolved.', 'corpus_id': 221137076, 'score': 1}, {'doc_id': '219326961', 'title': 'Evaluation of an electrostatic spray disinfectant technology for rapid decontamination of portable equipment and large open areas in the era of SARS-CoV-2', 'abstract': '\n               ABSTRACT\n               \n                  In the setting of the coronavirus disease 2019 pandemic, efficient methods are needed to decontaminate shared portable devices and large open areas such as waiting rooms. We found that wheelchairs, portable equipment, and waiting room chairs were frequently contaminated with potential pathogens. After minimal manual pre-cleaning of areas with visible soiling, application of a dilute sodium hypochlorite disinfectant using an electrostatic sprayer provided rapid and effective decontamination and eliminated the benign virus bacteriophage MS2 from inoculated surfaces.\n               \n            ', 'corpus_id': 219326961, 'score': 0}, {'doc_id': '221130443', 'title': 'Endoscope-Associated Infections (EAI): An Update and Future Directions', 'abstract': 'Kapil Gupta, MD Department of Medicine, University of Miami/JFK Medical Center Palm Beach Regional GME Consortium Internal Medicine Program, Atlantis, FL Tara Keihanian, MD Fellow, Division of Gastroenterology and Hepatology, Department of Medicine, University of Miami Miller School of Medicine, Miami, FL Amaninder S. Dhaliwal, MD Fellow, Division of Gastroenterology and Hepatology, University of Nebraska Medical Center, Omaha, NE James H. Tabibian, MD, PhD Health Sciences Clinical Associate Professor, David Geffen SOM at UCLA, Director of Endoscopy, Olive View-UCLA Medical Center, Los Angeles, CA Mohit Girotra MD FACP Associate Professor of Clinical Medicine, Division of Gastroenterology and Hepatology, University of Miami Miller School of Medicine, Miami, FL Kapil Gupta', 'corpus_id': 221130443, 'score': 1}]
90	Core Science	6c43305e9f8df421e53f2ab6d8c87af4	12304	{}	"[{'doc_id': '235282439', 'title': 'Degradation of Highly Filled Biocomposites Based on Synthetic Polymers and Natural Polysaccharides Under the Action of Climatic Weathering and Biodegradation (Review)', 'abstract': 'The abiotic and biological weathering of highly filled composites based on polyolefins and natural fillers was considered. It was found that majority of studies were devoted to artificial weathering of wood polymer composites (WPC) and its effect on composites properties. The biodegradability investigations of such materials were limited by outdoor weathering and model studies under laboratory conditions. The fungal decay was the main topical issue. In the case of natural filler composites (NFC) the biodegradation studies under the natural and laboratory conditions were discussed wider.', 'corpus_id': 235282439, 'score': 0}, {'doc_id': '235290464', 'title': 'Effect of Additive on Microstructure, Hydrophilicity and Ultrafiltration Performance of Polyethylene Terephthalate Membranes', 'abstract': 'Ultrafiltration is a pressure-driven separation process through a porous membrane that can separate particles or macromolecules from a solution. Ultrafiltration is mostly applied for water treatment processes in various industries such as pharmaceutical, chemical, food and beverage industries. Commercial ultrafiltration membranes are mostly fabricated from polymer materials such as polysulfone, polyethersulfone and cellulose acetate. These polymers are expensive, and for the time being they are not produced in Indonesia. In this work, polyethylene terephthalate (PET) bottle waste was used as the polymer material to prepare ultrafiltration membranes. PET is commonly used as a packaging material for foods and beverages due to its low price and excellent mechanical properties. The PET membranes were prepared via a phase-inversion technique using polyethylene glycol (PEG) as the additive. It was observed that the addition of PEG improved the flexibility and the hydrophilicity of the membranes. The water contact angle decreased to 61.5° by the addition of PEG. The microstructures of the membranes could be controlled by the molecular weight of the PEG. The result of the ultrafiltration experiment showed that the membrane with a high porosity, a large pore size and a high hydrophilicity exhibited a high water permeate flux. In the ultrafiltration experiment using a model feed solution of an aqueous solution containing Bovine Serum Albumin with a molecular weight of 66,000 Da, the membranes showed high rejection values of up to 90% with water permeate fluxes of 14.7 - 23.0 kg/(m2h).', 'corpus_id': 235290464, 'score': 0}, {'doc_id': '235436664', 'title': 'Bio-Based Polyurethane Networks Derived from Liquefied Sawdust', 'abstract': 'The utilization of forestry waste resources in the production of polyurethane resins is a promising green alternative to the use of unsustainable resources. Liquefaction of wood-based biomass gives polyols with properties depending on the reagents used. In this article, the liquefaction of forestry wastes, including sawdust, in solvents such as glycerol and polyethylene glycol was investigated. The liquefaction process was carried out at temperatures of 120, 150, and 170 °C. The resulting bio-polyols were analyzed for process efficiency, hydroxyl number, water content, viscosity, and structural features using the Fourier transform infrared spectroscopy (FTIR). The optimum liquefaction temperature was 150 °C and the time of 6 h. Comprehensive analysis of polyol properties shows high biomass conversion and hydroxyl number in the range of 238–815 mg KOH/g. This may indicate that bio-polyols may be used as a potential substitute for petrochemical polyols. During polyurethane synthesis, materials with more than 80 wt% of bio-polyol were obtained. The materials were obtained by a one-step method by hot-pressing for 15 min at 100 °C and a pressure of 5 MPa with an NCO:OH ratio of 1:1 and 1.2:1. Dynamical-mechanical analysis (DMA) showed a high modulus of elasticity in the range of 62–839 MPa which depends on the reaction conditions.', 'corpus_id': 235436664, 'score': 0}, {'doc_id': '136483406', 'title': 'Pressurised compressive chip pre-treatment of Norway spruce with a mill scale Impressafiner', 'abstract': 'Abstract Mill scale trials were performed to evaluate pressurised compressive chip pre-treatment with the Impressafiner installed in one of the thermomechanical pulp lines at Braviken paper mill (Holmen Paper AB). The aim of the study was to determine if earlier reported effects of the Impressafiner pre-treatment on spruce chips from pilot scale trials (i.e. energy reduction and extractives removal) could also be attained with the mill scale Impressafiner. The mill scale Impressafiner pre-treatment resulted in partial disintegration of chips into a material consisting of fragmented chips with cracks running along the longitudinal fibre axis. Splits or evidence for weaknesses were observed between the primary and secondary fibre walls of pre-treated chips. An increase in water uptake for pre-treated chips was also observed. The extractive content was reduced by up to 24% for pulps produced with pre-treated chips compared to pulps from untreated chips. Pulp produced from pre-treated chips had higher tensile- and tear indices, elongation and light scattering and lower freeness compared to pulps from untreated chips produced with the same total specific energy consumption. The total specific energy needed to reach a tensile index of 47 Nm/g was reduced by 120 kWh/bone dry ton (6%) with Impressafiner pre-treatment. A smaller refiner plate gap was needed to reach the same specific energy consumption for pre-treated chips compared to untreated chips.', 'corpus_id': 136483406, 'score': 1}, {'doc_id': '235126702', 'title': 'Microwave-induced in situ drug amorphization using a mixture of polyethylene glycol and polyvinylpyrrolidone.', 'abstract': 'The use of a mixture of polyethylene glycol (PEG) and polyvinylpyrrolidone (PVP) was investigated for microwave-induced in situ amorphization of celecoxib (CCX) inside compacts. Such amorphization requires the presence of a dipolar excipient in the formulation to ensure heating of the compact by absorption of the microwaves. Previously, the hygroscopic nature of PVP was exploited for this purpose. By exposing PVP-based compacts for set time intervals at defined relative humidity, controlled water sorption into the compacts was achieved. In the present study, PEG was proposed as the microwave absorbing excipient instead of water, to avoid the water sorption step. However, it was found that PEG alone melted upon exposure to microwave radiation and caused the compact to deform. Furthermore, CCX was found to recrystallize upon cooling in PEG-based formulations. Hence, a mixture of PEG and PVP was used, where the presence of PVP preserved the physical shape of the compact, and the physical state of the amorphous solid dispersion. To study the impact of the polymer mixture, different compact compositions of CCX, PEG and PVP were prepared. When exposing the compacts to microwave radiation, it was found that the PEG:PVP ratio was critical for in situ amorphization and that complete amorphization was only achieved above a certain temperature threshold.', 'corpus_id': 235126702, 'score': 0}, {'doc_id': '102885287', 'title': 'Mass balance of lipophilic extractives around impressafiner in mill and pilot scale', 'abstract': 'Abstract Removal of extractives from the pulp furnish is of great importance for the improvement of paper machine efficiency and also for reducing the energy consumption during the thermomechanical pulp refining process. Extractives can exist in many different forms in the process water; as colloidal particles, dissolved or attached to fines and fibres. It is therefore important to know in which form they exist in order to fully understand their behaviour. In this paper, we report on an evaluation of the removal of extractives released from chips of different raw materials pre-treated in an Impressafiner, in pilot and in mill scale. In pilot trial the raw materials used were loblolly pine and white spruce and in the mil scale trial the raw material used was norway spruce. The colloidal stability of extractives present in the pressate water from the Impressafiner and their flocculation behaviour by cationic polymers (CPAM and Poly-DADMAC) under different conditions was also investigated. Calculations of mass balances around an Impressafiner showed that it was possible to remove up to 40% of extractives before the refining process. The reduction in total extractives content was mainly due to released resin acids while fatty acids, triglycerides, steryl esters and sterols to a large extent remained in the wood chips after pre-treatment. The removal of extractives from pine was four times higher than from spruce chips. The results can be explained in terms of the extractive composition in the raw material and the morphological differences in the wood structure.', 'corpus_id': 102885287, 'score': 1}, {'doc_id': '39839038', 'title': 'Composition of root pressure exudate from conifers', 'abstract': 'Root pressure exudates collected from detopped seedlings of Douglas-fir, grand fir, noble fir, Pacific silver fir, ponderosa pine, lodgepole pine, and Engelmann spruce were analyzed for sugars, amino acids, organic acids, nitrogen, potassium, calcium and magnesium. Sugar concentrations ranged from 0.10 percent to 5 percent, and included glucose, sucrose, fructose, and two unknowns, possibly rhamnose, and ribose. Amino acid concentrations ranged from 0.02 to 0.10 percent, and organic acids from 0.01 to 0.06 percent. Aspartic acid, glutamic acid and glutamine were the main amino acids in the exudates of all species with lesser amounts of glycine, serine, asparagine, arginine, leucine, and alanine. Nitrogen concentrations ranged from 0.005 to 0.012 percent, potassium from 0.006 to 0.018 percent, and calcium from 0.001 to 0.005 percent. Magnesium concentration was 0.001 percent in all species. The pH of the exudates ranged from 5.3 to 5.7. Except for the relatively high concentrations of sugar in the exudates from grand fir and noble fir, there were no species-related differences in amounts of the various constituents.', 'corpus_id': 39839038, 'score': 1}, {'doc_id': '20875829', 'title': 'Acidic Solvolysis of Softwood in Recycled Polyethylene Glycol System', 'abstract': 'The acidic solvolysis of lignocellulose using a glycol solvent such as polyethylene glycol (PEG) is a promising process for separating its components and producing a valuable lignin product that can be used as thermoplastic and fusible materials. To decrease operational costs, a glycol solvent that is used as a solvolysis reagent must be recovered and reused. In the present study, PEG was recovered by the removal of water by evaporation from the supernatant after glycol lignin production by acidic solvolysis of Japanese cedar using PEG with an average molecular weight of 200 (PEG200). The recovered PEG200 worked as a solvolysis reagent and produced glycol lignin with appropriate yield. The thermomechanical analysis of glycol lignin from the fresh and recovered PEG200 systems exhibited two inflection points, which were assigned to a glass transition point (Tg) and a thermal softening point (Ts). The Ts of the glycol lignin from the recovered PEG200 system was higher than that from the fresh PEG200 system. These results suggest that the glycol lignin from the recovered PEG200 system had high thermostability as well as high thermal fusibility.', 'corpus_id': 20875829, 'score': 1}, {'doc_id': '72868729', 'title': 'Lucja Frey and the “Auriculo—temporal syndrome”', 'abstract': ""2 Frankel JP, Lees AJ, Kempster PA, Stern GM. Subcutaneous apomorphine in the treatment of Parkinson's disease J Neurol Neurosurg Psychiatry 1990;53:96-101. 3 Poewe W, Kleedorfer B, Gerstenbrand F, Oertel W Subcutaneous apomorphine in Parkinson's disease. Lancet 1988;i:943. 4 Pollak P. Champay AS, Gaio JM, Hommel M, Benabid AL, Perret J. Administration sous-cutanee d'apomorphine dans les fluctuations motrices de la maladie de Parkinson. Rev Neurol 1990;146:116-22. 5 Stibe CMH, Kempster PA, Lees AJ, Stern GM. Subcutaneous apomorphine in parkinsonian on-off fluctuations. Lancet 1988;i:403-6. 6 Durif F, Deffond D, Tournilhac M. Efficacv of sublingual apomorphine in Parkinson's disease. J7 N'eurol Neurosurg Psychiatry, 1990;53:1 105. 7 Gancher ST, Nutt JG, Woodward WR. Absorption of apomorphine by various routes in parkinsonism. Mov Dis 199 1;6:212-6. 8 Lees AJ, Montastruc JL, Turjanski N, Rascol 0, Kleedorfer B, Saint Paul H, Stern GM, Rascol A. Sublingual apomorphine and Parkinson's disease. J Neurol Neurosurg Psychiatry 1989;52:1440. 9 Hughes AJ, Webster R, Bovington M, Lees AJ, Stern GM. Sublingual apomorphine in the treatment of Parkinson's disease complicated by motor fluctuations. Clin Neuropharmacol 1991;14:556-61. 10 Fahn S, Elton RLL and members of the UPDRS committee. Unified Parkinson's Disease Rating Scale. In: Fahn S, Marsden CD, Goldstein M, Caine DB, eds. Recent developments in Parkinson's disease. NewYork: Macmillan, 1987:153-63 and appendices I, II. 11 Durif F, Jeanneau E, Serre-Debeauvais F, Deffond D, Eschalier A, Tournilhac M. Relationship between plasma concentration of sublingual apomorphine and motor score in Parkinson's disease. Eur 7 Clin Pharmacol 199 1;41 :493-94. 12 Montastruc JL, Rascol 0, Senard JM, et al. Sublingual apomorphine in Parkinson's disease: a clinical and pharmacokinetic study. Clin Neuropharmacol 1991; 14:432-37. 13 Cotzias GC, Papavasiliou PS, Tolosa ES, Mendez JS, BellMidura MM. Treatment of Parkinson's disease with aporphine. N EnglJi Med 1976;294:567-72. 14 Kleedorfer B, Turjanski N, Ryan R, Lees AJ, Milroy C, Stern GM. Intranasal apomorphine in Parkinson's disease. Neurology 1991;41:761-2."", 'corpus_id': 72868729, 'score': 0}, {'doc_id': '94708805', 'title': 'Polyethylene glycol (PEG) as a green solvent for carbon-carbon bond formation reactions☆', 'abstract': 'Abstract In recent years, polyethylene glycol (PEG) has attracted special attention as a green and inexpensive solvent in various chemical transformations. The current critical review deals with breakthrough achievements for the applications of PEG as an alternative reaction media well-known coupling reactions such as Suzuki, Heck, Stille, Sonogashira, Hiyama and some challenging organic reactions for carbon–carbon bond formation.', 'corpus_id': 94708805, 'score': 1}]"
91	AI Linguistics	466d94e681cfc0e3a3c8f5478aef0cf1	4729	{}	"[{'doc_id': '219424691', 'title': 'Patient Data-Sharing for AI: Ethical Challenges, Catholic Solutions', 'abstract': 'Recent news of Catholic and secular healthcare systems sharing electronic health record (EHR) data with technology companies for the purposes of developing artificial intelligence (AI) applications has drawn attention to the ethical and social challenges of such collaborations, including threats to patient privacy and confidentiality, undermining of patient consent, and lack of corporate transparency. Although the United States Catholic Conference of Bishops’ Ethical and Religious Directives for Health Care Services (ERDs) address collaborations between US Catholic healthcare providers and other entities, the ERDs do not adequately address the novel concerns seen in EHR data-sharing for AI development. Neither does the Health Insurance Portability and Accountability Act (HIPAA) privacy rule. This article describes ethical and social problems observed in recent patient data-sharing collaborations with AI companies and analyzes them in light of the guiding principles of the ERDs as well as the 2020 Rome Call to AI Ethics (RCAIE) document recently released by the Vatican. While both the ERDs and RCAIE guiding principles can inform future collaborations, we suggest that the next revision of the ERDs should consider addressing data-sharing and AI more directly. Summary: Electronic health record data-sharing with artificial intelligence developers presents unique ethical and social challenges that can be addressed with updated United States Catholic Conference of Bishops’ Ethical and Religious Directives and guidance from the Vatican’s 2020 Rome Call to AI Ethics.', 'corpus_id': 219424691, 'score': 0}, {'doc_id': '195006290', 'title': 'Metaphors We Live By', 'abstract': 'People use metaphors every time they speak. Some of those metaphors are literary - devices for making thoughts more vivid or entertaining. But most are much more basic than that - they\'re ""metaphors we live by"", metaphors we use without even realizing we\'re using them. In this book, George Lakoff and Mark Johnson suggest that these basic metaphors not only affect the way we communicate ideas, but actually structure our perceptions and understandings from the beginning. Bringing together the perspectives of linguistics and philosophy, Lakoff and Johnson offer an intriguing and surprising guide to some of the most common metaphors and what they can tell us about the human mind. And for this new edition, they supply an afterword both extending their arguments and offering a fascinating overview of the current state of thinking on the subject of the metaphor.', 'corpus_id': 195006290, 'score': 1}, {'doc_id': '2264085', 'title': 'CorMet: A Computational, Corpus-Based Conventional Metaphor Extraction System', 'abstract': ""CorMet is a corpus-based system for discovering metaphorical mappings between concepts. It does this by finding systematic variations in domain-specific selectional preferences, which are inferred from large, dynamically mined Internet corpora. Metaphors transfer structure from a source domain to a target domain, making some concepts in the target domain metaphorically equivalent to concepts in the source domain. The verbs that select for a concept in the source domain tend to select for its metaphorical equivalent in the target domain. This regularity, detectable with a shallow linguistic analysis, is used to find the metaphorical interconcept mappings, which can then be used to infer the existence of higher-level conventional metaphors. Most other computational metaphor systems use small, hand-coded semantic knowledge bases and work on a few examples. Although Cor Met's only knowledge base is Word Net (Fellbaum 1998) it can find the mappings constituting many conventional metaphors and in some cases recognize sentences instantiating those mappings. CorMet is tested on its ability to find a subset of the Master Metaphor List (Lakoff, Espenson, and Schwartz 1991)."", 'corpus_id': 2264085, 'score': 1}, {'doc_id': '235458124', 'title': 'WaveGrad 2: Iterative Refinement for Text-to-Speech Synthesis', 'abstract': 'This paper introduces WaveGrad 2, a non-autoregressive generative model for text-to-speech synthesis. WaveGrad 2 is trained to estimate the gradient of the log conditional density of the waveform given a phoneme sequence. The model takes an input phoneme sequence, and through an iterative refinement process, generates an audio waveform. This contrasts to the original WaveGrad vocoder which conditions on mel-spectrogram features, generated by a separate model. The iterative refinement process starts from Gaussian noise, and through a series of refinement steps (e.g., 50 steps), progressively recovers the audio sequence. WaveGrad 2 offers a natural way to trade-off between inference speed and sample quality, through adjusting the number of refinement steps. Experiments show that the model can generate high fidelity audio, approaching the performance of a state-of-the-art neural TTS system. We also report various ablation studies over different model configurations. Audio samples are available at https://wavegrad.github.io/v2.', 'corpus_id': 235458124, 'score': 1}, {'doc_id': '232185202', 'title': 'MERMAID: Metaphor Generation with Symbolism and Discriminative Decoding', 'abstract': 'Generating metaphors is a challenging task as it requires a proper understanding of abstract concepts, making connections between unrelated concepts, and deviating from the literal meaning. In this paper, we aim to generate a metaphoric sentence given a literal expression by replacing relevant verbs. Based on a theoretically-grounded connection between metaphors and symbols, we propose a method to automatically construct a parallel corpus by transforming a large number of metaphorical sentences from the Gutenberg Poetry corpus (CITATION) to their literal counterpart using recent advances in masked language modeling coupled with commonsense inference. For the generation task, we incorporate a metaphor discriminator to guide the decoding of a sequence to sequence model fine-tuned on our parallel data to generate high-quality metaphors. Human evaluation on an independent test set of literal statements shows that our best model generates metaphors better than three well-crafted baselines 66% of the time on average. A task-based evaluation shows that human-written poems enhanced with metaphors proposed by our model are preferred 68% of the time compared to poems without metaphors.', 'corpus_id': 232185202, 'score': 1}, {'doc_id': '237204793', 'title': 'On a Chomskyan postulation in conceptual metaphor theory', 'abstract': 'Abstract This paper is an attempt to make a comparison between Lakoff and Johnson’s conceptual metaphor theory and Chomsky’s transformational generative grammar, and to demonstrate a Chomskyan postulation in the former. Although Lakoff and Johnson regard Chomsky’s linguistics as a modern representative of traditional Western philosophies of language that tend to highlight the a priori assumptions rather than empirical findings, the cognitive theory of metaphor contains a Chomskyan metaphysical assumption as its most important notion, i.e. the assumption of conceptual metaphors. Thus, what the present paper wants to argue with ample evidence is that Lakoff and Johnson’s conceptual metaphor theory resembles Chomsky’s logic and that their notion of conceptual metaphors is very much a Chomskyan postulation. What the present study tries to further demonstrate is that the abovementioned two theories actually have many points in common, which also implies that Lakoff and Johnson have failed to avoid the paradigm that they believe is conflicting with their own.', 'corpus_id': 237204793, 'score': 1}, {'doc_id': '208011706', 'title': 'Spatial Frames of Reference in Miyako: Digging into Whorfian linguistic relativity', 'abstract': None, 'corpus_id': 208011706, 'score': 0}, {'doc_id': '237432347', 'title': 'Book review', 'abstract': 'This volume, edited by Ewa Dąbrowska from University of Birmingham and FAU Erlangen-Nürnberg and Dagmar Divjak from University of Birmingham, is the part of a three-volume set on Cognitive Linguistics published by Walter de Gruyter in 2019. As the first volume of this set, it discusses the cognitive processes and abilities of human beings which underlie language production, particularly concerning such concepts as embodiment, attention, and categorization, providing a state-of-the-art overview of the subfields in linguistics. Authors in this collection specially emphasize the direction of cognitive linguistic studies towards a more empirical, interdisciplinary, and social-oriented basis, and provide readers with insightful ideas and suggestions for future research in Cognitive Linguistics. To begin with, in the Introduction, the editors briefly introduce the assumptions, history, and current situation of Cognitive Linguistics, and give an outline of the topics in the three-volume set. In chapter 1, Benjamin Bergen expounds the historical conceptions of embodiment in Cognitive Science, describes some of the ways that embodiment has been used in Cognitive Linguistics, and discusses the directions that linguistic embodiment research is currently moving towards. According to the author, there have been three distinct phases in the application of the idea of embodiment to empirical work on language and cognition, containing the analytical phase, the process phase and the function phase. For the next chapter, Russell S. Tomlin and Andriy Myachykov review the evidence for a regular link between visual attention and syntactic organization. They propose that the grammatical role assignment mechanism and the positional assignment mechanism form a hierarchical dual-path system, which allows a grammatical representation of the perceptually salient referent in a sentence. In chapter 3, Dagmar Divjak and Catherine L. Caldwell-Harris present interpretations of frequency and entrenchment, and integrate perspectives from both Experimental Psychology and Cognitive Linguistics. They illustrate the origins of the interest in frequency and its applications, and also the review of the cognitive and neural mechanisms supporting language structures that vary in entrenchment. Categorization is discussed by Michael Ramscar and Robert Port in chapter 4. They suggest that human conceptual capabilities are systematic in that they are the products of a rich capacity to discriminate and learn systems of alternate responses (behaviors, affordances, words, etc.) and to use the systems in context, with a conclusion that conceptual knowledge is closely related to context of language use. As for chapter 5, R. Harald Baayen and Michael Ramscar explain three approaches that attempt to answer the question about the mechanism of structuring language, placing more emphasis on the process of abstraction, analogical reasoning, and basic principles of discrimination learning. In chapter 6, Ronald W. Langacker characterizes construal with reasonable precision and investigates their reprensentations in language. Five broad dimensions of construal are scrutinized, namely perspective, selection, promi-', 'corpus_id': 237432347, 'score': 0}, {'doc_id': '32708543', 'title': 'The NHS: Who is attacking, who is defending?', 'abstract': '7. Schensul, J. J. and Guest, B. H. (1994). Ethics, ethnicity, and health care reform. In, ""It Just Ain\'t Fair\'raThe Ethics of Health Care for African Americans, ed. by A. Dula and S. Goering, Praeger Publishers, Westport, CT. 8. Ferguson, W. (1994). The physician\'s responsibility to medically underserved poor people. In, \'It Just Ain\'t Fair\'--The Ethics of Health Care for African Americans, ed. by A. Dula and S. Goering, Praeger Publishers, Westport, CT. 9. Radical Health Statistics Group (1977). In Defence of the NHS, Radical Health Statistics Group, London. 10. Hahn, B. and Flood, A. B. (1995). No insurance, public insurance, and private insurance: do these options contribute to differences in general health? Journal of Health Care for the Poor and Underserved 6, 41-59. 11. Sorlie, P. D., Backlund, E. and Keller, J. B. (1995). USA mortality by economic, demographic, and social characteristics: the national longitudinal mortality study. American Journal of Public Health 85, 949-956. 12. Veatch, R. M. (1995). Abandoning informed consent. Hastings Center Report 25, 5-12.', 'corpus_id': 32708543, 'score': 0}, {'doc_id': '237239695', 'title': 'How to tear down the walls that separate linguists: Continuing the quest for clarity about general linguistics', 'abstract': 'The primary motivation for my target paper (“General linguistics must be based on universals”) was a perceived need for greater clarity in general linguistics. Many linguists seem to share the feeling that we often talk past each other because we understand terms like “theory”, “framework”, “explanation”, “analysis” and “description” in seemingly different ways. But even if different linguists (necessarily) prefer different methods, we need not disagree about these basic terms and concepts, and I think that some “walls” that separate linguists from different communities could be “torn down” if we became more aware of what unites all theoretical linguists: that we want to understand particular languages, and that we need them in order to understand Human Language in general. I am grateful to the commentators for their interesting contributions, and here I continue the discussion by responding to some of their remarks. To recapitulate, I made three key points in my target article:', 'corpus_id': 237239695, 'score': 0}]"
92	SPECTER	b5e48ef6e7d8f3ec65ed6d31839ecea7	5660	{}	[{'doc_id': '231662488', 'title': 'Evaluating Multilingual Text Encoders for Unsupervised Cross-Lingual Retrieval', 'abstract': 'Pretrained multilingual text encoders based on neural Transformer architectures, such as multilingual BERT (mBERT) and XLM, have achieved strong performance on a myriad of supervised language understanding tasks. Consequently, they have been adopted as a state-of-the-art paradigm for multilingual and cross-lingual representation learning and transfer, rendering cross-lingual word embeddings (CLWEs) effectively obsolete. However, questions remain to which extent this finding generalizes 1) to unsupervised settings and 2) for ad-hoc cross-lingual IR (CLIR) tasks. Therefore, in this work we present a systematic empirical study focused on the suitability of the state-of-the-art multilingual encoders for cross-lingual document and sentence retrieval tasks across a large number of language pairs. In contrast to supervised language understanding, our results indicate that for unsupervised document-level CLIR – a setup in which there are no relevance judgments for task-specific fine-tuning – the pretrained encoders fail to significantly outperform models based on CLWEs. For sentence-level CLIR, we demonstrate that state-of-the-art performance can be achieved. However, the peak performance is not met using the general-purpose multilingual text encoders ‘off-the-shelf’, but rather relying on their variants that have been further specialized for sentence understanding tasks.', 'corpus_id': 231662488, 'score': 0}, {'doc_id': '231709235', 'title': 'Evaluation of BERT and ALBERT Sentence Embedding Performance on Downstream NLP Tasks', 'abstract': 'Contextualized representations from a pre-trained language model are central to achieve a high performance on downstream NLP task. The pre-trained BERT and A Lite BERT (ALBERT) models can be fine-tuned to give state-of-the-art results in sentence-pair regressions such as semantic textual similarity (STS) and natural language inference (NLI). Although BERT-based models yield the [CLS] token vector as a reasonable sentence embedding, the search for an optimal sentence embedding scheme remains an active research area in computational linguistics. This paper explores on sentence embedding models for BERT and ALBERT. In particular, we take a modified BERT network with siamese and triplet network structures called Sentence-BERT (SBERT) and replace BERT with ALBERT to create Sentence-ALBERT (SALBERT). We also experiment with an outer CNN sentence-embedding network for SBERT and SALBERT. We evaluate performances of all sentence-embedding models considered using the STS and NLI datasets. The empirical results indicate that our CNN architecture improves ALBERT models substantially more than BERT models for STS benchmark. Despite significantly fewer model parameters, ALBERT sentence embedding is highly competitive to BERT in downstream NLP evaluations.', 'corpus_id': 231709235, 'score': 1}, {'doc_id': '231861432', 'title': 'Multi-Turn Dialogue Reading Comprehension With Pivot Turns and Knowledge', 'abstract': 'Multi-turn dialogue reading comprehension aims to teach machines to read dialogue contexts and solve tasks such as response selection and answering questions. The major challenges involve noisy history contexts and especial prerequisites of commonsense knowledge that is unseen in the given material. Existing works mainly focus on context and response matching approaches. This work thus makes the first attempt to tackle the above two challenges by extracting substantially important turns as pivot utterances and utilizing external knowledge to enhance the representation of context. We propose a pivot-oriented deep selection model (PoDS) on top of the Transformer-based language models for dialogue comprehension. In detail, our model first picks out the pivot utterances from the conversation history according to the semantic matching with the candidate response or question, if any. Besides, knowledge items related to the dialogue context are extracted from a knowledge graph as external knowledge. Then, the pivot utterances and the external knowledge are combined together with a well-designed mechanism for refining predictions. Experimental results on four dialogue comprehension benchmark tasks show that our proposed model achieves great improvements on baselines. A series of empirical comparisons are conducted to show how our selection strategies and the extra knowledge injection influence the results.', 'corpus_id': 231861432, 'score': 0}, {'doc_id': '231847008', 'title': 'Does She Wink or Does She Nod? A Challenging Benchmark for Evaluating Word Understanding of Language Models', 'abstract': 'Recent progress in pretraining language models on large corpora has resulted in significant performance gains on many NLP tasks. These large models acquire linguistic knowledge during pretraining, which helps to improve performance on downstream tasks via fine-tuning. To assess what kind of knowledge is acquired, language models are commonly probed by querying them with ‘fill in the blank’ style cloze questions. Existing probing datasets mainly focus on knowledge about relations between words and entities. We introduce WDLMPro (Word Definitions Language Model Probing) to evaluate word understanding directly using dictionary definitions of words. In our experiments, three popular pretrained language models struggle to match words and their definitions. This indicates that they understand many words poorly and that our new probing task is a difficult challenge that could help guide research on LMs in the future.', 'corpus_id': 231847008, 'score': 0}, {'doc_id': '231951580', 'title': 'Less is More: Pre-training a Strong Siamese Encoder Using a Weak Decoder', 'abstract': 'Many real-world applications use Siamese networks to efficiently match text sequences at scale, which require high-quality sequence encodings. This paper pre-trains language models dedicated to sequence matching in Siamese architectures. We first hypothesize that a representation is better for sequence matching if the entire sequence can be reconstructed from it, which, however, is unlikely to be achieved in standard autoencoders: A strong decoder can rely on its capacity and natural language patterns to reconstruct and bypass the needs of better sequence encodings. Therefore we propose a new self-learning method that pretrains the encoder with a weak decoder, which reconstructs the original sequence from the encoder’s [CLS] representations but is restricted in both capacity and attention span. In our experiments on web search and recommendation, the pre-trained SEED-Encoder, “SiamEsE oriented encoder by reconstructing from weak decoder”, shows significantly better generalization ability when fine-tuned in Siamese networks, improving overall accuracy and few-shot performances. Our code and models will be released.', 'corpus_id': 231951580, 'score': 1}, {'doc_id': '220347127', 'title': 'On-The-Fly Information Retrieval Augmentation for Language Models', 'abstract': 'Here we experiment with the use of information retrieval as an augmentation for pre-trained language models. The text corpus used in information retrieval can be viewed as form of episodic memory which grows over time. By augmenting GPT 2.0 with information retrieval we achieve a zero shot 15% relative reduction in perplexity on Gigaword corpus without any re-training. We also validate our IR augmentation on an event co-reference task.', 'corpus_id': 220347127, 'score': 1}, {'doc_id': '219635207', 'title': 'Performance Result for Detection of COVID-19 using Deep Learning', 'abstract': 'The 2019 novel coronavirus (COVID-19), which has sprawled fleetly among masses residing in distant nations, had a prefatory juncture in China. From both a safeness and a lucrative outlook, it has staggered the world with its hasty diffusion with conjectural vicious generic repercussions for the masses. Consequent to the escalating cases daily, there is a constricted fraction of COVID-19 inspection kits acquirable in healthcare institutions. Ergo, to obviate COVID-19 propagating betwixt masses, it is imperative to enforce an instinctive unveiling network as a prompt jack legging diagnosis appendage. The contemplated method embroils a convolutional neural networkbased model, namely ResNet50, concerted with a Fully Connected Layer (FCL), reinforced by Rectified Linear Unit (ReLU) for the unearthing of coronavirus pneumonia imparted sufferer by harnessing chest X-ray radiographs. The endorsed classification model, i.e. resnet50 affirmed by FCL and ReLU, compassed accuracy of 94% for unearthing COVID-19. When equated to diverse classification models, the purported model is preeminent. The aftereffect is premised on the attested X-ray images from the data appropriable in the arsenal of Kaggle.', 'corpus_id': 219635207, 'score': 0}, {'doc_id': '219619560', 'title': 'Improving effectiveness of different deep learning-based models for detecting COVID-19 from computed tomography (CT) images', 'abstract': 'Computerized Tomography (CT )has a prognostic role in the early diagnosis of COVID-19 due to it gives both fast and accurate results. This is very important to help decision making of clinicians for quick isolation and appropriate patient treatment. In this study, we combine methods such as segmentation, data augmentation and the generative adversarial network (GAN) to improve the effectiveness of learning models. We obtain the best performance with 99% accuracy for lung segmentation. Using the above improvements we get the highest rates in terms of accuracy (99.8%), precision (99.8%), recall (99.8%), f1-score (99.8%) and roc acu (99.9979%) with deep learning methods in this paper. Also we compare popular deep learning-based frameworks such as VGG16, VGG19, Xception, ResNet50, ResNet50V2, DenseNet121, DenseNet169, InceptionV3 and InceptionResNetV2 for automatic COVID-19 classification. The DenseNet169 amongst deep convolutional neural networks achieves the best performance with 99.8% accuracy. The second-best learner is InceptionResNetV2 with accuracy of 99.65%. The third-best learner is Xception and InceptionV3 with accuracy of 99.60%.', 'corpus_id': 219619560, 'score': 0}, {'doc_id': '215768677', 'title': 'SPECTER: Document-level Representation Learning using Citation-informed Transformers', 'abstract': 'Representation learning is a critical ingredient for natural language processing systems. Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power. For applications on scientific documents, such as classification and recommendation, accurate embeddings of documents are a necessity. We propose SPECTER, a new method to generate document-level embedding of scientific papers based on pretraining a Transformer language model on a powerful signal of document-level relatedness: the citation graph. Unlike existing pretrained language models, Specter can be easily applied to downstream applications without task-specific fine-tuning. Additionally, to encourage further research on document-level models, we introduce SciDocs, a new evaluation benchmark consisting of seven document-level tasks ranging from citation prediction, to document classification and recommendation. We show that Specter outperforms a variety of competitive baselines on the benchmark.', 'corpus_id': 215768677, 'score': 1}, {'doc_id': '230437704', 'title': 'Cross-Document Language Modeling', 'abstract': 'We introduce a new pretraining approach for language models that are geared to support multi-document NLP tasks. Our crossdocument language model (CD-LM) improves masked language modeling for these tasks with two key ideas. First, we pretrain with multiple related documents in a single input, via cross-document masking, which encourages the model to learn cross-document and long-range relationships. Second, extending the recent Longformer model, we pretrain with long contexts of several thousand tokens and introduce a new attention pattern that uses sequence-level global attention to predict masked tokens, while retaining the familiar local attention elsewhere. We show that our CD-LM sets new state-of-the-art results for several multi-text tasks, including crossdocument event and entity coreference resolution, paper citation recommendation, and documents plagiarism detection, while using a significantly reduced number of training parameters relative to prior works1.', 'corpus_id': 230437704, 'score': 1}]
93	Computer Vision for blood analysis	fb80e22ddcad557f3b5c5dc535b7d251	19811	{}	"[{'doc_id': '17324428', 'title': 'Segmentation and ABCD rule extraction for skin tumors classification', 'abstract': 'During the last years, computer vision-based diagnosis systems have been widely used in several hospitals and dermatology clinics, aiming at the early detection of malignant melanoma tumor, which is among the most frequent types of skin cancer. In this work, we present an automated diagnosis system based on the ABCD rule used in clinical diagnosis in order to discriminate benign from malignant skin lesions. First, to reduce the influence of small structures, a preprocessing step based on morphological and fast marching schemes is used. In the second step, an unsupervised approach for lesion segmentation is proposed. Iterative thresholding is applied to initialize level set automatically. As the detection of an automated border is an important step for the correctness of subsequent phases in the computerized melanoma recognition systems, we compare its accuracy with growcut and mean shift algorithms, and discuss how these results may influence in the following steps: the feature extraction and the final lesion classification. Relying on visual diagnosis four features: Asymmetry (A), Border (B), Color (C) and Diversity (D) are computed and used to construct a classification module based on artificial neural network for the recognition of malignant melanoma. This framework has been tested on a dermoscopic database [16] of 320 images. The classification results show an increasing true detection rate and a decreasing false positive rate.', 'corpus_id': 17324428, 'score': 0}, {'doc_id': '14443741', 'title': 'A novel approach to evaluate blood parameters using computer vision techniques', 'abstract': 'Current medical practice for determining hemoglobin concentration (which is especially important for anemic patients in need of blood transfusion) involves frequent blood tests. In this work, we propose an alternative, non-invasive approach to hemoglobin estimation, based on image analysis of a specific conjunctival region. Our ultimate goal is to develop an easy-to-use wearable device that patients themselves can employ at home to autonomously assess their need of blood transfusion. In this paper, we detail the prototype of our device and the methodology for extracting key information from the color values of the acquired image. Tests conducted on 77 anemic and healthy patients show significant correlation between the real hemoglobin value obtained through blood sampling and the value estimated by our algorithm. A prototypical binary classification algorithm for assessing the need of blood transfusion yielded good results in terms of accuracy, specificity and sensitivity, thus making it possible to avoid a significant number of blood tests.', 'corpus_id': 14443741, 'score': 1}, {'doc_id': '235742759', 'title': 'Automated age-related macular degeneration area estimation - first results', 'abstract': 'This work aims to research an automatic method for detecting Age-related Macular Degeneration (AMD) lesions in RGB eye fundus images. For this, we align invasively obtained eye fundus contrast images (the ""golden standard"" diagnostic) to the RGB ones and use them to hand-annotate the lesions. This is done using our custom-made tool. Using the data, we train and test five different convolutional neural networks: a custom one to classify healthy and AMD-affected eye fundi, and four well-known networks: ResNet50, ResNet101, MobileNetV3, and UNet to segment (localize) the AMD lesions in the affected eye fundus images. We achieve 93.55% accuracy or 69.71% Dice index as the preliminary best results in segmentation with MobileNetV3.', 'corpus_id': 235742759, 'score': 0}, {'doc_id': '235606421', 'title': 'Multi-Class Classification of Blood Cells - End to End Computer Vision based diagnosis case study', 'abstract': 'The diagnosis of blood-based diseases often involves identifying and characterizing patient blood samples. Automated methods to detect and classify blood cell subtypes have important medical applications. Automated medical image processing and analysis offers a powerful tool for medical diagnosis. In this work we tackle the problem of white blood cell classification based on the morphological characteristics of their outer contour, color. The work we would explore a set of preprocessing and segmentation (Color-based segmentation, Morphological processing, contouring) algorithms along with a set of features extraction methods ( Corner detection algorithms and Histogram of Gradients (HOG)), dimentionality reduction algorithms (Principal Component Analysis (PCA)) that are able to recognize and classify through various Unsupervised (k-nearest neighbors) and Supervised (Support Vector Machine, Decision Trees, Linear Discriminant Analysis, Quadratic Discriminant Analysis, Näıve Bayes) algorithms different categories of white blood cells to Eosinophil, Lymphocyte, Monocyte, and Neutrophil. We even take a step forwards to explore various Deep Convolutional Neural network architecture (Sqeezent, MobilenetV1, MobilenetV2, InceptionNet etc.) without preprocessing/segmentation and with preprocessing. We would like to explore many algorithms to identify the robust algorithm with least time complexity and low resource requirement. The outcome of this work can be a cue to selection of algorithms as per requirement for automated blood cell classification. ar X iv :2 10 6. 12 54 8v 1 [ cs .C V ] 2 3 Ju n 20 21', 'corpus_id': 235606421, 'score': 1}, {'doc_id': '235683217', 'title': 'Automated Onychomycosis Detection Using Deep Neural Networks', 'abstract': 'Clinical dermatology, still relies heavily on manual introspection of fungi within a Potassium Hydroxide (KOH) solution using a brightfield microscope. However, this method takes a long time, is based on the experience of the clinician, and has a low accuracy. With the increase of neural network applications in the field of clinical microscopy it is now possible to automate such manual processes increasing both efficiency and accuracy. This study presents a deep neural network structure that enables the rapid solutions for these problems and can perform automatic fungi detection in grayscale images without colorants. Microscopic images of 81 fungi and 235 ceratine were collected. Then, smaller patches were extracted containing 2062 fungi and 2142 ceratine. In order to detect fungus and ceratine, two models were created one of which was a custom neural network and the other was based on the VGG16 architecture. The developed custom model had 99.84% accuracy, and an area under the curve (AUC) value of 1.00, while the VGG16 model had 98.89% accuracy and an AUC value of 0.99. However, average accuracy and AUC value of clinicians is 72.8% and 0.87 respectively. This deep learning model allows the development of an automated system that can detect fungi within microscopic images.', 'corpus_id': 235683217, 'score': 1}, {'doc_id': '235592787', 'title': 'Creating A New Color Space utilizing PSO and FCM to Perform Skin Detection by using Neural Network and ANFIS', 'abstract': 'Skin color detection is an essential required step in various applications related to computer vision. These applications will include face detection, finding pornographic images in movies and photos, finding ethnicity, age, diagnosis, and so on. Therefore, proposing a proper skin detection method can provide solution to several problems. In this study, first a new color space is created using FCM and PSO algorithms. Then, skin classification has been performed in the new color space utilizing linear and nonlinear modes. Additionally, it has been done in RGB and LAB color spaces by using ANFIS and neural network. Skin detection in RBG color space has been performed using Mahalanobis distance and Euclidean distance algorithms. The BAO database is used for training phase of all the state-of-the-art methods that have been used for comparison. Skin detection’s results in new proposed color space utilizing non-linear conversion by ANFIS classification has shown the highest accuracy with 89.22% in compare with other nine recent methods investigated in this paper. In comparison, this method has 18.38% higher accuracy than the most accurate method on the same database. Additionally, this method has achieved 90.05% in equal error rate (1-EER) in testing COMPAQ dataset and 92.93% accuracy in testing Pratheepan dataset, which compared to the previous method on COMPAQ database, 1-EER has increased by %0.87.', 'corpus_id': 235592787, 'score': 0}, {'doc_id': '235606465', 'title': 'Diabetic Retinopathy Detection using Ensemble Machine Learning', 'abstract': 'Diabetic Retinopathy (DR) is among the world’s leading vision loss causes in diabetic patients. DR is a microvascular disease that affects the eye retina, which causes vessel blockage and therefore cuts the main source of nutrition for the retina tissues. Treatment for this visual disorder is most effective when it is detected in its earliest stages, as severe DR can result in irreversible blindness. Nonetheless, DR identification requires the expertise of Ophthalmologists which is often expensive and time-consuming. Therefore, automatic detection systems were introduced aiming to facilitate the identification process, making it available globally in a time and cost-efficient manner. However, due to the limited reliable datasets and medical records for this particular eye disease, the obtained predictions’ accuracies were relatively unsatisfying for eye specialists to rely on them as diagnostic systems. Thus, we explored an ensemble-based learning strategy, merging a substantial selection of well-known classification algorithms in one sophisticated diagnostic model. The proposed framework achieved the highest accuracy rates among all other common classification algorithms in the area. 4 subdatasets were generated to contain the top 5 and top 10 features of the Messidor dataset, selected by InfoGainEval. and WrapperSubsetEval., accuracies of 70.7% and 75.1% were achieved on the InfoGainEval. top 5 and original dataset respectively. The results imply the impressive performance of the subdataset, which significantly conduces to a less complex classification process when compared to the original complete Messidor dataset.', 'corpus_id': 235606465, 'score': 0}, {'doc_id': '235702945', 'title': 'Oxygen Saturation Estimation Based on Optimal Band Selection from Multi-band Video', 'abstract': 'In this study, we propose a method to estimate oxygen saturation by selecting the best bands from video images captured by a multiband camera. Oxygen saturation is one of the most important bioindicators for measuring human health. For example, when a person contracts COVID-19, which is currently prevalent, oxygen uptake does not work properly and oxygen saturation drops without the person being aware of it, which may lead to severe symptoms. Monitoring oxygen saturation is very important so that the person receives treatment before such a situation occurs. The commonly used contact sensor is uncomfortable because of its pressure and it is difficult to wear on a daily basis, so non-contact estimation of oxygen saturation is desirable. To estimate oxygen saturation using a contact sensor, the difference in the absorption coefficients of oxidized hemoglobin and deoxidized hemoglobin is used. Using the same principle, it is possible to estimate oxygen saturation without contact using the signals from two channels obtained by an RGB camera. Currently, many smartphones are equipped with infrared cameras for face recognition, and increasingly more models are equipped with multi-camera systems consisting of RGB and infrared cameras. In such cases, it is difficult to take advantage of the multiple bands because the optimal combination of bands for oxygen saturation estimation varies depending on the imaging environment and the subject. In this study, to select the optimal combination of bands from multi-band video images, we used a Monte Carlo simulation of light scattering on the skin to simulate pulse waves during oxygen saturation changes while measuring the signals with a multi-band camera. We further propose a method to select the most accurate combination for estimating the oxygen saturation based on the features obtained from the pulse wave.', 'corpus_id': 235702945, 'score': 1}, {'doc_id': '235265859', 'title': 'Analysis of Vision-based Abnormal Red Blood Cell Classification', 'abstract': 'Identification of abnormalities in red blood cells (RBC) is key to diagnosing a range of medical conditions from anaemia to liver disease. Currently this is done manually, a time-consuming and subjective process. This paper presents an automated process utilising the advantages of machine learning to increase capacity and standardisation of cell abnormality detection, and its performance is analysed. Three different machine learning technologies were used: a Support Vector Machine (SVM), a classical machine learning technology; TabNet, a deep learning architecture for tabular data; U-Net, a semantic segmentation network designed for medical image segmentation. A critical issue was the highly imbalanced nature of the dataset which impacts the efficacy of machine learning. To address this, synthesising minority class samples in feature space was investigated via Synthetic Minority Over-sampling Technique (SMOTE) and cost-sensitive learning. A combination of these two methods is investigated to improve the overall performance. These strategies were found to increase sensitivity to minority classes. The impact of unknown cells on semantic segmentation is demonstrated, with some evidence of the model applying learning of labelled cells to these anonymous cells. These findings indicate both classical models and new deep learning networks as promising methods in automating RBC abnormality detection.', 'corpus_id': 235265859, 'score': 1}]"
94	Test Research Feed (VR Manipulation)	0561161d185697e87eb589e54a0cfa7a	13795	{}	"[{'doc_id': '216033776', 'title': 'Social VR: A New Medium for Remote Communication and Collaboration', 'abstract': 'There is a growing need for effective remote communication, which has many positive societal impacts, such as reducing environmental pollution and travel costs, supporting rich collaboration by remotely connecting talented people. Social Virtual Reality (VR) invites multiple users to join a collaborative virtual environment, which creates new opportunities for remote communication. The goal of social VR is not to completely replicate reality, but to facilitate and extend the existing communication channels of the physical world. Apart from the benefits provided by social VR, privacy concerns and ethical risks are raised when the boundary between the real and the virtual world is blurred. This workshop is intended to spur discussions regarding technology, evaluation protocols, application areas, research ethics and legal regulations for social VR as an emerging immersive remote communication tool.', 'corpus_id': 216033776, 'score': 1}, {'doc_id': '122717061', 'title': 'Microbubble oscillations and stability for drug delivery', 'abstract': 'Ultrasound contrast agents have been developed from micron size bubbles whose gas core is enclosed by a polymer, lipid or protein shell. Furthermore, specific designs have been developed for drug delivery in which the ultrasound contrast agent acts as drug delivery vehicle. A drug may be suspended in the shell of these agents which is released at a particular site as the microbubble undergoes destruction. Localized delivery depends on the destruction of a sufficient number of bubbles within a confined geometry typically a capillary or small vessel. Experimental evidence suggests the composition and thickness of the shell play important role in the break‐up. Break‐up scenarios of buckling have been suggested for elastic shell agents and shape stability for fluid shell agents. However, these have not been rigorously examined theoretically. The formulations of contrast agent designed for drug delivery are highlighted including a description of a double polymeric layer design. Shape stability equations for fl...', 'corpus_id': 122717061, 'score': 0}, {'doc_id': '231602925', 'title': 'Proxemics and Social Interactions in an Instrumented Virtual Reality Workshop', 'abstract': 'Virtual environments (VEs) can create collaborative and social spaces, which are increasingly important in the face of remote work and travel reduction. Recent advances, such as more open and widely available platforms, create new possibilities to observe and analyse interaction in VEs. Using a custom instrumented build of Mozilla Hubs to measure position and orientation, we conducted an academic workshop to facilitate a range of typical workshop activities. We analysed social interactions during a keynote, small group breakouts, and informal networking/hallway conversations. Our mixed-methods approach combined environment logging, observations, and semi-structured interviews. The results demonstrate how small and large spaces influenced group formation, shared attention, and personal space, where smaller rooms facilitated more cohesive groups while larger rooms made small group formation challenging but personal space more flexible. Beyond our findings, we show how the combination of data and insights can fuel collaborative spaces’ design and deliver more effective virtual workshops.', 'corpus_id': 231602925, 'score': 1}, {'doc_id': '231648088', 'title': 'Extended Reality (XR) Remote Research: a Survey of Drawbacks and Opportunities', 'abstract': 'Extended Reality (XR) technology - such as virtual and augmented reality - is now widely used in Human Computer Interaction (HCI), social science and psychology experimentation. However, these experiments are predominantly deployed in-lab with a co-present researcher. Remote experiments, without co-present researchers, have not flourished, despite the success of remote approaches for non-XR investigations. This paper summarises findings from a 30-item survey of 46 XR researchers to understand perceived limitations and benefits of remote XR experimentation. Our thematic analysis identifies concerns common with non-XR remote research, such as participant recruitment, as well as XR-specific issues, including safety and hardware variability. We identify potential positive affordances of XR technology, including leveraging data collection functionalities builtin to HMDs (e.g. hand, gaze tracking) and the portability and reproducibility of an experimental setting. We suggest that XR technology could be conceptualised as an interactive technology and a capable data-collection device suited for remote experimentation.', 'corpus_id': 231648088, 'score': 1}, {'doc_id': '231837086', 'title': 'Virtual Reality Teleoperation Robot', 'abstract': 'The ability to access and interact virtually in remote, and sometimes dangerous, environments have become highly applicable in many industries. Our senior project team developed a virtual reality device to increase the accessibility of these environments. The system our team created utilizes a wireless controller to operate a robotic car and send visual feedback to a virtual reality headset through a camera stream. This system provided a safe and entertaining way for an operator to access environments remotely.', 'corpus_id': 231837086, 'score': 0}, {'doc_id': '218597922', 'title': 'Precise and realistic grasping and manipulation in Virtual Reality without force feedback', 'abstract': 'This paper introduces a physically-based approach of grasping and manipulation regarding virtual objects that would enable fine and stable grasping without haptic force feedback. The main contribution is to enhance an existing method which couples a virtual kinematic hand with a visual hand tracking system. The mismatches between the tracked and virtual hands often yield unstable grasps, especially for small objects. This is overcome by the implementation of grasping assistance based on virtual springs between the tracked and virtual hands. The assistance is triggered based on an analysis of usual grasping criteria, to determine whether a grasp is feasible or not. The proposed method has been validated in a supervised experiment which showed that our assistance improves speed and accuracy for a ""pick and place"" task involving an exhaustive object set, sized for precision grasp. Moreover, users’ feedback shows a clear preference for the present approach in terms of naturalness and efficiency.', 'corpus_id': 218597922, 'score': 1}, {'doc_id': '10131065', 'title': 'Augmented Reality versus Virtual Reality for 3D Object Manipulation', 'abstract': 'Virtual Reality (VR) Head-Mounted Displays (HMDs) are on the verge of becoming commodity hardware available to the average user and feasible to use as a tool for 3D work. Some HMDs include front-facing cameras, enabling Augmented Reality (AR) functionality. Apart from avoiding collisions with the environment, interaction with virtual objects may also be affected by seeing the real environment. However, whether these effects are positive or negative has not yet been studied extensively. For most tasks it is unknown whether AR has any advantage over VR. In this work we present the results of a user study in which we compared user performance measured in task completion time on a 9 degrees of freedom object selection and transformation task performed either in AR or VR, both with a 3D input device and a mouse. Our results show faster task completion time in AR over VR. When using a 3D input device, a purely VR environment increased task completion time by 22.5 percent on average compared to AR (<inline-formula> <tex-math notation=""LaTeX"">${p}<0.024$</tex-math><alternatives> <inline-graphic xlink:href=""krichenbauer-ieq1-2658570.gif""/></alternatives></inline-formula>). Surprisingly, a similar effect occurred when using a mouse: users were about 17.3 percent slower in VR than in AR (<inline-formula> <tex-math notation=""LaTeX"">${p}<0.04$</tex-math><alternatives> <inline-graphic xlink:href=""krichenbauer-ieq2-2658570.gif""/></alternatives></inline-formula>). Mouse and 3D input device produced similar task completion times in each condition (AR or VR) respectively. We further found no differences in reported comfort.', 'corpus_id': 10131065, 'score': 1}, {'doc_id': '231719049', 'title': '""Can I Touch This?"": Survey of Virtual Reality Interactions via Haptic Solutions', 'abstract': 'Haptic feedback has become crucial to enhance the user experiences in Virtual Reality (VR). This justifies the sudden burst of novel haptic solutions proposed these past years in the HCI community. This article is a survey of Virtual Reality interactions, relying on haptic devices. We propose two dimensions to describe and compare the current haptic solutions: their degree of physicality, as well as their degree of actuation. We depict a compromise between the user and the designer, highlighting how the range of required or proposed stimulation in VR is opposed to the haptic interfaces flexibility and their deployment in real-life use-cases. This paper (1) outlines the variety of haptic solutions and provides a novel perspective for analysing their associated interactions, (2) highlights the limits of the current evaluation criteria regarding these interactions, and finally (3) reflects the interaction, operation and conception potentials of ""encountered-type of haptic devices"".', 'corpus_id': 231719049, 'score': 1}, {'doc_id': '231861418', 'title': 'Towards Tangible Cultural Heritage Experiences - Enriching VR-Based Object Inspection with Haptic Feedback', 'abstract': 'VR/AR technology is a key enabler for new ways of immersively experiencing cultural heritage artifacts based on their virtual counterparts obtained from a digitization process. In this paper, we focus on enriching VR-based object inspection by additional haptic feedback, thereby creating tangible cultural heritage experiences. For this purpose, we present an approach for interactive and collaborative VR-based object inspection and annotation. Our system supports high-quality 3D models with accurate reflectance characteristics while additionally providing haptic feedback regarding the object shape features based on a 3D printed replica. The digital object model in terms of a printable representation of the geometry as well as reflectance characteristics are stored in a compact and streamable representation on a central server, which streams the data to remotely connected users/clients. The latter can jointly perform an interactive inspection of the object in VR with additional haptic feedback through the 3D printed replica. Evaluations regarding system performance, visual quality of the considered models as well as insights from a user study indicate an improved interaction, assessment and experience of the considered objects.', 'corpus_id': 231861418, 'score': 0}, {'doc_id': '230523848', 'title': 'Interpersonal distance in VR: reactions of older adults to the presence of a virtual agent', 'abstract': 'The rapid development of virtual reality technology has increased its availability and, consequently, increased the number of its possible applications. The interest in the new medium has grown due to the entertainment industry (games, VR experiences and movies). The number of freely available training and therapeutic applications is also increasing. Contrary to popular opinion, new technologies are also adopted by older adults. Creating virtual environments tailored to the needs and capabilities of older adults requires intense research on the behaviour of these participants in the most common situations, towards commonly used elements of the virtual environment, in typical sceneries. Comfortable immersion in a virtual environment is key to achieving the impression of presence. Presence is, in turn, necessary to obtain appropriate training, persuasive and therapeutic effects. A virtual agent (a humanoid representation of an algorithm or artificial intelligence) is often an element of the virtual environment interface. Maintaining an appropriate distance to the agent is, therefore, a key parameter for the creator of the VR experience. Older (65+) participants maintain greater distance towards an agent (a young white male) than younger ones (25-35). It may be caused by differences in the level of arousal, but also cultural norms. As a consequence, VR developers are advised to use algorithms that maintain the agent at the appropriate distance, depending on the user’s age.', 'corpus_id': 230523848, 'score': 0}]"
95	HRV	ba43027d2139b4d18ee8aeb84208ab25	16635	{'HRV': 'heart rate variability'}	"[{'doc_id': '233997132', 'title': 'Association of heart rate variability, blood pressure variability, and baroreflex sensitivity with gastric motility at rest and during cold pressor test', 'abstract': 'Aim: To understand the mutual interaction of gastric motility and autonomic functions, the present study evaluated the association of heart rate variability (HRV), blood pressure variability (BPV), and baroreflex sensitivity (BRS) with gastric motility assessed by electrogastrography (EGG) at rest and during CPT and explored the effect of sympathetic activation by cold pressor test (CPT) on gastric motility. Background: The autonomic nervous system has a significant influence on gastrointestinal motility. HRV is commonly employed to assess the functions of the autonomic nervous system. BPV and BRS are relatively newer techniques and give a more holistic picture of autonomic functions along with the short-term regulation of blood pressure (BP). Methods: In fourteen young, healthy subjects, gastric motility was assessed by EGG. Beat-to-beat BP and lead II ECG were recorded to assess HRV, BPV, and BRS. BPV and BRS parameters were calculated for systolic, mean, and diastolic BP. Parameters of HRV and BPV were calculated for time and frequency domains. BRS was calculated by sequence and spectral methods. Results Significant increases in diastolic BP (p = <0.0001) and EGG frequency (p = 0.0229) were observed during CPT. Significant correlations were observed between EGG frequencies and many of the HRV, BPV, and BRS parameters. The correlation coefficient was found to be highest between total power of HRV and EGG frequencies during baseline (p = 0.0107, r = -0.6571) and during CPT (p = 0.0059, r = -0.6935). Conclusion: EGG frequency can be decreased by an acute increase in sympathetic activity induced by CPT. The novel findings are the significant correlations between many of the HRV, BPV, and BRS parameters and EGG frequency.', 'corpus_id': 233997132, 'score': 0}, {'doc_id': '11104601', 'title': 'An Overview of Heart Rate Variability Metrics and Norms', 'abstract': 'Healthy biological systems exhibit complex patterns of variability that can be described by mathematical chaos. Heart rate variability (HRV) consists of changes in the time intervals between consecutive heartbeats called interbeat intervals (IBIs). A healthy heart is not a metronome. The oscillations of a healthy heart are complex and constantly changing, which allow the cardiovascular system to rapidly adjust to sudden physical and psychological challenges to homeostasis. This article briefly reviews current perspectives on the mechanisms that generate 24\u2009h, short-term (~5\u2009min), and ultra-short-term (<5\u2009min) HRV, the importance of HRV, and its implications for health and performance. The authors provide an overview of widely-used HRV time-domain, frequency-domain, and non-linear metrics. Time-domain indices quantify the amount of HRV observed during monitoring periods that may range from ~2\u2009min to 24\u2009h. Frequency-domain values calculate the absolute or relative amount of signal energy within component bands. Non-linear measurements quantify the unpredictability and complexity of a series of IBIs. The authors survey published normative values for clinical, healthy, and optimal performance populations. They stress the importance of measurement context, including recording period length, subject age, and sex, on baseline HRV values. They caution that 24\u2009h, short-term, and ultra-short-term normative values are not interchangeable. They encourage professionals to supplement published norms with findings from their own specialized populations. Finally, the authors provide an overview of HRV assessment strategies for clinical and optimal performance interventions.', 'corpus_id': 11104601, 'score': 1}, {'doc_id': '205352055', 'title': 'Breathing at a rate of 5.5 breaths per minute with equal inhalation-to-exhalation ratio increases heart rate variability.', 'abstract': 'OBJECTIVES\nPrior studies have found that a breathing pattern of 6 or 5.5 breaths per minute (bpm) was associated with greater heart rate variability (HRV) than that of spontaneous breathing rate. However, the effects of combining the breathing rate with the inhalation-to-exhalation ratio (I:E ratio) on HRV indices are inconsistent. This study aimed to examine the differences in HRV indices and subjective feelings of anxiety and relaxation among four different breathing patterns.\n\n\nMETHODS\nForty-seven healthy college students were recruited for the study, and a Latin square experimental design with a counterbalance in random sequences was applied. Participants were instructed to breathe at two different breathing rates (6 and 5.5 breaths) and two different I:E ratios (5:5 and 4:6). The HRV indices as well as anxiety and relaxation levels were measured at baseline (spontaneous breathing) and for the four different breathing patterns.\n\n\nRESULTS\nThe results revealed that a pattern of 5.5 bpm with an I:E ratio of 5:5 produced a higher NN interval standard deviation and higher low frequency power than the other breathing patterns. Moreover, the four different breathing patterns were associated with significantly increased feeling of relaxation compared with baseline.\n\n\nCONCLUSION\nThe study confirmed that a breathing pattern of 5.5 bpm with an I:E ratio of 5:5 achieved greater HRV than the other breathing patterns. This finding can be applied to HRV biofeedback or breathing training in the future.', 'corpus_id': 205352055, 'score': 1}, {'doc_id': '232773720', 'title': 'Neurophysiological Approach by Self-Control of Your Stress-Related Autonomic Nervous System with Depression, Stress and Anxiety Patients', 'abstract': 'Background: Heart Rate Variability Biofeedback (HRVB) is a treatment in which patients learn self-regulation of a physiological dysregulated vagal nerve function. While the therapeutic approach of HRVB is promising for a variety of disorders, it has not yet been regularly offered in a mental health treatment setting. Aim: To provide a systematic review about the efficacy of HRV-Biofeedback in treatment of anxiety, depression, and stress related disorders. Method: Systematic review in PubMed and Web of Science in 2020 with terms HRV, biofeedback, Post-Traumatic Stress Disorder (PTSD), depression, panic disorder, and anxiety disorder. Selection, critical appraisal, and description of the Random Controlled Trials (RCT) studies. Combined with recent meta-analyses. Results: The search resulted in a total of 881 studies. After critical appraisal, nine RCTs have been selected as well as two other relevant studies. The RCTs with control groups treatment as usual, muscle relaxation training and a “placebo“-biofeedback instrument revealed significant clinical efficacy and better results compared with control conditions, mostly significant. In the depression studies average reduction at the Beck Depression Inventory (BDI) scale was 64% (HRVB plus Treatment as Usual (TAU) versus 25% (control group with TAU) and 30% reduction (HRVB) at the PSQ scale versus 7% (control group with TAU). In the PTSD studies average reduction at the BDI-scale was 53% (HRV plus TAU) versus 24% (control group with TAU) and 22% (HRVB) versus 10% (TAU) with the PTSD Checklist (PCL). In other systematic reviews significant effects have been shown for HRV-Biofeedback in treatment of asthma, coronary artery disease, sleeping disorders, postpartum depression and stress and anxiety. Conclusion: This systematic review shows significant improvement of the non-invasive HRVB training in stress related disorders like PTSD, depression, and panic disorder, in particular when combined with cognitive behavioral therapy or different TAU. Effects were visible after four weeks of training, but clinical practice in a longer daily self-treatment of eight weeks is more promising. More research to integrate HRVB in treatment of stress related disorders in psychiatry is warranted, as well as research focused on the neurophysiological mechanisms.', 'corpus_id': 232773720, 'score': 0}, {'doc_id': '233225167', 'title': 'Intensification of functional neural control on heartbeat dynamics in subclinical depression', 'abstract': 'Subclinical depression (dysphoria) is a common condition that may increase the risk of major depression and leads to impaired quality of life and severe comorbid somatic diseases. Despite its prevalence, specific biological markers are unknown; consequently, the identification of dysphoria currently relies exclusively on subjective clinical scores and structured interviews. Based on recent neurocardiology studies that link brain and cardiovascular disorders, it was hypothesized that multi-system biomarkers of brain–body interplay may effectively characterize dysphoria. Thus, an ad hoc computational technique was developed to quantify the functional bidirectional brain–heart interplay. Accordingly, 32-channel electroencephalographic and heart rate variability series were obtained from 24 young dysphoric adults and 36 healthy controls. All participants were females of a similar age, and results were obtained during a 5-min resting state. The experimental results suggest that a specific feature of dysphoria is linked to an augmented functional central-autonomic control to the heart, which originates from central, frontopolar, and occipital oscillations and acts through cardiovascular sympathovagal activity. These results enable further development of a large set of novel biomarkers for mood disorders based on comprehensive brain–body measurements.', 'corpus_id': 233225167, 'score': 0}, {'doc_id': '233385894', 'title': 'Effect of Yoga on Pulse rate and Oxygen Saturation: Analysis of Psychophysiological Parameters', 'abstract': 'Introduction Pranayamic breathing is a process of continuous, regularity of inhalation, holding of breath and exhalation. All venous blood is converted to oxygenated blood . However, does deep breathing in which oxygen is inhaled in large amounts increase oxygen saturation or does the saturation decrease due to anaerobic metabolism associated with yoga? Does the psycho-physiological parameters of stress index, power, vegetative index & regulation, neurohumoral regulation, psycho-emotional state, energy resources, complex index, harmonization, biological age and energies in the spine get affected a""er a yogic intervention? Methods 52 subjects of age range from 15-70 years performed “Yoga module for the Healthy Heart” for 45 minutes at AYUSH, AIIMS, Bhopal. Pulse rate and oxygen saturation was measured by pulse oximeter a""er initial rest of ten minutes and a""er 45 minutes of yoga. A pilot study was conducted using the DINAMIKA HRV for ten yogic practitioner who were regular in their practice for last 10-15 years. #eir psycho-physiological parameters were measured before and a""er their yogic routine of 35 to 40 minutes by Dinamika Heart rate variability (HRV) instrument. Results #e readings were analysed using paired t  test. #e pulse rate dropped from 81.98 ± 13.05 to is 74.98 ± 11.64 at p value <0.0001 indicating a shi"" towards parasympathetic dominance. Oxygen saturation dropped from 97.40 +/1.11 to 97.21 +/1.30 at p value of 0.2736, indicating a shi"" to anaerobic metabolism during yoga practice #e psychophysiological parameters of pulse rate, stress index, power, vegetative index & regulation, neurohumoral regulation, psycho-emotional state, energy resources, complex index, harmonization, biological age and energies in the spine were statistically significant post yogic intervention. By the power of will the yogic practitioner is able to draw cosmic energy in the spine which helps to renew it. #e mind is able to overcome strong physical distractions, the body is relaxed and calm. Relaxation is achieved by stilling of muscles, calming and slowing down the activity of heart, respiration and circulation.', 'corpus_id': 233385894, 'score': 0}, {'doc_id': '232413559', 'title': 'Altered Heart Rate Variability in Patients With Schizophrenia During an Autonomic Nervous Test', 'abstract': 'Reduced heart rate variability (HRV) and dysfunction of the autonomic nervous system (ANS) have been observed in schizophrenia patients. HRV parameters of schizophrenia patients in the resting state have been well-documented; however, these parameters of schizophrenia patients who experience continuous psychophysiological stress remain unclear. The objective of this study was to systematically explore the linear and nonlinear HRV parameters between schizophrenia patients and normal controls and to detect the adaptive capabilities of HRV of schizophrenia patients during the stimulation tests of autonomic nervous system. Forty-five schizophrenia patients and forty-five normal controls, matched for age, sex and body mass index, completed a 14 min ANS test. Thirteen linear and nonlinear HRV parameters of all subjects under the ANS test were computed and statistically analyzed between groups and between sessions. The STROBE checklist was adhered to in this study. All time-domain HRV features in the ANS test were significantly different between schizophrenia patients and normal controls (p < 0.01). The schizophrenia patients showed significantly low values in the Poincaré indices, which revealed significantly decreased heart rate fluctuation complexity compared with that of normal controls (p < 0.001). In addition, the normal controls, not schizophrenia patients, showed significant differences between the recovery and stress states in the parameters of low frequency, high frequency, and nonlinear dynamics. Schizophrenia patients showed autonomic dysfunction of the heart in a series of stimulation tests of the autonomic nervous system and could not regain normal physiological functions after stress cessation. Our findings revealed that the dynamic parameters of HRV in psychophysiological stress are sensitive and practical for a diagnosis of schizophrenia.', 'corpus_id': 232413559, 'score': 0}, {'doc_id': '17621603', 'title': 'Heart rate variability biofeedback: how and why does it work?', 'abstract': 'In recent years there has been substantial support for heart rate variability biofeedback (HRVB) as a treatment for a variety of disorders and for performance enhancement (Gevirtz, 2013). Since conditions as widely varied as asthma and depression seem to respond to this form of cardiorespiratory feedback training, the issue of possible mechanisms becomes more salient. The most supported possible mechanism is the strengthening of homeostasis in the baroreceptor (Vaschillo et al., 2002; Lehrer et al., 2003). Recently, the effect on the vagal afferent pathway to the frontal cortical areas has been proposed. In this article, we review these and other possible mechanisms that might explain the positive effects of HRVB.', 'corpus_id': 17621603, 'score': 1}, {'doc_id': '14752473', 'title': ""A healthy heart is not a metronome: an integrative review of the heart's anatomy and heart rate variability"", 'abstract': ""Heart rate variability (HRV), the change in the time intervals between adjacent heartbeats, is an emergent property of interdependent regulatory systems that operate on different time scales to adapt to challenges and achieve optimal performance. This article briefly reviews neural regulation of the heart, and its basic anatomy, the cardiac cycle, and the sinoatrial and atrioventricular pacemakers. The cardiovascular regulation center in the medulla integrates sensory information and input from higher brain centers, and afferent cardiovascular system inputs to adjust heart rate and blood pressure via sympathetic and parasympathetic efferent pathways. This article reviews sympathetic and parasympathetic influences on the heart, and examines the interpretation of HRV and the association between reduced HRV, risk of disease and mortality, and the loss of regulatory capacity. This article also discusses the intrinsic cardiac nervous system and the heart-brain connection, through which afferent information can influence activity in the subcortical and frontocortical areas, and motor cortex. It also considers new perspectives on the putative underlying physiological mechanisms and properties of the ultra-low-frequency (ULF), very-low-frequency (VLF), low-frequency (LF), and high-frequency (HF) bands. Additionally, it reviews the most common time and frequency domain measurements as well as standardized data collection protocols. In its final section, this article integrates Porges' polyvagal theory, Thayer and colleagues' neurovisceral integration model, Lehrer et al.'s resonance frequency model, and the Institute of HeartMath's coherence model. The authors conclude that a coherent heart is not a metronome because its rhythms are characterized by both complexity and stability over longer time scales. Future research should expand understanding of how the heart and its intrinsic nervous system influence the brain."", 'corpus_id': 14752473, 'score': 1}, {'doc_id': '222178732', 'title': 'A Practical Guide to Resonance Frequency Assessment for Heart Rate Variability Biofeedback', 'abstract': 'Heart rate variability (HRV) represents fluctuations in the time intervals between successive heartbeats, which are termed interbeat intervals. HRV is an emergent property of complex cardiac-brain interactions and non-linear autonomic nervous system (ANS) processes. A healthy heart is not a metronome because it exhibits complex non-linear oscillations characterized by mathematical chaos. HRV biofeedback displays both heart rate and frequently, respiration, to individuals who can then adjust their physiology to improve affective, cognitive, and cardiovascular functioning. The central premise of the HRV biofeedback resonance frequency model is that the adult cardiorespiratory system has a fixed resonance frequency. Stimulation at rates near the resonance frequency produces large-amplitude blood pressure oscillations that can increase baroreflex sensitivity over time. The authors explain the rationale for the resonance frequency model and provide detailed instructions on how to monitor and assess the resonance frequency. They caution that patterns of physiological change must be compared across several breathing rates to evaluate candidate resonance frequencies. They describe how to fine-tune the resonance frequency following an initial assessment. Furthermore, the authors critically assess the minimum epochs required to measure key HRV indices, resonance frequency test-retest reliability, and whether rhythmic skeletal muscle tension can replace slow paced breathing in resonance frequency assessment.', 'corpus_id': 222178732, 'score': 1}]"
96	Pandemic & Efficacy	0d23a0ad6dafa02090547949846656dc	6837	{}	"[{'doc_id': '219443516', 'title': 'TRAINING FUNCTIONAL COMMUNICATION RESPONSES TO DECREASE CHALLENGING BEHAVIORS AMONG STUDENTS IN AN ALTERNATIVE EDUCATION SETTING', 'abstract': 'OF THESIS TRAINING FUNCTIONAL COMMUNICATION RESPONSES TO DECREASE CHALLENGING BEHAVIORS AMONG STUDENTS IN AN ALTERNATIVE EDUCATION SETTING This study answered a series of questions related to single case research design and potential use of functional communication-based intervention. It compared and contrasted multiple probe versus multiple baseline single case research designs, finding that multiple probe would be more appropriate to answer the intended research question. This study further analyzed the rigor, quality, and bias of Turton, Umbreit, & Mathur’s (2011) article in which functional communication-based intervention was used to decrease challenging behaviors among adolescents in an alternative education setting. Finally, this study described for practitioners the importance of studying the effectiveness of functional communication-based intervention and how such intervention might be implemented in classroom settings.', 'corpus_id': 219443516, 'score': 0}, {'doc_id': '220118107', 'title': 'The Potential of Research for Professional Development in Isolated Settings During the Covid-19 Crisis and Beyond', 'abstract': 'The global COVID-19 pandemic has created the urgent need for quality online instruction throughout all levels of education. However, this pandemic has found teachers physically isolated within their homes, and unprepared for the challenging tasks of teaching online. Many of the challenges faced by teachers due to this isolation, are similar to those faced by teachers in remote areas around the world. One such issue, is the lack of access to traditional professional development opportunities, which is frequently the only type of professional development offered to teachers in many countries. Thus, many teachers lack guidance on how to resolve many online teaching challenges during this period. Therefore, this study examines the potential of utilizing educational research for assisting teachers through this trying period of COVID-19.', 'corpus_id': 220118107, 'score': 1}, {'doc_id': '219177027', 'title': 'Critical pedagogy in the implementation of educational technologies', 'abstract': 'This paper presents a critical review of the challenges to the implementation of learning technologies with particular focus on developing countries. A comprehensive literature review on learning technologies was undertaken for the purpose of understanding the challenges in developing countries. The research question is: what extent does education empower learners to be full participants in a socially democratic society? The literature review identified 25 papers relevant to this topic. Challenges are interrelated and to bring about changes in developing countries, this paper proposes two educational technology frameworks based on: 1. cultural conceptual framework, and 2. problem-based constructivist psychology simulation model. The framework and simulation model are both useful to guide practice and research.', 'corpus_id': 219177027, 'score': 0}, {'doc_id': '220632211', 'title': 'EFFECTS OF COGNITIVE COACHING ON TEACHER EFFICACY', 'abstract': 'Educators in the American school system have been under pressure to increase academic achievement among all students especially within the past decade. Government initiatives and high-stakes testing have ushered in a new era of accountability. Many attempts at school reform have been made with little success. Current research has found that the variable with the greatest impact on student achievement is the quality of the instructor. Attempts at improving teacher effectiveness have included merit pay, professional development, utilizing a scripted curriculum, and coaching. Studies have shown that professional development embedded within the classroom is most beneficial in terms of improving instruction and increasing student achievement. Continuing the professional development after the initial delivery can prove to be difficult. Both Professional Learning Communities and peer coaching are techniques used to support instructors in implementing new concepts. However, Cognitive Coachingsm on a one-on-one basis has greatly influenced change in teacher behaviors, increasing teacher efficacy, and elevating student achievement scores. COGNITIVE COACHING AND TEACHER EFFICACY Table of', 'corpus_id': 220632211, 'score': 1}, {'doc_id': '219310491', 'title': 'Innovations in Teaching and Learning during a Time of Crisis', 'abstract': 'During the COVID-19 pandemic higher education institutions have faced many challenges, and among these are questions about how best to offer instruction in the face of sudden and mandatory college closures as well as in light of uncertainties about when campuses might open again. Coupled with these challenges have been signs that higher education is more innovative and flexible than we might have imagined, particularly when it comes to teaching and learning. As college and university educators have navigated the shifting landscape, they have found new and innovative ways to use technology to accomplish instructional goals; and they will most likely have reason to continue in the path of innovation going forward. With the advent of mass remote instruction, college and university faculty members have had to determine how to teach online quickly, with the knowledge that many students did not sign up for online learning and may only have mobile devices coupled with limited internet access. Educators have been remarkable in putting workable short-term technological solutions for remote teaching and learning in place. As a result, the adoption of technology for teaching and learning in recent months has been unprecedented, with synchronous learning taking the fore initially. In particular, video-conferencing apps like Zoom and Skype have provided faculty and students a lifeline during this challenging time. With the close of the spring academic term, however, colleges and university teachers around the world have recognized that there has been a shift in the way that we offer instruction, at least for the foreseeable future, as students may or may not return to campus in the fall. While some institutions have announced that they will be virtual in the fall, others have announced that students will return to campus; but the question of whether that will happen or not looms large. Some institutions have not yet made decisions. At the very least, many faculty members now find themselves planning for fall in a way that allows them to be flexible about whether the teaching they do will occur onsite or online. What this means in practice is that we need to be prepared to teach fully onsite and to shift to fully online teaching at a moment’s notice if an unanticipated campus closure should require it. As a result, many faculty members are now realizing that remote learning is just a first step in a longer journey to offering high quality online education. They are taking up new tools, which allow for Innovative Higher Education https://doi.org/10.1007/s10755-020-09514-w', 'corpus_id': 219310491, 'score': 1}, {'doc_id': '220728184', 'title': 'Nature of “STEM”?', 'abstract': 'In recent years, there has been an increasing emphasis on STEM (science, technology, engineering, and mathematics) education in international curriculum and policy documents (e.g., NSTA, 2020; Office of the Chief Scientist, 2014). A key argument in the proposals for STEM education is that science, technology, engineering, and mathematics workers play a pivotal role in economic growth and STEM education produces critical thinkers, scientifically literate professionals and citizens, and enables the next generation of innovators. The infusion of “engineering practices” in the Next Generation Science Standards in the USA signals a major shift in curriculum policy for integrating related domains to science teaching and learning. Furthermore, there has been plethora of journals, research centers, and community organizations that have made STEM a central educational goal, and many funding agencies are supporting research and development efforts to advance STEM education. But what exactly does “STEM” mean? Is there a particular “nature” to STEM or are there disciplinary variations across the “natures” of science, technology, engineering, and mathematics? What are the epistemic underpinnings of STEM and what do they imply for STEM education? A question in a similar vein had been raised by Erin Peters-Burton in an editorial of School Science and Mathematics a few years ago (Peters-Burton, 2014) but has since received little attention despite the wealth of interest in research on STEM education. The primary purpose of this special issue is, then, to address some questions about the nature of STEM and STEM education. The questions raised by the papers in the special issue relate to theoretical characterization of STEM as well as a range of educational considerations including the implications for curriculum reform as well as for students’ and teachers’ learning. A fundamental issue is whether or not “STEM” is a warranted notion in the first place. Despite the plethora of work on STEMeducation, what STEMpromises to be and how it manifests itself in education can be questioned. Hence, the special issue is set against a backdrop of some critiques of STEM education, followed by a set of studies that illustrate its merits. Reynante, Selback-Allen, and Pimentel question how many STEM education efforts have not explicitly accounted for the distinct epistemologies of the disciplines. The authors critically examine the concept of integrated Science & Education https://doi.org/10.1007/s11191-020-00150-6', 'corpus_id': 220728184, 'score': 0}, {'doc_id': '150125350', 'title': 'The Professional Development of Teachers', 'abstract': 'This chapter provides a brief synopsis of past and present teacher preparation and credentialing practices along with common professional development (PD) programs emphasized in the United States and their influence in shaping instructional activities that have dominated classroom instruction. Research studies reveal that the PD of teachers has been costly yet ineffective. Thus, a consensus view of effective PD based on the learning sciences has been formed and is being promoted by the US Department of Education, among others. The latest view of effective PD may still fall short if we do not address the underlying constructs that determine a teacher’s PPATs such as their mental model of how students learn, their implicit theories of intelligence, and the forms of knowledge they employ. Addressing these constructs has the potential to create conceptual change in teachers, adjusting their PPATs to be more learner centered, and supporting the use of formative assessment practices. In addition to addressing these constructs, a new vision should be cast based on what students need to know and be able to do in a knowledge economy and teachers need to be trained and prepared to equip them accordingly.', 'corpus_id': 150125350, 'score': 1}, {'doc_id': '220249784', 'title': 'An Analysis of Academic Performance of University Students in Namibia', 'abstract': 'Based on observations it seems that a considerable proportion of university students in Namibia need to enhance their academic performance in regard to be fully competitive in a globalised working environment. This specifically includes thorough understanding of mathematics, as it forms an integral part of modern knowledge disciplines such as information technology and the management sciences. Yet students struggle or fail to engage in relevant coursework, either because of an absence of adequate material, capable educators, their own will power, or a combination thereof. This study aims to investigate the critical factors that are related to academic performance of university students in Namibia.', 'corpus_id': 220249784, 'score': 0}, {'doc_id': '219523312', 'title': 'Whether the School Self-Developed e-Learning Platform is More Conducive to Learning during the COVID-19 Pandemic?', 'abstract': 'The school self-developed e-learning platform can provide students with real and non-real-time learning resources based on its personalized learning properties like self-owned teaching conditions and levels. Meanwhile, it is beneficial to students’ independent study and academic performance. During the COVID-19 pandemic, schools in China adopted e-learning platforms to conduct online teaching, whereas the efficacy of these online teaching platforms was not definitely known. It is necessary to review their effectiveness, especially for those developed by schools themselves. A total of 417 of 7th and 8th graders from two middle schools in Nanjing, Jiangsu Province were enrolled with 208 in the experimental group and 209 as the comparisons. The primary outcome of the study was students’ academic performance. The results demonstrated that: (i) the online platform-based self-learning was conducive to students’ grades; (ii) school self-developed e-learning platform was more effective in improving student achievement than other non-school self-developed ones.', 'corpus_id': 219523312, 'score': 1}, {'doc_id': '220425555', 'title': 'GUIDE for a blended learning system', 'abstract': ""This guide is proposed as an operational instrument for CONFRASIE member universities (Regional Rectors' Conference of AUF member institutions in Pacific-Asia) in their projects to set up a blended learning system for bachelor's, Master's and Doctorate degrees. It is structured in sections corresponding to a complete process of operationalizing a blended learning system, from the definition of an implementation strategy to the assessment of results. This guide covers also conceptual and theoretical fundamentals of distance learning as well as methodological and procedural tips and recommendations on how to implement blended learning in an existing face-to-face curriculum. It can serve for leaders of educational ICT-based projects as a guidance document to take pedagogical, technological and methodological decisions for the development, monitoring and assessment of a blended learning curricula. This guide can be augmented by other standards, tool and software manuals offering further training materials and guidelines on educational skills ans=d services."", 'corpus_id': 220425555, 'score': 0}]"
97	Poker	cea3a438b39dad8170fff14f292dde69	5115	{}	"[{'doc_id': '189928084', 'title': 'Solution of Two-Player Zero-Sum Game by Successive Relaxation', 'abstract': 'We consider the problem of two-player zero-sum game. In this setting, there are two agents working against each other. Both the agents observe the same state and the objective of the agents is to compute a strategy profile that maximizes their rewards. However, the reward of the second agent is negative of reward obtained by the first agent. Therefore, the objective of the second agent is to minimize the total reward obtained by the first agent. This problem is formulated as a min-max Markov game in the literature. The solution of this game, which is the max-min reward (of first player), starting from a given state is called the equilibrium value of the state. In this work, we compute the solution of the two-player zero-sum game utilizing the technique of successive relaxation. Successive relaxation has been successfully applied in the literature to compute a faster value iteration algorithm in the context of Markov Decision Processes. We extend the concept of successive relaxation to the two-player zero-sum games. We prove that, under a special structure, this technique computes the optimal solution faster than the techniques in the literature. We then derive a generalized minimax Q-learning algorithm that computes the optimal policy when the model information is not known. Finally, we prove the convergence of the proposed generalized minimax Q-learning algorithm.', 'corpus_id': 189928084, 'score': 1}, {'doc_id': '212634032', 'title': 'Min-Max Q-Learning for Multi-Player Pursuit-Evasion Games', 'abstract': ""In this paper, we address a pursuit-evasion game involving multiple players by utilizing tools and techniques from reinforcement learning and matrix game theory. In particular, we consider the problem of steering an evader to a goal destination while avoiding capture by multiple pursuers, which is a high-dimensional and computationally intractable problem in general. In our proposed approach, we first formulate the multi-agent pursuit-evasion game as a sequence of discrete matrix games. Next, in order to simplify the solution process, we transform the high-dimensional state space into a low-dimensional manifold and the continuous action space into a feature-based space, which is a discrete abstraction of the original space. Based on these transformed state and action spaces, we subsequently employ min-max Q-learning, to generate the entries of the payoff matrix of the game, and subsequently obtain the optimal action for the evader at each stage. Finally, we present extensive numerical simulations to evaluate the performance of the proposed learning-based evading strategy in terms of the evader's ability to reach the desired target location without being captured, as well as computational efficiency."", 'corpus_id': 212634032, 'score': 0}, {'doc_id': '195658101', 'title': 'Rethinking Formal Models of Partially Observable Multiagent Decision Making', 'abstract': 'Multiagent decision-making problems in partially observable environments are usually modeled as either extensive-form games (EFGs) within the game theory community or partially observable stochastic games (POSGs) within the reinforcement learning community. While most practical problems can be modeled in both formalisms, the communities using these models are mostly distinct with little sharing of ideas or advances. The last decade has seen dramatic progress in algorithms for EFGs, mainly driven by the challenge problem of poker. We have seen computational techniques achieving super-human performance, some variants of poker are essentially solved, and there are now sound local search algorithms which were previously thought impossible. While the advances have garnered attention, the fundamental advances are not yet understood outside the EFG community. This can be largely explained by the starkly different formalisms between the game theory and reinforcement learning communities and, further, by the unsuitability of the original EFG formalism to make the ideas simple and clear. This paper aims to address these hindrances, by advocating a new unifying formalism, a variant of POSGs, which we call Factored-Observation Games (FOGs). We prove that any timeable perfect-recall EFG can be efficiently modeled as a FOG as well as relating FOGs to other existing formalisms. Additionally, a FOG explicitly identifies the public and private components of observations, which is fundamental to the recent EFG breakthroughs. We conclude by presenting the two building-blocks of these breakthroughs --- counterfactual regret minimization and public state decomposition --- in the new formalism, illustrating our goal of a simpler path for sharing recent advances between game theory and reinforcement learning community.', 'corpus_id': 195658101, 'score': 1}, {'doc_id': '214803086', 'title': 'Using Multi-Agent Reinforcement Learning in Auction Simulations', 'abstract': 'Game theory has been developed by scientists as a theory of strategic interaction among players who are supposed to be perfectly rational. These strategic interactions might have been presented in an auction, a business negotiation, a chess game, or even in a political conflict aroused between different agents. In this study, the strategic (rational) agents created by reinforcement learning algorithms are supposed to be bidder agents in various types of auction mechanisms such as British Auction, Sealed Bid Auction, and Vickrey Auction designs. Next, the equilibrium points determined by the agents are compared with the outcomes of the Nash equilibrium points for these environments. The bidding strategy of the agents is analyzed in terms of individual rationality, truthfulness (strategy-proof), and computational efficiency. The results show that using a multi-agent reinforcement learning strategy improves the outcomes of the auction simulations.', 'corpus_id': 214803086, 'score': 0}, {'doc_id': '218870412', 'title': 'Single-Agent Optimization Through Policy Iteration Using Monte-Carlo Tree Search', 'abstract': 'The combination of Monte-Carlo Tree Search (MCTS) and deep reinforcement learning is state-of-the-art in two-player perfect-information games. In this paper, we describe a search algorithm that uses a variant of MCTS which we enhanced by 1) a novel action value normalization mechanism for games with potentially unbounded rewards (which is the case in many optimization problems), 2) defining a virtual loss function that enables effective search parallelization, and 3) a policy network, trained by generations of self-play, to guide the search. We gauge the effectiveness of our method in ""SameGame""---a popular single-player test domain. Our experimental results indicate that our method outperforms baseline algorithms on several board sizes. Additionally, it is competitive with state-of-the-art search algorithms on a public set of positions.', 'corpus_id': 218870412, 'score': 0}, {'doc_id': '218900609', 'title': 'Stochastic Potential Games', 'abstract': 'Computing the Nash equilibrium (NE) for N-player non-zerosum stochastic games is a formidable challenge. Currently, algorithmic methods in stochastic game theory are unable to compute NE for stochastic games (SGs) for settings in all but extreme cases in which the players either play as a team or have diametrically opposed objectives in a two-player setting. This greatly impedes the application of the SG framework to numerous problems within economics and practical systems of interest. In this paper, we provide a method of computing Nash equilibria in nonzero-sum settings and for populations of players more than two. In particular, we identify a subset of SGs known as stochastic potential games (SPGs) for which the (Markov perfect) Nash equilibrium can be computed tractably and in polynomial time. Unlike SGs for which, in general, computing the NE is PSPACE-hard, we show that SGs with the potential property are P-Complete. We further demonstrate that for each SPG there is a dual Markov decision process whose solution coincides with the MP-NE of the SPG. We lastly provide algorithms that tractably compute the MP-NE for SGs with more than two players.', 'corpus_id': 218900609, 'score': 0}, {'doc_id': '195892791', 'title': 'Superhuman AI for multiplayer poker', 'abstract': ""AI now masters six-player poker Computer programs have shown superiority over humans in two-player games such as chess, Go, and heads-up, no-limit Texas hold'em poker. However, poker games usually include six players—a much trickier challenge for artificial intelligence than the two-player variant. Brown and Sandholm developed a program, dubbed Pluribus, that learned how to play six-player no-limit Texas hold'em by playing against five copies of itself (see the Perspective by Blair and Saffidine). When pitted against five elite professional poker players, or with five copies of Pluribus playing against one professional, the computer performed significantly better than humans over the course of 10,000 hands of poker. Science, this issue p. 885; see also p. 864 An AI dubbed Pluribus performs significantly better than human professionals in six-player no-limit Texas hold’em poker. In recent years there have been great strides in artificial intelligence (AI), with games often serving as challenge problems, benchmarks, and milestones for progress. Poker has served for decades as such a challenge problem. Past successes in such benchmarks, including poker, have been limited to two-player games. However, poker in particular is traditionally played with more than two players. Multiplayer games present fundamental additional issues beyond those in two-player games, and multiplayer poker is a recognized AI milestone. In this paper we present Pluribus, an AI that we show is stronger than top human professionals in six-player no-limit Texas hold’em poker, the most popular form of poker played by humans."", 'corpus_id': 195892791, 'score': 1}, {'doc_id': '218628845', 'title': 'Competing in a Complex Hidden Role Game with Information Set Monte Carlo Tree Search', 'abstract': 'Advances in intelligent game playing agents have led to successes in perfect information games like Go and imperfect information games like Poker. The Information Set Monte Carlo Tree Search (ISMCTS) family of algorithms outperforms previous algorithms using Monte Carlo methods in imperfect information games. In this paper, Single Observer Information Set Monte Carlo Tree Search (SO-ISMCTS) is applied to Secret Hitler, a popular social deduction board game that combines traditional hidden role mechanics with the randomness of a card deck. This combination leads to a more complex information model than the hidden role and card deck mechanics alone. It is shown in 10108 simulated games that SO-ISMCTS plays as well as simpler rule based agents, and demonstrates the potential of ISMCTS algorithms in complicated information set domains.', 'corpus_id': 218628845, 'score': 0}, {'doc_id': '211066535', 'title': 'Multi Type Mean Field Reinforcement Learning', 'abstract': 'Mean field theory provides an effective way of scaling multiagent reinforcement learning algorithms to environments with many agents that can be abstracted by a virtual mean agent. In this paper, we extend mean field multiagent algorithms to multiple types. The types enable the relaxation of a core assumption in mean field games, which is that all agents in the environment are playing almost similar strategies and have the same goal. We conduct experiments on three different testbeds for the field of many agent reinforcement learning, based on the standard MAgents framework. We consider two different kinds of mean field games: a) Games where agents belong to predefined types that are known a priori and b) Games where the type of each agent is unknown and therefore must be learned based on observations. We introduce new algorithms for each type of game and demonstrate their superior performance over state of the art algorithms that assume that all agents belong to the same type and other baseline algorithms in the MAgent framework.', 'corpus_id': 211066535, 'score': 1}, {'doc_id': '76666594', 'title': 'Compact Representation of Value Function in Partially Observable Stochastic Games', 'abstract': 'Value methods for solving stochastic games with partial observability model the uncertainty of the players as a probability distribution over possible states, where the dimension of the belief space is the number of states. For many practical problems, there are exponentially many states which causes scalability problems. We propose an abstraction technique that addresses this curse of dimensionality by projecting the high-dimensional beliefs onto characteristic vectors of significantly lower dimension (e.g., marginal probabilities). Our main contributions are (1) a novel compact representation of the uncertainty in partially observable stochastic games and (2) a novel algorithm using this representation that is based on existing state-of-the-art algorithms for solving stochastic games with partial observability. Experimental evaluation confirms that the new algorithm using the compact representation dramatically increases scalability compared to the state of the art.', 'corpus_id': 76666594, 'score': 1}]"
98	Diagnosing changes - DSDM	1286f319f7b0759acedf5b7919382088	19867	{'DSDM': 'Dynamic Species Distribution Models'}	"[{'doc_id': '199098266', 'title': 'Tropical bird species richness is strongly associated with patterns of primary productivity captured by the Dynamic Habitat Indices', 'abstract': 'Abstract Biodiversity science and conservation alike require environmental indicators to understand species richness and predict species distribution patterns. The Dynamic Habitat Indices (DHIs) are a set of three indices that summarize annual productivity measures from satellite data for biodiversity applications, and include: a) cumulative annual productivity; b) minimum annual productivity; and c) variation in annual productivity. At global scales and in temperate regions the DHIs predict species diversity patterns well, but the DHIs have not been tested in the tropics, where higher levels of productivity lead to the saturation of many remotely sensed vegetation indices. Our goal was to explain bird species richness patterns based on the DHIs in tropical areas. We related the DHIs to species richness of resident landbirds for five guilds (forest, scrub, grassland, generalist, and all resident birds) based on a) species distribution model (SDM) maps for 217 species, and b) range map for 564 species across Thailand. We also quantified the relative importance of the DHIs in multiple regression models that included two measures of topography, and two climate metrics using multiple regression, best-subsets, and hierarchical partitioning analyses. We found that the three DHIs alone explained forest bird richness best (R2adj 0.61 for both SDM- and rangemap based richness; 0.15–0.54 for the other guilds). When combining the DHIs with topography and climate, the richness of both forest birds and all resident bird species was equally well explained (R2adj 0.85 and 0.67 versus 0.81 and 0.68). Among the three DHIs, cumulative annual productivity had the greatest explanatory power for all guilds based on SDM richness maps (R2adj 0.54–0.61). The strong relationship between the DHIs and bird species richness in Thailand suggests that the DHIs capture energy availability well and are useful in biodiversity assessments and potentially bird conservation in tropical areas.', 'corpus_id': 199098266, 'score': 1}, {'doc_id': '82120544', 'title': 'Multi‐temporal distribution modelling with satellite tracking data: predicting responses of a long‐distance migrant to changing environmental conditions', 'abstract': 'Summary\r\n\r\n\r\n1.\u2002Despite the wealth of data available from satellite tracking (ST) studies, such data have rarely been used to model species distributions. Using a novel method, we show how to exploit satellite data to analyse whether and how a migratory species responds to fluctuating environmental conditions in its wintering area. This is particularly crucial for establishing comprehensive conservation measures for rare species in areas that are threatened by increasing land use and climate change.\r\n\r\n\r\n\r\n2.\u2002We use ST data of Eleonora’s falcon Falco eleonorae, a long-distance migratory raptor that winters in Madagascar, and assess the performance of static species distribution models (SDM) as well as multi-temporal models. ST data were derived from seven falcons tracked during three consecutive wintering periods and for a total of 2410 bearings, of which 512 locations were used in SDMs. We employed environmental predictors (climate, topography and land cover) with a spatial resolution of 30\xa0arc seconds (c. 1\xa0km2) to match rigorously filtered ST data with an accuracy of ≤1\xa0km.\r\n\r\n\r\n\r\n3.\u2002We first created a model with low temporal but high spatial resolution (half-year). To predict suitable habitat for each month of the wintering season, we took advantage of the high temporal resolution inherent in ST data and employed temporally corresponding remote sensing data [Normalized Difference Vegetation Index (NDVI) 10-day composites] together with other variables to create monthly models.\r\n\r\n\r\n\r\n4.\u2002We show that ST data are suited to build robust and transferable SDMs despite a low number of tracked individuals. Multi-temporal SMDs further revealed seasonal responses of the study species to changing environmental conditions in its wintering area.\r\n\r\n\r\n\r\n5.\u2002Synthesis and applications. We present a transferable approach to predict the potential distribution of organisms as well as their dynamic response to changing environmental conditions. Future conservation management plans could include the prediction of a species’ reaction to changing land-use practices or climate change based on the methodology proposed here. This would provide an early warning system for the decline of populations wintering in remote areas that underlie strong climatic fluctuations.', 'corpus_id': 82120544, 'score': 1}, {'doc_id': '237261004', 'title': 'Not all species will migrate poleward as the climate warms: the case of the seven baobab species in Madagascar.', 'abstract': ""It is commonly accepted that species should move toward higher elevations and latitudes to track shifting isotherms as climate warms. However, temperature might not be the only limiting factor determining species distribution. Species might move to opposite directions to track changes in other climatic variables. Here, we used an extensive occurrence dataset and an ensemble modelling approach to model the climatic niche and to predict the distribution of the seven baobab species (genus Adansonia) present in Madagascar. Using climatic projections from three global circulation models, we predicted species' future distribution and extinction risk for 2055 and 2085 under two representative concentration pathways (RCPs) and two dispersal scenarios. We disentangled the role of each climatic variable in explaining species range shift looking at relative variable importance and future climatic anomalies. Four baobab species (A. rubrostipa, A. madagascariensis, A. perrieri¸ and A. suarezensis) could experience a severe range contraction in the future (> 70% for year 2085 under RCP 8.5, assuming a zero-dispersal hypothesis). For three out of the four threatened species, range contraction was mainly explained by an increase in temperature seasonality, especially in the North of Madagascar, where they are currently distributed. In tropical regions, where species are commonly adapted to low seasonality, we found that temperature seasonality will generally increase. It is thus very likely that many species in the tropics will be forced to move equatorward to avoid an increase in temperature seasonality. Yet, several ecological (e.g. equatorial limit, or unsuitable deforested habitat) or geographical barriers (absence of lands) could prevent species to move equatorward, thus increasing the extinction risk of many tropical species, like endemic baobab species in Madagascar."", 'corpus_id': 237261004, 'score': 0}, {'doc_id': '183589618', 'title': 'Informationsverarbeitung im Entscheidungsprozeß', 'abstract': 'In Abschnitt 2.1 wird zunachst der Begriff der Information fur die weitere Arbeit definiert. Ausgehend von der Bedeutung, die dem Produktionsfaktor Information zukommt, wird eine operationale Definition des Informationsbegriffs in Entscheidungssituationen vorgenommen. Die Grundlage bildet das Verstandnis der Information als handlungsrelevantem Wissen. Bei den hier interessierenden Entscheidungsprozessen liegt dieses Wissen in unterschiedlichen Auspragungen vor, die teils in einem komplementaren, teils in einem substitutiven Verhaltnis stehen. Fur die Entwicklung des objektorientierten MIS ist an dieser Stelle Faktenwissen sowie die Kenntnis von Kausalzusammenhangen und die Klassifikationsfahigkeit von besonderer Bedeutung.', 'corpus_id': 183589618, 'score': 0}, {'doc_id': '211012522', 'title': 'Integrating dynamic environmental predictors and species occurrences: Toward true dynamic species distribution models', 'abstract': ""Abstract While biological distributions are not static and change/evolve through space and time, nonstationarity of climatic and land‐use conditions is frequently neglected in species distribution models. Even recent techniques accounting for spatiotemporal variation of species occurrence basically consider the environmental predictors as static; specifically, in most studies using species distribution models, predictor values are averaged over a 50‐ or 30‐year time period. This could lead to a strong bias due to monthly/annual variation between the climatic conditions in which species' locations were recorded and those used to develop species distribution models or even a complete mismatch if locations have been recorded more recently. Moreover, the impact of land‐use change has only recently begun to be fully explored in species distribution models, but again without considering year‐specific values. Excluding dynamic climate and land‐use predictors could provide misleading estimation of species distribution. In recent years, however, open‐access spatially explicit databases that provide high‐resolution monthly and annual variation in climate (for the period 1901–2016) and land‐use (for the period 1992–2015) conditions at a global scale have become available. Combining species locations collected in a given month of a given year with the relative climatic and land‐use predictors derived from these datasets would thus lead to the development of true dynamic species distribution models (D‐SDMs), improving predictive accuracy and avoiding mismatch between species locations and predictor variables. Thus, we strongly encourage modelers to develop D‐SDMs using month‐ and year‐specific climatic data as well as year‐specific land‐use data that match the period in which species data were collected."", 'corpus_id': 211012522, 'score': 1}, {'doc_id': '91389846', 'title': 'Understanding species distribution in dynamic populations: a new approach using spatio‐temporal point process models', 'abstract': 'Understanding and predicting a species’ distribution across a landscape is of central importance in ecology, biogeography and conservation biology. However, it presents daunting challenges when populations are highly dynamic (i.e. increasing or decreasing their ranges), particularly for small populations where information about ecology and life history traits is lacking. Currently, many modelling approaches fail to distinguish whether a site is unoccupied because the available habitat is unsuitable or because a species expanding its range has not arrived at the site yet. As a result, habitat that is indeed suitable may appear unsuitable. To overcome some of these limitations, we use a statistical modelling approach based on spatio‐temporal log‐Gaussian Cox processes. These model the spatial distribution of the species across available habitat and how this distribution changes over time, relative to covariates. In addition, the model explicitly accounts for spatio‐temporal dynamics that are unaccounted for by covariates through a spatio‐temporal stochastic process. We illustrate the approach by predicting the distribution of a recently established population of Eurasian cranes Grus grus in England, UK, and estimate the effect of a reintroduction in the range expansion of the population. Our models show that wetland extent and perimeter‐to‐area ratio have a positive and negative effect, respectively, in crane colonisation probability. Moreover, we find that cranes are more likely to colonise areas near already occupied wetlands and that the colonisation process is progressing at a low rate. Finally, the reintroduction of cranes in SW England can be considered a human‐assisted long‐distance dispersal event that has increased the dispersal potential of the species along a longitudinal axis in S England. Spatio‐temporal log‐Gaussian Cox process models offer an excellent opportunity for the study of species where information on life history traits is lacking, since these are represented through the spatio‐temporal dynamics reflected in the model.', 'corpus_id': 91389846, 'score': 1}, {'doc_id': '61134020', 'title': 'Combining static and dynamic variables in species distribution models under climate change', 'abstract': ""1.Methods used to predict shifts in species' ranges because of climate change commonly involve species distribution (niche) modelling using climatic variables, future values of which are predicted for the next several decades by general circulation models. However, species' distributions also depend on factors other than climate, such as land cover, land use and soil type. Changes in some of these factors, such as soil type, occur over geologic time and are thus imperceptible over the timescale of these types of projections. Other factors, such as land use and land cover, are expected to change over shorter timescales, but reliable projections are not available. Some important predictor variables, therefore, must be treated as unchanging, or static, whether because of the properties of the variable or out of necessity. The question of how best to combine dynamic variables predicted by climate models with static variables is not trivial and has been dealt with differently in studies to date. Alternative methods include using the static variables as masks, including them as independent explanatory variables in the model, or excluding them altogether. 2.Using a set of simulated species, we tested various methods for combining static variables with future climate scenarios. Our results showed that including static variables in the model with the dynamic variables performed better or no worse than either masking or excluding the static variables. 3.The difference in predictive ability was most pronounced when there is an interaction between the static and dynamic variables. 4.For variables such as land use, our results indicate that if such variables affect species distributions, including them in the model is better than excluding them, even though this may mean making the unrealistic assumption that the variable will not change in the future. 5.These results demonstrate the importance of including static and dynamic non-climate variables in addition to climate variables in species distribution models designed to predict future change in a species' habitat or distribution as a result of climate change. © 2011 The Authors. Methods in Ecology and Evolution © 2011 British Ecological Society."", 'corpus_id': 61134020, 'score': 1}, {'doc_id': '52846325', 'title': 'Skeletal semantics and their interpretations', 'abstract': 'The development of mechanised language specification based on structured operational semantics, with applications to verified compilers and sound program analysis, requires huge effort. General theory and frameworks have been proposed to help with this effort. However, none of this work provides a systematic way of developing concrete and abstract semantics, connected together by a general consistency result. We introduce a skeletal semantics of a language, where each skeleton describes the complete semantic behaviour of a language construct. We define a general notion of interpretation, which provides a systematic and language-independent way of deriving semantic judgements from the skeletal semantics. We explore four generic interpretations: a simple well-formedness interpretation; a concrete interpretation; an abstract interpretation; and a constraint generator for flow-sensitive analysis. We prove general consistency results between interpretations, depending only on simple language-dependent lemmas. We illustrate our ideas using a simple While language.', 'corpus_id': 52846325, 'score': 0}, {'doc_id': '235814885', 'title': 'Rapid Ecosystem Change at the Southern Limit of the Canadian Arctic, Torngat Mountains National Park', 'abstract': 'Northern protected areas guard against habitat and species loss but are themselves highly vulnerable to environmental change due to their fixed spatial boundaries. In the low Arctic, Torngat Mountains National Park (TMNP) of Canada, widespread greening has recently occurred alongside warming temperatures and regional declines in caribou. Little is known, however, about how biophysical controls mediate plant responses to climate warming, and available observational data are limited in temporal and spatial scope. In this study, we investigated the drivers of land cover change for the 9700 km2 extent of the park using satellite remote sensing and geostatistical modelling. Random forest classification was used to hindcast and simulate land cover change for four different land cover types from 1985 to 2019 with topographic and surface reflectance imagery (Landsat archive). The resulting land cover maps, in addition to topographic and biotic variables, were then used to predict where future shrub expansion is likely to occur using a binomial regression framework. Land cover hindcasts showed a 235% increase in shrub and a 105% increase in wet vegetation cover from 1985/89 to 2015/19. Shrub cover was highly persistent and displaced wet vegetation in southern, low-elevation areas, whereas wet vegetation expanded to formerly dry, mid-elevations. The predictive model identified both biotic (initial cover class, number of surrounding shrub neighbors), and topographic variables (elevation, latitude, and distance to the coast) as strong predictors of future shrub expansion. A further 51% increase in shrub cover is expected by 2039/43 relative to 2014 reference data. Establishing long-term monitoring plots within TMNP in areas where rapid vegetation change is predicted to occur will help to validate remote sensing observations and will improve our understanding of the consequences of change for biotic and abiotic components of the tundra ecosystem, including important cultural keystone species.', 'corpus_id': 235814885, 'score': 0}, {'doc_id': '236968585', 'title': ""The evolutionary genomics of species' responses to climate change."", 'abstract': ""Climate change is a threat to biodiversity. One way that this threat manifests is through pronounced shifts in the geographical range of species over time. To predict these shifts, researchers have primarily used species distribution models. However, these models are based on assumptions of niche conservatism and do not consider evolutionary processes, potentially limiting their accuracy and value. To incorporate evolution into the prediction of species' responses to climate change, researchers have turned to landscape genomic data and examined information about local genetic adaptation using climate models. Although this is an important advancement, this approach currently does not include other evolutionary processes-such as gene flow, population dispersal and genomic load-that are critical for predicting the fate of species across the landscape. Here, we briefly review the current practices for the use of species distribution models and for incorporating local adaptation. We next discuss the rationale and theory for considering additional processes, reviewing how they can be incorporated into studies of species' responses to climate change. We summarize with a conceptual framework of how manifold layers of information can be combined to predict the potential response of specific populations to climate change. We illustrate all of the topics using an exemplar dataset and provide the source code as potential tutorials. This Perspective is intended to be a step towards a more comprehensive integration of population genomics with climate change science."", 'corpus_id': 236968585, 'score': 0}]"
99	Charged particle dynamics	b76103a1efdfa057aad573fca5b429b9	2040	{}	"[{'doc_id': '125982339', 'title': 'Energy behaviour of the Boris method for charged-particle dynamics', 'abstract': 'The Boris algorithm is a widely used numerical integrator for the motion of particles in a magnetic field. This article proves near-conservation of energy over very long times in the special cases where the magnetic field is constant or the electric potential is quadratic. When none of these assumptions is satisfied, it is illustrated by numerical examples that the numerical energy can have a linear drift or its error can behave like a random walk. If the system has a rotational symmetry and the magnetic field is constant, then also the momentum is approximately preserved over very long times, but in a spatially varying magnetic field this is generally not satisfied.', 'corpus_id': 125982339, 'score': 1}, {'doc_id': '53651085', 'title': 'Long-term analysis of a variational integrator for charged-particle dynamics in a strong magnetic field', 'abstract': 'The differential equations of motion of a charged particle in a strong non-uniform magnetic field have the magnetic moment as an adiabatic invariant. This quantity is nearly conserved over long time scales covering arbitrary negative powers of the small parameter, which is inversely proportional to the strength of the magnetic field. The numerical discretisation is studied for a variational integrator that is an analogue for charged-particle dynamics of the Störmer–Verlet method. This numerical integrator is shown to yield near-conservation of a modified magnetic moment and a modified energy over similarly long times. The proofs for both the continuous and the discretised equations use modulated Fourier expansions with state-dependent frequencies and eigenvectors.', 'corpus_id': 53651085, 'score': 1}, {'doc_id': '220363840', 'title': 'Analysis of a splitting scheme for a class of nonlinear stochastic Schrödinger equations', 'abstract': 'We analyze the qualitative properties and the order of convergence of a splitting scheme for a class of nonlinear stochastic Schrodinger equations driven by additive Ito noise. The class of nonlinearities of interest includes nonlocal interaction cubic nonlinearities. We show that the numerical solution is symplectic and preserves the expected mass for all times. On top of that, for the convergence analysis, some exponential moment bounds for the exact and numerical solutions are proved. This enables us to provide strong orders of convergence as well as orders of convergence in probability and almost surely. Finally, extensive numerical experiments illustrate the performance of the proposed numerical scheme.', 'corpus_id': 220363840, 'score': 0}, {'doc_id': '218971947', 'title': 'Drift-preserving numerical integrators for stochastic Poisson systems', 'abstract': 'We perform a numerical analysis of randomly perturbed Poisson systems. For the considered Ito perturbation of Poisson differential equations, we show the longtime behavior of the energy and quadratic Casimirs for the exact solution. We then propose and analyze a drift-preserving splitting scheme for such problems with the following properties: exact drift preservation of energy and quadratic Casimirs, mean-square order of convergence one, weak order of convergence two. Finally, extensive numerical experiments illustrate the performance of the proposed numerical scheme.', 'corpus_id': 218971947, 'score': 0}, {'doc_id': '24261012', 'title': 'The Cayley transform in the numerical solution of unitary differential systems', 'abstract': 'In recent years some numerical methods have been developed to integrate matrix differential systems whose solutions are unitary matrices. In this paper we propose a new approach that transforms the original problem into a skew-Hermitian differential system by means of the Cayley transform. The new methods are semi-explicit, that is, no iteration is required but the solution of a certain number of linear matrix systems at each step is needed. Several numerical comparisons with known unitary integrators are reported.', 'corpus_id': 24261012, 'score': 1}, {'doc_id': '28622593', 'title': 'Splitting methods for time integration of trajectories in combined electric and magnetic fields.', 'abstract': 'The equations of motion of a single particle subject to an arbitrary electric and a static magnetic field form a Poisson system. We present a second-order time integration method which preserves well the Poisson structure and compare it to commonly used algorithms, such as the Boris scheme. All the methods are represented in a general framework of splitting methods. We use the so-called ϕ functions, which give efficient ways for both analyzing and implementing the algorithms. Numerical experiments show an excellent long term stability for the method considered.', 'corpus_id': 28622593, 'score': 1}, {'doc_id': '218571410', 'title': 'Geometric numerical integration of Lìenard systems via a contact Hamiltonian approach', 'abstract': 'Starting from a contact Hamiltonian description of Liénard systems, we introduce a new family of explicit geometric integrators for these nonlinear dynamical systems. Focusing on the paradigmatic example of the van der Pol oscillator, we demonstrate that these integrators are particularly stable and preserve the qualitative features of the dynamics, even for relatively large values of the time step and in the stiff regime.', 'corpus_id': 218571410, 'score': 0}, {'doc_id': '218613769', 'title': 'A time splitting method for the three-dimensional linear Pauli equation', 'abstract': 'We present and analyze a numerical method to solve the time-dependent linear Pauli equation in three space-dimensions. The Pauli equation is a ""semi-relativistic"" generalization of the Schrodinger equation for 2-spinors which accounts both for magnetic fields and for spin, the latter missing in predeeding work on the linear magnetic Schrodinger equation. We use a four operator splitting in time, prove stability and convergence of the method and derive error estimates as well as meshing strategies for the case of given time-independent electromagnetic potentials (= ""linear"" case), thus providing a generalization of previous results for the magnetic Schrodinger equation. Some proof of concept examples of numerical simulations are presented.', 'corpus_id': 218613769, 'score': 0}, {'doc_id': '197431211', 'title': 'A filtered Boris algorithm for charged-particle dynamics in a strong magnetic field', 'abstract': 'A modification of the standard Boris algorithm, called filtered Boris algorithm, is proposed for the numerical integration of the equations of motion of charged particles in a strong non-uniform magnetic field in the asymptotic scaling known as maximal ordering. With an appropriate choice of filters, second-order error bounds in the position and in the parallel velocity, and first-order error bounds in the normal velocity are obtained with respect to the scaling parameter. This also yields a second-order approximation to the guiding center motion. The proof compares the modulated Fourier expansions of the exact and the numerical solutions. Numerical experiments illustrate the error behaviour of the filtered Boris algorithm.', 'corpus_id': 197431211, 'score': 1}, {'doc_id': '218889855', 'title': 'Compositions of pseudo-symmetric integrators with complex coefficients for the numerical integration of differential equations', 'abstract': 'Abstract In this paper, we are concerned with the construction and analysis of a new class of methods obtained as double jump compositions with complex coefficients and projection on the real axis. It is shown in particular that the new integrators are symmetric and symplectic up to high orders if one uses a symmetric and symplectic basic method. In terms of efficiency, the aforementioned technique requires fewer stages than standard compositions of the same orders and is thus expected to lead to faster methods.', 'corpus_id': 218889855, 'score': 0}]"
100	Cell division	26fb6152280bc2a482fcf52e900114fa	10838	{}	"[{'doc_id': '226229084', 'title': 'A tunable multicellular timer in bacterial consortia', 'abstract': 'Processing time-dependent information requires cells to quantify the durations of past regulatory events and program the time span of future signals. Such timer mechanisms are difficult to implement at the level of single cells, however, due to saturation in molecular components and stochasticity in the limited intracellular space. Multicellular implementations, on the other hand, outsource some of the components of information-processing circuits to the extracellular space, and thereby might escape those constraints. Here we develop a theoretical framework, based on a trilinear coordinate representation, to study the collective behavior of a three-strain bacterial population under stationary conditions. This framework reveals that distributing different processes (in our case the production, detection and degradation of a time-encoding signal) across distinct bacterial strains enables the robust implementation of a multicellular timer. Our analysis also shows the circuit to be easily tunable by varying the relative frequencies of the bacterial strains composing the consortium.', 'corpus_id': 226229084, 'score': 1}, {'doc_id': '234349421', 'title': 'Growth-dependent heterogeneity in the DNA damage response in Escherichia coli', 'abstract': 'In natural environments bacteria are frequently exposed to sub-lethal levels of DNA damage which leads to the induction of a stress response (the SOS response in Escherichia coli). Natural environments also vary in nutrient availability, resulting in distinct physiological changes in bacteria which may have direct implications on their capacity to repair their chromosomes. Here, we evaluated the impact of varying the nutrient availability on the expression of the SOS response induced by chronic sub-lethal DNA damage in E. coli. The expression of the SOS regulon was found to be highly heterogeneous at the single-cell level in all growth conditions. Surprisingly, we observed a larger fraction of high SOS-induced cells in slow growth as compared with fast growth, despite a higher rate of SOS induction in fast growth. This counter-intuitive result can be explained by the dynamic balance between the rate of SOS induction and the division rates of cells exposed to DNA damage. Taken together, our data illustrates how cell division and physiology come together to produce growth-dependent heterogeneity in the DNA damage response.', 'corpus_id': 234349421, 'score': 0}, {'doc_id': '173188329', 'title': 'Unification of cell division control strategies through continuous rate models.', 'abstract': 'Recent experiments support the adder model for E. coli division control. This model posits that bacteria grow, on average, a fixed size before division. It also predicts decorrelation between the noise in the added size and the size at birth. Here we develop a theory based on stochastic hybrid systems which could explain the main division strategies, including not only the adder strategy but the whole range from sizer to timer. We use experiments to explore the division control of E. coli growing with glycerol as carbon source. In this medium, the division strategy is sizerlike, which means that the added size decreases with the size at birth. We found, as our theory predicts, that in a sizerlike strategy the mean added size decreases with the size at birth while the noise in added size increases. We discuss possible molecular mechanisms underlying this strategy and propose a general model that encompasses the different division strategies.', 'corpus_id': 173188329, 'score': 1}, {'doc_id': '233175469', 'title': 'A single-cell resolved cell-cell communication model explains lineage commitment in hematopoiesis', 'abstract': 'Cells do not function in isolation. Arguably, every cell fate decision occurs in response to environmental signals. In many cases cell-cell communication alters the dynamics of a cell’s internal gene regulatory network to initiate cell fate transitions, yet models rarely take this into account. Here we develop a multiscale perspective to study the granulocyte-monocyte vs. megakaryocyte-erythrocyte fate decisions. This transition is dictated by the GATA1-PU.1 network, a classical example of a bistable cell fate system. We show that, for a wide range of cell communication topologies, even subtle changes in signaling can have pronounced effects on cell fate decisions. We go on to show how cell-cell coupling through signaling can spontaneously break the symmetry of a homogenous cell population. Noise, both intrinsic and extrinsic, shapes the decision landscape profoundly, and affects the transcriptional dynamics underlying this important hematopoietic cell fate decision-making system.', 'corpus_id': 233175469, 'score': 0}, {'doc_id': '44122072', 'title': 'Cell size control and gene expression homeostasis in single-cells.', 'abstract': 'Growth of a cell and its subsequent division into daughters is a fundamental aspect of all cellular living systems. During these processes, how do individual cells correct size aberrations so that they do not grow abnormally large or small? How do cells ensure that the concentration of essential gene products are maintained at desired levels, in spite of dynamic/stochastic changes in cell size during growth and division? Both these questions have fascinated researchers for over a century. We review how advances in singe-cell technologies and measurements are providing unique insights into these questions across organisms from prokaryotes to human cells. More specifically, diverse strategies based on timing of cell-cycle events, regulating growth, and number of daughters are employed to maintain cell size homeostasis. Interestingly, size homeostasis often results in size optimality - proliferation of individual cells in a population is maximized at an optimal cell size. We further discuss how size-dependent expression or gene-replication timing can buffer concentration of a gene product from cell-to-cell size variations within a population. Finally, we speculate on an intriguing hypothesis that specific size control strategies may have evolved as a consequence of gene-product concentration homeostasis.', 'corpus_id': 44122072, 'score': 1}, {'doc_id': '209481819', 'title': 'Efficient computation of stochastic cell-size transient dynamics', 'abstract': 'How small, fast-growing bacteria ensure tight cell-size distributions remains elusive. High-throughput measurement techniques have propelled efforts to build modeling tools that help to shed light on the relationships between cell size, growth and cycle progression. Most proposed models describe cell division as a discrete map between size at birth and size at division with stochastic fluctuations assumed. However, such models underestimate the role of cell size transient dynamics by excluding them. We propose an efficient approach for estimation of cell size transient dynamics. Our technique approximates the transient size distribution and statistical moment dynamics of exponential growing cells following an adder strategy with arbitrary precision. We approximate, up to arbitrary precision, the distribution of division times and size across time for the adder strategy in rod-shaped bacteria cells. Our approach is able to compute statistical moments like mean size and its variance from such distributions efficiently, showing close match with numerical simulations. Additionally, we observed that these distributions have periodic properties. Our approach further might shed light on the mechanisms behind gene product homeostasis.', 'corpus_id': 209481819, 'score': 1}, {'doc_id': '221557379', 'title': 'Population dynamics model and analysis for bacteria transformation and conjugation', 'abstract': ""We present a two-species population model in a well-mixed environment where the dynamics involves, in addition to birth and death, changes due to environmental factors and inter-species interactions. The novel dynamical components are motivated by two common mechanisms for developing antibiotic resistance in bacteria: plasmid {\\it transformation}, where external genetic material in the form of a plasmid is transferred inside a host cell; and {\\it conjugation} by which one cell transfers genetic material to another by direct cell-to-cell contact. Through analytical and numerical methods, we identify the effects of transformation and conjugation individually. With transformation only, the two-species system will evolve towards one species' extinction, or a stable co-existence in the long-time limit. With conjugation only, we discover interesting oscillations for the system. Further, we quantify the combined effects of transformation and conjugation, and chart the regimes of stable co-existence, a result with ecological implications."", 'corpus_id': 221557379, 'score': 1}, {'doc_id': '233223231', 'title': 'Probing bacterial cell wall growth by tracing wall-anchored protein complexes', 'abstract': 'The dynamic assembly of the cell wall is key to the maintenance of cell shape during bacterial growth. Here, we present a method for the analysis of Escherichia coli cell wall growth at high spatial and temporal resolution, which is achieved by tracing the movement of fluorescently labeled cell wall-anchored flagellar motors. Using this method, we clearly identify the active and inert zones of cell wall growth during bacterial elongation. Within the active zone, the insertion of newly synthesized peptidoglycan occurs homogeneously in the axial direction without twisting of the cell body. Based on the measured parameters, we formulate a Bernoulli shift map model to predict the partitioning of cell wall-anchored proteins following cell division.', 'corpus_id': 233223231, 'score': 0}, {'doc_id': '234791534', 'title': 'Cell-scale biophysical determinants of cell competition in epithelia', 'abstract': 'How cells with different genetic makeups compete in tissues is an outstanding question in developmental biology and cancer research. Studies in recent years have revealed that cell competition can either be driven by short-range biochemical signalling or by long-range mechanical stresses in the tissue. To date, cell competition has generally been characterised at the population scale, leaving the single-cell-level mechanisms of competition elusive. Here, we use high time-resolution experimental data to construct a multi-scale agent-based model for epithelial cell competition and use it to gain a conceptual understanding of the cellular factors that governs competition in cell populations within tissues. We find that a key determinant of mechanical competition is the difference in homeostatic density between winners and losers, while differences in growth rates and tissue organisation do not affect competition end result. In contrast, the outcome and kinetics of biochemical competition is strongly influenced by local tissue organisation. Indeed, when loser cells are homogenously mixed with winners at the onset of competition, they are eradicated; however, when they are spatially separated, winner and loser cells coexist for long times. These findings suggest distinct biophysical origins for mechanical and biochemical modes of cell competition.', 'corpus_id': 234791534, 'score': 0}, {'doc_id': '234597920', 'title': 'Single-Cell Stochastic Modeling of the Action of Antimicrobial Peptides on Bacteria', 'abstract': 'Antimicrobial peptides (AMPs) produced by multi-cellular organisms as their immune system’s defense against microbes are actively considered as natural alternatives to conventional antibiotics. Although a substantial progress has been achieved in studying the AMPs, the microscopic mechanisms of their functioning remain not well understood. Here, we develop a new theoretical framework to investigate how the AMPs are able to efficiently neutralize the bacteria. In our minimal theoretical model, the most relevant processes, AMPs entering into and the following inhibition of the single bacterial cell, are described stochastically. Using complementary master equations approaches, all relevant features of bacteria clearance dynamics by AMPs, such as the probability of inhibition and the mean times before the clearance, are explicitly evaluated. It is found that both processes, entering and inhibition, are equally important for the efficient functioning of AMPs. Our theoretical method naturally explains a wide spectrum of efficiencies of existing AMPs and their heterogeneity at the single-cell level. Theoretical calculations are also consistent with existing single-cell measurements. Thus, the presented theoretical approach clarifies some microscopic aspects of the action of AMPs on bacteria.', 'corpus_id': 234597920, 'score': 0}]"
101	Experiential Learning	afc7c4b9f47206f79fede067279a1d9a	16603	{}	"[{'doc_id': '233540461', 'title': 'Exploring business doctoral students attitudes, training, and use of classroom experiential learning activities', 'abstract': 'Abstract There are growing calls both within universities and from external stakeholders to utilize experiential pedagogy in business courses. In this mixed-methods study, we use the Theory of Planned Behavior and Social Learning Theory as frameworks to investigate how business doctoral students learn about and use experiential pedagogical practices. Findings from quantitative data show that a conceptual change (student-focused) attitude towards teaching has a positive relationship with the use of experiential learning activities. Interestingly, teaching norms and pedagogical training did not have a direct relationship with these pedagogical practices. Exploring qualitative responses revealed that doctoral student pedagogical practices are influenced by other doctoral students, observing faculty teaching behaviors, their own learning experiences as students, and previous experience before the doctoral program in education and training environments. This study contributes to ongoing conversations about business pedagogical practices and business doctoral training on teaching. Limitations and possibilities for future research are discussed.', 'corpus_id': 233540461, 'score': 1}, {'doc_id': '233327077', 'title': 'Integrating Research into the Undergraduate Curriculum: 2. Scaffolding Research Skills and Transitioning toward Independent Research.', 'abstract': 'Undergraduate research experiences are widely regarded as high-impact practices that foster meaningful mentoring relationships, enhance retention and graduation, and stimulate postbaccalaureate enrollment in STEM graduate and professional programs. Through immersion in a mentored original research project, student develop and apply their skills in critical thinking, problem solving, intellectual independence, communication, collaboration, project ownership, innovation, and leadership. These skills are readily transferable to a wide array of future careers in and beyond STEM that are well-served by evidence-based approaches. The 2019 Society for Neuroscience meeting included a well-attended workshop on integrating research into the curriculum at primarily undergraduate institutions (PUIs). This article is the second of three articles that summarize, analyze, and expand the workshop discussions. In this second article, we specifically describe approaches to transitional research courses that prepare students for independent research experiences such as undergraduate research theses. Educators can intentionally scaffold research experience and skills across the curriculum, to foster participation in scientific research and enhance diversity, equity, and inclusivity in research training. This article provides an overview of important goals and considerations for intermediate undergraduate research experiences, specific examples from several institutions of transitional courses that scaffold research preparation using different structures, and a summary of lessons learned from these experiences.', 'corpus_id': 233327077, 'score': 1}, {'doc_id': '233475079', 'title': 'Creating Connections: Engaging Student Library Employees through Experiential Learning', 'abstract': 'Abstract Academic libraries are finding that involving student employees in collaborative projects fosters student development while increasing library capability and impact. This article validates and builds on Denda and Hunter’s team-based engagement framework. The authors expound on that model with principles of experiential learning that apply to a broad scope of student library work. They demonstrate this approach through creative project examples, including the exhibit “Connection.” Engaging student library employees through workplace experiential learning connects them to people and purpose, fostering skill development and a service mindset for their library roles and future careers.', 'corpus_id': 233475079, 'score': 0}, {'doc_id': '233402193', 'title': 'Transformative Practices to Support First-Generation College Students as Academic Learners: Findings From a Systematic Literature Review', 'abstract': 'ABSTRACT Literature on first-generation college students contains a range of practices that can support first-generation college students as academic learners. To synthesize these practices for higher education practitioners, we conducted a systematic literature review of 53 articles. We identified three types of transformative practices from the literature: (a) promoting an institutional interdependent learning culture, (b) providing explicit support for academic learning, and (c) creating learning experiences that center communal goals. Across these three practices, we demonstrate the importance of educational practitioners drawing on students’ lived experiences as a valuable source of knowledge for their collegiate learning experience.', 'corpus_id': 233402193, 'score': 0}, {'doc_id': '233392145', 'title': 'Experiential Philanthropy : A New and Innovative Service-Learning Pedagogy', 'abstract': 'Experiential philanthropy is an innovative service-learning pedagogy in nonprofit management education. The pedagogy is intended to integrate academic learning with community engagement by allowing students an opportunity to study social problems and nonprofit organizations and then make decisions about directly or indirectly investing funds in them. Ultimately, experiential philanthropy is intended to teach students not only about issues associated with the management of nonprofit organizations but also about how to evaluate philanthropic responses to social issues. Introduction', 'corpus_id': 233392145, 'score': 0}, {'doc_id': '233600494', 'title': 'Enhancing Preservice Teacher Preparation through Formal and Informal Learning Experiences', 'abstract': 'Learning through formal and informal experiences is critical for building content knowledge, pedagogical skills, and self-efficacy/confidence for preservice teachers. teachHOUSTON offers numerous teacher enhancement opportunities outside the teacher education courses which allows preservice teachers to connect to the real world which includes being able to relate to a diverse population of students and to understand how the course content can be related to them, their families, and communities in their everyday experiences. Through formal and informal experiences such as professional development workshops, discipline specific courses, research experiences, and internships, preservice teachers have the opportunity to engage in hands-on science activities they can use with their students, develop lessons, and gain knowledge on how to deliver this content while managing their classroom. This chapter will give an overview of the formal and informal experiences offered through teachHOUSTON with a highlight on the structure and content of the six week Noyce Internship Program which engaged interns as counselors and teaching assistants in a summer STEM camp for underserved middle school students and introduces the interns to interactive sessions that model promising practices for teaching.', 'corpus_id': 233600494, 'score': 1}, {'doc_id': '233542115', 'title': 'Enhancing university student employability through practical experiential learning in the sport industry: An industry-academia cooperation case from Taiwan', 'abstract': 'Abstract The purpose of this study is to develop an industry-academia strategy to help undergraduate sport management students enhance employability through practical experiential learning in a specific sporting event. Both in-depth interviews and a questionnaire survey were administered to teachers, enterprise mentors and students to evaluate students’ learning outcomes and reveal how embedding PEL into a professional course could enhance employability. The results not only identify the necessary employability skills, attitudes, and traits that students should possess before graduation, but also address the benefits of incorporating PEL into professional curriculum through practicing the learning-by-doing philosophy.', 'corpus_id': 233542115, 'score': 0}, {'doc_id': '233663825', 'title': 'Using Integrated Course Design for Flipped Classroom to Promote Active Learning of Lean Six Sigma for Supply Chains', 'abstract': 'This paper showcases an educational experience for a course titled “Lean Six Sigma for Supply Chains” taught at university level, where an integrated course design to engage students in a flipped classroom method is used. The course design incorporates a synergy between Fink’s integrated course design model, experiential learning activities and formative assessments. The primary aim is to engage students in active learning of the course content. Therefore, all learning activities are designed according to the course learning goals, curricular and assessment requirements, prior knowledge of the students, and learning modes that are available to the students. These aspects allow educators to determine what is to be discussed in the classroom and how to facilitate active learning in-class and out-of-class. The course also leverages on the online learning space to develop both asynchronous and synchronous learning activities in order to engage student in their learning. To achieve the course learning goals, students are stimulated to participate in these learning activities and self-directed learning, in order to gain discipline specific knowledge and skills. This paper provides practical advice for course designers and programme leaders on how they can adopt an integrated course design approach in designing instructional activities for a flipped classroom setting to enhance student engagement and learning.', 'corpus_id': 233663825, 'score': 0}, {'doc_id': '233480907', 'title': 'Engagement in Practice: Engaging Undergraduate Students in a Multidisci- plinary Service-Learning Environment', 'abstract': 'Dr. Wei Lu is a Postdoctoral Researcher at the Department of Engineering Technology & Industrial Distribution at Texas A&M University. Her research focuses on Higher Education in Agriculture & Engineering, K-12 (STEM) Education, Communications, Marketing, and Social Economics. Master of Science, Agricultural Economics, Texas A&M University Doctor of Philosophy, Agricultural Leadership, Education& Communications, Texas A&M University', 'corpus_id': 233480907, 'score': 1}, {'doc_id': '29963640', 'title': 'Task-based internships: Fostering ideal learning through focused experience', 'abstract': ""Student internship programs are characterized by their non-traditional educational approach based on experiential learning, taking students out into the industry to gain experience in their field of study. This case study presents the new Task-Based Internship (TBI) course offered by the School of Business, University of Nicosia, Cyprus. The course is examined in terms of benefits and areas for improvement based on the students' course evaluation. The student feedback on their TBI experience has been elicited through an online questionnaire. The study provides useful information for institution administrators and educators and recommends the TBI course to be included in the various curriculums of the general Business Field."", 'corpus_id': 29963640, 'score': 1}]"
102	healthcare xai	6e615d0a6d50b32fed172fa11a39546b	20507	{}	"[{'doc_id': '236881288', 'title': 'Knowledge-Intensive Language Understanding for Explainable AI', 'abstract': ""AI systems have seen significant adoption in various domains. At the same time, further adoption in some domains is hindered by the inability to fully trust an AI system that it will not harm a human. Besides, fairness, privacy, transparency, and explainability are vital to developing trust in AI systems. As stated in Describing Trustworthy AI,aa.https://www.ibm.com/watson/trustworthy-ai. “Trust comes through understanding. How AI-led decisions are made and what determining factors were included are crucial to understand.” The subarea of explaining AI systems has come to be known as XAI. Multiple aspects of an AI system can be explained; these include biases that the data might have, lack of data points in a particular region of the example space, fairness of gathering the data, feature importances, etc. However, besides these, it is critical to have human-centered explanations directly related to decision-making, similar to how a domain expert makes decisions based on “domain knowledge,” including well-established, peer-validated explicit guidelines. To understand and validate an AI system's outcomes (such as classification, recommendations, predictions) that lead to developing trust in the AI system, it is necessary to involve explicit domain knowledge that humans understand and use. Contemporary XAI methods are yet addressed explanations that enable decision-making similar to an expert. Figure 1 shows the stages of adoption of an AI system into the real world."", 'corpus_id': 236881288, 'score': 0}, {'doc_id': '28079832', 'title': 'New Drugs for Chronic Myelogenous Leukemia', 'abstract': 'The introduction of tyrosine kinase inhibitors (TKIs) has changed the landscape of therapy for chronic myelogenous leukemia (CML). Once considered an incurable malignancy, CML now has become a manageable chronic condition. Despite the great advances that imatinib has brought to the treatment of CML, some patients still develop resistance to imatinib and other TKIs, such as dasatinib and nilotinib. Furthermore, none of the clinically available TKIs is capable of eradicating leukemia stem cells and therefore curing CML. Several new compounds have been developed in recent years in an attempt to manage TKI-resistant CML. These include third-generation TKIs (ponatinib, danusertib) and even old compounds such as omacetaxine, which were developed before imatinib and now find a possible niche in the treatment of imatinib-resistant CML. We review the current preclinical and clinical data on the most promising new compounds for the treatment of CML.', 'corpus_id': 28079832, 'score': 0}, {'doc_id': '237484026', 'title': 'Identifying the Main Risk Factors for CVD Prediction Using Machine Learning Algorithms', 'abstract': 'CVDs are a leading cause of death globally. In CVDs, the heart is unable to deliver enough blood to other body regions. Since effective and accurate diagnosis of CVDs is essential for CVD prevention and treatment, machine learning (ML) techniques can be effectively and reliably used to discern patients suffering from a CVD from those who do not suffer from any heart condition. Namely, machine learning algorithms (MLAs) play a key role in the diagnosis of CVDs through predictive models that allow us to identify the main risks factors influencing CVD development. In this study, we analyze the performance of ten MLAs on two datasets for CVD prediction and two for CVD diagnosis. Algorithm performance is analyzed on top-two and top-four dataset attributes/features with respect to five performance metrics –accuracy, precision, recall, f1-score, and roc-auc – using the train-test split technique and k-fold cross-validation. Our study identifies the top two and four attributes from each CVD diagnosis/prediction dataset. As our main findings, the ten MLAs exhibited appropriate diagnosis and predictive performance; hence, they can be successfully implemented for improving current CVD diagnosis efforts and help patients around the world, especially in regions where medical staff is lacking.', 'corpus_id': 237484026, 'score': 0}, {'doc_id': '237385754', 'title': 'The Role of Explainability in Assuring Safety of Machine Learning in Healthcare', 'abstract': 'Established approaches to assuring safety-critical systems and software are difficult to apply to systems employing machine learning (ML). In many cases, ML is used on ill-defined problems, e.g. optimising sepsis treatment, where there is no clear, pre-defined specification against which to assess validity. This problem is exacerbated by the “opaque” nature of ML where the learnt model is not amenable to human scrutiny. Explainable AI methods have been proposed to tackle this issue by producing human-interpretable representations of ML models which can help users to gain confidence and build trust in the ML system. However, there is not much work explicitly investigating the role of explainability for safety assurance in the context of ML development. This paper identifies ways in which explainable AI methods can contribute to safety assurance of ML-based systems. It then uses a concrete ML-based clinical decision support system, concerning weaning of patients from mechanical ventilation, to demonstrate how explainable AI methods can be employed to produce evidence to support safety assurance. The results are also represented in a safety argument to show where, and in what way, explainable AI methods can contribute to a safety case. Overall, we conclude that explainable AI methods have a valuable role in safety assurance of ML-based systems in healthcare but that they are not sufficient in themselves to assure safety.', 'corpus_id': 237385754, 'score': 1}, {'doc_id': '237556401', 'title': 'Opening the black box: the promise and limitations of explainable machine learning in cardiology.', 'abstract': 'Many clinicians remain wary of machine learning due to long-standing concerns about ""black box"" models. ""Black box"" is shorthand for models that are sufficiently complex that they are not straightforwardly interpretable to humans. Lack of interpretability in predictive models can undermine trust in those models, especially in health care where so many decisions are literally life and death. There has recently been an explosion of research in the field of explainable machine learning aimed at addressing these concerns. The promise of explainable machine learning is considerable, but it is important for cardiologists who may encounter these techniques in clinical decision support tools or novel research papers to have a critical understanding of both their strengths and their limitations. This paper reviews key concepts and techniques in the field of explainable machine learning as they apply to cardiology. Key concepts reviewed include interpretability versus explainability and global versus local explanations. Techniques demonstrated include permutation importance, surrogate decision trees, local interpretable model-agnostic explanations, and partial dependence plots. We discuss several limitations with explainability techniques, focusing on the how the nature of explanations as approximations may omit important information about how black box models work and why they make certain predictions. We conclude by proposing a rule of thumb about when it is appropriate to use black box models with explanations, rather than interpretable models.', 'corpus_id': 237556401, 'score': 1}, {'doc_id': '220605035', 'title': 'Explainable AI in Healthcare', 'abstract': 'Artificial Intelligence (AI) is an enabling technology that when integrated into healthcare applications and smart wearable devices such as Fitbits etc. can predict the occurrence of health conditions in users by capturing and analysing their health data. The integration of AI and smart wearable devices has a range of potential applications in the area of smart healthcare but there is a challenge in the black box operation of decisions made by AI models which have resulted in a lack of accountability and trust in the decisions made. Explainable AI (XAI) is a domain in which techniques are developed to explain predictions made by AI systems. In this paper, XAI is discussed as a technique that can used in the analysis and diagnosis of health data by AI-based systems and a proposed approach presented with the aim of achieving accountability. transparency, result tracing, and model improvement in the domain of healthcare.', 'corpus_id': 220605035, 'score': 1}, {'doc_id': '237278418', 'title': 'Improvement of a Prediction Model for Heart Failure Survival through Explainable Artificial Intelligence', 'abstract': 'Cardiovascular diseases and their associated disorder of heart failure are one of the major death causes globally, being a priority for doctors to detect and predict its onset and medical consequences. Artificial Intelligence (AI) allows doctors to discover clinical indicators and enhance their diagnosis and treatments. Specifically, “explainable AI” offers tools to improve the clinical prediction models that experience poor interpretability of their results. This work presents an explainability analysis and evaluation of a prediction model for heart failure survival by using a dataset that comprises 299 patients who suffered heart failure. The model employs a data workflow pipeline able to select the best ensemble tree algorithm as well as the best feature selection technique. Moreover, different post-hoc techniques have been used for the explainability analysis of the model. The paper’s main contribution is an explainability-driven approach to select the best prediction model for HF survival based on an accuracy-explainability balance. Therefore, the most balanced explainable prediction model implements an Extra Trees classifier over 5 selected features (follow-up time, serum creatinine, ejection fraction, age and diabetes) out of 12, achieving a balanced-accuracy of 85.1% and 79.5% with cross-validation and new unseen data respectively. The follow-up time is the most influencing feature followed by serum-creatinine and ejection-fraction. The explainable prediction model for HF survival presented in this paper would improve a further adoption of clinical prediction models by providing doctors with intuitions to better understand the reasoning of, usually, “black-box” AI clinical solutions, and make more reasonable and data-driven decisions.', 'corpus_id': 237278418, 'score': 1}, {'doc_id': '237418150', 'title': 'Discovering Composite Lifestyle Biomarkers With Artificial Intelligence From Clinical Studies to Enable Smart eHealth and Digital Therapeutic Services', 'abstract': 'Discovery of biomarkers is a continuous activity of the research community in the clinical domain that recently shifted its focus toward digital, non-traditional biomarkers that often use physiological, psychological, social, and environmental data to derive an intermediate biomarker. Such biomarkers, by triggering smart services, can be used in a clinical trial framework and eHealth or digital therapeutic services. In this work, we discuss the APACHE trial for determining the quality of life (QoL) of cervical cancer patients and demonstrate how we are discovering a biomarker for this therapeutic area that predicts significant QoL variations. To this extent, we present how real-world data can unfold a big potential for detecting the cervical cancer QoL biomarker and how it can be used for novel treatments. The presented methodology, derived in APACHE, is introduced by Healthentia eClinical solution, and it is beginning to be used in several clinical studies.', 'corpus_id': 237418150, 'score': 0}, {'doc_id': '220728700', 'title': 'Errata', 'abstract': '[This corrects the article DOI: 10.1590/1984-3143-AR2019-0130.][This corrects the article DOI: 10.1590/1984-3143-AR2019-0109.].', 'corpus_id': 220728700, 'score': 0}, {'doc_id': '237519952', 'title': 'Explainable Artificial Intelligence Based Framework for Non-Communicable Diseases Prediction', 'abstract': 'The rapid rise of non-communicable diseases (NCDs) becomes one of the serious health issues and the leading cause of death worldwide. In recent years, artificial intelligence-based systems have been developed to assist clinicians in decision-making to reduce morbidity and mortality. However, a common drawback of these modern studies is related to explanations of their output. In other words, understanding the inner logic behind the predictions is hidden to the end-user. Thus, clinicians struggle to interpret these models because of their black-box nature, and hence they are not acceptable in the medical practice. To address this problem, we have proposed a Deep Shapley Additive Explanations (DeepSHAP) based deep neural network framework equipped with a feature selection technique for NCDs prediction and explanation among the population in the United States. Our proposed framework comprises three components: First, representative features are done based on the elastic net-based embedded feature selection technique; second a deep neural network classifier is tuned with the hyper-parameters and used to train the model with the selected feature subset; third, two kinds of model explanation are provided by the DeepSHAP approach. Herein, (I) explaining the risk factors that affected the model’s prediction from the population-based perspective; (II) aiming to explain a single instance from the human-centered perspective. The experimental results indicated that the proposed model outperforms various state-of-the-art models. In addition, the proposed model can improve the medical understanding of NCDs diagnosis by providing general insights into the changes in disease risk at the global and local levels. Consequently, DeepSHAP based explainable deep learning framework contributes not only to the medical decision support systems but also can provide to real-world needs in other domains.', 'corpus_id': 237519952, 'score': 1}]"
103	Dengue and EWS	3eed2a4e9747d5288af8e4768cfeccc6	13848	{'EWS': 'Ewing sarcoma'}	"[{'doc_id': '19024384', 'title': 'Surveillance of Dengue Fever Virus: A Review of Epidemiological Models and Early Warning Systems', 'abstract': ""Dengue fever affects over a 100 million people annually hence is one of the world's most important vector-borne diseases. The transmission area of this disease continues to expand due to many direct and indirect factors linked to urban sprawl, increased travel and global warming. Current preventative measures include mosquito control programs, yet due to the complex nature of the disease and the increased importation risk along with the lack of efficient prophylactic measures, successful disease control and elimination is not realistic in the foreseeable future. Epidemiological models attempt to predict future outbreaks using information on the risk factors of the disease. Through a systematic literature review, this paper aims at analyzing the different modeling methods and their outputs in terms of acting as an early warning system. We found that many previous studies have not sufficiently accounted for the spatio-temporal features of the disease in the modeling process. Yet with advances in technology, the ability to incorporate such information as well as the socio-environmental aspect allowed for its use as an early warning system, albeit limited geographically to a local scale."", 'corpus_id': 19024384, 'score': 1}, {'doc_id': '230784497', 'title': 'Spatiotemporal Distribution of Zika Virus and Its Spatially Heterogeneous Relationship with the Environment', 'abstract': 'Infectious diseases have caused some of the most feared plagues and greatly harmed human health. However, despite the qualitative understanding that the occurrence and diffusion of infectious disease is related to the environment, the quantitative relations are unknown for many diseases. Zika virus (ZIKV) is a mosquito-borne virus that poses a fatal threat and has spread explosively throughout the world, impacting human health. From a geographical perspective, this study aims to understand the global hotspots of ZIKV as well as the spatially heterogeneous relationship between ZIKV and environmental factors using exploratory special data analysis (ESDA) model. A geographically weighted regression (GWR) model was used to analyze the influence of the dominant environmental factors on the spread of ZIKV at the continental scale. The results indicated that ZIKV transmission had obvious regional and seasonal heterogeneity. Population density, GDP per capita, and landscape fragmentation were the dominant environmental factors affecting the spread of ZIKV, which indicates that social factors had a greater influence than natural factors on the spread of it. As SARS-CoV-2 is spreading globally, this study can provide methodological reference for fighting against the pandemic.', 'corpus_id': 230784497, 'score': 1}, {'doc_id': '231138663', 'title': 'COVID-19 epidemic prediction and the impact of public health interventions: A review of COVID-19 epidemic models', 'abstract': '\n The coronavirus disease outbreak of 2019 (COVID-19) has been spreading rapidly to all corners of the word, in a very complex manner. A key research focus is in predicting the development trend of COVID-19 scientifically through mathematical modelling. We conducted a systematic review of epidemic prediction models of COVID-19 and the public health intervention strategies by searching the Web of Science database. 55 studies of the COVID-19 epidemic model were reviewed systematically. It was found that the COVID-19 epidemic models were different in the model type, acquisition method, hypothesis and distribution of key input parameters. Most studies used the gamma distribution to describe the key time period of COVID-19 infection, and some studies used the lognormal distribution, the Erlang distribution, and the Weibull distribution. The setting ranges of the incubation period, serial interval, infectious period and generation time were 4.9–7 days, 4.41–8.4 days, 2.3–10 days and 4.4–7.5 days, respectively, and more than half of the incubation periods were set to 5.1 or 5.2 days. Most models assumed that the latent period was consistent with the incubation period. Some models assumed that asymptomatic infections were infectious or pre-symptomatic transmission was possible, which overestimated the value of R0. For the prediction differences under different public health strategies, the most significant effect was in travel restrictions. There were different studies on the impact of contact tracking and social isolation, but it was considered that improving the quarantine rate and reporting rate, and the use of protective face mask were essential for epidemic prevention and control. The input epidemiological parameters of the prediction models had significant differences in the prediction of the severity of the epidemic spread. Therefore, prevention and control institutions should be cautious when formulating public health strategies by based on the prediction results of mathematical models.\n', 'corpus_id': 231138663, 'score': 0}, {'doc_id': '230582776', 'title': 'Development and dissemination of infectious disease dynamic transmission models during the COVID-19 pandemic: what can we learn from other pathogens and how can we move forward?', 'abstract': '\n The current COVID-19 pandemic has resulted in the unprecedented development and integration of infectious disease dynamic transmission models into policy making and public health practice. Models offer a systematic way to investigate transmission dynamics and produce short-term and long-term predictions that explicitly integrate assumptions about biological, behavioural, and epidemiological processes that affect disease transmission, burden, and surveillance. Models have been valuable tools during the COVID-19 pandemic and other infectious disease outbreaks, able to generate possible trajectories of disease burden, evaluate the effectiveness of intervention strategies, and estimate key transmission variables. Particularly given the rapid pace of model development, evaluation, and integration with decision making in emergency situations, it is necessary to understand the benefits and pitfalls of transmission models. We review and highlight key aspects of the history of infectious disease dynamic models, the role of rigorous testing and evaluation, the integration with data, and the successful application of models to guide public health. Rather than being an expansive history of infectious disease models, this Review focuses on how the integration of modelling can continue to be advanced through policy and practice in appropriate and conscientious ways to support the current pandemic response.\n', 'corpus_id': 230582776, 'score': 1}, {'doc_id': '80128651', 'title': 'Treatment adherence, health related quality of life and aging in HIV-1 infected patients', 'abstract': 'A meta-analysis on predictors and correlates of adherence to cART showed that adherence was most strongly associated with patients’ adherence-related beliefs. It suggests several potential options for interventions to improve adherence.One intervention, simplification of therapy to a fixed-dose was perceived to be more convenient, and resulted in improved adherence.There is no golden standard to measure adherence. A combination of methods were compared including EMD, diaries, self-reports, physician and nurse assessments, pill-counts, pharmacy refill, in-depth-interviews and TDM. The conclusion was that patient’s self-reports, the estimation by nurse and the combination with TDM are easy to implement and can be used successfully in daily practice.Another meta-analysis was carry out to assess the effectiveness of EMD combined with informed counselling. The contribution of just monitoring-informed counselling to improve levels of adherence and virological treatment response is difficult to establish. It reflects that adherence is a behaviour that may be affected by a multitude of factors addressed by multi-component interventions.The age of HIV-1 infected adults increasing and therefore the onset of age-associated co-morbidities increases which may negatively affect HRQL.A cohort of HIV-negative and HIV-positive individuals > 45 years were screened for the presence of co-morbidities and completed questionnaires about HRQL and depression. HIV-positive status was associated with poor physical and mental HRQL and with increased likelihood of developing depression. No evidence was found that the difference in HRQL between HIV-infected and uninfected individuals became more pronounced with a higher number of co-morbidities, or at an older age. Abbreviations : cART: combination antiretroviral therapy; EMD: electronic monitoring device; TDM: therapeutic drug monitoring; HRQL: health related quality of life.', 'corpus_id': 80128651, 'score': 0}, {'doc_id': '117287749', 'title': 'A group of automorphisms of the homotopy groups', 'abstract': 'It is well known that the fundamental group π 1 ( X ) of an arcwise connected topological space X operates on the n -th homotopy group π n ( X ) of X as a group of automorphisms. In this paper I intend to construct geometrically a group 𝒰 ( X ) of automorphisms of π n ( X ), for every integer n ≥ 1, which includes a normal subgroup isomorphic to π 1 ( X ) so that the factor group of 𝒰 ( X ) by π 1 ( X ) is completely determined by some invariant Σ ( X ) of the space X . The complete analysis of the operation of the group on π n ( X ) is given in §3, §4, and §5,', 'corpus_id': 117287749, 'score': 0}, {'doc_id': '227070795', 'title': 'Climatic and socio-economic factors supporting the co-circulation of dengue, Zika and chikungunya in three different ecosystems in Colombia', 'abstract': 'Dengue, Zika and chikungunya are diseases of global health significance caused by arboviruses and transmitted by the mosquito Aedes aegypti of worldwide circulation. The arrival of the Zika and chikungunya viruses to South America increased the complexity of transmission and morbidity caused by these viruses co-circulating in the same vector mosquito species. Here we present an integrated analysis of the reported arbovirus cases between 2007 and 2017 and local climate and socio-economic profiles of three distinct Colombian municipalities (Bello, Cucuta and Moniquira). These locations were confirmed as three different ecosystems given their contrasted geographic, climatic and socio-economic profiles. Correlational analyses were conducted with both generalised linear models and generalised additive models for the geographical data. Average temperature and wind speed were strongly correlated with disease incidence. The transmission of Zika during the 2016 epidemic appeared to decrease circulation of dengue in Cucuta, an area of sustained high incidence of dengue. Socio-economic factors such as barriers to health and childhood services, inadequate sanitation and poor water supply suggested an unfavourable impact on the transmission of dengue, Zika and chikungunya in all three ecosystems. Socio-demographic influencers were also discussed including the influx of people to Cucuta, fleeing political and economic instability from neighbouring Venezuela. Aedes aegypti is expanding its range and increasing the global threat of these diseases. It is therefore vital that we learn from the epidemiology of these arboviruses and translate it into an actionable knowledge base. This is even more acute given the recent historical high of dengue cases in the Americas in 2019, preceding the COVID-19 pandemic, which is itself hampering mosquito control efforts.', 'corpus_id': 227070795, 'score': 1}, {'doc_id': '231621305', 'title': 'Malaria in the USA: How Vulnerable Are We to Future Outbreaks?', 'abstract': 'Purpose of Review Malaria poses a threat to nearly half of the world’s population, and recent literature in the USA is lacking regarding understanding risk for local outbreaks. This article aims to review Anopheles mosquito data, vector-borne disease outbreak preparedness, and human travel data from large international gateway cities in an effort to examine risk for localized outbreaks. Recent Findings The majority of vector control organizations are widely unprepared for a vector-borne disease outbreak, and multiple mosquito species capable of transmitting malaria continue to persist throughout the USA. Summary Despite the lack of recent autochthonous cases in the USA, multiple risk factors suggest that local malaria outbreaks in the USA will continue to pose a public health threat due to large numbers of international travelers from endemic areas, multiple Anopheles spp. capable of transmitting the parasite, and unsatisfactory vector-borne disease outbreak preparedness. Climate conditions and recent changes in travel patterns will influence malaria across the globe.', 'corpus_id': 231621305, 'score': 0}, {'doc_id': '229652718', 'title': 'Spatial Risk Assessment of Mosquito-Borne Viral Diseases – Research at the Intersection of Ecology and Epidemiology.', 'abstract': 'The arrival and rapid spread of the mosquito-borne viral disease Chikungunya across the Americas is one of the most significant public health developments of recent years, preceding and mirroring the subsequent spread of Zika. Globalization in trade and travel can lead to the importation of these viruses, but climatic conditions strongly affect the efficiency of transmission in local settings. In order to direct preparedness for future outbreaks, it is necessary to anticipate global regions that could become suitable for Chikungunya transmission. Here, we present global correlative niche models for autochthonous Chikungunya transmission. These models were used as the basis for projections under the representative concentration pathway (RCP) 4.5 and 8.5 climate change scenarios. In a further step, hazard maps, which account for population densities, were produced. The baseline models successfully delineate current areas of active Chikungunya transmission. Projections under the RCP 4.5 and 8.5 scenarios suggest the likelihood of expansion of transmission-suitable areas in many parts of the world, including China, sub-', 'corpus_id': 229652718, 'score': 1}, {'doc_id': '231303818', 'title': 'Weather Variability and COVID-19 Transmission: A Review of Recent Research', 'abstract': 'Weather and climate play a significant role in infectious disease transmission, through changes to transmission dynamics, host susceptibility and virus survival in the environment. Exploring the association of weather variables and COVID-19 transmission is vital in understanding the potential for seasonality and future outbreaks and developing early warning systems. Previous research examined the effects of weather on COVID-19, but the findings appeared inconsistent. This review aims to summarize the currently available literature on the association between weather and COVID-19 incidence and provide possible suggestions for developing weather-based early warning system for COVID-19 transmission. Studies eligible for inclusion used ecological methods to evaluate associations between weather (i.e., temperature, humidity, wind speed and rainfall) and COVID-19 transmission. The review showed that temperature was reported as significant in the greatest number of studies, with COVID-19 incidence increasing as temperature decreased and the highest incidence reported in the temperature range of 0–17 °C. Humidity was also significantly associated with COVID-19 incidence, though the reported results were mixed, with studies reporting positive and negative correlation. A significant interaction between humidity and temperature was also reported. Wind speed and rainfall results were not consistent across studies. Weather variables including temperature and humidity can contribute to increased transmission of COVID-19, particularly in winter conditions through increased host susceptibility and viability of the virus. While there is less indication of an association with wind speed and rainfall, these may contribute to behavioral changes that decrease exposure and risk of infection. Understanding the implications of associations with weather variables and seasonal variations for monitoring and control of future outbreaks is essential for early warning systems.', 'corpus_id': 231303818, 'score': 0}]"
104	Algorithmic reasoning	322b1a8ff7782d6e5d1a56f83c670be4	20653	{}	"[{'doc_id': '170078913', 'title': 'What Can Neural Networks Reason About?', 'abstract': 'Neural networks have succeeded in many reasoning tasks. Empirically, these tasks require specialized network structures, e.g., Graph Neural Networks (GNNs) perform well on many such tasks, but less structured networks fail. Theoretically, there is limited understanding of why and when a network structure generalizes better than others, although they have equal expressive power. In this paper, we develop a framework to characterize which reasoning tasks a network can learn well, by studying how well its computation structure aligns with the algorithmic structure of the relevant reasoning process. We formally define this algorithmic alignment and derive a sample complexity bound that decreases with better alignment. This framework offers an explanation for the empirical success of popular reasoning models, and suggests their limitations. As an example, we unify seemingly different reasoning tasks, such as intuitive physics, visual question answering, and shortest paths, via the lens of a powerful algorithmic paradigm, dynamic programming (DP). We show that GNNs align with DP and thus are expected to solve these tasks. On several reasoning tasks, our theory is supported by empirical results.', 'corpus_id': 170078913, 'score': 1}, {'doc_id': '237430720', 'title': 'Relational Graph Reasoning for Knowledge-Augmented Question Answering', 'abstract': 'Pretrained language models have been widely used in various Natural Language Processing (NLP) tasks and achieved remarkable success. However, there are two shortcomings of such methods: (1) while language models do well in encoding word sequence based on its semantic meanings, it can’t introduce knowledge from other sources, which limits its performance on knowledge-guided NLP tasks; (2) language models understand semantics based on co-occurrence in the training corpus, which makes it hard to do complex reasoning. In this paper, we focus on the question answering task where external knowledge is necessary for both understanding the context and identifying the correct answer. Inspired by Relation Network (Santoro et al., 2017), we propose a framework to incorporate relevant facts from knowledge graph and do reasoning. Experiments on CommonsenseQA dataset demonstrate the effectiveness of our method and the value of external knowledge.', 'corpus_id': 237430720, 'score': 0}, {'doc_id': '237433348', 'title': 'Commonsense Question Answering: A Survey', 'abstract': 'When humans use their languages to communicate with each other, they often rely on broad implicit assumptions. Humans learn and use this kind of assumptions in everyday life, which makes their language concise without lacking precision. However, machines by nature don’t have such background knowledge. Machine learning models can’t accumulate human’s commonsense through interacting with the environment. Therefore, empowering Natural Language Processing (NLP) techniques with commonsense knowledge is one of the major long-term goals for Artificial Intelligence (AI). Question Answering (QA) is a Natural Language Understanding (NLU) task requiring both language processing and knowledge reasoning. When commonsense knowledge outside the given text is needed to answer the question, the task is called Commonsense Question Answering. Therefore, the main focus of the commonsense question answering task is how to incorporate commonsense knowledge and conduct reasoning.', 'corpus_id': 237433348, 'score': 0}, {'doc_id': '237431232', 'title': 'Advances in QA Models II: Machine Reading Comprehension and Multi-hop Reasoning', 'abstract': 'Machine reading comprehension (MRC) answers a query about a given context, which usually requires modeling complex interactions between the context and the query. Some harder questions require explicit modeling of multi-hop reasoning process. In this lecture, we will discuss advances in machine reading comprehension and multi-hop reasoning.', 'corpus_id': 237431232, 'score': 0}, {'doc_id': '237532584', 'title': 'Deep Algorithmic Question Answering: Towards a Compositionally Hybrid AI for Algorithmic Reasoning', 'abstract': 'An important aspect of artificial intelligence (AI) is the ability to reason in a step-by-step “algorithmic” manner that can be inspected and verified for its correctness. This is especially important in the domain of question answering (QA). We argue that the challenge of algorithmic reasoning in QA can be effectively tackled with a “systems” approach to AI which features a hybrid use of symbolic and sub-symbolic methods including deep neural networks. Additionally, we argue that while neural network models with end-to-end training pipelines perform well in narrow applications such as image classification and language modelling, they cannot, on their own, successfully perform algorithmic reasoning, especially if the task spans multiple domains. We discuss a few notable exceptions and point out how they are still limited when the QA problem is widened to include other intelligence-requiring tasks. However, deep learning, and machine learning in general, do play important roles as components in the reasoning process. We propose an approach to algorithm reasoning for QA, Deep Algorithmic Question Answering (DAQA), based on three desirable properties: interpretability, generalizability and robustness which such an AI system should possess and conclude that they are best achieved with a combination of hybrid and compositional AI.', 'corpus_id': 237532584, 'score': 1}, {'doc_id': '233864602', 'title': 'Neural algorithmic reasoning', 'abstract': 'We present neural algorithmic reasoning—the art of building neural networks that are able to execute algorithmic computation—and provide our opinion on its transformative potential for running classical algorithms on inputs previously considered inaccessible to them.', 'corpus_id': 233864602, 'score': 1}, {'doc_id': '236981984', 'title': 'Review of Algorithms for Artificial Intelligence on Low Memory Devices', 'abstract': 'The aim of the article is to conceptualise a more compact and efficient version of algorithms for artificial intelligence (AI). The core objective is to construct the design for a self-optimising and self-adapting autonomous artificial intelligence (AutoAI) that can be applied for edge analytics using real-time data. The methodology is based on synthesising existing knowledge on AI (i.e., knowledge modelling, symbolic reasoning, modal logic), with novel concepts from neuromorphic engineering in combination with deep learning algorithms (i.e., reinforcement learning, neural networks, evolutionary algorithms) and data science (i.e., statistics, linear regression, Bayesian methods). Far-reaching implications are expected from the unique integration of approaches in neuromorphic engineering and edge analytics.', 'corpus_id': 236981984, 'score': 0}, {'doc_id': '237012256', 'title': 'TERN OF LEARNING', 'abstract': 'Artificial Intelligence has been developed for decades with the achievement of great progress. Recently, deep learning shows its ability to solve many real world problems, e.g. image classification and detection, natural language processing, playing GO. Theoretically speaking, an artificial neural network can fit any function and reinforcement learning can learn from any delayed reward. But in solving real world tasks, we still need to spend a lot of effort to adjust algorithms to fit task unique features. This paper proposes that the reason of this phenomenon is the sparse feedback feature of the nature, and a single algorithm, no matter how we improve it, can only solve dense feedback tasks or specific sparse feedback tasks. This paper first analyses how sparse feedback affects algorithm perfomance, and then proposes a pattern that explains how to accumulate knowledge to solve sparse feedback problems.', 'corpus_id': 237012256, 'score': 0}, {'doc_id': '236980370', 'title': 'From Deep Learning to Deep Reasoning', 'abstract': 'The rise of big data and big compute has brought modern neural networks to many walks of digital life, thanks to the relative ease of constructing large models that scale to the real world. Current successes of Transformers and self-supervised pretraining on massive data have led some to believe that deep neural networks will be able to do almost everything once we have sufficient data and computational resources. However, neural networks are fast to exploit surface statistics but fail miserably to generalize to novel combinations. This is because they are not designed for deliberate reasoning -- the capacity to deliberately deduce new knowledge out of the contextualized data. This tutorial reviews recent developments to extend the capacity of neural networks to ""learning-to-reason\'\' from data, where the task is to determine if the data entails a conclusion. This capacity opens up new ways to generate insights from data through arbitrary compositional querying without the need of predefining a narrow set of tasks. The tutorial consists of four parts. The first part covers the learning-to-reason framework, and explains how neural networks can serve as a strong backbone for reasoning through its natural operations such as binding, attention & dynamic computational graphs. The second part goes into more detail on how neural networks perform reasoning over unstructured and structured data, and across modalities. The third part reviews neural memories and their role in reasoning. The last part discusses generalization to novel combinations, under less supervision and with more knowledge.', 'corpus_id': 236980370, 'score': 1}, {'doc_id': '236956827', 'title': 'Knowledge accumulating: The general pattern of learning', 'abstract': 'Artificial Intelligence has been developed for decades with the achievement of great progress. Recently, deep learning shows its ability to solve many real world problems, e.g. image classification and detection, natural language processing, playing GO. Theoretically speaking, an artificial neural network can fit any function and reinforcement learning can learn from any delayed reward. But in solving real world tasks, we still need to spend a lot of effort to adjust algorithms to fit task unique features. This paper proposes that the reason of this phenomenon is the sparse feedback feature of the nature, and a single algorithm, no matter how we improve it, can only solve dense feedback tasks or specific sparse feedback tasks. This paper first analyses how sparse feedback affects algorithm perfomance, and then proposes a pattern that explains how to accumulate knowledge to solve sparse feedback problems.', 'corpus_id': 236956827, 'score': 1}]"
105	biomedical-aortic-aneurysm	c1786f4f03d7286b8c0e87855960d1a6	18504	{}	[{'doc_id': '235700910', 'title': 'Automated Left Ventricle Ischemic Scar Detection in CT Using Deep Neural Networks', 'abstract': 'Objectives: The aim of this study is to develop a scar detection method for routine computed tomography angiography (CTA) imaging using deep convolutional neural networks (CNN), which relies solely on anatomical information as input and is compatible with existing clinical workflows. Background: Identifying cardiac patients with scar tissue is important for assisting diagnosis and guiding interventions. Late gadolinium enhancement (LGE) magnetic resonance imaging (MRI) is the gold standard for scar imaging; however, there are common instances where it is contraindicated. CTA is an alternative imaging modality that has fewer contraindications and is faster than Cardiovascular magnetic resonance imaging but is unable to reliably image scar. Methods: A dataset of LGE MRI (200 patients, 83 with scar) was used to train and validate a CNN to detect ischemic scar slices using segmentation masks as input to the network. MRIs were segmented to produce 3D left ventricle meshes, which were sampled at points along the short axis to extract anatomical masks, with scar labels from LGE as ground truth. The trained CNN was tested with an independent CTA dataset (25 patients, with ground truth established with paired LGE MRI). Automated segmentation was performed to provide the same input format of anatomical masks for the network. The CNN was compared against manual reading of the CTA dataset by 3 experts. Results: Note that 84.7% cross-validated accuracy (AUC: 0.896) for detecting scar slices in the left ventricle on the MRI data was achieved. The trained network was tested against the CTA-derived data, with no further training, where it achieved an 88.3% accuracy (AUC: 0.901). The automated pipeline outperformed the manual reading by clinicians. Conclusion: Automatic ischemic scar detection can be performed from a routine cardiac CTA, without any scar-specific imaging or contrast agents. This requires only a single acquisition in the cardiac cycle. In a clinical setting, with near zero additional cost, scar presence could be detected to triage images, reduce reading times, and guide clinical decision-making.', 'corpus_id': 235700910, 'score': 0}, {'doc_id': '4558131', 'title': 'Fully automatic detection and segmentation of abdominal aortic thrombus in post‐operative CTA images using Deep Convolutional Neural Networks', 'abstract': 'HIGHLIGHTSA DCNN‐based fully automatic thrombus detection and segmentation pipeline that is easily translatable to clinical practice is proposed.A new DCNN architecture adapted to post‐operative thrombus segmentation from CTA images is presented, which combines low level features with coarser representations.The well‐known Detectnet computer vision network is translated into the clinical domain, specifically for thrombus region of interest detection in CTA images.Automatic segmentation exceeds previous state of the art results, with a mean Dice similarity coefficient of 82%.In terms of clinical applicability, the obtained segmentation results lay within the experienced human observer variance without the need of human intervention in most common cases. ABSTRACT Computerized Tomography Angiography (CTA) based follow‐up of Abdominal Aortic Aneurysms (AAA) treated with Endovascular Aneurysm Repair (EVAR) is essential to evaluate the progress of the patient and detect complications. In this context, accurate quantification of post‐operative thrombus volume is required. However, a proper evaluation is hindered by the lack of automatic, robust and reproducible thrombus segmentation algorithms. We propose a new fully automatic approach based on Deep Convolutional Neural Networks (DCNN) for robust and reproducible thrombus region of interest detection and subsequent fine thrombus segmentation. The DetecNet detection network is adapted to perform region of interest extraction from a complete CTA and a new segmentation network architecture, based on Fully Convolutional Networks and a Holistically‐Nested Edge Detection Network, is presented. These networks are trained, validated and tested in 13 post‐operative CTA volumes of different patients using a 4‐fold cross‐validation approach to provide more robustness to the results. Our pipeline achieves a Dice score of more than 82% for post‐operative thrombus segmentation and provides a mean relative volume difference between ground truth and automatic segmentation that lays within the experienced human observer variance without the need of human intervention in most common cases.', 'corpus_id': 4558131, 'score': 1}, {'doc_id': '202733345', 'title': 'A fully automated pipeline for mining abdominal aortic aneurysm using image segmentation', 'abstract': 'Imaging software have become critical tools in the diagnosis and the treatment of abdominal aortic aneurysms (AAA). The aim of this study was to develop a fully automated software system to enable a fast and robust detection of the vascular system and the AAA. The software was designed from a dataset of injected CT-scans images obtained from 40 patients with AAA. Pre-processing steps were performed to reduce the noise of the images using image filters. The border propagation based method was used to localize the aortic lumen. An online error detection was implemented to correct errors due to the propagation in anatomic structures with similar pixel value located close to the aorta. A morphological snake was used to segment 2D or 3D regions. The software allowed an automatic detection of the aortic lumen and the AAA characteristics including the presence of thrombus and calcifications. 2D and 3D reconstructions visualization were available to ease evaluation of both algorithm precision and AAA properties. By enabling a fast and automated detailed analysis of the anatomic characteristics of the AAA, this software could be useful in clinical practice and research and be applied in a large dataset of patients.', 'corpus_id': 202733345, 'score': 1}, {'doc_id': '235639340', 'title': 'Fast Lung Localization in Computed Tomography by a 1D Detection Network', 'abstract': 'Deep learning models performed very well in many medical image analysis tasks. However, the majority of these results had been obtained on carefully selected datasets. At the same time, the real clinical flow of Computed Tomography studies often contains series with different properties. We address a particular discrepancy related to a much larger scanning interval, e.g., a single series for thorax, abdomen, and pelvis. We propose to use 1D body organ detection for coarse organ localization on thorax-abdomen CT scans. Localized segments, containing volumes of interests, could be further processed by a heavier task-specific network. We convert 3D CT images into multi-channel 2D coronal images, thus drastically decreasing the dimensionality of the data. We next train a conventional U-net like architecture to solve the task of body part regression and build simple threshold rules to localize lungs along the coronal plane. Additionally, this approach allows for the detection of organs only partially presented in the image. Our network was trained on 20 thousand thorax-abdomen volume segments and validated on three separate datasets. It shows high localization accuracy, stability across datasets and processes a high-resolution CT volume in no more than 200 ms.', 'corpus_id': 235639340, 'score': 0}, {'doc_id': '235627451', 'title': 'Visceral segment aortic thrombus is associated with proximal aortic degeneration after infrarenal abdominal aortic aneurysm repair.', 'abstract': 'OBJECTIVE\nTo identify predictors of aortic aneurysm formation at or above an infrarenal abdominal aortic aneurysm repair.\n\n\nMETHODS\nA total of 881 infrarenal abdominal aortic aneurysm repairs were identified at a single institution from 2004 to 2008; 187 of the repairs were identified that had pre-operative and post-operative computed tomography imaging at least one\u2009year or greater to evaluate for aortic degeneration following repair. Aortic diameters at the celiac, superior mesenteric, and renal arteries were measured on all available computed tomographic scans. Aortic thrombus and calcification volumes in the visceral and infrarenal abdominal aortic segments were calculated. Multivariable modeling was used with log transformed variables to determine potential predictors of future aortic aneurysm development after infrarenal abdominal aortic aneurysm repair.\n\n\nRESULTS\nOf the 187 patients in the cohort, 100 had an open abdominal aortic aneurysm repair while 87 were treated with endovascular repair. Proximal aortic aneurysms developed in 26% (n\u2009=\u200949) of the cohort during an average of 72\u2009±\u200934.2\u2009months of follow-up. After multivariable modeling, visceral segment aortic thrombus on pre-operative computed tomography imaging increased the risk of aortic aneurysm development above the infrarenal abdominal aortic aneurysm repair within both the open abdominal aortic aneurysm (hazard ratio 2.04, p\u2009=\u20090.033) and endovascular repair (hazard ratio 3.31, p\u2009=\u20090.004) cohorts. Endovascular repair was independently associated with a higher risk of future aortic aneurysm development after infrarenal abdominal aortic aneurysm repair when compared to open abdominal aortic aneurysm (hazard ratio 2.19, p\u2009=\u20090.025).\n\n\nCONCLUSIONS\nVisceral aortic thrombus present prior to abdominal aortic aneurysm repair and endovascular repair are both associated with an increased risk of future proximal aortic degeneration after infrarenal abdominal aortic aneurysm repair. These factors may predict patients at higher risk of developing proximal aortic aneurysms that may require complex aortic repairs.', 'corpus_id': 235627451, 'score': 0}, {'doc_id': '67856657', 'title': '3D convolutional neural network for abdominal aortic aneurysm segmentation', 'abstract': 'An abdominal aortic aneurysm (AAA) is a focal dilation of the aorta that, if not treated, tends to grow and may rupture. A significant unmet need in the assessment of AAA disease, for the diagnosis, prognosis and follow-up, is the determination of rupture risk, which is currently based on the manual measurement of the aneurysm diameter in a selected Computed Tomography Angiography (CTA) scan. However, there is a lack of standardization determining the degree and rate of disease progression, due to the lack of robust, automated aneurysm segmentation tools that allow quantitatively analyzing the AAA. In this work, we aim at proposing the first 3D convolutional neural network for the segmentation of aneurysms both from preoperative and postoperative CTA scans. We extensively validate its performance in terms of diameter measurements, to test its applicability in the clinical practice, as well as regarding the relative volume difference, and Dice and Jaccard scores. The proposed method yields a mean diameter measurement error of 3.3 mm, a relative volume difference of 8.58 %, and Dice and Jaccard scores of 87 % and 77 %, respectively. At a clinical level, an aneurysm enlargement of 10 mm is considered relevant, thus, our method is suitable to automatically determine the AAA diameter and opens up the opportunity for more complex aneurysm analysis.', 'corpus_id': 67856657, 'score': 1}, {'doc_id': '204832114', 'title': 'Abdominal Aortic Aneurysm Segmentation Using Convolutional Neural Networks Trained with Images Generated with a Synthetic Shape Model', 'abstract': 'An abdominal aortic aneurysm (AAA) is a ballooning of the abdominal aorta, that if not treated tends to grow and rupture. Computed Tomography Angiography (CTA) is the main imaging modality for the management of AAAs, and segmenting them is essential for AAA rupture risk and disease progression assessment. Previous works have shown that Convolutional Neural Networks (CNNs) can accurately segment AAAs, but have the limitation of requiring large amounts of annotated data to train the networks. Thus, in this work we propose a methodology to train a CNN only with images generated with a synthetic shape model, and test its generalization and ability to segment AAAs from new original CTA scans. The synthetic images are created from realistic deformations generated by applying principal component analysis to the deformation fields obtained from the registration of few datasets. The results show that the performance of a CNN trained with synthetic data to segment AAAs from new scans is comparable to the one of a network trained with real images. This suggests that the proposed methodology may be applied to generate images and train a CNN to segment other types of aneurysms, reducing the burden of obtaining large annotated image databases.', 'corpus_id': 204832114, 'score': 1}, {'doc_id': '235168325', 'title': 'Automatic cervical lymphadenopathy segmentation from CT data using deep learning.', 'abstract': 'PURPOSE\nThe purpose of this study was to develop a fast and automatic algorithm to detect and segment lymphadenopathy from head and neck computed tomography (CT) examination.\n\n\nMATERIALS AND METHODS\nAn ensemble of three convolutional neural networks (CNNs) based on a U-Net architecture were trained to segment the lymphadenopathies in a fully supervised framework. The resulting predictions were assessed using the Dice similarity coefficient (DSC) on examinations presenting one or more adenopathies. On examinations without adenopathies, the score was given by the formula M/(M+A) where M was the mean adenopathy volume per patient and A the volume segmented by the algorithm. The networks were trained on 117 annotated CT acquisitions.\n\n\nRESULTS\nThe test set included 150 additional CT acquisitions unseen during the training. The performance on the test set yielded a mean score of 0.63.\n\n\nCONCLUSION\nDespite limited available data and partial annotations, our CNN based approach achieved promising results in the task of cervical lymphadenopathy segmentation. It has the potential to bring precise quantification to the clinical workflow and to assist the clinician in the detection task.', 'corpus_id': 235168325, 'score': 0}, {'doc_id': '235226706', 'title': 'Comparing methods of detecting and segmenting unruptured intracranial aneurysms on TOF-MRAS: The ADAM challenge', 'abstract': 'Accurate detection and quantification of unruptured intracranial aneurysms (UIAs) is important for rupture risk assessment and to allow an informed treatment decision to be made. Currently, 2D manual measures used to assess UIAs on Time-of-Flight magnetic resonance angiographies (TOF-MRAs) lack 3D information and there is substantial inter-observer variability for both aneurysm detection and assessment of aneurysm size and growth (Forbes et al., 1996; Kim et al., 2017; White et al., 2000). 3D measures could be helpful to improve aneurysm detection and quantification but are time-consuming and would therefore benefit from a reliable automatic UIA detection and segmentation method. The Aneurysm Detection and segMentation (ADAM) challenge was organised in which methods for automatic UIA detection and segmentation were developed and submitted to be evaluated on a diverse clinical TOF-MRA dataset. A training set (113 cases with a total of 129 UIAs) was released, each case including a TOF-MRA, a structural MR image (T1, T2 or FLAIR), annotation of any present UIA(s) and the centre voxel of the UIA(s). A test set of 141 cases (with 153 UIAs) was used for evaluation. Two tasks were proposed: (1) detection and (2) segmentation of UIAs on TOF-MRAs. Teams developed and submitted containerised methods to be evaluated on the test set. Task 1 was evaluated using metrics of sensitivity and false positive count. Task 2 was evaluated using dice similarity coefficient, modified hausdorff distance (95th percentile) and volumetric similarity. For each task, a ranking was made based on the average of the metrics. In total, eleven teams participated in task 1 and nine of those teams participated in task 2. Task 1 was won by a method specifically designed for the detection task (i.e. not participating in task 2). Based on segmentation metrics, the top two methods for task 2 performed statistically significantly better than all other methods. The detection performance of the top-ranking methods was comparable to visual inspection for larger aneurysms. Segmentation performance of the top ranking method, after selection of true UIAs, was similar to interobserver performance. The ADAM challenge remains open for future submissions and improved submissions, with a live leaderboard to provide benchmarking for method developments at https://adam.isi.uu.nl/.', 'corpus_id': 235226706, 'score': 0}, {'doc_id': '221178714', 'title': 'Abdominal Aortic Aneurysm Segmentation from Contrast-Enhanced Computed Tomography Angiography Using Deep Convolutional Networks', 'abstract': 'One of the most common imaging methods for diagnosing an abdominal aortic aneurysm, and an endoleak detection is computed tomography angiography. In this paper, we address the problem of aorta and thrombus semantic segmentation, what is a mandatory step to estimate aortic aneurysm diameter. Three end-to-end convolutional neural networks were trained and evaluated. Finally, we proposed an ensemble of deep neural networks with underlying U-Net, ResNet, and VBNet frameworks. Our results show that we are able to outperform state-of-the-art methods by 3% on the Dice metric without any additional post-processing steps.', 'corpus_id': 221178714, 'score': 1}]
106	Conducting High-Quality Software Engineering Research	677d76b93645d53174eb620b970bd62f	3460	{}	[{'doc_id': '211171624', 'title': 'How Interaction Designers Use Tools to Manage Ideas', 'abstract': 'This article presents a grounded theory analysis based on a qualitative study of professional interaction designers (n = 20) with a focus on how they use tools to manage design ideas. Idea management can be understood as a subcategory of the field personal information management, which includes the activities around the capture, organization, retrieval, and use of information. Idea management pertains to the management and use of ideas, a particular type of information, as part of creative activities. The article identifies tool-supported idea management strategies and needs of professional interaction designers, and discusses the context and consequences of these strategies. Based on our analysis, we identify a conceptual framework of 10 strategies which are supported by tools: saving, externalizing, advancing, exploring, archiving, clustering, extracting, browsing, verifying, and collaborating. Finally, we discuss how this framework can be used to characterize and analyze existing and novel idea management tools.', 'corpus_id': 211171624, 'score': 0}, {'doc_id': '215827822', 'title': 'Code Review in the Classroom', 'abstract': 'This paper presents a case study to examine the affinity of the code review process among young developers in an academic setting. Code review is indispensable considering the positive outcomes it generates. However, it is not an individual activity and requires substantial interaction among stakeholders, deliverance, and acceptance of feedback, timely actions upon feedback as well as the ability to agree on a solution in the wake of diverse viewpoints. Young developers in a classroom setting provide a clear picture of the potential favourable and problematic areas of the code review process. Their feedback suggests that the process has been well received with some points to better the process. This paper can be used as guidelines to perform code reviews in the classroom.', 'corpus_id': 215827822, 'score': 0}, {'doc_id': '211020564', 'title': 'The Four Pillars of Research Software Engineering', 'abstract': 'We present four elements we believe are key to providing a comprehensive and sustainable support for research software engineering: software development, community, training, and policy. We also show how the wider developer community can learn from, and engage with, these activities.', 'corpus_id': 211020564, 'score': 1}, {'doc_id': '215754706', 'title': 'When to Update Systematic Literature Reviews in Software Engineering', 'abstract': '[Context] Systematic Literature Reviews (SLRs) have been adopted by the Software Engineering (SE) community for approximately 15 years to provide meaningful summaries of evidence on several topics. Many of these SLRs are now potentially outdated, and there are no systematic proposals on when to update SLRs in SE. [Objective] The goal of this paper is to provide recommendations on when to update SLRs in SE. [Method] We evaluated, using a three-step approach, a third-party decision framework (3PDF) employed in other fields, to decide whether SLRs need updating. First, we conducted a literature review of SLR updates in SE and contacted the authors to obtain their feedback relating to the usefulness of the 3PDF within the context of SLR updates in SE. Second, we used these authors feedback to see whether the framework needed any adaptation; none was suggested. Third, we applied the 3PDF to the SLR updates identified in our literature review. [Results] The 3PDF showed that 14 of the 20 SLRs did not need updating. This supports the use of a decision support mechanism (such as the 3PDF) to help the SE community decide when to update SLRs. [Conclusions] We put forward that the 3PDF should be adopted by the SE community to keep relevant evidence up to date and to avoid wasting effort with unnecessary updates.', 'corpus_id': 215754706, 'score': 1}, {'doc_id': '214612459', 'title': 'Rapid Reviews in Software Engineering', 'abstract': 'Integrating research evidence into practice is one of the main goals of evidence-based software engineering (EBSE). Secondary studies, one of the main EBSE products, are intended to summarize the “best” research evidence and make them easily consumable by practitioners. However, recent studies show that some secondary studies lack connections with software engineering practice. In this chapter, we present the concept of Rapid Reviews, which are lightweight secondary studies focused on delivering evidence to practitioners in a timely manner. Rapid reviews support practitioners in their decision-making, and should be conducted bounded to a practical problem, inserted into a practical context. Thus, Rapid Reviews can be easily integrated in a knowledge/technology transfer initiative. After describing the basic concepts, we present the results and experiences of conducting two Rapid Reviews. We also provide guidelines to help researchers and practitioners who want to conduct Rapid Reviews, and we finally discuss topics that may concern the research community about the feasibility of Rapid Reviews as an evidence-based method. In conclusion, we believe Rapid Reviews might be of interest to researchers and practitioners working on the intersection of software engineering research and practice.', 'corpus_id': 214612459, 'score': 1}, {'doc_id': '211677517', 'title': 'A systematic literature review of modern software visualization', 'abstract': 'Abstract We report on the state-of-the-art of software visualization. To ensure reproducibility, we adopted the Systematic Literature Review methodology. That is, we analyzed 1440 entries from IEEE Xplore and ACM Digital Library databases. We selected 105 relevant full papers published in 2013–2019, which we classified based on the aspect of the software system that is supported (i.e., structure, behavior, and evolution). For each paper, we extracted main dimensions that characterize software visualizations, such as software engineering tasks, roles of users, information visualization techniques, and media used to display visualizations. We provide researchers in the field an overview of the state-of-the-art in software visualization and highlight research opportunities. We also help developers to identify suitable visualizations for their particular context by matching software visualizations to development concerns and concrete details to obtain available visualization tools. Graphic abstract', 'corpus_id': 211677517, 'score': 1}, {'doc_id': '211082994', 'title': 'Measurement of Interpersonal Trust in Global Software Development: SLR Protocol', 'abstract': 'The purpose of this protocol is to be useful to identify, evaluate and synthesize reported knowledge about the measurement of interpersonal trust (IpT) in virtual software teams. To achieve this goal we applied a research technique known as Systematic Literature Review (SLR). The aim of a SLR is to be as objective, analytical, and repeatable as possible.', 'corpus_id': 211082994, 'score': 0}, {'doc_id': '212676032', 'title': 'Analyzing the Impact of Automated User Assistance Systems: A Systematic Review', 'abstract': 'Context: User assistance is generally defined as the guided assistance to a user of a software system in order to help accomplish tasks and enhance user experience. Automated user assistance systems are equipped with online help system that provides information to the user in an electronic format and which can be opened directly in the application. Various different automated user assistance approaches have been proposed in the literature. However, there has been no attempt to systematically review and report the impact of automated user assistance systems. Objective: The overall objective of this systematic review is to identify the state of art in automated user assistance systems, and describe the reported evidence for automated user assistance. Method: A systematic literature review is conducted by a multiphase study selection process using the published literature since 2002. Results: We reviewed 575 papers that are discovered using a well-planned review protocol, and 31 of them were assessed as primary studies related to our research questions. Conclusions: Our study shows that user assistance systems can provide important benefits for the user but still more research is required in this domain.', 'corpus_id': 212676032, 'score': 0}, {'doc_id': '211146636', 'title': 'Formal Methods: From Academia to Industrial Practice. A Travel Guide', 'abstract': 'For many decades, formal methods are considered to be the way forward to help the software industry to make more reliable and trustworthy software. However, despite this strong belief and many individual success stories, no real change in industrial software development seems to be occurring. In fact, the software industry itself is moving forward rapidly, and the gap between what formal methods can achieve and the daily software-development practice does not appear to be getting smaller (and might even be growing). \nIn the past, many recommendations have already been made on how to develop formal-methods research in order to close this gap. This paper investigates why the gap nevertheless still exists and provides its own recommendations on what can be done by the formal-methods-research community to bridge it. Our recommendations do not focus on open research questions. In fact, formal-methods tools and techniques are already of high quality and can address many non-trivial problems; we do give some technical recommendations on how tools and techniques can be made more accessible. To a greater extent, we focus on the human aspect: how to achieve impact, how to change the way of thinking of the various stakeholders about this issue, and in particular, as a research community, how to alter our behaviour, and instead of competing, collaborate to address this issue.', 'corpus_id': 211146636, 'score': 0}, {'doc_id': '215908486', 'title': 'On the Performance of Hybrid Search Strategies for Systematic Literature Reviews in Software Engineering', 'abstract': 'Abstract Context When conducting a Systematic Literature Review (SLR), researchers usually face the challenge of designing a search strategy that appropriately balances result quality and review effort. Using digital library (or database) searches or snowballing alone may not be enough to achieve high-quality results. On the other hand, using both digital library searches and snowballing together may increase the overall review effort. Objective The goal of this research is to propose and evaluate hybrid search strategies that selectively combine database searches with snowballing. Method We propose four hybrid search strategies combining database searches in digital libraries with iterative, parallel, or sequential backward and forward snowballing. We simulated the strategies over three existing SLRs in SE that adopted both database searches and snowballing. We compared the outcome of digital library searches, snowballing, and hybrid strategies using precision, recall, and F-measure to investigate the performance of each strategy. Results Our results show that, for the analyzed SLRs, combining database searches from the Scopus digital library with parallel or sequential snowballing achieved the most appropriate balance of precision and recall. Conclusion We put forward that, depending on the goals of the SLR and the available resources, using a hybrid search strategy involving a representative digital library and parallel or sequential snowballing tends to represent an appropriate alternative to be used when searching for evidence in SLRs.', 'corpus_id': 215908486, 'score': 1}]
107	Multi gnss geodesy brainstorming	861b738057735cb9c7dcaafebb9f05e6	17537	{}	[{'doc_id': '233437128', 'title': 'Single-epoch RTK performance assessment of tightly combined BDS-2 and newly complete BDS-3', 'abstract': 'The BeiDou global navigation satellite system (BDS-3) constellation deployment has been completed on June 23, 2020, with a full constellation comprising 30 satellites. In this study, we present the performance assessment of single-epoch Real-Time Kinematic (RTK) positioning with tightly combined BeiDou regional navigation satellite system (BDS-2) and BDS-3. We first investigate whether code and phase Differential Inter-System Biases (DISBs) exist between the legacy B1I/B3I signals of BDS-3/BDS-2. It is discovered that the DISBs are in fact about zero for the baselines with the same or different receiver types at their endpoints. These results imply that BDS-3 and BDS-2 are fully interoperable and can be regarded as one constellation without additional DISBs when the legacy B1I/B3I signals are used for precise relative positioning. Then we preliminarily evaluate the single-epoch short baseline RTK performance of tightly combined BDS-2 and the newly completed BDS-3. The performance is evaluated through ambiguity resolution success rate, ambiguity dilution of precision, as well as positioning accuracy in kinematic and static modes using the datasets collected in Wuhan. Experimental results demonstrate that the current BDS-3 only solutions can deliver comparable ambiguity resolution performance and much better positioning accuracy with respect to BDS-2 only solutions. Moreover, the RTK performance is much improved with tightly combined BDS-3/BDS-2, particularly in challenging or harsh conditions. The single-frequency single-epoch tightly combined BDS-3/BDS-2 solution could deliver an ambiguity resolution success rate of 96.9% even with an elevation cut-off angle of 40°, indicating that the tightly combined BDS-3/BDS-2 could achieve superior RTK positioning performance in the Asia–Pacific region. Meanwhile, the three-dimensional (East/North/Up) positioning accuracy of BDS-3 only solution (0.52\xa0cm/0.39\xa0cm/2.14\xa0cm) in the kinematic test is significantly better than that of the BDS-2 only solution (0.85\xa0cm/1.02\xa0cm/3.01\xa0cm) due to the better geometry of the current BDS-3 constellation. The tightly combined BDS-3/BDS-2 solution can provide the positioning accuracy of 0.52\xa0cm, 0.22\xa0cm, and 1.80\xa0cm, respectively.', 'corpus_id': 233437128, 'score': 0}, {'doc_id': '214092137', 'title': 'Multi-GNSS processing, positioning and applications', 'abstract': 'In the past few decades the Global Positioning System (GPS) has been the number one positioning tool in a range of Geodesy and Geophysics applications. The emerging Regional and Global Navigation Satellite Systems (RNSSs/GNSSs) can enhance positioning and non-positioning applications. GPS modernisation from dual-frequency to triplefrequency signals has lately also been complemented by global GLObal’ naya NAvigatsionnaya Sputnikovaya Sistema (GLONASS), Galileo and BeiDou Navigation Satellite System (BDS), as well as the regional Quasi-Zenith Satellite system (QZSS) and Navigation with Indian Constellation (NavIC). By 2024 it is expected that we will have access to more than 110 satellites in a multi-GNSS model transmitting their signals on a range of different frequencies. This will significantly improve current dual-frequency GPS positioning as well as non-positioning applications, such as atmospheric modelling and timing applications. Rigorous models and algorithms are however needed so as to link and integrate such multi-frequency, multi-GNSS signals to the estimable parameters of interest. Important topics in this field include singleand multi-frequency, multi-GNSS modelling, while making use of survey-grade and low-cost receiver and antenna equipment, including smartphones. With low-cost we refer to GNSS receivers and antennas having a cost of at most a few hundreds of dollars, whereas survey-grade receivers and antennas typically have a cost of several thousands of dollars. Other important topics include ionospheric and tropospheric delay estimation, and multi-GNSS Precise Point Positioning Real Time Kinematic (PPP-RTK). This Special Feature showcases the latest trends in multi-GNSS modelling. Five papers were accepted, based on their quality and significant contributions to the field. The topics covered are: multi-frequency and multiGNSS for stochastic modelling, single-baseline RTK and PPP-RTK, respectively, ionospheric modelling, and a PPP performance analysis of dual-frequency, multi-GNSS data collected in smartphones.', 'corpus_id': 214092137, 'score': 1}, {'doc_id': '126259910', 'title': 'An analytical study on the carrier-phase linear combinations for triple-frequency GNSS', 'abstract': 'The linear combinations of multi-frequency carrier-phase measurements for Global Navigation Satellite System (GNSS) are greatly beneficial to improving the performance of ambiguity resolution (AR), cycle slip correction as well as precise positioning. In this contribution, the existing definitions of the carrier-phase linear combination are reviewed and the integer property of the resulting ambiguity of the phase linear combinations is examined. The general analytical method for solving the optimal integer linear combinations for all triple-frequency GNSS is presented. Three refined triple-frequency integer combinations solely determined by the frequency values are introduced, which are the ionosphere-free (IF) combination that the Sum of its integer coefficients equal to 0 (IFS0), the geometry-free (GF) combination that the Sum of its integer coefficients equal to 0 (GFS0) and the geometry-free and ionosphere-free (GFIF) combination. Besides, the optimal GF, IF, extra-wide lane and ionosphere-reduced integer combinations for GPS and BDS are solved exhaustively by the presented method. Their potential applications in cycle slip detection, AR as well as precise positioning are discussed. At last, a more straightforward GF and IF AR scheme than the existing method is presented based on the GFIF integer combination.', 'corpus_id': 126259910, 'score': 1}, {'doc_id': '123981207', 'title': 'Multi-technique combination of space geodesy observations: Impact of the Jason-2 satellite on the GPS satellite orbits estimation', 'abstract': 'Abstract In order to improve the Precise Orbit Determination (POD) of the GPS constellation and the Jason-2 Low Earth Orbiter (LEO), we carry out a simultaneous estimation of GPS satellite orbits along with Jason-2 orbits, using GINS software. Along with GPS station observations, we use Jason-2 GPS, SLR and DORIS observations, over a data span of 6\xa0months (28/05/2011–03/12/2011). We use the Geophysical Data Records-D (GDR-D) orbit estimation standards for the Jason-2 satellite. A GPS-only solution is computed as well, where only the GPS station observations are used. It appears that adding the LEO GPS observations results in an increase of about 0.7% of ambiguities fixed, with respect to the GPS-only solution. The resulting GPS orbits from both solutions are of equivalent quality, agreeing with each other at about 7\xa0mm on Root Mean Square (RMS). Comparisons of the resulting GPS orbits to the International GNSS Service (IGS) final orbits show the same level of agreement for both the GPS-only orbits, at 1.38\xa0cm in RMS, and the GPS\xa0+\xa0Jason2 orbits at 1.33\xa0cm in RMS. We also compare the resulting Jason-2 orbits with the 3-technique Segment Sol multi-missions d’ALTimetrie, d’orbitographie et de localisation precise (SSALTO) POD products. The orbits show good agreement, with 2.02\xa0cm of orbit differences global RMS, and 0.98\xa0cm of orbit differences RMS on the radial component.', 'corpus_id': 123981207, 'score': 1}, {'doc_id': '235212649', 'title': 'Performance Evaluation of Triple-Frequency GPS/Galileo Techniques for Precise Static and Kinematic Applications', 'abstract': 'The objective of this research was to develop new precise point positioning (PPP) processing models using triple-frequency GPS/Galileo observations. Different triple-frequency PPP models were developed including undifferenced, between-satellite single-difference (BSSD) and semi-decoupled PPP models. Additionally, a dual-frequency ionosphere-free undifferenced PPP model was developed. The performance of our developed PPP models was evaluated for both static and kinematic applications. To validate the proposed PPP models for static applications, triple-frequency GPS/Galileo observations spanning three successive days from eight globally distributed reference stations were acquired. Then, the observations were processed using the four static PPP solutions. It is found that the 3D positioning accuracy of the triple-frequency semi-decoupled, BSSD and undifferenced PPP models is enhanced after 10 min by about 50, 41 and 29%, respectively, compared with the dual-frequency undifferenced PPP model. After 20 min of processing, improvements in the 3D positioning accuracy by 40, 31 and 21% are obtained for the triple-frequency semi-decoupled, BSSD and undifferenced PPP models, respectively, with respect to the dual-frequency PPP model. The 3D positioning accuracy is also improved after 60 min, compared with the dual-frequency solution, by 40, 40 and 35% for the triple-frequency semi-decoupled, BSSD and undifferenced PPP solutions, respectively. For kinematic application validation, a vehicle trajectory was carried out. The collected triple-frequency GPS/Galileo observations were processed using the four kinematic PPP solutions. It is shown that the triple-frequency semi-decupled, BSSD and undifferenced PPP solutions enhance the 3D positioning accuracy by 31, 23 and 10%, respectively, in comparison with the dual-frequency undifferenced PPP solutions.', 'corpus_id': 235212649, 'score': 0}, {'doc_id': '235274298', 'title': 'Functional model modification of precise point positioning considering the time-varying code biases of a receiver', 'abstract': 'Precise Point Positioning (PPP), initially developed for the analysis of the Global Positing System (GPS) data from a large geodetic network, gradually becomes an effective tool for positioning, timing, remote sensing of atmospheric water vapor, and monitoring of Earth’s ionospheric Total Electron Content (TEC). The previous studies implicitly assumed that the receiver code biases stay constant over time in formulating the functional model of PPP. In this contribution, it is shown this assumption is not always valid and can lead to the degradation of PPP performance, especially for Slant TEC (STEC) retrieval and timing. For this reason, the PPP functional model is modified by taking into account the time-varying receiver code biases of the two frequencies. It is different from the Modified Carrier-to-Code Leveling (MCCL) method which can only obtain the variations of Receiver Differential Code Biases (RDCBs), i.e., the difference between the two frequencies’ code biases. In the Modified PPP (MPPP) model, the temporal variations of the receiver code biases become estimable and their adverse impacts on PPP parameters, such as ambiguity parameters, receiver clock offsets, and ionospheric delays, are mitigated. This is confirmed by undertaking numerical tests based on the real dual-frequency GPS data from a set of global continuously operating reference stations. The results imply that the variations of receiver code biases exhibit a correlation with the ambient temperature. With the modified functional model, an improvement by 42% to 96% is achieved in the Differences of STEC (DSTEC) compared to the original PPP model with regard to the reference values of those derived from the Geometry-Free (GF) carrier phase observations. The medium and long term (1\u2009×\u2009104 to 1.5\u2009×\u2009104\xa0s) frequency stability of receiver clocks are also significantly improved.', 'corpus_id': 235274298, 'score': 0}, {'doc_id': '235310144', 'title': 'Estimation of fractional cycle bias for GPS/BDS-2/Galileo based on international GNSS monitoring and assessment system observations using the uncombined PPP model', 'abstract': 'The Fractional Cycle Bias (FCB) product is crucial for the Ambiguity Resolution (AR) in Precise Point Positioning (PPP). Different from the traditional method using the ionospheric-free ambiguity which is formed by the Wide Lane (WL) and Narrow Lane (NL) combinations, the uncombined PPP model is flexible and effective to generate the FCB products. This study presents the FCB estimation method based on the multi-Global Navigation Satellite System (GNSS) precise satellite orbit and clock corrections from the international GNSS Monitoring and Assessment System (iGMAS) observations using the uncombined PPP model. The dual-frequency raw ambiguities are combined by the integer coefficients (4,−\u20093) and (1,−\u20091) to directly estimate the FCBs. The details of FCB estimation are described with the Global Positioning System (GPS), BeiDou-2 Navigation Satellite System (BDS-2) and Galileo Navigation Satellite System (Galileo). For the estimated FCBs, the Root Mean Squares (RMSs) of the posterior residuals are smaller than 0.1 cycles, which indicates a high consistency for the float ambiguities. The stability of the WL FCBs series is better than 0.02 cycles for the three GNSS systems, while the STandard Deviation (STD) of the NL FCBs for BDS-2 is larger than 0.139 cycles. The combined FCBs have better stability than the raw series. With the multi-GNSS FCB products, the PPP AR for GPS/BDS-2/Galileo is demonstrated using the raw observations. For hourly static positioning results, the performance of the PPP AR with the three-system observations is improved by 42.6%, but only 13.1% for kinematic positioning results. The results indicate that precise and reliable positioning can be achieved with the PPP AR of GPS/BDS-2/Galileo, supported by multi-GNSS satellite orbit, clock, and FCB products based on iGMAS.', 'corpus_id': 235310144, 'score': 0}, {'doc_id': '128903173', 'title': 'Assessment of Several Interpolation Methods for Precise GPS Orbit', 'abstract': 'GPS applications such as Precise Point Positioning (PPP) require the availability of precise ephemeris at high rate. To support these applications, several institutions such as the International GNSS Service (IGS) have developed precise orbital service. Unfortunately, however, the data rate of such precise orbits is usually limited to 15 minutes. To overcome this problem, a number of orbital interpolation methods are proposed. This paper examines the performance of four interpolation methods for IGS precise GPS orbits, namely Lagrange, Newton Divided Difference, Cubic Spline and Trigonometric interpolation. In addition, the paper discusses a new approach, which utilizes the residuals between the broadcast and precise ephemeris to generate a high density precise ephemeris. It is shown that the new approach produces better results than previously reported orbital interpolation accuracy.', 'corpus_id': 128903173, 'score': 1}, {'doc_id': '235128448', 'title': 'Assessing IGS GPS/Galileo/BDS-2/BDS-3 phase bias products with PRIDE PPP-AR', 'abstract': 'Ambiguity Resolution in Precise Point Positioning (PPP-AR) is important to achieving high-precision positioning in wide areas. The International GNSS (Global Navigation Satellite System) Service (IGS) and some other academic organizations have begun to provide phase bias products to enable PPP-AR, such as the integer-clock like products by Centre National d’Etudes Spatials (CNES), Wuhan University (WUM) and the Center for Orbit Determination in Europe (CODE), as well as the Uncalibrated Phase Delay (UPD) products by School of Geodesy and Geomatics (SGG). To evaluate these disparate products, we carry out Global Positioning System (GPS)/Galileo Navigation Satellite System (Galileo) and BeiDou Navigation Satellite System (BDS-only) PPP-AR using 30\xa0days of data in 2019. In general, over 70% and 80% of GPS and Galileo ambiguity residuals after wide-lane phase bias corrections fall in\u2009±\u20090.1 cycles, in contrast to less than 50% for BeiDou Navigation Satellite (Regional) System (BDS-2); moreover, around 90% of GPS/Galileo narrow-lane ambiguity residuals are within\u2009±\u20090.1 cycles, while the percentage drops to about 55% in the case of BDS products. GPS/Galileo daily PPP-AR can usually achieve a positioning precision of 2, 2 and 6\xa0mm for the east, north and up components, respectively, for all phase bias products except those based on German Research Centre for Geosciences (GBM) rapid satellite orbits and clocks. Due to the insufficient number of BDS satellites during 2019, the BDS phase bias products perform worse than the GPS/Galileo products in terms of ambiguity fixing rates and daily positioning precisions. BDS-2 daily positions can only reach a precision of about 10\xa0mm in the horizontal and 20\xa0mm in the vertical components, which can be slightly improved after PPP-AR. However, for the year of 2020, BDS-2/BDS-3 (BDS-3 Navigation Satellite System) PPP-AR achieves about 50% better precisions for all three coordinate components.', 'corpus_id': 235128448, 'score': 0}, {'doc_id': '117162512', 'title': 'Multi-GNSS satellite clock estimation constrained with oscillator noise model in the existence of data discontinuity', 'abstract': 'During the past years, real-time precise point positioning has been proven to be an efficient tool in the applications of navigation, precise orbit determination of LEO as well as earthquake and tsunami early warning, etc. One of the most crucial issues of these applications is the high-precision real-time GNSS satellite clock. Though the performance and character of the GNSS onboard atomic frequency standard have been widely studied, the white noise model is still the most popular hypothesis that employed in the real-time GNSS satellite clock estimation. However, concerning the real-time applications, significant data discontinuity may arise either due to the fact that only regional stations involved, or the failure in the stations, satellites and network connections. These data discontinuity would result in an arbitrary clock jump between adjacent arcs when the clock offsets are modeled as white noise. In addition, it is also expected that the detection and identification of outliers would be benefited from the constrains of the satellite oscillator noise model. Thus in this contribution, based on the statistic analysis of almost 2-year multi-GNSS precise clock products, we developed the oscillator noise model for the satellites of GPS, GLONASS, BDS and Galileo according to the oscillator type as well as the block type. Then, the efficiency of this oscillator noise model in multi-GNSS satellite clock estimation is demonstrated with 2-months data for both regional and global networks in simultaneous real-time mode. For the regional network, the results suggest that compared with the traditional solution based on white noise model, the improvement is 44.4 and 12.1% on average for STD and RMS, respectively, and the improvement is mainly attributed to the efficiency of the oscillator noise model during the convergence period and the gross error resistance. Concerning the global experiment, since the stations guarantee the continuous tracking of the satellites with redundant observable, the improvement is not as evident as that of regional experiment for GPS, GLONASS and BDS. The STD of Galileo clock improves from 0.28 to 0.19\xa0ns due to that, the satellites E14 and E18 still suffer significant data discontinuity during our experimental period.', 'corpus_id': 117162512, 'score': 1}]
108	MM Proteomics	624217f0ad4e0d010543faadf059b9c8	15043	{}	[{'doc_id': '208437380', 'title': 'Next Generation Proteomics and Drug Sensitivity Resistance Testing Allow for the Identification of Distinct Sub-clones of Multiple Myeloma Patients', 'abstract': None, 'corpus_id': 208437380, 'score': 1}, {'doc_id': '213739140', 'title': 'Proteomics-inspired precision medicine for treating and understanding multiple myeloma', 'abstract': 'ABSTRACT Introduction: Remarkable progress in molecular characterization methods has led to significant improvements in how we manage multiple myeloma (MM). The introduction of novel therapies has led to significant improvements in overall survival over the past 10 years. However, MM remains incurable and treatment choice is largely based on outdated risk-adaptive strategies that do not factor in improved treatment outcomes in the context of modern therapies. Areas covered: This review discusses current risk-adaptive strategies in MM and the clinical application of proteomics in the monitoring of treatment response, disease progression, and minimal residual disease (MRD). We also discuss promising biomarkers of disease progression, treatment response, and chemoresistance. Finally, we will discuss an immunomics-based approach to monoclonal antibody (mAb), vaccine, and CAR-T cell development. Expert opinion: It is an exciting era in oncology with basic scientific knowledge translating in novel therapeutic approaches to improve patient outcomes. With the advent of effective immunotherapies and targeted therapies, it has become crucial to identify biomarkers to aid in the stratification of patients based on anticipated sensitivity to chemotherapy. As a paradigm of diseases highly dependent on protein homeostasis, multiple myeloma provides the perfect opportunity to investigate the use of proteomics to aid in precision medicine.', 'corpus_id': 213739140, 'score': 1}, {'doc_id': '210122077', 'title': 'Concurrent lipidomics and proteomics on malignant plasma cells from multiple myeloma patients: Probing the lipid metabolome', 'abstract': 'Background Multiple myeloma (MM) is a hematological malignancy characterized by the clonal expansion of malignant plasma cells. Though durable remissions are possible, MM is considered incurable, with relapse occurring in almost all patients. There has been limited data reported on the lipid metabolism changes in plasma cells during MM progression. Here, we evaluated the feasibility of concurrent lipidomics and proteomics analyses from patient plasma cells, and report these data on a limited number of patient samples, demonstrating the feasibility of the method, and establishing hypotheses to be evaluated in the future. Methods Plasma cells were purified from fresh bone marrow aspirates using CD138 microbeads. Proteins and lipids were extracted using a bi-phasic solvent system with methanol, methyl tert-butyl ether, and water. Untargeted proteomics, untargeted and targeted lipidomics were performed on 7 patient samples using liquid chromatography-mass spectrometry. Two comparisons were conducted: high versus low risk; relapse versus newly diagnosed. Proteins and pathways enriched in the relapsed group was compared to a public transcriptomic dataset from Multiple Myeloma Research Consortium reference collection (n = 222) at gene and pathways level. Results From one million purified plasma cells, we were able to extract material and complete untargeted (~6000 and ~3600 features in positive and negative mode respectively) and targeted lipidomics (313 lipids), as well as untargeted proteomics analysis (~4100 reviewed proteins). Comparative analyses revealed limited differences between high and low risk groups (according to the standard clinical criteria), hence we focused on drawing comparisons between the relapsed and newly diagnosed patients. Untargeted and targeted lipidomics indicated significant down-regulation of phosphatidylcholines (PCs) in relapsed MM. Although there was limited overlap of the differential proteins/transcripts, 76 significantly enriched pathways in relapsed MM were common between proteomics and transcriptomics data. Further evaluation of transcriptomics data for lipid metabolism network revealed enriched correlation of PC, ceramide, cardiolipin, arachidonic acid and cholesterol metabolism pathways to be exclusively correlated among relapsed but not in newly-diagnosed patients. Conclusions This study establishes the feasibility and workflow to conduct integrated lipidomics and proteomics analyses on patient-derived plasma cells. Potential lipid metabolism changes associated with MM relapse warrant further investigation.', 'corpus_id': 210122077, 'score': 1}, {'doc_id': '231789432', 'title': 'Long non‐coding RNAs: Promising new targets in pulmonary fibrosis', 'abstract': 'Pulmonary fibrosis is characterized by progressive and irreversible scarring in the lungs with poor prognosis and treatment. It is caused by various factors, including environmental and occupational exposures, and some rheumatic immune diseases. Even the rapid global spread of the COVID‐19 pandemic can also cause pulmonary fibrosis with a high probability. Functions attributed to long non‐coding RNAs (lncRNAs) make them highly attractive diagnostic and therapeutic targets in fibroproliferative diseases. Therefore, an understanding of the specific mechanisms by which lncRNAs regulate pulmonary fibrotic pathogenesis is urgently needed to identify new possibilities for therapy. In this review, we focus on the molecular mechanisms and implications of lncRNAs targeted protein‐coding and non‐coding genes during pulmonary fibrogenesis, and systematically analyze the communication of lncRNAs with various types of RNAs, including microRNA, circular RNA and mRNA. Finally, we propose the potential approach of lncRNA‐based diagnosis and therapy for pulmonary fibrosis. We hope that understanding these interactions between protein‐coding and non‐coding genes will contribute to the development of lncRNA‐based clinical applications for pulmonary fibrosis.', 'corpus_id': 231789432, 'score': 0}, {'doc_id': '232342121', 'title': 'Proteomic Characterization, Biodistribution, and Functional Studies of Immune-Therapeutic Exosomes: Implications for Inflammatory Lung Diseases', 'abstract': 'Dendritic cell (DC)-derived exosomes (DC EXO), natural nanoparticles of endosomal origin, are under intense scrutiny in clinical trials for various inflammatory diseases. DC EXO are eobiotic, meaning they are well-tolerated by the host; moreover, they can be custom-tailored for immune-regulatory or -stimulatory functions, thus presenting attractive opportunities for immune therapy. Previously we documented the efficacy of immunoregulatory DCs EXO (regDCs EXO) as immunotherapy for inflammatory bone disease, in an in-vivo model. We showed a key role for encapsulated TGFβ1 in promoting a bone sparing immune response. However, the on- and off-target effects of these therapeutic regDC EXO and how target signaling in acceptor cells is activated is unclear. In the present report, therapeutic regDC EXO were analyzed by high throughput proteomics, with non-therapeutic EXO from immature DCs and mature DCs as controls, to identify shared and distinct proteins and potential off-target proteins, as corroborated by immunoblot. The predominant expression in regDC EXO of immunoregulatory proteins as well as proteins involved in trafficking from the circulation to peripheral tissues, cell surface binding, and transmigration, prompted us to investigate how these DC EXO are biodistributed to major organs after intravenous injection. Live animal imaging showed preferential accumulation of regDCs EXO in the lungs, followed by spleen and liver tissue. In addition, TGFβ1 in regDCs EXO sustained downstream signaling in acceptor DCs. Blocking experiments suggested that sustaining TGFβ1 signaling require initial interaction of regDCs EXO with TGFβ1R followed by internalization of regDCs EXO with TGFβ1-TGFβ1R complex. Finally, these regDCs EXO that contain immunoregulatory cargo and showed biodistribution to lungs could downregulate the main severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) target receptor, ACE2 on recipient lung parenchymal cells via TGFβ1 in-vitro. In conclusion, these results in mice may have important immunotherapeutic implications for lung inflammatory disorders.', 'corpus_id': 232342121, 'score': 0}, {'doc_id': '54165255', 'title': 'Current Understanding of the Potential of Proteomics and Metabolomics Approaches in Cancer Chemoresistance: A Focus on Multiple Myeloma.', 'abstract': 'Chemoresistance is one of the major hurdles in cancer treatment leading to recurrence of cancer and affects the overall survival of patients. Cancer chemoresistance can be associated with various phenomena including modulation of vital cellular pathways. Unrevealing these alterations could provide a better understanding of chemoresistance and assist in the identification of new targets to overcome it. Recent advances in the field of proteomics and metabolomics have substantially helped in the identification of potential targets for chemoresistance in various cancers. This review highlights the potential of proteomics and metabolomics research to explore the putative targets associated with cancer chemoresistance with a special focus on Multiple Myeloma (MM). MM is a type of hematological malignancy which constitutes about 13% of all blood cell cancers. The therapeutic advancements for MM have increased the median overall survival rate to over 3-fold in the last one and half decade. Although in recent times, significant improvements in the overall survival rate of MM are achieved, MM remains an incurable disease with unpredictable refractory mechanisms. In spite of therapeutic advances, chemoresistance thrives to be a major hurdle in the treatment of multiple myeloma which demands a better understanding of chemoresistance. In this review, we have attempted to highlight the potential applications of proteomics and metabolomics research in the understanding of chemoresistance in MM.', 'corpus_id': 54165255, 'score': 1}, {'doc_id': '231856025', 'title': 'ACE2 Is a Prognostic Biomarker and Associated with Immune Infiltration in Kidney Renal Clear Cell Carcinoma: Implication for COVID-19', 'abstract': 'Background KIRC is one of the most common cancers with a poor prognosis. ACE2 was involved in tumor angiogenesis and progression in many malignancies. The role of ACE2 in KIRC is still ambiguous. Methods Various bioinformatics analysis tools were investigated to evaluate the prognostic value of ACE2 and its association with immune infiltration in KIRC. Results ACE2 was shown to be downregulated in KIRC at the mRNA and protein level. Low expression of ACE2 protein in KIRC patients was observed in subgroup analyses based on gender, age, weight, tumor grade, and cancer stage. Upregulation of ACE2 in KIRC was associated with a favorable prognosis. ACE2 mRNA expression showed a positive correlation with the abundance of immune cells (B cells, CD8+ T cells, macrophages, neutrophils, and dendritic cells) and the level of immune markers of different immune cells in KIRC. ACE2 expression could affect, in part, the immune infiltration and the advanced cancer stage. Moreover, enrichment analysis revealed that ACE2 in KIRC were mainly involved in translation factor activity, immunoglobulin binding, metabolic pathways, transcriptional misregulation in cancerous cells, cell cycle, and ribosomal activity. Several ACE2-associated kinases, miRNA, and transcription factor targets in KIRC were also identified. Conclusion ACE2 was downregulated in KIRC and served as a prognostic biomarker. It was also shown to be associated with immune infiltration.', 'corpus_id': 231856025, 'score': 0}, {'doc_id': '218535291', 'title': 'Proteomics and functional study reveal marginal zone B and B1 cell specific protein as a candidate marker of multiple myeloma.', 'abstract': 'Multiple myeloma (MM) is a plasma cell‑associated cancer and accounts for 13%\xa0of all hematological malignancies, worldwide. MM still remains an incurable plasma cell malignancy with a poor prognosis due to a lack of suitable markers. Therefore, discovering novel markers and targets for diagnosis and therapeutics of MM is essential. The present study aims to identify markers associated with MM malignancy using patient‑derived MM mononuclear cells (MNCs). Label‑free quantitative proteomics analysis revealed a total of 192\xa0differentially regulated proteins, in which 79\xa0proteins were upregulated and 113\xa0proteins were found to be downregulated in MM MNCs as compared to non‑hematological malignant samples. The identified differentially expressed candidate proteins were analyzed using various bioinformatics tools, including Ingenuity Pathway Analysis (IPA), Protein Analysis THrough Evolutionary Relationships (PANTHER), Search Tool for the Retrieval of Interacting Genes/Proteins (STRING) and Database for Annotation, Visualization and Integrated Discovery (DAVID) to determine their biological context. Among the 192\xa0candidate proteins, marginal zone\xa0B\xa0and\xa0B1 cell specific protein (MZB1) was investigated in detail using the RPMI-8226 cell line model of MM. The functional studies revealed that higher expression of MZB1 is associated with promoting the progression of MM pathogenesis and could be established as a potential target for MM in the future.', 'corpus_id': 218535291, 'score': 1}, {'doc_id': '231776957', 'title': 'Multi-omic Characterization of Human Tubular Epithelial Cell Response to Serum', 'abstract': 'Proteinuria, the spillage of serum proteins into the urine, is a feature of glomerulonephritides, podocyte disorders and diabetic nephropathy. However, the response of tubular epithelial cells to serum protein exposure has not been systematically characterized. Using transcriptomic profiling we studied serum-induced changes in primary human tubular epithelial cells cultured in 3D microphysiological devices. Serum proteins induced cellular proliferation, cytokine secretion and activated a coordinated stress response. We orthogonally confirmed our findings by comparing the transcriptomic and epigenomic landscapes of intact human kidney cortex and isolated tubular epithelial cells cultured in fetal bovine serum. Importantly, key transcriptomic programs in response to either type of serum exposure remained consistent, including comparisons to an established mouse model of kidney injury. This serum-induced transcriptional response was dominated by switching off of nuclear receptor-driven programs and activation of AP-1 and NF-κB signatures in the tubular epigenomic landscape. These features of active regulation were seen at canonical kidney injury genes (HAVCR1) and genes associated with COVID-19 (ACE2, IL6). Our data provide a reference map for dissecting the regulatory and transcriptional response of kidney tubular epithelial cells injury induced by serum.', 'corpus_id': 231776957, 'score': 0}, {'doc_id': '231791227', 'title': 'Proteomic investigation reveals dominant alterations of neutrophil degranulation and mRNA translation pathways in patients with COVID-19', 'abstract': '\n The altered molecular proteins and pathways in response to COVID-19 infection are still unclear. Here, we performed a comprehensive proteomics-based investigation of nasopharyngeal swab samples from COVID-19 patients to study the host response by employing simple extraction strategies. Few of the host proteins such as Interleukin-6, L-lactate dehydrogenase, C-reactive protein, Ferritin and Aspartate aminotransferase were found to be up-regulated only in COVID-19 positive patients using targeted Multiple Reaction Monitoring studies. The most important pathways identified by enrichment analysis were neutrophil degranulation, interleukin-12 signaling pathways and mRNA translation of proteins thus providing the detailed investigation of host response in COVID-19 infection. Thus, we conclude that mass spectrometry-detected host proteins have a potential for disease severity progression; however, suitable validation strategies should be deployed for the clinical translation. Furthermore, the in-silico docking of host proteins involved in the interleukin-12 signaling pathway might aid in COVID-19 therapeutic interventions.\n', 'corpus_id': 231791227, 'score': 0}]
109	Data Science Education	09358dd38e8b7e718d2092408a14c059	20852	{}	"[{'doc_id': '211521586', 'title': 'Cross-Disciplinary Faculty Development in Data Science Principles for Classroom Integration', 'abstract': ""Data science in practice leverages the expertise in computer science, mathematics and statistics with applications in any field using data. The formalization of data science educational and pedagogical strategic remain in their infancy. College faculty from various disciplines are tasked with designing and delivering data science instruction without the formal knowledge of how data science principles are executed in practice. We call this the data science instruction gap. Also, these faculties are implementing their discipline's standard pedagogical strategies to their understanding of data science. In this paper, we present our cross-disciplinary instructional program model designed to narrow the data science instruction gap for faculty. It is designed to scaffold college faculties' data science learning to support their discipline-specific data science instruction. We provide individualized and group-based support structures to instill data science principles and transition them from learners to educators in data science. Lastly, we share our model's impact on and value to faculty as well as make recommendations for model adoption."", 'corpus_id': 211521586, 'score': 1}, {'doc_id': '237376556', 'title': 'Peer teaching as bioinformatics training strategy: incentives, challenges, and benefits', 'abstract': 'As biomedical research becomes more data-intensive, bioinformatics is becoming essential to understanding biological processes, systems, and diseases. In this paper we describe the use of a series of peer teaching workshops as a strategy to respond to the bioinformatics training needs at a research-intensive institution. In addition to the data collected from the workshops, we also used personal experiences of researchers who participated as peer teachers to understand the incentives, challenges, and benefits of peer teaching. Developing communication skills such as confidence in teaching, explaining complex concepts, and better understanding of the topic emerged as primary benefits that the teachers obtained from this experience. Lack of time for teaching and the struggles of classroom management were identified as two major challenges. We suggest that peer teaching can be beneficial not only to train researchers in bioinformatics, but also as a professional development opportunity for graduate students and postdoctoral trainees.', 'corpus_id': 237376556, 'score': 0}, {'doc_id': '237532112', 'title': 'Student-centric graduate training in mathematics: A commentary', 'abstract': 'Career opportunities for PhDs in the mathematical sciences have never been better. Traditional faculty positions in mathematics departments in colleges and universities range from all teaching to combined teaching/research responsibilities. Beyond those, a wide array of careers has now opened up to freshly minted graduates, in academics, industry, business, and government. It is well-understood that these all require somewhat different preparations for Ph.D.s to be competitive. This commentary compares and contrasts mathematics graduate programs with Ph.D. programs in the life and biomedical sciences, which are structured in a way that allows considerable customization around students’ career goals. While these programs may not be appropriate templates for the mathematical sciences, they have some features that might be informative. This commentary is intended to add perspective to the ongoing discussion around PhD training in the mathematical sciences. It also provides some concrete proposals for changes.', 'corpus_id': 237532112, 'score': 0}, {'doc_id': '212702488', 'title': 'Targeted Curricular Innovations in Data Science', 'abstract': 'Many employers expect skills such as proper data generation, collection, storage, and analysis; however, these skills are often not taught in the undergraduate experience. Many STEM disciplines require computing coursework that includes coding in a modern programming language but does not explicitly address data stewardship. In this research, we present a faculty-focused data science program to address this educational gap. The faculty at two undergraduate institutions participate in a year-long slate of activities to learn about data science and design mechanisms for classroom dissemination. We describe the program details and provide sample classroom implementations as well as a summary of faculty post-participation perceptions.', 'corpus_id': 212702488, 'score': 1}, {'doc_id': '6311574', 'title': 'A Data Science Course for Undergraduates: Thinking With Data', 'abstract': 'Data science is an emerging interdisciplinary field that combines elements of mathematics, statistics, computer science, and knowledge in a particular application domain for the purpose of extracting meaningful information from the increasingly sophisticated array of data available in many settings. These data tend to be nontraditional, in the sense that they are often live, large, complex, and/or messy. A first course in statistics at the undergraduate level typically introduces students to a variety of techniques to analyze small, neat, and clean datasets. However, whether they pursue more formal training in statistics or not, many of these students will end up working with data that are considerably more complex, and will need facility with statistical computing techniques. More importantly, these students require a framework for thinking structurally about data. We describe an undergraduate course in a liberal arts environment that provides students with the tools necessary to apply data science. The course emphasizes modern, practical, and useful skills that cover the full data analysis spectrum, from asking an interesting question to acquiring, managing, manipulating, processing, querying, analyzing, and visualizing data, as well communicating findings in written, graphical, and oral forms. Supplementary materials for this article are available online. [Received June 2014. Revised July 2015.]', 'corpus_id': 6311574, 'score': 1}, {'doc_id': '88520302', 'title': 'Data Science in Statistics Curricula: Preparing Students to “Think with Data”', 'abstract': 'A growing number of students are completing undergraduate degrees in statistics and entering the workforce as data analysts. In these positions, they are expected to understand how to use databases and other data warehouses, scrape data from Internet sources, program solutions to complex problems in multiple languages, and think algorithmically as well as statistically. These data science topics have not traditionally been a major component of undergraduate programs in statistics. Consequently, a curricular shift is needed to address additional learning outcomes. The goal of this article is to motivate the importance of data science proficiency and to provide examples and resources for instructors to implement data science in their own statistics curricula. We provide case studies from seven institutions. These varied approaches to teaching data science demonstrate curricular innovations to address new needs. Also included here are examples of assignments designed for courses that foster engagement of undergraduates with data and data science. [Received November 2014. Revised July 2015.]', 'corpus_id': 88520302, 'score': 1}, {'doc_id': '237329695', 'title': 'The future of data(base) education: Is the ""cow book"" dead?', 'abstract': 'This panel encourages a debate over the future of database education and its relationship to Data Science: Are Computer Science (CS) and Data Science (DS) different disciplines about to split, and how does that effect how we teach our field? Is there a ""data"" course that belongs in CS that all of our students should take? Who is the traditional database course, e.g. based on the ""cow book"", relevant to? What traditional topics should we not be teaching in our core data course(s) and which ones should be added? What do we teach the student who has one elective for data science? How does our community position itself for leadership in CS given the popularity of DS? PVLDB Reference Format: Zachary G. Ives, Rachel Pottinger, Arun Kumar, Johannes Gehrke, and Jana Giceva. The future of data(base) education: Is the ""cow book"" dead?. PVLDB, 14(12): 3239 3240, 2021. doi:10.14778/3476311.3476394', 'corpus_id': 237329695, 'score': 0}, {'doc_id': '2531277', 'title': 'A Guide to Teaching Data Science', 'abstract': 'ABSTRACT Demand for data science education is surging and traditional courses offered by statistics departments are not meeting the needs of those seeking training. This has led to a number of opinion pieces advocating for an update to the Statistics curriculum. The unifying recommendation is that computing should play a more prominent role. We strongly agree with this recommendation, but advocate the main priority is to bring applications to the forefront as proposed by Nolan and Speed in 1999. We also argue that the individuals tasked with developing data science courses should not only have statistical training, but also have experience analyzing data with the main objective of solving real-world problems. Here, we share a set of general principles and offer a detailed guide derived from our successful experience developing and teaching a graduate-level, introductory data science course centered entirely on case studies. We argue for the importance of statistical thinking, as defined by Wild and Pfannkuch in 1999 and describe how our approach teaches students three key skills needed to succeed in data science, which we refer to as creating, connecting, and computing. This guide can also be used for statisticians wanting to gain more practical knowledge about data science before embarking on teaching an introductory course. Supplementary materials for this article are available online.', 'corpus_id': 2531277, 'score': 1}, {'doc_id': '237273810', 'title': 'Faculty Development Aimed at Sustaining and Enhancing Entrepreneurial-minded Learning', 'abstract': 'Many higher education institutions have begun promoting an entrepreneurial mindset (EM) in students and integrating entrepreneurship elements in engineering education. Various approaches, including curricular, extra-curricular and co-curricular initiatives, are being used to transform the education offered at these institutions. However, in order for this transformation to be sustained and broadened, efforts must target faculty as well as students. Helping faculty to embrace entrepreneurial minded learning (EML) and equipping them with relevant tools and resources will ensure true transformation and long-term success. At the University of New Haven, our efforts started with implementing an innovative curricular model designed to develop an entrepreneurial mindset in students and establishing initiatives to provide students other forms of engagement opportunities. The curricular model involved the development and integration of e-learning modules — targeting various entrepreneurial concepts and skills — into courses spanning all four years of all engineering and computer science programs. Innovation and pitch competitions, participation in the University Innovation Fellows program, and an entrepreneurial engineering living learning community were primary extra-curricular components. Faculty development opportunities were provided as part of these initiatives including training for effectively integrating the e-learning modules into courses, participation in workshops and conferences with a focus on entrepreneurial education, and involvement in organizing and facilitating student activities. While a significant number of our engineering and computer science faculty participated in these development opportunities, in general their enthusiasm related to entrepreneurial minded learning (EML) was not strong enough to sustain and further broaden EML within the college. Therefore, we implemented a faculty development program aimed at fostering EM champions from different engineering and computer science disciplines, as well as a mini-grant program to stimulate faculty to independently integrate EML into their courses. In this paper, we present these efforts, describe the program components, and report on findings. Sample products resulting from the faculty development efforts are also provided.', 'corpus_id': 237273810, 'score': 0}, {'doc_id': '237518352', 'title': 'Equipping and Empowering Faculty through Professional Development to Create a Future-Ready Workforce in Emerging Technologies', 'abstract': 'Tech industry, especially, some areas within tech fields, such as Emerging Technology (EmTech), like cybersecurity, data science, mobile development, machine learning, AI, and cloud computing, are expected to experience immense increases in job opportunities in coming years. While a variety of solutions are necessary to address the growing workforce needs in the EmTech industry, one of the largest untapped talent pools is women and underrepresented students. Clearly, HBCU and MSI hold great potential to broaden participation in EmTech because of their more diverse student populations, access to a large number of underrepresented students, and closer faculty-to-student interaction. However, faculties at these institutions, who are at the forefront of developing required skills in students are often overlooked. Faculties at these institutions need help designing and implementing effective and evidence-based instruction materials to develop skills that are in high-demand in the EmTech industry. The goal of this panel is to offer a platform that can provide insight into the development of faculty professional development programs in EmTech in traditional institutions and within the context of HBCU and MSI.', 'corpus_id': 237518352, 'score': 0}]"
110	NER with BERT	d7895b142efa2b3784dc30b0981a3872	12457	{'NER': 'nucleotide excision repair', 'BERT': 'Bidirectional Encoder Representations from Transformers'}	"[{'doc_id': '228063930', 'title': 'You Only Need Adversarial Supervision for Semantic Image Synthesis', 'abstract': 'Despite their recent successes, GAN models for semantic image synthesis still suffer from poor image quality when trained with only adversarial supervision. Historically, additionally employing the VGG-based perceptual loss has helped to overcome this issue, significantly improving the synthesis quality, but at the same time limiting the progress of GAN models for semantic image synthesis. In this work, we propose a novel, simplified GAN model, which needs only adversarial supervision to achieve high quality results. We re-design the discriminator as a semantic segmentation network, directly using the given semantic label maps as the ground truth for training. By providing stronger supervision to the discriminator as well as to the generator through spatially- and semantically-aware discriminator feedback, we are able to synthesize images of higher fidelity with better alignment to their input label maps, making the use of the perceptual loss superfluous. Moreover, we enable high-quality multi-modal image synthesis through global and local sampling of a 3D noise tensor injected into the generator, which allows complete or partial image change. We show that images synthesized by our model are more diverse and follow the color and texture distributions of real images more closely. We achieve an average improvement of $6$ FID and $5$ mIoU points over the state of the art across different datasets using only adversarial supervision.', 'corpus_id': 228063930, 'score': 0}, {'doc_id': '227231779', 'title': 'A Semi-Supervised BERT Approach for Arabic Named Entity Recognition', 'abstract': 'Named entity recognition (NER) plays a significant role in many applications such as information extraction, information retrieval, question answering, and even machine translation. Most of the work on NER using deep learning was done for non-Arabic languages like English and French, and only few studies focused on Arabic. This paper proposes a semi-supervised learning approach to train a BERT-based NER model using labeled and semi-labeled datasets. We compared our approach against various baselines, and state-of-the-art Arabic NER tools on three datasets: AQMAR, NEWS, and TWEETS. We report a significant improvement in F-measure for the AQMAR and the NEWS datasets, which are written in Modern Standard Arabic (MSA), and competitive results for the TWEETS dataset, which contains tweets that are mostly in the Egyptian dialect and contain many mistakes or misspellings.', 'corpus_id': 227231779, 'score': 1}, {'doc_id': '226965681', 'title': 'SAG-GAN: Semi-Supervised Attention-Guided GANs for Data Augmentation on Medical Images', 'abstract': 'Recently deep learning methods, in particular, convolutional neural networks (CNNs), have led to a massive breakthrough in the range of computer vision. Also, the large-scale annotated dataset is the essential key to a successful training procedure. However, it is a huge challenge to get such datasets in the medical domain. Towards this, we present a data augmentation method for generating synthetic medical images using cycle-consistency Generative Adversarial Networks (GANs). We add semi-supervised attention modules to generate images with convincing details. We treat tumor images and normal images as two domains. The proposed GANs-based model can generate a tumor image from a normal image, and in turn, it can also generate a normal image from a tumor image. Furthermore, we show that generated medical images can be used for improving the performance of ResNet18 for medical image classification. Our model is applied to three limited datasets of tumor MRI images. We first generate MRI images on limited datasets, then we trained three popular classification models to get the best model for tumor classification. Finally, we train the classification model using real images with classic data augmentation methods and classification models using synthetic images. The classification results between those trained models showed that the proposed SAG-GAN data augmentation method can boost Accuracy and AUC compare with classic data augmentation methods. We believe the proposed data augmentation method can apply to other medical image domains, and improve the accuracy of computer-assisted diagnosis.', 'corpus_id': 226965681, 'score': 0}, {'doc_id': '229924266', 'title': 'FREA-Unet: Frequency-aware U-net for Modality Transfer', 'abstract': ""While Positron emission tomography (PET) imaging has been widely used in diagnosis of number of diseases, it has costly acquisition process which involves radiation exposure to patients. However, magnetic resonance imaging (MRI) is a safer imaging modality that does not involve patient's exposure to radiation. Therefore, a need exists for an efficient and automated PET image generation from MRI data. In this paper, we propose a new frequency-aware attention U-net for generating synthetic PET images. Specifically, we incorporate attention mechanism into different U-net layers responsible for estimating low/high frequency scales of the image. Our frequency-aware attention Unet computes the attention scores for feature maps in low/high frequency layers and use it to help the model focus more on the most important regions, leading to more realistic output images. Experimental results on 30 subjects from Alzheimers Disease Neuroimaging Initiative (ADNI) dataset demonstrate good performance of the proposed model in PET image synthesis that achieved superior performance, both qualitative and quantitative, over current state-of-the-arts."", 'corpus_id': 229924266, 'score': 0}, {'doc_id': '221095574', 'title': 'GANBERT: Generative Adversarial Networks with Bidirectional Encoder Representations from Transformers for MRI to PET synthesis', 'abstract': 'Synthesizing medical images, such as PET, is a challenging task due to the fact that the intensity range is much wider and denser than those in photographs and digital renderings and are often heavily biased toward zero. Above all, intensity values in PET have absolute significance, and are used to compute parameters that are reproducible across the population. Yet, usually much manual adjustment has to be made in pre-/post- processing when synthesizing PET images, because its intensity ranges can vary a lot, e.g., between -100 to 1000 in floating point values. To overcome these challenges, we adopt the Bidirectional Encoder Representations from Transformers (BERT) algorithm that has had great success in natural language processing (NLP), where wide-range floating point intensity values are represented as integers ranging between 0 to 10000 that resemble a dictionary of natural language vocabularies. BERT is then trained to predict a proportion of masked values images, where its ""next sentence prediction (NSP)"" acts as GAN discriminator. Our proposed approach, is able to generate PET images from MRI images in wide intensity range, with no manual adjustments in pre-/post- processing. It is a method that can scale and ready to deploy.', 'corpus_id': 221095574, 'score': 1}, {'doc_id': '223957158', 'title': 'Coarse-to-Fine Pre-training for Named Entity Recognition', 'abstract': 'More recently, Named Entity Recognition hasachieved great advances aided by pre-trainingapproaches such as BERT. However, currentpre-training techniques focus on building lan-guage modeling objectives to learn a gen-eral representation, ignoring the named entity-related knowledge. To this end, we proposea NER-specific pre-training framework to in-ject coarse-to-fine automatically mined entityknowledge into pre-trained models. Specifi-cally, we first warm-up the model via an en-tity span identification task by training it withWikipedia anchors, which can be deemed asgeneral-typed entities. Then we leverage thegazetteer-based distant supervision strategy totrain the model extract coarse-grained typedentities. Finally, we devise a self-supervisedauxiliary task to mine the fine-grained namedentity knowledge via clustering.Empiricalstudies on three public NER datasets demon-strate that our framework achieves significantimprovements against several pre-trained base-lines, establishing the new state-of-the-art per-formance on three benchmarks. Besides, weshow that our framework gains promising re-sults without using human-labeled trainingdata, demonstrating its effectiveness in label-few and low-resource scenarios', 'corpus_id': 223957158, 'score': 1}, {'doc_id': '229923303', 'title': 'Few-Shot Named Entity Recognition: A Comprehensive Study', 'abstract': 'This paper presents a comprehensive study to efficiently build named entity recognition (NER) systems when a small number of indomain labeled data is available. Based upon recent Transformer-based self-supervised pretrained language models (PLMs), we investigate three orthogonal schemes to improve the model generalization ability for few-shot settings: (1) meta-learning to construct prototypes for different entity types, (2) supervised pre-training on noisy web data to extract entity-related generic representations and (3) self-training to leverage unlabeled in-domain data. Different combinations of these schemes are also considered. We perform extensive empirical comparisons on 10 public NER datasets with various proportions of labeled data, suggesting useful insights for future research. Our experiments show that (i) in the few-shot learning setting, the proposed NER schemes significantly improve or outperform the commonly used baseline, a PLM-based linear classifier fine-tuned on domain labels. (ii) We create new state-of-the-art results on both few-shot and training-free settings compared with existing methods. We will release our code and pretrained models for reproducible research.', 'corpus_id': 229923303, 'score': 1}, {'doc_id': '229923960', 'title': 'Generative Adversarial Network for Image Synthesis', 'abstract': 'This chapter reviews recent developments of generative adversarial networks (GAN)-based methods for medical and biomedical image synthesis tasks. These methods are classified into conditional GAN and Cycle-GAN according to the network architecture designs. For each category, a literature survey is given, which covers discussions of the network architecture designs, highlights important contributions and identifies specific challenges. keywords: Image synthesis, deep learning, Generative Adversarial Network, GAN.', 'corpus_id': 229923960, 'score': 0}, {'doc_id': '226476611', 'title': 'DUG-RECON: A Framework for Direct Image Reconstruction Using Convolutional Generative Networks', 'abstract': 'This article explores convolutional generative networks as an alternative to iterative reconstruction algorithms in medical image reconstruction. The task of medical image reconstruction involves mapping of projection domain data collected from the detector to the image domain. This mapping is done typically through iterative reconstruction algorithms which are time consuming and computationally expensive. Trained deep learning networks provide faster outputs as proven in various tasks across computer vision. In this work, we propose a direct reconstruction framework exclusively with deep learning architectures. The proposed framework consists of three segments, namely, denoising, reconstruction, and super resolution (SR). The denoising and the SR segments act as processing steps. The reconstruction segment consists of a novel double U-Net generator (DUG) which learns the sinogram-to-image transformation. This entire network was trained on positron emission tomography (PET) and computed tomography (CT) images. The reconstruction framework approximates 2-D mapping from the projection domain to the image domain. The architecture proposed in this proof-of-concept work is a novel approach to direct image reconstruction; further improvement is required to implement it in a clinical setting.', 'corpus_id': 226476611, 'score': 0}, {'doc_id': '226262399', 'title': 'Entity Enhanced BERT Pre-training for Chinese NER', 'abstract': 'Character-level BERT pre-trained in Chinese suffers a limitation of lacking lexicon information, which shows effectiveness for Chinese NER. To integrate the lexicon into pre-trained LMs for Chinese NER, we investigate a semi-supervised entity enhanced BERT pre-training method. In particular, we first extract an entity lexicon from the relevant raw text using a new-word discovery method. We then integrate the entity information into BERT using Char-Entity-Transformer, which augments the self-attention using a combination of character and entity representations. In addition, an entity classification task helps inject the entity information into model parameters in pre-training. The pre-trained models are used for NER fine-tuning. Experiments on a news dataset and two datasets annotated by ourselves for NER in long-text show that our method is highly effective and achieves the best results.', 'corpus_id': 226262399, 'score': 1}]"
111	AR/VR Collaboration	f6d67778f1309a488644a1ff54527639	13715	{}	"[{'doc_id': '231836916', 'title': 'Grand Challenges in Immersive Analytics', 'abstract': 'Immersive Analytics is a quickly evolving field that unites several areas such as visualisation, immersive environments, and human-computer interaction to support human data analysis with emerging technologies. This research has thrived over the past years with multiple workshops, seminars, and a growing body of publications, spanning several conferences. Given the rapid advancement of interaction technologies and novel application domains, this paper aims toward a broader research agenda to enable widespread adoption. We present 17 key research challenges developed over multiple sessions by a diverse group of 24 international experts, initiated from a virtual scientific workshop at ACM CHI 2020. These challenges aim to coordinate future work by providing a systematic roadmap of current directions and impending hurdles to facilitate productive and effective applications for Immersive Analytics.', 'corpus_id': 231836916, 'score': 0}, {'doc_id': '231602925', 'title': 'Proxemics and Social Interactions in an Instrumented Virtual Reality Workshop', 'abstract': 'Virtual environments (VEs) can create collaborative and social spaces, which are increasingly important in the face of remote work and travel reduction. Recent advances, such as more open and widely available platforms, create new possibilities to observe and analyse interaction in VEs. Using a custom instrumented build of Mozilla Hubs to measure position and orientation, we conducted an academic workshop to facilitate a range of typical workshop activities. We analysed social interactions during a keynote, small group breakouts, and informal networking/hallway conversations. Our mixed-methods approach combined environment logging, observations, and semi-structured interviews. The results demonstrate how small and large spaces influenced group formation, shared attention, and personal space, where smaller rooms facilitated more cohesive groups while larger rooms made small group formation challenging but personal space more flexible. Beyond our findings, we show how the combination of data and insights can fuel collaborative spaces’ design and deliver more effective virtual workshops.', 'corpus_id': 231602925, 'score': 1}, {'doc_id': '231171502', 'title': 'Mobile Multiuser AR/VR for Training', 'abstract': 'Augmented reality (AR) is a growing technology for building immersive and interactive applications for anyone to use. Integrated development environments such as Unity when enhanced with additional plugins like Unity’s AR Foundation and Photon Network, enable the rapid development of multiuser, mobile, augmented, and mixed reality (XR) applications that can be both entertaining and useful. This report is about a mobile AR and virtual reality (VR), or XR multiuser classroom prototype application. This project was developed on Unity with Unity’s AR Foundation Kit, Photon Network, and Android AR Core plugins to implement an Android based mobile phone application and a desktop application. The final multiuser applications include a real-time multiuser AR/VR environment for an emulated AR/VR Classroom, and an AR Spinning Top Demonstration. Keywords— augmented reality (AR). virtual reality (VR), mixed reality (XR), Unity, Photon Network, AR Foundation, AR Core.', 'corpus_id': 231171502, 'score': 1}, {'doc_id': '231719722', 'title': 'Art and Science Interaction Lab - A highly flexible and modular interaction science research facility', 'abstract': 'The Art and Science Interaction Lab (“ASIL”) is a unique, highly flexible and modular “interaction science” research facility to effectively bring, analyse and test experiences and interactions in mixed virtual/augmented contexts as well as to conduct research on next-gen immersive technologies. It brings together the expertise and creativity of engineers, performers, designers and scientists creating solutions and experiences shaping the lives of people. The lab is equipped with stateof-the-art visual, auditory and user-tracking equipment, fully synchronized and connected to a central backend. This synchronization allows for highly accurate multi-sensor measurements and analysis.', 'corpus_id': 231719722, 'score': 0}, {'doc_id': '210832977', 'title': 'CollabAR \x96 Investigating the Mediating Role of Mobile AR Interfaces on Co-Located Group Collaboration', 'abstract': 'Mobile Augmented Reality (AR) technology is enabling new applications for different domains including architecture, education or medical work. As AR interfaces project digital data, information and models into the real world, it allows for new forms of collaborative work. However, despite the wide availability of AR applications, very little is known about how AR interfaces mediate and shape collaborative practices. This paper presents a study which examines how a mobile AR (M-AR) interface for inspecting and discovering AR models of varying complexity impacts co-located group practices. We contribute new insights into how current mobile AR interfaces impact co-located collaboration. Our results show that M-AR interfaces induce high mental load and frustration, cause a high number of context switches between devices and group discussion, and overall leads to a reduction in group interaction. We present design recommendations for future work focusing on collaborative AR interfaces.', 'corpus_id': 210832977, 'score': 1}, {'doc_id': '231879568', 'title': 'A Survey on Synchronous Augmented, Virtual and Mixed Reality Remote Collaboration Systems', 'abstract': 'Remote collaboration systems have become increasingly important in today’s society, especially during times where physical distancing is advised. Industry, research and individuals face the challenging task of collaborating and networking over long distances. While video and teleconferencing are already widespread, collaboration systems in augmented, virtual, and mixed reality are still a niche technology. We provide an overview of recent developments of synchronous remote collaboration systems and create a taxonomy by dividing them into three main components that form such systems: Environment, Avatars, and Interaction. A thorough overview of existing systems is given, categorising their main contributions in order to help researchers working in different fields by providing concise information about specific topics such as avatars, virtual environment, visualisation styles and interaction. The focus of this work is clearly on synchronised collaboration from a distance. A total of 82 unique systems for remote collaboration are discussed, including more than 100 publications and 25 commercial systems.', 'corpus_id': 231879568, 'score': 1}, {'doc_id': '126913779', 'title': 'OVERVIEW OF GPS NETWORK FOR GIS CONTROL', 'abstract': 'There are several Florida cities and counties involved in programs to establish 3-mile grids of geodetic survey points as a part of the development of a city- or county-wide Geographic Information System (GIS). The Global Positioning System (GPS) has been used to establish the positions of the survey points in support of the program. This program is expected to continue until all counties in Florida are completed. To aid in the reliability of the geodetic reference system used to establish these local networks, the Florida Department of Natural Resources (FDNR), Florida Department of Transportation (FDOT), National Geodetic Survey (NGS), and Federal Aviation Administration (FAA) have entered into an agreement to establish a statewide geodetic network with a horizontal accuracy of order B (8 mm + 1:1,000,000). This paper discusses the GPS surveys which have been completed, the surveys in progress, and possible future surveys which will be done in support of GIS.', 'corpus_id': 126913779, 'score': 0}, {'doc_id': '97888511', 'title': 'New red light-emitting conjugated rigid-rod polymer: poly(benzobisthiazole-1,4-phenylenebisvinylene)', 'abstract': 'The solution-processable poly(benzobisthiazole-1,4-phenylenebisvinylene) (PBTPV) is synthesized. The protonated PBTPV in dilute methanesulfonic acid solutions exhibit a yellow-green emission with a peak at 544 nm and fluorescence quantum yield of 100%. PBTPV thin films, prepared from its soluble complexes in nitromethane, emit red light with an emission peak at 640 nm with an estimated quantum yield of ∼ 4-5%. PBTPV starts to decompose at 560°C under nitrogen', 'corpus_id': 97888511, 'score': 0}, {'doc_id': '227014601', 'title': 'Combining Gesture and Voice Control for Mid-Air Manipulation of CAD Models in VR Environments', 'abstract': 'Modeling 3D objects in domains like Computer Aided Design (CAD) is time-consuming and comes with a steep learning curve needed to master the design process as well as tool complexities. In order to simplify the modeling process, we designed and implemented a prototypical system that leverages the strengths of Virtual Reality (VR) hand gesture recognition in combination with the expressiveness of a voice-based interface for the task of 3D modeling. Furthermore, we use the Constructive Solid Geometry (CSG) tree representation for 3D models within the VR environment to let the user manipulate objects from the ground up, giving an intuitive understanding of how the underlying basic shapes connect. The system uses standard mid-air 3D object manipulation techniques and adds a set of voice commands to help mitigate the deficiencies of current hand gesture recognition techniques. A user study was conducted to evaluate the proposed prototype. The combination of our hybrid input paradigm shows to be a promising step towards easier to use CAD modeling.', 'corpus_id': 227014601, 'score': 0}, {'doc_id': '228102859', 'title': 'RealitySketch: An AR interface to create responsive sketches', 'abstract': ""Researchers at University of Calgary, Adobe Research and University of Colorado Boulder have recently created an augmented reality (AR) interface that can be used to produce responsive sketches, graphics and visualizations. Their work, initially pre-published on arXiv, won the Best Paper Honorable Mention and Best Demo Honorable Mention awards at the ACM Symposium on User Interface Software and Technology (UIST'20)."", 'corpus_id': 228102859, 'score': 1}]"
112	Inbred strains	3048764257ad1e534ce1898200ce5399	16333	{}	[{'doc_id': '232021281', 'title': 'Identifying Potentially Beneficial Genetic Mutations Associated with Monophyletic Selective Sweep and a Proof-of-Concept Study with Viral Genetic Data', 'abstract': 'In biology, research on evolution is important to understand the significance of genetic mutation. When there is a significantly beneficial mutation, a population of species with the mutation prospers and predominates, in a process called “selective sweep.” However, there are few methods that can find such a mutation causing selective sweep from genetic data. ABSTRACT Genetic mutations play a central role in evolution. For a significantly beneficial mutation, a one-time mutation event suffices for the species to prosper and predominate through the process called “monophyletic selective sweep.” However, existing methods that rely on counting the number of mutation events to detect selection are unable to find such a mutation in selective sweep. We here introduce a method to detect mutations at the single amino acid/nucleotide level that could be responsible for monophyletic selective sweep evolution. The method identifies a genetic signature associated with selective sweep using the population genetic test statistic Tajima’s D. We applied the algorithm to ebolavirus, influenza A virus, and severe acute respiratory syndrome coronavirus 2 to identify known biologically significant mutations and unrecognized mutations associated with potential selective sweep. The method can detect beneficial mutations, possibly leading to discovery of previously unknown biological functions and mechanisms related to those mutations. IMPORTANCE In biology, research on evolution is important to understand the significance of genetic mutation. When there is a significantly beneficial mutation, a population of species with the mutation prospers and predominates, in a process called “selective sweep.” However, there are few methods that can find such a mutation causing selective sweep from genetic data. We here introduce a novel method to detect such mutations. Applying the method to the genomes of ebolavirus, influenza viruses, and the novel coronavirus, we detected known biologically significant mutations and identified mutations the importance of which is previously unrecognized. The method can deepen our understanding of molecular and evolutionary biology.', 'corpus_id': 232021281, 'score': 0}, {'doc_id': '227157441', 'title': 'Mouse Genome Database (MGD): Knowledgebase for mouse–human comparative biology', 'abstract': 'Abstract The Mouse Genome Database (MGD; http://www.informatics.jax.org) is the community model organism knowledgebase for the laboratory mouse, a widely used animal model for comparative studies of the genetic and genomic basis for human health and disease. MGD is the authoritative source for biological reference data related to mouse genes, gene functions, phenotypes and mouse models of human disease. MGD is the primary source for official gene, allele, and mouse strain nomenclature based on the guidelines set by the International Committee on Standardized Nomenclature for Mice. MGD’s biocuration scientists curate information from the biomedical literature and from large and small datasets contributed directly by investigators. In this report we describe significant enhancements to the content and interfaces at MGD, including (i) improvements in the Multi Genome Viewer for exploring the genomes of multiple mouse strains, (ii) inclusion of many more mouse strains and new mouse strain pages with extended query options and (iii) integration of extensive data about mouse strain variants. We also describe improvements to the efficiency of literature curation processes and the implementation of an information portal focused on mouse models and genes for the study of COVID-19.', 'corpus_id': 227157441, 'score': 1}, {'doc_id': '232048785', 'title': 'Advancement of chromosome science in the genomics era', 'abstract': 'In 1946, an eminent plant cytogeneticist Hitoshi Kihara coined an aphorism “The history of the earth is recorded in the layers of its crust; the history of all organisms is inscribed in the chromosomes” (Crow 1994). We are all aware that chromosomes are carriers of genetic materials that pass the exact genetic information to the next generations. Chromosomes are linear structures with genes located along the specific sites of the chromosomes. However, chromosomes are stuffed with highly non-genic repetitive sequences so that only small fractions of the genomes encode proteins. Thus chromosomes are highly dynamic structures that ensure the transfer of intact genetic materials to next generations as well as regulating the expression of genes in the ocean of repetitive DNA sequences. The wealth genome sequences enhance our understanding of the chromosome dynamics. There was a conference of the 7th AsianPacific Chromosome Colloquium in November 26–28, 2020, in Pusan, Korea. The theme was “Advancement of chromosome science in the genomics era”. The conference was held virtual online in both oral and poster presentation due to the COVID-19 pandemic. There were five sessions; Chromosome structure, Sex chromosomes and B-chromosomes, Chromosomes and evolution, genomics and chromosomes, and epigenomics and chromosomes. Of the 26 speakers in the symposium, ten speakers summarized their presentations and submitted their articles to this special issues. The followings are summaries of the articles. Tada et al. (2021) demonstrated the evidence for divergence of DNA methylation maintenance and a conserved inhibitory mechanism from DNA demethylation in chickens and mammals. The role of DNA methylation is wellknown in epigenetic gene regulation which is evolutionary conserved from single cell yeasts to mammals. The levels and patterns of DNA methylation are regulated by a balance of antagonistic enzyme functions, DNA methyltransferases DNMT1/3A/3B and methylcytosine dioxygenases TET1/2/3. In mice, TET enzyme mediates the conversion of cytosine methylation (5mC) to 5-hydroxymethylcytosine and initiation of global loss of 5mC at the beginning of fertilization until gastrulation stage. However, the 5mC level increased when the cell differentiations occur during early embryonic development. The authors checked the global loss and gain of DNA methylation whether different regulation exists in diverged species such as chicken and mammals. The results revealed that chicken and mammals shared a common chromatin-based regulation of TET-DNA success, but chicken DNMT1 was involved in different target sequence recognition systems, suggesting that factors inducing DNMT-DNA association had already diverged between egg laying bird species and placenta mammalian species. Epigenetic regulation is a fundamental mechanism in eukaryotic gene expression. Tam and Leung (2021) reviewed current states of single-cell transcriptomic and epigenomic studies in understanding the complex pathologies of chronic inflammatory diseases (CIDs). CIDs are a diverse class of autoimmune diseases characterized by dysregulated and sustained immune responses. Approximately 60% of Americans suffer from some form of CIDs and complications relating to these diseases are significant causes of death. However, exact triggers and mechanisms underlying many CIDs still remain elusive. The authors reviewed the current understanding of an association between epigenomic and transcriptomic dysregulation and the phenotypes of CIDs, specially discussed in depth on the epigenetic changes at cis-regulatory elements (CREs). Dysregulation of transposable elements (TEs) could serve as CREs or trigger molecular mimicry in CIDs. TEs constitute about 45% of human genome and a number of reports have described the aberrant expression of retrotransposons in CIDs. For CID characterization and diagnosis, the recent advancements of sequencing technologies (i.e., single-cell genomics) have generated many useful data in understanding the etiology of these complex diseases. The authors provided a most updated summary table Online ISSN 2092-9293 Print ISSN 1976-9571', 'corpus_id': 232048785, 'score': 0}, {'doc_id': '3446023', 'title': 'Complete overview of protein-inactivating sequence variations in 36 sequenced mouse inbred strains', 'abstract': 'Significance We have developed a bioinformatics tool that allows us to compare the sequences of all protein-coding genes of 36 sequenced mouse inbred strains with the reference mouse strain C57BL/6J. We also provide an estimate of the effect on protein function of each deviant protein sequence and have built a searchable database of all these sequences, giving researchers the opportunity to search for abnormal alleles of any protein coding gene across these strains. The database makes the enormous richness of variant alleles present in these 36 inbred strains visible, accessible, and useful to the whole mouse research community. Mouse inbred strains remain essential in science. We have analyzed the publicly available genome sequences of 36 popular inbred strains and provide lists for each strain of protein-coding genes that acquired sequence variations that cause premature STOP codons, loss of STOP codons and single nucleotide polymorphisms, and short in-frame insertions and deletions. Our data give an overview of predicted defective proteins, including predicted impact scores, of all these strains compared with the reference mouse genome of C57BL/6J. These data can also be retrieved via a searchable website (mousepost.be) and allow a global, better interpretation of genetic background effects and a source of naturally defective alleles in these 36 sequenced classical and high-priority mouse inbred strains.', 'corpus_id': 3446023, 'score': 1}, {'doc_id': '233202919', 'title': 'Discoveries and biological implications of mammalian 45S rDNA variants and non-structural rRNAs', 'abstract': 'Repetitive nature of the ribosomal DNA (rDNA) gene makes the sequencing of hundreds copies of mammalian 45S rDNA (about 45-kb per copy) extremely difficult and its assembly is often excluded. Increasing evidence shows that 45S rDNA variations (copy number or single nucleotide), structural ribosomal RNA (rRNA) transcript variants, and non-structural rRNA transcripts (sense and anti-sense long noncoding rRNAs that include promoter rRNAs, and rRNA-derived fragments) play essential roles in mammalian development and diseases. Complete pictures of the hundreds copies of 45S rDNA and their rRNA transcripts require further innovation in sequencing techniques that include bioinformatics. The advancements in mammalian rDNA and rRNA sequencings and the discoveries of novel functions of the rDNA variants and rRNA transcripts are', 'corpus_id': 233202919, 'score': 0}, {'doc_id': '232364190', 'title': 'Functional analysis of SARS-CoV-2 proteins in Drosophila identifies Orf6-induced pathogenic effects with Selinexor as an effective treatment', 'abstract': 'Background SARS-CoV-2 causes COVID-19 with a widely diverse disease profile that affects many different tissues. The mechanisms underlying its pathogenicity in host organisms remain unclear. Animal models for studying the pathogenicity of SARS-CoV-2 proteins are lacking. Methods Using bioinformatic analysis, we found that 90% of the virus-host interactions involve human proteins conserved in Drosophila . Therefore, we generated a series of transgenic fly lines for individual SARS-CoV-2 genes, and used the Gal4-UAS system to express these viral genes in Drosophila to study their pathogenicity. Results We found that the ubiquitous expression of Orf6, Nsp6 or Orf7a in Drosophila led to reduced viability and tissue defects, including reduced trachea branching as well as muscle deficits resulting in a “held-up” wing phenotype and poor climbing ability. Furthermore, muscles in these flies showed dramatically reduced mitochondria. Since Orf6 was found to interact with nucleopore proteins XPO1, we tested Selinexor, a drug that inhibits XPO1, and found that it could attenuate the Orf6-induced lethality and tissue-specific phenotypes\xa0observed in flies. Conclusions Our study established Drosophila as a model for studying the function of SARS-CoV2 genes, identified Orf6 as a highly pathogenic protein in various tissues, and demonstrated the potential of Selinexor for inhibiting Orf6 toxicity using an in vivo animal model system.', 'corpus_id': 232364190, 'score': 0}, {'doc_id': '205166643', 'title': 'Several classical mouse inbred strains, including DBA/2, NOD/Lt, FVB/N, and SJL/J, carry a putative loss-of-function allele of Gpr84.', 'abstract': 'G protein-coupled receptor 84 (GPR84) is a 7-transmembrane protein expressed on myeloid cells that can bind to medium-chain free fatty acids in vitro. Here, we report the discovery of a 2-bp frameshift deletion in the second exon of the Gpr84 gene in several classical mouse inbred strains. This deletion generates a premature stop codon predicted to result in a truncated protein lacking the transmembrane domains 4-7. We sequenced Gpr84 exon 2 from 58 strains representing different groups in the mouse family tree and found that 14 strains are homozygous for the deletion. Some of these strains are DBA/1J, DBA/2J, FVB/NJ, LG/J, MRL/MpJ, NOD/LtJ, and SJL/J. However, the deletion was not found in any of the wild-derived inbred strains analyzed. Haplotype analysis suggested that the deletion originates from a unique mutation event that occurred more than 100 years ago, preceding the development of the first inbred strain (DBA), from a Mus musculus domesticus source. As GPR84 ostensibly plays a role in the biology of myeloid cells, it could be relevant 1) to consider the existence of this Gpr84 nonsense mutation in several mouse strains when choosing a mouse model to study immune processes and 2) to consider reevaluating data obtained using such strains.', 'corpus_id': 205166643, 'score': 1}, {'doc_id': '18790816', 'title': 'Development of SNP markers for C57BL/6N-derived mouse inbred strains', 'abstract': 'C57BL/6N inbred mice are used as the genetic background for producing knockout mice in large-scale projects worldwide; however, the genetic divergence among C57BL/6N-derived substrains has not been verified. Here, we identified novel single nucleotide polymorphisms (SNPs) specific to the C57BL/6NJ strain and selected useful SNPs for the genetic monitoring of C57BL/6N-derived substrains. Informative SNPs were selected from the public SNP database at the Wellcome Trust Sanger Institute by comparing sequence data from C57BL/6NJ and C57BL/6J mice. A total of 1,361 candidate SNPs from the SNP database could distinguish the C57BL/6NJ strain from 12 other inbred strains. We confirmed 277 C57BL/6NJ-specific SNPs including 10 nonsynonymous SNPs by direct sequencing, and selected 100 useful SNPs that cover all of the chromosomes except Y. Genotyping of 11 C57BL/6N-derived substrains at these 100 SNP loci demonstrated genetic differences among the substrains. This information will be useful for accurate genetic monitoring of mouse strains with a C57BL/6N-derived background.', 'corpus_id': 18790816, 'score': 1}, {'doc_id': '15096396', 'title': 'Genetic influences on exercise‐induced adult hippocampal neurogenesis across 12 divergent mouse strains', 'abstract': 'New neurons are continuously born in the hippocampus of several mammalian species throughout adulthood. Adult neurogenesis represents a natural model for understanding how to grow and incorporate new nerve cells into preexisting circuits in the brain. Finding molecules or biological pathways that increase neurogenesis has broad potential for regenerative medicine. One strategy is to identify mouse strains that display large vs. small increases in neurogenesis in response to wheel running so that the strains can be contrasted to find common genes or biological pathways associated with enhanced neuron formation. Therefore, mice from 12 different isogenic strains were housed with or without running wheels for 43 days to measure the genetic regulation of exercise‐induced neurogenesis. During the first 10 days mice received daily injections of 5‐bromo‐2′‐deoxyuridine (BrdU) to label dividing cells. Neurogenesis was measured as the total number of BrdU cells co‐expressing NeuN mature neuronal marker in the hippocampal granule cell layer by immunohistochemistry. Exercise increased neurogenesis in all strains, but the magnitude significantly depended on genotype. Strain means for distance run on wheels, but not distance traveled in cages without wheels, were significantly correlated with strain mean level of neurogenesis. Furthermore, certain strains displayed greater neurogenesis than others for a fixed level of running. Strain means for neurogenesis under sedentary conditions were not correlated with neurogenesis under runner conditions suggesting that different genes influence baseline vs. exercise‐induced neurogenesis. Genetic contributions to exercise‐induced hippocampal neurogenesis suggest that it may be possible to identify genes and pathways associated with enhanced neuroplastic responses to exercise.', 'corpus_id': 15096396, 'score': 1}, {'doc_id': '233453612', 'title': 'Identification and survival analysis of tTA/tetO-CCKR-2 double transgenic mice', 'abstract': 'BACKGROUND: The inducible forebrain-specific cholecystokinin receptor-2 (CCKR-2) double transgenic (tTA/tetO-CCKR-2 tg, abbreviated as dtg) mice are an ideal model of anxiety-related diseases. However, there is still a lack of model identification and life related data OBJECTIVE: To identify the genomic DNA of the offspring and the specific expression of CCKR-2 transgene in the forebrain, and to analyze the survival probability of dtg mice. METHODS: α-CaMKII/tTA single transgenic mice and tetO-CCKR-2 single transgenic mice were cross-fertilized to construct a dtg mouse model. The genomic DNA was extracted from the tail of the offspring, and the genotypes were detected by PCR and agarose gel electrophoresis. Wild-type (WT) mice were used as controls. In situ hybridization was used to detect the expression of CCKR-2. Survival of dtg mice and WT mice (30 females and 30 males) was observed and recorded within 2 years. The study protocol was approved by the Experimental Animal Ethics Committee of Southwest Medical University, with an approval No. 20150068. RESULTS AND CONCLUSION: Agarose gel electrophoresis results showed the molecular weight of the PCR products of dtg mice was consistent with the expected target gene fragment. In situ hybridization results showed a strong signal of CCKR-2 was detected in the forebrain of dtg mice, but hardly present in the WT mice. The median survival time of dtg mice was 76 weeks in females and 77 weeks in males. The survival probability was decreased with age in dtg mice. The survival probability of WT mice was significantly better than that of dtg mice (P < 0.001). There was no significant sex difference between males and females of dtg mice (P=0.577). Therefore, the specific expression of CCKR-2 transgene in the forebrain can be identified using PCR amplification, genomic DNA extraction, agarose gel electrophoresis, and in situ hybridization. tTA/tetO-CCKR-2 double transgenic induction may shorten the survival time of mice, but no significant difference is observed between the females and males of dtg mice.', 'corpus_id': 233453612, 'score': 0}]
113	Image classification driven by knowledge graph	1c30f7b7801fe4172c6874627c1f797e	9123	{}	[{'doc_id': '63537951', 'title': 'Text classification using a hidden markov model', 'abstract': 'Text categorization (TC) is the task of automatically categorizing textual digital documents into pre-set categories by analyzing their contents. The purpose of this study is to develop an effective TC model to resolve the difficulty of automatic classification. In this study, two primary goals are intended. First, a Hidden Markov Model (HAM is proposed as a relatively new method for text categorization. HMM has been applied to a wide range of applications in text processing such as text segmentation and event tracking, information retrieval, and information extraction. Few, however, have applied HMM to TC. Second, the Library of Congress Classification (LCC) is adopted as a classification scheme for the HMM-based TC model for categorizing digital documents. LCC has been used only in a handful of experiments for the purpose of automatic classification. In the proposed framework, a general prototype for an HMM-based TC model is designed, and an experimental model based on the prototype is implemented so as to categorize digitalized documents into LCC. A sample of abstracts from the ProQuest Digital Dissertations database is used for the test-base. Dissertation abstracts, which are pre-classified by professional librarians, form an ideal test-base for evaluating the proposed model of automatic TC. For comparative purposes, a Naive Bayesian model, which has been extensively used in TC applications, is also implemented. Our experimental results show that the performance of our model surpasses that of the Naive Bayesian model as measured by comparing the automatic classification of abstracts to the manual classification performed by professionals.', 'corpus_id': 63537951, 'score': 0}, {'doc_id': '222278341', 'title': 'AdaHGNN: Adaptive Hypergraph Neural Networks for Multi-Label Image Classification', 'abstract': 'Multi-label image classification is an important and challenging task in computer vision and multimedia fields. Most of the recent works only capture the pair-wise dependencies among multiple labels through statistical co-occurrence information, which cannot model the high-order semantic relations automatically. In this paper, we propose a high-order semantic learning model based on adaptive hypergraph neural networks (AdaHGNN) to boost multi-label classification performance. Firstly, an adaptive hypergraph is constructed by using label embeddings automatically. Secondly, image features are decoupled into feature vectors corresponding to each label, and hypergraph neural networks (HGNN) are employed to correlate these vectors and explore the high-order semantic interactions. In addition, multi-scale learning is used to reduce sensitivity to object size inconsistencies. Experiments are conducted on four benchmarks: MS-COCO, NUS-WIDE, Visual Genome, and Pascal VOC 2007, which cover large, medium, and small-scale categories. State-of-the-art performances are achieved on three of them. Results and analysis demonstrate that the proposed method has the ability to capture high-order semantic dependencies.', 'corpus_id': 222278341, 'score': 1}, {'doc_id': '2021646', 'title': 'The More You Know: Using Knowledge Graphs for Image Classification', 'abstract': 'One characteristic that sets humans apart from modern learning-based computer vision algorithms is the ability to acquire knowledge about the world and use that knowledge to reason about the visual world. Humans can learn about the characteristics of objects and the relationships that occur between them to learn a large variety of visual concepts, often with few examples. This paper investigates the use of structured prior knowledge in the form of knowledge graphs and shows that using this knowledge improves performance on image classification. We build on recent work on end-to-end learning on graphs, introducing the Graph Search Neural Network as a way of efficiently incorporating large knowledge graphs into a vision classification pipeline. We show in a number of experiments that our method outperforms standard neural network baselines for multi-label classification.', 'corpus_id': 2021646, 'score': 1}, {'doc_id': '209386858', 'title': 'Cross-Modality Attention with Semantic Graph Embedding for Multi-Label Classification', 'abstract': 'Multi-label image and video classification are fundamental yet challenging tasks in computer vision. The main challenges lie in capturing spatial or temporal dependencies between labels and discovering the locations of discriminative features for each class. In order to overcome these challenges, we propose to use cross-modality attention with semantic graph embedding for multi label classification. Based on the constructed label graph, we propose an adjacency-based similarity graph embedding method to learn semantic label embeddings, which explicitly exploit label relationships. Then our novel cross-modality attention maps are generated with the guidance of learned label embeddings. Experiments on two multi-label image classification datasets (MS-COCO and NUS-WIDE) show our method outperforms other existing state-of-the-arts. In addition, we validate our method on a large multi-label video classification dataset (YouTube-8M Segments) and the evaluation results demonstrate the generalization capability of our method.', 'corpus_id': 209386858, 'score': 1}, {'doc_id': '126883934', 'title': 'Сроки предпосевной обработки почвы под ранние яровые культуры', 'abstract': 'During the three-year period of studies it has been established that the highest possible yields of vetch-barley mixture can be obtained as result of the earliest presowing tillage of clayed loam soil at the period of its physical mellowness at the depth of 5-6 cm without preliminary harrowing of the ploughed field in early spring.', 'corpus_id': 126883934, 'score': 0}, {'doc_id': '199510834', 'title': 'Image Classification Based on Image Knowledge Graph and Semantics', 'abstract': 'Since the ImageNet competition was held in 2012, the machine learning algorithm has performed very well on the image classification task. In object classification task, there are still some problems. For example, similar categories are difficult to be distinguished in images. In addition, with the increasing number of object categories, the background of scene has become another crucial issue in object classification. This paper focuses on image object recognition and makes two major contributions to tackle these issues. Firstly, we propose a semantic refinement method that analyzes the relationship among similar categories in images from the perspective of semantic knowledge. The knowledge of the relationship between semantics comes from a wide range of open knowledge. Secondly, we utilize the knowledge graph method to create the image knowledge graph with multiple categories in images. Our method exploits the knowledge from the adjacency matrix computed on train data to merge relevant classes into graph. We conduct extensive experiments on large-scale image datasets (ImageNet), demonstrating the effectiveness of our approach. Further, our method participates in ILSVRC 2012 challenges, and obtain the new state-of-the-art results on the ImageNet (82.43%).', 'corpus_id': 199510834, 'score': 1}, {'doc_id': '221655766', 'title': 'GINet: Graph Interaction Network for Scene Parsing', 'abstract': 'Recently, context reasoning using image regions beyond local convolution has shown great potential for scene parsing. In this work, we explore how to incorporate the linguistic knowledge to promote context reasoning over image regions by proposing a Graph Interaction unit (GI unit) and a Semantic Context Loss (SC-loss). The GI unit is capable of enhancing feature representations of convolution networks over high-level semantics and learning the semantic coherency adaptively to each sample. Specifically, the dataset-based linguistic knowledge is first incorporated in the GI unit to promote context reasoning over the visual graph, then the evolved representations of the visual graph are mapped to each local representation to enhance the discriminated capability for scene parsing. GI unit is further improved by the SC-loss to enhance the semantic representations over the exemplar-based semantic graph. We perform full ablation studies to demonstrate the effectiveness of each component in our approach. Particularly, the proposed GINet outperforms the state-of-the-art approaches on the popular benchmarks, including Pascal-Context and COCO Stuff.', 'corpus_id': 221655766, 'score': 0}, {'doc_id': '221971200', 'title': 'Human-Object Interaction Detection: A Quick Survey and Examination of Methods', 'abstract': 'Human-object interaction detection is a relatively new task in the world of computer vision and visual semantic information extraction. With the goal of machines identifying interactions that humans perform on objects, there are many real-world use cases for the research in this field. To our knowledge, this is the first general survey of the state-of-the-art and milestone works in this field. We provide a basic survey of the developments in the field of human-object interaction detection. Many works in this field use multi-stream convolutional neural network architectures, which combine features from multiple sources in the input image. Most commonly these are the humans and objects in question, as well as the spatial quality of the two. As far as we are aware, there have not been in-depth studies performed that look into the performance of each component individually. In order to provide insight to future researchers, we perform an individualized-study that examines the performance of each component of a multi-stream convolutional neural network architecture for human-object interaction detection. Specifically, we examine the HORCNN architecture as it is a foundational work in the field. In addition, we provide an in-depth look at the HICO-DET dataset, a popular benchmark in the field of human-object interaction detection.', 'corpus_id': 221971200, 'score': 0}, {'doc_id': '149923229', 'title': 'Knowledge Graph-Based Image Classification Refinement', 'abstract': 'Biologically inspired ideas are important in image processing. Not only does more than 80% of the information received by humans comes from the visual system, but the human visual system also gives its fast, accurate, and efficient image processing capability. In the current image classification task, convolutional neural networks (CNNs) focus on processing pixels and often ignore the semantic relationships and human brain mechanisms. With the development of image analysis and processing techniques, the information in the image is becoming increasingly complicated. Humans can learn about the characteristics of objects and the relationships that occur between them to classify the images. It is a significant characteristic that sets humans apart from the modern learning-based computer vision algorithms. How to make full use of the semantic relationships in categories and how to apply the knowledge of biological vision to image classification are our main concerns. In this view, we propose the concept of the image knowledge graph (IKG) to incorporate the semantic association and the scene association to fully consider the relations between objects (external and internal). We take full advantage of the reasoning model of the knowledge graph that is closer to the biological visual information-processing model. We conduct extensive experiments on large-scale image datasets (ImageNet), demonstrating the effectiveness of our approach. Furthermore, our method participates in ILSVRC 2017 challenges and obtains the new state-of-the-art results on the ImageNet (82.43%).', 'corpus_id': 149923229, 'score': 1}, {'doc_id': '221319594', 'title': 'Weakly Supervised Learning with Side Information for Noisy Labeled Images', 'abstract': 'In many real-world datasets, like WebVision, the performance of DNN based classifier is often limited by the noisy labeled data. To tackle this problem, some image related side information, such as captions and tags, often reveal underlying relationships across images. In this paper, we present an efficient weakly supervised learning by using a Side Information Network (SINet), which aims to effectively carry out a large scale classification with severely noisy labels. The proposed SINet consists of a visual prototype module and a noise weighting module. The visual prototype module is designed to generate a compact representation for each category by introducing the side information. The noise weighting module aims to estimate the correctness of each noisy image and produce a confidence score for image ranking during the training procedure. The propsed SINet can largely alleviate the negative impact of noisy image labels, and is beneficial to train a high performance CNN based classifier. Besides, we released a fine-grained product dataset called AliProducts, which contains more than 2.5 million noisy web images crawled from the internet by using queries generated from 50,000 fine-grained semantic classes. Extensive experiments on several popular benchmarks (i.e. Webvision, ImageNet and Clothing-1M) and our proposed AliProducts achieve state-of-the-art performance. The SINet has won the first place in the classification task on WebVision Challenge 2019, and outperformed other competitors by a large margin.', 'corpus_id': 221319594, 'score': 0}]
114	Arboreal mites	d6b4855ff5bb479a955501752d0d798f	12931	{}	[{'doc_id': '230120573', 'title': 'TRADITIONAL KNOWLEDGE ON ETHNOMEDICINAL PLANTS AMONG LOCAL PEOPLE OF SARAIN RANGE, CHOPAL FOREST DIVISION, HIMACHAL PRADESH, INDIA', 'abstract': 'The present study was carried out to know the traditional knowledge on ethnomedicinal plants used by people of Sarain range, Chopal forest division, Himachal Pradesh. Information on these plants was collected from 115 people including traditional healers of twenty different villages through semi-structured interview during year 2017. A total of 53 ethnomedicinal plants recorded from the region belong to 41 families and are used to treat various diseases. Of these, 09 were trees, 10 shrubs, 31 herbs, 1 fern and 2 climber species. The family Lamiaceae had maximum 4 while Asteraceae and Pinaceae had 3 ethnomedicinal species each. Various plant parts such as leaves, bark, roots, flowers, etc., were used for medicinal purposes. Of these the most used part for medicinal purpose was root (34%) followed by leaves (17%). It was observed that 54% people interviewed did possess knowledge about the ethnomedicinal plants, clearly reflects the increasing trend in knowledge about the medicinal plants. Out of 115 people interviewed, 39% people were having educational qualification of matriculation while 18% of were illiterate. During the present study each of 53 medicinal plants were described in detail with their botanical name, local name, family, habits, parts used, habitat, mode of consumption and ethnobotanical usage. Documentation of the traditional knowledge is very important in the present prevailing situation as it will help in preservation of knowledge and also opportunity for remedial measures for conservation and sustainable harvesting of those species for additional income generation of local communities in perpetuity.', 'corpus_id': 230120573, 'score': 0}, {'doc_id': '19556381', 'title': 'Oribatid Mite Communities in the Canopy of Montane Abies amabilis and Tsuga heterophylla Trees on Vancouver Island, British Columbia', 'abstract': 'Abstract To study the oribatid mite community inhabiting microhabitats in the canopy of montane Abies amabilis [(Douglas ex D. Don) Lindl.] and Tsuga heterophylla [(Raf.) Sarg] tree species across five elevational sites, we collected 180 branch tips and 180 foliose/crustose lichen samples over three time periods. Thirty-three species of oribatid mites were identified from the study area. Mite species richness and abundance was significantly affected by microhabitat, and this association was independent of sampling time. At the microhabitat scale, distinct species assemblages were associated with lichen and branch tip habitats, and to a lesser degree, tree species. Conifer specificity was most apparent in the closely related species of Jugatala, where Jugatala tuberosa Ewing was only found on branch tips from A. amabilis and Jugatala sp. was primarily found on branch tips from T. heterophylla. Microhabitat specificity was most pronounced in Dendrozetes sp. where most individuals were found on branch tips and Anachiperia geminus Lindo et al. that occurred primarily on lichens. Principal components analysis of oribatid mite community composition further showed a high degree of association with microhabitat and tree species. Habitat profiles are difficult to discern for many species because tree, microhabitat, and elevation preferences confound distribution patterns. Given the significant tree-microhabitat associations in species composition in this montane canopy study, we suggest that sampling multiple microhabitats across elevations to look for patterns in community structure offers opportunities to explicitly test organizing principles in community ecology.', 'corpus_id': 19556381, 'score': 1}, {'doc_id': '198190698', 'title': 'Microhabitat distribution of arboreal oribatid mites (Oribatida), associated with the Siberian pine (Pinus sibirica) of Western Siberia', 'abstract': 'The species composition of arboreal oribatid mites that live on Siberian pine trees (Pinus sibirica) in the forest-tundra of Western Siberia was examined, specifically of three Siberian pines from two distinct forest stands (six trees in total). Samples of litter were taken near the tree trunk, as well as samples of bark, branches and needles from the tree. In total 144 samples were taken, from which close to 5000 mites were extracted. From the arboreal samples, the mites were extract by heptane flotation. Three species of oribatid mites were recorded for the first time in Russia: Diapterobates brevidentatus, Eueremaeus trionus and Cultroribula berolina. The highest density and the highest dominance index of these species were recorded in arboreal microhabitats. Thirty-one species of oribatid mites were identified in total. No oribatid mites were recovered from the needles of Siberian pine. The density of oribatid mites did not significantly differ among various heights of the crown and trunk. The lowest density of mites was recorded on young branches without needles, whereas other branch hypothetical microhabitats did not significantly differ from each other in terms of oribatid density. The greatest Simpson diversity index was recorded in the plant litter near tree trunks. On trees, the diversity index decreased with the height of the trunk and with the distance of branch sections from the trunk. The dominant species and the degree of their dominance varied among microhabitats and forest stands. Additionally, a high level of dominance of a single oribatid species was observed on tree branches, as well as on the trunk bark located above the bole. In the two forest stands, these species were D. brevidentatus and Ameronothrus dubinini. Based on the analysis of oribatid communities, three microhabitats were identified in the first forest stand: the bole bark, the bark of the trunk above the bole, and the branches. In the second forest stand, bases of tree branches were identified as an additional microhabitat. Oribatids inhabit Siberian pine trees in the severe conditions of the forest-tundra. Arboreal oribatid communities of various microhabitats vary in their qualitative and quantitative characteristics. Also, arboreal and forest litter communities of oribatid mites vary significantly.', 'corpus_id': 198190698, 'score': 1}, {'doc_id': '55575988', 'title': 'Oribatid mites (Acari: Oribatida) in the cano- py of a Central European mixed forest: Spe- cies richness and species similarity between tree species and habitat types', 'abstract': 'Oribatid mites are one of the most abundant and species-rich groups in forest � oor and canopy habitats. Low species overlap between canopy and ground assemblages make oribatid mites an ideal taxon with which to characterise canopy habitats. Habitat structure is a powerful driver of diversity for oribatid mite assemblages in soil. In the canopy, differences in tree architecture and habitat types may be expected to affect species richness and/or composition of associated faunas. We investigated assemblage patterns of oribatid mites in the canopy of a temperate Central European mixed forest using the Swiss Canopy Crane in Basel, Switzerland. We examined four tree species (oak, beech, larch, spruce) 20m above-ground and higher. Within the trees, we sampled three habitat types (outermost branch tips, thin branches and thick branches close to the trunk) by high-pressure water-rinsing. The canopy contained a typical assemblage of 18 arboreal oribatid mite species. This was less than in comparable studies of temperate regions; presumably the lack of suspended soils and epiphyte mats in the forest stand examined was related to low species richness. Observed and estimated', 'corpus_id': 55575988, 'score': 1}, {'doc_id': '85139501', 'title': 'Canopy Oribatida: Tree specific or microhabitat specific?', 'abstract': 'A diverse assemblage of oribatid mites inhabits the canopy of coniferous trees in western North America. We tested the hypothesis that oribatid mites are microhabitat specific in old-growth Douglas fir, Western hemlock and western redcedar at the Wind River Crane Canopy Research Facility, Washington, USA. The upper 3 m of canopy of the three tree species were accessed using the canopy crane. Oribatida were extracted from 4 to 12 g dwt samples of alecterioid and foliose lichens using the twig-washing technique. Overall species richness was low, 16 species representing 11 families, with no species unique to this site. Species were absent from samples taken contemporaneously from the forest floor. All oribatid species were found in foliose lichens, whereas only nine species, in seven families, were recovered from alecterioid lichens. Oribatid species richness was lichen specific depending on the tree species. On Western hemlock both lichens supported similarly rich communities, but on Douglas fir and western redcedar foliose lichens supported the richer community.', 'corpus_id': 85139501, 'score': 1}, {'doc_id': '226229171', 'title': 'Deinbollia onanae (Sapindaceae), a new, Endangered, montane tree species from the Cameroon Highlands', 'abstract': 'Deinbollia onanae (Sapindaceae-Litchi clade) is here formally named and characterised as a new species to science, previously known as Deinbollia sp. 2. Cameroon has the highest species-diversity and species endemism known in this African-Western Indian Ocean genus of 42 species. Deinbollia onanae is an infrequent tree species known from five locations in surviving islands of montane (or upper submontane) forest along the line of the Cameroon Highlands. It is here assessed as Endangered according to the IUCN 2012 standard, threatened mainly by clearance of forest for agriculture. The majority of tree species characteristic of montane forest (above 2000 m alt.) in the Cameroon Highlands are also widespread in East African mountains (i.e. are Afromontane). Deinbollia onanae is one of only a very small number of species that are endemic (globally restricted to) the mountain range. It is postulated that this new species is in a sister relationship with Deinbollia oreophila, which is a frequent species of a lower (submontane) altitudinal band of the same range. It is further postulated that seed dispersal is or was by frugivorous birds, potentially turacos, alternatively by primates such as Preuss s monkey.', 'corpus_id': 226229171, 'score': 0}, {'doc_id': '227162776', 'title': 'Marvellous Muscodor spp.: Update on Their Biology and Applications', 'abstract': 'Nearly 20 years ago, the first report appeared on the discovery of a novel genus—Muscodor. This organism was isolated as an endophyte from a cinnamon tree that had been introduced to Honduras from Sri Lanka in the early part of the last century. Characteristically, the original Muscodor albus, and all of its species isolated since that time are non-spore producers and each one exudes a characteristic spectrum of volatile bioactive compounds. The majority have a whitish mycelium, which is sometimes coiling, intertwined and decorated with variously shaped structures. Presently, there are at least 22 type species known/documented and each has been described as an endophyte from various plant families with widely varying habitats. An enormous variety of volatile organic compounds (VOCs) are produced by Muscodor spp. and some of these include esters, acids, aldehydes, ketones, aromatics, alkanes, alcohols, nitrosamides and terpenoids. The VOCs are both inhibitory and lethal to a wide variety of fungi and bacteria including some major pathogens of plants and humans. Interestingly, in almost all cases studied, no one compound by itself can mimic the bioactivity of the complete gas mixture, suggesting that the volatiles are acting in a synergistic manner and this has been tested with individual as well as the VOCs in various mixtures and concentrations. This review will discuss some of the recent findings in all aspects of this unique fungal genus whilst at the same time pointing out some of the major questions that remain about its biology, ecology and its applications in agriculture, medicine and other sectors. Most importantly, the authors provide arguments supporting the claim that Muscodor is taxonomically distinct from Induratia, a recently proposed change to its nomenclature.', 'corpus_id': 227162776, 'score': 0}, {'doc_id': '226300147', 'title': 'Mycorrhizal association of common European tree species shapes biomass and growth of bacterial and fungal communities in soil', 'abstract': 'Recent studies have revealed effects of various tree species on soil physical and chemical properties. However, effects of various tree species on composition and activity of soil microbiota and the relevant controls remain poorly understood. We evaluated the influence of tree species associated with two different mycorhizal types, ectomycorrhiza (EcM) and arbuscular mycorrhiza (AM), on growth, biomass and metabolic activity of soil fungal and bacterial communities using common garden tree species experimens throughout Denmark. The soil microbial communities differed between six European tree species as well as between EcM (beech, lime, oak and spruce) and AM (ash and maple) tree species. The EcM tree species had higher fungal biomass, fungal growth and bacterial biomass, while AM species showed higher total microbial biomass and bacterial growth. The results indicated that microbial community composition and functioning differed between groups of tree species with distinct litter qualities that mediate soil C/N ratio and soil pH. The grouping of tree species partly coincided with their mycorrhizal association since lime was more similar to AM tree species. In addition, our results indicated that tree species-mediated soil pH and C/N ratio were the most important variables shaping microbial communities with a positive effect on bacterial and a negative effect on fungal growth rates. In light of previous studies of soil C stocks, the results support that tree species mediated microbial and particularly bacterial processing may be an important driver of mineral soil C stocks and forms in AM vs. EcM species.', 'corpus_id': 226300147, 'score': 0}, {'doc_id': '229338075', 'title': 'Ectoparasites and Pathogens of Kuhl’s Pipistrelle Pipistrellus kuhlii (Kuhl, 1817) (Chiroptera: Vespertilionidae): Our Own and Published Data Review', 'abstract': 'Here we report the results of our own survey and literary published data on the ectoparasite fauna and pathogens of the alien bat species, the Kuhl’s pipistrelle Pipistrellus kuhlii (Kuhl, 1817) (Chiroptera: Vespertilionidae). This bat is a host of 36 species of parasitic mites, ticks and insects (including accidental findings) and 13 species of pathogens (protozoa, bacteria, viruses). The flea Ischnopsyllus variabilis is recorded on this host for the first time. We have found that outside of the host ancestral range, the core of the bat parasite fauna is significantly different due to the loss of host species-specific ectoparasites. Particularly, in Russia, only 6 species of parasitic arthropods have been recorded for Kuhl’s pipistrelle and all of them are host genus-specific. At the same time, the features of ecology and occasional finds of extrinsic parasites allow to suggest that P. kuhlii has wide contacts with animals which are the reservoirs of zoonotic infections, that in combination with the fact of isolation of several pathogens from this species (including two coronaviruses) points to a possible medical importance of Kuhl’s pipistrelle.', 'corpus_id': 229338075, 'score': 0}, {'doc_id': '84654135', 'title': 'A comparison of microarthropod assemblages with emphasis on oribatid mites in canopy suspended soils and forest floors associated with ancient western redcedar trees', 'abstract': 'Summary Microarthropod abundance, oribatid mite species richness and community composition were assessed in the high canopy (ca. 35\xa0m) of an ancient temperate rainforest and compared with microarthropod communities of the forest floor. Microarthropods were extracted from 72 core samples of suspended soils and 72 core samples from forest floors associated with six western redcedar trees in the Walbran Valley on the southwest coast of Vancouver Island, Canada. Total microarthropod abundances, mesostigmatid and astigmatid mites, Collembola and other microarthropod abundances were significantly greater in forest floors compared to canopy habitats. Oribatid and prostigmatid mite abundance were not significantly different between habitats. The relative abundances of all microarthropod groups considered in this study differed significantly between habitats. Eighty-eight species of oribatid mites were identified from the study area. Eighteen of the 53 species observed in suspended soils were unique to the canopy. Cluster analysis indicates that the arboreal oribatid mite community is distinct and not a taxonomic subset of the forest floor assemblage, however, canopy oribatid mite communities are more heterogeneous in species composition than in the forest floor.', 'corpus_id': 84654135, 'score': 1}]
115	Priestesses Dissertation	1941c14ab1936bf54ad8c4cbb3e87f5c	15857	{}	[{'doc_id': '233282077', 'title': 'Discovering Neverland: São Tomé and Príncipe and the development of the agricultural heritage of a multi-ethnic population', 'abstract': 'The history of São Tomé and Príncipe (STP) shows that the development of the roças, plantations established in colonial times, form a heritage linked to the human development of STP. Various agricultural products have characterized the historical periods of migration, slavery, creolization, and gender emancipation up to the present day; agricultural products and the history of creolization make STP unique, while the relationship between culture and nature provides a useful tool for a better understanding of its historical roots. The essay argues that STP’s sustainable development could be fostered by valorizing its historical agricultural heritage. Agrifood geographical indications (GIs), which directly link territories, peoples, and traditions could also serve this purpose. GIs could lead to raising the export price of STP’s cocoa, coffee, and pepper, at the same time increasing cultivation of a number of other crops, especially indigenous fruits, which are usually planted in combination. These systems have proven to lead to better prices for products and increase the specialist labour market; they could also foster a multi-faceted approach to territorial development, including eco-tourism. However, challenges remain, as the country is still lacking in proper infrastructures, skilled labour, management, and institutional support. The question will be whether STP’s fragile agri-food setting is able to support these new mechanisms, which require strong value chains, respect for territorial biodiversity, and a fresh look at the role played by small farmers running AgriSMEs.', 'corpus_id': 233282077, 'score': 0}, {'doc_id': '189594602', 'title': 'The Vestal Virgins', 'abstract': None, 'corpus_id': 189594602, 'score': 1}, {'doc_id': '232223576', 'title': 'ASPECTS OF STORAGE AT ROME', 'abstract': 'imperial freedmen and the increase in their wealth, they became a new target for the senatorial elite during the Principate. Chapter 7, by C.J. Berry, inspects the integration of the ideal of frugalitas into Christian thought and its repercussions in post-Classical times, in the cases of David Hume and Adam Smith’s recalibrations of frugality, which focus on the economic benefits of frugal resource management. This book makes a distinctive contribution and is an original and engaging volume employing multidisciplinary evidence related to Roman frugality and modes of moderation. Thus, it is greatly welcomed and much recommended. It is not the aim and scope of this book to provide an account of frugal ideology from late antiquity onwards (p. 107). However, there is a future wish. The last decade has yielded immense material evidence (pottery, glass, coin finds) regarding the late antique economy. This forces us to re-think established concepts such as resilience, transformation of the empire and so on. One decline, however, was recorded in the epigraphic habit from the fourth century AD onwards, especially in funerary inscriptions. Could we simply relate this to the general economic change affecting the use of marble? What was the impact of Christian thought and ideology on funerary customs? Modesty, simplicity? More concern about the afterlife than about the mundane world? Thus, further study on Christianity and its impact (if there is one) on consumption culture and economic behaviour in general are suggested (for some preliminary indications, see pp. 101–7).', 'corpus_id': 232223576, 'score': 1}, {'doc_id': '165994848', 'title': 'Images of Vestal Virgins, and questions about interpreting the evidence', 'abstract': 'The virgin priestesses of Vesta (virgines Vestales) have long fascinated classicists and the general public alike. That an empire as mighty and long-lasting as Rome’s symbolically based its survival on a group of women and their bodies’ integrity required an explanation. Given the utmost religious and political importance of both cult and priestesses over a span of more than a thousand years, it is striking how fragmentary and scattered a historical record they have left behind. It is equally remarkable that attempts at a comprehensive study that includes archaeological and visual material have been published only in the present millennium, M. Lindner’s book being the most recent of them. Since it is based on her University of Michigan dissertation of 1996, the author should be credited for being among the first to have drawn attention to a long-neglected body of material; besides the architecture of the atrium Vestae, it includes various sculptural remains found therein (most importantly, portraits of the Vestals), additional archival sources (she obtained access to unpublished reports from R. Lanciani’s excavations in the late 19th c.), and other depictions on reliefs and coins. In the 19 years since the dissertation’s completion scholarship on the Vestals has flourished. Sophisticated and theoretically-informed historical and philological studies on the Vestals’ sexual and political status have appeared.1 The epigraphic and archaeological evidence has also been systematically studied, preparing material that Lindner did not consider at the time but which she could draw on here.2 However, Lindner regrettably misses the opportunity to reconsider some of her arguments, despite serious methodological objections that had already been voiced.3 In the following, I will give a short overview of the book’s content before addressing some of the major concerns.', 'corpus_id': 165994848, 'score': 1}, {'doc_id': '232264267', 'title': 'Follow the bodies: Global capitalism, global war, global crisis and feminist IPE', 'abstract': 'Global Capitalism, Global War, Global Crisis by Andreas Bieler and Adam Morton makes a persuasive case for the enduring relevance of historical materialism as the hermeneutical tool for analysis and understanding of contemporary political economy.1 The most interesting aspect of the book – and the one which sets it apart from other recent critical works in International Political Economy (IPE) in the same tradition – is the concept of ‘internal relations’, which allows Bieler and Morton to endogenise wars and crises within the capitalism itself.2 Arguing against the habitual dualisms in the discipline of International Relations – between states and markets, agents and structures, material conditions and ideologies – Bieler and Morton suggest instead that global capitalism, war and crisis should be analysed ‘in terms of their internality’.3 Their ‘relational method’ thus ‘captures capital’s internalisation through the states system of uneven and combined development, geopolitics and the global crisis conditions facing humanity that are themselves embedded within world ecology’.4 In this intervention, I would like to elaborate upon this ‘radical ontology’ of ‘internal relations’ from the perspective of feminist IPE. There are significant overlaps between feminist IPE and Bieler and Morton’s analysis in this book, none the least their effort to integrate what Nancy Fraser aptly calls ‘Marx’s hidden abode’ – the background conditions of expropriation and social reproduction – with Marx’s ‘front story’ of exploitation and capitalist production.5 However, while Bieler and Morton noticeably expand Marx’s framework to incorporate and relate aspects of the Gramscian ‘social factory’ within the world of class struggle and geopolitical contestations, feminists begin their analyses by looking at the world through the lens of gendered hierarchies and embodied (rather than', 'corpus_id': 232264267, 'score': 0}, {'doc_id': '233271202', 'title': 'Editorial: Understanding the Conundrum Secular States, Fundamentalist Politics', 'abstract': 'The theme of this special issue of Feminist Dissent focuses on the ways in which religious fundamentalist movements have become hegemonic in many secular states around the world. This purported paradox of fundamentalist politics gaining power in secular states is all the more challenging to analyse in the context of both the consolidation and re-articulation of neoliberalism as an ideology and framework for organising economy and society in the era of late capitalism and its successive crises. Specifically, we are interested in exploring the ways in which these transformations within state, society and the economy have affected women’s positions and gender relations. The illustrative case studies we examine in this issue are India, Israel and Turkey.', 'corpus_id': 233271202, 'score': 0}, {'doc_id': '232115721', 'title': 'Karl Marx and the Birth of Modern Society: The Life of Marx and the Development of His Work', 'abstract': 'Michael Heinrich’s five-volume opus may be the most definitive and comprehensive biography of Marx ever written, and it may turn out to be the best ever written. It could have been subtitled ‘‘everything you wanted to know about Marx and didn’t have years to research.’’ Heinrich’s biography reveals things about the life of Marx that others don’t. It is a bit difficult for a sociologist to summarize the text, given the differences between a historical biography, rich in minute details, and a sociological perspective, more focused on larger-scale events or processes. Such a biography is based on extensive research of various archives, letters, and events, as well as critiques of other, earlier biographies. Moreover, the intellectual context of Marx’s embrace of philosophy, from the pre-Socratics of his dissertation to German Idealism, especially Hegel, Bauer, and the like, and the many writings and fragments in the Marx-Engels-Gesamtausgabe (MEGA) may not be too familiar to many. But given current conditions and the growing interest in Marx, Heinrich’s Karl Marx and the Birth of Modern Society: The Life of Marx and the Development of His Work— showing the importance of a radical critique of everything and informed by a fervent embrace of freedom—is essential reading. ‘‘This book is not concerned with a cult of personality . . . but the historical process in which Karl Marx developed as a person, as a theorist, as a political activist, and as a revolutionary’’ and will consider his ‘‘school days, his attempts at poetry, his engagement with religion and the philosophy of religion’’ (pp. 9–10). Heinrich informs the reader of the historical and political-economic contexts of Trier, where young Marx grew up; Prussia, both from within and without; the impact of Lutheranism; and the tensions between the legacies of dynastic tradition and forces of modernity, from Rousseau’s Discourse on Inequality to Kant on ‘‘Enlightenment.’’ And then came the rise of Napoleon, trade with France, wars with Prussia, and the spread of rational, bourgeois law, typically opposed by monarchs. Along with the events of 1776, 1789, and 1848 and the beginnings of capitalist industrialization, Marx was the product of emerging capitalist modernity as well as its witness and, eventually, its most trenchant critic. For the sake of clarity, if not brevity, I would suggest that we look at five intertwined themes of Marx’s life.', 'corpus_id': 232115721, 'score': 0}, {'doc_id': '217063024', 'title': 'The Veiled Exploitation of the Vestal Virgins', 'abstract': None, 'corpus_id': 217063024, 'score': 1}, {'doc_id': '232281755', 'title': 'Book Review: Kees Boterbloem: Russia as Empire: Past and Present and Robin Milner-Gulland: Patterns of Russia: History, Culture, Spaces', 'abstract': 'transformations of the notion of historical progress alongside conceptions of the origins and processes of modernity. The core of her book concerns five fundamental presuppositions on which the world view of the ‘Modern Time Regime’ rest, namely ‘rupture’ (p. 93), ‘the fiction of beginning’ (p. 105), ‘creative destruction’ (p. 116), ‘destroying and preserving’ (p. 126) and ‘acceleration’ (p. 135). Having critically examined their complex interrelation, Assmann challenges herself and her readers not only to recognize that ‘Hamlet’s old claim that “the time is out of joint” today means that something indeed seems to be wrong in the relation between the temporal registers of past, present, and future’ (p. 190), but also to consider how we might overcome this, at least in an historiographical sense, without overreliance on an alarmist and ambiguous discourse of finger-pointing. For Assmann, repairing the ‘Modern Time Regime’ must concern the recentring of three elements in society: culture, identity and memory. After all, she affirms, ‘the temporal structure of the past, present, and future has collapsed and cannot simply be reproduced, but rather must be newly joined together’ (p. 229). If, as Bruno Latour once suggested, modernism should be recalled like a defective industrial product, then it is important to remember that when any product is recalled, this is not done with a view to disposing of it, but rather improving it. In a world still reeling from the ongoing Covid-19 pandemic, Assmann’s advocation that such a reconfiguration become ‘not the task of individual theorists of time but of society as a whole’ (p. 229) has never sounded more urgent.', 'corpus_id': 232281755, 'score': 0}, {'doc_id': '133065485', 'title': 'Some Observations on the Worship of Vesta, and the Holy Fire, in Ancient Rome: with an Account of the Vestal Virgins.', 'abstract': None, 'corpus_id': 133065485, 'score': 1}]
116	EUR200 I. ESSAY	16a248c745ad35170dc6cf83ec7c7d32	20296	{}	"[{'doc_id': '236462276', 'title': 'The Constitution of Identity New Modalities of Nationality, Citizenship, Belonging and Being', 'abstract': 'In recent decades there has emerged a large and diverse body of socio‐legal literature engaging in identity politics, or what some theorists call the politics of difference (Taylor 1992: 38). Drawing on the theories and insights of scholars working in cultural studies, feminist studies, sociology, anthropology, geography, political science, history and law, this literature grew out of the civil rights movements of the 1960s and 1970s and gained momentum through the rise of new social movements and debates over multiculturalism in the 1980s and 1990s. More recently, socio‐legal literature on the politics of identity has had to expand in scale and reach in seeking to analyze the complex relations between individuals and the nation‐state in the context of globalization (Lacey 2004). This expansion speaks to the ways people conceptualize their legal subjectivity and relations to others in emerging sociopolitical contexts that include the mobilization of global social movements, an expanding international human rights regime, and mass migrations of people that make some people “illegal” and “stateless” and includes millions of refugees fleeing wars, poverty, and various natural and man‐made disasters. This expansion in the socio‐legal literature also reflects new sociopolitical contexts of a less obviously global nature The Constitution of Identity New Modalities of Nationality, Citizenship, Belonging and Being', 'corpus_id': 236462276, 'score': 0}, {'doc_id': '237533048', 'title': 'Populism and foreign policy: a research agenda (Introduction)', 'abstract': 'The worldwide rise of populist governments represents one of the most crucial political developments of recent years. In Europe in particular, a range of populist parties and leaders have been voted into power and have formed (or joined) governments over the past decade—this is true, for instance, in Austria, Estonia, Finland, Greece, Hungary, Italy or Poland. As populist actors leave the opposition to seize the reins of executive power, they have entirely new possibilities to directly shape not only domestic policies, but also their countries’ foreign policy and European politics more generally. This could have important repercussions on the European integration project, on relations among European member states and with external powers such as Russia and China, on EU policies in areas such as migration or support to democratization, and on international norms and organizations more generally. Yet, surprisingly, how and to what extent populist government formation (or participation) concretely influences foreign policy has not been studied systematically so far. Particularly after the election of Donald Trump in the United States, this question has been the object of many (often sensationalist) comments in the media and in policy debates. Also, the literature on the international dimensions of populism has been growing rapidly (see below), but methodical, theory-driven and evidence-based analyses of the impact of populism on foreign policy are still scarce. While the literature on the domestic causes and manifestations of populism has been thriving for decades in the field of Comparative Politics (see, among many others, Meny and Surel 2002; Rooduijn 2018; Norris and Inglehart 2019; Taggart and', 'corpus_id': 237533048, 'score': 0}, {'doc_id': '159270327', 'title': 'A House Divided: Federalism and Social Conflict in Italy', 'abstract': 'Looking at Italy, the article argues that government serves as an intervening variable that can mediate the implication of federalism and social division. Its overall argument is that the Italian state maintained its unity through a governmental practice of configuring social division so as not to align on the North/South divide, while engaging in a comprehensive devolution of competencies to the subnational level. Through readings of Carlo Cattaneo and Guiseppe Mazzini, the first part of the article considers the conjunctural factors that allowed for the creation, against all odds, of Italy as a unitary state. The second part considers by what strategies the political parties colluded in preserving the unity of the national territory, and by what forms of devolution power was transferred to the subnational level. In conclusion, the article considers the rise of federalism in Italian politics from the 1990s.', 'corpus_id': 159270327, 'score': 1}, {'doc_id': '236165865', 'title': 'Counter-movement at a critical juncture : A neo-Polanyian interpretation of the rise of the illiberal Right in Poland', 'abstract': 'This article seeks to explain the causes of the growing popularity of the illiberal right, taking the Polish political party Law and Justice as an example. The adopted analytical approach combines insights derived from the work of Karl Polanyi and the tradition of historical institutionalism.The victory of Law and Justice in the 2015 Polish parliamentary elections is argued to constitute a critical juncture that initiated a fundamental break with the liberal order. Following Polanyi, we argue that the seeds of the recent anti-liberal counter-revolution can be found in the malfunctioning of the Polish economic order built during the period of transition. However, Law and Justice has managed to make use of the critical juncture arising from social discontent and has used it instrumentally to dismantle liberal constitutionalism and the rule of law.', 'corpus_id': 236165865, 'score': 0}, {'doc_id': '41981743', 'title': 'Culture, Values and Social Basis of Northern Italian Centrifugal Regionalism. A Contextual Political Analysis of the Lega Nord', 'abstract': 'The Lega Nord intended to create a party which represented the whole of the northern regions, capable to defend their interests and culture. If we examine the profile of people that voted for the Lega Nord, we find some discordant features compared to the project. A large majority of the party\'s electorate naturally supports federalism and devolution. However, the party\'s electorate paradoxically displays value trends and convictions that, for many aspects are in countertendency compared to the population of northern Italy, and much closer to those of the southern electorate. The relationship between the Lega Nord and northern society remain ambivalent and the party is unable to represent the whole of value instances that its citizens consider most important. Bossi\'s party gave only partly expression to the dominant ideas and values in northern Italy. It would seem a quite strange paradox for a regionalist party born to draw attention to the northern question. A paradox which is all interior to political dynamics, where actors interact in a complex way, considering the context in which they are included, that binds them, but doesn\'t determine them: far from being the expression of a homogeneous territory as regards culture and values, the Lega Nord is a particular political actor that builds its own identity by selecting themes and questions to represent. On the other hand, the North presents some very noteworthy interior differences both in territorial and socio-cultural terms; even if in the last years some converging dynamics are emerging around the medium enterprise model with long networks. Centrifugal regionalism reveals deep conflicts not only between territories, but also and above all, within the same regional territory, of the same ""regional culture"", showing this way the value heterogeneity internal to every territory. The Lega was able to intercept and mobilize a specific type of electorate - present above all in particular territorial contexts and within the popular and less educated sectors of the population - that on the point of view of values, of public spirit and of social attitudes, is distinguishable from the prevailing trends within the northern population and resembles the most diffused ones in the South. The Lega Nord was able to obtain support especially in these areas, by reinforcing fears, prejudices and some specific value trends of its inhabitants. By acting in such way, however, it progressively mobilized around a number of political matters quite distinctive different from the territorial ones, engaging itself rather in the defence of identity and national frontiers. The chapter presents inedit analysis and new survey data', 'corpus_id': 41981743, 'score': 1}, {'doc_id': '143019773', 'title': 'Regional culture in post-war Friuli: Literature in dialect, nationalism and friulanità', 'abstract': 'Summary The unprecedented increase in literary production in Friuli in the post-war period has coincided with the rise of popular ethno-nationalism in the region. Although there is an evident connection between the political, social and cultural fields in Friuli, this relationship is both complex and full of potential conflicts. This paper provides a brief overview of Friulian regionalism, before considering the specific role assigned to literature in Friulian by proponents of regional autonomy. It examines the problematic nature of the dominant ideology of friulanità and discusses the responses of a number of authors to the prevailing themes of cultural discourse in the region. In conclusion, it examines the ideological conflicts caused by modernisation in the region, and considers the impact that the transformation of the region has had on the literary debate, concentrating on the difficulties caused by Friulian linguistic purism.', 'corpus_id': 143019773, 'score': 1}, {'doc_id': '236974330', 'title': 'Federalism in Nepal: Issues and Challenges', 'abstract': ""1. Background: After 10 years armed conflict and 19 days people's movement, Nepal has entered to the phase of making a new constitution. It is evident that new constitution will not only be a democratic, also will be federal and inclusive too. Though the idea of federalism was mooted in Nepal around the 1990s when Constitution of the Kingdom of Nepal 1990 was framed, but at that time it got no importance. Establishment of the multi-party democracy, and parliamentary system were the major political issue of that time. Nepal has been exercising a unitary and centralized system for more than 240 years. During this period rulers always preferred to the concentration of power and opposed to every effort of the devolution of power. During Panchayat and Parliamentary period decentralization was adopted as the constitutional provision but no sincere efforts were made to implement such provisions. Even the legal instruments like Local Self-Government Act were not properly used to implement the provision made in the constitution. The successful completion of the Second People's movement brought two basic issues to the fore. That was Republic and Federalism. The constitutional amendments made after constituent assembly elections declared Nepal a Federal Republic. Now a vast majority of the people stands infavour of these provisions. As its result, 240 year old monarchy is abolished by the parliament and country is involved in to the debate what sort of federal structure is to be adopted for Nepal? In regard to this issue many diverse views prevail in Nepalese politics. This paper is intended to throw light on the issues and challenges to proposed federalism in Nepal."", 'corpus_id': 236974330, 'score': 0}, {'doc_id': '237210024', 'title': 'The Impact of Immigration on Danish Politics: The Rise of Populism and the Fall of Social Democracy', 'abstract': 'Perhaps the most volatile current issue in Denmark is immigration and its impact upon Danish society and politics. The Danish population is incredibly homogenous. Only 7.3% of the population is composed of either immigrants or citizens of non-Nordic descent. While the rate of immigration has remained relatively steady throughout the past twenty years, it soared in 1995: over 40,000 immigrants settled in Denmark in 1995 alone, many of them coming from less economically developed countries like Sri Lanka, Somalia, and Ethiopia. The political ramifications of these developments, which are still unfolding, have been tremendous. This paper explores the rising force of anti-immigration sentiments in Danish political discussions since the early 1990s. Our specific focus concerns the effect of growing anti-immigrant sentiments amongst some elements of the citizenry upon political party behavior within the Danish assembly, the Folketing. Have the policy positions of the major parties in Danish politics been affected by this increasingly important ideological/policy dimension? What has been the impact upon the process of coalition formation and the traditional coalition partners? These are the questions studied in this paper, where particular attention will be directed at the major player in Danish politics, the Social Democratic party. Prepared for presentation at the Tenth Annual Illinois Conference for Students of Political Science, Illinois State University, Normal, Illinois, April 4, 2002.', 'corpus_id': 237210024, 'score': 0}, {'doc_id': '153327653', 'title': 'The Re-invention of the European Radical Right: Populism, Regionalism, and the Italian Lega Nord', 'abstract': ""Combining an in-depth case study of the Italian Northern League with a comparative focus on other parties, Andrej Zaslove employs a socio-economic, institutional, and ideological analysis to argue that the new wave of right-wing parties in Western Europe converged into a radical right populist party family in the 1990s. He examines the transformation of the Northern League from its regionalist roots while focusing on the party's nationalism, authoritarianism, support for a market economy, opposition to globalization, and scepticism regarding Italian integration into the European Union. He also scrutinizes the Northern League's participation in political power between 2001 and 2006 and its influence on federalism, immigration, economic policy, and European integration. A thorough and thought-provoking work, The Re-invention of the European Radical Right offers remarkable insight into the ongoing effects of radical right populism on politics and public policy in Europe."", 'corpus_id': 153327653, 'score': 1}, {'doc_id': '152396251', 'title': 'The economic impact of the Friuli-Venezia Giulia autonomy: a synthetic control analysis of asymmetric Italian federalism', 'abstract': 'This article illustrates a case study on the economic impact of autonomy of one of the five Italian special-statute regions, namely Friuli-Venezia Giulia. This region had, and continues to have, legislative, administrative and financial prerogatives in areas of public intervention that are the duty of the central government in the 15 Italian ordinary regions. Accordingly, Friuli-Venezia Giulia could have year by year exploited such prerogatives to achieve an economic development higher than that attainable in the absence of its autonomy. In other words, Friuli-Venezia Giulia long-run economic growth would have been less than that actually experienced if the region had been an ordinary one. To test this hypothesis, the synthetic control method has been adopted. A suitable synthetic Friuli-Venezia Giulia has been constructed to contrast the evolution of regional real per capita GDP, observed over the post-autonomy-policy period, with the corresponding evolution of the same aggregate for the synthetic counterpart. This comparison reveals that if Friuli-Venezia Giulia were not an autonomous region, its per capita GDP would be significantly lower than that effectively observed.', 'corpus_id': 152396251, 'score': 1}]"
117	Domain adaptation	294dda336b86e987a45b5f2595fd3ed0	5781	{}	"[{'doc_id': '218470396', 'title': 'Inexpensive Domain Adaptation of Pretrained Language Models: Case Studies on Biomedical NER and Covid-19 QA', 'abstract': 'Domain adaptation of Pretrained Language Models (PTLMs) is typically achieved by unsupervised pretraining on target-domain text. While successful, this approach is expensive in terms of hardware, runtime and CO 2 emissions. Here, we propose a cheaper alternative: We train Word2Vec on target-domain text and align the resulting word vectors with the wordpiece vectors of a general-domain PTLM. We evaluate on eight English biomedical Named Entity Recognition (NER) tasks and compare against the recently proposed BioBERT model. We cover over 60% of the BioBERT - BERT F1 delta, at 5% of BioBERT’s CO 2 footprint and 2% of its cloud compute cost. We also show how to quickly adapt an existing general-domain Question Answering (QA) model to an emerging domain: the Covid-19 pandemic.', 'corpus_id': 218470396, 'score': 1}, {'doc_id': '215238886', 'title': 'Inexpensive Domain Adaptation of Pretrained Language Models: A Case Study on Biomedical Named Entity Recognition', 'abstract': ""Domain adaptation of Pretrained Language Models (PTLMs) is typically achieved by pretraining on in-domain text. While successful, this approach is expensive in terms of hardware, runtime and CO_2 emissions. Here, we propose a cheaper alternative: We train Word2Vec on in-domain text and align the resulting word vectors with the input space of a general-domain PTLM (here: BERT). We evaluate on eight biomedical Named Entity Recognition (NER) tasks and compare against the recently proposed BioBERT model (Lee et al., 2020). We cover over 50% of the BioBERT-BERT F1 delta, at 5% of BioBERT's CO_2 footprint and 2% of its cloud compute cost."", 'corpus_id': 215238886, 'score': 1}, {'doc_id': '216867827', 'title': 'SegaBERT: Pre-training of Segment-aware BERT for Language Understanding', 'abstract': 'Pre-trained language models have achieved state-of-the-art results in various natural language processing tasks. Most of them are based on the Transformer architecture, which distinguishes tokens with the token position index of the input sequence. However, sentence index and paragraph index are also important to indicate the token position in a document. We hypothesize that better contextual representations can be generated from the text encoder with richer positional information. To verify this, we propose a segment-aware BERT, by replacing the token position embedding of Transformer with a combination of paragraph index, sentence index, and token index embeddings. We pre-trained the SegaBERT on the masked language modeling task in BERT but without any affiliated tasks. Experimental results show that our pre-trained model can outperform the original BERT model on various NLP tasks.', 'corpus_id': 216867827, 'score': 0}, {'doc_id': '218719869', 'title': 'BERTweet: A pre-trained language model for English Tweets', 'abstract': 'We present BERTweet, the first public large-scale pre-trained language model for English Tweets. Our BERTweet, having the same architecture as BERT-base (Devlin et al., 2019), is trained using the RoBERTa pre-training procedure (Liu et al., 2019). Experiments show that BERTweet outperforms strong baselines RoBERTa-base and XLM-R-base (Conneau et al., 2020), producing better performance results than the previous state-of-the-art models on three Tweet NLP tasks: Part-of-speech tagging, Named-entity recognition and text classification. We release BERTweet under the MIT License to facilitate future research and applications on Tweet data. Our BERTweet is available at https://github.com/VinAIResearch/BERTweet', 'corpus_id': 218719869, 'score': 1}, {'doc_id': '7879294', 'title': 'Detection of rare functional variants using group ISIS', 'abstract': 'Genome-wide association studies have been firmly established in investigations of the associations between common genetic variants and complex traits or diseases. However, a large portion of complex traits and diseases cannot be explained well by common variants. Detecting rare functional variants becomes a trend and a necessity. Because rare variants have such a small minor allele frequency (e.g., <0.05), detecting functional rare variants is challenging. Group iterative sure independence screening (ISIS), a fast group selection tool, was developed to select important genes and the single-nucleotide polymorphisms within. The performance of the group ISIS and group penalization methods is compared for detecting important genes in the Genetic Analysis Workshop 17 data. The results suggest that the group ISIS is an efficient tool to discover genes and single-nucleotide polymorphisms associated to phenotypes.', 'corpus_id': 7879294, 'score': 0}, {'doc_id': '216915402', 'title': 'A Focused Study to Compare Arabic Pre-training Models on Newswire IE Tasks', 'abstract': 'The Arabic language is a morphological rich language, posing many challenges for information extraction (IE) tasks, including Named Entity Recognition (NER), Part-of-Speech tagging (POS), Argument Role Labeling (ARL) and Relation Extraction (RE). A few multilingual pre-trained models have been proposed and show good performance for Arabic, however, most experiment results are reported on language understanding tasks, such as natural language inference, question answering and sentiment analysis. Their performance on the IE tasks is less known, in particular, the cross-lingual transfer capability from English to Arabic. In this work, we pre-train a Gigaword-based bilingual language model (GigaBERT) to study these two distant languages as well as zero-short transfer learning on the information extraction tasks. Our GigaBERT model can outperform mBERT and XLM-R-base on NER, POS and ARL tasks, with regarding to the per-language and/or zero-transfer performance. We make our pre-trained models publicly available at this https URL to facilitate the research of this field.', 'corpus_id': 216915402, 'score': 0}, {'doc_id': '219530533', 'title': 'Pre-training Polish Transformer-based Language Models at Scale', 'abstract': 'Transformer-based language models are now widely used in Natural Language Processing (NLP). This statement is especially true for English language, in which many pre-trained models utilizing transformer-based architecture have been published in recent years. This has driven forward the state of the art for a variety of standard NLP tasks such as classification, regression, and sequence labeling, as well as text-to-text tasks, such as machine translation, question answering, or summarization. The situation have been different for low-resource languages, such as Polish, however. Although some transformer-based language models for Polish are available, none of them have come close to the scale, in terms of corpus size and the number of parameters, of the largest English-language models. In this study, we present two language models for Polish based on the popular BERT architecture. The larger model was trained on a dataset consisting of over 1 billion polish sentences, or 135GB of raw text. We describe our methodology for collecting the data, preparing the corpus, and pre-training the model. We then evaluate our models on thirteen Polish linguistic tasks, and demonstrate improvements over previous approaches in eleven of them.', 'corpus_id': 219530533, 'score': 0}, {'doc_id': '214802823', 'title': 'Improved Pretraining for Domain-specific Contextual Embedding Models', 'abstract': 'We investigate methods to mitigate catastrophic forgetting during domain-specific pretraining of contextual embedding models such as BERT, DistilBERT, and RoBERTa. Recently proposed domain-specific models such as BioBERT, SciBERT and ClinicalBERT are constructed by continuing the pretraining phase on a domain-specific text corpus. Such pretraining is susceptible to catastrophic forgetting, where the model forgets some of the information learned in the general domain. We propose the use of two continual learning techniques (rehearsal and elastic weight consolidation) to improve domain-specific training. Our results show that models trained by our proposed approaches can better maintain their performance on the general domain tasks, and at the same time, outperform domain-specific baseline models on downstream domain tasks.', 'corpus_id': 214802823, 'score': 1}, {'doc_id': '216080466', 'title': 'Don’t Stop Pretraining: Adapt Language Models to Domains and Tasks', 'abstract': 'Language models pretrained on text from a wide variety of sources form the foundation of today’s NLP. In light of the success of these broad-coverage models, we investigate whether it is still helpful to tailor a pretrained model to the domain of a target task. We present a study across four domains (biomedical and computer science publications, news, and reviews) and eight classification tasks, showing that a second phase of pretraining in-domain (domain-adaptive pretraining) leads to performance gains, under both high- and low-resource settings. Moreover, adapting to the task’s unlabeled data (task-adaptive pretraining) improves performance even after domain-adaptive pretraining. Finally, we show that adapting to a task corpus augmented using simple data selection strategies is an effective alternative, especially when resources for domain-adaptive pretraining might be unavailable. Overall, we consistently find that multi-phase adaptive pretraining offers large gains in task performance.', 'corpus_id': 216080466, 'score': 1}, {'doc_id': '218889376', 'title': 'ParsBERT: Transformer-based Model for Persian Language Understanding', 'abstract': 'The surge of pre-trained language models has begun a new era in the field of Natural Language Processing (NLP) by allowing us to build powerful language models. Among these models, Transformer-based models such as BERT have become increasingly popular due to their state-of-the-art performance. However, these models are usually focused on English, leaving other languages to multilingual models with limited resources. This paper proposes a monolingual BERT for the Persian language (ParsBERT), which shows its state-of-the-art performance compared to other architectures and multilingual models. Also, since the amount of data available for NLP tasks in Persian is very restricted, a massive dataset for different NLP tasks as well as pre-training the model is composed. ParsBERT obtains higher scores in all datasets, including existing ones as well as composed ones and improves the state-of-the-art performance by outperforming both multilingual BERT and other prior works in Sentiment Analysis, Text Classification and Named Entity Recognition tasks.', 'corpus_id': 218889376, 'score': 0}]"
118	Mol-Psych	16bc00a852c522f5ef39a0d79b89ad6c	9346	{}	[{'doc_id': '10045228', 'title': '5-HT2A receptor deficiency alters the metabolic and transcriptional, but not the behavioral, consequences of chronic unpredictable stress', 'abstract': 'Chronic stress enhances risk for psychiatric disorders, and in animal models is known to evoke depression-like behavior accompanied by perturbed neurohormonal, metabolic, neuroarchitectural and transcriptional changes. Serotonergic neurotransmission, including serotonin2A (5-HT2A) receptors, have been implicated in mediating specific aspects of stress-induced responses. Here we investigated the influence of chronic unpredictable stress (CUS) on depression-like behavior, serum metabolic measures, and gene expression in stress-associated neurocircuitry of the prefrontal cortex (PFC) and hippocampus in 5-HT2A receptor knockout (5-HT2A−/−) and wild-type mice of both sexes. While 5-HT2A−/− male and female mice exhibited a baseline reduced anxiety-like state, this did not alter the onset or severity of behavioral despair during and at the cessation of CUS, indicating that these mice can develop stress-evoked depressive behavior. Analysis of metabolic parameters in serum revealed a CUS-evoked dyslipidemia, which was abrogated in 5-HT2A−/− female mice with a hyperlipidemic baseline phenotype. 5-HT2A−/− male mice in contrast did not exhibit such a baseline shift in their serum lipid profile. Specific stress-responsive genes (Crh, Crhr1, Nr3c1, and Nr3c2), trophic factors (Bdnf, Igf1) and immediate early genes (IEGs) (Arc, Fos, Fosb, Egr1-4) in the PFC and hippocampus were altered in 5-HT2A−/− mice both under baseline and CUS conditions. Our results support a role for the 5-HT2A receptor in specific metabolic and transcriptional, but not behavioral, consequences of CUS, and highlight that the contribution of the 5-HT2A receptor to stress-evoked changes is sexually dimorphic.', 'corpus_id': 10045228, 'score': 1}, {'doc_id': '207410907', 'title': 'Hippocampal transcriptional and neurogenic changes evoked by combination yohimbine and imipramine treatment', 'abstract': 'Adjunct α2-adrenoceptor antagonism is a potential strategy to accelerate the behavioral effects of antidepressants. Co-administration of the α2-adrenoceptor antagonist yohimbine hastens the behavioral and neurogenic effects of the antidepressant imipramine. We examined the transcriptional targets of short duration (7days), combination treatment of yohimbine and imipramine (Y+I) within the adult rat hippocampus. Using microarray and qPCR analysis we observed functional enrichment of genes involved in intracellular signaling cascades, plasma membrane, cellular metal ion homeostasis, multicellular stress responses and neuropeptide signaling pathways in the Y+I transcriptome. We noted reduced expression of the α2A-adrenoceptor (Adra2a), serotonin 5HT2C receptor (Htr2c) and the somatostatin receptor 1 (Sstr1), which modulate antidepressant action. Further, we noted a regulation of signaling pathway genes like inositol monophosphatase 2 (Impa2), iodothyronine deiodinase 3 (Dio3), regulator of G-protein signaling 4 (Rgs4), alkaline ceramidase 2 (Acer2), doublecortin-like kinase 2 (Dclk2), nuclear factor of kappa light polypeptide gene enhancer in B-cells inhibitor, alpha (Nfkbia) and serum/glucocorticoid-regulated kinase 1 (Sgk1), several of which are implicated in the pathophysiology of mood disorders. Comparative analysis revealed an overlap in the hippocampal regulation of Acer2, Nfkbia, Sgk1 and Impa2 between Y+I treatment, the fast-acting electroconvulsive seizure (ECS) paradigm, and the slow-onset chronic (21days) imipramine treatment. Further, Y+I treatment enhanced the quiescent neural progenitor pool in the hippocampal neurogenic niche similar to ECS, and distinct from chronic imipramine treatment. Taken together, our results provide insight into the molecular and cellular targets of short duration Y+I treatment, and identify potential leads for the development of rapid-action antidepressants.', 'corpus_id': 207410907, 'score': 1}, {'doc_id': '205245266', 'title': 'Early emergence of altered 5-HT2A receptor-evoked behavior, neural activation and gene expression following maternal separation', 'abstract': 'The early stress of Maternal Separation (MS) contributes to the establishment of adult psychopathology. The serotonergic (5‐HT) system is implicated during this temporal window in mediating the development of mood‐related behaviors. MS is reported to evoke altered 5‐HT2A receptor function in adulthood. However, the ontogeny of altered 5‐HT2A receptor responsivity following MS remains unknown. Here, we examined 5‐HT2A receptor agonist, DOI (1‐(2,5‐dimethoxy‐4‐iodophenyl)‐2‐aminopropane) (2 mg/kg) evoked responses, namely stereotypical head‐twitch behaviors in control and MS Sprague‐Dawley rat pups at postnatal day 21 (P21). MS involved a separation of pups from the dam for 3 h daily from postnatal day 2–14. MS pups at P21 exhibited significantly enhanced head‐twitch behaviors compared to controls. Using c‐Fos cell counting we examined neural activation in control and MS pups following DOI treatment. MS pups exhibited altered DOI‐evoked c‐Fos expression within all mPFC subdivisions, but not in the hippocampus, lateral septum and hypothalamus, suggesting differential prefrontal neural activation upon 5‐HT2A receptor stimulation following early stress. Gene profiling of 5‐HT2A receptor‐regulated immediate early genes (IEGs) indicated a decline in the expression of Fos, Fra1 and Egr1 mRNA under baseline conditions in the mPFC of MS pups. MS pups also showed an altered pattern in the regulation of several 5‐HT2A receptor‐regulated IEGs (Fos, Fra1, Bdnf, Egr1, Egr3) following DOI treatment. Collectively, these results highlight an early emergence of altered 5‐HT2A receptor‐evoked behavioral responses and neural activation patterns in multiple brain regions in animals with a history of MS.', 'corpus_id': 205245266, 'score': 1}, {'doc_id': '221642532', 'title': 'Insulin potentiates the synchronous firing of arcuate nucleus Kiss1 neurons that protects against diet-induced obesity', 'abstract': 'Kisspeptin neurons in the hypothalamic arcuate nucleus (Kiss1ARH) co-express kisspeptin, neurokinin B, dynorphin and provide an episodic, excitatory drive to gonadotropin-releasing hormone (GnRH) neurons, which is critical for pubertal development and fertility. Previously, we showed that high frequency firing of Kiss1ARH neurons co-releases NKB and dynorphin onto neighboring Kiss1ARH neurons to generate a slow excitatory postsynaptic potential (EPSP) that entrains intermittent, synchronous firing of Kiss1ARH neurons (Qiu et al., 2016). Presently, we discovered that insulin significantly increased the amplitude of the slow EPSP, which we documented is mediated by TRPC5 channels, and augmented synchronous GCaMP6s ([Ca]i) oscillations in Kiss1ARH neurons. Deletion of the endoplasmic reticulum calcium-sensing protein stromal interaction molecule 1 in Kiss1ARH neurons amplified insulin9s actions and protected ovariectomized female mice from developing obesity and glucose intolerance with high-fat dieting. Therefore, insulin appears to be critical for facilitating synchronous firing of Kiss1ARH neurons and coordinating energy homeostasis with fertility.', 'corpus_id': 221642532, 'score': 0}, {'doc_id': '221816613', 'title': 'Delayed microglial depletion after spinal cord injury reduces chronic inflammation and neurodegeneration in the brain and improves neurological recovery in male mice', 'abstract': 'Neuropsychological deficits, including impairments in learning and memory, occur after spinal cord injury (SCI). In experimental SCI models, we and others have reported that such changes reflect sustained microglia activation in the brain that is associated with progressive neurodegeneration. In the present study, we examined the effect of pharmacological depletion of microglia on posttraumatic cognition, depressive-like behavior, and brain pathology after SCI in mice. Methods: Young adult male C57BL/6 mice were subjected to moderate/severe thoracic spinal cord contusion. Microglial depletion was induced with the colony-stimulating factor 1 receptor (CSF1R) antagonist PLX5622 administered starting either 3 weeks before injury or one day post-injury and continuing through 6 weeks after SCI. Neuroinflammation in the injured spinal cord and brain was assessed using flow cytometry and NanoString technology. Neurological function was evaluated using a battery of neurobehavioral tests including motor function, cognition, and depression. Lesion volume and neuronal counts were quantified by unbiased stereology. Results: Flow cytometry analysis demonstrated that PLX5622 pre-treatment significantly reduced the number of microglia, as well as infiltrating monocytes and neutrophils, and decreased reactive oxygen species production in these cells from injured spinal cord at 2-days post-injury. Post-injury PLX5622 treatment reduced both CD45int microglia and CD45hi myeloid counts at 7-days. Following six weeks of PLX5622 treatment, there were substantial changes in the spinal cord and brain transcriptomes, including those involved in neuroinflammation. These alterations were associated with improved neuronal survival in the brain and neurological recovery. Conclusion: These findings indicate that pharmacological microglia-deletion reduces neuroinflammation in the injured spinal cord and brain, improving recovery of cognition, depressive-like behavior, and motor function.', 'corpus_id': 221816613, 'score': 0}, {'doc_id': '3530138', 'title': 'Early Stress History Alters Serum Insulin‐Like Growth Factor‐1 and Impairs Muscle Mitochondrial Function in Adult Male Rats', 'abstract': 'Early‐life adversity is associated with an enhanced risk for adult psychopathology. Psychiatric disorders such as depression exhibit comorbidity for metabolic dysfunction, including obesity and diabetes. However, it is poorly understood whether, besides altering anxiety and depression‐like behaviour, early stress also evokes dysregulation of metabolic pathways and enhances vulnerability for metabolic disorders. We used the rodent model of the early stress of maternal separation (ES) to examine the effects of early stress on serum metabolites, insulin‐like growth factor (IGF)‐1 signalling, and muscle mitochondrial content. Adult ES animals exhibited dyslipidaemia, decreased serum IGF1 levels, increased expression of liver IGF binding proteins, and a decline in the expression of specific metabolic genes in the liver and muscle, including Pck1, Lpl, Pdk4 and Hmox1. These changes occurred in the absence of alterations in body weight, food intake, glucose tolerance, insulin tolerance or insulin levels. ES animals also exhibited a decline in markers of muscle mitochondrial content, such as mitochondrial DNA levels and expression of TFAM (transcription factor A, mitochondrial). Furthermore, the expression of several genes involved in mitochondrial function, such as Ppargc1a, Nrf1, Tfam, Cat, Sesn3 and Ucp3, was reduced in skeletal muscle. Adult‐onset chronic unpredictable stress resulted in overlapping and distinct consequences from ES, including increased circulating triglyceride levels, and a decline in the expression of specific metabolic genes in the liver and muscle, with no change in the expression of genes involved in muscle mitochondrial function. Taken together, our results indicate that a history of early adversity can evoke persistent changes in circulating IGF‐1 and muscle mitochondrial function and content, which could serve to enhance predisposition for metabolic dysfunction in adulthood.', 'corpus_id': 3530138, 'score': 1}, {'doc_id': '220726852', 'title': 'The Role of Time and Nicotine Dose on Anxiety Measured with Light Enhanced Startle', 'abstract': 'Two experiments using Light-enhanced startle (LES) examined dose-dependent and timedependent effects of acute nicotine on anxiety. In Experiment 1 rats were exposed to saline, .15 mg/kg, or .40 mg/kg (i.p.) nicotine and 5 minutes later were behaviorally tested in LES. Data suggested that nicotine at both doses was anxiolytic in males but not anxiolytic in females. In females, the higher nicotine dose, .40 mg/kg, was anxiogenic but only during later portions of the test session. In both males and females, within-session variation in LES provided evidence that LES increased in magnitude as time since nicotine administration increased. Therefore, in Experiment 2, longer drug-to-test intervals were applied in order to examine possible timedependent increases in anxiety produced by nicotine. In Experiment 2, rats were exposed to saline or .40 mg/kg nicotine and were behaviorally tested 15 min or 35 min after nicotine administration in LES. Trends in the available data suggested an anxiogenic profile of nicotine when tested 15 min following drug administration but an anxiolytic profile when tested 35 min following drug administration. At the short 5 min drug-to-test interval used in Experiment 1, findings contradict those in other experiments using the social interaction test of anxiety. Collectively, results suggest Light-enhanced startle is sensitive to dose and time-dependent effects of nicotine on anxiety. Possible differences between reflexive and non-reflexive measures of fear and gonadal influences in anxiety expression are discussed. Time and Nicotine in LES 4 The Role of Time and Nicotine Dose on Anxiety Measured in Light Enhanced Startle A recent article published by the American Heart Association indicates that Americans are more likely to use electronic cigarettes than traditional cigarettes because Americans believe that electronic cigarettes are less harmful (American Heart Association, 2019). Though under current debate, even if it is found to be true that e-cigarette vapor contains fewer contaminants than traditional tobacco smoke, the primary neuroactive compound in these devices, nicotine, is still present and can alter typical brain function (American Heart Association, 2019; World Health Organization, 2019). Although nicotine has been studied extensively in the past 40 years the effects of this compound in the brain, which are known to produce anxiety behavior in both humans and animal models, remains unclear (Parrot, 1999). Nicotine is a nicotinic acetylcholine receptor (nAChR) agonist and parasympathetic alkaloid with a high affinity for the α2β4 nAChR subtype (Abou-Donia, 2015). Stimulation of nAChRs by nicotine has been shown to modulate anxiety in stressful situations (Piccioto et al., 2002; Irvine, Cheeta, & File, 2001). Molecularly, nicotine alters the central nervous system by binding to nAChRs which then causes this ligand gated ion channel to turn and open (Wonnacott et al., 2005). The activation of these nAChRs causes an influx of calcium and sodium ions because of said consequential opening of the channel (Kirsch et al., 2016). This molecular cascade can generate or dissipate states of anxiety through its interactions in the basal lateral amygdala (BLA), an area of the brain which is widely recognized for its involvement and production of neural responses to anxiety inducing stimuli (Sharp, 2019). Activation of glutamatergic output neurons of the BLA in part characterize the brain’s response to stressinducing environmental stimuli and play a central role in anxiety. Clinically aberrant activity of the BLA, for example, has been shown to be associated with exaggerated stress responses leading to the progression of anxiety disorders in humans (Graham & Milad, 2011). Importantly, both glutamatergic and GABAergic neurons of the BLA express nAChRs. Stimulation of nAChRs located on glutamatergic neurons of the BLA can directly increase excitatory output of the BLA. Moreover, previous research has shown that increases in glutamatergic responses via nAChR stimulation can contribute to changes in anxiety-like behavior in rats (Ryu et al., 2017). However, nAChRs also expressed on GABAergic afferents to the BLA can increase GABA transmission and modify neural activity by decreasing the glutamatergic excitatory output of the BLA. Through this mechanism, nicotine could potentially decrease anxiety through increased transmission of inhibitory GABA neurons. In summary, both excitatory and inhibitory modulation of the BLA can be mediated by nicotine stimulating nAChRs within the circuitry of the BLA. This is important to nicotine’s relationship with anxiety because the glutamatergic neurons expressing nAChRs and GABAergic neurons expressing nAChRs reveal different routes of activation on BLA that could either increase or decrease anxiety. Animal models are especially valuable in isolating causal influences of nicotine on anxiety. File, Gonzalez, & Andrews (1998) for example, demonstrated in rats that decreases in anxiety measured in the social interaction test, possibly due to nicotine, were blocked by the nonselective nAChR antagonist mecamylamine; revealing a causal role of nAChRs in anxiety. In the social interaction test, anxiety is measured as a reduction in normal prosocial behavior. However, consistent with the dual routes through which nicotine can affect BLA activity described above, there is a current debate in the literature whether nicotine is anxiolytic or anxiogenic in the social interaction test among others. Some studies report nicotine is anxiolytic (Brioni et al., 1993; Costall et al. ,1989; Vale & Green, 1986; Villégier, et al., 2010) whereas others report that nicotine is anxiogenic (Ouagazzal, Kenny, & File, 1999a, 1999b; Morrison, 1969). These paradoxical results could be attributed to the behavioral paradigm used, differences Time and Nicotine in LES 5 in dose and route of administration of nicotine, and the nicotine-to-test interval used, among other experimental parameters (for a review see Picciotto, Brunzell, & Caldarone, 2002; see Morissette et al., 2007 for a related discussion of interrelationships between nicotine and anxiety in humans). Assessment of the delay between nicotine administration and behavioral (or neural) testing is a critical factor because nicotine can modulate the release of neurotransmitters in different parts of the brain and it is likely that nAChRs have different time courses of inactivation and activation which could have time-dependent effects on brain systems important to anxiety (Picciotto et al., 2002). Interestingly, some studies using the same behavioral task report differing effects of nicotine when the nicotine-to-test interval is varied. In one striking example, Irvine, Cheeta, and File (1999) varied the nicotine-to-test delay and reported in the social interaction test with rats that the same dose of nicotine (.1 mg/kg) had anxiogenic effects with a 5 min nicotine-to-test interval, but anxiolytic effects at a longer 30 min nicotine-to-test interval. Thus, the same dose of nicotine had opposite effects on behavior. The current research systematically examines the impact of a nicotine-to-test interval similar to that of Irvine et al. (1999) but examines anxiety in light-enhanced startle, a validated animal model of anxiety (Walker & Davis, 1997). Unlike Irvine et al. (1999) the current research additionally examines the effect of nicotine dose on anxiety, and explores possible sex differences. In the light-enhanced startle (LES) paradigm the rat’s acoustic startle reflex to brief but loud bursts of white noise is measured in a dark setting and then is measured again in the presence of a bright light, an innately aversive stimulus to rodents (Walker & Davis, 1997). In this model, high illumination levels are thought to increase anxiety and thereby increase the magnitude of the acoustic startle reflex in light compared to dark sessions (Walker & Davis 1997). This paradigm advantageously provides a way to assess acoustic startle response to unconditioned fear stimuli implicated in anxiety (de Jongh et al, 2003). Additionally, research has shown that increases of startle reflex in LES reflect an influence of higher anxiety levels while low levels of startle reflect states of lower anxiety (Walker & Davis 2001). LES also provides further advantages over a related paradigm, fear-potentiated startle (FPS), another model of anxiety, because LES reflects unlearned fear to unconditioned stimuli whereas FPS is specifically designed to assess anxiety responses to learned, conditioned fear stimuli. As a result, LES can be repeatedly tested without contaminating factors of learned fear extinction or conditioned sensitization (Walker & Davis 1997; 2001). Finally, LES serves a valuable tool because it measures an unbiased and ‘pure’ form of anxiety. As described, LES is not subject to extinction in repeated testing thereby allowing the behavioral measure of anxiety in LES to be uncoupled from learning related changes in anxiety behavior. Additionally, LES, as a startle paradigm using the acoustic startle reflex as primary response measure, engages relatively simple hindbrain circuits (Walker & Davis 1997; 2001) unlike anxiety measured in the social interaction test of Irvine et al. (1999) which is known to recruit higher brain processing centers including the hippocampus (File, Kenny, & Ouagazzal, 1998) and frontal cortex (Ko, 2017). Tests of anxiety in the social interaction test involve choice and decision making whereas anxiety measured in LES involves innate unlearned reflexes. Examining nicotine’s effect on anxiety specifically in LES may therefore be of further interest because it will permit comparison of nicotine’s impact on reflexive and non-reflexive aspects of fear. The present research had two goals. Experiment 1 examined the effect of nicotine dose on the magnitude of LES in order to clarify anxiolytic vs. anxiogenic profile of nicotine on an', 'corpus_id': 220726852, 'score': 0}, {'doc_id': '222069723', 'title': 'Hydroxychloroquine increased psychiatric-like behaviors and disrupted the expression of related genes in the mouse brain', 'abstract': 'Hydroxychloroquine (HCQ), which has been proposed as a therapeutic or prophylactic drug for SARS-COV-2, has been administered to thousands of individuals with varying efficacy; however, our understanding of its adverse effects is insufficient. It was reported that HCQ induced psychiatric symptoms in a few patients with autoimmune diseases, but it is still uncertain whether HCQ poses a risk to mental health. Therefore, in this study, we treated healthy mice with two different doses of HCQ that are comparable to clinically administered doses for 7 days. Psychiatric-like behaviors and the expression of related molecules in the brain were evaluated at two time points, i.e., 24 h and 10 days after drug administration. We found that HCQ increased anxiety behavior at both 24 h and 10 days and enhanced depressive behavior at 24 h. Furthermore, HCQ decreased the mRNA expression of interleukin-1beta and corticotropin-releasing hormone (Crh) in the hippocampus and decreased the mRNA expression of brain-derived neurotrophic factor (Bdnf) in both the hippocampus and amygdala. Most of these behavioral and molecular changes were sustained beyond 10 days after drug administration, and some of them were dose-dependent. Although this animal study does not prove that HCQ has a similar effect in humans, it indicates that HCQ poses a significant risk to mental health and suggests that further clinical investigation is essential. According to our data, we recommend that HCQ be carefully used as a prophylactic drug in people who are susceptible to mental disorders.', 'corpus_id': 222069723, 'score': 0}, {'doc_id': '14475102', 'title': 'Early stress evokes dysregulation of histone modifiers in the medial prefrontal cortex across the life span.', 'abstract': 'Early stress has been hypothesized to recruit epigenetic mechanisms to mediate persistent molecular, cellular, and behavioral changes. Here, we have examined the consequence of the early life stress of maternal separation (ES) on the gene expression of several histone modifiers that regulate histone acetylation and methylation within the medial prefrontal cortex (mPFC), a key limbic brain region that regulates stress responses and mood-related behavior. ES animals exhibit gene regulation of both writer (histone acetyltransferases and histone methyltransferases) and eraser (histone deacetylases and histone lysine demethylases) classes of histone modifiers. While specific histone modifiers (Kat2a, Smyd3, and Suv420h1) and the sirtuin, Sirt4 were downregulated across life within the mPFC of ES animals, namely at postnatal Day 21, 2 months, and 15 months of age, we also observed gene regulation restricted to these specific time points. Despite the decline noted in expression of several histone modifiers within the mPFC following ES, this was not accompanied by any change in global or residue-specific H3 acetylation and methylation. Our findings indicate that ES results in the regulation of several histone modifiers within the mPFC across life, and suggest that such perturbations may contribute to the altered prefrontal structural and functional plasticity observed following early adversity.', 'corpus_id': 14475102, 'score': 1}, {'doc_id': '221015182', 'title': 'Brain Testosterone-CYP1B1 (Cytochrome P450 1B1) Generated Metabolite 6β-Hydroxytestosterone Promotes Neurogenic Hypertension and Inflammation', 'abstract': 'Supplemental Digital Content is available in the text. Previously, we showed that peripheral administration of 6β-hydroxytestosterone, a CYP1B1 (cytochrome P450 1B1)-generated metabolite of testosterone, promotes angiotensin II-induced hypertension in male mice. However, the site of action and the underlying mechanism by which 6β-hydroxytestosterone contributes to angiotensin II-induced hypertension is not known. Angiotensin II increases blood pressure by its central action, and CYP1B1 is expressed in the brain. This study was conducted to determine whether testosterone-CYP1B1 generated metabolite 6β-hydroxytestosterone locally in the brain promotes the effect of systemic angiotensin II to produce hypertension in male mice. Central CYP1B1 knockdown in wild-type (Cyp1b1+/+) mice by intracerebroventricular-adenovirus-GFP (green fluorescence protein)-CYP1B1-short hairpin (sh)RNA attenuated, whereas reconstitution of CYP1B1 by adenovirus-GFP-CYP1B1-DNA in the paraventricular nucleus but not in subfornical organ in Cyp1b1−/− mice restored angiotensin II-induced increase in systolic blood pressure measured by tail-cuff. Intracerebroventricular-testosterone in orchidectomized (Orchi)-Cyp1b1+/+ but not in Orchi-Cyp1b1−/−, and intracerebroventricular-6β-hydroxytestosterone in the Orchi-Cyp1b1−/− mice restored the angiotensin II-induced: (1) increase in mean arterial pressure measured by radiotelemetry, and autonomic imbalance; (2) reactive oxygen species production in the subfornical organ and paraventricular nucleus; (3) activation of microglia and astrocyte, and neuroinflammation in the paraventricular nucleus. The effect of intracerebroventricular-6β-hydroxytestosterone to restore the angiotensin II-induced increase in mean arterial pressure and autonomic imbalance in Orchi-Cyp1b1−/− mice was inhibited by intracerebroventricular-small interfering (si)RNA-androgen receptor (AR) and GPRC6A (G protein-coupled receptor C6A). These data suggest that testosterone-CYP1B1-generated metabolite 6β-hydroxytestosterone, most likely in the paraventricular nucleus via AR and GPRC6A, contributes to angiotensin II-induced hypertension and neuroinflammation in male mice.', 'corpus_id': 221015182, 'score': 0}]
119	Youtube Popularity	25006a84fdc75d526daa2325c98a1bbb	19084	{}	[{'doc_id': '235280616', 'title': 'Classification and Evaluation for Microblog Popularity Prediction', 'abstract': 'In recent years, with the rapid development of the Internet, especially the mobile Internet, social networks have entered the stage of vigorous development and become one of the main sources of information. User-generated contents (UGC) on social platforms can spread information along social networks at an astonishing speed. Existing literature has proposed many prediction methods for the popularity prediction on social networks. This paper presents a classification and establishes a unified evaluation framework of popularity prediction methods for microblogs. More specifically, we divide these mainstream prediction methods into four types: feature based methods, time series methods, collaborative filtering methods and deep learning methods and conduct experiments on the real-world weibo data using these methods to predict. Finally, according to four indicators, including accuracy, efficiency, robustness and bias, we evaluate and compare the methods. Based on the prediction and evaluation results, this paper summarizes and draws the following research conclusions:(1) The deep learning method has the characteristics of high accuracy, high robustness and low bias. The DeepFM method, one of the deep learning methods, performs better than the other three prediction methods when using temporal data as its input. (2) The feature based methods only using temporal features are basically consistent with those using all available features, indicating that the temporal feature has strong prediction power. Therefore, the ‘peeking’ strategy that monitors the early response of users in the initial period after the items are posted is effective. Additionally, the predictive power of temporary features can be further amplified in time series methods and deep learning methods. (3) Due to the sparse user-item interaction in social networks, the accuracy and efficiency of collaborative filtering methods are low, which makes it impossible to predict the popularity of items in social networks well.', 'corpus_id': 235280616, 'score': 1}, {'doc_id': '197466931', 'title': 'Characterization and Early Detection of Evergreen News Articles', 'abstract': 'Although the majority of news articles are only viewed for days or weeks, there are a small fraction of news articles that are read across years, thus named as evergreen news articles. Because evergreen articles maintain a timeless quality and are consistently of interests to the public, understanding their characteristics better has huge implications for news outlets and platforms yet there are few studies that have explicitly investigated on evergreen articles. Addressing this gap, in this paper, we first propose a flexible parameterized definition of evergreen articles to capture their long-term high traffic patterns. Using a real dataset from the Washington Post, then, we unearth several distinctive characteristics of evergreen articles and build an early prediction model with encouraging results. Although less than \\(1\\%\\) of news articles were identified as evergreen, our model achieves 0.961 in ROC AUC and 0.172 in PR AUC in 10-fold cross validation.', 'corpus_id': 197466931, 'score': 1}, {'doc_id': '235421752', 'title': 'Incomplete Gamma Integrals for Deep Cascade Prediction using Content, Network, and Exogenous Signals', 'abstract': 'The behavior of information cascades (such as retweets) has been modeled extensively. While point process-based generative models have long been in use for estimating cascade growths, deep learning has greatly enhanced diverse feature integration. We observe two significant temporal signals in cascade data that have not been emphasized or reported to our knowledge. First, the popularity of the cascade root is known to influence cascade size strongly; but the effect falls off rapidly with time. Second, there is a measurable positive correlation between the novelty of the root content (with respect to a streaming external corpus) and the relative size of the resulting cascade. Responding to these observations, we propose GammaCas, a new cascade growth model as a parametric function of time, which combines deep influence signals from content (e.g., tweet text), network features (e.g., followers of the root user), and exogenous event sources (e.g., online news). Specifically, our model processes these signals through a customized recurrent network, whose states then provide the parameters of the cascade rate function, which is integrated over time to predict the cascade size. The network parameters are trained end-to-end using observed cascades. GammaCas outperforms seven recent and diverse baselines significantly on a large-scale dataset of retweet cascades coupled with time-aligned online news — it beats the best baseline with 18.98% increase in terms of Kendall’s τ correlation and 35.63 reduction in Mean Absolute Percentage Error. Extensive ablation and case studies unearth interesting insights regarding retweet cascade dynamics.', 'corpus_id': 235421752, 'score': 0}, {'doc_id': '235294032', 'title': 'PP-Rec: News Recommendation with Personalized User Interest and Time-aware News Popularity', 'abstract': 'Personalized news recommendation methods are widely used in online news services. These methods usually recommend news based on the matching between news content and user interest inferred from historical behaviors. However, these methods usually have difficulties in making accurate recommendations to cold-start users, and tend to recommend similar news with those users have read. In general, popular news usually contain important information and can attract users with different interests. Besides, they are usually diverse in content and topic. Thus, in this paper we propose to incorporate news popularity information to alleviate the cold-start and diversity problems for personalized news recommendation. In our method, the ranking score for recommending a candidate news to a target user is the combination of a personalized matching score and a news popularity score. The former is used to capture the personalized user interest in news. The latter is used to measure timeaware popularity of candidate news, which is predicted based on news content, recency, and real-time CTR using a unified framework. Besides, we propose a popularity-aware user encoder to eliminate the popularity bias in user behaviors for accurate interest modeling. Experiments on two real-world datasets show our method can effectively improve the accuracy and diversity for news recommendation.', 'corpus_id': 235294032, 'score': 0}, {'doc_id': '235422421', 'title': 'Predicting the Popularity of Reddit Posts with AI', 'abstract': 'Social media creates crucial mass changes, as popular posts and opinions cast a significant influence on users’ decisions and thought processes. For example, the recent Reddit uprising inspired by r/wallstreetbets which had remarkable economic impact was started with a series of posts on the thread. The prediction of posts that may have a notable impact will allow for the preparation of possible following trends. This study aims to develop a machine learning model capable of accurately predicting the popularity of a Reddit post. Specifically, the model is predicting the number of upvotes a post will receive based on its textual content. I experimented with three different models: a baseline linear regression model, a random forest regression model, and a neural network. I collected Reddit post data from an online data set and analyzed the model’s performance when trained on a single subreddit and a collection of subreddits. The results showed that the neural network model performed the best when the loss of the models were compared. With the use of a machine learning model to predict social trends through the reaction users have to post, a better picture of the near future can be envisioned.', 'corpus_id': 235422421, 'score': 1}, {'doc_id': '235829852', 'title': 'Linking Health News to Research Literature', 'abstract': 'Accurately linking news articles to scientific research works is a critical component in a number of applications, such as measuring the social impact of a research work and detecting inaccuracies or distortions in science news. Although the lack of links between news and literature has been a challenge in these applications, it is a relatively unexplored research problem. In this paper we designed and evaluated a new approach that consists of (1) augmenting latest named-entity recognition techniques to extract various metadata, and (2) designing a new elastic search engine that can facilitate the use of enriched metadata queries. To evaluate our approach, we constructed two datasets of paired news articles and research papers: one is used for training models to extract metadata, and the other for evaluation. Our experiments showed that the new approach performed significantly better than a baseline approach used by altmetric.com (0.89 vs 0.32 in terms of top-1 accuracy). To further demonstrate the effectiveness of the approach, we also conducted a study on 37,600 health-related press releases published on EurekAlert!, which showed that our approach was able to identify the corresponding research papers with a top-1 accuracy of at least 0.97.', 'corpus_id': 235829852, 'score': 0}, {'doc_id': '12327099', 'title': 'Using early view patterns to predict the popularity of youtube videos', 'abstract': 'Predicting Web content popularity is an important task for supporting the design and evaluation of a wide range of systems, from targeted advertising to effective search and recommendation services. We here present two simple models for predicting the future popularity of Web content based on historical information given by early popularity measures. Our approach is validated on datasets consisting of videos from the widely used YouTube video-sharing portal. Our experimental results show that, compared to a state-of-the-art baseline model, our proposed models lead to significant decreases in relative squared errors, reaching up to 20% reduction on average, and larger reductions (of up to 71%) for videos that experience a high peak in popularity in their early days followed by a sharp decrease in popularity.', 'corpus_id': 12327099, 'score': 1}, {'doc_id': '236447821', 'title': 'Clickbait Detection in YouTube Videos', 'abstract': 'YouTube videos often include captivating descriptions and intriguing thumbnails designed to increase the number of views, and thereby increase the revenue for the person who posted the video. This creates an incentive for people to post clickbait videos, in which the content might deviate significantly from the title, description, or thumbnail. In effect, users are tricked into clicking on clickbait videos. In this research, we consider the challenging problem of detecting clickbait YouTube videos. We experiment with multiple state-of-the-art machine learning techniques using a variety of textual features.', 'corpus_id': 236447821, 'score': 0}, {'doc_id': '235446411', 'title': 'Personalized News Recommendation: A Survey', 'abstract': 'Personalized news recommendation is an important technique to help users find their interested news information and alleviate their information overload. It has been extensively studied over decades and has achieved notable success in improving users’ news reading experience. However, there are still many unsolved problems and challenges that need to be further studied. To help researchers master the advances in personalized news recommendation over the past years, in this paper we present a comprehensive overview of personalized news recommendation. Instead of following the conventional taxonomy of news recommendation methods, in this paper we propose a novel perspective to understand personalized news recommendation based on its core problems and the associated techniques and challenges. We first review the techniques for tackling each core problem in a personalized news recommender system and the challenges they face. Next, we introduce the public datasets and evaluation methods for personalized news recommendation. We then discuss the key points on improving the responsibility of personalized news recommender systems. Finally, we raise several research directions that are worth investigating in future. This paper can provide up-to-date and comprehensive views to help readers understand the personalized news recommendation field. We hope this paper can facilitate research on personalized news recommendation and as well as related fields in natural language processing and data mining.', 'corpus_id': 235446411, 'score': 0}, {'doc_id': '2570045', 'title': 'Predicting the Popularity of News Articles', 'abstract': 'Consuming news articles is an integral part of our daily lives and news agencies such as The Washington Post (WP) expend tremendous effort in providing high quality reading experiences for their readers. Journalists and editors are faced with the task of determining which articles will become popular so that they can efficiently allocate resources to support a better reading experience. The reasons behind the popularity of news articles are typically varied, and might involve contemporariness, writing quality, and other latent factors. In this paper, we cast the problem of popularity prediction problem as regression, engineer several classes of features (metadata, contextual or content-based, temporal, and social), and build models for forecasting popularity. The system presented here is deployed in a real setting at The Washington Post; we demonstrate that it is able to accurately predict article popularity with an R ≈ 0.8 using features harvested within 30 minutes of publication time.', 'corpus_id': 2570045, 'score': 1}]
120	Scientific DM	3ff5182ada8bc3f9f0c88d9f0b4a7415	18785	{}	"[{'doc_id': '235351378', 'title': 'Stimulating Marketing Strategy Innovation with Entrepreneurs in Uganda: Examining the Impact of Skype-aided Business Coaching on Firm Sales', 'abstract': 'This paper studies the impact of Skype-aided remote business coaching on the strategies and sales of emerging market entrepreneurs. It sheds light on three novel research questions: (1) What is the effect of remote business coaching on firm sales? (2) What is the mechanism through which this effect occurs; specifically, does remote coaching stimulate changes in marketing strategies (pivots)? (3) Do entrepreneurs benefit more from remote coaching when they are less strategic in their decision-making? We conducted a randomized controlled field experiment with 930 entrepreneurs in Uganda to examine the impact of a remote coaching intervention that connects management professionals in primarily advanced markets and entrepreneurs in emerging markets with the aim of improving business performance. The analysis finds a positive and significant main effect on firm sales – treated entrepreneurs increase monthly sales by 27.6% on average. In addition, entrepreneurs who receive remote coaching are 63.3% more likely to have “pivoted” or shifted their marketing strategy in a new direction. And consistent with this mechanism of inducing strategic business changes, the results show that entrepreneurs who receive remote coaching tend to do better when they (ex ante) lack strategic focus. These results have important implications for the development of marketing strategies by entrepreneurs and multinational managers, as well as for organizations interested in improving the performance of small firms in emerging markets and beyond.', 'corpus_id': 235351378, 'score': 0}, {'doc_id': '235079323', 'title': 'Design and Evaluation of Optimal Free Trials', 'abstract': 'Free trial promotions are a commonly used customer acquisition strategy in the Software as a Service (SaaS) industry. We use data from a large-scale field experiment conducted by a leading SaaS firm to study the effect of trial length on outcomes and the returns from personalized trial length assignment. We find that the 7-days trial is the best average treatment that maximizes customer acquisition, retention, and profitability. In terms of mechanism, we rule out the demand cannibalization theory, find support for the consumer learning hypothesis, and show that long stretches of inactivity at the end of the trial are associated with lower conversions. We then develop a framework for personalized targeting policy design and evaluation. We first learn a lasso model of outcomes as a function of users’ pre-treatment variables and treatment. Next, we use individual-level predictions of the outcome to assign the optimal treatment to each user. We then evaluate the personalized policy using the inverse propensity score reward estimator. We find that a personalization based on lasso leads to 6.8% improvement in subscription compared to a uniform 30-days for all policy. It also performs well on long-term customer retention and revenues. Segmentation analysis suggests that skilled and experienced users are more likely to benefit from longer trials. Finally, we see that personalized policies based on many outcome estimators and heterogeneous treatment effects estimators (e.g., generalized random forests) perform poorly. This suggests that adopting a simple uniform policy can be better than personalizing policies based on these methods.', 'corpus_id': 235079323, 'score': 0}, {'doc_id': '235657886', 'title': 'Go West Young Firm: The Benefits of Startup Relocation to Silicon Valley', 'abstract': 'I study the benefits to entrepreneurial migration, focused on firms moving to Silicon Valley. Using a machine learning estimator and panel data, I find moving to Silicon Valley leads to higher startup performance on equity outcomes, financing, patenting, products, and revenue. These results are robust to a stringent coefficient stability test, and show no evidence of pre-trends. The benefits are partially driven by knowledge spillovers, and sensitive to capital market conditions during migration. Despite the positive benefits to migration, most startups do not move. A simple analysis suggests this may be due to the personal costs of moving for founders themselves.', 'corpus_id': 235657886, 'score': 0}, {'doc_id': '169673212', 'title': 'Experimental Evidence of a Scientific Approach to Decision Making of Entrepreneurial Firms', 'abstract': 'This study examines the impact of a scientific approach to decision making on early-stage entrepreneurial firms. In these early-stage projects, we argue that using a scientific approach to decision making leads to a higher chance to uncover false positives and false negatives associated with new business ideas. We hypothesize that this precision helps entrepreneurs to understand both if their ideas have negative financial returns and if tweaking their idea (pivot) can lead to more positive financial returns. We embed a field experiment in a pre-accelerator program geared towards early-stage entrepreneurial firms, and use this initiative to provide treatment to a group of entrepreneurs (130 entrepreneurial teams) by training them on how to use a scientific approach to business development. The control group (128 entrepreneurial teams) receives the same amount and type of training, but is not taught to use a scientific approach to business development. Consistently with our predictions, we find that the use...', 'corpus_id': 169673212, 'score': 1}, {'doc_id': '235375442', 'title': 'Experimentation and Incrementalism: The Impact of the Adoption of A/B Testing', 'abstract': 'This paper studies how the adoption of experimentation as a selection method shapes the direction of innovation. The spread of A/B testing, or digital randomized experiments, has made experimentation one of the most common methods organizations use to evaluate and select internally generated ideas. The strength of experimental evidence can be a force for radical innovation. By providing seemingly irrefutable evidence, well-designed and wellexecuted experiments can disabuse people of their false beliefs and generate breakthrough discoveries. However, I argue that the adoption of experimentation can also result in incrementalism, whereby firms focus on minuscule yet reliable improvements. These divergent outcomes can be explained by the incentives driving the people who design and implement experiments. The incentives of managers in established firms may lead them to use experiments in a way that undermines the pursuit of novelty while encouraging the search for incremental improvements. I investigate the relationship between experimentation and innovation in the context of US newspaper websites and their adoption of A/B testing. Using a historical archive of US newspaper websites and a novel computational method, I find that the adoption of A/B testing decreases the likelihood of radical change and makes websites more likely to change incrementally. ∗I am grateful to Jesper B. Sørensen, Julien Clement, William Barnett, J.P. Eggers, Hazjier Pourkhalkhali, seminar participants at Stanford GSB, Academy of Management, University of Chicago Computational Social Science Workshop, UC Berkeley Haas School of Business, Wisconsin Business School, University of Toronto Rotman School of Management, Harvard Business School, UCLA Anderson, UCL School of Management, Sabanci University, HEC Paris, Trans Atlantic Doctoral Conference, and Organizational Theory and Economic Sociology Conference for their helpful comments and feedback. The content is solely the responsibility of the author and does not represent the affiliated institutions.', 'corpus_id': 235375442, 'score': 1}, {'doc_id': '235195356', 'title': 'Board Reforms and Innovation', 'abstract': ""We study the effect of board reforms on firms' research and development investments utilizing a sample of 40 countries. Using a difference-in-differences analysis, we find that firms' invest more in research and development following corporate governance reforms. Of these, two reforms - having an independent audit committee and board independence - have a greater impact on innovation. Additionally, we show that reforms have the largest impact on research and development investment in hi-tech industries and the health sector."", 'corpus_id': 235195356, 'score': 0}, {'doc_id': '235312261', 'title': 'CESifo Working Paper no. 6642', 'abstract': 'Today, startups often obtain financing via the Internet through many small contributions of nonsophisticated investors. Yet little is known about whether these startups can ultimately build enduring businesses. In this article, we hand-collected data from 14 different equity crowdfunding (ECF) portals and 426 firms that ran at least one successful ECF campaign in Germany or the United Kingdom. We empirically analyze different factors affecting follow-up funding and firm failure. The findings show that German firms that received ECF stood a higher chance of obtaining follow-up funding through business angels or venture capitalists, but also had a higher likelihood of failure. The number of senior managers, subsequent successful ECF campaigns, and the number of venture capital investors all had a positive impact on obtaining post-campaign financing, while firm age had a negative impact. Subsequent successful ECF campaigns were significant predictors decreasing firm failure. JEL-Codes: G240, M130.', 'corpus_id': 235312261, 'score': 1}, {'doc_id': '235783834', 'title': 'Fear of the unknown - The effect of economic policy uncertainty on start-up financing and success', 'abstract': 'This paper investigates the effect of economic policy uncertainty on the financing and success probability of start-ups in the European venture capital market. Specifically, our results show that venture capital investors are less likely to engage in financing activities and start-ups are less likely to have a successful exit under high economic policy uncertainty. Even after controlling for economic uncertainty and industry trends, our results remain stable and significant, highlighting the importance of uncertainty surrounding government policies. However, we point out the special role of governmental venture capital as it bridges the financing gap for start-ups under high levels of economic policy uncertainty. Besides, governments seem to be better investors facing higher uncertainty leading to crucial implications for government policies. JEL classification: G24, G28, G34, H11', 'corpus_id': 235783834, 'score': 0}, {'doc_id': '149314182', 'title': 'A Scientific Approach to Entrepreneurial Decision-Making: Evidence from a Randomized Control Trial', 'abstract': 'A classical approach to collecting and elaborating information to make entrepreneurial decisions combines search heuristics, such as trial and error, effectuation, and confirmatory search. This paper develops a framework for exploring the implications of a more scientific approach to entrepreneurial decision making. The panel sample of our randomized control trial includes 116 Italian startups and 16 data points over a period of about one year. Both the treatment and control groups receive 10 sessions of general training on how to obtain feedback from the market and gauge the feasibility of their idea. We teach the treated startups to develop frameworks for predicting the performance of their idea and conduct rigorous tests of their hypotheses, very much as scientists do in their research. We let the firms in the control group instead follow their intuitions about how to assess their idea, which has typically produced fairly standard search heuristics. We find that entrepreneurs who behave like scientists perform better, are more likely to pivot to a different idea, and are not more likely to drop out than the control group in the early stages of the startup. These results are consistent with the main prediction of our theory: a scientific approach improves precision—it reduces the odds of pursuing projects with false positive returns and increases the odds of pursuing projects with false negative returns.', 'corpus_id': 149314182, 'score': 1}, {'doc_id': '164710866', 'title': 'Small Changes with Big Impact: Experimental Evidence of a Scientific Approach to Decision-Making of Entrepreneurs', 'abstract': 'This study examines the impact of a scientific approach to decision-making on early-stage entrepreneurial firms. We argue that using a scientific approach to decision-making increases the probabili...', 'corpus_id': 164710866, 'score': 1}]"
121	StyleGAN Face Editing	83adc0d2c4dcf800e792b03fe0750c84	12537	{}	"[{'doc_id': '229924297', 'title': 'OSTeC: One-Shot Texture Completion', 'abstract': 'The last few years have witnessed the great success of non-linear generative models in synthesizing high-quality photorealistic face images. Many recent 3D facial texture reconstruction and pose manipulation from a single image approaches still rely on large and clean face datasets to train image-to-image Generative Adversarial Networks (GANs). Yet the collection of such a large scale highresolution 3D texture dataset is still very costly and difficult to maintain age/ethnicity balance. Moreover, regressionbased approaches suffer from generalization to the in-thewild conditions and are unable to fine-tune to a targetimage. In this work, we propose an unsupervised approach for one-shot 3D facial texture completion that does not require large-scale texture datasets, but rather harnesses the knowledge stored in 2D face generators. The proposed approach rotates an input image in 3D and fill-in the unseen regions by reconstructing the rotated image in a 2D face generator, based on the visible parts. Finally, we stitch the most visible textures at different angles in the UV imageplane. Further, we frontalize the target image by projecting the completed texture into the generator. The qualitative and quantitative experiments demonstrate that the completed UV textures and frontalized images are of high quality, resembles the original identity, can be used to train a texture GAN model for 3DMM fitting and improve poseinvariant face recognition.1', 'corpus_id': 229924297, 'score': 0}, {'doc_id': '231698400', 'title': 'cGANs for Cartoon to Real-life Images', 'abstract': 'Image-to-image translation is a learning task to establish visual mapping between an input and output image. The task has several variations differentiated based on the purpose of the translation, such as synthetic− →real translation [17][21], photo− →caricature translation [23] and many others. The problem has been tackled using different approaches, either through traditional computer vision methods [7], as well as deep learning approaches in recent trends. One approach currently deemed popular and effective is using conditional generative adversarial network, also known shortly as cGAN [15]. It is adapted to perform image-to-image translation tasks with typically two networks: a generator and a discriminator[10]. The generator attempts to generate a duplicated imitation of the input data distribution from a noise distribution while the discriminator classifies whether the generator’s input is fake, i.e. imitation from the generated distribution, or real, i.e. ground truth from the original distribution. Previous research has focused on specific purpose for cross-image translation. Efros et al. [4] proposed a simple model to synthesize an image based on stitches of input images, which represents a traditional approach directly from input image. Fergus et al. [5] addressed specific problems of blurry image into higher crispness, which provided insights on retaining the crispness of duplicated imitation. Additionally, through user studies and an automated mechanism to select images, Chen et al. [3] developed a model to optimize image selection process before cross-image translation. This project is based on an existing implementation of a network called Pix2Pix[10], a cGAN implementation based on U-Net architecture and convolution Markovian discriminator. The use of U-Net architecture [19], instead of the general encoder-decoder architecture is to improve the efficiency of processing input and output with higher resolution while facilitating the transfer of shared, low-level information directly across layers. The U-Net architecture enables cross-layer communication and therefore could more effi-', 'corpus_id': 231698400, 'score': 0}, {'doc_id': '229340598', 'title': 'An Assessment of GANs for Identity-related Applications', 'abstract': 'Generative Adversarial Networks (GANs) are now capable of producing synthetic face images of exceptionally high visual quality. In parallel to the development of GANs themselves, efforts have been made to develop metrics to objectively assess the characteristics of the synthetic images, mainly focusing on visual quality and the variety of images. Little work has been done, however, to assess overfitting of GANs and their ability to generate new identities. In this paper we apply a state of the art biometric network to various datasets of synthetic images and perform a thorough assessment of their identity-related characteristics. We conclude that GANs can indeed be used to generate new, imagined identities meaning that applications such as anonymisation of image sets and augmentation of training datasets with distractor images are viable applications. We also assess the ability of GANs to disentangle identity from other image characteristics and propose a novel GAN triplet loss that we show to improve this disentanglement.', 'corpus_id': 229340598, 'score': 1}, {'doc_id': '227151907', 'title': 'Unsupervised Discovery of Disentangled Manifolds in GANs', 'abstract': 'As recent generative models can generate photo-realistic images, people seek to understand the mechanism behind the generation process. Interpretable generation process is beneficial to various image editing applications. In this work, we propose a framework to discover interpretable directions in the latent space given arbitrary pre-trained generative adversarial networks. We propose to learn the transformation from prior one-hot vectors representing different attributes to the latent space used by pre-trained models. Furthermore, we apply a centroid loss function to improve consistency and smoothness while traversing through different directions. We demonstrate the efficacy of the proposed framework on a wide range of datasets. The discovered direction vectors are shown to be visually corresponding to various distinct attributes and thus enable attribute editing.', 'corpus_id': 227151907, 'score': 1}, {'doc_id': '227161905', 'title': 'StyleUV: Diverse and High-fidelity UV Map Generative Model', 'abstract': 'Reconstructing 3D human faces in the wild with the 3D Morphable Model (3DMM) has become popular in recent years. While most prior work focuses on estimating more robust and accurate geometry, relatively little attention has been paid to improving the quality of the texture model. Meanwhile, with the advent of Generative Adversarial Networks (GANs), there has been great progress in reconstructing realistic 2D images. Recent work demonstrates that GANs trained with abundant high-quality UV maps can produce high-fidelity textures superior to those produced by existing methods. However, acquiring such high-quality UV maps is difficult because they are expensive to acquire, requiring laborious processes to refine. In this work, we present a novel UV map generative model that learns to generate diverse and realistic synthetic UV maps without requiring high-quality UV maps for training. Our proposed framework can be trained solely with in-the-wild images (i.e., UV maps are not required) by leveraging a combination of GANs and a differentiable renderer. Both quantitative and qualitative evaluations demonstrate that our proposed texture model produces more diverse and higher fidelity textures compared to existing methods.', 'corpus_id': 227161905, 'score': 0}, {'doc_id': '221083302', 'title': 'Face identity disentanglement via latent space mapping', 'abstract': 'Learning disentangled representations of data is a fundamental problem in artificial intelligence. Specifically, disentangled latent representations allow generative models to control and compose the disentangled factors in the synthesis process. Current methods, however, require extensive supervision and training, or instead, noticeably compromise quality. In this paper, we present a method that learns how to represent data in a disentangled way, with minimal supervision, manifested solely using available pre-trained networks. Our key insight is to decouple the processes of disentanglement and synthesis, by employing a leading pre-trained unconditional image generator, such as StyleGAN. By learning to map into its latent space, we leverage both its state-of-the-art quality, and its rich and expressive latent space, without the burden of training it. We demonstrate our approach on the complex and high dimensional domain of human heads. We evaluate our method qualitatively and quantitatively, and exhibit its success with de-identification operations and with temporal identity coherency in image sequences. Through extensive experimentation, we show that our method successfully disentangles identity from other facial attributes, surpassing existing methods, even though they require more training and supervision.', 'corpus_id': 221083302, 'score': 1}, {'doc_id': '225039998', 'title': 'Few-Shot Adaptation of Generative Adversarial Networks', 'abstract': 'Generative Adversarial Networks (GANs) have shown remarkable performance in image synthesis tasks, but typically require a large number of training samples to achieve high-quality synthesis. This paper proposes a simple and effective method, Few-Shot GAN (FSGAN), for adapting GANs in few-shot settings (less than 100 images). FSGAN repurposes component analysis techniques and learns to adapt the singular values of the pre-trained weights while freezing the corresponding singular vectors. This provides a highly expressive parameter space for adaptation while constraining changes to the pretrained weights. We validate our method in a challenging few-shot setting of 5-100 images in the target domain. We show that our method has significant visual quality gains compared with existing GAN adaptation methods. We report qualitative and quantitative results showing the effectiveness of our method. We additionally highlight a problem for few-shot synthesis in the standard quantitative metric used by data-efficient image synthesis works. Code and additional results are available at this http URL.', 'corpus_id': 225039998, 'score': 1}, {'doc_id': '227338901', 'title': 'TediGAN: Text-Guided Diverse Image Generation and Manipulation', 'abstract': 'In this work, we propose TediGAN, a novel framework for multi-modal image generation and manipulation with textual descriptions. The proposed method consists of three components: StyleGAN inversion module, visual-linguistic similarity learning, and instance-level optimization. The inversion module is to train an image encoder to map real images to the latent space of a well-trained StyleGAN. The visual-linguistic similarity is to learn the text-image matching by mapping the image and text into a common embedding space. The instance-level optimization is for identity preservation in manipulation. Our model can provide the lowest effect guarantee, and produce diverse and high-quality images with an unprecedented resolution at 1024. Using a control mechanism based on style-mixing, our TediGAN inherently supports image synthesis with multi-modal inputs, such as sketches or semantic labels with or without instance (text or real image) guidance. To facilitate text-guided multi-modal synthesis, we propose the Multi-Modal CelebA-HQ, a large-scale dataset consisting of real face images and corresponding semantic segmentation map, sketch, and textual descriptions. Extensive experiments on the introduced dataset demonstrate the superior performance of our proposed method. Code and data are available at https://github.com/weihaox/TediGAN.', 'corpus_id': 227338901, 'score': 0}, {'doc_id': '214743268', 'title': 'StyleRig: Rigging StyleGAN for 3D Control Over Portrait Images', 'abstract': ""StyleGAN generates photorealistic portrait images of faces with eyes, teeth, hair and context (neck, shoulders, background), but lacks a rig-like control over semantic face parameters that are interpretable in 3D, such as face pose, expressions, and scene illumination. Three-dimensional morphable face models (3DMMs) on the other hand offer control over the semantic parameters, but lack photorealism when rendered and only model the face interior, not other parts of a portrait image (hair, mouth interior, background). We present the first method to provide a face rig-like control over a pretrained and fixed StyleGAN via a 3DMM. A new rigging network, \\textit{RigNet} is trained between the 3DMM's semantic parameters and StyleGAN's input. The network is trained in a self-supervised manner, without the need for manual annotations. At test time, our method generates portrait images with the photorealism of StyleGAN and provides explicit control over the 3D semantic parameters of the face."", 'corpus_id': 214743268, 'score': 1}, {'doc_id': '229298022', 'title': 'Self-Supervised Sketch-to-Image Synthesis', 'abstract': ""Imagining a colored realistic image from an arbitrarily drawn sketch is one of the human capabilities that we eager machines to mimic. Unlike previous methods that either requires the sketch-image pairs or utilize low-quantity detected edges as sketches, we study the exemplar-based sketch-to-image (s2i) synthesis task in a self-supervised learning manner, eliminating the necessity of the paired sketch data. To this end, we first propose an unsupervised method to efficiently synthesize line-sketches for general RGB-only datasets. With the synthetic paired-data, we then present a self-supervised Auto-Encoder (AE) to decouple the content/style features from sketches and RGB-images, and synthesize images that are both content-faithful to the sketches and style-consistent to the RGB-images. While prior works employ either the cycle-consistence loss or dedicated attentional modules to enforce the content/style fidelity, we show AE's superior performance with pure self-supervisions. To further improve the synthesis quality in high resolution, we also leverage an adversarial network to refine the details of synthetic images. Extensive experiments on 1024*1024 resolution demonstrate a new state-of-art-art performance of the proposed model on CelebA-HQ and Wiki-Art datasets. Moreover, with the proposed sketch generator, the model shows a promising performance on style mixing and style transfer, which require synthesized images to be both style-consistent and semantically meaningful. Our code is available on this https URL, and please visit this https URL for an online demo of our model."", 'corpus_id': 229298022, 'score': 0}]"
122	RCAI	226125391c686a54b1778b918525583b	18226	{'RCAI': 'Ricinus communis agglutinin I'}	[{'doc_id': '235212488', 'title': 'Joint-DetNAS: Upgrade Your Detector with NAS, Pruning and Dynamic Distillation', 'abstract': 'We propose Joint-DetNAS, a unified NAS framework for object detection, which integrates 3 key components: Neural Architecture Search, pruning, and Knowledge Distillation. Instead of naively pipelining these techniques, our Joint-DetNAS optimizes them jointly. The algorithm consists of two core processes: student morphism optimizes the student’s architecture and removes the redundant parameters, while dynamic distillation aims to find the optimal matching teacher. For student morphism, weight inheritance strategy is adopted, allowing the student to flexibly update its architecture while fully utilize the predecessor’s weights, which considerably accelerates the search; To facilitate dynamic distillation, an elastic teacher pool is trained via integrated progressive shrinking strategy, from which teacher detectors can be sampled without additional cost in subsequent searches. Given a base detector as the input, our algorithm directly outputs the derived student detector with high performance without additional training. Experiments demonstrate that our Joint-DetNAS outperforms the naive pipelining approach by a great margin. Given a classic R101-FPN as the base detector, JointDetNAS is able to boost its mAP from 41.4 to 43.9 on MS COCO and reduce the latency by 47%, which is on par with the SOTA EfficientDet while requiring less search cost. We hope our proposed method can provide the community with a new way of jointly optimizing NAS, KD and pruning.', 'corpus_id': 235212488, 'score': 1}, {'doc_id': '234787856', 'title': 'Bayesian Differentiable Architecture Search for Efficient Domain Matching Fault Diagnosis', 'abstract': 'Intelligent fault diagnosis is essential for downtime reduction in modern industries. However, domain (i.e., working condition) variants are unavoidable for fault diagnosis under changing environment. Though one can obtain a customized deep learning model for a given domain, it is not practical to design deep neural networks manually for multiple domains. Differentiable architecture search (DARTS), as an automatic machine learning technique, can automate the network design process for a specific domain efficiently by using hypernetwork and differentiable search strategy. Nevertheless, the representation of hypernetwork is restricted by several unfair factors, and the searched architecture of DARTS is overconfident. To address these issues, a one-shot neural architecture search approach, which involves two-stage learning, is proposed for efficient domain matching fault diagnosis. In the first training stage, the warmup and path-dropout strategies are taken to enhance the competitiveness of the parametric operators and alleviate the coadaptation problem to obtain an intuitively fair hypernetwork. In the second matching stage, variational inference is introduced into a differentiable search strategy to estimate the uncertainty of model matching, and a scale mixture prior is used to softly constrain the matching stage. Then, a candidate architectures set can be sampled and ordered from the posterior. Multidomain experiment is implemented by adding noise to the raw signal, and the proposed method outperforms four commonly used deep neural networks for aeroengine bevel gear fault diagnosis.', 'corpus_id': 234787856, 'score': 0}, {'doc_id': '139104282', 'title': 'Progressive Differentiable Architecture Search: Bridging the Depth Gap Between Search and Evaluation', 'abstract': 'Recently, differentiable search methods have made major progress in reducing the computational costs of neural architecture search. However, these approaches often report lower accuracy in evaluating the searched architecture or transferring it to another dataset. This is arguably due to the large gap between the architecture depths in search and evaluation scenarios. In this paper, we present an efficient algorithm which allows the depth of searched architectures to grow gradually during the training procedure. This brings two issues, namely, heavier computational overheads and weaker search stability, which we solve using search space approximation and regularization, respectively. With a significantly reduced search time (~7 hours on a single GPU), our approach achieves state-of-the-art performance on both the proxy dataset (CIFAR10 or CIFAR100) and the target dataset (ImageNet). Code is available at https://github.com/chenxin061/pdarts', 'corpus_id': 139104282, 'score': 1}, {'doc_id': '234336288', 'title': 'Performance Analysis of Deep Neural Network based on Transfer Learning for Pet Classification', 'abstract': 'Deep learning frameworks have progressed beyond human recognition capabilities and, now it’s the perfect opportunity to optimize them for implementation on the embedded platforms. The present deep learning architectures support learning capabilities, but they lack flexibility for applying learned knowledge on the tasks in other unfamiliar domains. This work tries to fill this gap with the deep neural network-based solution for object detection in unrelated domains with a focus on the reduced footprint of the developed model. Knowledge distillation provides efficient and effective teacher-student learning for a variety of different visual recognition tasks. A lightweight student network can be easily trained under the guidance of the high-capacity teacher networks. The teacher-student architecture implementation on binary classes shows a 20% improvement in accuracy within the same training iterations using the transfer learning approach. The scalability of the student model is tested with binary, ternary and multiclass and their performance is compared on basis of inference speed. The results show that the inference speed does not depend on the number of classes. For similar recognition accuracy, the inference speed of about 50 frames per second or 20ms per image. Thus, this approach can be generalized as per the application requirement with minimal changes, provided the dataset format compatibility.', 'corpus_id': 234336288, 'score': 0}, {'doc_id': '235632013', 'title': 'Stabilizing Equilibrium Models by Jacobian Regularization', 'abstract': 'Deep equilibrium networks (DEQs) are a new class of models that eschews traditional depth in favor of finding the fixed point of a single nonlinear layer. These models have been shown to achieve performance competitive with the stateof-the-art deep networks while using significantly less memory. Yet they are also slower, brittle to architectural choices, and introduce potential instability to the model. In this paper, we propose a regularization scheme for DEQ models that explicitly regularizes the Jacobian of the fixed-point update equations to stabilize the learning of equilibrium models. We show that this regularization adds only minimal computational cost, significantly stabilizes the fixed-point convergence in both forward and backward passes, and scales well to high-dimensional, realistic domains (e.g., WikiText-103 language modeling and ImageNet classification). Using this method, we demonstrate, for the first time, an implicit-depth model that runs with approximately the same speed and level of performance as popular conventional deep networks such as ResNet-101, while still maintaining the constant memory footprint and architectural simplicity of DEQs. Code is available here.', 'corpus_id': 235632013, 'score': 0}, {'doc_id': '233210099', 'title': 'MobileStyleGAN: A Lightweight Convolutional Neural Network for High-Fidelity Image Synthesis', 'abstract': 'In recent years, the use of Generative Adversarial Networks (GANs) has become very popular in generative image modeling. While style-based GAN architectures yield stateof-the-art results in high-fidelity image synthesis, computationally, they are highly complex. In our work, we focus on the performance optimization of style-based generative models. We analyze the most computationally hard parts of StyleGAN2, and propose changes in the generator network to make it possible to deploy style-based generative networks in the edge devices. We introduce MobileStyleGAN architecture, which has x3.5 fewer parameters and is x9.5 less computationally complex than StyleGAN2, while providing comparable quality.', 'corpus_id': 233210099, 'score': 1}, {'doc_id': '235356070', 'title': 'COMPACTER: Efficient Low-Rank Hypercomplex Adapter Layers', 'abstract': 'Adapting large-scale pretrained language models to downstream tasks via 1 fine-tuning is the standard method for achieving state-of-the-art performance on 2 NLP benchmarks. However, fine-tuning all weights of models with millions or 3 billions of parameters is sample-inefficient, unstable in low-resource settings, and 4 wasteful as it requires storing a separate copy of the model for each task. Recent 5 work has developed parameter-efficient fine-tuning methods, but these approaches 6 either still require a relatively large number of parameters or underperform standard 7 fine-tuning. In this work, we propose COMPACTER, a method for fine-tuning 8 large-scale language models with a better trade-off between task performance and 9 the number of trainable parameters than prior work. COMPACTER accomplishes this 10 by building on top of ideas from adapters, low-rank optimization, and parameterized 11 hypercomplex multiplication layers. 12 Specifically, COMPACTER inserts task-specific weight matrices into a pretrained 13 model’s weights, which are computed efficiently as a sum of Kronecker products be14 tween shared “slow” weights and “fast” rank-one matrices defined per COMPACTER 15 layer. By only training 0.047% of a pretrained model’s parameters, COMPACTER 16 performs on par with standard fine-tuning on GLUE and outperforms fine-tuning 17 in low-resource settings. 18', 'corpus_id': 235356070, 'score': 0}, {'doc_id': '3638969', 'title': 'Efficient Neural Architecture Search via Parameter Sharing', 'abstract': 'We propose Efficient Neural Architecture Search (ENAS), a fast and inexpensive approach for automatic model design. In ENAS, a controller learns to discover neural network architectures by searching for an optimal subgraph within a large computational graph. The controller is trained with policy gradient to select a subgraph that maximizes the expected reward on the validation set. Meanwhile the model corresponding to the selected subgraph is trained to minimize a canonical cross entropy loss. Thanks to parameter sharing between child models, ENAS is fast: it delivers strong empirical performances using much fewer GPU-hours than all existing automatic model design approaches, and notably, 1000x less expensive than standard Neural Architecture Search. On the Penn Treebank dataset, ENAS discovers a novel architecture that achieves a test perplexity of 55.8, establishing a new state-of-the-art among all methods without post-training processing. On the CIFAR-10 dataset, ENAS designs novel architectures that achieve a test error of 2.89%, which is on par with NASNet (Zoph et al., 2018), whose test error is 2.65%.', 'corpus_id': 3638969, 'score': 1}, {'doc_id': '235265630', 'title': 'Training ELECTRA Augmented with Multi-word Selection', 'abstract': 'Pre-trained text encoders such as BERT and its variants have recently achieved state-of-the-art performances on many NLP tasks. While being effective, these pre-training methods typically demand massive computation resources. To accelerate pre-training, ELECTRA trains a discriminator that predicts whether each input token is replaced by a generator. However, this new task, as a binary classification, is less semantically informative. In this study, we present a new text encoder pre-training method that improves ELECTRA based on multi-task learning. Specifically, we train the discriminator to simultaneously detect replaced tokens and select original tokens from candidate sets. We further develop two techniques to effectively combine all pre-training tasks: (1) using attention-based networks for task-specific heads, and (2) sharing bottom layers of the generator and the discriminator. Extensive experiments on GLUE and SQuAD datasets demonstrate both the effectiveness and the efficiency of our proposed method.', 'corpus_id': 235265630, 'score': 0}, {'doc_id': '208513081', 'title': 'Block-Wisely Supervised Neural Architecture Search With Knowledge Distillation', 'abstract': 'Neural Architecture Search (NAS), aiming at automatically designing network architectures by machines, is expected to bring about a new revolution in machine learning. Despite these high expectation, the effectiveness and efficiency of existing NAS solutions are unclear, with some recent works going so far as to suggest that many existing NAS solutions are no better than random architecture selection. The ineffectiveness of NAS solutions may be attributed to inaccurate architecture evaluation. Specifically, to speed up NAS, recent works have proposed under-training different candidate architectures in a large search space concurrently by using shared network parameters; however, this has resulted in incorrect architecture ratings and furthered the ineffectiveness of NAS. In this work, we propose to modularize the large search space of NAS into blocks to ensure that the potential candidate architectures are fully trained; this reduces the representation shift caused by the shared parameters and leads to the correct rating of the candidates. Thanks to the block-wise search, we can also evaluate all of the candidate architectures within each block. Moreover, we find that the knowledge of a network model lies not only in the network parameters but also in the network architecture. Therefore, we propose to distill the neural architecture (DNA) knowledge from a teacher model to supervise our block-wise architecture search, which significantly improves the effectiveness of NAS. Remarkably, the performance of our searched architectures has exceeded the teacher model, demonstrating the practicability of our method. Finally, our method achieves a state-of-the-art 78.4% top-1 accuracy on ImageNet in a mobile setting. All of our searched models along with the evaluation code are available at https://github.com/changlin31/DNA.', 'corpus_id': 208513081, 'score': 1}]
123	Human-in-the-loop	b2f1590c5c9c9e5e5f5b88bec0190654	1148	{}	"[{'doc_id': '31444613', 'title': ""Process analysis of judges' commitment decisions: a preliminary empirical study."", 'abstract': 'The current debate over the ""police powers"" versus parens patriae rationales for involuntary hospitalization of the mentally ill underscores the need for empirical study of the process of judicial decision making in civil commitment and determinations of competence. The authors report the ratings on 26 descriptive variables made by five Massachusetts district court judges for 35 patients in civil commitment hearings. Nearly all of the hearings resulted in commitments. These findings suggest that psychiatrists may be setting too high a threshold for petitioning for commitment. Experienced judges appeared to be sensitive to the kinds of clinical issues that earlier studies have shown to contribute significantly to the psychiatrist\'s decision to petition for commitment.', 'corpus_id': 31444613, 'score': 0}, {'doc_id': '8943607', 'title': 'Principles of mixed-initiative user interfaces', 'abstract': 'Recent debate has centered on the relative promise of focusinguser-interface research on developing new metaphors and tools thatenhance users abilities to directly manipulate objects versusdirecting effort toward developing interface agents that provideautomation. In this paper, we review principles that show promisefor allowing engineers to enhance human-computer interactionthrough an elegant coupling of automated services with directmanipulation. Key ideas will be highlighted in terms of the Lookoutsystem for scheduling and meeting management.', 'corpus_id': 8943607, 'score': 1}, {'doc_id': '220381441', 'title': 'PinnerSage: Multi-Modal User Embedding Framework for Recommendations at Pinterest', 'abstract': ""Latent user representations are widely adopted in the tech industry for powering personalized recommender systems. Most prior work infers a single high dimensional embedding to represent a user, which is a good starting point but falls short in delivering a full understanding of the user's interests. In this work, we introduce PinnerSage, an end-to-end recommender system that represents each user via multi-modal embeddings and leverages this rich representation of users to provides high quality personalized recommendations. PinnerSage achieves this by clustering users' actions into conceptually coherent clusters with the help of a hierarchical clustering method (Ward) and summarizes the clusters via representative pins (Medoids) for efficiency and interpretability. PinnerSage is deployed in production at Pinterest and we outline the several design decisions that makes it run seamlessly at a very large scale. We conduct several offline and online A/B experiments to show that our method significantly outperforms single embedding methods."", 'corpus_id': 220381441, 'score': 0}, {'doc_id': '220380881', 'title': 'Modeling and Mitigating Human Annotation Errors to Design Efficient Stream Processing Systems with Human-in-the-loop Machine Learning', 'abstract': 'High-quality human annotations are necessary for creating effective machine learning-driven stream processing systems. We study hybrid stream processing systems based on a Human-In-The-Loop Machine Learning (HITL-ML) paradigm, in which one or many human annotators and an automatic classifier (trained at least partially by the human annotators) label an incoming stream of instances. This is typical of many near-real time social media analytics and web applications, including the annotation of social media posts during emergencies by digital volunteer groups. From a practical perspective, low-quality human annotations result in wrong labels for retraining automated classifiers and indirectly contribute to the creation of inaccurate classifiers. \nConsidering human annotation as a psychological process allows us to address these limitations. We show that human annotation quality is dependent on the ordering of instances shown to annotators, and can be improved by local changes in the instance sequence/ordering provided to the annotators, yielding a more accurate annotation of the stream. We design a theoretically-motivated human error framework for the human annotation task to study the effect of ordering instances (i.e., an ""annotation schedule""). Further, we propose an error-avoidance approach to the active learning (HITL-ML) paradigm for stream processing applications that is robust to these likely human errors when deciding a human annotation schedule. We validate the human error framework using crowdsourcing experiments and evaluate the proposed algorithm against standard baselines for active learning via extensive experimentation on classification tasks of filtering relevant social media posts during natural disasters.', 'corpus_id': 220380881, 'score': 1}, {'doc_id': '220364234', 'title': 'Tree-Augmented Cross-Modal Encoding for Complex-Query Video Retrieval', 'abstract': 'The rapid growth of user-generated videos on the Internet has intensified the need for text-based video retrieval systems. Traditional methods mainly favor the concept-based paradigm on retrieval with simple queries, which are usually ineffective for complex queries that carry far more complex semantics. Recently, embedding-based paradigm has emerged as a popular approach. It aims to map the queries and videos into a shared embedding space where semantically-similar texts and videos are much closer to each other. Despite its simplicity, it forgoes the exploitation of the syntactic structure of text queries, making it suboptimal to model the complex queries. To facilitate video retrieval with complex queries, we propose a Tree-augmented Cross-modal Encoding method by jointly learning the linguistic structure of queries and the temporal representation of videos. Specifically, given a complex user query, we first recursively compose a latent semantic tree to structurally describe the text query. We then design a tree-augmented query encoder to derive structure-aware query representation and a temporal attentive video encoder to model the temporal characteristics of videos. Finally, both the query and videos are mapped into a joint embedding space for matching and ranking. In this approach, we have a better understanding and modeling of the complex queries, thereby achieving a better video retrieval performance. Extensive experiments on large scale video retrieval benchmark datasets demonstrate the effectiveness of our approach.', 'corpus_id': 220364234, 'score': 0}, {'doc_id': '86866942', 'title': 'Guidelines for Human-AI Interaction', 'abstract': 'Advances in artificial intelligence (AI) frame opportunities and challenges for user interface design. Principles for human-AI interaction have been discussed in the human-computer interaction community for over two decades, but more study and innovation are needed in light of advances in AI and the growing uses of AI technologies in human-facing applications. We propose 18 generally applicable design guidelines for human-AI interaction. These guidelines are validated through multiple rounds of evaluation including a user study with 49 design practitioners who tested the guidelines against 20 popular AI-infused products. The results verify the relevance of the guidelines over a spectrum of interaction scenarios and reveal gaps in our knowledge, highlighting opportunities for further research. Based on the evaluations, we believe the set of design guidelines can serve as a resource to practitioners working on the design of applications and features that harness AI technologies, and to researchers interested in the further development of human-AI interaction design principles.', 'corpus_id': 86866942, 'score': 1}, {'doc_id': '212114261', 'title': 'What is ""intelligent"" in intelligent user interfaces?: a meta-analysis of 25 years of IUI', 'abstract': 'This reflection paper takes the 25th IUI conference milestone as an opportunity to analyse in detail the understanding of intelligence in the community: Despite the focus on intelligent UIs, it has remained elusive what exactly renders an interactive system or user interface ""intelligent"", also in the fields of HCI and AI at large. We follow a bottom-up approach to analyse the emergent meaning of intelligence in the IUI community: In particular, we apply text analysis to extract all occurrences of ""intelligent"" in all IUI proceedings. We manually review these with regard to three main questions: 1) What is deemed intelligent? 2) How (else) is it characterised? and 3) What capabilities are attributed to an intelligent entity? We discuss the community\'s emerging implicit perspective on characteristics of intelligence in intelligent user interfaces and conclude with ideas for stating one\'s own understanding of intelligence more explicitly.', 'corpus_id': 212114261, 'score': 1}, {'doc_id': '220713394', 'title': 'Learning User-Preferred Mappings for Intuitive Robot Control', 'abstract': 'When humans control drones, cars, and robots, we often have some preconceived notion of how our inputs should make the system behave. Existing approaches to teleoperation typically assume a one-size-fits-all approach, where the designers pre-define a mapping between human inputs and robot actions, and every user must adapt to this mapping over repeated interactions. Instead, we propose a personalized method for learning the human’s preferred or preconceived mapping from a few robot queries. Given a robot controller, we identify an alignment model that transforms the human’s inputs so that the controller’s output matches their expectations. We make this approach data-efficient by recognizing that human mappings have strong priors: we expect the input space to be proportional, reversable, and consistent. Incorporating these priors ensures that the robot learns an intuitive mapping from few examples. We test our learning approach in robot manipulation tasks inspired by assistive settings, where each user has different personal preferences and physical capabilities for teleoperating the robot arm. Our simulated and experimental results suggest that learning the mapping between inputs and robot actions improves objective and subjective performance when compared to manually defined alignments or learned alignments without intuitive priors. The supplementary video showing these user studies can be found at: https://youtu.be/rKHka0_48-Q', 'corpus_id': 220713394, 'score': 0}, {'doc_id': '220381402', 'title': 'From API to NLI: A New Interface for Library Reuse', 'abstract': 'Abstract Developers frequently reuse APIs from existing libraries to implement certain functionality. However, learning APIs is difficult due to their large scale and complexity. In this paper, we design an abstract framework NLI2Code to ease the reuse process. Under the framework, users can reuse library functionalities with a high-level, automatically-generated NLI (Natural Language Interface) instead of the detailed API elements. The framework consists of three components: a functional feature extractor to summarize the frequently-used library functions in natural language form, a code pattern miner to give a code template for each functional feature, and a synthesizer to complete code patterns into well-typed snippets. From the perspective of a user, a reuse task under NLI2Code starts from choosing a functional feature and our framework will guide the user to synthesize the desired solution. We instantiated the framework as a tool to reuse Java libraries. The evaluation shows our tool can generate a high-quality natural language interface and save half of the coding time for newcomers to solve real-world programming tasks.', 'corpus_id': 220381402, 'score': 0}, {'doc_id': '220425376', 'title': 'When Humans and Machines Make Joint Decisions: A Non-Symmetric Bandit Model', 'abstract': 'How can humans and machines learn to make joint decisions? This has become an important question in domains such as medicine, law and finance. We approach the question from a theoretical perspective and formalize our intuitions about human-machine decision making in a non-symmetric bandit model. In doing so, we follow the example of a doctor who is assisted by a computer program. We show that in our model, exploration is generally hard. In particular, unless one is willing to make assumptions about how human and machine interact, the machine cannot explore efficiently. We highlight one such assumption, policy space independence, which resolves the coordination problem and allows both players to explore independently. Our results shed light on the fundamental difficulties faced by the interaction of humans and machines. We also discuss practical implications for the design of algorithmic decision systems.', 'corpus_id': 220425376, 'score': 1}]"
124	Spectral soil (NIRS,XRF, lab measurement)	c810857bc011b38d409c7b83945f6809	10895	{'XRF': 'X-ray fluorescence'}	"[{'doc_id': '226290197', 'title': 'X-ray imaging detector for radiological applications in the harsh environments of low-income countries', 'abstract': 'This paper describes the development of a novel medical Xray imaging system adapted to the needs and constraints of low and middle income countries. The developed system is based on an indirect conversion chain: a scintillator plate produces visible light when excited by the Xrays, then a calibrated multi camera architecture converts the visible light from the scintillator into a set of digital images. The partial images are then unwarped, enhanced and stitched through parallel processing units and a specialized software. All the detector components were carefully selected focusing on optimizing the system s image quality, robustness, cost, effectiveness and capability to work in harsh tropical environments. With this aim, different customized and commercial components were characterized. The resulting detector can generate high quality medical diagnostic images with DQE levels up to 60 percent, at 2.34 micro Gray, even under harsh environments i.e. 60 degrees Celsius and 98 percent humidity.', 'corpus_id': 226290197, 'score': 0}, {'doc_id': '137172688', 'title': 'Construction and calibration of a low cost X-ray Fluorescence apparatus for compositional analysis of materials', 'abstract': None, 'corpus_id': 137172688, 'score': 1}, {'doc_id': '73458884', 'title': 'Soil granular dynamics on-a-chip: fluidization inception under scrutiny.', 'abstract': 'Predicting soil evolution remains a scientific challenge. This process involves poorly understood aspects of disordered granular matter and dense suspension dynamics. This study presents a novel two-dimensional experiment on a small-scale chip structure; this allows the observation of the deformation at the particle scale of a large-grained sediment bed, under conditions where friction dominates over cohesive and thermal forces, and with an imposed fluid flow. Experiments are performed under conditions which span the particle resuspension criterion, and particle motion is detected and analyzed. The void size population and statistics of particle trajectories bring insight into the sediment dynamics near fluidization conditions. Specifically, particle rearrangement and net bed compaction are observed at flow rates significantly below the criterion for instability growth. Above a threshold flowrate, a channel forms and grows in the vertical direction; and eventually it crosses the entire bed. In the range of flow rates where channelization can occur, the coexistence of compacting and dilating bed scenarios is observed. The results of the study enhance our capacity for modeling of both slow dynamics and eventual rapid destabilization of sediment beds. Microfluidic channel soil-on-a-chip studies open avenues to new investigations including dissolution-precipitation, fine particle transport, or micro-organism swimming and population growth, which may depend on the mechanics of the porous medium itself.', 'corpus_id': 73458884, 'score': 0}, {'doc_id': '191151013', 'title': 'Microsegmented flow-assisted miniaturized culturing for isolation and characterization of heavy metal-tolerant bacteria', 'abstract': 'Soils are complex ecosystem, and their function in the environment is mainly determined by the microbial communities. Metal-tolerant micro-organisms have an important function in the formation of soil and the development of microbial communities in all areas where heavy metals are released by natural erosion processes or by human activities. The investigation of dose-dependent growth and behaviour is an essential part of the search for heavy metal-tolerant microorganism communities and their characterization. In this study, next-generation sequencing was used for the analysis of soil sample and reduced communities and droplet-based microfluidics was used to assess the growth behaviour of unknown bacterial communities and single strains in response to different heavy metal ions. Highly resolved dose–response functions of the bacterial communities reflect the specific character in their concentration-dependent response to different culture media and heavy metals of copper, nickel and cobalt. Besides the characterization of community responses, they allowed to characterize newly isolated strains. Concentration-dependent growth patterns of the micro-organisms in the droplets could be observed. The investigation demonstrates the potential of droplet-based microfluidics for miniaturized eco-toxicological studies and their suitability for the discovery of novel strains with special tolerance features.', 'corpus_id': 191151013, 'score': 0}, {'doc_id': '212830043', 'title': 'Custom-engineered micro-habitats for characterizing rhizosphere interactions', 'abstract': 'The interactions amongst plants and microorganisms within the rhizosphere have a profound influence on global biogeochemical cycles, and a better understanding of these interactions will benefit society through improved climate change prediction, increased food security, and enhanced bioenergy production. However, the rhizosphere is one of the most complex and bio-diverse ecosystems on earth, making it difficult to parse apart specific interactions between species. This difficulty is compounded by the inability to directly visualize rhizosphere interactions through the soil. Additionally, conventional laboratory techniques do not offer real-time, high-resolution visualization or the proper environmental control to isolate and probe these interactions. A knowledge gap persists in how to design appropriate culturing platforms that allow researchers to collect spatially and temporally sensitive information about physical and chemical interactions in the rhizosphere. This dissertation addresses that gap by demonstrating the design and use of several customengineered micro-habitats in characterizing plant-microbe interactions. Specifically this thesis introduces novel protocols for culturing plants and microorganisms together in microfluidic platforms, pairing platforms to multi-modal imaging techniques with organelle scale resolution, and recreating the structural complexity of the rhizosphere in a microfluidic habitat. Not only does this thesis introduce novel engineered systems, but the work contained herein also goes beyond proof-of-concept experiments and demonstrates the ability of these platforms to generate hypotheses and answer outstanding biological questions.', 'corpus_id': 212830043, 'score': 0}, {'doc_id': '135052648', 'title': 'Using rule-based regression models to predict and interpret soil properties from X-ray powder diffraction data', 'abstract': ""Abstract Data mining is often used to derive calibrations for soil property prediction from diffuse reflectance spectroscopy, facilitating inference of organic and mineral contributions to given properties. In contrast to spectroscopy, X-ray powder diffraction (XRPD) offers a more direct probe into the complexities of soil mineralogy. Here a national scale XRPD dataset of Scottish soils is used in combination with the rule-based regression algorithm ‘Cubist’ for prediction of eight soil properties (total carbon and nitrogen, cation exchange capacity, pH, aqua regia extractable potassium, and the sand, silt and clay size fractions), and interpretation of soil property–mineralogy relationships. Precision sample preparation methods prior to XRPD analysis eliminated effects of preferred orientation, creating reproducible data appropriate for data mining. For direct comparison, Cubist was also applied to an equivalent dataset of near infrared spectroscopy (NIRS) measurements. In terms of predictive performance, XRPD surpassed NIRS for prediction of six of the eight soil properties investigated. Notably, diffuse scattering from X-ray amorphous organic matter facilitated relatively accurate predictions of total carbon and nitrogen from XRPD. Aqua regia extractable potassium was predicted with substantial accuracy and confirmed to reflect the phyllosilicate potassium. The particle size fractions were predicted with moderate-substantial agreement using combinations of quartz, phyllosilicate and feldspar variables. This approach introduces the value of XRPD datasets in enhancing the understanding of soil mineralogy–property relationships whilst contributing to soil mineralogy's advance into the digital soil typing paradigm."", 'corpus_id': 135052648, 'score': 1}, {'doc_id': '52092152', 'title': 'Emergent Properties of Microbial Activity in Heterogeneous Soil Microenvironments: Different Research Approaches Are Slowly Converging, Yet Major Challenges Remain', 'abstract': 'Over the last 60 years, soil microbiologists have accumulated a wealth of experimental data showing that the bulk, macroscopic parameters (e.g., granulometry, pH, soil organic matter, and biomass contents) commonly used to characterize soils provide insufficient information to describe quantitatively the activity of soil microorganisms and some of its outcomes, like the emission of greenhouse gasses. Clearly, new, more appropriate macroscopic parameters are needed, which reflect better the spatial heterogeneity of soils at the microscale (i.e., the pore scale) that is commensurate with the habitat of many microorganisms. For a long time, spectroscopic and microscopic tools were lacking to quantify processes at that scale, but major technological advances over the last 15 years have made suitable equipment available to researchers. In this context, the objective of the present article is to review progress achieved to date in the significant research program that has ensued. This program can be rationalized as a sequence of steps, namely the quantification and modeling of the physical-, (bio)chemical-, and microbiological properties of soils, the integration of these different perspectives into a unified theory, its upscaling to the macroscopic scale, and, eventually, the development of new approaches to measure macroscopic soil characteristics. At this stage, significant progress has been achieved on the physical front, and to a lesser extent on the (bio)chemical one as well, both in terms of experiments and modeling. With regard to the microbial aspects, although a lot of work has been devoted to the modeling of bacterial and fungal activity in soils at the pore scale, the appropriateness of model assumptions cannot be readily assessed because of the scarcity of relevant experimental data. For significant progress to be made, it is crucial to make sure that research on the microbial components of soil systems does not keep lagging behind the work on the physical and (bio)chemical characteristics. Concerning the subsequent steps in the program, very little integration of the various disciplinary perspectives has occurred so far, and, as a result, researchers have not yet been able to tackle the scaling up to the macroscopic level. Many challenges, some of them daunting, remain on the path ahead. Fortunately, a number of these challenges may be resolved by brand new measuring equipment that will become commercially available in the very near future.', 'corpus_id': 52092152, 'score': 0}, {'doc_id': '104363410', 'title': 'DIY XES - development of an inexpensive, versatile, and easy to fabricate XES analyzer and sample delivery system.', 'abstract': 'The application of X-ray emission spectroscopy (XES) has grown substantially with the development of X-ray free electron lasers, third and fourth generation synchrotron sources and high-power benchtop sources. By providing the high X-ray flux required for XES, these sources broaden the availability and application of this method of probing electronic structure. As the number of sources increase, so does the demand for X-ray emission detection and sample delivery systems that are cost effective and customizable. Here, we present a detailed fabrication protocol for von Hamos X-ray optics and give details for a 3D-printed spectrometer design. Additionally, we outline an automated, externally triggered liquid sample delivery system that can be used to repeatedly deliver nanoliter droplets onto a plastic substrate for measurement. These systems are both low cost, efficient and easy to recreate or modify depending on the application. A low cost multiple X-ray analyzer system enables measurement of dilute samples, whereas the sample delivery limits sample loss and replaces spent sample with fresh sample in the same position. While both systems can be used in a wide range of applications, the design addresses several challenges associated specifically with time-resolved XES (TRXES). As an example application, we show results from TRXES measurements of photosystem II, a dilute, photoactive protein.', 'corpus_id': 104363410, 'score': 1}, {'doc_id': '222141641', 'title': 'Global soil moisture from in-situ measurements using machine learning - SoMo.ml', 'abstract': 'While soil moisture information is essential for a wide range of hydrologic and climate applications, spatially-continuous soil moisture data is only available from satellite observations or model simulations. Here we present a global, long-term dataset of soil moisture generated from in-situ measurements using machine learning, this http URL. We train a Long Short-Term Memory (LSTM) model to extrapolate daily soil moisture dynamics in space and in time, based on in-situ data collected from more than 1,000 stations across the globe. this http URL provides multi-layer soil moisture data (0-10 cm, 10-30 cm, and 30-50 cm) at 0.25° spatial and daily temporal resolution over the period 2000-2019. The performance of the resulting dataset is evaluated through cross validation and inter-comparison with existing soil moisture datasets. this http URL performs especially well in terms of temporal dynamics, making it particularly useful for applications requiring time-varying soil moisture, such as anomaly detection and memory analyses. this http URL complements the existing suite of modelled and satellite-based datasets given its independent and novel derivation, to support large-scale hydrological, meteorological, and ecological analyses.', 'corpus_id': 222141641, 'score': 1}, {'doc_id': '139172594', 'title': 'Portable grazing exit X-ray fluorescence system using a low-power X-ray tube', 'abstract': 'In this work was developed a portable system of grazing exit X-ray fluorescence (geometric 90° - 0°) that can be applied in several areas science and technology. GE-XRF portable system is formed by a mini X-ray tube of low power (anode of Au) and a SiPIN detector. The reflectors used as sample support (sampler carrier) were quartz discs. The grazing exit angle was experimentally determined by measuring a cooper solution (10 μg.g-1). The accuracy of the system was checked using multielement reference solution as standard reference material. The relative errors between measured and certified values are in the range of 4 to 19%. The first results showed a background was drastically reduced at grazing exit angles, enabling trace elemental analysis. The system of GE-XRF proved to be quite stable and reproducible. This paper shows that it is possible to produce a portable system of grazing exit X-ray fluorescence compact, efficient, low-cost and easy-to-handle instrumentation using a low power X-ray tube and a SiPIN compact detector.', 'corpus_id': 139172594, 'score': 1}]"
125	Interpret	ad79692ebd033933fada334aec61010b	131	{}	"[{'doc_id': '102352287', 'title': 'An Analysis of Attention over Clinical Notes for Predictive Tasks', 'abstract': 'The shift to electronic medical records (EMRs) has engendered research into machine learning and natural language technologies to analyze patient records, and to predict from these clinical outcomes of interest. Two observations motivate our aims here. First, unstructured notes contained within EMR often contain key information, and hence should be exploited by models. Second, while strong predictive performance is important, interpretability of models is perhaps equally so for applications in this domain. Together, these points suggest that neural models for EMR may benefit from incorporation of attention over notes, which one may hope will both yield performance gains and afford transparency in predictions. In this work we perform experiments to explore this question using two EMR corpora and four different predictive tasks, that: (i) inclusion of attention mechanisms is critical for neural encoder modules that operate over notes fields in order to yield competitive performance, but, (ii) unfortunately, while these boost predictive performance, it is decidedly less clear whether they provide meaningful support for predictions.', 'corpus_id': 102352287, 'score': 1}, {'doc_id': '67855860', 'title': 'Attention is not Explanation', 'abstract': 'Attention mechanisms have seen wide adoption in neural NLP models. In addition to improving predictive performance, these are often touted as affording transparency: models equipped with attention provide a distribution over attended-to input units, and this is often presented (at least implicitly) as communicating the relative importance of inputs. However, it is unclear what relationship exists between attention weights and model outputs. In this work we perform extensive experiments across a variety of NLP tasks that aim to assess the degree to which attention weights provide meaningful “explanations” for predictions. We find that they largely do not. For example, learned attention weights are frequently uncorrelated with gradient-based measures of feature importance, and one can identify very different attention distributions that nonetheless yield equivalent predictions. Our findings show that standard attention modules do not provide meaningful explanations and should not be treated as though they do.', 'corpus_id': 67855860, 'score': 1}, {'doc_id': '208089663', 'title': 'Natural Language Generation Challenges for Explainable AI', 'abstract': 'Good quality explanations of artificial intelligence (XAI) reasoning must be written (and evaluated) for an explanatory purpose, targeted towards their readers, have a good narrative and causal structure, and highlight where uncertainty and data quality affect the AI output. I discuss these challenges from a Natural Language Generation (NLG) perspective, and highlight four specific NLG for XAI research challenges.', 'corpus_id': 208089663, 'score': 1}, {'doc_id': '202537436', 'title': 'Improving Neural Question Generation using World Knowledge', 'abstract': 'In this paper, we propose a method for incorporating world knowledge (linked entities and fine-grained entity types) into a neural question generation model. This world knowledge helps to encode additional information related to the entities present in the passage required to generate human-like questions. We evaluate our models on both SQuAD and MS MARCO to demonstrate the usefulness of the world knowledge features. The proposed world knowledge enriched question generation model is able to outperform the vanilla neural question generation model by 1.37 and 1.59 absolute BLEU 4 score on SQuAD and MS MARCO test dataset respectively.', 'corpus_id': 202537436, 'score': 0}, {'doc_id': '202889005', 'title': 'Spoken Conversational Search for General Knowledge', 'abstract': 'We present a spoken conversational question answering proof of concept that is able to answer questions about general knowledge from Wikidata. The dialogue component does not only orchestrate various components but also solve coreferences and ellipsis.', 'corpus_id': 202889005, 'score': 0}, {'doc_id': '202660960', 'title': 'Conversational AI : Open Domain Question Answering and Commonsense Reasoning', 'abstract': 'Our research is focused on making a human-like question answering system which can answer rationally. The distinguishing characteristic of our approach is that it will use automated common sense reasoning to truly ""understand"" dialogues, allowing it to converse like a human. Humans often make many assumptions during conversations. We infer facts not told explicitly by using our common sense. Incorporating commonsense knowledge in a question answering system will simply make it more robust.', 'corpus_id': 202660960, 'score': 0}, {'doc_id': '207852642', 'title': 'Semantic Noise Matters for Neural Natural Language Generation', 'abstract': 'Neural natural language generation (NNLG) systems are known for their pathological outputs, i.e. generating text which is unrelated to the input specification. In this paper, we show the impact of semantic noise on state-of-the-art NNLG models which implement different semantic control mechanisms. We find that cleaned data can improve semantic correctness by up to 97%, while maintaining fluency. We also find that the most common error is omitting information, rather than hallucination.', 'corpus_id': 207852642, 'score': 0}, {'doc_id': '202750077', 'title': 'Attention Interpretability Across NLP Tasks', 'abstract': ""The attention layer in a neural network model provides insights into the model's reasoning behind its prediction, which are usually criticized for being opaque. Recently, seemingly contradictory viewpoints have emerged about the interpretability of attention weights (Jain & Wallace, 2019; Vig & Belinkov, 2019). Amid such confusion arises the need to understand attention mechanism more systematically. In this work, we attempt to fill this gap by giving a comprehensive explanation which justifies both kinds of observations (i.e., when is attention interpretable and when it is not). Through a series of experiments on diverse NLP tasks, we validate our observations and reinforce our claim of interpretability of attention through manual evaluation."", 'corpus_id': 202750077, 'score': 1}, {'doc_id': '203626746', 'title': 'Abstractive Dialog Summarization with Semantic Scaffolds', 'abstract': 'The demand for abstractive dialog summary is growing in real-world applications. For example, customer service center or hospitals would like to summarize customer service interaction and doctor-patient interaction. However, few researchers explored abstractive summarization on dialogs due to the lack of suitable datasets. We propose an abstractive dialog summarization dataset based on MultiWOZ. If we directly apply previous state-of-the-art document summarization methods on dialogs, there are two significant drawbacks: the informative entities such as restaurant names are difficult to preserve, and the contents from different dialog domains are sometimes mismatched. To address these two drawbacks, we propose Scaffold Pointer Network (SPNet)to utilize the existing annotation on speaker role, semantic slot and dialog domain. SPNet incorporates these semantic scaffolds for dialog summarization. Since ROUGE cannot capture the two drawbacks mentioned, we also propose a new evaluation metric that considers critical informative entities in the text. On MultiWOZ, our proposed SPNet outperforms state-of-the-art abstractive summarization methods on all the automatic and human evaluation metrics.', 'corpus_id': 203626746, 'score': 0}, {'doc_id': '202583616', 'title': 'Learning to Deceive with Attention-Based Explanations', 'abstract': 'Attention mechanisms are ubiquitous components in neural architectures applied to natural language processing. In addition to yielding gains in predictive accuracy, attention weights are often claimed to confer interpretability, purportedly useful both for providing insights to practitioners and for explaining why a model makes its decisions to stakeholders. We call the latter use of attention mechanisms into question by demonstrating a simple method for training models to produce deceptive attention masks. Our method diminishes the total weight assigned to designated impermissible tokens, even when the models can be shown to nevertheless rely on these features to drive predictions. Across multiple models and tasks, our approach manipulates attention weights while paying surprisingly little cost in accuracy. Through a human study, we show that our manipulated attention-based explanations deceive people into thinking that predictions from a model biased against gender minorities do not rely on the gender. Consequently, our results cast doubt on attention’s reliability as a tool for auditing algorithms in the context of fairness and accountability.', 'corpus_id': 202583616, 'score': 1}]"
126	Functional morphology	ffff5cabc6e5f29b7181ca3cdc580211	19952	{}	[{'doc_id': '232412326', 'title': 'Pseudohydrosme bogneri sp. nov. (Araceae), a spectacular Critically Endangered (Possibly Extinct) species from Gabon, long confused with Anchomanes nigritianus', 'abstract': 'A Gabonese taxon cultivated for decades in the botanic gardens of Europe as Anchomanes nigritianus is shown to be a new species to science, and on current evidence, is best placed as the fourth species of the Gabonese-centred, poorly known genus Pseudohydrosme. Data on the morphological separation between Anchomanes and Pseudohydrosme are reviewed. Although phylogenomic studies may show in future that the two genera need to be merged, for the moment their separation is reinforced on morphological grounds. Anchomanes lacks the spathe tube, ovoid-globose, 2 – 4 locular pistil and thick, lobed stigma on a symmetric, stout style that we show to characterise the redelimited Pseudohydrosme. (Anchomanes has oblong, polygonal, 1-locular pistils, stigmas asymmetric, sessile, thin and disc-like or on asymmetrical conical styles and are pointed or brush-like). In addition, Pseudohydrosme (where known) has stipitate (versus sessile) fruits and on current evidence lacks the lacticifers recorded from Anchomanes. We test the hypothesis that the taxon is a new species to science, naming it as Pseudohydrosme bogneri, and conclude that it is Critically Endangered (Possibly Extinct) using the IUCN 2012 standard. Pseudohydrosme bogneri appears to be the tenth documented probable global extinction of a plant species that has occurred among the narrowly endemic plant species of the Libreville area, Gabon.', 'corpus_id': 232412326, 'score': 0}, {'doc_id': '172137258', 'title': 'Morphological and functional heterogeneity in olfactory perception between antennae and maxillary palps in the pumpkin fruit fly, Bactrocera depressa.', 'abstract': 'The morphology and ultrastructure of the olfactory sensilla on the antennae and maxillary palps were investigated through scanning electron microscopy (SEM) and transmission electron microscopy (TEM), and their responses to five volatile compounds were measured using electroantenogram (EAG) and electropalpogram (EPG) techniques in the pumpkin fruit fly, Bactrocera depressa (Shiraki; Diptera: Tephritidae). Male and female B. depressa displayed distinct morphological types of olfactory sensilla in the antennae and maxillary palps, with predominant populations of trichoid, basiconic, and coeloconic sensilla. Basiconic sensilla, the most abundant type of olfactory sensilla in the antennae, could be further classified into two different types. In contrast, the maxillary palps exhibited predominant populations of a single type of curved basiconic sensilla. High-resolution SEM observation revealed the presence of multiple nanoscale wall-pores on the cuticular surface of trichoid and basiconic sensilla, indicating that their primary function is olfactory. In contrast, coeloconic sensilla displayed several longitudinal grooves around the sensillum peg. The TEM observation of individual antennal olfactory sensilla indicates that the basiconic sensilla are thin-walled, while the trichoid sensilla are thick-walled. The profile of EAG responses of male B. depressa was different from their EPG response profile, indicating that the olfactory function of maxillary palps is different from that of antennae in this species. The structural and functional variation in the olfactory sensilla between antennae and maxillary palps suggests that each plays an independent role in the perception of olfactory signals in B. depressa.', 'corpus_id': 172137258, 'score': 1}, {'doc_id': '46857483', 'title': 'MetrIntPair—A Novel Accurate Metric for the Comparison of Two Cooperative Multiagent Systems Intelligence Based on Paired Intelligence Measurements', 'abstract': 'In this paper, we propose a novel metric called MetrIntPair (Metric for Pairwise Intelligence Comparison of Agent‐Based Systems) for comparison of two cooperative multiagent systems problem‐solving intelligence. MetrIntPair is able to make an accurate comparison by taking into consideration the variability in intelligence in problem‐solving. The metric could treat the outlier intelligence indicators, intelligence measures that are statistically different from those others. For evaluation of the proposed metric, we realized a case study for two cooperative multiagent systems applied for solving a class of NP‐hard problems. The results of the case study proved that the small difference in the measured intelligence of the multiagent systems is the consequence of the variability. There is no statistical difference between the intelligence quotients/level of the multiagent systems. Both multiagent systems should be classified in the same intelligence class.', 'corpus_id': 46857483, 'score': 0}, {'doc_id': '211232765', 'title': 'Mechanical ecology of fruit-insect interaction in the adult Mediterranean fruit fly Ceratitis capitata (Diptera: Tephritidae).', 'abstract': 'Fruit features represent a trade-off between dispersal and protection against frugivore insects. To prevent insect attack, plants evolved chemical and physical barriers, mainly studied in leaves, while limited knowledge is available for fruits, especially concerning mechanical barriers. We used the Mediterranean fruit fly to shed light on the mechanical ecology of insect-fruit attachment in a pest species. We tested the following hypotheses: is there any sexual dimorphism in attachment devices and attachment ability? Can the attachment ability of females of Ceratitis capitata to fruits of various host plants vary according to fruit surfaces with different morphology (smooth, hairy, waxy) or physico-chemical properties? The tarsal attachment devices were studied using Cryo-SEM and TEM. The maximum friction forces of C. capitata females on fruit surfaces of typical host plants were evaluated using a load cell force transducer. The attachment ability of both sexes on artificial surfaces was evaluated using a centrifugal force tester. Our data revealed sexual dimorphism in the size of pulvilli, which are wider in females. A higher friction force is exerted by females in comparison with males, in agreement with the need to firmly adhere to the host plant fruit during oviposition. Among the tested fruits, the stronger friction force was recorded on hairy or rough surfaces while a force reduction was recorded on waxy fruits. To unravel the mechanical ecology of insect-plant interaction between plants and species of Tephritidae can be useful to develop non-chemical methods to control these important crop pests.', 'corpus_id': 211232765, 'score': 1}, {'doc_id': '247802', 'title': 'Static antennae act as locomotory guides that compensate for visual motion blur in a diurnal, keen-eyed predator', 'abstract': 'High visual acuity allows parallel processing of distant environmental features, but only when photons are abundant enough. Diurnal tiger beetles (Carabidae: Cicindelinae) have acute vision for insects and visually pursue prey in open, flat habitats. Their fast running speed causes motion blur that degrades visual contrast, forces stop-and-go pursuit and potentially impairs obstacle detection. We demonstrate here that vision is insufficient for obstacle detection during running, and show instead that antennal touch is both necessary and sufficient for obstacle detection. While running, tiger beetle vision appears to be photon-limited in a way reminiscent of animals in low-light habitats. Such animals often acquire wide-field spatial information through mechanosensation mediated by longer, more mobile appendages. We show that a nocturnal tiger beetle species waves its antennae in elliptical patterns typical of poorly sighted insects. While antennae of diurnal species are also used for mechanosensation, they are rigidly held forward with the tips close to the substrate. This enables timely detection of path obstructions followed by an increase in body pitch to avoid collision. Our results demonstrate adaptive mechanosensory augmentation of blurred visual information during fast locomotion, and suggest that future studies may reveal non-visual sensory compensation in other fast-moving animals.', 'corpus_id': 247802, 'score': 1}, {'doc_id': '235237979', 'title': 'Six new species of Allorhogas (Hymenoptera, Braconidae, Doryctinae) from south and southeast Brazil with host-plant record', 'abstract': 'Six new Brazilian species of the gall-associated Doryctinae genus Allorhogas are described and illustrated: A. copaiba sp. nov., A. ilexaffinis sp. nov., A. inquilinus sp. nov., A. quarentenus sp. nov., A. vassununga sp. nov. and A. viridis sp. nov. We provide host plant records for five of these species, three and one of which are new host plant genera (Ilex L., Copaifera L. and Eugenia P. Micheli ex L.) and new host plant family (Aquifoliaceae) records, respectively. Allorhogas inquilinus sp. nov., whose biology was previously reported, represents the first confirmed case of phytophagous inquilinism in the genus. An updated key to Brazilian species of Allorhogas is provided.', 'corpus_id': 235237979, 'score': 0}, {'doc_id': '234891432', 'title': 'Redescription of the bug Aschistocoris brevicornis (Heteroptera: Coreidae) and first report on its life history from northern Maharashtra, India', 'abstract': 'We redescribe Aschistocoris brevicornis (Dallas, 1852) from northern Maharashtra, India, a species belonging to the tribe Homoeocerini (Heteroptera: Coreidae: Coreinae). Here we have provided detailed, well-illustrated morphology, including the male genitalia, and notes on its bionomics for the first time.', 'corpus_id': 234891432, 'score': 0}, {'doc_id': '91035867', 'title': 'Antennal sensilla of the armoured ground cricket, Eugaster powysi Kirby, 1891 (Orthoptera, Tettigoniidae, Hetrodinae)', 'abstract': 'Sensilla on the antennae of the adult armoured ground cricket, Eugaster powysi, were studied using scanning electron microscopy to determine the sensilla morphology and their possible chemoand mechanoreceptive functions for food detection. Twelve types or subtypes occur in males and thirteen in females: aporous sensilla chaetica (62.6 % of total sensilla in males, 63.5% in females) with a tactile mechanoreceptive function; uniporous sensilla chaetica of three subtypes (12.5-12.7%) with a contact chemoreceptive function; multiporous sensilla trichodea (5.6-7.9%); multiporous sensilla basiconica of three subtypes (13.1-13.9%); multiporous sensilla coeloconica (0.8-0.9%), all multiporous sensilla being olfactory receptors; aporous sensilla coeloconica (2.2%) and aporous sensilla coelocapitula (0.1% in female) with a possible thermohygroreceptive function; aporous Böhm sensilla (0.4-0.5%) and sensilla campaniformia (0.5-0.6%) which are proprioceptors. These results are related to the behaviour of Eugaster and compared with those obtained in other Orthoptera, Mantodea and Blattodea.', 'corpus_id': 91035867, 'score': 1}, {'doc_id': '82760570', 'title': 'Antennal movements and mechanoreception: neurobiology of active tactile sensors', 'abstract': 'Publisher Summary This chapter focuses on the biology of the antennal tactile sense of insects and crustaceans. It covers the biomechanics, kinematics, and behavioral biology of antennal movements, the peripheral and central neurobiology of antennal mechanoreception, and interdisciplinary aspects of biology and engineering of tactile sensing. The discussion is concentrated on a set of insect model organisms, all of which employ their antennae in active tactile sensing. Work on other species, particularly on decapod crustaceans, is used to complete and contrast the insights from the insect model organisms. The chapter discusses three aspects of the antennal tactile sense that draw on information from various sections and, therefore, require separate treatment. It addresses the behavioral similarities between insect and crustacean antennal tactile sensing, which are particularly remarkable when bearing in mind the morphological differences between these taxa. The chapter also provides a contrast in the categories of passive and active sensing, by relating them to a behavior-based classification of tactile sensing.', 'corpus_id': 82760570, 'score': 1}, {'doc_id': '236431689', 'title': 'The morphology of Colpocephalum pectinatum (Phthiraptera: Amblycera: Menoponidae) under scanning electron microscopy.', 'abstract': 'Here, we describe under scanning electron microscopy (SEM) the morphology of Colpocephalum pectinatum (Phthiraptera, Menoponidae), an ectoparasite found in burrowing owls, Athene cunicularia. We devote particular attention to the morphology of the main structures of the head (antennae and mouth-parts) and legs (tarsi and femoral ctenidia). Moreover, we describe the main peripheral sensory organs, located in the labial palpi and the distal end of antennae. We also detected that the structure of antennae and antennal sensilla arrangement are very similar to that described for other Colpocephalum and Menoponid species, and we discuss the function of each type of sensilla. We suggest that SEM studies combined with other microscopy and physiological techniques could be useful for elucidate the function of each structure, lice behaviour, as well as their taxonomy.', 'corpus_id': 236431689, 'score': 0}]
127	PPL	050761d8d3202128b4990287d187f09a	13189	{'PPL': 'porcine pancreatic lipase'}	[{'doc_id': '225040558', 'title': 'Conditional independence by typing', 'abstract': 'A central goal of probabilistic programming languages (PPLs) is to separate modelling from inference. However, this goal is hard to achieve in practice. Users are often forced to re-write their models in order to improve efficiency of inference or meet restrictions imposed by the PPL. Conditional independence (CI) relationships among parameters are a crucial aspect of probabilistic models that captures a qualitative summary of the specified model and can facilitate more efficient inference. \nWe present an information flow type system for probabilistic programming that captures conditional independence (CI) relationships, and show that, for a well-typed program in our system, the distribution it implements is guaranteed to have certain CI-relationships. Further, by using type inference, we can statically \\emph{deduce} which CI-properties are present in a specified model. \nAs a practical application, we consider the problem of how to perform inference on models with mixed discrete and continuous parameters. Inference on such models is challenging in many existing PPLs, but can be improved through a workaround, where the discrete parameters are used \\textit{implicitly}, at the expense of manual model re-writing. We present a source-to-source semantics-preserving transformation, which uses our CI-type system to automate this workaround by eliminating the discrete parameters from a probabilistic program. The resulting program can be seen as a hybrid inference algorithm on the original program, where continuous parameters can be drawn using efficient gradient-based inference methods, while the discrete parameters are drawn using variable elimination. \nWe implement our CI-type system and its example application in SlicStan: a compositional variant of Stan.', 'corpus_id': 225040558, 'score': 1}, {'doc_id': '221138521', 'title': 'Verifying Graph Programs with First-Order Logic', 'abstract': 'We consider Hoare-style verification for the graph programming language GP 2. In previous work, graph properties were specified by so-called E-conditions which extend nested graph conditions. However, this type of assertions is not easy to comprehend by programmers that are used to formal specifications in standard first-order logic. In this paper, we present an approach to verify GP 2 programs with a standard first-order logic. We show how to construct a strongest liberal postcondition with respect to a rule schema and a precondition. We then extend this construction to obtain strongest liberal postconditions for arbitrary loop-free programs. Compared with previous work, this allows to reason about a vastly generalised class of graph programs. In particular, many programs with nested loops can be verified with the new calculus.', 'corpus_id': 221138521, 'score': 0}, {'doc_id': '230523953', 'title': 'Hierarchical Sampler for Probabilistic Programs via Separation of Control and Data', 'abstract': 'We introduce a novel sampling algorithm for Bayesian inference on imperative probabilistic programs. It features a hierarchical architecture that separates control flows from data: the top-level samples a control flow, and the bottom level samples data values along the control flow picked by the top level. This separation allows us to plug various language-based analysis techniques in probabilistic program sampling; specifically, we use logical backward propagation of observations for sampling efficiency. We implemented our algorithm on top of Anglican. The experimental results demonstrate our algorithm’s efficiency, especially for programs with while loops and rare observations.', 'corpus_id': 230523953, 'score': 1}, {'doc_id': '226299988', 'title': 'Petr4: formal foundations for p4 data planes', 'abstract': 'P4 is a domain-specific language for programming and specifying packet-processing systems. It is based on an elegant design with high-level abstractions like parsers and match-action pipelines that can be compiled to efficient implementations in software or hardware. Unfortunately, like many industrial languages, P4 has developed without a formal foundation. The P4 Language Specification is a 160-page document with a mixture of informal prose, graphical diagrams, and pseudocode, leaving many aspects of the language semantics up to individual compilation targets. The P4 reference implementation is a complex system, running to over 40KLoC of C++ code, with support for only a few targets. Clearly neither of these artifacts is suitable for formal reasoning about P4 in general. This paper presents a new framework, called Petr4, that puts P4 on a solid foundation. Petr4 consists of a clean-slate definitional interpreter and a core calculus that models a fragment of P4. Petr4 is not tied to any particular target: the interpreter is parameterized over an interface that collects features delegated to targets in one place, while the core calculus overapproximates target-specific behaviors using non-determinism. We have validated the interpreter against a suite of over 750 tests from the P4 reference implementation, exercising our target interface with tests for different targets. We validated the core calculus with a proof of type-preserving termination. While developing Petr4, we reported dozens of bugs in the language specification and the reference implementation, many of which have been fixed.', 'corpus_id': 226299988, 'score': 0}, {'doc_id': '220255905', 'title': 'Strongly Normalizing Higher-Order Relational Queries', 'abstract': 'Language-integrated query is a powerful programming construct allowing database queries and ordinary program code to interoperate seamlessly and safely. Language-integrated query techniques rely on classical results about monadic comprehension calculi, including the conservativity theorem for nested relational calculus. Conservativity implies that query expressions can freely use nesting and unnesting, yet as long as the query result type is a flat relation, these capabilities do not lead to an increase in expressiveness over flat relational queries. Wong showed how such queries can be translated to SQL via a constructive rewriting algorithm, and Cooper and others advocated higher-order nested relational calculi as a basis for language-integrated queries in functional languages such as Links and F#. However there is no published proof of the central strong normalization property for higher-order nested relational queries: a previous proof attempt does not deal correctly with rewrite rules that duplicate subterms. This paper fills the gap in the literature, explaining the difficulty with a previous proof attempt, and showing how to extend the $\\top\\top$-lifting approach of Lindley and Stark to accommodate duplicating rewrites. We also show how to extend the proof to a recently-introduced calculus for heterogeneous queries mixing set and multiset semantics.', 'corpus_id': 220255905, 'score': 0}, {'doc_id': '229340124', 'title': 'Probabilistic Dependency Graphs', 'abstract': 'We introduce Probabilistic Dependency Graphs (PDGs), a new class of directed graphical models. PDGs can capture inconsistent beliefs in a natural way and are more modular than Bayesian Networks (BNs), in that they make it easier to incorporate new information and restructure the representation. We show by example how PDGs are an especially natural modeling tool. We provide three semantics for PDGs, each of which can be derived from a scoring function (on joint distributions over the variables in the network) that can be viewed as representing a distribution’s incompatibility with the PDG. For the PDG corresponding to a BN, this function is uniquely minimized by the distribution the BN represents, showing that PDG semantics extend BN semantics. We show further that factor graphs and their exponential families can also be faithfully represented as PDGs, while there are significant barriers to modeling a PDG with a factor graph.', 'corpus_id': 229340124, 'score': 1}, {'doc_id': '227106064', 'title': 'Programming and reasoning with partial observability', 'abstract': 'Computer programs are increasingly being deployed in partially-observable environments. A partially observable environment is an environment whose state is not completely visible to the program, but from which the program receives partial observations. Developers typically deal with partial observability by writing a state estimator that, given observations, attempts to deduce the hidden state of the environment. In safety-critical domains, to formally verify safety properties developers may write an environment model. The model captures the relationship between observations and hidden states and is used to prove the software correct. In this paper, we present a new methodology for writing and verifying programs in partially observable environments. We present belief programming, a programming methodology where developers write an environment model that the program runtime automatically uses to perform state estimation. A belief program dynamically updates and queries a belief state that captures the possible states the environment could be in. To enable verification, we present Epistemic Hoare Logic that reasons about the possible belief states of a belief program the same way that classical Hoare logic reasons about the possible states of a program. We develop these concepts by defining a semantics and a program logic for a simple core language called BLIMP. In a case study, we show how belief programming could be used to write and verify a controller for the Mars Polar Lander in BLIMP. We present an implementation of BLIMP called CBLIMP and evaluate it to determine the feasibility of belief programming.', 'corpus_id': 227106064, 'score': 0}, {'doc_id': '229678194', 'title': 'Verifying C11-style weak memory libraries', 'abstract': 'Deductive verification of concurrent programs under weak memory has thus far been limited to simple programs over a monolithic state space. For scalabiility, we also require modular techniques with verifiable library abstractions. We address this challenge in the context of RC11 RAR, a subset of the C11 memory model that admits relaxed and release-acquire accesses, but disallows, so-called, load-buffering cycles. We develop a simple framework for specifying abstract objects that precisely characterises the observability guarantees of abstract method calls. Our framework is integrated with an operational semantics that enables verification of client programs that execute abstract method calls from a library it uses. We implement such abstractions in RC11 RAR by developing a (contextual) refinement framework for abstract objects. Our framework has been mechanised in Isabelle/HOL.', 'corpus_id': 229678194, 'score': 0}, {'doc_id': '208637478', 'title': 'Normalizing Flows for Probabilistic Modeling and Inference', 'abstract': 'Normalizing flows provide a general mechanism for defining expressive probability distributions, only requiring the specification of a (usually simple) base distribution and a series of bijective transformations. There has been much recent work on normalizing flows, ranging from improving their expressive power to expanding their application. We believe the field has now matured and is in need of a unified perspective. In this review, we attempt to provide such a perspective by describing flows through the lens of probabilistic modeling and inference. We place special emphasis on the fundamental principles of flow design, and discuss foundational topics such as expressive power and computational trade-offs. We also broaden the conceptual framing of flows by relating them to more general probability transformations. Lastly, we summarize the use of flows for tasks such as generative modeling, approximate inference, and supervised learning.', 'corpus_id': 208637478, 'score': 1}, {'doc_id': '226237072', 'title': 'Bayesian Workflow.', 'abstract': 'The Bayesian approach to data analysis provides a powerful way to handle uncertainty in all observations, model parameters, and model structure using probability theory. Probabilistic programming languages make it easier to specify and fit Bayesian models, but this still leaves us with many options regarding constructing, evaluating, and using these models, along with many remaining challenges in computation. Using Bayesian inference to solve real-world problems requires not only statistical skills, subject matter knowledge, and programming, but also awareness of the decisions made in the process of data analysis. All of these aspects can be understood as part of a tangled workflow of applied Bayesian statistics. Beyond inference, the workflow also includes iterative model building, model checking, validation and troubleshooting of computational problems, model understanding, and model comparison. We review all these aspects of workflow in the context of several examples, keeping in mind that in practice we will be fitting many models for any given problem, even if only a subset of them will ultimately be relevant for our conclusions.', 'corpus_id': 226237072, 'score': 1}]
128	Spaced Repetition And Memory	543ef3092e8d730e1554e6d0d1d6f3fd	2485	{}	"[{'doc_id': '211004035', 'title': 'Encoding-based Memory Modules for Recurrent Neural Networks', 'abstract': 'Learning to solve sequential tasks with recurrent models requires the ability to memorize long sequences and to extract task-relevant features from them. In this paper, we study the memorization subtask from the point of view of the design and training of recurrent neural networks. We propose a new model, the Linear Memory Network, which features an encoding-based memorization component built with a linear autoencoder for sequences. We extend the memorization component with a modular memory that encodes the hidden state sequence at different sampling frequencies. Additionally, we provide a specialized training algorithm that initializes the memory to efficiently encode the hidden activations of the network. The experimental results on synthetic and real-world datasets show that specializing the training algorithm to train the memorization component always improves the final performance whenever the memorization of long sequences is necessary to solve the problem.', 'corpus_id': 211004035, 'score': 0}, {'doc_id': '210942708', 'title': 'MEMO: A Deep Network for Flexible Combination of Episodic Memories', 'abstract': 'Recent research developing neural network architectures with external memory have often used the benchmark bAbI question and answering dataset which provides a challenging number of tasks requiring reasoning. Here we employed a classic associative inference task from the human neuroscience literature in order to more carefully probe the reasoning capacity of existing memory-augmented architectures. This task is thought to capture the essence of reasoning -- the appreciation of distant relationships among elements distributed across multiple facts or memories. Surprisingly, we found that current architectures struggle to reason over long distance associations. Similar results were obtained on a more complex task involving finding the shortest path between nodes in a path. We therefore developed a novel architecture, MEMO, endowed with the capacity to reason over longer distances. This was accomplished with the addition of two novel components. First, it introduces a separation between memories/facts stored in external memory and the items that comprise these facts in external memory. Second, it makes use of an adaptive retrieval mechanism, allowing a variable number of ‘memory hops’ before the answer is produced. MEMO is capable of solving our novel reasoning tasks, as well as all 20 tasks in bAbI.', 'corpus_id': 210942708, 'score': 0}, {'doc_id': '27857584', 'title': 'Spaced Learning Enhances Subsequent Recognition Memory by Reducing Neural Repetition Suppression', 'abstract': 'Spaced learning usually leads to better recognition memory as compared with massed learning, yet the underlying neural mechanisms remain elusive. One open question is whether the spacing effect is achieved by reducing neural repetition suppression. In this fMRI study, participants were scanned while intentionally memorizing 120 novel faces, half under the massed learning condition (i.e., four consecutive repetitions with jittered interstimulus interval) and the other half under the spaced learning condition (i.e., the four repetitions were interleaved). Recognition memory tests afterward revealed a significant spacing effect: Participants recognized more items learned under the spaced learning condition than under the massed learning condition. Successful face memory encoding was associated with stronger activation in the bilateral fusiform gyrus, which showed a significant repetition suppression effect modulated by subsequent memory status and spaced learning. Specifically, remembered faces showed smaller repetition suppression than forgotten faces under both learning conditions, and spaced learning significantly reduced repetition suppression. These results suggest that spaced learning enhances recognition memory by reducing neural repetition suppression.', 'corpus_id': 27857584, 'score': 1}, {'doc_id': '207515683', 'title': 'Mnemonic Encoding and Cortical Organization in Parietal and Prefrontal Cortices', 'abstract': ""Persistent activity within the frontoparietal network is consistently observed during tasks that require working memory. However, the neural circuit mechanisms underlying persistent neuronal encoding within this network remain unresolved. Here, we ask how neural circuits support persistent activity by examining population recordings from posterior parietal (PPC) and prefrontal (PFC) cortices in two male monkeys that performed spatial and motion direction-based tasks that required working memory. While spatially selective persistent activity was observed in both areas, robust selective persistent activity for motion direction was only observed in PFC. Crucially, we find that this difference between mnemonic encoding in PPC and PFC is associated with the presence of functional clustering: PPC and PFC neurons up to ∼700 μm apart preferred similar spatial locations, and PFC neurons up to ∼700 μm apart preferred similar motion directions. In contrast, motion-direction tuning similarity between nearby PPC neurons was much weaker and decayed rapidly beyond ∼200 μm. We also observed a similar association between persistent activity and functional clustering in trained recurrent neural network models embedded with a columnar topology. These results suggest that functional clustering facilitates mnemonic encoding of sensory information. SIGNIFICANCE STATEMENT Working memory refers to our ability to temporarily store and manipulate information. Numerous studies have observed that, during working memory, neurons in higher cortical areas, such as the parietal and prefrontal cortices, mnemonically encode the remembered stimulus. However, several recent studies have failed to observe mnemonic encoding during working memory, raising the question as to why mnemonic encoding is observed during some, but not all, conditions. In this study, we show that mnemonic encoding occurs when a cortical area is organized such that nearby neurons preferentially respond to the same stimulus. This result provides plausible neuronal conditions that allow for mnemonic encoding, and gives us further understanding of the brain's mechanisms that support working memory."", 'corpus_id': 207515683, 'score': 1}, {'doc_id': '211259472', 'title': 'Learning to Continually Learn', 'abstract': 'Continual lifelong learning requires an agent or model to learn many sequentially ordered tasks, building on previous knowledge without catastrophically forgetting it. Much work has gone towards preventing the default tendency of machine learning models to catastrophically forget, yet virtually all such work involves manually-designed solutions to the problem. We instead advocate meta-learning a solution to catastrophic forgetting, allowing AI to learn to continually learn. Inspired by neuromodulatory processes in the brain, we propose A Neuromodulated Meta-Learning Algorithm (ANML). It differentiates through a sequential learning process to meta-learn an activation-gating function that enables context-dependent selective activation within a deep neural network. Specifically, a neuromodulatory (NM) neural network gates the forward pass of another (otherwise normal) neural network called the prediction learning network (PLN). The NM network also thus indirectly controls selective plasticity (i.e. the backward pass of) the PLN. ANML enables continual learning without catastrophic forgetting at scale: it produces state-of-the-art continual learning performance, sequentially learning as many as 600 classes (over 9,000 SGD updates).', 'corpus_id': 211259472, 'score': 1}, {'doc_id': '9027434', 'title': 'Analytical regularization method for hollow waveguides modelling', 'abstract': 'Mathematically strong and numerically efficient approach for waveguides and cylindrical resonators of arbitrary smooth profile simulation is suggested. The approach is based on new implementation of Analytical Regularization Method. Numerical simulation of the waveguides and resonators for E-(TM-) polarized waves demonstrate the method efficiency and reliability.', 'corpus_id': 9027434, 'score': 0}, {'doc_id': '55200429', 'title': 'Effects of Amount of Immediate and of Delayed Practice on Retention of Mathematical Rules.', 'abstract': 'ABSTRACT This study was conducted in order to compare the effects of immediate practice and delayed practice on retention of learned rules. Seventh-grade students (N=103) were instructed on three rules of operation with exponents by programmed text. These subjects were randomly assigned to five treatment groups: (1) no practice, (2) one practice trial on the first day, (3) five practice trials on the first day, (4) one practice trial on the fifth day, and (5) five practice trials on the fifth day. On the twenty-first day of the study, all subjects were given a retention test. Findings indicated that delayed practice was significantly more effective than immediate practice as a means of providing for retention. Five delayed trials were not more effective than one delayed trial, nor was immediate practice more effective than no practice. (SD)', 'corpus_id': 55200429, 'score': 1}, {'doc_id': '1087803', 'title': 'Mnemonic networks in the hippocampal formation: from spatial maps to temporal and conceptual codes.', 'abstract': 'The hippocampal formation has been associated with a wide variety of functions including spatial navigation and planning, memory encoding and retrieval, relational processing, novelty detection, and imagination. These functions are dissimilar in terms of their behavioral consequences and modality of representation. Consequently, theoretical standpoints have focused on explaining the role of the hippocampal formation in terms of either its spatial or nonspatial functions. Contrary to this dichotomy, we propose that it is essential to look beyond these traditional boundaries between mnemonic and spatial functions and focus instead on the processes that these functions have in common. In this framework, we use electrophysiology data from the spatial domain to predict effects on the systems level, both in spatial and nonspatial domains. We initially outline the results of studies that have used findings from spatial navigation in rodents to predict the patterns of brain activity observable in people who are exploring virtual environments. We discuss how certain properties of space-defining neurons enable space to be represented as a mental map of interconnected locations, which are expressed at multiple spatial scales in separate modules in the hippocampal formation. We then suggest that memories are also organized in networks, characterized by mnemonic and temporal hierarchies. We finish by discussing how virtual-reality techniques can be used to create novel lifelike episodes allowing us to look at episodic memory processes while multivariate analysis tools can be used to explore the organizational structure of mnemonic networks.', 'corpus_id': 1087803, 'score': 1}, {'doc_id': '136469422', 'title': 'Preliminary Application of Terrestrial 3D LiDar in Corridor Defects Detection of Overhead Transmission Lines', 'abstract': 'Based on a brief introduction of the operational principle of terrestrial 3D LiDar, this paper describes some typical defects detection methods of security distance in the overhead transmission line with living examples. These tests are realized by the Z+F Imager 5010 terrestrial 3D LiDar and Polyworks. The research results show that terrestrial LiDar technology can achieve the defect evaluation of defect detection of security distance in the overhead transmission lines. And this technology has many advantages, such as portable, security, intuitive and high precision.', 'corpus_id': 136469422, 'score': 0}, {'doc_id': '216080707', 'title': 'Adaptive Forgetting Curves for Spaced Repetition Language Learning', 'abstract': 'The forgetting curve has been extensively explored by psychologists, educationalists and cognitive scientists alike. In the context of Intelligent Tutoring Systems, modelling the forgetting curve for each user and knowledge component (e.g. vocabulary word) should enable us to develop optimal revision strategies that counteract memory decay and ensure long-term retention. In this study we explore a variety of forgetting curve models incorporating psychological and linguistic features, and we use these models to predict the probability of word recall by learners of English as a second language. We evaluate the impact of the models and their features using data from an online vocabulary teaching platform and find that word complexity is a highly informative feature which may be successfully learned by a neural network model.', 'corpus_id': 216080707, 'score': 1}]"
129	Freemium access	afdd0f52dc6fa470c5eafc15f669ce47	14327	{}	"[{'doc_id': '230797916', 'title': 'ECONOMIC AND REGIONAL STUDIES', 'abstract': 'Subject and purpose of work: The aim of the article is to present selected aspects of the digitization process of the banking sector in Poland, including the role of the digital channel in the sale of banking products. Materials and methods: The research is of comparative character and is based on data from the financial website PRNews.pl, NBP, KNF and the ECB Bank. Results: Mobile banking in Poland is highly concentrated. In 2016-2019, the five largest banks serviced 82% of all mobile banking customers, and their number increased by 27% annually. Conclusions: Digitization simplifies the process of obtaining and processing information and contributes to reducing operating costs. However it also forces banks to prepare appropriate security and implement costly technological investments. It increases the availability and quality of banking services, and moves customer service from banking branches to the digital channel. Additionally it contributes to reducing the network of branches and the number of employees employed in them.', 'corpus_id': 230797916, 'score': 0}, {'doc_id': '231846690', 'title': 'Policy options for digital infrastructure strategies: A simulation model for broadband universal service in Africa', 'abstract': 'Internet access is essential for economic development and helping to deliver the Sustainable Development Goals, especially as even basic broadband can revolutionize available economic opportunities. Yet, more than one billion people still live without internet access. Governments must make strategic choices to connect these citizens, but currently have few independent, transparent and scientifically reproducible assessments to rely on. This paper develops open-source software to test broadband universal service strategies which meet the 10 Mbps target being considered by the UN Broadband Commission. The private and government costs of different infrastructure decisions are quantified in six East and West African countries (Côte D’Ivoire, Mali, Senegal, Kenya, Tanzania and Uganda). The results provide strong evidence that ‘leapfrogging’ straight to 4G in unconnected areas is the least-cost option for providing broadband universal service, with savings between 13-51% over 3G. The results also demonstrate how the extraction of spectrum and tax revenues in unviable markets provide no net benefit, as for every $1 taken in revenue, a $1 infrastructure subsidy is required from government to achieve broadband universal service. Importantly, the use of a Shared Rural Network in unviable locations provides impressive cost savings (up to 78%), while retaining the benefits of dynamic infrastructure competition in viable urban and suburban areas. This paper provides evidence to design national and international policies aimed at broadband universal service.', 'corpus_id': 231846690, 'score': 1}, {'doc_id': '231638147', 'title': 'Governing data and digital platforms in middle income countries: regulations, competition and industrial policies, with sectoral case studies from South Africa', 'abstract': 'This paper addresses the implications of digital platforms for middle-income countries seeking to build advanced productive capabilities. To do so it develops a new ‘Digital Platform Dynamics’ framework to consider platform functions and platform power in value creation and value extraction along with the potential roles for regulation, competition and industrial policies. The highly heterogeneous nature of digital platforms is considered along with the ways in which value is created and extracted, drawing from international competition cases and inquiries. The framework is developed from a review of the economics of digital platforms and the key development challenges facing countries in overcoming a ‘middle-income technology trap’. To build on local productive capabilities, countries have to link into global value chains, where digital platforms are increasingly important in keeping pace with technological developments, while linking back to local production systems to ensure that dynamic efficiencies are realised. We argue that this requires an ‘entrepreneurial-regulatory state’ to engage with the power and potential of the global digital platforms. The issues identified are considered in the case of South Africa drawing on an established research base on the effects of digitalisation and the importance of platforms in different sectors. We find that South Africa has established competition authorities and industrial capabilities, along with local digital platforms, however, it is grappling with an integrated strategy which aligns competition, regulation and industrial policies for value creation and capture in support of local economic activity. The South Africa sectoral cases highlight the relevance of the Digital Platform Dynamics framework in governing and capturing opportunities from digital platforms in processes of digital industrial development.', 'corpus_id': 231638147, 'score': 1}, {'doc_id': '231843315', 'title': 'Opportunities and challenges of e-commerce in Mauritius', 'abstract': 'This study explores the status, challenges and opportunities of e-commerce in Mauritius. The share of the population making online purchases was 14 per cent in 2017, the secondhighest level (after Libya) in Africa, largely due to increases in internet use and penetration, coupled with increased credit card usage and the development of secure online payment systems. And Mauritius topped the United Nations Conference on Trade and Development (UNCTAD) B2C E-commerce Index (e-readiness) for Africa. A survey of customers revealed high levels of satisfaction with online shopping, due to wider choices, the ability to save time, accessibility and the relative ease of searching for products online. Major concerns included uneasiness over disclosure of personal information and limited ability to contact vendors. Respondents who have not shopped online cited concerns over navigating online, payment security and high costs. Online sellers expressed considerable optimism over future market growth, but also were concerned over a local bias towards international websites, technical limitations of internet service and the small market size. Interviews with policymakers cited the strong legal and regulatory framework supporting electronic payments, but described a need for stronger regulatory cooperation with other countries on e-commerce, and more work to collect statistics. Technical assistance would be useful in these efforts. * The contents of this chapter are the sole responsibility of the authors and are not meant to represent the position or opinions of the WTO or its members.', 'corpus_id': 231843315, 'score': 0}, {'doc_id': '230121353', 'title': 'Monetary Impact of National Exchange Carrier Association Tariffs on Internet Access Cost in Rural Areas', 'abstract': 'This study examines the monetary impact of tariffs on consumer internet access via a digital subscriber line (DSL). Historically, the Communications Act of 1934 mandated that telephony services be provided to everyone, without exclusion. With the rapid emergence of the internet, the Connect America Fund was recently implemented to ensure equal access to both voice and broadband services. Since many telephony companies have emerged as internet service providers, DSL internet service is often bundled together with a landline telephone, especially in rural areas. This study examines the total cost of such bundled DSL internet service across the classes of cities in the state of Nebraska as well as the total monetary impact. In recent years, a rapid reduction in landline telephone service subscribers has occurred. This steady decline in telephone service subscribers supports the idea that consumers typically do not want a landline. However, findings reveal that National Exchange Carrier Association tariffs enforce internet access plans which require the customer to purchase a landline telephone in order to obtain DSL internet access. Findings also reveal substantial cost differences between rural and urban populations’ internet access when factoring in the cost of a required landline telephone. The study holds significant importance for community leaders, state telecommunications regulators and any entity interested in the issue rural/urban equity of internet bandwidth and cost.', 'corpus_id': 230121353, 'score': 1}, {'doc_id': '229679971', 'title': 'A Technological Perspective on Net Neutrality', 'abstract': 'This paper serves as a brief technical examination of Net Neutrality and the Internet fundamentals relevant to the discussion. This document seeks to provide sufficient technical perspective that it may inform the political and economic debate surrounding the issue in the United States. Further, this research demonstrates that existing Internet economics are based strictly on usage, and that this model can account for all uses. Finally, I will argue that there should be some legislation and regulation of ISPs with regard to Net Neutrality in the U.S.', 'corpus_id': 229679971, 'score': 1}, {'doc_id': '229615010', 'title': 'D2C (Direct To Consumer) Business Model: Efficacious Strategy for the Businesses to Grow During COVID-19 Scenario', 'abstract': 'Introduction: This paper aims to analyse D2C (direct to consumer) e-commerce strategy used by businesses or companies to sell to end-consumers directly during Covid-19 in organized retail. \nBackground: The pandemic has fuelled an explosion in online shopping, yet too many brands are only along for the ride, relying on their retail partners to share glimpses of first-party data that show past demand rather than a clear and predictive road map to future growth. The roots of direct marketing date back to trade catalogues, among the first tools of direct marketing. \nMethods: Existing literature on COVID-19 was analysed through secondary information to identify an explosion of D2C (direct to consumer) brands globally and India and its effect on business and commerce. \nConclusion: D2C is becoming the strongest weapon of the businesses against counterfeiters and growing their brand equity at the same time.', 'corpus_id': 229615010, 'score': 0}, {'doc_id': '231968635', 'title': 'COVID-19 AND COMMONWEALTH FDI : IMMEDIATE IMPACTS AND FUTURE PROSPECTS', 'abstract': 'CBC is the recognized Business Member Organization, established as a private sector institution of COMESA. We represent the interests of businesses sectors at a regional level. The services provided go beyond advocacy, to actively promote business participation in regional integration, investment and global trade. This is done by facilitating the growth of strong business synergies, the development of business opportunities, business alliances, legislative and strategic advocacy. We provide custom tailored services that are driven by both industry and enterprise interests. BIZNET WEEKLY', 'corpus_id': 231968635, 'score': 0}, {'doc_id': '231983740', 'title': 'DIGITAL ADOPTION BY INDIAN SMBs', 'abstract': 'mall and Medium Businesses (SMBs) form the backbone of the Indian economy S make large contribu ons to the na onal economic indicators as well as household incomes. Digital technologies have transformed the opera onal landscape of SMBs, mainly led by consumers using mobile internet. Government ac ons, such as the Digital India Ini a ve, have been complemented by private sector investments to raise digital awareness and skills among Indian SMBs. Internet adop on and digi sa on can help a SMB boost its revenue & profits up to 2x faster and scale domes c and interna onal boundaries. However, despite significant digital dividends, digital adop on by Indian SMBs has been rela vely low in the past. Rising internet penetra on, coupled with constraints of the COVID-19 pandemic, are encouraging technology adop on across the sector.', 'corpus_id': 231983740, 'score': 0}, {'doc_id': '168822366', 'title': 'Freemium Internet: Next Generation Business Model to connect next billion', 'abstract': 'This paper analyses a business model for providing free basic-rate Internet to everyone with a data capable phone, living in mobile coverage areas. The model is called Freemium Internet. The term ""freemium"" refers to a well-known business model for digital services, where free services or applications provide basic functions and more advanced functions are available by paying a fee. We examine the impact of applying the freemium business model to the provision of mobile Internet access and discuss net neutrality issues, mobile operator strategies, benefits for consumers, and the potential impact on government e-service programmes. The paper further investigates policy options and regulatory incentives to facilitate the adoption of Freemium Internet.', 'corpus_id': 168822366, 'score': 1}]"
130	DOP	f08692555891e1161c5aef5b2748d7cb	14659	{'DOP': 'dioctyl phthalate'}	"[{'doc_id': '17563796', 'title': 'Bootstrapping structure into language : alignment-based learning', 'abstract': ""refined and abstract meanings largely grow out of more concrete meanings. Bloomfield (1933) \n \nThis thesis introduces a new unsupervised learning framework, called Alignment-Based Learning, which is based on the alignment of sentences and Harris's (1951) notion of substitutability . Instances of the framework can be applied to an untagged, unstructured corpus of natural language sentences, resulting in a labelled, bracketed version of that corpus. Firstly, the framework aligns all sentences in the corpus in pairs, resulting in a partition of the sentences consisting of parts of the sentences that are equal in both sentences and parts that are unequal. Unequal parts of sen tences can be seen as being substitutable for each other, since substituting one unequal part for the other results in another valid sentence. The unequal parts of the sentences are thus considered to be possible (possibly overlapping) constituents, called hypotheses. \n \nSecondly , the selection learning phase considers all hypotheses found by the alignment learning phase and selects the best of these. The hypotheses are selected based on the order in which they were found, or based on a probabilistic function. The framework can be extended with a grammar extraction phase. This extended framework is called parseABL. Instead of returning a structured version of the unstructured input corpus, like the ABL system, this system also returns a stochastic context-free or tree substitution grammar. \n \nDifferent instances of the framework have been tested on the English ATIS corpus, the Dutch OVIS corpus and the Wall Street Journal corpus. One of the interesting results, apart from the encouraging numerical results, is that all instances can (and do) learn recursive structures."", 'corpus_id': 17563796, 'score': 1}, {'doc_id': '209439016', 'title': 'A pr 2 00 1 Bootstrapping Structure using Similarity', 'abstract': 'In this paper a new similarity-based learning algorithm, inspired by string edit-distance (Wagner and Fischer, 1974), is applied to the problem of bootstrapping structure from scratch. The algorithm takes a corpus of unannotated sentences as input and returns a corpus of bracketed sentences. The method works on pairs of unstructured sentences or sentences partially bracketed by the algorithm that have one or more words in common. It finds parts of sentences that are interchangeable (i.e. the parts of the sentences that are different in both sentences). These parts are taken as possible constituents of the same type. While this corresponds to the basic bootstrapping step of the algorithm, further structure may be learned from comparison with other (similar) sentences. We used this method for bootstrapping structure from the flat sentences of the Penn Treebank ATIS corpus, and compared the resulting structured sentences to the structured sentences in the ATIS corpus. Similarly, the algorithm was tested on the OVIS corpus. We obtained 86.04 % non-crossing brackets precision on the ATIS corpus and 89.39 % non-crossing brackets precision on the OVIS corpus.', 'corpus_id': 209439016, 'score': 1}, {'doc_id': '232268898', 'title': 'Parsing Penn Chinese Treebank Based on Lexicalized Model', 'abstract': 'Syntactic parsing is one of the most important technologies of natural language processing. The development of Penn Chinese Treebank (CTB) spurred the research of Chinese parsing. This paper describes a lexicalized statistical Chinese parser. First, a lexicalized model based on hidden Markov model is proposed for part of speech tagging. Second, a well-known lexicalized model i.e. the head-driven model is adapted to parse the automatically POS tagged Chinese sentences. The construction of the parser is described, and the effects of details that can make great difference in the parsing performance are analyzed. On sentences of length less than 100 words, the parser performances at 80.08% precision and 78.45% recall on, surpassing the best published results.', 'corpus_id': 232268898, 'score': 0}, {'doc_id': '233386610', 'title': 'What’s in a Span? Evaluating the Creativity of a Span-Based Neural Constituency Parser', 'abstract': 'Constituency parsing is generally evaluated superficially, particularly in a multiple language setting, with only F-scores being reported. As new state-of-the-art chart-based parsers have resulted in a transition from traditional PCFG-based grammars to span-based approaches (Stern et al., 2017; Gaddy et al., 2018), we do not have a good understanding of how such fundamentally different approaches interact with various treebanks as results show improvements across treebanks (Kitaev and Klein, 2018), but it is unclear what influence annotation schemes have on various treebank performance (Kitaev et al., 2019). In particular, a span-based parser’s capability of creating novel rules is an unknown factor. We perform an analysis of how span-based parsing performs across 11 treebanks in order to examine the overall behavior of this parsing approach and the effect of the treebanks’ specific annotations on results. We find that the parser tends to prefer flatter trees, but the approach works well because it is robust enough to adapt to differences in annotation schemes across treebanks and languages.', 'corpus_id': 233386610, 'score': 0}, {'doc_id': '233023663', 'title': 'ADVERSARIAL TESTING OF STATISTICAL PARSERS : THE CASE OF TEMPORARY AMBIGUITIES', 'abstract': 'Garden-path sentences, containing a temporary attachment ambiguity of a noun phrase as either the object or subject of a sentence, are often studied in psycholinguistics because they can reveal whether verb subcategorization, semantics, or pragmatics are employed as part of on-line human parsing. We propose that these experiments can jump the human/machine divide and be applied for adversarial testing of broadcoverage dependency-based parsers for which little is known outside of black-box testing. In particular, here we use experimentally verified garden-path sentences to examine whether such systems have acquired cognitively accurate knowledge of language. This reveals a surprising lack of cognitive knowledge of certain basic aspects of English, as well as a contrast between the performance of these systems and traditional probabilistic context-free parsers.', 'corpus_id': 233023663, 'score': 0}, {'doc_id': '726421', 'title': 'Bootstrapping structure using similarity', 'abstract': 'In this paper a new similarity-based learning algorithm, inspired by string edit-distance (Wagner and Fischer, 1974), is applied to the problem of bootstrapping structure from scratch. The algorithm takes a corpus of unannotated sentences as input and returns a corpus of bracketed sentences. The method works on pairs of unstructured sentences or sentences partially bracketed by the algorithm that have one or more words in common. It finds parts of sentences that are interchangeable (i.e. the parts of the sentences that are different in both sentences). These parts are taken as possible constituents of the same type. While this corresponds to the basic bootstrapping step of the algorithm, further structure may be learned from comparison with other (similar) sentences. We used this method for bootstrapping structure from the flat sentences of the Penn Treebank ATIS corpus, and compared the resulting structured sentences to the structured sentences in the ATIS corpus. Similarly, the algorithm was tested on the OVIS corpus. We obtained 86.04 % non-crossing brackets precision on the ATIS corpus and 89.39 % non-crossing brackets precision on the OVIS corpus.', 'corpus_id': 726421, 'score': 1}, {'doc_id': '233735504', 'title': 'Parsing the Less-configurational Georgian Language with a Context-Free Grammar', 'abstract': 'A large part of the methodology for Natural Language Processing has been developed for languages with a strong syntactic configuration. At the other end of the configurational spectrum there are languages with rich derivational and inflectional morphology. These languages for morphologically rich and less-configurational features are referred to as MR&LC. In our study we have addressed Georgian a language with less-configurational constraints, though, with a rich inflectional morphology and a very little fixed structure on the sentence level, and therefore, the most syntax-level information for the Georgian language is conveyed by its productive morphology. This paper features issues concerned with development of a crucial NLP resource for the Georgian language a Context-Free Syntactic Parser.', 'corpus_id': 233735504, 'score': 0}, {'doc_id': '233274057', 'title': 'Title: Unsupervised Statistical Learning of Context-free Grammar', 'abstract': 'In this paper, we address the problem of inducing (weighted) context-free grammar (WCFG) on data given. The induction is performed by using a new model of grammatical inference, i.e., weighted Grammar-based Classifier System (wGCS). wGCS derives from learning classifier systems and searches grammar structure using a genetic algorithm and covering. Weights of rules are estimated by using a novelty Inside-Outside Contrastive Estimation algorithm. The proposed method employs direct negative evidence and learns WCFG both form positive and negative samples. Results of experiments on three synthetic context-free languages show that wGCS is competitive with other statistical-based method for unsupervised CFG learning.', 'corpus_id': 233274057, 'score': 0}, {'doc_id': '2276625', 'title': 'Exemplar-based syntax: How to get productivity from examples', 'abstract': 'Abstract Exemplar-based models of language propose that human language production and understanding operate with a store of concrete linguistic experiences rather than with abstract linguistic rules. While exemplar-based models are well acknowledged in areas like phonology and morphology, common wisdom has it that they are intrinsically flawed for syntax where infinite generative capacity is needed. This article shows that this common wisdom is wrong. It starts out by reviewing an exemplar-based syntactic model, known as Data-Oriented Parsing, or DOP, which operates on a corpus of phrase-structure trees. While this model is productive, it is inadequate from the point of grammatical productivity. We therefore extend it to the more sophisticated linguistic representations proposed by Lexical-Functional Grammar theory, resulting in the model known as LFG-DOP, which does allow for meta-linguistic judgments of acceptability. We show how DOP deals with first language acquisition, suggesting a unified model for language learning and language use, and go into a number of syntactic phenomena that can be explained by DOP but that challenge rule-based models. We argue that if there is anything innate in language cognition it is not Universal Grammar but “Universal Representation”.', 'corpus_id': 2276625, 'score': 1}, {'doc_id': '188927610', 'title': 'Representations and Rules in Language', 'abstract': None, 'corpus_id': 188927610, 'score': 1}]"
131	Relation extraction	a0d23bdd9e0a7e77c53e2b5d6f686dcc	4719	{}	"[{'doc_id': '215768766', 'title': 'Coreferential Reasoning Learning for Language Representation', 'abstract': 'Language representation models such as BERT could effectively capture contextual semantic information from plain text, and have been proved to achieve promising results in lots of downstream NLP tasks with appropriate fine-tuning. However, existing language representation models seldom consider coreference explicitly, the relationship between noun phrases referring to the same entity, which is essential to a coherent understanding of the whole discourse. To address this issue, we present CorefBERT, a novel language representation model designed to capture the relations between noun phrases that co-refer to each other. According to the experimental results, compared with existing baseline models, the CorefBERT model has made significant progress on several downstream NLP tasks that require coreferential reasoning, while maintaining comparable performance to previous models on other common NLP tasks.', 'corpus_id': 215768766, 'score': 0}, {'doc_id': '52118895', 'title': 'Multi-Task Identification of Entities, Relations, and Coreference for Scientific Knowledge Graph Construction', 'abstract': 'We introduce a multi-task setup of identifying entities, relations, and coreference clusters in scientific articles. We create SciERC, a dataset that includes annotations for all three tasks and develop a unified framework called SciIE with shared span representations. The multi-task setup reduces cascading errors between tasks and leverages cross-sentence relations through coreference links. Experiments show that our multi-task model outperforms previous models in scientific information extraction without using any domain-specific features. We further show that the framework supports construction of a scientific knowledge graph, which we use to analyze information in scientific literature.', 'corpus_id': 52118895, 'score': 1}, {'doc_id': '44163645', 'title': 'SemEval-2018 Task 7: Semantic Relation Extraction and Classification in Scientific Papers', 'abstract': 'This paper describes the first task on semantic relation extraction and classification in scientific paper abstracts at SemEval 2018. The challenge focuses on domain-specific semantic relations and includes three different subtasks. The subtasks were designed so as to compare and quantify the effect of different pre-processing steps on the relation classification results. We expect the task to be relevant for a broad range of researchers working on extracting specialized knowledge from domain corpora, for example but not limited to scientific or bio-medical information extraction. The task attracted a total of 32 participants, with 158 submissions across different scenarios.', 'corpus_id': 44163645, 'score': 1}, {'doc_id': '218630327', 'title': 'Document-Level Event Role Filler Extraction using Multi-Granularity Contextualized Encoding', 'abstract': 'Few works in the literature of event extraction have gone beyond individual sentences to make extraction decisions. This is problematic when the information needed to recognize an event argument is spread across multiple sentences. We argue that document-level event extraction is a difficult task since it requires a view of a larger context to determine which spans of text correspond to event role fillers. We first investigate how end-to-end neural sequence models (with pre-trained language model representations) perform on document-level role filler extraction, as well as how the length of context captured affects the models’ performance. To dynamically aggregate information captured by neural representations learned at different levels of granularity (e.g., the sentence- and paragraph-level), we propose a novel multi-granularity reader. We evaluate our models on the MUC-4 event extraction dataset, and show that our best system performs substantially better than prior work. We also report findings on the relationship between context length and neural model performance on the task.', 'corpus_id': 218630327, 'score': 0}, {'doc_id': '59599768', 'title': 'Extracting Multiple-Relations in One-Pass with Pre-Trained Transformers', 'abstract': 'Many approaches to extract multiple relations from a paragraph require multiple passes over the paragraph. In practice, multiple passes are computationally expensive and this makes difficult to scale to longer paragraphs and larger text corpora. In this work, we focus on the task of multiple relation extractions by encoding the paragraph only once. We build our solution upon the pre-trained self-attentive models (Transformer), where we first add a structured prediction layer to handle extraction between multiple entity pairs, then enhance the paragraph embedding to capture multiple relational information associated with each entity with entity-aware attention. We show that our approach is not only scalable but can also perform state-of-the-art on the standard benchmark ACE 2005.', 'corpus_id': 59599768, 'score': 1}, {'doc_id': '218628819', 'title': 'PERLEX: A Bilingual Persian-English Gold Dataset for Relation Extraction', 'abstract': 'Relation extraction is the task of extracting semantic relations between entities in a sentence. It is an essential part of some natural language processing tasks such as information extraction, knowledge extraction, and knowledge base population. The main motivations of this research stem from a lack of a dataset for relation extraction in the Persian language as well as the necessity of extracting knowledge from the growing big-data in the Persian language for different applications. In this paper, we present ""PERLEX"" as the first Persian dataset for relation extraction, which is an expert-translated version of the ""Semeval-2010-Task-8"" dataset. Moreover, this paper addresses Persian relation extraction utilizing state-of-the-art language-agnostic algorithms. We employ six different models for relation extraction on the proposed bilingual dataset, including a non-neural model (as the baseline), three neural models, and two deep learning models fed by multilingual-BERT contextual word representations. The experiments result in the maximum f-score 77.66% (provided by BERTEM-MTB method) as the state-of-the-art of relation extraction in the Persian language.', 'corpus_id': 218628819, 'score': 0}, {'doc_id': '56481808', 'title': 'Leveraging Unannotated Texts for Scientific Relation Extraction', 'abstract': 'A tremendous amount of knowledge is present in the evergrowing scientific literature. In order to efficiently grasp such knowledge, various computational tasks are proposed that train machines to read and analyze scientific documents. One of these tasks, Scientific Relation Extraction, aims at automatically capturing scientific semantic relationships among entities in scientific documents. Conventionally, only a limited number of commonly used knowledge bases, such as Wikipedia, are used as a source of background knowledge for relation extraction. In this work, we hypothesize that unannotated scientific papers could also be utilized as a source of external background information for relation extraction. Based on our hypothesis, we propose a model that is capable of extracting background information from unannotated scientific papers. Our experiments on the RANIS corpus [1] prove the effectiveness of the proposed model on relation extraction from scientific articles. key words: relation extraction, scientific document, word embedding, semantically related word', 'corpus_id': 56481808, 'score': 1}, {'doc_id': '211268325', 'title': 'Probing Linguistic Features of Sentence-Level Representations in Neural Relation Extraction', 'abstract': 'Despite the recent progress, little is known about the features captured by state-of-the-art neural relation extraction (RE) models. Common methods encode the source sentence, conditioned on the entity mentions, before classifying the relation. However, the complexity of the task makes it difficult to understand how encoder architecture and supporting linguistic knowledge affect the features learned by the encoder. We introduce 14 probing tasks targeting linguistic properties relevant to RE, and we use them to study representations learned by more than 40 different encoder architecture and linguistic feature combinations trained on two datasets, TACRED and SemEval 2010 Task 8. We find that the bias induced by the architecture and the inclusion of linguistic features are clearly expressed in the probing task performance. For example, adding contextualized word representations greatly increases performance on probing tasks with a focus on named entity and part-of-speech information, and yields better results in RE. In contrast, entity masking improves RE, but considerably lowers performance on entity type related probing tasks.', 'corpus_id': 211268325, 'score': 0}, {'doc_id': '202784158', 'title': 'Multi-Input Multi-Output Sequence Labeling for Joint Extraction of Fact and Condition Tuples from Scientific Text', 'abstract': 'Condition is essential in scientific statement. Without the conditions (e.g., equipment, environment) that were precisely specified, facts (e.g., observations) in the statements may no longer be valid. Existing ScienceIE methods, which aim at extracting factual tuples from scientific text, do not consider the conditions. In this work, we propose a new sequence labeling framework (as well as a new tag schema) to jointly extract the fact and condition tuples from statement sentences. The framework has (1) a multi-output module to generate one or multiple tuples and (2) a multi-input module to feed in multiple types of signals as sequences. It improves F1 score relatively by 4.2% on BioNLP2013 and by 6.2% on a new bio-text dataset for tuple extraction.', 'corpus_id': 202784158, 'score': 1}, {'doc_id': '213004368', 'title': 'Beheshti-NER: Persian Named Entity Recognition Using BERT', 'abstract': 'Named entity recognition is a natural language processing task to recognize and extract spans of text associated with named entities and classify them in semantic Categories. \nGoogle BERT is a deep bidirectional language model, pre-trained on large corpora that can be fine-tuned to solve many NLP tasks such as question answering, named entity recognition, part of speech tagging and etc. In this paper, we use the pre-trained deep bidirectional network, BERT, to make a model for named entity recognition in Persian. \nWe also compare the results of our model with the previous state of the art results achieved on Persian NER. Our evaluation metric is CONLL 2003 score in two levels of word and phrase. This model achieved second place in NSURL-2019 task 7 competition which associated with NER for the Persian language. our results in this competition are 83.5 and 88.4 f1 CONLL score respectively in phrase and word level evaluation.', 'corpus_id': 213004368, 'score': 0}]"
132	Gamification	36673f9e5251791d3bf2b01d81a6c5c5	4681	{}	"[{'doc_id': '215814147', 'title': 'SportsXR - Immersive Analytics in Sports', 'abstract': 'We wish to thank Coach Kathy Delaney-Smith, Mike Roux, Mark Kaliris, and Lindsay Werner at Harvard Women’s Basketball, and Mike Sotsky and Casey Brinn at Harvard Men’s Basketball for their time and expertise. This research is supported in part by King Abdullah University of Science and Technology (KAUST) and the KAUST Office of Sponsored Research (OSR) award OSR-2015-CCF-2533-01.', 'corpus_id': 215814147, 'score': 0}, {'doc_id': '219721421', 'title': 'On the environment-destructive probabilistic trends: a perceptual and behavioral study on video game players', 'abstract': ""Currently, gaming is the world's favorite form of entertainment. Various studies have shown how games impact players' perceptions and behaviors, prompting opportunities for purposes beyond entertainment. This study uses Animal Crossing: New Horizons (ACNH), a real-time life-simulation game, as a unique case study of how video games can affect humans' environmental perceptions. A dataset of 584 observations from a survey of ACNH players and the Hamiltonian MCMC technique has enabled us to explore the relationship between in-game behaviors and perceptions. The findings indicate a probabilistic trend towards exploiting the in-game environment despite players' perceptions, suggesting that the simplification of commercial game design may overlook opportunities to engage players in pro-environmental activities."", 'corpus_id': 219721421, 'score': 1}, {'doc_id': '218719915', 'title': 'User Attention and Behaviour in Virtual Reality Art Encounter', 'abstract': 'With the proliferation of consumer virtual reality (VR) headsets and creative tools, content creators have started to experiment with new forms of interactive audience experience using immersive media. Understanding user attention and behaviours in virtual environment can greatly inform creative processes in VR. We developed an abstract VR painting and an experimentation system to study audience encounters through eye gaze and movement tracking. The data from a user experiment with 35 participants reveal a range of user activity patterns in art exploration. Deep learning models are used to study the connections between behavioural data and audience background. New integrated methods to visualise user attention as part of the artwork are also developed as a feedback loop to the content creator.', 'corpus_id': 218719915, 'score': 0}, {'doc_id': '219573376', 'title': 'When Science is a Game', 'abstract': ""What happens when scientists are, at certain points in a field's development, playing a game? I present a framework for such an analysis that draws on the theory of games provided by the historian Johan Huizinga. Huizinga gives five conditions for a social practice to become a game: free engagement, disconnection, boundedness in time and arena, the order-creation of rules, and the presence of tension. Application of this theory to scientific practice predicts patterns of behavior that can be tested by quantitative analysis: the emergence of hard boundaries between disciplines, the closure of loopholes in theory creation, resistance to certain innovations in journal publication, and the ways in which scientists fail to prosecute colleagues who engage in questionable research practices."", 'corpus_id': 219573376, 'score': 1}, {'doc_id': '215744827', 'title': 'Computers in Secondary Schools: Educational Games', 'abstract': 'This entry introduces educational games in secondary schools. Educational games include three main types of educational activities with a playful learning intention supported by digital technologies: educational serious games, educational gamification, and learning through game creation. Educational serious games are digital games that support learning objectives. Gamification is defined as the use of ""game design elements and game thinking in a non-gaming context"" (Deterding et al. 2011, p. 13). Educational gamification is not developed through a digital game but includes game elements for supporting the learning objectives. Learning through game creation is focused on the process of designing and creating a prototype of a game to support a learning process related to the game creation process or the knowledge mobilized through the game creation process. Four modalities of educational games in secondary education are introduced in this entry to describe educational games in secondary education: educational purpose of entertainment games, serious games, gamification, and game design.', 'corpus_id': 215744827, 'score': 0}, {'doc_id': '219955873', 'title': 'Data-Driven Game Development: Ethical Considerations', 'abstract': 'In recent years, the games industry has made a major move towards data-driven development, using data analytics and player modeling to inform design decisions. Data-driven techniques are beneficial as they allow for the study of player behavior at scale, making them very applicable to modern digital game development. However, with this move towards data driven decision-making comes a number of ethical concerns. Previous work in player modeling [45] as well as work in the fields of AI and machine learning [9, 53] have demonstrated several ways in which algorithmic decision-making can be flawed due to data or algorithmic bias or lack of data from specific groups. Further, black box algorithms create a trust problem due to lack of interpretability and transparency of the results or models developed based on the data, requiring blind faith in the results. In this position paper, we discuss several factors affecting the use of game data in the development cycle. In addition to issues raised by previous work, we also raise issues with algorithms marginalizing certain player groups and flaws in the resulting models due to their inability to reason about situational factors affecting players’ decisions. Further, we outline some work that seeks to address these problems and identify some open problems concerning ethics and game data science.', 'corpus_id': 219955873, 'score': 1}, {'doc_id': '4564962', 'title': 'Gamification. using game-design elements in non-gaming contexts', 'abstract': '""Gamification"" is an informal umbrella term for the use of video game elements in non-gaming systems to improve user experience (UX) and user engagement. The recent introduction of \'gamified\' applications to large audiences promises new additions to the existing rich and diverse research on the heuristics, design patterns and dynamics of games and the positive UX they provide. However, what is lacking for a next step forward is the integration of this precise diversity of research endeavors. Therefore, this workshop brings together practitioners and researchers to develop a shared understanding of existing approaches and findings around the gamification of information systems, and identify key synergies, opportunities, and questions for future research.', 'corpus_id': 4564962, 'score': 1}, {'doc_id': '219955919', 'title': 'Reflection in Game-Based Learning: A Survey of Programming Games', 'abstract': 'Reflection is a critical aspect of the learning process. However, educational games tend to focus on supporting learning concepts rather than supporting reflection. While reflection occurs in educational games, the educational game design and research community can benefit from more knowledge of how to facilitate player reflection through game design. In this paper, we examine educational programming games and analyze how reflection is currently supported. We find that current approaches prioritize accuracy over the individual learning process and often only support reflection post-gameplay. Our analysis identifies common reflective features, and we develop a set of open areas for future work. We discuss these promising directions towards engaging the community in developing more mechanics for reflection in educational games.', 'corpus_id': 219955919, 'score': 0}, {'doc_id': '215768748', 'title': 'Embracing Companion Technologies', 'abstract': 'As an increasing number of interactive devices offer human-like assistance, there is a growing need to understand our experience of interactive agents. When interactive artefacts become intertwined in our everyday experience, we need to make sure that they assume the right roles and contribute to our wellbeing. In this theoretical exploration, we propose a reframing of our understanding of the experience of interactions with everyday technologies by proposing the metaphor of companion technologies. We employ theory in the philosophy of empathy to propose a framework for understanding how users develop relationships with digital agents. The experiential framework for companion technologies provides connections between the users’ psychological needs and companion features of interactive systems. Our work provides a theoretical basis for rethinking the user experience of everyday artefacts with an empathy-oriented mindset and poses future challenges for HCI.', 'corpus_id': 215768748, 'score': 0}, {'doc_id': '220438671', 'title': 'HCI in Games: Second International Conference, HCI-Games 2020, Held as Part of the 22nd HCI International Conference, HCII 2020, Copenhagen, Denmark, July 19–24, 2020, Proceedings', 'abstract': 'General game-playing artificial intelligence (AI) has recently seen important advances due to the various techniques known as ‘deep learning’. However, in terms of human-computer interaction, the advances conceal a major limitation: these algorithms do not incorporate any sense of what human players find meaningful in games. I argue that adaptive game AI will be enhanced by a generalised player model, because games are inherently human artefacts which require some encoding of the human perspective in order to respond naturally to individual players. The player model provides constraints on the adaptive AI, which allow it to encode aspects of what human players find meaningful. I propose that a general player model requires parameters for the subjective experience of play, including: player psychology, game structure, and actions of play. I argue that such a player model would enhance efficiency of per-game solutions, and also support study of game-playing by allowing (within-player) comparison between games, or (within-game) comparison between players (human and AI). Here we detail requirements for functional adaptive AI, arguing from first-principles drawn from games research literature, and propose a formal specification for a generalised player model based on our ‘Behavlets’ method for psychologically-derived player modelling.', 'corpus_id': 220438671, 'score': 1}]"
133	Currencies	dfcfc43722eef1eab1e4a12e50a068b1	14959	{}	[{'doc_id': '231699106', 'title': 'Uncertainty Network Risk and Currency Returns', 'abstract': 'We examine the pricing of a horizon specific uncertainty network risk, extracted from option implied variances on exchange rates, in the cross-section of currency returns. Buying currencies that are receivers and selling currencies that are transmitters of short-term shocks exhibits a high Sharpe ratio and yields a significant alpha when controlling for standard dollar, carry trade, volatility, variance risk premium and momentum strategies. This profitability stems primarily from the causal nature of shock propagation and not from contemporaneous dynamics. Shock propagation at longer horizons is priced less, indicating a downward-sloping term structure of uncertainty network risk in currency markets.', 'corpus_id': 231699106, 'score': 1}, {'doc_id': '230102613', 'title': 'COVID-19 and the liquidity crisis of non-banks: lessons for the future', 'abstract': 'At the same time, the crisis has been a stark reminder that there are still considerable vulnerabilities in the financial sector. In particular, there has been a divergence between the comparatively lean regulation of the non-bank financial sector and its increasing role in financial intermediation across the globe. This divergence has measurably augmented the risks of perilous macro-financial feedback loops, which may also affect the conduct of monetary policy.', 'corpus_id': 230102613, 'score': 0}, {'doc_id': '231839694', 'title': 'Liquidity Stress Testing using Optimal Portfolio Liquidation', 'abstract': 'We build an optimal portfolio liquidation model for OTC markets, aiming at minimizing the trading costs via the choice of the liquidation time. We work in the Locally Linear Order Book framework of [12] to obtain the market impact as a function of the traded volume. We find that the optimal terminal time for a linear execution of a small order is proportional to the square root of the ratio between the amount being bought or sold and the average daily volume. Numerical experiments on real market data illustrate the method on a portfolio of corporate bonds.', 'corpus_id': 231839694, 'score': 0}, {'doc_id': '152995588', 'title': 'Economic Momentum and Currency Returns', 'abstract': 'Past trends in fundamentals linked to economic activity and inflation predict currency returns. We find that a trading strategy that goes long currencies with strong economic momentum and short currencies with weak economic momentum exhibits an annualized Sharpe ratio of 0.70 and yields a significant alpha when controlling for standard carry, momentum, and value strategies. The economic momentum strategy subsumes the alpha of carry trades, suggesting that differences in past economic trends capture cross-country differences in carry.', 'corpus_id': 152995588, 'score': 1}, {'doc_id': '229938319', 'title': 'CLO Performance', 'abstract': 'We study the performance of collateralized loan obligations (CLOs) to understand the market imperfections giving rise to these vehicles and their corresponding economic costs. CLO equity tranches earn positive abnormal returns from the risk-adjusted price differential between leveraged loans and CLO debt tranches. Debt tranches offer higher returns than similarly rated corporate bonds, making them attractive to banks and insurers that face risk-based capital requirements. Temporal variation in equity performance highlights the resilience of CLOs to market volatility due to their closed-end structure, long-term funding, and embedded options to reinvest principal proceeds.', 'corpus_id': 229938319, 'score': 0}, {'doc_id': '231632378', 'title': 'Dynamic Industry Uncertainty Networks and the Business Cycle', 'abstract': 'We argue that uncertainty network structures extracted from option prices contain valuable information for business cycles. Classifying U.S. industries according to their contribution to system-related uncertainty across business cycles, we uncover an uncertainty hub role for the communications, industrials and information technology sectors, while shocks to materials, real estate and utilities do not create strong linkages in the network. Moreover, we find that this ex-ante network of uncertainty is a useful predictor of business cycles, especially when it is based on uncertainty hubs. The industry uncertainty network behaves counter-cyclically in that a tighter network tends to associate with future business cycle contractions.', 'corpus_id': 231632378, 'score': 1}, {'doc_id': '230523654', 'title': 'Optimal Hedging with Margin Constraints and Default Aversion and its Application to Bitcoin Perpetual Futures', 'abstract': 'We consider a futures hedging problem subject to a budget constraint that limits the ability of a hedger with default aversion to meet margin requirements. We derive a semi-closed form for an optimal hedging strategy with dual objectives – to minimize both the variance of the hedged portfolio and the probability of forced liquidations due to margin calls. An empirical analysis of bitcoin shows that the optimal strategy not only achieves superior hedge effectiveness, but also reduces the probability of forced liquidations to an acceptable level. We also compare how the hedger’s default aversion impacts the performance of optimal hedging based on minute-level data across major bitcoin spot and perpetual futures markets.', 'corpus_id': 230523654, 'score': 0}, {'doc_id': '231779615', 'title': 'Changes in monetary policy operating procedures over the last decade: insights from a new database', 'abstract': 'We introduce a new interactive database that allows users to easily retrieve and customise detailed information on central banks’ monetary policy operating procedures (MPOPs). These procedures govern the day-to-day implementation of monetary policy in markets. After a highlevel conceptual overview of how MPOPs have evolved over the past decade, we showcase common trends and selected cross-country differences. We discuss, in particular, how the persistent environment of excess liquidity and the effective interest rate lower bound shaped MPOPs in the aftermath of the Great Financial Crisis of 2007–09.', 'corpus_id': 231779615, 'score': 1}, {'doc_id': '232054128', 'title': 'Does Regulatory Cooperation Help Integrate Equity Markets?', 'abstract': 'This study tests whether cooperation between securities regulators influences global market integration. I measure cooperation using arrangements between securities regulators that enable enhanced cross-border enforcement, better regulatory decisions, and reduced compliance obligations for cross-border activities. These arrangements—formed at different times for different country pairs—are associated with an 11% increase in cross-border investment. I find similar increases using other proxies for market integration. Cross-border investment and market integration thus depend, in part, on regulators working together to extend legal and institutional capacities across borders. This reframes our understanding of the role of institutions in global capital markets.', 'corpus_id': 232054128, 'score': 0}, {'doc_id': '219389258', 'title': 'Business Cycles and Currency Returns', 'abstract': 'We find a strong link between currency excess returns and the relative strength of the business cycle. Buying currencies of strong economies and selling currencies of weak economies generates high returns both in the cross section and time series of countries. These returns stem primarily from spot exchange rate predictability, are uncorrelated with common currency investment strategies, and cannot be understood using traditional currency risk factors in either unconditional or conditional asset pricing tests. We also show that a business cycle factor implied by our results is priced in a broad currency cross section.', 'corpus_id': 219389258, 'score': 1}]
134	Audio2Score alignment	2d40a835cb8318ab4900d913bb382d30	19762	{}	[{'doc_id': '236318462', 'title': 'Multi-Channel Automatic Music Transcription Using Tensor Algebra', 'abstract': 'Music is an art, perceived in unique ways by every listener, coming from acoustic signals. In the meantime, standards as musical scores exist to describe it. Even if humans can make this transcription, it is costly in terms of time and efforts, even more with the explosion of information consecutively to the rise of the Internet. In that sense, researches are driven in the direction of Automatic Music Transcription. While this task is considered solved in the case of single notes, it is still open when notes superpose themselves, forming chords. This report aims at developing some of the existing techniques towards Music Transcription, particularly matrix factorization, and introducing the concept of multi-channel automatic music transcription. This concept will be explored with mathematical objects called tensors.', 'corpus_id': 236318462, 'score': 0}, {'doc_id': '236957162', 'title': 'A Unified Model for Zero-shot Music Source Separation, Transcription and Synthesis', 'abstract': 'We propose a unified model for three inter-related tasks: 1) to separate individual sound sources from a mixed music audio, 2) to transcribe each sound source to MIDI notes, and 3) to synthesize new pieces based on the timbre of separated sources. The model is inspired by the fact that when humans listen to music, our minds can not only separate the sounds of different instruments, but also at the same time perceive high-level representations such as score and timbre. To mirror such capability computationally, we designed a pitch-timbre disentanglement module based on a popular encoder-decoder neural architecture for source separation. The key inductive biases are vector-quantization for pitch representation and pitchtransformation invariant for timbre representation. In addition, we adopted a query-by-example method to achieve zero-shot learning, i.e., the model is capable of doing source separation, transcription, and synthesis for unseen instruments. The current design focuses on audio mixtures of two monophonic instruments. Experimental results show that our model outperforms existing multi-task baselines, and the transcribed score serves as a powerful auxiliary for separation tasks.', 'corpus_id': 236957162, 'score': 0}, {'doc_id': '226964998', 'title': 'Learning Frame Similarity using Siamese networks for Audio-to-Score Alignment', 'abstract': 'Audio-to-score alignment aims at generating an accurate mapping between a performance audio and the score of a given piece. Standard alignment methods are based on Dynamic Time Warping (DTW) and employ handcrafted features, which cannot be adapted to different acoustic conditions. We propose a method to overcome this limitation using learned frame similarity for audio-to-score alignment. We focus on offline audio-to-score alignment of piano music. Experiments on music data from different acoustic conditions demonstrate that our method achieves higher alignment accuracy than a standard DTW-based method that uses handcrafted features, and generates robust alignments whilst being adaptable to different domains at the same time.', 'corpus_id': 226964998, 'score': 1}, {'doc_id': '236635109', 'title': 'On-Line Audio-to-Lyrics Alignment Based on a Reference Performance', 'abstract': 'Audio-to-lyrics alignment has become an increasingly active research task in MIR, supported by the emergence of several open-source datasets of audio recordings with word-level lyrics annotations. However, there are still a number of open problems, such as a lack of robustness in the face of severe duration mismatches between audio and lyrics representation; a certain degree of language-specificity caused by acoustic differences across languages; and the fact that most successful methods in the field are not suited to work in real-time. Real-time lyrics alignment (tracking) would have many useful applications, such as fully automated subtitle display in live concerts and opera. In this work, we describe the first real-timecapable audio-to-lyrics alignment pipeline that is able to robustly track the lyrics of different languages, without additional language information. The proposed model predicts, for each audio frame, a probability vector over (European) phoneme classes, using a very small temporal context, and aligns this vector with a phoneme posteriogram matrix computed beforehand from another recording of the same work, which serves as a reference and a proxy to the written-out lyrics. We evaluate our system’s tracking accuracy on the challenging genre of classical opera. Finally, robustness to out-of-training languages is demonstrated in an experiment on Jingju (Beijing opera).', 'corpus_id': 236635109, 'score': 0}, {'doc_id': '6222246', 'title': 'An Approach to Score Following for Piano Performances With the Sustained Effect', 'abstract': 'One challenge in score following for piano music is the sustained effect, i.e., the waveform of a note lasts longer than what is notated in the score. This can be caused by expressive performing styles such as the legato articulation and the usage of the sustain and the sostenuto pedals and can also be caused by the reverberation in the recording environment. This effect creates nonnotated overlappings between sustained notes and latter notes in the audio. It decreases the audio-score alignment accuracy and robustness of score following systems and makes them be prone to delay errors, i.e., aligning audio to a score position that is earlier than the correct position. In this paper, we propose to modify the feature representation of the audio to attenuate the sustained effect. We show that this idea can be applied to both the chromagram and the spectral-peak representations, which are commonly used in score following systems. Experiments on the MAPS dataset show that the proposed method significantly improves the alignment accuracy and robustness of score following systems for piano performances, in both anechoic and highly reverberant environments.', 'corpus_id': 6222246, 'score': 1}, {'doc_id': '236493514', 'title': 'Pitch-Informed Instrument Assignment Using a Deep Convolutional Network with Multiple Kernel Shapes', 'abstract': 'This paper proposes a deep convolutional neural network for performing note-level instrument assignment. Given a polyphonic multi-instrumental music signal along with its ground truth or predicted notes, the objective is to assign an instrumental source for each note. This problem is addressed as a pitch-informed classification task where each note is analysed individually. We also propose to utilise several kernel shapes in the convolutional layers in order to facilitate learning of efficient timbre-discriminative feature maps. Experiments on the MusicNet dataset using 7 instrument classes show that our approach is able to achieve an average F-score of 0.904 when the original multi-pitch annotations are used as the pitch information for the system, and that it also excels if the note information is provided using third-party multi-pitch estimation algorithms. We also include ablation studies investigating the effects of the use of multiple kernel shapes and comparing different input representations for the audio and the note-related information.', 'corpus_id': 236493514, 'score': 0}, {'doc_id': '236134377', 'title': 'Sequence-to-Sequence Piano Transcription with Transformers', 'abstract': 'Automatic Music Transcription has seen significant progress in recent years by training custom deep neural networks on large datasets. However, these models have required extensive domain-specific design of network architectures, input/output representations, and complex decoding schemes. In this work, we show that equivalent performance can be achieved using a generic encoderdecoder Transformer with standard decoding methods. We demonstrate that the model can learn to translate spectrogram inputs directly to MIDI-like output events for several transcription tasks. This sequence-to-sequence approach simplifies transcription by jointly modeling audio features and language-like output dependencies, thus removing the need for task-specific architectures. These results point toward possibilities for creating new Music Information Retrieval models by focusing on dataset creation and labeling rather than custom model design.', 'corpus_id': 236134377, 'score': 0}, {'doc_id': '3521879', 'title': 'Score-Informed Identification of Missing and Extra Notes in Piano Recordings', 'abstract': 'A main goal in music tuition is to enable a student to play a score without mistakes, where common mistakes include missing notes or playing additional extra ones. To automatically detect these mistakes, a first idea is to use a music transcription method to detect notes played in an audio recording and to compare the results with a corresponding score. However, as the number of transcription errors produced by standard methods is often considerably higher than the number of actual mistakes, the results are often of limited use. In contrast, our method exploits that the score already provides rough information about what we seek to detect in the audio, which allows us to construct a tailored transcription method. In particular, we employ score-informed source separation techniques to learn for each score pitch a set of templates capturing the spectral properties of that pitch. After extrapolating the resulting template dictionary to pitches not in the score, we estimate the activity of each MIDI pitch over time. Finally, making again use of the score, we choose for each pitch an individualized threshold to differentiate note onsets from spurious activity in an optimized way. We indicate the accuracy of our approach on a dataset of piano pieces commonly used in education.', 'corpus_id': 3521879, 'score': 1}, {'doc_id': '236447646', 'title': 'Audio-to-Score Alignment Using Deep Automatic Music Transcription', 'abstract': 'Audio-to-score alignment (A2SA) is a multimodal task consisting in the alignment of audio signals to music scores. Recent literature confirms the benefits of Automatic Music Transcription (AMT) for A2SA at the frame-level. In this work, we aim to elaborate on the exploitation of AMT Deep Learning (DL) models for achieving alignment at the note-level. We propose a method which benefits from HMM-based score-toscore alignment and AMT, showing a remarkable advancement beyond the state-of-the-art. We design a systematic procedure to take advantage of large datasets which do not offer an aligned score. Finally, we perform a thorough comparison and extensive tests on multiple datasets.', 'corpus_id': 236447646, 'score': 1}, {'doc_id': '7181659', 'title': 'Audio-to-score alignment of piano music using RNN-based automatic music transcription', 'abstract': 'We propose a framework for audio-to-score alignment on piano performance that employs automatic music transcription (AMT) using neural networks. Even though the AMT result may contain some errors, the note prediction output can be regarded as a learned feature representation that is directly comparable to MIDI note or chroma representation. To this end, we employ two recurrent neural networks that work as the AMT-based feature extractors to the alignment algorithm. One predicts the presence of 88 notes or 12 chroma in frame-level and the other detects note onsets in 12 chroma. We combine the two types of learned features for the audio-to-score alignment. For comparability, we apply dynamic time warping as an alignment algorithm without any additional post-processing. We evaluate the proposed framework on the MAPS dataset and compare it to previous work. The result shows that the alignment framework with the learned features significantly improves the accuracy, achieving less than 10 ms in mean onset error.', 'corpus_id': 7181659, 'score': 1}]
135	Novel NNs	6809e48ad630eaf104794851269f4e76	6078	{}	"[{'doc_id': '219792967', 'title': 'A Shooting Formulation of Deep Learning', 'abstract': 'Continuous-depth neural networks can be viewed as deep limits of discrete neural networks whose dynamics resemble a discretization of an ordinary differential equation (ODE). Although important steps have been taken to realize the advantages of such continuous formulations, most current techniques are not truly continuous-depth as they assume identical layers. Indeed, existing works throw into relief the myriad difficulties presented by an infinite-dimensional parameter space in learning a continuous-depth neural ODE. To this end, we introduce a shooting formulation which shifts the perspective from parameterizing a network layer-by-layer to parameterizing over optimal networks described only by a set of initial conditions. For scalability, we propose a novel particle-ensemble parametrization which fully specifies the optimal weight trajectory of the continuous-depth neural network. Our experiments show that our particle-ensemble shooting formulation can achieve competitive performance, especially on long-range forecasting tasks. Finally, though the current work is inspired by continuous-depth neural networks, the particle-ensemble shooting formulation also applies to discrete-time networks and may lead to a new fertile area of research in deep learning parametrization.', 'corpus_id': 219792967, 'score': 1}, {'doc_id': '219687376', 'title': 'Finding trainable sparse networks through Neural Tangent Transfer', 'abstract': 'Deep neural networks have dramatically transformed machine learning, but their memory and energy demands are substantial. The requirements of real biological neural networks are rather modest in comparison, and one feature that might underlie this austerity is their sparse connectivity. In deep learning, trainable sparse networks that perform well on a specific task are usually constructed using label-dependent pruning criteria. In this article, we introduce Neural Tangent Transfer, a method that instead finds trainable sparse networks in a label-free manner. Specifically, we find sparse networks whose training dynamics, as characterized by the neural tangent kernel, mimic those of dense networks in function space. Finally, we evaluate our label-agnostic approach on several standard classification tasks and show that the resulting sparse networks achieve higher classification performance while converging faster.', 'corpus_id': 219687376, 'score': 1}, {'doc_id': '219720969', 'title': 'MetaSDF: Meta-learning Signed Distance Functions', 'abstract': 'Neural implicit shape representations are an emerging paradigm that offers many potential benefits over conventional discrete representations, including memory efficiency at a high spatial resolution. Generalizing across shapes with such neural implicit representations amounts to learning priors over the respective function space and enables geometry reconstruction from partial or noisy observations. Existing generalization methods rely on conditioning a neural network on a low-dimensional latent code that is either regressed by an encoder or jointly optimized in the auto-decoder framework. Here, we formalize learning of a shape space as a meta-learning problem and leverage gradient-based meta-learning algorithms to solve this task. We demonstrate that this approach performs on par with auto-decoder based approaches while being an order of magnitude faster at test-time inference. We further demonstrate that the proposed gradient-based method outperforms encoder-decoder based methods that leverage pooling-based set encoders.', 'corpus_id': 219720969, 'score': 0}, {'doc_id': '219558713', 'title': 'Learning to Stop While Learning to Predict', 'abstract': ""There is a recent surge of interest in designing deep architectures based on the update steps in traditional algorithms, or learning neural networks to improve and replace traditional algorithms. While traditional algorithms have certain stopping criteria for outputting results at different iterations, many algorithm-inspired deep models are restricted to a ``fixed-depth'' for all inputs. Similar to algorithms, the optimal depth of a deep architecture may be different for different input instances, either to avoid ``over-thinking'', or because we want to compute less for operations converged already. In this paper, we tackle this varying depth problem using a steerable architecture, where a feed-forward deep model and a variational stopping policy are learned together to sequentially determine the optimal number of layers for each input instance. Training such architecture is very challenging. We provide a variational Bayes perspective and design a novel and effective training procedure which decomposes the task into an oracle model learning stage and an imitation stage. Experimentally, we show that the learned deep model along with the stopping policy improves the performances on a diverse set of tasks, including learning sparse recovery, few-shot meta learning, and computer vision tasks."", 'corpus_id': 219558713, 'score': 0}, {'doc_id': '202539738', 'title': 'Deep Equilibrium Models', 'abstract': 'We present a new approach to modeling sequential data: the deep equilibrium model (DEQ). Motivated by an observation that the hidden layers of many existing deep sequence models converge towards some fixed point, we propose the DEQ approach that directly finds these equilibrium points via root-finding. Such a method is equivalent to running an infinite depth (weight-tied) feedforward network, but has the notable advantage that we can analytically backpropagate through the equilibrium point using implicit differentiation. Using this approach, training and prediction in these networks require only constant memory, regardless of the effective “depth” of the network. We demonstrate how DEQs can be applied to two state-of-the-art deep sequence models: self-attention transformers and trellis networks. On large-scale language modeling tasks, such as the WikiText-103 benchmark, we show that DEQs 1) often improve performance over these state-of-the-art models (for similar parameter counts); 2) have similar computational requirements to existing models; and 3) vastly reduce memory consumption (often the bottleneck for training large sequence models), demonstrating an up-to 88% memory reduction in our experiments. The code is available at https://github.com/locuslab/deq.', 'corpus_id': 202539738, 'score': 1}, {'doc_id': '201070029', 'title': 'Implicit Deep Learning', 'abstract': 'Implicit deep learning prediction rules generalize the recursive rules of feedforward neural networks. Such rules are based on the solution of a fixed-point equation involving a single vector of hidden features, which is thus only implicitly defined. The implicit framework greatly simplifies the notation of deep learning, and opens up many new possibilities, in terms of novel architectures and algorithms, robustness analysis and design, interpretability, sparsity, and network architecture optimization.', 'corpus_id': 201070029, 'score': 1}, {'doc_id': '213896662', 'title': 'Gradients as Features for Deep Representation Learning', 'abstract': 'We address the challenging problem of deep representation learning--the efficient adaption of a pre-trained deep network to different tasks. Specifically, we propose to explore gradient-based features. These features are gradients of the model parameters with respect to a task-specific loss given an input sample. Our key innovation is the design of a linear model that incorporates both gradient features and the activation of the network. We show that our model provides a local linear approximation to a underlying deep model, and discuss important theoretical insight. Moreover, we present an efficient algorithm for the training and inference of our model without computing the actual gradients. Our method is evaluated across a number of representation learning tasks on several datasets and using different network architectures. We demonstrate strong results in all settings. And our results are well-aligned with our theoretical insight.', 'corpus_id': 213896662, 'score': 1}, {'doc_id': '218863030', 'title': 'PruneNet: Channel Pruning via Global Importance', 'abstract': 'Channel pruning is one of the predominant approaches for accelerating deep neural networks. Most existing pruning methods either train from scratch with a sparsity inducing term such as group lasso, or prune redundant channels in a pretrained network and then fine tune the network. Both strategies suffer from some limitations: the use of group lasso is computationally expensive, difficult to converge and often suffers from worse behavior due to the regularization bias. The methods that start with a pretrained network either prune channels uniformly across the layers or prune channels based on the basic statistics of the network parameters. These approaches either ignore the fact that some CNN layers are more redundant than others or fail to adequately identify the level of redundancy in different layers. In this work, we investigate a simple-yet-effective method for pruning channels based on a computationally light-weight yet effective data driven optimization step that discovers the necessary width per layer. Experiments conducted on ILSVRC-$12$ confirm effectiveness of our approach. With non-uniform pruning across the layers on ResNet-$50$, we are able to match the FLOP reduction of state-of-the-art channel pruning results while achieving a $0.98\\%$ higher accuracy. Further, we show that our pruned ResNet-$50$ network outperforms ResNet-$34$ and ResNet-$18$ networks, and that our pruned ResNet-$101$ outperforms ResNet-$50$.', 'corpus_id': 218863030, 'score': 0}, {'doc_id': '219963786', 'title': 'NeuralScale: Efficient Scaling of Neurons for Resource-Constrained Deep Neural Networks', 'abstract': 'Deciding the amount of neurons during the design of a deep neural network to maximize performance is not intuitive. In this work, we attempt to search for the neuron (filter) configuration of a fixed network architecture that maximizes accuracy. Using iterative pruning methods as a proxy, we parametrize the change of the neuron (filter) number of each layer with respect to the change in parameters, allowing us to efficiently scale an architecture across arbitrary sizes. We also introduce architecture descent which iteratively refines the parametrized function used for model scaling. The combination of both proposed methods is coined as NeuralScale. To prove the efficiency of NeuralScale in terms of parameters, we show empirical simulations on VGG11, MobileNetV2 and ResNet18 using CIFAR10, CIFAR100 and TinyImageNet as benchmark datasets. Our results show an increase in accuracy of 3.04%, 8.56% and 3.41% for VGG11, MobileNetV2 and ResNet18 on CIFAR10, CIFAR100 and TinyImageNet respectively under a parameter-constrained setting (output neurons (filters) of default configuration with scaling factor of 0.25).', 'corpus_id': 219963786, 'score': 0}, {'doc_id': '213729382', 'title': 'Dynamic Model Pruning with Feedback', 'abstract': 'Deep neural networks often have millions of parameters. This can hinder their deployment to low-end devices, not only due to high memory requirements but also because of increased latency at inference. We propose a novel model compression method that generates a sparse trained model without additional overhead: by allowing (i) dynamic allocation of the sparsity pattern and (ii) incorporating feedback signal to reactivate prematurely pruned weights we obtain a performant sparse model in one single training pass (retraining is not needed, but can further improve the performance). We evaluate the method on CIFAR-10 and ImageNet, and show that the obtained sparse models can reach the state-of-the-art performance of dense models and further that their performance surpasses all previously proposed pruning schemes (that come without feedback mechanisms).', 'corpus_id': 213729382, 'score': 0}]"
136	Intraop. hypotension and outcomes	2f7044c4307a172f4108b9b137de4878	15862	{}	"[{'doc_id': '232377129', 'title': 'COVID-19 and Acute Cervical Spinal Cord Injury—Case Report of 2 Patients', 'abstract': 'Study Design: This was a case series. Objective: The authors sought to examine the high-risk population of COVID-positive patients with acute cervical spinal cord injury (SCI) in a large level 1 trauma and tertiary referral center. Summary of Background Data: There are limited studies regarding the surgical management of patients with acute SCI in the setting of the recent coronavirus pandemic. Methods: The authors describe the cases of 2 patients who died from COVID-related complications after acute cervical SCI. Results: Patients with SCI are at increased risk of pulmonary complications. COVID-19 infection represents a double hit in this patient population, increasing potential morbidity and mortality in the perioperative time frame. Careful consideration must be made regarding the timing of potential surgical intervention in the treatment of acute SCI. Conclusions: Nationwide database of COVID-positive patients with acute spinal cord injury should be collected and analyzed to better understand how to manage acute SCI in the COVID-19 era. The authors recommend preoperative discussion in patients with acute cervical SCI with COVID-19, specifically emphasizing the increased risk of respiratory complications and mortality.', 'corpus_id': 232377129, 'score': 0}, {'doc_id': '231821706', 'title': '2020 Clinical Update in Liver Transplantation', 'abstract': '\n The gold standard treatment of end-stage liver disease continues to be liver transplantation (LT). The challenges of LT require skilled anesthesiologists to anticipate physiologic changes associated with end-stage liver disease (ESLD) as well as surgical considerations that affect multiple organ systems. While on the waiting list, patients may be placed new anticoagulation medications that can confound already complex coagulopathy in LT patients. Pain management is often an afterthought for such a complex procedure but appropriate medications can help control pain while limiting opioid medications. Surgical stress and medications for immunosuppression can affect perioperative glucose management in ways that have implications for patient and graft survival. 2020 provided a new challenge for anesthesiologist, the COVID-19 pandemic. The uncertainty of the novel respiratory virus challenged providers in beyond just LT patients.\n', 'corpus_id': 231821706, 'score': 0}, {'doc_id': '232613411', 'title': 'Impediment of cardiac surgery associated acute kidney injury by a resourceful perfusion protocol: A propensity-scored analysis', 'abstract': 'Acute kidney injury develops in up to 30% of patients who undergo cardiac surgery, with up to 3% of patients requiring dialysis. The requirement for dialysis after cardiac surgery is associated with an increased risk of infection, prolonged stay in critical care units and long-term need for dialysis. The development of acute kidney injury is independently associated with substantial short- and long-term morbidity and mortality. Its pathogenesis involves multiple pathways. Haemodynamic, inflammatory, metabolic and nephrotoxic factors are involved and overlap each other leading to kidney injury. Clinical studies have identified predictors for cardiac surgery-associated acute kidney injury that can be used effectively to determine the risk for acute kidney injury in patients undergoing cardiac surgery. High-risk patients can be targeted for renal protective strategies. Nonetheless, there is little compelling evidence from randomized trials supporting specific interventions to protect or prevent acute kidney injury in cardiac surgery patients. Several strategies have shown some promise, including less invasive procedures in those at greatest risk, natriuretic peptide, fenoldopam, preoperative hydration, preoperative optimization of anaemia and postoperative early use of renal replacement therapy. The efficacy of larger-scale trials remains to be confirmed.', 'corpus_id': 232613411, 'score': 1}, {'doc_id': '211234028', 'title': 'Intraoperative hypotension: Pathophysiology, clinical relevance, and therapeutic approaches', 'abstract': 'Intraoperative hypotension (IOH) i.e., low arterial blood pressure (AP) during surgery is common in patients having non-cardiac surgery under general anaesthesia. It has a multifactorial aetiology, and is associated with major postoperative complications including acute kidney injury, myocardial injury and death. Therefore, IOH may be a modifiable risk factor for postoperative complications. However, there is no uniform definition for IOH. IOH not only occurs during surgery but also after the induction of general anaesthesia before surgical incision. However, the optimal therapeutic approach to IOH remains elusive. There is evidence from one small randomised controlled trial that individualising AP targets may reduce the risk of postoperative organ dysfunction compared with standard care. More research is needed to define individual AP harm thresholds, to develop therapeutic strategies to treat and avoid IOH, and to integrate new technologies for continuous AP monitoring.', 'corpus_id': 211234028, 'score': 1}, {'doc_id': '232579083', 'title': 'The occurrence of intra-operative hypotension varies between hospitals: observational analysis of over 147000 anaesthesia', 'abstract': ""Background: Hypotension, a common intra-operative incident, bears an important potential for morbidity. It is most often manageable and sometimes preventable, which renders its study important. Therefore, we aimed at examining hospital variations in the occurrence of intraoperative hypotension and its predictors. As secondary endpoints, we determined to what extent hypotension relates to the risk of postoperative incidents and death.\nMethods: We used the Anaesthesia Databank Switzerland, built on routinely and prospectively collected data on all anaesthesias in 21 hospitals. The three outcomes were assessed using multi-level logistic regression models.\nResults: Among 147573 anaesthesia, hypotension ranged from 0.6 to 5.2% in participating hospitals, and from 0.3 up to 12% in different surgical specialties. Most (73.4%) were minor single events. Age, ASA status, combined general and regional anaesthesia techniques, duration of surgery, and hospitalization were significantly associated to hypotension. Although significantly associated, the emergency status of the surgery had a weaker effect. Hospitals' Odds Ratios for hypotension varied between 0.12 to 2.50 (p ≤0.001) with respect to the mean prevalence of 3.1%, even after adjusting for patient and anaesthesia factors, and for type of surgery. At least one postoperative incident occurred in 9.7% of the interventions, including 0.03% deaths. Intra-operative hypotension was associated with higher risk of post-operative incidents and death.\nConclusions: Wide variations in the occurrence of hypotension amongst hospitals remain after adjustment for risk factors. Although differential reporting from hospitals may exist, variations in anesthesia techniques and blood pressure maintenance could have also contributed. Intra-operative hypotension is associated with morbidities and sometimes death, and constant vigilance must thus be advocated."", 'corpus_id': 232579083, 'score': 1}, {'doc_id': '231766736', 'title': 'Management of diabetes and hyperglycaemia in the hospital.', 'abstract': 'Hyperglycaemia in people with and without diabetes admitted to the hospital is associated with a substantial increase in morbidity, mortality, and health-care costs. Professional societies have recommended insulin therapy as the cornerstone of inpatient pharmacological management. Intravenous insulin therapy is the treatment of choice in the critical care setting. In non-intensive care settings, several insulin protocols have been proposed to manage patients with hyperglycaemia; however, meta-analyses comparing different treatment regimens have not clearly endorsed the benefits of any particular strategy. Clinical guidelines recommend stopping oral antidiabetes drugs during hospitalisation; however, in some countries continuation of oral antidiabetes drugs is commonplace in some patients with type 2 diabetes admitted to hospital, and findings from clinical trials have suggested that non-insulin drugs, alone or in combination with basal insulin, can be used to achieve appropriate glycaemic control in selected populations. Advances in diabetes technology are revolutionising day-to-day diabetes care and work is ongoing to implement these technologies (ie, continuous glucose monitoring, automated insulin delivery) for inpatient care. Additionally, transformations in care have occurred during the COVID-19 pandemic, including the use of remote inpatient diabetes management-research is needed to assess the effects of such adaptations.', 'corpus_id': 231766736, 'score': 0}, {'doc_id': '206095303', 'title': 'Permissive hypotension versus conventional resuscitation strategies in adult trauma patients with hemorrhagic shock: A systematic review and meta-analysis of randomized controlled trials', 'abstract': 'BACKGROUND Aggressive fluid resuscitation in trauma promotes deleterious effects such as clot disruption, dilutional coagulopathy and hypothermia. Animal studies suggest that permissive hypotension maintains appropriate organ perfusion, reduces bleeding and improves mortality. This review assesses the efficacy and safety of permissive hypotension in adult trauma patients with hemorrhagic shock. METHODS We searched the MEDLINE and EMBASE databases from inception to May 2017 for randomized controlled trials comparing permissive hypotension vs. conventional resuscitation following traumatic injury. We included preoperative and intraoperative resuscitation strategies. The primary outcome was 30-day or in-hospital mortality. Secondary outcomes included blood product utilization, estimated blood loss and in-hospital complications. Pooling was performed with a random-effects model. RESULTS We screened 722 abstracts, from which five randomized trials evaluating 1,158 patients were included. Blood pressure targets in the intervention arms varied from systolic BP 50 mm Hg to 70 mm Hg or mean arterial pressure of 50 mm Hg or higher as compared to systolic BP 65 mm Hg to 100 mm Hg or mean arterial pressure of 65 or higher in the control arms. Two studies evaluated only patients with penetrating injury while the remaining three additionally included blunt injuries. Four trials suggested a survival benefit for 30-day or in-hospital mortality with hypotensive resuscitation, although three studies were insufficiently powered to find statistical significance. Studies were of poor to moderate quality due to poor protocol reporting and lack of blinding. The pooled odds ratio was 0.70 (95% confidence interval, 0.53–0.92), suggesting a survival benefit for permissive hypotension. Those patients received fewer blood products and had lesser estimated blood loss. CONCLUSION Permissive hypotension may offer a survival benefit over conventional resuscitation for patients with hemorrhagic injury. It may additionally reduce blood loss and blood product utilization. However, the majority of studies were underpowered, thus reflecting a need for high quality, adequately powered trials. PROSPERO REGISTRATION Systematic Review, level II. CRD42017070526.', 'corpus_id': 206095303, 'score': 1}, {'doc_id': '232762258', 'title': 'Controlled hypotension during neuraxial anesthesia is not associated with increased odds of in-hospital common severe medical complications in patients undergoing elective primary total hip arthroplasty – A retrospective case control study', 'abstract': 'Introduction The use of controlled hypotension during neuraxial anesthesia for joint arthroplasty is controversial. We conducted a large institutional database analysis to assess common in-hospital complications and mortality of patients undergoing primary total hip arthroplasty (THA) under controlled hypotension and neuraxial anesthesia. Methods We conducted a large retrospective case control study of 11,292 patients who underwent primary THA using neuraxial anesthesia between March 2016 and May 2019 in a single institution devoted to musculoskeletal care. The degree and duration of various mean arterial pressure (MAP) thresholds were analyzed for adjusted odds ratios with composite common severe complications (in-hospital myocardial infarction, stroke, and/or acute kidney injury) as the primary outcome. Results Sixty-eight patients developed common severe complications (0.60%). Patients with complications were older (median age 75.6 vs 64.0 years) and had a higher American Society of Anesthesiologists (ASA) classification (45.6% vs 17.6% ASA III). The duration of hypotension at various MAP thresholds (45 to 70 mm Hg) was not associated with increasing odds of common severe medical complications. Conclusions Controlled hypotension (ranging from 45 to 70 mmHg) for a moderate duration during neuraxial anesthesia was not associated with increased odds of common severe complications (myocardial infarction, stroke, and/or acute kidney injury) among patients receiving neuraxial anesthesia for elective THA.', 'corpus_id': 232762258, 'score': 1}, {'doc_id': '232023788', 'title': 'Fatal venous air embolism during lumbar spondylolisthesis surgery', 'abstract': 'How to cite this article: Kar AK. Gas pipeline error: Time to verify all the terminal central outlets? Indian J Anaesth 2021;65:169‐71. © 2021 Indian Journal of Anaesthesia | Published by Wolters Kluwer ‐ Medknow This is an open access journal, and articles are distributed under the terms of the Creative Commons Attribution‐NonCommercial‐ShareAlike 4.0 License, which allows others to remix, tweak, and build upon the work non‐commercially, as long as appropriate credit is given and the new creations are licensed under the identical terms. Access this article online', 'corpus_id': 232023788, 'score': 0}, {'doc_id': '231827393', 'title': 'Oxygen administration for patients with ARDS', 'abstract': 'Acute respiratory distress syndrome (ARDS) is a fatal condition with insufficiently clarified etiology. Supportive care for severe hypoxemia remains the mainstay of essential interventions for ARDS. In recent years, adequate ventilation to prevent ventilator-induced lung injury (VILI) and patient self-inflicted lung injury (P-SILI) as well as lung-protective mechanical ventilation has an increasing attention in ARDS. Ventilation-perfusion mismatch may augment severe hypoxemia and inspiratory drive and consequently induce P-SILI. Respiratory drive and effort must also be carefully monitored to prevent P-SILI. Airway occlusion pressure ( P 0.1 ) and airway pressure deflection during an end-expiratory airway occlusion ( P occ ) could be easy indicators to evaluate the respiratory drive and effort. Patient-ventilator dyssynchrony is a time mismatching between patient’s effort and ventilator drive. Although it is frequently unrecognized, dyssynchrony can be associated with poor clinical outcomes. Dyssynchrony includes trigger asynchrony, cycling asynchrony, and flow delivery mismatch. Ventilator-induced diaphragm dysfunction (VIDD) is a form of iatrogenic injury from inadequate use of mechanical ventilation. Excessive spontaneous breathing can lead to P-SILI, while excessive rest can lead to VIDD. Optimal balance between these two manifestations is probably associated with the etiology and severity of the underlying pulmonary disease. High-flow nasal cannula (HFNC) and non-invasive positive pressure ventilation (NPPV) are non-invasive techniques for supporting hypoxemia. While they are beneficial as respiratory supports in mild ARDS, there can be a risk of delaying needed intubation. Mechanical ventilation and ECMO are applied for more severe ARDS. However, as with HFNC/NPPV, inappropriate assessment of breathing workload potentially has a risk of delaying the timing of shifting from ventilator to ECMO. Various methods of oxygen administration in ARDS are important. However, it is also important to evaluate whether they adequately reduce the breathing workload and help to improve ARDS.', 'corpus_id': 231827393, 'score': 0}]"
137	DOC related	382c5ed88ef5955e86bc00fd1ead9c30	6636	{'DOC': 'dissolved organic carbon'}	"[{'doc_id': '220681495', 'title': 'Distant mood monitoring for depressive and bipolar disorders: a systematic review', 'abstract': 'Background Broadening our knowledge of the longitudinal course of mood symptoms is cardinal to providing effective long-term treatments. Research indicates that patients with mental illness are willing to engage in the use of telemonitoring and mobile technology to assess and monitor their mood states. However, without the provision of distant support, adverse outcomes and events may be difficult to prevent and manage through self-monitoring. Understanding patient perspectives is important to achieving the best balance of self-monitoring, patient empowerment, and distant supporter involvement. Methods This systematic review synthesises quantitative and qualitative evidence of the effectiveness and feasibility of daily/weekly/monthly remote mood monitoring that includes distant support in participants with mood disorders. Inclusion criteria comprised mood monitoring of mood disorder patients as main intervention, study design, method of monitoring, and presence of psychotherapy and psychoeducation. Effectiveness was defined by the change in depression and/or mania scores. Feasibility was determined on participant feedback and completion/attrition rates. Studies were assessed for quality using the Mixed Methods Appraisal Tool version 2018. Results Nine studies of acceptable quality met the inclusion criteria. Distant mood monitoring was effective in improving depression scores but not mania scores. Feasibility, as measured through compliance and completion rates and participant feedback, varied. Conclusion Distant mood monitoring with support may be a useful, acceptable, and feasible intervention for diverse groups of patients in terms of age and ethnicity. Further, it may be effective in improving symptoms of depression, increasing treatment adherence, and facilitating the prevention and management of adverse outcomes. As a task-shifting intervention, distant mood monitoring may help to alleviate the burden on mental health providers in developing countries.', 'corpus_id': 220681495, 'score': 0}, {'doc_id': '219636142', 'title': 'Probing fMRI brain connectivity and activity changes during emotion regulation by EEG neurofeedback', 'abstract': 'Neurofeedback is a non-invasive brain training with long-term medical and non-medical applications. Despite the existence of several emotion regulation studies using neurofeedback, further investigation is needed to understand interactions of the brain regions involved in the process. We implemented EEG neurofeedback with simultaneous fMRI using a modified happiness-inducing task through autobiographical memories to upregulate positive emotion. The results showed increased activity of prefrontal, occipital, parietal, and limbic regions and increased functional connectivity between prefrontal, parietal, limbic system, and insula in the experimental group. New connectivity links were identified by comparing the functional connectivity of different experimental conditions within the experimental group and between the experimental and control groups. The proposed multimodal approach quantified the changes in the brain activity (up to 1.9% increase) and connectivity (FDR-corrected for multiple comparison, q = 0.05) during emotion regulation in/between prefrontal, parietal, limbic, and insula regions. Psychometric assessments confirmed significant changes in positive and negative mood states by neurofeedback with a p-value smaller than 0.002 in the experimental group. This study quantifies the effects of EEG neurofeedback in changing functional connectivity of all brain regions involved in emotion regulation. For the brain regions involved in emotion regulation, we found significant BOLD and functional connectivity increases due to neurofeedback in the experimental group but no learning effect was observed in the control group. The results reveal the neurobiological substrate of emotion regulation by the EEG neurofeedback and separate the effect of the neurofeedback and the recall of the autobiographical memories.', 'corpus_id': 219636142, 'score': 0}, {'doc_id': '54524352', 'title': 'Detecting Awareness in the Vegetative State', 'abstract': 'We used functional magnetic resonance imaging to demonstrate preserved conscious awareness in a patient fulfilling the criteria for a diagnosis of vegetative state. When asked to imagine playing tennis or moving around her home, the patient activated predicted cortical areas in a manner indistinguishable from that of healthy volunteers.', 'corpus_id': 54524352, 'score': 1}, {'doc_id': '219966840', 'title': 'Locked in Syndrome Machine Learning Classification using Sentence Comprehension EEG Data', 'abstract': 'Locked-in Syndrome patients are often misdiagnosed and face pessimistic prognosis because of similarities with disorders of consciousness, a lack of objective biomarkers and a difficult-to-recognize pathogenesis. Biomarkers show promise in identifying similar conditions, utilizing electroencephalography (EEG) data. This data, particularly in the form of event-related potentials (ERPs), while successful in varying applications, suffers from methodological constraints and interpretation obstacles. The study documented in this body of work explores a machine learning paradigm with regards to N400 ERP data retrieved from a sentence comprehension task to tackle these hindrances and proposes a new auxiliary diagnostic tool for LIS and possibly disorders of consciousness. A support vector machine (SVC) and a random forest classifier (RF) were able to classify conscious individuals from unconscious ones with optimistic performance metrics. Based on these results, the proposed models and continuations thereof present valuable opportunities for the development of an auxiliary diagnostic tool for the classification of LIS patients, aiding diagnosis, improving prognosis, stimulating recovery and reducing mortality rates.', 'corpus_id': 219966840, 'score': 1}, {'doc_id': '218840586', 'title': 'A Holistic Approach to Neuro-informed Music Therapy for Acute TBI Rehabilitation', 'abstract': 'This capstone project is the development of a holistic neuro-informed method of music therapy for acute rehabilitation from traumatic brain injury. This method is grounded in research in the fields of neuroscience and neurochemistry and has been developed and implemented through the lens of a holistic patient centered theory of music therapy. The review of literature focuses on previous research in neuroscience, neurochemistry, neurologic music therapy, medical music therapy and the overall relationship between music and the brain. The method was explored using one 17-year-old female participant at a Boston area children’s hospital recovering from traumatic brain injury. The patient’s entire catalog of presenting problems upon admission as well as any physical and psychological history were taken into consideration in developing a holistic neuro-informed music therapy treatment plan. The plan was implemented over a sixweek period of time, and all documentation was recorded via MediTech, the hospital’s medical recordkeeping database. The results were recorded based on patient self-report and showed improvements in gait, balance, respiratory strength, and oral motor control as well as increased emotional expression and regulation. Further research and explorations of neuro-informed music therapy in various populations will help to progress the method and clarify its efficacy. HOLISTIC NEURO-INFORMED MUSIC THERAPY 3 A Holistic Approach to Neuro-informed Music Therapy for Acute TBI Rehabilitation', 'corpus_id': 218840586, 'score': 1}, {'doc_id': '219621336', 'title': 'Report of EEG Finding on Critically Ill Patients with COVID‐19', 'abstract': 'In March 2020, we treated a cohort of 26 critically ill hospitalized SARS‐CoV‐2 infected patients who received EEGs to assess unexplained altered mental status, loss of consciousness, or poor arousal and responsiveness. Of the 26 patients studied, 5 patients had EEGs that showed Periodic Discharges (PD) consisting of high amplitude frontal monomorphic delta waves with absence of epileptic activity. These findings may suggest CNS injury potentially related to COVID‐19 in these patients. This article is protected by copyright. All rights reserved.', 'corpus_id': 219621336, 'score': 0}, {'doc_id': '59603238', 'title': 'Opportunities and challenges for a maturing science of consciousness', 'abstract': 'Scientific research on consciousness is critical to multiple scientific, clinical, and ethical issues. The growth of the field could also be beneficial to several areas including neurology and mental health research. To achieve this goal, we need to set funding priorities carefully and address problems such as job creation and potential media misrepresentation.', 'corpus_id': 59603238, 'score': 1}, {'doc_id': '220280876', 'title': 'Functional MRI applications for psychiatric disease subtyping: a review', 'abstract': 'Psychiatric disorders have historically been classified using symptom information alone. With the advent of new technologies that allowed researchers to investigate brain mechanisms more directly, interest in the mechanistic rationale behind defined pathologies and aetiology redefinition has greatly increased. This is particularly appealing for the field of personalised medicine, which searches for data-driven approaches to improve individual diagnosis, prognosis and treatment selection. Here we intend to systematically analyse the usage of functional MRI on both the elucidation of psychiatric disease biotypes and the interpretation of subtypes obtained via unsupervised learning applied to symptom or biomarker data. We searched the existing literature for functional MRI applications to the obtention or interpretation of psychiatric disease subtypes. The PRISMA guidelines were applied to filter the retrieved studies, and the active learning framework ASReviews was applied for article prioritization. From the 20 studies that met the inclusion criteria, 5 used functional MRI data to interpret symptom-derived disease clusters, 4 used it for the interpretation of clusters derived from biomarker data other than fMRI itself, and 11 applied clustering to fMRI directly. Major depression disorder and schizophrenia were the two most studied pathologies, followed by ADHD, psychosis, autism disorder, and early violence. No trans-diagnostic studies were retrieved. While interest in personalised medicine and data-driven disease subtyping is on the rise and psychiatry is not the exception, unsupervised analyses of functional MRI data are inconsistent to date, and much remains to be done in terms of gathering and centralising data, standardising pipelines and model validation, and method refinement. The usage of fMRI in the field of trans-diagnostic psychiatry remains vastly unexplored.', 'corpus_id': 220280876, 'score': 0}, {'doc_id': '4122974', 'title': 'The Clinical Diagnostic Utility of Electrophysiological Techniques in Assessment of Patients With Disorders of Consciousness Following Acquired Brain Injury: A Systematic Review', 'abstract': 'Objective: To investigate the diagnostic utility of electrophysiological recordings during active cognitive tasks in detecting residual cognitive capacities in patients with disorders of consciousness (DoC) after severe acquired brain injury. Design: Systematic review of empirical research in MEDLINE, Embase, PsycINFO, and Cochrane from January 2002 to March 2016. Main Measures: Data extracted included sample size, type of electrophysiological technique and task design, rate of cognitive responders, false negatives and positives, and excluded subjects from the study analysis. The Quality Assessment of Diagnostic Accuracy Studies–2 (QUADAS-2) was used for quality appraisal of the retrieved literature. Results: Twenty-four studies examining electrophysiological signs of command-following in patients with DoC were identified. Sensitivity rates in healthy controls demonstrated variable accuracy across the studies, ranging from 71% to 100%. In patients with DoC, specificity and sensitivity rates varied in the included studies, ranging from 0% to 100%. Pronounced heterogeneity was found between studies regarding methodological approaches, task design, and procedures of analysis, rendering comparison between studies challenging. Conclusion: We are still far from establishing precise recommendations for standardized electrophysiological diagnostic procedures in DoC, but electrophysiological methods may add supplemental diagnostic information of covert cognition in some patients with DoC.', 'corpus_id': 4122974, 'score': 1}, {'doc_id': '219531878', 'title': 'Towards precise resting-state fMRI biomarkers in psychiatry: synthesizing developments in transdiagnostic research, dimensional models of psychopathology, and normative neurodevelopment', 'abstract': ""Searching for biomarkers has been a chief pursuit of the field of psychiatry. Toward this end, studies have catalogued candidate resting-state biomarkers in nearly all forms of mental disorder. However, it is becoming increasingly clear that these biomarkers lack specificity, limiting their capacity to yield clinical impact. We discuss three avenues of research that are overcoming this limitation: (i) the adoption of transdiagnostic research designs, which involve studying and explicitly comparing multiple disorders from distinct diagnostic axes of psychiatry; (ii) dimensional models of psychopathology that map the full spectrum of symptomatology and that cut across traditional disorder boundaries; and (iii) modeling individuals' unique functional connectomes throughout development. We provide a framework for tying these subfields together that draws on tools from machine learning and network science."", 'corpus_id': 219531878, 'score': 0}]"
138	Baby Kangaroos	d297a66c31c8bd5cc0c4c719f8cbf00c	20595	{}	"[{'doc_id': '237396959', 'title': 'Birthweight measurement processes and perceived value: a qualitative study in Temeke Hospital, Tanzania', 'abstract': ""Background: Globally an estimated 20.5 million liveborn babies are low birthweight (LBW) each year, weighing less than 2500 g. LBW babies have increased risk of mortality even beyond the neonatal period, with an ongoing risk of stunting and non-communicable diseases. LBW is a priority global health indicator. Now almost 80% of births are in facilities, yet birthweight data are lacking in most high-mortality burden countries and are of poor quality, notably with heaping especially on values ending in 00. We aimed to undertake qualitative research in a regional hospital in Dar es Salaam, Tanzania, observing birthweight practices, exploring barriers and enablers to weighing at birth as well as perceived value of birthweight data to health workers, women and stakeholders. Methods: Observations were undertaken on type of birthweight scale availability in hospital wards. In-depth semistructured interviews (n = 21) were conducted with three groups: women in postnatal and kangaroo mother care wards, health workers involved in birthweight measurement/recording, and with stakeholders involved in data aggregation in Temeke Hospital, Tanzania, a site in the EN-BIRTH study. An inductive thematic analysis was undertaken of translated interview transcripts. Results: Of five wards that were expected to have scales, three had functional scales, and only one of the functional scales was digital. The Labour ward weighed the most newborns using an analogue scale which was not consistently zeroed. Hospital birthweight data were aggregated monthly for reporting into the health management information system. Birthweight measurement was highly valued by all respondents, notably families and healthcare workers, and local use of data was considered an enabler. Perceived barriers to high quality birthweight data included: gaps in availability of precise weighing equipment, adequate health workers and imprecise measurement practices. (Continued on next page) © The Author(s). 2020 Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/. The Creative Commons Public Domain Dedication waiver (http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated in a credit line to the data. * Correspondence: joy.lawn@lshtm.ac.uk Joy E Lawn is senior author Miriam E Gladstonea and Nahya Salim are joint first authors Centre for Maternal, Adolescent, Reproductive, & Child Health (MARCH), London School of Hygiene and Tropical Medicine (LSHTM), Keppel Street, London WC1E 7HT, UK Full list of author information is available at the end of the article"", 'corpus_id': 237396959, 'score': 0}, {'doc_id': '237090324', 'title': 'Impact of early kangaroo mother care versus standard care on survival of mild-moderately unstable neonates <2000 grams: A randomised controlled trial', 'abstract': 'Background Understanding the effect of early kangaroo mother care on survival of mild-moderately unstable neonates <2000 g is a high-priority evidence gap for small and sick newborn care. Methods This non-blinded pragmatic randomised clinical trial was conducted at the only teaching hospital in The Gambia. Eligibility criteria included weight <2000g and age 1–24 h with exclusion if stable or severely unstable. Neonates were randomly assigned to receive either standard care, including KMC once stable at >24 h after admission (control) versus KMC initiated <24 h after admission (intervention). Randomisation was stratified by weight with twins in the same arm. The primary outcome was all-cause mortality at 28 postnatal days, assessed by intention to treat analysis. Secondary outcomes included: time to death; hypothermia and stability at 24 h; breastfeeding at discharge; infections; weight gain at 28d and admission duration. The trial was prospectively registered at www.clinicaltrials.gov (NCT03555981). Findings Recruitment occurred from 23rd May 2018 to 19th March 2020. Among 1,107 neonates screened for participation 279 were randomly assigned, 139 (42% male [n = 59]) to standard care and 138 (43% male [n = 59]) to the intervention with two participants lost to follow up and no withdrawals. The proportion dying within 28d was 24% (34/139, control) vs. 21% (29/138, intervention) (risk ratio 0·84, 95% CI 0·55 – 1·29, p = 0·423). There were no between-arm differences for secondary outcomes or serious adverse events (28/139 (20%) for control and 30/139 (22%) for intervention, none related). One-third of intervention neonates reverted to standard care for clinical reasons. Interpretation The trial had low power due to halving of baseline neonatal mortality, highlighting the importance of implementing existing small and sick newborn care interventions. Further mortality effect and safety data are needed from varying low and middle-income neonatal unit contexts before changing global guidelines.', 'corpus_id': 237090324, 'score': 1}, {'doc_id': '237505283', 'title': 'Scaling up Kangaroo Mother Care in Ethiopia and India: a multi-site implementation research study', 'abstract': 'Objectives Kangaroo Mother Care (KMC), prolonged skin-to-skin care of the low birth weight baby with the mother plus exclusive breastfeeding reduces neonatal mortality. Global KMC coverage is low. This study was conducted to develop and evaluate context-adapted implementation models to achieve improved coverage. Design This study used mixed-methods applying implementation science to develop an adaptable strategy to improve implementation. Formative research informed the initial model which was refined in three iterative cycles. The models included three components: (1) maximising access to KMC-implementing facilities, (2) ensuring KMC initiation and maintenance in facilities and (3) supporting continuation at home postdischarge. Participants 3804 infants of birth weight under 2000\u2009g who survived the first 3\u2009days, were available in the study area and whose mother resided in the study area. Main outcome measures The primary outcomes were coverage of KMC during the 24 hours prior to discharge and at 7\u2009days postdischarge. Results Key barriers and solutions were identified for scaling up KMC. The resulting implementation model achieved high population-based coverage. KMC initiation reached 68%–86% of infants in Ethiopian sites and 87% in Indian sites. At discharge, KMC was provided to 68% of infants in Ethiopia and 55% in India. At 7\u2009days postdischarge, KMC was provided to 53%–65% of infants in all sites, except Oromia (38%) and Karnataka (36%). Conclusions This study shows how high coverage of KMC can be achieved using context-adapted models based on implementation science. They were supported by government leadership, health workers’ conviction that KMC is the standard of care, women’s and families’ acceptance of KMC, and changes in infrastructure, policy, skills and practice. Trial registration numbers ISRCTN12286667; CTRI/2017/07/008988; NCT03098069; NCT03419416; NCT03506698.', 'corpus_id': 237505283, 'score': 1}, {'doc_id': '236991276', 'title': 'Placental growth factor in assessment of women with suspected pre-eclampsia to reduce maternal morbidity: a stepped wedge cluster randomised control trial (PARROT Ireland)', 'abstract': ""Abstract Objective To determine whether the addition of placental growth factor (PlGF) measurement to current clinical assessment of women with suspected pre-eclampsia before 37 weeks' gestation would reduce maternal morbidity without increasing neonatal morbidity. Design Stepped wedge cluster randomised control trial from 29 June 2017 to 26 April 2019. Setting National multisite trial in seven maternity hospitals throughout the island of Ireland Participants Women with a singleton pregnancy between 20+0 to 36+6 weeks’ gestation, with signs or symptoms suggestive of evolving pre-eclampsia. Of the 5718 women screened, 2583 were eligible and 2313 elected to participate. Intervention Participants were assigned randomly to either usual care or to usual care plus the addition of point-of-care PlGF testing based on the randomisation status of their maternity hospital at the time point of enrolment. Main outcomes measures Co-primary outcomes of composite maternal morbidity and composite neonatal morbidity. Analysis was on an individual participant level using mixed-effects Poisson regression adjusted for time effects (with robust standard errors) by intention-to-treat. Results Of the 4000 anticipated recruitment target, 2313 eligible participants (57%) were enrolled, of whom 2219 (96%) were included in the primary analysis. Of these, 1202 (54%) participants were assigned to the usual care group, and 1017 (46%) were assigned the intervention of additional point-of-care PlGF testing. The results demonstrate that the integration of point-of-care PlGF testing resulted in no evidence of a difference in maternal morbidity—457/1202 (38%) of women in the control group versus 330/1017 (32%) of women in the intervention group (adjusted risk ratio (RR) 1.01 (95% CI 0.76 to 1.36), P=0.92)—or in neonatal morbidity—527/1202 (43%) of neonates in the control group versus 484/1017 (47%) in the intervention group (adjusted RR 1.03 (0.89 to 1.21), P=0.67). Conclusions This was a pragmatic evaluation of an interventional diagnostic test, conducted nationally across multiple sites. These results do not support the incorporation of PlGF testing into routine clinical investigations for women presenting with suspected preterm pre-eclampsia, but nor do they exclude its potential benefit. Trial registration ClinicalTrials.gov NCT02881073."", 'corpus_id': 236991276, 'score': 0}, {'doc_id': '203667650', 'title': 'Effect of community-initiated kangaroo mother care on survival of infants with low birthweight: a randomised controlled trial', 'abstract': 'BACKGROUND\nCoverage of kangaroo mother care remains very low despite WHO recommendations for its use for babies with low birthweight in health facilities for over a decade. Initiating kangaroo mother care at the community level is a promising strategy to increase coverage. However, knowledge of the efficacy of community-initiated kangaroo mother care is still lacking. We aimed to assess the effect of community-initiated kangaroo mother care provided to babies weighing 1500-2250 g on neonatal and infant survival.\n\n\nMETHODS\nIn this randomised controlled, superiority trial, undertaken in Haryana, India, we enrolled babies weighing 1500-2250 g at home within 72 h of birth, if not already initiated in kangaroo mother care, irrespective of place of birth (ie, home or health facility) and who were stable and feeding. The first eligible infants in households were randomly assigned (1:1) to the intervention (community-initiated kangaroo mother care) or control group by block randomisation using permuted blocks of variable size. Twins were allocated to the same group. For second eligible infants in the same household as an enrolled infant, if the first infant was assigned to the intervention group the second infant was also assigned to this group, whereas if the first infant was assigned to the control group the second infant was randomly assigned (1:1) to the intervention or control group. Mothers and infants in the intervention group were visited at home (days 1-3, 5, 7, 10, 14, 21, and 28) to support kangaroo mother care (ie, skin-to-skin contact and exclusive breastfeeding). The control group received routine care. The two primary outcomes were mortality between enrolment and 28 days and between enrolment and 180 days. Analysis was by intention to treat and adjusted for clustering within households. The effect of the intervention on mortality was assessed with person-time in the denominator using Cox proportional hazards model. This study is registered with ClinicalTrials.gov, NCT02653534 and NCT02631343, and is now closed to new participants.\n\n\nFINDINGS\nBetween July 30, 2015, and Oct 31, 2018, 8402 babies were enrolled, of whom 4480 were assigned to the intervention group and 3922 to the control group. Most births (6837 [81·4%]) occurred at a health facility, 36·2% (n=3045) had initiated breastfeeding within 1 h of birth, and infants were enrolled at an average of about 30 h (SD 17) of age. Vital status was known for 4470 infants in the intervention group and 3914 in the control group at age 28 days, and for 3653 in the intervention group and 3331 in the control group at age 180 days. Between enrolment and 28 days, 73 infants died in 4423 periods of 28 days in the intervention group and 90 deaths in 3859 periods of 28 days in the control group (hazard ratio [HR] 0·70, 95% CI 0·51-0·96; p=0·027). Between enrolment and 180 days, 158 infants died in 3965 periods of 180 days in the intervention group and 184 infants died in 3514 periods of 180 days in the control group (HR 0·75, 0·60-0·93; p=0·010). The risk ratios for death were almost the same as the HRs (28-day mortality 0·71, 95% CI 0·52- 0·97; p=0·032; 180-day mortality 0·76, 0·60-0·95; p=0·017).\n\n\nINTERPRETATION\nCommunity-initiated kangaroo mother care substantially improves newborn baby and infant survival. In low-income and middle-income countries, incorporation of kangaroo mother care for all infants with low birthweight, irrespective of place of birth, could substantially reduce neonatal and infant mortality.\n\n\nFUNDING\nResearch Council of Norway and University of Bergen.', 'corpus_id': 203667650, 'score': 1}, {'doc_id': '237510451', 'title': 'Effect of Daily Intermittent Kangaroo Mother Care on Vital Physiological Parameters of Low Birth Weight Newborns', 'abstract': 'Objective: To determine the effect of daily intermittent Kangaroo Mother Care (KMC) on vital physiological parameters of low birth weight newborns. Methods: This Quasi-experimental study was conducted in the Department of Neonatal Paediatrics, King Edward Medical University / Mayo Hospital Lahore from July 2017 to March 2018. Total of 84 low birth weight (< 2500gms) neonates were recruited by non-probability convenient sampling. Kangaroo mother care was provided for 3 consecutive hours a day, for 3 consecutive days. Vital physiological parameters including temperature, heart rate, respiratory rate and oxygen saturation of every baby were recorded immediately before and after KMC. Data was analyzed through SPSS 20.0. Median values of all four vital parameters (pre and post KMC) were compared by Wilcoxon signed-rank test after applying KolmogrovSmirnov test for normality distribution. Results: In 84 newborns, there was no change in median body temperature after KMC on day 1, while it was + 0.25 0C on day 2 and + 0.4 0C on day 3(p-values < 0.001). There was no change in median respiratory rate after KMC on day 1 (p-value = 0.412), while it reduced by 2 breaths/min on day 2 (p-value = 0.01) and 2 breaths/min on day 3 (p-value < 0.001). There was no change in median heart rate after KMC on day 1 (p-value = 0.765), but it decreased favorably by 5 beats/min (p-value = 0.008) and 4 beats/min (p-values< 0.001) on day 2 and 3 respectively. The median oxygen saturation after KMC increased by 1% on all 3 days ( p-values < 0.001). Conclusion: Except respiratory rate and heart rate on day 1, all vital physiological parameters in low birth weight babies showed statistical improvement after KMC. Corresponding Author | Dr. Sadia Shabir, Department of Paediatrics, KEMU/ Mayo Hospital, Lahore, Email: drsadiahussain@yahoo.com', 'corpus_id': 237510451, 'score': 1}, {'doc_id': '237426135', 'title': 'Saving babies’ lives (SBL) – a programme to reduce neonatal mortality in rural Cambodia: study protocol for a stepped-wedge cluster-randomised trial', 'abstract': 'Background Neonatal mortality remains unacceptably high. Many studies successful at reducing neonatal mortality have failed to realise similar gains at scale. Effective implementation and scale-up of interventions designed to tackle neonatal mortality is a global health priority. Multifaceted programmes targeting the continuum of neonatal care, with sustainability and scalability built into the design, can provide practical insights to solve this challenge. Cambodia has amongst the highest neonatal mortality rates in South-East Asia, with rural areas particularly affected. The primary objective of this study is the design, implementation, and assessment of the Saving Babies’ Lives programme, a package of interventions designed to reduce neonatal mortality in rural Cambodia. Methods This study is a five-year stepped-wedge cluster-randomised trial conducted in a rural Cambodian province with an estimated annual delivery rate of 6615. The study is designed to implement and evaluate the Saving Babies’ Lives programme, which is the intervention. The Saving Babies’ Lives programme is an iterative package of neonatal interventions spanning the continuum of care and integrating into the existing health system. The Saving Babies’ Lives programme comprises two major components: participatory learning and action with community health workers, and capacity building of primary care facilities involving facility-based mentorship. Standard government service continues in control arms. Data collection covering the whole study area includes surveillance of all pregnancies, verbal and social autopsies, and quality of care surveys. Mixed methods data collection supports iteration of the complex intervention, and facilitates impact, outcome, process and economic evaluation. Discussion Our study uses a robust study design to evaluate and develop a holistic, innovative, contextually relevant and sustainable programme that can be scaled-up to reduce neonatal mortality. Trial registration ClinicalTrials.gov: NCT04663620 . Registered on 11th December 2020, retrospectively registered.', 'corpus_id': 237426135, 'score': 0}, {'doc_id': '237329145', 'title': 'Enablers and barriers for enteral feeding with mother`s own milk in preterm very low birth weight infants in a tertiary care neonatal intensive care unit.', 'abstract': 'BACKGROUND\nThe management of lactation in preterm mothers is a real challenge for Neonatal Intensive Care Unit (NICU) care, providers. The study aimed to evaluate the enablers and barriers for enteral feeding with mothers` own milk (MOM) in preterm very low birth weight (VLBW) infants in a tertiary care neonatal unit.\n\n\nMETHODS\nThis prospective observational study took place at a tertiary level NICU of a high-risk obstetric unit in a private hospital. All VLBW infants and mothers were incorporated into the study. Data on enablers and barriers were gathered from mother-baby dyads at the time of birth, at the end of the 7th day, and then weekly till the discharge of the baby from the unit.\n\n\nRESULTS\nWe studied 87 mother-baby dyads. Mean (SD) maternal age, gestation age and birth weight were 29.3 (4.7) years, 30.8 (2.0) weeks, and 1196 (196) grams respectively. We categorized our data into 2 groups based on outcome estimates done during the entire hospital stay or pre-discharge (48 hours before the discharge). On comparison of perinatal and post-natal factors, the enablers were maternal dwelling from the rural locality, number of milk expression son day 1 after the birth, number of night expressions in the first week postnatally, and MOM volume till day 3, day 7, and 2 weeks postnatally. The enablers of MOM in the pre-discharge group were the number of expressions in the first 3 days, the number of night expressions in week 1, mother`s visit, and the number of maternal visits on day 1 to NICU and MOM volume expressed from day 1 until the second week after birth. The main barriers for MOM (48 hours pre-discharge) were extremely low birth weight (ELBW) and intrauterine growth-restricted infants (IUGR).\n\n\nCONCLUSIONS\nELBW infants and IUGR infants are susceptible to low MOM feeding. The total of milk expressions in the first 3 days, number of night expressions in the first week, maternal visits on day 1 and the average MOM amount in the first 2 weeks are enablers for MOM feeding.', 'corpus_id': 237329145, 'score': 0}, {'doc_id': '237630601', 'title': 'A randomised comparative study of coconut oil massage for effect on weight change in low birth weight neonates', 'abstract': 'Background: Low birth weight (LBW) babies have more the risk of the neurological complications, physiological problems and mental retardation. Topical massage with natural oil is routinely practiced in India. The positive effects of massage are weight gain, improved sleep/wake pattern, decreased the stress, early discharge from the neonatal intensive care unit (NICU), improve the skin integrity and enhanced parent’s infant bonding.Methods:This prospective interventional randomised comparative study was conducted among 64 LBW babies at Paediatric department of Swami Dayanand hospital (SDH), Delhi. Out of which 31 were in intervention group and 33 were in control group.\xa0 In the intervention group, mothers were encouraged to massage their babies with 10 ml of coconut oil for 15 min, twice a day until 10 days of life. Those allocated to the control group were received care as usual. Weight and head circumference was measured at enrolment and on day 11 in both the groups.Results: Basic characteristics of neonates of intervention and control group were almost similar. Mean weight gain in intervention group was 352.26±101.05 g while it was 209.70±124.66 g in control group (p=0.0001). Similarly mean weight gain velocity was significantly higher in intervention group (32.02±19.19 g/day) as compared to control group (19.09±11.33 g/day, p=0.0001).Conclusions:The present study supports significant increase in weight gain in LBW preterm and term neonates with coconut oil massage. Coconut oil is easily available in the market and it should be recommended to LBW babies for their better weight gain.', 'corpus_id': 237630601, 'score': 0}, {'doc_id': '237392173', 'title': 'Effects of Intermittent Kangaroo Mother Care in Preterm Low Birth Weight Babies: A Randomized Controlled Trial', 'abstract': 'Background: Prematurity is the largest cause of neonatal mortality. They need incubators or radiant warmers which are expensive and very difficult to arrange in a resource constraint country. Kangaroo mother care (KMC) had been proposed as an alternative to conventional neonatal care for low birthweight (LBW) babies. Objectives: To observe the benefits of Kangaroo mother care in preterm low birth weight babies. Methods: This randomized controlled trial was conducted over 6 months in Dhaka Shishu Hospital. Neonates who were <1800 gm and hemodynamically stable were enrolled. Total 80 neonates were enrolled and divided into 2 groups: Kangaroo mother care group and conventional method care group (incubator/warmer). The mother or caregiver were taught for KMC, supervised by trained nurses round the clock. KMC was given at least 2 hours at a time and at least 12 hours in a day. When the baby was not in KMC at that time the baby was placed in cot with adequate coverings. During hospital stay both the groups were monitored. Results: In KMC group 25% and conventional care group 40% neonates became hypothermic. Among the study population 35% neonates in KMC and 65% neonates in conventional care groups developed sepsis (p= 0.007). More KMC babies were exclusively breastfed at the end of the study (95% vs 60%). The KMC babies had shown better growth: weight gain per day (18.35±7.81 grams vs 13.55±4.89 p<0.001) and length (0.99±0.70 vs 0.71±0.44 cm, p = 0.03). KMC babies were discharged earlier than conventional care baby. Conclusion: KMC provides significant improvement in exclusive breast feeding, reduction of infection, decrease hospital stay and gaining weight of the babies. It also helps in maintaining temperature better than conventional care.', 'corpus_id': 237392173, 'score': 1}]"
139	Art Evolution	cfb97295ac27eb5812ffe8302c34e9f3	10853	{}	[{'doc_id': '221366046', 'title': 'To Participate in the Image: Reification & Reproduction', 'abstract': 'Spanning sculpture, video, and installation, I develop my work through techniques of collage and (re)appropriation to investigate and criticize Western hegemonic perceptions and representations of the (Asian) Other. I investigate perceptions of Asianness in the Western imagination from Orientalism to Techno-Orientalism, demonization and fetishization, focusing on the circulation of imagery in art, media, and popular culture. Further, I draw attention to the internalization of these standards and what it means to participate in the construction and distribution of the image, its implications and perceptions.', 'corpus_id': 221366046, 'score': 0}, {'doc_id': '85461776', 'title': 'Understanding Aesthetics and Fitness Measures in Evolutionary Art Systems', 'abstract': 'One of the general aims of evolutionary art research is to build a computer system capable of creating interesting, beautiful, or creative results, including images, videos, animations, text, and performances. In this context, it is crucial to understand how fitness is conceived and implemented to explore the “interestingness,” beauty, or creativity that the system is capable of. In this paper, we survey the recent research on fitness for evolutionary art related to aesthetics. We also cover research in the psychology of aesthetics, including relation between complexity and aesthetics, measures of complexity, and complexity predictors. We try to establish connections between human perception and understanding of aesthetics with current evolutionary techniques.', 'corpus_id': 85461776, 'score': 1}, {'doc_id': '222291109', 'title': 'Defining Computer Art: Methods, Themes, and the Aesthetic Problematic', 'abstract': 'The application of computer technology in the field of art has given rise to novel modes of artistic practice, including media art, and it is a necessity to find a commensurable conceptual fundament. Therefore, computer art starting from the 1950s reenters the view. To clarify the definition, major methods for defining are reviewed, and it is argued that the thematic definition guided by situational logic provides a feasible approach. There is a triad of themes: the relationship between art and technology, the problem of machine creation, and the ontology of art. Consisted of primitive and mutually supportive questions, a logical space of questioning, i.e. a problematic, is formed as the basis for the identity of computer art. Among them, the problem of the ontology of art is located at the logical starting point of questioning, and this problematic is therefore an aesthetic one. The anticipation that computer art presents and responds to the above-mentioned aesthetic problematic suggests the plausibility of computer art being a legitimate category of art.', 'corpus_id': 222291109, 'score': 0}, {'doc_id': '221668674', 'title': 'Creating Object-Based Learning for the Anthropocene: A Critical Reflection', 'abstract': 'The Anthropocene is the human age. Its undeniable significance has been ascribed across disciplines from geology, to cultural studies, to fine art. Through reflective analysis, this paper explores the role and significance of creative practice, the found object, and the use of object-based adventures in teaching the Anthropocene. It also considers the role of virtual object-based learning in a digital age through a “Gallery of Late Humanity”, through the reflexive lens of a lecturer who teaches both environmental sciences and cultural geography. These methods successfully encouraged learning across and beyond the disciplinary boundaries of geography and fine art, providing creative re-imaginings, visualisations and understandings of the Anthropocene. These approaches illustrate how the quotidian materialities of home can be reconfigured as a field site for the late Anthropocene.', 'corpus_id': 221668674, 'score': 0}, {'doc_id': '161707355', 'title': 'Archaeologists, Indians, and Evolutionary Psychology: Aspects of Rock Art Research', 'abstract': 'David S. Whitley is a principal at ASM Affiliates, Inc., Tehachapi, CA, an adjunct professor at the School of Geographical Sciences, Arizona State University, Tempe, and a senior research fellow at the Rock Art Research Institute, University of the Witwatersrand. His research focuses on North American rock art and Native American ethnography. His latest book is Cave Paintings and the Human Spirit: The Origin of Creativity and Belief. huitli53@gmail.com', 'corpus_id': 161707355, 'score': 1}, {'doc_id': '152253095', 'title': 'Reflections on Art Making and Evolutionary Psychology', 'abstract': 'This paper outlines a case for suggesting that all human beings have an innate need to create and confer aesthetic significance. It argues that beauty is important for us and that humans have an innate sense of aesthetic “rightness”; we confer aesthetic significance on a wide range of phenomena, but “art” has a particular role to play. Drawing upon the work of Denis Dutton and Ellen Dissanayake, a working definition of “art” from the perspective of evolutionary psychology is given, and an argument is presented based on the notion that art making (in its broadest sense) and its appreciation are inherited predispositions and are not necessarily culturally determined. The author contends that the thing that sums up our fundamental character is our innate capacity for creating and conferring aesthetic significance and that to deny young people the opportunity to do this would be to deny their essential humanity.', 'corpus_id': 152253095, 'score': 1}, {'doc_id': '224837798', 'title': 'Value Intentions in Future Art Teachers’ Professional Training', 'abstract': 'The article is devoted to the study of future art teachers’ value sphere. A wide range of applications of the theory of values in various fields of knowledge is shown: culturology, psychology, pedagogy, art education, economics; classification of values in science is given. A number of issues is actualized, concerning values in the context of cultural development and at the same time global crises; professional development, competitiveness and life views and needs; development of person’s abilities and his/her self-realization in art creativity. The artistic-communicative, professional-hermeneutic and motivational-need spheres in the future art teachers’ creative self-realization are singled out. These areas are subject to pedagogical influence, while becoming a conglomeration of value intentions of the individual. Theoretical substantiation is conducted and the essence of the phenomenon of future art teachers’ value intentions is determined, which combines the specified multi-vector values. The meaning of the terms “intentionality” and “intentions” is clarified. Emphasis is placed on the importance of value intentions of the individual in the economic projection and compliance of this socio-personal construct with the current concept of student-centered learning. The results of the study of future art teachers’ value intentions, which are formed during professional training and creative activity, are given. The results of the value intentions diagnostics and dynamics of their changes in accordance with future musical art teachers’ professional training are highlighted. Scientific approaches and technologies that effectively influence their value intentions are outlined. It is proved that definition of a conglomeration of value intentions of an individual allows a qualitative choice of methods for the formation of creative and competitive personality of future musical art teachers and teachers of art disciplines in general.', 'corpus_id': 224837798, 'score': 0}, {'doc_id': '191759688', 'title': 'Standpoints in the Evolution of Art', 'abstract': 'Different evolutionary theories and models were suggested trying to reason the emergence of the symbolic culture and symbolic behavior, giving rise to art, alongside with language, rituals, and religion. The article reviews different perspectives in the issue, giving sample studies for each school or standpoint. Reviewed theoretical positions include classic Darwinian standpoint, the revolutionary theory with its different branches, socio biological or Neo-Darwinian standpoint with its different approaches including evolutionary psychological school, behavioral ecological school, and feminist school. Scholars main Criticism for each theoretical position is reviewed whenever possible.', 'corpus_id': 191759688, 'score': 1}, {'doc_id': '194402641', 'title': 'Blending in the Evolution of Art', 'abstract': 'Conceptual blending plays a widespread role in the evolution of art. This article, a draft of a commentary for a special issue of a journal on the evolution of art, reviews the work of conceptual blending in conceiving of other minds, joint attention, objects of art and ritual, and imagined scenes of blended joint attention. It focuses especially on the use of conceptual blending in art to prompt for mentally tractable compressions that make it possible for human beings to handle vast mental networks that stretch across great ranges of time, space, causation, and agency.', 'corpus_id': 194402641, 'score': 1}, {'doc_id': '221949242', 'title': 'Deep learning of individual aesthetics', 'abstract': 'Accurate evaluation of human aesthetic preferences represents a major challenge for creative evolutionary and generative systems research. Prior work has tended to focus on feature measures of the artefact, such as symmetry, complexity and coherence. However, research models from psychology suggest that human aesthetic experiences encapsulate factors beyond the artefact, making accurate computational models very difficult to design. The interactive genetic algorithm circumvents the problem through human-in-the-loop, subjective evaluation of aesthetics, but is limited due to user fatigue and small population sizes. In this paper, we look at how recent advances in deep learning can assist in automating personal aesthetic judgement. Using a leading artist’s computer art dataset, we investigate the relationship between image measures, such as complexity, and human aesthetic evaluation. We use dimension reduction methods to visualise both genotype and phenotype space in order to support the exploration of new territory in a generative system. Convolutional neural networks trained on the artist’s prior aesthetic evaluations are used to suggest new possibilities similar or between known high-quality genotype-phenotype mappings. We integrate this classification and discovery system into a software tool for evolving complex generative art and design.', 'corpus_id': 221949242, 'score': 0}]
140	Peds COVID	746fc0bd9a30b660d0ea079863f8408b	11978	{'COVID': 'coronavirus disease'}	"[{'doc_id': '225068415', 'title': 'Coronavirus-2019 Disease (COVID-19) in Children', 'abstract': 'Abstract COVID-19 disease affects all ages, but severe cases of the disease and mortality are very rarely seen among children. In most cases, they acquire the virus from their parents or from an another infected person. The exact reasons why the disease has a milder course in children is unknown but high numbers of Angiotensin Converting Enzyme-2 (ACE2) receptors, underdeveloped immune responses, cross-reaction with other viruses, protective effect of fetal hemoglobin and fewer outdoor activities as well as journeys, and nonexposure to air pollution, and smoking. Although many cases are asymptomatic, they can still shed the virus. Materno-fetal vertical transmission has not been shown so far. In symptomatic cases, clinical findings include fever and respiratory symptoms, followed by diarrhea and vomiting. There are signs indicating a possible association between Kawasaki disease and COVID-19. Clinical findings and diagnostic procedures in newborns, and older children are similar. Supportive therapy is essential and antiviral agents are not required in most cases. During cytokine storm, anti-inflammatory treatments may be tried. There is no evidence for transmission through breastmilk; therefore infected mothers should breastfeed their infants by taking all precautions. Routine immunizations of children should not be deferred during COVID-19 outbreak period. Psychological support for children who need to stay at home and for healthcare personnel should be provided.', 'corpus_id': 225068415, 'score': 0}, {'doc_id': '229207643', 'title': 'COVID 19: Children should be Treated Even in Absence of Symptoms', 'abstract': 'The current pandemic by COVID 19 leaves new teachings at every moment. One of them is that children (especially those from early childhood) have a viral load of COVID 19 up to 10 times higher than adults, even though they are, in their vast majority, asymptomatic. This is of enormous sanitary importance, since they are ""healthy"" carriers, who can transmit the disease. For this reason, the authors emphasize the need to treat this age group with nasal and oral carrageenan, in order to cut the chain of contagion.', 'corpus_id': 229207643, 'score': 1}, {'doc_id': '224805730', 'title': 'COVID-19 infection prevalence in pediatric population: Etiology, clinical presentation, and outcome', 'abstract': '\n Novel COVID-19 infections caused major morbidity and mortality globally in the adult age group. Likewise, SARS-COV-2 infections in children are highly risky in the selected patient population. We performed a focused literature search of published reports from December 1, 2019, till August 20, 2020. The aim was to explore the etiology, clinical presentations, and outcome of pediatric COVID-19 patients. Viral respiratory infections are associated with high societal costs for children. In addition, children with asymptomatic SARS-COV-2 infections can be a source of COVID-19 spread to parents and caregivers. The major reported risk factors for pediatric COVID-19 cases were close contact with a SARS-COV-2 positive family member, a history of travel, and/or living in endemic areas. Children with COVID-19 who required ICU care had various comorbidities, such as malignancy. As the pandemic evolved, multiple cases of multisystem inflammatory syndrome in children and adolescents temporarily related to covid-19 (MIS-C) were reported. A unique population is neonates born to COVID-19 affected mothers, as there is an urgent need to optimize their management and outcome during this rapidly evolving pandemic. The early identification of SARS-COV-2 infection in infants and children has important direct management effects in these children and public health implications because of the effects on disease transmission control measures.\n', 'corpus_id': 224805730, 'score': 0}, {'doc_id': '226496140', 'title': 'SARS-CoV-2 infection in children/ Çocuklarda sars-cov-2 enfeksiyonu', 'abstract': 'SARS-CoV-2, a RNA virus that emerged in December 2019 in the city of Wuhan in China and took hold of the whole world, affects children as well as all age groups. In our country, we started to observe the first cases by March 2020. SARS-CoV-2, which is transmitted by droplets and by way of contact with surfaces contaminated by these droplets, is generally transmitted to children from adults through close contact. There is no proven information about other transmission routes such as fecal-oral transmission. Similar to adults, the primary symptoms at presentation include fever, cough, sore throat, malaise, nasal dis-charge, and rarely, vomiting and diarrhea in children. Although the majority of pediatric patients are asymptomatic or have a mild clinical course, severe cases have been reported in children with underlying chronic diseases. There is currently no specific antiviral treatment against the SARS-CoV-2 virus. Supportive treatment is recommended in children with a mild course, and some treatments are recommended in children with comorbidities or in children who are observed to have a more severe course. Asymptomatic pediatric patients or pediatric patients who have a mild course constitute an important group in terms of transmission of the infection to the advanced age group who carry high risk. Prevention of infection is very important in terms of reducing new cases and alleviating the load on the health-care system. In order to prevent transmission of SARS-CoV-2, hygienic rules should be pursued in the community, social distancing should be observed, and the family members and contacts of patients who have been diagnosed should be screened and isolated.', 'corpus_id': 226496140, 'score': 0}, {'doc_id': '222216733', 'title': 'COVID-19 in children: current evidence and key questions', 'abstract': 'Purpose of review SARS-CoV-2 infection in children has been less well characterized than in adults, primarily due to a significantly milder clinical phenotype meaning many cases have gone undocumented by health professionals or researchers. This review outlines the current evidence of the epidemiology of infection in children, the clinical manifestations of disease, the role of children in transmission of the virus and the recently described hyperinflammatory syndrome observed later during the first phase of the pandemic. Recent findings International seroprevalence studies have found younger children to have lower prevalence of antibodies to SARS-CoV-2, indicating they have not been infected as much as adults. This may be due to shielding by school closures, or by a reduced susceptibility to infection, as indicated by a significantly lower attack rate in children than adults in household contact tracing studies. The most well recognized symptoms in adults of cough, fever, anosmia and ageusia are less frequent in children, who may often present with mild and nonspecific symptoms, or with gastrointestinal symptoms alone. Risk factors for severe disease in children include chronic lung, cardiac or neurological disease, and malignancy. However, the absolute risk still appears very low for these cohorts. A new hyperinflammatory syndrome has emerged with an apparent immune cause. Summary Important questions remain unanswered regarding why children have mild disease compared with adults; how children of different ages contribute to asymptomatic community transmission of the virus; and the pathophysiology of and most appropriate investigation and treatment strategies for the novel hyperinflammatory syndrome.', 'corpus_id': 222216733, 'score': 1}, {'doc_id': '219118986', 'title': 'Epidemiology of COVID-19 Among Children in China', 'abstract': 'This study examined the epidemiological characteristics and transmission patterns of 2135 pediatric patients with COVID-19 using a retrospective analytical approach. OBJECTIVE: To identify the epidemiological characteristics and transmission patterns of pediatric patients with the 2019 novel coronavirus disease (COVID-19) in China. METHODS: Nationwide case series of 2135 pediatric patients with COVID-19 reported to the Chinese Center for Disease Control and Prevention from January 16, 2020, to February 8, 2020, were included. The epidemic curves were constructed by key dates of disease onset and case diagnosis. Onset-to-diagnosis curves were constructed by fitting a log-normal distribution to data on both onset and diagnosis dates. RESULTS: There were 728 (34.1%) laboratory-confirmed cases and 1407 (65.9%) suspected cases. The median age of all patients was 7 years (interquartile range: 2–13 years), and 1208 case patients (56.6%) were boys. More than 90% of all patients had asymptomatic, mild, or moderate cases. The median time from illness onset to diagnoses was 2 days (range: 0–42 days). There was a rapid increase of disease at the early stage of the epidemic, and then there was a gradual and steady decrease. The disease rapidly spread from Hubei province to surrounding provinces over time. More children were infected in Hubei province than any other province. CONCLUSIONS: Children of all ages appeared susceptible to COVID-19, and there was no significant sex difference. Although clinical manifestations of children’s COVID-19 cases were generally less severe than those of adult patients, young children, particularly infants, were vulnerable to infection. The distribution of children’s COVID-19 cases varied with time and space, and most of the cases were concentrated in Hubei province and surrounding areas. Furthermore, this study provides strong evidence of human-to-human transmission.', 'corpus_id': 219118986, 'score': 1}, {'doc_id': '227172135', 'title': 'COVID-19 in Pediatric Patients: A Focus on CHD Patients', 'abstract': 'Coronavirus disease 2019 (COVID-19) is a global pandemic caused by SARS-CoV-2 virus. As of the 30th of September 2020, around 34,000,000 cases have been reported globally. Pediatrics with underlying congenital heart disease represent a small yet a critical proportion of these patients. In general, the majority of infected children experience mild to moderate disease with significant interindividual variability in laboratory and radiographic findings. Nevertheless, in healthy children with COVID-19, cardiac involvement has been documented and is attributed to various causes. Myocarditis, arrhythmias, cardiogenic shock, and serious multisystem inflammatory syndrome in children are all encountered. Since COVID-19 is a recent novel disease and based on previous experience with respiratory infections, children with underlying congenital heart disease should be given special attention. To date, little data is available about COVID-19 presentation, complications, and appropriate treatment in this population. However, variable and inconsistent disease presentation and severity have been observed. This paper discusses COVID-19 course of illness in pediatric population with a special emphasis on the cardiac manifestations of the disease in healthy population and also on the disease course in congenital heart disease patients in particular.', 'corpus_id': 227172135, 'score': 0}, {'doc_id': '225138420', 'title': 'CHARACTERISTICS and considerations in the medical treatment of COVID‐19 in children', 'abstract': 'It is rare for children to be in serious condition or die from coronavirus disease 2019 (COVID‐19) caused by the 2019 novel coronavirus (severe acute respiratory syndrome coronavirus 2 [SARS‐CoV‐2]) except for those with underlying diseases such as chronic lung disease (including asthma), cardiovascular disease, and immunosuppressive disease. Recently, patients with hyperinflammatory shock have been identified among children who are confirmed to have or are suspected of having SARS‐CoV‐2 infection. The presenting signs and symptoms are characterized by prolonged fever, abdominal pain, and cardiac involvement without any signs of pneumonia on chest computed tomography. However, it is uncertain at this time whether SARS‐CoV‐2 infection affects this syndrome. Compared with adults, quite a few children are asymptomatic even when infected with SARS‐CoV‐2, which could make these children serious sources of infection at home or in medical institutions. Considering these characteristics, it is important to take appropriate precautions during medical examinations and perform infection control in emergency departments to save the lives of both the children and adult patients. Most healthy children are suffering from huge stress due to restrictions against going outside and school closures as social means to control infection. It is possible that children are socially isolated when they come to the emergency department, and they might require mental or social support even if they are only complaining about their physical condition. Health‐care providers are required to examine the children’s circumstances carefully and cooperate with workers in other professions appropriately.', 'corpus_id': 225138420, 'score': 0}, {'doc_id': '222352864', 'title': 'COVID-19, children and non-communicable diseases: translating evidence into action', 'abstract': 'The world faces an existential, once in a lifetime pandemic due to a novel coronavirus (SARS-CoV-2) which has to date infected over 25\u2009million people across the world, with nearly 850\u2009000 deaths.1 The disease, labelled COVID-19 by the WHO, has now spread to almost all the countries of the world and crippled the global economy. While high-income countries have been able to tap into their resources and reserves, for many low-income and middle-income countries, rising unemployment, population lock downs and closure of businesses have inflicted crippling damage on fragile economies, with rising inequalities and worsening poverty.\n\nWhile early reports of the infection2 3 suggested that the infection may be generally mild in children with COVID-19, with general case fatality rate less than 1%, there are increasing reports of complications among children and adolescents.4 In addition, a recent series of cases with multisystem inflammatory response merits reconsideration of these risks.5 There are also clear signals of predictors for adverse outcomes from COVID-19 infections. The disease has disproportionately taken a toll among the elderly population in long-term care facilities, with many dying without even being tested for COVID-19 infection.6 There is clear evidence of excess mortality in subgroups, especially those with comorbidities, most commonly related to non-communicable diseases (NCDs), such as diabetes, hypertension, obesity, heart disease and cancer.7 The same appears to be true among paediatric COVID-19 infections. A systematic review analysed a total of 7780 paediatric COVID-19 positive cases globally, and found that patients with information on underlying conditions (n=655) included the following comorbidities: immunosuppression (30.5%), respiratory conditions (20%) and cardiovascular disease (14%).8 A recent report from the UK of 651 hospitalised children with COVID-19 from 260 hospitals identified comorbidities in 42% (276/651) of cases.9 Comorbidities most commonly associated with …', 'corpus_id': 222352864, 'score': 1}, {'doc_id': '225050243', 'title': 'Clinical and Epidemiologic Analysis of COVID-19 Children Cases in Colombia PEDIACOVID', 'abstract': 'Objective: The COVID pandemic has affected Colombia with a high number of cases and deceases; however, no studies have been published regarding pediatric population. An epidemiologic analysis of the nationwide COVID register, therefore, is necessary to outline and describe the impact in such population. Methods: A retrospective analysis was made of the characteristics of a cohort of 5062 patients <18 years of age, until June 16, 2020, reported at the National Institute of Health—INS (https://www.ins.gov.co/News./Pages/Coronavirus.aspx), through the national public access database, with all subjects confirmed with COVID-19 or severe acute respiratory syndrome-CoV-2. Results: Reviewed on June 16, 2020, a total of 54,971 confirmed cases were reported nationwide for COVID-19, of which 5062 (9.2%) are cases in patients under 18 years of age. There was a statistically significant difference between groups; age was statistically significantly higher in the asymptomatic, compared with: deceased, severe and moderate cases; moreover, age was statistically significantly higher in the mild, compared with: deceased, severe and moderate. Statistically significant difference determined with one-way ANOVA was found between groups (F = 16.08, P < 0.001). Post hoc analysis reveals significant differences between groups, the age of patients at home (9.39 years) and those recovered (9.3 years) being significantly higher than those in intensive care unit (4.9 years), in hospital (6.1 years), or than the deceased (2.9 years). Conclusion: The results of this study show that, at the nationwide level, patients in more severe states (deceased, severe and moderate), are significantly younger than those in the milder state (asymptomatic and mild).', 'corpus_id': 225050243, 'score': 1}]"
141	Covid Long Term 2	f79373d73438cef9a1bcbfde7137aa4e	13968	{}	"[{'doc_id': '222092207', 'title': 'Understanding the long-term health effects of COVID-19', 'abstract': ""6 months after WHO declared it a pandemic, SARS-CoV-2 is still spreading worldwide, and COVID-19 disease has had an overwhelming impact on public health and the global economy. With more than 31 million cases reported worldwide as of Sept 21st, 2020, the number of recovered patients with persisting symptoms and unexpected sequelae is increasing. Previous outbreaks of Spanish influenza, severe acute respiratory syndrome, and Ebola have shown that survivors can suffer from long-term complications. However, research is still required to determine the longterm effects of SARS-CoV-2. Although SARS-CoV-2 primarily affects the lungs, it has been found to damage the vascular endothelium of several other organs, resulting in complaints such as brain fog, palpitations, and fatigue, among others. The extrapulmonary manifestations of COVID-19 are varied, and the heart, brain, and kidneys are particularly susceptible to damage. This vascular component of COVID-19 might help to explain why certain patients still struggle with severe symptoms months after clearing the viral infection. Cardiovascular disease was shown to play an important role in COVID-19 pathology early in the pandemic. Myocardial inflammation after recovery from COVID-19, even in asymptomatic or mildly symptomatic patients, has been reported. Furthermore, a study by Marc Dweck and colleagues (University of Edinburgh, Edinburgh, UK), published in the European Heart Journal—Cardiovascular Imaging in June, 2020, revealed that 55% of 1216 patients with COVID-19 had an abnormal echocardiogram, with evidence of left and right ventricular abnormalities, and myocardial infarction. An abnormal echocardiograph was also observed in 314 of 581 children with paediatric inflammatory multisystem syndrome associated with COVID-19, as reported in a systematic review by Mubbasheer Ahmed and colleagues (Texas Children's Hospital, Houston, TX, USA), published in EClinicalMedicine in September, 2020. A study by Valentina Puntmann and colleagues (University Hospital Frankfurt, Germany), published in JAMA Cardiology in July, 2020, suggests that there is a possibility of residual left ventricular dysfunction and ongoing inflammation months after a COVID-19 diagnosis, which might progress to heart failure and other cardiovascular complications. Additionally, by damaging the endothelium, COVID-19 might result in abnormal blood clotting, with estimates suggesting that up to 30% of people with severe COVID-19 develop blood clots. It is unclear how long the prothrombotic environment persists in recovered patients, but in the July, 2020, issue of EClinicalMedicine, Amy Rapkiewicz and colleagues (Long Island School of Medicine, Mineola, NY, USA) showed blood clots to affect multiple organs. Evidence for the various neurological presentations associated with COVID-19 is increasing. A study by Yiping Lu and colleagues"", 'corpus_id': 222092207, 'score': 1}, {'doc_id': '226311934', 'title': 'Long-Term Respiratory and Neurological Sequelae of COVID-19', 'abstract': 'Since the initial reports of coronavirus disease 2019 (COVID-19) in China in late 2019, infections from severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) have spread rapidly, resulting in a global pandemic that has caused millions of deaths. Initially, the large number of infected people required the direction of global healthcare resources to provide supportive care for the acutely ill population in an attempt to reduce mortality. While clinical trials for safe and effective antiviral agents are ongoing, and vaccine development programs are being accelerated, long-term sequelae of SARS-CoV-2 infection have become increasingly recognized and concerning. Although the upper and lower respiratory tracts are the main sites of entry of SARS-CoV-2 into the body, resulting in COVID-19 pneumonia as the most common presentation, acute lung damage may be followed by pulmonary fibrosis and chronic impairment of lung function, with impaired quality of life. Also, increasing reports have shown that SARS-CoV-2 infection involves the central nervous system (CNS) and the peripheral nervous system (PNS) and directly or indirectly damages neurons, leading to long-term neurological sequelae. This review aims to provide an update on the mechanisms involved in the development of the long-term sequelae of SARS-CoV-2 infection in the 3 main areas of lung injury, neuronal injury, and neurodegenerative diseases, including Alzheimer disease, Parkinson disease, and multiple sclerosis, and highlights the need for patient monitoring following the acute stage of infection with SARS-CoV-2 to provide a rationale for the prevention, diagnosis, and management of these potential long-term sequelae.', 'corpus_id': 226311934, 'score': 1}, {'doc_id': '219755182', 'title': 'The cognitive consequences of the COVID-19 epidemic: collateral damage?', 'abstract': 'Abstract Recovery from coronavirus disease 2019 (COVID-19) will be principally defined in terms of remission from respiratory symptoms; however, both clinical and animal studies have shown that coronaviruses may spread to the nervous system. A systematic search on previous viral epidemics revealed that while there has been relatively little research in this area, clinical studies have commonly reported neurological disorders and cognitive difficulties. Little is known with regard to their incidence, duration or underlying neural basis. The hippocampus appears to be particularly vulnerable to coronavirus infections, thus increasing the probability of post-infection memory impairment, and acceleration of neurodegenerative disorders such as Alzheimer’s disease. Future knowledge of the impact of COVID-19, from epidemiological studies and clinical practice, will be needed to develop future screening and treatment programmes to minimize the long-term cognitive consequences of COVID-19.', 'corpus_id': 219755182, 'score': 1}, {'doc_id': '220439415', 'title': 'Bruns Syndrome – An Unusual Presentation', 'abstract': 'Bruns syndrome is characterized by attacks of sudden severe headache, vomiting, and vertigo precipitated due to abrupt movements of the head due to presence of mobile deformable intraventricular lesion causing episodic obstructive hydrocephalus. Proposed underlying mechanism is intermittent or positional CSF obstruction resulting from ball-valve mechanism. Most common etiologies are NCC and intraventricular tumors. Here we present an unusual case of Bruns syndrome that was initially MRI negative.', 'corpus_id': 220439415, 'score': 0}, {'doc_id': '229176435', 'title': 'Presentations and mechanisms of CNS disorders related to COVID-19', 'abstract': 'Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) is the cause of the coronavirus disease 2019 (COVID-19) pandemic. In addition to severe respiratory symptoms, there are a growing number of reports showing a wide range of CNS complications in patients with COVID-19. Here, we review the literature on these complications, ranging from nonspecific symptoms to necrotizing encephalopathies, encephalitis, myelitis, encephalomyelitis, endotheliitis, and stroke. We postulate that there are several different mechanisms involved in COVID-19–associated CNS dysfunction, particularly activation of inflammatory and thrombotic pathways and, in a few patients, a direct viral effect on the endothelium and the parenchyma. Last, critically ill patients frequently present with protracted cognitive dysfunction in the setting of septic encephalopathy likely due to multifactorial mechanisms. Further studies are needed to clarify the relative contribution of each of these mechanisms, but available data suggest that CNS complications in COVID-19 are rare and probably not directly caused by the virus.', 'corpus_id': 229176435, 'score': 0}, {'doc_id': '220892495', 'title': '[COVID-19: A Pneumological Point of View - Long-Term Sequelae of COVID-19 - Implications For Follow-up In Respiratory Medicine].', 'abstract': 'The long-term sequelae of COVID-19 on are not yet predictable. Radiological and histopathological data on COVID-19 and observational studies after the SARS-CoV-1 pandemic 2003/2004 suggest that in a proportion of COVID-19 patients, functional limitations due to pulmonary fibrosis and other patterns of lung damage may persist. Systematic follow-up, based on prudent pulmonary function testing, is warranted for the correct diagnosis, graduation and treatment of the underlying pathology at an early stage. This review summarizes the potential spectrum of Post-COVID-19 pulmonary disease patterns and provides recommendations for the follow-up care of COVID-19 patients in the field of respiratory medicine.', 'corpus_id': 220892495, 'score': 1}, {'doc_id': '25896405', 'title': 'Backbone statistical potential from local sequence-structure interactions in protein loops.', 'abstract': 'Native proteins have been optimized by evolution simultaneously for structure and sequence. Structural databases reflect this interdependency. In this paper, we present a new statistical potential for a reduced backbone representation that has both structure and sequence characteristics as variables. We use information from structural data available in the Protein Coil Library, selected on the basis of resolution and refinement factor. In these structures, the nonlocal interactions are randomly distributed and, thus, average out in statistics, so structural propensities due to local backbone-based interactions can be studied separately. We collect data in the form of local sequence-specific phi-psi backbone dihedral pairs. From these data, we construct dihedral probability density functions (DPDFs) that quantify any adjacent phi-psi pair distribution in the context of all possible combinations of local residue types. We use a probabilistic analysis to deduce how the correlations encoded in the various DPDFs as well as in residue frequencies propagate along the sequence and can be cumulated in a statistical potential capable of efficiently scoring a loop by its backbone conformation and sequence only. Our potential is able to identify with high accuracy the native structure of a loop with a given sequence among possible alternative conformations from sets of well-constructed decoys. Conversely, the potential can also be used for sequence prediction problems and is shown to score the native sequence of a given loop structure among the most fit of the possible sequence combinations. Applications for both structure prediction and sequence design are discussed.', 'corpus_id': 25896405, 'score': 0}, {'doc_id': '230709610', 'title': 'Overview of COVID-19 Lung Damage Clinical Trial Using Cellular Stromal Vascular Fraction (CSVF) and Functional Respiratory Imaging (FRI) Analysis of Pulmonary Injury & Post-Viral (SARS=Cov-2) Adult Respiratory Distress Syndrome (ARDS)', 'abstract': 'A Coronavirus (nCoV-2) has caused a worldwide pandemic, beginning in Wuhan, China. Patient’s acquiring viral infection (COVID-19) which is featured by elevation of temperature, cough, severe fatigue, and a variety of symptoms mimicking the flu and pneumonias. This gradually extended to a progressive and serious respiratory failure with associated ARDS and severe pneumonias with residual scarring of the interstitial areas of the lung alveoli, resulting in acute life-threatening, and permanent compromised gas exchange. Those that survive controlled ventilation (low tidal volumes), medical induced coma, and prone positioning, are often left with reduction in pulmonary function abilities. This paper reviews some pertinent knowledge and advances in understanding of the pulmonary injuries and long-term damage of the COVID-19 patients, and reports a study to ameliorate such damage with use of cSVF for those who successfully recover. There now exists technology to perform sophisticated analytics (Functional Respiratory Imaging, FRI) developed by Fluidda (Belgium, EU) which provides great detail and information, including the ability to help triage the infected patients early to help assist in prediction of ventilation and ICU needs. In addition, author’s experience with use in Chronic Obstructive Pulmonary Disease (COPD) and Fibrotic Lung Diseases, the ability to evaluate effectiveness of use of cellular Stromal Vascular Fraction (cSVF) following intravascular deployment of isolated and concentrated cSVF. The hypothesis is that reduction of the widespread pulmonary inflammatory and immune responses which destroy much of the gas exchange abilities in the acute and residual damage which appears to create a long-term problem in the surviving patient population. Fluidda imaging analytics are effective at predictive studies which may prove of great help in anticipating which patient group is at most risk in the earlier infections to require ICU and potential ventilation needs. The clinical trial involves standard sterile microcannula harvesting of the SVF (GEMS, Tulip Medical, USA), followed by isolation/concentration of the cSVF within a closed system with incubation, agitation of enzyme in Centricyte 1000 (Healeon Medical, Newbury Park, CA, USA). The cSVF is then re-suspended and slowly delivered via peripheral IV deployment using 150 micron in-line filters. Serial examinations using High Resolution Computerized Tomography Lungs (HRCT Lungs) at both Total Lung Capacity (TLC) and Functional Residual Capacity (FRC) at baseline and 3 month intervals. In addition, tracking of functional lung testing, and oxygen demands (if appropriate). Experiences and research confirms the ability of components of the SVF to assist in the modulation of inflammatory and immune responses, which now appear be some of the most threatening and damaging effects in the COVID-19 patient population. This is a Phase 0/I Clinical Trial (10 Patients), with the plan to follow up with a full randomized Phase I/II study.', 'corpus_id': 230709610, 'score': 1}, {'doc_id': '231831378', 'title': 'Histopathological basis of COVID-19: A short review', 'abstract': 'Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) is the cause of the global pandemic in 2020 resulting a massive morbidity & mortality The lung is the predominant organ involved in symptomatic patients with Coronavirus disease-2019 (COVID-19) along with involvement of other organs Understanding the histopathological changes of lung and other organs in COVID-19 become essential for not only formulating future management protocols but also for determing prognosis The collection of potentially contaminated tissues during the autopsy and further processing of them in histopathology laboratory with proper maintenance of safety protocol is of immense importance A review of the available scientific articles shows the diffuse alveolar damage and microvascular thrombi are common observation found in lung tissue in patients who died due to COVID-19 © 2020 BioMed Central Ltd unless otherwise stated Part of Springer Nature', 'corpus_id': 231831378, 'score': 0}, {'doc_id': '227257719', 'title': 'Complications and Pathophysiology of COVID-19 in the Nervous System', 'abstract': 'The coronavirus disease (COVID-19) pandemic, caused by the severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2), has become a global public health threat. Majority of the patients with COVID-19 have fever, cough, and fatigue. Critically ill patients can develop dyspnea and acute respiratory distress syndrome. In addition to respiratory symptoms, neurological damage also occurs in some patients. However, the mechanisms by which SARS-CoV-2 invades the nervous system have not been elucidated yet. In order to provide some reference for designing optimal therapeutic strategies, we have discussed the complications and potential mechanisms of COVID-19 in the nervous system in this review.', 'corpus_id': 227257719, 'score': 0}]"
142	semantic parsing	28fd142f4cce2972cdb2401c73f62eaf	238	{}	"[{'doc_id': '202677562', 'title': 'A Split-and-Recombine Approach for Follow-up Query Analysis', 'abstract': 'Context-dependent semantic parsing has proven to be an important yet challenging task. To leverage the advances in context-independent semantic parsing, we propose to perform follow-up query analysis, aiming to restate context-dependent natural language queries with contextual information. To accomplish the task, we propose STAR, a novel approach with a well-designed two-phase process. It is parser-independent and able to handle multifarious follow-up scenarios in different domains. Experiments on the FollowUp dataset show that STAR outperforms the state-of-the-art baseline by a large margin of nearly 8%. The superiority on parsing results verifies the feasibility of follow-up query analysis. We also explore the extensibility of STAR on the SQA dataset, which is very promising.', 'corpus_id': 202677562, 'score': 0}, {'doc_id': '204851974', 'title': 'A Hybrid Semantic Parsing Approach for Tabular Data Analysis', 'abstract': 'This paper presents a novel approach to translating natural language questions to SQL queries for given tables, which meets three requirements as a real-world data analysis application: cross-domain, multilingualism and enabling quick-start. Our proposed approach consists of: (1) a novel data abstraction step before the parser to make parsing table-agnosticism; (2) a set of semantic rules for parsing abstracted data-analysis questions to intermediate logic forms as tree derivations to reduce the search space; (3) a neural-based model as a local scoring function on a span-based semantic parser for structured optimization and efficient inference. Experiments show that our approach outperforms state-of-the-art algorithms on a large open benchmark dataset WikiSQL. We also achieve promising results on a small dataset for more complex queries in both English and Chinese, which demonstrates our language expansion and quick-start ability.', 'corpus_id': 204851974, 'score': 1}, {'doc_id': '202783651', 'title': 'A Pilot Study for Chinese SQL Semantic Parsing', 'abstract': 'The task of semantic parsing is highly useful for dialogue and question answering systems. Many datasets have been proposed to map natural language text into SQL, among which the recent Spider dataset provides cross-domain samples with multiple tables and complex queries. We build a Spider dataset for Chinese, which is currently a low-resource language in this task area. Interesting research questions arise from the uniqueness of the language, which requires word segmentation, and also from the fact that SQL keywords and columns of DB tables are typically written in English. We compare character- and word-based encoders for a semantic parser, and different embedding schemes. Results show that word-based semantic parser is subject to segmentation errors and cross-lingual word embeddings are useful for text-to-SQL.', 'corpus_id': 202783651, 'score': 0}, {'doc_id': '174802873', 'title': 'SParC: Cross-Domain Semantic Parsing in Context', 'abstract': 'We present SParC, a dataset for cross-domainSemanticParsing inContext that consists of 4,298 coherent question sequences (12k+ individual questions annotated with SQL queries). It is obtained from controlled user interactions with 200 complex databases over 138 domains. We provide an in-depth analysis of SParC and show that it introduces new challenges compared to existing datasets. SParC demonstrates complex contextual dependencies, (2) has greater semantic diversity, and (3) requires generalization to unseen domains due to its cross-domain nature and the unseen databases at test time. We experiment with two state-of-the-art text-to-SQL models adapted to the context-dependent, cross-domain setup. The best model obtains an exact match accuracy of 20.2% over all questions and less than10% over all interaction sequences, indicating that the cross-domain setting and the con-textual phenomena of the dataset present significant challenges for future research. The dataset, baselines, and leaderboard are released at https://yale-lily.github.io/sparc.', 'corpus_id': 174802873, 'score': 1}, {'doc_id': '207852944', 'title': 'Knowledge Guided Text Retrieval and Reading for Open Domain Question Answering', 'abstract': 'We introduce an approach for open-domain question answering (QA) that retrieves and reads a passage graph, where vertices are passages of text and edges represent relationships that are derived from an external knowledge base or co-occurrence in the same article. Our goals are to boost coverage by using knowledge-guided retrieval to find more relevant passages than text-matching methods, and to improve accuracy by allowing for better knowledge-guided fusion of information across related passages. Our graph retrieval method expands a set of seed keyword-retrieved passages by traversing the graph structure of the knowledge base. Our reader extends a BERT-based architecture and updates passage representations by propagating information from related passages and their relations, instead of reading each passage in isolation. Experiments on three open-domain QA datasets, WebQuestions, Natural Questions and TriviaQA, show improved performance over non-graph baselines by 2-11% absolute. Our approach also matches or exceeds the state-of-the-art in every case, without using an expensive end-to-end training regime.', 'corpus_id': 207852944, 'score': 0}, {'doc_id': '98533190', 'title': 'Segmentation of the Australian Wine Market Using a Wine-Related Lifestyle Approach', 'abstract': 'As wine increasingly becomes a lifestyle beverage and more acceptable and desired by a wider spectrum of consumers, there is a greater need to understand wine consumer values, consumption patterns and profiles. This research recognises that lifestyle is inextricably linked to values and the processes by which people seek to achieve their values through various modes of expression, including the consumption of wine. For this purpose, this study developed a new wine-related lifestyle (WRL) measurement instrument for segmenting the Australian domestic wine market. Although the study was exploratory in nature, there is clear evidence that five wine-related consumer lifestyle segments exist in the Australian domestic wine market. These segments are: purposeful inconspicuous premium wine drinkers, ritual oriented conspicuous wine enthusiasts, enjoyment seeking social wine drinkers, fashion/image oriented wine drinkers, and basic wine drinkers.', 'corpus_id': 98533190, 'score': 0}, {'doc_id': '52815560', 'title': 'Spider: A Large-Scale Human-Labeled Dataset for Complex and Cross-Domain Semantic Parsing and Text-to-SQL Task', 'abstract': 'We present Spider, a large-scale complex and cross-domain semantic parsing and text-to-SQL dataset annotated by 11 college students. It consists of 10,181 questions and 5,693 unique complex SQL queries on 200 databases with multiple tables covering 138 different domains. We define a new complex and cross-domain semantic parsing and text-to-SQL task so that different complicated SQL queries and databases appear in train and test sets. In this way, the task requires the model to generalize well to both new SQL queries and new database schemas. Therefore, Spider is distinct from most of the previous semantic parsing tasks because they all use a single database and have the exact same program in the train set and the test set. We experiment with various state-of-the-art models and the best model achieves only 9.7% exact matching accuracy on a database split setting. This shows that Spider presents a strong challenge for future research. Our dataset and task with the most recent updates are publicly available at https://yale-lily.github.io/seq2sql/spider.', 'corpus_id': 52815560, 'score': 1}, {'doc_id': '207853114', 'title': 'CommonGen: A Constrained Text Generation Dataset Towards Generative Commonsense Reasoning', 'abstract': 'Rational humans can generate sentences that cover a certain set of concepts while describing natural and common scenes. For example, given {apple(noun), tree(noun), pick(verb)}, humans can easily come up with scenes like ""a boy is picking an apple from a tree"" via their generative commonsense reasoning ability. However, we find this capacity has not been well learned by machines. Most prior works in machine commonsense focus on discriminative reasoning tasks with a multi-choice question answering setting. Herein, we present CommonGen: a challenging dataset for testing generative commonsense reasoning with a constrained text generation task. We collect 37k concept-sets as inputs and 90k human-written sentences as associated outputs. Additionally, we also provide high-quality rationales behind the reasoning process for the development and test sets from the human annotators. We demonstrate the difficulty of the task by examining a wide range of sequence generation methods with both automatic metrics and human evaluation. The state-of-the-art pre-trained generation model, UniLM, is still far from human performance in this task. Our data and code is publicly available at this http URL .', 'corpus_id': 207853114, 'score': 0}, {'doc_id': '202565697', 'title': 'CoSQL: A Conversational Text-to-SQL Challenge Towards Cross-Domain Natural Language Interfaces to Databases', 'abstract': 'We present CoSQL, a corpus for building cross-domain, general-purpose database (DB) querying dialogue systems. It consists of 30k+ turns plus 10k+ annotated SQL queries, obtained from a Wizard-of-Oz (WOZ) collection of 3k dialogues querying 200 complex DBs spanning 138 domains. Each dialogue simulates a real-world DB query scenario with a crowd worker as a user exploring the DB and a SQL expert retrieving answers with SQL, clarifying ambiguous questions, or otherwise informing of unanswerable questions. When user questions are answerable by SQL, the expert describes the SQL and execution results to the user, hence maintaining a natural interaction flow. CoSQL introduces new challenges compared to existing task-oriented dialogue datasets: (1) the dialogue states are grounded in SQL, a domain-independent executable representation, instead of domain-specific slot value pairs, and (2) because testing is done on unseen databases, success requires generalizing to new domains. CoSQL includes three tasks: SQL-grounded dialogue state tracking, response generation from query results, and user dialogue act prediction. We evaluate a set of strong baselines for each task and show that CoSQL presents significant challenges for future research. The dataset, baselines, and leaderboard will be released at https://yale-lily.github.io/cosql.', 'corpus_id': 202565697, 'score': 1}, {'doc_id': '2439226', 'title': 'Cross-domain Semantic Parsing via Paraphrasing', 'abstract': 'Existing studies on semantic parsing mainly focus on the in-domain setting. We formulate cross-domain semantic parsing as a domain adaptation problem: train a semantic parser on some source domains and then adapt it to the target domain. Due to the diversity of logical forms in different domains, this problem presents unique and intriguing challenges. By converting logical forms into canonical utterances in natural language, we reduce semantic parsing to paraphrasing, and develop an attentive sequence-to-sequence paraphrase model that is general and flexible to adapt to different domains. We discover two problems, small micro variance and large macro variance, of pre-trained word embeddings that hinder their direct use in neural networks, and propose standardization techniques as a remedy. On the popular Overnight dataset, which contains eight domains, we show that both cross-domain training and standardized pre-trained word embeddings can bring significant improvement.', 'corpus_id': 2439226, 'score': 1}]"
143	Reading Software Manuals	f9bc8ee0a2681604d14bac5999bfb283	6892	{}	"[{'doc_id': '219259926', 'title': 'Exploring Context-Aware Conversational Agents in Software Development', 'abstract': ""Software development is a complex endeavor that depends on a wide variety of contextual factors involving a large amount of distributed information. This knowledge could include: technology-related tasks, software operating environments and stakeholder requirements. A major roadblock to using this knowledge in software development is that most of this information is implicit and captured in the developers' minds (tacit) or spread through volumes of documentation. Developers, as they work often have to maintain mental models of these tasks as they produce the software. As a result, context can be easily lost or forgotten and developers often use trial-and-error approaches while finishing the project. This study aims at analyzing whether supporting software developers with a chatbot during task execution can improve the overall development experience. The chatbot can assist the developers in executing different tasks based on implicit contextual information. We propose an implementation to explore the viability of using textual chatbots to assist developers automatically and proactively with software development project activities that recur."", 'corpus_id': 219259926, 'score': 1}, {'doc_id': '220364391', 'title': 'Automatically Generating Codes from Graphical Screenshots Based on Deep Autocoder', 'abstract': 'During software front-end development, the work to convert Graphical User Interface(GUI) image to the corresponding front-end code is an inevitable tedious work. There have been some attempts to make this work to be automatic. However, the GUI code generated by these models is not accurate due to the lack of attention mechanism guidance. To solve this problem, we propose PixCoder based on an artificially supervised attention mechanism. The approach is to train a neural network to predict the style sheets in the input GUI image and then output a vector. PixCoder generate the GUI code targeting specific platform according to the output vector. The experimental results have shown the accuracy of the GUI code generated by PixCoder is over 95%.', 'corpus_id': 220364391, 'score': 0}, {'doc_id': '218581602', 'title': 'BabyWalk: Going Farther in Vision-and-Language Navigation by Taking Baby Steps', 'abstract': 'Learning to follow instructions is of fundamental importance to autonomous agents for vision-and-language navigation (VLN). In this paper, we study how an agent can navigate long paths when learning from a corpus that consists of shorter ones. We show that existing state-of-the-art agents do not generalize well. To this end, we propose BabyWalk, a new VLN agent that is learned to navigate by decomposing long instructions into shorter ones (BabySteps) and completing them sequentially. A special design memory buffer is used by the agent to turn its past experiences into contexts for future steps. The learning process is composed of two phases. In the first phase, the agent uses imitation learning from demonstration to accomplish BabySteps. In the second phase, the agent uses curriculum-based reinforcement learning to maximize rewards on navigation tasks with increasingly longer instructions. We create two new benchmark datasets (of long navigation tasks) and use them in conjunction with existing ones to examine BabyWalk’s generalization ability. Empirical results show that BabyWalk achieves state-of-the-art results on several metrics, in particular, is able to follow long instructions better. The codes and the datasets are released on our project page: https://github.com/Sha-Lab/babywalk.', 'corpus_id': 218581602, 'score': 0}, {'doc_id': '49559663', 'title': ""Amanuensis: The Programmer's Apprentice"", 'abstract': ""This document provides an overview of the material covered in a course taught at Stanford in the spring quarter of 2018. The course draws upon insight from cognitive and systems neuroscience to implement hybrid connectionist and symbolic reasoning systems that leverage and extend the state of the art in machine learning by integrating human and machine intelligence. As a concrete example we focus on digital assistants that learn from continuous dialog with an expert software engineer while providing initial value as powerful analytical, computational and mathematical savants. Over time these savants learn cognitive strategies (domain-relevant problem solving skills) and develop intuitions (heuristics and the experience necessary for applying them) by learning from their expert associates. By doing so these savants elevate their innate analytical skills allowing them to partner on an equal footing as versatile collaborators - effectively serving as cognitive extensions and digital prostheses, thereby amplifying and emulating their human partner's conceptually-flexible thinking patterns and enabling improved access to and control over powerful computing resources."", 'corpus_id': 49559663, 'score': 1}, {'doc_id': '218684707', 'title': 'Human Instruction-Following with Deep Reinforcement Learning via Transfer-Learning from Text', 'abstract': 'Recent work has described neural-network-based agents that are trained with reinforcement learning (RL) to execute language-like commands in simulated worlds, as a step towards an intelligent agent or robot that can be instructed by human users. However, the optimisation of multi-goal motor policies via deep RL from scratch requires many episodes of experience. Consequently, instruction-following with deep RL typically involves language generated from templates (by an environment simulator), which does not reflect the varied or ambiguous expressions of real users. Here, we propose a conceptually simple method for training instruction-following agents with deep RL that are robust to natural human instructions. By applying our method with a state-of-the-art pre-trained text-based language model (BERT), on tasks requiring agents to identify and position everyday objects relative to other objects in a naturalistic 3D simulated room, we demonstrate substantially-above-chance zero-shot transfer from synthetic template commands to natural instructions given by humans. Our approach is a general recipe for training any deep RL-based system to interface with human users, and bridges the gap between two research directions of notable recent success: agent-centric motor behavior and text-based representation learning.', 'corpus_id': 218684707, 'score': 0}, {'doc_id': '219177119', 'title': 'Translating Natural Language Instructions for Behavioral Robot Navigation with a Multi-Head Attention Mechanism', 'abstract': 'We propose a multi-head attention mechanism as a blending layer in a neural network model that translates natural language to a high level behavioral language for indoor robot navigation. We follow the framework established by (Zang et al., 2018a) that proposes the use of a navigation graph as a knowledge base for the task. Our results show significant performance gains when translating instructions on previously unseen environments, therefore, improving the generalization capabilities of the model.', 'corpus_id': 219177119, 'score': 0}, {'doc_id': '29445018', 'title': 'APIBot: Question answering bot for API documentation', 'abstract': 'As the carrier of Application Programming Interfaces (APIs) knowledge, API documentation plays a crucial role in how developers learn and use an API. It is also a valuable information resource for answering API-related questions, especially when developers cannot find reliable answers to their questions online/offline. However, finding answers to API-related questions from API documentation might not be easy because one may have to manually go through multiple pages before reaching the relevant page, and then read and understand the information inside the relevant page to figure out the answers. To deal with this challenge, we develop APIBot, a bot that can answer API questions given API documentation as an input. APIBot is built on top of SiriusQA, the QA system from Sirius, a state of the art intelligent personal assistant. To make SiriusQA work well under software engineering scenario, we make several modifications over SiriusQA by injecting domain specific knowledge. We evaluate APIBot on 92 API questions, answers of which are known to be present in Java 8 documentation. Our experiment shows that APIBot can achieve a Hit@5 score of 0.706.', 'corpus_id': 29445018, 'score': 1}, {'doc_id': '219956747', 'title': 'Neural Program Synthesis with a Differentiable Fixer', 'abstract': ""We present a new program synthesis approach that combines an encoder-decoder based synthesis architecture with a differentiable program fixer. Our approach is inspired from the fact that human developers seldom get their program correct on the first attempt, and perform iterative testing-based program fixing to get to the desired program functionality. Similarly, our approach first learns a distribution over programs conditioned on an encoding of a set of input-output examples, and then iteratively performs fix operations using the differentiable fixer. The fixer takes as input the original examples and the current program's outputs on example inputs, and generates a new distribution over the programs with the goal of reducing the discrepancies between the current program outputs and the desired example outputs. We train our architecture end-to-end on the RobustFill domain, and show that the addition of the fixer module leads to a significant improvement on synthesis accuracy compared to using beam search."", 'corpus_id': 219956747, 'score': 0}, {'doc_id': '1174836', 'title': 'Reading to Learn: Constructing Features from Semantic Abstracts', 'abstract': 'Machine learning offers a range of tools for training systems from data, but these methods are only as good as the underlying representation. This paper proposes to acquire representations for machine learning by reading text written to accommodate human learning. We propose a novel form of semantic analysis called reading to learn, where the goal is to obtain a high-level semantic abstract of multiple documents in a representation that facilitates learning. We obtain this abstract through a generative model that requires no labeled data, instead leveraging repetition across multiple documents. The semantic abstract is converted into a transformed feature space for learning, resulting in improved generalization on a relational learning task.', 'corpus_id': 1174836, 'score': 1}, {'doc_id': '7415169', 'title': 'An architecture for intelligent assistance in software development', 'abstract': 'We define an architecture for a software engineering environment that behaves as an intelligent assistant. Our architecture consists of three key aspects: an objectbase, a model of the software development activities, and controlled automation. Our objectbase is adapted from other research, but our model is unique in that is consists primarily of rules that define the preconditions and multiple postconditions of software development activities. Our most significant contribution is opportunistic processing, whereby the environment performs software development activities through controlled automation. This is accomplished by a forward and backward chaining interpretation of the rule set. Activities are automatically carried out at some time between when their preconditions are satisfied and when their postconditions are required. Automation is controlled through strategies that guide the assistant in choosing an appropriate point for carrying out each activity.', 'corpus_id': 7415169, 'score': 1}]"
144	Data activism	58e6dad9d2778ea91a0eb982388ee3bd	20864	{}	"[{'doc_id': '237343236', 'title': 'The Ethics of Using Digital Trace Data in Education: A Thematic Review of the Research Landscape', 'abstract': 'This article presents the findings of a systematic qualitative analysis of research in the ethics of digital trace data use in learning and education. From the resulting analysis of 77 peer-reviewed studies, we (1) map the characteristics of research by study type, academic community, institutional setting, and national context; (2) identify the primary ethical concerns and related responses; and (3) highlight the research gaps. Four areas of focus are identified in this emerging area: (1) privacy, informed consent, and data ownership; (2) validity and integrity; (3) ethical decision making; and (4) governance and accountability. We highlight the lack of evidence particularly for preschool and school-aged children and the disparate communities working in this domain, and we suggest a more cohesive approach, where the wider learning and educational ecosystem is recognized, explicit engagement with ethical theory is central, and mid- to long-term ethical issues are considered alongside immediate concerns.', 'corpus_id': 237343236, 'score': 0}, {'doc_id': '237951607', 'title': 'Soldering Toward Media Democracy: Technical Practice as Symbolic Value in Radio Activism', 'abstract': 'This article follows radio activists engaged in a combination of policy advocacy and broadening access to technology and skills through hands-on work. In practice, this largely played out as a systematic elevation of “technical” work and downplaying of policy/advocacy expertise, even though both were salient features of their work. The article argues that radio activists cultivated a technical identity that served to mark boundaries between their group and others in the terrain of media democracy work. Technical identity also took on special significance as the group grappled with organizational maturation, mitigating the anxiety felt by workers as they experienced the shift from an inexperienced, though highly driven and successful activist collective, to a more sustainable nonprofit activist organization. The article concludes by naming technological activism as one strategy in the wider spectrum of work to promote media democracy and speculates on the consequences of technical identity within the wider movement.', 'corpus_id': 237951607, 'score': 0}, {'doc_id': '237334743', 'title': 'Social News Use & Citizen Participation among Young Activists in Singapore', 'abstract': 'This article presents a study of how civically engaged young adults engage with news on social media, within the context of a developing democracy – Singapore. Based on in-depth interviews with 20 young activists, it discusses how they approach social media as a source of news, what motivates them to engage in more than one social news platform, and how social news use fits into their political lexicon. The results reveal that despite their affinity towards news-related content on social media, they are neither partial towards mainstream, nor alternative news providers on this medium. Their primary social news platform is perceived to offer the best means to disseminate news-related information. However, they are also concerned about their privacy and practice certain strategies to mitigate this. Despite its drawbacks, the activists accept social news use as a viable means of political socialisation and mobilisation.', 'corpus_id': 237334743, 'score': 0}, {'doc_id': '237537721', 'title': 'The Environmental Justice Movement as a Model Politics of Risk', 'abstract': 'Risk carries unique significance for democratic politics today as it faces the challenges of rising inequality, neoliberalism, and systemic racism. To show how, the article divides “risk” into two complementary political models: a technocratic logic of risk allocation, concerned primarily with safety, and a forensic logic of risk attribution, concerned with holding risk takers to account. Both have had pervasive effects on a transformed welfare state, increasingly focused on “personal responsibility” and privatized risk-management. But risk has also played a key role in the way post-1968 movements have organized and challenged the logic of privatization. Risk-based movements, the article argues, especially from the political margins, are key agents in promoting a new political form founded on risk attribution. The article focuses on the exemplary case of the American environmental justice movement in the 1980s–90s as it reframed social justice around three core demands: accountability from decision makers, equitable risk distribution, and broad participation in decisions about danger and communities’ well-being.', 'corpus_id': 237537721, 'score': 0}, {'doc_id': '55124003', 'title': 'Living with Data: Aligning Data Studies and Data Activism Through a Focus on Everyday Experiences of Datafication', 'abstract': 'It is now widely accepted that data are oiling the twenty-first century (Toonders 2014). Data gathering and tracking are practically universal, and datafication (the quantification of aspects of life previously experienced in qualitative, non-numeric form, such as communication, relationships, health and fitness, transport and mobility, democratic participation, leisure and consumption) is a transformation disrupting the social world in all its forms (Couldry 2016). Statistics confirm the assertion that the datafication of almost everything is growing relentlessly: in 2012 it was claimed that 90% of the world’s data had been created in the previous two years (IBM 2012), and a future 40% annual rise in data generation has been estimated (Manyika et al. 2011). \n \nLess commonly noted is the place of everyday experience in the machine of datafication. The Berliner Gazette (nd) has claimed that 75% of these newly available data are by-products of people’s everyday activities, and Michael and Lupton also note the centrality of the everyday in the current Big Data moment: \n \nHuman actors contribute to big datasets when they engage in activities such as making calls and using apps on mobile phones, using online search engines such as Google, purchasing goods or services online or taking part in customer loyalty programmes, uploading contributions to social media platforms, using wearable self-tracking devices or moving around in spaces that are equipped with digital sensing or recording devices (Michael and Lupton 2015, 104). \n \nDespite the significance of such everyday practices in the production of large-scale data, little attention has been paid to people’s thoughts and feelings about these data-producing processes. These issues have not, on the whole, been the focus of the emerging field of data studies, which seeks to understand the new roles played by data in times of datafication. This is a problem for a number of reasons. First, if we do not understand whether data condition everyday experiences as it is claimed, and our thinking about these matters is not informed by the perspectives of the people upon whose data datafication is built, scholarship about data-in-society will be incomplete. Second, and importantly for this special issue, in the absence of such knowledge, data activism, which seeks to challenge existing data power relations and to mobilise data in order to enhance social justice, will rely upon the judgments of elite technical actors and activists about what would constitute more just data practices. In contrast, I argue that to build a picture of what just data arrangements (that is, the practices of organisations that handle and produce data, the policies that govern these practices, and provisions for the development of skills that people need in order to engage with data) might look like, it is important to take account of what non-expert citizens themselves say would enable them to live better with data, based on their everyday experiences of datafication. Greater understanding of everyday living with data can contribute significantly to the knowledge base on which data activism is built. A third problem, then, is that by not focusing on these issues, the field of data studies is not currently as well aligned to the aims of data activism as it might be. This paper explores how we might address this gap.', 'corpus_id': 55124003, 'score': 1}, {'doc_id': '85463482', 'title': 'Data infrastructure literacy', 'abstract': 'A recent report from the UN makes the case for “global data literacy” in order to realise the opportunities afforded by the “data revolution”. Here and in many other contexts, data literacy is characterised in terms of a combination of numerical, statistical and technical capacities. In this article, we argue for an expansion of the concept to include not just competencies in reading and working with datasets but also the ability to account for, intervene around and participate in the wider socio-technical infrastructures through which data is created, stored and analysed – which we call “data infrastructure literacy”. We illustrate this notion with examples of “inventive data practice” from previous and ongoing research on open data, online platforms, data journalism and data activism. Drawing on these perspectives, we argue that data literacy initiatives might cultivate sensibilities not only for data science but also for data sociology, data politics as well as wider public engagement with digital data infrastructures. The proposed notion of data infrastructure literacy is intended to make space for collective inquiry, experimentation, imagination and intervention around data in educational programmes and beyond, including how data infrastructures can be challenged, contested, reshaped and repurposed to align with interests and publics other than those originally intended.', 'corpus_id': 85463482, 'score': 1}, {'doc_id': '157153676', 'title': 'Data Activism as the New Frontier of Media Activism', 'abstract': 'With the diffusion of big data, citizens become increasingly aware of the critical role of information in contemporary societies. This awareness nurtures new social practices rooted in technology and data, which I term ‘data activism’. Data activism addresses massive data collection as both a challenge to individual rights and a novel set of opportunities for social change. It represents the new frontier of media activism, as it appropriates technological innovation for political purposes. It emerges from pre-existing sociotechnical networks, such as the hacker and the open-source movements, but overcomes their elitist character to involve ordinary users, thus signaling a change in perspective towards massive data collection emerging within civil society. This chapter offers a theoretical and empirical approach to investigate data activism. It places data activism in relation to the global social movement ecology, and examines the evolution of the “media activist” figure and role in relation to technological innovation.', 'corpus_id': 157153676, 'score': 1}, {'doc_id': '237951735', 'title': 'The Politics of Civil Society Forms: Urban Environmental Activists and Democracy in Jakarta', 'abstract': ""Despite the ongoing debate regarding how and to what extent civil society enhances democratic practices, it is generally agreed that there is a reasonable link between civil society and democracy under certain conditions. This paper aims to explore the politics of civil society forms and understand their contribution to the maintenance of democratic practices in Jakarta. Building on a neo-Tocquevillian understanding of civil society, this article analyses urban environmental activists' strategic adoption of voluntary associations and environmental spin-off campaigns as forms of civic engagement to improve public policy. This paper asks how and to what extent these forms of civic engagement provide alternative understandings of civil society's efforts to promote local democracy. We argue that urban environmental activists' spin-off campaigns and voluntary associations represent a particular form of civil society politics, and thus provide different routes to understand local democracy by facilitating diagonal accountability mechanisms. However, further analysis found that the forms adopted by urban environmental activists suffer horizontal and vertical accountability problems similar to those frequently found in more established forms of civil society (e.g. non-government organisations). Nonetheless, the discussion in this paper illustrates civil society's ingenuity in pushing for democratic practices amidst Indonesia's 'democratic recession'."", 'corpus_id': 237951735, 'score': 0}, {'doc_id': '150478992', 'title': 'Data activism and social change', 'abstract': 'obtain skills and jobs), they are fundamentally different in structure and mission. The authors imply that analyzing these differentiations is key to understanding the complexities and nuances ofmodels of SIM programing, which can help to duplicate successes in other contexts. Chapter Five’s description of the creative non-profit workforce sets up the reader for Chapter Seven, where the authors describe the motivations for adult workers in non-profit youth programing like those described in Chapters Six and Eight. Again, the authors take an interdisciplinary approach to the topic by engaging theories from sociology and philosophy to read their ethnographic results from the field, leading them to determine that creative nonprofit workers do their work with reputation and reciprocal behavior in mind. They also acknowledge the role that social media play in promoting recognition for good deeds, increasing the visibility and, therein, capacity for these motivators. The book concludes with a critical discussion on “the intense preoccupation with appraisal (prospective assessment) and evaluation (retrospective assessment) in the SIM field” (p. 108), where the expectation for funding in today’s economy requires assurance in some way of a return on investment. This is a complicated task for programs with the aim of “social benefit,” and the authors recognize and critique the culture of assessment that non-profit work must cater to, such as quantifying successes and then aiming primarily for those benchmarks. That said, they provide an extensive review of methodology and conclude that participatory ethnographic evaluation, although resource-demanding, is the optimal approach to evaluating programs like those presented in this book. Using media successfully bridges multiple areas of literature to comprise an insightful applied survey to SIM. Not only do the authors provide socio-economic context alongside thick descriptions of successes in the field of Social Innovation Media, the authors skillfully integrate both contemporary and canonical social theory throughout each chapter to underscore the relevance of these applied practices to an academic audience. That said, because the text jumps from case studies to empirical data, the flow of the book suggests it is best suited as an encyclopedic resource for creative developers in the non-profit world, with a secondary audience being students of strategic or non-profit communication who should be assigned specific chapters as needed.', 'corpus_id': 150478992, 'score': 1}, {'doc_id': '40865524', 'title': 'Civic hacking as data activism and advocacy: A history from publicity to open government data', 'abstract': 'The civic hacker tends to be described as anachronistic, an ineffective “white hat” compared to more overtly activist cousins. By contrast, I argue that civic hackers’ politics emerged from a distinct historical milieu and include potentially powerful modes of political participation. The progressive roots of civic data hacking can be found in early 20th-century notions of “publicity” and the right to information movement. Successive waves of activists saw the Internet as a tool for transparency. The framing of openness shifted in meaning from information to data, weakening of mechanisms for accountability even as it opened up new forms of political participation. Drawing on a year of interviews and participant observation, I suggest civic data hacking can be framed as a form of data activism and advocacy: requesting, digesting, contributing to, modeling, and contesting data. I conclude civic hackers are utopian realists involved in the crafting of algorithmic power and discussing ethics of technology design. They may be misunderstood because open data remediates previous forms of openness. In the process, civic hackers transgress established boundaries of political participation.', 'corpus_id': 40865524, 'score': 1}]"
145	Mesolitic-Neolithic Transition	d9faa2a2328ae1dd9cd5b620ff94800e	5026	{}	"[{'doc_id': '128026031', 'title': 'Mesolithic/Neolithic Interactions in the Balkans and in the Middle Danube Basin', 'abstract': '9 papers from the session on Mesolithic/Neolithic Interactions in the Balkans and in the Middle Danube Basin held at the 15th UISPP Congress in Lisbon in September 2006. Contents: 1) Mesolithic/Neolithic interactions in the Balkan Peninsula and the Carpathian Basin: an introduction (Marek Nowak); 2) The chipped stone assemblages of Mentese and the problem of the earliest occupation of Marmara region (Ivan Gatsov, Petranka Nedelcheva); 3) Late Mesolithic of Serbia and Montenegro (Duan Mihailoviae); 4) Mesolithic-Neolithic interactions in the Danube Gorges (Duan Boric); 5) Palaeogeographical background of the Mesolithic and Early Neolithic settlement in the Carpathian Basin (Pal Suemegi); 6) Mesolithic foragers and the spread of agriculture in Western Hungary by (Eszter Banffy, William J. Eichmann, Tibor Marton); 7) Early Neolithic raw material economies in the Carpathian Basin (Katalin T. Biro); 8) Neolithisation of the upper Tisza basin (Janusz K. Kozlowski, Marek Nowak); 9) Problems in reading MesolithicNeolithic relations in South-Eastern Europe (Janusz K. Kozlowski, Marek Nowak).""', 'corpus_id': 128026031, 'score': 1}, {'doc_id': '218539287', 'title': 'Biodiversity Research and Innovation in Antarctica and the Southern Ocean', 'abstract': 'This article examines biodiversity research and innovation in Antarctica and the Southern Ocean based on a review of 150,401 scientific articles and 29,690 patent families for Antarctic species. The paper exploits the growing availability of open access databases, such as the Lens and Microsoft Academic Graph, along with taxonomic data from the Global Biodiversity Information Facility (GBIF) to explore the scientific and patent literature for the Antarctic at scale. The paper identifies the main contours of scientific research in Antarctica before exploring commercially oriented biodiversity research and development in the scientific literature and patent publications. The paper argues that biodiversity is not a free good and must be paid for. Ways forward in debates on commercial research and development in Antarctica can be found through increasing attention to the valuation of ecosystem services, new approaches to natural capital accounting and payment for ecosystem services that would bring the Antarctic, and the Antarctic Treaty System, into the wider fold of work on the economics of biodiversity. Economics based approaches can be criticised for reducing biodiversity to monetary exchange values at the expense of recognition of the wider values of biodiversity and its services. However, approaches grounded in the economics of biodiversity provide a transparent framework for approaching commercial activity in the Antarctic and introducing requirements for investments in the conservation of Antarctic biodiversity by those who seek to profit from it.', 'corpus_id': 218539287, 'score': 0}, {'doc_id': '218944570', 'title': 'Reset redux: possible evolutionary pathways towards the transformation of tourism in a COVID-19 world', 'abstract': 'Abstract With international arrivals surpassing 1.5 billion for the first time in 2019 the long-term evolution of tourism demonstrates prolific path dependence with a decade of growth since the global financial crisis. This latest period of unfettered international tourism development has come to an abrupt end as the impact of COVID-19 has brought the sector to a near standstill. As the world grapples with the realities of the global pandemic there is an opportunity to rethink exactly what tourism will look like for the decades ahead. Key concepts in evolutionary economic geography, especially path dependence/creation and institutional inertia/innovation, show variations in pathways for travel and tourism in a COVID-19 world. A path that leads to transformation in tourism can be realized if sufficient institutional innovation occurs on both the demand and supply side of tourism that can foster the emergence of new paths. COVID-19 presents a once in a generation opportunity where the institutional pump is primed for transformation. Whether that leads to a radical transformation of the tourism sector remains to be seen, but the imprint it will leave on both the demand and supply of tourism will have long-term, incremental impacts for years to come and ultimately move us closer towards the transformation of tourism.', 'corpus_id': 218944570, 'score': 0}, {'doc_id': '218727927', 'title': 'Effect of photoperiod and plant growth regulators on in vitro mass bulblet proliferation of Narcissus tazzeta L. (Amaryllidaceae), a potential source of galantamine', 'abstract': 'Narcissus tazetta L., a bulbous plant belongs to the Amaryllidaceae family, contains alkaloid galantamine (GAL) with acetylcholinesterase inhibitory activity which has been recently considered to treat Alzheimer’s disease (AD). In the current work, the effect of photoperiod (16/8 h light/dark and 24 h dark) and various concentrations of NAA, BAP, and GA3 (0, 0.5, 1 and 2 mg l‒1) on the in vitro mass bulblet regeneration of N. tazetta was studied. The GAL production ability of the regenerated bulblets was assessed by HPLC-UV-MS. Light treatments significantly affected the number of bulblet and leaf, the ratio of bulblet/leaf, and leaf length. The maximum number of bulblet (31.0\u2009±\u20091.58) and leaf (13.3\u2009±\u20091.33) was recorded from the cultures fortified with NAA and BAP (2 mg l‒1) kept in 16/8 h light/dark, while the maximum leaf length (2.1\u2009±\u20090.92 cm) was measured on the MS medium containing 0.5 mg l‒1 NAA and 2 mg l‒1 BAP incubated in the same photoperiod. The average ratio of bulblet proliferation per explant was significantly different between studied photoperiod (1.1\u2009±\u20090.86) and 24 h dark (0.62\u2009±\u20090.31). The regenerated bulblets contained 40 and 20 µg g‒1 DW GAL underexposed photoperiod and 24 h dark, respectively. This information could be useful in the commercial production of GAL as a valuable anti-AD compound through in vitro mass bulblet proliferation of N. tazetta. The regenerated mass bulblets of Narcissus tazetta (Amaryllidaceae) on MS medium containing 2 mg l‒1 NAA and BAP kept in 16/8 h light/dark are recommended to produce galanatamine and lycorine.', 'corpus_id': 218727927, 'score': 0}, {'doc_id': '56087251', 'title': 'Early Neolithic pottery dispersals and demic diffusion in southeastern Europe', 'abstract': 'The 14C gradient of pottery dispersal suggests that the sites in the southern Balkans are not significantly older than those in the northern and eastern Balkans. A gradual demic diffusion model from south to north and a millennium time span vector thus find no confirmation in the set of AMS 14C dates and associated contexts that mark pottery dispersal within Southeastern Europe. The first \'demic event\' that was hypothesised to reshape significantly European population structure and generate a uniform process of neolithisation of southestern Europe has no confirmation in fre- quency of Y-chromosome subhaplogroups J2b and E3b1 distribution within modern population in Southeastern Europe. IZVLE! EK - 14C datumi prve keramike ka! ejo, da zgodnje neolitska najdi""# a na jugu Balkana niso starej"" a od onih na severu. AMS 14C datumi ne potrjujejo modela postopne demske difuzije od juga proti severu in tiso# letni # asovni zamik pri "" iritvi keramike. Prvi \'demski dogodek\', ki naj bi domnev- no preoblikoval evropsko populacijsko sestavo, v jugovzhodni Evropi pa povzro# il proces enovite neo- litizacije, ni potrjen s pogostostjo pojavljanja Y-kromosomskih haploskupin J2b in E3b1 pri sedanjih populacijah v jugovzhodni Evropi.', 'corpus_id': 56087251, 'score': 1}, {'doc_id': '214623307', 'title': 'Widening of the Andes: An interplay between subduction dynamics and crustal wedge tectonics', 'abstract': 'Shortening of the continental lithosphere is generally accommodated by the growth of crustal wedges building above megathrusts in the mantle lithosphere. We show that the locus of shortening in the western margin of South America has largely been controlled by the geometry of the slab. Numerical models confirm that horizontal subduction favors compression far from the trench, above the asthenospheric wedge and steeply dipping segment of the subducting slab. As a result, a second crustal wedge grows in the hinterland of the continent, and widens the Andes. In the Bolivian orocline, this wedge corresponds to the Eastern Cordillera, whose growth was triggered by a major episode of horizontal subduction. When the slab returned to a steeper dip angle, shortening and uplift pursued, facilitated by the structural and thermo-chemical alteration of the continental lithosphere. We review the successive episodes of horizontal subduction that have occurred beneath South America at different latitudes and show that they explain the diachronic widening of the Andes. We infer that the present-day segmented physiography of the Andes results from the latitudinally variable, transient interplay between slab dynamics and upper plate tectonics over the Cenozoic. We emphasize that slab flattening, or absence thereof, is a major driving mechanism that sets the width of the Andes, at any latitude.', 'corpus_id': 214623307, 'score': 0}, {'doc_id': '30379036', 'title': 'A systematic review of wild grass exploitation in relation to emerging cereal cultivation throughout the Epipalaeolithic and aceramic Neolithic of the Fertile Crescent', 'abstract': 'The present study investigates the occurrence of wild grasses at Epipalaeolithic and aceramic Neolithic sites in the Near East in order to assess their role in subsistence economies alongside the emergence of cereal cultivation. We use Chogha Golan in the foothills of the central Zagros Mountains (ca. 11.7–9.6 ka cal. BP) as a case study, where the archaeobotanical data suggest the frequent exploitation of a complex of wild grasses for almost 2,000 years. Domesticated emmer replaced these wild grasses as the major food resources towards the end of occupation at the site (ca. 9.8 ka cal. BP). We discuss possible implications of this development and conclude that the traditional concept of pre-domestication cultivation seems unsuited for explaining the patterns from Chogha Golan. These data are in good accordance with the overall picture in the Zagros Mountains, where wild grasses were routinely gathered throughout the early Holocene. In contrast, wild grasses were gradually replaced by wild cereals in the Levantine corridor since the end of the Pleistocene. However, several sites located in this region provide evidence for a continuous exploitation of wild grasses alongside emerging cereal cultivation and most of these taxa were part of the earliest segetal floras that evolved with the appearance of domestic cereals throughout the 11th millennium cal. BP. Some sites contemporary to the Pre-Pottery Neolithic B still provide evidence for the usage of wild grasses, which possibly reflects the utilization of edible arable weeds and continuous gathering of wild grasses by more mobile groups.', 'corpus_id': 30379036, 'score': 1}, {'doc_id': '218642831', 'title': 'Correction to: Converting Home Spaces into Food Gardens at the Time of Covid-19 Quarantine: all the Benefits of Plants in this Difficult and Unprecedented Period', 'abstract': 'The number of deaths due to Covid-19 in Italy was incorrectly written as “15,35”. It should be corrected to ""15,915"". The original article has been corrected.', 'corpus_id': 218642831, 'score': 0}, {'doc_id': '215786184', 'title': 'Refining the Uluzzian through a new lithic assemblage from Roccia San Sebastiano (Mondragone, southern Italy)', 'abstract': 'Abstract Roccia San Sebastiano is a tectonic-karstic cave located at the foot of the southern slope of Mt. Massico, in the territory of Mondragone (Caserta) in Campania (southern Italy). Systematic excavation has been carried out since 2001, leading to the partial exploration of an important Pleistocene deposit, extraordinarily rich in lithic and faunal remains. The aim of this paper is to (1) present the stratigraphic sequence of Roccia San Sebastiano, and (2) technologically describe the lithic materials of squares F14 t18, t19, t20; E16 t16, t17, t18 recently recognised as Uluzzian. The stratigraphic sequence is more than 3\xa0m thick and dates from the Middle to the Upper Palaeolithic. It contains different techno-complexes: Gravettian, Aurignacian, Uluzzian and Mousterian. In the Uluzzian lithic assemblage mostly local pebbles of chert were used in order to produce small-sized objects. The concept of debitage mainly deals with unidirectional debitage with absent or fairly accurate management of the convexities and angles; the striking platforms are usually natural or made by one stroke. It is attested the use of both direct freehand percussion and bipolar technique on anvil in the same reduction sequence. Amongst the retouched tools the presence of two lunates is of note. This study of the Roccia San Sebastiano Uluzzian lithic complexes is significant for understanding the dynamics of the transition from Middle to Upper Palaeolithic in the Tyrrhenian margin of southern Italy.', 'corpus_id': 215786184, 'score': 1}, {'doc_id': '134166255', 'title': 'Before the neolithization: Causes of mesolithic diversity in the Southern Balkans', 'abstract': 'The Balkans, particularly southern and central, were sparsely populated in the Mesolithic and the occupation networks in that period were discontinous and highly diversified, contrasting with the density and homogeneity of the Early Neolithic. The aim of this paper is to describe the environmental conditions of the Mesolithic sites in relation to Early Holocene climatic fluctuations and to discuss the causes of specificity and diversity of culture and behaviour at this period. \nSome general trends are observable in the adaptation to Early Holocene environments (trends in faunal exploitation; for ex. shift from high ranked large game to low ranked small animals) but also particular adaptations to local conditions (technological changes due to difficulties in access to better quality lithic raw materials, adaptations to coastal or to terrestrial resources reflecting the unique features of site use, etc). \nThe diversity of the Mesolithic is also reflected in cultural taxonomy: in some sequences continuity of the Balkan Epigravettian techno-morphological tradition can be seen as opposed, in other sequences, to highly isolated groups with technology and tool morphology adapted to local raw materials and specific activities. The Balkan Mesolithic was not completely cut-off from the Western Mediterranean techno-morphological influences (particularly in Southern Greece) and from the Anatolian lithic traditions (seen only in the Northern Aegean). A more intensive network of marine contacts is confirmed by obsidian circulation in the Aegean Basin.', 'corpus_id': 134166255, 'score': 1}]"
146	Neural-symbolic	426a81c72403211df1f97546b26a4d1d	5226	{}	"[{'doc_id': '215745291', 'title': 'Toward Subgraph Guided Knowledge Graph Question Generation with Graph Neural Networks', 'abstract': 'Knowledge graph question generation (QG) aims to generate natural language questions from KG and target answers. Most previous works mainly focusing on the simple setting are to generate questions from a single KG triple. In this work, we focus on a more realistic setting, where we aim to generate questions from a KG subgraph and target answers. In addition, most of previous works built on either RNN-based or Transformer-based models to encode a KG sugraph, which totally discard the explicit structure information contained in a KG subgraph. To address this issue, we propose to apply a bidirectional Graph2Seq model to encode the KG subgraph. In addition, we enhance our RNN decoder with node-level copying mechanism to allow directly copying node attributes from the input graph to the output question. We also explore different ways of initializing node/edge embeddings and handling multi-relational graphs. Our model is end-to-end trainable and achieves new state-of-the-art scores, outperforming existing methods by a significant margin on the two benchmarks.', 'corpus_id': 215745291, 'score': 0}, {'doc_id': '216080851', 'title': 'Syntactic Structure from Deep Learning', 'abstract': 'Modern deep neural networks achieve impressive performance in engineering applications that require extensive linguistic skills, such as machine translation. This success has sparked interest in probing whether these models are inducing human-like grammatical knowledge from the raw data they are exposed to, and, consequently, whether they can shed new light on long-standing debates concerning the innate structure necessary for language acquisition. In this article, we survey representative studies of the syntactic abilities of deep networks, and discuss the broader implications that this work has for theoretical linguistics.', 'corpus_id': 216080851, 'score': 0}, {'doc_id': '5276660', 'title': 'Neural Module Networks', 'abstract': 'Visual question answering is fundamentally compositional in nature-a question like where is the dog? shares substructure with questions like what color is the dog? and where is the cat? This paper seeks to simultaneously exploit the representational capacity of deep networks and the compositional linguistic structure of questions. We describe a procedure for constructing and learning neural module networks, which compose collections of jointly-trained neural ""modules"" into deep networks for question answering. Our approach decomposes questions into their linguistic substructures, and uses these structures to dynamically instantiate modular networks (with reusable components for recognizing dogs, classifying colors, etc.). The resulting compound networks are jointly trained. We evaluate our approach on two challenging datasets for visual question answering, achieving state-of-the-art results on both the VQA natural image dataset and a new dataset of complex questions about abstract shapes.', 'corpus_id': 5276660, 'score': 1}, {'doc_id': '195346132', 'title': 'Tensor Product Generation Networks', 'abstract': 'We present a new tensor product generation network (TPGN) that generates natural language descriptions for images. The model has a novel architecture that instantiates a general framework for encoding and processing symbolic structure through neural network computation. This framework is built on Tensor Product Representations (TPRs). We evaluated the proposed TPGN on the MS COCO image captioning task. The experimental results show that the TPGN outperforms the LSTM based state-of-the-art baseline with a significant margin. Further, we show that our caption generation model can be interpreted as generating sequences of grammatical categories and retrieving words by their categories from a plan encoded as a distributed representation.', 'corpus_id': 195346132, 'score': 1}, {'doc_id': '215548225', 'title': 'Injecting Numerical Reasoning Skills into Language Models', 'abstract': 'Large pre-trained language models (LMs) are known to encode substantial amounts of linguistic information. However, high-level reasoning skills, such as numerical reasoning, are difficult to learn from a language-modeling objective only. Consequently, existing models for numerical reasoning have used specialized architectures with limited flexibility. In this work, we show that numerical reasoning is amenable to automatic data generation, and thus one can inject this skill into pre-trained LMs, by generating large amounts of data, and training in a multi-task setup. We show that pre-training our model, GenBERT, on this data, dramatically improves performance on DROP (49.3 –> 72.3 F1), reaching performance that matches state-of-the-art models of comparable size, while using a simple and general-purpose encoder-decoder architecture. Moreover, GenBERT generalizes well to math word problem datasets, while maintaining high performance on standard RC tasks. Our approach provides a general recipe for injecting skills into large pre-trained LMs, whenever the skill is amenable to automatic data augmentation.', 'corpus_id': 215548225, 'score': 1}, {'doc_id': '216553210', 'title': 'Semantic Graphs for Generating Deep Questions', 'abstract': 'This paper proposes the problem of Deep Question Generation (DQG), which aims to generate complex questions that require reasoning over multiple pieces of information about the input passage. In order to capture the global structure of the document and facilitate reasoning, we propose a novel framework that first constructs a semantic-level graph for the input document and then encodes the semantic graph by introducing an attention-based GGNN (Att-GGNN). Afterward, we fuse the document-level and graph-level representations to perform joint training of content selection and question decoding. On the HotpotQA deep-question centric dataset, our model greatly improves performance over questions requiring reasoning over multiple facts, leading to state-of-the-art performance. The code is publicly available at https://github.com/WING-NUS/SG-Deep-Question-Generation.', 'corpus_id': 216553210, 'score': 0}, {'doc_id': '218487314', 'title': 'Visual Question Answering with Prior Class Semantics', 'abstract': ""We present a novel mechanism to embed prior knowledge in a model for visual question answering. The open-set nature of the task is at odds with the ubiquitous approach of training of a fixed classifier. We show how to exploit additional information pertaining to the semantics of candidate answers. We extend the answer prediction process with a regression objective in a semantic space, in which we project candidate answers using prior knowledge derived from word embeddings. We perform an extensive study of learned representations with the GQA dataset, revealing that important semantic information is captured in the relations between embeddings in the answer space. Our method brings improvements in consistency and accuracy over a range of question types. Experiments with novel answers, unseen during training, indicate the method's potential for open-set prediction."", 'corpus_id': 218487314, 'score': 0}, {'doc_id': '214612170', 'title': 'Linguistically Driven Graph Capsule Network for Visual Question Reasoning', 'abstract': 'Recently, studies of visual question answering have explored various architectures of end-to-end networks and achieved promising results on both natural and synthetic datasets, which require explicitly compositional reasoning. However, it has been argued that these black-box approaches lack interpretability of results, and thus cannot perform well on generalization tasks due to overfitting the dataset bias. In this work, we aim to combine the benefits of both sides and overcome their limitations to achieve an end-to-end interpretable structural reasoning for general images without the requirement of layout annotations. Inspired by the property of a capsule network that can carve a tree structure inside a regular convolutional neural network (CNN), we propose a hierarchical compositional reasoning model called the ""Linguistically driven Graph Capsule Network"", where the compositional process is guided by the linguistic parse tree. Specifically, we bind each capsule in the lowest layer to bridge the linguistic embedding of a single word in the original question with visual evidence and then route them to the same capsule if they are siblings in the parse tree. This compositional process is achieved by performing inference on a linguistically driven conditional random field (CRF) and is performed across multiple graph capsule layers, which results in a compositional reasoning process inside a CNN. Experiments on the CLEVR dataset, CLEVR compositional generation test, and FigureQA dataset demonstrate the effectiveness and composition generalization ability of our end-to-end model.', 'corpus_id': 214612170, 'score': 0}, {'doc_id': '216553434', 'title': 'A Heterogeneous Graph with Factual, Temporal and Logical Knowledge for Question Answering Over Dynamic Contexts', 'abstract': 'We study question answering over a dynamic textual environment. Although neural network models achieve impressive accuracy via learning from input-output examples, they rarely leverage various types of knowledge and are generally not interpretable. In this work, we propose a graph-based approach, where a heterogeneous graph is automatically built with factual knowledge of the context, temporal knowledge of the past states, and logical knowledge that combines human-curated knowledge bases and rule bases. We develop a graph neural network over the constructed graph, and train the model in an end-to-end manner. Experimental results on a benchmark dataset show that the injection of various types of knowledge improves a strong neural network baseline. An additional benefit of our approach is that the graph itself naturally serves as a rational behind the decision making.', 'corpus_id': 216553434, 'score': 1}, {'doc_id': '36396995', 'title': 'Tensor Product Generation Networks for Deep NLP Modeling', 'abstract': 'We present a new approach to the design of deep networks for natural language processing (NLP), based on the general technique of Tensor Product Representations (TPRs) for encoding and processing symbol structures in distributed neural networks. A network architecture — the Tensor Product Generation Network (TPGN) — is proposed which is capable in principle of carrying out TPR computation, but which uses unconstrained deep learning to design its internal representations. Instantiated in a model for image-caption generation, TPGN outperforms LSTM baselines when evaluated on the COCO dataset. The TPR-capable structure enables interpretation of internal representations and operations, which prove to contain considerable grammatical content. Our caption-generation model can be interpreted as generating sequences of grammatical categories and retrieving words by their categories from a plan encoded as a distributed representation.', 'corpus_id': 36396995, 'score': 1}]"
147	clinical-trials-IR	456c18229ad5b140c81ef35b0dc265ac	9484	{}	"[{'doc_id': '218977368', 'title': 'Subtitles to Segmentation: Improving Low-Resource Speech-to-TextTranslation Pipelines', 'abstract': 'In this work, we focus on improving ASR output segmentation in the context of low-resource language speech-to-text translation. ASR output segmentation is crucial, as ASR systems segment the input audio using purely acoustic information and are not guaranteed to output sentence-like segments. Since most MT systems expect sentences as input, feeding in longer unsegmented passages can lead to sub-optimal performance. We explore the feasibility of using datasets of subtitles from TV shows and movies to train better ASR segmentation models. We further incorporate part-of-speech (POS) tag and dependency label information (derived from the unsegmented ASR outputs) into our segmentation model. We show that this noisy syntactic information can improve model accuracy. We evaluate our models intrinsically on segmentation quality and extrinsically on downstream MT performance, as well as downstream tasks including cross-lingual information retrieval (CLIR) tasks and human relevance assessments. Our model shows improved performance on downstream tasks for Lithuanian and Bulgarian.', 'corpus_id': 218977368, 'score': 0}, {'doc_id': '221819468', 'title': 'BioALBERT: A Simple and Effective Pre-trained Language Model for Biomedical Named Entity Recognition', 'abstract': '\n Background: In recent years, with the growing amount of biomedical documents, coupled with advancement in natural language processing algorithms, the research on biomedical named entity recognition (BioNER) has increased exponentially. However, BioNER research is challenging as NER in the biomedical domain are: (i) often restricted due to limited amount of training data, (ii) an entity can refer to multiple types and concepts depending on its context and, (iii) heavy reliance on acronyms that are sub-domain specific. Existing BioNER approaches often neglect these issues and directly adopt the state-of-the-art (SOTA) models trained in general corpora which often yields unsatisfactory results. Results: We propose biomedical ALBERT (A Lite Bidirectional Encoder Representations from Transformers for Biomedical Text Mining) - bioALBERT - an effective domain-specific pre-trained language model trained on huge biomedical corpus designed to capture biomedical context-dependent NER. We adopted self-supervised loss function used in ALBERT that targets on modelling inter-sentence coherence to better learn context-dependent representations and incorporated parameter reduction strategies to minimise memory usage and enhance the training time in BioNER. In our experiments, BioALBERT outperformed comparative SOTA BioNER models on eight biomedical NER benchmark datasets with four different entity types. The performance is increased for; (i) disease type corpora by 7.47% (NCBI-disease) and 10.63% (BC5CDR-disease); (ii) drug-chem type corpora by 4.61% (BC5CDR-Chem) and 3.89 (BC4CHEMD); (iii) gene-protein type corpora by 12.25% (BC2GM) and 6.42% (JNLPBA); and (iv) Species type corpora by 6.19% (LINNAEUS) and 23.71% (Species-800) is observed which leads to a state-of-the-art results. Conclusions: The performance of proposed model on four different biomedical entity types shows that our model is robust and generalizable in recognizing biomedical entities in text. We trained four different variants of BioALBERT models which are available for the research community to be used in future research.', 'corpus_id': 221819468, 'score': 1}, {'doc_id': '208639707', 'title': 'Improving reference prioritisation with PICO recognition', 'abstract': 'BackgroundMachine learning can assist with multiple tasks during systematic reviews to facilitate the rapid retrieval of relevant references during screening and to identify and extract information relevant to the study characteristics, which include the PICO elements of patient/population, intervention, comparator, and outcomes. The latter requires techniques for identifying and categorising fragments of text, known as named entity recognition.MethodsA publicly available corpus of PICO annotations on biomedical abstracts is used to train a named entity recognition model, which is implemented as a recurrent neural network. This model is then applied to a separate collection of abstracts for references from systematic reviews within biomedical and health domains. The occurrences of words tagged in the context of specific PICO contexts are used as additional features for a relevancy classification model. Simulations of the machine learning-assisted screening are used to evaluate the work saved by the relevancy model with and without the PICO features. Chi-squared and statistical significance of positive predicted values are used to identify words that are more indicative of relevancy within PICO contexts.ResultsInclusion of PICO features improves the performance metric on 15 of the 20 collections, with substantial gains on certain systematic reviews. Examples of words whose PICO context are more precise can explain this increase.ConclusionsWords within PICO tagged segments in abstracts are predictive features for determining inclusion. Combining PICO annotation model into the relevancy classification pipeline is a promising approach. The annotations may be useful on their own to aid users in pinpointing necessary information for data extraction, or to facilitate semantic search.', 'corpus_id': 208639707, 'score': 1}, {'doc_id': '59379420', 'title': 'Clinical Concept Embeddings Learned from Massive Sources of Multimodal Medical Data', 'abstract': 'Word embeddings are a popular approach to unsupervised learning of word relationships that are widely used in natural language processing. In this article, we present a new set of embeddings for medical concepts learned using an extremely large collection of multimodal medical data. Leaning on recent theoretical insights, we demonstrate how an insurance claims database of 60 million members, a collection of 20 million clinical notes, and 1.7 million full text biomedical journal articles can be combined to embed concepts into a common space, resulting in the largest ever set of embeddings for 108,477 medical concepts. To evaluate our approach, we present a new benchmark methodology based on statistical power specifically designed to test embeddings of medical concepts. Our approach, called cui2vec, attains state-of-the-art performance relative to previous methods in most instances. Finally, we provide a downloadable set of pre-trained embeddings for other researchers to use, as well as an online tool for interactive exploration of the cui2vec embeddings.', 'corpus_id': 59379420, 'score': 1}, {'doc_id': '221738844', 'title': 'Deep Learning Approaches for Extracting Adverse Events and Indications of Dietary Supplements from Clinical Text', 'abstract': 'OBJECTIVE\nWe sought to demonstrate the feasibility of utilizing deep learning models to extract safety signals related to the use of dietary supplements (DSs) in clinical text.\n\n\nMATERIALS AND METHODS\nTwo tasks were performed in this study. For the named entity recognition (NER) task, Bi-LSTM-CRF (bidirectional long short-term memory conditional random field) and BERT (bidirectional encoder representations from transformers) models were trained and compared with CRF model as a baseline to recognize the named entities of DSs and events from clinical notes. In the relation extraction (RE) task, 2 deep learning models, including attention-based Bi-LSTM and convolutional neural network as well as a random forest model were trained to extract the relations between DSs and events, which were categorized into 3 classes: positive (ie, indication), negative (ie, adverse events), and not related. The best performed NER and RE models were further applied on clinical notes mentioning 88 DSs for discovering DSs adverse events and indications, which were compared with a DS knowledge base.\n\n\nRESULTS\nFor the NER task, deep learning models achieved a better performance than CRF, with F1 scores above 0.860. The attention-based Bi-LSTM model performed the best in the RE task, with an F1 score of 0.893. When comparing DS event pairs generated by the deep learning models with the knowledge base for DSs and event, we found both known and unknown pairs.\n\n\nCONCLUSIONS\nDeep learning models can detect adverse events and indication of DSs in clinical notes, which hold great potential for monitoring the safety of DS use.', 'corpus_id': 221738844, 'score': 0}, {'doc_id': '112885819', 'title': 'Direct Drive Volume Control Hydraulic System', 'abstract': 'Energy-saving and structure-simplifying are two obvious characteristics of the Direct Drive Volume Control system(DDVC).It has been developed rapidly over the past decade in foreign countries,the application is also very extensive.The DDVC system is new technology assembled of AC servo motor basing on the technic of hydrostatic drive.It reflected the speediness and flexibility of motor control and the large output of hydraulic technology.The typical DDVC systems were introduced in the paper and some suggestions were put forward.', 'corpus_id': 112885819, 'score': 0}, {'doc_id': '222090856', 'title': 'Extracting Concepts for Precision Oncology from the Biomedical Literature', 'abstract': 'This paper describes an initial dataset and automatic natural language processing (NLP) method for extracting concepts related to precision oncology from biomedical research articles. We extract five concept types: Cancer, Mutation, Population, Treatment, Outcome. A corpus of 250 biomedical abstracts were annotated with these concepts following standard double-annotation procedures. We then experiment with BERT-based models for concept extraction. The best-performing model achieved a precision of 63.8%, a recall of 71.9%, and an F1 of 67.1. Finally, we propose additional directions for research for improving extraction performance and utilizing the NLP system in downstream precision oncology applications.', 'corpus_id': 222090856, 'score': 1}, {'doc_id': '10076504', 'title': 'Evaluating Unconventional Monetary Policies ─ Why', 'abstract': 'We use a general equilibrium \x85nance model that features explicit government purchases of private debts to shed light on some of the principal working mechanisms of the Federal Reserve\x92s large-scale asset purchases (LSAP) and their macroeconomic e¤ects. Our model predicts that unless private asset purchases are highly persistent and extremely large (on the order of more than 50% of annual GDP), money injections through LSAP cannot e¤ectively boost aggregate output and employment even if in\x87ation is fully anchored and the real interest rate signi\x85cantly reduced. Our framework also sheds light on some longstanding \x85nancial puzzles and monetary policy questions facing central banks around the world, such as (i) the \x87ight to liquidity under a credit crunch and debt crisis, (ii) the liquidity trap, (iii) the inverted yield curve, and (iv) the low in\x87ation puzzle under quantitative easing.', 'corpus_id': 10076504, 'score': 0}, {'doc_id': '221640671', 'title': 'RadLex Normalization in Radiology Reports', 'abstract': ""Radiology reports have been widely used for extraction of various clinically significant information about patients' imaging studies. However, limited research has focused on standardizing the entities to a common radiology-specific vocabulary. Further, no study to date has attempted to leverage RadLex for standardization. In this paper, we aim to normalize a diverse set of radiological entities to RadLex terms. We manually construct a normalization corpus by annotating entities from three types of reports. This contains 1706 entity mentions. We propose two deep learning-based NLP methods based on a pre-trained language model (BERT) for automatic normalization. First, we employ BM25 to retrieve candidate concepts for the BERT-based models (re-ranker and span detector) to predict the normalized concept. The results are promising, with the best accuracy (78.44%) obtained by the span detector. Additionally, we discuss the challenges involved in corpus construction and propose new RadLex terms."", 'corpus_id': 221640671, 'score': 0}, {'doc_id': '222177200', 'title': 'Understanding Clinical Trial Reports: Extracting Medical Entities and Their Relations', 'abstract': 'The best evidence concerning comparative treatment effectiveness comes from clinical trials, the results of which are reported in unstructured articles. Medical experts must manually extract information from articles to inform decision-making, which is time-consuming and expensive. Here we consider the end-to-end task of both (a) extracting treatments and outcomes from full-text articles describing clinical trials (entity identification) and, (b) inferring the reported results for the former with respect to the latter (relation extraction). We introduce new data for this task, and evaluate models that have recently achieved state-of-the-art results on similar tasks in Natural Language Processing. We then propose a new method motivated by how trial results are typically presented that outperforms these purely data-driven baselines. Finally, we run a fielded evaluation of the model with a non-profit seeking to identify existing drugs that might be re-purposed for cancer, showing the potential utility of end-to-end evidence extraction systems.', 'corpus_id': 222177200, 'score': 1}]"
148	trapped ion	6881e82575832de834dd23b767d1cdad	5505	{}	[{'doc_id': '208098245', 'title': 'Laser-cooled ytterbium-ion microwave frequency standard', 'abstract': 'We report on the development of a trapped-ion, microwave frequency standard based on the 12.6\xa0GHz hyperfine transition in laser-cooled ytterbium-171 ions. The entire system fits into a 6U 19-in. rack unit $$(51\\times 49\\times 28\\,\\mathrm{{cm}})$$ and comprises laser, electronics, and physics package subsystems. As a first step towards a full evaluation of the system capability, we have measured the frequency instability of our system which is $$3.6\\times 10^{-12}/\\surd \\tau $$ for averaging times between 30 and $$1500\\,\\mathrm{{s}}$$.', 'corpus_id': 208098245, 'score': 1}, {'doc_id': '215745031', 'title': 'Identification of molecular quantum states using phase-sensitive forces', 'abstract': 'Quantum-logic techniques used to manipulate quantum systems are now increasingly being applied to molecules. Previous experiments on single trapped diatomic species have enabled state detection with excellent fidelities and highly precise spectroscopic measurements. However, for complex molecules with a dense energy-level structure improved methods are necessary. Here, we demonstrate an enhanced quantum protocol for molecular state detection using state-dependent forces. Our approach is based on interfering a reference and a signal force applied to a single atomic and molecular ion. By changing the relative phase of the forces, we identify states embedded in a dense molecular energy-level structure and monitor state-to-state inelastic scattering processes. This method can also be used to exclude a large number of states in a single measurement when the initial state preparation is imperfect and information on the molecular properties is incomplete. While the present experiments focus on N\\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$${}_{2}^{+}$$\\end{document}2+, the method is general and is expected to be of particular benefit for polyatomic systems.', 'corpus_id': 215745031, 'score': 0}, {'doc_id': '218971713', 'title': 'Studies of thorium and ytterbium ion trap loading from laser ablation for gravity monitoring with nuclear clocks', 'abstract': 'Compact and robust ion traps for thorium are enabling technology for the next generation of atomic clocks based on a low-energy isomeric transition in the thorium-229 nucleus. We aim at a laser ablation loading of single triply ionized thorium in a radio-frequency electromagnetic linear Paul trap. Detection of ions is based on a modified mass spectrometer and a channeltron with single-ion sensitivity. In this study, we successfully created and detected 232Th+ and 232Th2+ ions from plasma plumes, studied their yield evolution, and compared the loading to a quadrupole ion trap with Yb. We explore the feasibility of laser ablation loading for future low-cost 229Th3+ trapping. The thorium ablation yield shows a strong depletion, suggesting that we have ablated oxide layers from the surface and the ions were a result of the plasma plume evolution and collisions. Our results are in good agreement with similar experiments for other elements and their oxides.', 'corpus_id': 218971713, 'score': 1}, {'doc_id': '215745140', 'title': 'Efficient ground-state cooling of large trapped-ion chains with an EIT tripod scheme', 'abstract': 'We report the electromagnetically-induced-transparency (EIT) cooling of a large trapped Yb ion chain to the quantum ground state. Unlike conventional EIT cooling, we engage a four-level tripod structure and achieve fast sub-Doppler cooling over all motional modes. We observe simultaneous ground-state cooling across the complete transverse mode spectrum of up to 40 ions, occupying a bandwidth of over 3 MHz. The cooling time is observed to be less than 300μs, independent of the number of ions. Such efficient cooling across the entire spectrum is essential for high-fidelity quantum operations using trapped ion crystals for quantum simulators or quantum computers.', 'corpus_id': 215745140, 'score': 1}, {'doc_id': '231718882', 'title': 'Fast high-fidelity single-qubit gates for flip-flop qubits in silicon', 'abstract': 'The flip-flop qubit, encoded in the states with antiparallel donor-bound electron and donor nuclear spins in silicon, showcases long coherence times, good controllability, and, in contrast to other donorspin-based schemes, long-distance coupling. Electron spin control near the interface, however, is likely to shorten the relaxation time by many orders of magnitude, reducing the overall qubit quality factor. Here, we theoretically study the multilevel system that is formed by the interacting electron and nuclear spins and derive analytical effective two-level Hamiltonians with and without periodic driving. We then propose an optimal control scheme that produces fast and robust single-qubit gates in the presence of low-frequency noise and relatively weak magnetic fields without relying on parametrically restrictive sweet spots. This scheme increases considerably both the relaxation time and the qubit quality factor.', 'corpus_id': 231718882, 'score': 0}, {'doc_id': '218527805', 'title': 'Detection of metastable electronic states by Penning trap mass spectrometry', 'abstract': 'State-of-the-art optical clocks 1 achieve precisions of 10 −18 or better using ensembles of atoms in optical lattices 2 , 3 or individual ions in radio-frequency traps 4 , 5 . Promising candidates for use in atomic clocks are highly charged ions 6 (HCIs) and nuclear transitions 7 , which are largely insensitive to external perturbations and reach wavelengths beyond the optical range 8 that are accessible to frequency combs 9 . However, insufficiently accurate atomic structure calculations hinder the identification of suitable transitions in HCIs. Here we report the observation of a long-lived metastable electronic state in an HCI by measuring the mass difference between the ground and excited states in rhenium, providing a non-destructive, direct determination of an electronic excitation energy. The result is in agreement with advanced calculations. We use the high-precision Penning trap mass spectrometer PENTATRAP to measure the cyclotron frequency ratio of the ground state to the metastable state of the ion with a precision of 10 −11 —an improvement by a factor of ten compared with previous measurements 10 , 11 . With a lifetime of about 130 days, the potential soft-X-ray frequency reference at 4.96\xa0×\xa010 16 hertz\xa0(corresponding to a transition energy of 202 electronvolts) has a linewidth of only 5\xa0×\xa010 −8 hertz and one of the highest electronic quality factors (10 24 ) measured experimentally so far. The low uncertainty of our method will enable searches for further soft-X-ray clock transitions 8 , 12 in HCIs, which are required for precision studies of fundamental physics 6 . Penning trap mass spectrometry is used to measure the electronic transition energy from a long-lived metastable state to the ground state in highly charged rhenium ions with a precision of 10 −11 .', 'corpus_id': 218527805, 'score': 0}, {'doc_id': '218613790', 'title': 'Experimental setup for studying an ultracold mixture of trapped \nYb+–Li6', 'abstract': 'We describe and characterize an experimental apparatus that has been used to study interactions between ultracold lithium atoms and ytterbium ions. The preparation of ultracold clouds of Li atoms is described as well as their subsequent transport and overlap with ${\\mathrm{Yb}}^{+}$ ions trapped in a Paul trap. We show how the kinetic energy of the ion after interacting with the atoms can be obtained by laser spectroscopy. We analyze the dynamics of the buffer-gas-cooled ion after releasing the atoms, which indicates that background heating, due to electric-field noise, limits attainable buffer gas cooling temperatures. This effect can be mitigated by increasing the density of the Li gas in order to improve its cooling power. Imperfections in the Paul trap lead to so-called excess micromotion, which poses another limitation to the buffer gas cooling. We describe in detail how we measure and subsequently minimize excess micromotion in our setup. We measure the effect of excess micromotion on attainable ion temperatures after buffer gas cooling and compare this to molecular dynamics simulations, which describe the observed data very well.', 'corpus_id': 218613790, 'score': 1}, {'doc_id': '214775130', 'title': 'Three-Dimensional Cooling of an Atom-Beam Source for High-Contrast Atom Interferometry', 'abstract': 'We present a compact, two-stage atomic beam source that produces a continuous, narrow, collimated and high-flux beam of rubidium atoms with sub-Doppler temperatures in three dimensions, which features very low emission of near-resonance fluorescence along the atomic trajectory. The atom beam source originates in a pushed two-dimensional magneto-optical trap (2D$^+$ MOT) feeding a slightly off-axis three-dimensional moving optical molasses stage that continuously cools and redirects the atom beam. The capture velocity of the moving optical molasses is deliberately chosen to be low, $\\sim 3$ m/s, to reduce fluorescence, and the cooling light is detuned by several atomic linewidths from resonance to reduce the absorption cross-section of cooling-induced fluorescence. Near-resonance light from the 2D$^+$ MOT and the push beam does not propagate to the output atomic trajectory due to a 10 degree bend in the atomic trajectory. The atomic beam emitted from the two-stage source has a flux up to $1.6(3)\\times 10^9\\;\\textrm{atoms/s}$, with an optimized temperature of $15.0(2)\\;\\mu$K. We employ continuous Raman-Ramsey interference measurements at the atom beam output to study the sources of decoherence in the presence of continuous cooling, and demonstrate that the atom beam source effectively preserves high fringe contrast even during cooling. This cold-atom beam source is appropriate for use in atom interferometers and clocks, where continuous operation eliminates dead time, the slow atom beam velocity (6 - 16 m/s) improves sensitivity, the narrow 3D velocity distribution improves fringe contrast, and the low reabsorption of scattered light mitigates decoherence caused by the continuous cooling process.', 'corpus_id': 214775130, 'score': 0}, {'doc_id': '214605669', 'title': 'VECSEL systems for quantum information processing with trapped beryllium ions', 'abstract': 'Two vertical-external-cavity surface-emitting laser (VECSEL) systems producing ultraviolet (UV) radiation at 235 nm and 313 nm are demonstrated. The systems are suitable for quantum information processing applications with trapped beryllium ions. Each system consists of a compact, single-frequency, continuous-wave VECSEL producing high-power near-infrared light, tunable over tens of nanometers. One system generates 2.4 W at 940 nm, using a gain mirror based on GaInAs/GaAs quantum wells, which is converted to 54 mW of 235 nm light for photoionization of neutral beryllium atoms. The other system uses a novel gain mirror based on GaInNAs/GaAs quantum-wells, enabling wavelength extension with manageable strain in the GaAs lattice. This system generates 1.6 W at 1252 nm, which is converted to 41 mW of 313 nm light that is used to laser cool trapped $^{9}$Be$^{+}$ ions and to implement quantum state preparation and detection. The 313 nm system is also suitable for implementing high-fidelity quantum gates, and more broadly, our results extend the capabilities of VECSEL systems for applications in atomic, molecular, and optical physics.', 'corpus_id': 214605669, 'score': 1}]
149	Lidar-SLAM	dd4d57d672e0b5b1213d80cc501b9305	1586	{'SLAM': 'signaling lymphocyte activation molecule'}	[{'doc_id': '3911724', 'title': 'Incremental-Segment-Based Localization in 3-D Point Clouds', 'abstract': 'Localization in 3-D point clouds is a highly challenging task due to the complexity associated with extracting information from 3-D data. This letter proposes an incremental approach addressing this problem efficiently. The presented method first accumulates the measurements in a dynamic voxel grid and selectively updates the point normals affected by the insertion. An incremental segmentation algorithm, based on region growing, tracks the evolution of single segments, which enables an efficient recognition strategy using partitioning and caching of geometric consistencies. We show that the incremental method can perform global localization at 10\xa0Hz in an urban driving environment, a speedup of $\\times$7.1 over the compared batch solution. The efficiency of the method makes it suitable for applications where real-time localization is required and enables its usage on cheaper low-energy systems. Our implementation is available open source along with instructions for running the system. (The implementation is available at  https://github.com/ethz-asl/segmatch and a video demonstration is available at https://youtu.be/cHfs3HLzc2Y .)', 'corpus_id': 3911724, 'score': 1}, {'doc_id': '210928259', 'title': 'Online LiDAR-SLAM for Legged Robots with Robust Registration and Deep-Learned Loop Closure', 'abstract': 'In this paper, we present a 3D factor-graph LiDAR-SLAM system which incorporates a state-of-the-art deeply learned feature-based loop closure detector to enable a legged robot to localize and map in industrial environments. Point clouds are accumulated using an inertial-kinematic state estimator before being aligned using ICP registration. To close loops we use a loop proposal mechanism which matches individual segments between clouds. We trained a descriptor offline to match these segments. The efficiency of our method comes from carefully designing the network architecture to minimize the number of parameters such that this deep learning method can be deployed in real-time using only the CPU of a legged robot, a major contribution of this work. The set of odometry and loop closure factors are updated using pose graph optimization. Finally we present an efficient risk alignment prediction method which verifies the reliability of the registrations. Experimental results at an industrial facility demonstrated the robustness and flexibility of our system, including autonomous following paths derived from the SLAM map.', 'corpus_id': 210928259, 'score': 1}, {'doc_id': '19233296', 'title': 'SegMap: 3D Segment Mapping using Data-Driven Descriptors', 'abstract': 'When performing localization and mapping, working at the level of structure can be advantageous in terms of robustness to environmental changes and differences in illumination. This paper presents SegMap: a map representation solution to the localization and mapping problem based on the extraction of segments in 3D point clouds. In addition to facilitating the computationally intensive task of processing 3D point clouds, working at the level of segments addresses the data compression requirements of real-time single- and multi-robot systems. While current methods extract descriptors for the single task of localization, SegMap leverages a data-driven descriptor in order to extract meaningful features that can also be used for reconstructing a dense 3D map of the environment and for extracting semantic information. This is particularly interesting for navigation tasks and for providing visual feedback to end-users such as robot operators, for example in search and rescue scenarios. These capabilities are demonstrated in multiple urban driving and search and rescue experiments. Our method leads to an increase of area under the ROC curve of 28.3% over current state of the art using eigenvalue based features. We also obtain very similar reconstruction capabilities to a model specifically trained for this task. The SegMap implementation will be made available open-source along with easy to run demonstrations at this http URL. A video demonstration is available at this https URL.', 'corpus_id': 19233296, 'score': 1}, {'doc_id': '3516878', 'title': 'IMLS-SLAM: Scan-to-Model Matching Based on 3D Data', 'abstract': 'The Simultaneous Localization And Mapping (SLAM) problem has been well studied in the robotics community, especially using mono, stereo cameras or depth sensors. 3D depth sensors, such as Velodyne LiDAR, have proved in the last 10 years to be very useful to perceive the environment in autonomous driving, but few methods exist that directly use these 3D data for odometry. We present a new low-drift SLAM algorithm based only on 3D LiDAR data. Our method relies on a scan-to-model matching framework. We first have a specific sampling strategy based on the LiDAR scans. We then define our model as the previous localized LiDAR sweeps and use the Implicit Moving Least Squares (IMLS) surface representation. We show experiments with the Velodyne HDL32 with only 0.40% drift over a 4 km acquisition without any loop closure (i.e., 16 $m$ drift after 4 km). We tested our solution on the KITTI benchmark with a Velodyne HDL64 and ranked among the best methods (against mono, stereo and LiDAR methods) with a global drift of only 0.69%.', 'corpus_id': 3516878, 'score': 1}, {'doc_id': '210714034', 'title': 'Review: deep learning on 3D point clouds', 'abstract': 'Point cloud is point sets defined in 3D metric space. Point cloud has become one of the most significant data format for 3D representation. Its gaining increased popularity as a result of increased availability of acquisition devices, such as LiDAR, as well as increased application in areas such as robotics, autonomous driving, augmented and virtual reality. Deep learning is now the most powerful tool for data processing in computer vision, becoming the most preferred technique for tasks such as classification, segmentation, and detection. While deep learning techniques are mainly applied to data with a structured grid, point cloud, on the other hand, is unstructured. The unstructuredness of point clouds makes use of deep learning for its processing directly very challenging. Earlier approaches overcome this challenge by preprocessing the point cloud into a structured grid format at the cost of increased computational cost or lost of depth information. Recently, however, many state-of-the-arts deep learning techniques that directly operate on point cloud are being developed. This paper contains a survey of the recent state-of-the-art deep learning techniques that mainly focused on point cloud data. We first briefly discussed the major challenges faced when using deep learning directly on point cloud, we also briefly discussed earlier approaches which overcome the challenges by preprocessing the point cloud into a structured grid. We then give the review of the various state-of-the-art deep learning approaches that directly process point cloud in its unstructured form. We introduced the popular 3D point cloud benchmark datasets. And we also further discussed the application of deep learning in popular 3D vision tasks including classification, segmentation and detection.', 'corpus_id': 210714034, 'score': 0}, {'doc_id': '211677612', 'title': '3D Point Cloud Processing and Learning for Autonomous Driving', 'abstract': 'We present a review of 3D point cloud processing and learning for autonomous driving. As one of the most important sensors in autonomous vehicles, light detection and ranging (LiDAR) sensors collect 3D point clouds that precisely record the external surfaces of objects and scenes. The tools for 3D point cloud processing and learning are critical to the map creation, localization, and perception modules in an autonomous vehicle. While much attention has been paid to data collected from cameras, such as images and videos, an increasing number of researchers have recognized the importance and significance of LiDAR in autonomous driving and have proposed processing and learning algorithms to exploit 3D point clouds. We review the recent progress in this research area and summarize what has been tried and what is needed for practical and safe autonomous vehicles. We also offer perspectives on open issues that are needed to be solved in the future.', 'corpus_id': 211677612, 'score': 0}, {'doc_id': '211069171', 'title': 'StickyPillars: Robust feature matching on point clouds using Graph Neural Networks', 'abstract': 'StickyPillars introduces a sparse feature matching method on point clouds. It is the first approach applying Graph Neural Networks on point clouds to stick points of interest. The feature estimation and assignment relies on the optimal transport problem, where the cost is based on the neural network itself. We utilize a Graph Neural Network for context aggregation with the aid of multihead self and cross attention. In contrast to image based feature matching methods, the architecture learns feature extraction in an end-to-end manner. Hence, the approach does not rely on handcrafted features. Our method outperforms state-of-the art matching algorithms, while providing real-time capability.', 'corpus_id': 211069171, 'score': 1}, {'doc_id': '214775247', 'title': 'Monocular Camera Localization in Prior LiDAR Maps with 2D-3D Line Correspondences', 'abstract': 'Light-weight camera localization in existing maps is essential for vision-based navigation. Currently, visual and visual-inertial odometry (VO&VIO) techniques are well-developed for state estimation but with inevitable accumulated drifts and pose jumps upon loop closure. To overcome these problems, we propose an efficient monocular camera localization method in prior LiDAR maps using direct 2D-3D line correspondences. To handle the appearance differences and modality gaps between LiDAR point clouds and images, geometric 3D lines are extracted offline from LiDAR maps while robust 2D lines are extracted online from video sequences. With the pose prediction from VIO, we can efficiently obtain coarse 2D-3D line correspondences. Then the camera poses and 2D-3D correspondences are iteratively optimized by minimizing the projection error of correspondences and rejecting outliers. Experimental results on the EurocMav dataset and our collected dataset demonstrate that the proposed method can efficiently estimate camera poses without accumulated drifts or pose jumps in structured environments.', 'corpus_id': 214775247, 'score': 0}, {'doc_id': '211677268', 'title': 'Triangle-Net: Towards Robustness in Point Cloud Classification', 'abstract': '3D object recognition is becoming a key desired capability for many computer vision systems such as autonomous vehicles, service robots and surveillance drones to operate more effectively in unstructured environments. These real-time systems require effective classification methods that are robust to sampling resolution, measurement noise, and pose configuration of the objects. Previous research has shown that sparsity, rotation and positional variance of points can lead to a significant drop in the performance of point cloud based classification techniques. In this regard, we propose a novel approach for 3D classification that takes sparse point clouds as input and learns a model that is robust to rotational and positional variance as well as point sparsity. To this end, we introduce new feature descriptors which are fed as an input to our proposed neural network in order to learn a robust latent representation of the 3D object. We show that such latent representations can significantly improve the performance of object classification and retrieval. Further, we show that our approach outperforms PointNet and 3DmFV by 34.4% and 27.4% respectively in classification tasks using sparse point clouds of only 16 points under arbitrary SO(3) rotation.', 'corpus_id': 211677268, 'score': 0}, {'doc_id': '212628458', 'title': 'D3Feat: Joint Learning of Dense Detection and Description of 3D Local Features', 'abstract': 'A successful point cloud registration often lies on robust establishment of sparse matches through discriminative 3D local features. Despite the fast evolution of learning-based 3D feature descriptors, little attention has been drawn to the learning of 3D feature detectors, even less for a joint learning of the two tasks. In this paper, we leverage a 3D fully convolutional network for 3D point clouds, and propose a novel and practical learning mechanism that densely predicts both a detection score and a description feature for each 3D point. In particular, we propose a keypoint selection strategy that overcomes the inherent density variations of 3D point clouds, and further propose a self-supervised detector loss guided by the on-the-fly feature matching results during training. Finally, our method achieves state-of-the-art results in both indoor and outdoor scenarios, evaluated on 3DMatch and KITTI datasets, and shows its strong generalization ability on the ETH dataset. Towards practical use, we show that by adopting a reliable feature detector, sampling a smaller number of features is sufficient to achieve accurate and fast point cloud alignment.', 'corpus_id': 212628458, 'score': 0}]
150	faithfulness	3bdf571f1046b217a922496c89676077	10161	{}	"[{'doc_id': '233204406', 'title': 'Annotating and Modeling Fine-grained Factuality in Summarization', 'abstract': 'Recent pre-trained abstractive summarization systems have started to achieve credible performance, but a major barrier to their use in practice is their propensity to output summaries that are not faithful to the input and that contain factual errors. While a number of annotated datasets and statistical models for assessing factuality have been explored, there is no clear picture of what errors are most important to target or where current techniques are succeeding and failing. We explore both synthetic and human-labeled data sources for training models to identify factual errors in summarization, and study factuality at the word-, dependency-, and sentence-level. Our observations are threefold. First, exhibited factual errors differ significantly across datasets, and commonly-used training sets of simple synthetic errors do not reflect errors made on abstractive datasets like XSum. Second, human-labeled data with fine-grained annotations provides a more effective training signal than sentence-level annotations or synthetic data. Finally, we show that our best factuality detection model enables training of more factual XSum summarization models by allowing us to identify non-factual tokens in the training data.', 'corpus_id': 233204406, 'score': 1}, {'doc_id': '215548661', 'title': 'Asking and Answering Questions to Evaluate the Factual Consistency of Summaries', 'abstract': 'Practical applications of abstractive summarization models are limited by frequent factual inconsistencies with respect to their input. Existing automatic evaluation metrics for summarization are largely insensitive to such errors. We propose QAGS (pronounced “kags”), an automatic evaluation protocol that is designed to identify factual inconsistencies in a generated summary. QAGS is based on the intuition that if we ask questions about a summary and its source, we will receive similar answers if the summary is factually consistent with the source. To evaluate QAGS, we collect human judgments of factual consistency on model-generated summaries for the CNN/DailyMail (Hermann et al., 2015) and XSUM (Narayan et al., 2018) summarization datasets. QAGS has substantially higher correlations with these judgments than other automatic evaluation metrics. Also, QAGS offers a natural form of interpretability: The answers and questions generated while computing QAGS indicate which tokens of a summary are inconsistent and why. We believe QAGS is a promising tool in automatically generating usable and factually consistent text. Code for QAGS will be available at https://github.com/W4ngatang/qags.', 'corpus_id': 215548661, 'score': 1}, {'doc_id': '233231257', 'title': 'Ask what’s missing and what’s useful: Improving Clarification Question Generation using Global Knowledge', 'abstract': 'The ability to generate clarification questions i.e., questions that identify useful missing information in a given context, is important in reducing ambiguity. Humans use previous experience with similar contexts to form a global view and compare it to the given context to ascertain what is missing and what is useful in the context. Inspired by this, we propose a model for clarification question generation where we first identify what is missing by taking a difference between the global and the local view and then train a model to identify what is useful and generate a question about it. Our model outperforms several baselines as judged by both automatic metrics and humans.', 'corpus_id': 233231257, 'score': 0}, {'doc_id': '233476302', 'title': 'The Factual Inconsistency Problem in Abstractive Text Summarization: A Survey', 'abstract': 'Recently, various neural encoder-decoder models pioneered by Seq2Seq framework have been proposed to achieve the goal of generating more abstractive summaries by learning to map input text to output text. At a high level, such neural models can freely generate summaries without any constraint on the words or phrases used. Moreover, their format is closer to human-edited summaries and output is more readable and fluent. However, the neural model’s abstraction ability is a doubleedged sword. A commonly observed problem with the generated summaries is the distortion or fabrication of factual information in the article. This inconsistency between the original text and the summary has caused various concerns over its applicability, and the previous evaluation methods of text summarization are not suitable for this issue. In response to the above problems, the current research direction is predominantly divided into two categories, one is to design fact-aware evaluation metrics to select outputs without factual inconsistency errors, and the other is to develop new summarization systems towards factual consistency. In this survey, we focus on presenting a comprehensive review of these fact-specific evaluation methods and text summarization models.', 'corpus_id': 233476302, 'score': 0}, {'doc_id': '233033613', 'title': 'Efficient Attentions for Long Document Summarization', 'abstract': 'The quadratic computational and memory complexities of large Transformers have limited their scalability for long document summarization. In this paper, we propose Hepos, a novel efficient encoder-decoder attention with head-wise positional strides to effectively pinpoint salient information from the source. We further conduct a systematic study of existing efficient self-attentions. Combined with Hepos, we are able to process ten times more tokens than existing models that use full attentions. For evaluation, we present a new dataset, GovReport, with significantly longer documents and summaries. Results show that our models produce significantly higher ROUGE scores than competitive comparisons, including new state-of-the-art results on PubMed. Human evaluation also shows that our models generate more informative summaries with fewer unfaithful errors.', 'corpus_id': 233033613, 'score': 0}, {'doc_id': '234679223', 'title': 'QAConv: Question Answering on Informative Conversations', 'abstract': 'This paper introduces QAConv, a new ques001 tion answering (QA) dataset that uses conver002 sations as a knowledge source. We focus on 003 informative conversations, including business 004 emails, panel discussions, and work channels. 005 Unlike open-domain and task-oriented dia006 logues, these conversations are usually long, 007 complex, asynchronous, and involve strong do008 main knowledge. In total, we collect 34,608 009 QA pairs, including span-based and unanswer010 able questions, from 10,259 selected conversa011 tions with both human-written and machine012 generated questions. We use a question gen013 erator and a dialogue summarizer as auxil014 iary tools to collect multi-hop questions. The 015 dataset has two testing scenarios: chunk mode 016 and full mode, depending on whether the 017 grounded partial conversation is provided or 018 retrieved. Experimental results show that state019 of-the-art pretrained QA systems have limited 020 zero-shot performance and tend to predict our 021 questions as unanswerable. Our dataset pro022 vides a new training and evaluation testbed to 023 facilitate QA on conversations research. 024', 'corpus_id': 234679223, 'score': 0}, {'doc_id': '233189560', 'title': 'Discourse Understanding and Factual Consistency in Abstractive Summarization', 'abstract': 'We introduce a general framework for abstractive summarization with factual consistency and distinct modeling of the narrative flow in an output summary. Our work addresses current limitations of models for abstractive summarization that often hallucinate information or generate summaries with coherence issues. To generate abstractive summaries with factual consistency and narrative flow, we propose Cooperative Generator-Discriminator Networks (Co-opNet), a novel transformer-based framework where the generator works with a discriminator architecture to compose coherent long-form summaries. We explore four different discriminator objectives which each capture a different aspect of coherence, including whether salient spans of generated abstracts are hallucinated or appear in the input context, and the likelihood of sentence adjacency in generated abstracts. We measure the ability of Co-opNet to learn these objectives with arXiv scientific papers, using the abstracts as a proxy for gold long-form scientific article summaries. Empirical results from automatic and human evaluations demonstrate that Co-opNet learns to summarize with considerably improved global coherence compared to competitive baselines.', 'corpus_id': 233189560, 'score': 0}, {'doc_id': '129436568', 'title': 'A laboratory study of the effect of magnetite on NMR relaxation rates', 'abstract': 'We conducted a laboratory study to measure the effect of magnetite concentration and grain size on proton nuclear magnetic resonance (NMR) relaxation rates of sand mixtures and to determine the dominant mechanism by which relaxation occurs. We measured mixtures of quartz and three different forms of magnetite: a powdered synthetic magnetite; a small-grained, natural magnetite; and a large-grained, natural magnetite. The powdered synthetic magnetite was mixed with quartz in five concentrations ranging from 0.14 to 1.4% magnetite by weight; both sizes of natural magnetite were mixed with quartz in concentrations of 1 and 2% magnetite by weight. The NMR response of the water-saturated samples was measured and used to calculate four averaged relaxation rates for each magnetite concentration: the total mean log, bulk fluid, surface, and diffusion relaxation rates. The results of this study show that: 1) surface relaxation was the dominant relaxation mechanism for all samples except the powdered synthetic magnetite sample containing 1.4% magnetite; 2) the surface relaxivity is a function of the fraction of the surface area in the sample composed of magnetite; 3) there is no clear dependence of the diffusion relaxation rate on the concentration of magnetite.', 'corpus_id': 129436568, 'score': 0}, {'doc_id': '193049753', 'title': ""Dispositif pour le réglage automatique de la largeur de travail d'un premier corps de charrue en fonction de la largeur de travail variable de corps de charrue suivants"", 'abstract': ""L'invention concerne un dispositif pour regler automatiquement la largeur de travail du premier corps de charrue (111) d'une charrue (1) en fonction de la largeur de travail variable de corps de charrue suivants (112, 113), dans lequel un premier cylindre hydraulique (13) est concu pour deplacer une section de châssis arriere (117) lateralement par rapport a une section de châssis avant (116), et un second cylindre hydraulique (14) est concu pour pivoter la section de châssis arriere (117) autour d'un premier axe de pivotement vertical (119) pour regler ainsi un espacement transversal des corps de charrue (111, 112, 113) par les corps de charrue (111, 112, 113) pivotant autour de seconds axes de pivotement vertical (115), le premier cylindre hydraulique (13) etant un cylindre a etages multiples dans lequel un premier etage de cylindre (131) forme une commande laterale principale, et un second etage de cylindre (132) forme une commande de largeur de travail automatique pour le premier corps de charrue (111), le second etage de cylindre (132) etant en communication fluidique hydraulique avec le second cylindre hydraulique (14) dans une configuration maitre-esclave, le second etage de cylindre (132) etant le maitre ou l'esclave en fonction de la direction de mouvement de l'etage de cylindre (132), et le second cylindre hydraulique (14) etant respectivement l'esclave ou le maitre, et les diametres de piston (D 1a , D 2 ) du second etage de cylindre (132) et du second cylindre hydraulique (14) correspondant l'un a l'autre de telle sorte qu'un reglage de la largeur de travail des corps de charrue (111, 112, 113) par le second cylindre hydraulique (14) realise une correction du deplacement lateral de la section de châssis avant (116)."", 'corpus_id': 193049753, 'score': 0}, {'doc_id': '233289483', 'title': 'Q2: Evaluating Factual Consistency in Knowledge-Grounded Dialogues via Question Generation and Question Answering', 'abstract': 'Neural knowledge-grounded generative models for dialogue often produce content that is factually inconsistent with the source text they rely on. As a consequence, such models are unreliable, limiting their real-world applicability. Inspired by recent work on evaluating factual consistency in abstractive summarization (Durmus et al., 2020; Wang et al., 2020), we propose an automatic evaluation metric for factual consistency in knowledge-grounded dialogue models using automatic question generation and question answering. Unlike previous works which use naïve token-based comparison of answer spans, our metric makes use of co-reference resolution and natural language inference capabilities which greatly improve its performance. To foster proper evaluation, we curate a novel dataset of state-of-theart dialogue system outputs for the Wizard-ofWikipedia dataset (Dinan et al., 2019), which we manually annotate for factual consistency. We perform a thorough meta-evaluation of our metric against other metrics using the new dataset and two others, where it greatly outperforms the baselines.1', 'corpus_id': 233289483, 'score': 1}]"
151	CSS Reviews	a2212662c828eff5ea177d5b4df6a04d	20567	{'CSS': 'Computational Social Science'}	[{'doc_id': '237248345', 'title': 'How social media shapes polarization', 'abstract': 'This article reviews the empirical evidence on the relationship between social media and political polarization. We argue that social media shapes polarization through the following social, cognitive, and technological processes: partisan selection, message content, and platform design and algorithms.', 'corpus_id': 237248345, 'score': 0}, {'doc_id': '44767798', 'title': 'Computational Social Science', 'abstract': 'centered on a large database, but in this case it is entirely of living organisms, the marine bivalves. Over 28,000 records of bivalve genera and subgenera from 322 locations around the world have now been compiled by these authors, giving a global record of some 854 genera and subgenera and 5132 species. No fossils are included in the database, but because bivalves have a good fossil record, it is possible to estimate accurately the age of origin of almost all extant genera. It is then possible to plot a backward survivorship curve (8) for each of the 27 global bivalve provinces (9). On the basis of these curves, Krug et al. find that origination rates of marine bivalves increased significantly almost everywhere immediately after the K-Pg mass extinction event. The highest K-Pg origination rates all occurred in tropical and warm-temperate regions. A distinct pulse of bivalve diversification in the early Cenozoic was concentrated mainly in tropical and subtropical regions (see the figure). The steepest part of the global backward survivorship curve for bivalves lies between 65 and 50 million years ago, pointing to a major biodiversification event in the Paleogene (65 to 23 million years ago) that is perhaps not yet captured in Alroy et al.’s database (5, 7). The jury is still out on what may have caused this event. But we should not lose sight of the fact that the steep rise to prominence of many modern floral and faunal groups in the Cenozoic may bear no simple relationship to climate or any other type of environmental change (10, 11).', 'corpus_id': 44767798, 'score': 1}, {'doc_id': '3881717', 'title': 'Computational social science ≠ computer science + social data', 'abstract': 'The important intersection of computer science and social science.', 'corpus_id': 3881717, 'score': 1}, {'doc_id': '238106573', 'title': 'What is Household Crowding? And how does it affect Children’s Enrolment Rates in the Early Year’s Education (EYE): The Case of Kenya in the Covid Era.', 'abstract': 'A research article published in International Journal of Social Science and Humanities Research', 'corpus_id': 238106573, 'score': 0}, {'doc_id': '237434691', 'title': 'Introduction to the special issue on COMPLEX NETWORKS 2019', 'abstract': 'This special issue of Network Science contains a collection of extended papers from the 8th International Conference on Complex Networks & their Applications (COMPLEX NETWORKS 2019). This major international event in network science brings together every year researchers from around the globe. The great diversity of the participants’ scientific backgrounds ranges from Finance and Economics, Medicine and Neuroscience, Biology and Earth Sciences, Sociology and Political Science to Mathematics and Computer Science, Physics, and many others, making it a special opportunity to review the current state of the field and formulate new directions. This edition of the conference took place at the Calouste Gulbenkian Foundation in Lisbon (Portugal) from December 10 to December 12, 2019. It attracted 470 submissions with authors from 58 countries all over the world. After thorough review, 161 papers were selected to be included in the proceedings Cherifi et al. (2020a,b). The conference program also included keynote presentations from Lada Adamic (Facebook, Inc., USA), Reka Albert (Pennsylvania State University, USA), Ulrik Brandes (ETH Zurich, Switzerland), Stefan Thurner (Medical University of Vienna, Austria), Jari Saramki (Aalto University, Finland), andMichalis Vazirgiannis (LIX, cole Polytechnique, France). Papers invited for this special issue have been selected from the accepted contributions based on relevance to the journal and excellent reviews of the conference version of the papers. The authors were asked to submit an extended version of their conference submission for journal publication in accordance with the customary practice of adding 30% new material. These submissions went through the standard double-blind review process dictated by the journal guidelines. The seven papers accepted to this special issue provide a remarkable sample illustrating the diversity of issues studied in network science research. In Horn and Nelsen (2020), the authors investigate the celebrated PageRank centrality measure from the point of view of differential geometry. More precisely, they answer the following question: how do variation in teleportation constants influence PageRank scores? Exploiting the relationship between heat flow and PageRank, they develop inequalities that bound a particular “gradient” of PageRank over the graph. Their gradient estimate shows that the PageRank vector smoothes out as the jumping constant changes. This work illustrates a beautiful mathematical theory that connects differential equations, PageRank, and diffusion processes over graphs. Liu et al. (2020) introduce a convolutional neural network framework and preprocessing techniques to estimate graphlet counts. Exploiting the correlation between the structural graph information and graphlet counts, they propose to predict graphlet count from previously studied graphs coming from a similar distribution with known graphlet counts. Extensive experiments conducted on three types of random graphs (Erdős–Rényi, Barabási–Albert, and geometric) and real-world graphs (biochemistry, collaboration, and social network) for 3,4,5-node graphlet counting show that their framework offer substantial speedup on estimating graphlet counts of new graphs with high accuracy.', 'corpus_id': 237434691, 'score': 1}, {'doc_id': '237063346', 'title': 'Basic Computer Programs in Science and Engineering', 'abstract': 'Basic computer programs in science and engineering , Basic computer programs in science and engineering , مرکز فناوری اطلاعات و اطلاع رسانی کشاورزی', 'corpus_id': 237063346, 'score': 0}, {'doc_id': '219037410', 'title': 'Computational Social Science and Sociology', 'abstract': 'The integration of social science with computer science and engineering fields has produced a new area of study: computational social science. This field applies computational methods to novel sour...', 'corpus_id': 219037410, 'score': 1}, {'doc_id': '237960990', 'title': 'In Short About Bulgarian Academy of\xa0Sciences', 'abstract': 'Short notes concerning the history of computer science and informatics in Bulgarian Academy of Sciences are given.', 'corpus_id': 237960990, 'score': 0}, {'doc_id': '238146283', 'title': 'Participating in social work', 'abstract': 'How can the professional field of social work, decisive for democracy, integrate participation? Contributions advocate a community of expertise (scientific, professional and user-based) concerning the conditions of existence, of functionning and of evolution of social work.', 'corpus_id': 238146283, 'score': 0}, {'doc_id': '221342526', 'title': 'Computational social science: Obstacles and opportunities', 'abstract': 'Data sharing, research ethics, and incentives must improve The field of computational social science (CSS) has exploded in prominence over the past decade, with thousands of papers published using observational data, experimental designs, and large-scale simulations that were once unfeasible or unavailable to researchers. These studies have greatly improved our understanding of important phenomena, ranging from social inequality to the spread of infectious diseases. The institutions supporting CSS in the academy have also grown substantially, as evidenced by the proliferation of conferences, workshops, and summer schools across the globe, across disciplines, and across sources of data. But the field has also fallen short in important ways. Many institutional structures around the field—including research ethics, pedagogy, and data infrastructure—are still nascent. We suggest opportunities to address these issues, especially in improving the alignment between the organization of the 20th-century university and the intellectual requirements of the field.', 'corpus_id': 221342526, 'score': 1}]
152	appropriate trust of AI	6ea89f1b8779846a18ca6c7e2fdf213c	20506	{}	"[{'doc_id': '237635290', 'title': 'Discovering and Validating AI Errors With Crowdsourced Failure Reports', 'abstract': 'AI systems can fail to learn important behaviors, leading to real-world issues like safety concerns and biases. Discovering these systematic failures often requires significant developer attention, from hypothesizing potential edge cases to collecting evidence and validating patterns. To scale and streamline this process, we introduce crowdsourced failure reports, end-user descriptions of how or why a model failed, and show how developers can use them to detect AI errors. We also design and implement Deblinder, a visual analytics system for synthesizing failure reports that developers can use to discover and validate systematic failures. In semi-structured interviews and think-aloud studies with 10 AI practitioners, we explore the affordances of the Deblinder system and the applicability of failure reports in real-world settings. Lastly, we show how collecting additional data from the groups identified by developers can improve model performance.', 'corpus_id': 237635290, 'score': 0}, {'doc_id': '236965949', 'title': 'Examining correlation between trust and transparency with explainable artificial intelligence', 'abstract': 'Trust between humans and artificial intelligence(AI) is an issue which has implications in many fields of human computer interaction. The current issue with artificial intelligence is a lack of transparency into its decision making, and literature shows that increasing transparency increases trust. Explainable artificial intelligence has the ability to increase transparency of AI, which could potentially increase trust for humans. This paper attempts to use the task of predicting yelp review star ratings with assistance from an explainable and non explainable artificial intelligence to see if trust is increased with increased transparency. Results show that for these tasks, explainable artificial intelligence provided significant increase in trust as a measure of influence.', 'corpus_id': 236965949, 'score': 1}, {'doc_id': '220128138', 'title': 'Does the Whole Exceed its Parts? The Effect of AI Explanations on Complementary Team Performance', 'abstract': 'Many researchers motivate explainable AI with studies showing that human-AI team performance on decision-making tasks improves when the AI explains its recommendations. However, prior studies observed improvements from explanations only when the AI, alone, outperformed both the human and the best team. Can explanations help lead to complementary performance, where team accuracy is higher than either the human or the AI working solo? We conduct mixed-method user studies on three datasets, where an AI with accuracy comparable to humans helps participants solve a task (explaining itself in some conditions). While we observed complementary improvements from AI augmentation, they were not increased by explanations. Rather, explanations increased the chance that humans will accept the AI’s recommendation, regardless of its correctness. Our result poses new challenges for human-centered AI: Can we develop explanatory approaches that encourage appropriate trust in AI, and therefore help generate (or improve) complementary performance?', 'corpus_id': 220128138, 'score': 1}, {'doc_id': '237576342', 'title': 'From SAE-Levels to Cooperative Task Distribution:An Efficient and Usable Way to Deal with System Limitations?', 'abstract': 'Automated driving seems to be a promising approach to increase traffic safety, efficiency, and driver comfort. The defined automation capability levels (SAE) recommend a distinct takeover of the vehicle’s control from the human driver. This implies that if the system reaches a system boundary, the control falls back to the human. However, another possibility might be the cooperative approach of task distribution: The driver provides the missing information to the automation, which will stay activated. In a driving simulator study, we compared both a classical and a cooperative approach (N = 18). An automated car was driving on a rural road when a slower leading vehicle made it impossible for the automation to overtake. The participants could either initiate the overtake by providing the missing information cooperatively or fully taking over the vehicle’s control. Results showed that the cooperative approach has a higher usage and reduces workload. Therefore, the suggested cooperative approach seems to be more promising.', 'corpus_id': 237576342, 'score': 0}, {'doc_id': '210023849', 'title': 'Effect of confidence and explanation on accuracy and trust calibration in AI-assisted decision making', 'abstract': ""Today, AI is being increasingly used to help human experts make decisions in high-stakes scenarios. In these scenarios, full automation is often undesirable, not only due to the significance of the outcome, but also because human experts can draw on their domain knowledge complementary to the model's to ensure task success. We refer to these scenarios as AI-assisted decision making, where the individual strengths of the human and the AI come together to optimize the joint decision outcome. A key to their success is to appropriately calibrate human trust in the AI on a case-by-case basis; knowing when to trust or distrust the AI allows the human expert to appropriately apply their knowledge, improving decision outcomes in cases where the model is likely to perform poorly. This research conducts a case study of AI-assisted decision making in which humans and AI have comparable performance alone, and explores whether features that reveal case-specific model information can calibrate trust and improve the joint performance of the human and AI. Specifically, we study the effect of showing confidence score and local explanation for a particular prediction. Through two human experiments, we show that confidence score can help calibrate people's trust in an AI model, but trust calibration alone is not sufficient to improve AI-assisted decision making, which may also depend on whether the human can bring in enough unique knowledge to complement the AI's errors. We also highlight the problems in using local explanation for AI-assisted decision making scenarios and invite the research community to explore new approaches to explainability for calibrating human trust in AI."", 'corpus_id': 210023849, 'score': 1}, {'doc_id': '237492114', 'title': 'Perceptions of Fairness and Trustworthiness Based on Explanations in Human vs. Automated Decision-Making', 'abstract': 'Automated decision systems (ADS) have become ubiquitous in many high-stakes domains. Those systems typically involve sophisticated yet opaque artificial intelligence (AI) techniques that seldom allow for full comprehension of their inner workings, particularly for affected individuals. As a result, ADS are prone to deficient oversight and calibration, which can lead to undesirable (e.g., unfair) outcomes. In this work, we conduct an online study with 200 participants to examine people’s perceptions of fairness and trustworthiness towards ADS in comparison to a scenario where a human instead of an ADS makes a high-stakes decision— and we provide thorough identical explanations regarding decisions in both cases. Surprisingly, we find that people perceive ADS as fairer than human decisionmakers. Our analyses also suggest that people’s AI literacy affects their perceptions, indicating that people with higher AI literacy favor ADS more strongly over human decision-makers, whereas low-AI-literacy people exhibit no significant differences in their perceptions.', 'corpus_id': 237492114, 'score': 1}, {'doc_id': '236956490', 'title': 'Left, Right, and Gender: Exploring Interaction Traces to Mitigate Human Biases', 'abstract': 'Human biases impact the way people analyze data and make decisions. Recent work has shown that some visualization designs can better support cognitive processes and mitigate cognitive biases (i.e., errors that occur due to the use of mental ""shortcuts""). In this work, we explore how visualizing a user\'s interaction history (i.e., which data points and attributes a user has interacted with) can be used to mitigate potential biases that drive decision making by promoting conscious reflection of one\'s analysis process. Given an interactive scatterplot-based visualization tool, we showed interaction history in real-time while exploring data (by coloring points in the scatterplot that the user has interacted with), and in a summative format after a decision has been made (by comparing the distribution of user interactions to the underlying distribution of the data). We conducted a series of in-lab experiments and a crowd-sourced experiment to evaluate the effectiveness of interaction history interventions toward mitigating bias. We contextualized this work in a political scenario in which participants were instructed to choose a committee of 10 fictitious politicians to review a recent bill passed in the U.S. state of Georgia banning abortion after 6 weeks, where things like gender bias or political party bias may drive one\'s analysis process. We demonstrate the generalizability of this approach by evaluating a second decision making scenario related to movies. Our results are inconclusive for the effectiveness of interaction history (henceforth referred to as interaction traces) toward mitigating biased decision making. However, we find some mixed support that interaction traces, particularly in a summative format, can increase awareness of potential unconscious biases.', 'corpus_id': 236956490, 'score': 0}, {'doc_id': '124573144', 'title': 'Hysteretic Nonlinearity in Inverted Pendulum Problem', 'abstract': 'This work is dedicated to the problem of inverted pendulum under hysteretic nonlinearity in the form of backlash in the suspension point. We present the results for various motion of the suspension point, namely, the vertical and horizontal motions. We consider the mathematical model of inverted pendulum with vertically oscillating suspension and in the frame of presented model the explicit stability criteria for the linearized equations of motion are found. Dependencies between initial conditions and driven parameters, that provide periodic oscillations of the pendulum, are obtained. In the next step we consider the mathematical model of inverted pendulum under state feedback control (horizontal motion of suspension). Analytic results for the stability criteria as well as for the solution of linearized equation are observed and analyzed. The theorems that determine stabilization of the considered system are formulated and discussed together with the question on the optimal control. We also investigate the elastic inverted pendulum with backlash in the suspension point (horizontal motion). The problem of stabilization together with an optimization problem for such a system is considered. Algorithm (based on the bionic model) which provides the effective procedure for finding of optimal parameters is presented and applied to considered system. Phase portraits and dynamics of the Lyapunov function are also presented and discussed.', 'corpus_id': 124573144, 'score': 0}, {'doc_id': '237492896', 'title': 'The Effect of Explanations on Trust in an Assistance System for Public Transport Users and the Role of the Propensity to Trust', 'abstract': 'The present study aimed to investigate whether explanations increase trust in an assistance system. Moreover, we wanted to take the role of the individual propensity to trust in technology into account. We conducted an empirical study in a virtual reality environment where 40 participants interacted with a specific assistance system for public transport users. The study was in a 2x2 mixed design with the within-subject factor assistance system feature (trip planner and connection request) and the between-subject factor explanation (with or without). We measured trust as explicit trust via a questionnaire and as implicit trust via an operationalization of the participants’ behavior. The results showed that trust propensity predicted explicit trust, and explanations increased explicit trust significantly. This was not the case for implicit trust, though, suggesting that explicit and implicit trust do not necessarily coincide. In conclusion, our results complement the literature on explainable artificial intelligence and trust in automation and provide topics for future research regarding the effect of explanations on trust in assistance systems or other technologies.', 'corpus_id': 237492896, 'score': 1}, {'doc_id': '7992649', 'title': 'Geometric and physical modelling in medical image processing: methods, applications and examples', 'abstract': 'Geometrical and physical modelling is of increasing importance in medical image processing. Geometric models are typically used in segmentation and registration methods to support image visualization and the extraction of measurements from images. Increasingly more, image derived geometrical models are the input for a more refined analysis using physical models. An example is the analysis of blood flow patterns in the heart or in aneurysms with CFD simulations.', 'corpus_id': 7992649, 'score': 0}]"
153	ML+Fire 2	09f0f579e0d237ba2f28e379f99f3f87	5553	{}	[{'doc_id': '213883397', 'title': 'Combustion stability monitoring through flame imaging and stacked sparse autoencoder based deep neural network', 'abstract': 'Combustion instability is a well-known problem in the combustion processes and closely linked to lower combustion efficiency and higher pollutant emissions. Therefore, it is important to monitor combustion stability for optimizing efficiency and maintaining furnace safety. However, it is difficult to establish a robust monitoring model with high precision through traditional data-driven methods, where prior knowledge of labeled data is required. This study proposes a novel approach for combustion stability monitoring through stacked sparse autoencoder based deep neural network. The proposed stacked sparse autoencoder is firstly utilized to extract flame representative features from the unlabeled images, and an improved loss function is used to enhance the training efficiency. The extracted features are then used to identify the classification label and stability index through clustering and statistical analysis. Classification and regression models incorporating the stacked sparse autoencoder are established for the qualitative and quantitative characterization of combustion stability. Experiments were carried out on a gas combustor to establish and evaluate the proposed models. It has been found that the classification model provides an F1-score of 0.99, whilst the R-squared of 0.98 is achieved through the regression model. Results obtained from the experiments demonstrated that the stacked sparse autoencoder model is capable of extracting flame representative features automatically without having manual interference. The results also show that the proposed model provides a higher prediction accuracy in comparison to the traditional data-driven methods and also demonstrates as a promising tool for monitoring the combustion stability accurately.', 'corpus_id': 213883397, 'score': 1}, {'doc_id': '218665373', 'title': 'Temperate Fish Detection and Classification: a Deep Learning based Approach', 'abstract': 'A wide range of applications in marine ecology extensively uses underwater cameras. Still, to efficiently process the vast amount of data generated, we need to develop tools that can automatically detect and recognize species captured on film. Classifying fish species from videos and images in natural environments can be challenging because of noise and variation in illumination and the surrounding habitat. In this paper, we propose a two-step deep learning approach for the detection and classification of temperate fishes without pre-filtering. The first step is to detect each single fish in an image, independent of species and sex. For this purpose, we employ the You Only Look Once (YOLO) object detection technique. In the second step, we adopt a Convolutional Neural Network (CNN) with the Squeeze-and-Excitation (SE) architecture for classifying each fish in the image without pre-filtering. We apply transfer learning to overcome the limited training samples of temperate fishes and to improve the accuracy of the classification. This is done by training the object detection model with ImageNet and the fish classifier via a public dataset (Fish4Knowledge), whereupon both the object detection and classifier are updated with temperate fishes of interest. The weights obtained from pre-training are applied to post-training as a priori. Our solution achieves the state-of-the-art accuracy of 99.27\\% on the pre-training. The percentage values for accuracy on the post-training are good; 83.68\\% and 87.74\\% with and without image augmentation, respectively, indicating that the solution is viable with a more extensive dataset.', 'corpus_id': 218665373, 'score': 0}, {'doc_id': '219176719', 'title': 'Thermal Object Detection using Domain Adaptation through Style Consistency', 'abstract': 'A recent fatal accident of an autonomous vehicle opens a debate about the use of infrared technology in the sensor suite for autonomous driving to increase visibility for robust object detection. Thermal imaging has an advantage over lidar, radar, and camera because it can detect the heat difference emitted by objects in the infrared spectrum. In contrast, lidar and camera capture in the visible spectrum, and adverse weather conditions can impact their accuracy. The limitations of object detection in images from conventional imaging sensors can be catered to by thermal images. This paper presents a domain adaptation method for object detection in thermal images. We explore multiple ideas of domain adaption. First, a generative adversarial network is used to transfer the low-level features from the visible spectrum to the infrared spectrum domain through style consistency. Second, a cross-domain model with style consistency is used for object detection in the infrared spectrum by transferring the trained visible spectrum model. The proposed strategies are evaluated on publicly available thermal image datasets (FLIR ADAS and KAIST Multi-Spectral). We find that adapting the low-level features from the source domain to the target domain through domain adaptation increases in mean average precision by approximately 10%.', 'corpus_id': 219176719, 'score': 0}, {'doc_id': '218595824', 'title': 'Hyperspectral Images Classification Based on Multi-scale Residual Network', 'abstract': 'Because hyperspectral remote sensing images contain a lot of redundant information and the data structure is highly non-linear, leading to low classification accuracy of traditional machine learning methods. The latest research shows that hyperspectral image classification based on deep convolutional neural network has high accuracy. However, when a small amount of data is used for training, the classification accuracy of deep learning methods is greatly reduced. In order to solve the problem of low classification accuracy of existing algorithms on small samples of hyperspectral images, a multi-scale residual network is proposed. The multi-scale extraction and fusion of spatial and spectral features is realized by adding a branch structure into the residual block and using convolution kernels of different sizes in the branch. The spatial and spectral information contained in hyperspectral images are fully utilized to improve the classification accuracy. In addition, in order to improve the speed and prevent overfitting, the model uses dynamic learning rate, BN and Dropout strategies. The experimental results show that the overall classification accuracy of this method is 99.07% and 99.96% respectively in the data set of Indian Pines and Pavia University, which is better than other algorithms.', 'corpus_id': 218595824, 'score': 0}, {'doc_id': '218613728', 'title': 'RISE Video Dataset: Recognizing Industrial Smoke Emissions', 'abstract': 'Industrial smoke emissions pose a significant concern to human health. Prior works have shown that using Computer Vision (CV) techniques to identify smoke as visual evidence can influence the attitude of regulators and empower citizens in pursuing environmental justice. However, existing datasets do not have sufficient quality nor quantity for training robust CV models to support air quality advocacy. We introduce RISE, the first large-scale video dataset for Recognizing Industrial Smoke Emissions. We adopt the citizen science approach to collaborate with local community members in annotating whether a video clip has smoke emissions. Our dataset contains 12,567 clips with 19 distinct views from cameras on three sites that monitored three different industrial facilities. The clips are from 30 days that spans four seasons in two years in the daytime. We run experiments using deep neural networks developed for video action recognition to establish a performance baseline and reveal the challenges for smoke recognition. Our data analysis also shows opportunities for integrating citizen scientists and crowd workers into the application of Artificial Intelligence for social good.', 'corpus_id': 218613728, 'score': 1}, {'doc_id': '49195215', 'title': 'Flame detection using deep learning', 'abstract': 'Flame detection is an increasingly important issue in intelligent surveillance. In fire flame detection, we need to extract visual features from video frames for training and test. Based on them, a group of shallow learning models have been developed to detect flames, such as color-based model, fuzzy-based model, motion and shape-based model, etc. Deep learning is a novel method which could be much efficient and accurate in flame detection. In this paper, we use YOLO model to implement flame detection and compare it with those shallow learning methods so as to determine the most efficient one for flame detection. Our contribution of this paper is to make use of the optimized YOLO model for flame detection from video frames. We collected the dataset and trained them using Google platform TensorFlow, the obtained accuracy of our proposed flame detection is up to 76%.', 'corpus_id': 49195215, 'score': 1}, {'doc_id': '201878752', 'title': 'Combustion Condition Monitoring Through Deep Learning Networks', 'abstract': 'Combustion condition monitoring is essential in a power plant for maintaining stable operations and operational safety. Therefore it is crucial to develop an intelligent combustion monitoring system. Existing traditional methods not only need a large quantity of labeled data but also require rebuilding monitoring model for new conditions. Aiming these problems, the present study proposes a novel approach combining denoising auto-encoder (DAE) and generative adversarial network (GAN) to monitor combustion condition. By using the learning mechanism of the GAN, the robust feature extraction ability of DAE as a generator is improved. These features are then fed into the Gaussian process classifier (GPC) for condition identification. Especially, newly occurring conditions can be correctly classified by simply training the GPC, rather than training from scratch. Experiments performed on a gaseous combustor indicate that the proposed approach can extract representative features accurately and achieve high performance in combustion condition monitoring with the accuracy of 98.5% for original conditions and 97.8% for the new conditions.', 'corpus_id': 201878752, 'score': 1}, {'doc_id': '214596388', 'title': 'Lightweight and efficient octave convolutional neural network for fire recognition', 'abstract': 'Fire recognition from visual scenes is a demanding task due to the high variance of color and texture. In recent years, several fire-recognition approaches based on deep learning methods have been proposed to overcome this problem. However, building deep convolutional neural networks usually involves hundreds of layers and thousands of channels, thus requiring excessive computational cost, and a considerable amount of data. Therefore, applying deep networks in real-world scenarios remains an open challenge, especially when using devices with limitations in hardware and computing power, e.g., robots or mobile devices. To address this challenge, in this paper, we propose a lightweight and efficient octave convolutional neural network for fire recognition in visual scenes. Extensive experiments are conducted on FireSense, CairFire, FireNet, and FiSmo datasets. In overall, our architecture comprises fewer layers and fewer parameters in comparison with previously proposed architectures. Experimental results show that our model achieves higher accuracy recognition, in comparison to state-of-the-art methods, for all tested datasets.', 'corpus_id': 214596388, 'score': 1}, {'doc_id': '219304063', 'title': 'DFR-TSD: A Deep Learning Based Framework for Robust Traffic Sign Detection Under Challenging Weather Conditions', 'abstract': 'Robust traffic sign detection and recognition (TSDR) is of paramount importance for the successful realization of autonomous vehicle technology. The importance of this task has led to a vast amount of research efforts and many promising methods have been proposed in the existing literature. However, the SOTA (SOTA) methods have been evaluated on clean and challenge-free datasets and overlooked the performance deterioration associated with different challenging conditions (CCs) that obscure the traffic images captured in the wild. In this paper, we look at the TSDR problem under CCs and focus on the performance degradation associated with them. To overcome this, we propose a Convolutional Neural Network (CNN) based TSDR framework with prior enhancement. Our modular approach consists of a CNN-based challenge classifier, Enhance-Net, an encoder-decoder CNN architecture for image enhancement, and two separate CNN architectures for sign-detection and classification. We propose a novel training pipeline for Enhance-Net that focuses on the enhancement of the traffic sign regions (instead of the whole image) in the challenging images subject to their accurate detection. We used CURE-TSD dataset consisting of traffic videos captured under different CCs to evaluate the efficacy of our approach. We experimentally show that our method obtains an overall precision and recall of 91.1% and 70.71% that is 7.58% and 35.90% improvement in precision and recall, respectively, compared to the current benchmark. Furthermore, we compare our approach with SOTA object detection networks, Faster-RCNN and R-FCN, and show that our approach outperforms them by a large margin.', 'corpus_id': 219304063, 'score': 0}, {'doc_id': '216553545', 'title': 'Hyperspectral image classification based on multi-scale residual network with attention mechanism', 'abstract': None, 'corpus_id': 216553545, 'score': 0}]
154	Power Quality 1	cfc5d8bc5d5a61074633a4418f1836b4	19395	{}	"[{'doc_id': '3051004', 'title': 'Study of the effect of harmonics on measurments of the energy meters', 'abstract': ""An investigation of the effect of voltage and current harmonics on the accuracy of readings of energy meters was carried out. Harmonic distortion is one of the power quality problems, which typically arises from the consumer's electrical equipment. Such harmonics produced by discontinuous conducting devices and non-linear loads create recognized problems for most power distribution systems. These problems can lead to over heating and power losses of power system distribution components. Such conditions involved differences in voltage and current magnitudes as well as differences in the voltage and current total harmonic distortion levels. This study presents comparison between the electromechanical energy meters and the electronic digital energy meters. The study also includes the accuracy of readings when the meters are subjected to the same non-linear similar conditions. The harmonics were measured using VIP system III illustrating the THD of voltage and current when power factor compensating capacitors were used or not. It was found that the accuracy and the sensitivity as well as the precision of digital electronic meters are higher than that of the electromechanical one. When using both meters to read the energy consumed by the loads, it was noticed that there is a difference between readings of the two compared types as the digital electronic meter has a difference of (± 0.5% to 1%) which is a considerable amount of energy for large consumers. An economical assessment was made and showed that the digital meter is saving about L.E 1.8 to 3.6 million per annum for the distribution company. It is recommended to distribution companies to use digital meters instead of electromechanical ones especially for large and medium consumers."", 'corpus_id': 3051004, 'score': 1}, {'doc_id': '236441483', 'title': 'Effect of K-Factor on Capability in Power Transformers', 'abstract': 'Harmonic currents generated by nonlinear loads can cause overheating and premature failure of power transformers. According to IEEE Std C57.110TM - 2008, Eddy Current losses are considered proportional to the harmonic current squared multiplied by the harmonic number. This paper will discuss 2 case studies, namely: a transformer without using a harmonic filter and a transformer using a harmonic filter. This research was conducted by measuring the amount of harmonics in arc furnace customers using power quality analysis equipment for 7 days. From the research results obtained indicate that the value of the k-factor is inversely proportional to the maximum transformer capability and is directly proportional to the decrease in transformer capability. So that the transformer that is installed using a harmonic filter has a k-factor value and derating capability is smaller than the transformer without using a harmonic filter, in order not to derating capability of the transformer without using a harmonic filter or a transformer that uses a harmonic filter, it is necessary to use the K-Factor Transformer K-4.', 'corpus_id': 236441483, 'score': 0}, {'doc_id': '236918930', 'title': 'Research on Harmonic Traceability Method of Reference Meter Based on Power Comparison', 'abstract': 'The harmonics generated by a large number of power electronic equipment in the power grid will seriously affect the measurement accuracy of electric energy measuring instruments. Therefore, in order to determine the anti-harmonic interference performance of electric energy meters, harmonic influence tests are required in the national standards of electric energy meters and the type evaluation outline. As the main standard device used in the harmonic impact test of the electric energy meter, the reference meter has passed the measurement verification, but its verification items do not include the detection of the harmonic measurement performance. In order to ensure the validity of the harmonic impact test of the electric energy meter, it is necessary to test the harmonic measurement performance of the reference meter. This paper designs a set of measurement and detection schemes that uses power comparison method to trace the source of harmonics of reference meters. It has important reference value for improving the accuracy of harmonic measurement and the reliability of electric energy meter type test.', 'corpus_id': 236918930, 'score': 1}, {'doc_id': '236940201', 'title': 'Power Quality Analyzers Calibration on Total Harmonics Distortion of Voltage and Current by Reference Square Waveform Signal', 'abstract': 'For calibrating the Power Quality Analyzer by Total Harmonic Distortion of voltage and current, a calibration approach is proposed, by using a reference square waveform signal. A mathematical model for Total Harmonic Distortion calibration is presented and an algorithm is implemented for control the calibration process, collection and processing of results. The experimental results obtained from the performed Total Harmonic Distortion calibrations, confirm the applicability of the proposed approach.', 'corpus_id': 236940201, 'score': 1}, {'doc_id': '159436939', 'title': 'Energy meters evolution in smart grids: A review', 'abstract': 'Abstract Intelligent Energy Networks are comprised of devices capable of fulfilling their functions in an energy-efficient fashion and with communication and remote control capabilities. Therefore, some of these devices, such as smart energy meters, become attractive for use in the power generation and distribution industry, achieving the vision of Smart Grids. However, many are the challenges that need to be overcome in order to reach a fully-functional and security-aware smart grid. Providing measurement, control, communication, power, display, and synchronization capabilities shall be no easy task for smart meters. In this context, this paper elaborates on a detailed description of the main functionalities that smart meters must provide, along with the analysis of existing solutions that make use of smart meters for smart grids. Moreover, open challenges in the topic are identified and discussed. By the end of this research piece, the reader should be able to have a detailed view of the capabilities already offered by smart meters and the ones they will have available in order to tackle the challenges smart grids present.', 'corpus_id': 159436939, 'score': 1}, {'doc_id': '235755492', 'title': 'Cellular, Wide-Area, and Non-Terrestrial IoT: A Survey on 5G Advances and the Road Towards 6G', 'abstract': 'The next wave of wireless technologies is proliferating in connecting things among themselves as well as to humans. In the era of the Internet of things (IoT), billions of sensors, machines, vehicles, drones, and robots will be connected, making the world around us smarter. The IoT will encompass devices that must wirelessly communicate a diverse set of data gathered from the environment for myriad new applications. The ultimate goal is to extract insights from this data and develop solutions that improve quality of life and generate new revenue. Providing large-scale, long-lasting, reliable, and near real-time connectivity is the major challenge in enabling a smart connected world. This paper provides a comprehensive survey on existing and emerging communication solutions for serving IoT applications in the context of cellular, wide-area, as well as non-terrestrial networks. Specifically, wireless technology enhancements for providing IoT access in fifth-generation (5G) and beyond cellular networks, and communication networks over the unlicensed spectrum are presented. Aligned with the main key performance indicators of 5G and beyond 5G networks, we investigate solutions and standards that enable energy efficiency, reliability, low latency, and scalability (connection density) of current and future IoT networks. The solutions include grant-free access and channel coding for short-packet communications, nonorthogonal multiple access, and on-device intelligence. Further, a vision of new paradigm shifts in communication networks in the 2030s is provided, and the integration of the associated new technologies like artificial intelligence, non-terrestrial networks, and new spectra is elaborated. In particular, the potential of using emerging deep learning and federated learning techniques for enhancing the efficiency and security of IoT communication are discussed, and their promises and challenges are introduced. Finally, future research directions toward beyond 5G IoT networks are pointed out.', 'corpus_id': 235755492, 'score': 0}, {'doc_id': '235810502', 'title': 'Fuzzy logic , PI and ANN in improvement of power quality using unified Power quality conditioner', 'abstract': ""Introduction With the advent of power semiconductor switching devices, like thyristors, GTO's (Gate Turn off thyristors), IGBT's (Insulated Gate Bipolar Transistors) and many more devices, control of electric power has become a reality. Such power electronic controllers are widely used to feed electric power to electrical loads, such as adjustable speed drives (ASD's), furnaces, computer power supplies, HVDC systems etc. The power electronic devices due to their inherent nonlinearity draw harmonic and reactive power from the supply. In three phase systems, they could also cause unbalance and draw excessive neutral currents. The injected harmonics, reactive power burden, unbalance, and excessive neutral currents cause low system efficiency and poor power factor. In addition to this, the power system is subjected to various transients like voltage sags, swells, flickers etc. These transients would affect the voltage at distribution levels. Excessive reactive power of loads would increase the generating capacity of generating stations and increase the transmission losses in lines. Hence supply of reactive power at the load ends becomes essential. Power Quality (PQ) has become an important issue since many loads at various distribution ends like adjustable speed drives, process industries, printers, domestic utilities, computers, microprocessor based equipments etc. have become intolerant to voltage fluctuations, harmonic content and interruptions. Power Quality (PQ) mainly deals with issues like maintaining a fixed voltage at the Point of Common Coupling (PCC) for various distribution voltage levels irrespective of voltage fluctuations, maintaining near unity power factor power drawn from the supply, blocking of voltage and current unbalance from passing upwards from various distribution levels, reduction of voltage and current harmonics in the system and suppression of excessive supply neutral current. Conventionally, passive LC filters and fixed compensating devices with some degree of variation like thyristor switched capacitors, thyristor switched reactors were employed to improve the power factor of ac loads. Such devices have the demerits of fixed compensation, large size, ageing and resonance. Nowadays equipments using power semiconductor devices, generally known as active power filters (APF's), Active Power Line Conditioners (APLC's) etc. are used for the power quality issues due to their dynamic and adjustable solutions. Flexible AC Transmission Systems (FACTS) and Custom Power products like STATCOM synchronous COMpensator), DVR (Dynamic voltage Restorer), etc. deal with the issues related to power quality using similar control strategies and concepts. Basically, they are different only in the location in a power system where they are deployed and the objectives for which they are deployed. UPQC: Active Power Filters can be classified, based on converter type, topology and the number of phases. Converter types are Current Source Inverter (CSI) with inductive energy storage or Voltage Source Inverter (VSI) with capacitive energy storage. The topology can be shunt, series or combination of both. The third classification is based on the number of phases, such as single phase systems, three phase systems or three phase four wire systems. The Objective of this paper, one such APLC known as Unified Power Quality Conditioner (UPQC), which can be used at the PCC for improving power quality, is designed, simulated using proposed control strategy and the performance is evaluated for various nonlinear loads (steel plant loads). Unified Power Quality Conditioner (UPQC) using PLL with PWM Control is discussed and simulated. Case study of a typical steel plant has been given. Simulated proposed UPQC for various non-linear loads of steel plant and results with installation of STATCOM and UPQC are reported."", 'corpus_id': 235810502, 'score': 0}, {'doc_id': '235614032', 'title': 'Smarter Grid in the 5G Era: A Framework Integrating Power Internet of Things With a Cyber Physical System', 'abstract': 'As the energy infrastructure of smart cities, smart grid upgrades traditional power grid systems with state-of-the-art information and communication technologies. In particular, as the full deployment of the Internet of Things in the power grid (a.k.a. power Internet of Things or PIoT), the newly introduced information flow together with inherent energy flow makes it more efficient for power generation, transmission, distribution, and consumption. To further exploit the precious energy and the latest 5G technologies, this article boosts to add a value flow in the smart grid, mainly including the value created by innovative services and market mechanisms and the value added by the information flow. Specifically, by integrating PIoT with cyber-physical systems, this article sketches a conceptual framework of the cyber-physical power system (CPPS). The CPPS carries out holistic perception and ubiquitous connection of distributed energy sources and electrical facilities and builds up a smarter power grid with global information interaction, intelligent decision-making, and real-time agile control. Finally, for illustration purposes, we conduct a case study regarding an intelligent home management system.', 'corpus_id': 235614032, 'score': 0}, {'doc_id': '30523191', 'title': 'Analysis of electricity meters under distorted load conditions', 'abstract': ""Due to the rapid growth of non-linear loads in power systems in recent decades, harmonic pollution is becoming more and more serious. Energy measurement devices are commonly designed for working at sine wave. In this paper the results of an analysis on different electricity meters used in distribution network under different load conditions are presented. Study shows that error of energy meter's reading depends on the type of the meter, harmonic distortion, reactive energy and direction of reactive power. Measured errors were higher than 6% under some conditions."", 'corpus_id': 30523191, 'score': 1}, {'doc_id': '235824045', 'title': 'Unified Meter for Electricity, Gas and Water with Automatic Billing and Payment', 'abstract': 'Every month it is seen that there are three bills generated for the consumption of the basic necessities like electricity, gas and water we use on a daily basis. The proposed system measures the consumption of all these resources and provides a unified billing and payment system for it. This has two sub-systems consisting of a sub-system for measurement and the server-controlled sub-system for data storage and analysis. Communication between these two subsystems is done through wireless network. This system monitors the consumption of electricity, water and gasoline resources. The amount of resources consumed is updated in the server, and a consolidated bill is generated. Every user will be provided a mobile application where the billing details are updated on a monthly basis with a portal for payment. This process is achieved by using the concept of embedded system and IoT.', 'corpus_id': 235824045, 'score': 0}]"
155	Scanner Harmonization	7f2d5a02a721258788dec24469474d12	1423	{}	"[{'doc_id': '212658055', 'title': 'Fine-grain atlases of functional modes for fMRI analysis', 'abstract': 'Population imaging markedly increased the size of functional-imaging datasets, shedding new light on the neural basis of inter-individual differences. Analyzing these large data entails new scalability challenges, computational and statistical. For this reason, brain images are typically summarized in a few signals, for instance reducing voxel-level measures with brain atlases or functional modes. A good choice of the corresponding brain networks is important, as most data analyses start from these reduced signals. We contribute finely-resolved atlases of functional modes, comprising from 64 to 1024 networks. These dictionaries of functional modes (DiFuMo) are trained on millions of fMRI functional brain volumes of total size 2.4TB, spanned over 27 studies and many research groups. We demonstrate the benefits of extracting reduced signals on our fine-grain atlases for many classic functional data analysis pipelines: stimuli decoding from 12,334 brain responses, standard GLM analysis of fMRI across sessions and individuals, extraction of resting-state functional-connectomes biomarkers for 2,500 individuals, data compression and meta-analysis over more than 15,000 statistical maps. In each of these analysis scenarii, we compare the performance of our functional atlases with that of other popular references, and to a simple voxel-level analysis. Results highlight the importance of using high-dimensional ""soft"" functional atlases, to represent and analyse brain activity while capturing its functional gradients. Analyses on high-dimensional modes achieve similar statistical performance as at the voxel level, but with much reduced computational cost and higher interpretability. In addition to making them available, we provide meaningful names for these modes, based on their anatomical location. It will facilitate reporting of results.', 'corpus_id': 212658055, 'score': 0}, {'doc_id': '211020980', 'title': 'Improved inter-scanner MS lesion segmentation by adversarial training on longitudinal data', 'abstract': 'The evaluation of white matter lesion progression is an important biomarker in the follow-up of MS patients and plays a crucial role when deciding the course of treatment. Current automated lesion segmentation algorithms are susceptible to variability in image characteristics related to MRI scanner or protocol differences. We propose a model that improves the consistency of MS lesion segmentations in inter-scanner studies. First, we train a CNN base model to approximate the performance of icobrain, an FDA-approved clinically available lesion segmentation software. A discriminator model is then trained to predict if two lesion segmentations are based on scans acquired using the same scanner type or not, achieving a 78% accuracy in this task. Finally, the base model and the discriminator are trained adversarially on multi-scanner longitudinal data to improve the inter-scanner consistency of the base model. The performance of the models is evaluated on an unseen dataset containing manual delineations. The inter-scanner variability is evaluated on test-retest data, where the adversarial network produces improved results over the base model and the FDA-approved solution.', 'corpus_id': 211020980, 'score': 1}, {'doc_id': '8087199', 'title': 'Reliability in multi-site structural MRI studies: Effects of gradient non-linearity correction on phantom and human data', 'abstract': 'Longitudinal and multi-site clinical studies create the imperative to characterize and correct technological sources of variance that limit image reproducibility in high-resolution structural MRI studies, thus facilitating precise, quantitative, platform-independent, multi-site evaluation. In this work, we investigated the effects that imaging gradient non-linearity have on reproducibility of multi-site human MRI. We applied an image distortion correction method based on spherical harmonics description of the gradients and verified the accuracy of the method using phantom data. The correction method was then applied to the brain image data from a group of subjects scanned twice at multiple sites having different 1.5 T platforms. Within-site and across-site variability of the image data was assessed by evaluating voxel-based image intensity reproducibility. The image intensity reproducibility of the human brain data was significantly improved with distortion correction, suggesting that this method may offer improved reproducibility in morphometry studies. We provide the source code for the gradient distortion algorithm together with the phantom data.', 'corpus_id': 8087199, 'score': 1}, {'doc_id': '3682932', 'title': 'Harmonization of cortical thickness measurements across scanners and sites', 'abstract': '&NA; With the proliferation of multi‐site neuroimaging studies, there is a greater need for handling non‐biological variance introduced by differences in MRI scanners and acquisition protocols. Such unwanted sources of variation, which we refer to as “scanner effects”, can hinder the detection of imaging features associated with clinical covariates of interest and cause spurious findings. In this paper, we investigate scanner effects in two large multi‐site studies on cortical thickness measurements across a total of 11 scanners. We propose a set of tools for visualizing and identifying scanner effects that are generalizable to other modalities. We then propose to use ComBat, a technique adopted from the genomics literature and recently applied to diffusion tensor imaging data, to combine and harmonize cortical thickness values across scanners. We show that ComBat removes unwanted sources of scan variability while simultaneously increasing the power and reproducibility of subsequent statistical analyses. We also show that ComBat is useful for combining imaging data with the goal of studying life‐span trajectories in the brain. HighlightsCortical thickness (CT) measurements are highly scanner specific.Identifying scanner effects is crucial for inference and biomarker development.We propose to use ComBat to harmonize cortical thickness values across scanners.', 'corpus_id': 3682932, 'score': 1}, {'doc_id': '211818317', 'title': 'Explainable and Scalable Machine-Learning Algorithms for Detection of Autism Spectrum Disorder using fMRI Data', 'abstract': 'Diagnosing Autism Spectrum Disorder (ASD) is a challenging problem, and is based purely on behavioral descriptions of symptomology (DSM-5/ICD-10), and requires informants to observe children with disorder across different settings (e.g. home, school). Numerous limitations (e.g., informant discrepancies, lack of adherence to assessment guidelines, informant biases) to current diagnostic practices have the potential to result in over-, under-, or misdiagnosis of the disorder. Advances in neuroimaging technologies are providing a critical step towards a more objective assessment of the disorder. Prior research provides strong evidence that structural and functional magnetic resonance imaging (MRI) data collected from individuals with ASD exhibit distinguishing characteristics that differ in local and global spatial, and temporal neural-patterns of the brain. Our proposed deep-learning model ASD-DiagNet exhibits consistently high accuracy for classification of ASD brain scans from neurotypical scans. We have for the first time integrated traditional machine-learning and deep-learning techniques that allows us to isolate ASD biomarkers from MRI data sets. Our method, called Auto-ASD-Network, uses a combination of deep-learning and Support Vector Machines (SVM) to classify ASD scans from neurotypical scans. Such interpretable models would help explain the decisions made by deep-learning techniques leading to knowledge discovery for neuroscientists, and transparent analysis for clinicians.', 'corpus_id': 211818317, 'score': 1}, {'doc_id': '212726155', 'title': 'Image Quality Transfer Enhances Contrast and Resolution of Low-Field Brain MRI in African Paediatric Epilepsy Patients', 'abstract': '1.5T or 3T scanners are the current standard for clinical MRI, but low-field (<1T) scanners are still common in many lower- and middle-income countries for reasons of cost and robustness to power failures. Compared to modern high-field scanners, low-field scanners provide images with lower signal-to-noise ratio at equivalent resolution, leaving practitioners to compensate by using large slice thickness and incomplete spatial coverage. Furthermore, the contrast between different types of brain tissue may be substantially reduced even at equal signal-to-noise ratio, which limits diagnostic value. Recently the paradigm of Image Quality Transfer has been applied to enhance 0.36T structural images aiming to approximate the resolution, spatial coverage, and contrast of typical 1.5T or 3T images. A variant of the neural network U-Net was trained using low-field images simulated from the publicly available 3T Human Connectome Project dataset. Here we present qualitative results from real and simulated clinical low-field brain images showing the potential value of IQT to enhance the clinical utility of readily accessible low-field MRIs in the management of epilepsy.', 'corpus_id': 212726155, 'score': 0}, {'doc_id': '209500678', 'title': 'Statistical agnostic mapping: a framework in neuroimaging based on concentration inequalities', 'abstract': 'In the 70s a novel branch of statistics emerged focusing its effort in selecting a function in the pattern recognition problem, which fulfils a definite relationship between the quality of the approximation and its complexity. These data-driven approaches are mainly devoted to problems of estimating dependencies with limited sample sizes and comprise all the empirical out-of sample generalization approaches, e.g. cross validation (CV) approaches. Although the latter are not designed for testing competing hypothesis or comparing different models in neuroimaging, there are a number of theoretical developments within this theory which could be employed to derive a Statistical Agnostic (non-parametric) Mapping (SAM) at voxel or multi-voxel level. Moreover, SAMs could relieve i) the problem of instability in limited sample sizes when estimating the actual risk via the CV approaches, e.g. large error bars, and provide ii) an alternative way of Family-wise-error (FWE) corrected p-value maps in inferential statistics for hypothesis testing. In this sense, we propose a novel framework in neuroimaging based on concentration inequalities, which results in (i) a rigorous development for model validation with a small sample/dimension ratio, and (ii) a less-conservative procedure than FWE p-value correction, to determine the brain significance maps from the inferences made using small upper bounds of the actual risk.', 'corpus_id': 209500678, 'score': 0}, {'doc_id': '9155849', 'title': 'Report on a multicenter fMRI quality assurance protocol', 'abstract': 'Temporal stability during an fMRI acquisition is very important because the blood oxygen level‐dependent (BOLD) effects of interest are only a few percent in magnitude. Also, studies involving the collection of groups of subjects over time require stable scanner performance over days, weeks, months, and even years. We describe a protocol designed by one of the authors that has been tested for several years within the context of a large, multicenter collaborative fMRI research project (FIRST‐BIRN). A full description of the phantom, the quality assurance (QA) protocol, and the several calculations used to measure performance is provided. The results obtained with this protocol at multiple sites over time are presented. These data can be used as benchmarks for other centers involved in fMRI research. Some issues with the various protocol measures are highlighted and discussed, and possible protocol improvements are also suggested. Overall, we expect that other fMRI centers will find this approach to QA useful and this report may facilitate developing a similar QA protocol locally. Based on the findings reported herein, the authors are convinced that monitoring QA in this way will improve the quality of fMRI data. J. Magn. Reson. Imaging 2006. © 2006 Wiley‐Liss, Inc.', 'corpus_id': 9155849, 'score': 1}, {'doc_id': '210920478', 'title': 'PIRACY: An Optimized Pipeline for Functional Connectivity Analysis in the Rat Brain', 'abstract': 'Resting state functional MRI (rs-fMRI) is a widespread and powerful tool for investigating functional connectivity (FC) and brain disorders. However, FC analysis can be seriously affected by random and structured noise from non-neural sources, such as physiology. Thus, it is essential to first reduce thermal noise and then correctly identify and remove non-neural artifacts from rs-fMRI signals through optimized data processing methods. However, existing tools that correct for these effects have been developed for human brain and are not readily transposable to rat data. Therefore, the aim of the present study was to establish a data processing pipeline that can robustly remove random and structured noise from rat rs-fMRI data. It includes a novel denoising approach based on the Marchenko-Pastur Principal Component Analysis (MP-PCA) method, FMRIB’s ICA-based Xnoiseifier (FIX) for automatic artifact classification and cleaning, and global signal regression (GSR). Our results show that: (I) MP-PCA denoising substantially improves the temporal signal-to-noise ratio, (II) the pre-trained FIX classifier achieves a high accuracy in artifact classification, and (III) both independent component analysis (ICA) cleaning and GSR are essential steps in correcting for possible artifacts and minimizing the within-group variability in control animals while maintaining typical connectivity patterns. Reduced within-group variability also facilitates the exploration of potential between-group FC changes, as illustrated here in a rat model of sporadic Alzheimer’s disease.', 'corpus_id': 210920478, 'score': 0}, {'doc_id': '49642502', 'title': 'Statistical harmonization corrects site effects in functional connectivity measurements from multi‐site fMRI data', 'abstract': 'Acquiring resting‐state functional magnetic resonance imaging (fMRI) datasets at multiple MRI scanners and clinical sites can improve statistical power and generalizability of results. However, multi‐site neuroimaging studies have reported considerable nonbiological variability in fMRI measurements due to different scanner manufacturers and acquisition protocols. These undesirable sources of variability may limit power to detect effects of interest and may even result in erroneous findings. Until now, there has not been an approach that removes unwanted site effects. In this study, using a relatively large multi‐site (4 sites) fMRI dataset, we investigated the impact of site effects on functional connectivity and network measures estimated by widely used connectivity metrics and brain parcellations. The protocols and image acquisition of the dataset used in this study had been homogenized using identical MRI phantom acquisitions from each of the neuroimaging sites; however, intersite acquisition effects were not completely eliminated. Indeed, in this study, we found that the magnitude of site effects depended on the choice of connectivity metric and brain atlas. Therefore, to further remove site effects, we applied ComBat, a harmonization technique previously shown to eliminate site effects in multi‐site diffusion tensor imaging (DTI) and cortical thickness studies. In the current work, ComBat successfully removed site effects identified in connectivity and network measures and increased the power to detect age associations when using optimal combinations of connectivity metrics and brain atlases. Our proposed ComBat harmonization approach for fMRI‐derived connectivity measures facilitates reliable and efficient analysis of retrospective and prospective multi‐site fMRI neuroimaging studies.', 'corpus_id': 49642502, 'score': 1}]"
156	PLB Feed_1	a6ddbcd9da5aafaa4284d9b3e18c595f	6087	{'PLB': 'phospholamban'}	"[{'doc_id': '118486016', 'title': 'Challenges to Self-Acceleration in Modified Gravity from Gravitational Waves and Large-Scale Structure', 'abstract': 'Abstract With the advent of gravitational-wave astronomy marked by the aLIGO GW150914 and GW151226 observations, a measurement of the cosmological speed of gravity will likely soon be realised. We show that a confirmation of equality to the speed of light as indicated by indirect Galactic observations will have important consequences for a very large class of alternative explanations of the late-time accelerated expansion of our Universe. It will break the dark degeneracy of self-accelerated Horndeski scalar–tensor theories in the large-scale structure that currently limits a rigorous discrimination between acceleration from modified gravity and from a cosmological constant or dark energy. Signatures of a self-acceleration must then manifest in the linear, unscreened cosmological structure. We describe the minimal modification required for self-acceleration with standard gravitational-wave speed and show that its maximum likelihood yields a 3σ poorer fit to cosmological observations compared to a cosmological constant. Hence, equality between the speeds challenges the concept of cosmic acceleration from a genuine scalar–tensor modification of gravity.', 'corpus_id': 118486016, 'score': 1}, {'doc_id': '220244781', 'title': 'Temperature and Humidity Do Not Influence Global COVID-19 Incidence as Inferred from Causal Models', 'abstract': 'The relationship between meteorological factors such as temperature and humidity with COVID-19 incidence is still unclear after 6 months of the beginning of the pandemic. Some literature confirms the association of temperature with disease transmission while some oppose the same. This work intends to determine whether there is a causal association between temperature, humidity and Covid-19 cases. Three different causal models were used to capture stochastic, chaotic and symbolic natured time-series data and to provide a robust & unbiased analysis by constructing networks of causal relationships between the variables. Granger-Causality method, Transfer Entropy method & Convergent Cross-Mapping (CCM) was done on data from regions with different temperatures and cases greater than 50,000 as of 13th May 2020. From the Granger-Causality test we found that in only Canada, the United Kingdom, temperature and daily new infections are causally linked. The same results were obtained from Convergent Cross Mapping for India. Again using Granger-Causality test, we found that in Russia only, relative humidity is causally linked to daily new cases. Thus, a Generalized Additive Model with a smoothing spline function was fitted for these countries to understand the directionality. Using the combined results of the said models, we were able to conclude that there is no evidence of a causal association between temperature, humidity and Covid-19 cases.', 'corpus_id': 220244781, 'score': 0}, {'doc_id': '119448713', 'title': 'Status of neutrino oscillations 2018: 3σ hint for normal mass ordering and improved CP sensitivity', 'abstract': 'Abstract We present a new global fit of neutrino oscillation parameters within the simplest three-neutrino picture, including new data which appeared since our previous analysis [1] . In this update we include new long-baseline neutrino data involving the antineutrino channel in T2K, as well as new data in the neutrino channel, data from NOνA, as well as new reactor data, such as the Daya Bay 1230 days electron antineutrino disappearance spectrum data and the 1500 live days prompt spectrum from RENO, as well as new Double Chooz data. We also include atmospheric neutrino data from the IceCube DeepCore and ANTARES neutrino telescopes and from Super-Kamiokande. Finally, we also update our solar oscillation analysis by including the 2055-day day/night spectrum from the fourth phase of the Super-Kamiokande experiment. With the new data we find a preference for the atmospheric angle in the upper octant for both neutrino mass orderings, with maximal mixing allowed at Δ χ 2 = 1.6 ( 3.2 ) for normal (inverted) ordering. We also obtain a strong preference for values of the CP phase δ in the range [ π , 2 π ] , excluding values close to π / 2 at more than 4σ. More remarkably, our global analysis shows a hint in favor of the normal mass ordering over the inverted one at more than 3σ. We discuss in detail the status of the mass ordering, CP violation and octant sensitivities, analyzing the interplay among the different neutrino data samples.', 'corpus_id': 119448713, 'score': 1}, {'doc_id': '119262604', 'title': 'Distance and de Sitter conjectures on the Swampland', 'abstract': ""Among Swampland conditions, the distance conjecture characterizes the geometry of scalar fields and the de Sitter conjecture constrains allowed potentials on it. We point out a connection between the distance conjecture and a refined version of the de Sitter conjecture in any parametrically controlled regime of string theory by using Bousso's covariant entropy bound. The refined version turns out to evade all counter-examples at scalar potential maxima that have been raised. We comment on the relation of our result to the Dine–Seiberg problem."", 'corpus_id': 119262604, 'score': 1}, {'doc_id': '119252887', 'title': 'On the cosmological implications of the string Swampland', 'abstract': 'Abstract We study constraints imposed by two proposed string Swampland criteria on cosmology. These criteria involve an upper bound on the range traversed by scalar fields as well as a lower bound on | ∇ ϕ V | / V when V > 0 . We find that inflationary models are generically in tension with these two criteria. Applying these same criteria to dark energy in the present epoch, we find that specific quintessence models can satisfy these bounds and, at the same time, satisfy current observational constraints. Assuming the two Swampland criteria are valid, we argue that the universe will undergo a phase transition within a few Hubble times. These criteria sharpen the motivation for future measurements of the tensor-to-scalar ratio r and the dark energy equation of state w, and for tests of the equivalence principle for dark matter.', 'corpus_id': 119252887, 'score': 1}, {'doc_id': '220152572', 'title': 'Ben ik duidelijk?', 'abstract': 'Verzorgende Nicolette Westerhof werkte drie weken in een team dat alleen mensen ondersteunde die mogelijk besmet waren met het coronavirus. Al tijdens haar eerste dienst merkte ze hoe moeilijk je verstaanbaar bent met een mondmasker, dus moest ze harder praten dan anders. ‘Ik voelde me net een maanmannetje. Ik benoemde bij de cliënten eerst altijd dat die kleding en afstand ook voor hen waarschijnlijk vreemd waren, maar dat het nu eenmaal nodig is. Iedereen reageerde begripvol.’ Over het voorkomen van verspreiding van het virus moest Westerhof sommige cliënten een en ander uitleggen. ‘Ik zag soms iemand zijn neus snuiten in een linnen zakdoek, om deze vervolgens terug in de broekzak te stoppen. Dan legde ik uit waarom dat de kans op besmetting met het virus vergroot. “Heel goed dat u zo vaak uw handen wast”, zei ik dan, “maar het virus is zo klein dat we het niet zien. Misschien zit het nu al op uw zakdoek, en straks weer op uw hand, de deurkruk of het lichtknopje enzovoort. En dan kan het u ziek maken, en misschien andere mensen ook wel.”’ Dus liever een papieren zakdoekje gebruiken en dit meteen weggooien, adviseerde Westerhof. De cliënten beloofden dat ze eraan zouden denken.', 'corpus_id': 220152572, 'score': 0}, {'doc_id': '31839440', 'title': 'Evidence for collectivity in pp collisions at the LHC', 'abstract': 'Measurements of two- and multi-particle angular correlations in pp collisions at View the MathML sources=5,7, and 13TeV are presented as a function of charged-particle multiplicity. The data, corresponding to integrated luminosities of View the MathML source1.0pb−1 (5\u2009TeV), View the MathML source6.2pb−1 (7\u2009TeV), and View the MathML source0.7pb−1 (13\u2009TeV), were collected using the CMS detector at the LHC. The second-order (v2v2) and third-order (v3v3) azimuthal anisotropy harmonics of unidentified charged particles, as well as v2v2 of View the MathML sourceKS0 and View the MathML sourceΛ/Λ‾ particles, are extracted from long-range two-particle correlations as functions of particle multiplicity and transverse momentum. For high-multiplicity pp events, a mass ordering is observed for the v2v2 values of charged hadrons (mostly pions), View the MathML sourceKS0, and View the MathML sourceΛ/Λ‾, with lighter particle species exhibiting a stronger azimuthal anisotropy signal below View the MathML sourcepT≈2GeV/c. For 13\u2009TeV data, the v2v2 signals are also extracted from four- and six-particle correlations for the first time in pp collisions, with comparable magnitude to those from two-particle correlations. These observations are similar to those seen in pPb and PbPb collisions, and support the interpretation of a collective origin for the observed long-range correlations in high-multiplicity pp collisions.', 'corpus_id': 31839440, 'score': 1}, {'doc_id': '220244953', 'title': 'Superspreading in Early Transmissions of COVID-19 in Indonesia', 'abstract': 'We estimate the basic reproduction number R0 and the overdispersion parameter K at two COVID-19 clusters in Indonesia: Jakarta-Depok and Batam. Based on the first 397 confirmed cases in both clusters, we find a high degree of individual-level variation in the transmission. The basic reproduction number R0 is estimated at 6.79 and 2.47, while the overdispersion parameter K of a negative-binomial distribution is estimated at 0.08 and 0.2 for Jakarta-Depok and Batam, respectively. This suggests that superspreading events played a key role in the early stage of the outbreak, i.e., a small number of infected individuals are responsible for large amounts of COVID-19 transmission.', 'corpus_id': 220244953, 'score': 0}, {'doc_id': '220128528', 'title': 'Delving deep into the structural aspects of a furin cleavage site inserted into the spike protein of SARS-CoV-2: A structural biophysical perspective', 'abstract': '\n Abstract\n \n One notable feature of the SARS-CoV-2 genome, the spike (S) protein of SARS-CoV-2 has a polybasic furin cleavage site (FCS) at its S1-S2 boundary through the insertion of 12 nucleotides encoding four amino acid residues PRRA. Quite intriguingly, this polybasic FCS is absent in coronaviruses of the same clade as SARS-CoV-2. Thus, with currently available experimental structural data for S protein, this short article presents a set of comprehensive structural characterization of the insertion of FCS into S protein, and argues against a hypothesis of the origin of SARS-CoV-2 from purposeful manipulation: (1), the inserted FCS is spatially located at a random coil loop region, mostly distantly solvent-exposed (instead of deeply buried), with no structural proximity to the other part of the S protein; (2), the insertion of FCS itself does not alter, neither stabilize nor de-stabilize, the three-dimensional structure of S; (3), the net result here is the insertion of a furin cleavage site into S protein, whose S1 and S2 subunits will still be strongly electrostatically bonded together from a structural and biophysical point of view, even if the polybasic FCS is actually cleaved by furin protease before or after viral cell entry.\n \n', 'corpus_id': 220128528, 'score': 0}, {'doc_id': '220128552', 'title': 'Two-stage DEA in banks: Terminological controversies and future directions☆', 'abstract': '\n Abstract\n \n Given the importance that two-stage Data Envelopment Analysis (DEA) models have attained in recent years, this paper presents a systematic review of the literature on the topic focusing on the banking industry. We discuss the two-stage terminology itself, which is not yet not consolidated. We also discuss the current state-of-the-art and present opportunities, as well as challenges, for future studies. We analyse 59 papers, divided them into ten classes that cover various perspectives of two stage DEA studies, such as the economic context, geographic region of the banking units, methodological characteristics, and type of the models, either internal or external. Additionally, we investigate several controversial points regarding two-stage DEA models, such as the variable selection approach, the technique used in the second stage, and the possible impact of non-discretionary variables on efficiency. Results of the literature review indicate the lack of a uniform or universal terminology for two-stage DEA models in the baking industry. Moreover, the main objective of most papers involves extending or improving DEA models. Radial models, with variable returns of scale, and the intermediation approach are the most frequent configurations. Finally, we identify seven gaps in the literature for both internal and external two-stage DEA models and two specific gaps to external ones.Each gap is discussed in depth in the text and can be considered opportunities for future studies.\n \n', 'corpus_id': 220128552, 'score': 0}]"
157	GPT	1703f91c5c4c0ce169e1713fe58318a9	12279	{'GPT': 'glutamic pyruvic transaminase'}	"[{'doc_id': '109734952', 'title': 'A Time to Sow and a Time to Reap: Growth Based on General Purpose Technologies', 'abstract': ""We develop a model of growth driven by successive improvements in 'General Purpose Technologies' (GPT's), such as the steam engine, electricity, or micro-electronics. Each new generation of GPT's prompts investments in complementary inputs, and impacts the economy after enough such compatible inputs become available. The long-run dynamics take the form of recurrent cycles: during the first phase of each cycle output and productivity grow slowly or even decline, and it is only in the second phase that growth starts in earnest. The historical record of productivity growth associated with electrification, and perhaps also of computerization lately, may offer supportive evidence for this pattern. In lieu of analytical comparative dynamics, we conduct simulations of the model over a wide range of parameters, and analyze the results statistically. We extend the model to allow for skilled and unskilled labor, and explore the implications for the behavior over time of their relative wages. We also explore diffusion in the context of a multi-sector economy."", 'corpus_id': 109734952, 'score': 1}, {'doc_id': '229359460', 'title': 'ADBI Working Paper Series DIGITAL TRANSFORMATION: SOME IMPLICATIONS FOR FINANCIAL AND MACROECONOMIC STABILITY', 'abstract': 'Digital transformation is changing how and by whom financial services are provided, how payments are made within an economy and across borders, and how and where goods and services are produced in a globalized economy. These transformations bring significant benefits in the form of greater variety and convenience of financial services, faster speed of payment transactions, and more efficient production processes. But there are also potential costs. Traditional commercial banking models are challenged by unregulated FinTech and BigTech firms, possibly threatening systemic financial stability. Globalization of production processes has led to greater spillovers of economic fluctuations across borders, thereby complicating macroeconomic policy decisions. This paper reviews how digital transformation is likely to impact financial stability, payment systems, and macroeconomic stability, and discusses the need for changes in regulatory and macroeconomic policies to mitigate the associated risks. The paper ends with reflections on the possible consequences of the current Coronavirus pandemic for the analysis and conclusions.', 'corpus_id': 229359460, 'score': 0}, {'doc_id': '223669721', 'title': 'The conceptual basis for the empirical estimation is a multiplicative production function of new innovation in country i in technology j along the lines of the one specified in Acemoglu and others', 'abstract': 'The most closely related paper to this analysis is Johnstone and others (2010). Similar to our paper, it analyses in a cross-country setup the effect of broad policy measures on climatechange mitigating innovation. Our analysis however benefits from a much more recent sample, a more precise technological classification and more standardized policy indicators, namely the environmental policy stringency (EPS) indicator published by the OECD. This allows us to better capture the dramatic increase in clean innovation of the early 2000s, but also the flattening and partial reversal since 2010.4 Our analysis relies on the environment-related technology (ERT) classification proposed by Haščič and Migotto (2015). However, rather than relying on all ERT technologies, we focus on the climate change mitigation technologies related to energy. These are among the technologies with the biggest potential for emissions reductions and most closely targeted by climate-related policies. Unlike the technologies investigated by Johnstone and others (2010), they include not only renewable energy, but also technologies related to improved efficiency in energy generation, transmission and distribution. In addition, we use a technological specification proposed by Dechelepretre and others (2017) to look more closely at technologies related to electricity. The classification has the advantage of not only identifying clean technologies, but also dirty as well gray one, where the latter are innovation that improve the environmental impact of dirty technologies (e.g. biofuel, waste incineration plants). This allows us to study the relative benefits from tightening environmental policies for these different types of technologies, as well as the impact on electricity innovation overall.', 'corpus_id': 223669721, 'score': 0}, {'doc_id': '229504355', 'title': 'De-Globalisation? Global Value Chains in the Post-COVID-19 Age', 'abstract': 'This paper evaluates the extent to which the world economy has entered a phase of de-globalisation, and it offers some speculative thoughts on the future of global value chains in the post-COVID-19 age. Although the growth of international trade flows relative to that of GDP has slowed down since the Great Recession, this paper finds little systematic evidence indicating that the world economy has already entered an era of de-globalisation. Instead, the observed slowdown in globalisation is a natural sequel to the unsustainable increase in globalisation experienced in the late 1980s, 1990s and early 2000s. I offer a description of the mechanisms leading to that earlier expansionary phase, together with a discussion of why these forces might have run out of steam, and of the extent to which they may be reversible. I conclude that the main challenge for the future of globalisation is institutional and political in nature rather than technological, although new technologies might aggravate the trends in inequality that have created the current political backlash against globalisation. Zooming in on the COVID-19 global pandemic, I similarly conclude that the current health crisis may further darken the future of globalisation if it aggravates policy tensions across countries.', 'corpus_id': 229504355, 'score': 0}, {'doc_id': '112062027', 'title': 'General Purpose Technologies', 'abstract': 'This chapter selectively surveys the literature on general purpose technologies (GPTs), focusing on incentives and aggregate growth implications. The literature on classical GPTs (steam, electricity, computers) and on classical great economic transformations (industrial revolutions, the information age) are linked to the theoretical and empirical literatures. The implications of GPT analysis for understanding the history of productivity growth in the late twentieth century are taken up on the concluding remarks.', 'corpus_id': 112062027, 'score': 1}, {'doc_id': '222177052', 'title': 'Cyclical phenomena in technological change.', 'abstract': 'The process of technological change can be regarded as a non-deterministic system governed by factors of a cumulative nature that generate cyclical phenomena. In this context, the process of growth and decline of technology can be systematically analyzed to design best practices for technology management of firms and innovation policy of nations. In this perspective, this study focuses on the evolution of technologies in the U.S. recorded music industry. Empirical findings reveal that technological change in the sector under study here has recurring fluctuations of technological innovations. In particular, cycle of technology has up wave phase longer than down wave phase in the process of evolution in markets before it is substituted by a new technology. Results suggest that radical innovation is one of the main sources of cyclical phenomena for industrial and corporate change, and as a consequence, economic and social change.', 'corpus_id': 222177052, 'score': 0}, {'doc_id': '169653681', 'title': 'Ai as the Next Gpt: A Political-Economy Perspective', 'abstract': 'History suggests that dismal prophecies regarding the impact of great technological advances rarely come to pass. Yet, as many occupations will indeed vanish with the advent of AI as the new General Purpose Technology (GPT), we should search for ways to ameliorate the detrimental effects of AI, and enhance its positive ones, particularly in: (1) education and skills development: need to move away from the centuries-old ""factory model"" of education, and develop instead skills relevant for an AI-based economy â€“ analytical, creative, interpersonal, and emotional. (2) The professionalization of personal care occupations, particularly in healthcare and education; these are to provide the bulk of future employment growth, yet as performed today involve little training and technology, and confer low wages. New, higher standards and academic requirements should be set for these occupations, which would enable AI to benefit both providers and users. (3) Affect the direction of technical advance â€“ we distinguish between ""human-enhancing innovations"" (HEI), that magnify and enhance sensory, motoric, and other such human capabilities, and ""human-replacing innovations"" (HRI), which replace human intervention, and often leave for humans mostly ""dumb"" jobs. AI-based HEI\'s have the potential to unleash a new wave of creativity and productivity, particularly in services, whereas HRI\'s might just decrease employment and give rise to unworthy jobs.', 'corpus_id': 169653681, 'score': 1}, {'doc_id': '109412507', 'title': 'Diffusion of General Purpose Technologies', 'abstract': ""History and theory alike suggest that General Purpose Technologies (GPT's), such as the steam engine or electricity, may play a key role in economic growth. In a previous paper (Helpman and Trajtenberg, 1994) we incorporated this notion into a Grossman-Helpman growth model, and explored the economy-wide dynamics that a GPT generates. The present paper deals with the diffusion of the GPT over heterogeneous final-good sectors. We show that the gradual adoption of the GPT by each user sector generates a sequence of two-phased cycles, culminating in a bringing about a spell of sustained growth. We also analyze the welfare implications of the order of adoption, by way of numerical simulations. As a diffusion of the transistor (the first embodiment of semiconductors, the dominant GPT of our era), and seek to characterize both the early adopters and the laggards in terms of the parameters of the model."", 'corpus_id': 109412507, 'score': 1}, {'doc_id': '73601156', 'title': 'General purpose technologies in theory, application and controversy: a review', 'abstract': 'Distinguishing characteristics of General Purpose Technologies (GPTs) are identified and definitions discussed. Our definition includes multipurpose and single-purpose technologies, defining them according to their micro-technological characteristics, not their macro-economic effects. Identifying technologies as GPTs requires recognizing their evolutionary nature, and accepting possible uncertainties concerning marginal cases. Many of the existing ‘tests’ of whether particular technologies are GPTs are based on misunderstandings either of what GPT theory predicts or what such tests can establish. The development of formal GPT theories is outlined, showing that only the early theories predicted the inevitability of GPT-induced showdown and surges. More recent GPT theories, designed to model the characteristics of GPTs, do not imply the necessity of specific macro effects. We show that GPTs can rejuvenate the growth process without causing slowdowns or surges. We conclude that existing criticisms of GPT theory can be resolved and that the concept remains useful for economic theory.', 'corpus_id': 73601156, 'score': 1}, {'doc_id': '222131019', 'title': 'The transformation of jobs and working conditions: Towards a policy response', 'abstract': 'The introduction of new technologies and their impact on workers is not a new topic among scholars; innovation is perceived as an embedded feature of capitalism and necessary for capital renewal (Hall 2010). However, there are some aspects which make the current changes different from previous waves of technological revolutions: the speed of innovation and its destructive potential regarding technologies currently in use but which are quickly becoming obsolete (Komlos 2016); their association with jobless growth (Brynjolfsson and McAfee 2012); and their facilitation of new business models that reach across the globe with minimum physical capital and with a very low number of employed workers, which is especially relevant in the IT sector (Soete 2018). In production areas, new technologies are used to reduce costs by limiting the input of labour while preserving or even increasing production levels. The transformation of working conditions and the reduction in job opportunities are consequences of the deployment of new technologies in the production process which deserve researchers’ attention.', 'corpus_id': 222131019, 'score': 0}]"
158	Deinterleaving radar pulses	ebb0c6755f0584310c69527022c7052a	4309	{}	"[{'doc_id': '108893931', 'title': 'Improved algorithm for the deinterleaving of radar pulses', 'abstract': 'The paper presents an improved method for the deinterleaving of radar signals, based on a time of arrival analysis and the use of the sequential difference histogram (SDIF) for determining the pulse repetition interval (PRI). The optimal detection threshold in the SDIF histogram is derived, which greatly contributes to the efficiency of the algorithm. The algorithm is applied to classic, frequency-agile and staggered PRI radar signals. It is shown that the new method is very successful in high-pulse-density radar environments and for complex signal types. Special attention is given to an application of this method to the multiple-parameter deinterleaving algorithm.', 'corpus_id': 108893931, 'score': 1}, {'doc_id': '64199076', 'title': 'An Improved Algorithm for Deinterleaving of Radar Pulses', 'abstract': 'This paper introduced an improved algorithm for deinterleaving of radar pulses, which was based on TOA and frequency. The two_parameter algorithm was applied to classic, frequence_agile and staggered PRI radar signals. It is shown that the new method is successful in high_pulse_density radar environments and for complex signal types.', 'corpus_id': 64199076, 'score': 1}, {'doc_id': '61445250', 'title': 'New techniques for the deinterleaving of repetitive sequences', 'abstract': 'Radar signals are generally characterised by repetitive patterns in time. An ESM receiver must intercept and identify several interleaved radar signals. Time-of-arrival (TOA) deinterleaving is employed in ESM processing to identify and extract the pulses of each radar signal. This task is extremely processor intensive and new techniques are required to operate on complex signals in high pulse densities. A new algorithm employing novel techniques is presented for fast, accurate deinterleaving of several repetitive signals. A cumulative TOA difference histogram gives an indication of probable pulse repetition intervals (PRIs) with a minimum number of computations. Validation and identification is given by searching for a sequence of these pulse intervals. The technique presented is less sensitive to interfering pulses and more robust to missed pulses than conventional published techniques. Weighting is used to enhance detection of sequences and a three-pulse priming sequence dramatically reduces unsuccessful searches. By employing a learning process, the efficiency is increased still further. The application of this algorithm to agile PRI signals is shown.', 'corpus_id': 61445250, 'score': 1}, {'doc_id': '115018981', 'title': 'Multi-hypothesis method in pulses deinterleaving', 'abstract': 'In the naval electronic environment, pulses emitted by radars are collected by electronic support measures receivers. The aim is to gather these pulses in a such way that one cluster corresponds to one radar despite the waveform parameters agility. To achieve it, this paper describes a pulse train deinterleaving process using a multi-hypotheses architecture. A hypothesis tree, built from pulse measurements, represents all the possibilities to associate pulses to emitters, according to one hypothesis of pulses association. Different processes are used to find the valid hypothesis that correspond to emitters. These processes have two aims: avoiding the combinatorial explosion of the number of hypotheses, and to lead to the solution tree: one branch corresponds to one effective emitter. Radars waveforms with agile parameters are considered. A new process has to be added to take into account missing pulses.', 'corpus_id': 115018981, 'score': 1}, {'doc_id': '212657432', 'title': 'Coding schemes and Applications for Weather Radars.', 'abstract': 'In this paper, we describe the evolution of a pair of polyphase coded waveform for use in second trip suppression in weather radar. The polyphase codes were designed and tested on NASA weather radar. The NASA dual-frequency, dual-polarization Doppler radar (D3R) was developed primarily as a ground validation tool for the GPM satellite dual-frequency radar. Recently, the D3R radar was upgraded with new versions of digital receiver hardware and firmware, which supports larger filter lengths and multiple phase coded waveforms, and also newer IF sub-systems. This has enhanced the capabilities of radar manifolds.', 'corpus_id': 212657432, 'score': 0}, {'doc_id': '218470533', 'title': 'Probability of Pilot Interference in Pulsed Radar-Cellular Coexistence: Fundamental Insights on Demodulation and Limited CSI Feedback', 'abstract': 'This letter considers an underlay pulsed radar-cellular spectrum sharing scenario, where the cellular system uses pilot-aided demodulation, statistical channel state information (S-CSI) estimation and limited feedback schemes. Under a realistic system model, upper and lower bounds are derived on the probability that at least a specified number of pilot signals are interfered by a radar pulse train in a finite CSI estimation window. Exact probabilities are also derived for important special cases which reveal operational regimes where the lower bound is achieved. Using these results, this letter (a) provides insights on pilot interference-minimizing schemes for accurate coherent symbol demodulation, and (b) demonstrates that pilot-aided methods fail to accurately estimate S-CSI of the pulsed radar interference channel for a wide range of radar repetition intervals.', 'corpus_id': 218470533, 'score': 0}, {'doc_id': '211677680', 'title': 'AARTFAAC discovery of extreme-fluence pulses from PSR B0950+08', 'abstract': 'Here we report on the detection of extreme-fluence pulses (EFP) from PSR B0950+08 with the Amsterdam-Astron Radio Transient Facility And Analysis Center (AARTFAAC), a parallel transient detection instrument operating as a subsystem of the LOw Frequency ARray (LOFAR). During processing of our Northern Hemisphere survey for low frequency (58.3 and 61.8 MHz) radio transients, a sample of 275 pulses with fluences ranging from 42k to 177k Jy ms were detected in one-second snapshot images. The brightest pulses are more than two orders of magnitude brighter than those previously reported at 42 and 74 MHz. Although the power-law pulse-energy distribution index agrees well with the previous results, the average rate of EFPs is much higher. Given the number of EFPs observed, and the power-law index of the pulse-fluence distribution at high fluence, a single power-law cannot be extended to the typical pulse population. Activity was found to be highly variable, with only two three-hour observations accounting for nearly half of the pulses detected in the 96 hours surveyed. The rate of EFPs varied from 0 to 30 detected per hour between consecutive days of observation. However, no clustering was observed within a single active three-hour span. The spectra appear intrinsically structured with narrow band emission, confined, at times, within 195.3 kHz sub-bands, and dynamic, with the pulse spectra changing on timescale of $\\sim$10 minutes. This narrow emission bandwidth provides strong evidence that the EFPs are intrinsically higher energy, rather than being magnified by propagation effects.', 'corpus_id': 211677680, 'score': 0}, {'doc_id': '61250144', 'title': 'Deinterleaving Pulse Trains Using Discrete-Time', 'abstract': 'Pulse trains from a number of different sources are often received on the one communication channel. It is then of interest to identify which pulses are from which source, based on different source characteristics. This sorting task is termed dein- terleaving. In this paper we next propose time-domain techniques for deinterleaving pulse trains from a finite number of periodic sources based on the time of arrival (TOA) and pulse energy, if available, of the pulses received on the one communication channel. We formulate the pulse train deinterleaving problem as a stochastic discrete-time dynamic linear model (DLM), the ""discrete-time"" variable k being associated with the kth received pulse. The time-varying parameters of the DLM depend on the se- quence of active sources. The deinterleaving detectionlestimation task can then be done optimally via linear signal processing using the Kalman filter (or recursive least squares when the source periods are constant) and tree searching. The optimal solution, however, is computationally infeasible for other than small data lengths since the number of possible sequences grow exponentially with data length. Here we propose and study two of a number of possible suboptimal solutions: 1) Forward dynamic programming with fixed look-ahead rather than total look-ahead as required for the optimal scheme; 2) a probabilistic teacher Kalman filtering for the detection/estimation task. In simulation studies we show that when the number of sources is small, the proposed suboptimal schemes yield near-optimal estimates even in the presence of relatively large jitter noise. Also, issues of robustness and generalizations of the approach to the case of missing pulses, unknown source number, and non-Gaussian jitter noise are addressed.', 'corpus_id': 61250144, 'score': 1}, {'doc_id': '215238709', 'title': 'Successive Eigenvalue Removal for Multi-Soliton Spectral Amplitude Estimation', 'abstract': ""Optical nonlinear Fourier transform-based communication systems require an accurate estimation of a signal's nonlinear spectrum, computed usually by piecewise approximation methods on the signal samples. We propose an algorithm, named successive eigenvalue removal, to improve the spectrum estimation of a multi-soliton pulse. It exploits a property of the Darboux transform that allows removing eigenvalues from the nonlinear spectrum. This results in a smaller pulse duration and smaller bandwidth. The spectral coefficients are estimated successively after removing the eigenvalues of a signal. As a beneficial application, we show that the algorithm decreases the computational complexity by iteratively reducing the pulse duration."", 'corpus_id': 215238709, 'score': 0}, {'doc_id': '211296793', 'title': 'Analysing Multibeam, Cooperative, Ground Based Radar in a Bistatic Configuration', 'abstract': 'Recent advances in digital beam forming for phased arrays in combination with digital signal processing should enable the development of multibeam radar in a bistatic configuration. In the bistatic setting, the pulse travelling outward from the transmitter should be followed or “chased” by the receiver. During transmission, depending on the location of the transmitter, receiver, and pulse, the number of digital beams and their location at the transmitter vary. In this paper, we analyse the geometrically depending number of digital beams and the beam switching rate of the receiver needed for pulse chasing. In addition, we derive the pulse repetition frequency (PRF) for the bistatic configuration based on the desired detection range. It is shown that the PRF in the bistatic case can be increased compared to its monostatic counterpart when the distance between the transmitter and the receiver is increased. Our results are applied on the scenario of an air traffic control radar to show the feasibility of a multibeam, ground based bistatic surveillance radar. It will be demonstrated that the maximum PRF can almost be doubled and an adaptive sensing and tracking paradigm can lead to a maximum of 64 simultaneous receiver beams for the bistatic surveillance and tracking setting.', 'corpus_id': 211296793, 'score': 0}]"
159	relevant	f552707df89102a4e839ffd34a546896	763	{}	"[{'doc_id': '210164864', 'title': 'Towards High Performance Java-based Deep Learning Frameworks', 'abstract': ""The advent of modern cloud services along with the huge volume of data produced on a daily basis, have set the demand for fast and efficient data processing. This demand is common among numerous application domains, such as deep learning, data mining, and computer vision. Prior research has focused on employing hardware accelerators as a means to overcome this inefficiency. This trend has driven software development to target heterogeneous execution, and several modern computing systems have incorporated a mixture of diverse computing components, including GPUs and FPGAs. However, the specialization of the applications' code for heterogeneous execution is not a trivial task, as it requires developers to have hardware expertise in order to obtain high performance. The vast majority of the existing deep learning frameworks that support heterogeneous acceleration, rely on the implementation of wrapper calls from a high-level programming language to a low-level accelerator backend, such as OpenCL, CUDA or HLS. \nIn this paper we have employed TornadoVM, a state-of-the-art heterogeneous programming framework to transparently accelerate Deep Netts; a Java-based deep learning framework. Our initial results demonstrate up to 8x performance speedup when executing the back propagation process of the network's training on AMD GPUs against the sequential execution of the original Deep Netts framework."", 'corpus_id': 210164864, 'score': 0}, {'doc_id': '214802265', 'title': 'The collection Virtual Machine: an abstraction for multi-frontend multi-backend data analysis', 'abstract': 'Getting the best performance from the ever-increasing number of hardware platforms has been a recurring challenge for data processing systems. In recent years, the advent of data science with its increasingly numerous and complex types of analytics has made this challenge even more difficult. In practice, system designers are overwhelmed by the number of combinations and typically implement a single analytics type on one platform, leading to repeated implementation effort---and a plethora of semi-compatible tools for data scientists. In this paper, we propose the ""Collection Virtual Machine"" (or CVM)---an extensible compiler framework designed to keep the specialization process of data analytics systems tractable. It can capture at the same time the essence of a large span of low-level, hardware-specific implementation techniques as well as high-level operations of different types of analyses. At its core lies a language for defining nested, collection-oriented intermediate representations (IRs). Frontends produce programs in their IR flavors defined in that language, which get optimized through a series of rewritings (possibly changing the IR flavor multiple times) until the program is finally expressed in an IR of platform-specific operators. While reducing the overall implementation effort, this also improves the interoperability of both analyses and hardware platforms. We have used CVM successfully to build specialized backends for platforms as diverse as multi-core CPUs, RDMA clusters, and serverless computing infrastructure in the cloud and expect similar results for many more frontends and hardware platforms in the near future.', 'corpus_id': 214802265, 'score': 0}, {'doc_id': '211043646', 'title': 'A Language for Describing Optimization Strategies', 'abstract': 'Optimizing programs to run efficiently on modern parallel hardware is hard but crucial for many applications. The predominantly used imperative languages - like C or OpenCL - force the programmer to intertwine the code describing functionality and optimizations. This results in a nightmare for portability which is particularly problematic given the accelerating trend towards specialized hardware devices to further increase efficiency. \nMany emerging DSLs used in performance demanding domains such as deep learning, automatic differentiation, or image processing attempt to simplify or even fully automate the optimization process. Using a high-level - often functional - language, programmers focus on describing functionality in a declarative way. In some systems such as Halide or TVM, a separate schedule specifies how the program should be optimized. Unfortunately, these schedules are not written in well-defined programming languages. Instead, they are implemented as a set of ad-hoc predefined APIs that the compiler writers have exposed. \nIn this paper, we present Elevate: a functional language for describing optimization strategies. Elevate follows a tradition of prior systems used in different contexts that express optimization strategies as composition of rewrites. In contrast to systems with scheduling APIs, in Elevate programmers are not restricted to a set of built-in optimizations but define their own optimization strategies freely in a composable way. We show how user-defined optimization strategies in Elevate enable the effective optimization of programs expressed in a functional data-parallel language demonstrating competitive performance with Halide and TVM.', 'corpus_id': 211043646, 'score': 1}, {'doc_id': '211082718', 'title': 'Machine Learning in Python: Main developments and technology trends in data science, machine learning, and artificial intelligence', 'abstract': 'Smarter applications are making better use of the insights gleaned from data, having an impact on every industry and research discipline. At the core of this revolution lies the tools and the methods that are driving it, from processing the massive piles of data generated each day to learning from and taking useful action. Deep neural networks, along with advancements in classical ML and scalable general-purpose GPU computing, have become critical components of artificial intelligence, enabling many of these astounding breakthroughs and lowering the barrier to adoption. Python continues to be the most preferred language for scientific computing, data science, and machine learning, boosting both performance and productivity by enabling the use of low-level libraries and clean high-level APIs. This survey offers insight into the field of machine learning with Python, taking a tour through important topics to identify some of the core hardware and software paradigms that have enabled it. We cover widely-used libraries and concepts, collected together for holistic comparison, with the goal of educating the reader and driving the field of Python machine learning forward.', 'corpus_id': 211082718, 'score': 1}, {'doc_id': '202230751', 'title': 'SystemDS: A Declarative Machine Learning System for the End-to-End Data Science Lifecycle', 'abstract': 'Machine learning (ML) applications become increasingly common in many domains. ML systems to execute these workloads include numerical computing frameworks and libraries, ML algorithm libraries, and specialized systems for deep neural networks and distributed ML. These systems focus primarily on efficient model training and scoring. However, the data science process is exploratory, and deals with underspecified objectives and a wide variety of heterogeneous data sources. Therefore, additional tools are employed for data engineering and debugging, which requires boundary crossing, unnecessary manual effort, and lacks optimization across the lifecycle. In this paper, we introduce SystemDS, an open source ML system for the end-to-end data science lifecycle from data integration, cleaning, and preparation, over local, distributed, and federated ML model training, to debugging and serving. To this end, we aim to provide a stack of declarative languages with R-like syntax for the different lifecycle tasks, and users with different expertise. We describe the overall system architecture, explain major design decisions (motivated by lessons learned from Apache SystemML), and discuss key features and research directions. Finally, we provide preliminary results that show the potential of end-to-end lifecycle optimization.', 'corpus_id': 202230751, 'score': 1}, {'doc_id': '211171885', 'title': 'MLModelScope: A Distributed Platform for Model Evaluation and Benchmarking at Scale', 'abstract': 'Machine Learning (ML) and Deep Learning (DL) innovations are being introduced at such a rapid pace that researchers are hard-pressed to analyze and study them. The complicated procedures for evaluating innovations, along with the lack of standard and efficient ways of specifying and provisioning ML/DL evaluation, is a major ""pain point"" for the community. This paper proposes MLModelScope, an open-source, framework/hardware agnostic, extensible and customizable design that enables repeatable, fair, and scalable model evaluation and benchmarking. We implement the distributed design with support for all major frameworks and hardware, and equip it with web, command-line, and library interfaces. To demonstrate MLModelScope\'s capabilities we perform parallel evaluation and show how subtle changes to model evaluation pipeline affects the accuracy and HW/SW stack choices affect performance.', 'corpus_id': 211171885, 'score': 0}, {'doc_id': '210157176', 'title': 'Multi-layer optimizations for end-to-end data analytics', 'abstract': ""We consider the problem of training machine learning models over multi-relational data. The mainstream approach is to first construct the training dataset using a feature extraction query over input database and then use a statistical software package of choice to train the model. In this paper we introduce Iterative Functional Aggregate Queries (IFAQ), a framework that realizes an alternative approach. IFAQ treats the feature extraction query and the learning task as one program given in the IFAQ's domain-specific language, which captures a subset of Python commonly used in Jupyter notebooks for rapid prototyping of machine learning applications. The program is subject to several layers of IFAQ optimizations, such as algebraic transformations, loop transformations, schema specialization, data layout optimizations, and finally compilation into efficient low-level C++ code specialized for the given workload and data. We show that a Scala implementation of IFAQ can outperform mlpack, Scikit, and TensorFlow by several orders of magnitude for linear regression and regression tree models over several relational datasets."", 'corpus_id': 210157176, 'score': 1}, {'doc_id': '211004086', 'title': 'DIVA: A Declarative and Reactive Language for in situ Visualization', 'abstract': 'The use of adaptive workflow management for in situ visualization and analysis has been a growing trend in large-scale scientific simulations. However, coordinating adaptive workflows with traditional procedural programming languages can be difficult because system flow is determined by unpredictable scientific phenomena, which often appear in an unknown order and can evade event handling. This makes the implementation of adaptive workflows tedious and error-prone. Recently, reactive and declarative programming paradigms have been recognized as well-suited solutions to similar problems in other domains. However, there is a dearth of research on adapting these approaches to in situ visualization and analysis. With this paper, we present a language design and runtime system for developing adaptive systems through a declarative and reactive programming paradigm. We illustrate how an adaptive workflow programming system is implemented using our approach and demonstrate it with a use case from a combustion simulation.', 'corpus_id': 211004086, 'score': 0}, {'doc_id': '211296505', 'title': ""MLIR: A Compiler Infrastructure for the End of Moore's Law"", 'abstract': 'This work presents MLIR, a novel approach to building reusable and extensible compiler infrastructure. MLIR aims to address software fragmentation, improve compilation for heterogeneous hardware, significantly reduce the cost of building domain specific compilers, and aid in connecting existing compilers together. MLIR facilitates the design and implementation of code generators, translators and optimizers at different levels of abstraction and also across application domains, hardware targets and execution environments. The contribution of this work includes (1) discussion of MLIR as a research artifact, built for extension and evolution, and identifying the challenges and opportunities posed by this novel design point in design, semantics, optimization specification, system, and engineering. (2) evaluation of MLIR as a generalized infrastructure that reduces the cost of building compilers-describing diverse use-cases to show research and educational opportunities for future programming languages, compilers, execution environments, and computer architecture. The paper also presents the rationale for MLIR, its original design principles, structures and semantics.', 'corpus_id': 211296505, 'score': 1}, {'doc_id': '210023441', 'title': 'Vamsa: Tracking Provenance in Data Science Scripts', 'abstract': ""Machine learning (ML) which was initially adopted for search ranking and recommendation systems has firmly moved into the realm of core enterprise operations like sales optimization and preventative healthcare. For such ML applications, often deployed in regulated environments, the standards for user privacy, security, and data governance are substantially higher. This imposes the need for tracking provenance end-to-end, from the data sources used for training ML models to the predictions of the deployed models. \nIn this work, we take a first step towards this direction by introducing the ML provenance tracking problem in the context of data science scripts. The fundamental idea is to automatically identify the relationships between data and ML models and in particular, to track which columns in a dataset have been used to derive the features of a ML model. We discuss the challenges in capturing such provenance information in the context of Python, the most common language used by data scientists. We then, present Vamsa, a modular system that extracts provenance from Python scripts without requiring any changes to the user's code. Using up to 450K real-world data science scripts from Kaggle and publicly available Python notebooks, we verify the effectiveness of Vamsa in terms of coverage, and performance. We also evaluate Vamsa's accuracy on a smaller subset of manually labeled data. Our analysis shows that Vamsa's precision and recall range from 87.5% to 98.3% and its latency is typically in the order of milliseconds for scripts of average size."", 'corpus_id': 210023441, 'score': 0}]"
160	Summarization	7c1a5b5db8edee25af927877833386b8	8729	{}	"[{'doc_id': '231632954', 'title': 'Neural Abstractive Text Summarizer for Telugu Language', 'abstract': 'Abstractive text summarization is the process of constructing semantically relevant shorter sentences which captures the essence of the overall meaning of the source text. It is actually difficult and very time consuming for humans to summarize manually large documents of text. Much of work in abstractive text summarization is being done in English, and almost no significant work has been reported in Telugu abstractive text summarization. So, we would like to propose an abstractive text summarization approach for Telugu language using deep learning. In this paper, we are proposing an abstractive text summarization deep learning model for Telugu language. The proposed architecture is based on encoder–decoder sequential models with attention mechanism. We have applied this model on manually created dataset to generate a one sentence summary of the source text and have got good results measured qualitatively.', 'corpus_id': 231632954, 'score': 0}, {'doc_id': '214802728', 'title': 'Boosting Factual Correctness of Abstractive Summarization', 'abstract': 'A commonly observed problem with abstractive summarization is the distortion or fabrication of factual information in the article. This inconsistency between summary and original text has led to various concerns over its applicability. In this paper, we firstly propose a Fact-Aware Summarization model, FASum, which extracts factual relations from the article and integrates this knowledge into the decoding process via neural graph computation. Then, we propose a Factual Corrector model, FC, that can modify abstractive summaries generated by any model to improve factual correctness. Empirical results show that FASum generates summaries with significantly higher factual correctness compared with state-of-the-art abstractive summarization systems, both under an independently trained factual correctness evaluator and human evaluation. And FC improves the factual correctness of summaries generated by various models via only modifying several entity tokens.', 'corpus_id': 214802728, 'score': 1}, {'doc_id': '218487034', 'title': 'On Faithfulness and Factuality in Abstractive Summarization', 'abstract': 'It is well known that the standard likelihood training and approximate decoding objectives in neural text generation models lead to less human-like responses for open-ended tasks such as language modeling and story generation. In this paper we have analyzed limitations of these models for abstractive document summarization and found that these models are highly prone to hallucinate content that is unfaithful to the input document. We conducted a large scale human evaluation of several neural abstractive summarization systems to better understand the types of hallucinations they produce. Our human annotators found substantial amounts of hallucinated content in all model generated summaries. However, our analysis does show that pretrained models are better summarizers not only in terms of raw metrics, i.e., ROUGE, but also in generating faithful and factual summaries as evaluated by humans. Furthermore, we show that textual entailment measures better correlate with faithfulness than standard metrics, potentially leading the way to automatic evaluation metrics as well as training and decoding criteria.', 'corpus_id': 218487034, 'score': 1}, {'doc_id': '227054154', 'title': 'Fact-level Extractive Summarization with Hierarchical Graph Mask on BERT', 'abstract': 'Most current extractive summarization models generate summaries by selecting salient sentences. However, one of the problems with sentence-level extractive summarization is that there exists a gap between the human-written gold summary and the oracle sentence labels. In this paper, we propose to extract fact-level semantic units for better extractive summarization. We also introduce a hierarchical structure, which incorporates the multi-level of granularities of the textual information into the model. In addition, we incorporate our model with BERT using a hierarchical graph mask. This allows us to combine BERT’s ability in natural language understanding and the structural information without increasing the scale of the model. Experiments on the CNN/DaliyMail dataset show that our model achieves state-of-the-art results.', 'corpus_id': 227054154, 'score': 0}, {'doc_id': '227746376', 'title': 'Cross-lingual Approach to Abstractive Summarization', 'abstract': 'Automatic text summarization is a process of extracting important information from texts and presenting that information in the form of a summary. Abstractive summarization approaches progressed using deep neural networks, but results are not yet satisfactory, especially for languages where large training sets do not exist. In several natural language processing tasks, cross-lingual model transfers are succesfully applied for low-resource languages where large enough datasets are not available. For summarization such cross-lingual transfer was so far not attempted due to non-reusable decoder side of neural models. In our work, we used a pretrained English summarization model based on deep neural networks and sequence-to-sequece architecture to summarize Slovene news articles. We solved the problem with inadequate decoder by using an additional language model for target language text generation. We developed five models with different training sample sizes. The results were assessed by automatic and human evaluation. Our cross-lingual model performance is similar to the existing Slovene abstractive summarizer. We also discuss some interdisciplinary aspects, raised by our work.', 'corpus_id': 227746376, 'score': 0}, {'doc_id': '227254906', 'title': 'Bengali Abstractive News Summarization(BANS): A Neural Attention Approach', 'abstract': ""Abstractive summarization is the process of generating novel sentences based on the information extracted from the original text document while retaining the context. Due to abstractive summarization's underlying complexities, most of the past research work has been done on the extractive summarization approach. Nevertheless, with the triumph of the sequence-to-sequence (seq2seq) model, abstractive summarization becomes more viable. Although a significant number of notable research has been done in the English language based on abstractive summarization, only a couple of works have been done on Bengali abstractive news summarization (BANS). In this article, we presented a seq2seq based Long Short-Term Memory (LSTM) network model with attention at encoder-decoder. Our proposed system deploys a local attention-based model that produces a long sequence of words with lucid and human-like generated sentences with noteworthy information of the original document. We also prepared a dataset of more than 19k articles and corresponding human-written summaries collected from bangla.bdnews24.com1 which is till now the most extensive dataset for Bengali news document summarization and publicly published in Kaggle2. We evaluated our model qualitatively and quantitatively and compared it with other published results. It showed significant improvement in terms of human evaluation scores with state-of-the-art approaches for BANS."", 'corpus_id': 227254906, 'score': 0}, {'doc_id': '211171764', 'title': 'Transfer Learning for Abstractive Summarization at Controllable Budgets', 'abstract': 'Summarizing a document within an allocated budget while maintaining its major concepts is a challenging task. If the budget can take any arbitrary value and not known beforehand, it becomes even more difficult. Most of the existing methods for abstractive summarization, including state-of-the-art neural networks are data intensive. If the number of available training samples becomes limited, they fail to construct high-quality summaries. We propose MLS, an end-to-end framework to generate abstractive summaries with limited training data at arbitrary compression budgets. MLS employs a pair of supervised sequence-to-sequence networks. The first network called the \\textit{MFS-Net} constructs a minimal feasible summary by identifying the key concepts of the input document. The second network called the Pointer-Magnifier then generates the final summary from the minimal feasible summary by leveraging an interpretable multi-headed attention model. Experiments on two cross-domain datasets show that MLS outperforms baseline methods over a range of success metrics including ROUGE and METEOR. We observed an improvement of approximately 4% in both metrics over the state-of-art convolutional network at lower budgets. Results from a human evaluation study also establish the effectiveness of MLS in generating complete coherent summaries at arbitrary compression budgets.', 'corpus_id': 211171764, 'score': 1}, {'doc_id': '210698398', 'title': 'Abstractive summarization of long texts by representing multiple compositionalities with temporal hierarchical pointer generator network', 'abstract': 'In order to tackle the problem of abstractive summarization of long multi-sentence texts, it is critical to construct an efficient model, which can learn and represent multiple compositionalities better. In this paper, we introduce a temporal hierarchical pointer generator network that can represent multiple compositionalities in order to handle longer sequences of texts with a deep structure. We demonstrate how a multilayer gated recurrent neural network organizes itself with the help of an adaptive timescale in order to represent the compositions. The temporal hierarchical network is implemented with a multiple timescale architecture where the timescale of each layer is also learned during the training process through error backpropagation through time. We evaluate our proposed model using an Introduction-Abstract summarization dataset from scientific articles and the CNN/Daily Mail summarization benchmark dataset. The results illustrate that, we successfully implement a summary generation system for long texts by using the multiple timescale with adaptation concept. We also show that we have improved the summary generation system with our proposed model on the benchmark dataset.', 'corpus_id': 210698398, 'score': 1}, {'doc_id': '210838813', 'title': 'Length-controllable Abstractive Summarization by Guiding with Summary Prototype', 'abstract': 'We propose a new length-controllable abstractive summarization model. Recent state-of-the-art abstractive summarization models based on encoder-decoder models generate only one summary per source text. However, controllable summarization, especially of the length, is an important aspect for practical applications. Previous studies on length-controllable abstractive summarization incorporate length embeddings in the decoder module for controlling the summary length. Although the length embeddings can control where to stop decoding, they do not decide which information should be included in the summary within the length constraint. Unlike the previous models, our length-controllable abstractive summarization model incorporates a word-level extractive module in the encoder-decoder model instead of length embeddings. Our model generates a summary in two steps. First, our word-level extractor extracts a sequence of important words (we call it the ""prototype text"") from the source text according to the word-level importance scores and the length constraint. Second, the prototype text is used as additional input to the encoder-decoder model, which generates a summary by jointly encoding and copying words from both the prototype text and source text. Since the prototype text is a guide to both the content and length of the summary, our model can generate an informative and length-controlled summary. Experiments with the CNN/Daily Mail dataset and the NEWSROOM dataset show that our model outperformed previous models in length-controlled settings.', 'corpus_id': 210838813, 'score': 1}]"
161	distributed team	fe99ebd6cb100687a1f1f9fb9593e064	6486	{}	"[{'doc_id': '220496258', 'title': 'The critical role of fresh teams in creating original and multi-disciplinary research', 'abstract': 'Teamwork is one of the most prominent features in modern science. It is now well-understood that the team size is an important factor that affects team creativity. However, the crucial question of how the character of research studies is influenced by the freshness of the team remains unclear. In this paper, we quantify the team freshness according to the absent of prior collaboration among team members. Our results suggest that fresher teams tend to produce works of higher originality and more multi-disciplinary impact. These effects are even magnified in larger teams. Furthermore, we find that freshness defined by new team members in a paper is a more effective indicator of research originality and multi-disciplinarity compared to freshness defined by new collaboration relations among team members. Finally, we show that career freshness of members also plays an important role in increasing the originality and multi-disciplinarity of produced papers.', 'corpus_id': 220496258, 'score': 1}, {'doc_id': '220525889', 'title': 'Literature Review of Constructivism in Online Science Courses', 'abstract': ""Reform movements in science education, such as inquiry-based instruction, have been heavily influenced by constructivist learning theories (National Research Council, 2000). These learning theories place the learner as the sole constructor of knowledge and emphasize the importance of the learner's inquiry process (Yilmaz, 2008). In constructivist inquiry-based science education, lab experiences frequently play an important role in instruction as they provide students with opportunities to observe and make sense of the world around them (National Research Council, 2000), which raises the question of how inquiry-based science instruction can be translated to online environments. There are several models for lab experiences in online science courses, including hands-on labs where students directly manipulate materials, remote labs where students manipulate materials through a computer, and virtual labs and simulations where students work with simulated materials (Powell, et al., 2010). Hands-on labs play an important role, especially given constructivist views that students construct meaning by making observations of the world around them, but there is evidence that simulated and virtual labs can play an important role and may even be better suited to some instructional goals than hands-on labs. Constructivist instruction also requires students to make their process visible and teachers to be responsive to student thinking, both of which are more challenging in online environments (Crippen, et al., 2013). However, with intentional design, these features can be incorporated into online science courses (Jaber, et al., 2018; Jang, 2009)."", 'corpus_id': 220525889, 'score': 0}, {'doc_id': '226195056', 'title': 'Using Virtual Teams to Map Digital New Generation Learning Environments into Tertiary Online Learning Spaces', 'abstract': 'The sudden emergence and rapid spread of COVID-19 is a reminder of our frailty, where the magnitude of the impact facing the world as a result of the COVID-19 pandemic has become increasingly apparent. One way that we have effectively responded to this emergency educationally, has to become innovative in our relationship with technology. \nThe value of Virtual Teams to globally collaborate is well known, and can be adopting to address student learning in the university online environment. In order to successfully embed a Virtual Team model into an online platform however, a framework to support its delivery is required. We suggest using a new generation learning environment (NGLE) design to meet this end. Our conceptual paper discusses how NGLEs can be mapped into online learning environments and illustrates how Virtual Teams can effectively align within a NGLE framework. Finally, we suggest a model for Virtual Teams for an online tertiary course, highlighting the importance of implementing a Virtual Team and NGLE approach in tertiary education.', 'corpus_id': 226195056, 'score': 1}, {'doc_id': '219608346', 'title': 'WhatsApp supported language teacher development: A case study in the Zataari refugee camp', 'abstract': 'This paper explores the possibilities and challenges of using the social media tool WhatsApp to support language teacher development in the Zataari refugee camp in Jordan. It takes a sociocultural perspective on teacher development where WhatsApp is a mediating tool in the broader sociocultural landscape. A thematic analysis of the postings and exchanges from the WhatsApp group revealed three main uses of the WhatsApp chat: for interpersonal interactions, for professional development, and for organisational purposes. The analysis suggests the WhatsApp group contributed to the teachers’ English language knowledge, provided a platform for them to share and discuss issues related to the challenges of their particular context, enabled them to contribute to the development of some teaching materials and begin to address some of the issues they had in a meaningful way. It also raises issues of participation, access, equity and sustainability. We conclude by suggesting there is good potential for the use of social media tools such as WhatsApp for teacher development in challenging contexts, despite the contextual constraints observed and described. While this specific case involves language teachers, the general findings can potentially be applied to any teacher education or training context where access to training or education might be curtailed for a number of reasons, including the most recent changes enforced by the global COVID-19 pandemic.', 'corpus_id': 219608346, 'score': 0}, {'doc_id': '220363692', 'title': 'Understanding coordination in global software engineering: A mixed-methods study on the use of meetings and Slack', 'abstract': 'Given the relevance of coordination in the field of global software engineering, this work was carried out to further understand coordination mechanisms. Specifically, we investigated meetings and the collaboration tool Slack. We conducted a longitudinal case study using a mixed-methods approach with surveys, observations, interviews, and chat logs. Our quantitative results show that employees in global projects spend 7 hours 45 minutes per week on average in scheduled meetings and 8 hours 54 minutes in unscheduled meetings. Furthermore, distributed teams were significantly larger than co-located teams, and people working in distributed teams spent somewhat more time in meetings per day. We found that low availability of key people, absence of organizational support for unscheduled meetings and unbalanced activity from team members in meetings and on Slack were barriers for effective coordination across sites. The positive aspects of using collaboration tools in distributed teams were increased team awareness and informal communication and reduced the need for e-mail. Our study emphasizes the importance of reflecting on how global software engineering teams use meetings and collaboration tools to coordinate. We provide practical advice for conducting better meetings and give suggestions for more efficient use of collaboration tools in global projects.', 'corpus_id': 220363692, 'score': 1}, {'doc_id': '167130859', 'title': 'Subjective Distance and Team Collaboration in Distributed Teams', 'abstract': ""This paper challenges the conventional wisdom that objective distance between team members (e.g., measured in miles) translates directly and fully into subjective distance (i.e., a team's perception of distance between its members). Drawing on social information processing theory, we argue that the level of subjective distance is likely to predict important team outcomes better than the level of objective distance. Using responses from 678 team leaders and team members pertaining to 161 new product development projects in the software industry, our results show that the subjective perception of distance is affected rather by team members' national diversity than their physical distance. We also find that subjective distance has a significant impact on team collaboration, while objective distance measures, however, have no impact on team collaboration. Implications for theory and practice are discussed."", 'corpus_id': 167130859, 'score': 1}, {'doc_id': '219401417', 'title': 'How (UN) Happiness Impacts on Software Engineers in Agile Teams?', 'abstract': 'Information technology (IT) organizations are increasing the use of agile practices, which are based on a people-centred culture alongside the software development process. Thus, it is vital to understand the social and human factors of the individuals working in agile environments, such as happiness and unhappiness and how these factors impact this kind of environment. Therefore, five case-studies were developed inside agile projects, in a company that values innovation, aiming to identify how (un)happiness impacts software engineers in agile environments. According to the answers gathered from 67 participants through a survey, interviews and using a cross-analysis, happiness factors identified by agile teams were effective communication, motivated members, collaboration among members, proactive members, and present leaders.', 'corpus_id': 219401417, 'score': 0}, {'doc_id': '216562322', 'title': 'The more you ask, the less you get: the negative impact of collaborative overload on performance', 'abstract': ""This paper is about the possible negative impact of excessive collaboration on the performance of top employees. With the rise of participatory culture and developments in communications technology, management practices require greater conceptual awareness about possible outcomes of increased organizational interconnectivity. While there exists a sound theoretical basis for possible burdens brought by collaborative overload, the literature never really manage to measure and empirically test this phenomenon. We address this gap by developing a methodological framework for the identification of organizational actors at risk of operational capacity overload. Drawing on social network analysis as the widely applied approach for the estimation of employees' involvement in the information exchange networks, this paper describes potential personal and organizational causes leading to the emergence of collaborative overload. Relying on primary data gathered through a survey conducted among employees in a large insurance company, we present a testable model for overload detection. A second merit of the paper consists in finding a novel identification strategy for empirical works on cross-sectional network data, which often face the issue of endogeneity. This research suggests that active collaborative activity does not cause a decrease throughout every aspect of performance. We found that expertise sharing depends on a few key players who take core knowledge assets upon themselves and thus run higher risks of exposure to overload."", 'corpus_id': 216562322, 'score': 1}, {'doc_id': '219601505', 'title': 'Students’ sense of community and perspectives of taking synchronous and asynchronous online courses', 'abstract': 'The Coronavirus (COVID-19) outbreak has forced all universities in China to shut campuses and moved to distance learning in response to epidemic prevention and control. This is the first time that college courses have been completely delivered online across the nation. Therefore, this study examines Chinese college students’ sense of community and their perspectives of taking online courses in synchronous and asynchronous teaching formats. A total of 1189 students participated in the study from a northeastern university in China. Results indicate that students have a stronger sense of community towards interacting, discussing, and sharing ideas in asynchronous online courses. Findings additionally highlight the benefits of taking courses in these two distance learning formats. For instance, active interaction is often stimulated through synchronous distance learning, while students can learn on their own pace in asynchronous online learning environments. Challenges are also perceived in both formats, such as being distracted by classmates in synchronous online classes or feeling social isolated in asynchronous online classes. It is expected that this study would enlighten Chinese higher education professionals to develop a tight online community and establish a supportive distance learning environment.', 'corpus_id': 219601505, 'score': 0}, {'doc_id': '216035823', 'title': 'A Quantitative Exploration of the 9-Factor Theory: Distribution of Leadership Roles Between Scrum Master and Agile Team', 'abstract': 'A number of qualitative studies find that team leadership is one essential success factor for evolving into a mature agile team. One such qualitative study suggests the 9-Factor Theory of Scrum Master roles, which claims that the Scrum Master performs a set of 9 leadership roles which are transferred to the team over time [14]. We aimed at conducting a quantitative exploration that examines the presence and change of the 9-Factor Theory in relation to team maturity. We conducted an online survey with 67 individuals at the conglomerate Robert Bosch GmbH. Descriptive statistics reveal that the Scrum Master and the agile team score differently on the 9 factors and that the Scrum Master role is most often distributed in teams that had been working between 3 and 5 months in an agile manner. Yet, we also find that the leadership roles predominantly remain with one dedicated Scrum Master. Based on our results we suggest to group the 9-Factor Theory into three clusters: the Scrum Master is rather linked to psychological team factors (1), while the team tends to be linked to rather product-related factors (2). Organizational factors (3) are less often present. Our practical implications suggest an extension of the Scrum Master description. Furthermore, our study lays groundwork for future quantitative testing of leadership in agile teams.', 'corpus_id': 216035823, 'score': 0}]"
162	Amsterdam	3eb8670d999ac077dd0e2c345cb7c905	17272	{}	"[{'doc_id': '233468001', 'title': 'The perpetual chase of banality: Performing long-term new urban tourism in Rotterdam', 'abstract': 'As an emergent way of doing tourism that is rapidly changing urban spaces, the phenomenon of new urban tourism has not been clearly demarcated before in academia. Whereas previous research has set the base for new urban tourists’ characteristics and behaviour, this study aims to further define the notion of new urban tourism and way in which places are constructed by its tourists through their performances. The main research question provides the structure for this thesis and is as follows: ‘How is place image mutually constructed through long-term new urban tourists’ performances and tourism structures created by professionals in the field?‘. Through use of a qualitative case study of Rotterdam involving twelve semi-structured interviews with new urban tourists and three with professionals familiar with the city’s tourism policies, supplemented by content analysis of three policy documents, thematic analysis of the data resulted in four main themes. Firstly, Rotterdam’s policies show how frontrunners and long-term new urban tourists share common ground yet slightly deviate from new urban tourism. The second theme shows how this long-term tourist group fits in with the new urban tourist typology. Yet, they are more likely to construct place image built on constructive authenticity and are heavily influenced by liminality. Thirdly, the activities through which long-term new urban tourists construct authenticity show how they continuously search to live like a local and explore, perform reflexive behaviour and show first signs of a reaction to new urban tourism through pomposity, but mostly highly value immersing and connecting with a place and its people. This connection starts with encounters, comparable to Urry and Larsen’s (2011) Tourist Gaze 3.0, but then further solidifies through understanding a place, establishing emotionaland most prominently personal connection, calling for a possible Tourist Gaze 4.0. Fourthly, it is portrayed whereas shortterm visitors form a place image based on front stage behaviour, long-term new urban tourists engage in immersion with the back stages, which allow for creation of a lasting sense of authenticity. While place branding in its traditional, direct form – being the use of a marketing slogan – is still engaged with by institutions, new urban tourists indicate to prefer branding through an indirect and interpersonal approach like word-of-mouth branding. This supports the main argument that while there can be mutual awareness in the construction of place image, a new urban tourism experience and place image is mostly constructed through new urban tourists’ own activities and construction of authenticity, which is strengthened and added to in the case of long-term tourism.', 'corpus_id': 233468001, 'score': 1}, {'doc_id': '233285166', 'title': 'CONTEMPORARY FORMS OF TOURISM IN DEVELOPMENT AND BRANDING OF TOURIST DESTINATIONS OFFERS-CASE STUDY OF KOTOR', 'abstract': 'When observing the sustainability of the tourism product, it is necessary to identify all forms of tourism that can be an integral part of tourism offer. The involvement and better positioning of cultural tourism will be emphasized, primarily due to the fact that the cultural and natural resources represent a key segment of the tourism product of the observed destinations in this the case of Kotor. The paper will present EU experiences on this issue, present in details a part of an integrated tourism product that relates to cultural resources, reflection of cultural events in the development of tourist destinations, as well as further measures to incorporate the above type of tourism, identify market niches, the said categories as an indicator of potentials in terms of developing the said markets, as well as branding of destinations on this basis.', 'corpus_id': 233285166, 'score': 0}, {'doc_id': '233229866', 'title': 'OVERCOME GAPS AND IDENTIFY CITY BRANDING PERFORMANCE (MEDAN CITY BRANDING)', 'abstract': ""Marketing places as a popular approach and essential instruments to strengthen the region's economy and global competitiveness are growing rapidly in the last decade. The concept of marketing places will rely heavily on how to build, communicate and manage the image of a city. Therefore, the object of city marketing is the city image will eventually be a starting point in developing a city branding. Since January 2012, the city of Medan (3rd of biggest city in Indonesia) uses the marketing approach through city branding: This is Medan!. A logo representing the brand along with the characteristics of the population which welcome visitors and are expected to provide benefits as its philosophical to attract tourists, value of investment or relocation of people to settle in the city of Medan. Balmer’s AC2ID test of corporate identity is applied to identify gaps in that city branding, revealing conflicting messages between local government policy and different stakeholder groups. The result is the city of Medan has lost its essence as a brand, to distinguish fundamental Medan with its competitors. A city branding has to be honest, so communicated identity in accordance with actual identity. Against these results, it is expected the city of Medan need to immediately take action towards the city branding, to prevent an identity crisis in the future. Instead, researcher believes that Medan has great potential into a brand that is controlled by the city government with a focus on stakeholder involvement and physical environment improvement. If this is implemented, it will create a trust (external) and ownership (internal) for the city of Medan."", 'corpus_id': 233229866, 'score': 0}, {'doc_id': '233251188', 'title': 'THE OPPORTUNITY OF DEVELOPING RURAL WELLBEING TOURISM IN PUGLIA REGION, ITALY IN THE TIME OF COVID-19 PANDEMIC', 'abstract': 'The tourism industry is facing nowadays a wide range of issues caused by the COVID-19 pandemic or accentuated by it. The dramatic decline in tourist flows in every destination forced stakeholders to rethink the actual model of business and to adapt to the new reality. The need for social distancing and the fear of tourists to visit crowded places prepared the field for more sustainable tourist activity, based on green products and services. In this context, new and innovative products like rural wellness tourism can take momentum and offer the visitors the opportunity of spending the free time in nature, in less polluted areas and with a higher impact of health. Italy is one of the most representative tourist destinations worldwide, with a wide range of attractions that make both urban and rural spots to be visited annually by millions of tourists. Tourism also supports the local economy and helps local communities from less developed regions to diversify their sources of income, as is the case of Puglia region, known for its agricultural potential. Puglia is one of the Italian regions where local authorities encourage the development of tourist activities, especially those specific to the ecotourism niche, as a real alternative to agriculture. In the current context of the COVID-19 pandemic, ecotourism can represent the solution for a faster economic recovery of the southern region of the country that was dramatically affected by the lockdown imposed by the authorities in the first part of 2020. Based on the wellness tradition from the region and the international reputation of the Termes from this area, Puglia region can develop the concept of rural wellbeing tourism and promote a more sustainable tourist model. The present paper analyses the potential of Puglia region to introduce in its offer rural wellbeing tourism packages taking into account the profile of a spa and thermal center in the area in terms of structure and business model. The results were obtained by applying a structured questionnaire in one of the most representative thermal units in the region and the answers were interpreted using Qualitative Content Analysis. The results show that Puglia region could thrive the concept of rural wellbeing tourism and adapt it to SWS INTERNATIONAL SOCIETY ISSN: 2664 0104 SWS Journal of Social Sciences and Art ISSUE 3, 2020 SWS Journal of Social Sciences and Art DOI 10.35603/SSA2020/ISSUE3.01 2 3 DOI 10.35603/SSA2020/ISSUE3.0', 'corpus_id': 233251188, 'score': 0}, {'doc_id': '233282774', 'title': 'The role of urban branding to attract foreign tourists', 'abstract': ""In today's world, Abstract City-marketing and urban-branding strategies play an essential role in the tourism industry. Tourism activists compete with each other to attract more tourists to their attractions. In promoting a touristic attraction, branding, both physical and spiritual aspects are crucial. Developing tourism infrastructures, built heritage, iconic architecture is essential as well as reinforcement of historical references, an attractive way of life, or cultural values. Since Certain cities are attracting all of the assets and the attention, while others are becoming more or less invisible, one of the critical prerequisites for the success of cities is their overall image, or so-called „city branding.“ In this research, we tried to investigate the urban branding of Kerman-a touristic and historical city located in the south of Iran- and its role in attracting foreign tourists toward this destination. After gathering data through library and field studies and analyzing data, we come to this result that urban branding has a crucial role in attracting foreign tourists to this town. The out coming from the subsidiary hypothesis tests showed that the six aspects of branding effects on attracting tourists. Our results could be useful for promoting Kerman tourism attractions."", 'corpus_id': 233282774, 'score': 1}, {'doc_id': '225539578', 'title': ""Policing the 'Anti-Social' Tourist. Mass Tourism and 'Disorderly Behaviors' in Venice, Amsterdam and Barcelona."", 'abstract': ""In the last years, several cities in Europe and around the world have witnessed the emergence of social movements critical of mass tourism, underlining a diversity of 'externalities' associated to this highly-complex global industry. The so-defined 'anti-social' (i.e. unruly, offensive, inappropriate) behavior of tourists has been highlighted by both social movements and the local and global media among these negative effects of tourism, and local authorities have responded with many campaigns and strategies to regulate the impact of visitors in the lives of locals. By focusing on three European city-cases (namely Amsterdam, Venice and Barcelona), this paper discusses the current efforts to regulate 'disruptive behavior', while examining the limits of these initiatives and the challenges that these approaches create to the daily management of public spaces."", 'corpus_id': 225539578, 'score': 1}, {'doc_id': '233364373', 'title': 'Mass-tourism caused by cruise ships in Tallinn: Reaching for a sustainable way of cruise ship tourism in Tallinn on a social and economic level', 'abstract': 'Although nowadays cruise tourism is on hold due to the COVID-19 pandemic, it is a booming part of the tourism sector, especially in the relatively new destination of Tallinn in Estonia. During the last few decades, tourism in Estonia grew rapidly, including its cruise tourism sector. In mainstream media the impacts cruise tourism has on destinations are mostly limited to mass-tourism and over-tourism, but the impacts are more than the quantity of disembarking passengers. It provides employment and income and it leads to innovation. However, it could also impact a destination in a negative way. Not only by huge numbers of disembarking passengers but also via congestions on roads, an increase in criminal incidents or Disneyfication. This means that cruise tourism has an impact on various dimensions and aspects. As this part of the tourism industry is a sector that keeps growing, it is necessary to guide it in the right direction and to make sure that the negative impacts do not exceed the positive impacts. Therefore, the concept of sustainable tourism has been introduced which aims to maintain or improve the tourism sector at a destination in such a way that it is sustainable for all stakeholders at the social, economic and environmental level. In this thesis, this concept of sustainable tourism is the central point. Related to this concept is the framework of indicators that could show how sustainable cruise tourism is at a particular destination. The framework in this thesis distinguishes two dimensions: the social and the economic. Each dimension contains several indicators, varying from local income to passengers’ spending and from community restrictions to health and wellbeing. These fifteen indicators are operationalized and used to research the sustainability of Tallinn’s cruise tourism. The thesis makes uses of three data collection methods: literature review, interviews and observation. The points of view from the various stakeholders (visitors, industry and hosting community) have been taken into account and are represented. Via the collected data the current state of sustainability of the cruise tourism industry in Tallinn has been investigated. Furthermore, it became possible to research how this industry could be improved and how it could become (more) sustainable. Based on the results, it can be concluded that the cruise tourism industry in Tallinn is relatively sustainable: the economic benefits are present and many people work in the tourism industry, whether or not temporary. The downside of cruise tourism is apparent as well, though this is mainly related to the amount of disembarking passengers. The ratio of residents who live in the touristic area to cruise passengers is developing negatively and at this moment, this is the biggest threat that Tallinn faces. To solve this, the stakeholders of the cruise tourism industry should cooperate and focus on spreading of the cruise passengers. This can be done by spreading the cruise passengers across the city during their disembarking, but also by reconsidering the cruise arrival schedule. To conclude, to tackle the few negative impacts of cruise tourism in Tallinn, one should focus on cooperation. By doing that, the cruise tourism industry in Tallinn will stay an import sector in the economy of the city.', 'corpus_id': 233364373, 'score': 0}, {'doc_id': '199353992', 'title': 'Overcrowded Amsterdam: striving for a balance between trade, tolerance and tourism.', 'abstract': ""Abstract\n This chapter deals with the overcrowding of Amsterdam, one of the world's foremost tourist cities. Insights are provided into how societal and economic changes have influenced the policies of public and private (tourism) sector organizations, city marketer practices, resident attitudes and, more recently, new ways of 'city-making'. Achieving a balance between trade and tolerance has been a recurring mantra for the city since its inception, and particularly now that tourism has become an additional important aspect. The chapter emphasizes the application of co-creation and collaboration as the means for finding inclusive and sustainable tourism solutions for the city."", 'corpus_id': 199353992, 'score': 1}, {'doc_id': '52938688', 'title': 'The Centrality of the Peripheral Nerves: The Anatomical Record Showcases New Findings on Regeneration of Peripheral Nerves in our Latest Thematic Papers Issue', 'abstract': 'In the realm of science there are words that at times “miss the mark,” sometimes even unintentionally denigrating the item or topic that they represent. Take the term “gross,” for example. Both authors of this Editorial proudly teach gross anatomy and have thoroughly enjoyed opening the minds of budding physicians and surgeons by enabling their first mesmerizing encounters with a heart or a brain. Yet when we tell people what we teach, we often engender a sneer of derision. “How gross!” is the sarcastic remark we have often heard, as our students or colleagues see the word in its adjectival meaning of unacceptable, shameless, course, vulgar, unpleasant or (for one of us) fat (as in “I’ve put on weight and feel gross in my pants.”) Uh, no, dear colleagues, the word “gross” in our field denotes that which you see with the naked eye. It is used in contradistinction to “microanatomy,” that which one needs a microscope to visualize. Oh, how many times we have had to explain this (along with our interesting odor) to our family and/or nonanatomical colleagues! In the world of the nervous system a similar relentless denigration has occurred with the topic of this Thematic Papers Issue: Peripheral Nerves. While President Bush the Elder anointed 1990–1999 as “The Decade of the Brain,” with every millimeter of that magnificent orb given focus, attention, and love (not to mention lots of extra funding!) nary a whisper was made about those peripheral nerves. For many, its literal meaning of nerves serving the peripheral corpus away from the brain has been interpreted as secondary, less consequential, innervation. While there is a Peripheral Nerve Society, with its own eponymous journal, that gives a clarion call for peripheral neuroanatomists, their voices are overwhelmed by the cacophony of Brain publications that claim the headlines. To many, peripheral is just that, and the mindset that comes from the name has pushed much crucial recognition and discovery to the back burner. This current Thematic Papers Issue of The Anatomical Record on “Peripheral Nerve Regeneration and Repair,” Guest Edited by internationally renowned neuroanatomist and neurologist Xavier Navarro of the Institut de Neurociencies, Universitat Autonomia de Barcelona, brings peripheral nerve science to the fore by showcasing exciting current research (Navarro et al. 2018, this issue). The issue stemmed from presentations at the 4th International Symposium on Peripheral Nerve Regeneration that occurred 6–8 July 2017 in Barcelona, Spain. José Luis Trejo of the famed Cajal Institute in Madrid, our intrepid Associate Editor specializing in aspects of neuronal circuitry, was integral in working with Professor Navarro in bringing the presentations to our journal and shepherding this wonderful Thematic Issue to fruition (Trejo 2018, this issue.) The Anatomical Record boasts a long and proud history of reporting findings on both the basic descriptive and comparative anatomy of mammalian peripheral nerves sensu lato and also on the specific topic of this issue, peripheral nerve development and regeneration. Indeed, from the very birth of The Anatomical Record, and our first volume in 1907, the topic of nerve growth has been featured. Our first volume included both Abstracts of recent presentations and full-length reportage by none other than Ross Granville Harrison, arguably the greatest scientist of the 20th century who did not receive a Nobel Prize (Nichols 1961; Federoff 1987; Noden 1987; Palay 1987). Harrison’s name and contributions are known and sacred to most anatomists, zoologists, embryologists, and neuroanatomists (if you don’t know about him, shame on you, and move away from that microscope!) A little background on Harrison and his accomplishments are in order. He took his PhD from his hometown Johns Hopkins in 1894 and his MD from Bonn, Germany in 1899 where he met his future wife (this becomes relevant later) then returned to Hopkins to commence his career. In 1907, he moved to Yale, his home for the remainder of his scientific life, both in the Department of Zoology that he chaired, and in the medical school. The list of research students and faculty he mentored is extraordinary (Noden 1987) and place him as the root of many academic lineages (e.g., one of us, JL, whose graduate degrees are from Yale, is an academic great-grandchild of Harrison). Harrison’s genius was multi-dimensional, but arguably his greatest contribution came from his groundbreaking work in tissue culture, of which he is regarded as the founding father (Federoff 1987). Together with his equally creative work on embryonic THE ANATOMICAL RECORD 301:1603–1605 (2018)', 'corpus_id': 52938688, 'score': 0}, {'doc_id': '13610575', 'title': 'When the spell is broken: gentrification, urban tourism and privileged discontent in the Amsterdam canal district', 'abstract': 'Expansion of urban tourism in historic districts in European cities is putting increasing pressure on these areas as places to live. In Amsterdam, an ever-growing number of tourists visit the famous canal district, which also forms the home of a group of long-term, upper-middle-class residents. While such residents are generally depicted as instigators of urban transformation, in this case, they are on the receiving end. Bringing together the literature on the socio-spatial impact of tourism, belonging and the lived experience of place, this article explores the changing relationship between these established residents and their neighbourhood and provides insight into their growing sense of discontent and even powerlessness in the face of neighbourhood change.', 'corpus_id': 13610575, 'score': 1}]"
163	Student Voice and Agency	947deb2919c31f9f59070a1a9e8be87d	11617	{}	"[{'doc_id': '226193234', 'title': 'The Effect of Students’ Experience with the Transition from Primary to Secondary School on Self-Regulated Learning and Motivation', 'abstract': 'The transition from primary to secondary school is more successful when students’ learning is consistent. Students are also more likely to enjoy school, engage with learning, and have a high academic achievement in secondary school when they feel motivated. This is a critical aspect, especially in cases in which global pandemic situations allow only online schooling opportunities. Students that are away from school lack the traditional sources of motivation and self-regulated learning skills; thus, research is needed to identify other important factors that can be developed in remote settings. The aim of this study was to find out how students perceive their experience with the transition from primary to secondary school and how such a transition influences students’ self-regulated learning (SRL) and motivation. Self-reported data were collected during the COVID-19 breakout from a total of n = 80 sixth and seventh grade students aged 12–14 years old. The results showed that students had a successful transition, especially when they were supported by their parents and teachers. Next, bivariate Pearson correlation analysis indicated that students’ perceptions about their experience with the transition from primary to secondary school, their self-regulated learning, and their motivation were significantly correlated. No gender differences were found among any of the main study variables. Teachers can foster students’ SRL skills by implementing effective teaching methods and by guiding them towards SRL-enhancing techniques.', 'corpus_id': 226193234, 'score': 1}, {'doc_id': '224804631', 'title': 'Developing inclusive education in Portugal: Evidence and challenges', 'abstract': 'This article assesses evidence of and challenges to the development of inclusive education in Portugal, which is built on three pillars: access to, participation in, and achievement in education for all children and young people. It presents an overview of the present policy framework, followed by an analysis of available statistical data on Portuguese students with disabilities in mainstream schools. The article also discusses significant achievements at the policy and practice levels, namely the attempt to align curriculum and pedagogy and the presence of almost 100% of students with disabilities in mainstream schools. It also considers challenges, such as the issue of monitoring achievement (both at the student and system level) and investments in the system and in teacher education.', 'corpus_id': 224804631, 'score': 0}, {'doc_id': '222315598', 'title': 'Stepping back and stepping in: Facilitating learner-centered experiences in MOOCs', 'abstract': '\n While the hype around Massive Open Online Courses (MOOCs) has subsided in the past few years, such environments provide a rich opportunity to explore ongoing questions at the intersection of teaching, learning, and technology. This paper explores how a set of facilitation teams described enacting their learner-centered pedagogical aspirations through MOOC platforms. Drawing on in-depth interviews, we present a set of six facilitator actions: “giving up control,” “distributing facilitation,” “being live,” “amplifying,” “modeling,” and “being explicit.” We discuss these actions as emerging from the negotiation between existing pedagogical aspirations and the realities of a new medium, highlighting how they involve facilitators both stepping back (making space for and foregrounding learner expertise and perspectives) and stepping in (intervening and directing as a facilitator). This research contributes to the ongoing work of articulating the substance and specificity of teaching in learner-centered pedagogy and the persistent challenges of enacting that pedagogy in massive, online spaces.\n', 'corpus_id': 222315598, 'score': 1}, {'doc_id': '221780536', 'title': 'Propelling Children’s Empathy and Friendship', 'abstract': 'Schools play a crucial role in creating supportive and safe environments, and positive feelings are key in fostering such environments. Schools as Learning Communities, based on the dialogic participation of the whole community, are improving social cohesion. However, the underlying processes leading to such transformations remain underexplored. This article suggests that successful educational actions (SEAs) implemented in a school as a learning community, analyzed in this case study, promote positive feelings such as friendship and empathy, contributing to a safe and supportive environment. The purpose of this study was to analyze how SEAs generate friendship and empathy and their impact in the environment in a school as a learning community in Spain. To that end, the methods used were interviews with 18 students and 10 teachers, and reviews of two documentary films featuring the school. Results suggest that SEAs generate friendship and empathy among many children by promoting mutual support and sharing narratives in such dialogic settings. In addition, developing friendship and empathy contributes to reducing violent behaviors and promoting more inclusive attitudes among many students. This study concludes by providing insights on how SEAs can contribute to safe and supportive environments through fostering friendship and empathy.', 'corpus_id': 221780536, 'score': 1}, {'doc_id': '224911757', 'title': 'America’s Moment: Investing in Positive Youth Development to Transform Youth and Society', 'abstract': 'As the COVID-19 pandemic wears on, America’s youth are suffering in unprecedented ways as their journey to adulthood is interrupted by multiple societal effects. This thought leader piece explores the power of positive youth development in a time of national crisis. The paper outlines the effects of COVID‑19 on youths’ mental health, educational engagement, and workforce opportunities, all of which have been profoundly affected by the pandemic. The paper makes the case for increasing investment in positive youth development programs and people and highlights key areas where such programs can help support and transform youth, and in-turn society writ large. These areas include increasing equitable access to youth development programs, addressing gaps in opportunities for youth, creating a workforce pipeline, elevating youth voice, and promoting civil discourse and engagement.', 'corpus_id': 224911757, 'score': 1}, {'doc_id': '227249494', 'title': 'Virtual Teaching in the Time of COVID-19: Rethinking Our WEIRD Pedagogical Commitments to Teacher Education', 'abstract': 'Teacher Educators confront a professional future in which online instruction will play an increased role in student learning. As instructional activities are delivered online, a critical challenge for teacher educators will be to continue supporting those ideals key to the missions of many Schools and Colleges of Education—the creation of an instructional environment that is culturally responsive, committed to equity and inclusion, and able to support a diverse and “well” student body.', 'corpus_id': 227249494, 'score': 0}, {'doc_id': '222210245', 'title': 'Learning from the COVID-19 home-schooling experience: Listening to pupils, parents/carers and teachers', 'abstract': 'In Spring 2020, schools in many countries had to close in response to the COVID-19 virus pandemic and move to remote teaching. This paper explores the views of pupils, parents/carers and teachers of ‘home-school’ in one Norwegian municipality, gathered through parallel online surveys in April 2020 during the peak of the COVID-19 lockdown period. It finds that adaptation happened very quickly and that home-school was well received by pupils and parents. There was more creative learning, better progress, more useful feedback and greater student independence. School leaders reported that they wanted to implement changes based on the experience of remote learning enforced by the lockdown, so that the crisis has become an opportunity for grassroots innovation.', 'corpus_id': 222210245, 'score': 0}, {'doc_id': '225078222', 'title': 'Rethinking schools, rethinking learning', 'abstract': 'The COVID-19 pandemic and social unrest offer an opportunity to clarify what learning is and rethink how to design and assess a “good” school. Schools — online, hybrid, or in-person — should foster learning for all students. Yet, too often, schools paradoxically act as both drivers of equity and reproducers of inequities, both inviting and foreclosing certain types of learning. Maxine McKinney de Royston, Carol Lee, Na’ilah Suad Nasir, and Roy Pea argue that a good school takes an expansive understanding of learning and recognizes learning as fundamentally cultural and rooted in human systems of power and ideology.', 'corpus_id': 225078222, 'score': 0}, {'doc_id': '224883518', 'title': ""Prioritizing School Social Workers' Roles and Responsibilities to Combat Oppression in K-12 Schools: Perspectives from Educators with Anti-oppressive Orientations"", 'abstract': 'This study used a subset of data from a larger qualitative research study that investigated anti-oppressive practices in K-12 education. Eleven educators with anti-oppressive orientations provided insight into various ways school social workers can combat oppression in K-12 schools. A flexible coding approach was used to analyze the data. Findings suggest that school social workers should consider prioritizing the following activities to combat oppression in schools: 1) Provide leadership in social justice work and antioppressive practice; 2) Increase visibility and integration on campus and in the classroom; and 3) Complement student interventions with psycho-education and social-emotional support for teachers. The findings support literature that endorses the utilization of systems change strategies in addition to direct interventions. Implications for school social work practice, research, and education are discussed.', 'corpus_id': 224883518, 'score': 0}, {'doc_id': '146404091', 'title': 'Increasing Student Voice in High School Reform', 'abstract': ""While we often write about adolescents as full of turmoil and angst, focusing on `student voice' instead highlights ways in which young people can learn democratic principles by sharing their opinions and working to improve school conditions for themselves and others. This article examines the connection between the types of student voice initiatives desired and the contexts in which student voice is pursued. Drawing upon cases from the USA and Australia, we suggest that turbulence theory can influence the way that student voice is received at a school and its ability to achieve desired goals. Student voice can help to increase the tension and focus on pressing issues when needed; it also can help to calm turbulence occurring within individual adolescents and also in school contexts that need resolution."", 'corpus_id': 146404091, 'score': 1}]"
164	story	bec670e5a55424d840db8636ecc28828	20036	{}	"[{'doc_id': '3144902', 'title': 'Dental Care with Manual Toothbrushes during Fixed Orthodontic Treatment—a New Testing Procedure', 'abstract': 'Aim:The aim of this investigation was to employ a new in-vitro testing system for manual toothbrushes in order to distinguish the more effective from those less so for dental care during fixed appliance treatment.Materials and Methods:The testing apparatus consisted of a sliding carriage able to execute a horizontal brushing movement, and a row of artificial teeth upon which the various toothbrushes were manipulated. The artificial row of teeth was fixed on a sensor that recorded in all three dimensions the forces and moments caused by the toothbrushes on the toothbrush field. All the tests were executed with a weight of 110 g on a tooth field with a multibracket appliance. Tests were also carried out with five toothbrushes having weights of 200 g, 250 g and 300 g. Here, the decisive target values were 1) the degree of exertion necessary in the brushing direction to move a brush over the artificial teeth, and 2) the maximum force occurring in the brushing direction. High target values indicated high interaction between toothbrush bristles and the surfaces being brushed.Results:From testing five toothbrushes with four different weights, we have established profiles confirming the beneficial and less beneficial properties of certain toothbrushes involving various high contact forces.ZusammenfassungZiel:Das Ziel dieser Untersuchung war, ein neues In-vitro-Testsystem für manuelle Zahnbürsten zu nutzen, um günstige von weniger günstigen Beborstungen für die Pflege von Zähnen mit Multibracketapparaturen zu unterscheiden.Material und Methoden:Die Testapparatur bestand aus einem Schlitten, der eine horizontale Bürstbewegung ausführen konnte, und einer künstlichen Zahnreihe, auf der die verschiedenen Zahnbürsten bewegt wurden. Die künstliche Zahnreihe war auf einem Sensor befestigt, der Kräfte und Drehmomente, die durch die Zahnbürsten auf dem Zahnbürstenfeld verursacht wurden, in allen Raumebenen aufzeichnete. Alle Tests wurden mit einem Auflagegewicht von 110 g auf einem Zahnfeld mit einer Multibracketapparatur durchgeführt. Mit fünf Bürsten erfolgten zusätzliche Tests mit Gewichten von 200 g, 250 g und 300 g. Die ausschlaggebenden Zielgrößen waren dabei 1) die Arbeit, die in Putzrichtung geleistet werden musste, um eine Bürste über die künstlichen Zähne zu bewegen, und 2) die maximal auftretenden Kräfte in Putzrichtung. Hohe Zielwerte lassen auf eine hohe Interaktion zwischen Zahnbürstenborsten und den zu reinigenden Flächen schließen.Ergebnisse:Aus den Tests der fünf Zahnbürsten mit vier unterschiedlichen Gewichten ließen sich Profile erstellen, die einer Zahnbürste günstige oder ungünstige Eigenschaften bei verschieden hohen Anpresskräften bescheinigen.', 'corpus_id': 3144902, 'score': 0}, {'doc_id': '213827782', 'title': 'A Character-Centric Neural Model for Automated Story Generation', 'abstract': 'Automated story generation is a challenging task which aims to automatically generate convincing stories composed of successive plots correlated with consistent characters. Most recent generation models are built upon advanced neural networks, e.g., variational autoencoder, generative adversarial network, convolutional sequence to sequence model. Although these models have achieved prompting results on learning linguistic patterns, very few methods consider the attributes and prior knowledge of the story genre, especially from the perspectives of explainability and consistency. To fill this gap, we propose a character-centric neural storytelling model, where a story is created encircling the given character, i.e., each part of a story is conditioned on a given character and corresponded context environment. In this way, we explicitly capture the character information and the relations between plots and characters to improve explainability and consistency. Experimental results on open dataset indicate that our model yields meaningful improvements over several strong baselines on both human and automatic evaluations.', 'corpus_id': 213827782, 'score': 1}, {'doc_id': '237290099', 'title': 'Using BERT Encoding and Sentence-Level Language Model for Sentence Ordering', 'abstract': 'Discovering the logical sequence of events is one of the cornerstones in Natural Language Understanding. One approach to learn the sequence of events is to study the order of sentences in a coherent text. Sentence ordering can be applied in various tasks such as retrieval-based Question Answering, document summarization, storytelling, text generation, and dialogue systems. Furthermore, we can learn to model text coherence by learning how to order a set of shuffled sentences. Previous research has relied on RNN, LSTM, and BiLSTM architecture for learning text language models. However, these networks have performed poorly due to the lack of attention mechanisms. We propose an algorithm for sentence ordering in a corpus of short stories. Our proposed method uses a language model based on Universal Transformers (UT) that captures sentences’ dependencies by employing an attention mechanism. Our method improves the previous state-of-the-art in terms of Perfect Match Ratio (PMR) score in the ROCStories dataset, a corpus of nearly 100K short human-made stories. The proposed model includes three components: Sentence Encoder, Language Model, and Sentence Arrangement with Brute Force Search. The first component generates sentence embeddings using SBERT-WK pre-trained model fine-tuned on the ROCStories data. Then a Universal Transformer network generates a sentence-level language model. For decoding, the network generates a candidate sentence as the following sentence of the current sentence. We use cosine similarity as a scoring function to assign scores to the candidate embedding and the embeddings of other sentences in the shuffled set. Then a Brute Force Search is employed to maximize the sum of similarities between pairs of consecutive sentences.', 'corpus_id': 237290099, 'score': 0}, {'doc_id': '237352999', 'title': 'LOT: A Benchmark for Evaluating Chinese Long Text Understanding and Generation', 'abstract': 'Standard multi-task benchmarks are essential for driving the progress of general pretraining models to generalize to various downstream tasks. However, existing benchmarks such as GLUE and GLGE tend to focus on short text understanding and generation tasks, without considering long text modeling, which requires many distinct capabilities such as modeling long-range commonsense and discourse relations, as well as the coherence and controllability of generation. The lack of standardized benchmarks makes it difficult to fully evaluate these capabilities of a model and fairly compare different models, especially Chinese pretraining models. Therefore, we propose LOT, a benchmark including two understanding and two generation tasks for Chinese long text modeling evaluation. We construct the datasets for the tasks based on various kinds of human-written Chinese stories. Besides, we release an encoderdecoder Chinese long text pretraining model named LongLM with up to 1 billion parameters.We pretrain LongLM on 120G Chinese novels with two generative tasks including text infilling and conditional continuation. Extensive experiments on LOT demonstrate that LongLM matches the performance of similar-sized pretraining models on the understanding tasks and outperforms strong baselines substantially on the generation tasks.', 'corpus_id': 237352999, 'score': 0}, {'doc_id': '236477366', 'title': 'IgSEG: Image-guided Story Ending Generation', 'abstract': 'In this work, we propose a new task called Image-guided Story Ending Generation (IgSEG). Given a multi-sentence story plot and an ending-related image, IgSEG aims to generate a story ending that conforms to the contextual logic and the relevant visual concepts. In contrast to the story ending generation task, which generates open-ended endings, the major challenges of IgSEG are to comprehend the given context and image sufficiently, and mine the appropriate semantics from the image to make the generated story ending informative, reasonable, and coherent. To address the challenges, we propose a Multi-layer Graph convolution and Cascade-LSTM (MGCL) based model which mainly comprises of two collaborative modules: i) a multi-layer graph convolutional network to learn the dependency relations of sentences and the logical clue of the context; ii) a multiple context-image attention module to generate the story endings by gradually incorporating textual and visual semantic concepts. Our MGCL is thus capable of building logically consistent and semantically rich story endings. To evaluate the proposed model, we modify the existing VIST dataset to obtain the VIST-Ending dataset. Empirically, our MGCL outperforms all the strong baselines on both automatic and human evaluation.', 'corpus_id': 236477366, 'score': 1}, {'doc_id': '236478143', 'title': 'Entity-Aware Abstractive Multi-Document Summarization', 'abstract': 'Entities and their mentions convey significant semantic information in documents. In multidocument summarization, the same entity may appear across different documents. Capturing such cross-document entity information can be beneficial – intuitively, it allows the system to aggregate diverse useful information around the same entity for better summarization. In this paper, we present EMSum, an entityaware model for abstractive multi-document summarization. Our model augments the classical Transformer-based encoder-decoder framework with a heterogeneous graph consisting of text units and entities as nodes, which allows rich cross-document information to be captured. In the decoding process, we design a novel two-level attention mechanism, allowing the model to deal with saliency and redundancy issues explicitly. Our model can also be used together with pre-trained language models, arriving at improved performance. We conduct comprehensive experiments on the standard datasets and the results show the effectiveness of our approach.', 'corpus_id': 236478143, 'score': 1}, {'doc_id': '236460044', 'title': 'Capturing Relations between Scientific Papers: An Abstractive Model for Related Work Section Generation', 'abstract': 'Given a set of related publications, related work section generation aims to provide researchers with an overview of the specific research area by summarizing these works and introducing them in a logical order. Most of existing related work section generation models follow the inflexible extractive style, which directly extract sentences from multiple original papers to form a related work discussion. Hence, in this paper, we propose a Relationaware Related work Generator (RRG), which generates an abstractive related work section from multiple scientific papers in the same research area. Concretely, we propose a relationaware multi-document encoder that relates one document to another according to their content dependency in a relation graph. The relation graph and the document representation interact and are refined iteratively, complementing each other in the training process. We also contribute two public datasets composed of related work sections and their corresponding papers1. Extensive experiments on the two datasets show that the proposed model brings substantial improvements over several strong baselines. We hope that this work will promote advances in related work section generation task.', 'corpus_id': 236460044, 'score': 0}, {'doc_id': '221818956', 'title': 'Content Planning for Neural Story Generation with Aristotelian Rescoring', 'abstract': ""Long-form narrative text generated from large language models manages a fluent impersonation of human writing, but only at the local sentence level, and lacks structure or global cohesion. We posit that many of the problems of story generation can be addressed via high-quality content planning, and present a system that focuses on how to learn good plot structures to guide story generation. We utilize a plot-generation language model along with an ensemble of rescoring models that each implement an aspect of good story-writing as detailed in Aristotle's Poetics. We find that stories written with our more principled plot-structure are both more relevant to a given prompt and higher quality than baselines that do not content plan, or that plan in an unprincipled way."", 'corpus_id': 221818956, 'score': 1}, {'doc_id': '236950731', 'title': 'Sentence Semantic Regression for Text Generation', 'abstract': 'Recall the classical text generation works, the generation framework can be briefly divided into two phases: idea reasoning and surface realization. The target of idea reasoning is to figure out the main idea which will be presented in the following talking/writing periods. Surface realization aims to arrange the most appropriate sentence to depict and convey the information distilled from the main idea. However, the current popular tokenby-token text generation methods ignore this crucial process and suffer from many serious issues, such as idea/topic drift. To tackle the problems and realize this twophase paradigm, we propose a new framework named Sentence Semantic Regression (SSR) based on sentence-level language modeling. For idea reasoning, two architectures SSR-AR and SSR-NonAR are designed to conduct sentence semantic regression autoregressively (like GPT2/3) and bidirectionally (like BERT). In the phase of surface realization, a mixed-granularity sentence decoder is designed to generate text with better consistency by jointly incorporating the predicted sentence-level main idea as well as the preceding contextual token-level information. We conduct experiments on four tasks of story ending prediction, story ending generation, dialogue generation, and sentence infilling. The results show that SSR can obtain better performance in terms of automatic metrics and human evaluation.', 'corpus_id': 236950731, 'score': 0}, {'doc_id': '237262883', 'title': 'Augmented Neural Story Generation with Commonsense Inference', 'abstract': 'Transformer-based language model ap001 proaches to automated story generation 002 currently provide state-of-the-art results. 003 However, they still suffer from plot incoher004 ence when generating narratives over time, 005 and critically lack basic commonsense reason006 ing. Furthermore, existing methods generally 007 focus only on single-character stories, or fail 008 to track characters at all. To improve the co009 herence of generated narratives and to expand 010 the scope of character-centric narrative gener011 ation, we introduce Commonsense-inference 012 Augmented neural StoryTelling (CAST), a 013 framework for introducing commonsense 014 reasoning into the generation process while 015 modeling the interaction between multiple 016 characters. We find that our CAST method 017 produces significantly more coherent and 018 on-topic two-character stories, outperforming 019 baselines in dimensions including plot plausi020 bility and staying on topic. We also show how 021 the CAST method can be used to further train 022 language models that generate more coherent 023 stories and reduce computation cost. 024', 'corpus_id': 237262883, 'score': 1}]"
165	AND	558ffc8f5770d8e4f95f51d822685532	19336	{'AND': 'Author Name Disambiguation'}	[{'doc_id': '235790517', 'title': 'Bib2Auth: Deep Learning Approach for Author Disambiguation using Bibliographic Data', 'abstract': 'Author name ambiguity remains a critical open problem in digital libraries due to synonymy and homonymy of names. In this paper, we propose a novel approach to link author names to their real-world entities by relying on their co-authorship pattern and area of research. Our supervised deep learning model identifies an author by capturing his/her relationship with his/her co-authors and area of research, which is represented by the titles and sources of the target author’s publications. These attributes are encoded by their semantic and symbolic representations. To this end, Bib2Auth uses ∼ 22K bibliographic records from DBLP repository and is trained with each pair of co-authors. The extensive experiments have proved the capability of the approach to distinguish between authors sharing the same name and recognize authors with different name variations. Bib2Auth has shown good performance on a relatively large dataset, which qualifies it to be directly integrated into bibliographic indices.', 'corpus_id': 235790517, 'score': 1}, {'doc_id': '235396850', 'title': 'Name Disambiguation Based on Graph Convolutional Network', 'abstract': 'Recently, massive online academic resources have provided convenience for scientific study and research. However, the author name ambiguity degrades the user experience in retrieving the literature bases. Extracting the features of papers and calculating the similarity for clustering constitute themainstreamof present name disambiguation approaches, which can be divided into two branches: clustering based on attribute features and clustering based on linkage information.)ey cannot however get high performance. In order to improve the efficiency of literature retrieval and provide technical support for the accurate construction of literature bases, a name disambiguation method based on Graph Convolutional Network (GCN) is proposed.)e disambiguation model based on GCN designed in this paper combines both attribute features and linkage information. We first build paper-to-paper graphs, coauthor graphs, and paper-to-author graphs for each reference item of a name.)e nodes in the graphs contain attribute features and the edges contain linkage features.)e graphs are then fed to a specialized GCN and output a hybrid representation. Finally, we use the hierarchical clustering algorithm to divide the papers into disjoint clusters. Finally, we cluster the papers using a hierarchical algorithm.)e experimental results show that the proposedmodel achieves average F1 value of 77.10% on three name disambiguation datasets. In order to let themodel automatically select the appropriate number of convolution layers and adapt to the structure of different local graphs, we improve upon the prior GCN model by utilizing attention mechanism. Compared with the original GCN model, it increases the average precision and F1 value by 2.05% and 0.63%, respectively.What is more, we build a bilingual dataset, BAT, which contains various forms of academic achievements and will be an alternative in future research of name disambiguation.', 'corpus_id': 235396850, 'score': 1}, {'doc_id': '236460062', 'title': 'Benchmarking Scalable Methods for Streaming Cross Document Entity Coreference', 'abstract': 'Streaming cross document entity coreference (CDC) systems disambiguate mentions of named entities in a scalable manner via incremental clustering. Unlike other approaches for named entity disambiguation (e.g., entity linking), streaming CDC allows for the disambiguation of entities that are unknown at inference time. Thus, it is well-suited for processing streams of data where new entities are frequently introduced. Despite these benefits, this task is currently difficult to study, as existing approaches are either evaluated on datasets that are no longer available, or omit other crucial details needed to ensure fair comparison. In this work, we address this issue by compiling a large benchmark adapted from existing free datasets, and performing a comprehensive evaluation of a number of novel and existing baseline models.1 We investigate: how to best encode mentions, which clustering algorithms are most effective for grouping mentions, how models transfer to different domains, and how bounding the number of mentions tracked during inference impacts performance. Our results show that the relative performance of neural and feature-based mention encoders varies across different domains, and in most cases the best performance is achieved using a combination of both approaches. We also find that performance is minimally impacted by limiting the number of tracked mentions.', 'corpus_id': 236460062, 'score': 0}, {'doc_id': '26385203', 'title': 'AiX-Analytics: Analytics Tool at RWTH Aachen University', 'abstract': 'In this paper we present the AiX-Analytics prototype. The tool is a web-based prototype that visualizes usage statistics and analytics of the learning platform’s log data. The prototype has three parts: data management, RESTful application engine, and user interface(s). The data management receives, cleans, analyzes and aggregates the learning platform logs, and saves the derived results and analytics. The RESTful application engine is a Web API application that provides data and analytics to the front-end (UI). The user interface provides a set of visualizations, which interactively presents the learning data for different stakeholders in higher education scenarios. The user interface has two main aspects. The first is intended for the university’s administration and eLearning coordinators, in order to help in understanding how different faculties and institutes use the learning platform. The second aspect focuses on individual courses, where the teaching staff and students can take a look which modules, when, and how were used during the ongoing semester(s). The presented work is an ongoing research project at the Learning Technologies Research Group at RWTH Aachen University.', 'corpus_id': 26385203, 'score': 0}, {'doc_id': '205395413', 'title': 'Generalized anxiety disorder.', 'abstract': 'GENERAL FEATURES • Generalized anxiety disorder is characterized by excessive, uncontrollable worry that causes distress and interferes with a patient’s ability to function normally. • Generalized anxiety disorder is twice as common in women as it is in men and affects nearly 7 million adults in the United States. • Comorbid illnesses often found with generalized anxiety disorder include major depressive disorder, panic disorder, phobias, and other anxiety disorders such as obsessive-compulsive disorder, social anxiety, and posttraumatic stress disorder. • Patients commonly present with physical complaints related to a heightened state of arousal. These may include insomnia; fatigue; headaches; and neck, shoulder, or back pain.', 'corpus_id': 205395413, 'score': 0}, {'doc_id': '235428128', 'title': 'Chinese Personal Name Disambiguation Based on Clustering', 'abstract': 'Personal name disambiguation is a significant issue in natural language processing, which is the basis for many tasks in automatic information processing. This research explores the Chinese personal name disambiguation based on clustering technique. Preprocessing is applied to transform raw corpus into standardized format at the beginning. And then, Chinese word segmentation, part-of-speech tagging, and named entity recognition are accomplished by lexical analysis. Furthermore, we make an effort to extract features that can better disambiguate Chinese personal names. Some rules for identifying target personal names are created to improve the experimental effect. Additionally, many calculation methods of feature weights are implemented such as bool weight, absolute frequency weight, tf-idf weight, and entropy weight. As for clustering algorithm, an agglomerative hierarchical clustering is selected by comparison with other clustering methods. Finally, a labeling approach is employed to bring forward feature words that can represent each cluster. The experiment achieves a good result for five groups of Chinese personal names.', 'corpus_id': 235428128, 'score': 1}, {'doc_id': '232233421', 'title': 'S2AND: A Benchmark and Evaluation System for Author Name Disambiguation', 'abstract': 'Author Name Disambiguation (AND) is the task of resolving which author mentions in a bibliographic database refer to the same real-world person, and is a critical ingredient of digital library applications such as search and citation analysis. While many AND algorithms have been proposed, comparing them is difficult because they often employ distinct features and are evaluated on different datasets. In response to this challenge, we present S2AND, a unified benchmark dataset for AND on scholarly papers, as well as an open-source reference model implementation. Our dataset harmonizes eight disparate AND datasets into a uniform format, with a single rich feature set drawn from the Semantic Scholar S2 database. Our evaluation suite for S2AND reports performance split by facets like publication year and number of papers, allowing researchers to track both global performance and measures of fairness across facet values. Our experiments show that because previous datasets tend to cover idiosyncratic and biased slices of the literature, algorithms trained to perform well on one on them may generalize poorly to others. By contrast, we show how training on a union of datasets in S2AND results in more robust models that perform well even on datasets unseen in training. The resulting AND model also substantially improves over the production algorithm in S2, reducing error by over 50% in terms of B^3 F1. We release our unified dataset, model code, trained models, and evaluation suite to the research community. this https URL', 'corpus_id': 232233421, 'score': 1}, {'doc_id': '236486108', 'title': 'Recovering Lexically and Semantically Reused Texts', 'abstract': 'Writers often repurpose material from existing texts when composing new documents. Because most documents have more than one source, we cannot trace these connections using only models of document-level similarity. Instead, this paper considers methods for local text reuse detection (LTRD), detecting localized regions of lexically or semantically similar text embedded in otherwise unrelated material. In extensive experiments, we study the relative performance of four classes of neural and bag-of-words models on three LTRD tasks – detecting plagiarism, modeling journalists’ use of press releases, and identifying scientists’ citation of earlier papers. We conduct evaluations on three existing datasets and a new, publicly-available citation localization dataset. Our findings shed light on a number of previously-unexplored questions in the study of LTRD, including the importance of incorporating document-level context for predictions, the applicability of of-the-shelf neural models pretrained on “general” semantic textual similarity tasks such as paraphrase detection, and the trade-offs between more efficient bag-of-words and feature-based neural models and slower pairwise neural models.', 'corpus_id': 236486108, 'score': 0}, {'doc_id': '235433328', 'title': 'KaggleDBQA: Realistic Evaluation of Text-to-SQL Parsers', 'abstract': 'The goal of database question answering is to enable natural language querying of real-life relational databases in diverse application domains. Recently, large-scale datasets such as Spider andWikiSQL facilitated novel modeling techniques for text-to-SQL parsing, improving zero-shot generalization to unseen databases. In this work, we examine the challenges that still prevent these techniques from practical deployment. First, we present KaggleDBQA, a new cross-domain evaluation dataset of real Web databases, with domain-specific data types, original formatting, and unrestricted questions. Second, we re-examine the choice of evaluation tasks for text-to-SQL parsers as applied in real-life settings. Finally, we augment our in-domain evaluation task with database documentation, a naturally occurring source of implicit domain knowledge. We show that KaggleDBQA presents a challenge to state-ofthe-art zero-shot parsers but a more realistic evaluation setting and creative use of associated database documentation boosts their accuracy by over 13.2%, doubling their performance.', 'corpus_id': 235433328, 'score': 0}, {'doc_id': '235660282', 'title': 'Aggregating large-scale databases for PubMed author name disambiguation', 'abstract': 'OBJECTIVE\nPubMed has suffered from the author ambiguity problem for many years. Existing studies on author name disambiguation (AND) for PubMed only used internal metadata for development. However, some of them are incomplete (eg, a large number of names are only abbreviated and their full names are not available) or less discriminative. To this end, we present a new disambiguation method, namely AggAND, by aggregating information from external databases.\n\n\nMATERIALS AND METHODS\nWe address this issue by exploring Microsoft Academic Graph, Semantic Scholar, and PubMed Knowledge Graph to enhance the built-in name metadata, and extend the internal metadata with some external and more discriminative metadata.\n\n\nRESULTS\nExperimental results on enhanced name metadata demonstrate comparable performance to 3 author identifier systems, as well as show superiority over the original name metadata. More importantly, our method, AggAND, incorporating both enhanced name and extended metadata, yields F1 scores of 95.80% and 93.71% on 2 datasets and outperforms the state-of-the-art method by a large margin (3.61% and 6.55%, respectively).\n\n\nCONCLUSIONS\nThe feasibility and good performance of our methods not only help better understand the importance of external databases for disambiguation, but also point to a promising direction for future AND studies in which information aggregated from multiple bibliographic databases can be effective in improving disambiguation performance. The methodology shown here can be generalized to broader bibliographic databases beyond PubMed. Our code and data are available online (https://github.com/carmanzhang/PubMed-AND-method).', 'corpus_id': 235660282, 'score': 1}]
166	human	99e9bae675b12967251c175696f00a70	8990	{}	"[{'doc_id': '221397221', 'title': 'LiftFormer: 3D Human Pose Estimation using attention models', 'abstract': ""Estimating the 3D position of human joints has become a widely researched topic in the last years. Special emphasis has gone into defining novel methods that extrapolate 2-dimensional data (keypoints) into 3D, namely predicting the root-relative coordinates of joints associated to human skeletons. The latest research trends have proven that the Transformer Encoder blocks aggregate temporal information significantly better than previous approaches. Thus, we propose the usage of these models to obtain more accurate 3D predictions by leveraging temporal information using attention mechanisms on ordered sequences human poses in videos. \nOur method consistently outperforms the previous best results from the literature when using both 2D keypoint predictors by 0.3 mm (44.8 MPJPE, 0.7% improvement) and ground truth inputs by 2mm (MPJPE: 31.9, 8.4% improvement) on Human3.6M. It also achieves state-of-the-art performance on the HumanEva-I dataset with 10.5 P-MPJPE (22.2% reduction). The number of parameters in our model is easily tunable and is smaller (9.5M) than current methodologies (16.95M and 11.25M) whilst still having better performance. Thus, our 3D lifting model's accuracy exceeds that of other end-to-end or SMPL approaches and is comparable to many multi-view methods."", 'corpus_id': 221397221, 'score': 1}, {'doc_id': '67856425', 'title': 'Deep High-Resolution Representation Learning for Human Pose Estimation', 'abstract': 'In this paper, we are interested in the human pose estimation problem with a focus on learning reliable high-resolution representations. Most existing methods recover high-resolution representations from low-resolution representations produced by a high-to-low resolution network. Instead, our proposed network maintains high-resolution representations through the whole process. We start from a high-resolution subnetwork as the first stage, gradually add high-to-low resolution subnetworks one by one to form more stages, and connect the mutli-resolution subnetworks in parallel. We conduct repeated multi-scale fusions such that each of the high-to-low resolution representations receives information from other parallel representations over and over, leading to rich high-resolution representations. As a result, the predicted keypoint heatmap is potentially more accurate and spatially more precise. We empirically demonstrate the effectiveness of our network through the superior pose estimation results over two benchmark datasets: the COCO keypoint detection dataset and the MPII Human Pose dataset. In addition, we show the superiority of our network in pose tracking on the PoseTrack dataset. The code and models have been publicly available at https://github.com/leoxiaobin/deep-high-resolution-net.pytorch.', 'corpus_id': 67856425, 'score': 1}, {'doc_id': '13613792', 'title': 'Stacked Hourglass Networks for Human Pose Estimation', 'abstract': 'This work introduces a novel convolutional network architecture for the task of human pose estimation. Features are processed across all scales and consolidated to best capture the various spatial relationships associated with the body. We show how repeated bottom-up, top-down processing used in conjunction with intermediate supervision is critical to improving the performance of the network. We refer to the architecture as a “stacked hourglass” network based on the successive steps of pooling and upsampling that are done to produce a final set of predictions. State-of-the-art results are achieved on the FLIC and MPII benchmarks outcompeting all recent methods.', 'corpus_id': 13613792, 'score': 1}, {'doc_id': '149067327', 'title': 'Judicial Considerations of Reasonable Conduct by Survivors of Child Sexual Abuse', 'abstract': 'The consequences of child sexual abuse are summarised focusing on post-traumatic stress disorder (PTSD), before outlining the statutory provisions for extensions of time. Case studies of applications by survivors of this abuse with PTSD to extend time are synthesised.', 'corpus_id': 149067327, 'score': 0}, {'doc_id': '220665598', 'title': 'Graph-PCNN: Two Stage Human Pose Estimation with Graph Pose Refinement', 'abstract': 'Recently, most of the state-of-the-art human pose estimation methods are based on heatmap regression. The final coordinates of keypoints are obtained by decoding heatmap directly. In this paper, we aim to find a better approach to get more accurate localization results. We mainly put forward two suggestions for improvement: 1) different features and methods should be applied for rough and accurate localization, 2) relationship between keypoints should be considered. Specifically, we propose a two-stage graph-based and model-agnostic framework, called Graph-PCNN, with a localization subnet and a graph pose refinement module added onto the original heatmap regression network. In the first stage, heatmap regression network is applied to obtain a rough localization result, and a set of proposal keypoints, called guided points, are sampled. In the second stage, for each guided point, different visual feature is extracted by the localization subnet. The relationship between guided points is explored by the graph pose refinement module to get more accurate localization results. Experiments show that Graph-PCNN can be used in various backbones to boost the performance by a large margin. Without bells and whistles, our best model can achieve a new state-of-the-art 76.8% AP on COCO test-dev split.', 'corpus_id': 220665598, 'score': 0}, {'doc_id': '206592152', 'title': 'DeepPose: Human Pose Estimation via Deep Neural Networks', 'abstract': 'We propose a method for human pose estimation based on Deep Neural Networks (DNNs). The pose estimation is formulated as a DNN-based regression problem towards body joints. We present a cascade of such DNN regres- sors which results in high precision pose estimates. The approach has the advantage of reasoning about pose in a holistic fashion and has a simple but yet powerful formula- tion which capitalizes on recent advances in Deep Learn- ing. We present a detailed empirical analysis with state-of- art or better performance on four academic benchmarks of diverse real-world images.', 'corpus_id': 206592152, 'score': 1}, {'doc_id': '221507903', 'title': 'SSP-Net: Scalable Sequential Pyramid Networks for Real-Time 3D Human Pose Regression', 'abstract': 'In this paper we propose a highly scalable convolutional neural network, end-to-end trainable, for real-time 3D human pose regression from still RGB images. We call this approach the Scalable Sequential Pyramid Networks (SSP-Net) as it is trained with refined supervision at multiple scales in a sequential manner. Our network requires a single training procedure and is capable of producing its best predictions at 120 frames per second (FPS), or acceptable predictions at more than 200 FPS when cut at test time. We show that the proposed regression approach is invariant to the size of feature maps, allowing our method to perform multi-resolution intermediate supervisions and reaching results comparable to the state-of-the-art with very low resolution feature maps. We demonstrate the accuracy and the effectiveness of our method by providing extensive experiments on two of the most important publicly available datasets for 3D pose estimation, Human3.6M and MPI-INF-3DHP. Additionally, we provide relevant insights about our decisions on the network architecture and show its flexibility to meet the best precision-speed compromise.', 'corpus_id': 221507903, 'score': 0}, {'doc_id': '53806352', 'title': '3D Human Pose Estimation in Video With Temporal Convolutions and Semi-Supervised Training', 'abstract': 'In this work, we demonstrate that 3D poses in video can be effectively estimated with a fully convolutional model based on dilated temporal convolutions over 2D keypoints. We also introduce back-projection, a simple and effective semi-supervised training method that leverages unlabeled video data. We start with predicted 2D keypoints for unlabeled video, then estimate 3D poses and finally back-project to the input 2D keypoints. In the supervised setting, our fully-convolutional model outperforms the previous best result from the literature by 6 mm mean per-joint position error on Human3.6M, corresponding to an error reduction of 11%, and the model also shows significant improvements on HumanEva-I. Moreover, experiments with back-projection show that it comfortably outperforms previous state-of-the-art results in semi-supervised settings where labeled data is scarce. Code and models are available at https://github.com/facebookresearch/VideoPose3D', 'corpus_id': 53806352, 'score': 1}, {'doc_id': '220633382', 'title': 'HDNet: Human Depth Estimation for Multi-Person Camera-Space Localization', 'abstract': 'Current works on multi-person 3D pose estimation mainly focus on the estimation of the 3D joint locations relative to the root joint and ignore the absolute locations of each pose. In this paper, we propose the Human Depth Estimation Network (HDNet), an end-to-end framework for absolute root joint localization in the camera coordinate space. Our HDNet first estimates the 2D human pose with heatmaps of the joints. These estimated heatmaps serve as attention masks for pooling features from image regions corresponding to the target person. A skeleton-based Graph Neural Network (GNN) is utilized to propagate features among joints. We formulate the target depth regression as a bin index estimation problem, which can be transformed with a soft-argmax operation from the classification output of our HDNet. We evaluate our HDNet on the root joint localization and root-relative 3D pose estimation tasks with two benchmark datasets, i.e., Human3.6M and MuPoTS-3D. The experimental results show that we outperform the previous state-of-the-art consistently under multiple evaluation metrics. Our source code is available at: this https URL.', 'corpus_id': 220633382, 'score': 0}, {'doc_id': '87608582', 'title': 'Physicochemical and antioxidant properties of spray dried preparations from Psidium guajava L', 'abstract': 'This work evaluated the antioxidant activity and physicochemical properties of spray dried preparations (SDP) from leaves of Psidium guajava L. Different drying carriers, namely: maltodextrin, colloidal silicon dioxide, Arabic gum and β-cyclodextrin at concentrations of 40 and 80% relative to solids content were added to drying composition. SDP were characterized through determination of the total phenolic, tannins and flavonoid content. Antioxidant activity of the SDP was assessed by two assays: cellular test that measure the luminol-enhanced chemiluminescence (LumCL) produced by neutrophils stimulated with phorbol myristate acetate (PMA) and the DPPH radical scavenging. SDP containing 40% of maltodextrin showed the best result with respect to total phenolics (18.09% w/w), tannins (10.21% w/w) and flavonoid contents (16.56% w/w). SDP were effective inhibitors of the PMA-stimulated neutrophil function [IC50 (in µg/106cells) ranging from 5.42 to 6.50 µg/mL. LumCL (IC50 of quercetin=1.67 µg/mL]. SDP showed antioxidant activities (IC50) ranging from 6.16 to 10.31 µg/mL by the DPPH method (IC50 of quercetin=0.96 µg/mL). The neutrophil reactive oxygen species generation, triggered by PMA and assessed by luminol was inhibited by SDP in a concentration-dependent manner, with insignificant toxicity to the cells under the tested conditions. Two chemical markers, catechin and quercetin which could be used in the monitoring of the quality of SDP were identified. Structural information of the compounds was obtained from the retention times, UV and mass spectra. In conclusion, SDP from Psidium guajava presented significant antioxidant activity with high potential as an active phytopharmaceutical ingredient for herbal medicine.', 'corpus_id': 87608582, 'score': 0}]"
167	Rana freezing	96fed301679fe09723101cbde252dd15	20233	{}	"[{'doc_id': '19918652', 'title': 'Inoculative freezing by environmental ice nuclei in the freeze-tolerant wood frog, Rana sylvatica.', 'abstract': 'Efficacy of inoculative freezing by ice nuclei in a simulated winter environment was studied in the wood frog (Rana sylvatica), a freeze-tolerant species that overwinters on the forest floor beneath organic detritus. Adult frogs were confined to plastic canisters and cooled to -2 degrees C over 24 hr with their ventral skin in contact with substrate (humic soil hydrated to 40, 10, or 5%, or soil/peat mixture hydrated to 20 or 10%, w/w), or their dorsal skin in contact with damp leaf mould. Whereas only 20% of control frogs cooled in dry, plastic canisters froze, freezing occurred in nearly all (98%) frogs contacting soil or leaf mould. Inoculation was briefly delayed in frogs exposed to drier substrates. Frogs exposed to an unfreezable substrate (humic soil, 5% moisture) themselves froze, apparently due to the action of constituent nuclei which commonly occur in natural materials. Although the surface over which inoculation can occur is greater in larger frogs, inoculation susceptibility was not correlated with body mass in our frogs (mean +/- SE body mass = 14.0 +/- 0.2 g; range, 9.8-17.8 g). We conclude that the high susceptibility to inoculative freezing in R. sylvatica, which is conferred by its moist, highly permeable integument, promotes freeze tolerance by ensuring that inoculation commences at relatively high temperatures.', 'corpus_id': 19918652, 'score': 1}, {'doc_id': '43990076', 'title': 'Seasonality of Freeze Tolerance in a Subarctic Population of the Wood Frog, Rana sylvatica', 'abstract': 'We compared physiological characteristics and responses to experimental freezing and thawing in winter and spring samples of the wood frog, Rana sylvatica, indigenous to Interior Alaska, USA. Whereas winter frogs can survive freezing at temperatures at least as low as −16°C, the lower limit of tolerance for spring frogs was between −2.5°C and −5°C. Spring frogs had comparatively low levels of the urea in blood plasma, liver, heart, brain, and skeletal muscle, as well as a smaller hepatic reserve of glycogen, which is converted to glucose after freezing begins. Consequently, following freezing (−2.5°C, 48\u2009h) tissue concentrations of these cryoprotective osmolytes were 44–88% lower than those measured in winter frogs. Spring frogs formed much more ice and incurred extensive cryohemolysis and lactate accrual, indicating that they had suffered marked cell damage and hypoxic stress during freezing. Multiple, interactive stresses, in addition to diminished cryoprotectant levels, contribute to the reduced capacity for freeze tolerance in posthibernal frogs.', 'corpus_id': 43990076, 'score': 1}, {'doc_id': '237216381', 'title': 'Reproductive Dormancy in Overwintering Adult Eucryptorrhynchus brandti (Coleoptera: Curculionidae)', 'abstract': 'Abstract Dormancy is important for overwintering insects to resist and adapt to adverse conditions. Dormancy generally contains quiescence and diapause. Eucryptorrhynchus brandti Harold (Coleoptera: Curculionidae), tree-of-heaven trunk weevil (TTW), is a destructive pest and highly host-specific to Ailanthus altissima in China. TTW has one generation per year and overwinters as both larvae and adults. In this study, to examined dormancy type of adults and find a method to store overwintering adults, we collected adults from 20 October 2018 to 13 March 2019. We studied the behavior and reproductive development of adults under field cold conditions for 0 and 10 d and laboratory warm conditions for 5 and 10 d. We recorded developing eggs in females, and the clarity of the testis edge, the yellow point in the testis lobe, the ratio of the inner content in the accessory gland, and the accessory gland color in males. Adults transferred from the field to the laboratory had resumed reproductive development directly. Results indicated that the dormancy type of TTW adults was quiescence. Adults stored in the field were still in a dormant state and the field-storage method was effective. Current study provided basic data for controlling overwintering TTW adults and solve the storage of insect sources during the winter.', 'corpus_id': 237216381, 'score': 0}, {'doc_id': '236429731', 'title': 'Winter survival of the unicellular green alga Micrasterias denticulata: insights from field monitoring and simulation experiments.', 'abstract': 'Peat bog pools around Tamsweg (Lungau, Austria) are typical habitats of the unicellular green alga Micrasterias denticulata. By measurement of water temperature and irradiation throughout a 1-year period (2018/2019), it was intended to assess the natural environmental strain in winter. Freezing resistance of Micrasterias cells and their ability to frost harden and become tolerant to ice encasement were determined after natural hardening and exposure to a cold acclimation treatment that simulated the natural temperature decrease in autumn. Transmission electron microscopy (TEM) was performed in laboratory-cultivated cells, after artificial cold acclimation treatment and in cells collected from field. Throughout winter, the peat bog pools inhabited by Micrasterias remained unfrozen. Despite air temperature minima down to -17.3 °C, the water temperature was mostly close to +0.8 °C. The alga was unable to frost harden, and upon ice encasement, the cells showed successive frost damage. Despite an unchanged freezing stress tolerance, significant ultrastructural changes were observed in field-sampled cells and in response to the artificial cold acclimation treatment: organelles such as the endoplasmic reticulum and thylakoids of the chloroplast showed distinct membrane bloating. Still, in the field samples, the Golgi apparatus appeared in an impeccable condition, and multivesicular bodies were less frequently observed suggesting a lower overall stress strain. The observed ultrastructural changes in winter and after cold acclimation are interpreted as cytological adjustments to winter or a resting state but are not related to frost hardening as Micrasterias cells were unable to improve their freezing stress tolerance.', 'corpus_id': 236429731, 'score': 0}, {'doc_id': '237344158', 'title': 'Metabolic responses of plasma to extreme environments in overwintering Tibetan frogs Nanorana parkeri: a metabolome integrated analysis', 'abstract': 'Many animals lower their metabolic rate in response to low temperatures and scarcity of food in the winter in phenomena called hibernation or overwintering. Living at high altitude on the Tibetan Plateau where winters are very cold, the frog Nanorana parkeri , survives in one of the most hostile environments on Earth but, to date, relatively little is known about the biochemical and physiological adjustments for overwintering by this species. The present study profiled changes in plasma metabolites of N. parkeri between winter and summer using UHPLC-QE-MS non-target metabolomics in order to explore metabolic adaptations that support winter survival. The analysis showed that, in total, 11 metabolites accumulated and 95 were reduced in overwintering frogs compared with summer-active animals. Metabolites that increased included some that may have antioxidant functions (canthaxanthin, galactinol), act as a metabolic inhibitor (mono-ethylhexylphthalate), or accumulate as a product of anaerobic metabolism (lactate). Most other metabolites in plasma showed reduced levels in winter and were generally involved in energy metabolism including 11 amino acids (proline, isoleucine, leucine, valine, phenylalanine, tyrosine, arginine, tryptophan, methionine, threonine and histidine) and 4 carbohydrates (glucose, citrate, succinate, and malate). Pathway analysis indicated that aminoacyl-tRNA biosynthesis, phenylalanine, tyrosine and tryptophan biosynthesis, and nitrogen metabolism were potentially the most prominently altered pathways in overwintering frogs. Changes to these pathways are likely due to fasting and global metabolic depression in overwintering frogs. Concentrations of glucose and urea, commonly used as cryoprotectants by amphibians that winter on land, were significantly reduced during underwater hibernation in N. parkeri . In conclusion, winter survival of the high-altitude frog, N. parkeri was accompanied by substantial changes in metabolomic profiles and this study provides valuable information towards understanding the special adaptive mechanisms of N. parkeri to winter stresses.', 'corpus_id': 237344158, 'score': 1}, {'doc_id': '84943201', 'title': 'Dynamics of body water during freezing and thawing in a freeze-tolerant frog (Rana sylvatica)', 'abstract': 'Abstract 1. 1.|Calorimetric analyses showed that wood frogs ( Rana sylvatica ) frozen to −2.5°C contained 7.8 g ice, as 65.4% of the body water had frozen. 2. 2.|During 24 h of freezing, water content decreased in liver (58.9%), intestine (58.6%) and skeletal muscle (22–36%). Complete rehydration during thawing at 3.5°C required from 3 to >;48 h, depending on the organ. 3. 3.|Because organs dehydrate, increases in tissue metabolite concentrations associated with freezing, if calculated on a per wet-weight basis, may be greatly exaggerated. 4. 4.|Reversible organ dehydration during freezing may enhance freeze tolerance of R. sylvatica by concentrating cryoprotectant and reducing cryoinjury to tissues.', 'corpus_id': 84943201, 'score': 1}, {'doc_id': '236255610', 'title': 'Variation in thermal tolerance response associated with geographic location during early development of the neogastropod Ocenebra erinaceus (Linnaeus, 1758)', 'abstract': ""Abstract Environmental temperature plays an important role in shaping the distribution and abundance of marine ectothermic organisms. As a general rule, larvae and juveniles are more sensitive to thermal stress than adults and, as a consequence, represent key life stages that determine in part the geographic range of a species. Identifying critical thermal limits during ontogeny allows for the prediction of the potential impacts of climate warming on the distribution of marine ectotherms. However, thermal tolerance - and therefore the potential to meet the challenge of warming- is known to vary at population scale for many species. In order to fully appreciate a species' future under climate warming, multiple populations studies from different thermal environments are necessary. In this study, we compared the thermal tolerance response during the intracapsular development of the marine gastropod Ocenebra erinaceus between two geographically separated populations: one from the middle (Solent, UK) and another from the south of the species' geographic range (Arcachon, France). The results show that the thermal tolerance response was influenced by geographic origin. Embryos from the relatively warm-water southern population (France) show a warm-eurythermal tolerance window with optimal temperatures between 12 and 18\xa0°C. On the contrary, embryos from the cold-water northern population (UK) exhibit a narrow, warm-stenothermal, thermal tolerance window with optimal temperatures between 14 and 16\xa0°C. In both populations, temperatures outside of the thermal range cause lethal and sub-lethal effects. Importantly, previously observed dispersal polymorphism was not observed at hatching time in either population in our study. Our study demonstrates that during early developmental stages, embryos are adapted to local thermal conditions and that they live very close to their upper thermal limits. Temperatures outside this range cause detrimental and contrasting effects on embryonic development of O. erinaceus , implying that the effects of future warming will depend on the population response to local environmental history. Our results suggest that global warming could shift the geographical distribution range of O. erinaceus poleward."", 'corpus_id': 236255610, 'score': 0}, {'doc_id': '237321555', 'title': 'Adaptation of an Invasive Pest to Novel Environments: Life History Traits of Drosophila suzukii in Coastal and Mainland Areas of Greece during Overwintering', 'abstract': 'Simple Summary Drosophila suzukii, also known as the spotted wing Drosophila, is a notorious pest of several high-value fruits including strawberries and sweet cherries. Adult D. suzukii flies exhibit two morphs: summer morphs (SM) and winter morphs (WM). The two seasonal phenotypes help this pest to perform better in temperate climates. WM have a darker cuticle and larger wings compared to SM, while WM females experience reproductive dormancy. We estimated the lifespan, the reproductive status of females and the number of produced offspring for WM and SM exposed to mild and cold winter field conditions, prevailing in two different geographic areas (coastal and mainland). Overall, WM exhibited a longer lifespan than SM and this difference was more pronounced for adults kept in the cold mainland area. The majority of SM females produced offspring during overwintering in the mild coastal area, but only a few SM were reproductively active in the cold mainland area. Some WM females produced progeny during overwintering in the mild conditions of the coastal area, but all WM females were in reproductive arrest in the mainland area. Overwintering females in the coastal area had a shorter lifespan and produced more progeny than those kept in the mainland area. High survival rates of WM provide indications of the successful performance of this phenotype in the adverse conditions of the cold climates. Additionally, the continuous reproductive activity of SM females and the onset of progeny production by WM females during overwintering in the coastal area indicate that the insect remains reproductively active throughout the year in areas with mild climatic conditions. Our findings support the successful adaptation of D. suzukii in both areas tested and can be used for the development of area-specific population models, based on the prevailing climatic conditions. Abstract Drosophila suzukii is a polyphagous pest of small and soft fruit, originating from Asia, which has spread and established in Europe and the USA. Adults exhibit seasonal phenotypes, i.e., summer morphs (SM) and winter morphs (WM) to cope with fluctuating environmental conditions. WM have a darker cuticle and larger wings compared to SM, while WM females experience reproductive dormancy. We studied the life history traits (lifespan, female reproductive status and number of produced offspring) of WM and SM that were exposed to winter field conditions of a coastal and a mainland agricultural area, with mild and cold winter climates, respectively. Mated adults of each phenotype were individually placed in vials bearing nutritional/oviposition substrate, and transferred to the field from November 2019 to May 2020, when the death of the last individual was recorded. Almost all SM females (90%) and no WM female carried mature ovarioles before being transferred to the field. WM exhibited a longer lifespan than SM adjusting for location and sex. Differences in survival between the two phenotypes were more pronounced for adults kept in the mainland area. The majority of SM females produced offspring during overwintering in the mild coastal area, but only a few SM were reproductively active in the cold mainland area. Some WM females produced progeny during overwintering in the mild conditions of the coastal area, but all WM females were in reproductive arrest in the mainland area. Overwintering females in the coastal area had a shorter lifespan and produced more progeny than those kept in the mainland area. High survival rates of WM provide indications of the successful performance of this phenotype in the adverse conditions of the cold climates. Additionally, the continuous reproductive activity of SM females and the onset of progeny production by WM females during overwintering in the coastal area indicate that the insect remains reproductively active throughout the year in areas with mild climatic conditions. Our findings support the successful adaptation of D. suzukii in both areas tested and can be used for the development of area-specific population models, based on the prevailing climatic conditions.', 'corpus_id': 237321555, 'score': 0}, {'doc_id': '23519208', 'title': 'Hepatocyte responses to in vitro freezing and β-adrenergic stimulation: Insights into the extreme freeze tolerance of subarctic Rana sylvatica.', 'abstract': 'The wood frog, Rana sylvatica LeConte 1825, is a freeze-tolerant amphibian widely distributed in North America. Subarctic populations of this species can survive experimental freezing to temperatures below -16 °C, whereas temperate populations tolerate freezing only at temperatures above -6 °C. We investigated whether hepatocytes isolated from frogs indigenous to Interior Alaska (subarctic) or southern Ohio (temperate) had distinct characteristics that could contribute to this variation in freeze tolerance capacity. Following in vitro freezing, cell damage, as assessed from lactate dehydrogenase leakage, was similar between samples from Alaskan and Ohioan frogs. Preincubation of cells in media containing glucose or urea, the two primary cryoprotectants used by R. sylvatica, markedly reduced freezing damage to hepatocytes; however, results suggested that cells of the northern phenotype were comparatively more amenable to cryoprotection by urea. Stimulation of isolated hepatocytes with β-adrenergic agonists, which simulates the freezing-induced cryoprotectant mobilization response, gave rates of glucose production from endogenous glycogen reserves that were similar between the populations. Our findings suggest that extreme freeze tolerance in subarctic R. sylvatica does not require an enhanced ability of the liver to resist freezing stress or rapidly mobilize cryoprotectant.', 'corpus_id': 23519208, 'score': 1}, {'doc_id': '237122370', 'title': 'Rapid climate change-related growth decline at the southern range edge of Fagus sylvatica', 'abstract': 'Studies on Fagus sylvatica show that growth in populations toward the southern limit of this species’ distribution is limited strongly by drought. Warming temperatures in the Mediterranean region are expected to exacerbate drought where they are not accompanied by increases in precipitation. We studied levels of annual growth in mature F. sylvatica trees over the last half-century in the Montseny Mountains in Catalonia (northeast Spain). Our results show significantly lower growth of mature trees at the lower limit of this species’ distribution when compared with trees at higher altitudes. Growth at the lower Fagus limit is characterized by a rapid recent decline starting in approximately 1975. By 2003, growth of mature trees had fallen by 49% when compared with predecline levels. This is not an age-related phenomenon, nor is it seen in comparable populations at higher altitudes. Analysis of climate-growth relationships suggests that the observed decline in growth is a result of warming temperatures and that, as precipitation in the region has not increased, precipitation is now insufficient to ameliorate the negative effects of increased temperatures on tree growth. As the climateresponse of the studied forest is comparable with that of F. sylvatica forests in other southern European regions, it is possible that this growth decline is a more widespread phenomenon. Warming temperatures may lead to a rapid decline in the growth of rangeedge populations and a consequent retreat of the species distribution in southern Europe. Assessment of long-term growth trends across the southern range edge of F. sylvatica therefore merits further attention.', 'corpus_id': 237122370, 'score': 0}]"
168	Systematic Generalization / Compositionality	6c030250c435f67b0bbb2078f2dc4373	1500	{}	"[{'doc_id': '211204736', 'title': 'REALM: Retrieval-Augmented Language Model Pre-Training', 'abstract': 'Language model pre-training has been shown to capture a surprising amount of world knowledge, crucial for NLP tasks such as question answering. However, this knowledge is stored implicitly in the parameters of a neural network, requiring ever-larger networks to cover more facts. \nTo capture knowledge in a more modular and interpretable way, we augment language model pre-training with a latent knowledge retriever, which allows the model to retrieve and attend over documents from a large corpus such as Wikipedia, used during pre-training, fine-tuning and inference. For the first time, we show how to pre-train such a knowledge retriever in an unsupervised manner, using masked language modeling as the learning signal and backpropagating through a retrieval step that considers millions of documents. \nWe demonstrate the effectiveness of Retrieval-Augmented Language Model pre-training (REALM) by fine-tuning on the challenging task of Open-domain Question Answering (Open-QA). We compare against state-of-the-art models for both explicit and implicit knowledge storage on three popular Open-QA benchmarks, and find that we outperform all previous methods by a significant margin (4-16% absolute accuracy), while also providing qualitative benefits such as interpretability and modularity.', 'corpus_id': 211204736, 'score': 0}, {'doc_id': '209515274', 'title': 'oLMpics-On What Language Model Pre-training Captures', 'abstract': 'Abstract Recent success of pre-trained language models (LMs) has spurred widespread interest in the language capabilities that they possess. However, efforts to understand whether LM representations are useful for symbolic reasoning tasks have been limited and scattered. In this work, we propose eight reasoning tasks, which conceptually require operations such as comparison, conjunction, and composition. A fundamental challenge is to understand whether the performance of a LM on a task should be attributed to the pre-trained representations or to the process of fine-tuning on the task data. To address this, we propose an evaluation protocol that includes both zero-shot evaluation (no fine-tuning), as well as comparing the learning curve of a fine-tuned LM to the learning curve of multiple controls, which paints a rich picture of the LM capabilities. Our main findings are that: (a) different LMs exhibit qualitatively different reasoning abilities, e.g., RoBERTa succeeds in reasoning tasks where BERT fails completely; (b) LMs do not reason in an abstract manner and are context-dependent, e.g., while RoBERTa can compare ages, it can do so only when the ages are in the typical range of human ages; (c) On half of our reasoning tasks all models fail completely. Our findings and infrastructure can help future work on designing new datasets, models, and objective functions for pre-training.', 'corpus_id': 209515274, 'score': 0}, {'doc_id': '225040262', 'title': 'Stronger Transformers for Neural Multi-Hop Question Generation', 'abstract': 'Prior work on automated question generation has almost exclusively focused on generating simple questions whose answers can be extracted from a single document. However, there is an increasing interest in developing systems that are capable of more complex multi-hop question generation, where answering the questions requires reasoning over multiple documents. In this work, we introduce a series of strong transformer models for multi-hop question generation, including a graph-augmented transformer that leverages relations between entities in the text. While prior work has emphasized the importance of graph-based models, we show that we can substantially outperform the state-of-the-art by 5 BLEU points using a standard transformer architecture. We further demonstrate that graph-based augmentations can provide complimentary improvements on top of this foundation. Interestingly, we find that several important factors--such as the inclusion of an auxiliary contrastive objective and data filtering could have larger impacts on performance. We hope that our stronger baselines and analysis provide a constructive foundation for future work in this area.', 'corpus_id': 225040262, 'score': 0}, {'doc_id': '215827766', 'title': 'Compositionality and Generalization In Emergent Languages', 'abstract': 'Natural language allows us to refer to novel composite concepts by combining expressions denoting their parts according to systematic rules, a property known as compositionality. In this paper, we study whether the language emerging in deep multi-agent simulations possesses a similar ability to refer to novel primitive combinations, and whether it accomplishes this feat by strategies akin to human-language compositionality. Equipped with new ways to measure compositionality in emergent languages inspired by disentanglement in representation learning, we establish three main results: First, given sufficiently large input spaces, the emergent language will naturally develop the ability to refer to novel composite concepts. Second, there is no correlation between the degree of compositionality of an emergent language and its ability to generalize. Third, while compositionality is not necessary for generalization, it provides an advantage in terms of language transmission: The more compositional a language is, the more easily it will be picked up by new learners, even when the latter differ in architecture from the original agents. We conclude that compositionality does not arise from simple generalization pressure, but if an emergent language does chance upon it, it will be more likely to survive and thrive.', 'corpus_id': 215827766, 'score': 1}, {'doc_id': '221139844', 'title': 'Compositional Generalization via Neural-Symbolic Stack Machines', 'abstract': 'Despite achieving tremendous success, existing deep learning models have exposed limitations in compositional generalization, the capability to learn compositional rules and apply them to unseen cases in a systematic manner. To tackle this issue, we propose the Neural-Symbolic Stack Machine (NeSS). It contains a neural network to generate traces, which are then executed by a symbolic stack machine enhanced with sequence manipulation operations. NeSS combines the expressive power of neural sequence models with the recursion supported by the symbolic stack machine. Without training supervision on execution traces, NeSS achieves 100% generalization performance in four domains: the SCAN benchmark of language-driven navigation tasks, the task of few-shot learning of compositional instructions, the compositional machine translation benchmark, and context-free grammar parsing tasks.', 'corpus_id': 221139844, 'score': 1}, {'doc_id': '208974153', 'title': 'Learning Compositional Rules via Neural Program Synthesis', 'abstract': 'Many aspects of human reasoning, including language, require learning rules from very little data. Humans can do this, often learning systematic rules from very few examples, and combining these rules to form compositional rule-based systems. Current neural architectures, on the other hand, often fail to generalize in a compositional manner, especially when evaluated in ways that vary systematically from training. In this work, we present a neuro-symbolic model which learns entire rule systems from a small set of examples. Instead of directly predicting outputs from inputs, we train our model to induce the explicit system of rules governing a set of previously seen examples, drawing upon techniques from the neural program synthesis literature. Our rule-synthesis approach outperforms neural meta-learning techniques in three domains: an artificial instruction-learning domain used to evaluate human learning, the SCAN challenge datasets, and learning rule-based translations of number words into integers for a wide range of human languages.', 'corpus_id': 208974153, 'score': 1}, {'doc_id': '212658007', 'title': 'A Benchmark for Systematic Generalization in Grounded Language Understanding', 'abstract': 'Human language users easily interpret expressions that describe unfamiliar situations composed from familiar parts (""greet the pink brontosaurus by the ferris wheel""). Modern neural networks, by contrast, struggle to interpret compositions unseen in training. In this paper, we introduce a new benchmark, gSCAN, for evaluating compositional generalization in models of situated language understanding. We take inspiration from standard models of meaning composition in formal linguistics. Going beyond an earlier related benchmark that focused on syntactic aspects of generalization, gSCAN defines a language grounded in the states of a grid world. This allows us to build novel generalization tasks that probe the acquisition of linguistically motivated rules. For example, agents must understand how adjectives such as \'small\' are interpreted relative to the current world state or how adverbs such as \'cautiously\' combine with new verbs. We test a strong multi-modal baseline model and a state-of-the-art compositional method finding that, in most cases, they fail dramatically when generalization requires systematic compositional rules.', 'corpus_id': 212658007, 'score': 1}, {'doc_id': '225067188', 'title': 'Modularity Improves Out-of-Domain Instruction Following', 'abstract': 'We propose a modular architecture for following natural language instructions that describe sequences of diverse subgoals, such as navigating to landmarks or picking up objects. Standard, non-modular, architectures used in instruction following do not exploit subgoal compositionality and often struggle on out-of-distribution tasks and environments. In our approach, subgoal modules each carry out natural language instructions for a specific subgoal type. A sequence of modules to execute is chosen by learning to segment the instructions and predicting a subgoal type for each segment. When compared to standard sequence-to-sequence approaches on ALFRED, a challenging instruction following benchmark, we find that modularization improves generalization to environments unseen in training and to novel tasks.', 'corpus_id': 225067188, 'score': 0}, {'doc_id': '216562596', 'title': 'Unnatural Language Processing: Bridging the Gap Between Synthetic and Natural Language Data', 'abstract': ""Large, human-annotated datasets are central to the development of natural language processing models. Collecting these datasets can be the most challenging part of the development process. We address this problem by introducing a general purpose technique for ``simulation-to-real'' transfer in language understanding problems with a delimited set of target behaviors, making it possible to develop models that can interpret natural utterances without natural training data. We begin with a synthetic data generation procedure, and train a model that can accurately interpret utterances produced by the data generator. To generalize to natural utterances, we automatically find projections of natural language utterances onto the support of the synthetic language, using learned sentence embeddings to define a distance metric. With only synthetic training data, our approach matches or outperforms state-of-the-art models trained on natural language data in several domains. These results suggest that simulation-to-real transfer is a practical framework for developing NLP applications, and that improved models for transfer might provide wide-ranging improvements in downstream tasks."", 'corpus_id': 216562596, 'score': 1}, {'doc_id': '229156144', 'title': 'Iterative Utterance Segmentation for Neural Semantic Parsing', 'abstract': 'Neural semantic parsers usually fail to parse long and complex utterances into correct meaning representations, due to the lack of exploiting the principle of compositionality. To address this issue, we present a novel framework for boosting neural semantic parsers via iterative utterance segmentation. Given an input utterance, our framework iterates between two neural modules: a segmenter for segmenting a span from the utterance, and a parser for mapping the span into a partial meaning representation. Then, these intermediate parsing results are composed into the final meaning representation. One key advantage is that this framework does not require any handcraft templates or additional labeled data for utterance segmentation: we achieve this through proposing a novel training method, in which the parser provides pseudo supervision for the segmenter. Experiments on Geo, ComplexWebQuestions, and Formulas show that our framework can consistently improve performances of neural semantic parsers in different domains. On data splits that require compositional generalization, our framework brings significant accuracy gains: Geo 63.1 to 81.2, Formulas 59.7 to 72.7, ComplexWebQuestions 27.1 to 56.3.', 'corpus_id': 229156144, 'score': 0}]"
169	IMU-odo	e05f7b0204e82e954798f407f7144bd5	2638	{'IMU': 'inertial measurement unit'}	"[{'doc_id': '201830604', 'title': 'End-to-End Learning Framework for IMU-Based 6-DOF Odometry', 'abstract': 'This paper presents an end-to-end learning framework for performing 6-DOF odometry by using only inertial data obtained from a low-cost IMU. The proposed inertial odometry method allows leveraging inertial sensors that are widely available on mobile platforms for estimating their 3D trajectories. For this purpose, neural networks based on convolutional layers combined with a two-layer stacked bidirectional LSTM are explored from the following three aspects. First, two 6-DOF relative pose representations are investigated: one based on a vector in the spherical coordinate system, and the other based on both a translation vector and an unit quaternion. Second, the loss function in the network is designed with the combination of several 6-DOF pose distance metrics: mean squared error, translation mean absolute error, quaternion multiplicative error and quaternion inner product. Third, a multi-task learning framework is integrated to automatically balance the weights of multiple metrics. In the evaluation, qualitative and quantitative analyses were conducted with publicly-available inertial odometry datasets. The best combination of the relative pose representation and the loss function was the translation and quaternion together with the translation mean absolute error and quaternion multiplicative error, which obtained more accurate results with respect to state-of-the-art inertial odometry techniques.', 'corpus_id': 201830604, 'score': 1}, {'doc_id': '220363761', 'title': 'Preintegrated IMU Features For Efficient Deep Inertial Odometry', 'abstract': ""MEMS Inertial Measurement Units (IMUs) are inexpensive and effective sensors that provide proprioceptive motion measurements for many robots and consumer devices. However, their noise characteristics and manufacturing imperfections lead to complex ramifications in classical fusion pipelines. While deep learning models provide the required flexibility to model these complexities from data, they have higher computation and memory requirements, making them impractical choices for low-power and embedded applications. This paper attempts to address the mentioned conflict by proposing a computationally, efficient inertial representation for deep inertial odometry. Replacing the raw IMU data in deep Inertial models, preintegrated features improves the model's efficiency. The effectiveness of this method has been demonstrated for the task of pedestrian inertial odometry, and its efficiency has been shown through its embedded implementation on a microcontroller with restricted resources."", 'corpus_id': 220363761, 'score': 1}, {'doc_id': '220363628', 'title': 'TLIO: Tight Learned Inertial Odometry', 'abstract': 'In this letter we propose a tightly-coupled Extended Kalman Filter framework for IMU-only state estimation. Strap-down IMU measurements provide relative state estimates based on IMU kinematic motion model. However the integration of measurements is sensitive to sensor bias and noise, causing significant drift within seconds. Recent research by Yan et al. (RoNIN) and Chen et al. (IONet) showed the capability of using trained neural networks to obtain accurate 2D displacement estimates from segments of IMU data and obtained good position estimates from concatenating them. This letter demonstrates a network that regresses 3D displacement estimates and its uncertainty, giving us the ability to tightly fuse the relative state measurement into a stochastic cloning EKF to solve for pose, velocity and sensor biases. We show that our network, trained with pedestrian data from a headset, can produce statistically consistent measurement and uncertainty to be used as the update step in the filter, and the tightly-coupled system outperforms velocity integration approaches in position estimates, and AHRS attitude filter in orientation estimates. Video materials and code can be found on our project page: http://cathias.github.io/TLIO/.', 'corpus_id': 220363628, 'score': 1}, {'doc_id': '119111419', 'title': 'AI-IMU Dead-Reckoning', 'abstract': 'In this paper, we propose a novel accurate method for dead-reckoning of wheeled vehicles based only on an Inertial Measurement Unit (IMU). In the context of intelligent vehicles, robust and accurate dead-reckoning based on the IMU may prove useful to correlate feeds from imaging sensors, to safely navigate through obstructions, or for safe emergency stops in the extreme case of exteroceptive sensors failure. The key components of the method are the Kalman filter and the use of deep neural networks to dynamically adapt the noise parameters of the filter. The method is tested on the KITTI odometry dataset, and our dead-reckoning inertial method based only on the IMU accurately estimates 3D position, velocity, orientation of the vehicle and self-calibrates the IMU biases. We achieve on average a 1.10% translational error and the algorithm competes with top-ranked methods which, by contrast, use LiDAR or stereo vision.', 'corpus_id': 119111419, 'score': 1}, {'doc_id': '166119370', 'title': 'El presupuesto por áreas de gestión como herramienta de control en la asignación de recursos por parte de la Alcaldía municipal de Santa Cruz Michapa, departamento de Cuscatlán.', 'abstract': 'El Gobierno de El Salvador ha implementado como parte del proceso de modernizacion el Presupuesto General de la Nacion, empleando la tecnica por Areas de Gestion, siendo las Dependencias Centralizadas y Descentralizadas de la Republica las obligadas a presentar su presupuesto bajo esta tecnica. Sin embargo en la actualidad la Alcaldia Municipal de Santa Cruz Michapa no ha implementado adecuadamente la tecnica, lo que conlleva a que esta no pueda orientar correctamente la asignacion de recursos utilizados por objetivos o metas. Dentro de las principales deficiencias detectadas en el diagnostico realizado, se identifico la necesidad de un instrumento tecnico que oriente en el proceso de formulacion presupuestaria y que a su vez aporte los conocimientos teoricos y tecnicos al personal involucrado, a fin de contribuir a solventarla problematica existente. Dentro de los factores que influyen para que se den estas deficiencias es la centralizacion de la planificacion y el proceso operativo de formulacion presupuestaria, lo que con lleva a una participacion parcial de los mandos medios, excluyendo la opinion y experiencia del personal involucrado directamente en la ejecucion de los objetivos y proyectos de la municipalidad. Por lo tanto se presenta a la Alcaldia Municipal de Santa Cruz Michapa el Diseno de un Sistema de Presupuesto por Areas de Gestion que contribuya a la asignacion y distribucion de los recursos de forma eficiente, exponiendo las fases del proceso presupuestario.', 'corpus_id': 166119370, 'score': 0}, {'doc_id': '170079303', 'title': 'RoNIN: Robust Neural Inertial Navigation in the Wild: Benchmark, Evaluations, & New Methods', 'abstract': 'This paper sets a new foundation for data-driven inertial navigation research, where the task is the estimation of horizontal positions and heading direction of a moving subject from a sequence of IMU sensor measurements from a phone. In contrast to existing methods, our method can handle varying phone orientations and placements.More concretely, the paper presents 1) a new benchmark containing more than 40 hours of IMU sensor data from 100 human subjects with ground-truth 3D trajectories under natural human motions; 2) novel neural inertial navigation architectures, making significant improvements for challenging motion cases; and 3) qualitative and quantitative evaluations of the competing methods over three inertial navigation benchmarks. We share the code and data to promote further research. (http://ronin.cs.sfu.ca).', 'corpus_id': 170079303, 'score': 1}, {'doc_id': '221186591', 'title': 'DronePose: Photorealistic UAV-Assistant Dataset Synthesis for 3D Pose Estimation via a Smooth Silhouette Loss', 'abstract': 'In this work we consider UAVs as cooperative agents supporting human users in their operations. In this context, the 3D localisation of the UAV assistant is an important task that can facilitate the exchange of spatial information between the user and the UAV. To address this in a data-driven manner, we design a data synthesis pipeline to create a realistic multimodal dataset that includes both the exocentric user view, and the egocentric UAV view. We then exploit the joint availability of photorealistic and synthesized inputs to train a single-shot monocular pose estimation model. During training we leverage differentiable rendering to supplement a state-of-the-art direct regression objective with a novel smooth silhouette loss. Our results demonstrate its qualitative and quantitative performance gains over traditional silhouette objectives. Our data and code are available at this https URL', 'corpus_id': 221186591, 'score': 0}, {'doc_id': '97888511', 'title': 'New red light-emitting conjugated rigid-rod polymer: poly(benzobisthiazole-1,4-phenylenebisvinylene)', 'abstract': 'The solution-processable poly(benzobisthiazole-1,4-phenylenebisvinylene) (PBTPV) is synthesized. The protonated PBTPV in dilute methanesulfonic acid solutions exhibit a yellow-green emission with a peak at 544 nm and fluorescence quantum yield of 100%. PBTPV thin films, prepared from its soluble complexes in nitromethane, emit red light with an emission peak at 640 nm with an estimated quantum yield of ∼ 4-5%. PBTPV starts to decompose at 560°C under nitrogen', 'corpus_id': 97888511, 'score': 0}, {'doc_id': '221266488', 'title': 'Towards Resilient Autonomous Navigation of Drones', 'abstract': 'Robots and particularly drones are especially useful in exploring extreme environments that pose hazards to humans. To ensure safe operations in these situations, usually perceptually degraded and without good GNSS, it is critical to have a reliable and robust state estimation solution. The main body of literature in robot state estimation focuses on developing complex algorithms favoring accuracy. Typically, these approaches rely on a strong underlying assumption: the main estimation engine will not fail during operation. In contrast, we propose an architecture that pursues robustness in state estimation by considering redundancy and heterogeneity in both sensing and estimation algorithms. The architecture is designed to expect and detect failures and adapt the behavior of the system to ensure safety. To this end, we present HeRO (Heterogeneous Redundant Odometry): a stack of estimation algorithms running in parallel supervised by a resiliency logic. This logic carries out three main functions: a) perform confidence tests both in data quality and algorithm health; b) re-initialize those algorithms that might be malfunctioning; c) generate a smooth state estimate by multiplexing the inputs based on their quality. The state and quality estimates are used by the guidance and control modules to adapt the mobility behaviors of the system. The validation and utility of the approach are shown with real experiments on a flying robot for the use case of autonomous exploration of subterranean environments, with particular results from the STIX event of the DARPA Subterranean Challenge.', 'corpus_id': 221266488, 'score': 0}, {'doc_id': '221266081', 'title': 'Towards Autonomous Driving: a Multi-Modal 360$^{\\circ}$ Perception Proposal', 'abstract': 'In this paper, a multi-modal 360$^{\\circ}$ framework for 3D object detection and tracking for autonomous vehicles is presented. The process is divided into four main stages. First, images are fed into a CNN network to obtain instance segmentation of the surrounding road participants. Second, LiDAR-to-image association is performed for the estimated mask proposals. Then, the isolated points of every object are processed by a PointNet ensemble to compute their corresponding 3D bounding boxes and poses. Lastly, a tracking stage based on Unscented Kalman Filter is used to track the agents along time. The solution, based on a novel sensor fusion configuration, provides accurate and reliable road environment detection. A wide variety of tests of the system, deployed in an autonomous vehicle, have successfully assessed the suitability of the proposed perception stack in a real autonomous driving application.', 'corpus_id': 221266081, 'score': 0}]"
170	Style transfer	4b159ef96ec260b9d48d513adb3f676d	6175	{}	[{'doc_id': '114021962', 'title': 'Are we getting the best out of our intranet system? A practical case study', 'abstract': 'The construction industry as other industries is exposed to considerable advancement in information technology. Construction companies are no longer judged exclusively by the level of their past performance but also how they can sell their expertise and services by employing the latest technology. Investment in new hardware and software technologies is necessary to gain a competitive edge but this investment needs to be justified particularly by demonstrating that companies are actually benefiting from the investment It is also vitally important that the knowledge gained on construction projects is captured and shared for continuous improvement to avoid ‘re-inventing the wheel’ and to prevent repetition of previous mistakes. This is particularly useful for construction companies that undertake PFI (Private Finance Initiative) projects where they are responsible for maintenance of the building for a long period of time. By using a practical case study, this paper examines how one of the largest privately owned European companies provides information & knowledge to their employees, what their employees feel about the system and if the company is getting maximum return for their investment.', 'corpus_id': 114021962, 'score': 0}, {'doc_id': '218516586', 'title': 'Russian Natural Language Generation: Creation of a Language Modelling Dataset and Evaluation with Modern Neural Architectures', 'abstract': 'Generating coherent, grammatically correct, and meaningful text is very challenging, however, it is crucial to many modern NLP systems. So far, research has mostly focused on English language, for other languages both standardized datasets, as well as experiments with state-of-the-art models, are rare. In this work, we i) provide a novel reference dataset for Russian language modeling, ii) experiment with popular modern methods for text generation, namely variational autoencoders, and generative adversarial networks, which we trained on the new dataset. We evaluate the generated text regarding metrics such as perplexity, grammatical correctness and lexical diversity.', 'corpus_id': 218516586, 'score': 0}, {'doc_id': '218889852', 'title': 'CERT: Contrastive Self-supervised Learning for Language Understanding', 'abstract': 'Pretrained language models such as BERT, GPT have shown great effectiveness in language understanding. The auxiliary predictive tasks in existing pretraining approaches are mostly defined on tokens, thus may not be able to capture sentence-level semantics very well. To address this issue, we propose CERT: Contrastive self-supervised Encoder Representations from Transformers, which pretrains language representation models using contrastive self-supervised learning at the sentence level. CERT creates augmentations of original sentences using back-translation. Then it finetunes a pretrained language encoder (e.g., BERT) by predicting whether two augmented sentences originate from the same sentence. CERT is simple to use and can be flexibly plugged into any pretraining-finetuning NLP pipeline. We evaluate CERT on 11 natural language understanding tasks in the GLUE benchmark where CERT outperforms BERT on 7 tasks, achieves the same performance as BERT on 2 tasks, and performs worse than BERT on 2 tasks. On the averaged score of the 11 tasks, CERT outperforms BERT. The data and code are available at https://github.com/UCSD-AI4H/CERT', 'corpus_id': 218889852, 'score': 1}, {'doc_id': '123924360', 'title': 'Impurity effects on trapped electron mode in tokamak plasmas', 'abstract': 'The effects of impurity ions on the trapped electron mode (TEM) in tokamak plasmas are numerically investigated with the gyrokinetic integral eigenmode equation. It is shown that in the case of large electron temperature gradient ( ηe), the impurity ions have stabilizing effects on the TEM, regardless of peaking directions of their density profiles for all normalized electron density gradient R/Lne. Here, R is the major radius and Lne is the electron density gradient scale length. In the case of intermediate and/or small ηe, the light impurity ions with conventional inwardly (outwardly) peaked density profiles have stabilizing effects on the TEM for large (small) R/Lne, while the light impurity ions with steep inwardly (outwardly) peaked density profiles can destabilize the TEM for small (large) R/Lne. Besides, the TEM driven by density gradient is stabilized (destabilized) by the light carbon or oxygen ions with inwardly (outwardly) peaked density profiles. In particular, for flat and/or moderate R/Lne, ...', 'corpus_id': 123924360, 'score': 0}, {'doc_id': '218684947', 'title': 'GPT-too: A Language-Model-First Approach for AMR-to-Text Generation', 'abstract': 'Abstract Meaning Representations (AMRs) are broad-coverage sentence-level semantic graphs. Existing approaches to generating text from AMR have focused on training sequence-to-sequence or graph-to-sequence models on AMR annotated data only. In this paper, we propose an alternative approach that combines a strong pre-trained language model with cycle consistency-based re-scoring. Despite the simplicity of the approach, our experimental results show these models outperform all previous techniques on the English LDC2017T10 dataset, including the recent use of transformer architectures. In addition to the standard evaluation metrics, we provide human evaluation experiments that further substantiate the strength of our approach.', 'corpus_id': 218684947, 'score': 0}, {'doc_id': '216562404', 'title': 'MultiMix: A Robust Data Augmentation Strategy for Cross-Lingual NLP', 'abstract': 'Transfer learning has yielded state-of-the-art results in many supervised natural language processing tasks. However, annotated data for every target task in every target language is rare, especially for low-resource languages. In this work, we propose MultiMix, a novel data augmentation method for semi-supervised learning in zero-shot transfer learning scenarios. In particular, MultiMix targets to solve cross-lingual adaptation problems from a source (language) distribution to an unknown target (language) distribution assuming it has no training labels in the target language task. In its heart, MultiMix performs simultaneous self-training with data augmentation and unsupervised sample selection. To show its effectiveness, we have performed extensive experiments on zero-shot transfers for cross-lingual named entity recognition (XNER) and natural language inference (XNLI). Our experiments show sizeable improvements in both tasks outperforming the baselines by a good margin.', 'corpus_id': 216562404, 'score': 0}, {'doc_id': '219721430', 'title': 'Fine-grained Sentiment Controlled Text Generation', 'abstract': 'Controlled text generation techniques aim to regulate specific attributes (e.g. sentiment) while preserving the attribute independent content. The state-of-the-art approaches model the specified attribute as a structured or discrete representation while making the content representation independent of it to achieve a better control. However, disentangling the text representation into separate latent spaces overlooks complex dependencies between content and attribute, leading to generation of poorly constructed and not so meaningful sentences. Moreover, such an approach fails to provide a finer control on the degree of attribute change. To address these problems of controlled text generation, in this paper, we propose DE-VAE, a hierarchical framework which captures both information enriched entangled representation and attribute specific disentangled representation in different hierarchies. DE-VAE achieves better control of sentiment as an attribute while preserving the content by learning a suitable lossless transformation network from the disentangled sentiment space to the desired entangled representation. Through feature supervision on a single dimension of the disentangled representation, DE-VAE maps the variation of sentiment to a continuous space which helps in smoothly regulating sentiment from positive to negative and vice versa. Detailed experiments on three publicly available review datasets show the superiority of DE-VAE over recent state-of-the-art approaches.', 'corpus_id': 219721430, 'score': 1}, {'doc_id': '218531919', 'title': 'Improving Disentangled Text Representation Learning with Information-Theoretic Guidance', 'abstract': 'Learning disentangled representations of natural language is essential for many NLP tasks, e.g., conditional text generation, style transfer, personalized dialogue systems, etc. Similar problems have been studied extensively for other forms of data, such as images and videos. However, the discrete nature of natural language makes the disentangling of textual representations more challenging (e.g., the manipulation over the data space cannot be easily achieved). Inspired by information theory, we propose a novel method that effectively manifests disentangled representations of text, without any supervision on semantics. A new mutual information upper bound is derived and leveraged to measure dependence between style and content. By minimizing this upper bound, the proposed method induces style and content embeddings into two independent low-dimensional spaces. Experiments on both conditional text generation and text-style transfer demonstrate the high quality of our disentangled representation in terms of content and style preservation.', 'corpus_id': 218531919, 'score': 1}, {'doc_id': '219966835', 'title': 'Efficient text generation of user-defined topic using generative adversarial networks', 'abstract': 'This study focused on efficient text generation using generative adversarial networks (GAN). Assuming that the goal is to generate a paragraph of a user-defined topic and sentimental tendency, conventionally the whole network has to be re-trained to obtain new results each time when a user changes the topic. This would be time-consuming and impractical. Therefore, we propose a User-Defined GAN (UD-GAN) with two-level discriminators to solve this problem. The first discriminator aims to guide the generator to learn paragraph-level information and sentence syntactic structure, which is constructed by multiple-LSTMs. The second one copes with higher-level information, such as the user-defined sentiment and topic for text generation. The cosine similarity based on TF-IDF and length penalty are adopted to determine the relevance of the topic. Then, the second discriminator is re-trained with the generator if the topic or sentiment for text generation is modified. The system evaluations are conducted to compare the performance of the proposed method with other GAN-based ones. The objective results showed that the proposed method is capable of generating texts with less time than others and the generated text is related to the user-defined topic and sentiment. We will further investigate the possibility of incorporating more detailed paragraph information such as semantics into text generation to enhance the result.', 'corpus_id': 219966835, 'score': 1}, {'doc_id': '215786180', 'title': 'Do Sequence-to-sequence VAEs Learn Global Features of Sentences?', 'abstract': 'A longstanding goal in NLP is to compute global sentence representations. Such representations would be useful for sample-efficient semi-supervised learning and controllable text generation. To learn to represent global and local information separately, Bowman & al. (2016) proposed to train a sequence-to-sequence model with the variational auto-encoder (VAE) objective. What precisely is encoded in these latent variables expected to capture global features? We measure which words benefit most from the latent information by decomposing the reconstruction loss per position in the sentence. Using this method, we see that VAEs are prone to memorizing the first words and the sentence length, drastically limiting their usefulness. To alleviate this, we propose variants based on bag-of-words assumptions and language model pretraining. These variants learn latents that are more global: they are more predictive of topic or sentiment labels, and their reconstructions are more faithful to the labels of the original documents.', 'corpus_id': 215786180, 'score': 1}]
171	position embedding	4fda7975cb53a06524c55718e83c7241	979	{}	"[{'doc_id': '211252408', 'title': 'A Deep Learning System to Screen Novel Coronavirus Disease 2019 Pneumonia', 'abstract': '\n Abstract\n \n The real-time reverse transcription-polymerase chain reaction (RT-PCR) detection of viral RNA from sputum or nasopharyngeal swab had a relatively low positive rate in the early stage of coronavirus disease 2019 (COVID-19). Meanwhile, the manifestations of COVID-19 as seen through computed tomography (CT) imaging show individual characteristics that differ from those of other types of viral pneumonia such as Influenza-A viral pneumonia (IAVP). This study aimed to establish an early screening model to distinguish COVID-19 pneumonia from IAVP and healthy cases through pulmonary CT images using deep learning techniques. A total of 618 CT samples were collected: 219 samples from 110 patients with COVID-19 (mean age 50 years; 63 (57.3%) male patients); 224 samples from 224 patients with IAVP (mean age 61 years; 156 (69.6%) male patients); and 175 samples from 175 healthy cases (mean age 39 years; 97 (55.4%) male patients). All CT samples were contributed from three COVID-19-designated hospitals in Zhejiang Province, China. First, the candidate infection regions were segmented out from the pulmonary CT image set using a 3D deep learning model. These separated images were then categorized into the COVID-19, IAVP, and irrelevant to infection (ITI) groups, together with the corresponding confidence scores, using a location-attention classification model. Finally, the infection type and overall confidence score for each CT case were calculated using the Noisy-or Bayesian function. The experimental result of the benchmark dataset showed that the overall accuracy rate was 86.7% in terms of all the CT cases taken together. The deep learning models established in this study were effective for the early screening of COVID-19 patients and were demonstrated to be a promising supplementary diagnostic method for frontline clinical doctors.\n \n', 'corpus_id': 211252408, 'score': 0}, {'doc_id': '212725429', 'title': 'Understanding Epidemic Data and Statistics: A case study of COVID-19', 'abstract': ""The 2019-Novel-Coronavirus (COVID-19) has affected 181 countries and out of about 1197405 confirmed cases (By April 5). Understanding the transmission dynamics of the infection in each country which affected on a daily basis and evaluating the effectiveness of control policies is critical for our further actions. To date, the statistics of COVID-19 reported cases show more than 80 percent of infected had a mild case of disease, while around 14 percent of infected experienced a severe one and about 5 percent are categorized as critical disease victims. Today's report (2020-04-05; daily updates in the prepared website) shows the confirmed cases of COVID-19 in the US, Spain, Italy, and Germany are 308850, 126168, 124632, and 96092; respectively. Calculating the total Case Fatality Rate (CFR) of Italy (2020-04-04), about 13.3% of confirmed cases passed away. Compared to South Korea's rate of 1.8% (7 times lower than Italy) and China's 4% (69% lower than Italy), the CFR of Italy is too high. There are some effective policies that yield significant changes in the trend of cases. The lockdown policy in China, Italy, and Spain (the effect observed after some days), Shutdown of all non-essential companies in Hubei (the effect observed after 5 days), combined policy in South Korea, and reducing working hours in Iran."", 'corpus_id': 212725429, 'score': 0}, {'doc_id': '211677475', 'title': 'PhoBERT: Pre-trained language models for Vietnamese', 'abstract': 'We present PhoBERT with two versions, PhoBERT-base and PhoBERT-large, the first public large-scale monolingual language models pre-trained for Vietnamese. Experimental results show that PhoBERT consistently outperforms the recent best pre-trained multilingual model XLM-R (Conneau et al., 2020) and improves the state-of-the-art in multiple Vietnamese-specific NLP tasks including Part-of-speech tagging, Dependency parsing, Named-entity recognition and Natural language inference. We release PhoBERT to facilitate future research and downstream applications for Vietnamese NLP. Our PhoBERT models are available at https://github.com/VinAIResearch/PhoBERT', 'corpus_id': 211677475, 'score': 0}, {'doc_id': '211476589', 'title': 'Understanding of COVID‐19 based on current evidence', 'abstract': 'Since December 2019, a series of unexplained pneumonia cases have been reported in Wuhan, China. On 12 January 2020, the World Health Organization (WHO) temporarily named this new virus as the 2019 novel coronavirus (2019‐nCoV). On 11 February 2020, the WHO officially named the disease caused by the 2019‐nCoV as coronavirus disease (COVID‐19). The COVID‐19 epidemic is spreading all over the world, especially in China. Based on the published evidence, we systematically discuss the characteristics of COVID‐19 in the hope of providing a reference for future studies and help for the prevention and control of the COVID‐19 epidemic.', 'corpus_id': 211476589, 'score': 0}, {'doc_id': '209515395', 'title': 'LayoutLM: Pre-training of Text and Layout for Document Image Understanding', 'abstract': ""Pre-training techniques have been verified successfully in a variety of NLP tasks in recent years. Despite the widespread use of pre-training models for NLP applications, they almost exclusively focus on text-level manipulation, while neglecting layout and style information that is vital for document image understanding. In this paper, we propose the LayoutLM to jointly model interactions between text and layout information across scanned document images, which is beneficial for a great number of real-world document image understanding tasks such as information extraction from scanned documents. Furthermore, we also leverage image features to incorporate words' visual information into LayoutLM. To the best of our knowledge, this is the first time that text and layout are jointly learned in a single framework for document-level pre-training. It achieves new state-of-the-art results in several downstream tasks, including form understanding (from 70.72 to 79.27), receipt understanding (from 94.02 to 95.24) and document image classification (from 93.07 to 94.42). The code and pre-trained LayoutLM models are publicly available at https://aka.ms/layoutlm."", 'corpus_id': 209515395, 'score': 1}, {'doc_id': '85528598', 'title': 'Graph Convolution for Multimodal Information Extraction from Visually Rich Documents', 'abstract': 'Visually rich documents (VRDs) are ubiquitous in daily business and life. Examples are purchase receipts, insurance policy documents, custom declaration forms and so on. In VRDs, visual and layout information is critical for document understanding, and texts in such documents cannot be serialized into the one-dimensional sequence without losing information. Classic information extraction models such as BiLSTM-CRF typically operate on text sequences and do not incorporate visual features. In this paper, we introduce a graph convolution based model to combine textual and visual information presented in VRDs. Graph embeddings are trained to summarize the context of a text segment in the document, and further combined with text embeddings for entity extraction. Extensive experiments have been conducted to show that our method outperforms BiLSTM-CRF baselines by significant margins, on two real-world datasets. Additionally, ablation studies are also performed to evaluate the effectiveness of each component of our model.', 'corpus_id': 85528598, 'score': 1}, {'doc_id': '52967399', 'title': 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding', 'abstract': 'We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).', 'corpus_id': 52967399, 'score': 1}, {'doc_id': '212676036', 'title': 'Prediction and analysis of Coronavirus Disease 2019', 'abstract': ""In December 2019, a novel coronavirus was found in a seafood wholesale market in Wuhan, China. WHO officially named this coronavirus as COVID-19. Since the first patient was hospitalized on December 12, 2019, China has reported a total of 78,824 confirmed CONID-19 cases and 2,788 deaths as of February 28, 2020. Wuhan's cumulative confirmed cases and deaths accounted for 61.1% and 76.5% of the whole China mainland , making it the priority center for epidemic prevention and control. Meanwhile, 51 countries and regions outside China have reported 4,879 confirmed cases and 79 deaths as of February 28, 2020. COVID-19 epidemic does great harm to people's daily life and country's economic development. This paper adopts three kinds of mathematical models, i.e., Logistic model, Bertalanffy model and Gompertz model. The epidemic trends of SARS were first fitted and analyzed in order to prove the validity of the existing mathematical models. The results were then used to fit and analyze the situation of COVID-19. The prediction results of three different mathematical models are different for different parameters and in different regions. In general, the fitting effect of Logistic model may be the best among the three models studied in this paper, while the fitting effect of Gompertz model may be better than Bertalanffy model. According to the current trend, based on the three models, the total number of people expected to be infected is 49852-57447 in Wuhan,12972-13405 in non-Hubei areas and 80261-85140 in China respectively. The total death toll is 2502-5108 in Wuhan, 107-125 in Non-Hubei areas and 3150-6286 in China respetively. COVID-19 will be over p robably in late-April, 2020 in Wuhan and before late-March, 2020 in other areas respectively."", 'corpus_id': 212676036, 'score': 0}, {'doc_id': '211230955', 'title': 'Incubation Period and Other Epidemiological Characteristics of 2019 Novel Coronavirus Infections with Right Truncation: A Statistical Analysis of Publicly Available Case Data', 'abstract': 'The geographic spread of 2019 novel coronavirus (COVID-19) infections from the epicenter of Wuhan, China, has provided an opportunity to study the natural history of the recently emerged virus. Using publicly available event-date data from the ongoing epidemic, the present study investigated the incubation period and other time intervals that govern the epidemiological dynamics of COVID-19 infections. Our results show that the incubation period falls within the range of 2–14 days with 95% confidence and has a mean of around 5 days when approximated using the best-fit lognormal distribution. The mean time from illness onset to hospital admission (for treatment and/or isolation) was estimated at 3–4 days without truncation and at 5–9 days when right truncated. Based on the 95th percentile estimate of the incubation period, we recommend that the length of quarantine should be at least 14 days. The median time delay of 13 days from illness onset to death (17 days with right truncation) should be considered when estimating the COVID-19 case fatality risk.', 'corpus_id': 211230955, 'score': 0}, {'doc_id': '211096989', 'title': 'Insights from early mathematical models of 2019-nCoV acute respiratory disease (COVID-19) dynamics', 'abstract': 'In December 2019, a novel coronavirus (SARS-CoV-2) has been identified to cause acute respiratory disease in humans. An outbreak of this disease has been reported in mainland China with the city of Wuhan as the recognized epicenter. The disease has also been exported to other countries, including the Philippines, but the level of spread is still under control (as of 08 February 2020). To describe and predict the dynamics of the disease, several preliminary mathematical models are formulated by various international study groups. Here, the insights that can be drawn from these models are discussed, especially as inputs for designing strategies to control the epidemics. Proposed model-based strategies on how to prevent the spread of the disease in local setting, such as during large social gatherings, are also presented. The model shows that the exposure time is a significant factor in spreading the disease. With a basic reproduction number equal to 2, and 14-day infectious period, an infected person staying more than 9 hours in the event could infect other people. Assuming the exposure time is 18 hours, the model recommends that attendees of the social gathering should have a protection with more than 70 percent effectiveness.', 'corpus_id': 211096989, 'score': 0}]"
172	Ancient Enhancers	da9bcd60e8c488792db9e3496a272a15	16371	{}	"[{'doc_id': '226255235', 'title': 'Regulation in common: Sponge to zebrafish', 'abstract': 'Developmental enhancers from a sponge regulate gene expression in zebrafish neurons During the development of multicellular animals, distal gene regulatory sequences called enhancers are involved in determining when, where, and how much a gene is expressed (1). Enhancers contain transcription factor binding sites (TFBSs), and the combination of TFs bound determines the activity of an enhancer. There is a lack of understanding about how an enhancer sequence interprets regulatory state (input) to drive target gene expression (output), and how these sequences are constrained and/or modified over time. On page 681 of this issue, Wong et al. (2) identify a series of enhancers in a marine sponge (Amphimedon queenslandica) that respond to TFs expressed during zebrafish (Danio rerio) development. Given the large evolutionary distance between zebrafish and sponge (their common ancestor existed more than 700 million years ago), these enhancers lack detectable sequence homology with vertebrate genomes, yet their ability to function as enhancers in zebrafish demonstrates some kind of functional conservation without sequence conservation.', 'corpus_id': 226255235, 'score': 1}, {'doc_id': '25875826', 'title': 'The origins of developmental gene regulation', 'abstract': ""The leap from simple unicellularity to complex multicellularity remains one of life's major enigmas. The origins of metazoan developmental gene regulatory mechanisms are sought by analyzing gene regulation in extant eumetazoans, sponges, and unicellular organisms. The main hypothesis of this manuscript is that, developmental enhancers evolved from unicellular inducible promoters that diversified the expression of regulatory genes during metazoan evolution. Promoters and enhancers are functionally similar; both can regulate the transcription of distal promoters and both direct local transcription. Additionally, enhancers have experimentally characterized structural features that reveal their origin from inducible promoters. The distal co‐operative regulation among promoters identified in unicellular opisthokonts possibly represents the precursor of distal regulation of promoters by enhancers. During metazoan evolution, constitutive‐type promoters of regulatory genes would have acquired novel receptivity to distal regulatory inputs from promoters of inducible genes that eventually specialized as enhancers. The novel regulatory interactions would have caused constitutively expressed genes controlling differential gene expression in unicellular organisms to become themselves differentially expressed. The consequence of the novel regulatory interactions was that regulatory pathways of unicellular organisms became interlaced and ultimately evolved into the intricate developmental gene regulatory networks (GRNs) of extant metazoans."", 'corpus_id': 25875826, 'score': 1}, {'doc_id': '121170915', 'title': 'The optical continuum of solar and stellar flares', 'abstract': ""A further development of the Kostyuk-Pikelner's model is presented. The response of the chromosphere heated by non-thermal electrons of the power-law energy spectrum has been studied on the basis of the numerical solution of the one-dimensional time-dependent equations of gravitational gas dynamics. The ionization and energy loss for the emissions in the Lyman and Balmer lines have been determined separately for the optically thin and thick Lα-line layers. Due to the initial heating, a higher-pressure region is formed. From this region, disturbances propagate upwards (a shock wave with a velocity of more than 1000 km s-1) and downwards. A temperature jump propagates downwards, and a shock is formed in front of the thermal wave. During a period of several seconds after the beginning of this process, the temperature jump intensifies the downward shock wave and the large radiative loss gives rise to the high density jump (ϱ2/ϱ1 ∼ 100). The numerical solution has been analyzed in detail for the case heating of the ionized and neutral plasma, and a value of this heating is close to the upper limit of the admissible values. In this case, the condensation located between the temperature jump and the shock wave front, may emit in the observed optical continuum.In their essential features, the gas dynamic processes during the flares in red dwarf atmospheres are the same as those in the solar atmosphere. However, the high atmospheric densities, smaller height scale in red dwarf atmospheres, and greater energy of this processes in stellar flares, give rise, in practice, to the regular generation of optical continuum. The photometric parameters of a source with n ∼ 015 cm-3, T ∼ 9000 K, and Δz ∼ 10 km are in a good agreement with observations."", 'corpus_id': 121170915, 'score': 0}, {'doc_id': '43552266', 'title': 'Transphyletic conservation of developmental regulatory state in animal evolution', 'abstract': 'Specific regulatory states, i.e., sets of expressed transcription factors, define the gene expression capabilities of cells in animal development. Here we explore the functional significance of an unprecedented example of regulatory state conservation from the cnidarian Nematostella to Drosophila, sea urchin, fish, and mammals. Our probe is a deeply conserved cis-regulatory DNA module of the SRY-box B2 (soxB2), recognizable at the sequence level across many phyla. Transphyletic cis-regulatory DNA transfer experiments reveal that the plesiomorphic control function of this module may have been to respond to a regulatory state associated with neuronal differentiation. By introducing expression constructs driven by this module from any phyletic source into the genomes of diverse developing animals, we discover that the regulatory state to which it responds is used at different levels of the neurogenic developmental process, including patterning and development of the vertebrate forebrain and neurogenesis in the Drosophila optic lobe and brain. The regulatory state recognized by the conserved DNA sequence may have been redeployed to different levels of the developmental regulatory program during evolution of complex central nervous systems.', 'corpus_id': 43552266, 'score': 1}, {'doc_id': '233396612', 'title': 'Fish-Ing for Enhancers in the Heart', 'abstract': 'Precise control of gene expression is crucial to ensure proper development and biological functioning of an organism. Enhancers are non-coding DNA elements which play an essential role in regulating gene expression. They contain specific sequence motifs serving as binding sites for transcription factors which interact with the basal transcription machinery at their target genes. Heart development is regulated by intricate gene regulatory network ensuring precise spatiotemporal gene expression program. Mutations affecting enhancers have been shown to result in devastating forms of congenital heart defect. Therefore, identifying enhancers implicated in heart biology and understanding their mechanism is key to improve diagnosis and therapeutic options. Despite their crucial role, enhancers are poorly studied, mainly due to a lack of reliable way to identify them and determine their function. Nevertheless, recent technological advances have allowed rapid progress in enhancer discovery. Model organisms such as the zebrafish have contributed significant insights into the genetics of heart development through enabling functional analyses of genes and their regulatory elements in vivo. Here, we summarize the current state of knowledge on heart enhancers gained through studies in model organisms, discuss various approaches to discover and study their function, and finally suggest methods that could further advance research in this field.', 'corpus_id': 233396612, 'score': 0}, {'doc_id': '233248530', 'title': 'Mechanisms of enhancer action: the known and the unknown', 'abstract': 'Differential gene expression mechanisms ensure cellular differentiation and plasticity to shape ontogenetic and phylogenetic diversity of cell types. A key regulator of differential gene expression programs are the enhancers, the gene-distal cis -regulatory sequences that govern spatiotemporal and quantitative expression dynamics of target genes. Enhancers are widely believed to physically contact the target promoters to effect transcriptional activation. However, our understanding of the full complement of regulatory proteins and the definitive mechanics of enhancer action is incomplete. Here, we review recent findings to present some emerging concepts on enhancer action and also outline a set of outstanding questions.', 'corpus_id': 233248530, 'score': 0}, {'doc_id': '28391439', 'title': 'Human Developmental Enhancers Conserved between Deuterostomes and Protostomes', 'abstract': 'The identification of homologies, whether morphological, molecular, or genetic, is fundamental to our understanding of common biological principles. Homologies bridging the great divide between deuterostomes and protostomes have served as the basis for current models of animal evolution and development. It is now appreciated that these two clades share a common developmental toolkit consisting of conserved transcription factors and signaling pathways. These patterning genes sometimes show common expression patterns and genetic interactions, suggesting the existence of similar or even conserved regulatory apparatus. However, previous studies have found no regulatory sequence conserved between deuterostomes and protostomes. Here we describe the first such enhancers, which we call bilaterian conserved regulatory elements (Bicores). Bicores show conservation of sequence and gene synteny. Sequence conservation of Bicores reflects conserved patterns of transcription factor binding sites. We predict that Bicores act as response elements to signaling pathways, and we show that Bicores are developmental enhancers that drive expression of transcriptional repressors in the vertebrate central nervous system. Although the small number of identified Bicores suggests extensive rewiring of cis-regulation between the protostome and deuterostome clades, additional Bicores may be revealed as our understanding of cis-regulatory logic and sample of bilaterian genomes continue to grow.', 'corpus_id': 28391439, 'score': 1}, {'doc_id': '232217795', 'title': 'A flexible repertoire of transcription factor binding sites and a diversity threshold determines enhancer activity in embryonic stem cells.', 'abstract': 'Transcriptional enhancers are critical for development and phenotype evolution and are often mutated in disease contexts; however, even in well-studied cell types, the sequence code conferring enhancer activity remains unknown. To examine the enhancer regulatory code for pluripotent stem cells, we identified genomic regions with conserved binding of multiple transcription factors in mouse and human embryonic stem cells (ESCs). Examination of these regions revealed that they contain on average 12.6 conserved transcription factor binding site (TFBS) sequences. Enriched TFBSs are a diverse repertoire of 70 different sequences representing the binding sequences of both known and novel ESC regulators. Using a diverse set of TFBSs from this repertoire was sufficient to construct short synthetic enhancers with activity comparable to native enhancers. Site-directed mutagenesis of conserved TFBSs in endogenous enhancers or TFBS deletion from synthetic sequences revealed a requirement for 10 or more different TFBSs. Furthermore, specific TFBSs, including the POU5F1:SOX2 comotif, are dispensable, despite cobinding the POU5F1 (also known as OCT4), SOX2, and NANOG master regulators of pluripotency. These findings reveal that a TFBS sequence diversity threshold overrides the need for optimized regulatory grammar and individual TFBSs that recruit specific master regulators.', 'corpus_id': 232217795, 'score': 0}, {'doc_id': '232432991', 'title': 'Identification and prediction of developmental enhancers in sea urchin embryos', 'abstract': 'Background The transcription of developmental regulatory genes is often controlled by multiple cis-regulatory elements. The identification and functional characterization of distal regulatory elements remains challenging, even in tractable model organisms like sea urchins. Results We evaluate the use of chromatin accessibility, transcription and RNA Polymerase II for their ability to predict enhancer activity of genomic regions in sea urchin embryos. ATAC-seq, PRO-seq, and Pol II ChIP-seq from early and late blastula embryos are manually contrasted with experimental cis-regulatory analyses available in sea urchin embryos, with particular attention to common developmental regulatory elements known to have enhancer and silencer functions differentially deployed among embryonic territories. Using the three functional genomic data types, machine learning models are trained and tested to classify and quantitatively predict the enhancer activity of several hundred genomic regions previously validated with reporter constructs in vivo. Conclusions Overall, chromatin accessibility and transcription have substantial power for predicting enhancer activity. For promoter-overlapping cis-regulatory elements in particular, the distribution of Pol II is the best predictor of enhancer activity in blastula embryos. Furthermore, ATAC- and PRO-seq predictive value is stage dependent for the promoter-overlapping subset. This suggests that the sequence of regulatory mechanisms leading to transcriptional activation have distinct relevance at different levels of the developmental gene regulatory hierarchy deployed during embryogenesis.', 'corpus_id': 232432991, 'score': 0}, {'doc_id': '226255377', 'title': 'Deep conservation of the enhancer regulatory code in animals', 'abstract': 'Enhancer function, from sponges to humans Identifying the function of enhancers, DNA regions that help to regulate gene expression and evolve rapidly, has been difficult. This area of research has been hampered by the difficultly in identifying functional conservation. Wong et al. now show that despite low sequence conservation, enhancer function is strongly conserved through the animal kingdom (see the Perspective by Harmston). Transgenic expression of sponge enhancers in zebrafish and mice demonstrates that these sequences can drive cell type–specific gene expression across species. These results suggest an unexpectedly deep level of conservation of gene regulation across the animal kingdom maintained over the course of metazoan evolution. Science, this issue p. eaax8137; see also p. 657 Across the animal kingdom, enhancer function is conserved throughout 700 million years of evolution. INTRODUCTION In animals, gene regulatory networks specify cell identity in space and time. Transcription of genes in these networks is modulated by a class of cis-regulatory elements called enhancers that contain short (~10 base pairs) DNA sequence motifs recognized by transcription factors (TFs). In contrast to TFs, whose histories have been largely traced to the origin of the animal kingdom or earlier, the origin and evolution of enhancers have been relatively difficult to discern. Although not a single enhancer has been shown to be conserved across the animal kingdom, enhancers may be as ancient and conserved as the TFs with which they interact. This inability to identify conserved enhancers is apparently because they evolve faster than both the TFs they interact with and the genes they regulate. RATIONALE Putative enhancers in the sponge Amphimedon queenslandica had previously been identified on the basis of combinatorial patterns of histone modifications. Here, we sought to determine whether sponges share functionally conserved enhancers with bilaterians. We primarily focused on deeply conserved metazoan microsyntenic gene pairs. These pairs are thought to be conserved because the cis-regulatory elements that regulate the developmental expression of one gene (the target gene) are located in the other gene (the bystander gene). This proposed regulatory linkage may underlie the maintenance of these microsyntenic gene pairs across 700 million years of independent evolution. RESULTS We found that enhancers present in Amphimedon microsyntenic regions drive consistent patterns of cell type–specific gene expression in zebrafish and mouse embryos. Although these sponge enhancers do not share significant sequence identity with vertebrates, they are in microsyntenic regions that are orthologous with microsyntenic regions in other metazoans and have strong histone H3 Lys4 methylation (H3K4me1) enhancer signals. Focusing on an Islet enhancer in the Islet-Scaper microsyntenic region, we found that the sponge 709–base pair enhancer, independent of its orientation, drives green fluorescent protein (GFP) expression in zebrafish cells in the hindbrain neuroepithelial region, the roof plate around the midline, the pectoral fin, and the otic vesicle; the activity overlaps with endogenous Isl2a expression. Systematic removal of sequences from the Amphimedon Islet enhancer revealed that both the 5′ and 3′ regions of this enhancer are required for consistent cell type–specific activity in zebrafish. We then used the number and frequency of TF binding motifs in the Amphimedon Islet enhancer to identify putative enhancers in human, mouse, and fly Islet-Scaper regions. The candidate orthologous enhancers from humans and mice drove gene expression patterns similar to those in sponges and endogenous Islet enhancers in zebrafish. We also demonstrated that a number of putative Amphimedon enhancers, which are outside conserved microsyntenic regions, can also drive unique expression patterns: Enhancers of sponge housekeeping genes drive broader expression patterns in zebrafish. CONCLUSION These results suggest the existence of an ancient and conserved, yet flexible, genomic regulatory syntax that (i) can be interpreted by the available TFs present in cells constituting disparate developmental systems and cell types, and (ii) has been repeatedly co-opted into cell type–specific networks across the animal kingdom. This common regulatory code maintains a repertoire of conserved TF binding motifs that stabilize and preserve enhancer functionality over evolution. Once established, these enhancers may be maintained as part of conserved gene regulatory network modules over evolution. Although robust, these enhancers can evolve through the expansion and integration of new TF binding motifs and the loss of others. We posit that the expansion of TFs and enhancers may underlie the evolution of complex body plans. Islet enhancer activity is conserved across animal evolution. Enhancers located within conserved microsyntenic units in the sponge Amphimedon queenslandica are tested in a zebrafish transgenic reporter system. In zebrafish, the sponge Islet enhancer drives a GFP reporter expression pattern similar to that of human, mouse, and zebrafish enhancers identified within the Islet-Scaper microsyntenic region. This suggests the conservation of regulatory syntax specified by flexible organizations of motifs. Interactions of transcription factors (TFs) with DNA regulatory sequences, known as enhancers, specify cell identity during animal development. Unlike TFs, the origin and evolution of enhancers has been difficult to trace. We drove zebrafish and mouse developmental transcription using enhancers from an evolutionarily distant marine sponge. Some of these sponge enhancers are located in highly conserved microsyntenic regions, including an Islet enhancer in the Islet-Scaper region. We found that Islet enhancers in humans and mice share a suite of TF binding motifs with sponges, and that they drive gene expression patterns similar to those of sponge and endogenous Islet enhancers in zebrafish. Our results suggest the existence of an ancient and conserved, yet flexible, genomic regulatory syntax that has been repeatedly co-opted into cell type–specific gene regulatory networks across the animal kingdom.', 'corpus_id': 226255377, 'score': 1}]"
173	MD_simulation	efc565e8c5b8162559f8ce61d704cd49	5231	{}	"[{'doc_id': '165140010', 'title': 'Effect of Light Quality on Physiological Disorder, Growth, and Secondary Metabolite Content of Water Spinach (Ipomoea aquatica Forsk) Cultivated in a Closed-type Plant Production System', 'abstract': 'Light quality is a critical factor that affects plant quality, including phytochemical accumulation and marketable characteristics, in closed-type plant production systems. The purpose of this study was to determine the appropriate light quality for production of good quality water spinach in terms of its appearance and accumulation of phytochemicals in an artificial environment. Plants were hydroponically cultured under five different light quality conditions: red, blue, green, red and blue (as a control), and red and blue with far red at a photosynthetic photon flux density of 200 μmol·m-2·s-1 for 14 days after transplantation. Shoot (stem and leaf) fresh weights (FW) under red-containing light conditions increased more than 39.7% compared to that under monochromatic blue light, and monochromatic red light produced significantly higher stem FW but lower leaf FW compared to that under blue-containing light conditions. Monochromatic blue light significantly increased the antioxidant activity capacity in leaves and stems more than 210.0% compared to other treatments. However, blue-containing light significantly stimulated physiological disorder (intumescence injury) in stems and suppressed stem elongation compared to monochromatic red or green light. Monochromatic red light reduced the number of intumescent lesions by 95.8% and enhanced stem elongation compared to control. These findings suggest that red-rich light promotes growth of water spinach with less intumescence. Additional key words: antioxidant activity, hydroponic, intumescence, light-emitting diodes, monochromatic light', 'corpus_id': 165140010, 'score': 0}, {'doc_id': '218764757', 'title': 'Two mutations P/L and Y/C in SARS-CoV-2 helicase domain exist together and influence helicase RNA binding', 'abstract': 'RNA helicases play pivotal role in RNA replication by catalysing the unwinding of complex RNA duplex structures into single strands in ATP/NTP dependent manner. SARS coronavirus 2 (SARS-CoV-2) is a single stranded positive sense RNA virus belonging to the family Coronaviridae. The viral RNA encodes non structural protein Nsp13 or the viral helicase protein that helps the viral RNA dependent RNA polymerase (RdRp) to execute RNA replication by unwinding the RNA duplexes. In this study we identified a novel mutation at position 541of the helicase where the tyrosine (Y) got substituted with cytosine (C). We found that Y541C is a destabilizing mutation increasing the molecular flexibility and leading to decreased affinity of helicase binding with RNA. Earlier we had reported a mutation P504L in the helicase protein for which had not performed RNA binding study. Here we report that P504L mutation leads to increased affinity of helicase RNA interaction. So, both these mutations have opposite effects on RNA binding. Moreover, we found a significant fraction of isolate population where both P504L and Y541C mutations were co-existing.', 'corpus_id': 218764757, 'score': 0}, {'doc_id': '216072055', 'title': 'Cryo-EM structures reveal transcription initiation steps by yeast mitochondrial RNA polymerase', 'abstract': 'Cryo-EM structures of transcription pre-initiation complex (PIC) and initiation complex (IC) of yeast mitochondrial RNA polymerase show fully resolved transcription bubbles and explain promoter melting, template alignment, DNA scrunching, transition into elongation, and abortive synthesis. Promoter melting initiates in PIC with MTF1 trapping the −4 to −2 non-template (NT) bases in its NT-groove. Transition to IC is marked by a large-scale movement that aligns the template with RNA at the active site. RNA synthesis scrunches the NT strand into an NT-loop, which interacts with centrally positioned MTF1 C-tail. Steric clashes of the C-tail with RNA:DNA and NT-loop, and dynamic scrunching-unscrunching of DNA explain abortive synthesis and transition into elongation. Capturing the catalytically active IC-state with UTPαS poised for incorporation enables modeling toxicity of antiviral nucleosides/nucleotides.', 'corpus_id': 216072055, 'score': 0}, {'doc_id': '52096989', 'title': 'Molecular Modeling Applied to Nucleic Acid-Based Molecule Development', 'abstract': 'Molecular modeling by means of docking and molecular dynamics (MD) has become an integral part of early drug discovery projects, enabling the screening and enrichment of large libraries of small molecules. In the past decades, special emphasis was drawn to nucleic acid (NA)-based molecules in the fields of therapy, diagnosis, and drug delivery. Research has increased dramatically with the advent of the SELEX (systematic evolution of ligands by exponential enrichment) technique, which results in single-stranded DNA or RNA sequences that bind with high affinity and specificity to their targets. Herein, we discuss the role and contribution of docking and MD to the development and optimization of new nucleic acid-based molecules. This review focuses on the different approaches currently available for molecular modeling applied to NA interaction with proteins. We discuss topics ranging from structure prediction to docking and MD, highlighting their main advantages and limitations and the influence of flexibility on their calculations.', 'corpus_id': 52096989, 'score': 1}, {'doc_id': '39425134', 'title': 'Molecular dynamics simulations of nucleic acid-protein complexes.', 'abstract': 'Molecular dynamics simulation studies of protein-nucleic acid complexes are more complicated than studies of either component alone-the force field has to be properly balanced, the systems tend to become very large, and a careful treatment of solvent and of electrostatic interactions is necessary. Recent investigations into several protein-DNA and protein-RNA systems have shown the feasibility of the simulation approach, yielding results of biological interest not readily accessible to experimental methods.', 'corpus_id': 39425134, 'score': 1}, {'doc_id': '4581176', 'title': 'Molecular dynamic simulations of protein/RNA complexes: CRISPR/Csy4 endoribonuclease.', 'abstract': ""BACKGROUND\nMany prokaryotic genomes comprise Clustered Regularly Interspaced Short Palindromic Repeats (CRISPRs) offering defense against foreign nucleic acids. These immune systems are conditioned by the production of small CRISPR-derived RNAs matured from long RNA precursors. This often requires a Csy4 endoribonuclease cleaving the RNA 3'-end.\n\n\nMETHODS\nWe report extended explicit solvent molecular dynamic (MD) simulations of Csy4/RNA complex in precursor and product states, based on X-ray structures of product and inactivated precursor (55 simulations; ~3.7μs in total).\n\n\nRESULTS\nThe simulations identify double-protonated His29 and deprotonated terminal phosphate as the likely dominant protonation states consistent with the product structure. We revealed potential substates consistent with Ser148 and His29 acting as the general base and acid, respectively. The Ser148 could be straightforwardly deprotonated through solvent and could without further structural rearrangements deprotonate the nucleophile, contrasting similar studies investigating the general base role of nucleobases in ribozymes. We could not locate geometries consistent with His29 acting as general base. However, we caution that the X-ray structures do not always capture the catalytically active geometries and then the reactive structures may be unreachable by the simulation technique.\n\n\nCONCLUSIONS\nWe identified potential catalytic arrangement of the Csy4/RNA complex but we also report limitations of the simulation technique. Even for the dominant protonation state we could not achieve full agreement between the simulations and the structural data.\n\n\nGENERAL SIGNIFICANCE\nPotential catalytic arrangement of the Csy4/RNA complex is found. Further, we provide unique insights into limitations of simulations of protein/RNA complexes, namely, the influence of the starting experimental structures and force field limitations. This article is part of a Special Issue entitled Recent developments of molecular dynamics."", 'corpus_id': 4581176, 'score': 1}, {'doc_id': '131122344', 'title': 'Abstract: Seismic Stratigraphy of Upper Neogene Shelf Break Position Variations between East and West Offshore Louisiana', 'abstract': 'ABSTRACT The processes of sedimentation, subsidence, and sea level oscillation dictate the geologic evolution of a passive continental margin. Seismic stratigraphy can help decipher the synthesis of these processes. The Mississippi River controlled sedimentation during the Upper Neogene evolution of the Louisiana margin. There is a marked difference between the seismic stratigraphy and paleo-physiography of the outer shelf/upper slope of the east and west Louisiana offshore (i.e., Mississippi Canyon as contrasted with Garden Banks/Green Canyon). In the Mississippi Canyon area, the shelf break retreated 6 miles (11 km) from 10.0 to 8.2 mybp, then advanced 55 miles (100 km) from 8.2 to 2.8 mybp, followed by a retreat of 30 miles (55 km) from 2.8 to 0.7 mybp. Since then, the shelf break has advanced 20 miles (36 km). In contrast, the west Louisiana shelf break prograded 100 miles (185 km) during the last 6.7 my. These oscillations are dated from paleontological determinations. The term ""shelf break"" refers to an ecological environment based on water depth. These determinations are averages based on samples collected during a high-low sea level continuum, with a bias toward highstand. The following seismic stratigraphic/paleophysiographic description in the Mississippi Canyon area pertains to the area beyond the present shelf break: (The description will proceed up the stratigraphic column.) During the shelf break advance from 8.2 to 2.8 mybp, the sediments change from mid to upper slope. Midslope sediments appear as continuous to semicontinuous high-amplitude reflectors with moderate to low dips, suggestive of turbidites. There is a gradual change to less continuous, variable amplitude reflectors characterized by much steeper dips. Some of the reflectors, although intermittent, appear to line up along a single, gently concave-upward horizon. Along that horizon, there are patches of concave-downward reflectors. The concave-upward horizons often converge down-dip. Such a reflector pattern is compatible with an interpretation of deepsea fans. Interspersed, there are single, high-continuity, high-amplitude reflectors that may represent highstand deposits. During the shelf break retreat from 2.8 to 0.7 my ago, the reflectors show a marked increase in continuity. The reflector pattern in this section is reminiscent of the lower late Miocene section already noted. This lower Pleistocene section is suggestive of turbidites deposited along mid-slope. The upper Pleistocene and Recent section, the last 0.7 my, is characterized by low continuity, variable amplitude, and sometimes chaotic reflectors. Such reflector patterns confirm the shelf break advance as it is now in the immediate area. In sharp contrast, the west Louisiana offshore over the past 6.7 my is marked by a basinward progradation of shelf break up to the present. Shelf sediments are characterized by sub-horizontal reflectors paralleling the present shelf. At the shelf break, these sub-horizontal reflectors show a marked increase in dip toward the basin. Within the slope seismic facies, there is an alternating of single or double, high-continuity, high-amplitude reflectors, with low-continuity, variable-amplitude, and occasionally chaotic reflectors. Seismic facies analysis and paleophysiographic analysis, derived from paleontology, both show a continuous progradation of shelf break into the basin. Features interpreted include: submarine canyons, slump blocks, debris flows, turbidites, and submarine fans. We note greater continuity of reflectors farther down slope and away from the shelf break. Thus, a judicious sorting by continuity and amplitude of reflectors would appear to be diagnostic of energy of deposition/erosion, and thereby, the environment of deposition. End_Page 584------------------------ Additionally, there are sea level oscillations occurring at 100,000 years and less. Periodicities of these 4th order changes exhibit sea level ranges from 400 ft (125 m) to 150-75 ft (50-25 m). Such oscillations could change shoreline position by 90 miles (165 km) to 30 miles (55 km), respectively. The basinward depositional profile of coarse to fine sediments would be correspondingly offset. High energy shelf break/upper slope deposits appear as high-continuity, low-amplitude reflectors. Low energy highstand deposits appear as high-continuity, high-amplitude reflectors. End_of_Record - Last_Page 585-------', 'corpus_id': 131122344, 'score': 0}, {'doc_id': '212740602', 'title': 'Computer simulations explain mutation-induced effects on the DNA editing by adenine base editors', 'abstract': 'We uncovered structural and functional changes induced by mutations that dictate the DNA editing activity of ABE enzymes. Adenine base editors, which were developed by engineering a transfer RNA adenosine deaminase enzyme (TadA) into a DNA editing enzyme (TadA*), enable precise modification of A:T to G⋮C base pairs. Here, we use molecular dynamics simulations to uncover the structural and functional roles played by the initial mutations in the onset of the DNA editing activity by TadA*. Atomistic insights reveal that early mutations lead to intricate conformational changes in the structure of TadA*. In particular, the first mutation, Asp108Asn, induces an enhancement in the binding affinity of TadA to DNA. In silico and in vivo reversion analyses verify the importance of this single mutation in imparting functional promiscuity to TadA* and demonstrate that TadA* performs DNA base editing as a monomer rather than a dimer.', 'corpus_id': 212740602, 'score': 1}, {'doc_id': '215800977', 'title': 'Identification of potential binders of the main protease 3CLpro of the COVID-19 via structure-based ligand design and molecular modeling', 'abstract': '\n Abstract\n \n We have applied a computational strategy, using a combination of virtual screening, docking and molecular dynamics techniques, aimed at identifying possible lead compounds for the non-covalent inhibition of the main protease 3CLpro of the SARS-CoV2 Coronavirus. Based on the X-ray structure (PDB code: 6LU7), ligands were generated using a multimodal structure-based design and then docked to the monomer in the active state. Docking calculations show that ligand-binding is strikingly similar in SARS-CoV and SARS-CoV2 main proteases. The most potent docked ligands are found to share a common binding pattern with aromatic moieties connected by rotatable bonds in a pseudo-linear arrangement.\n \n', 'corpus_id': 215800977, 'score': 0}, {'doc_id': '15397223', 'title': 'Investigating dynamic and energetic determinants of protein nucleic acid recognition: analysis of the zinc finger zif268-DNA complexes', 'abstract': ""BackgroundProtein-DNA recognition underlies fundamental biological processes ranging from transcription to replication and modification. Herein, we present a computational study of the sequence modulation of internal dynamic properties and of intraprotein networks of aminoacid interactions that determine the stability and specificity of protein-DNA complexes.ResultsTo this aim, we apply novel theoretical approaches to analyze the dynamics and energetics of biological systems starting from MD trajectories. As model system, we chose different sequences of Zinc Fingers (ZF) of the Zif268 family bound with different sequences of DNA. The complexes differ for their experimental stability properties, but share the same overall 3 D structure and do not undergo structural modifications during the simulations. The results of our analysis suggest that the energy landscape for DNA binding may be populated by dynamically different states, even in the absence of major conformational changes. Energetic couplings between residues change in response to protein and/or DNA sequence variations thus modulating the selectivity of recognition and the relative importance of different regions for binding.ConclusionsThe results show differences in the organization of the intra-protein energy-networks responsible for the stabilization of the protein conformations recognizing and binding DNA. These, in turn, are reflected into different modulation of the ZF's internal dynamics. The results also show a correlation between energetic and dynamic properties of the different proteins and their specificity/selectivity for DNA sequences. Finally, a dynamic and energetic model for the recognition of DNA by Zinc Fingers is proposed."", 'corpus_id': 15397223, 'score': 1}]"
174	Charity website research	a93e18ffdd89e354231221944b2e9a02	18965	{}	"[{'doc_id': '235797315', 'title': 'Moral Judgments of COVID-19 Social Distancing Violations: The Roles of Perceived Harm and Impurity.', 'abstract': ""Can perceptions of impurity uniquely explain moral judgment? Or is moral judgment reducible to perceptions of harm? Whereas some perspectives posit that purity violations may drive moral judgment distinctly from harm violations, other perspectives contend that perceived harm is an essential precursor of moral condemnation. We tested these competing hypotheses through five preregistered experiments (total N = 2,944) investigating U.S. adults' perceptions of social distancing violations during the COVID-19 pandemic. Perceived harm was more strongly related to moral judgment than was perceived impurity. Nevertheless, over and above perceived harm, perceived impurity reliably explained unique variance in moral judgment. Effects of perceived harm and impurity were significant among both liberal and conservative participants but were larger among liberals. Results suggest that appraisals of both harm and impurity provide valuable insights into moral cognition. We discuss implications of these findings for dyadic morality, moral foundations, act versus character judgments, and political ideology."", 'corpus_id': 235797315, 'score': 0}, {'doc_id': '236163194', 'title': 'Advertising in a Context Harm Crisis', 'abstract': 'Abstract Context harm crises concern the challenges of advertising morally sound products in a context that is failing, as during COVID-19. Following Koselleck, we argue that crises interrupt the trajectory of existing social processes, thereby preventing consumers’ expected future outcomes. We propose a three-step future framing advertising strategy in response: (1) mourning a future that was lost to facilitate emotional adaptation; (2) reconstructing a new future to facilitate rational action under conditions of ambivalence; and (3) establishing mythologies for future-oriented identity work to facilitate the existential demands of crises. We then discuss health messaging from the perspective of future framing.', 'corpus_id': 236163194, 'score': 1}, {'doc_id': '235743743', 'title': 'Theory of Planned Behavior Analysis of Social Distancing During the COVID-19 Pandemic: Focusing on the Intention–Behavior Gap', 'abstract': 'Abstract Background As COVID-19 continues to spread globally, it is important to understand psychological factors that may influence compliance with social distancing. Purpose The present study examined whether Theory of Planned Behavior (TPB) constructs were associated with social distancing, with a focus on exploring moderators of the intention–behavior relationship. Methods Using a longitudinal design, U.S. adults (N = 507) self-reported TPB constructs and social distancing behavior at baseline and 3 months later. Participants were from 48 U.S. States and the District of Columbia and were on average 50.39 years old (SD = 15.32, range = 18–80). The majority were Non-Hispanic White (71.6%), had a bachelor’s degree or higher (55.3%), and resided in suburban areas (55.8%). Results While positive attitudes toward social distancing increased over time (p = .002), subjective norms weakened (p < .001) and perceived behavioral control (PBC) remained stable (p = .22). Interestingly, despite an increase in intentions from baseline to follow-up (p < .001), there was a significant decrease in social distancing behavior over time (p < .001). Consistent with the TPB, baseline attitudes (p < .001), subjective norms (p < .001), and PBC (p < .001) for social distancing were all associated with baseline intentions to social distance. In turn, baseline intentions were significantly associated with social distancing behavior at follow-up (p < .001). Younger adults (p < .001) and non-White participants (p = .002) displayed a greater intention–behavior gap relative to older and White participants. In contrast, participants with more stable intentions over time displayed a stronger intention–behavior relationship (p < .001). Conclusions Targeting individuals’ attitudes, norms, and PBC may effectively promote protective behaviors intended to mitigate the spread of COVID-19 and similar viral outbreaks. Future research should examine effective strategies for translating social distancing intentions into actions.', 'corpus_id': 235743743, 'score': 0}, {'doc_id': '234946295', 'title': 'Himpathy? The Impact of Defendant Social Status on Perceptions of a Rape Legal Case', 'abstract': 'OF THESIS Himpathy? The Impact of Defendant Social Status on Perceptions of a Rape Legal Case There is limited work regarding multiple indicators of social status in the legal system (e.g., power and SES). The present study investigated the influence of defendant social status on case judgments in a first-degree rape case. The experiment used a 2 (defendant power: high vs. low) x 2 (defendant SES: high vs. low) x 2 (participant gender) between-subjects design. A sample of 282 community members were recruited via Amazon’s Mechanical Turk. Participants were presented with a case summary, asked to make guilt and credibility judgments, complete the system justification gender scale (gender SJ: Jost & Kay, 2005), and answer standard demographic questions. Main effects were found such that female participants and defendants rated higher in power (i.e., legal authority), led to increases in pro-victim judgments (i.e., guilty verdicts). No main effect of defendant SES was found. Further, the effects of power and wealth were mediated by victim credibility, such that increases in defendant power led to increased victim credibility, raising the number of guilty verdicts. However, this mediation varied based on participants’ gender SJ scores. Overall findings indicate that when a defendant was rated as high in legal authority, participants viewed rape as an abuse of power, and/or the victim as braver (i.e., credible) for coming forward.', 'corpus_id': 234946295, 'score': 0}, {'doc_id': '233480956', 'title': ""VIDEO ADS' CREATIVITY AND STRUCTURE INFLUENCE ON BRAND CONGRUENCE AND ENGAGEMENT"", 'abstract': ""Social networks play an important role in the life of today's societies and consumers are engaging more online with brands. Brands create and share information and special video ads on social networks, and, in the context of the COVID-19 pandemic, social networks allow brands to communicate with their consumers simply and quickly. Therefore, creativity and narrative structures are important for consumers. Brands are producing video ads that show consumers’ day context in order to obtain greater social media engagement. Considering that, this paper aims to study whether that goal is being achieved. The empirical research, from which we obtained 427 responses and which was tested using structural equations using the AMOS software, allows to conclude that creativity, the structure of the narrative and the consumer's congruence with the brand are determinants of engagement in social media. Further, presents practical and theoretical recommendations."", 'corpus_id': 233480956, 'score': 0}, {'doc_id': '234902969', 'title': 'Anonymity in COVID-19 Online Donations: A Cross-Cultural Analysis on Fundraising Platforms', 'abstract': 'Donating money anonymously is often perceived as an act of altruism in Western culture and a similar concept of ‘ikhlas’ (sincerity) in Indonesia Yet, this prosocial behavior can also be utilized to cope with unpleasant feelings associated with such donations (e g , fear of social judgment, guilt) making it otherwise a rather self-serving act In that regard, we analyzed 20,000 individual donation transactions made for COVID-19 campaigns on two popular fundraising platforms: GoFundMe in the United States and Kitabisa in Indonesia We found that GoFundMe donors tended to self-identify (33 18% opted for anonymity) while Kitabisa donors tended to conceal their identities (73 89% opted for anonymity) Adjusting the donations to the fractions of GDPs, we further found that anonymous donors on Kitabisa donated significantly less amounts of money (M = 11, SD = 54) in contrast to their self-identified counterparts (M = 26, SD = 3 63), who donated even higher amounts of money than anonymous donors on GoFundMe (M = 16, SD = 66) Even though the amount of money may not always entail the rate of altruism nor ikhlas, the significant findings bring the cultural belief associated with such anonymous donations into questions © 2021, The Author(s), under exclusive license to Springer Nature Switzerland AG', 'corpus_id': 234902969, 'score': 1}, {'doc_id': '235807558', 'title': 'The impact of narrative writing on empathy, perspective-taking, and attitude: Two randomized controlled experiments on violations of Covid-19 protection regulations', 'abstract': 'Objective Two randomized controlled experiments investigated if writing a narrative text about a fictional person who shows disapproved of behavior in the Covid-19 pandemic influenced empathy, perspective-taking, attitude, and attribution of causes regarding that person’s behavior. Methods In both studies, a fictional scenario was described, and participants answered questions regarding empathy, perspective-taking, attitude, and attribution regarding a fictional person’s disapproved of behavior (pre-post-measurement). Participants were randomly assigned to one of two conditions. In the experimental condition, they wrote a narrative text about the fictional person. In the control condition, they wrote about an unrelated topic. Results We found that writing a narrative text increased empathy more strongly than writing about an unrelated topic; Study 1: p = 0.004, part.η2 = 0.06, Study 2: p < .001, part.η2 = 0.19. This did not apply to perspective-taking; Study 1: p = 0.415; Study 2: p = 0.074. We also found that writing a narrative text about a fictional person resulted in a more positive attitude toward this person; Study 1: p = 0.005, part.η2 = 0.06; Study 2: p<0.001, part.η2 = 0.10. Finally, in Study 2 we found that participants who wrote a narrative text attributed the person’s behavior to internal causes to a lesser degree; p = 0.007, part.η2 = 0.05. Conclusion Our findings indicate that empathy and attitude are positively modifiable through narrative writing tasks. Empathy training could potentially prevent discrimination related to Covid-19. Trial registration The studies presented in this article were pre-registered on the pre-registration platform AsPredicted (aspredicted.org) before we began data collection; registration numbers and URL: #44754 https://aspredicted.org/vx37t.pdf (Study 1), and #44753 https://aspredicted.org/ig7kq.pdf (Study 2).', 'corpus_id': 235807558, 'score': 1}, {'doc_id': '236163281', 'title': 'Advertising during COVID-19: Exploring Perceived Brand Message Authenticity and Potential Psychological Reactance', 'abstract': 'Abstract This study seeks to examine under what circumstances brand advertising that advocated for socially desirable behaviors (e.g., mask wearing) during the COVID-19 pandemic may be successful in increasing the intention to engage in such behaviors. In doing so, we examine the effectiveness of real brands’ persuasive attempts via activist advertising, focusing on messages used in real advertisements that may cause psychological reactance. Using a 2 (message type: threat to freedom vs. no threat) × 2 (brand: Uber vs. Heineken) between-subjects design, our study confirms that threat to freedom messages lead to lower ad attitudes, lower intention to engage in socially responsible behaviors, and greater psychological reactance. Brands should be careful when using “threat to freedom” language in their advertising. Further, our results suggest that perceived authenticity of a brand’s messaging is key to the success of activist advertising.', 'corpus_id': 236163281, 'score': 1}, {'doc_id': '235394339', 'title': 'Impact of donors’ financial fairness perception on donation intention in nonprofit organizations after COVID-19 outbreak', 'abstract': 'Based on the investigation of financial fairness perception and donation intention of individual donors in non-profit organizations (NPOs), this paper uses structural equation model to analyze the impact of individual donors’ financial fairness perception on donation intention. The results show that individual donors’ perceptions on financial result fairness, financial procedure fairness and financial information fairness all have positive impact on donation intention; among which the perception on financial result fairness only has direct impact on individual donation intention, while the perceptions on financial procedure fairness and financial information fairness have direct and indirect impact on individual donation intention.', 'corpus_id': 235394339, 'score': 1}, {'doc_id': '235197390', 'title': 'Consumer Ethicality Perception and Legitimacy: Competitive Advantages in COVID-19 Crisis', 'abstract': 'The article aims to analyze the cause–effect relationship between Brand Ethicality Perception (CPE), legitimacy and purchase intention during the COVID-19 first wave, taking into consideration the mediation effect of the country of residence. Data collection was based on a survey launched during the COVID-19 lockdown in Madrid and New York. To analyze the established hypotheses and to test the multigroup analysis, we applied a structural modelling with SmartPLS. The research contributes to the field of brand management, and specifically of ethical branding, since it will analyze how stakeholders’ expectations fulfillment is key to build a consistent and valued brand meaning in crisis’ situations, demonstrating that ethical behaviors are key for gaining corporate legitimacy and, therefore, for improving business performances.', 'corpus_id': 235197390, 'score': 0}]"
175	consensus clustering	0a7f50021a8a5c9a001568eb69dbc3a9	8619	{}	"[{'doc_id': '221377218', 'title': 'Structured Graph Learning for Clustering and Semi-supervised Classification', 'abstract': 'Abstract Graphs have become increasingly popular in modeling structures and interactions in a wide variety of problems during the last decade. Graph-based clustering and semi-supervised classification techniques have shown impressive performance. This paper proposes a graph learning framework to preserve both the local and global structure of data. Specifically, our method uses the self-expressiveness of samples to capture the global structure and adaptive neighbor approach to respect the local structure. Furthermore, most existing graph-based methods conduct clustering and semi-supervised classification on the graph learned from the original data matrix, which doesn’t have explicit cluster structure, thus they might not achieve the optimal performance. By considering rank constraint, the achieved graph will have exactly c connected components if there are c clusters or classes. As a byproduct of this, graph learning and label inference are jointly and iteratively implemented in a principled way. Theoretically, we show that our model is equivalent to a combination of kernel k-means and k-means methods under certain condition. Extensive experiments on clustering and semi-supervised classification demonstrate that the proposed method outperforms other state-of-the-art methods.', 'corpus_id': 221377218, 'score': 0}, {'doc_id': '211026691', 'title': 'Graph-based Semi-Supervised Classification for Online Customer Reviews Using Consensus Clustering', 'abstract': ""The purpose of this research is to present a graph-based semi-supervised learning (GBSSL) method with high classification accuracy for handling a large number of customer reviews. Following recent developments in information and communication technology, it has become essential for companies to employ efficient methods for analyzing customer reviews for improving their products and services. In analyzing these reviews, it is necessary that they are classified based on the review content. However there are only a few labelled customer reviews that can be used for the purpose of classification. Semi-supervised learning is effective in such case. In this research, we introduce GBSSL and show how it improves classification accuracy with the use of unsupervised clustering. The proposed learning method defines the result of consensus clustering based on the similarity between nodes. This research classifies customer reviews for a beauty salon using the proposed method and compares its accuracy with that of other machine learning methods to demonstrate the former's effectiveness."", 'corpus_id': 211026691, 'score': 1}, {'doc_id': '198984816', 'title': 'Mining of Self-Organizing Map Gene-Expression Portraits Reveals Prognostic Stratification of HPV-Positive Head and Neck Squamous Cell Carcinoma', 'abstract': 'Patients (pts) with head and neck squamous cell carcinoma (HNSCC) have different epidemiologic, clinical, and outcome behaviors in relation to human papillomavirus (HPV) infection status, with HPV-positive patients having a 70% reduction in their risk of death. Little is known about the molecular heterogeneity in HPV-related cases. In the present study, we aim to disclose the molecular subtypes with potential biological and clinical relevance. Through a literature review, 11 studies were retrieved with a total of 346 gene-expression data points from HPV-positive HNSCC pts. Meta-analysis and self-organizing map (SOM) approaches were used to disclose relevant meta-gene portraits. Unsupervised consensus clustering provided evidence of three biological subtypes in HPV-positive HNSCC: Cl1, immune-related; Cl2, epithelial–mesenchymal transition-related; Cl3, proliferation-related. This stratification has a prognostic relevance, with Cl1 having the best outcome, Cl2 the worst, and Cl3 an intermediate survival rate. Compared to recent literature, which identified immune and keratinocyte subtypes in HPV-related HNSCC, we confirmed the former and we separated the latter into two clusters with different biological and prognostic characteristics. At present, this paper reports the largest meta-analysis of HPV-positive HNSCC studies and offers a promising molecular subtype classification. Upon further validation, this stratification could improve patient selection and pave the way for the development of a precision medicine therapeutic approach.', 'corpus_id': 198984816, 'score': 1}, {'doc_id': '221090677', 'title': 'A Novel Community Detection Based Genetic Algorithm for Feature Selection', 'abstract': 'The selection of features is an essential data preprocessing stage in data mining. The core principle of feature selection seems to be to pick a subset of possible features by excluding features with almost no predictive information as well as highly associated redundant features. In the past several years, a variety of meta-heuristic methods were introduced to eliminate redundant and irrelevant features as much as possible from high-dimensional datasets. Among the main disadvantages of present meta-heuristic based approaches is that they are often neglecting the correlation between a set of selected features. In this article, for the purpose of feature selection, the authors propose a genetic algorithm based on community detection, which functions in three steps. The feature similarities are calculated in the first step. The features are classified by community detection algorithms into clusters throughout the second step. In the third step, features are picked by a genetic algorithm with a new community-based repair operation. Nine benchmark classification problems were analyzed in terms of the performance of the presented approach. Also, the authors have compared the efficiency of the proposed approach with the findings from four available algorithms for feature selection. The findings indicate that the new approach continuously yields improved classification accuracy.', 'corpus_id': 221090677, 'score': 0}, {'doc_id': '16218273', 'title': 'EXCLUVIS: A MATLAB GUI Software for Comparative Study of Clustering and Visualization of Gene Expression Data', 'abstract': 'Clustering is a popular data mining technique that aims to partition an input space into multiple homogeneous regions. There exist several clustering algorithms in the literature. The performance of a clustering algorithm depends on its input parameters which can substantially affect the behavior of the algorithm. Cluster validity indices determine the partitioning that best fits the underlying data. In bioinformatics, microarray gene expression technology has made it possible to measure the gene expression levels of thousands of genes simultaneously. Many genomic studies, which aim to analyze the functions of some genes, highly rely on some clustering technique for grouping similarly expressed genes in one cluster or partitioning tissue samples based on similar expression values of genes. In this work, an application package called EXCLUVIS (gene EXpression data CLUstering and VISualization) has been developed using MATLAB Graphical User Interface (GUI) environment for analyzing the performances of different clustering algorithms on gene expression datasets. In this application package, the user needs to select a number of parameters such as internal validity indices, external validity indices and number of clusters from the active windows for evaluating the performance of the clustering algorithms. EXCLUVIS compares the performances of K-means, fuzzy C-means, hierarchical clustering and multiobjective evolutionary clustering algorithms. Heatmap and cluster profile plots are used for visualizing the results. EXCLUVIS allows the users to easily find the goodness of clustering solutions as well as provides visual representations of the clustering outcomes.', 'corpus_id': 16218273, 'score': 0}, {'doc_id': '1941265', 'title': 'Integrative Analysis of Head and Neck Cancer Identifies Two Biologically Distinct HPV and Three Non-HPV Subtypes', 'abstract': 'Purpose: Current classification of head and neck squamous cell carcinomas (HNSCC) based on anatomic site and stage fails to capture biologic heterogeneity or adequately inform treatment. Experimental Design: Here, we use gene expression-based consensus clustering, copy number profiling, and human papillomavirus (HPV) status on a clinically homogenous cohort of 134 locoregionally advanced HNSCCs with 44% HPV+ tumors together with additional cohorts, which in total comprise 938 tumors, to identify HNSCC subtypes and discover several subtype-specific, translationally relevant characteristics. Results: We identified five subtypes of HNSCC, including two biologically distinct HPV subtypes. One HPV+ and one HPV− subtype show a prominent immune and mesenchymal phenotype. Prominent tumor infiltration with CD8+ lymphocytes characterizes this inflamed/mesenchymal subtype, independent of HPV status. Compared with other subtypes, the two HPV subtypes show low expression and no copy number events for EGFR/HER ligands. In contrast, the basal subtype is uniquely characterized by a prominent EGFR/HER signaling phenotype, negative HPV-status, as well as strong hypoxic differentiation not seen in other subtypes. Conclusion: Our five-subtype classification provides a comprehensive overview of HPV+ as well as HPV− HNSCC biology with significant translational implications for biomarker development and personalized care for patients with HNSCC. Clin Cancer Res; 21(4); 870–81. ©2014 AACR.', 'corpus_id': 1941265, 'score': 1}, {'doc_id': '3068944', 'title': 'Cluster Ensembles --- A Knowledge Reuse Framework for Combining Multiple Partitions', 'abstract': ""This paper introduces the problem of combining multiple partitionings of a set of objects into a single consolidated clustering without accessing the features or algorithms that determined these partitionings. We first identify several application scenarios for the resultant 'knowledge reuse' framework that we call cluster ensembles. The cluster ensemble problem is then formalized as a combinatorial optimization problem in terms of shared mutual information. In addition to a direct maximization approach, we propose three effective and efficient techniques for obtaining high-quality combiners (consensus functions). The first combiner induces a similarity measure from the partitionings and then reclusters the objects. The second combiner is based on hypergraph partitioning. The third one collapses groups of clusters into meta-clusters which then compete for each object to determine the combined clustering. Due to the low computational costs of our techniques, it is quite feasible to use a supra-consensus function that evaluates all three approaches against the objective function and picks the best solution for a given situation. We evaluate the effectiveness of cluster ensembles in three qualitatively different application scenarios: (i) where the original clusters were formed based on non-identical sets of features, (ii) where the original clustering algorithms worked on non-identical sets of objects, and (iii) where a common data-set is used and the main purpose of combining multiple clusterings is to improve the quality and robustness of the solution. Promising results are obtained in all three situations for synthetic as well as real data-sets."", 'corpus_id': 3068944, 'score': 1}, {'doc_id': '221246167', 'title': 'ConiVAT: Cluster Tendency Assessment and Clustering with Partial Background Knowledge', 'abstract': 'The VAT method is a visual technique for determining the potential cluster structure and the possible number of clusters in numerical data. Its improved version, iVAT, uses a path-based distance transform to improve the effectiveness of VAT for ""tough"" cases. Both VAT and iVAT have also been used in conjunction with a single-linkage(SL) hierarchical clustering algorithm. However, they are sensitive to noise and bridge points between clusters in the dataset, and consequently, the corresponding VAT/iVAT images are often in-conclusive for such cases. In this paper, we propose a constraint-based version of iVAT, which we call ConiVAT, that makes use of background knowledge in the form of constraints, to improve VAT/iVAT for challenging and complex datasets. ConiVAT uses the input constraints to learn the underlying similarity metric and builds a minimum transitive dissimilarity matrix, before applying VAT to it. We demonstrate ConiVAT approach to visual assessment and single linkage clustering on nine datasets to show that, it improves the quality of iVAT images for complex datasets, and it also overcomes the limitation of SL clustering with VAT/iVAT due to ""noisy"" bridges between clusters. Extensive experiment results on nine datasets suggest that ConiVAT outperforms the other three semi-supervised clustering algorithms in terms of improved clustering accuracy.', 'corpus_id': 221246167, 'score': 0}, {'doc_id': '211727257', 'title': 'Single-cell RNA-seq clustering: datasets, models, and algorithms', 'abstract': 'ABSTRACT Single-cell RNA sequencing (scRNA-seq) technologies allow numerous opportunities for revealing novel and potentially unexpected biological discoveries. scRNA-seq clustering helps elucidate cell-to-cell heterogeneity and uncover cell subgroups and cell dynamics at the group level. Two important aspects of scRNA-seq data analysis were introduced and discussed in the present review: relevant datasets and analytical tools. In particular, we reviewed popular scRNA-seq datasets and discussed scRNA-seq clustering models including K-means clustering, hierarchical clustering, consensus clustering, and so on. Seven state-of-the-art scRNA clustering methods were compared on five public available datasets. Two primary evaluation metrics, the Adjusted Rand Index (ARI) and the Normalized Mutual Information (NMI), were used to evaluate these methods. Although unsupervised models can effectively cluster scRNA-seq data, these methods also have challenges. Some suggestions were provided for future research directions.', 'corpus_id': 211727257, 'score': 1}, {'doc_id': '220546282', 'title': 'Evaluating and Validating Cluster Results', 'abstract': 'Clustering is the technique to partition data according to their characteristics. Data that are similar in nature belong to the same cluster [1]. There are two types of evaluation methods to evaluate clustering quality. One is an external evaluation where the truth labels in the data sets are known in advance and the other is internal evaluation in which the evaluation is done with data set itself without true labels. In this paper, both external evaluation and internal evaluation are performed on the cluster results of the IRIS dataset. In the case of external evaluation Homogeneity, Correctness and V-measure scores are calculated for the dataset. For internal performance measures, the Silhouette Index and Sum of Square Errors are used. These internal performance measures along with the dendrogram (graphical tool from hierarchical Clustering) are used first to validate the number of clusters. Finally, as a statistical tool, we used the frequency distribution method to compare and provide a visual representation of the distribution of observations within a clustering result and the original data.', 'corpus_id': 220546282, 'score': 0}]"
176	Causal Reasoning	8d0b59b5eb0418053193fe299b739eb9	4026	{}	[{'doc_id': '226965627', 'title': 'Shortcomings of Counterfactual Fairness and a Proposed Modification', 'abstract': 'In this paper, I argue that counterfactual fairness does not constitute a necessary condition for an algorithm to be fair, and subsequently suggest how the constraint can be modified in order to remedy this shortcoming. To this end, I discuss a hypothetical scenario in which counterfactual fairness and an intuitive judgment of fairness come apart. Then, I turn to the question how the concept of discrimination can be explicated in order to examine the shortcomings of counterfactual fairness as a necessary condition of algorithmic fairness in more detail. I then incorporate the insights of this analysis into a novel fairness constraint, causal relevance fairness, which is a modification of the counterfactual fairness constraint that seems to circumvent its shortcomings.', 'corpus_id': 226965627, 'score': 0}, {'doc_id': '226222279', 'title': 'Thinking About Causation: A Causal Language with Epistemic Operators', 'abstract': 'This paper proposes a formal framework for modeling the interaction of causal and (qualitative) epistemic reasoning. To this purpose, we extend the notion of a causal model with a representation of the epistemic state of an agent. On the side of the object language, we add operators to express knowledge and the act of observing new information. We provide a sound and complete axiomatization of the logic, and discuss the relation of this framework to causal team semantics.', 'corpus_id': 226222279, 'score': 0}, {'doc_id': '221900965', 'title': 'The computational philosophy: simulation as a core philosophical method', 'abstract': 'Modeling and computer simulations, we claim, should be considered core philosophical methods. More precisely, we will defend two theses. First, philosophers should use simulations for many of the same reasons we currently use thought experiments. In fact, simulations are superior to thought experiments in achieving some philosophical goals. Second, devising and coding computational models instill good philosophical habits of mind. Throughout the paper, we respond to the often implicit objection that computer modeling is “not philosophical.”', 'corpus_id': 221900965, 'score': 0}, {'doc_id': '24869090', 'title': 'Functional domains of APOBEC3G required for antiviral activity', 'abstract': 'The viral protein, Vif, is essential for the production of infectious progeny virions in natural target cells of human immunodeficiency virus type 1 (HIV‐1). Several recent reports indicate that Vif acts by antagonizing the activity of an endogenous human antiviral protein, APOBEC3G. To investigate this route to restrict HIV‐1 infection, we employed mutagenesis to assess APOBEC3G function during HIV‐1 infection including interaction with Vif, localization, and activity in virions. We found that APOBEC3G binds Vif in infected cells and the C′‐terminal region is required for this interaction. APOBEC3G was only incorporated into virions in the absence of Vif and deletion of either the N′‐terminal or C′‐terminal regions of APOBEC3G abrogated virion localization. Assaying endogenous reverse transcription we found that APOBEC3G and its C′‐terminal deletion mutant inhibited full‐length cDNA synthesis, possibly through binding to viral RNA, a function revealed through gel‐shift assays. Taken together, our studies suggest that APOBEC3G inhibits HIV‐1 infection through interference with reverse transcription and that Vif counteracts APOBEC3G by impeding its entry into virions. © 2004 Wiley‐Liss, Inc.', 'corpus_id': 24869090, 'score': 0}, {'doc_id': '52910554', 'title': 'Challenges of Using Text Classifiers for Causal Inference', 'abstract': 'Causal understanding is essential for many kinds of decision-making, but causal inference from observational data has typically only been applied to structured, low-dimensional datasets. While text classifiers produce low-dimensional outputs, their use in causal inference has not previously been studied. To facilitate causal analyses based on language data, we consider the role that text classifiers can play in causal inference through established modeling mechanisms from the causality literature on missing data and measurement error. We demonstrate how to conduct causal analyses using text classifiers on simulated and Yelp data, and discuss the opportunities and challenges of future work that uses text data in causal inference.', 'corpus_id': 52910554, 'score': 1}, {'doc_id': '7572568', 'title': 'The algorithmization of counterfactuals', 'abstract': 'Recent advances in causal reasoning have given rise to a computation model that emulates the process by which humans generate, evaluate and distinguish counterfactual sentences. Though compatible with the “possible worlds” account, this model enjoys the advantages of representational economy, algorithmic simplicity and conceptual clarity. Using this model, the paper demonstrates the processing of counterfactual sentences on a classical example due to Ernest Adam. It then gives a panoramic view of several applications where counterfactual reasoning has benefited problem areas in the empirical sciences.', 'corpus_id': 7572568, 'score': 1}, {'doc_id': '225062514', 'title': 'Algorithms for Causal Reasoning in Probability Trees', 'abstract': 'Probability trees are one of the simplest models of causal generative processes. They possess clean semantics and -- unlike causal Bayesian networks -- they can represent context-specific causal dependencies, which are necessary for e.g. causal induction. Yet, they have received little attention from the AI and ML community. Here we present concrete algorithms for causal reasoning in discrete probability trees that cover the entire causal hierarchy (association, intervention, and counterfactuals), and operate on arbitrary propositional and causal events. Our work expands the domain of causal reasoning to a very general class of discrete stochastic processes.', 'corpus_id': 225062514, 'score': 1}, {'doc_id': '52092606', 'title': 'A speculation on the tandem fasciclin 1 repeat of FLA4 proteins in angiosperms', 'abstract': 'ABSTRACT The Arabidopsis thaliana Fasciclin like arabinogalactan protein 4 (FLA4) locus is required for normal root growth in a linear genetic pathway with the FEI1 and FEI2 loci coding for receptor-like kinases. The two Fas1 domains of FLA4 are conserved among angiosperms but only the C-terminal Fas1 domain is required for genetic function. We show that at low salt deletion of the N-terminal Fas1 domain of transgenic FLA4 leads to enhanced root elongation compared to the tandem Fas1 wild type version. Modeling the hypothetical interaction between FLA4 and FEI1 we show that the predicted interaction is predominantly involving the C-terminal Fas1 domain. Relative conformational mobility between the two FLA4 Fas1 domains might regulate the interaction with the FEI receptor kinases. We therefore speculate that the FLA4 FEI complex might be a sensor for environmental conditions in the apoplast.', 'corpus_id': 52092606, 'score': 0}, {'doc_id': '53061670', 'title': 'Causal and Counterfactual Inference', 'abstract': 'All accounts of rationality presuppose knowledge of how actions affect the state of the world and how the world would change had alternative actions been taken. The paper presents a framework called Structural Causal Model (SCM) which operationalizes this knowledge and explicates how it can be derived from both theories and data. In particular, we show how counterfactuals are computed and how they can be embedded in a calculus that solves critical problems in the empirical sciences.', 'corpus_id': 53061670, 'score': 1}, {'doc_id': '227209308', 'title': 'Braid: Weaving Symbolic and Statistical Knowledge into Coherent Logical Explanations', 'abstract': 'Traditional symbolic reasoning engines, while attractive for their precision and explicability, have a few major drawbacks: the use of brittle inference procedures that rely on exact matching/unification of logical terms, an inability to deal with uncertainty, and the need for a precompiled rule-base of knowledge (the “knowledge acquisition” problem). These issues are particularly severe for the Natural Language Understanding (NLU) task, where we often use implicit background knowledge to understand and reason about text, resort to imperfect/fuzzy alignment of concepts and relations during reasoning, and constantly deal with ambiguity in representations. To address these issues, we devise a novel FOL-based reasoner, called Braid, that supports probabilistic rules, and uses the notion of custom unification functions and dynamic rule generation to overcome the brittle matching and knowledge-gap problem prevalent in traditional reasoners. In this paper, we describe the reasoning algorithms used in Braid-BC (the backchaining component of Braid), and their implementation in a distributed task-based framework that builds proof/explanation graphs for an input query in a highly scalable manner. We use a simple QA example from a children’s story to motivate Braid-BC’s design and explain how the various components work together to produce a coherent logical explanation.', 'corpus_id': 227209308, 'score': 1}]
177	COVID Disparities	5ee8f636c063eff63eaa50cccd19d6ff	4537	{'COVID': 'coronavirus disease'}	[{'doc_id': '211476589', 'title': 'Understanding of COVID‐19 based on current evidence', 'abstract': 'Since December 2019, a series of unexplained pneumonia cases have been reported in Wuhan, China. On 12 January 2020, the World Health Organization (WHO) temporarily named this new virus as the 2019 novel coronavirus (2019‐nCoV). On 11 February 2020, the WHO officially named the disease caused by the 2019‐nCoV as coronavirus disease (COVID‐19). The COVID‐19 epidemic is spreading all over the world, especially in China. Based on the published evidence, we systematically discuss the characteristics of COVID‐19 in the hope of providing a reference for future studies and help for the prevention and control of the COVID‐19 epidemic.', 'corpus_id': 211476589, 'score': 0}, {'doc_id': '218580243', 'title': 'Epidemiology of CoVID-19 and predictors of recovery in the Republic of Korea', 'abstract': 'Background: The recent CoVID-19 pandemic has emerged as a threat to global health. Though current evidence on the epidemiology of the disease is emerging, very little is known about the predictors of recovery. We describe the epidemiology of confirmed CoVID-19 patients in Republic of Korea and identify predictors of recovery. Materials and methods: Using publicly available data for confirmed CoVID-19 cases from the Korea Centers for Disease Control and Prevention from January 20, 2020 to April 30, 2020, we undertook descriptive analyses of cases stratified by sex, age group, place of exposure, date of confirmation and province. Correlation was tested among all predictors (sex, age group, place of exposure and province) with the Pearsons correlation coefficient. Associations between recovery from CoVID-19 and predictors were estimated using a multivariable logistic regression model. Results: Majority of the confirmed cases were females (56 percent), from 20-29 age group (24.3 percent), and primarily from three provinces Gyeongsangbuk (36.9 percent), Gyeonggi (20.5 percent) and Seoul (17.1 percent). Case fatality ratio was 2.1 percent and 41.6 percent cases recovered. Older patients, patients from certain provinces such as Daegu, Gyeonggi, Gyeongsangbuk, Jeju, Jeollabuk and Jeollanam, and those contracting the disease from healthcare settings had lower recovery. Conclusions: Our study adds to the very limited evidence base on potential predictors of recovery among confirmed CoVID-19 cases. We call additional research to explore the predictors of recovery and support development of policies to protect the vulnerable patient groups.', 'corpus_id': 218580243, 'score': 0}, {'doc_id': '218596624', 'title': 'Epidemiological, socio-demographic and clinical features of the early phase of the COVID-19 epidemic in Ecuador', 'abstract': 'Abstract Background: The SARS-CoV-2 virus has spread all over the world infecting more than 3,585,936 people from over 210 countries and caused more than 245,803 deaths worldwide. We report the first epidemiological, socio-demographic, and clinical findings for the first 9,468 confirmed COVID-19 cases in Ecuador. Methods: We conducted a descriptive cross-sectional analysis of 9,468 COVID-19 confirmed cases in Ecuador from 27 February to 18 April 2020. The overall incidence, mortality, and case fatality rate was computed according to the entire population at risk living in a canton or a province. Disability adjusted life years, attack and crude mortality rates as well as relative risk and odds ratios were computed as an outcome. Results: Since the first case reported in Ecuador on 27 Feb 2020, at least 9,468 positive COVID-19 cases of which 474 deaths were officially registered over a 54-day period. Men accounted for 55.40% (n = 5, 247) of the overall cases with an incidence rate of 60.5 per 100,000 while women accounted for 44.60 % (n = 4, 221) representing 47.2 per 100,000. The mortality rate per canton showed that cantons with a lower attack rate had higher mortality rates. Coastal cantons have a lower attack rate than the highlands and living above >2,500 m seems to be linked with a lower risk of dying (RR: 0.63 [CI 95% 0.50 - 0.79]). Fatigue was reported in 53.2% of the patients, followed by headache (43%), dry cough (41.7%), ageusia (37.1%) and anosmia (36.1%). Conclusion: This study is the first of its kind in Ecuador. The results of this analysis show that men are at higher risk of dying from COVID-19 than women, which increases as with age and the presence of comorbidities. Areas with better testing capabilities reported lower CFR% and mortality, additionally cantons located above 2,500 m have lower attack and mortality rates although the risk of dying is greater among highlanders. Keywords: COVID-19; SARS-CoV-2; Ecuador; Epidemiology; Latin America', 'corpus_id': 218596624, 'score': 0}, {'doc_id': '216653305', 'title': 'Racial and Ethnic Disparities in SARS-CoV-2 Pandemic: Analysis of a COVID-19 Observational Registry for a Diverse U.S. Metropolitan Population', 'abstract': 'Importance: Despite emerging reports of poor COVID-19 outcomes among African Americans, data on race and ethnic susceptibility to SARS-CoV-2 infection are limited. Objective: To determine socio-demographic factors associated with higher likelihood of SARS-CoV-2 infection. To explore mediating pathways for race disparities in the SARS-CoV-2 pandemic. Design: Cross sectional analysis of COVID-19 Surveillance and Outcomes Registry (CURATOR). Multivariable logistic regression models were fitted to provide likelihood estimates (adjusted Odds Ratios: aOR, 95% confidence intervals: CI) of positive SARS-CoV-2 test. Structural Equation Modeling (SEM) framework was utilized to explore three mediation pathways (low income, high population density, high comorbidity burden) for association between African American race and SARS-CoV-2 infection. Setting: A large healthcare system comprising of one central tertiary care, seven large community hospitals and an expansive ambulatory and emergency care network in the Greater Houston area. Participants: Individuals of all ages, races, ethnicities and sex tested for SARS-CoV-2. Exposure: Socio-demographic (age, sex, race, ethnicity, household income, residence population density) and comorbidity (hypertension, diabetes, obesity, cardiac disease) factors. Main Outcome: Positive reverse transcriptase polymerized chain reaction test for SARS-CoV-2. Results: Among 4,513 tested individuals, 754 (16.7%) tested positive. Overall mean (SD) age was 50.6 (18.9) years, 62% females and 26% were African American. African American race was associated with higher comorbidity burden, lower socio-economic status, and higher population density residence. In the fully adjusted model, African American race (vs. White; aOR, CI: 1.84, 1.49-2.27) and Hispanic ethnicity (vs. non-Hispanic; aOR, CI: 1.70, 1.35-2.14) had a higher likelihood of infection. Older individuals and males were also at a higher risk of SARS-CoV-2 infection. The SEM framework demonstrated a statistically significant (p = 0.008) indirect effect of African American race on SARS-CoV-2 infection mediated via a pathway that included residence in densely populated zip code. Conclusions and Relevance: There is strong evidence of race and ethnic disparities in the SARS-CoV-2 pandemic potentially mediated through unique social determinants of health.', 'corpus_id': 216653305, 'score': 1}, {'doc_id': '216588655', 'title': 'Epidemiological and clinical characteristics of the early phase of the COVID-19 epidemic in Brazil', 'abstract': 'Background: The first case of COVID-19 was detected in Brazil on February 25, 2020. We report the epidemiological, demographic, and clinical findings for confirmed COVID-19 cases during the first month of the epidemic in Brazil. Methods: Individual-level and aggregated COVID-19 data were analysed to investigate demographic profiles, socioeconomic drivers and age-sex structure of COVID-19 tested cases. Basic reproduction numbers (R0) were investigated for Sao Paulo and Rio de Janeiro. Multivariate logistic regression analyses were used to identify symptoms associated with confirmed cases and risk factors associated with hospitalization. Laboratory diagnosis for eight respiratory viruses were obtained for 2,429 cases. Findings: By March 25, 1,468 confirmed cases were notified in Brazil, of whom 10% (147 of 1,468) were hospitalised. Of the cases acquired locally (77.8%), two thirds (66.9% of 5,746) were confirmed in private laboratories. Overall, positive association between higher per capita income and COVID-19 diagnosis was identified. The median age of detected cases was 39 years (IQR 30-53). The median R0 was 2.9 for Sao Paulo and Rio de Janeiro. Cardiovascular disease/hypertension were associated with hospitalization. Co-circulation of six respiratory viruses, including influenza A and B and human rhinovirus was detected in low levels. Interpretation: Socioeconomic disparity determines access to SARS-CoV-2 testing in Brazil. The lower median age of infection and hospitalization compared to other countries is expected due to a younger population structure. Enhanced surveillance of respiratory pathogens across socioeconomic statuses is essential to better understand and halt SARS-CoV-2 transmission.', 'corpus_id': 216588655, 'score': 0}, {'doc_id': '211230955', 'title': 'Incubation Period and Other Epidemiological Characteristics of 2019 Novel Coronavirus Infections with Right Truncation: A Statistical Analysis of Publicly Available Case Data', 'abstract': 'The geographic spread of 2019 novel coronavirus (COVID-19) infections from the epicenter of Wuhan, China, has provided an opportunity to study the natural history of the recently emerged virus. Using publicly available event-date data from the ongoing epidemic, the present study investigated the incubation period and other time intervals that govern the epidemiological dynamics of COVID-19 infections. Our results show that the incubation period falls within the range of 2–14 days with 95% confidence and has a mean of around 5 days when approximated using the best-fit lognormal distribution. The mean time from illness onset to hospital admission (for treatment and/or isolation) was estimated at 3–4 days without truncation and at 5–9 days when right truncated. Based on the 95th percentile estimate of the incubation period, we recommend that the length of quarantine should be at least 14 days. The median time delay of 13 days from illness onset to death (17 days with right truncation) should be considered when estimating the COVID-19 case fatality risk.', 'corpus_id': 211230955, 'score': 0}, {'doc_id': '218594557', 'title': 'The Relationship of Diabetes and COVID-19: A Health Disparity', 'abstract': 'The COVID-19 pandemic has become one of the most devastating events in the world. It has been particularly severe in the United States relative to cases, hospitalizations, and deaths. As the pandemic has progressed in the United States it has become obvious that cases and deaths of COVID-19 are not randomly distributed in the population. A disproportionate number of cases and deaths have occurred in racial minorities. Underlying conditions may be contributing to COVID-19 deaths. This study’s objectives are to evaluate the number and rates of cases and deaths among racial minorities, identify the distribution of underlying conditions in COVID-19 cases and deaths, and to review the relationship of COVID-19 and diabetes. Diabetes, as well as several diabetes comorbidities, are considered health disparities among racial minorities. The study findings concluded that the areas with high number of COVID-19 and high morbidity and mortality rates have a high percentage of Blacks in their populations. The percentage of Blacks in areas with the highest COVID-19 mortality rate is between 70.9% to 60.2 percent. The prevalence of diabetes in these areas ranges from 14% to 10 percent. The prevalence of obesity in these areas varies from 43.0% to 37.0 percent. Diabetes seems to be contributing to COVID-19 classification as an infectious disease health disparity. ISSN 2639-9326 Research Article Citation: Peter J Fos, Peggy A Honoré, Katrina Kellum. The Relationship of Diabetes and COVID-19: A Health Disparity. Diabetes Complications. 2020; 4(1); 1-8.', 'corpus_id': 218594557, 'score': 0}, {'doc_id': '214636811', 'title': 'Title : Understanding of COVID-19 based on current evidence Running head : Current understanding of COVID-19', 'abstract': 'Since December 2019, a series of unexplained pneumonia cases has been reported in Wuhan, China. On January 12, 2020, the World Health Organization (WHO) temporarily named this new virus as the 2019 novel coronavirus (2019-nCoV). On February 11, 2020, the WHO officially named the disease caused by the 2019-nCoV as Corona Virus Disease (COVID-19). The COVID-19 epidemic is spreading all over the world, especially in China. Based on the published evidence, we systematically discuss the characteristics of COVID-19 in the hope of providing a reference for future studies and help for the prevention and control of the COVID-19 epidemic.', 'corpus_id': 214636811, 'score': 0}, {'doc_id': '218538818', 'title': 'Assessing Differential Impacts of COVID-19 on Black Communities', 'abstract': 'Purpose Given incomplete data reporting by race, we used data on COVID-19 cases and deaths in US counties to describe racial disparities in COVID-19 disease and death and associated determinants. Methods Using publicly available data (accessed April 13, 2020), predictors of COVID-19 cases and deaths were compared between disproportionately (>13%) black and all other (<13% black) counties. Rate ratios were calculated and population attributable fractions (PAF) were estimated using COVID-19 cases and deaths via zero-inflated negative binomial regression model. National maps with county-level data and an interactive scatterplot of COVID-19 cases were generated. Results Nearly ninety-seven percent of disproportionately black counties (656/677) reported a case and 49% (330/677) reported a death versus 81% (1987/2,465) and 28% (684/ 2465), respectively, for all other counties. Counties with higher proportions of black people have higher prevalence of comorbidities and greater air pollution. Counties with higher proportions of black residents had more COVID-19 diagnoses (RR 1.24, 95% CI 1.17-1.33) and deaths (RR 1.18, 95% CI 1.00-1.40), after adjusting for county-level characteristics such as age, poverty, comorbidities, and epidemic duration. COVID-19 deaths were higher in disproportionally black rural and small metro counties. The PAF of COVID-19 diagnosis due to lack of health insurance was 3.3% for counties with <13% black residents and 4.2% for counties with >13% black residents. Conclusions Nearly twenty-two percent of US counties are disproportionately black and they accounted for 52% of COVID-19 diagnoses and 58% of COVID-19 deaths nationally. County-level comparisons can both inform COVID-19 responses and identify epidemic hot spots. Social conditions, structural racism, and other factors elevate risk for COVID-19 diagnoses and deaths in black communities.', 'corpus_id': 218538818, 'score': 1}, {'doc_id': '216595802', 'title': 'Contact Tracing for COVID-19: An Opportunity to Reduce Health Disparities and End the Human Immunodeficiency Virus/AIDS Epidemic in the United States', 'abstract': 'Abstract Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) testing and contact tracing have been proposed as critical components of a safe and effective coronavirus disease 2019 (COVID-19) public health strategy. We argue that COVID-19 contact tracing may provide a unique opportunity to also conduct widespread HIV testing, among other health-promotion activities.', 'corpus_id': 216595802, 'score': 1}]
178	Neurosymbolic DL and complexity theory	eae9acf0843382381acabda4850fbf50	13008	{}	"[{'doc_id': '219966125', 'title': 'Discovering Symbolic Models from Deep Learning with Inductive Biases', 'abstract': 'We develop a general approach to distill symbolic representations of a learned deep model by introducing strong inductive biases. We focus on Graph Neural Networks (GNNs). The technique works as follows: we first encourage sparse latent representations when we train a GNN in a supervised setting, then we apply symbolic regression to components of the learned model to extract explicit physical relations. We find the correct known equations, including force laws and Hamiltonians, can be extracted from the neural network. We then apply our method to a non-trivial cosmology example-a detailed dark matter simulation-and discover a new analytic formula which can predict the concentration of dark matter from the mass distribution of nearby cosmic structures. The symbolic expressions extracted from the GNN using our technique also generalized to out-of-distribution data better than the GNN itself. Our approach offers alternative directions for interpreting neural networks and discovering novel physical principles from the representations they learn.', 'corpus_id': 219966125, 'score': 1}, {'doc_id': '226299669', 'title': 'Learning for Integer-Constrained Optimization through Neural Networks with Limited Training', 'abstract': 'In this paper, we investigate a neural network-based learning approach towards solving an integer-constrained programming problem using very limited training. To be specific, we introduce a symmetric and decomposed neural network structure, which is fully interpretable in terms of the functionality of its constituent components. By taking advantage of the underlying pattern of the integer constraint, as well as of the affine nature of the objective function, the introduced neural network offers superior generalization performance with limited training, as compared to other generic neural network structures that do not exploit the inherent structure of the integer constraint. In addition, we show that the introduced decomposed approach can be further extended to semi-decomposed frameworks. The introduced learning approach is evaluated via the classification/symbol detection task in the context of wireless communication systems where available training sets are usually limited. Evaluation results demonstrate that the introduced learning strategy is able to effectively perform the classification/symbol detection task in a wide variety of wireless channel environments specified by the 3GPP community.', 'corpus_id': 226299669, 'score': 0}, {'doc_id': '226222006', 'title': 'Less is More: Data-Efficient Complex Question Answering over Knowledge Bases', 'abstract': 'Abstract Question answering is an effective method for obtaining information from knowledge bases (KB). In this paper, we propose the Neural-Symbolic Complex Question Answering (NS-CQA) model, a data-efficient reinforcement learning framework for complex question answering by using only a modest number of training samples. Our framework consists of a neural generator and a symbolic executor that, respectively, transforms a natural-language question into a sequence of primitive actions, and executes them over the knowledge base to compute the answer. We carefully formulate a set of primitive symbolic actions that allows us to not only simplify our neural network design but also accelerate model convergence. To reduce search space, we employ the copy and masking mechanisms in our encoder–decoder architecture to drastically reduce the decoder output vocabulary and improve model generalizability. We equip our model with a memory buffer that stores high-reward promising programs. Besides, we propose an adaptive reward function. By comparing the generated trial with the trials stored in the memory buffer, we derive the curriculum-guided reward bonus, i.e., the proximity and the novelty. To mitigate the sparse reward problem, we combine the adaptive reward and the reward bonus, reshaping the sparse reward into dense feedback. Also, we encourage the model to generate new trials to avoid imitating the spurious trials while making the model remember the past high-reward trials to improve data efficiency. Our NS-CQA model is evaluated on two datasets: CQA, a recent large-scale complex question answering dataset, and WebQuestionsSP, a multi-hop question answering dataset. On both datasets, our model outperforms the state-of-the-art models. Notably, on CQA, NS-CQA performs well on questions with higher complexity, while only using approximately 1% of the total training samples.', 'corpus_id': 226222006, 'score': 0}, {'doc_id': '219792252', 'title': 'Erdos Goes Neural: an Unsupervised Learning Framework for Combinatorial Optimization on Graphs', 'abstract': ""Combinatorial optimization problems are notoriously challenging for neural networks, especially in the absence of labeled instances. This work proposes an unsupervised learning framework for CO problems on graphs that can provide integral solutions of certified quality. Inspired by Erdos' probabilistic method, we use a neural network to parametrize a probability distribution over sets. Crucially, we show that when the network is optimized w.r.t. a suitably chosen loss, the learned distribution contains, with controlled probability, a low-cost integral solution that obeys the constraints of the combinatorial problem. The probabilistic proof of existence is then derandomized to decode the desired solutions. We demonstrate the efficacy of this approach to obtain valid solutions to the maximum clique problem and to perform local graph clustering. Our method achieves competitive results on both real datasets and synthetic hard instances."", 'corpus_id': 219792252, 'score': 1}, {'doc_id': '221567739', 'title': 'Cryptanalysis of RSA: Integer Prime Factorization Using Genetic Algorithms', 'abstract': 'In recent years, researchers have been exploring alternative methods to solving Integer Prime Factorization, the decomposition of an integer into its prime factors. This has direct application to cryptanalysis of RSA, as one means of breaking such a cryptosystem requires factorization of a large number that is the product of two prime numbers. This paper applies three different genetic algorithms to solve this issue, utilizing mathematical knowledge concerning distribution of primes to improve the algorithms. The best of the three genetic algorithms has a chromosome that represents m in the equation prime = 6 m ± 1, and is able to factor a number of up to 22 decimal digits. This is a significantly larger number than the largest factored by comparable methods in earlier work. This leads to the conclusion that approaches such as genetic algorithms are a promising avenue of research into the problem of integer factorization.', 'corpus_id': 221567739, 'score': 1}, {'doc_id': '227335683', 'title': 'Why Unsupervised Deep Networks Generalize', 'abstract': ""Promising resolutions of the generalization puzzle observe that the actual number of parameters in a deep network is much smaller than naive estimates suggest. The renormalization group is a compelling example of a problem which has very few parameters, despite the fact that naive estimates suggest otherwise. Our central hypothesis is that the mechanisms behind the renormalization group are also at work in deep learning, and that this leads to a resolution of the generalization puzzle. We show detailed quantitative evidence that proves the hypothesis for an RBM, by showing that the trained RBM is discarding high momentum modes. Specializing attention mainly to autoencoders, we give an algorithm to determine the network's parameters directly from the learning data set. The resulting autoencoder almost performs as well as one trained by deep learning, and it provides an excellent initial condition for training, reducing training times by a factor between 4 and 100 for the experiments we considered. Further, we are able to suggest a simple criterion to decide if a given problem can or can not be solved using a deep network."", 'corpus_id': 227335683, 'score': 0}, {'doc_id': '231602957', 'title': 'On the quantization of recurrent neural networks', 'abstract': 'Integer quantization of neural networks can be defined as the approximation of the high precision computation of the canonical neural network formulation, using reduced integer precision. It plays a significant role in the efficient deployment and execution of machine learning (ML) systems, reducing memory consumption and leveraging typically faster computations. In this work, we present an integer-only quantization strategy for Long Short-Term Memory (LSTM) neural network topologies, which themselves are the foundation of many production ML systems. Our quantization strategy is accurate (e.g. works well with quantization post-training), efficient and fast to execute (utilizing 8 bit integer weights and mostly 8 bit activations), and is able to target a variety of hardware (by leveraging instructions sets available in common CPU architectures, as well as available neural accelerators).', 'corpus_id': 231602957, 'score': 0}, {'doc_id': '228083996', 'title': 'Neurosymbolic AI: The 3rd Wave', 'abstract': 'Current advances in Artificial Intelligence (AI) and Machine Learning (ML) have achieved unprecedented impact across research communities and industry. Nevertheless, concerns about trust, safety, interpretability and accountability of AI were raised by influential thinkers. Many have identified the need for well-founded knowledge representation and reasoning to be integrated with deep learning and for sound explainability. Neural-symbolic computing has been an active area of research for many years seeking to bring together robust learning in neural networks with reasoning and explainability via symbolic representations for network models. In this paper, we relate recent and early research results in neurosymbolic AI with the objective of identifying the key ingredients of the next wave of AI systems. We focus on research that integrates in a principled way neural network-based learning with symbolic knowledge representation and logical reasoning. The insights provided by 20 years of neural-symbolic computing are shown to shed new light onto the increasingly prominent role of trust, safety, interpretability and accountability of AI. We also identify promising directions and challenges for the next decade of AI research from the perspective of neural-symbolic systems.', 'corpus_id': 228083996, 'score': 1}, {'doc_id': '12427973', 'title': 'Neural networks for NP-complete problems', 'abstract': 'Abstract combinatorial optimization is an active field of research in Neural Networks. Since the first attempts to solve the travelling salesman problem with Hopfield nets several progresses have been made. I will present some Neural Network approximate solutions for NP-complete problems that have a sound mathematical foundation and that, beside their theoretical interest, are also numerically encouraging. These algorithms easily deal with problems with thousands of instances taking Neural Network approaches out of the “toy-problem” era.', 'corpus_id': 12427973, 'score': 1}, {'doc_id': '227151896', 'title': 'A Use of Even Activation Functions in Neural Networks', 'abstract': 'Despite broad interest in applying deep learning techniques to scientific discovery, learning interpretable formulas that accurately describe scientific data is very challenging because of the vast landscape of possible functions and the ""black box"" nature of deep neural networks. The key to success is to effectively integrate existing knowledge or hypotheses about the underlying structure of the data into the architecture of deep learning models to guide machine learning. Currently, such integration is commonly done through customization of the loss functions. Here we propose an alternative approach to integrate existing knowledge or hypotheses of data structure by constructing custom activation functions that reflect this structure. Specifically, we study a common case when the multivariate target function $f$ to be learned from the data is partially exchangeable, \\emph{i.e.} $f(u,v,w)=f(v,u,w)$ for $u,v\\in \\mathbb{R}^d$. For instance, these conditions are satisfied for the classification of images that is invariant under left-right flipping. Through theoretical proof and experimental verification, we show that using an even activation function in one of the fully connected layers improves neural network performance. In our experimental 9-dimensional regression problems, replacing one of the non-symmetric activation functions with the designated ""Seagull"" activation function $\\log(1+x^2)$ results in substantial improvement in network performance. Surprisingly, even activation functions are seldom used in neural networks. Our results suggest that customized activation functions have great potential in neural networks.', 'corpus_id': 227151896, 'score': 0}]"
179	COVID19	8586b34f62fc3c323ce381ed1d08514d	3400	{'COVID19': 'coronavirus disease 2019'}	"[{'doc_id': '214757583', 'title': ""You're only as young as your immune system"", 'abstract': '\n               \n                  There has never been a more important time to keep your immune system fit and healthy. And as Graham Lawton discovers, there are now ways to keep it younger than you are\n               \n            ', 'corpus_id': 214757583, 'score': 1}, {'doc_id': '25619458', 'title': 'A randomised controlled trial of the effectiveness of an exercise training program in patients recovering from severe acute respiratory syndrome', 'abstract': '\n \n The aim of this study was to evaluate the effectiveness of an exercise training program on cardiorespiratory and musculoskeletal performance and health-related quality of life of patients who were recovering from severe acute respiratory syndrome (SARS). A 6-week supervised exercise training program was carried out in the physiotherapy department of a university teaching hospital. One hundred and thirty-three patients referred from a SARS Review Clinic solely for physiotherapy were included. Cardiorespiratory fitness (6-minute walk test, Chester Step Test for predicting VO2max), musculoskeletal performance (isometric deltoid and gluteal muscles strength, handgrip strength, 1-minute curl-up and push-up tests) and health-related quality of life (SF-36) were measured and evaluated. Patients were assigned randomly to either a control group (standardised educational session about exercise rehabilitation) or an exercise group. After 6 weeks, significantly greater improvement was shown in the exercise group in the 6-minute walk test (77.4 m vs 20.7 m, p < 0.001), VO2max (3.6 ml/kg/min vs 1 ml/kg/min, p = 0.04), and musculoskeletal performance (handgrip strength, curl-up and push-up tests, p < 0.05). Effects on health-related quality of life were not statistically significant. It was concluded that the exercise training program was effective in improving both the cardiorespiratory and musculoskeletal fitness in patients recovering from SARS. However, health-related quality of life was not affected by physical training.\n \n', 'corpus_id': 25619458, 'score': 1}, {'doc_id': '214785388', 'title': 'Impact of the COVID-19 Pandemic on Mental Health and Quality of Life among Local Residents in Liaoning Province, China: A Cross-Sectional Study', 'abstract': 'Our study aimed to investigate the immediate impact of the COVID-19 pandemic on mental health and quality of life among local Chinese residents aged ≥18 years in Liaoning Province, mainland China. An online survey was distributed through a social media platform between January and February 2020. Participants completed a modified validated questionnaire that assessed the Impact of Event Scale (IES), indicators of negative mental health impacts, social and family support, and mental health-related lifestyle changes. A total of 263 participants (106 males and 157 females) completed the study. The mean age of the participants was 37.7 ± 14.0 years, and 74.9% had a high level of education. The mean IES score in the participants was 13.6 ± 7.7, reflecting a mild stressful impact. Only 7.6% of participants had an IES score ≥26. The majority of participants (53.3%) did not feel helpless due to the pandemic. On the other hand, 52.1% of participants felt horrified and apprehensive due to the pandemic. Additionally, the majority of participants (57.8–77.9%) received increased support from friends and family members, increased shared feeling and caring with family members and others. In conclusion, the COVID-19 pandemic was associated with mild stressful impact in our sample, even though the COVID-19 pandemic is still ongoing. These findings would need to be verified in larger population studies.', 'corpus_id': 214785388, 'score': 0}, {'doc_id': '215801066', 'title': 'The impact of nutrition on COVID-19 susceptibility and long-term consequences', 'abstract': '\n Abstract\n \n While all groups are affected by the COVID-19 pandemic, the elderly, underrepresented minorities, and those with underlying medical conditions are at the greatest risk. The high rate of consumption of diets high in saturated fats, sugars, and refined carbohydrates (collectively called Western diet, WD) worldwide, contribute to the prevalence of obesity and type II diabetes, and could place these populations at an increased risk for severe COVID-19 pathology and mortality. WD consumption activates the innate immune system and impairs adaptive immunity, leading to chronic inflammation and impaired host defense against viruses. Furthermore, peripheral inflammation caused by COVID-19 may have long-term consequences in those that recover, leading to chronic medical conditions such as dementia and neurodegenerative disease, likely through neuroinflammatory mechanisms that can be compounded by an unhealthy diet. Thus, now more than ever, wider access to healthy foods should be a top priority and individuals should be mindful of healthy eating habits to reduce susceptibility to and long-term complications from COVID-19.\n \n', 'corpus_id': 215801066, 'score': 1}, {'doc_id': '215782457', 'title': 'Prevalence and Factors Associated with Depression and Anxiety of Hospitalized Patients with COVID-19', 'abstract': 'Objective: The 2019 coronavirus disease (COVID-19) epidemic has raised international concern. Mental health is becoming an issue that cannot be ignored in our fight against it. This study aimed to explore the prevalence and factors linked to anxiety and depression in hospitalized patients with COVID-19. Methods: A total of 144 patients diagnosed with COVID-19 were included in this study. We assessed depression and anxiety symptoms using the Hospital Anxiety and Depression Scale (HADS), and social support using the Perceived Social Support Scale (PSSS) among patients at admission. Multivariate linear regression analyses were performed to identify factors associated with symptoms of anxiety and depression. Results: Of the 144 participants, 34.72% and 28.47% patients with COVID-19 had symptoms of anxiety or depression, respectively. The bivariate correlations showed that less social support was correlated with more anxious (r=-0.196, p<0.05) and depressive (r=-0.360，p<0.05) symptoms among patients with COVID-19. The multiple linear regression analysis showed that gender (β=1.446, p=0.034), age (β=0.074, p=0.003), oxygen saturation (β =-2.140, p=0.049), and social support (β =-1.545, p=0.017) were associated with anxiety for COVID-19 patients. Moreover, age (β=0.084, p=0.001), family infection with SARS-CoV-2 (β =1.515, p=0.027) and social support (β =-2.236, p＜0.001) were the factors associated with depression. Conclusion: Hospitalized patients with COVID-19 presented features of anxiety and depression. Mental concern and appropriate intervention are essential parts of clinical care for those who are at risk.', 'corpus_id': 215782457, 'score': 0}, {'doc_id': '21167794', 'title': 'The global burden of diabetes and its complications: an emerging pandemic', 'abstract': 'The number of patients with type 2 diabetes is increasing rapidly in both developed and developing countries around the world. The emerging pandemic is driven by the combined effects of population ageing, rising levels of obesity and inactivity, and greater longevity among patients with diabetes that is attributable to improved management. The vascular complications of type 2 diabetes account for the majority of the social and economic burden among patients and society more broadly. This review summarizes the burden of type 2 diabetes, impaired glucose tolerance, and their vascular complications. It is projected that by 2025 there will be 380 million people with type 2 diabetes and 418 million people with impaired glucose tolerance. Diabetes is a major global cause of premature mortality that is widely underestimated, because only a minority of persons with diabetes dies from a cause uniquely related to the condition. Approximately one half of patients with type 2 diabetes die prematurely of a cardiovascular cause and approximately 10% die of renal failure. Global excess mortality attributable to diabetes in adults was estimated to be 3.8 million deaths. Eur J Cardiovasc Prev Rehabil 17 (Suppl 1):S3-S8 © 2010 The European Society of Cardiology', 'corpus_id': 21167794, 'score': 1}, {'doc_id': '214789001', 'title': 'Psychological status of medical workforce during the COVID-19 pandemic: A cross-sectional study', 'abstract': '\n Abstract\n \n The pandemic of 2019 coronavirus disease (COVID-19) has burdened an unprecedented psychological stress on people around the world, especially\xa0the medical workforce. The study focuses on assess the psychological status of them. The authors conducted a single-center, cross-sectional survey via online questionnaires. Occurrence of fear, anxiety and depression were measured by the numeric rating scale (NRS) on fear, Hamilton Anxiety Scale (HAMA), and Hamilton Depression Scale (HAMD), respectively. A total of 2299 eligible participants were enrolled from the authors’ institution, including 2042 medical staff and 257 administrative staff. The severity of fear, anxiety and depression were significantly different between two groups. Furthermore, as compared to the non-clinical staff, front line medical staff with close contact with infected patients, including working in the departments of respiratory,\xa0emergency,\xa0infectious disease, and ICU, showed higher scores on fear scale, HAMA and HAMD, and they were 1.4 times more likely to feel fear, twice\xa0more\xa0likely\xa0to suffer\xa0anxiety and depression. The medical staff especially working\xa0in above-mentioned departments made them more susceptible to psychological disorders. Effective strategies toward to improving the mental health should be provided to these individuals.\n \n', 'corpus_id': 214789001, 'score': 0}, {'doc_id': '221346558', 'title': 'COVID-19 in adult patients with pre-existing chronic cardiac, respiratory and metabolic disease: a critical literature review with clinical recommendations', 'abstract': 'Background A high burden of severe disease and death from the coronavirus disease 2019 (COVID-19) has been consistently observed in older patients, especially those with pre-existing medical co-morbidities. The global pandemic lockdown has isolated many patients with chronic illnesses from their routine medical care. This narrative review article analyses the multitude of issues faced by individuals with underlying medical conditions during the COVID-19 pandemic. Methods Sources for this publication were identified through searches of PubMed for articles published between 31st December 2019 and 4th June 2020, using combinations of search terms. Guidelines and updates from reputable agencies were also consulted. Only articles published in the English language were included. Results The volume of literature on COVID-19 continues to expand, with 17,845 articles indexed on PubMed by 4th June 2020, 130 of which were deemed particularly relevant to the subject matter of this review. Older patients are more likely to progress to severe COVID-19 disease requiring intensive care unit (ICU) admission. Patients with pre-existing cardiovascular disease, especially hypertension and coronary heart disease, are at greatly increased risk of developing severe and fatal COVID-19 disease. A controversial aspect of the management of COVID-19 disease has been the use of angiotensin-converting enzyme inhibitors and angiotensin receptor blockers. Obese COVID-19 patients are more likely to require complex ICU management. Putative mechanisms of increased COVID-19 disease severity in diabetes include hyperglycaemia, altered immune function, sub-optimal glycaemic control during hospitalisation, a pro-thrombotic and pro-inflammatory state. Patients with mental health disorders are particularly vulnerable to social isolation, and this has been compounded by the suspension of non-emergency care in hospitals around the world, making it difficult for patients with chronic mental illness to attend outpatient appointments. Conclusions The global pandemic of COVID-19 disease has had a disproportionately negative impact on patients living with chronic medical illness. Future research should be directed at efforts to protect vulnerable patients from possible further waves of COVID-19 and minimising the negative impact of pandemic mitigation strategies on these individuals.', 'corpus_id': 221346558, 'score': 1}, {'doc_id': '213238982', 'title': 'Consistent analysis of f1(1285) meson form factors', 'abstract': 'Parameterization of the form factors of $f_1$ meson is proposed. This parameterization is consistent with the available experimental data on the cross sections of $f_1$ meson production in the processes $e^+e^-\\to f_1$ and $e^+e^-\\to e^+e^-f_1$, as well as on the widths of the decays $f_1\\to e^+e^-$, $f_1\\to \\rho^0\\gamma$, $f_1\\to \\rho^0\\pi^+\\pi^-$, and $f_1\\to 2\\pi^+2\\pi^-$. Our parameterization is also consistent with the predictions for the asymptotic behavior of these form factors.', 'corpus_id': 213238982, 'score': 0}, {'doc_id': '214616969', 'title': 'Factors Associated With Mental Health Outcomes Among Health Care Workers Exposed to Coronavirus Disease 2019', 'abstract': 'Key Points Question What factors are associated with mental health outcomes among health care workers in China who are treating patients with coronavirus disease 2019 (COVID-19)? Findings In this cross-sectional study of 1257 health care workers in 34 hospitals equipped with fever clinics or wards for patients with COVID-19 in multiple regions of China, a considerable proportion of health care workers reported experiencing symptoms of depression, anxiety, insomnia, and distress, especially women, nurses, those in Wuhan, and front-line health care workers directly engaged in diagnosing, treating, or providing nursing care to patients with suspected or confirmed COVID-19. Meaning These findings suggest that, among Chinese health care workers exposed to COVID-19, women, nurses, those in Wuhan, and front-line health care workers have a high risk of developing unfavorable mental health outcomes and may need psychological support or interventions.', 'corpus_id': 214616969, 'score': 0}]"
180	hRL in humans	63d3e191e010f570079eb4e0665a8411	16654	{}	[{'doc_id': '234097493', 'title': 'Reward prediction for representation learning and reward shaping', 'abstract': 'One of the fundamental challenges in reinforcement learning (RL) is the one of data efficiency: modern algorithms require a very large number of training samples, especially compared to humans, for solving environments with high-dimensional observations. The severity of this problem is increased when the reward signal is sparse. In this work, we propose learning a state representation in a self-supervised manner for reward prediction. The reward predictor learns to estimate either a raw or a smoothed version of the true reward signal in environment with a single, terminating, goal state. We augment the training of out-of-the-box RL agents by shaping the reward using our reward predictor during policy learning. Using our representation for preprocessing high-dimensional observations, as well as using the predictor for reward shaping, is shown to significantly enhance Actor Critic using Kronecker-factored Trust Region and Proximal Policy Optimization in single-goal environments with visual inputs.', 'corpus_id': 234097493, 'score': 0}, {'doc_id': '232368067', 'title': 'State-Temporal Compression in Reinforcement Learning with the Reward-Restricted Geodesic Metric.', 'abstract': 'It is difficult to solve complex tasks that involve large state spaces and long-term decision processes by reinforcement learning (RL) algorithms. A common and promising method to address this challenge is to compress a large RL problem into a small one. Towards this goal, the compression should be state-temporal and optimality-preserving (i.e., the optimal policy of the compressed problem should correspond to that of the uncompressed problem). In this paper, we propose a reward-restricted geodesic (RRG) metric, which can be learned by a neural network, to perform state-temporal compression in RL. We prove that compression based on the RRG metric is approximately optimality-preserving for the raw RL problem endowed with temporally abstract actions. With this compression, we design an RRG metric-based reinforcement learning (RRG-RL) algorithm to solve complex tasks. Experiments in both discrete (2D Minecraft) and continuous (Doom) environments demonstrated the superiority of our method over existing RL approaches.', 'corpus_id': 232368067, 'score': 0}, {'doc_id': '233313053', 'title': 'Learning Simulator: A simulation software for animal and human learning', 'abstract': 'Learning Simulator was developed to study learning in animals and humans. The current version implements associative learning (AL) and reinforcement learning (RL) algorithms, apt to study instrumental (operant) learning and Pavlovian (classical) learning (Bouton, 2016; Pearce, 2013), including in complex situations such as social learning or maze learning. A plugin system to add more learning mechanisms is planned for a future version.', 'corpus_id': 233313053, 'score': 0}, {'doc_id': '234242200', 'title': 'Efficient Reinforcement Learning for StarCraft by Abstract Forward Models and Transfer Learning', 'abstract': 'Injecting human knowledge is an effective way to accelerate reinforcement learning (RL). However, these methods are underexplored. This paper presents our discovery that an abstract forward model (Thought-game (TG)) combined with transfer learning is an effective way. We take StarCraft II as the study environment. With the help of a designed TG, the agent can learn a 99\\% win-rate on a 64$\\times$64 map against the Level-7 built-in AI, using only 1.08 hours in a single commercial machine. We also show that the TG method is not as restrictive as it was thought to be. It can work with roughly designed TGs, and can also be useful when the environment changes. Comparing with previous model-based RL, we show TG is more effective. We also present a TG hypothesis that gives the influence of fidelity levels of TG. For real games that have unequal state and action spaces, we proposed a novel XfrNet of which usefulness is validated while achieving a 90\\% win-rate against the cheating Level-10 AI. We argue the TG method might shed light on further studies of efficient RL with human knowledge.', 'corpus_id': 234242200, 'score': 0}, {'doc_id': '233172804', 'title': 'Atypical Reinforcement Learning in Developmental Dyslexia.', 'abstract': 'OBJECTIVES\nAccording to the Procedural Deficit Hypothesis, abnormalities in corticostriatal pathways could account for the language-related deficits observed in developmental dyslexia. The same neural network has also been implicated in the ability to learn contingencies based on trial and error (i.e., reinforcement learning [RL]). On this basis, the present study tested the assumption that dyslexic individuals would be impaired in RL compared with neurotypicals in two different tasks.\n\n\nMETHODS\nIn a probabilistic selection task, participants were required to learn reinforcement contingencies based on probabilistic feedback. In an implicit transitive inference task, participants were also required to base their decisions on reinforcement histories, but feedback was deterministic and stimulus pairs were partially overlapping, such that participants were required to learn hierarchical relations.\n\n\nRESULTS\nAcross tasks, results revealed that although the ability to learn from positive/negative feedback did not differ between the two groups, the learning of reinforcement contingencies was poorer in the dyslexia group compared with the neurotypicals group. Furthermore, in novel test pairs where previously learned information was presented in new combinations, dyslexic individuals performed similarly to neurotypicals.\n\n\nCONCLUSIONS\nTaken together, these results suggest that learning of reinforcement contingencies occurs less robustly in individuals with developmental dyslexia. Inferences for the neuro-cognitive mechanisms of developmental dyslexia are discussed.', 'corpus_id': 233172804, 'score': 0}, {'doc_id': '227154093', 'title': 'Computational evidence for hierarchically structured reinforcement learning in humans', 'abstract': 'Humans have the fascinating ability to achieve goals in a complex and constantly changing world, still surpassing modern machine-learning algorithms in terms of flexibility and learning speed. It is generally accepted that a crucial factor for this ability is the use of abstract, hierarchical representations, which employ structure in the environment to guide learning and decision making. Nevertheless, how we create and use these hierarchical representations is poorly understood. This study presents evidence that human behavior can be characterized as hierarchical reinforcement learning (RL). We designed an experiment to test specific predictions of hierarchical RL using a series of subtasks in the realm of context-based learning and observed several behavioral markers of hierarchical RL, such as asymmetric switch costs between changes in higher-level versus lower-level features, faster learning in higher-valued compared to lower-valued contexts, and preference for higher-valued compared to lower-valued contexts. We replicated these results across three independent samples. We simulated three models—a classic RL, a hierarchical RL, and a hierarchical Bayesian model—and compared their behavior to human results. While the flat RL model captured some aspects of participants’ sensitivity to outcome values, and the hierarchical Bayesian model captured some markers of transfer, only hierarchical RL accounted for all patterns observed in human behavior. This work shows that hierarchical RL, a biologically inspired and computationally simple algorithm, can capture human behavior in complex, hierarchical environments and opens the avenue for future research in this field.', 'corpus_id': 227154093, 'score': 1}, {'doc_id': '233175455', 'title': 'Neural mechanisms of distributed value representations and learning strategies', 'abstract': 'Learning appropriate representations of the reward environment is extremely challenging in the real world where there are many options to learn about and these options have many attributes or features. Despite existence of alternative solutions for this challenge, neural mechanisms underlying emergence and adoption of value representations and learning strategies remain unknown. To address this, we measured learning and choice during a novel multi-dimensional probabilistic learning task in humans and trained recurrent neural networks (RNNs) to capture our experimental observations. We found that participants estimate stimulus-outcome associations by learning and combining estimates of reward probabilities associated with the informative feature followed by those of informative conjunctions. Through analyzing representations, connectivity, and lesioning of the RNNs, we demonstrate this mixed learning strategy relies on a distributed neural code and distinct contributions of inhibitory and excitatory neurons. Together, our results reveal neural mechanisms underlying emergence of complex learning strategies in naturalistic settings.', 'corpus_id': 233175455, 'score': 1}, {'doc_id': '233415948', 'title': 'Rich and lazy learning of task representations in brains and neural networks', 'abstract': 'How do neural populations code for multiple, potentially conflicting tasks? Here, we used computational simulations involving neural networks to define “lazy” and “rich” coding solutions to this multitasking problem, which trade off learning speed for robustness. During lazy learning the input dimensionality is expanded by random projections to the network hidden layer, whereas in rich learning hidden units acquire structured representations that privilege relevant over irrelevant features. For context-dependent decision-making, one rich solution is to project task representations onto low-dimensional and orthogonal manifolds. Using behavioural testing and neuroimaging in humans, and analysis of neural signals from macaque prefrontal cortex, we report evidence for neural coding patterns in biological brains whose dimensionality and neural geometry are consistent with the rich learning regime.', 'corpus_id': 233415948, 'score': 1}, {'doc_id': '233025184', 'title': 'Thalamocortical contribution to solving credit assignment in neural systems', 'abstract': 'Animal brains evolved to optimize behavior in dynamically changing environments, selecting actions that maximize future rewards. A large body of experimental work indicates that such optimization changes the wiring of neural circuits, appropriately mapping environmental input onto behavioral outputs. A major unsolved scientific question is how optimal wiring adjustments, which must target the connections responsible for rewards, can be accomplished when the relation between sensory inputs, action taken, environmental context with rewards is ambiguous. The computational problem of properly targeting cues, contexts and actions that lead to reward is known as structural, contextual and temporal credit assignment respectively. In this review, we survey prior approaches to these three types of problems and advance the notion that the brain’s specialized neural architectures provide efficient solutions. Within this framework, the thalamus with its cortical and basal ganglia interactions serve as a systems-level solution to credit assignment. Specifically, we propose that thalamocortical interaction is the locus of meta-learning where the thalamus provides cortical control functions that parametrize the cortical activity association space. By selecting among these control functions, the basal ganglia hierarchically guide thalamocortical plasticity across two timescales to enable meta-learning. The faster timescale establishes contextual associations to enable rapid behavioral flexibility while the slower one enables generalization to new contexts. Incorporating different thalamic control functions under this framework clarifies how thalamocortical-basal ganglia interactions may simultaneously solve the three credit assignment problems. Introduction Learning which action to choose in an uncertain environment is a hallmark of intelligence [1–3]. When animals explore unfamiliar environments, they tend to reinforce actions that lead to unexpected rewards. A common notion in contemporary neuroscience is that such behavioral reinforcement emerges from changes in synaptic connectivity, where synapses that contribute to the unexpected reward are strengthened [4–8]. A prominent model for connecting synaptic to behavioral reinforcement is dopaminergic innervation of Computer Science & Artificial Intelligence Laboratory, Massachusetts Institute of Technology, Cambridge, MA. Department of Brain and Cognitive Science, Massachusetts Institute of Technology, Cambridge MA. 1 ar X iv :2 10 4. 01 47 4v 1 [ qbi o. N C ] 3 A pr 2 02 1 basal ganglia (BG), where dopamine (DA) carries the reward prediction error (RPE) signals to guide synaptic learning [9–12]. This circuit motif is thought to implement a basic form of the reinforcement learning algorithm [13–19], which has had much success in explaining simple Pavlovian and instrumental conditioning [3,15,19,20]. However, what allows this circuit to reinforce the appropriate connections in complex natural environments where animals are presented with multiple cues in multiple contexts and make multiple actions before they receive the reward, is unknown. If one naively credits all synapses with the RPE signals, the learning will be highly inefficient since different cues, contexts and actions contribute to the RPE signals differently. To properly credit the cues, context and actions that lead to unexpected reward is a challenging problem, known as the credit assignment problem [8,21–23]. One can roughly categorize the credit assignment into structural credit assignment, contextual credit assignment and temporal credit assignment (Figure 1). In structural credit assignment, animals may make decisions in a multi-cue environment and should be able to credit those cues that contribute to the rewarding outcome. Similarly, if actions are being chosen based on internal decision variables, then the underlying activity states must also be reinforced. In such cases, neurons that are selective to external cues or internal latent variables need to adjust their downstream connectivity based on its contribution of their downstream targets to the RPE. This is a challenging computation to implement because, for upstream neurons, the RPE will be dependent on downstream neurons that are several connections away. For example, a sensory neuron needs to know the action chosen in the motor cortex to selectively credit the sensory synapses that contribute to the action. In contextual credit assignment, animals not only need to appropriately credit the sensory cues and actions that lead to the reward but also need to credit the sensorimotor combination in the right context. For example, when one is in the United States, one learns to first look left before crossing the street, whereas, in the United Kingdom, one learns to look right instead. However, after spending time in the UK, someone from the US should not unlearn the behavior of looking left first when they return home because their brain ought to properly assign the credit to a different context. In the temporal credit assignment problem, animals make decisions in an environment with distant rewards and need to figure out which past sensory cues and actions lead to the current reward. For example, in a game of Go, even though the result of the game is only revealed after hundreds of hands, professional players can recognize which moves in the past are good and reinforce such moves. In this review, we will first go over common approaches from machine learning to tackle these three credit assignment problems. In doing so, we highlight the challenge in their efficient implementation within biological neural circuits. We also highlight some recent proposals that advance the notion of specialized neural hardware that approximate more general solutions for credit assignment [24–37]. Along these lines, we propose an efficient systems-level solution involving the thalamus and its interaction with the cortex and BG for these three credit assignment problems. Common machine learning approaches to credit assignment One solution to structural credit assignment in machine learning is backpropagation [22]. Backpropagation recursively computes the vector-valued error signal for synapses based on their contribution', 'corpus_id': 233025184, 'score': 1}, {'doc_id': '233983416', 'title': 'Neural systems underlying the learning of cognitive effort costs.', 'abstract': 'People balance the benefits of cognitive work against the costs of cognitive effort. Models that incorporate prospective estimates of the costs of cognitive effort into decision making require a mechanism by which these costs are learned. However, it remains an open question what brain systems are important for this learning, particularly when learning is not tied explicitly to a decision about what task to perform. In this fMRI experiment, we parametrically manipulated the level of effort a task requires by increasing task switching frequency across six task contexts. In a scanned learning phase, participants implicitly learned about the task switching frequency in each context. In a subsequent test phase, participants made selections between pairs of these task contexts. We modeled learning within a reinforcement learning framework, and found that effort expectations that derived from task-switching probability and response time (RT) during learning were the best predictors of later choice behavior. Prediction errors (PE) from these two models were associated with FPN during distinct learning epochs. Specifically, PE derived from expected RT was most correlated with the fronto-parietal network early in learning, whereas PE derived from expected task switching frequency was correlated with the fronto-parietal network late in learning. These results suggest that multiple task-related factors are tracked by the brain while performing a task that can drive subsequent estimates of effort costs.', 'corpus_id': 233983416, 'score': 1}]
181	Predictability of science	46929c67ed5af61d90b119ed35f9364f	4659	{}	"[{'doc_id': '218665306', 'title': 'Success in creative careers depends little on product quality', 'abstract': 'In the recent article Janosov, Battiston, & Sinatra report that they separated the inputs of talent and luck in creative careers. They build on the previous work of Sinatra et al which introduced the Q-model. Under the model the popularity of different elements of culture is a product of two factors: a random factor and a Qfactor, or talent. The latter is fixed for an individual but randomly distributed among different people. This way they explain how some individuals can consistently produce high-impact work. They extract the Q-factors for different scientists, writers, and movie makers from statistical data on popularity of their work. However, in their article they reluctantly state that there is little correlation between popularity and quality ratings of of books and movies (correlation coefficients 0.022 and 0.15). I analyzed the data of the original Q-factor article and obtained a correlation between the citation-based Q-factor and Nobel Prize winning of merely 0.19. I also briefly review few other experiments that found a meager, sometimes even negative, correlation between popularity and quality of cultural products. I conclude that, if there is an ability associated with a high Q-factor it should be more of a marketing ability than an ability to produce a higher quality product. Janosov,', 'corpus_id': 218665306, 'score': 0}, {'doc_id': '218595974', 'title': 'Co-author Weighting in Bibliometric Methodology and Subfields of a Scientific Discipline', 'abstract': 'Abstract Purpose To give a theoretical framework to measure the relative impact of bibliometric methodology on the subfields of a scientific discipline, and how that impact depends on the method of evaluation used to credit individual scientists with citations and publications. The authors include a study of the discipline of physics to illustrate the method. Indicators are introduced to measure the proportion of a credit space awarded to a subfield or a set of authors. Design/methodology/approach The theoretical methodology introduces the notion of credit spaces for a discipline. These quantify the total citation or publication credit accumulated by the scientists in the discipline. One can then examine how the credit is divided among the subfields. The design of the physics study uses the American Physical Society print journals to assign subdiscipline classifications to articles and gather citation, publication, and author information. Credit spaces for the collection of Physical Review Journal articles are computed as a proxy for physics. Findings There is a substantial difference in the value or impact of a specific subfield depending on the credit system employed to credit individual authors. Research limitations Subfield classification information is difficult to obtain. In the illustrative physics study, subfields are treated in groups designated by the Physical Review journals. While this collection of articles represents a broad part of the physics literature, it is not all the literature nor a random sample. Practical implications The method of crediting individual scientists has consequences beyond the individual and affects the perceived impact of whole subfields and institutions. Originality/value The article reveals the consequences of bibliometric methodology on subfields of a disciple by introducing a systematic theoretical framework for measuring the consequences.', 'corpus_id': 218595974, 'score': 0}, {'doc_id': '24015860', 'title': 'Quantifying the evolution of individual scientific impact', 'abstract': ""Scientific impact—that is the Q Are there quantifiable patterns behind a successful scientific career? Sinatra et al. analyzed the publications of 2887 physicists, as well as data on scientists publishing in a variety of fields. When productivity (which is usually greatest early in the scientist's professional life) is accounted for, the paper with the greatest impact occurs randomly in a scientist's career. However, the process of generating a high-impact paper is not an entirely random one. The authors developed a quantitative model of impact, based on an element of randomness, productivity, and a factor Q that is particular to each scientist and remains constant during the scientist's career. Science, this issue p. 596 Productivity, ability, and luck are incorporated into an algorithm describing a scientist’s impact. INTRODUCTION In most areas of human performance, from sport to engineering, the path to a major accomplishment requires a steep learning curve and long practice. Science is not that different: Outstanding discoveries are often preceded by publications of less memorable impact. However, despite the increasing desire to identify early promising scientists, the temporal career patterns that characterize the emergence of scientific excellence remain unknown. RATIONALE How do impact and productivity change over a scientific career? Does impact, arguably the most relevant performance measure, follow predictable patterns? Can we predict the timing of a scientist’s outstanding achievement? Can we model, in quantitative and predictive terms, scientific careers? Driven by these questions, here we quantify the evolution of impact and productivity throughout thousands of scientific careers. We do so by reconstructing the publication record of scientists from seven disciplines, associating to each paper its long-term impact on the scientific community, as quantified by citation metrics. RESULTS We find that the highest-impact work in a scientist’s career is randomly distributed within her body of work. That is, the highest-impact work can be, with the same probability, anywhere in the sequence of papers published by a scientist—it could be the first publication, could appear mid-career, or could be a scientist’s last publication. This random-impact rule holds for scientists in different disciplines, with different career lengths, working in different decades, and publishing solo or with teams and whether credit is assigned uniformly or unevenly among collaborators. The random-impact rule allows us to develop a quantitative model, which systematically untangles the role of productivity and luck in each scientific career. The model assumes that each scientist selects a project with a random potential p and improves on it with a factor Qi, resulting in a publication of impact Qip. The parameter Qi captures the ability of scientist i to take advantage of the available knowledge in a way that enhances (Qi > 1) or diminishes (Qi < 1) the potential impact p of a paper. The model predicts that truly high-impact discoveries require a combination of high Q and luck (p) and that increased productivity alone cannot substantially enhance the chance of a very high impact work. We also show that a scientist’s Q, capturing her sustained ability to publish high-impact papers, is independent of her career stage. This is in contrast with all current metrics of excellence, from the total number of citations to the h-index, which increase with time. The Q model provides an analytical expression of these traditional impact metrics and allows us to predict their future time evolution for each individual scientist, being also predictive of independent recognitions, like Nobel prizes. CONCLUSION The random-impact rule and the Q parameter, representing two fundamental characteristics of a scientific career, offer a rigorous quantitative framework to explore the evolution of individual careers and understand the emergence of scientific excellence. Such understanding could help us better gauge scientific performance and offers a path toward nurturing high-impact scientists, potentially informing future policy decisions. Random-impact rule. The publication history of two Nobel laureates, Frank A. Wilczek (Nobel Prize in Physics, 2004) and John B. Fenn (Nobel Prize in Chemistry, 2002), illustrating that the highest-impact work can be, with the same probability, anywhere in the sequence of papers published by a scientist. Each vertical line corresponds to a research paper. The height of each line corresponds to paper impact, quantified with the number of citations the paper received after 10 years. Wilczek won the Nobel Prize for the very first paper he published, whereas Fenn published his Nobel-awarded work late in his career, after he was forcefully retired by Yale. [Image of Frank A. Wilczek is reprinted with permission of STS/Society for Science & the Public. Image of John B. Fenn is available for public domain use on Wikipedia.org.] Despite the frequent use of numerous quantitative indicators to gauge the professional impact of a scientist, little is known about how scientific impact emerges and evolves in time. Here, we quantify the changes in impact and productivity throughout a career in science, finding that impact, as measured by influential publications, is distributed randomly within a scientist’s sequence of publications. This random-impact rule allows us to formulate a stochastic model that uncouples the effects of productivity, individual ability, and luck and unveils the existence of universal patterns governing the emergence of scientific success. The model assigns a unique individual parameter Q to each scientist, which is stable during a career, and it accurately predicts the evolution of a scientist’s impact, from the h-index to cumulative citations, and independent recognitions, such as prizes."", 'corpus_id': 24015860, 'score': 1}, {'doc_id': '15192102', 'title': 'Predicting scientific success based on coauthorship networks', 'abstract': 'We address the question to what extent the success of scientific articles is due to social influence. Analyzing a data set of over 100,000 publications from the field of Computer Science, we study how centrality in the coauthorship network differs between authors who have highly cited papers and those who do not. We further show that a Machine Learning classifier, based only on coauthorship network centrality metrics measured at the time of publication, is able to predict with high precision whether an article will be highly cited five years after publication. By this we provide quantitative insight into the social dimension of scientific publishing – challenging the perception of citations as an objective, socially unbiased measure of scientific success.', 'corpus_id': 15192102, 'score': 1}, {'doc_id': '1167579', 'title': 'Can Scientific Impact Be Predicted?', 'abstract': ""A widely used measure of scientific impact is citations. However, due to their heavy-tailed distribution, citations are fundamentally difficult to predict. Instead, to characterize scientific impact, we address two analogous questions asked by many scientific researchers: “How will my h-index evolve over time, and which of my previously or newly published papers will contribute to it?” To answer these questions, we perform two related tasks. First, we develop a model to predict authors' future h-indices based on their current scientific impact. Second, we examine the factors that drive papers-either previously or newly published-to increase their authors' predicted future h-indices. By leveraging relevant factors, we can predict an author's h-index in five years with an R2 value of 0.92 and whether a previously (newly) published paper will contribute to this future h-index with an F1 score of 0.99 (0.77). We find that topical authority and publication venue are crucial to these effective predictions, while topic popularity is surprisingly inconsequential. Further, we develop an online tool that allows users to generate informed h-index predictions. Our work demonstrates the predictability of scientific impact, and can help researchers to effectively leverage their scholarly position of “standing on the shoulders of giants”."", 'corpus_id': 1167579, 'score': 1}, {'doc_id': '214605836', 'title': 'Statistical Indicators of the Scientific Publications Importance: A Stochastic Model and Critical Look', 'abstract': 'A model of scientific citation distribution is given. We apply it to understand the role of the Hirsch index as an indicator of scientific publication importance in Mathematics and some related fields. The proposed model is based on a generalization of such well-known distributions as geometric and Sibuja laws included now in a family of distributions. Real data analysis of the Hirsch index and corresponding citation numbers is given.', 'corpus_id': 214605836, 'score': 0}, {'doc_id': '158691267', 'title': 'Future impact: Predicting scientific success', 'abstract': 'Daniel E. Acuna, Stefano Allesina and Konrad P. Kording present a formula to estimate the future h-index of life scientists.', 'corpus_id': 158691267, 'score': 1}, {'doc_id': '3403165', 'title': 'ERV1 Overexpression in Myeloid Cells Protects against High Fat Diet Induced Obesity and Glucose Intolerance', 'abstract': 'Non-resolving inflammation is a central pathologic component of obesity, insulin resistance, type 2 diabetes and associated morbidities. The resultant hyperglycemia is deleterious to the normal function of many organs and its control significantly improves survival and quality of life for patients with diabetes. Macrophages play critical roles in both onset and progression of obesity-associated insulin resistance. Here we show that systemic activation of inflammation resolution prevents from morbid obesity and hyperglycemia under dietary overload conditions. In gain-of-function studies using mice overexpressing the human resolvin E1 receptor (ERV1) in myeloid cells, monocyte phenotypic shifts to increased patrolling-to-inflammatory ratio controlled inflammation, reduced body weight gain and protected from hyperglycemia on high-fat diet. Administration of a natural ERV1 agonist, resolvin E1, recapitulated the pro-resolving actions gained by ERV1 overexpression. This protective metabolic impact is in part explained by systemic activation of resolution programs leading to increased synthesis of specialized pro-resolving mediators.', 'corpus_id': 3403165, 'score': 0}, {'doc_id': '211258765', 'title': 'Citations Systematically Misrepresent the Quality and Impact of Research Articles: Survey and Experimental Evidence from Thousands of Citers', 'abstract': ""Citations are ubiquitous in evaluating research, but how exactly they relate to what they are thought to measure (quality and intellectual impact) is unclear. We investigate the relationships between citations, quality, and impact using a survey with an embedded experiment in which 12,670 authors in 15 academic fields describe about 25K specific referencing decisions. Results suggest that citation counts, when equated with quality and impact, are biased in opposite directions. First, experimentally exposing papers' actual citation counts during the survey causes respondents to perceive all but the top 10% cited papers as of lower quality. Because perceptions of quality are a key factor in citing decisions, citation counts are likely to endogenously cause more citing of top papers and equating them with quality overestimates the actual quality of those papers. Conversely, 54% of references had either zero or minor influence on authors who cite them, but references to highly cited papers were about 200% more likely to denote substantial impact. Equating citations with impact thus underestimates the impact of highly cited papers. Real citation practices thus reveal that citations are biased measures of quality and impact."", 'corpus_id': 211258765, 'score': 0}, {'doc_id': '90623078', 'title': 'Bile Acid Induced Morphotype Switch Mediates Intestinal Colonization in Vancomycin Resistant Enterococcus', 'abstract': 'Vancomycin resistant Enterococcus (VRE) is a highly antibiotic-resistant and readily transmissible pathogen that causes severe infections in hospitalized patients. We discovered that lithocholic acid (LCA), a secondary bile acid prevalent in the cecum and colon of mice and humans, impaired separation of growing VRE diplococci causing the formation of long chains, increasing biofilm formation and enhancing VRE’s ability to colonize the host intestine. Divalent cations reversed the switch to chaining and biofilm formation. Experimental evolution in the presence of LCA yielded mutations in yycG/walK and liaR that locked VRE in diplococcal mode, impaired biofilm formation and increased susceptibility to Daptomycin. These strains were deficient in host colonization specifically by affecting VRE’s ability to compete with intestinal microbiota. This morphotype switch presents a non-bactericidal therapeutic target that may help to clear VRE from the intestines of dominated patients, as occurs frequently during hematopoietic stem cell transplantation.', 'corpus_id': 90623078, 'score': 0}]"
182	CoVid-19 	c08fe989169a544117070fb716e72cb2	1089	{}	[{'doc_id': '212740851', 'title': 'Clinical outcome of 55 asymptomatic cases at the time of hospital admission infected with SARS-Coronavirus-2 in Shenzhen, China.', 'abstract': 'An epidemic caused by SARS-Coronavirus-2 infection has spread unexpectedly in Wuhan, Hubei Province, China since December 2019. It is rarely reported about asymptomatic cases screened from close contacts. We study epidemiological and clinical outcome of 55 asymptomatic carriers who were laboratory-confirmed positive for the SARS-Coronavirus-2 by testing the nucleic acid of the pharyngeal swab samples. The evidence showed that asymptomatic carriers occurred more often in middle aged people who had close contact with infected family members. The majority of the cases developed to be mild and ordinary COVID-19 during hospital.', 'corpus_id': 212740851, 'score': 1}, {'doc_id': '212730137', 'title': 'Equine Coronavirus-Associated Colitis in Horses: A Retrospective Study', 'abstract': '\n Abstract\n \n Equine coronavirus (ECoV) is a known cause of fever, anorexia, and lethargy in adult horses. Although there are multiple reports of ECoV outbreaks, less is known about the clinical presentation of individual horses during a nonoutbreak situation. The purpose of this study was to describe the clinical presentation of horses diagnosed with ECoV infection that were not associated with an outbreak. Medical records of all horses admitted to Washington State University, Veterinary Teaching Hospital, during an 8-year period were reviewed (2010–2018). The five horses included in this study were older than 1 year of age, were diagnosed with colitis, tested positive for ECoV using real-time polymerase chain reaction, and were negative to other enteric pathogens. Interestingly, 4 of 5 horses had moderate to severe diarrhea,\xa03 had abnormal large colon ultrasonography, 2 had transient ventricular tachycardia and 2 had clinicopathologic evidence of liver dysfunction. ECoV should be included as a differential diagnosis for individual horses presenting with anorexia, fever, lethargy, and colitis. Early identification of ECoV cases is key to implement appropriate biosecurity measures to prevent the potential spread of this disease.\n \n', 'corpus_id': 212730137, 'score': 0}, {'doc_id': '212743313', 'title': 'Epidemiology, causes, clinical manifestation and diagnosis, prevention and control of coronavirus disease (COVID-19) during the early outbreak period: a scoping review', 'abstract': 'Background The coronavirus disease (COVID-19) has been identified as the cause of an outbreak of respiratory illness in Wuhan, Hubei Province, China beginning in December 2019. As of 31 January 2020, this epidemic had spread to 19 countries with 11\u2009791 confirmed cases, including 213 deaths. The World Health Organization has declared it a Public Health Emergency of International Concern. Methods A scoping review was conducted following the methodological framework suggested by Arksey and O’Malley. In this scoping review, 65 research articles published before 31 January 2020 were analyzed and discussed to better understand the epidemiology, causes, clinical diagnosis, prevention and control of this virus. The research domains, dates of publication, journal language, authors’ affiliations, and methodological characteristics were included in the analysis. All the findings and statements in this review regarding the outbreak are based on published information as listed in the references. Results Most of the publications were written using the English language (89.2%). The largest proportion of published articles were related to causes (38.5%) and a majority (67.7%) were published by Chinese scholars. Research articles initially focused on causes, but over time there was an increase of the articles related to prevention and control. Studies thus far have shown that the virus’ origination is in connection to a seafood market in Wuhan, but specific animal associations have not been confirmed. Reported symptoms include fever, cough, fatigue, pneumonia, headache, diarrhea, hemoptysis, and dyspnea. Preventive measures such as masks, hand hygiene practices, avoidance of public contact, case detection, contact tracing, and quarantines have been discussed as ways to reduce transmission. To date, no specific antiviral treatment has proven effective; hence, infected people primarily rely on symptomatic treatment and supportive care. Conclusions There has been a rapid surge in research in response to the outbreak of COVID-19. During this early period, published research primarily explored the epidemiology, causes, clinical manifestation and diagnosis, as well as prevention and control of the novel coronavirus. Although these studies are relevant to control the current public emergency, more high-quality research is needed to provide valid and reliable ways to manage this kind of public health emergency in both the short- and long-term.', 'corpus_id': 212743313, 'score': 1}, {'doc_id': '211230955', 'title': 'Incubation Period and Other Epidemiological Characteristics of 2019 Novel Coronavirus Infections with Right Truncation: A Statistical Analysis of Publicly Available Case Data', 'abstract': 'The geographic spread of 2019 novel coronavirus (COVID-19) infections from the epicenter of Wuhan, China, has provided an opportunity to study the natural history of the recently emerged virus. Using publicly available event-date data from the ongoing epidemic, the present study investigated the incubation period and other time intervals that govern the epidemiological dynamics of COVID-19 infections. Our results show that the incubation period falls within the range of 2–14 days with 95% confidence and has a mean of around 5 days when approximated using the best-fit lognormal distribution. The mean time from illness onset to hospital admission (for treatment and/or isolation) was estimated at 3–4 days without truncation and at 5–9 days when right truncated. Based on the 95th percentile estimate of the incubation period, we recommend that the length of quarantine should be at least 14 days. The median time delay of 13 days from illness onset to death (17 days with right truncation) should be considered when estimating the COVID-19 case fatality risk.', 'corpus_id': 211230955, 'score': 0}, {'doc_id': '213191214', 'title': '[Chemotherapy strategy for colorectal cancer under the outbreak of corona virus disease 2019].', 'abstract': 'The outbreak of corona virus disease 2019 (COVID-19) makes the medical treatment of colorectal cancers difficult. Cancer patients are more susceptible to infection and tumor history is defined as an important factor of poor prognosis, which challenges both doctors and patients. For metastatic colorectal cancer (CRC) patients, maintenance therapy is the optimal choice. The patients with tumor progression or poor biological behavior should receive or continue combination chemotherapy. Adjuvant chemotherapy should reduce the intensity of treatment and shorten the therapy time. Fever patients during chemotherapy need to receive differential diagnosis and screening according to national standards. Patients with stable diseases and good general conditions may delay imaging examination. Clinicians should make individual clinical decisions based on the specifics of each patient during epidemic situation.', 'corpus_id': 213191214, 'score': 0}, {'doc_id': '212739116', 'title': 'Clinical Features of 69 Cases with Coronavirus Disease 2019 in Wuhan, China', 'abstract': 'Abstract Background From December 2019 to February 2020, 2019 severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) has caused a serious outbreak of coronavirus disease 2019 (COVID-19) in Wuhan, China. Related clinical features are needed. Methods We reviewed 69 patients who were hospitalized in Union hospital in Wuhan between January 16 to January 29, 2020. All patients were confirmed to be infected with SARS-CoV-2 and the final date of follow-up was February 4, 2020. Results The median age of 69 enrolled patients was 42.0 years (IQR 35.0-62.0), and 32 patients (46%) were men. The most common symptoms were fever (60[87%]), cough (38[55%]), and fatigue (29[42%]). Most patients received antiviral therapy (66 [98.5%] of 67 patients) and antibiotic therapy (66 [98.5%] of 67 patients). As of February 4, 2020, 18 (26.9%) of 67 patients had been discharged, and five patients had died, with a mortality rate of 7.5%. According to the lowest SpO2 during admission, cases were divided into the SpO2≥90% group (n=55) and the SpO2<90% group (n=14). All 5 deaths occurred in the SpO2<90% group. Compared with SpO2≥90% group, patients of the SpO2<90% group were older, and showed more comorbidities and higher plasma levels of IL6, IL10, lactate dehydrogenase, and c reactive protein. Arbidol treatment showed tendency to improve the discharging rate and decrease the mortality rate. Conclusions COVID-19 appears to show frequent fever, dry cough, and increase of inflammatory cytokines, and induced a mortality rate of 7.5%. Older patients or those with underlying comorbidities are at higher risk of death.', 'corpus_id': 212739116, 'score': 1}, {'doc_id': '214590831', 'title': '[Several suggestions of operation for colorectal cancer under the outbreak of corona virus disease 2019 in China].', 'abstract': 'Pneumonia caused by 2019-nCoV infection has been reported in Wuhan since December 2019, and spread rapidly across the country. The radical operation of colorectal cancer is semi-elective operation. Patients with colorectal cancer should receive operation as soon as possible after elective operation is resumed in each hospital. 2019-nCoV virus can be transmitted by asymptomatic infectors, and it has been confirmed to be transmitted by droplets and contact. However, fecal-oral transmission and aerosol transmission have not been excluded. Based on our experience with laparoscopic colorectal operation, we propose some surgery strategies for colorectal cancer patients under the corona virus disease 2019(COVID-19) situation: the screening process should be strictly carried out before surgery to reduce the risk of nosocomial infection in the later stage; laparoscopic-assisted surgery is recommended for radical surgery for patients with colorectal cancer; strict aerosol management must be made during the operation; natural orifice specimen extraction surgery and transanal total mesorectal excision are should be performed prudently; scientific and reasonable prophylactic stoma should be done; personnel protection in surgical ward and operation room must be strengthened.', 'corpus_id': 214590831, 'score': 0}, {'doc_id': '211476589', 'title': 'Understanding of COVID‐19 based on current evidence', 'abstract': 'Since December 2019, a series of unexplained pneumonia cases have been reported in Wuhan, China. On 12 January 2020, the World Health Organization (WHO) temporarily named this new virus as the 2019 novel coronavirus (2019‐nCoV). On 11 February 2020, the WHO officially named the disease caused by the 2019‐nCoV as coronavirus disease (COVID‐19). The COVID‐19 epidemic is spreading all over the world, especially in China. Based on the published evidence, we systematically discuss the characteristics of COVID‐19 in the hope of providing a reference for future studies and help for the prevention and control of the COVID‐19 epidemic.', 'corpus_id': 211476589, 'score': 0}, {'doc_id': '211112058', 'title': 'Risk for Transportation of Coronavirus Disease from Wuhan to Other Cities in China', 'abstract': 'On January 23, 2020, China quarantined Wuhan to contain coronavirus disease (COVID-19). We estimated the probability of transportation of COVID-19 from Wuhan to 369 other cities in China before the quarantine. Expected COVID-19 risk is >50% in 130 (95% CI 89–190) cities and >99% in the 4 largest metropolitan areas.', 'corpus_id': 211112058, 'score': 0}, {'doc_id': '213193072', 'title': '[Treatment strategies for colorectal cancer patients in tumor hospitals under the background of corona virus disease 2019].', 'abstract': 'In December 2019, a new outbreak of corona virus disease 2019 began to occur. Its pathogen is 2019-nCoV, which has the characteristics of strong infectivity and general susceptibility. The current situation of prevention and control of new coronavirus pneumonia is severe. In this context, as front-line medical workers bearing important responsibilities and pressure, while through strict management strategy, we can minimize the risk of infection exposure. By summarizing the research progress and guidelines in recent years in the fields of colorectal cancer disease screening, treatment strategies (including early colorectal cancer, locally advanced colorectal cancer, obstructive colorectal cancer, metastatic colorectal cancer and the treatment of patients after neoadjuvant therapy), the choice of medication and time limit for adjuvant therapy, the protective measures for patients undergoing emergency surgery, the re-examination of postoperative patients and the protection of medical staff, etc., authors improve treatment strategies in order to provide more choices for patients to obtain the best treatment under the severe epidemic situation of new coronavirus pneumonia. Meanwhile we hope that it can also provide more timely treatment modeling schemes for colleagues.', 'corpus_id': 213193072, 'score': 0}]
183	Respi Rhythm - Evolution	4fd8e60ecb06acfaea4ea18070886fcf	19197	{}	"[{'doc_id': '235690151', 'title': 'Conservation of locomotion-induced oculomotor activity through evolution in higher tetrapods', 'abstract': 'Efference copies are neural replicas of motor outputs used to anticipate the sensory consequences of a self-generated motor action or to coordinate neural networks involved in distinct motor behaviors1. An established example of this motor-to-motor coupling is the efference copy of the propulsive motor command that supplements classical visuo-vestibular reflexes to ensure gaze stabilization during amphibian larval locomotion2. Such feedforward replica from spinal pattern-generating circuits produces a spino-extraocular motor coupled activity that evokes eye movements, spatio-temporally coordinated to tail undulation independently of any sensory signal3,4. Exploiting the evolutionary-development characteristic of the frog1, studies in metamorphing Xenopus demonstrated the persistence of this spino-extraocular motor command in adults, and its developmental adaptation to tetrapodal locomotion5,6. Here, we demonstrate for the first time the existence of a comparable locomotor-to-ocular motor coupling in the mouse. In neonates, ex vivo nerve recordings from brainstem-spinal cord preparation reveals a spino-extraocular motor coupled activity similar to the one described in Xenopus. In adult mice, trans-synaptic rabies injection in lateral rectus eye muscle labels cervical spinal cord neurons projecting directly to abducens motor neurons. Finally, treadmill-elicited locomotion in decerebrated preparations7 evokes rhythmic eye movements in synchrony with the limb gait pattern. Overall, our data are evidence for the conservation of locomotor-induced eye movements in higher tetrapods. Thus, in mammals as in amphibians, during locomotion CPG-efference copy feedforward signals might interact with sensory feedback to ensure efficient gaze control. Highlights Spino-extraocular motor coupling is evidenced from newborn mice ex vivo preparations Adult decerebrated mice exhibit conjugated rhythmic eye movements during treadmill locomotion Locomotor-induced oculomotor activity occurs in absence of visuo-vestibular inputs Conserved CPG-based efference copy signal in vertebrates with common features. eTOC blurb We report a functional coupling between spinal locomotor and oculomotor networks in the mouse, similar to the one previously described in Amphibians. This is the first evidence for the direct contribution of locomotor networks to gaze control in mammals, suggesting a conservation of the spino-extraocular coupling in higher tetrapods during sustained locomotion.', 'corpus_id': 235690151, 'score': 0}, {'doc_id': '5171968', 'title': 'Respiratory rhythm generation: triple oscillator hypothesis', 'abstract': 'Breathing is vital for survival but also interesting from the perspective of rhythm generation. This rhythmic behavior is generated within the brainstem and is thought to emerge through the interaction between independent oscillatory neuronal networks. In mammals, breathing is composed of three phases – inspiration, post-inspiration, and active expiration – and this article discusses the concept that each phase is generated by anatomically distinct rhythm-generating networks: the preBötzinger complex (preBötC), the post-inspiratory complex (PiCo), and the lateral parafacial nucleus (pF L), respectively. The preBötC was first discovered 25 years ago and was shown to be both necessary and sufficient for the generation of inspiration. More recently, networks have been described that are responsible for post-inspiration and active expiration. Here, we attempt to collate the current knowledge and hypotheses regarding how respiratory rhythms are generated, the role that inhibition plays, and the interactions between the medullary networks. Our considerations may have implications for rhythm generation in general.', 'corpus_id': 5171968, 'score': 1}, {'doc_id': '39622908', 'title': 'Phylogenetic trends in respiratory rhythmogenesis: Insights from ectothermic vertebrates', 'abstract': 'Understanding the neural substrate driving breathing has puzzled physiologists for more than a century. The discovery of the pre-Bötzinger complex (preBötC) in newborn rodents as a structure with a unique physiological function in respiratory rhythm generation was an important progress in respiratory neurobiology that stimulated much research. Owing to the extensive literature describing the location, organisation, and function of the preBötC mainly in newborn rodents, this structure has become the point of reference in studies addressing respiratory rhythm generation in other mammals and various classes of vertebrates. This paper reviews recent progress made in non-mammalian vertebrates in our understanding of the location and function of the neural networks driving respiratory activity. As in newborn rodents, data from lampreys, air breathing fish, and amphibians show that the production of eupnea is the result of interactions between multiple (at least two) rhythmogenic networks. These networks are located in anatomically distinct areas and show different functional properties in terms of their ability to produce (or not) bursting activity in the absence of synaptic inputs (e.g. pacemaker neurons) and their sensitivity to specific neuromodulators such as substance P, somatostatin, and opioids. Current data indicate that respiratory rhythmogenesis is a phylogenetically ancient function that was highly conserved throughout evolution and that a comparative approach remains important to derive broader biological principles and a more comprehensive view.', 'corpus_id': 39622908, 'score': 1}, {'doc_id': '3267690', 'title': 'Phenotypic specification of hindbrain rhombomeres and the origins of rhythmic circuits in vertebrates.', 'abstract': 'This essay considers the ontogeny and phylogeny of the cranial neural circuitry producing rhythmic behaviors in vertebrates. These behaviors are characterized by predictable temporal patterns established by a neuronal network variously referred to as either a pacemaker, neural oscillator or central pattern generator. Comparative vertebrate studies have demonstrated that the embryonic hindbrain is divided into segmented compartments called rhombomeres, each of which gives rise to a distinct complement of cranial motoneurons and, as yet, unidentified populations of interneurons. We now propose that novel rhythmic circuits were innovations associated with the adoption of cardiac and respiratory pumps during the protochordate-vertebrate transition. We further suggest that the pattern-generating circuits of more recent innovations, such as the vocal, electromotor and extraocular systems, have originated from the same Hox gene-specified compartments of the embryonic hindbrain (rhombomeres 7-8) that gave rise to rhythmically active cardiac and respiratory circuits. Lastly, we propose that the capability for pattern generation by neurons originating from rhombomeres 7 and 8 is due to their electroresponsive properties producing pacemaker oscillations, as best typified by the inferior olive which also has origins from these same hindbrain compartments and has been suggested to establish rhythmic oscillations coupled to sensorimotor function throughout the neuraxis of vertebrates.', 'corpus_id': 3267690, 'score': 1}, {'doc_id': '52003073', 'title': 'Defining the Rhythmogenic Elements of Mammalian Breathing.', 'abstract': ""Breathing's remarkable ability to adapt to changes in metabolic, environmental, and behavioral demands stems from a complex integration of its rhythm-generating network within the wider nervous system. Yet, this integration complicates identification of its specific rhythmogenic elements. Based on principles learned from smaller rhythmic networks of invertebrates, we define criteria that identify rhythmogenic elements of the mammalian breathing network and discuss how they interact to produce robust, dynamic breathing."", 'corpus_id': 52003073, 'score': 1}, {'doc_id': '3519925', 'title': 'Microcircuits in respiratory rhythm generation: commonalities with other rhythm generating networks and evolutionary perspectives', 'abstract': 'Rhythmicity is critical for the generation of rhythmic behaviors and higher brain functions. This review discusses common mechanisms of rhythm generation, including the role of synaptic inhibition and excitation, with a focus on the mammalian respiratory network. This network generates three phases of breathing and is highly integrated with brain regions associated with numerous non-ventilatory behaviors. We hypothesize that during evolution multiple rhythmogenic microcircuits were recruited to accommodate the generation of each breathing phase. While these microcircuits relied primarily on excitatory mechanisms, synaptic inhibition became increasingly important to coordinate the different microcircuits and to integrate breathing into a rich behavioral repertoire that links breathing to sensory processing, arousal, and emotions as well as learning and memory.', 'corpus_id': 3519925, 'score': 1}, {'doc_id': '236165231', 'title': 'Astrocytic contribution to glutamate-related central respiratory chemoreception in vertebrates', 'abstract': 'Central respiratory chemoreceptors play a key role in the respiratory homeostasis by sensing CO2 and H+ in brain and activating the respiratory neural network. This ability of specific brain regions to respond to acidosis and hypercapnia is based on neuronal and glial mechanisms. Several decades ago, glutamatergic transmission was proposed to be involved as a main mechanism in central chemoreception. However, a complete identification of mechanism has been elusive. At the rostral medulla, chemosensitive neurons of the retrotrapezoid nucleus (RTN) are glutamatergic and they are stimulated by ATP released by RTN astrocytes in response to hypercapnia. In addition, recent findings show that caudal medullary astrocytes in brainstem can also contribute as CO2 and H+ sensors that release D-serine and glutamate, both gliotransmitters able to activate the respiratory neural network. In this review, we describe the mammalian astrocytic glutamatergic contribution to the central respiratory chemoreception trying to trace in vertebrates the emergence of several components involved in this process.', 'corpus_id': 236165231, 'score': 0}, {'doc_id': '236209099', 'title': 'Modeling Post-Scratching Locomotion with Two Rhythm Generators and a Shared Pattern Formation', 'abstract': 'Simple Summary Post-scratching locomotion in cats refers to the spontaneous occurrence of an episode of locomotion generated after an event of scratching. This phenomenon suggests the potential existence of shared neurons in the spinal cord mediating the transition from one rhythmic motor task to another. Here, we examine this possibility with a mathematical model, reproducing the experimental observations. Our findings reveal a possible mechanism in which the central nervous system could share neuronal circuits from two central pattern generators to produce a sequence of different rhythmic motor actions. Abstract This study aimed to present a model of post-scratching locomotion with two intermixed central pattern generator (CPG) networks, one for scratching and another for locomotion. We hypothesized that the rhythm generator layers for each CPG are different, with the condition that both CPGs share their supraspinal circuits and their motor outputs at the level of their pattern formation networks. We show that the model reproduces the post-scratching locomotion latency of 6.2 ± 3.5 s, and the mean cycle durations for scratching and post-scratching locomotion of 0.3 ± 0.09 s and 1.7 ± 0.6 s, respectively, which were observed in a previous experimental study. Our findings show how the transition of two rhythmic movements could be mediated by information exchanged between their CPG circuits through routes converging in a common pattern formation layer. This integrated organization may provide flexible and effective connectivity despite the rigidity of the anatomical connections in the spinal cord circuitry.', 'corpus_id': 236209099, 'score': 0}, {'doc_id': '235378252', 'title': 'Comodulation of h- and Na+/K+ Pump Currents Expands the Range of Functional Bursting in a Central Pattern Generator by Navigating between Dysfunctional Regimes', 'abstract': ""Central pattern generators (CPGs), specialized oscillatory neuronal networks controlling rhythmic motor behaviors such as breathing and locomotion, must adjust their patterns of activity to a variable environment and changing behavioral goals. Neuromodulation adjusts these patterns by orchestrating changes in multiple ionic currents. In the medicinal leech, the endogenous neuromodulator myomodulin speeds up the heartbeat CPG by reducing the electrogenic Na+/K+ pump current and increasing h-current in pairs of mutually inhibitory leech heart interneurons (HNs), which form half-center oscillators (HN HCOs). Here we investigate whether the comodulation of two currents could have advantages over a single current in the control of functional bursting patterns of a CPG. We use a conductance-based biophysical model of an HN HCO to explain the experimental effects of myomodulin. We demonstrate that, in the model, comodulation of the Na+/K+ pump current and h-current expands the range of functional bursting activity by avoiding transitions into nonfunctional regimes, such as asymmetric bursting and plateau-containing seizure-like activity. We validate the model by finding parameters that reproduce temporal bursting characteristics matching experimental recordings from HN HCOs under control, three different myomodulin concentrations, and Cs+ treated conditions. The matching cases are located along the border of an asymmetric regime away from the border with more dangerous seizure-like activity. We found a simple comodulation mechanism with an inverse relation between the pump and h-currents makes a good fit of the matching cases and comprises a general mechanism for the robust and flexible control of oscillatory neuronal networks. SIGNIFICANCE STATEMENT Rhythm-generating neuronal circuits adjust their oscillatory patterns to accommodate a changing environment through neuromodulation. In different species, chemical messengers participating in such processes may target two or more membrane currents. In medicinal leeches, the neuromodulator myomodulin speeds up the heartbeat central pattern generator by reducing Na+/K+ pump current and increasing h-current. In a computational model, we show that this comodulation expands the range of central pattern generator's functional activity by navigating the circuit between dysfunctional regimes resulting in a much wider range of cycle period. This control would not be attainable by modulating only one current, emphasizing the synergy of combined effects. Given the prevalence of h-current and Na+/K+ pump current in neurons, similar comodulation mechanisms may exist across species."", 'corpus_id': 235378252, 'score': 0}, {'doc_id': '235708536', 'title': 'Computational Modeling of Spinal Locomotor Circuitry in the Age of Molecular Genetics', 'abstract': 'Neuronal circuits in the spinal cord are essential for the control of locomotion. They integrate supraspinal commands and afferent feedback signals to produce coordinated rhythmic muscle activations necessary for stable locomotion. For several decades, computational modeling has complemented experimental studies by providing a mechanistic rationale for experimental observations and by deriving experimentally testable predictions. This symbiotic relationship between experimental and computational approaches has resulted in numerous fundamental insights. With recent advances in molecular and genetic methods, it has become possible to manipulate specific constituent elements of the spinal circuitry and relate them to locomotor behavior. This has led to computational modeling studies investigating mechanisms at the level of genetically defined neuronal populations and their interactions. We review literature on the spinal locomotor circuitry from a computational perspective. By reviewing examples leading up to and in the age of molecular genetics, we demonstrate the importance of computational modeling and its interactions with experiments. Moving forward, neuromechanical models with neuronal circuitry modeled at the level of genetically defined neuronal populations will be required to further unravel the mechanisms by which neuronal interactions lead to locomotor behavior.', 'corpus_id': 235708536, 'score': 0}]"
184	contact inhibition (actin) 	20bc68ba205f953a9c5f5d6c828e06f7	2212	{}	[{'doc_id': '211004037', 'title': 'Leader-cell-driven epithelial sheet fingering.', 'abstract': 'Collective cell migration is crucial in many biological processes such as wound healing, tissue morphogenesis, and tumor progression. The leading front of a collective migrating epithelial cell layer often destabilizes into multicellular finger-like protrusions, each of which is guided by a leader cell at the fingertip. Here, we develop a subcellular-element-based model of this fingering instability, which incorporates leader cells and other related properties of a monolayer of epithelial cells. Our model recovers multiple aspects of the dynamics, especially the traction force patterns and velocity fields, observed in experiments on MDCK cells. Our model predicts the necessity of the leader cell and its minimal functions for the formation and maintenance of a stable finger pattern. Meanwhile, our model allows for an analysis of the role of supra-cellular actin cable on the leading front, predicting that while this observed structure helps maintain the shape of the finger, it is not required in order to form a finger. In addition, we also study the driving instability in the context of continuum active fluid model, which justifies some of our assumptions in the computational approach. In particular, we show that in our model no finger protrusions would emerge in a phenotypically homogenous active fluid and hence the role of the leader cell and its followers are often critical.', 'corpus_id': 211004037, 'score': 0}, {'doc_id': '210991189', 'title': 'Icariin promotes osteogenic differentiation of BMSCs by upregulating BMAL1 expression via BMP signaling', 'abstract': 'Increasing research has demonstrated that expression of brain and muscle ARNT-like 1 (BMAL1) and other circadian clock genes can be regulated by drugs and toxicants. We previously demonstrated that icariin, extracted from Herba Epimedii, sromotes osteogenic differentiation. However, the mechanism underlying the association between icariin and BMAL1 in osteogenic differentiation of bone marrow-derived mesenchymal stem cells (BMSCs) remains unclear. The present study was designed with an aim to clarify the association between icariin and BMAL1 in osteogenic differentiation of BMSCs. The Cell Counting Kit-8 assay was used to evaluate cell proliferation. The expression of bone morphogenetic protein 2 (BMP2), RUNX family transcription factor 2 (RUNX2), alkaline phosphatase (ALP), osteocalcin (OC) and BMAL1 in BMSCs was evaluated by reverse transcription-quantitative PCR and western blotting. ALP and Alizarin red S (ARS) staining were also performed. Icariin promoted BMSC proliferation, and upregulated expression of osteogenic genes and BMAL1. In addition, expression of the osteogenic genes BMP2, RUNX2, ALP and OC were upregulated by BMAL1 overexpression. Furthermore, we confirmed that BMAL1 deficiency suppressed osteogenic differentiation in BMSCs. Finally, ARS staining of BMAL1−/− BMSCs revealed that BMAL1 was an essential intermediary in matrix mineralization during osteogenic differentiation. In conclusion, these results demonstrated that icariin promoted osteogenic differentiation through BMAL1-BMP2 signaling in BMSCs. The present study thus described a novel target of icariin that has potential applications in the treatment of osteogenic disorders.', 'corpus_id': 210991189, 'score': 0}, {'doc_id': '211068886', 'title': 'The architecture of co-culture spheroids regulates tumor invasion within a 3D extracellular matrix.', 'abstract': 'Tumor invasion, the process by which tumor cells break away from their primary tumor and gain access to vascular systems, is an important step in cancer metastasis. Most current 3D tumor invasion assays consisted of single tumor cells embedded within an extracellular matrix (ECM). These assays taught us much of what we know today on how key biophysical (e.g. ECM stiffness) and biochemical (e.g. cytokine gradients) parameters within the tumor microenvironment guided and regulated tumor invasion. One limitation of the single tumor cell invasion assay was that it did not account for cell-cell adhesion within the tumor. In this article, we developed a micrometer scale 3D co-culture spheroid invasion assay that was compatible with microscopic imaging. Micrometer scale co-culture spheroids (1:1 ratio of metastatic breast cancer MDA-MB-231 and non-tumorigenic epithelial MCF-10A cells) were made using an array of microwells, and then were embedded within a collagen matrix in a microfluidic platform. Real time imaging of tumor spheroid invasion revealed that the spatial distribution of the two cell types within the tumor spheroid critically regulated tumor invasion. This work linked tumor architecture with tumor invasion and highlighted the importance of the biophysical cues within the bulk of the tumor in tumor invasion.', 'corpus_id': 211068886, 'score': 0}, {'doc_id': '3720107', 'title': 'Formin-mediated actin polymerization at cell–cell junctions stabilizes E-cadherin and maintains monolayer integrity during wound repair', 'abstract': 'Cadherin-mediated cell–cell adhesion is required for epithelial tissue integrity in homeostasis, during development, and in tissue repair. Fmnl3 and mDia1 cooperate in stabilizing E-cadherin at cell–cell junctions and facilitate strong cell adhesion and monolayer cohesion during collective cell migration.', 'corpus_id': 3720107, 'score': 1}, {'doc_id': '211086643', 'title': 'Regulation of Nucleotide Metabolism and Germline Proliferation in Response to Nucleotide Imbalance and Genotoxic Stresses by EndoU Nuclease', 'abstract': 'SUMMARY Nucleotide deprivation and imbalance present detrimental conditions for animals and are thus expected to trigger cellular responses that direct protective changes in metabolic, developmental, and behavioral programs, albeit such mechanisms are vastly underexplored. Following our previous finding that Caenorhabditis elegans shut down germ cell proliferation in response to pyrimidine deprivation, we find in this study that endonuclease ENDU-2 regulates nucleotide metabolism and germ cell proliferation in response to nucleotide imbalance and other genotoxic stress, and that it affects mitotic chromosomal segregation in the intestine and lifespan. ENDU-2 expression is induced by nucleotide imbalance and genotoxic stress, and ENDU-2 exerts its function in the intestine, mostly by inhibiting the phosphorylation of CTPS-1 through repressing the PKA pathway and histone deacetylase HDA-1. Human EndoU also affects the response to genotoxic drugs. Our work reveals an unknown role of ENDU-2 in regulating nucleotide metabolism and animals’ response to genotoxic stress, which may link EndoU function to cancer treatment.', 'corpus_id': 211086643, 'score': 0}, {'doc_id': '210936011', 'title': 'Life and death agendas of actin filaments', 'abstract': 'Cancer cells have now been shown to lack rigidity-sensing due to alteration in cytoskeletal sensor proteins, but can be reversed from a transformed to a rigidity-dependent growth state by the sensor proteins, resulting in restoration of contractility and adhesion.', 'corpus_id': 210936011, 'score': 1}, {'doc_id': '8028470', 'title': 'Homeostatic Actin Cytoskeleton Networks Are Regulated by Assembly Factor Competition for Monomers', 'abstract': 'Controlling the quantity and size of organelles through competition for a limited supply of components is quickly emerging as an important cellular regulatory mechanism. Cells assemble diverse actin filament (F-actin) networks for fundamental processes including division, motility, and polarization. F-actin polymerization is tightly regulated by activation of assembly factors such as the Arp2/3 complex and formins at specific times and places. We directly tested an additional hypothesis that diverse F-actin networks are in homeostasis, whereby competition for actin monomers (G-actin) is critical for regulating F-actin network size. Here we show that inhibition of Arp2/3 complex in the fission yeast Schizosaccharomyces pombe not only depletes Arp2/3-complex-mediated endocytic actin patches, but also induces a dramatic excess of formin-assembled F-actin. Conversely, disruption of formin increases the density of Arp2/3-complex-mediated patches. Furthermore, modification of actin levels significantly perturbs the fission yeast actin cytoskeleton. Increasing actin favors Arp2/3-complex-mediated actin assembly, whereas decreasing actin favors formin-mediated contractile rings. Therefore, the specific actin concentration in a cell is critical, and competition for G-actin helps regulate the proper amount of F-actin assembly for diverse processes.', 'corpus_id': 8028470, 'score': 1}, {'doc_id': '1970068', 'title': 'N-Cadherin adhesive interactions modulate matrix mechanosensing and fate commitment of mesenchymal stem cells', 'abstract': 'During mesenchymal development, the microenvironment gradually transitions from one that is rich in cell-cell interactions to one that is dominated by cell-extracellular-matrix (ECM) interactions. Because these cues cannot readily be decoupled in vitro or in vivo, how they converge to regulate mesenchymal stem cell (MSC) mechanosensing is not fully understood. Here, we show that a hyaluronic acid hydrogel system enables, across a physiological range of ECM stiffness, the independent co-presentation of the HAVDI adhesive motif from the EC1 domain of N-Cadherin and the RGD adhesive motif from fibronectin. Decoupled presentation of these cues revealed that HAVDI ligation (at constant RGD ligation) reduced the contractile state and thereby nuclear YAP/TAZ localization in MSCs, resulting in altered interpretation of ECM stiffness and subsequent changes in downstream cell proliferation and differentiation. Our findings reveal that, in an evolving developmental context, HAVDI/N-Cadherin interactions can alter stem cell perception of the stiffening extracellular microenvironment.', 'corpus_id': 1970068, 'score': 1}, {'doc_id': '211531426', 'title': 'Protein Trafficking in the Biosynthetic Pathway', 'abstract': 'In the absence of specific mechanisms to recognize and sequester cargo destined for transport and retrieval, communicating organelles would quickly lose their identity. The biosynthetic pathway, comprising of the endoplasmic reticulum, the Golgi complex, vesicular-tubular clusters (also known as ERGIC or pre-Golgi intermediates) and vesicular/tubular transport carriers has been a frequent target for the examination of such transport processes. In the biosynthetic pathway, proteins are directed to the endoplasmic reticulum by an N-terminal signal peptide. Within the lumen of the ER, proteins may be proteolytically processed, folded, glycosylated and assembled into their tertiary or quaternary structures. Improperly folded proteins are passed by retrograde transport through the ER translocon to be degraded by a proteasome. Correctly folded proteins are packaged into vesicles or tubules at ER exit sites and transported to the Golgi apparatus where further maturation occurs. Though many questions remain regarding mechanisms of protein retention, sorting, packaging, and recycling in the biosynthetic pathway, new discoveries through complementary methods have assisted in the clarification of a number of the steps involved. The Endoplasmic Reticulum Structure and Function The endoplasmic reticulum (ER), originally discovered by Porter and coworkers in 1945 (Palade 1975), is the first organelle in the biosynthetic pathway (Figure 1). The ER is a labyrinthine array of membrane-bounded tubules and cisternae which extend throughout the cell. The position and organization of the ER has been demonstrated to depend, at least in part, on microtubules (Lee et al., 1989, Lane and Allan 1998, Marsh et al., 2001). The functions of the ER include: protein synthesis, folding, assembly, and degradation; lipid biosynthesis and metabolism; detoxification; nucleus compartmentalization; ion gradient retention; and membrane transport (Rooney and Meldolesi 1996, Lippincott-Schwartz et al., 2000). ER membranes form a continuous interconnected system of rough (RER) and smooth (SER) regions, depending on whether ribosomes are associated with their cytoplasmic surfaces (Cole et al., 1996a, Dayel et al., 1999). RER is the site of cotranslational or posttranslational protein insertion, while SER is proposed to be the site of lipid biosynthesis and detoxification (Amar-Costesec et al., 1984, Lippincott-Schwartz et al., 2000). The RER represents the entry point for newly synthesized proteins destined for the biosynthetic and secretory pathways. The process of how the appropriate proteins are targeted to the ER, and thus, the true starting point of research on protein trafficking, is • MALDI-TOF Mass Spectrometry in Microbiology Edited by: M Kostrzewa, S Schubert (2016) www.caister.com/malditof • Aspergillus and Penicillium in the Post-genomic Era Edited by: RP Vries, IB Gelber, MR Andersen (2016) www.caister.com/aspergillus2 • The Bacteriocins: Current Knowledge and Future Prospects Edited by: RL Dorit, SM Roy, MA Riley (2016) www.caister.com/bacteriocins • Omics in Plant Disease Resistance Edited by: V Bhadauria (2016) www.caister.com/opdr • Acidophiles: Life in Extremely Acidic Environments Edited by: R Quatrini, DB Johnson (2016) www.caister.com/acidophiles • Climate Change and Microbial Ecology: Current Research and Future Trends Edited by: J Marxsen (2016) www.caister.com/climate • Biofilms in Bioremediation: Current Research and Emerging Technologies Edited by: G Lear (2016) www.caister.com/biorem • Microalgae: Current Research and Applications Edited by: MN Tsaloglou (2016) www.caister.com/microalgae • Gas Plasma Sterilization in Microbiology: Theory, Applications, Pitfalls and New Perspectives Edited by: H Shintani, A Sakudo (2016) www.caister.com/gasplasma • Virus Evolution: Current Research and Future Directions Edited by: SC Weaver, M Denison, M Roossinck, et al. (2016) www.caister.com/virusevol • Arboviruses: Molecular Biology, Evolution and Control Edited by: N Vasilakis, DJ Gubler (2016) www.caister.com/arbo • Shigella: Molecular and Cellular Biology Edited by: WD Picking, WL Picking (2016) www.caister.com/shigella • Aquatic Biofilms: Ecology, Water Quality and Wastewater Treatment Edited by: AM Romaní, H Guasch, MD Balaguer (2016) www.caister.com/aquaticbiofilms • Alphaviruses: Current Biology Edited by: S Mahalingam, L Herrero, B Herring (2016) www.caister.com/alpha • Thermophilic Microorganisms Edited by: F Li (2015) www.caister.com/thermophile • Flow Cytometry in Microbiology: Technology and Applications Edited by: MG Wilkinson (2015) www.caister.com/flow • Probiotics and Prebiotics: Current Research and Future Trends Edited by: K Venema, AP Carmo (2015) www.caister.com/probiotics • Epigenetics: Current Research and Emerging Trends Edited by: BP Chadwick (2015) www.caister.com/epigenetics2015 • Corynebacterium glutamicum: From Systems Biology to Biotechnological Applications Edited by: A Burkovski (2015) www.caister.com/cory2 • Advanced Vaccine Research Methods for the Decade of Vaccines Edited by: F Bagnoli, R Rappuoli (2015) www.caister.com/vaccines • Antifungals: From Genomics to Resistance and the Development of Novel Agents Edited by: AT Coste, P Vandeputte (2015) www.caister.com/antifungals • Bacteria-Plant Interactions: Advanced Research and Future Trends Edited by: J Murillo, BA Vinatzer, RW Jackson, et al. (2015) www.caister.com/bacteria-plant • Aeromonas Edited by: J Graf (2015) www.caister.com/aeromonas • Antibiotics: Current Innovations and Future Trends Edited by: S Sánchez, AL Demain (2015) www.caister.com/antibiotics • Leishmania: Current Biology and Control Edited by: S Adak, R Datta (2015) www.caister.com/leish2 • Acanthamoeba: Biology and Pathogenesis (2nd edition) Author: NA Khan (2015) www.caister.com/acanthamoeba2 • Microarrays: Current Technology, Innovations and Applications Edited by: Z He (2014) www.caister.com/microarrays2 • Metagenomics of the Microbial Nitrogen Cycle: Theory, Methods and Applications Edited by: D Marco (2014) www.caister.com/n2 Caister Academic Press is a leading academic publisher of advanced texts in microbiology, molecular biology and medical research. Full details of all our publications at caister.com Further Reading Order from caister.com/order explained by the signal hypothesis. This hypothesis postulates that an amino-terminal leader peptide serves as a signal that directs the protein to the ER and then is cleaved off by a signal peptidase in the ER membrane before the polypeptide chain is fully synthesized (Blobel and Dobberstein 1975, Walter and Johnson 1994). In mammalian cells, greater detail has been elucidated. The ER signal peptide on the nascent polypeptide chain is bound by the signal-recognition particle (SRP), which cycles between the ER membrane and the cytosol. The binding of the SRP to the signal peptide and ribosome induces a pause in translation and permits the binding of the SRP-ribosome complex to the SRP receptor, which is located adjacent to the translocon. The translocon is a complex molecular machine that regulates the movement of polypeptides in both directions through the ER bilayer while maintaining the membrane permeability barrier. Once the SRP-ribosome complex is bound to the SRP receptor, the nascent chain is inserted into the aqueous translocon pore and a GTP-dependent interaction of SRP with its receptor triggers the release of SRP from the ribosome and the SRP receptor from the translocon (Bacher et al., 1996). The nascent chain is prevented by Binding Protein (BiP) from passing through the luminal end of the pore until the chain reaches ~ 70 amino acids in length (Crowley et al., 1994). The delayed release of BiP and the final opening of the pore may constitute a safety mechanism to ensure that one end of the pore is not opened before the other end is completely sealed. The release of the BiP may be elicited after the unfolded nascent chain becomes long enough to bind to BiP, alter its conformation, and release BiP from the translocon. Cotranslational protein translocation then proceeds through the aqueous pore, which is now sealed at its cytoplasmic end by tight binding of the ribosome to the translocon. When the chain length totals ~150 residues, the signal peptidase cleaves off the leader peptide. After termination of translation, the ribosome is released and the pore contracts, sealed on its luminal side by BiP (Hamman et al., 1998, Johnson and van Waes 1999). For soluble proteins, cleavage of the signal peptide releases the polypeptide into the ER lumen. Other proteins remain anchored to the phospholipid bilayer by a covalently attatched glycosylphosphatidylinositol (GPI) membrane anchor. These proteins are initially anchored to the ER membrane by an internal stop-transfer membrane anchor sequence. However, a short sequence of amino acids in the exoplasmic domain adjacent to the membrane-spanning domain is recognized by an endoprotease that simultaneously cleaves off the stop-transfer membrane-anchor sequence and transfers the remainder of the protein to a pre-formed GPI anchor in the membrane (Kodukula et al., 1992). In contrast, single or multiple-pass integral membrane proteins remain in the ER membrane after cleavage by the signal peptidase due to the presence of uncleaved stop-transfer membrane-anchor sequences or internal signal anchor sequences. Because membrane proteins are always inserted from the cytosolic side of the ER in a programmed manner, all copies of the same polypeptide chain will have the same orientation in the lipid bilayer. This generates an asymmetrical ER membrane in which the protein domains exposed on one side differ from those exposed on the other. This asymmetry is maintained as proteins made in the ER are transported to other cell membranes (Lodish et al., 2000). Quality Control Once in the ER, proteins are covalently modified (N-glycosylation, oligosaccharide trimming, formation of disulfide bonds) and acqu', 'corpus_id': 211531426, 'score': 0}, {'doc_id': '16562039', 'title': 'Mechanical memory and dosing influence stem cell fate', 'abstract': 'We investigated whether stem cells remember past physical signals and whether these can be exploited to dose cells mechanically. We found that the activation of the Yes-associated protein (YAP) and transcriptional coactivator with PDZ-binding domain (TAZ) as well as the pre-osteogenic transcription factor RUNX2 in human mesenchymal stem cells (hMSCs) cultured on soft poly(ethylene glycol) (PEG) hydrogels (Young’s modulus E ~ 2 kPa) depended on prior culture time on stiff tissue culture polystyrene (TCPS; E ~ 3 GPa). Additionally, mechanical dosing of hMSCs cultured on initially stiff (E ~ 10 kPa) and then soft (E ~ 2 kPa) phototunable PEG hydrogels resulted in either reversible - or above a threshold mechanical dose, irreversible - activation of YAP/TAZ and RUNX2. We also found that increased mechanical dosing on supraphysiologically stiff TCPS biases hMSCs toward osteogenic differentiation. We conclude that stem cells possess mechanical memory - with YAP/TAZ acting as an intracellular mechanical rheostat - that stores information from past physical environments and influences the cells’ fate.', 'corpus_id': 16562039, 'score': 1}]
185	Convergent Evolution	5c2a934a8fc3df0cc0e083443f72deab	16303	{}	[{'doc_id': '233328739', 'title': 'Convergent selection on juvenile hormone signaling is associated with the evolution of eusociality in bees', 'abstract': 'Life’s most dramatic innovations, from the emergence of self-replicating molecules to highly-integrated societies, often involve increases in biological complexity. Some groups traverse different levels of complexity, providing a framework to identify key factors shaping these evolutionary transitions. Halictid bees span the transition from individual to group reproduction, with repeated gains and losses of eusociality. We generated chromosome-length genome assemblies for 17 species and searched for genes that both experienced positive selection when eusociality arose and relaxed selection when eusociality was secondarily lost. Loci exhibiting these complementary evolutionary signatures are predicted to carry costs outweighed by their importance for traits in eusocial lineages. Strikingly, these loci included two proteins that bind and transport juvenile hormone (JH) – a key regulator of insect development and reproduction. Though changes in JH abundance are frequently associated with polymorphisms, the mechanisms coupling JH to novel phenotypes are not well understood. Our results suggest novel links between JH and eusociality arose in halictids by altering transport and availability of JH in a tissue-specific manner, including in the brain. Through genomic comparisons of species encompassing both the emergence and breakdown of eusociality, we provide insights into the mechanisms targeted by selection to shape a key evolutionary transition.', 'corpus_id': 233328739, 'score': 0}, {'doc_id': '232224270', 'title': 'Complementary evolution of coding and noncoding sequence underlies mammalian hairlessness', 'abstract': 'Body hair is a defining mammalian characteristic, but several mammals, such as whales, naked mole-rats, and humans, have notably less hair than others. To find the genetic basis of reduced hair quantity, we used our evolutionary-rates-based method, RERconverge, to identify coding and noncoding sequences that evolve at significantly different rates in so-called hairless mammals compared to hairy mammals. Using RERconverge, we performed an unbiased, genome-wide scan over 62 mammal species using 19,149 genes and 343,598 conserved noncoding regions to find genetic elements that evolve at significantly different rates in hairless mammals compared to hairy mammals. We show that these rate shifts resulted from relaxation of evolutionary constraint on hair-related sequences in hairless species. In addition to detecting known and potential novel hair-related genes, we also discovered hundreds of putative hair-related regulatory elements. Computational investigation revealed that genes and their associated noncoding regions show different evolutionary patterns and influence different aspects of hair growth and development. Many genes under accelerated evolution are associated with the structure of the hair shaft itself, while evolutionary rate shifts in noncoding regions also included the dermal papilla and matrix regions of the hair follicle that contribute to hair growth and cycling. Genes that were top-ranked for coding sequence acceleration included known hair and skin genes KRT2, KRT35, PKP1, and PTPRM that surprisingly showed no signals of evolutionary rate shifts in nearby noncoding regions. Conversely, accelerated noncoding regions are most strongly enriched near regulatory hair-related genes and microRNAs, such as mir205, ELF3, and FOXC1, that themselves do not show rate shifts in their protein-coding sequences. Such dichotomy highlights the interplay between the evolution of protein sequence and regulatory sequence to contribute to the emergence of a convergent phenotype.', 'corpus_id': 232224270, 'score': 1}, {'doc_id': '232313442', 'title': 'ActiveDriverDB: Interpreting Genetic Variation in Human and Cancer Genomes Using Post-translational Modification Sites and Signaling Networks (2021 Update)', 'abstract': 'Deciphering the functional impact of genetic variation is required to understand phenotypic diversity and the molecular mechanisms of inherited disease and cancer. While millions of genetic variants are now mapped in genome sequencing projects, distinguishing functional variants remains a major challenge. Protein-coding variation can be interpreted using post-translational modification (PTM) sites that are core components of cellular signaling networks controlling molecular processes and pathways. ActiveDriverDB is an interactive proteo-genomics database that uses more than 260,000 experimentally detected PTM sites to predict the functional impact of genetic variation in disease, cancer and the human population. Using machine learning tools, we prioritize proteins and pathways with enriched PTM-specific amino acid substitutions that potentially rewire signaling networks via induced or disrupted short linear motifs of kinase binding. We then map these effects to site-specific protein interaction networks and drug targets. In the 2021 update, we increased the PTM datasets by nearly 50%, included glycosylation, sumoylation and succinylation as new types of PTMs, and updated the workflows to interpret inherited disease mutations. We added a recent phosphoproteomics dataset reflecting the cellular response to SARS-CoV-2 to predict the impact of human genetic variation on COVID-19 infection and disease course. Overall, we estimate that 16-21% of known amino acid substitutions affect PTM sites among pathogenic disease mutations, somatic mutations in cancer genomes and germline variants in the human population. These data underline the potential of interpreting genetic variation through the lens of PTMs and signaling networks. The open-source database is freely available at www.ActiveDriverDB.org.', 'corpus_id': 232313442, 'score': 0}, {'doc_id': '233246520', 'title': 'Intron Losses and Gains in Nematodes: Not Eccentric at All', 'abstract': 'The evolution of spliceosomal introns has been widely studied among various eukaryotic groups. Researchers nearly reached the consensuses on the pattern and the mechanisms of intron losses and gains across eukaryotes. However, according to previous studies that analyzed a few genes or genomes of nematodes, Nematoda seem to be an eccentric group. Taking advantage of the recent accumulation of sequenced genomes, we carried out an extensive analysis on the intron losses and gains using 104 nematodes genomes across all the five Clades of the phylum. Nematodes have a wide range of intron density, from less than one to more than nine per 1kbp coding sequence. The rates of intron losses and gains exhibit significant heterogeneity both across different nematode lineages and across different evolutionary stages of the same lineage. The frequency of intron losses far exceeds that of intron gains. Five pieces of evidence supporting the model of cDNA-mediated intron loss have been observed in ten Caenorhabditis species, the dominance of the precise intron losses, frequent loss of adjacent introns, and high-level expression of the intron-lost genes, preferential losses of short introns, and the preferential losses of introns close to 3′-ends of genes. Like studies in most eukaryotic groups, we cannot find the source sequences for the limited number of intron gains detected in the Caenorhabditis genomes. All the results indicate that nematodes are a typical eukaryotic group rather than an outlier in intron evolution.', 'corpus_id': 233246520, 'score': 0}, {'doc_id': '220309142', 'title': 'A fully-automated method discovers loss of mouse-lethal and human-monogenic disease genes in 58 mammals', 'abstract': 'Abstract Gene losses provide an insightful route for studying the morphological and physiological adaptations of species, but their discovery is challenging. Existing genome annotation tools focus on annotating intact genes and do not attempt to distinguish nonfunctional genes from genes missing annotation due to sequencing and assembly artifacts. Previous attempts to annotate gene losses have required significant manual curation, which hampers their scalability for the ever-increasing deluge of newly sequenced genomes. Using extreme sequence erosion (amino acid deletions and substitutions) and sister species support as an unambiguous signature of loss, we developed an automated approach for detecting high-confidence gene loss events across a species tree. Our approach relies solely on gene annotation in a single reference genome, raw assemblies for the remaining species to analyze, and the associated phylogenetic tree for all organisms involved. Using human as reference, we discovered over 400 unique human ortholog erosion events across 58 mammals. This includes dozens of clade-specific losses of genes that result in early mouse lethality or are associated with severe human congenital diseases. Our discoveries yield intriguing potential for translational medical genetics and evolutionary biology, and our approach is readily applicable to large-scale genome sequencing efforts across the tree of life.', 'corpus_id': 220309142, 'score': 1}, {'doc_id': '232200338', 'title': 'Novel regulators of growth identified in the evolution of fin proportion in flying fish', 'abstract': 'Identifying the genetic foundations of trait variation and evolution is challenging as it is often difficult to parse meaningful signals from confounding signatures such as drift and epistasis. However, identification of the genetic loci underlying morphological and physiological traits can be honed through the use of comparative and complementary genetic approaches, whereby shared sets of genes that are repeatedly implicated across large evolutionary time periods as under selection can illuminate important pathways and epistatic relationships that function as novel regulators of trait development. Here we intersect comparative genomic analyses with unbiased mutagenesis screens in distantly related species to define the control of proportional growth, as changes in the size and relative proportions of tissues underlie a large degree of the variant forms seen in nature. Through a phylogenomic analysis of genome-wide variation in 35 species of flying fishes and relatives, we identify genetic signatures in both coding and regulatory regions underlying the convergent evolution of increased paired fin size and aerial gliding behaviors, key innovations for flying fishes and flying halfbeaks. To refine our analysis, we intersected convergent phylogenomic signatures with mutants identified in distantly related zebrafish with altered fin size. Through these paired approaches, we identify a surprising role for an L-type amino acid transporter, lat4a, and the potassium channel, kcnh2a, in the regulation of fin proportion. We show that specific epistatic interaction between these genetic loci in zebrafish closely phenocopies the observed fin proportions of flying fishes. The congruence of experimental and phylogenomic findings point to a conserved, non-canonical signaling interaction that integrates bioelectric cues and amino acid transport in the establishment of relative size in development and evolution.', 'corpus_id': 232200338, 'score': 0}, {'doc_id': '233015559', 'title': 'Single individual structural variant detection uncovers widespread hemizygosity in molluscs', 'abstract': 'The advent of complete genomic sequencing has opened a window into genomic phenomena obscured by fragmented assemblies. A good example of these is the existence of hemizygous regions of autosomal chromosomes, which can result in marked differences in gene content between individuals within species. While these hemizygous regions, and presence/absence variation of genes that can result, are well known in plants, firm evidence has only recently emerged for their existence in metazoans. Here, we use recently published, complete genomes from wild-caught molluscs to investigate the prevalence of hemizygosity across a well-known and ecologically important clade. We show that hemizygous regions are widespread in mollusc genomes, not clustered in individual chromosomes, and often contain genes linked to transposition, DNA repair and stress response. With targeted investigations of HSP70-12 and C1qDC, we also show how individual gene families are distributed within pan-genomes. This work suggests that extensive pan-genomes are widespread across the conchiferan Mollusca, and represent useful tools for genomic evolution, allowing the maintenance of additional genetic diversity within the population. As genomic sequencing and re-sequencing becomes more routine, the prevalence of hemizygosity, and its impact on selection and adaptation, are key targets for research across the tree of life. This article is part of the Theo Murphy meeting issue ‘Molluscan genomics: broad insights and future directions for a neglected phylum’.', 'corpus_id': 233015559, 'score': 0}, {'doc_id': '203622006', 'title': 'A functional enrichment test for molecular convergent evolution finds a clear protein-coding signal in echolocating bats and whales', 'abstract': 'Significance Echolocation is a prime example of convergent evolution, the independent gain of similar features in species of different lineages. Is phenotypic convergence driven by underlying molecular convergence? If so, could molecular convergence include contributions from highly constrained, often-pleotropic, coding regions? We develop a generalizable test that offers a resounding “yes” to both extensively debated questions. Our test highlights molecular convergence in genes regulating the cochlear ganglion of echolocating bats and whales, the skin of aquatic mammals, and the lung of high-altitude mammals. Importantly, the approach correctly dismisses confounding convergence-like patterns, such as those from sequence decay of vision genes in blind subterranean species, and is readily applicable to the thousands of genomes sequenced across the tree of life. Distantly related species entering similar biological niches often adapt by evolving similar morphological and physiological characters. How much genomic molecular convergence (particularly of highly constrained coding sequence) contributes to convergent phenotypic evolution, such as echolocation in bats and whales, is a long-standing fundamental question. Like others, we find that convergent amino acid substitutions are not more abundant in echolocating mammals compared to their outgroups. However, we also ask a more informative question about the genomic distribution of convergent substitutions by devising a test to determine which, if any, of more than 4,000 tissue-affecting gene sets is most statistically enriched with convergent substitutions. We find that the gene set most overrepresented (q-value = 2.2e-3) with convergent substitutions in echolocators, affecting 18 genes, regulates development of the cochlear ganglion, a structure with empirically supported relevance to echolocation. Conversely, when comparing to nonecholocating outgroups, no significant gene set enrichment exists. For aquatic and high-altitude mammals, our analysis highlights 15 and 16 genes from the gene sets most affected by molecular convergence which regulate skin and lung physiology, respectively. Importantly, our test requires that the most convergence-enriched set cannot also be enriched for divergent substitutions, such as in the pattern produced by inactivated vision genes in subterranean mammals. Showing a clear role for adaptive protein-coding molecular convergence, we discover nearly 2,600 convergent positions, highlight 77 of them in 3 organs, and provide code to investigate other clades across the tree of life.', 'corpus_id': 203622006, 'score': 1}, {'doc_id': '96435050', 'title': 'Convergent regulatory evolution and loss of flight in paleognathous birds', 'abstract': 'All roads lead to regulation Species from widely divergent taxa can experience similar changes in traits. What underlying genetic drivers cause these parallel changes remains an open question. Sackton et al. looked across groups of birds that have repeatedly lost flight, the ratites and tinamous, and found that there is convergence in the regulatory regions associated with genes related to flight, but not within the protein coding regions. Changes within these regulatory regions influenced limb development and may represent quick paths toward convergent change across taxa. Science, this issue p. 74 Changes in regulatory regions led to the evolution of flightlessness in birds. A core question in evolutionary biology is whether convergent phenotypic evolution is driven by convergent molecular changes in proteins or regulatory regions. We combined phylogenomic, developmental, and epigenomic analysis of 11 new genomes of paleognathous birds, including an extinct moa, to show that convergent evolution of regulatory regions, more so than protein-coding genes, is prevalent among developmental pathways associated with independent losses of flight. A Bayesian analysis of 284,001 conserved noncoding elements, 60,665 of which are corroborated as enhancers by open chromatin states during development, identified 2355 independent accelerations along lineages of flightless paleognaths, with functional consequences for driving gene expression in the developing forelimb. Our results suggest that the genomic landscape associated with morphological convergence in ratites has a substantial shared regulatory component.', 'corpus_id': 96435050, 'score': 1}, {'doc_id': '232283165', 'title': 'Ecological correlates of gene family size: the draft genome of the redheaded', 'abstract': None, 'corpus_id': 232283165, 'score': 0}]
186	game optimization	19f43b90f56fad52be6871530859e33f	16953	{}	[{'doc_id': '172137612', 'title': 'OnabotulinumtoxinA injections: treatment of reversible cerebral vasoconstriction syndrome chronic daily headaches', 'abstract': 'Reversible cerebral vasoconstriction syndrome (RCVS) is a rare condition characterised by repetitive, multifocal, vasofluctuations of cerebral arteries. A key symptom is chronic, disabling ‘thunderclap’ headaches, which are extremely difficult to treat as established medications may exacerbate the pathophysiology of RCVS. OnabotulinumtoxinA (OBT-A) injections are used for the prophylaxis of chronic daily headaches (CDH). The mechanism of action of OBT-A significantly differs from oral headache treatments. Thus, OBT-A may be an effective, safe treatment of RCVS-CDH. A 51-year-old woman with RCVS-CDH presented to outpatient clinic. This case report describes the first, believed, documented treatment of RCVS-CDH by OBT-A injections. In 2018, the consented patient received a total of 200 units of OBT-A, 155 units to the 31 approved U.S. Food and Drug Administration (FDA) sites and 45 units injected into the bilateral occipital belly of occipitofrontalis muscles. The patient reported 3\u2009months of excellent pain relief (60% reduction). Three rounds of OBT-A injection, each 3 months apart, resulted in 80% reduction. OBT-A injections may prove a successful, novel treatment for RCVS-CDH.', 'corpus_id': 172137612, 'score': 0}, {'doc_id': '233394398', 'title': 'Solving a Class of Non-Convex Min-Max Games Using Adaptive Momentum Methods', 'abstract': 'Adaptive momentum methods have recently attracted a lot of attention for training of deep neural networks. They use an exponential moving average of past gradients of the objective function to update both search directions and learning rates. However, these methods are not suited for solving min-max optimization problems that arise in training generative adversarial networks. In this paper, we propose an adaptive momentum min-max algorithm that generalizes adaptive momentum methods to the non-convex min-max regime. Further, we establish non-asymptotic rates of convergence for it when used in a reasonably broad class of non-convex min-max optimization problems. Experimental results illustrate its superior performance vis-a-vis benchmark methods for solving such problems.', 'corpus_id': 233394398, 'score': 1}, {'doc_id': '232360541', 'title': 'Understanding Overparameterization in Generative Adversarial Networks', 'abstract': 'A broad class of unsupervised deep learning methods such as Generative Adversarial Networks (GANs) involve training of overparameterized models where the number of parameters of the model exceeds a certain threshold. Indeed, most successful GANs used in practice are trained using overparameterized generator and discriminator networks, both in terms of depth and width. A large body of work in supervised learning have shown the importance of model overparameterization in the convergence of the gradient descent (GD) to globally optimal solutions. In contrast, the unsupervised setting and GANs in particular involve non-convex concave mini-max optimization problems that are often trained using Gradient Descent/Ascent (GDA). The role and benefits of model overparameterization in the convergence of GDA to a global saddle point in non-convex concave problems is far less understood. In this work, we present a comprehensive analysis of the importance of model overparameterization in GANs both theoretically and empirically. We theoretically show that in an overparameterized GAN model with a 1-layer neural network generator and a linear discriminator, GDA converges to a global saddle point of the underlying non-convex concave min-max problem. To the best of our knowledge, this is the first result for global convergence of GDA in such settings. Our theory is based on a more general result that holds for a broader class of nonlinear generators and discriminators that obey certain assumptions (including deeper generators and random feature discriminators). Our theory utilizes and builds upon a novel connection with the convergence analysis of linear timevarying dynamical systems which may have broader implications for understanding the convergence behavior of GDA for non-convex concave problems involving overparameterized models. We also empirically study the role of model overparameterization in GANs using several large-scale experiments on CIFAR-10 and Celeb-A datasets. Our experiments show that overparameterization improves the quality of generated samples across various model architectures and datasets. Remarkably, we observe that overparameterization leads to faster and more stable convergence behavior of GDA across the board.', 'corpus_id': 232360541, 'score': 1}, {'doc_id': '232417206', 'title': 'The Complexity of Nonconvex-Strongly-Concave Minimax Optimization', 'abstract': 'This paper studies the complexity for finding approximate stationary points of nonconvex-stronglyconcave (NC-SC) smooth minimax problems, in both general and averaged smooth finite-sum settings. We establish nontrivial lower complexity bounds of Ω( √ κ∆Lε−2) and Ω(n+ √ nκ∆Lε−2) for the two settings, respectively, where κ is the condition number, L is the smoothness constant, and ∆ is the initial gap. Our result reveals substantial gaps between these limits and bestknown upper bounds in the literature. To close these gaps, we introduce a generic acceleration scheme that deploys existing gradient-based methods to solve a sequence of crafted strongly-convexstrongly-concave subproblems. In the general setting, the complexity of our proposed algorithm nearly matches the lower bound; in particular, it removes an additional poly-logarithmic dependence on accuracy present in previous works. In the averaged smooth finite-sum setting, our proposed algorithm improves over previous algorithms by providing a nearly-tight dependence on the condition number.', 'corpus_id': 232417206, 'score': 1}, {'doc_id': '232404790', 'title': 'A Unifying Framework of Accelerated First-Order Approach to Strongly Monotone Variational Inequalities', 'abstract': 'In this paper, we propose a unifying framework incorporating several momentum-related search directions for solving strongly monotone variational inequalities. The specific combinations of the search directions in the framework are made to guarantee the optimal iteration complexity bound of O (κ ln(1/ )) to reach an -solution, where κ is the condition number. This framework provides the flexibility for algorithm designers to train – among different parameter combinations – the one that best suits the structure of the problem class at hand. The proposed framework includes the following iterative points and directions as its constituents: the extra-gradient, the optimistic gradient descent ascent (OGDA) direction (aka “optimism”), the “heavy-ball” direction, and Nesterov’s extrapolation points. As a result, all the afore-mentioned methods become the special cases under the general scheme of extra points. We also specialize this approach to strongly convex minimization, and show that a similar extra-point approach achieves the optimal iteration complexity bound of O( √ κ ln(1/ )) for this class of problems.', 'corpus_id': 232404790, 'score': 1}, {'doc_id': '233296988', 'title': 'Random Reshuffling with Variance Reduction: New Analysis and Better Rates', 'abstract': 'Virtually all state-of-the-art methods for training supervised machine learning models are variants of SGD enhanced with a number of additional tricks, such as minibatching, momentum, and adaptive stepsizes. One of the tricks that works so well in practice that it is used as default in virtually all widely used machine learning software is random reshuffling (RR). However, the practical benefits of RR have until very recently been eluding attempts at being satisfactorily explained using theory. Motivated by recent development due to Mishchenko, Khaled and Richtárik (2020), in this work we provide the first analysis of SVRG under Random Reshuffling (RR-SVRG) for general finite-sum problems. First, we show that RR-SVRG converges linearly with the rate O(κ) in the strongly-convex case, and can be improved further to O(κ) in the big data regime (when n > O(κ)), where κ is the condition number. This improves upon the previous best rate O(κ) known for a variance reduced RR method in the strongly-convex case due to Ying, Yuan and Sayed (2020). Second, we obtain the first sublinear rate for general convex problems. Third, we establish similar fast rates for Cyclic-SVRG and Shuffle-Once-SVRG. Finally, we develop and analyze a more general variance reduction scheme for RR, which allows for less frequent updates of the control variate. We corroborate our theoretical results with suitably chosen experiments on synthetic and real datasets.', 'corpus_id': 233296988, 'score': 0}, {'doc_id': '233231406', 'title': 'Oracle Complexity in Nonsmooth Nonconvex Optimization', 'abstract': 'It is well-known that given a smooth, bounded-from-below, and possibly nonconvex function, standard gradient-based methods can find -stationary points (with gradient norm less than ) in O(1/ ) iterations. However, many important nonconvex optimization problems, such as those associated with training modern neural networks, are inherently not smooth, making these results inapplicable. In this paper, we study nonsmooth nonconvex optimization from an oracle complexity viewpoint, where the algorithm is assumed to be given access only to local information about the function at various points. We provide two main results (under mild assumptions): First, we consider the problem of getting near -stationary points. This is perhaps the most natural relaxation of finding -stationary points, which is impossible in the nonsmooth nonconvex case. We prove that this relaxed goal cannot be achieved efficiently, for any distance and smaller than some constants. Our second result deals with the possibility of tackling nonsmooth nonconvex optimization by reduction to smooth optimization: Namely, applying smooth optimization methods on a smooth approximation of the objective function. For this approach, we prove an inherent trade-off between oracle complexity and smoothness: On the one hand, smoothing a nonsmooth nonconvex function can be done very efficiently (e.g., by randomized smoothing), but with dimension-dependent factors in the smoothness parameter, which can strongly affect iteration complexity when plugging into standard smooth optimization methods. On the other hand, these dimension factors can be eliminated with suitable smoothing methods, but only by making the oracle complexity of the smoothing process exponentially large.', 'corpus_id': 233231406, 'score': 0}, {'doc_id': '233322798', 'title': 'A DAPTIVE NORMS FOR DEEP LEARNING WITH REGULARIZED N EWTON', 'abstract': 'We investigate the use of regularized Newton methods with adaptive norms for optimizing neural networks. This approach can be seen as a second-order counterpart of adaptive gradient methods, which we here show to be interpretable as first-order trust region methods with ellipsoidal constraints. In particular, we prove that the preconditioning matrix used in RMSProp and Adam satisfies the necessary conditions for provable convergence of second-order trust region methods with standard worst-case complexities on general non-convex objectives. Furthermore, we run experiments across different neural architectures and datasets to find that the ellipsoidal constraints constantly outperform their spherical counterpart both in terms of number of backpropagations and asymptotic loss value. Finally, we find comparable performance to state-of-the-art first-order methods in terms of backpropagations, but further advances in hardware are needed to render Newton methods competitive in terms of computational time.', 'corpus_id': 233322798, 'score': 0}, {'doc_id': '234742600', 'title': 'Optimal Convergence Rates for the Proximal Bundle Method', 'abstract': 'We study convergence rates of the classic proximal bundle method for a variety of nonsmooth convex optimization problems. We show that, without any modification, this algorithm adapts to converge faster in the presence of smoothness or a Hölder growth condition. Our analysis reveals that with a constant stepsize, the bundle method is adaptive, yet it exhibits suboptimal convergence rates. We overcome this shortcoming by proposing nonconstant stepsize schemes with optimal rates. These schemes use function information such as growth constants, which might be prohibitive in practice. We complete the paper with a new parallelizable variant of the bundle method that attains near-optimal rates without prior knowledge of function parameters. These results improve on the limited existing convergence rates and provide a unified analysis approach across problem settings and algorithmic details. Numerical experiments support our findings and illustrate the effectiveness of the parallel bundle method.', 'corpus_id': 234742600, 'score': 0}, {'doc_id': '232068945', 'title': 'Local Stochastic Gradient Descent Ascent: Convergence Analysis and Communication Efficiency', 'abstract': 'Local SGD is a promising approach to overcome the communication overhead in distributed learning by reducing the synchronization frequency among worker nodes. Despite the recent theoretical advances of local SGD in empirical risk minimization, the efficiency of its counterpart in minimax optimization remains unexplored. Motivated by large scale minimax learning problems, such as adversarial robust learning and training generative adversarial networks (GANs), we propose local Stochastic Gradient Descent Ascent (local SGDA), where the primal and dual variables can be trained locally and averaged periodically to significantly reduce the number of communications. We show that local SGDA can provably optimize distributed minimax problems in both homogeneous and heterogeneous data with reduced number of communications and establish convergence rates under strongly-convex-strongly-concave and nonconvex-strongly-concave settings. In addition, we propose a novel variant local SGDA+, to solve nonconvex-nonconcave problems. We give corroborating empirical evidence on different distributed minimax problems.', 'corpus_id': 232068945, 'score': 1}]
187	In-cell NMR	35ecd937c4d5fcc6e39bbfdfdc31afc3	19398	{'NMR': 'nuclear magnetic resonance'}	"[{'doc_id': '234785076', 'title': 'The Unconventional Self-Cleavage of Selenoprotein K', 'abstract': 'Through known association with other proteins, human selenoprotein K (selenok) is currently implicated in the palmitoylation of proteins, degradation of misfolded proteins, innate immune response, and the life cycle of SARS-CoV-2 virus. However, neither the catalytic function of selenok’s selenocysteine (Sec), which, curiously, resides in an intrinsically disordered protein segment nor selenok’s specific role in these pathways are known to date. This report casts these questions in a new light as it describes that selenok is able -both in vitro and in vivo- to cleave some of its own peptide bonds. The cleavages not only release selenok segments that contain its reactive Sec, but as the specific cleavage sites were identified, they proved to cluster tightly near sites through which selenok interacts with protein partners. Furthermore, it is shown that selenok’s cleavage activity is neither restricted to itself nor promiscuous but selectively extends to at least one of its protein partners. Together, selenok’s cleavage ability and its features have all hallmarks of a regulatory mechanism that could play a central role in selenok’s associations with other proteins and its cellular functions overall.', 'corpus_id': 234785076, 'score': 0}, {'doc_id': '25957300', 'title': 'In-cell NMR spectroscopy in Escherichia coli.', 'abstract': 'A living cell is a complex system that contains many biological macromolecules and small molecules necessary for survival, in a relatively small volume. It is within this crowded and complex cellular environment that proteins function making in-cell studies of protein structure and binding interactions an exciting and important area of study. Nuclear magnetic resonance (NMR) spectroscopy is a particularly attractive method for in-cell studies of proteins since it provides atomic-level data noninvasively in solution. In addition, NMR has recently undergone significant advances in instrumentation to increase sensitivity and in methods development to reduce data acquisition times for multidimensional experiments. Thus, NMR spectroscopy lends itself to studying proteins within a living cell, and recently ""in-cell NMR"" studies have been reported from several laboratories. To date, this technique has been successfully applied in Escherichia coli (E. coli), Xenopus laevis (X. laevis) oocytes, and HeLa host cells. Demonstrated applications include protein assignment as well as de novo 3D protein structure determination. The most common use, however, is to probe binding interactions and structural modifications directly from proton nitrogen correlation spectra. E. coli is the most extensively used cell type thus far and this chapter is largely confined to reviewing recent literature and describing methods and detailed protocols for in-cell NMR studies in this bacterial cell.', 'corpus_id': 25957300, 'score': 1}, {'doc_id': '235248290', 'title': 'Cysteine post-translational modifications: ten years from chemical proteomics to bioinformatics', 'abstract': 'As the only thiol-bearing amino acid, cysteine (Cys) residues in proteins have the reactive thiol side chain, which is susceptible to a series of post-translational modifications (PTMs). These PTMs participate in a wide range of biological activities including the alteration of enzymatic reactions, protein-protein interactions and protein stability. Here we summarize the advance of cysteine PTM identification technologies and the features of the various kinds of the PTMs. We also discuss in silico approaches for the prediction of the different types of cysteine modified sites, giving directions for future study.', 'corpus_id': 235248290, 'score': 0}, {'doc_id': '207123025', 'title': 'Differential dynamical effects of macromolecular crowding on an intrinsically disordered protein and a globular protein: implications for in-cell NMR spectroscopy.', 'abstract': 'In-cell NMR provides a valuable means to assess how macromolecules, with concentrations up to 300 g/L in the cytoplasm, affect the structure and dynamics of proteins at atomic resolution. Here an intrinsically disordered protein, alpha-synuclein (alphaSN), and a globular protein, chymotrypsin inhibitor 2 (CI2) were examined by using in-cell NMR. High-resolution in-cell spectra of alphaSN can be obtained, but CI2 leaks from the cell and the remaining intracellular CI2 is not detectable. Even after stabilizing the cells from leakage by using alginate encapsulation, no CI2 signal is detected. From in vitro studies we conclude that this difference in detectability is the result of the differential dynamical response of disordered and ordered proteins to the changes of motion caused by the increased viscosity in cells.', 'corpus_id': 207123025, 'score': 1}, {'doc_id': '3829980', 'title': 'Protein structure determination in living cells by in-cell NMR spectroscopy', 'abstract': 'Investigating proteins ‘at work’ in a living environment at atomic resolution is a major goal of molecular biology, which has not been achieved even though methods for the three-dimensional (3D) structure determination of purified proteins in single crystals or in solution are widely used. Recent developments in NMR hardware and methodology have enabled the measurement of high-resolution heteronuclear multi-dimensional NMR spectra of macromolecules in living cells (in-cell NMR). Various intracellular events such as conformational changes, dynamics and binding events have been investigated by this method. However, the low sensitivity and the short lifetime of the samples have so far prevented the acquisition of sufficient structural information to determine protein structures by in-cell NMR. Here we show the first, to our knowledge, 3D protein structure calculated exclusively on the basis of information obtained in living cells. The structure of the putative heavy-metal binding protein TTHA1718 from Thermus thermophilus HB8 overexpressed in Escherichia coli cells was solved by in-cell NMR. Rapid measurement of the 3D NMR spectra by nonlinear sampling of the indirectly acquired dimensions was used to overcome problems caused by the instability and low sensitivity of living E. coli samples. Almost all of the expected backbone NMR resonances and most of the side-chain NMR resonances were observed and assigned, enabling high quality (0.96\u2009ångström backbone root mean squared deviation) structures to be calculated that are very similar to the in vitro structure of TTHA1718 determined independently. The in-cell NMR approach can thus provide accurate high-resolution structures of proteins in living environments.', 'corpus_id': 3829980, 'score': 1}, {'doc_id': '235486638', 'title': 'The Inherent Dynamics and Interaction Sites of the SARS-CoV-2 Nucleocapsid N-Terminal Region', 'abstract': '\n The nucleocapsid protein is one of four structural proteins encoded by SARS-CoV-2 and plays a central role in packaging viral RNA and manipulating the host cell machinery, yet its dynamic behavior and promiscuity in nucleotide binding has made standard structural methods to address its atomic-resolution details difficult. To begin addressing the SARS-CoV-2 nucleocapsid protein interactions with both RNA and the host cell along with its dynamic behavior, we have specifically focused on the folded N-terminal domain (NTD) and its flanking regions using nuclear magnetic resonance solution studies. Studies performed here reveal a large repertoire of interactions, which includes a temperature-dependent self-association mediated by the disordered flanking regions that also serve as binding sites for host cell cyclophilin-A while nucleotide binding is largely mediated by the central NTD core. NMR studies that include relaxation experiments have revealed the complicated dynamic nature of this viral protein. Specifically, while much of the N-terminal core domain exhibits micro-millisecond motions, a central β-hairpin shows elevated inherent flexibility on the pico-nanosecond timescale and the serine/arginine-rich region of residues 176-209 undergoes multiple exchange phenomena. Collectively, these studies have begun to reveal the complexities of the nucleocapsid protein dynamics and its preferred interaction sites with its biological targets.\n', 'corpus_id': 235486638, 'score': 0}, {'doc_id': '20663362', 'title': 'Screening of small molecule interactor library by using in-cell NMR spectroscopy (SMILI-NMR).', 'abstract': 'We developed an in-cell NMR assay for screening small molecule interactor libraries (SMILI-NMR) for compounds capable of disrupting or enhancing specific interactions between two or more components of a biomolecular complex. The method relies on the formation of a well-defined biocomplex and utilizes in-cell NMR spectroscopy to identify the molecular surfaces involved in the interaction at atomic scale resolution. Changes in the interaction surface caused by a small molecule interfering with complex formation are used as a read-out of the assay. The in-cell nature of the experimental protocol insures that the small molecule is capable of penetrating the cell membrane and specifically engaging the target molecule(s). Utility of the method was demonstrated by screening a small dipeptide library against the FKBP-FRB protein complex involved in cell cycle arrest. The dipeptide identified by SMILI-NMR showed biological activity in a functional assay in yeast.', 'corpus_id': 20663362, 'score': 1}, {'doc_id': '235244218', 'title': 'Production of human translation-competent lysates using dual centrifugation', 'abstract': 'Protein synthesis is a central process in gene expression and the development of efficient in vitro translation systems has been the focus of scientific efforts for decades. The production of translation-competent lysates originating from human cells or tissues remains challenging, mainly due to the variability of cell lysis conditions. Here we present a robust and fast method based on dual centrifugation that allows for detergent-free cell lysis under controlled mechanical forces. We optimized the lysate preparation to yield cytoplasmic extracts from human cells that efficiently translate mRNAs in a cap-dependent as wells as in an IRES-mediated way. Reduction of the phosphorylation state of eIF2α using recombinant GADD34 and 2-aminopurine considerably boosts the protein output, reinforcing the potential of this method for the production of recombinant proteins from human lysates.', 'corpus_id': 235244218, 'score': 0}, {'doc_id': '133647', 'title': 'In-cell NMR in E. coli to Monitor Maturation Steps of hSOD1', 'abstract': 'In-cell NMR allows characterizing the folding state of a protein as well as posttranslational events at molecular level, in the cellular context. Here, the initial maturation steps of human copper, zinc superoxide dismutase 1 are characterized in the E. coli cytoplasm by in-cell NMR: from the apo protein, which is partially unfolded, to the zinc binding which causes its final quaternary structure. The protein selectively binds only one zinc ion, whereas in vitro also the copper site binds a non-physiological zinc ion. However, no intramolecular disulfide bridge formation occurs, nor copper uptake, suggesting the need of a specific chaperone for those purposes.', 'corpus_id': 133647, 'score': 1}, {'doc_id': '235474753', 'title': 'SARS-CoV-2 envelope-protein corruption of homeostatic signaling mechanisms in mammalian cells', 'abstract': 'During a SARS-CoV2 infection, host cells produce large amounts of the viral envelope protein (Ep-CoV2). Ep-CoV2 is partially inserted into the membrane of nascent viral particles and into cellular membranes. To mimic the pathophysiological impact of the cellular protein fraction, Ep-CoV2 was overexpressed in mammalian cells and effects on key signaling parameters were monitored. By tagging with green fluorescent protein (GFP), we found that Ep-CoV2 protein is mostly present in the endoplasmic reticulum with additional trace amounts in the plasma membrane. We observed that wild-type Ep-CoV2 and, to a lesser extent, its mutants (N15A, V25F) corrupted some of the most important homeostatic mechanisms in cells. The same was observed with isolated transmembrane domains of the protein. The Ep-CoV2-evoked elevation of intracellular Ca2+ and pH as well as the induced membrane depolarization produced by the presence of the protein interfere with major signal transduction cascades in host cells. These functions of Ep-CoV2, which likely contribute to the pathogenesis of the viral protein, result from the ion-channel activity of the viral protein. Two independent assays, a functional reconstitution of Ep-CoV2 protein in artificial membranes and a rescue of K+-deficient yeast mutants, confirm that Ep-CoV2 generates a cation-conducting channel with a low unitary conductance and a complex ion selectivity. The data presented here suggest that specific channel function inhibitors of Ep-CoV2 can provide cell protection and virostatic effects.', 'corpus_id': 235474753, 'score': 0}]"
188	Money and HCI	3b59c4472ff0263764c42f1c65ba0c4b	15708	{'HCI': 'human-computer interaction'}	"[{'doc_id': '232210552', 'title': 'From GenderMag to InclusiveMag:: A Journey for University IT', 'abstract': ""Does your college/university say one of its core values is diversity, equity, and inclusion? If so, is the IT they are using equitable and inclusive for usage by diverse populations of users? The evidence suggests “no” — and in this talk, we will consider how to address this problem by answering the following questions: How can IT professionals assess whether their IT supports diverse users? And if they find problems, how can they fix them? Although there are empirical processes that can be used to find “inclusivity bugs” piecemeal in software, web apps, websites, and so on, what is often needed is a systematic inspection method to assess IT's support for diverse populations. To help fill this gap, we developed GenderMag, a method for finding and fixing “gender inclusivity bugs” — gender biases in IT interfaces and workflows. We then introduced InclusiveMag, a generalization of GenderMag that can be used to generate systematic inclusiveness methods for other dimensions of diversity. In this talk, we present the latest GenderMag results, what OSU's IT professionals are doing with it, and provide a glimpse into the future by briefly introducing InclusiveMag and our early experiences with it."", 'corpus_id': 232210552, 'score': 0}, {'doc_id': '233270674', 'title': 'The U in Crypto Stands for Usable: An Empirical Study of User Experience with Mobile Cryptocurrency Wallets', 'abstract': 'In a corpus of 45,821 app reviews of the top five mobile cryptocurrency wallets, we identified and qualitatively analyzed 6,859 reviews pertaining to the user experience (UX) with those wallets. Our analysis suggests that both new and experienced users struggle with general and domain-specific UX issues that, aside from frustration and disengagement, might lead to dangerous errors and irreversible monetary losses. We reveal shortcomings of current wallet UX as well as users’ misconceptions, some of which can be traced back to a reliance on their understanding of conventional payment systems. For example, some users believed that transactions were free, reversible, and could be canceled anytime, which is not the case in reality. Correspondingly, these beliefs often resulted in unmet expectations. Based on our findings, we provide recommendations on how to design cryptocurrency wallets that both alleviate the identified issues and counteract some of the misconceptions in order to better support newcomers.', 'corpus_id': 233270674, 'score': 1}, {'doc_id': '232290705', 'title': ""How Social Are Social Media The Dark Patterns In Facebook's Interface"", 'abstract': 'Many researchers have been concerned with social media and possible negative impacts on the well-being of their audience. With the popularity of social networking sites (SNS) steadily increasing, psychological and social sciences have shown great interest in their effects and consequences on humans. Unfortunately, it appears to be difficult to find correlations between SNS and the results of their works. We, therefore, investigate Facebook using the tools of HCI to find connections between interface features and the concerns raised by these domains. With a nod towards Dark Patterns, we use an empirical design analysis to identify interface interferences that impact users’ online privacy. We further discuss how HCI can help to work towards more ethical user interfaces in the future.', 'corpus_id': 232290705, 'score': 0}, {'doc_id': '232073184', 'title': 'Zopa’s lambs: Video ads, internet investment, and the financialization of affect', 'abstract': 'This article is about how affect is mobilized through video advertising to encourage people to try new practices: discuss money and use peer-to-peer banking. A 2013 television commercial for a UK-based peer-to-peer lending firm demonstrates how affect is mobilized in the context of financialization in an age of austerity and increasing social inequality. The commercial, ‘Zopa Lambs,’ assembles imagery of an idealized rural England to obscure geographical and class differences among its customers while positioning the firm as a trustworthy upholder of conservative banking values against predatory payday lenders and irresponsible global banking firms. While the firm is entirely internet-based, in an environment of relatively low financial and technological literacy, trust is constructed heavily through the use of traditional media. While financial instruments generally are marketed through affective associations with particular status circles, here that circle is constructed neither as a wealthy urban elite nor as a populist mass, but as the ‘sensible:’ a weighted term carrying affective resonance with times of austerity, capital investment rather than consumption, and an idealized rural past. Affect and its capture In December 2013 the peer-to-peer (P2P) lending firm Zopa Limited (‘Zopa’) aired a television commercial in the UK which began with an announcer’s plummy tones declaring that ‘when it comes to money, there’s a particular Lword that people don’t like to talk about.’ That ‘L word’ was ‘loan:’ the commercial then substituted every mention of the word ‘loan’ in describing its product with the term ‘lamb,’ an image of a fluffy lamb obscuring the word ‘loan’ in the commercial’s text. The 30-second spot ends with the tagline ‘Sensible ephemera: theory & politics in organization 16(4): 33-49 34 | article Loans for Sensible People,’ the only time in which the actual L-word at issue is used. Our study of Zopa began with a set of expectations, nearly all of which were to prove incorrect during the year and a half of our work with the firm, examining its role in the ecology of UK-based alternatives to high-street banking. Key among those expectations was the conception of P2P as being grounded in a technolibertarian rhetoric related to that of the Pirate Bay, the Pirate Party, filesharing communities, and open-source software development. What we found was the keyword ‘sensible,’ a term freighted in Zopa’s usage with social class and regional overtones, deployed to position the firm, its customers, and its products in a particular niche within not only retail financial products in the UK but within an ongoing, contentious discourse around class identity playing out in the popular media. ‘Zopa Lambs’ represents a distillation of that discourse, and this work attempts to elucidate it as a study of a manifestation of affective capitalism in an era of austerity, financialization, and growing inequality. We assemble a theory of affect from media and cultural studies focusing on how affect is generated, circulated, captured, and capitalized on media and technological systems. Key to the theory of affective economies is the notion that producers and audiences are recursively linked through mediated networks. Emotional connection may develop resulting in either empathic activist networks or ‘resonant collectives’ or self-interested mobs like rioters or ‘idiot collectives’ (Hands, 2014). Affective collectivities aggregated through television advertising would be designated as ‘idiotic’ in so far that they are first self-serving entities collated only because of emotional resonance and consumeristic affinities. These idiotic collectives are joined by emotional affinity but not empathy. In this case, Zopa hail’s an audience as a sensible class of consumers conscious of cost and tentative about public discussions of money. This we argue, is the mechanism for the generation of affect in television viewers. The advertisement is aired on television, a conservative medium of sobriety. Zopa uses this familiar medium to disarm a rather radical notion, that individuals would use the internet to lend money to each other thereby routing around the central role played by banks. In this manner, a comforting affect is circulated on a traditional medium intended to facilitate the consumption of a relatively revolutionary concept: P2P lending. The video needs to motivate television advertising viewers towards online practices. Thus, the affective dimension of ‘Zopa Lambs’ is not only its call to emotions but also the way the video is communicated across online communities. John Carter McKnight and Adam Fish Zopa’s lambs article | 35 These efforts in televisual trust development succeed, however, only when they link to online practices, that is, to people actually using the Zopa interface to lend and borrow money. It is this transition from lean-back television viewing to leanforward online interactivity that the affect is ‘captured’ in an affective economy. It is instructive to consider affective economies in light of Deleuze’s concept of the ‘double-movement of liberation and capture’ (1972). Affective economies provide corporations opportunities to capture social capital in the form of affective intensities. Chow says captivation ‘is semantically suspended between an aggressive move and an affective state, and carries within it the force of the trap in both active and reactive senses’ (Chow, 2012: 48, in Berry, 2014). As a doublemovement, affect is not exclusively an economic unit; as emotion also has the capacity to motivate people towards political activism and other ‘resonant collective’ activities. In our case, affect is captured by corporations capable of situating themselves at key junctures in the affective economy. The political potential of P2P lending, namely its capacity to decentralize banking, is completely ignored in this movement towards the sensible. Finally, the affect that is generated in television audiences, circulated on television and online, and captured in an online banking platform, is eventually capitalized on by Zopa. The concept of ‘affective economy’ provides an insight into the capitalization of affect. Ahmed introduces the concept of ‘affective economy’ as an economy where emotions are not just experienced, but constantly being accumulated and exchanged between people. As Ahmed argues, ‘emotions work as a form of capital: affect does not reside positively in the sign or commodity, but is produced as an effect of its circulation’ (2004: 45). Thus, what scholars of affect in new media point out is the capacity of emotions to be produced, circulated, and ‘captured’ in online economies. Scholars often mobilize affect to theorize not quotidian but rather intense states of being, political passion, for instance. We consider the more conservative and ‘idiotic’ manifestations of affect, how the mundane emotion of sensibility is constructed. Thus this article is about how affect is mobilized through video advertising to encourage people to try new things: talk about money and peer-to-peer banking. Zopa attempted to do this through producing a television commercial which substituted a discussion of money with a discussion of ‘lambs.’ This switch, it was hoped, would entice a collective of people reticent to do so to discuss money and use an unfamiliar online lending system. In the process of the analysis, we endeavor to contribute to affective theory by situating the theory in terms of media-based circulation, personal economics, and class. Our position is that corporations use affect to create the conditions for the financialization of affect— or how discourse of money becomes embedded entertwined with other forms of being, namely, sensibility. These are not politicized, ‘resonant collectives’ but ephemera: theory & politics in organization 16(4): 33-49 36 | article rather, classed-based idiotic collectives aggregated by the persuasive powers of advertising. This article illustrates the findings of an 18-month investigation into P2P banking that included limited participant observation, select longitudinal interviews, and an analysis of primary marketing documents. Zopa and P2P lending Recently, P2P lending systems have emerged as a popular vehicle for unsecured consumer and small-business lending. Crowd-funding systems operate as a mixture of charitable donation and pre-purchase. By contrast, P2P lending involves the exchange of funds at commercial rates of interest, competing largely in the personal-investment market on price, by ‘disintermediating’ banks with relatively high overhead costs. Where filesharing sites provide minimal architecture for the mutual transfer of data, auction sites like eBay.com operate as marketplace infrastructure, enabling transactions between buyers and sellers and providing a reputation system driven by customer rankings. P2P lending firms are similar to auction sites, but offer critical intermediation, typically by evaluating the creditworthiness of potential borrowers through credit checks and underwriting due diligence. Those failing credit checks cannot borrow through the firm. While Zopa’s ethos of P2P may appear inclusive of a variety of financial classes, it must reaffirm pre-existing class demarcations. Zopa was the first P2P financial firm in the UK, founded in 2006, and has remained the market leader, with £697 million lent as of 18 December 2014. (Zopa.com, 2014). P2P consumer finance comprises 31% of the £1.7 billion UK alternative finance market as of 2014, a market which nearly tripled in size during 2014 (Nesta, 2014). However, the same survey notes that awareness and understanding of the alternative finance market is low within the UK, with 42% completely unaware of such practices and platforms, and 60% unlikely to use them out of lack of knowledge and fear of risk (ibid.). While growing at an impressive rate, Zopa must familiarize potential clients with its rather ', 'corpus_id': 232073184, 'score': 0}, {'doc_id': '203158369', 'title': 'Making Digital Money ""Work"" for Low-Income Users: Critical Reflections for HCI', 'abstract': ""This paper adds to the research on digitization and money in HCI. By presenting a case of rickshaw drivers in India and their use of Ola, an app-based taxi service like Uber, and Ola Money, an embedded m-wallet, this paper makes a threefold contribution. First, it shows how cash and digital money are not simply different manifestations of the ‘same' money for users. They provide distinct affordances and have different meanings and values, yielding rich insights for design. Second, it seeks to highlight the hidden work done by users around making digital money ‘work' for them. In doing so, it calls for a broader understanding of ‘moneywork' that goes beyond a temporal analysis, through the concept of ‘mobility work'. Finally, it highlights the role of ‘friction' in design. Friction is crucial to users' negotiation of the trade-off between consumption and saving, and can be leveraged to provoke reflection and user-awareness."", 'corpus_id': 203158369, 'score': 1}, {'doc_id': '127738696', 'title': 'Penerapan Model Pembelajaran Langsung Untuk Meningkatkan Hasil Belajar Siswa pada Mata Pelajaran Seni Budaya (Musik) di Kelas VIII-B SMP Negeri 9 Tebo', 'abstract': 'Penelitian ini bertujuan untuk meningkatkan hasil belajar siswa pada mata pelajaran \nseni budaya (Musik) di kelas VIII B SMP Negeri 9 Tebo dengan menerapkan model \npembelajaran langsung. Penelitian ini termasuk dalam jenis penelitian tindakan kelas (PTK). \nPenelitian dilakukan dalam dua siklus, dengan setiap siklus terdiri atas perencanaan, \ntindakan, observasi, dan refleksi. Subyek penelitian adalah adalah kelas VIII-B SMP Negeri 9 \nTebo dengan jumlah siswa sebanyak 24 orang. Teknik pengumpulan data yaitu melalui \nobservasi, wawancara, dan dokumentasi. Teknik analisis data dilakukan dengan 3 tahapan \nyaitu seleksi data, klasifikasi data, dan presentase data. \nHasil penelitian ini jika dilihat dari data nilai hasil belajar siswa meningkat dari pra \nsiklus ke siklus 1, dan lebih meningkat lagi pada siklus 2. Jumlah nilai hasil belajar siswa \nsebelum menerapkan model pembelajaran langsung yaitu pada pra siklus hanya sebanyak \n1696 dengan rata-rata nilai 70,7 yang kemudian meningkat pada siklus 1 yaitu sebesar 1853 \ndengan rata-rata nilai 77,2 dan kemudian meningkat lagi pada siklus ke 2 yaitu meningkat \nmenjadi 2001 dengan rata-rata nilai 83,4. \nJika dilihat dari aspek ketuntasan hasil belajar siswa dengan menerapkan model \npembelajaran langsung meningkat secara signifikan, dari pra siklus sebanyak 7 orang siswa \nyang tuntas lalu meningkat menjadi 19 orang pada siklus 1 kemudian meningkat lagi menjadi \n24 orang siswa atau bisa dikatakan tuntas 100% pada siklus ke 2. Untuk jumlah siswa yang \ntidak tuntas juga mengalami penurunan, yaitu pada pra siklus adalah sebanyak 17 orang \nkemudian menurun menjadi 5 orang pada siklus ke 1 dan kemudian menurun bahkan tidak \nada lagi siswa yang tidak tuntas pada siklus ke 2.', 'corpus_id': 127738696, 'score': 0}, {'doc_id': '9497418', 'title': '#CHImoney: financial interactions, digital cash, capital exchange and mobile money', 'abstract': ""Interactions around money and financial services are a critical part of our lives on and off-line. New technologies and new ways of interacting with these technologies are of huge interest; they enable new business models and ways of making sense of this most important aspect of our everyday lives. At the same time, money is an essential element in HCI research and design. This workshop is intended to bring together researchers and practitioners involved in the design and use of systems that combine digital and new media with monetary and financial interactions to build on an understanding of these technologies and their impacts on users' behaviors. The workshop will focus on social, technical, and economic aspects around everyday user interactions with money and emerging financial technologies and systems."", 'corpus_id': 9497418, 'score': 1}, {'doc_id': '232066855', 'title': '""Now You\'re Speaking My Language"": Towards a Seamless Localized Product Experience', 'abstract': ""Increased internet access and improvements in localization processes means that more people have access to information and entertainment than ever before. However, many people still find it difficult to navigate online and offline resources in their preferred language. In this talk, I will describe some of the challenges people face using streaming services and consuming video content in different languages. I will present research on the challenges we've uncovered at Netflix and how we've approached addressing questions around members' language experience. These examples demonstrate how moving away from an English-centric experience can benefit a global audience."", 'corpus_id': 232066855, 'score': 0}, {'doc_id': '231958783', 'title': 'Money vs. Social Life: Why People Choose Not to Use Facebook Messenger Payment', 'abstract': 'As money and mobile P2P payment experiences have become an emerging research agenda in HCI, prior studies have focused on the factors that promote the use of P2P payment services as well as related design implications. Yet, few have investigated why people decide not to use such services (e.g., technology non-use) and how people perceive the increasing trend of integrating P2P payments with social media services. To explore these issues, we identified factors that hindered people from using Facebook Messenger payment and their perceptions of integrating of P2P payments with social media based on 158 social media posts and eight interviews. This study not only extends our existing understanding of technology non-use through the lens of Facebook payment but also helps HCI researchers and designers innovate emerging financial technologies and better understand the global phenomenon of all-in-one design.', 'corpus_id': 231958783, 'score': 1}, {'doc_id': '23647705', 'title': 'Money talks: tracking personal finances', 'abstract': 'How do people keep track of their money? In this paper we present a preliminary scoping study of how 14 individuals in the San Francisco Bay Area earn, save, spend and understand money and their personal and family finances. We describe the practices we developed for exploring the sensitive topic of money, and then discuss three sets of findings. The first is the emotional component of the relationship people have with their finances. Second, we discuss the tools and processes people used to keep track of their financial situation. Finally we discuss how people account for the unknown and unpredictable nature of the future through their financial decisions. We conclude by discussing the future of studies of money and finance in HCI, and reflect on the opportunities for improving tools to aid people in managing and planning their finances.', 'corpus_id': 23647705, 'score': 1}]"
189	biomedical-seg-priors	0014098238183ad087e76cc325ed5669	12992	{}	[{'doc_id': '232282296', 'title': 'Distance ordinal regression loss for an improved nuclei segmentation', 'abstract': 'In digital pathology, nuclei segmentation still remains a challenging task due to the high heterogeneity and variability in the characteristics of nuclei, in particular, the clustered and overlapping nuclei. We propose a distance ordinal regression loss for an improved nuclei instance segmentation in digitized tissue specimen images. A convolutional neural network with two decoder branches is built. The first decoder branch conducts the nuclear pixel prediction and the second branch predicts the distance to the nuclear center, which is utilized to identify the nuclear boundary and to separate out overlapping nuclei. Adopting a distance-decreasing discretization strategy, we recast the problem of the distance prediction as an ordinal regression problem. To evaluate the proposed method, we conduct experiments on multiple independent multitissue histology image datasets. The experimental results on the multi-tissue datasets demonstrate the effectiveness of the proposed model.', 'corpus_id': 232282296, 'score': 0}, {'doc_id': '5081394', 'title': 'ConnNet: A Long-Range Relation-Aware Pixel-Connectivity Network for Salient Segmentation', 'abstract': 'Salient segmentation aims to segment out attention-grabbing regions, a critical yet challenging task and the foundation of many high-level computer vision applications. It requires semantic-aware grouping of pixels into salient regions and benefits from the utilization of global multi-scale contexts to achieve good local reasoning. Previous works often address it as two-class segmentation problems utilizing complicated multi-step procedures, including refinement networks and complex graphical models. We argue that semantic salient segmentation can instead be effectively resolved by reformulating it as a simple yet intuitive pixel-pair-based connectivity prediction task. Following the intuition that salient objects can be naturally grouped via semantic-aware connectivity between neighboring pixels, we propose a pure Connectivity Net (ConnNet). ConnNet predicts the connectivity probabilities of each pixel with its neighboring pixels by leveraging multi-level cascade contexts embedded in the image and long-range pixel relations. We investigate our approach on two tasks, namely, salient object segmentation and salient instance-level segmentation, and illustrate that consistent improvements can be obtained by modeling these tasks as connectivity instead of binary segmentation tasks for a variety of network architectures. We achieve the state-of-the-art performance, outperforming or being comparable to existing approaches while reducing inference time due to our less complex approach.', 'corpus_id': 5081394, 'score': 1}, {'doc_id': '231932050', 'title': 'TECHNICAL UNIVERSITY OF MUNICH TUM Data Innovation Lab Uncertainty Estimation for Deep Medical Image Segmentation', 'abstract': 'Predictive uncertainty estimation on medical image segmentation tasks can provide risk analysis in particular when applying deep learning methods for segmentation and is a new and exciting branch of research. In medical image segmentation, an automatic process of semantic segmentation on Optical Coherence Tomography (OCT) images (B-scans) of the retina detects fluids representing different retinal diseases and thus can help to facilitate prognosis. Currently, modern machine learning methods including deep learning have achieved state-of-the-art performance on semantic segmentation tasks. Specifically, U-Net-based neural networks have dominated medical image segmentation, and have been successfully applied on the OCT image segmentation challenge RETOUCH, which aims at segmenting and detecting three fluid types in retinal regions of human eyes. Although benchmark segmentation results have been established on the challenge, a credible estimation of the uncertainty of the segmentation due to the input-output black-box characteristic of U-Net is still missing. Therefore, we present a range of uncertainty estimation methods on the RETOUCH segmentation task to obtain a reliable estimation of both epistemic and aleatoric uncertainties and then evaluate the quality of this estimation using calibration metrics to compare the quality of these methods.', 'corpus_id': 231932050, 'score': 0}, {'doc_id': '4425800', 'title': 'DeepVesselNet: Vessel Segmentation, Centerline Prediction, and Bifurcation Detection in 3-D Angiographic Volumes', 'abstract': 'We present DeepVesselNet, an architecture tailored to the challenges faced when extracting vessel trees and networks and corresponding features in 3-D angiographic volumes using deep learning. We discuss the problems of low execution speed and high memory requirements associated with full 3-D networks, high-class imbalance arising from the low percentage (<3%) of vessel voxels, and unavailability of accurately annotated 3-D training data—and offer solutions as the building blocks of DeepVesselNet. First, we formulate 2-D orthogonal cross-hair filters which make use of 3-D context information at a reduced computational burden. Second, we introduce a class balancing cross-entropy loss function with false-positive rate correction to handle the high-class imbalance and high false positive rate problems associated with existing loss functions. Finally, we generate a synthetic dataset using a computational angiogenesis model capable of simulating vascular tree growth under physiological constraints on local network structure and topology and use these data for transfer learning. We demonstrate the performance on a range of angiographic volumes at different spatial scales including clinical MRA data of the human brain, as well as CTA microscopy scans of the rat brain. Our results show that cross-hair filters achieve over 23% improvement in speed, lower memory footprint, lower network complexity which prevents overfitting and comparable accuracy that does not differ from full 3-D filters. Our class balancing metric is crucial for training the network, and transfer learning with synthetic data is an efficient, robust, and very generalizable approach leading to a network that excels in a variety of angiography segmentation tasks. We observe that sub-sampling and max pooling layers may lead to a drop in performance in tasks that involve voxel-sized structures. To this end, the DeepVesselNet architecture does not use any form of sub-sampling layer and works well for vessel segmentation, centerline prediction, and bifurcation detection. We make our synthetic training data publicly available, fostering future research, and serving as one of the first public datasets for brain vessel tree segmentation and analysis.', 'corpus_id': 4425800, 'score': 1}, {'doc_id': '233004350', 'title': 'Deep ensembles based on Stochastic Activation Selection for Polyp Segmentation', 'abstract': 'Semantic segmentation has a wide array of applications ranging from medical-image analysis, scene understanding, autonomous driving and robotic navigation. This work deals with medical image segmentation and in particular with accurate polyp detection and segmentation during colonoscopy examinations. Several convolutional neural network architectures have been proposed to effectively deal with this task and with the problem of segmenting objects at different scale input. The basic architecture in image segmentation consists of an encoder and a decoder: the first uses convolutional filters to extract features from the image, the second is responsible for generating the final output. In this work, we compare some variant of the DeepLab architecture obtained by varying the decoder backbone. We compare several decoder architectures, including ResNet, Xception, EfficentNet, MobileNet and we perturb their layers by substituting ReLU activation layers with other functions. The resulting methods are used to create deep ensembles which are shown to be very effective. Our experimental evaluations show that our best ensemble produces good segmentation results by achieving high evaluation scores with a dice coefficient of 0.884, and a mean Intersection over Union (mIoU) of 0.818 for the Kvasir-SEG dataset. To improve reproducibility and research efficiency the MATLAB source code used for this research is available at GitHub: https://github.com/LorisNanni.', 'corpus_id': 233004350, 'score': 0}, {'doc_id': '203734704', 'title': 'A Topological Loss Function for Deep-Learning based Image Segmentation using Persistent Homology', 'abstract': 'We introduce a method for training neural networks to perform image or volume segmentation in which prior knowledge about the topology of the segmented object can be explicitly provided and then incorporated into the training process. By using the differentiable properties of persistent homology, a concept used in topological data analysis, we can specify the desired topology of segmented objects in terms of their Betti numbers and then drive the proposed segmentations to contain the specified topological features. Importantly this process does not require any ground-truth labels, just prior knowledge of the topology of the structure being segmented. We demonstrate our approach in four experiments: one on MNIST image denoising and digit recognition, one on left ventricular myocardium segmentation from magnetic resonance imaging data from the UK Biobank, one on the ACDC public challenge dataset and one on placenta segmentation from 3-D ultrasound. We find that embedding explicit prior knowledge in neural network segmentation tasks is most beneficial when the segmentation task is especially challenging and that it can be used in either a semi-supervised or post-processing context to extract a useful training gradient from images without pixelwise labels.', 'corpus_id': 203734704, 'score': 1}, {'doc_id': '232293845', 'title': 'Deep Learning in Medical Applications: Lesion Segmentation in Skin Cancer Images Using Modified and Improved Encoder-Decoder Architecture', 'abstract': 'The rise of deep learning techniques, such as a convolutional neural network (CNN) in solving medical image problems, offered fascinating results that motivated researchers to design automatic diagnostic systems. Image segmentation is one of the crucial and challenging steps in the design of a computer-aided diagnosis system owing to the presence of low contrast between skin lesion and background, noise artifacts, color variations, and irregular lesion boundaries. In this paper, we propose a modified and improved encoder-decoder architecture with a smaller network depth and a smaller number of kernels to enhance the segmentation process. The network performs segmentation for skin cancer images to obtain information about the infected area. The proposed model utilizes the power of the VGG19 network’s weight layers for calculating rich features. The deconvolutional layers were designed to regain spatial information of the image. In addition to this, optimized training parameters were adopted to further improve the network’s performance. The designed network was evaluated for two publicly available benchmarked datasets ISIC, and PH2 consists of dermoscopic skin cancer images. The experimental observations proved that the proposed network achieved the higher average values of segmentation accuracy 95.67%, IoU 96.70%, and BF-score of 89.20% on ISIC 2017 and accuracy 98.50%, IoU 93.25%, and BF-score 84.08% on PH2 datasets as compared to other state-of-the-art algorithms on the same datasets.', 'corpus_id': 232293845, 'score': 0}, {'doc_id': '226964989', 'title': 'High-level Prior-based Loss Functions for Medical Image Segmentation: A Survey', 'abstract': 'Today, deep convolutional neural networks (CNNs) have demonstrated state of the art performance for supervised medical image segmentation, across various imaging modalities and tasks. Despite early success, segmentation networks may still generate anatomically aberrant segmentations, with holes or inaccuracies near the object boundaries. To mitigate this effect, recent research works have focused on incorporating spatial information or prior knowledge to enforce anatomically plausible segmentation. If the integration of prior knowledge in image segmentation is not a new topic in classical optimization approaches, it is today an increasing trend in CNN based image segmentation, as shown by the growing literature on the topic. In this survey, we focus on high level prior, embedded at the loss function level. We categorize the articles according to the nature of the prior: the object shape, size, topology, and the inter-regions constraints. We highlight strengths and limitations of current approaches, discuss the challenge related to the design and the integration of prior-based losses, and the optimization strategies, and draw future research directions.', 'corpus_id': 226964989, 'score': 1}, {'doc_id': '232428560', 'title': 'clDice-a Novel Topology-Preserving Loss Function for Tubular Structure Segmentation', 'abstract': 'Accurate segmentation of tubular, network-like structures, such as vessels, neurons, or roads, is relevant to many fields of research. For such structures, the topology is their most important characteristic; particularly preserving connectedness: in the case of vascular networks, missing a connected vessel entirely alters the blood-flow dynamics. We introduce a novel similarity measure termed centerlineDice (short clDice), which is calculated on the intersection of the segmentation masks and their (morphological) skeleta. We theoretically prove that clDice guarantees topology preservation up to homotopy equivalence for binary 2D and 3D segmentation. Extending this, we propose a computationally efficient, differentiable loss function (soft-clDice) for training arbitrary neural segmentation networks. We benchmark the soft-clDice loss on five public datasets, including vessels, roads and neurons (2D and 3D). Training on soft-clDice leads to segmentation with more accurate connectivity information, higher graph similarity, and better volumetric scores.', 'corpus_id': 232428560, 'score': 1}, {'doc_id': '232765374', 'title': 'Improved segmentation by adversarial U-Net', 'abstract': 'Medical image segmentation has a fundamental role in many computer-aided diagnosis (CAD) applications. Accurate segmentation of medical images is a key step in tracking changes over time, contouring during radiotherapy planning, and more. One of the state-of-the-art models for medical image segmentation is the U–Net that consists of an encoder-decoder based architecture. Many variations exist to the U–Net architecture. In this work, we present a new training procedure that combines U–Net with an adversarial training we refer to as Adversarial U–Net. We show that Adversarial U–Net outperformes the conventional U–Net in three versatile domains that differ in the acquisition method as well as the physical characteristics and yields smooth and improved segmentation maps.', 'corpus_id': 232765374, 'score': 0}]
190	Data network effects	9323aed118d184c767b3e52fbb95472e	14439	{}	[{'doc_id': '52001119', 'title': 'Pipelines, platforms, and the new rules of strategy', 'abstract': 'For decades, the five-forces model of competition has dominated the thinking about strategy. But it describes competition among traditional “pipeline” businesses, which succeed by optimizing the activities in their value chains—most of which they own or control. “Platform” businesses that bring together consumers and producers, as Uber, Alibaba, and Airbnb do, require a different approach to strategy. The critical asset of a platform is external—the community of members. The focus shifts from controlling resources to orchestrating them, and firms win by facilitating more external interactions and creating “network effects” that increase the value provided to all participants. In this new world, competition can emerge from seemingly unrelated industries and even from within the platform itself. The authors, three platform strategists, walk executives through the choices they must make when building platforms, outlining the different metrics needed to manage them. Businesses that fail to learn the new rules will struggle, they argue. When a platform enters the marketplace of a pure pipeline business, the platform nearly always wins. That’s exactly what happened when the iPhone came on the scene in 2007. By 2015, it accounted for 92% of global profits in mobile phones, while most of the giants that once ruled the industry made no profit at all. INSETS: Networks Invert the Firm.;HARNESSING SPILLOVERS', 'corpus_id': 52001119, 'score': 1}, {'doc_id': '55229637', 'title': 'Role of extracellular signal-regulated kinase (ERK) signaling in nucleotide excision repair and genotoxicity in response to As(III) and Pb(II)', 'abstract': 'Arsenic and lead can induce genetic injuries and epigenetic signaling pathways in cultured mammalian cells. To test whether signaling pathways affect the extent of genetic injuries, we explored the impacts of extracellular signal-regulated kinase 1 and 2 (ERK) on nucleotide excision repair (NER), cytotoxicity, and genotoxicity following sodium arsenite [As(III)] and lead acetate [Pb(II)]. Sustained ERK activation was observed in human cells exposed to As(III) and Pb(II). As(III) inhibited the cellular NER synthesis capability; conversely, Pb(II) stimulated it. ERK activation contributed to the As(III)-induced NER inhibition and micronucleus formation. In contrast, this signal was required for inducing cellular NER activity and preventing mutagenesis following Pb(II). ERK activation by Pb(II) was dependent on protein kinase C (PKCα) that also exhibited anti-mutagenicity. Enforced expression of ERK signaling markedly elevated the cellular NER activity, which was suppressed by As(III). Nonetheless, ERK activation could counteract the cytotoxicity caused by these two metals. Together, the results indicate that pro-survival ERK signaling exhibits dual and opposing impacts on NER process following As(III) and Pb(II) exposures. The findings also suggest that ERK is an important epigenetic signaling in the determination of metal genotoxicity.', 'corpus_id': 55229637, 'score': 0}, {'doc_id': '231771652', 'title': 'Overcoming Barriers to Supply Chain Agility', 'abstract': 'Globalisation of trade and decades-long innovation in supply chain networks have resulted in significant benefits for all stakeholders – greater efficiencies, lower costs, greater access to markets to name just a few. Yet, Covid-19 has exposed vulnerabilities in global supply chains. Dispersed supply chains offer more possibilities for shocks to penetrate and spread, and practices such as “just-in-time” and single sourcing can amplify shocks and lengthen recovery time.', 'corpus_id': 231771652, 'score': 0}, {'doc_id': '56210799', 'title': '1 Data Network Effects : Implications for Data Business', 'abstract': 'This paper aims to investigate the existence of “data network effects” in data platform services such as Big Data, Internet-of-Things (IoT) and Artificial Intelligence (AI) and their influence on the diffusion of the services. It intends to present a preliminary formal analysis of the effects of data network externalities. Policy implications will be discussed in terms of the diffusion of services.', 'corpus_id': 56210799, 'score': 1}, {'doc_id': '218560637', 'title': 'Data-enabled learning, network effects and competitive advantage∗', 'abstract': 'We provide a model of competition in which firms can improve their products through learning from the data they obtain on customers they serve. The model is used to explore the implications for competitive dynamics of three new features of such learning compared to traditional learning-by-doing settings: (i) learning increases a firm’s demand rather than reducing its marginal cost, (ii) firms can improve their products for individual customers based on each customer’s particular usage experience, and (iii) the learning happens while a firm’s customers are still consuming the product. We show when and how network effects arise from these new features. We also analyze the consequences of data sharing, as well as the role of consumer beliefs, the nature of the learning curve, and other factors that affect an incumbent’s competitive advantage.', 'corpus_id': 218560637, 'score': 1}, {'doc_id': '210054798', 'title': 'Data Network Effects: The Example of Internet Search', 'abstract': 'The rise of dominant firms in data driven industries is often credited to their alleged data advantage. Empirical evidence lending support to this conjecture is lacking. In this paper, we show that data as an input into machine learning tasks displays features that favor the hypothesis that data is a source of market power. We study the search result quality for search keywords on Yahoo!. Search result quality improves when more users search a keyword. In addition to this direct network effect caused by more users, we observe an additional externality that is caused by the amount of data that the search engine collects on the users. More data on the users reinforces the direct network effect. We propose to view this reinforcement effect due to additional userspecific data as a data network effect. Our findings are consistent with the consensus that data display diminishing returns to scale for a given prediction task. This feature of data is often regarded as incompatible with the hypothesis that data is a source of market power. Our results rationalize the market power hypothesis through a different mechanism by suggesting that data, in addition to being an input, is also a technology shifter. ∗Berlin Scool of Economics, DIW Berlin and Technische Universität Berlin, mschaefer@diw.de †European Commission DG COMP Chief Economist Team and Düsseldorf Institute for Competition Economics, Heinrich-Heine-Universität Düsseldorf, 40204 Düsseldorf, Germany, sapi@dice.uni-duesseldorf.de The views expressed in this article are solely those of the authors and may not, under any circumstances, be regarded as representing an official position of the European Commission. This is personal research based entirely on publicly available information and not related to any activity of the European Commission. The authors thank Tomaso Duso and Hannes Ullrich for their continued advice and support. We are also grateful for the input provided by Andres Hervas Drane, Chiara Farronato, Szabolcs Lorincz, Christian Peukert, Ananya Sen, Jean Tirole, Kevin Tran, Catherine Tucker, Tommaso Valletti, Christoph Wolf and the participants of the Data Workshop at the University of Zürich (2017), the 4th edition of the Industrial Organization in the Digital Economy Workshop (Liège, 2018), the 16th International Industrial Organization Conference (Indianapolis, 2018), the Mannheim Centre for Competition and Innovation Conference (Mannheim, 2018), the 10th bi-annual Postal Economic Conference (Toulouse, 2018), the 10th Conference on Digital Economics (Paris, 2018), the 2nd Doctoral Workshop on the Economics of Digitization (Paris, 2018), the 20th Summer Workshop for Young Economists (Mannheim, 2018) and the 46th Annual Conference of the European Association for Research in Industrial Economics (Barcelona, 2019). All errors are our own.', 'corpus_id': 210054798, 'score': 1}, {'doc_id': '216160110', 'title': 'The Role of Artificial Intelligence and Data Network Effects for Creating User Value', 'abstract': 'Some of the world’s most profitable firms own platforms that exhibit network effects. A platform exhibits network effects if the more people that use it, the more valuable that it becomes to each u...', 'corpus_id': 216160110, 'score': 1}, {'doc_id': '231573242', 'title': 'Mechanistic Framework of Global Value Chains', 'abstract': 'Indeed, the global production (as a system of creating values) is eventually forming like a gigantic and complex network/web of value chains that explains the transitional structures of global trade and development of the global economy. It’s truly a newwave of globalisation, andwe term it as the global value chains (GVCs), creating the nexus among firms, workers and consumers around the globe. The emergence of this new scenario asks– how an economy’s firms, producers and workers connect in the global economy. And how are they capturing the gains out of it in terms of different dimensions of economic development? This GVC approach is very crucial for understanding the organisation of the global industries and firms. It requires the statics and dynamics of diverse players involved in this complex global production network. Its broad notion deals with different global issues (including regional value chains also) from the top down to the bottom up, founding a scope for policy analysis (Gereffi & Fernandez-Stark 2011). But it is true that, as Feenstra (1998) points out, any single computational framework is not sufficient to quantification this whole range of economic activities. We should adopt an integrative framework for accurate projection of this dynamic multidimensional phenomenon.', 'corpus_id': 231573242, 'score': 0}, {'doc_id': '131985585', 'title': 'The wireless barcode scanner with capability of scanning lcd barcode', 'abstract': 'PURPOSE: A wireless barcode scanner with LCD(Liquid-Crystal display)-barcode scanning capability is provided to easily identify customer information through the use of the wireless barcode scanner by deciphering barcodes on LCD, such as that of a cellular phone. CONSTITUTION: The wireless barcode scanner comprises of a trigger switch(TSW) triggering barcode scanning activity, a memory(10) with the record of a firmware used in controlling each parts, a micro-processor(20), performing activity controls in each parts according to the firmware in the memory, detecting signals from the TSW, controlling signal detection corresponding to light irradiation and barcodes, deciphering inputted LCD-barcode information and controlling the transmission of the deciphered data to other applications, a LED light source(30) analyzing the 660nm light of the LCD-barcode to be scanned according to the controls from the micro-processor(20), a lens and an optical filter(40) to condense the light reflected off of the LCD-barcode, a CCD(Charge coupled device) sensor(50) to convert the capacity of condensed light from the lens and the optical filter into electrical signal, a signal detector(60) to amplify the analogue signal from the CCD sensor(50) and digitalize the analogue signal.', 'corpus_id': 131985585, 'score': 0}, {'doc_id': '229615010', 'title': 'D2C (Direct To Consumer) Business Model: Efficacious Strategy for the Businesses to Grow During COVID-19 Scenario', 'abstract': 'Introduction: This paper aims to analyse D2C (direct to consumer) e-commerce strategy used by businesses or companies to sell to end-consumers directly during Covid-19 in organized retail. \nBackground: The pandemic has fuelled an explosion in online shopping, yet too many brands are only along for the ride, relying on their retail partners to share glimpses of first-party data that show past demand rather than a clear and predictive road map to future growth. The roots of direct marketing date back to trade catalogues, among the first tools of direct marketing. \nMethods: Existing literature on COVID-19 was analysed through secondary information to identify an explosion of D2C (direct to consumer) brands globally and India and its effect on business and commerce. \nConclusion: D2C is becoming the strongest weapon of the businesses against counterfeiters and growing their brand equity at the same time.', 'corpus_id': 229615010, 'score': 0}]
191	sme procurement	a1b8906cdd5cefaf9f870e31513d2933	4782	{}	"[{'doc_id': '168664841', 'title': ""Risk management in construction sector : New methodology for procurement in SME'S"", 'abstract': None, 'corpus_id': 168664841, 'score': 1}, {'doc_id': '189341667', 'title': 'SME development programmes : Capacity building of SMEs to participate in procurement', 'abstract': None, 'corpus_id': 189341667, 'score': 1}, {'doc_id': '216081373', 'title': 'Digitalization in management accounting and control: an editorial', 'abstract': 'Digitalization has the potential to disrupt the management accounting domain. It may not only affect the digital landscape of the organization and the associated business models, but also management accounting and control practices as well as the role of the controller. This editorial discusses these developments by introducing the concept of digitalization and describing its impact on the field of management accounting and control.', 'corpus_id': 216081373, 'score': 0}, {'doc_id': '214714491', 'title': 'Sustainable Banking; Evaluation of the European Business Models', 'abstract': 'Sustainability has become one of the challenges of today’s banks. Since sustainable business models are responsible for the environment and society along with generating economic benefits, they are an attractive approach to sustainability. Sustainable business models also offer banks competitive advantages such as increasing brand reputation and cost reduction. However, no framework is presented to evaluate the sustainability of banking business models. To bridge this theoretical gap, the current study using A Delphi-Analytic Hierarchy Process method, firstly, developed a sustainable business model to evaluate the sustainability of the business model of banks. In the second step, the sustainability performance of sixteen banks from eight European countries including Norway, The UK, Poland, Hungary, Germany, France, Spain, and Italy, assessed. The proposed business model components of this study were ranked in terms of their impact on achieving sustainability goals. Consequently, the proposed model components of this study, based on their impact on sustainability, are respectively value proposition, core competencies, financial aspects, business processes, target customers, resources, technology, customer interface, and partner network. The results of the comparison of the banks studied by each country disclosed that the sustainability of the Norwegian and German banks’ business models is higher than in other counties. The studied banks of Hungary and Spain came in second, the banks of The UK, Poland, and France ranked third, and finally, the Italian banks ranked fourth in the sustainability of their business models.', 'corpus_id': 214714491, 'score': 0}, {'doc_id': '215403709', 'title': 'Insurance uptake among small and medium-sized tourism and hospitality enterprises in a resource-scarce environment', 'abstract': ""\n Abstract\n \n Small and medium-sized tourism and hospitality enterprises (SMTHEs) are often susceptible to various hazards, which result in risk concerns. Insurance is recognised as one of the risk management strategies, but evidence indicates that insurance uptake among SMTHEs has been low. Yet, researchers have hardly researched into the factors that influence insurance uptake among SMTHEs. Two-hundred and fifty (250) respondents were selected using a multi-stage sampling technique. Confirmatory factor analysis, multivariate logit and probit regression techniques were used to determine factors underlying SMTHEs' insurance uptake. Risk concerns, the firm's characteristics, the perceived benefits of insurance and other informal risk coping mechanisms, as well as insurance service provision concerns were identified as determinants of insurance uptake. This is one of the first papers to offer a holistic understanding of the factors influencing SMTHEs' insurance subscription in a resource-scarce destination of Sub-Saharan Africa. The practical and theoretical implications of the paper are discussed.\n \n"", 'corpus_id': 215403709, 'score': 0}, {'doc_id': '215737230', 'title': 'Current Practices in the Information Collection for Enterprise Architecture Management', 'abstract': ""The digital transformation influences business models, processes, and enterprise IT landscape as a whole. Therefore, business-IT alignment is becoming more important than ever before. Enterprise architecture management (EAM) is designed to support and improve this business-IT alignment. The success of EAM crucially depends on the information available about a company's enterprise architecture, such as infrastructure components, applications, and business processes. This paper discusses the results of a qualitative expert survey with 26 experts in the field of EAM. The goal of this survey was to highlight current practices in the information collection for EAM and identify relevant information from enterprise-external data sources. The results provide a comprehensive overview of collected and utilized information in the industry, including an assessment of the relevance of such information. Furthermore, the results highlight challenges in practice and point out investments that organizations plan in the field of EAM."", 'corpus_id': 215737230, 'score': 0}, {'doc_id': '27675354', 'title': 'Antibody targeting of TGF-β in cancer patients.', 'abstract': ""The role of TGF-β in tumor development and progression is complex. Genetic mutations that disrupt the antiproliferative signaling effects of TGF-β play a key role in the process of malignant transformation for many types of tumors. Paradoxically, this loss of sensitivity to TGF-β's inhibitory actions often leads to TGF-β overexpression by the tumor cells or by normal cells that are recruited to the tumor microenvironment. Elevated concentrations of TGF-β in the tumor microenvironment have been shown to facilitate tumor growth and metastasis. Numerous published studies have provided evidence that inhibition of TGF-β using antibodies, soluble receptors and small molecule inhibitors of TGF-β signal transduction can have beneficial effects in murine models of cancer. Given the pleiotropic nature of TGF-β and its homeostatic role in numerous biological processes, serious concerns have been expressed regarding the safety of administering TGF-β antagonists to human patients. Interestingly, the results of numerous animal toxicology studies of TGF-β antibodies in normal rodents and primates have shown that administration of neutralizing anti-TGF-β antibodies is well tolerated and any adverse effects were reversible or self-limiting. Likewise, administration of a human anti-TGF-β antibody (fresolimumab) in three separate human phase 1 clinical trials has also been shown to be well tolerated."", 'corpus_id': 27675354, 'score': 0}, {'doc_id': '213306568', 'title': 'Hanze Procurement Education & Team Research', 'abstract': 'The presentation covers recent and current procurement management team research activities at Hanze university, notably SME procurement, sustainable procurement, and innovation procurement.', 'corpus_id': 213306568, 'score': 1}, {'doc_id': '168261098', 'title': 'Impact of green procurement practices in small and medium enterprises in Nairobi', 'abstract': ""The impact of Green Procurement Practices (GPP) in small and medium enterprises involves spending and the investment process typically assoc iated with economic gain and environmental friendly results at the end of the day. GPP among S ME`s in Nairobi s is not as straightforward as for governments because companies themselves hav to be self-motivated to embrace sustainability depending on their mission, vision and objectives. GPP is largely linked to the wider agenda of sustainable development and SME`s p racticing sustainable procurement to meet their needs for goods, services, utilities; work no t only on a cost benefit analysis, but with a view to maximizing net benefits for themselves and the wider world market. In doing so, SME`s incorporate acquired cost considerations into deci sions alongside the procurement criteria of price and quality, and also in the practice of the sustainable impacts of potential supplier`s approach often assessed as a form of quality c onsideration. These considerations focus on all areas of any organization in terms of the enviro mental, economic and social effects. The researcher used the descriptive research design to determine the impact of green procurement practices in Small and Medium Enterprises in Nairob i whereby it was found that; GPP is embedded in the principle of pollution prevention, which strives to eliminate and or to reduce risks to human health and the environment. This mea ns valuating purchases based on a variety of criteria, ranging from the necessity of the purc hase of the product from the first place to the options available for its eventual disposal. Cons umers, investors, shareholders and regulatory agencies in SME`s are increasingly demanding that o rganizations behave in an environmentally responsible manner. Therefore practicing green pr ocurement practices demonstrates an organization's commitment to considering and minimi z ng the environmental consequences of its activities thus making both environmental and eco nomic sense. The drivers influencing the adoption of green procurement practices by the SMEs are government laws and regulations requirement, changing customer demands and expect ations, the company initiatives, global purchase and production standards, employee initi at ves and suppliers influence. Conclusions made were green products are generally produced in a manner that consumes less natural resources. They may involve less energy in thei r manufacture and may consume less energy when being used, and they generally contain fewer h azardous or toxic materials. Green products are also generally designed with the intention of r educing the amount of waste created and may contain recycled material or use less packaging. Green procurement practices also offer cost savings for the organization. Consequently, GPP h as benefits for health and safety, both of workplaces and of the wider community. Organizat ions that practice green procurement are recognized as good corporate citizens, and have inf luence over other organizations around them."", 'corpus_id': 168261098, 'score': 1}, {'doc_id': '6229983', 'title': 'Research on the Procurement Issues of Small and Medium-Sized Enterprise under the Conditions of Price Uncertainty', 'abstract': 'The funds and scales of small and medium-sized enterprise are all smaller, for lowering operation cost, The SME in general is very concerned about their supplies of raw materials. And in the market, the price is volatile. To ensure that in such cases SME can make the best purchasing decision-making, this paper presents the use of dynamic programming ideas to solve this problem, Constructs of the procurement decision-making model.', 'corpus_id': 6229983, 'score': 1}]"
192	Turbulence	2e65f984d6d087f022127261fc079377	6400	{}	"[{'doc_id': '220127973', 'title': 'Winds in Star Clusters Drive Kolmogorov Turbulence', 'abstract': 'Intermediate and massive stars drive fast and powerful isotropic winds that interact with the winds of nearby stars in star clusters and the surrounding interstellar medium (ISM). Wind-ISM collisions generate astrospheres around these stars that contain hot $T\\sim 10^7$ K gas that adiabatically expands. As individual bubbles expand and collide they become unstable, potentially driving turbulence in star clusters. In this paper we use hydrodynamic simulations to model a densely populated young star cluster within a homogeneous cloud to study stellar wind collisions with the surrounding ISM. We model a mass-segregated cluster of 20 B-type young main sequence stars with masses ranging from 3--17 $M_{\\odot}$. We evolve the winds for $\\sim$11 kyrs and show that wind-ISM collisions and over-lapping wind-blown bubbles around B-stars mixes the hot gas and ISM material generating Kolmogorov-like turbulence on small scales early in its evolution. We discuss how turbulence driven by stellar winds may impact the subsequent generation of star formation in the cluster', 'corpus_id': 220127973, 'score': 1}, {'doc_id': '201625759', 'title': 'PURANA NARRATOLOGY AND THOMAS KING: REWRITING OF COLONIAL HISTORY IN THE MEDICINE RIVER AND JOE THE PAINTER AND THE DEER ISLAND MASSACRE', 'abstract': ""I Resume Puranas are essentially Hindu storytelling versions of the holy Vedas. They interpret complex truths to the masses. The author suggests that Tom King does the same thing in his fiction. He writes about the complicated sociopolitical and historical realities of Canadian colonialism affecting Native people. He fictionalizes these events to interpret truths to a mass audience, both Native and non-Native, using a narrative style to remind readers of these ongoing problems. Les Puranas sont essentiellement des contes fondes sur les Vedas sacres. lis interpretent des verites complexes pour les masses. Le present article met de I'avant que I'ecrivain Tom King fait la meme chose dans ses oeuvres de fiction. Ce dernier traite des realites socio-politiques et historiques complexes du colonialisme canadien aI'egard des peuples autochtones. II romance les evenements pour interpreter diverses verites pour Ie grand public (Autochtones et non-Autochtones) en utilisant un style narratif afin de rappeler ases lecteurs les problemes courants du colonialisme. The Canadian Journal of Native Studies XXII, 1(2002):65-80."", 'corpus_id': 201625759, 'score': 0}, {'doc_id': '14446723', 'title': 'Simulating star formation in molecular cloud cores. I. The influence of low levels of turbulence on fragmentation and multiplicity', 'abstract': 'We present the results of an ensemble of simulations of the collapse and fragmentation of dense star-forming cores. We show that even with very low levels of turbulence the outcome is usually a binary, or higher-order multiple, system. We take as the initial conditions for these simulations a typical low-mass core, based on the average properties of a large sample of observed cores. All the simulated cores start with a mass of Mtotal = 5.4 M� , a flattened central density profile, a ratio of thermal to gravitational energy αtherm = 0.45 and a ratio of turbulent to gravitational energy αturb = 0.05 . Even this low level of turbulence - much lower than in most previous simulations - is sufficient to produce multiple star formation in 80% of the cores; the mean number of stars and brown dwarfs formed from a single core is 4.55, and the maximum is 10. At the outset, the cores have no large-scale rotation. The only difference between each individual simulation is the detailed structure of the turbulent velocity field. The multiple systems formed in the simulations have properties consistent with observed multiple systems. Dynamical evolution tends preferentially to eject lower mass stars and brown dwarves whilst hardening the remaining binaries so that the median semi-major axis of binaries formed is ∼30 AU. Ejected objects are usually single low-mass stars and brown dwarfs, yielding a strong correlation between mass and multiplicity. Brown dwarves are ejected with a higher average velocity than stars, and over time this should lead to mass segregation in the parent cluster. Our simulations suggest a natural mechanism for forming binary stars that does not require large-scale rotation, capture, or large amounts of turbulence.', 'corpus_id': 14446723, 'score': 1}, {'doc_id': '220265638', 'title': 'First Results from SMAUG: Characterization of Multiphase Galactic Outflows from a Suite of Local Star-forming Galactic Disk Simulations', 'abstract': 'Large scale outflows in star-forming galaxies are observed to be ubiquitous, and are a key aspect of theoretical modeling of galactic evolution in a cosmological context, the focus of the SMAUG (Simulating Multiscale Astrophysics to Understand Galaxies) project. Gas blown out from galactic disks, similar to gas within galaxies, consists of multiple phases with large contrasts of density, temperature, and other properties. To study multiphase outflows as emergent phenomena, we run a suite of ~pc-resolution local galactic disk simulations using the TIGRESS framework. Explicit modeling of the interstellar medium (ISM), including star formation and self-consistent radiative heating plus supernova feedback, regulates ISM properties and drives the outflow. We investigate the scaling of outflow mass, momentum, energy, and metal loading factors with galactic disk properties, including star formation rate (SFR) surface density (\\Sigma_SFR~10^{-4}-1 M_sun/kpc^2/yr), gas surface density (~1-100 M_sun/pc^2), and total midplane pressure (or weight) (~10^3-10^6 k_B cm^{-3} K). The main components of outflowing gas are mass-delivering cool gas (T~10^4 K) and energy/metal-delivering hot gas (T~10^6 K). Cool mass outflow rates measured at outflow launch points (one or two scale heights) are 1-100 times the SFR (decreasing with \\Sigma_SFR), although in massive galaxies most mass falls back due to insufficient outflow velocity. The hot galactic outflow carries mass comparable to 10% of the SFR, together with 10-20% of the energy and 30-60% of the metal mass injected by SN feedback. The characteristic outflow velocities of both phases scale very weakly with SFR, as v_out \\propto \\Sigma_SFR^{0.1~0.2}, consistent with observations. Importantly, our analysis demonstrates that in any physically-motivated cosmological wind model, it is crucial to include at least two distinct thermal wind components.', 'corpus_id': 220265638, 'score': 0}, {'doc_id': '17438270', 'title': 'Simulating star formation in molecular cloud cores. IV. The role of turbulence and thermodynamics', 'abstract': 'Context. Observations suggest that low-mass stars condense out of dense, relatively isolated, molecular cloud cores, with each core spawning a small-N cluster of stars. Aims. Our aim is to identify the physical processes shaping the collapse and fragmentation of a 5.4 Mcore, and to understand how these processes influence the mass distribution, kinematics, and binary statistics of the resulting stars. Methods. We perform SPH simulations of the collapse and fragmentation of cores having different initial levels of turbulence (αTURB = 0.05, 0.10, 0.25). We use a new treatment of the energy equation that captures (i) excitation of the rotational and vibrational degrees of freedom of H2, dissociation of H2, ionisation of H and He; and (ii) the transport of cooling radiation against opacity due to both dust and gas (including the effects of dust sublimation, molecules, and H − ions). We also perform comparison simulations using a standard barotropic equation of state. Results. We find that - when compared with the barotropic equation of state - our more realistic treatment of the energy equation results in more protostellar objects being formed, and a higher proportion of brown dwarfs; the multiplicity frequency is essentially unchanged, but the multiple systems tend to have shorter periods (by a factor ∼3), higher eccentricities, and higher mass ratios. The reason for this is that small fragments are able to cool more effectively with the new treatment, as compared with the barotropic equation of state. We also note that in our simulations the process of fragmentation is often bimodal, in the following sense. The first protostar to form is usually, at the end, the most massive, i.e. the primary. However, frequently a disc-like structure subsequently forms round this primary, and then, once it has accumulated sufficient mass, quickly fragments to produce several secondaries. Conclusions. We believe that this delayed fragmentation of a disc-like structure is likely to be an important source of very low-mass stars in nature (both low-mass hydrogen-burning stars and brown dwarf stars); hence it may be fundamental to understanding the way in which the statistical properties of stars change - continuously but monotonically - with decreasing mass. However, in our simulations the individual cores probably produce too many stars, and hence too many single stars. We list the physical and numerical features that still need to be included in our simulations to make them more realistic; in particular, radiative and mechanical feedback, non-ideal magneto-hydrodynamic effects, and a more sophisticated implementation of sink particles.', 'corpus_id': 17438270, 'score': 1}, {'doc_id': '219573652', 'title': 'Powering galactic superwinds with small-scale AGN winds', 'abstract': 'We present a new implementation for active galactic nucleus (AGN) feedback through small-scale, ultra-fast winds in the moving-mesh hydrodynamic code AREPO. The wind is injected by prescribing mass, momentum and energy fluxes across a spherical boundary centred on a supermassive black hole according to available constraints for accretion disc winds. After sweeping-up a mass equal to their own, small-scale winds thermalise, powering energy-driven outflows with dynamics, structure and cooling properties in excellent agreement with those of analytic wind solutions. Momentum-driven solutions do not easily occur, because the Compton cooling radius is usually much smaller than the free-expansion radius of the small-scale winds. Through various convergence tests, we demonstrate that our implementation yields wind solutions which are well converged down to the typical resolution achieved in cosmological simulations. We test our model in hydrodynamic simulations of isolated Milky Way - mass galaxies. Above a critical AGN luminosity, initially spherical, small-scale winds power bipolar, energy-driven super-winds that break out of the galactic nucleus, flowing at speeds $> 1000 \\rm \\, km \\, s^{-1}$ out to $\\sim 10 \\, \\rm kpc$. These energy-driven outflows result in moderate, but long-term, reduction in star formation, which becomes more pronounced for higher AGN luminosities and faster small-scale winds. Suppression of star formation proceeds through a rapid mode that involves the removal of the highest-density, nuclear gas and through a slower mode that effectively halts halo gas accretion. Our new implementation makes it possible to model AGN-driven winds in a physically meaningful and validated way in simulations of galaxy evolution, the interstellar medium and black hole accretion flows.', 'corpus_id': 219573652, 'score': 0}, {'doc_id': '8304302', 'title': 'Star formation in molecular cores - III. The effect of the turbulent power spectrum', 'abstract': 'We investigate the effect of the turbulent power spectrum ($P(k) \\propto k^{-n}$, with $n = 3,\\,4\\;{\\rm or}\\;5$) on the fragmentation of low-mass cores, by means of SPH simulations. We adopt initial density profiles and low levels of turbulence based on observation, and for each n -value we conduct an ensemble of simulations with different initial seeds for the turbulent velocity field, so as to obtain reasonable statistics. We find that when power is concentrated at larger scales (i.e. for larger n ), more protostellar objects form and there is a higher proportion of low-mass stars and brown dwarfs. This is in direct contrast with the recent results of Delgado Donate et\xa0al., presumably because they adopted much higher levels of turbulence.', 'corpus_id': 8304302, 'score': 1}, {'doc_id': '220403457', 'title': 'The Role of Stellar Feedback in the Chemical Evolution of a Low Mass Dwarf Galaxy', 'abstract': 'We investigate how each aspect of a multi-channel stellar feedback model drives the chemodynamical evolution of a low-mass, isolated dwarf galaxy using a suite of high-resolution simulations. Our model follows individual star particles sampled randomly from an adopted initial mass function, considering independently feedback from: supernovae; stellar radiation causing photoelectric heating of dust grains, ionization and associated heating, Lyman-Werner (LW) dissociation of H$_2$, and radiation pressure; and winds from massive main sequence (neglecting their energy input) and asymptotic giant branch (AGB) stars. Radiative transfer is done by ray tracing. We consider the effects each of these processes have on regulating the star formation rate, global properties, multi-phase interstellar medium (ISM), and driving of galactic winds. We follow individual metal species from distinct nucleosynthetic enrichment channels (AGB winds, massive star stellar winds, core collapse and Type Ia supernovae) and pay particular attention to how these feedback processes regulate metal mixing in the ISM, the metal content of outflows, and the stellar abundance patterns in our galaxy. We find that---for a low-metallicity, low-mass dwarf galaxy ---stellar radiation, particularly ionizing radiation and LW radiation, are important sources of stellar feedback whose effects dominate over photoelectric heating and HI radiation pressure. However, feedback is coupled non-linearly, and the inclusion or exclusion of each process produces non-negligible effects. We find strong variations with: the star formation history; the ejection fractions of metals, mass, and energy; and the distribution of elements from different nucleosynthetic sources in both the gas and stars.', 'corpus_id': 220403457, 'score': 0}, {'doc_id': '15225772', 'title': 'Simulating star formation in molecular cores. II. The effects of different levels of turbulence', 'abstract': ""We explore, by means of a large ensemble of SPH simulations, how the level of turbulence affects the collapse and fragmentation of a star-forming core. All our simulated cores have the same mass (5.4 M ○. ), the same initial density profile (chosen to fit observations of L1544), and the same barotropic equation of state, but we vary (a) the initial level of turbulence (as measured by the ratio of turbulent to gravitational energy, α turb = U turb /|Ω| = 0, 0.01, 0.025, 0.05, 0.10 and 0.25) and (b), for fixed α turb , the details of the initial turbulent velocity field (so as to obtain good statistics). A low level of turbulence (α turb ∼ 0.05) suffices to produce multiple systems, and as α turb is increased, the number of objects formed and the companion frequency both increase. The mass function is bimodal, with a flat low-mass segment representing single objects ejected from the core before they can accrete much, and a Gaussian high-mass segment representing objects which because they remain in the core grow by accretion and tend to pair up in multiple systems. The binary statistics reported for field G-dwarfs by Duquennoy & Mayor (1991, A&A, 248, 485) are only reproduced with α turb ∼ 0.05. For much lower values of α turb (≤0.025), insufficient binaries are formed. For higher values of α turb (≥0,10), there is a significant sub-population of binaries with small semi-major axis and large mass-ratio (i.e. close binaries with components of comparable mass). This sub-population is not present in Duquennoy & Mayor's sample, although there is some evidence for it in the pre-Main Sequence population of Taurus analyzed by White & Ghez (2001, ApJ, 556, 265). It arises because with larger α turb , more low-mass objects are formed, and so there is more scope for the binaries remaining in the core to be hardened by ejecting these low-mass objects. Hard binaries thus formed then tend to grow towards comparable mass by competitive accretion of material with relatively high specific angular momentum."", 'corpus_id': 15225772, 'score': 1}, {'doc_id': '5585566', 'title': 'Label-free solution-based kinetic study of aptamer-small molecule interactions by kinetic capillary electrophoresis with UV detection revealing how kinetics control equilibrium.', 'abstract': 'Here we demonstrate a label-free solution-based approach for studying the kinetics of biopolymer-small molecule interactions. The approach utilizes kinetic capillary electrophoresis (KCE) separation and UV light absorption detection of the unlabeled small molecule. In this proof-of-concept work, we applied KCE-UV to study kinetics of interaction between a small molecule and a DNA aptamer. From the kinetic analysis of a series of aptamers, we found that dissociation rather than binding controls the stability of the complex. Because of its label-free features and generic nature, KCE-UV promises to become a practical tool for challenging kinetic studies of biopolymer-small molecule interactions.', 'corpus_id': 5585566, 'score': 0}]"
193	predatory	1b75bb583ebea526ccc2a3babab58cc6	5155	{}	"[{'doc_id': '194393357', 'title': 'The Writing Style of Predatory Publishers', 'abstract': 'In 2010, librarian Jeffrey Beall started a list of journals that allegedly use predatory practices to recruit manuscripts for publication. Coined “Beall’s List,” this working catalogue highlights over two hundred open-access journals that may feign editorial processes, peerreview, or other procedures of a reputable publisher. Given the recent attention to scientific misconduct, an important question is whether there are methods to detect predatory publishers from authentic ones. In this study, we apply an automated language analysis technique from the social sciences to examine how predatory and authentic journals differ in their writing style in the About Us and Aim/Scope sections of their websites. Compared to authentic journals, predatory journals use more discrepancy terms (e.g., “should” “would”) and positive emotions (e.g., “exciting”) but fewer function words, including articles (e.g., “a” “the”), and prepositions (e.g., “before” “in”), quantifiers (e.g., “more” “less”), and terms related to causality (e.g., “therefore”). These results follow recent patterns in the deception literature, suggesting that language features may be useful when evaluating predatory versus authentic publishers. In addition to analyzing writing style, we examined meta-linguistic properties of predatory publishers (i.e., editorial statistics, website features, and contact information) from the same database of journals. Compared to authentic publishers, predatory publishers use more third-party email addresses, claim false impact factors, fake rapid peer review, and simulate academic expertise. This is the first study to investigate predatory publishing through an empirical social science lens and our results suggest that there are quantifiable linguistic and meta-linguistic indicators that can, to some degree, distinguish between predatory publishers and those journals that seek to publish honestly. Introduction In recent years, the rise of scientific misconduct has drawn attention to the “publish or perish” mentality consuming academia, which highlights a drive for researchers to publish early and often in their career. The pressure to publish regularly can tamper with the quality of research and the moral compass of academics across disciplines. When misconducts are discovered (i.e., data fabrication or falsification), publications are typically withdrawn from the journal via retractions. In the past decade, the number of retraction notices has increased tenfold and the problem is steadily becoming worse. Until recently, little academic work has examined scientific misconduct. High-profile cases of fraud in Social Psychology have popularized the issue, but the majority of the empirical literature has surveyed the prevalence of misconduct to understand its scope. Another form of deception in science has been overlooked, however: predatory publishing. This describes a phenomenon of open-access publishers feigning editorial processes, peer-review, or other procedures of a reputable publisher in order to attract high publication fees. Predatory publishing impacts the science enterprise because it questions whether academics can distinguish between real and fake research. Furthermore, if scientists base their own research on publications that have not been thoroughly vetted, this may lead to a significant decrease in the quality of research in circulation. P ge 24259.3 Predatory journals act as a revolving door for manuscripts and academics who want to publish quickly and effortlessly. Many predatory journals appear genuine and the journal title may match an authentic publisher. For example, the Journal of Cloud Computing is a publication by SpringerOpen (authentic) and IBIMA Publishing (predatory). This duplicity can mislead researchers and becomes problematic when scaled to larger domains of science. Given the difficulty academics may experience in distinguishing predatory from authentic journals, an important empirical question asks if there are differences in how these journals market themselves to academics. By using automated linguistic-analysis techniques from the social sciences we provide the first study measuring how predatory journals describe themselves differently on their websites relative to authentic publishers. To complete our investigation, we also incorporate a meta-linguistic analysis to understand how certain predatory traits are reflected online. Predatory Publishing Explained Jeffrey Beall, a librarian and Associate Professor at the University of Colorado Denver, first documented an issue with academic publishing in 2010 and created “Beall’s List.” This working document highlights over two hundred journals that fail to meet standards akin to authentic, established publishers. He uncovered that predatory publishers typically have six defining features. According to Beall’s taxonomy, predatory publishers: 1. “Publish papers already published in other venues/outlets without providing appropriate credits. 2. Use language claiming to be a ‘leading publisher’ even though the publisher may only be a startup or a novice organization. 3. Operate in a Western country chiefly for the purpose of functioning as a vanity press for scholars in a developing country. 4. Do minimal or no copyediting. 5. Publish papers that are not academic at all, e.g. essays by laypeople or obvious pseudoscience. 6. Have a ‘contact us’ page that only includes a web form, and the publisher hides or does not reveal its location.” Essentially, predatory publishers deceive academics by faking the practices and policies of top journals (e.g., peer review, editorial boards, impact factors). In doing so, predatory journals have the potential to degrade the quality of research in circulation and they reduce the likelihood of scholars publishing in reputable and prestigious journals. With scientific misconduct on the rise, it is important to understand how predatory publishers may contribute to concerns about integrity in science. In this study, we use publically available data (i.e., information on a publisher’s website) to compare language differences between predatory and authentic journal text from the About Us and Aim/Scope sections. Do predatory journals write and describe themselves differently than authentic journals? In order to answer this question, we first address relevant work in the deception field to examine the role that language plays in understanding truthful and dishonest discourse patterns. Deception and Writing Style Prior to 2000, the majority of deception research sought to uncover nonverbal cues, such as fidgeting, gaze aversion, or particular hand-movements, that may reveal deception in face-toP ge 24259.4 face interactions. Contrary to popular belief, there are no universal cues that reliably indicate deception, and the search for “Pinocchio’s nose” has met with little success. This realization has led many scholars to study deception from another angle: language and word patterns. With modern advances in computer science, the method of using computerized text-analysis to differentiate false from truthful speech has been widely applied across a range of contexts. In a seminal work from Newman, Pennebaker, Berry, and Richards (2003), automated linguistic analysis uncovered differences between false and truthful speech on abortion attitudes, attitudes towards friends, and a mock crime. Their work started a trend of researchers using computers to analyze writing style, particularly because of this method’s objectivity and the resources that were saved by not employing humans to judge word types. Writing style differences have been an interest for deception researchers in a variety of areas. Deceptive and truthful utterances have been compared across Computer-Mediated Communication technologies, statements made by American presidents, online dating profiles, and user-generated content including hotel reviews. These studies have revealed that psychological dynamics associated with deception can be revealed in language. For example, compared to honest language, deceptive language tends to feature an increase of negative emotion terms (e.g., hate, aggression, hurt) as a reflection of the guilty or anxiety associated with lying. Many of our social relationships are built on honesty, and deception has the potential to jeopardize interpersonal trust. Therefore, it becomes distressing to lie to a friend or colleague, and an increase of negative emotions reflects this apprehension. Deceptive speech also tends to feature fewer first-person singular pronouns (e.g., I, me) as a mechanism of psychological distancing. Liars typically distance themselves from deceit by using fewer “Iwords” and increasing the number of social references (e.g., he, she, they) in order to deflect attention from the self. It is personally and socially damaging to be called a liar; therefore, removing the self from a lie is ideal for face-saving and relationship-saving strategies. Finally, lies are usually less detailed than truths because it is difficult to give specifics about information that is fabricated. The number of quantifiers (e.g., more, less) is one example of language specificity, and lies often contain fewer quantifiers than truths. The Influence of Context on Deceptive Writing Style Despite the aforementioned findings, deception patterns are not identical across research domains. As circumstances change, the psychological dynamics and associated patterns of language use should change as well. That is, deception cues are not universal because of important shifts in context along at least three proposed dimensions: psychological dynamics, goals, and genre conventions. In order to successfully understand how people lie in different scenarios, the psychological impact of the deception (i.e., emotional and cognitive involvement) must be considered. An understanding of the pragmatic goals for engaging in deception may reveal the purpose of the lie and ex', 'corpus_id': 194393357, 'score': 1}, {'doc_id': '215793692', 'title': 'References From Predatory Publishers: Policy Statement for the Journal of Human Lactation', 'abstract': None, 'corpus_id': 215793692, 'score': 1}, {'doc_id': '218764694', 'title': 'Science Communication in Multiple Languages Is Critical to Its Effectiveness', 'abstract': 'In 1967, English was recognized as the language of international science (Gordin, 2015) and it continues to dominate global scientific activities to this day. Around 80% of all journals indexed in SCOPUS are published in English (van Weijen, 2012). The linguistic domination of English is also observed in scientific journalism worldwide, which heavily depends on English-only sources (Nguyen and Tran, 2019). While the use of a single international language of science facilitates the dissemination of knowledge across national and cultural boundaries, the English language often acts as a gatekeeper to scientific discourse (Tardy, 2004). The hegemony of English in science promotes and enforces the imposition of one particular cultural point-of-view over others (Alves and Pozzebon, 2013). By ignoring other languages, traditional mass media (e.g., newspapers, magazines), social media, and scientific journals ignore the cultures and perspectives of non-English speaking communities (Gibbs, 1995; Canagarajah, 1996, 2002; Kachru, 1997). A recent Google search (February, 2020) of the term “science” in 11 languages with the largest numbers of native speakers exemplifies the disproportionate dominance of English (Figure 1). It is clear that English is overrepresented in these search results, even after normalizing for the total number of native speakers per language (Figure 1). One explanation could be that the term “science” may not be as engaging and meaningful as other science-related terms in other languages. An alternative explanation could be that scientific communication in a language correlates with scientific activity in the corresponding countries. Such is the case in the field of bioinformatics, where the nations with the highest impact (h-index) are those that are the most active in academic publishing (Chasapi et al., 2020). Nonetheless, English search results are still∼8 times more popular even when compared to languages spoken in countries with a strong history of scientific production like Germany and Russia (Figure 1). Facing the biggest existential threats to humanity requires understanding and support of science at a global scale, as exemplified by a multitude of climate-related natural disasters (Garcia Escobar and Rabanales, 2020; Stone, 2020) and the recent COVID-19 outbreak (Zarocostas, 2020). This opinion piece discusses some consequences of the (almost exclusive) use of English in the current global scientific landscape, and provides recommendations to expand both formal and informal science communication beyond the English language.', 'corpus_id': 218764694, 'score': 0}, {'doc_id': '218581938', 'title': 'Peer Review: Objectivity, Anonymity, Trust', 'abstract': 'This dissertation is focused on the role of objectivity in peer review. Through an examination of aspects of peer review including anonymity, trust, expertise, and the question of who has standing to evaluate research, we find that objectivity in peer review differs significantly from other uses of the term objectivity in science. In peer review it is not required for this objectivity to have correspondence to an outside world, instead it is enough for it to operate inside the ""rules"" of the community. Neither is the objectivity here empirical in the sense of using data about the scientific problem in question. Rather, the objectivity is one of judgment, cleaving to the epistemological standards of a community that are formed by background assumptions and beliefs. As a consequence, we highlight the role of subjectivity in what is usually taken as a practice of objectivity, and arrive at the insight that objectivity is not defined by one core value, but a balance of transparency, confidentiality, trust, representation, and living up to community standards. As such, objectivity in peer review is a highly specific sense of the term that is not reducible to that used in other aspects of scientific practice.', 'corpus_id': 218581938, 'score': 0}, {'doc_id': '218487486', 'title': 'Examining Citations of Natural Language Processing Literature', 'abstract': 'We extracted information from the ACL Anthology (AA) and Google Scholar (GS) to examine trends in citations of NLP papers. We explore questions such as: how well cited are papers of different types (journal articles, conference papers, demo papers, etc.)? how well cited are papers from different areas of within NLP? etc. Notably, we show that only about 56% of the papers in AA are cited ten or more times. CL Journal has the most cited papers, but its citation dominance has lessened in recent years. On average, long papers get almost three times as many citations as short papers; and papers on sentiment classification, anaphora resolution, and entity recognition have the highest median citations. The analyses presented here, and the associated dataset of NLP papers mapped to citations, have a number of uses including: understanding how the field is growing and quantifying the impact of different types of papers.', 'corpus_id': 218487486, 'score': 0}, {'doc_id': '205542365', 'title': 'Analysis of thirteen predatory publishers: a trap for eager-to-publish researchers', 'abstract': 'Abstract Objective: To demonstrate a strategy employed by predatory publishers to trap eager-to-publish authors or researchers into submitting their work. Methods: This was a case study of 13 potential, possible, or probable predatory scholarly open-access publishers with similar characteristics. Eleven publishers were included from Beall’s list and two additional publishers were identified from a Google web search. Each publisher’s site was visited and its content analyzed. Publishers publishing biomedical journals were further explored and additional data was collected regarding their volumes, details of publications and editorial-board members. Results: Overall, the look and feel of all 13 publishers was similar including names of publishers, website addresses, homepage content, homepage images, list of journals and subject areas, as if they were copied and pasted. There were discrepancies in article-processing charges within the publishers. None of the publishers identified names in their contact details and primarily included only email addresses. Author instructions were similar across all 13 publishers. Most publishers listed journals of varied subject areas including biomedical journals (12 publishers) covering different geographic locations. Most biomedical journals published none or very few articles. The highest number of articles published by any single biomedical journal was 28. Several editorial-board members were listed across more than one journals, with one member listed 81 times in different 69 journals (i.e. twice in 12 journals). Conclusion: There was a strong reason to believe that predatory publishers may have several publication houses with different names under a single roof to trap authors from different geographic locations.', 'corpus_id': 205542365, 'score': 1}, {'doc_id': '218674426', 'title': 'Do journals flipping to gold open access show an OA citation or publication advantage?', 'abstract': 'The effects of Open Access (OA) upon journal performance are investigated. The key research question holds: How does the citation impact and publication output of journals switching (“flipping”) from non-OA to Gold-OA develop after their switch to Gold-OA? A review is given of the literature, with an emphasis on studies dealing with flipping journals. Two study sets with 119 and 100 flipping journals, derived from two different OA data sources (DOAJ and OAD), are compared with two control groups, one based on a standard bibliometric criterion, and a second controlling for a journal’s national orientation. Comparing post-switch indicators with pre-switch ones in paired T-tests, evidence was obtained of an OA Citation advantage but not of an OA Publication Advantage. Shifts in the affiliation countries of publishing and citing authors are characterized in terms of countries’ income class and geographical world region. Suggestions are made for qualitative follow-up studies to obtain more insight into OA flipping or reverse-flipping.', 'corpus_id': 218674426, 'score': 0}, {'doc_id': '218559790', 'title': 'Science Magazine', 'abstract': 'A fter decades of debate on the feasibility of open access (OA) to scientific publications, we may be nearing a tipping point. A number of recent developments, such as Plan S, suggest that OA upon publication could become the default in the sciences within the next several years. Despite uncertainty about the long-term sustainability of OA models, many publishers who had been reluctant to abandon the subscription business model are showing openness to OA (1). Although more OA can mean more immediate, global access to scholarship, there remains a need for practical, sustainable models, for careful analysis of the consequences of business model choices, and for “caution in responding to passionate calls for a ‘default to open’” (2). Of particular concern for the academic community, as subscription revenues decline in the transition to OA and some publishers prioritize other sources of revenue, is the growing ownership of data analytics, hosting, and portal services by large scholarly publishers. This may enhance publishers’ ability to lock in institutional customers through combined offerings that condition open access to journals upon purchase of other services. Even if such “bundled” arrangements have a near-term benefit of increasing openly licensed scholarship, they may run counter to long-term interests of the academic community by reducing competition and the diversity of service offerings. The healthy functioning of the academic community, including fair terms and conditions from commercial partners, requires that the global marketplace for data analytics and knowledge infrastructure be kept open to real competition. INSIGHTS', 'corpus_id': 218559790, 'score': 0}, {'doc_id': '197890491', 'title': 'A Wolf in Sheep’s Clothing: Open Access, and Predatory Publishers', 'abstract': None, 'corpus_id': 197890491, 'score': 1}, {'doc_id': '209325154', 'title': 'Predatory Publishers and Conference Organizers', 'abstract': 'The rise of disreputable and dishonest journal publishers and conferences organizers—known as “predatory” journals and conferences—has made deciding where to publish articles and attend conferences a surprisingly difficult task. Whereas some journals and conferences can easily be dismissed as untrustworthy, others require further investigation and evaluation. Awareness of the publishing practices and selection criteria used by a journal can help you avoid being drawn in by publishers whose review practices fail to exclude works of low value and by conference organizers whose lack of selectivity may result in a poor-quality conference experience for attendees. Neither for-profit nor open-access status necessarily tells you if a publisher or conference organizer is unworthy of your attention. This article outlines criteria you can use to determine if journals or conferences should be considered worthy of publishing your manuscripts or hosting your presentations. Your time and work are limited and valuable. Where you choose to showcase them is important to both you and to other professionals in your field. Nutr Today. 2019;54(6):261–270', 'corpus_id': 209325154, 'score': 1}]"
194	Diagnosing changes - spatial	d0d91540e889212dc86f09e7c9a0ed4c	19732	{}	"[{'doc_id': '236999951', 'title': 'Temporal Assessment of Eastern Spotted Skunk Geographic Distribution', 'abstract': 'Abstract Spilogale putorius (Eastern Spotted Skunk) experienced range-wide population declines beginning in the mid-1900s with no clear understanding of the causal mechanism or whether such declines were associated with range contractions. Species-distribution models can provide a powerful framework to assess changes in landscape suitability in response to changing environmental conditions. Herein, we modeled time-stepped distributions of suitable environmental conditions for Eastern Spotted Skunks from 1938 to 2016 in Maxent, incorporating climate and land-cover predictors. Climate and land-cover variables reliably predicted landscape suitability of Eastern Spotted Skunks over time. We found a 37% decline in suitable area from historic predictions, consistent with reports of population declines in these areas. Our predicted landscape-suitability maps can be used to evaluate the current distribution of environmentally suitable conditions for the species as well as guide research and conservation efforts.', 'corpus_id': 236999951, 'score': 0}, {'doc_id': '2352667', 'title': 'Predicting species distribution: offering more than simple habitat models.', 'abstract': 'In the last two decades, interest in species distribution models (SDMs) of plants and animals has grown dramatically. Recent advances in SDMs allow us to potentially forecast anthropogenic effects on patterns of biodiversity at different spatial scales. However, some limitations still preclude the use of SDMs in many theoretical and practical applications. Here, we provide an overview of recent advances in this field, discuss the ecological principles and assumptions underpinning SDMs, and highlight critical limitations and decisions inherent in the construction and evaluation of SDMs. Particular emphasis is given to the use of SDMs for the assessment of climate change impacts and conservation management issues. We suggest new avenues for incorporating species migration, population dynamics, biotic interactions and community ecology into SDMs at multiple spatial scales. Addressing all these issues requires a better integration of SDMs with ecological theory.', 'corpus_id': 2352667, 'score': 1}, {'doc_id': '215802454', 'title': 'Comparing and synthesizing quantitative distribution models and qualitative vulnerability assessments to project marine species distributions under climate change', 'abstract': 'Species distribution shifts are a widely reported biological consequence of climate-driven warming across marine ecosystems, creating ecological and social challenges. To meet these challenges and inform management decisions, we need accurate projections of species distributions. Quantitative species distribution models (SDMs) are routinely used to make these projections, while qualitative climate change vulnerability assessments are becoming more common. We constructed SDMs, compared SDM projections to expectations from a qualitative expert climate change vulnerability assessment, and developed a novel approach for combining the two methods to project the distribution and relative biomass of 49 marine species in the Northeast Shelf Large Marine Ecosystem under a “business as usual” climate change scenario. A forecasting experiment using SDMs highlighted their ability to capture relative biomass patterns fairly well (mean Pearson’s correlation coefficient between predicted and observed biomass = 0.24, range = 0–0.6) and pointed to areas needing improvement, including reducing prediction error and better capturing fine-scale spatial variability. SDM projections suggest the region will undergo considerable biological changes, especially in the Gulf of Maine, where commercially-important groundfish and traditional forage species are expected to decline as coastal fish species and warmer-water forage species historically found in the southern New England/Mid-Atlantic Bight area increase. The SDM projections only occasionally aligned with vulnerability assessment expectations, with agreement more common for species with adult mobility and population growth rates that showed low sensitivity to climate change. Although our blended approach tried to build from the strengths of each method, it had no noticeable improvement in predictive ability over SDMs. This work rigorously evaluates the predictive ability of SDMs, quantifies expected species distribution shifts under future climate conditions, and tests a new approach for integrating SDMs and vulnerability assessments to help address the complex challenges arising from climate-driven species distribution shifts.', 'corpus_id': 215802454, 'score': 1}, {'doc_id': '236095454', 'title': 'Quest for New Space for Restricted Range Mammals: The Case of the Endangered Walia Ibex', 'abstract': 'Populations of large mammals have declined at alarming rates, especially in areas with intensified land use where species can only persist in small habitat fragments. To support conservation planning, we developed habitat suitability models for the Walia ibex (Capra walie), an endangered wild goat endemic to the Simen Mountains, Ethiopia. We calibrated several models that differ in statistical properties to estimate the spatial extent of suitable habitats of the Walia ibex in the Simen Mountains, as well as in other parts of the Ethiopian highlands to assess potentially suitable areas outside the current distribution range of the species. We further addressed the potential consequences of future climate change using a climate model with four emission scenarios. Model projections estimated the potential suitable habitat under current climate to 501–672 km2 in Simen and 6,251–7,732 km2 in other Ethiopian mountains. Under projected climate change by 2,080, the suitable habitat became larger in Simen but smaller in other parts of Ethiopia. The projected expansion in Simen is contrary to the general expectation of shrinking suitable habitats for high-elevation species under climate warming and may partly be due to the ruggedness of these particular mountains. The Walia ibex has a wide altitudinal range and is able to exploit very steep slopes, allowing it to track the expected vegetation shift to higher altitudes. However, this potential positive impact may not last long under continued climate warming, as the species will not have much more new space left to colonize. Our study indicates that the current distribution range can be substantially increased by reintroducing and/or translocating the species to other areas with suitable habitat. Indeed, to increase the viability and prospects for survival of this flagship species, we strongly recommend human-assisted reintroduction to other Ethiopian mountains. Emulating the successful reintroduction of the Alpine ibex that has spread from a single mountain in Italy to its historical ranges of the Alps in Europe might contribute to saving the Walia ibex from extinction.', 'corpus_id': 236095454, 'score': 0}, {'doc_id': '83666790', 'title': 'Is my species distribution model fit for purpose? Matching data and models to applications', 'abstract': ""Species distribution models (SDMs) are used to inform a range of ecological, biogeographical and conservation applications. However, users often underestimate the strong links between data type, model output and suitability for end-use. We synthesize current knowledge and provide a simple framework that summarizes how interactions between data type and the sampling process (i.e. imperfect detection and sampling bias) determine the quantity that is estimated by a SDM. We then draw upon the published literature and simulations to illustrate and evaluate the information needs of the most common ecological, biogeographical and conservation applications of SDM outputs. We find that, while predictions of models fitted to the most commonly available observational data (presence records) suffice for some applications, others require estimates of occurrence probabilities, which are unattainable without reliable absence records. Our literature review and simulations reveal that, while converting continuous SDM outputs into categories of assumed presence or absence is common practice, it is seldom clearly justified by the application's objective and it usually degrades inference. Matching SDMs to the needs of particular applications is critical to avoid poor scientific inference and management outcomes. This paper aims to help modellers and users assess whether their intended SDM outputs are indeed fit for purpose."", 'corpus_id': 83666790, 'score': 1}, {'doc_id': '236436900', 'title': 'Long-term changes in populations of rainforest birds in the Australia Wet Tropics bioregion: a climate-driven biodiversity emergency', 'abstract': 'Many authors have suggested that the vulnerability of montane biodiversity to climate change worldwide is significantly higher than in most other ecosystems. Despite the extensive variety of studies predicting severe impacts of climate change globally, few studies have empirically validated the predicted changes in distribution and population density. Here, we used 17 years of bird monitoring across latitudinal/elevational gradients in the rainforest of the Australian Wet Tropics World Heritage Area to assess changes in local abundance and distribution. We used relative abundance in 1977 surveys across 114 sites ranging from 0-1500m above sea level and utilised a trend analysis approach (TRIM) to investigate elevational shifts in abundance of 42 species between 2000 – 2016. The local abundance of most mid and high elevation species has declined at the lower edges of their distribution by >40% while lowland species increased by up to 190% into higher elevation areas. Upland-specialised species and regional endemics have undergone dramatic population declines of almost 50%. The “Outstanding Universal Value” of the Australian Wet Tropics World Heritage Area, one of the most irreplaceable biodiversity hotspots on Earth, is rapidly degrading. These observed impacts are likely to be similar in many tropical montane ecosystems globally.', 'corpus_id': 236436900, 'score': 0}, {'doc_id': '19019299', 'title': 'Correlative and mechanistic models of species distribution provide congruent forecasts under climate change.', 'abstract': 'Good forecasts of climate change impacts on extinction risks are critical for effective conservation management responses. Species distribution models (SDMs) are central to extinction risk analyses. The reliability of predictions of SDMs has been questioned because models often lack a mechanistic underpinning and rely on assumptions that are untenable under climate change. We show how integrating predictions from fundamentally different modeling strategies produces robust forecasts of climate change impacts on habitat and population parameters. We illustrate the principle by applying mechanistic (Niche Mapper) and correlative (Maxent, Bioclim) SDMs to predict current and future distributions and fertility of an Australian gliding possum. The two approaches make congruent, accurate predictions of current distribution and similar, dire predictions about the impact of a warming scenario, supporting previous correlative-only predictions for similar species. We argue that convergent lines of independent evidence provide a robust basis for predicting and managing extinctions risks under climate change.', 'corpus_id': 19019299, 'score': 1}, {'doc_id': '236968585', 'title': ""The evolutionary genomics of species' responses to climate change."", 'abstract': ""Climate change is a threat to biodiversity. One way that this threat manifests is through pronounced shifts in the geographical range of species over time. To predict these shifts, researchers have primarily used species distribution models. However, these models are based on assumptions of niche conservatism and do not consider evolutionary processes, potentially limiting their accuracy and value. To incorporate evolution into the prediction of species' responses to climate change, researchers have turned to landscape genomic data and examined information about local genetic adaptation using climate models. Although this is an important advancement, this approach currently does not include other evolutionary processes-such as gene flow, population dispersal and genomic load-that are critical for predicting the fate of species across the landscape. Here, we briefly review the current practices for the use of species distribution models and for incorporating local adaptation. We next discuss the rationale and theory for considering additional processes, reviewing how they can be incorporated into studies of species' responses to climate change. We summarize with a conceptual framework of how manifold layers of information can be combined to predict the potential response of specific populations to climate change. We illustrate all of the topics using an exemplar dataset and provide the source code as potential tutorials. This Perspective is intended to be a step towards a more comprehensive integration of population genomics with climate change science."", 'corpus_id': 236968585, 'score': 0}, {'doc_id': '11283373', 'title': 'More than the sum of the parts: forest climate response from joint species distribution models.', 'abstract': 'The perceived threat of climate change is often evaluated from species distribution models that are fitted to many species independently and then added together. This approach ignores the fact that species are jointly distributed and limit one another. Species respond to the same underlying climatic variables, and the abundance of any one species can be constrained by competition; a large increase in one is inevitably linked to declines of others. Omitting this basic relationship explains why responses modeled independently do not agree with the species richness or basal areas of actual forests. We introduce a joint species distribution modeling approach (JSDM), which is unique in three ways, and apply it to forests of eastern North America. First, it accommodates the joint distribution of species. Second, this joint distribution includes both abundance and presence-absence data. We solve the common issue of large numbers of zeros in abundance data by accommodating zeros in both stem counts and basal area data, i.e., a new approach to zero inflation. Finally, inverse prediction can be applied to the joint distribution of predictions to integrate the role of climate risks across all species and identify geographic areas where communities will change most (in terms of changes in abundance) with climate change. Application to forests in the eastern United States shows that climate can have greatest impact in the Northeast, due to temperature, and in the Upper Midwest, due to temperature and precipitation. Thus, these are the regions experiencing the fastest warming and are also identified as most responsive at this scale.', 'corpus_id': 11283373, 'score': 1}, {'doc_id': '235647715', 'title': 'Coral distribution and bleaching vulnerability areas in Southwestern Atlantic under ocean warming', 'abstract': 'Global climate change is a major threat to reefs by increasing the frequency and severity of coral bleaching events over time, reducing coral cover and diversity. Ocean warming may cause shifts in coral communities by increasing temperatures above coral’s upper thermal limits in tropical regions, and by making extratropical regions (marginal reefs) more suitable and potential refugia. We used Bayesian models to project coral occurrence, cover and bleaching probabilities in Southwestern Atlantic and predicted how these probabilities will change under a high-emission scenario (RCP8.5). By overlapping these projections, we categorized areas that combine high probabilities of coral occurrence, cover and bleaching as vulnerability-hotspots. Current coral occurrence and cover probabilities were higher in the tropics (1°S–20°S) but both will decrease and shift to new suitable extratropical reefs (20°S–27°S; tropicalization) with ocean warming. Over 90% of the area present low and mild vulnerability, while the vulnerability-hotspots represent\u2009~\u20093% under current and future scenarios, but include the most biodiverse reef complex in South Atlantic (13°S–18°S; Abrolhos Bank). As bleaching probabilities increase with warming, the least vulnerable areas that could act as potential refugia are predicted to reduce by 50%. Predicting potential refugia and highly vulnerable areas can inform conservation actions to face climate change.', 'corpus_id': 235647715, 'score': 0}]"
195	Scientific Documents	4b0941fe53368a171144284116955316	6267	{}	"[{'doc_id': '220381366', 'title': 'scb-mt-en-th-2020: A Large English-Thai Parallel Corpus', 'abstract': ""The primary objective of our work is to build a large-scale English-Thai dataset for machine translation. We construct an English-Thai machine translation dataset with over 1 million segment pairs, curated from various sources, namely news, Wikipedia articles, SMS messages, task-based dialogs, web-crawled data and government documents. Methodology for gathering data, building parallel texts and removing noisy sentence pairs are presented in a reproducible manner. We train machine translation models based on this dataset. Our models' performance are comparable to that of Google Translation API (as of May 2020) for Thai-English and outperform Google when the Open Parallel Corpus (OPUS) is included in the training data for both Thai-English and English-Thai translation. The dataset, pre-trained models, and source code to reproduce our work are available for public use."", 'corpus_id': 220381366, 'score': 0}, {'doc_id': '218570919', 'title': 'CAiRE-COVID: A Question Answering and Multi-Document Summarization System for COVID-19 Research', 'abstract': 'To address the need for refined information in COVID-19 pandemic, we propose a deep learning-based system that uses state-of-the-art natural language processing (NLP) question answering (QA) techniques combined with summarization for mining the available scientific literature. Our system leverages the Information Retrieval (IR) system and QA models to extract relevant snippets from the existing literature given a query. Fluent summaries are also provided to help understand the content in a more efficient way. In this paper, we describe our CAiRE-COVID system architecture and methodology for building the system. To bootstrap the further study, the code for our system is available at this https URL', 'corpus_id': 218570919, 'score': 1}, {'doc_id': '220403590', 'title': 'Learning Neural Textual Representations for Citation Recommendation', 'abstract': 'With the rapid growth of the scientific literature, manually selecting appropriate citations for a paper is becoming increasingly challenging and time-consuming. While several approaches for automated citation recommendation have been proposed in the recent years, effective document representations for citation recommendation are still elusive to a large extent. For this reason, in this paper we propose a novel approach to citation recommendation which leverages a deep sequential representation of the documents (Sentence-BERT) cascaded with Siamese and triplet networks in a submodular scoring function. To the best of our knowledge, this is the first approach to combine deep representations and submodular selection for a task of citation recommendation. Experiments have been carried out using a popular benchmark dataset - the ACL Anthology Network corpus - and evaluated against baselines and a state-of-the-art approach using metrics such as the MRR and F1@$k$ score. The results show that the proposed approach has been able to outperform all the compared approaches in every measured metric.', 'corpus_id': 220403590, 'score': 1}, {'doc_id': '220403560', 'title': 'KQA Pro: A Large Diagnostic Dataset for Complex Question Answering over Knowledge Base', 'abstract': 'Complex question answering over knowledge base (Complex KBQA) is challenging because it requires the compositional reasoning capability. Existing benchmarks have three shortcomings that limit the development of Complex KBQA: 1) they only provide QA pairs without explicit reasoning processes; 2) questions are either generated by templates, leading to poor diversity, or on a small scale; and 3) they mostly only consider the relations among entities but not attributes. To this end, we introduce KQA Pro, a large-scale dataset for Complex KBQA. We generate questions, SPARQLs, and functional programs with recursive templates and then paraphrase the questions by crowdsourcing, giving rise to around 120K diverse instances. The SPARQLs and programs depict the reasoning processes in various manners, which can benefit a large spectrum of QA methods. We contribute a unified codebase and conduct extensive evaluations for baselines and state-of-the-arts: a blind GRU obtains 31.58\\%, the best model achieves only 35.15\\%, and humans top at 97.5\\%, which offers great research potential to fill the gap.', 'corpus_id': 220403560, 'score': 0}, {'doc_id': '219982272', 'title': 'Evaluating German Transformer Language Models with Syntactic Agreement Tests', 'abstract': 'Pre-trained transformer language models (TLMs) have recently refashioned natural language processing (NLP): Most state-of-the-art NLP models now operate on top of TLMs to benefit from contextualization and knowledge induction. To explain their success, the scientific community conducted numerous analyses. Besides other methods, syntactic agreement tests were utilized to analyse TLMs. Most of the studies were conducted for the English language, however. In this work, we analyse German TLMs. To this end, we design numerous agreement tasks, some of which consider peculiarities of the German language. Our experimental results show that state-of-the-art German TLMs generally perform well on agreement tasks, but we also identify and discuss syntactic structures that push them to their limits.', 'corpus_id': 219982272, 'score': 0}, {'doc_id': '220381370', 'title': 'What Gives the Answer Away? Question Answering Bias Analysis on Video QA Datasets', 'abstract': ""Question answering biases in video QA datasets can mislead multimodal model to overfit to QA artifacts and jeopardize the model's ability to generalize. Understanding how strong these QA biases are and where they come from helps the community measure progress more accurately and provide researchers insights to debug their models. In this paper, we analyze QA biases in popular video question answering datasets and discover pretrained language models can answer 37-48% questions correctly without using any multimodal context information, far exceeding the 20% random guess baseline for 5-choose-1 multiple-choice questions. Our ablation study shows biases can come from annotators and type of questions. Specifically, annotators that have been seen during training are better predicted by the model and reasoning, abstract questions incur more biases than factual, direct questions. We also show empirically that using annotator-non-overlapping train-test splits can reduce QA biases for video QA datasets."", 'corpus_id': 220381370, 'score': 0}, {'doc_id': '218889719', 'title': 'SciSight: Combining faceted navigation and research group detection for COVID-19 exploratory scientific search', 'abstract': 'The COVID-19 pandemic has sparked unprecedented mobilization of scientists, generating a deluge of papers that makes it hard for researchers to keep track and explore new directions. Search engines are designed for targeted queries, not for discovery of connections across a corpus. In this paper, we present SciSight, a system for exploratory search of COVID-19 research integrating two key capabilities: first, exploring associations between biomedical facets automatically extracted from papers (e.g., genes, drugs, diseases, patient outcomes); second, combining textual and network information to search and visualize groups of researchers and their ties. SciSight1 has so far served over 15K users with over 42K page views and 13% returns.', 'corpus_id': 218889719, 'score': 1}, {'doc_id': '212645369', 'title': 'Explanation-Based Tuning of Opaque Machine Learners with Application to Paper Recommendation', 'abstract': ""Research in human-centered AI has shown the benefits of machine-learning systems that can explain their predictions. Methods that allow users to tune a model in response to the explanations are similarly useful. While both capabilities are well-developed for transparent learning models (e.g., linear models and GA2Ms), and recent techniques (e.g., LIME and SHAP) can generate explanations for opaque models, no method currently exists for tuning of opaque models in response to explanations. This paper introduces LIMEADE, a general framework for tuning an arbitrary machine learning model based on an explanation of the model's prediction. We apply our framework to Semantic Sanity, a neural recommender system for scientific papers, and report on a detailed user study, showing that our framework leads to significantly higher perceived user control, trust, and satisfaction."", 'corpus_id': 212645369, 'score': 1}, {'doc_id': '220381288', 'title': 'Exploring Heterogeneous Information Networks via Pre-Training', 'abstract': 'To explore heterogeneous information networks (HINs), network representation learning (NRL) is proposed, which represents a network in a low-dimension space. Recently, graph neural networks (GNNs) have drawn a lot of attention which are very expressive for mining a HIN, while they suffer from low efficiency issue. In this paper, we propose a pre-training and fine-tuning framework PF-HIN to capture the features of a HIN. Unlike traditional GNNs that have to train the whole model for each downstream task, PF-HIN only needs to fine-tune the model using the pre-trained parameters and minimal extra task-specific parameters, thus improving the model efficiency and effectiveness. Specifically, in pre-training phase, we first use a ranking-based BFS strategy to form the input node sequence. Then inspired by BERT, we adopt deep bi-directional transformer encoders to train the model, which is a variant of GNN aggregator that is more powerful than traditional deep neural networks like CNN and LSTM. The model is pre-trained based on two tasks, i.e., masked node modeling (MNM) and adjacent node prediction (ANP). Additionally, we leverage factorized embedding parameterization and cross-layer parameter sharing to reduce the parameters. In fine-tuning stage, we choose four benchmark downstream tasks, i.e., link prediction, similarity search, node classification and node clustering. We use node sequence pairs as input for link prediction and similarity search, and a single node sequence as input for node classification and clustering. The experimental results of the above tasks on four real-world datasets verify the advancement of PF-HIN, as it outperforms state-of-the-art alternatives consistently and significantly.', 'corpus_id': 220381288, 'score': 0}, {'doc_id': '215745352', 'title': 'paper2repo: GitHub Repository Recommendation for Academic Papers', 'abstract': 'GitHub has become a popular social application platform, where a large number of users post their open source projects. In particular, an increasing number of researchers release repositories of source code related to their research papers in order to attract more people to follow their work. Motivated by this trend, we describe a novel item-item cross-platform recommender system, paper2repo, that recommends relevant repositories on GitHub that match a given paper in an academic search system such as Microsoft Academic. The key challenge is to identify the similarity between an input paper and its related repositories across the two platforms, without the benefit of human labeling. Towards that end, paper2repo integrates text encoding and constrained graph convolutional networks (GCN) to automatically learn and map the embeddings of papers and repositories into the same space, where proximity offers the basis for recommendation. To make our method more practical in real life systems, labels used for model training are computed automatically from features of user actions on GitHub. In machine learning, such automatic labeling is often called distant supervision. To the authors’ knowledge, this is the first distant-supervised cross-platform (paper to repository) matching system. We evaluate the performance of paper2repo on real-world data sets collected from GitHub and Microsoft Academic. Results demonstrate that it outperforms other state of the art recommendation methods.', 'corpus_id': 215745352, 'score': 1}]"
196	Dengue and Satellite Images	57816af596f2fbca4d0383375b4b75bb	13847	{}	[{'doc_id': '24961971', 'title': 'Relative risk estimation of dengue disease at small spatial scale', 'abstract': 'BackgroundDengue is a high incidence arboviral disease in tropical countries around the world. Colombia is an endemic country due to the favourable environmental conditions for vector survival and spread. Dengue surveillance in Colombia is based in passive notification of cases, supporting monitoring, prediction, risk factor identification and intervention measures. Even though the surveillance network works adequately, disease mapping techniques currently developed and employed for many health problems are not widely applied. We select the Colombian city of Bucaramanga to apply Bayesian areal disease mapping models, testing the challenges and difficulties of the approach.MethodsWe estimated the relative risk of dengue disease by census section (a geographical unit composed approximately by 1–20 city blocks) for the period January 2008 to December 2015. We included the covariates normalized difference vegetation index (NDVI) and land surface temperature (LST), obtained by satellite images. We fitted Bayesian areal models at the complete period and annual aggregation time scales for 2008–2015, with fixed and space-varying coefficients for the covariates, using Markov Chain Monte Carlo simulations. In addition, we used Cohen’s Kappa agreement measures to compare the risk from year to year, and from every year to the complete period aggregation.ResultsWe found the NDVI providing more information than LST for estimating relative risk of dengue, although their effects were small. NDVI was directly associated to high relative risk of dengue. Risk maps of dengue were produced from the estimates obtained by the modeling process. The year to year risk agreement by census section was sligth to fair.ConclusionThe study provides an example of implementation of relative risk estimation using Bayesian models for disease mapping at small spatial scale with covariates. We relate satellite data to dengue disease, using an areal data approach, which is not commonly found in the literature. The main difficulty of the study was to find quality data for generating expected values as input for the models. We remark the importance of creating population registry at small spatial scale, which is not only relevant for the risk estimation of dengue but also important to the surveillance of all notifiable diseases.', 'corpus_id': 24961971, 'score': 1}, {'doc_id': '124739950', 'title': 'A study on image processing for satellite and digital images', 'abstract': 'The studies of crop forecasting and prediction of plant leaf disease refer to the studies of visually observable patterns of a particular crop and plant. Image processing techniques are used for crop area estimation procedures and crop yield models, based on the application of satellite remote sensing, statistics and other allied disciplines as well as prediction of plant leaf disease based on the digital images. The research intends to take the help of satellite images, digital image, regional plant information, regional disease forecasting, regional crop information, regional crop forecasting, forecasting methods and image processing techniques. This paper presents an overview of how to use satellite and digital imagery for forecasting crop and leaf disease using image processing techniques.', 'corpus_id': 124739950, 'score': 1}, {'doc_id': '229181216', 'title': 'Detection of Anomalies in a Time Series Data using InfluxDB and Python', 'abstract': 'Analysis of water and environmental data is an important aspect of many intelligent water and environmental system applications where inference from such analysis plays a significant role in decision making. Quite often these data that are collected through sensible sensors can be anomalous due to different reasons such as systems breakdown, malfunctioning of sensor detectors, and more. Regardless of their root causes, such data severely affect the results of the subsequent analysis. This paper demonstrates data cleaning and preparation for time-series data and further proposes cost-sensitive machine learning algorithms as a solution to detect anomalous data points in time-series data. The following models: Logistic Regression, Random Forest, Support Vector Machines have been modified to support the cost-sensitive learning which penalizes misclassified samples thereby minimizing the total misclassification cost. Our results showed that Random Forest outperformed the rest of the models at predicting the positive class (i.e anomalies). Applying predictive model improvement techniques like data oversampling seems to provide little or no improvement to the Random Forest model. Interestingly, with recursive feature elimination, we achieved a better model performance thereby reducing the dimensions in the data. Finally, with Influxdb and Kapacitor the data was ingested and streamed to generate new data points to further evaluate the model performance on unseen data, this will allow for early recognition of undesirable changes in the drinking water quality and will enable the water supply companies to rectify on a timely basis whatever undesirable changes abound.', 'corpus_id': 229181216, 'score': 0}, {'doc_id': '231709481', 'title': 'Fire Risk Analysis by using Sentinel-2 Data: The Case Study of the Vesuvius in Campania, Italy', 'abstract': 'As sadly known, forest fires are part of a set of natural disasters that have always affected regions of the world typically characterized by a tropical climate with long periods of drought. However, due to climate changes of the recent years, other regions of our planet that were not affected by this plague have also had to deal with this phenomenon. One of them is certainly the Italian peninsula, and especially the regions of southern Italy. For this reason, the scientific community, and in particular that one of the remote sensing, plays an important role in the development of reliable techniques to provide useful support to the competent authorities. Therefore, in this work, the capability of the Normalized Differential Water Index (NDWI), derived from spaceborne remote sensing (RS) data, is assessed to monitor the forest fires occurred on a specific study area during the summer of 2017: the volcano Vesuvius, near Naples (in Campania, Italy). In particular, the index is obtained from Sentinel-2 multispectral images of the European Space Agency (ESA), which are free of charge and open accessible. Moreover, the twin Sentinel-2 (S-2) sensors allows to overcome some restrictions on time delivery and high frequency observation. These requirements are goodly matched by other spaceborne sensors, such as MODIS and VIIRS satellites, but at the expense of a lower spatial resolution.', 'corpus_id': 231709481, 'score': 0}, {'doc_id': '2998106', 'title': 'Remote sensing and epidemiology: examples of applications for two vector-borne diseases.', 'abstract': 'Remote sensing techniques have greatly contributed to improve our capacity to observe our environment and its processes. For about 15 years, the use of satellite images for epidemiological purposes has been largely promoted to determine diseases distributions and their variations through time. In some circumstances, when diseases are strongly related to environmental data such as climate, vegetation or land-use, radiation values can be included in prediction models. In other cases, remote sensing data provide information for drawing thematic layers involved in the epidemiological processes, which may differ according to the different ecotypes and ecosystems. According to its final goal, the users can choose from the panel of available radiometers with specific characteristics including spatial resolution and frequency of data. In this paper, two examples of major vector-borne diseases, namely Animal Trypanosomosis and Bluetongue, illustrate these applications.', 'corpus_id': 2998106, 'score': 1}, {'doc_id': '12983957', 'title': 'Hepatocellular adenoma management: call for shared guidelines and multidisciplinary approach.', 'abstract': 'Hepatocellular adenomas are rare benign nodules developed mainly in women taking oral contraceptives. They are solitary or multiple. Their size is highly variable. There is no consensus in the literature for their management except that once their size exceeds 5 cm nodules are taken out to prevent 2 major complications: bleeding and malignant transformation. There are exceptions particularly in men where it is recommended to remove smaller nodules. Since the beginning of this century, major scientific contributions have unveiled the heterogeneity of the disease. HCA are composed of four major subtypes. HNF1A (coding for hepatocyte nuclear factor 1a) inactivating mutations (H-HCA); inflammatory adenomas (IHCA); the β-catenin-mutated HCAs (β-HCA) and unclassified HCA (UHCA) occurring in 30-40%, 40-50%, 10-15% and 10% of all HCA, respectively. Half of β-HCAs are also inflammatory (β-IHCA). Importantly, β-catenin mutations are associated with a high risk of malignant transformation. HCA subtypes can be identified on liver tissue, including biopsies using specific immunomarkers with a good correspondence with molecular data. Recent data has shown that TERT promoter mutation was a late event in the malignant transformation of β-HCA, β-IHCA. Furthermore, in addition to β-catenin exon 3 mutations, other mutations do exist (exon 7 and 8) with a lower risk of malignant transformation. With these new scientific informations, we have the tools to better know the natural history of the different subtypes, in terms of growth, disappearance, bleeding, malignant transformation and to investigate HCA in diseased livers (vascular diseases, alcoholic cirrhosis). A better knowledge of HCA should lead to a more rational management of HCA. This can be done only if the different subspecialties, including hepatologists, liver pathologists, radiologists and surgeons work altogether in close relationship with molecular biologists. It is a long way to go.', 'corpus_id': 12983957, 'score': 0}, {'doc_id': '231654057', 'title': 'Prediction modelling of COVID using machine learning methods from B-cell dataset', 'abstract': '\n Coronavirus is a pandemic that has become a concern for the whole world. This disease has stepped out to its greatest extent and is expanding day by day. Coronavirus, termed as a worldwide disease, has caused more than 8 lakh deaths worldwide. The foremost cause of the spread of coronavirus is SARS-CoV and SARS-CoV-2, which are part of the coronavirus family. Thus, predicting the patients suffering from such pandemic diseases would help to formulate the difference in inaccurate and infeasible time duration. This paper mainly focuses on the prediction of SARS-CoV and SARS-CoV-2 using the B-cells dataset. The paper also proposes different ensemble learning strategies that came out to be beneficial while making predictions. The predictions are made using various machine learning models. The numerous machine learning models, such as SVM, Naïve Bayes, K-nearest neighbors, AdaBoost, Gradient boosting, XGBoost, Random forest, ensembles, and neural networks are used in predicting and analyzing the dataset. The most accurate result was obtained using the proposed algorithm with 0.919 AUC score and 87.248% validation accuracy for predicting SARS-CoV and 0.923 AUC and 87.7934% validation accuracy for predicting SARS-CoV-2 virus.\n', 'corpus_id': 231654057, 'score': 0}, {'doc_id': '88508865', 'title': 'Consideration about Ozone Generation Method Using Electrical Discharge', 'abstract': 'The author has considered an ozone generation method using electrical discharge by means of separating the ozone generation process into an oxygen atom formation process and an ozone formation process. In the oxygen atom formation process, a nonequilibrium electric discharge dissociates oxygen molecules into oxygen atoms efficiently under a low gas pressure (less than a few tens [Tory]), and a high gas temperature (more than 700[k]). The oxygen atoms have a considerable long life time under such a low gas pressure and a high gas temperature. In the ozone formation process, the oxygen atoms are mixed with air, the 2nd material gas, to be efficiently converted into ozone molecules by three-body collisions under a high gas pressure (more than 760[Torr]) and a low gas temperature (less than 350[k]). The concept of a novel ozone generator with 0/03 different space generation system is constructed with combining the oxygen formation process and the ozone formation process without loss of oxygen atoms during transportation. According to the simulation result, it can generate the 03 of 0.498[mol%] with the efficiency of 280[g/kwh] by 1 stage reaction system and the 03 of 2.93[mol%] with the efficiency of 172[g/kwh] by 4 stage one. In comparison with the silent discharge ozone generator, 0/03 same-space-generation system, the 0/03 different-space-generation system has the following advantages; (1) Reduction of energy loss by ion current, (2) Removal of ozone destruction by electrons in the discharge space, (3) Reduction of oxygen material gas by replacing the large part of the oxygen material gas into air.', 'corpus_id': 88508865, 'score': 0}, {'doc_id': '102483279', 'title': 'Deep Landscape Features for Improving Vector-borne Disease Prediction', 'abstract': 'The global population at risk of mosquito-borne diseases such as dengue, yellow fever, chikungunya and Zika is expanding. Infectious disease models commonly incorporate environmental measures like temperature and precipitation. Given increasing availability of high-resolution satellite imagery, here we consider including landscape features from satellite imagery into infectious disease prediction models. To do so, we implement a Convolutional Neural Network (CNN) model trained on Imagenet data and labelled landscape features in satellite data from London. We then incorporate landscape features from satellite image data from Pakistan, labelled using the CNN, in a well-known Susceptible-Infectious-Recovered epidemic model, alongside dengue case data from 2012-2016 in Pakistan. We study improvement of the prediction model for each of the individual landscape features, and assess the feasibility of using image labels from a different place. We find that incorporating satellite-derived landscape features can improve prediction of outbreaks, which is important for proactive and strategic surveillance and control programmes.', 'corpus_id': 102483279, 'score': 1}, {'doc_id': '19186615', 'title': 'Modeling Dengue Vector Population Using Remotely Sensed Data and Machine Learning', 'abstract': 'Mosquitoes are vectors of many human diseases. In particular, Aedes ægypti (Linnaeus) is the main vector for Chikungunya, Dengue, and Zika viruses in Latin America and it represents a global threat. Public health policies that aim at combating this vector require dependable and timely information, which is usually expensive to obtain with field campaigns. For this reason, several efforts have been done to use remote sensing due to its reduced cost. The present work includes the temporal modeling of the oviposition activity (measured weekly on 50 ovitraps in a north Argentinean city) of Aedes ægypti (Linnaeus), based on time series of data extracted from operational earth observation satellite images. We use are NDVI, NDWI, LST night, LST day and TRMM-GPM rain from 2012 to 2016 as predictive variables. In contrast to previous works which use linear models, we employ Machine Learning techniques using completely accessible open source toolkits. These models have the advantages of being non-parametric and capable of describing nonlinear relationships between variables. Specifically, in addition to two linear approaches, we assess a support vector machine, an artificial neural networks, a K-nearest neighbors and a decision tree regressor. Considerations are made on parameter tuning and the validation and training approach. The results are compared to linear models used in previous works with similar data sets for generating temporal predictive models. These new tools perform better than linear approaches, in particular nearest neighbor regression (KNNR) performs the best. These results provide better alternatives to be implemented operatively on the Argentine geospatial risk system that is running since 2012.', 'corpus_id': 19186615, 'score': 1}]
197	Poetry generation	99ca2fe32b9d3d988e8ad7c8992f627c	3162	{}	"[{'doc_id': '212628796', 'title': 'EmpTransfo: A Multi-head Transformer Architecture for Creating Empathetic Dialog Systems', 'abstract': 'Understanding emotions and responding accordingly is one of the biggest challenges of dialog systems. This paper presents EmpTransfo, a multi-head Transformer architecture for creating an empathetic dialog system. EmpTransfo utilizes state-of-the-art pre-trained models (e.g., OpenAI-GPT) for language generation, though models with different sizes can be used. We show that utilizing the history of emotions and other metadata can improve the quality of generated conversations by the dialog system. Our experimental results using a challenging language corpus show that the proposed approach outperforms other models in terms of Hit@1 and PPL (Perplexity).', 'corpus_id': 212628796, 'score': 0}, {'doc_id': '211043879', 'title': 'Stimulating Creativity with FunLines: A Case Study of Humor Generation in Headlines', 'abstract': 'Building datasets of creative text, such as humor, is quite challenging. We introduce FunLines, a competitive game where players edit news headlines to make them funny, and where they rate the funniness of headlines edited by others. FunLines makes the humor generation process fun, interactive, collaborative, rewarding and educational, keeping players engaged and providing humor data at a very low cost compared to traditional crowdsourcing approaches. FunLines offers useful performance feedback, assisting players in getting better over time at generating and assessing humor, as our analysis shows. This helps to further increase the quality of the generated dataset. We show the effectiveness of this data by training humor classification models that outperform a previous benchmark, and we release this dataset to the public.', 'corpus_id': 211043879, 'score': 0}, {'doc_id': '198313274', 'title': 'A Brief Introduction to Natural Language Generation within Computational Creativity', 'abstract': 'This paper is an introduction to The 3rd Workshop on Computational Creativity in Language Generation (CC-NLG 2018), collocated with The 11th International Conference on Natural Language Generation (INLG 2018). The workshop acts as an overview of projects from the Computational Creativity field present within the general field of Natural Language Generation, and references further reviews within the area. Accepted papers to the workshop will use contemporary NLG methods or approach NLG problems from a creative perspective. The workshop intends to showcase creative applications as a worthwhile and meaningful pursuit to other NLG researchers.', 'corpus_id': 198313274, 'score': 1}, {'doc_id': '215416194', 'title': 'Conditional Rap Lyrics Generation with Denoising Autoencoders', 'abstract': 'We develop a method for automatically synthesizing a rap verse given an input text written in another form, such as a summary of a news article. Our approach is to train a Transformer-based denoising autoencoder to reconstruct rap lyrics from content words. We study three different approaches for automatically stripping content words that convey the essential meaning of the lyrics. Moreover, we propose a BERT-based paraphrasing scheme for rhyme enhancement and show that it increases the average rhyme density of the lyrics by 10%. Experimental results on three diverse input domains -- existing rap lyrics, news, and movie plot summaries -- show that our method is capable of generating coherent and technically fluent rap verses that preserve the input content words. Human evaluation demonstrates that our approach gives a good trade-off between content preservation and style transfer compared to a strong information retrieval baseline.', 'corpus_id': 215416194, 'score': 1}, {'doc_id': '212734256', 'title': 'PO-EMO: Conceptualization, Annotation, and Modeling of Aesthetic Emotions in German and English Poetry', 'abstract': 'Most approaches to emotion analysis of social media, literature, news, and other domains focus exclusively on basic emotion categories as defined by Ekman or Plutchik. However, art (such as literature) enables engagement in a broader range of more complex and subtle emotions. These have been shown to also include mixed emotional responses. We consider emotions in poetry as they are elicited in the reader, rather than what is expressed in the text or intended by the author. Thus, we conceptualize a set of aesthetic emotions that are predictive of aesthetic appreciation in the reader, and allow the annotation of multiple labels per line to capture mixed emotions within their context. We evaluate this novel setting in an annotation experiment both with carefully trained experts and via crowdsourcing. Our annotation with experts leads to an acceptable agreement of k = .70, resulting in a consistent dataset for future large scale analysis. Finally, we conduct first emotion classification experiments based on BERT, showing that identifying aesthetic emotions is challenging in our data, with up to .52 F1-micro on the German subset. Data and resources are available at https://github.com/tnhaider/poetry-emotion.', 'corpus_id': 212734256, 'score': 1}, {'doc_id': '211066248', 'title': 'Introducing Aspects of Creativity in Automatic Poetry Generation', 'abstract': 'Poetry Generation involves teaching systems to automatically generate text that resembles poetic work. A deep learning system can learn to generate poetry on its own by training on a corpus of poems and modeling the particular style of language. In this paper, we propose taking an approach that fine-tunes GPT-2, a pre-trained language model, to our downstream task of poetry generation. We extend prior work on poetry generation by introducing creative elements. Specifically, we generate poems that express emotion and elicit the same in readers, and poems that use the language of dreams—called dream poetry. We are able to produce poems that correctly elicit the emotions of sadness and joy 87.5 and 85 percent, respectively, of the time. We produce dreamlike poetry by training on a corpus of texts that describe dreams. Poems from this model are shown to capture elements of dream poetry with scores of no less than 3.2 on the Likert scale. We perform crowdsourced human-evaluation for all our poems. We also make use of the Coh-Metrix tool, outlining metrics we use to gauge the quality of text generated.', 'corpus_id': 211066248, 'score': 1}, {'doc_id': '215745354', 'title': 'You Impress Me: Dialogue Generation via Mutual Persona Perception', 'abstract': 'Despite the continuing efforts to improve the engagingness and consistency of chit-chat dialogue systems, the majority of current work simply focus on mimicking human-like responses, leaving understudied the aspects of modeling understanding between interlocutors. The research in cognitive science, instead, suggests that understanding is an essential signal for a high-quality chit-chat conversation. Motivated by this, we propose Pˆ2 Bot, a transmitter-receiver based framework with the aim of explicitly modeling understanding. Specifically, Pˆ2 Bot incorporates mutual persona perception to enhance the quality of personalized dialogue generation. Experiments on a large public dataset, Persona-Chat, demonstrate the effectiveness of our approach, with a considerable boost over the state-of-the-art baselines across both automatic metrics and human evaluations.', 'corpus_id': 215745354, 'score': 0}, {'doc_id': '214641015', 'title': 'Generating Major Types of Chinese Classical Poetry in a Uniformed Framework', 'abstract': 'Poetry generation is an interesting research topic in the field of text generation. As one of the most valuable literary and cultural heritages of China, Chinese classical poetry is very familiar and loved by Chinese people from generation to generation. It has many particular characteristics in its language structure, ranging from form, sound to meaning, thus is regarded as an ideal testing task for text generation. In this paper, we propose a GPT-2 based uniformed framework for generating major types of Chinese classical poems. We define a unified format for formulating all types of training samples by integrating detailed form information, then present a simple form- stressed weighting method in GPT-2 to strengthen the control to the form of the generated poems, with special emphasis on those forms with longer body length. Preliminary experimental results show this enhanced model can generate Chinese classical poems of major types with high quality in both form and content, validating the effectiveness of the proposed strategy. The model has been incorporated into Jiuge, the most influential Chinese classical poetry generation system developed by Tsinghua University.', 'corpus_id': 214641015, 'score': 1}, {'doc_id': '210932334', 'title': 'Bringing Stories Alive: Generating Interactive Fiction Worlds', 'abstract': 'World building forms the foundation of any task that requires narrative intelligence. In this work, we focus on procedurally generating interactive fiction worlds---text-based worlds that players ""see"" and ""talk to"" using natural language. Generating these worlds requires referencing everyday and thematic commonsense priors in addition to being semantically consistent, interesting, and coherent throughout. Using existing story plots as inspiration, we present a method that first extracts a partial knowledge graph encoding basic information regarding world structure such as locations and objects. This knowledge graph is then automatically completed utilizing thematic knowledge and used to guide a neural language generation model that fleshes out the rest of the world. We perform human participant-based evaluations, testing our neural model\'s ability to extract and fill-in a knowledge graph and to generate language conditioned on it against rule-based and human-made baselines. Our code is available at this https URL.', 'corpus_id': 210932334, 'score': 0}, {'doc_id': '214802375', 'title': 'PROTOTYPE-TO-STYLE: Dialogue Generation With Style-Aware Editing on Retrieval Memory', 'abstract': 'The ability of dialogue systems to express pre-specified style during conversations has a direct, positive impact on their usability and user satisfaction. While it has attracted much research interest, existing methods often generate stylistic responses at the cost of content quality. In this work, we introduce a prototype-to-style (PS) framework to tackle the challenge of stylistic dialogue generation. The proposed framework first exploits an Information Retrieval (IR) system and extracts a response prototype from the retrieved response. A stylistic response generator then takes the response prototype and the desired style as input to produce a high-quality and stylistic response. To effectively train the proposed model and imitate the real testing environment, we introduce a new style-aware learning objective and a denoising learning strategy. Results on three benchmark datasets (gender, emotion, and sentiment) from two languages demonstrate that the proposed approach significantly outperforms existing baselines both in terms of in-domain and cross-domain evaluations.', 'corpus_id': 214802375, 'score': 0}]"
198	Sleep model	436cb9e596d35c105efa3ecfce26d784	3414	{}	"[{'doc_id': '46487092', 'title': 'Sleep and synaptic homeostasis: a hypothesis', 'abstract': 'During much of sleep, the cerebral cortex is rippled by slow waves, which appear in the electroencephalogram as oscillations between 0.5 and 4.5 Hz. Slow waves are regulated as a function of previous wakefulness, being maximal at the beginning of sleep and then progressively returning to a baseline level. This paper discusses a hypothesis about the significance of slow-wave activity and its homeostatic regulation. The hypothesis is as follows: 1. Wakefulness is associated with synaptic potentiation in several cortical circuits; 2. Synaptic potentiation is tied to the homeostatic regulation of slow-wave activity; 3. Slow-wave activity is associated with synaptic downscaling; 4. Synaptic downscaling is tied to the beneficial effects of sleep on performance. The hypothesized link between sleep and synaptic homeostasis is supported by several lines of evidence and leads to testable predictions.', 'corpus_id': 46487092, 'score': 1}, {'doc_id': '14700979', 'title': 'Sleep and synaptic renormalization: a computational study.', 'abstract': 'Recent evidence indicates that net synaptic strength in cortical and other networks increases during wakefulness and returns to a baseline level during sleep. These homeostatic changes in synaptic strength are accompanied by corresponding changes in sleep slow wave activity (SWA) and in neuronal firing rates and synchrony. Other evidence indicates that sleep is associated with an initial reactivation of learned firing patterns that decreases over time. Finally, sleep can enhance performance of learned tasks, aid memory consolidation, and desaturate the ability to learn. Using a large-scale model of the corticothalamic system equipped with a spike-timing dependent learning rule, in agreement with experimental results, we demonstrate a net increase in synaptic strength in the waking mode associated with an increase in neuronal firing rates and synchrony. In the sleep mode, net synaptic strength decreases accompanied by a decline in SWA. We show that the interplay of activity and plasticity changes implements a control loop yielding an exponential, self-limiting renormalization of synaptic strength. Moreover, when the model ""learns"" a sequence of activation during waking, the learned sequence is preferentially reactivated during sleep, and reactivation declines over time. Finally, sleep-dependent synaptic renormalization leads to increased signal-to-noise ratios, increased resistance to interference, and desaturation of learning capabilities. Although the specific mechanisms implemented in the model cannot capture the variety and complexity of biological substrates, and will need modifications in line with future evidence, the present simulations provide a unified, parsimonious account for diverse experimental findings coming from molecular, electrophysiological, and behavioral approaches.', 'corpus_id': 14700979, 'score': 1}, {'doc_id': '211126796', 'title': 'What is the function of inter-hemispheric inhibition?.', 'abstract': ""It is widely supposed that following unilateral brain injury, there arises an asymmetry in inter-hemispheric inhibition which has an adverse influence upon motor control. I argue that this 'inter-hemispheric imbalance' model arises from a fundamental misunderstanding of the roles played by inter-hemispheric (callosal) projections in mammalian brains. Drawing upon a large body of empirical data, derived largely from animal models, and associated theoretical modeling, it is demonstrated that inter-hemispheric projections perform contrast enhancing and integrative functions via mechanisms such as surround/lateral inhibition. The principal functional unit of callosal influence comprises a facilitatory centre and a depressing peripheral zone, that together shape the influence of converging inputs to pyramidal neurons. Inter-hemispheric inhibition is an instance of a more general feature of mammalian neural systems, whereby inhibitory interneurons act not simply to prevent over-excitation but to sculpt the output of specific circuits. The narrowing of the excitatory focus that occurs through crossed surround inhibition is a highly conserved motif of transcallosal interactions in mammalian sensory and motor cortices. A case is presented that the notion of 'inter-hemispheric imbalance' has been sustained, and clinical interventions derived from this model promoted, by erroneous assumptions concerning that revealed by investigative techniques such as transcranial magnetic stimulation (TMS). The alternative perspective promoted by the present analysis, also permits the basis of positive (e.g. post stroke) associations between the structural integrity of transcallosal projections and motor capability to be better understood."", 'corpus_id': 211126796, 'score': 0}, {'doc_id': '12913031', 'title': 'Mathematical models of sleep regulation.', 'abstract': 'The level of EEG slow-wave activity (SWA) is determined by the duration of prior sleep and waking. SWA is a marker of nonREM sleep intensity and may serve as an indicator of sleep homeostasis. The two-process model of sleep regulation posits the interaction of the homeostatic Process S and the circadian Process C. Also models of neurobehavioral functions (three-process model; interactive models of alertness and cognitive throughput) are based on the concept of an interaction between homeostatic and circadian factors. Whether the interaction is linear or non-linear is still unresolved. Models may serve as a guiding principle for specifying the relationship between processes occurring at the macroscopic and microscopic level of analysis.', 'corpus_id': 12913031, 'score': 1}, {'doc_id': '211818363', 'title': 'Night-to-night variability of sleep electroencephalography-based brain age measurements', 'abstract': ""OBJECTIVE\nBrain Age Index (BAI), calculated from sleep electroencephalography (EEG), has been proposed as a biomarker of brain health. This study quantifies night-to-night variability of BAI and establishes probability thresholds for inferring underlying brain pathology based on a patient's BAI.\n\n\nMETHODS\n86 patients with multiple nights of consecutive EEG recordings were selected from Epilepsy Monitoring Unit patients whose EEGs reported as within normal limits. While EEGs with epileptiform activity were excluded, the majority of patients included in the study had a diagnosis of chronic epilepsy. BAI was calculated for each 12-hour segment of patient data using a previously established algorithm, and the night-to-night variability in BAI was measured.\n\n\nRESULTS\nThe within-patient night-to-night standard deviation in BAI was 7.5\xa0years. Estimates of BAI derived by averaging over 2, 3, and 4 nights had standard deviations of 4.7, 3.7, and 3.0\xa0years, respectively.\n\n\nCONCLUSIONS\nAveraging BAI over n nights reduces night-to-night variability of BAI by a factor of n, rendering BAI a more suitable biomarker of brain health at the individual level. A brain age risk lookup table of results provides thresholds above which a patient has a high probability of excess BAI.\n\n\nSIGNIFICANCE\nWith increasing ease of EEG acquisition, including wearable technology, BAI has the potential to track brain health and detect deviations from normal physiologic function. The measure of night-to-night variability and how this is reduced by averaging across multiple nights provides a basis for using BAI in patients' homes to identify patients who should undergo further investigation or monitoring."", 'corpus_id': 211818363, 'score': 0}, {'doc_id': '269890', 'title': 'Toward a detailed computational model for the mammalian circadian clock', 'abstract': 'We present a computational model for the mammalian circadian clock based on the intertwined positive and negative regulatory loops involving the Per, Cry, Bmal1, Clock, and Rev-Erb α genes. In agreement with experimental observations, the model can give rise to sustained circadian oscillations in continuous darkness, characterized by an antiphase relationship between Per/Cry/Rev-Erbα and Bmal1 mRNAs. Sustained oscillations correspond to the rhythms autonomously generated by suprachiasmatic nuclei. For other parameter values, damped oscillations can also be obtained in the model. These oscillations, which transform into sustained oscillations when coupled to a periodic signal, correspond to rhythms produced by peripheral tissues. When incorporating the light-induced expression of the Per gene, the model accounts for entrainment of the oscillations by light-dark cycles. Simulations show that the phase of the oscillations can then vary by several hours with relatively minor changes in parameter values. Such a lability of the phase could account for physiological disorders related to circadian rhythms in humans, such as advanced or delayed sleep phase syndrome, whereas the lack of entrainment by light-dark cycles can be related to the non-24h sleep-wake syndrome. The model uncovers the possible existence of multiple sources of oscillatory behavior. Thus, in conditions where the indirect negative autoregulation of Per and Cry expression is inoperative, the model indicates the possibility that sustained oscillations might still arise from the negative autoregulation of Bmal1 expression.', 'corpus_id': 269890, 'score': 1}, {'doc_id': '206027570', 'title': 'The sleep switch: hypothalamic control of sleep and wakefulness', 'abstract': 'More than 70 years ago, von Economo predicted a wake-promoting area in the posterior hypothalamus and a sleep-promoting region in the preoptic area. Recent studies have dramatically confirmed these predictions. The ventrolateral preoptic nucleus contains GABAergic and galaninergic neurons that are active during sleep and are necessary for normal sleep. The posterior lateral hypothalamus contains orexin/hypocretin neurons that are crucial for maintaining normal wakefulness. A model is proposed in which wake- and sleep-promoting neurons inhibit each other, which results in stable wakefulness and sleep. Disruption of wake- or sleep-promoting pathways results in behavioral state instability.', 'corpus_id': 206027570, 'score': 1}, {'doc_id': '1168675', 'title': 'Foundation of a computable solid modeling', 'abstract': 'Solid modelling and computational geometry are based on classical topology and geometry in which the basic predicates and operations, such as membership, subset inclusion, union and intersection, are not continuous and therefore not computable. But a sound computational framework for solids and geometry can only be built in a framework with computable predicates and operations. In practice, correctness of algorithms in computational geometry is usually proved using the unrealistic Real RAM machine model of computation, which allows comparison of real numbers, with the undesirable result that correct algorithms, when implemented, turn into unreliable programs. Here, we use a domain-theoretic approach to recursive analysis to develop the basis of an e3ective and realistic framework for solid modelling. This framework is equipped with a well de5ned and realistic notion of computability which re6ects the observable properties of real solids. The basic predicates and operations on solids are computable in this model which admits regular and non-regular sets and supports a design methodology for actual robust algorithms. Moreover, the model is able to capture the uncertainties of input data in actual CAD situations. c 2002 Elsevier Science B.V. All rights reserved.', 'corpus_id': 1168675, 'score': 0}, {'doc_id': '215548638', 'title': 'State-dependent regulation of cortical processing speed via gain modulation', 'abstract': 'To thrive in dynamic environments, animals must be capable of rapidly and flexibly adapting behavioral responses to a changing context and internal state. Examples of behavioral flexibility include faster stimulus responses when attentive and slower responses when distracted. Contextual or state-dependent modulations may occur early in the cortical hierarchy and may be implemented via top-down projections from cortico-cortical or neuromodulatory pathways. However, the computational mechanisms mediating the effects of such projections are not known. Here, we introduce a theoretical framework to classify the effects of cell-type specific top-down perturbations on the information processing speed of cortical circuits. Our theory demonstrates that perturbation effects on stimulus processing can be predicted by intrinsic gain modulation, which controls the timescale of the circuit dynamics. Our theory leads to counter-intuitive effects such as improved performance with increased input variance. We tested the model predictions using large-scale electrophysiological recordings from the visual hierarchy in freely running mice, where we found that a decrease in single-cell intrinsic gain during locomotion led to an acceleration of visual processing. Our results establish a novel theory of cell-type specific perturbations, applicable to top-down modulation as well as optogenetic and pharmacological manipulations. Our theory links connectivity, dynamics, and information processing via gain modulation.', 'corpus_id': 215548638, 'score': 0}, {'doc_id': '214641382', 'title': 'Effect of Diverse Recoding of Granule Cells on Optokinetic Response in A Cerebellar Ring Network with Synaptic Plasticity', 'abstract': 'We consider a cerebellar ring network for the optokinetic response (OKR), and investigate the effect of diverse recoding of granule (GR) cells on OKR by varybing the connection probability pc from Golgi to GR cells. For an optimal value of , individual GR cells exhibit diverse spiking patterns which are in-phase, anti-phase, or complex out-of-phase with respect to their population-averaged firing activity. Then, these diversely-recoded signals via parallel fibers (PFs) from GR cells are effectively depressed by the error-teaching signals via climbing fibers from the inferior olive which are also in-phase ones. Synaptic weights at in-phase PF-Purkinje cell (PC) synapses of active GR cells are strongly depressed via strong long-term depression (LTD), while those at anti-phase and complex out-of-phase PF-PC synapses are weakly depressed through weak LTD. This kind of “effective” depression (i.e., strong/weak LTD) at the PF-PC synapses causes a big modulation in firings of PCs, which then exert effective inhibitory coordination on the vestibular nucleus (VN) neuron (which evokes OKR). For the firing of the VN neuron, the learning gain degree ℒg, corresponding to the modulation gain ratio, increases with increasing the learning cycle, and it saturates at about the 300th cycle. By varying pc from , we find that a plot of saturated learning gain degree versus pc forms a bell-shaped curve with a peak at (where the diversity degree in spiking patterns of GR cells is also maximum). Consequently, the more diverse in recoding of GR cells, the more effective in motor learning for the OKR adaptation.', 'corpus_id': 214641382, 'score': 0}]"
199	Syocin RF	41e5ac90f26c6c0c35cfc733dbee23bb	8548	{}	"[{'doc_id': '92671064', 'title': 'Bacteriocins active against plant pathogenic bacteria. Biochem Soc Trans', 'abstract': None, 'corpus_id': 92671064, 'score': 1}, {'doc_id': '58553652', 'title': 'Bacteriocins: Classification, synthesis, mechanism of action and resistance development in food spoilage causing bacteria.', 'abstract': 'Huge demand of safe and natural preservatives has opened new area for intensive research on bacteriocins to unravel the novel range of antimicrobial compounds that could efficiently fight off the food-borne pathogens. Since food safety has become an increasingly important international concern, the application of bacteriocins from lactic acid bacteria that target food spoilage/pathogenic bacteria without major adverse effects has received great attention. Different modes of actions of these bacteriocins have been suggested and identified, like pore-forming, inhibition of cell-wall/nucleic acid/protein synthesis. However, development of resistance in the food spoilage and pathogenic bacteria against these bacteriocins is a rising concern. Emergence and spread of mutant strains resistant to bacteriocins is hampering food safety. It has spurred an interest to understand the bacteriocin resistance phenomenon displayed by the food pathogens, which will be helpful in mitigating the resistance problem. Therefore, present review is focused on the different resistance mechanisms adopted by food pathogens to overcome bacteriocin.', 'corpus_id': 58553652, 'score': 1}, {'doc_id': '221691975', 'title': 'Impact of Dietary Modification on Microbiome:Exploring Therapeutic Implications', 'abstract': 'The Human Gut Microbiome: The gastro-intestinal tract and various other organs harbour large and diverse communities of bacteria, viruses, and other microscopic life. In the human gut, there inhabit microbial members as residents (autochthonous), while others (allochthonous) are from ingested food, water and other components of the environment. The adult human gut microbiota is dominated by mainly two bacteria, the Bacteroidetes and Firmicutes and an archaea, Metanobrevibacter smithii.', 'corpus_id': 221691975, 'score': 0}, {'doc_id': '792798', 'title': 'Bacteriocins from the rhizosphere microbiome – from an agriculture perspective', 'abstract': 'Bacteria produce and excrete a versatile and dynamic suit of compounds to defend against microbial competitors and mediate local population dynamics. These include a wide range of broad-spectrum non-ribosomally synthesized antibiotics, lytic enzymes, metabolic by-products, proteinaceous exotoxins, and ribosomally produced antimicrobial peptides (bacteriocins). Most bacteria produce at least one bacteriocin. Bacteriocins are of interest in the food industry as natural preservatives and in the probiotics industry, leading to extensive studies on lactic acid bacteria (colicin produced by Escherichia coli is a model bacteriocin). Recent studies have projected use of bacteriocins in veterinary medicine and in agriculture, as biostimulants of plant growth and development and as biocontrol agents. For example, bacteriocins such as Cerein 8A, Bac-GM17, putidacin, Bac 14B, amylocyclicin have been studied for their mechanisms of anti-microbial activity. Bac IH7 promotes tomato and musk melon plant growth. Thuricin 17 (Th17) is the only bacteriocin studied extensively for plant growth promotion, including at the molecular level. Th17 functions as a bacterial signal compound, promoting plant growth in legumes and non-legumes. In Arabidopsis thaliana and Glycine max Th17 increased phytohormones IAA and SA at 24 h post treatment. At the proteome level Th17 treatment of 3-week-old A. thaliana rosettes led to >2-fold changes in activation of the carbon and energy metabolism pathway proteins, 24 h post treatment. At 250 mM NaCl stress, the control plants under osmotic-shock shut down most of carbon-metabolism and activated energy-metabolism and antioxidant pathways. Th17 treated plants, at 250 mM NaCl, retained meaningful levels of the light harvesting complex, photosystems I and II proteins and energy and antioxidant pathways were activated, so that rosettes could better withstand the salt stress. In Glycine max, Th17 helped seeds germinate in the presence of NaCl stress, and was most effective at 100 mM NaCl. The 48 h post germination proteome suggested efficient and speedier partitioning of storage proteins, activation of carbon, nitrogen and energy metabolisms in Th17 treated seeds both under optimal and 100 mM NaCl. This review focuses on the bacteriocins produced by plant-rhizosphere colonizers and plant-pathogenic bacteria, that might have uses in agriculture, veterinary, and human medicine.', 'corpus_id': 792798, 'score': 1}, {'doc_id': '10657033', 'title': 'Bacteriocins active against plant pathogenic bacteria.', 'abstract': 'Gram-negative phytopathogens cause significant losses in a diverse range of economically important crop plants. The effectiveness of traditional countermeasures, such as the breeding and introduction of resistant cultivars, is often limited by the dearth of available sources of genetic resistance. An alternative strategy to reduce loss to specific bacterial phytopathogens is to use narrow-spectrum protein antibiotics such as colicin-like bacteriocins as biocontrol agents. A number of colicin-like bacteriocins active against phytopathogenic bacteria have been described previously as have strategies for their application to biocontrol. In the present paper, we discuss these strategies and our own recent work on the identification and characterization of candidate bacteriocins and how these potent and selective antimicrobial agents can be effectively applied to the control of economically important plant disease.', 'corpus_id': 10657033, 'score': 1}, {'doc_id': '210871744', 'title': 'Antimicrobials for food and feed; a bacteriocin perspective.', 'abstract': 'Bacteriocins are natural antimicrobials that have been consumed via fermented foods for millennia and have been the focus of renewed efforts to identify novel bacteriocins, and their producing microorganisms, for use as food biopreservatives and other applications. Bioengineering bacteriocins or combining bacteriocins with multiple modes of action (hurdle approach) can enhance their preservative effect and reduces the incidence of antimicrobial resistance. In addition to their role as food biopreservatives, bacteriocins are gaining credibility as health modulators, due to their ability to regulate the gut microbiota, which is strongly associated with human wellbeing. Indeed the strengthening link between the gut microbiota and obesity make bacteriocins ideal alternatives to Animal Growth Promoters (AGP) in animal feed also. Here we review recent advances in bacteriocin research that will contribute to the development of functional foods and feeds as a consequence of roles in food biopreservation and human/animal health.', 'corpus_id': 210871744, 'score': 1}, {'doc_id': '221402862', 'title': 'Probiotics: Versatile Bioactive Components in Promoting Human Health', 'abstract': 'The positive impact of probiotic strains on human health has become more evident than ever before. Often delivered through food, dietary products, supplements, and drugs, different legislations for safety and efficacy issues have been prepared. Furthermore, regulatory agencies have addressed various approaches toward these products, whether they authorize claims mentioning a disease’s diagnosis, prevention, or treatment. Due to the diversity of bacteria and yeast strains, strict approaches have been designed to assess for side effects and post-market surveillance. One of the most essential delivery systems of probiotics is within food, due to the great beneficial health effects of this system compared to pharmaceutical products and also due to the increasing importance of food and nutrition. Modern lifestyle or various diseases lead to an imbalance of the intestinal flora. Nonetheless, as the amount of probiotic use needs accurate calculations, different factors should also be taken into consideration. One of the novelties of this review is the presentation of the beneficial effects of the administration of probiotics as a potential adjuvant therapy in COVID-19. Thus, this paper provides an integrative overview of different aspects of probiotics, from human health care applications to safety, quality, and control.', 'corpus_id': 221402862, 'score': 0}, {'doc_id': '220629959', 'title': 'Artemisia annua, a Traditional Plant Brought to Light', 'abstract': 'Traditional remedies have been used for thousand years for the prevention and treatment of infectious diseases, particularly in developing countries. Of growing interest, the plant Artemisia annua, known for its malarial properties, has been studied for its numerous biological activities including metabolic, anti-tumor, anti-microbial and immunomodulatory properties. Artemisia annua is very rich in secondary metabolites such as monoterpenes, sesquiterpenes and phenolic compounds, of which the biological properties have been extensively studied. The purpose of this review is to gather and describe the data concerning the main chemical components produced by Artemisia annua and to describe the state of the art about the biological activities reported for this plant and its compounds beyond malaria.', 'corpus_id': 220629959, 'score': 0}, {'doc_id': '221728087', 'title': 'Strategies of Plant Biotechnology to Meet the Increasing Demand of Food and Nutrition in India', 'abstract': ""A groundbreaking application of biotechnology research during the recent past has been improvement of crop health and production. India being one of the most rapidly developing countries with an enormous population and remarkable biodiversity, plant biotechnology promises significant potential to contribute to characterization and conservation of the biodiversity, increasing its usefulness. However, India’s green revolution was noted to be insufficient to feed the country's teeming millions. Therefore, novel approaches in crop biotechnology had to be aimed at ensuring better productivity and quality of cultivars. This paper provides a comprehensive review of research undertaken mainly in the last couple of decades along with potential strategies in plant biotechnology focusing on specific grain and seed crops of key agricultural as well as dietary importance to meet the growing demand of food and nutrition in India, while also proposing potential application of relevant global research findings in the Indian context. The analysis would help address the ever-increasing worldwide socioeconomic necessity for greater food security, particularly during times of crisis such as the recent Coronavirus Infectious Disease 2019 (COVID-19) pandemic."", 'corpus_id': 221728087, 'score': 0}, {'doc_id': '221507911', 'title': 'Investigation of the Cyprus donkey milk bacterial diversity by 16SrDNA high-throughput sequencing in a Cyprus donkey farm', 'abstract': 'The interest in milk originating from donkeys is growing worldwide due to its claimed functional and nutritional properties, especially for sensitive population groups, such as infants with cow milk protein allergy. The current study aimed to assess the microbiological quality of donkey milk produced in a donkey farm in Cyprus using culture-based and high-throughput sequencing techniques. The culture-based microbiological analysis showed very low microbial counts, whereas important food-borne pathogens were not detected in any sample. In addition, high-throughput sequencing was applied to characterize the bacterial communities of donkey milk samples. Donkey milk mostly composed of gram-negative Proteobacteria, including Sphingomonas, Pseudomonas, Mesorhizobium, and Acinetobacter; lactic acid bacteria, including Lactobacillus and Streptococcus; the endospores forming Clostridium; and the environmental genera Flavobacterium and Ralstonia, detected in lower relative abundances. The results of the study support existing findings that donkey milk contains mostly gram-negative bacteria. Moreover, it raises questions regarding the contribution of (1) antimicrobial agents (i.e., lysozyme, peptides) in shaping the microbial communities and (2) bacterial microbiota to the functional value of donkey milk.', 'corpus_id': 221507911, 'score': 0}]"
