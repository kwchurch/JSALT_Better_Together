	doc_id	corpus_id	title	abstract	index	retweets	count	mentions
0	1406.6048	118384481	A STATISTICAL RECONSTRUCTION OF THE PLANET POPULATION AROUND KEPLER SOLAR-TYPE STARS	Using the cumulative catalog of planets detected by the NASA Kepler mission, we reconstruct the intrinsic occurrence of Earth- to Neptune-size (1–4 R⊕) planets and their distributions with radius and orbital period. We analyze 76,711 solar-type (0.8 < R*/R☉ < 1.2) stars with 430 planets on 20–200 day orbits, excluding close-in planets that may have been affected by the proximity to the host star. Our analysis considers errors in planet radii and includes an “iterative simulation” technique that does not bin the data. We find a radius distribution that peaks at 2–2.8 Earth radii, with lower numbers of smaller and larger planets. These planets are uniformly distributed with logarithmic period, and the mean number of such planets per star is 0.46 ± 0.03. The occurrence is ∼0.66 if planets interior to 20 days are included. We estimate the occurrence of Earth-size planets in the “habitable zone” (defined as 1–2 R⊕, 0.99–1.7 AU for solar-twin stars) as . Our results largely agree with those of Petigura et al., although we find a higher occurrence of 2.8–4 Earth-radii planets. The reasons for this excess are the inclusion of errors in planet radius, updated Huber et al. stellar parameters, and also the exclusion of planets that may have been affected by proximity to the host star.	72348	0.5882353186607361	2	2.5880000591278076
1	1901.08276	59222778	Traditional and Heavy-Tailed Self Regularization in Neural Network Models	Random Matrix Theory (RMT) is applied to analyze the weight matrices of Deep Neural Networks (DNNs), including both production quality, pre-trained models such as AlexNet and Inception, and smaller models trained from scratch, such as LeNet5 and a miniature-AlexNet. Empirical and theoretical results clearly indicate that the empirical spectral density (ESD) of DNN layer matrices displays signatures of traditionally-regularized statistical models, even in the absence of exogenously specifying traditional forms of regularization, such as Dropout or Weight Norm constraints. Building on recent results in RMT, most notably its extension to Universality classes of Heavy-Tailed matrices, we develop a theory to identify \emph{5+1 Phases of Training}, corresponding to increasing amounts of \emph{Implicit Self-Regularization}. For smaller and/or older DNNs, this Implicit Self-Regularization is like traditional Tikhonov regularization, in that there is a `size scale' separating signal from noise. For state-of-the-art DNNs, however, we identify a novel form of \emph{Heavy-Tailed Self-Regularization}, similar to the self-organization seen in the statistical physics of disordered systems. This implicit Self-Regularization can depend strongly on the many knobs of the training process. By exploiting the generalization gap phenomena, we demonstrate that we can cause a small model to exhibit all 5+1 phases of training simply by changing the batch size.	110021	0.5882353186607361	4	4.5879998207092285
2	1907.03266	195833175	Complexity of planar signed graph homomorphisms to cycles		124327	0.0	3	3.0
3	1110.3368	761095	Viral Evolution and Adaptation as a Multivariate Branching Process	"In the present work we analyze the problem of adaptation and evolution of RNA virus populations, by defining the basic stochastic model as a multivariate branching process in close relation with the branching process advanced by Demetrius, Schuster and Sigmund (""Polynucleotide evolution and branching processes"", Bull. Math. Biol. 46 (1985) 239-262), in their study of polynucleotide evolution. We show that in the absence of beneficial forces the model is exactly solvable. As a result it is possible to prove several key results directly related to known typical properties of these systems like (i) proof, in the context of the theory of branching processes, of the lethal mutagenesis criterion proposed by Bull, Sanju\'an and Wilke (""Theory of lethal mutagenesis for viruses"", J. Virology 18 (2007) 2930-2939); (ii) a new proposal for the notion of relaxation time with a quantitative prescription for its evaluation and (iii) the quantitative description of the evolution of the expected values in four distinct regimes: transient, ""stationary"" equilibrium, extinction threshold and lethal mutagenesis. Moreover, new insights on the dynamics of evolving virus populations can be foreseen."	14941	0.0	4	4.0
4	1308.5365	18595339	Increasing herd immunity with influenza revaccination	SUMMARY Seasonal influenza is a significant public health concern globally. While influenza vaccines are the single most effective intervention to reduce influenza morbidity and mortality, there is considerable debate surrounding the merits and consequences of repeated seasonal vaccination. Here, we describe a two-season influenza epidemic contact network model and use it to demonstrate that increasing the level of continuity in vaccination across seasons reduces the burden on public health. We show that revaccination reduces the influenza attack rate not only because it reduces the overall number of susceptible individuals, but also because it better protects highly connected individuals, who would otherwise make a disproportionately large contribution to influenza transmission. We also demonstrate that our results hold on an empirical contact network, in the presence of assortativity in vaccination status, and are robust for a range of vaccine coverage and efficacy levels. Our work contributes a population-level perspective to debates about the merits of repeated influenza vaccination and advocates for public health policy to incorporate individual vaccine histories.	49264	1.1764706373214722	2	3.1760001182556152
5	1407.0726	14666609	Low-rank matrix recovery in poisson noise	This paper describes a new algorithm for recovering low-rank matrices from their linear measurements contaminated with Poisson noise: the Poisson noise Maximum Likelihood Singular Value thresholding (PMLSV) algorithm. We propose a convex optimization formulation with a cost function consisting of the sum of a likelihood function and a regularization function which is proportional to the nuclear norm of the matrix. Instead of solving the optimization problem directly by semi-definite program (SD-P), we derive an iterative singular value thresholding algorithm by expanding the likelihood function. We demonstrate the good performance of the proposed algorithm on recovery of solar flare images with Poisson noise: the algorithm is more efficient than solving SDP using the interior-point algorithm and it generates a good approximate solution compared to that solved from SDP.	73151	0.0	3	3.0
6	1302.7025	2182328	Maximizing acceptance probability for active friending in online social networks	Friending recommendation has successfully contributed to the explosive growth of online social networks. Most friending recommendation services today aim to support passive friending, where a user passively selects friending targets from the recommended candidates. In this paper, we advocate a recommendation support for active friending, where a user actively specifies a friending target. To the best of our knowledge, a recommendation designed to provide guidance for a user to systematically approach his friending target has not been explored for existing online social networking services. To maximize the probability that the friending target would accept an invitation from the user, we formulate a new optimization problem, namely, Acceptance Probability Maximization (APM), and develop a polynomial time algorithm, called Selective Invitation with Tree and In-Node Aggregation (SITINA), to find the optimal solution. We implement an active friending service with SITINA on Facebook to validate our idea. Our user study and experimental results reveal that SITINA outperforms manual selection and the baseline approach in solution quality efficiently.	37677	0.0	3	3.0
7	1907.05443	196471131	Learning Key-Value Store Design	"We introduce the concept of design continuums for the data layout of key-value stores. A design continuum unifies major distinct data structure designs under the same model. The critical insight and potential long-term impact is that such unifying models 1) render what we consider up to now as fundamentally different data structures to be seen as views of the very same overall design space, and 2) allow seeing new data structure designs with performance properties that are not feasible by existing designs. The core intuition behind the construction of design continuums is that all data structures arise from the very same set of fundamental design principles, i.e., a small set of data layout design concepts out of which we can synthesize any design that exists in the literature as well as new ones. We show how to construct, evaluate, and expand, design continuums and we also present the first continuum that unifies major data structure designs, i.e., B+tree, B-epsilon-tree, LSM-tree, and LSH-table. 
The practical benefit of a design continuum is that it creates a fast inference engine for the design of data structures. For example, we can predict near instantly how a specific design change in the underlying storage of a data system would affect performance, or reversely what would be the optimal data structure (from a given set of designs) given workload characteristics and a memory budget. In turn, these properties allow us to envision a new class of self-designing key-value stores with a substantially improved ability to adapt to workload and hardware changes by transitioning between drastically different data structure designs to assume a diverse set of performance properties at will."	125296	3.529411792755127	5	8.529000282287598
8	1906.08629	195218470	On structural and dynamical factors determining the integrated basin instability of power-grid nodes.	In electric power systems delivering alternating current, it is essential to maintain its synchrony of the phase with the rated frequency. The synchronization stability that quantifies how well the power-grid system recovers its synchrony against perturbation depends on various factors. As an intrinsic factor that we can design and control, the transmission capacity of the power grid affects the synchronization stability. Therefore, the transition pattern of the synchronization stability with the different levels of transmission capacity against external perturbation provides the stereoscopic perspective to understand the synchronization behavior of power grids. In this study, we extensively investigate the factors affecting the synchronization stability transition by using the concept of basin stability as a function of the transmission capacity. For a systematic approach, we introduce the integrated basin instability, which literally adds up the instability values as the transmission capacity increases. We first take simple 5-node motifs as a case study of building blocks of power grids, and a more realistic IEEE 24-bus model to highlight the complexity of decisive factors. We find that both structural properties such as gate keepers in network topology and dynamical properties such as large power input/output at nodes cause synchronization instability. The results suggest that evenly distributed power generation and avoidance of bottlenecks can improve the overall synchronization stability of power-grid systems.	121261	0.5882353186607361	3	3.5880000591278076
9	1304.6138	50735848	Quantization Domains	We study the quantization of certain classical field theories using reflection positivity. We give elementary conditions that ensure the resulting vacuum state is cyclic for products of quantum field operators, localized in a bounded Euclidean spacetime region O at positive time. We call such a domain a quantization domain for the classical field. The fact that bounded regions are quantization domains in classical field theory is similar to the “Reeh-Schlieder” property in axiomatic quantum field theory. 1. Quantization Domains The Reeh-Schlieder property of quantum field theory states that one can recover all the properties described by the field, or by bounded functions of the field called observables, from information localized in any open (bounded) space-time domain O. The Reeh-Schlieder theorem states that any Wightman field theory, or any Haag-Kastler local quantum theory, has this property [8, 9, 4]. This result is a consequence of analyticity properties that follow in turn from the positivity of the energy and Lorentz covariance, as well as locality. Here we consider the analog of this result in the context of a classical field on a Euclidean space-time X . We assume that the classical field can be quantized by using the Osterwalder-Schrader property of reflection positivity. We say that a domain O for the classical field is a quantization domain, if the quantization of fields supported in O gives a dense set of vectors in the physical Hilbert space. Of course, the are well-known equivalence theorems for Wightman theory and Osterwalder-Schrader (OS) theory ensure that any bounded, positive-time domains in an OS theory is a quantization domain. However, we are interested in investigating field theories for which all the standard axioms may not apply or cases in which they have not been verified. Thus we pose weaker assumptions about the classical theory than the full OS axioms. For example, we replace the relativistic spectrum condition with a weaker assumption. 1.1. Quantization. Consider d = s+ 1 dimensional space-times X = R× Σ , where Σ = X1 × · · · × Xs and Xj = R or Xj = S . Call the variable t ≡ x0 ∈ R the time coordinate. 1 2 RATNARAJAN HOOLE, ARTHUR JAFFE, AND CHRISTIAN JÄKEL Now consider the Fock space E(X) over the one-particle space L2(X). Denote the Fock vacuum vector by ΩE0 ∈ E(X) and assume that the classical scalar field Φ(f) = ∫ Φ(x)f(x)dx , f ∈ C 0 (X) , defined in terms of the usual creation and annihilation operators, gives rise to the characteristic functional S(f) = 〈ΩE0, e Ω0〉 = ∞ ∑ n=0 i n! 〈ΩE0,Φ(f) Ω0〉 , which is convergent for f ∈ C 0 (X). Assume that the abelian spacetime translation group T (a) and the time reflection θ act covariantly on the field and leave the characteristic functional S(f) invariant. Divide X into a union of three disjoint parts X = X− ∪X0 ∪X+ , with the reflection θ leaving X0 invariant and interchanging X±. Let E±,0 ⊂ E denote the subspace of finite linear combinations	41016	0.5882353186607361	3	3.5880000591278076
10	1111.1198	118419326	A new simple class of superpotentials in SUSY quantum mechanics	In this work, we introduce the class of quantum mechanics superpotentials W(x) = gε(x)x2n and study in detail the cases n = 0 and 1. The n = 0 superpotential is shown to lead to the known problem of two supersymmetrically related Dirac delta potentials (well and barrier). The n = 1 case results in the potentials V±(x) = g2x4 ± 2g|x|. For V−, we present the exact ground-state solution and study the excited states by a variational technique. Starting from the ground state of V− and using logarithmic perturbation theory, we study the ground states of V+ and also of V(x) = g2x4 and compare the result obtained in this new way with other results for this last potential in the literature.	15856	0.0	3	3.0
11	1907.07406	197431222	Constructions and properties of a class of random scale-free networks	Complex networks have abundant and extensive applications in real life. Recently, researchers have proposed a large variety of complex networks, in which some are deterministic and others are random. The goal of this paper is to generate a class of random scale-free networks. To achieve this, we introduce three types of operations, i.e., rectangle operation, diamond operation, and triangle operation, and provide the concrete process for generating random scale-free networks N(p,q,r,t), where probability parameters p,q,r hold on p+q+r=1 with 0≤p,q,r≤1. We then discuss their topological properties, such as average degree, degree distribution, diameter, and clustering coefficient. First, we calculate the average degree of each member and discover that each member is a sparse graph. Second, by computing the degree distribution of our network N(p,q,r,t), we find that degree distribution obeys the power-law distribution, which implies that each member is scale-free. Next, according to our analysis of the diameter of our network N(p,q,r,t), we reveal the fact that the diameter may abruptly transform from small to large. Afterward, we give the calculation process of the clustering coefficient and discover that its value is mainly determined by r.	126118	0.5882353186607361	3	3.5880000591278076
12	1409.1904	18231391	Exploring exoplanet populations with NASA’s Kepler Mission	The Kepler Mission is exploring the diversity of planets and planetary systems. Its legacy will be a catalog of discoveries sufficient for computing planet occurrence rates as a function of size, orbital period, star type, and insolation flux. The mission has made significant progress toward achieving that goal. Over 3,500 transiting exoplanets have been identified from the analysis of the first 3 y of data, 100 planets of which are in the habitable zone. The catalog has a high reliability rate (85–90% averaged over the period/radius plane), which is improving as follow-up observations continue. Dynamical (e.g., velocimetry and transit timing) and statistical methods have confirmed and characterized hundreds of planets over a large range of sizes and compositions for both single- and multiple-star systems. Population studies suggest that planets abound in our galaxy and that small planets are particularly frequent. Here, I report on the progress Kepler has made measuring the prevalence of exoplanets orbiting within one astronomical unit of their host stars in support of the National Aeronautics and Space Administration’s long-term goal of finding habitable environments beyond the solar system.	78382	1.1764706373214722	3	4.176000118255615
13	1910.01062	251643886	Stabilizing Deep Reinforcement Learning with Conservative Updates	In recent years, advances in deep learning have enabled the application of reinforcement learning algorithms in complex domains. However, they lack the theoretical guarantees which are present in the tabular setting and suffer from many stability and reproducibility problems \citep{henderson2018deep}. In this work, we suggest a simple approach for improving stability and providing probabilistic performance improvement in off-policy actor-critic deep reinforcement learning regimes. Experiments on continuous action spaces, in the MuJoCo control suite, show that our proposed method reduces the variance of the process and improves the overall performance.	140785	0.0	5	5.0
14	1207.6186	153187749	A Dynamical Model for Operational Risk in Banks	Operational risk is the risk relative to monetary losses caused by failures of bank internal processes due to heterogeneous causes. A dynamical model including both spontaneous generation of losses and generation via interactions between different processes is presented; the efforts made by the bank to avoid the occurrence of losses is also taken into account. Under certain hypotheses, the model can be exactly solved and, in principle, the solution can be exploited to estimate most of the model parameters from real data. The forecasting power of the model is also investigated and proved to be surprisingly remarkable.	26769	0.5882353186607361	2	2.5880000591278076
15	1409.3040	9133760	Towards Optimal Algorithms for Prediction with Expert Advice	"We study the classical problem of prediction with expert advice in the adversarial setting with a geometric stopping time. In 1965, Cover gave the optimal algorithm for the case of 2 experts. In this paper, we design the optimal algorithm, adversary and regret for the case of 3 experts. Further, we show that the optimal algorithm for $2$ and $3$ experts is a probability matching algorithm (analogous to Thompson sampling) against a particular randomized adversary. Remarkably, our proof shows that the probability matching algorithm is not only optimal against this particular randomized adversary, but also minimax optimal. 
Our analysis develops upper and lower bounds simultaneously, analogous to the primal-dual method. Our analysis of the optimal adversary goes through delicate asymptotics of the random walk of a particle between multiple walls. We use the connection we develop to random walks to derive an improved algorithm and regret bound for the case of $4$ experts, and, provide a general framework for designing the optimal algorithm and adversary for an arbitrary number of experts."	78682	0.5882353186607361	2	2.5880000591278076
16	1812.07872	56475959	Fast Adjustable Threshold For Uniform Neural Network Quantization		104505	0.0	4	4.0
17	1901.03365	119716385	Local Uniformization through monomialization of key elements	We give a new proof of the simultaneous embedded local uniformization Theorem in zero characteristic for essentially of finite type rings and for quasi excellent rings. The results are a consequence of the simultaneaous monomialization presented here. The methods develop the key elements theory that is a more subtle notion than the notion of key polynomials.	107980	1.1764706373214722	1	2.1760001182556152
18	1904.04831	119192593	The SXS collaboration catalog of binary black hole simulations	Accurate models of gravitational waves from merging black holes are necessary for detectors to observe as many events as possible while extracting the maximum science. Near the time of merger, the gravitational waves from merging black holes can be computed only using numerical relativity. In this paper, we present a major update of the Simulating eXtreme Spacetimes (SXS) Collaboration catalog of numerical simulations for merging black holes. The catalog contains 2018 distinct configurations (a factor of 11 increase compared to the 2013 SXS catalog), including 1426 spin-precessing configurations, with mass ratios between 1 and 10, and spin magnitudes up to 0.998. The median length of a waveform in the catalog is 39 cycles of the dominant gravitational-wave mode, with the shortest waveform containing 7.0 cycles and the longest 351.3 cycles. We discuss improvements such as correcting for moving centers of mass and extended coverage of the parameter space. We also present a thorough analysis of numerical errors, finding typical truncation errors corresponding to a waveform mismatch of  ∼10−4. The simulations provide remnant masses and spins with uncertainties of 0.03% and 0.1% (90th percentile), about an order of magnitude better than analytical models for remnant properties. The full catalog is publicly available at www.black-holes.org/waveforms.	114487	1.1764706373214722	2	3.1760001182556152
19	1410.1100	119120478	Neutron-Antineutron Oscillations: Theoretical Status and Experimental Prospects		80582	0.5882353186607361	3	3.5880000591278076
20	1910.03239	203902781	Metric Pose Estimation for Human-Machine Interaction Using Monocular Vision	The rapid growth of collaborative robotics in production requires new automation technologies that take human and machine equally into account. In this work, we describe a monocular camera based system to detect human-machine interactions from a bird's-eye perspective. Our system predicts poses of humans and robots from a single wide-angle color image. Even though our approach works on 2D color input, we lift the majority of detections to a metric 3D space. Our system merges pose information with predefined virtual sensors to coordinate human-machine interactions. We demonstrate the advantages of our system in three use cases.	141777	0.0	4	4.0
21	1902.02513	59608835	Advances on CNN-Based Super-Resolution of Sentinel-2 Images	Thanks to their temporal-spatial coverage and free access, Sentinel-2 images are very interesting for the community. However, a relatively coarse spatial resolution, compared to that of state-of-the-art commercial products, motivates the study of super-resolution techniques to mitigate such a limitation. Specifically, thirtheen bands are sensed simultaneously but at different spatial resolutions: 10, 20, and 60 meters depending on the spectral location. Here, building upon our previous convolutional neural network (CNN) based method [1], we propose an improved CNN solution to super-resolve the 20-m resolution bands benefiting spatial details conveyed by the accompanying 10-m spectral bands.	112553	5.294117450714111	3	8.293999671936035
22	1901.03597	57825725	CT-GAN: Malicious Tampering of 3D Medical Imagery using Deep Learning	"In 2018, clinics and hospitals were hit with numerous attacks leading to significant data breaches and interruptions in medical services. An attacker with access to medical records can do much more than hold the data for ransom or sell it on the black market. 
In this paper, we show how an attacker can use deep-learning to add or remove evidence of medical conditions from volumetric (3D) medical scans. An attacker may perform this act in order to stop a political candidate, sabotage research, commit insurance fraud, perform an act of terrorism, or even commit murder. We implement the attack using a 3D conditional GAN and show how the framework (CT-GAN) can be automated. Although the body is complex and 3D medical scans are very large, CT-GAN achieves realistic results which can be executed in milliseconds. 
To evaluate the attack, we focused on injecting and removing lung cancer from CT scans. We show how three expert radiologists and a state-of-the-art deep learning AI are highly susceptible to the attack. We also explore the attack surface of a modern radiology network and demonstrate one attack vector: we intercepted and manipulated CT scans in an active hospital network with a covert penetration test. 
Demo video: this https URL 
Source code: this https URL"	108087	6.470588207244873	4	10.470999717712402
23	1401.1319	119185258	Self-consistent description of single-particle levels of magic nuclei	Single-particle levels of seven magic nuclei are calculated within the Energy Density Functional (EDF) method by Fayans et al. Three versions of the EDF are used, the initial Fayans functional DF3 and its two variations, DF3-a and DF3-b, with different values of spin-orbit parameters. Comparison is made with predictions of the Skyrme-Hartree-Fock method with the HFB-17 functional. For the DF3-a functional, phonon coupling (PC) corrections to single-particle energies are found self-consistently with an approximate account for the tadpole diagram. Account for the PC corrections improves agreement with the data for heavy nuclei, e.g. for 208 Pb. On the other hand, for lighter nuclei, e.g. 40,48 Ca, PC corrections make the agreement a little worse. As estimations show, the main reason is that the approximation we use for the tadpole term is less accurate for the light nuclei.	58810	0.5882353186607361	2	2.5880000591278076
24	1907.05195	195886117	retina-VAE: Variationally Decoding the Spectrum of Macular Disease	In this paper, we seek a clinically-relevant latent code for representing the spectrum of macular disease. Towards this end, we construct retina-VAE, a variational autoencoder-based model that accepts a patient profile vector (pVec) as input. The pVec components include clinical exam findings and demographic information. We evaluate the model on a subspectrum of the retinal maculopathies, in particular, exudative age-related macular degeneration, central serous chorioretinopathy, and polypoidal choroidal vasculopathy. For these three maculopathies, a database of 3000 6-dimensional pVecs (1000 each) was synthetically generated based on known disease statistics in the literature. The database was then used to train the VAE and generate latent vector representations. We found training performance to be best for a 3-dimensional latent vector architecture compared to 2 or 4 dimensional latents. Additionally, for the 3D latent architecture, we discovered that the resulting latent vectors were strongly clustered spontaneously into one of 14 clusters. Kmeans was then used only to identify members of each cluster and to inspect cluster properties. These clusters suggest underlying disease subtypes which may potentially respond better or worse to particular pharmaceutical treatments such as anti-vascular endothelial growth factor variants. The retina-VAE framework will potentially yield new fundamental insights into the mechanisms and manifestations of disease. And will potentially facilitate the development of personalized pharmaceuticals and gene therapies.	125167	0.0	3	3.0
25	1907.05053	195886421	Free boundary minimal surfaces in the unit ball : recent advances and open questions	In this survey, we discuss some recent results on free boundary minimal surfaces in the Euclidean unit-ball. The subject has been a very active field of research in the past few years due to the seminal work of Fraser and Schoen on the extremal Steklov eigenvalue problem. We review several different techniques of constructing examples of embedded free boundary minimal surfaces in the unit ball. Next, we discuss some uniqueness results for free boundary minimal disks and the conjecture about the uniqueness of critical catenoid. We also discuss several Morse index estimates for free boundary minimal surfaces. Moreover, we describe estimates for the first Steklov eigenvalue on such free boundary minimal surfaces and various smooth compactness results. Finally, we mention some sharp area bounds for free boundary minimal submanifolds and related questions.	125098	2.3529412746429443	2	4.353000164031982
26	1812.10860	57189285	Looking for ELMo's friends: Sentence-Level Pretraining Beyond Language Modeling	Work on the problem of contextualized word representation—the development of reusable neural network components for sentence understanding—has recently seen a surge of progress centered on the unsupervised pretraining task of language modeling with methods like ELMo (Peters et al., 2018). This paper contributes the first large-scale systematic study comparing different pretraining tasks in this context, both as complements to language modeling and as potential alternatives. The primary results of the study support the use of language modeling as a pretraining task and set a new state of the art among comparable models using multitask learning with language models. However, a closer look at these results reveals worryingly strong baselines and strikingly varied results across target tasks, suggesting that the widely-used paradigm of pretraining and freezing sentence encoders may not be an ideal platform for further work.	106151	1.1764706373214722	3	4.176000118255615
27	1909.06493	202578037	Flight Controller Synthesis Via Deep Reinforcement Learning	"Traditional control methods are inadequate in many deployment settings involving control of Cyber-Physical Systems (CPS). In such settings, CPS controllers must operate and respond to unpredictable interactions, conditions, or failure modes. Dealing with such unpredictability requires the use of executive and cognitive control functions that allow for planning and reasoning. Motivated by the sport of drone racing, this dissertation addresses these concerns for state-of-the-art flight control by investigating the use of deep neural networks to bring essential elements of higher-level cognition for constructing low level flight controllers. 
This thesis reports on the development and release of an open source, full solution stack for building neuro-flight controllers. This stack consists of the methodology for constructing a multicopter digital twin for synthesize the flight controller unique to a specific aircraft, a tuning framework for implementing training environments (GymFC), and a firmware for the world's first neural network supported flight controller (Neuroflight). GymFC's novel approach fuses together the digital twinning paradigm for flight control training to provide seamless transfer to hardware. Additionally, this thesis examines alternative reward system functions as well as changes to the software environment to bridge the gap between the simulation and real world deployment environments. 
Work summarized in this thesis demonstrates that reinforcement learning is able to be leveraged for training neural network controllers capable, not only of maintaining stable flight, but also precision aerobatic maneuvers in real world settings. As such, this work provides a foundation for developing the next generation of flight control systems."	137207	5.882352828979492	8	13.881999969482422
28	1908.02391	199472581	Bag of Negatives for Siamese Architectures	Training a Siamese architecture for re-identification with a large number of identities is a challenging task due to the difficulty of finding relevant negative samples efficiently. In this work we present Bag of Negatives (BoN), a method for accelerated and improved training of Siamese networks that scales well on datasets with a very large number of identities. BoN is an efficient and loss-independent method, able to select a bag of high quality negatives, based on a novel online hashing strategy.	129904	0.5882353186607361	2	2.5880000591278076
29	1305.5330	15664639	A toy model of information retrieval system based on quantum probability	Recent numerical results show that non-Bayesian knowledge revision may be helpful in search engine training and optimization. In order to demonstrate how basic assumption about about the physical nature (and hence the observed statistics) of retrieved documents can affect the performance of search engines we suggest an idealized toy model with minimal number of parameters.	42977	0.0	3	3.0
30	1902.01177	59599871	Unsupervised Clinical Language Translation	As patients' access to their doctors' clinical notes becomes common, translating professional, clinical jargon to layperson-understandable language is essential to improve patient-clinician communication. Such translation yields better clinical outcomes by enhancing patients' understanding of their own health conditions, and thus improving patients' involvement in their own care. Existing research has used dictionary-based word replacement or definition insertion to approach the need. However, these methods are limited by expert curation, which is hard to scale and has trouble generalizing to unseen datasets that do not share an overlapping vocabulary. In contrast, we approach the clinical word and sentence translation problem in a completely unsupervised manner. We show that a framework using representation learning, bilingual dictionary induction and statistical machine translation yields the best precision at 10 of 0.827 on professional-to-consumer word translation, and mean opinion scores of 4.10 and 4.28 out of 5 for clinical correctness and layperson readability, respectively, on sentence translation. Our fully-unsupervised strategy overcomes the curation problem, and the clinically meaningful evaluation reduces biases from inappropriate evaluators, which are critical in clinical machine learning.	111953	0.0	3	3.0
31	1905.11850	167217328	The nanoscale structure of the Pt-water double layer under bias revealed		116380	0.5882353186607361	2	2.5880000591278076
32	1410.0708	117135690	Exploring Critical Collapse in the Semilinear Wave Equation using Space-Time Finite Elements	1 Department of Mathematics and Statistics, South Dakota State University, Brookings, SD 57007, USA and2 Center for Research in Extreme Scale Technologies,Indiana University, Bloomington, IN 47405, USA(Dated: October 8, 2014)A fully implicit numerical approach based on the space-time nite element method is implementedfor the semilinear wave equation in 1(space) + 1(time) dimensions to explore critical collapse andsearch for self-similar solutions. Previous work studied this behavior by exploring the threshold ofsingularity formation using time marching nite di erence techniques while this work introducesan adaptive time parallel numerical method to the problem. The semilinear wave equation witha p = 7 term is examined in spherical symmetry. The impact of mesh re nement and the timeadditive Schwarz preconditioner in conjunction with Krylov Subspace Methods are examined.I. INTRODUCTION	80494	0.5882353186607361	2	2.5880000591278076
33	1812.05854	119669881	The cubic Schrödinger regime of the Landau–Lifshitz equation with a strong easy-axis anisotropy	"We pursue our work on the asymptotic regimes of the Landau-Lifshitz equation for bi-axial ferromagnets. We put the focus on the cubic Schr{\""o}dinger equation, which is known to describe the dynamics in a regime of strong easy-axis anisotropy. In any dimension, we rigorously prove this claim for solutions with sufficient regularity. In this regime, we additionally classify the one-dimensional solitons of the Landau-Lifshitz equation and quantify their convergence towards the solitons of the one-dimensional cubic Schr{\""o}dinger equation."	103230	0.5882353186607361	2	2.5880000591278076
34	1908.07974	201125292	Deterministic Epidemic Models for Ebola Infection with Time-Dependent Controls	In this paper, we have studied epidemiological models for Ebola infection using nonlinear ordinary differential equations and optimal control theory. We considered optimal control analysis of SIR and SEIR models for the deadly Ebola infection using vaccination, treatment and educational campaign as time-dependent controls functions. We have applied indirect methods to study existing deterministic optimal control epidemic models for Ebola virus disease. These methods in optimal control are based on Hamiltonian function and the Pontryagin's maximum principle to construct adjoint equations and optimality systems. The forward-backward sweep numerical scheme with fourth-order Runge-Kutta method is used to solve the optimality system for the various control strategies. From our numerical illustrations, we can conclude that, effective educational campaigns and vaccination of susceptible individuals as were as effective treatments of infected individuals can help reduce the disease transmission.	132458	1.1764706373214722	2	3.1760001182556152
35	1406.0001	119192926	Discovery of a Thorne–Żytkow object candidate in the Small Magellanic Cloud	Thorne-Zytkow objects (TZOs) are a theoretical class of star in which a compact neutron star is surrounded by a large, diffuse envelope. Supergiant TZOs are predicted to be almost identical in appearance to red supergiants (RSGs). The best features that can be used at present to distinguish TZOs from the general RSG population are the unusually strong heavy-element and Li lines present in their spectra, products of the star's fully convective envelope linking the photosphere with the extraordinarily hot burning region in the vicinity of the neutron star core. Here we present our discovery of a TZO candidate in the Small Magellanic Cloud. It is the first star to display the distinctive chemical profile of anomalous element enhancements thought to be unique to TZOs. The positive detection of a TZO will provide the first direct evidence for a completely new model of stellar interiors, a theoretically predicted fate for massive binary systems, and never-before-seen nucleosynthesis processes that would offer a new channel for Li and heavy-element production in our universe.	70532	0.0	5	5.0
36	1901.10513	59413762	Adversarial Examples Are a Natural Consequence of Test Error in Noise	Over the last few years, the phenomenon of adversarial examples --- maliciously constructed inputs that fool trained machine learning models --- has captured the attention of the research community, especially when the adversary is restricted to small modifications of a correctly handled input. Less surprisingly, image classifiers also lack human-level performance on randomly corrupted images, such as images with additive Gaussian noise. In this paper we provide both empirical and theoretical evidence that these are two manifestations of the same underlying phenomenon, establishing close connections between the adversarial robustness and corruption robustness research programs. This suggests that improving adversarial robustness should go hand in hand with improving performance in the presence of more general and realistic image corruptions. Based on our results we recommend that future adversarial defenses consider evaluating the robustness of their methods to distributional shift with benchmarks such as Imagenet-C.	110983	1.1764706373214722	5	6.176000118255615
37	1906.02129	131768973	Cold Debris Disks as Strategic Targets for the 2020s	"Cold debris disks (T$<$200 K) are analogues to the dust in the Solar System's Kuiper belt--dust generated from the evaporation and collision of minor bodies perturbed by planets, our Sun, and the local interstellar medium. Scattered light from debris disks acts as both a signpost for unseen planets as well as a source of contamination for directly imaging terrestrial planets, but many details of these disks are poorly understood. We lay out a critical observational path for the study of nearby debris disks that focuses on defining an empirical relationship between scattered light and thermal emission from a disk, probing the dynamics and properties of debris disks, and directly determining the influence of planets on disks. 
We endorse the findings and recommendations published in the National Academy reports on Exoplanet Science Strategy and Astrobiology Strategy for the Search for Life in the Universe. This white paper extends and complements the material presented therein with a focus on debris disks around nearby stars. Separate complementary papers are being submitted regarding the inner warm regions of debris disks (Mennesson et al.), the modeling of debris disk evolution (Gaspar et al.), studies of dust properties (Chen et al.), and thermal emission from disks (Su et al.)."	118334	1.1764706373214722	2	3.1760001182556152
38	1901.03688	57825711	Quantifying echo chamber effects in information spreading over political communication networks		108133	1.7647058963775635	4	5.764999866485596
39	1910.01313	203641816	Assessing the predictive ability of the UPDRS for falls classification in early stage Parkinson's disease	Identification of risk factors associated with falls in people with Parkinson's Disease (PD) is important due to their high risk of falling. In this study, various ways of utilizing the Unified Parkinson's Disease Rating Scale (UPDRS) were assessed for the identification of risk factors and for the prediction of falls. Three statistical methods for classification were considered:decision trees, random forests, and logistic regression. UPDRS measurements on 51 participants with early stage PD, who completed monthly falls diaries for 12 months of follow-up were analyzed. All classification methods applied produced similar results in regards to classification accuracy and the selected important variables. The highest classification rates were obtained from model with individual items of the UPDRS with 80% accuracy (85% sensitivity and 77% specificity), higher than in any previous study. A comparison of the independent performance of the four parts of the UPDRS revealed the comparably high classification rates for Parts II and III of the UPDRS. Similar patterns with slightly different classification rates were observed for the 6- and 12-month of follow-up times. Consistent predictors for falls selected by all classification methods at two follow-up times are: thought disorder for UPDRS I, dressing and falling for UPDRS II, hand pronate/supinate for UPDRS III, and sleep disturbance and symptomatic orthostasis for UPDRS IV. While for the aggregate measures, subtotal 2 (sum of UPDRS II items) and bradykinesia showed high association with fall/non-fall. Fall/non-fall occurrences were more associated with individual items of the UPDRS than with the aggregate measures. UPDRS parts II and III produced comparably high classification rates for fall/non-fall prediction. Similar results were obtained for modelling data at 6-month and 12-month follow-up times.	140934	2.3529412746429443	1	3.3529999256134033
40	1908.06136	201070342	Transductive Auxiliary Task Self-Training for Neural Multi-Task Models	Multi-task learning and self-training are two common ways to improve a machine learning model’s performance in settings with limited training data. Drawing heavily on ideas from those two approaches, we suggest transductive auxiliary task self-training: training a multi-task model on (i) a combination of main and auxiliary task training data, and (ii) test instances with auxiliary task labels which a single-task version of the model has previously generated. We perform extensive experiments on 86 combinations of languages and tasks. Our results are that, on average, transductive auxiliary task self-training improves absolute accuracy by up to 9.56% over the pure multi-task model for dependency relation tagging and by up to 13.03% for semantic tagging.	131655	0.5882353186607361	2	2.5880000591278076
41	1307.5830	9437637	Detection of B-mode polarization in the cosmic microwave background with data from the South Pole Telescope.	"Gravitational lensing of the cosmic microwave background generates a curl pattern in the observed polarization. This ""B-mode"" signal provides a measure of the projected mass distribution over the entire observable Universe and also acts as a contaminant for the measurement of primordial gravity-wave signals. In this Letter we present the first detection of gravitational lensing B modes, using first-season data from the polarization-sensitive receiver on the South Pole Telescope (SPTpol). We construct a template for the lensing B-mode signal by combining E-mode polarization measured by SPTpol with estimates of the lensing potential from a Herschel-SPIRE map of the cosmic infrared background. We compare this template to the B modes measured directly by SPTpol, finding a nonzero correlation at 7.7σ significance. The correlation has an amplitude and scale dependence consistent with theoretical expectations, is robust with respect to analysis choices, and constitutes the first measurement of a powerful cosmological observable."	47016	0.0	5	5.0
42	1910.01409	203641992	A General Upper Bound for Unsupervised Domain Adaptation	In this work, we present a novel upper bound of target error to address the problem for unsupervised domain adaptation. Recent studies reveal that a deep neural network can learn transferable features which generalize well to novel tasks. Furthermore, a theory proposed by Ben-David et al. (2010) provides a upper bound for target error when transferring the knowledge, which can be summarized as minimizing the source error and distance between marginal distributions simultaneously. However, common methods based on the theory usually ignore the joint error such that samples from different classes might be mixed together when matching marginal distribution. And in such case, no matter how we minimize the marginal discrepancy, the target error is not bounded due to an increasing joint error. To address this problem, we propose a general upper bound taking joint error into account, such that the undesirable case can be properly penalized. In addition, we utilize constrained hypothesis space to further formalize a tighter bound as well as a novel cross margin discrepancy to measure the dissimilarity between hypotheses which alleviates instability during adversarial learning. Extensive empirical evidence shows that our proposal outperforms related approaches in image classification error rates on standard domain adaptation benchmarks.	140981	0.0	5	5.0
43	1902.02479	119314640	Intertwining operators between one-dimensional homogeneous quantum walks	The subject of this paper is a kind of dynamical systems called quantum walks. We study one-dimensional homogeneous analytic quantum walks U. We explain how to identify the space of all the uniform intertwining operators between these walks. We can also determine whether U can be realized by a (not necessarily homogeneous) continuous-time uniform quantum walk on Z. Several examples of quantum walks, which can not be realized by continuous-time uniform quantum walks, are presented. The 4-state Grover walk is one of them. Before stating the main theorems, we clarify the definition of one-dimensional quantum walks. For the first half of this paper, we study basic properties of one-dimensional quantum walks, which are not necessarily homogeneous. An equivalence relation between quantum walks called similarity is also introduced. This allows us to manipulate quantum walks in a flexible manner.	112536	1.1764706373214722	1	2.1760001182556152
44	1909.00321	202121070	Deep Mesh Reconstruction From Single RGB Images via Topology Modification Networks	Reconstructing the 3D mesh of a general object from a single image is now possible thanks to the latest advances of deep learning technologies. However, due to the nontrivial difficulty of generating a feasible mesh structure, the state-of-the-art approaches often simplify the problem by learning the displacements of a template mesh that deforms it to the target surface. Though reconstructing a 3D shape with complex topology can be achieved by deforming multiple mesh patches, it remains difficult to stitch the results to ensure a high meshing quality. In this paper, we present an end-to-end single-view mesh reconstruction framework that is able to generate high-quality meshes with complex topologies from a single genus-0 template mesh. The key to our approach is a novel progressive shaping framework that alternates between mesh deformation and topology modification. While a deformation network predicts the per-vertex translations that reduce the gap between the reconstructed mesh and the ground truth, a novel topology modification network is employed to prune the error-prone faces, enabling the evolution of topology. By iterating over the two procedures, one can progressively modify the mesh topology while achieving higher reconstruction accuracy. Moreover, a boundary refinement network is designed to refine the boundary conditions to further improve the visual quality of the reconstructed mesh. Extensive experiments demonstrate that our approach outperforms the current state-of-the-art methods both qualitatively and quantitatively, especially for the shapes with complex topologies.	134473	1.1764706373214722	1	2.1760001182556152
45	1910.01155	203641983	Stochastic gradient descent for hybrid quantum-classical optimization	Within the context of hybrid quantum-classical optimization, gradient descent based optimizers typically require the evaluation of expectation values with respect to the outcome of parameterized quantum circuits. In this work, we explore the consequences of the prior observation that estimation of these quantities on quantum hardware results in a form of stochastic gradient descent optimization. We formalize this notion, which allows us to show that in many relevant cases, including VQE, QAOA and certain quantum classifiers, estimating expectation values with $k$ measurement outcomes results in optimization algorithms whose convergence properties can be rigorously well understood, for any value of $k$. In fact, even using single measurement outcomes for the estimation of expectation values is sufficient. Moreover, in many settings the required gradients can be expressed as linear combinations of expectation values -- originating, e.g., from a sum over local terms of a Hamiltonian, a parameter shift rule, or a sum over data-set instances -- and we show that in these cases $k$-shot expectation value estimation can be combined with sampling over terms of the linear combination, to obtain ``doubly stochastic'' gradient descent optimizers. For all algorithms we prove convergence guarantees, providing a framework for the derivation of rigorous optimization results in the context of near-term quantum devices. Additionally, we explore numerically these methods on benchmark VQE, QAOA and quantum-enhanced machine learning tasks and show that treating the stochastic settings as hyper-parameters allows for state-of-the-art results with significantly fewer circuit executions and measurements.	140837	2.3529412746429443	4	6.353000164031982
46	1407.1453	152986753	No-arbitrage for informational discrete time market models	Abstract This paper focuses on the stability of no-arbitrage, for discrete time market models, under additional uncertainty generated by a random time . At the practical level, this random time represents the death time, the default time of a firm, or any occurrence time of an event that might affect the market somehow. We address the no-arbitrage issue for the resulting new flow of information (filtration) which makes the random time either a nontrivial stopping time (progressive enlargement) or a known time from the beginning (initial enlargement). Our main conclusions are twofold. On the one hand, for a fixed initial market S, we completely and precisely characterize the interplay between S and such that the no-arbitrage is preserved for the new market model. On the other hand, we give the necessary and sufficient conditions on to ensure the preservation of the no-arbitrage under the additional uncertainty of for any market. Two concrete examples are presented to illustrate the results.	73424	0.5882353186607361	2	2.5880000591278076
47	1308.3907	119180556	Unstable Schwarzschild-Tangherlini black holes in fourth-order gravity	We study the stability of Schwarzschild-Tangherlini (ST) black holes in fourth-order gravity which provides a higher dimensional linearized massive equation. The linearized-Ricci tensor perturbation is employed to exhibit unstable modes featuring the Gregory-Laflamme (GL) instability of higher dimensional black strings, in comparison to the stable ST black holes in Einstein gravity. It turns out that the GL instability of the ST black holes in the fourth-order gravity originates from the massiveness, but not a nature of fourth-order derivative theories giving ghost states.	48847	2.3529412746429443	1	3.3529999256134033
48	1702.03260	12181994	A Deterministic and Generalized Framework for Unsupervised Learning with Restricted Boltzmann Machines	Restricted Boltzmann machines (RBMs) are energy-based neural-networks which are commonly used as the building blocks for deep architectures neural architectures. In this work, we derive a deterministic framework for the training, evaluation, and use of RBMs based upon the Thouless-Anderson-Palmer (TAP) mean-field approximation of widely-connected systems with weak interactions coming from spin-glass theory. While the TAP approach has been extensively studied for fully-visible binary spin systems, our construction is generalized to latent-variable models, as well as to arbitrarily distributed real-valued spin systems with bounded support. In our numerical experiments, we demonstrate the effective deterministic training of our proposed models and are able to show interesting features of unsupervised learning which could not be directly observed with sampling. Additionally, we demonstrate how to utilize our TAP-based framework for leveraging trained RBMs as joint priors in denoising problems.	93645	1.1764706373214722	2	3.1760001182556152
49	1810.11868	53096845	Sequential anatomy localization in fetal echocardiography videos	Fetal heart motion is an important diagnostic indicator for structural detection and functional assessment of congenital heart disease. We propose an approach towards integrating deep convolutional and recurrent architectures that utilize localized spatial and temporal features of different anatomical substructures within a global spatiotemporal context for interpretation of fetal echocardiography videos. We formulate our task as a cardiac structure localization problem with convolutional architectures for aggregating global spatial context and detecting anatomical structures on spatial region proposals. This information is aggregated temporally by recurrent architectures to quantify the progressive motion patterns. We experimentally show that the resulting architecture combines anatomical landmark detection at the frame-level over multiple video sequences-with temporal progress of the associated anatomical motions to encode local spatiotemporal fetal heart dynamics and is validated on a real-world clinical dataset.	101292	0.0	3	3.0
50	1812.07454	119593891	A note on BPS structures and Gopakumar–Vafa invariants	We regard the work of Maulik and Toda, proposing a sheaf-theoretic approach to Gopakumar-Vafa invariants, as defining a BPS structure, that is, a collection of BPS invariants together with a central charge. Assuming their conjectures, we show that a canonical flat section of the flat connection corresponding to this BPS structure, at the level of formal power series, reproduces the Gromov-Witten partition function for all genera, up to some error terms in genus 0 and 1. This generalises a result of Bridgeland and Iwaki for the contribution from genus 0 Gopakumar-Vafa invariants.	104224	2.3529412746429443	1	3.3529999256134033
51	1311.5738	13713370	Search for continuous gravitational waves: improving robustness versus instrumental artifacts	The standard multidetector F-statistic for continuous gravitational waves is susceptible to false alarms from instrumental artifacts, for example monochromatic sinusoidal disturbances (“lines”). This vulnerability to line artifacts arises because the F-statistic compares the signal hypothesis to a Gaussian-noise hypothesis, and hence is triggered by anything that resembles the signal hypothesis more than Gaussian noise. Various ad-hoc veto methods to deal with such line artifacts have been proposed and used in the past. Here we develop a Bayesian framework that includes an explicit alternative hypothesis to model disturbed data. We introduce a simple line model that defines lines as signal candidates appearing only in one detector. This allows us to explicitly compute the odds between the signal hypothesis and an extended noise hypothesis, resulting in a new detection statistic that is more robust to instrumental artifacts. We present and discuss results from Monte-Carlo tests on both simulated data and on detector data from the fifth LIGO science run. We find that the line-robust statistic retains the detection power of the standard F-statistic in Gaussian noise. In the presence of line artifacts it is more sensitive, even compared to the popular F-statistic consistency veto, over which it improves by as much as a factor of two in detectable signal strength.	55721	0.5882353186607361	2	2.5880000591278076
52	1106.0872	16418133	Model of Opinion Spreading in Social Networks	"We proposed a new model, which capture the main difference between information and opinion spreading. In information spreading additional exposure to certain information has a small effect. Contrary, when an actor is exposed to 2 opinioned actors the probability to adopt the opinion is significant higher than in the case of contact with one such actor (called by J. Kleinberg ""the 0-1-2 effect""). In each time step if an actor does not have an opinion, we randomly choose 2 his network neighbors. If one of them has an opinion, the actor adopts opinion with some low probability, if two - with a higher probability. Opinion spreading was simulated on different real world social networks and similar random scale-free networks. The results show that small world structure has a crucial impact on tipping point time. The ""0-1-2"" effect causes a significant difference between ability of the actors to start opinion spreading. Actor is an influencer according to his topological position in the network."	13623	0.5882353186607361	4	4.5879998207092285
53	1901.05577	58014107	Generating Realistic Sequences of Customer-Level Transactions for Retail Datasets	In order to better engage with customers, retailers rely on extensive customer and product databases which allows them to better understand customer behaviour and purchasing patterns. This has long been a challenging task as customer modelling is a multi-faceted, noisy and time-dependent problem. The most common way to tackle this problem is indirectly through task-specific supervised learning prediction problems, with relatively little literature on modelling a customer by directly simulating their future transactions. In this paper we propose a method for generating realistic sequences of baskets that a given customer is likely to purchase over a period of time. Customer embedding representations are learned using a Recurrent Neural Network (RNN) which takes into account the entire sequence of transaction data. Given the customer state at a specific point in time, a Generative Adversarial Network (GAN) is trained to generate a plausible basket of products for the following week. The newly generated basket is then fed back into the RNN to update the customer's state. The GAN is thus used in tandem with the RNN module in a pipeline alternating between basket generation and customer state updating steps. This allows for sampling over a distribution of a customer's future sequence of baskets, which then can be used to gain insight into how to service the customer more effectively. The methodology is empirically shown to produce baskets that appear similar to real baskets and enjoy many common properties, including frequencies of different product types, brands, and prices. Furthermore, the generated data is able to replicate most of the strongest sequential patterns that exist between product types in the real data.	108917	4.117647171020508	10	14.118000030517578
54	1404.2265	119154966	The laws of planetary motion, derived from those of a harmonic oscillator (following Arnold)	Kepler's laws of planetary motion are deduced from those of a harmonic oscillator following Arnold. Conversely, the circular orbits through the Earth's center suggested by Galilei are consistent with an $r^{-5}$ potential as found before by Newton. Both the Kepler/oscillator correspondance and circular orbits are examples of dual potentials.	66393	0.5882353186607361	2	2.5880000591278076
55	1901.06403	58981728	Multisource Region Attention Network for Fine-Grained Object Recognition in Remote Sensing Imagery	Fine-grained object recognition concerns the identification of the type of an object among a large number of closely related subcategories. Multisource data analysis that aims to leverage the complementary spectral, spatial, and structural information embedded in different sources is a promising direction toward solving the fine-grained recognition problem that involves low between-class variance, small training set sizes for rare classes, and class imbalance. However, the common assumption of coregistered sources may not hold at the pixel level for small objects of interest. We present a novel methodology that aims to simultaneously learn the alignment of multisource data and the classification model in a unified framework. The proposed method involves a multisource region attention network that computes per-source feature representations, assigns attention scores to candidate regions sampled around the expected object locations by using these representations, and classifies the objects by using an attention-driven multisource representation that combines the feature representations and the attention scores from all sources. All components of the model are realized using deep neural networks and are learned in an end-to-end fashion. Experiments using RGB, multispectral, and LiDAR elevation data for classification of street trees showed that our approach achieved 64.2% and 47.3% accuracies for the 18-class and 40-class settings, respectively, which correspond to 13% and 14.3% improvement relative to the commonly used feature concatenation approach from multiple sources.	109294	0.0	3	3.0
56	1406.3781	1414571	From Stochastic Mixability to Fast Rates	Empirical risk minimization (ERM) is a fundamental learning rule for statistical learning problems where the data is generated according to some unknown distribution P and returns a hypothesis f chosen from a fixed class ℱ with small loss l. In the parametric setting, depending upon (l, ℱ, P) ERM can have slow (1/√n) or fast (1/n) rates of convergence of the excess risk as a function of the sample size n. There exist several results that give sufficient conditions for fast rates in terms of joint properties of l, ℱ, and P, such as the margin condition and the Bernstein condition. In the non-statistical prediction with expert advice setting, there is an analogous slow and fast rate phenomenon, and it is entirely characterized in terms of the mixability of the loss l (there being no role there for ℱ or P). The notion of stochastic mixability builds a bridge between these two models of learning, reducing to classical mixability in a special case. The present paper presents a direct proof of fast rates for ERM in terms of stochastic mixability of (l, ℱ, P), and in so doing provides new insight into the fast-rates phenomenon. The proof exploits an old result of Kemperman on the solution to the general moment problem. We also show a partial converse that suggests a characterization of fast rates for ERM in terms of stochastic mixability is possible.	71699	0.0	3	3.0
57	1407.7908	1351865	Social conformity despite individual preferences for distinctiveness	We demonstrate that individual behaviours directed at the attainment of distinctiveness can in fact produce complete social conformity. We thus offer an unexpected generative mechanism for this central social phenomenon. Specifically, we establish that agents who have fixed needs to be distinct and adapt their positions to achieve distinctiveness goals, can nevertheless self-organize to a limiting state of absolute conformity. This seemingly paradoxical result is deduced formally from a small number of natural assumptions and is then explored at length computationally. Interesting departures from this conformity equilibrium are also possible, including divergence in positions. The effect of extremist minorities on these dynamics is discussed. A simple extension is then introduced, which allows the model to generate and maintain social diversity, including multimodal distinctiveness distributions. The paper contributes formal definitions, analytical deductions and counterintuitive findings to the literature on individual distinctiveness and social conformity.	75317	1.1764706373214722	3	4.176000118255615
58	1906.02564	174802477	Analysis of Automatic Annotation Suggestions for Hard Discourse-Level Tasks in Expert Domains	Many complex discourse-level tasks can aid domain experts in their work but require costly expert annotations for data creation. To speed up and ease annotations, we investigate the viability of automatically generated annotation suggestions for such tasks. As an example, we choose a task that is particularly hard for both humans and machines: the segmentation and classification of epistemic activities in diagnostic reasoning texts. We create and publish a new dataset covering two domains and carefully analyse the suggested annotations. We find that suggestions have positive effects on annotation speed and performance, while not introducing noteworthy biases. Envisioning suggestion models that improve with newly annotated texts, we contrast methods for continuous model adjustment and suggest the most effective setup for suggestions in future expert tasks.	118594	0.5882353186607361	3	3.5880000591278076
59	1902.03373	60440482	An Optimal-Storage Approach to Semidefinite Programming using Approximate Complementarity	"This paper develops a new storage-optimal algorithm that provably solves generic semidefinite programs (SDPs) in standard form. This method is particularly effective for weakly constrained SDPs. The key idea is to formulate an approximate complementarity principle: Given an approximate solution to the dual SDP, the primal SDP has an approximate solution whose range is contained in the eigenspace with small eigenvalues of the dual slack matrix. For weakly constrained SDPs, this eigenspace has very low dimension, so this observation significantly reduces the search space for the primal solution. 
This result suggests an algorithmic strategy that can be implemented with minimal storage: 
(1) Solve the dual SDP approximately; 
(2) compress the primal SDP to the eigenspace with small eigenvalues of the dual slack matrix; 
(3) solve the compressed primal SDP. 
The paper also provides numerical experiments showing that this approach is successful for a range of interesting large-scale SDPs."	112975	1.7647058963775635	3	4.764999866485596
60	1211.4031	119284618	Effective conductance method for the primordial recombination spectrum	As atoms formed for the ﬁrst time during primordial recombination, they emitted bound-bound and free-bound radiation leading to spectral distortions to the cosmic microwave background. These distortions might become observable in the future with high-sensitivity spectrometers, and provide a new window into physical conditions in the early universe. The standard multilevel atom method habitually used to compute the recombination spectrum is computationally expensive, im-peding a detailed quantitative exploration of the information contained in spectral distortions thus far. In this work it is shown that the emissivity in optically thin allowed transitions can be factored into a computationally expensive but cosmology-independent part and a computationally cheap, cosmology-dependent part. The slow part of the computation consists in pre-computing temperature-dependent eﬀective “conductances”, linearly relating line or continuum intensity to departures from Saha equilibrium of the lowest-order excited states (2 s and 2 p ), that can be seen as “voltages”. The computation of these departures from equilibrium as a function of redshift is itself very fast, thanks to the eﬀective multilevel atom method introduced in an earlier work. With this factorization, the recurring cost of a single computation of the recombination spectrum is only a fraction of a second on a standard laptop, more than four orders of magnitude shorter than standard computations. The spectrum from helium recombination can be eﬃciently computed in an identical way, and a fast code computing the full primordial recombination spectrum with this method will be made publicly available soon.	32858	0.0	3	3.0
61	1412.6140	9041427	Whoo.ly: facilitating information seeking for hyperlocal communities using social media	Social media systems promise powerful opportunities for people to connect to timely, relevant information at the hyper local level. Yet, finding the meaningful signal in noisy social media streams can be quite daunting to users. In this paper, we present and evaluate Whoo.ly, a web service that provides neighborhood-specific information based on Twitter posts that were automatically inferred to be hyperlocal. Whoo.ly automatically extracts and summarizes hyperlocal information about events, topics, people, and places from these Twitter posts. We provide an overview of our design goals with Whoo.ly and describe the system including the user interface and our unique event detection and summarization algorithms. We tested the usefulness of the system as a tool for finding neighborhood information through a comprehensive user study. The outcome demonstrated that most participants found Whoo.ly easier to use than Twitter and they would prefer it as a tool for exploring their neighborhoods.	86435	1.1764706373214722	1	2.1760001182556152
62	1905.05944	155093253	Electric-field-induced Z2 topological phase transition in strained single bilayer Bi(111)	For controlling critical electric fields of the topological phase transition in a single bilayer Bi(111), we investigated topological phases in a strained system through first-principles calculations. We found a quadratic band touching semimetallic state at tensile strain ϵ = 0.5%. Around this strain, the topological phase can be switched to a trivial insulator by an infinitesimal electric field. The momentum positions at which Dirac cones appear in the electric-field-induced topological phase transition changed for the strain ϵ > 0.5% and ϵ < 0.5%. Our results indicate that this topological phase transition could be applied to novel spintronic devices.	115314	1.7647058963775635	1	2.765000104904175
63	1907.10706	198893641	The haloes and environments of nearby galaxies (HERON) – I. Imaging, sample characteristics, and envelope diameters	"
 We use a dedicated 0.7-m telescope to image the haloes of 119 galaxies in the Local Volume to μr ∼ 28–30 mag arcsec−2. The sample is primarily from the Two Micron All Sky Survey Large Galaxy Atlas (Jarrett et al. 2003) and extended to include nearby dwarf galaxies and more distant giant ellipticals, and spans fully the galaxy colour–magnitude diagram including the blue cloud and red sequence. We present an initial overview, including deep images of our galaxies. Our observations reproduce previously reported low surface brightness structures, including extended plumes in M 51, and a newly discovered tidally extended dwarf galaxy in NGC 7331. Low surface brightness structures, or ‘envelopes’, exceeding 50 kpc in diameter are found mostly in galaxies with MV < −20.5, and classic interaction signatures are infrequent. Defining a halo diameter at the surface brightness 28 mag arcsec−2, we find that halo diameter is correlated with total galaxy luminosity. Extended signatures of interaction are found throughout the galaxy colour–magnitude diagram without preference for the red or blue sequences, or the green valley. Large envelopes may be found throughout the colour–magnitude diagram with some preference for the bright end of the red sequence. Spiral and S0 galaxies have broadly similar sizes, but ellipticals extend to notably greater diameters, reaching 150 kpc. We propose that the extended envelopes of disc galaxies are dominated by an extension of the disc population rather than by a classical Population II halo."	127595	2.3529412746429443	2	4.353000164031982
64	1211.3102	154752602	Can we predict long-run economic growth?	For those concerned with the long-term value of their accounts, it can be a challenge to plan in the present for inflation-adjusted economic growth over coming decades. Here, I argue that there exists an economic constant that carries through time, and that this can help us to anticipate the more distant future: global economic wealth has a fixed link to civilization's overall rate of energy consumption from all sources; the ratio of these two quantities has not changed over the past 40 years that statistics are available. Power production and wealth rise equally quickly because civilization, like any other system in the universe, must consume and dissipate its energy reserves in order to sustain its current size. One perspective might be that financial wealth must ultimately collapse as we deplete our energy reserves. However, we can also expect that highly aggregated quantities like global wealth have inertia, and that growth rates must persist. Exceptionally rapid innovation in the two decades following 1950 allowed for unprecedented acceleration of inflation-adjusted rates of return. But today, real innovation rates are more stagnant. This means that, over the coming decade or so, global GDP and wealth should rise fairly steadily at an inflation-adjusted rate of about 2.2% per year.	32685	0.5882353186607361	4	4.5879998207092285
65	1412.2742	118712079	Ocean Wave Model and Wave Drift Caused by the Asymmetry of Crest and Trough	It follows from the review on classical wave models that the asymmetry of crest and trough is the direct cause for wave drift. Based on this, a new model of Lagrangian form is constructed. Relative to the Gerstner model, its improvement is reflected in the horizontal motion which includes an explicit drift term. On the one hand, the depth-decay factor for the new drift accords well with that of the particle’s horizontal velocity. It is more rational than that of Stokes drift. On the other hand, the new formula needs no Taylor expansion as for Stokes drift and is applicable for the waves with big slopes. In addition, the new formula can also yield a more rational magnitude for the surface drift than that of Stokes.	85405	0.5882353186607361	3	3.5880000591278076
66	1909.02809	202539178	#MeTooMaastricht: Building a chatbot to assist survivors of sexual harassment		135532	0.0	5	5.0
67	1307.5098	118728273	An electromagnetic analogue of gravitational wave memory	We present an electromagnetic analogue of gravitational wave memory. That is, we consider what change has occurred to a detector of electromagnetic radiation after the wave has passed. Rather than a distortion in the detector, as occurs in the gravitational wave case, we find a residual velocity (a ‘kick’) to the charges in the detector. In analogy with the two types of gravitational wave memory (‘ordinary’ and ‘nonlinear’) we find two types of electromagnetic kick.	46863	1.1764706373214722	1	2.1760001182556152
68	1403.1576	116992766	Calculations of resonance enhancement factor in axion-search tube-experiments	It is pointed out that oscillating current density, produced due to the coupling between an external magnetic field and the cosmic axion field, can excite the TM resonant modes inside an open-ended cavity (tube). By systematically solving the field equations of axion-electrodynamics we obtain explicit expressions for the oscillating fields induced inside a cylindrical tube. We calculate the enhancement factor when a resonance condition is met. While the power obtained for TM modes replicates the previous result, we emphasize that the knowledge of explicit field configurations inside a tube opens up new ways to design axion experiments including a recent proposal to detect the induced fields using a superconducting LC circuit. In addition, as an example, we estimate the induced fields in a cylindrical tube in the presence of a static uniform magnetic field applied only to a part of its volume.	63590	0.5882353186607361	2	2.5880000591278076
69	1909.11414	202750019	Inequality is rising where social network segregation interacts with urban topology		139291	4.705882549285889	5	9.706000328063965
70	1908.07371	201106801	Hierarchical Bayesian Personalized Recommendation: A Case Study and Beyond	Items in modern recommender systems are often organized in hierarchical structures. These hierarchical structures and the data within them provide valuable information for building personalized recommendation systems. In this paper, we propose a general hierarchical Bayesian learning framework, i.e., \emph{HBayes}, to learn both the structures and associated latent factors. Furthermore, we develop a variational inference algorithm that is able to learn model parameters with fast empirical convergence rate. The proposed HBayes is evaluated on two real-world datasets from different domains. The results demonstrate the benefits of our approach on item recommendation tasks, and show that it can outperform the state-of-the-art models in terms of precision, recall, and normalized discounted cumulative gain. To encourage the reproducible results, we make our code public on a git repo: \url{this https URL}.	132193	0.5882353186607361	2	2.5880000591278076
71	1110.3792	119301683	Incompressible fluids of the de Sitter horizon and beyond		15030	0.5882353186607361	2	2.5880000591278076
72	1906.05895	189897731	Learning to Forget for Meta-Learning	Few-shot learning is a challenging problem where the goal is to achieve generalization from only few examples. Model-agnostic meta-learning (MAML) tackles the problem by formulating prior knowledge as a common initialization across tasks, which is then used to quickly adapt to unseen tasks. However, forcibly sharing an initialization can lead to conflicts among tasks and the compromised (undesired by tasks) location on optimization landscape, thereby hindering the task adaptation. Further, we observe that the degree of conflict differs among not only tasks but also layers of a neural network. Thus, we propose task-and-layer-wise attenuation on the compromised initialization to reduce its influence. As the attenuation dynamically controls (or selectively forgets) the influence of prior knowledge for a given task and each layer, we name our method as L2F (Learn to Forget). The experimental results demonstrate that the proposed method provides faster adaptation and greatly improves the performance. Furthermore, L2F can be easily applied and improve other state-of-the-art MAML-based frameworks, illustrating its simplicity and generalizability.	120055	0.0	4	4.0
73	1908.03963	199543559	A Review of Cooperative Multi-Agent Deep Reinforcement Learning	Deep Reinforcement Learning has made significant progress in multi-agent systems in recent years. In this review article, we have mostly focused on recent papers on Multi-Agent Reinforcement Learning (MARL) than the older papers, unless it was necessary. Several ideas and papers are proposed with different notations, and we tried our best to unify them with a single notation and categorize them by their relevance. In particular, we have focused on five common approaches on modeling and solving multi-agent reinforcement learning problems: (I) independent-learners, (II) fully observable critic, (III) value function decomposition, (IV) consensus, (IV) learn to communicate. Moreover, we discuss some new emerging research areas in MARL along with the relevant recent papers. In addition, some of the recent applications of MARL in real world are discussed. Finally, a list of available environments for MARL research are provided and the paper is concluded with proposals on the possible research directions.	130619	0.5882353186607361	6	6.5879998207092285
74	1910.01460	203169900	3D Neighborhood Convolution: Learning Depth-Aware Features for RGB-D and RGB Semantic Segmentation	A key challenge for RGB-D segmentation is how to effectively incorporate 3D geometric information from the depth channel into 2D appearance features. We propose to model the effective receptive field of 2D convolution based on the scale and locality from the 3D neighborhood. Standard convolutions are local in the image space (u, v), often with a fixed receptive field of 3x3 pixels. We propose to define convolutions local with respect to the corresponding point in the 3D real world space (x, y, z), where the depth channel is used to adapt the receptive field of the convolution, which yields the resulting filters invariant to scale and focusing on the certain range of depth. We introduce 3D Neighborhood Convolution (3DN-Conv), a convolutional operator around 3D neighborhoods. Further, we can use estimated depth to use our RGB-D based semantic segmentation model from RGB input. Experimental results validate that our proposed 3DN-Conv operator improves semantic segmentation, using either ground-truth depth (RGB-D) or estimated depth (RGB).	141014	0.0	3	3.0
75	1309.6896	118500451	Observational issues in loop quantum cosmology	Quantum gravity is sometimes considered as a kind of metaphysical speculation. In this review, we show that, although still extremely difficult to reach, observational signatures can in fact be expected. The early universe is an invaluable laboratory to probe ‘Planck scale physics’. Focusing on loop quantum gravity as one of the best candidate for a non-perturbative and background-independent quantization of gravity, we detail some expected features.	51521	0.5882353186607361	3	3.5880000591278076
76	1301.1471	18096827	The Foster-Hart Measure of Riskiness for General Gambles	Foster and Hart proposed an operational measure of riskiness for discrete random variables. We show that their defining equation has no solution for many common continuous distributions. We show how to extend consistently the definition of riskiness to continuous random variables. For many continuous random variables, the risk measure is equal to the worst-case risk measure, i.e. the maximal possible loss incurred by that gamble. We also extend the Foster-Hart risk measure to dynamic environments for general distributions and probability spaces, and we show that the extended measure avoids bankruptcy in infinitely repeated gambles.	34839	0.0	6	6.0
77	1902.02625	119610647	On divisibility by primes in columns of character tables of symmetric groups		112607	1.1764706373214722	1	2.1760001182556152
78	1908.03182	199502012	Dynamic Scale Inference by Entropy Minimization	Given the variety of the visual world there is not one true scale for recognition: objects may appear at drastically different sizes across the visual field. Rather than enumerate variations across filter channels or pyramid levels, dynamic models locally predict scale and adapt receptive fields accordingly. The degree of variation and diversity of inputs makes this a difficult task. Existing methods either learn a feedforward predictor, which is not itself totally immune to the scale variation it is meant to counter, or select scales by a fixed algorithm, which cannot learn from the given task and data. We extend dynamic scale inference from feedforward prediction to iterative optimization for further adaptivity. We propose a novel entropy minimization objective for inference and optimize over task and structure parameters to tune the model to each input. Optimization during inference improves semantic segmentation accuracy and generalizes better to extreme scale variations that cause feedforward dynamic inference to falter.	130281	0.0	4	4.0
79	1901.01989	57721148	Towards Self-constructive Artificial Intelligence: Algorithmic basis (Part I)	"Artificial Intelligence frameworks should allow for ever more autonomous and general systems in contrast to very narrow and restricted (human pre-defined) domain systems, in analogy to how the brain works. Self-constructive Artificial Intelligence ($SCAI$) is one such possible framework. We herein propose that $SCAI$ is based on three principles of organization: self-growing, self-experimental and self-repairing. Self-growing: the ability to autonomously and incrementally construct structures and functionality as needed to solve encountered (sub)problems. Self-experimental: the ability to internally simulate, anticipate and take decisions based on these expectations. Self-repairing: the ability to autonomously re-construct a previously successful functionality or pattern of interaction lost from a possible sub-component failure (damage). To implement these principles of organization, a constructive architecture capable of evolving adaptive autonomous agents is required. We present Schema-based learning as one such architecture capable of incrementally constructing a myriad of internal models of three kinds: predictive schemas, dual (inverse models) schemas and goal schemas as they are necessary to autonomously develop increasing functionality. 
We claim that artificial systems, whether in the digital or in the physical world, can benefit very much form this constructive architecture and should be organized around these principles of organization. To illustrate the generality of the proposed framework, we include several test cases in structural adaptive navigation in artificial intelligence systems in Paper II of this series, and resilient robot motor control in Paper III of this series. Paper IV of this series will also include $SCAI$ for problem structural discovery in predictive Business Intelligence."	107312	0.5882353186607361	3	3.5880000591278076
80	1910.04420	42736128	Learning Beyond Predefined Label Space via Bayesian Nonparametric Topic Modelling		142315	0.5882353186607361	3	3.5880000591278076
81	1909.10122	202718775	Towards Best Experiment Design for Evaluating Dialogue System Output	To overcome the limitations of automated metrics (e.g. BLEU, METEOR) for evaluating dialogue systems, researchers typically use human judgments to provide convergent evidence. While it has been demonstrated that human judgments can suffer from the inconsistency of ratings, extant research has also found that the design of the evaluation task affects the consistency and quality of human judgments. We conduct a between-subjects study to understand the impact of four experiment conditions on human ratings of dialogue system output. In addition to discrete and continuous scale ratings, we also experiment with a novel application of Best-Worst scaling to dialogue evaluation. Through our systematic study with 40 crowdsourced workers in each task, we find that using continuous scales achieves more consistent ratings than Likert scale or ranking-based experiment design. Additionally, we find that factors such as time taken to complete the task and no prior experience of participating in similar studies of rating dialogue system output positively impact consistency and agreement amongst raters.	138788	0.0	3	3.0
82	1310.4494	119256268	Particle acceleration around a five-dimensional Kerr black hole	In the paper [1] authors have studied the center-ofmass (CM) energy of colliding two particles near the rotating black hole. They observed the divergence of the CM energy in the extreme rotating black hole case with the fine-tuning of the momentum of one of the particles (so called BSW process after the names of the authors - Banados, Silk, and West). The analysis of the CM energy of two colliding particles at the equatorial plane tends to extremely high energies for the extremal central black hole when it rotates with maximal speed and the maximal rotating black hole can be considered as high energy scale collider of normal and dark matter particles which can be detected by the observer at infinity. Since BSW process has been introduced the mechanism of particle acceleration near the black hole has been intensively studied by many of authors for the different space-time metric describing black hole. Authors of the Ref. [2] have studied two particles acceleration, the multiple scattering and their CM energy in case of non extreme rotating black hole. The acceleration mechanism of the particles when they are in the stable circular orbits has been considered in [3]. Recent studies have shown that the naked singularities that are formed due to the gravitational collapse of massive stars provide a suitable environment where particles could get accelerated and collide at arbitrarily high center-of-mass energies [4–7]. Authors of Ref. [8] studied the collision of two particles with the different rest masses moving in the equatorial plane in a Kerr-Taub-NUT spacetime and found that the CM energy depends not only on the rotation parameter,	52889	0.5882353186607361	2	2.5880000591278076
83	1307.2609	119326154	Quantum Mechanical Effects from Deformation Theory	We consider deformations of quantum mechanical operators by using the novel construction of warped convolutions. The deformation enables us to obtain several quantum mechanical effects where electromagnetic and gravitomagnetic fields play a role. Furthermore, a quantum plane can be defined by using the deformation techniques. This in turn gives an experimentally verifiable effect.	46191	1.1764706373214722	2	3.1760001182556152
84	1908.01790	199452984	Attribute-Guided Coupled GAN for Cross-Resolution Face Recognition	In this paper, we propose a novel attribute-guided cross-resolution (low-resolution to high-resolution) face recognition framework that leverages a coupled generative adversarial network (GAN) structure with adversarial training to find the hidden relationship between the low-resolution and high-resolution images in a latent common embedding subspace. The coupled GAN framework consists of two sub-networks, one dedicated to the low-resolution domain and the other dedicated to the high-resolution domain. Each sub-network aims to find a projection that maximizes the pair-wise correlation between the two feature domains in a common embedding subspace. In addition to projecting the images into a common subspace, the coupled network also predicts facial attributes to improve the cross-resolution face recognition. Specifically, our proposed coupled framework exploits facial attributes to further maximize the pair-wise correlation by implicitly matching facial attributes of the low and high-resolution images during the training, which leads to a more discriminative embedding subspace resulting in performance enhancement for cross-resolution face recognition. The efficacy of our approach compared with the state-of-the-art is demonstrated using the LFWA, Celeb-A, SCFace and UCCS datasets.	129632	0.5882353186607361	4	4.5879998207092285
85	1908.03032	199501702	On the Trade-off Between Consistency and Coverage in Multi-label Rule Learning Heuristics		130212	2.941176414489746	4	6.940999984741211
86	1907.11435	198953171	Challenges in Community Discovery on Temporal Networks		127934	1.1764706373214722	5	6.176000118255615
87	1208.3224	117925315	Electromagnetic Origin for Planck Mass and Dark Energy	The origin of dark energy remains to be one of the challenges of modern cosmology. We modify Jordan-Brans-Dicke theory using a vector field instead of a scalar field and theory becomes similar to a simple Einstein-aether theory. The time component of the vector field picks up a cosmological background value. Identifying the vector field to be the photon field, a small photon mass leads to late time inflation. The time dependent background electrical potential of the photon permeates the universe and explains the weakness of the gravitational interaction by coupling to curvature. This theory relates the smallness of the photon mass to the smallness of the Hubble parameter. The model predicted photon mass is far below observational constraints.	27768	0.0	3	3.0
88	1310.5719	2855024	Overlimiting current and shock electrodialysis in porous media.	Most electrochemical processes, such as electrodialysis, are limited by diffusion, but in porous media, surface conduction and electroosmotic flow also contribute to ionic flux. In this article, we report experimental evidence for surface-driven overlimiting current (faster than diffusion) and deionization shocks (propagating salt removal) in a porous medium. The apparatus consists of a silica glass frit (1 mm thick with a 500 nm mean pore size) in an aqueous electrolyte (CuSO4 or AgNO3) passing ionic current from a reservoir to a cation-selective membrane (Nafion). The current-voltage relation of the whole system is consistent with a proposed theory based on the electroosmotic flow mechanism over a broad range of reservoir salt concentrations (0.1 mM to 1.0 M) after accounting for (Cu) electrode polarization and pH-regulated silica charge. Above the limiting current, deionized water (≈10 μM) can be continuously extracted from the frit, which implies the existence of a stable shock propagating against the flow, bordering a depleted region that extends more than 0.5 mm across the outlet. The results suggest the feasibility of shock electrodialysis as a new approach to water desalination and other electrochemical separations.	53262	1.1764706373214722	1	2.1760001182556152
89	1907.03620	195833152	Contraction Clustering (RASTER): A Very Fast Big Data Algorithm for Sequential and Parallel Density-Based Clustering in Linear Time, Constant Memory, and a Single Pass	Clustering is an essential data mining tool for analyzing and grouping similar objects. In big data applications, however, many clustering algorithms are infeasible due to their high memory requirements and/or unfavorable runtime complexity. In contrast, Contraction Clustering (RASTER) is a single-pass algorithm for identifying density-based clusters with linear time complexity. Due to its favorable runtime and the fact that its memory requirements are constant, this algorithm is highly suitable for big data applications where the amount of data to be processed is huge. It consists of two steps: (1) a contraction step which projects objects onto tiles and (2) an agglomeration step which groups tiles into clusters. This algorithm is extremely fast in both sequential and parallel execution. Our quantitative evaluation shows that a sequential implementation of RASTER performs significantly better than various standard clustering algorithms. Furthermore, the parallel speedup is significant: on a contemporary workstation, an implementation in Rust processes a batch of 500 million points with 1 million clusters in less than 50 seconds on one core. With 8 cores, the algorithm is about four times faster.	124444	1.1764706373214722	2	3.1760001182556152
90	1909.12829	61317378	Decision Models for Workforce and Technology Planning in Services	Many of today’s service companies operate in a technology-oriented and knowledge-intensive environment while recruiting and training individuals from an increasingly diverse population. One of the resulting challenges for these service companies is ensuring strategic alignment between their two key resources—technology and workforce—through the resource planning and allocation processes. The traditional hierarchical decision approach to resource planning and allocation considers only technology planning as a strategic-level decision, with workforce recruiting and training planning as a subsequent tactical-level decision. However, two other decision approaches—joint and integrated—elevate workforce planning to the same strategic level as technology planning. Thus, we investigate the impact of strategically aligning technology and workforce decisions through the comparison of joint and integrated models to each other and to a baseline hierarchical model in terms of total cost. Numerical experiments are conducted to characterize key features of solutions provided by these approaches under conditions typically found in this type of service company. Our results show that the integrated model has the lowest cost across all conditions. This is because the integrated approach maintains a small but skilled workforce that can operate new and more advanced technology with higher capacity. However, the cost performance of the joint model is very close to the integrated model under a number of conditions and is easier to implement computationally and managerially, making it a good choice in many environments. Managerial insights derived from this study can serve as a valuable guide for choosing the proper decision approach for technology-oriented and knowledge-intensive service companies.	139960	0.5882353186607361	3	3.5880000591278076
91	1812.06975	125985138	The Risk of Contagion Spreading and its Optimal Control in the Economy	The global crisis of 2008 provoked a heightened interest among scientists to study the phenomenon, its propagation and negative consequences. The process of modelling the spread of a virus is commonly used in epidemiology. Conceptually, the spread of a disease among a population is similar to the contagion process in economy. This similarity allows considering the contagion in the world financial system using the same mathematical model of infection spread that is often used in epidemiology. Our research focuses on the dynamic behaviour of contagion spreading in the global financial network. The effect of infection by a systemic spread of risks in the network of national banking systems of countries is tested. An optimal control problem is then formulated to simulate a control that may avoid significant financial losses. The results show that the proposed approach describes well the reality of the world economy, and emphasizes the importance of international relations between countries on the financial stability.	103928	0.0	3	3.0
92	1909.13179	203593893	Modelling the health impact of food taxes and subsidies with price elasticities: The case for additional scaling of food consumption using the total food expenditure elasticity	Background Food taxes and subsidies are one intervention to address poor diets. Price elasticity (PE) matrices are commonly used to model the change in food purchasing. Usually a PE matrix is generated in one setting then applied to another setting with differing starting consumptions and prices of foods. This violates econometric assumptions resulting in likely mis-estimation of total food consumption. In this paper we demonstrate this problem, canvass possible options for rescaling all consumption after applying a PE matrix, and illustrate the use of a total food expenditure elasticity (TFEe; the expenditure elasticity for all food combined given the policy-induced change in the total price of food). We use case studies of: NZ$2 per 100g saturated fat (SAFA) tax, NZ$0.4 per 100g sugar tax, and a 20% fruit and vegetable (F&V) subsidy. Methods We estimated changes in food purchasing using a NZ PE matrix applied conventionally, and then with TFEe adjustment. Impacts were quantified for pre- to post-policy changes in total food expenditure and health adjusted life years (HALYs) for the total NZ population alive in 2011 over the rest of their lifetime using a multistate lifetable model. Results Two NZ studies gave TFEe’s of 0.68 and 0.83, with international estimates ranging from 0.46 to 0.90 (except a UK outlier of 0.04). Without TFEe adjustment, total food expenditure decreased with the tax policies and increased with the F&V subsidy–implausible directions of shift given economic theory and the external TFEe estimates. After TFEe adjustment, HALY gains reduced by a third to a half for the two taxes and reversed from an apparent health loss to a health gain for the F&V subsidy. With TFEe adjustment, HALY gains (in 1000’s) were: 1,805 (95% uncertainty interval 1,337 to 2,340) for the SAFA tax; 1,671 (1,220 to 2,269) for the sugar tax; and 953 (453 to 1,308) for the F&V subsidy. Conclusions If PE matrices are applied in settings beyond where they were derived, additional scaling is likely required. We suggest that the TFEe is a useful scalar, but we also encourage other researchers to examine this issue and propose alternative options.	140138	0.5882353186607361	2	2.5880000591278076
93	1908.00650	199405352	Bayesian gamma-negative binomial modeling of single-cell RNA sequencing data		129161	2.941176414489746	1	3.940999984741211
94	1804.03396	4753015	QA4IE: A Question Answering based Framework for Information Extraction		97748	1.1764706373214722	4	5.176000118255615
95	1908.10499	201650901	On Computing the Nonlinearity Interval in Parametric Semidefinite Optimization	This paper revisits the parametric analysis of semidefinite optimization problems with respect to the perturbation of the objective function along a fixed direction. We review the notions of invariancy set, nonlinearity interval, and transition point of the optimal partition, and we investigate their characterizations. We show that the set of transition points is finite and the continuity of the optimal set mapping, on the basis of Painlevé–Kuratowski set convergence, might fail on a nonlinearity interval. Under a local nonsingularity condition, we then develop a methodology, stemming from numerical algebraic geometry, to efficiently compute nonlinearity intervals and transition points of the optimal partition. Finally, we support the theoretical results by applying our procedure to some numerical examples.	133621	1.7647058963775635	1	2.765000104904175
96	1906.11881	195750508	Explicit Disentanglement of Appearance and Perspective in Generative Models	Disentangled representation learning finds compact, independent and easy-to-interpret factors of the data. Learning such has been shown to require an inductive bias, which we explicitly encode in a generative model of images. Specifically, we propose a model with two latent spaces: one that represents spatial transformations of the input data, and another that represents the transformed data. We find that the latter naturally captures the intrinsic appearance of the data. To realize the generative model, we propose a Variationally Inferred Transformational Autoencoder (VITAE) that incorporates a spatial transformer into a variational autoencoder. We show how to perform inference in the model efficiently by carefully designing the encoders and restricting the transformation class to be diffeomorphic. Empirically, our model separates the visual style from digit type on MNIST, separates shape and pose in images of human bodies and facial features from facial shape on CelebA.	122696	0.0	3	3.0
97	1403.0013	118361713	Three unsolved problems in physics of Pc1 magnetospheric waves	"In this paper we analyzed the open problems in the physics of ultra-low-frequency electromagnetic waves Pc1, which also known as the ""pearls"" (frequency range 0.2 -5 Hz). A number of theoretical problems arose because the standard model of excitation and propagation of waves in the magnetosphere, the foundations of which were laid half a century ago, is unable to explain observable properties of Pc1. In this article the problems are formulated and discussed in context of the general physics of geoelectromagnetic waves. In addition, one problem of experimental character is considered. The essence of the problem is that the observations testify in favor of the idea of some human impact on the Pc1 oscillation mode, but difficulties in interpreting of observations indicate the need for further experimental research."	63096	0.5882353186607361	2	2.5880000591278076
98	1908.08286	201313648	Learning stochastic differential equations using RNN with log signature features	"This paper contributes to the challenge of learning a function on streamed multimodal data through evaluation. The core of the result of our paper is the combination of two quite different approaches to this problem. One comes from the mathematically principled technology of signatures and log-signatures as representations for streamed data, while the other draws on the techniques of recurrent neural networks (RNN). The ability of the former to manage high sample rate streams and the latter to manage large scale nonlinear interactions allows hybrid algorithms that are easy to code, quicker to train, and of lower complexity for a given accuracy. 
We illustrate the approach by approximating the unknown functional as a controlled differential equation. Linear functionals on solutions of controlled differential equations are the natural universal class of functions on data streams. Following this approach, we propose a hybrid Logsig-RNN algorithm that learns functionals on streamed data. By testing on various datasets, i.e. synthetic data, NTU RGB+D 120 skeletal action data, and Chalearn2013 gesture data, our algorithm achieves the outstanding accuracy with superior efficiency and robustness."	132625	2.3529412746429443	4	6.353000164031982
99	1906.01038	174797999	Transforming Complex Sentences into a Semantic Hierarchy	We present an approach for recursively splitting and rephrasing complex English sentences into a novel semantic hierarchy of simplified sentences, with each of them presenting a more regular structure that may facilitate a wide variety of artificial intelligence tasks, such as machine translation (MT) or information extraction (IE). Using a set of hand-crafted transformation rules, input sentences are recursively transformed into a two-layered hierarchical representation in the form of core sentences and accompanying contexts that are linked via rhetorical relations. In this way, the semantic relationship of the decomposed constituents is preserved in the output, maintaining its interpretability for downstream applications. Both a thorough manual analysis and automatic evaluation across three datasets from two different domains demonstrate that the proposed syntactic simplification approach outperforms the state of the art in structural text simplification. Moreover, an extrinsic evaluation shows that when applying our framework as a preprocessing step the performance of state-of-the-art Open IE systems can be improved by up to 346% in precision and 52% in recall. To enable reproducible research, all code is provided online.	117795	1.1764706373214722	2	3.1760001182556152
100	1809.10241	52880882	Classifying Mammographic Breast Density by Residual Learning	Mammographic breast density, a parameter used to describe the proportion of breast tissue fibrosis, is widely adopted as an evaluation characteristic of the likelihood of breast cancer incidence. In this study, we present a radiomics approach based on residual learning for the classification of mammographic breast densities. Our method possesses several encouraging properties such as being almost fully automatic, possessing big model capacity and flexibility. It can obtain outstanding classification results without the necessity of result compensation using mammographs taken from different views. The proposed method was instantiated with the INbreast dataset and classification accuracies of 92.6% and 96.8% were obtained for the four BI-RADS (Breast Imaging and Reporting Data System) category task and the two BI-RADS category task,respectively. The superior performances achieved compared to the existing state-of-the-art methods along with its encouraging properties indicate that our method has a great potential to be applied as a computer-aided diagnosis tool.	100433	4.117647171020508	2	6.118000030517578
101	1908.10402	201651902	A Near-Optimal Change-Detection Based Algorithm for Piecewise-Stationary Combinatorial Semi-Bandits	We investigate the piecewise-stationary combinatorial semi-bandit problem. Compared to the original combinatorial semi-bandit problem, our setting assumes the reward distributions of base arms may change in a piecewise-stationary manner at unknown time steps. We propose an algorithm, GLR-CUCB, which incorporates an efficient combinatorial semi-bandit algorithm, CUCB, with an almost parameter-free change-point detector, the Generalized Likelihood Ratio Test (GLRT). Our analysis shows that the regret of GLR-CUCB is upper bounded by O(√NKT log T), where N is the number of piecewise-stationary segments, K is the number of base arms, and T is the number of time steps. As a complement, we also derive a nearly matching regret lower bound on the order of Ω(√NKT), for both piecewise-stationary multi-armed bandits and combinatorial semi-bandits, using information-theoretic techniques and judiciously constructed piecewise-stationary bandit instances. Our lower bound is tighter than the best available regret lower bound, which is Ω(√T). Numerical experiments on both synthetic and real-world datasets demonstrate the superiority of GLR-CUCB compared to other state-of-the-art algorithms.	133579	1.7647058963775635	5	6.764999866485596
102	1902.08283	67856312	Capuchin: Causal Database Repair for Algorithmic Fairness	Fairness is increasingly recognized as a critical component of machine learning systems. However, it is the underlying data on which these systems are trained that often reflect discrimination, suggesting a database repair problem. Existing treatments of fairness rely on statistical correlations that can be fooled by statistical anomalies, such as Simpson's paradox. Proposals for causality-based definitions of fairness can correctly model some of these situations, but they require specification of the underlying causal models. In this paper, we formalize the situation as a database repair problem, proving sufficient conditions for fair classifiers in terms of admissible variables as opposed to a complete causal model. We show that these conditions correctly capture subtle fairness violations. We then use these conditions as the basis for database repair algorithms that provide provable fairness guarantees about classifiers trained on their training labels. We evaluate our algorithms on real data, demonstrating improvement over the state of the art on multiple fairness metrics proposed in the literature while retaining high utility.	113561	0.0	5	5.0
103	1908.03592	199543803	The Reliability of the Low-latency Estimation of Binary Neutron Star Chirp Mass	The LIGO and Virgo Collaborations currently conduct searches for gravitational waves from compact binary coalescences in real time. For promising candidate events, a sky map and distance estimation are released in low latency to facilitate their electromagnetic follow-up. Currently, no information is released about the masses of the compact objects. Recently, Margalit & Metzger suggested that knowledge of the chirp mass of the detected binary neutron stars could be useful to prioritize the electromagnetic follow-up effort, and urged the LIGO-Virgo collaboration to release chirp mass information in low latency. One might worry that low-latency searches for compact binaries make simplifying assumptions that could introduce biases in the mass parameters: neutron stars are treated as point particles with dimensionless spins below 0.05 perfectly aligned with the orbital angular momentum. Furthermore, the template bank used to search for them has a finite resolution. In this paper we show that none of these limitations can introduce chirp mass biases larger than ∼10−3 M⊙. Even the total mass is usually accurately estimated, with biases smaller than 6%. The mass ratio and effective inspiral spins, on the other hand, can suffer from more severe biases.	130469	5.882352828979492	2	7.881999969482422
104	1907.06981	196831562	Astro2020 APC White Paper: Elevating the Role of Software as a Product of the Research Enterprise	Software is a critical part of modern research, and yet there are insufficient mechanisms in the scholarly ecosystem to acknowledge, cite, and measure the impact of research software. The majority of academic fields rely on a one-dimensional credit model whereby academic articles (and their associated citations) are the dominant factor in the success of a researcher's career. In the petabyte era of astronomical science, citing software and measuring its impact enables academia to retain and reward researchers that make significant software contributions. These highly skilled researchers must be retained to maximize the scientific return from petabyte-scale datasets. Evolving beyond the one-dimensional credit model requires overcoming several key challenges, including the current scholarly ecosystem and scientific culture issues. This white paper will present these challenges and suggest practical solutions for elevating the role of software as a product of the research enterprise.	125924	10.0	1	11.0
105	1412.3408	119213480	High School Students' Understandings and Representations of the Electric Field	This study investigates the representations and understandings of electric fields expressed by Chinese high school students 15 to 16 years old who have not received high school level physics instruction. The physics education research literature has reported students conceptions of electric fields post-instruction as indicated by students performance on textbook-style questions. It has, however, inadequately captured student ideas expressed in other situations yet informative to educational research. In this study, we explore students ideas of electric fields pre-instruction as shown by students representations produced in open-ended activities. 92 participant students completed a worksheet that involved drawing comic strips about electric charges as characters of a cartoon series. Three students who had spontaneously produced arrow diagrams were interviewed individually after class. We identified nine ideas related to electric fields that these three students spontaneously leveraged in the comic strip activity. In this paper, we describe in detail each idea and its situated context. As most research in the literature has understood students as having relatively fixed conceptions and mostly identified divergences in those conceptions from canonical targets, this study shows students reasoning to be more variable in particular moments, and that variability includes common sense resources that can be productive for learning about electric fields.	85628	0.5882353186607361	3	3.5880000591278076
106	1309.2938	118569735	Exoplanet transit variability: bow shocks and winds around HD 189733b	By analogy with the solar system, it is believed that stellar winds will form bow shocks around exoplanets. For hot Jupiters the bow shock will not form directly between the planet and the star, causing an asymmetric distribution of mass around the exoplanet and hence an asymmetric transit. As the planet orbits thorough varying wind conditions, the strength and geometry of its bow shock will change, thus producing transits of varying shape. We model this process using magnetic maps of HD 189733 taken one year apart, coupled with a 3D stellar wind model, to determine the local stellar wind conditions throughout the orbital path of the planet. We predict the time-varying geometry and density of the bow shock that forms around the magnetosphere of the planet and simulate transit light curves. Depending on the nature of the stellar magnetic field, and hence its wind, we find that both the transit duration and ingress time can vary when compared to optical light curves. We conclude that consecutive near-UV transit light curves may vary significantly and can therefore provide an insight into the structure and evolution of the stellar wind.	50439	0.5882353186607361	3	3.5880000591278076
107	1606.07356	12304778	Analyzing the Behavior of Visual Question Answering Models	"Recently, a number of deep-learning based models have been proposed for the task of Visual Question Answering (VQA). The performance of most models is clustered around 60-70%. In this paper we propose systematic methods to analyze the behavior of these models as a first step towards recognizing their strengths and weaknesses, and identifying the most fruitful directions for progress. We analyze two models, one each from two major classes of VQA models -- with-attention and without-attention and show the similarities and differences in the behavior of these models. We also analyze the winning entry of the VQA Challenge 2016. 
Our behavior analysis reveals that despite recent progress, today's VQA models are ""myopic"" (tend to fail on sufficiently novel instances), often ""jump to conclusions"" (converge on a predicted answer after 'listening' to just half the question), and are ""stubborn"" (do not change their answers across images)."	92369	0.0	3	3.0
108	1812.08356	119355521	Dark Matter Distribution of Four Low-z Clusters of Galaxies	We present here the weak gravitational lensing detection of four nearby galaxy clusters in the southern sky: A2029, A85, A1606, and A2457. The weak lensing detections of A1606 and A2457 are the first in the literature. This work capitalizes on the wide field of view of the Dark Energy Camera at the Cerro Tololo Inter-American Observatory, which we use to obtain deep, multiwavelength imaging of all targets. We publish maps of the clusters’ projected mass distributions and obtain the M200 of their clusters through Navarro–Frenk–White profile fits to the 2D tangential ellipticity signal.	104830	1.1764706373214722	1	2.1760001182556152
109	1901.01318	86845563	Generalized Kasha's Scheme for Classifying Two-Dimensional Excitonic Molecular Aggregates: Temperature Dependent Absorption Peak Frequency Shift	Author(s): Chuang, Chern; Bennett, Doran IG; Caram, Justin R; Aspuru-Guzik, Alan; Bawendi, Moungi G; Cao, Jianshu | Abstract: We propose a generalized theoretical framework for classifying two-dimensional (2D) excitonic molecular aggregates based on an analysis of temperature dependent spectra. In addition to the monomer-aggregate absorption peak shift, which defines the conventional J- and H-aggregates, we incorporate the peak shift associated with increasing temperature as a measure to characterize the exciton band structure. First we show that there is a one-to-one correspondence between the monomer-aggregate and the T-dependent peak shifts for Kasha's well-established model of 1D aggregates, where J-aggregates exhibit further redshift upon increasing temperature and H-aggregates exhibit further blueshift. On the contrary, 2D aggregate structures are capable of supporting the two other combinations: blueshifting J-aggregates and redshifting H-aggregates, owing to their more complex exciton band structures. Secondly, using spectral lineshape theory, the T-dependent shift is associated with the relative abundance of states on each side of the bright state. We further establish that the density of states can be connected to the microscopic packing condition leading to these four classes of aggregates by separately considering the short and long-range contribution to the excitonic couplings. In particular the T-dependent shift is shown to be an unambiguous signature for the sign of net short-range couplings: Aggregates with net negative (positive) short-range couplings redshift (blueshift) with increasing temperature. Lastly, comparison with experiments shows that our theory can be utilized to quantitatively account for the observed but previously unexplained T-dependent absorption lineshapes. Thus, our work provides a firm ground for elucidating the structure-function relationships for molecular aggregates and is fully compatible with existing experimental and theoretical structure characterization tools.	107059	0.5882353186607361	2	2.5880000591278076
110	1901.11186	59523658	On Intra-Class Variance for Deep Learning of Classifiers	Abstract A novel technique for deep learning of image classifiers is presented. The learned CNN models higher offer better separation of deep features (also known as embedded vectors) measured by Euclidean proximity and also no deterioration of the classification results by class membership probability. The latter feature can be used for enhancing image classifiers having the classes at the model’s exploiting stage different from from classes during the training stage. While the Shannon information of SoftMax probability for target class is extended for mini-batch by the intra-class variance, the trained network itself is extended by the Hadamard layer with the parameters representing the class centers. Contrary to the existing solutions, this extra neural layer enables interfacing of the training algorithm to the standard stochastic gradient optimizers, e.g. AdaM algorithm. Moreover, this approach makes the computed centroids immediately adapting to the updating embedded vectors and finally getting the comparable accuracy in less epochs.	111269	3.529411792755127	3	6.5289998054504395
111	1901.03712	119483913	Compact Galaxies at intermediate redshifts quench faster than normal-sized Galaxies	Massive quiescent compact galaxies have been discovered at high redshifts, associated with rapid compaction and cessation of star formation (SF). In this work we set out to quantify the time-scales in which SF is quenched in compact galaxies at intermediate redshifts. For this, we select a sample of green valley galaxies within the COSMOS field in the midst of quenching their SF at $0.5<z<1.0$ that exhibit varying degrees of compactness. Based on the H$\delta$ absorption line and the 4000 \AA \ break of coadded zCOSMOS spectra for sub-samples of normal-sized and compact galaxies we determine quenching time-scales as a function of compactness. We find that the SF quenching time-scales in green valley compact galaxies are much shorter than in normal-sized ones. In an effort to understand this trend, we use the Illustris simulation to trace the evolution of the SF history, the growth rate of the central super massive black hole (SMBH) {\bf and the AGN-feedback in compact and normal-sized galaxies. We find that the key difference between their SF quenching time-scales is linked to the mode of the AGN-feedback. In the compact galaxies predominates the kinematic-mode, which is highly efficient at quenching the SF by depleting the internal gas. On the normal-sized galaxies, the prevailing thermal-mode injects energy in the circumgalactic gas, impeding the cold gas supply and quenching the SF via the slower strangulation mechanism.} These results are consistent with the violent disk instability and gas-rich mergers scenarios, followed by strong AGN and stellar feedback. Although this kind of event is most expected to occur at $z=2-3$, we find evidences that the formation of compact quiescent galaxies can occur at $z<1$.	108146	0.5882353186607361	2	2.5880000591278076
112	1812.08789	197935291	Steerable ePCA: Rotationally Invariant Exponential Family PCA	"In photon-limited imaging, the pixel intensities are affected by photon count noise. Many applications require an accurate estimation of the covariance of the underlying 2-D clean images. For example, in X-ray free electron laser (XFEL) single molecule imaging, the covariance matrix of 2-D diffraction images is used to reconstruct the 3-D molecular structure. Accurate estimation of the covariance from low-photon-count images must take into account that pixel intensities are Poisson distributed, hence the classical sample covariance estimator is highly biased. Moreover, in single molecule imaging, including in-plane rotated copies of all images could further improve the accuracy of covariance estimation. In this paper we introduce an efficient and accurate algorithm for covariance matrix estimation of count noise 2-D images, including their uniform planar rotations and possibly reflections. Our procedure, <italic>steerable</italic> <inline-formula> <tex-math notation=""LaTeX"">$e$ </tex-math></inline-formula> <italic>PCA</italic>, combines in a novel way two recently introduced innovations. The first is a methodology for principal component analysis (PCA) for Poisson distributions, and more generally, exponential family distributions, called <inline-formula> <tex-math notation=""LaTeX"">$e$ </tex-math></inline-formula>PCA. The second is steerable PCA, a fast and accurate procedure for including all planar rotations when performing PCA. The resulting principal components are invariant to the rotation and reflection of the input images. We demonstrate the efficiency and accuracy of steerable <inline-formula> <tex-math notation=""LaTeX"">$e$ </tex-math></inline-formula>PCA in numerical experiments involving simulated XFEL datasets and rotated face images from Yale Face Database B."	105110	1.1764706373214722	4	5.176000118255615
113	1901.00943	57572899	Self-supervised Learning of Image Embedding for Continuous Control	Operating directly from raw high dimensional sensory inputs like images is still a challenge for robotic control. Recently, Reinforcement Learning methods have been proposed to solve specific tasks end-to-end, from pixels to torques. However, these approaches assume the access to a specified reward which may require specialized instrumentation of the environment. Furthermore, the obtained policy and representations tend to be task specific and may not transfer well. In this work we investigate completely self-supervised learning of a general image embedding and control primitives, based on finding the shortest time to reach any state. We also introduce a new structure for the state-action value function that builds a connection between model-free and model-based methods, and improves the performance of the learning algorithm. We experimentally demonstrate these findings in three simulated robotic tasks.	106878	1.7647058963775635	5	6.764999866485596
114	1909.02392	202122345	Rewarding Coreference Resolvers for Being Consistent with World Knowledge	Unresolved coreference is a bottleneck for relation extraction, and high-quality coreference resolvers may produce an output that makes it a lot easier to extract knowledge triples. We show how to improve coreference resolvers by forwarding their input to a relation extraction system and reward the resolvers for producing triples that are found in knowledge bases. Since relation extraction systems can rely on different forms of supervision and be biased in different ways, we obtain the best performance, improving over the state of the art, using multi-task reinforcement learning.	135328	2.3529412746429443	1	3.3529999256134033
115	1401.4540	18747204	From mobile phone data to the spatial structure of cities		59776	0.5882353186607361	3	3.5880000591278076
116	1410.7172	7989893	Heteroscedastic Treed Bayesian Optimisation	Optimising black-box functions is important in many disciplines, such as tuning machine learning models, robotics, finance and mining exploration. Bayesian optimisation is a state-of-the-art technique for the global optimisation of black-box functions which are expensive to evaluate. At the core of this approach is a Gaussian process prior that captures our belief about the distribution over functions. However, in many cases a single Gaussian process is not flexible enough to capture non-stationarity in the objective function. Consequently, heteroscedasticity negatively affects performance of traditional Bayesian methods. In this paper, we propose a novel prior model with hierarchical parameter learning that tackles the problem of non-stationarity in Bayesian optimisation. Our results demonstrate substantial improvements in a wide range of applications, including automatic machine learning and mining exploration.	81941	0.0	3	3.0
117	1907.03672	195833038	In Search of Stable Geometric Structures	We will look for stable structures in four situations and discuss what is known and unknown.	124459	1.1764706373214722	1	2.1760001182556152
118	1908.09844	201646577	Machine-assisted Semi-Simulation Model (MSSM): Estimating Galactic Baryonic Properties from Their Dark Matter Using A Machine Trained on Hydrodynamic Simulations	"
 We present a pipeline to estimate baryonic properties of a galaxy inside a dark matter (DM) halo in DM-only simulations using a machine trained on high-resolution hydrodynamic simulations. As an example, we use the IllustrisTNG hydrodynamic simulation of a (75  h−1Mpc)3 volume to train our machine to predict e.g., stellar mass and star formation rate in a galaxy-sized halo based purely on its DM content. An extremely randomized tree (ERT) algorithm is used together with multiple novel improvements we introduce here such as a refined error function in machine training and two-stage learning. Aided by these improvements, our model demonstrates a significantly increased accuracy in predicting baryonic properties compared to prior attempts — in other words, the machine better mimics IllustrisTNG’s galaxy-halo correlation. By applying our machine to the MultiDark-Planck DM-only simulation of a large (1  h−1Gpc)3 volume, we then validate the pipeline that rapidly generates a galaxy catalogue from a DM halo catalogue using the correlations the machine found in IllustrisTNG. We also compare our galaxy catalogue with the ones produced by popular semi-analytic models (SAMs). Our so-called machine-assisted semi-simulation model (MSSM) is shown to be largely compatible with SAMs, and may become a promising method to transplant the baryon physics of galaxy-scale hydrodynamic calculations onto a larger-volume DM-only run. We discuss the benefits that machine-based approaches like this entail, as well as suggestions to raise the scientific potential of such approaches."	133344	0.5882353186607361	2	2.5880000591278076
119	1805.05838	21677034	Understanding and Controlling User Linkability in Decentralized Learning	"Machine Learning techniques are widely used by online services (e.g. Google, Apple) in order to analyze and make predictions on user data. As many of the provided services are user-centric (e.g. personal photo collections, speech recognition, personal assistance), user data generated on personal devices is key to provide the service. In order to protect the data and the privacy of the user, federated learning techniques have been proposed where the data never leaves the user's device and ""only"" model updates are communicated back to the server. In our work, we propose a new threat model that is not concerned with learning about the content - but rather is concerned with the linkability of users during such decentralized learning scenarios. 
We show that model updates are characteristic for users and therefore lend themselves to linkability attacks. We show identification and matching of users across devices in closed and open world scenarios. In our experiments, we find our attacks to be highly effective, achieving 20x-175x chance-level performance. 
In order to mitigate the risks of linkability attacks, we study various strategies. As adding random noise does not offer convincing operation points, we propose strategies based on using calibrated domain-specific data; we find these strategies offers substantial protection against linkability threats with little effect to utility."	98123	0.0	4	4.0
120	1906.01705	199543410	Topological currents in the bulk in the absence of gapless states	We provide evidence that, alongside symmetry protected edge states, two-dimensional topological phases also support bulk currents. These currents are activated by local potential gradients in the bulk, while all parts of the system are adiabatically connected to the same phase. To understand their origin one can view the bulk of a homogeneous topological insulator as a perfectly entangled state of pairs of edge-like currents, adding up to a zero net flow. Potential gradients strain those states, progressively disentangling the hidden currents through a transfer of population. This produces a localised bulk current that is transverse to the strain, even when the potential is always below the energy gap, where one expects only edge currents to appear. Bulk currents are topologically protected and behave like edge currents under external influence, such as temperature or local disorder. The resilience and the tuning of bulk currents with local potentials makes them an appealing medium for technological applications.	118122	1.7647058963775635	1	2.765000104904175
121	1908.03468	199528335	On the accuracy of symplectic integrators for secularly evolving planetary systems	"
 Symplectic integrators have made it possible to study the long-term evolution of planetary systems with direct N-body simulations. In this paper we reassess the accuracy of such simulations by running a convergence test on 20 Myr integrations of the Solar System using various symplectic integrators. We find that the specific choice of metric for determining a simulation’s accuracy is important. Only looking at metrics related to integrals of motions such as the energy error can overestimate the accuracy of a method. As one specific example, we show that symplectic correctors do not improve the accuracy of secular frequencies compared to the standard Wisdom–Holman method without symplectic correctors, despite the fact that the energy error is three orders of magnitudes smaller. We present a framework to trace the origin of this apparent paradox to one term in the shadow Hamiltonian. Specifically, we find a term that leads to negligible contributions to the energy error but introduces non-oscillatory errors that result in artificial periastron precession. This term is the dominant error when determining secular frequencies of the system. We show that higher order symplectic methods such as the Wisdom–Holman method with a modified kernel or the SABAC family of integrators perform significantly better in secularly evolving systems because they remove this specific term."	130410	2.941176414489746	2	4.940999984741211
122	1907.11394	198953251	A comparative study of high-recall real-time semantic segmentation based on swift factorized network	Semantic Segmentation (SS) is the task to assign a semantic label to each pixel of the observed images, which is of crucial significance for autonomous vehicles, navigation assistance systems for the visually impaired, and augmented reality devices. However, there is still a long way for SS to be put into practice as there are two essential challenges that need to be addressed: efficiency and evaluation criterions for practical application. For specific application scenarios, different criterions need to be adopted. Recall rate is an important criterion for many tasks like autonomous vehicles. For autonomous vehicles, we need to focus on the detection of the traffic objects like cars, buses, and pedestrians, which should be detected with high recall rates. In other words, it is preferable to detect it wrongly than miss it, because the other traffic objects will be dangerous if the algorithm miss them and segment them as safe roadways. In this paper, our main goal is to explore possible methods to attain high recall rate. Firstly, we propose a real-time SS network named Swift Factorized Network (SFN). The proposed network is adapted from SwiftNet, whose structure is a typical U-shape structure with lateral connections. Inspired by ERFNet and Global Convolution Networks (GCNet), we propose two different blocks to enlarge valid receptive field. They do not take up too much calculation resources, but significantly enhance the performance compared with the baseline network. Secondly, we explore three ways to achieve higher recall rate, i.e loss function, classifier and decision rules. We perform a comprehensive set of experiments on state-of-the-art datasets including CamVid and Cityscapes. We demonstrate that our SS convolutional neural networks reach excellent performance. Furthermore, we make a detailed analysis and comparison of the three proposed methods on the promotion of recall rate.	127922	0.0	3	3.0
123	1201.4371	119301038	Picogauss magnetic fields in voids from ultra-high energy cosmic rays	We argue that ultra-high energy cosmic rays emitted by galaxy clusters result in electric currents in filaments of the large-scale structure that are sufficient to generate magnetic fields in voids of the magnitude of ~1e-12 G and the coherence length of up to tens of Mpc. These fields satisfy both the lower and upper observational bounds on magnetic fields in voids without a need of any primordial component.	18192	0.0	3	3.0
124	1410.6791	3175113	Bayesian Manifold Learning: The Locally Linear Latent Variable Model (LL-LVM)	We introduce the Locally Linear Latent Variable Model (LL-LVM), a probabilistic model for non-linear manifold discovery that describes a joint distribution over observations, their manifold coordinates and locally linear maps conditioned on a set of neighbourhood relationships. The model allows straightforward variational optimisation of the posterior distribution on coordinates and locally linear maps from the latent space to the observation space given the data. Thus, the LL-LVM encapsulates the local-geometry preserving intuitions that underlie non-probabilistic methods such as locally linear embedding (LLE). Its probabilistic semantics make it easy to evaluate the quality of hypothesised neighbourhood relationships, select the intrinsic dimensionality of the manifold, construct out-of-sample extensions and to combine the manifold model with additional probabilistic models that capture the structure of coordinates within the manifold.	81843	0.5882353186607361	3	3.5880000591278076
125	1901.06783	58981386	Dynamic Curriculum Learning for Imbalanced Data Classification	Human attribute analysis is a challenging task in the field of computer vision. One of the significant difficulties is brought from largely imbalance-distributed data. Conventional techniques such as re-sampling and cost-sensitive learning require prior-knowledge to train the system. To address this problem, we propose a unified framework called Dynamic Curriculum Learning (DCL) to adaptively adjust the sampling strategy and loss weight in each batch, which results in better ability of generalization and discrimination. Inspired by curriculum learning, DCL consists of two-level curriculum schedulers: (1) sampling scheduler which manages the data distribution not only from imbalance to balance but also from easy to hard; (2) loss scheduler which controls the learning importance between classification and metric learning loss. With these two schedulers, we achieve state-of-the-art performance on the widely used face attribute dataset CelebA and pedestrian attribute dataset RAP.	109456	0.5882353186607361	2	2.5880000591278076
126	1909.10059	202719342	"On the Essential Spectrum of Schr\""odinger Operators on Graphs"	"This work studies geometrical characterizations of the essential spectrum $\sigma_{\text ess}$ of Schrodinger operators on graphs. Especially we focus on generalizing characterizations which are given in terms of the concept of right limits. Intuitively the set of right limits of a Schrodinger operator $H$ on $\ell^2(\mathbb{N})$ includes the limit operators which are obtained by a sequence of left-shifts (moving away to infinity) of $H$. One characterization, which is known for such operators is that $\sigma_{\text ess}(H)$ is equal to the union over the spectra of right limits of $H$. Additionally, the essential spectrum equals to the union over the sets of ""eigenvalues"" corresponding to bounded eigenfunctions of the right limits of $H$. The first characterization above (and its generalization to $\mathbb{Z}^n$) is essentially due to a work by Last-Simon from 2006. In this work we study the possibility of generalizing these characterizations of $\sigma_{ess}(H)$ to Schrodinger operators on graphs. The work is composed of the following chapters: 
Chapter 1. An overview, including the setting and the proposed generalization of the concept of right limits to general graphs. 
Chapter 2. On extending the original Last-Simon argument for the proof of the first characterization to graphs. We give both a partial positive result and an example of a graph on which this characterization fails. 
Chapter 3. A Focus on characterizing the essential spectrum on trees. 
Chapter 4. A study of the possible generalization of the second characterization to graphs, from which we also get the first characterization on a general family of graphs."	138771	3.529411792755127	1	4.5289998054504395
127	1901.11008	59523747	GAMer 2: A system for 3D mesh processing of cellular electron micrographs	Recent advances in electron microscopy have, for the first time, enabled imaging of single cells in 3D at a nanometer length scale resolution. An uncharted frontier for in silico biology is the ability to simulate cellular processes using these observed geometries. Enabling such simulations will require a system for going from electron micrographs to 3D volume meshes, which can then form the basis of computer simulations of such processes using numerical techniques such as the Finite Element Method (FEM). In this paper, we develop an end-to-end pipeline for this task by adapting and extending computer graphics mesh processing and smoothing algorithms. Our workflow makes use of our recently rewritten mesh processing software, GAMer 2, which implements several mesh conditioning algorithms and serves as a platform to connect different pipeline steps. We apply this pipeline to a series of electron micrographs of neuronal dendrite morphology explored at three different length scales and demonstrate that the resultant meshes are suitable for finite element simulations. Our pipeline, which consists of free and open-source community driven tools, is a step towards routine physical simulations of biological processes in realistic geometries. We posit that a new frontier at the intersection of computational technologies and single cell biology is now open. Innovations in algorithms to reconstruct and simulate cellular length scale phenomena based on emerging structural data will enable realistic physical models and advance discovery. Author summary 3D imaging of cellular components and associated reconstruction methods have made great strides in the past decade, opening windows into the complex intraceullar organization. These advances also mean that computational tools need to be developed to work with these images not just for purposes of visualization but also for biophysical simulations. Here, we describe a pipeline that takes images from electron microscopy as input and produces smooth surface and volume meshes as output. These meshes are suitable for building high-quality finite element simulations of cellular processes modeled by ordinary and partial differential equations, bringing us closer to realizing the goal of generating high-resolution simulations of such phenomena in realistic geometries. We demonstrate the utility of this pipeline by meshing 3D reconstructions of dendritic spines, calculating the curvatures of the different component membranes, and conducting finite-element simulations of reaction-diffusion equations using the generated meshes. The software tools employed in our pipeline are community driven, open source, and free. We believe that technologies such as those presented will enable a new frontier in biophysical simulations in realistic geometries.	111175	0.5882353186607361	2	2.5880000591278076
128	1402.7111	118448023	General relativistic laser interferometric observables of the GRACE-Follow-On mission	We develop a high-precision model for laser ranging interferometric (LRI) observables of the GRACE Follow-On (GRACE-FO) mission. For this, we study the propagation of an electromagnetic wave in the gravitational field in the vicinity of an extended body, in the post-Newtonian approximation of the general theory of relativity. We present a general relativistic model for the phase of a plane wave that accounts for contributions of all the multipoles of the gravitating body, its angular momentum, as well as the contribution of tidal fields produced by external sources. We develop a new approach to model a coherent signal transmission in the gravitational field of the solar system that relies on a relativistic treatment of the phase. We use this approach to describe high-precision interferometric measurements on GRACE-FO and formulate the key LRI observables, namely the phase and phase rate of a coherent laser link between the two spacecraft. We develop a relativistic model for the LRI-enabled range between the two GRACE-FO spacecraft, accurate to less than 1 nm, and a high-precision model for the corresponding range rate, accurate to better than 0.1 nm/s. We also formulate high-precision relativistic models for the double one-way range (DOWR) and DOWR-enabled range rate observables originally used on GRACE and now studied for interferometric measurements on GRACE-FO. Our formulation justifies the basic assumptions behind the design of the GRACE-FO mission and highlights the importance of achieving nearly circular and nearly identical orbits for the GRACE-FO spacecraft.	63000	0.5882353186607361	3	3.5880000591278076
129	1112.0740	1282874	Thermal pure quantum states at finite temperature.	An equilibrium state can be represented by a pure quantum state, which we call a thermal pure quantum (TPQ) state. We propose a new TPQ state and a simple method of obtaining it. A single realization of the TPQ state suffices for calculating all statistical-mechanical properties, including correlation functions and genuine thermodynamic variables, of a quantum system at finite temperature.	16857	3.529411792755127	1	4.5289998054504395
130	1901.00069	57373758	Recurrent Neural Networks for Time Series Forecasting	Time series forecasting is difficult. It is difficult even for recurrent neural networks with their inherent ability to learn sequentiality. This article presents a recurrent neural network based time series forecasting framework covering feature engineering, feature importances, point and interval predictions, and forecast evaluation. The description of the method is followed by an empirical study using both LSTM and GRU networks.	106326	1.1764706373214722	2	3.1760001182556152
131	1908.11102	201670135	Field-effect transistor based on surface negative refraction in Weyl nanowire	Weyl semimetals are characterized by their bulk Weyl points -- conical band touching points that carry a topological monopole charge -- and Fermi arc states that span between the Weyl points on the surface of the material. Recently, significant progress has been made towards understanding and measuring the physical properties of Weyl semimetals. Yet, potential applications remain relatively sparse. Here, we propose Weyl semimetal nanowires as field-effect transistors, dubbed WEYLFETs. Specifically, applying gradient gate voltage along the nanowire, an electrical field is generated that effectively tilts the open surfaces, thus, varying the relative orientation between Fermi arcs on different surfaces. As a result, perfect negative refraction between adjacent surfaces can occur and longitudinal conductance along the wire is suppressed. The WEYLFET offers a high on/off ratio with low power consumption. Adverse effects due to dispersive Fermi arcs and surface disorder are studied.	133919	4.117647171020508	1	5.118000030517578
132	1901.00723	57375763	Efficient Evolutionary Methods for Game Agent Optimisation: Model-Based is Best	This paper introduces a simple and fast variant of Planet Wars as a test-bed for statistical planning based Game AI agents, and for noisy hyper-parameter optimisation. Planet Wars is a real-time strategy game with simple rules but complex game-play. The variant introduced in this paper is designed for speed to enable efficient experimentation, and also for a fixed action space to enable practical inter-operability with General Video Game AI agents. If we treat the game as a win-loss game (which is standard), then this leads to challenging noisy optimisation problems both in tuning agents to play the game, and in tuning game parameters. Here we focus on the problem of tuning an agent, and report results using the recently developed N-Tuple Bandit Evolutionary Algorithm and a number of other optimisers, including Sequential Model-based Algorithm Configuration (SMAC). Results indicate that the N-Tuple Bandit Evolutionary offers competitive performance as well as insight into the effects of combinations of parameter choices.	106767	0.0	3	3.0
133	1909.09142	202712494	Using Quantifier Elimination to Enhance the Safety Assurance of Deep Neural Networks	Advances in the field of Machine Learning and Deep Neural Networks (DNNs) has enabled rapid development of sophisticated and autonomous systems. However, the inherent complexity to rigorously assure the safe operation of such systems hinders their real-world adoption in safety-critical domains such as aerospace and medical devices. Hence, there is a surge in interest to explore the use of advanced mathematical techniques such as formal methods to address this challenge. In fact, the initial results of such efforts are promising. Along these lines, we propose the use of quantifier elimination (QE) - a formal method technique, as a complimentary technique to the state-of-the-art static analysis and verification procedures. Using an airborne collision avoidance DNN as a case example, we illustrate the use of QE to formulate the precise range forward propagation through a network as well as analyze its robustness. We discuss the initial results of this ongoing work and explore the future possibilities of extending this approach and/or integrating it with other approaches to perform advanced safety assurance of DNNs.	138389	0.0	7	7.0
134	1408.5148	118389378	Quantum many-body systems out of equilibrium		77166	0.5882353186607361	3	3.5880000591278076
135	1909.11912	202889130	Improving the Intelligibility of Electric and Acoustic Stimulation Speech Using Fully Convolutional Networks Based Speech Enhancement	The combined electric and acoustic stimulation (EAS) has demonstrated better speech recognition than conventional cochlear implant (CI) and yielded satisfactory performance under quiet conditions. However, when noise signals are involved, both the electric signal and the acoustic signal may be distorted, thereby resulting in poor recognition performance. To suppress noise effects, speech enhancement (SE) is a necessary unit in EAS devices. Recently, a time-domain speech enhancement algorithm based on the fully convolutional neural networks (FCN) with a short-time objective intelligibility (STOI)-based objective function (termed FCN(S) in short) has received increasing attention due to its simple structure and effectiveness of restoring clean speech signals from noisy counterparts. With evidence showing the benefits of FCN(S) for normal speech, this study sets out to assess its ability to improve the intelligibility of EAS simulated speech. Objective evaluations and listening tests were conducted to examine the performance of FCN(S) in improving the speech intelligibility of normal and vocoded speech in noisy environments. The experimental results show that, compared with the traditional minimum-mean square-error SE method and the deep denoising autoencoder SE method, FCN(S) can obtain better gain in the speech intelligibility for normal as well as vocoded speech. This study, being the first to evaluate deep learning SE approaches for EAS, confirms that FCN(S) is an effective SE approach that may potentially be integrated into an EAS processor to benefit users in noisy environments.	139560	0.0	3	3.0
136	1909.05503	202565892	The Randomized Midpoint Method for Log-Concave Sampling	Sampling from log-concave distributions is a well researched problem that has many applications in statistics and machine learning. We study the distributions of the form $p^{*}\propto\exp(-f(x))$, where $f:\mathbb{R}^{d}\rightarrow\mathbb{R}$ has an $L$-Lipschitz gradient and is $m$-strongly convex. In our paper, we propose a Markov chain Monte Carlo (MCMC) algorithm based on the underdamped Langevin diffusion (ULD). It can achieve $\epsilon\cdot D$ error (in 2-Wasserstein distance) in $\tilde{O}\left(\kappa^{7/6}/\epsilon^{1/3}+\kappa/\epsilon^{2/3}\right)$ steps, where $D\overset{\mathrm{def}}{=}\sqrt{\frac{d}{m}}$ is the effective diameter of the problem and $\kappa\overset{\mathrm{def}}{=}\frac{L}{m}$ is the condition number. Our algorithm performs significantly faster than the previously best known algorithm for solving this problem, which requires $\tilde{O}\left(\kappa^{1.5}/\epsilon\right)$ steps \cite{chen2019optimal,dalalyan2018sampling}. Moreover, our algorithm can be easily parallelized to require only $O(\kappa\log\frac{1}{\epsilon})$ parallel steps. To solve the sampling problem, we propose a new framework to discretize stochastic differential equations. We apply this framework to discretize and simulate ULD, which converges to the target distribution $p^{*}$. The framework can be used to solve not only the log-concave sampling problem, but any problem that involves simulating (stochastic) differential equations.	136752	0.0	3	3.0
137	1403.3119	16438286	Readout Optical System of Sapphire Disks intended for Long-Term Data Storage	The development of long-term data storage technology is one of the urging problems of our time. This paper presents the results of implementation of technical solution for long-term data storage technology proposed a few years ago on the basis of single crystal sapphire. It is shown that the problem of reading data through a substrate of negative single crystal sapphire can be solved by using for reading a special optical system with a plate of positive single crystal quartz. The experimental results confirm the efficiency of the proposed method of compensation.	64064	0.0	5	5.0
138	1204.2128	118577692	Can Free Will Emerge from Determinism in Quantum Theory		21418	0.5882353186607361	2	2.5880000591278076
139	1206.1106	5886221	No more pesky learning rates	The performance of stochastic gradient descent (SGD) depends critically on how learning rates are tuned and decreased over time. We propose a method to automatically adjust multiple learning rates so as to minimize the expected error at any one time. The method relies on local gradient variations across samples. In our approach, learning rates can increase as well as decrease, making it suitable for non-stationary problems. Using a number of convex and non-convex learning tasks, we show that the resulting algorithm matches the performance of SGD or other adaptive approaches with their best settings obtained through systematic search, and effectively removes the need for learning rate tuning.	24192	0.0	3	3.0
140	1907.07679	197544816	User-Interactive Machine Learning Model for Identifying Structural Relationships of Code Features	Traditional machine learning based intelligent systems assist users by learning patterns in data and making recommendations. However, these systems are limited in that the user has little means of understanding the rationale behind the systems suggestions, communicating their own understanding of patterns, or correcting system behavior. In this project, we outline a model for intelligent software based on a human computer feedback loop. The Machine Learning (ML) systems recommendations are reviewed by the user, and in turn, this information shapes the systems decision making. Our model was applied to developing an HTML editor that integrates ML with user interaction to ascertain structural relationships between HTML document features and apply them for code completion. The editor utilizes the ID3 algorithm to build decision trees, sequences of rules for predicting code the user will type. The editor displays the decision trees rules in the Interactive Rules Interface System (IRIS), which allows developers to prioritize, modify, or delete them. These interactions alter the data processed by ID3, providing the developer some control over the autocomplete system. Validation indicates that, absent user interaction, the ML model is able to predict tags with 78.4 percent accuracy, attributes with 62.9 percent accuracy, and values with 12.8 percent accuracy. Based off of the results of the user study, user interaction with the rules interface corrects feature relationships missed or mistaken by the automated process, enhancing autocomplete accuracy and developer productivity. Additionally, interaction is proven to help developers work with greater awareness of code patterns. Our research demonstrates the viability of a software integration of machine intelligence with human feedback.	126227	1.7647058963775635	5	6.764999866485596
141	1906.01620	174798361	Evaluating Scalable Bayesian Deep Learning Methods for Robust Computer Vision	While deep neural networks have become the go-to approach in computer vision, the vast majority of these models fail to properly capture the uncertainty inherent in their predictions. Estimating this predictive uncertainty can be crucial, for example in automotive applications. In Bayesian deep learning, predictive uncertainty is commonly decomposed into the distinct types of aleatoric and epistemic uncertainty. The former can be estimated by letting a neural network output the parameters of a certain probability distribution. Epistemic uncertainty estimation is a more challenging problem, and while different scalable methods recently have emerged, no extensive comparison has been performed in a real-world setting. We therefore accept this task and propose a comprehensive evaluation framework for scalable epistemic uncertainty estimation methods in deep learning. Our proposed framework is specifically designed to test the robustness required in real-world computer vision applications. We also apply this framework to provide the first properly extensive and conclusive comparison of the two current state-of-the- art scalable methods: ensembling and MC-dropout. Our comparison demonstrates that ensembling consistently provides more reliable and practically useful uncertainty estimates. Code is available at https://github.com/fregu856/evaluating_bdl.	118066	0.0	3	3.0
142	1910.01201	202889071	Gluon polarization measurements from longitudinally polarized proton-proton collisions at STAR	The gluon polarization contribution to the proton spin is an integral part to solve the longstanding proton spin puzzle. At the Relativistic Heavy Ion Collider (RHIC), the STAR experiment has measured jets produced in mid-pseudo-rapidity, |η| < 1.0, and full azimuth, ϕ, from longitudinally polarized pp collisions to study the gluon polarization in the proton. At center of mass energies s = 200 and 510 GeV, jet production is dominated by hard QCD scattering processes such as gluon-gluon (gg) and quark-gluon (qg), thus making the longitudinal double-spin asymmetry (ALL ) sensitive to the gluon polarization. Early STAR inclusive jet ALL results at s = 200 GeV provided the first evidence of the non-zero gluon polarization at momentum fraction x > 0.05. The higher center of mass energy s = 510 GeV allows to explore the gluon polarization as low as x ∼ 0.015. In this talk we will present the recent STAR inclusive jet and dijet ALL results at s = 510 GeV, and discuss the relevant new analysis techniques for the estimation of trigger bias and reconstruction uncertainty, the underlying event correction on the jet energy and its effect on jet ALL . Dijet results are shown for different topologies in regions of pseudo-rapidity, effectively scanning the x-dependence of the gluon polarization.	140867	0.5882353186607361	2	2.5880000591278076
143	1402.4914	11219121	Building fast Bayesian computing machines out of intentionally stochastic, digital parts	The brain interprets ambiguous sensory information faster and more reliably than modern computers, using neurons that are slower and less reliable than logic gates. But Bayesian inference, which underpins many computational models of perception and cognition, appears computationally challenging even given modern transistor speeds and energy budgets. The computational principles and structures needed to narrow this gap are unknown. Here we show how to build fast Bayesian computing machines using intentionally stochastic, digital parts, narrowing this efficiency gap by multiple orders of magnitude. We find that by connecting stochastic digital components according to simple mathematical rules, one can build massively parallel, low precision circuits that solve Bayesian inference problems and are compatible with the Poisson firing statistics of cortical neurons. We evaluate circuits for depth and motion perception, perceptual learning and causal reasoning, each performing inference over 10,000+ latent variables in real time - a 1,000x speed advantage over commodity microprocessors. These results suggest a new role for randomness in the engineering and reverse-engineering of intelligent computation.	62318	0.0	3	3.0
144	1908.01079	199441951	Arithmetic and geometry of a K3 surface emerging from virtual corrections to Drell–Yan scattering	We study a K3 surface, which appears in the two-loop mixed electroweak-quantum chromodynamic virtual corrections to Drell--Yan scattering. A detailed analysis of the geometric Picard lattice is presented, computing its rank and discriminant in two independent ways: first using explicit divisors on the surface and then using an explicit elliptic fibration. We also study in detail the elliptic fibrations of the surface and use them to provide an explicit Shioda--Inose structure. Moreover, we point out the physical relevance of our results.	129363	1.1764706373214722	1	2.1760001182556152
145	1310.1392	118637272	Chandra Imaging of Gamma-Ray Binaries	We review the multiwavelength properties of the few known gamma-ray binaries, focusing on extended emission recently resolved with Chandra. We discuss the implications of these findings for the nature of compact objects and for physical processes operating in these systems.	52104	0.0	3	3.0
146	1011.3912	6960223	Artificial Hormone Reaction Networks - Towards Higher Evolvability in Evolutionary Multi-Modular Robotics	The semi-automatic or automatic synthesis of robot controller software is both desirable and challenging. Synthesis of rather simple behaviors such as collision avoidance by applying artificial evolution has been shown multiple times. However, the difficulty of this synthesis increases heavily with increasing complexity of the task that should be performed by the robot. We try to tackle this problem of complexity with Artificial Homeostatic Hormone Systems (AHHS), which provide both intrinsic, homeostatic processes and (transient) intrinsic, variant behavior. By using AHHS the need for pre-defined controller topologies or information about the field of application is minimized. We investigate how the principle design of the controller and the hormone network size affects the overall performance of the artificial evolution (i.e., evolvability). This is done by comparing two variants of AHHS that show different effects when mutated. We evolve a controller for a robot built from five autonomous, cooperating modules. The desired behavior is a form of gait resulting in fast locomotion by using the modules' main hinges.	6121	1.1764706373214722	1	2.1760001182556152
147	1306.4411	16934701	Event-Object Reasoning with Curated Knowledge Bases: Deriving Missing Information		44730	0.0	3	3.0
148	1902.00751	59599816	Parameter-Efficient Transfer Learning for NLP	Fine-tuning large pre-trained models is an effective transfer mechanism in NLP. However, in the presence of many downstream tasks, fine-tuning is parameter inefficient: an entire new model is required for every task. As an alternative, we propose transfer with adapter modules. Adapter modules yield a compact and extensible model; they add only a few trainable parameters per task, and new tasks can be added without revisiting previous ones. The parameters of the original network remain fixed, yielding a high degree of parameter sharing. To demonstrate adapter's effectiveness, we transfer the recently proposed BERT Transformer model to 26 diverse text classification tasks, including the GLUE benchmark. Adapters attain near state-of-the-art performance, whilst adding only a few parameters per task. On GLUE, we attain within 0.4% of the performance of full fine-tuning, adding only 3.6% parameters per task. By contrast, fine-tuning trains 100% of the parameters per task.	111804	28.823530197143555	11	39.82400131225586
149	1409.8185	8352617	Adaptive Low-Complexity Sequential Inference for Dirichlet Process Mixture Models	We develop a sequential low-complexity inference procedure for Dirichlet process mixtures of Gaussians for online clustering and parameter estimation when the number of clusters are unknown a-priori. We present an easily computable, closed form parametric expression for the conditional likelihood, in which hyperparameters are recursively updated as a function of the streaming data assuming conjugate priors. Motivated by large-sample asymptotics, we propose a novel adaptive low-complexity design for the Dirichlet process concentration parameter and show that the number of classes grow at most at a logarithmic rate. We further prove that in the large-sample limit, the conditional likelihood and data predictive distribution become asymptotically Gaussian. We demonstrate through experiments on synthetic and real data sets that our approach is superior to other online state-of-the-art methods.	80159	0.0	3	3.0
150	1909.00114	202541543	Towards Learning Affine-Invariant Representations via Data-Efficient CNNs	In this paper we propose integrating a priori knowledge into both design and training of convolutional neural networks (CNNs) to learn object representations that are invariant to affine transformations (i.e. translation, scale, rotation). Accordingly we propose a novel multi-scale maxout CNN and train it end-to-end with a novel rotation-invariant regularizer. This regularizer aims to enforce the weights in each 2D spatial filter to approximate circular patterns. In this way, we manage to handle affine transformations in training using convolution, multi-scale maxout, and circular filters. Empirically we demonstrate that such knowledge can significantly improve the data-efficiency as well as generalization and robustness of learned models. For instance, on the Traffic Sign data set and trained with only 10 images per class, our method can achieve 84.15% that outperforms the state-of-the-art by 29.80% in terms of test accuracy.	134375	10.0	4	14.0
151	1909.04853	202558460	Bayesian inference on volatility in the presence of infinite jump activity and microstructure noise	"Volatility estimation based on high-frequency data is key to accurately measure and control the risk of financial assets. A L\'{e}vy process with infinite jump activity and microstructure noise is considered one of the simplest, yet accurate enough, models for financial data at high-frequency. Utilizing this model, we propose a ""purposely misspecified"" posterior of the volatility obtained by ignoring the jump-component of the process. The misspecified posterior is further corrected by a simple estimate of the location shift and re-scaling of the log likelihood. Our main result establishes a Bernstein-von Mises (BvM) theorem, which states that the proposed adjusted posterior is asymptotically Gaussian, centered at a consistent estimator, and with variance equal to the inverse of the Fisher information. In the absence of microstructure noise, our approach can be extended to inferences of the integrated variance of a general It\^o semimartingale. Simulations are provided to demonstrate the accuracy of the resulting credible intervals, and the frequentist properties of the approximate Bayesian inference based on the adjusted posterior."	136440	0.0	3	3.0
152	1909.01133	202539314	Oxygen yields as a constraint on feedback processes in galaxies	"
 We study the interplay between several properties determined from optical and a combination of optical/radio measurements, such as the effective oxygen yield (yeff), the star formation efficiency, gas metallicity, depletion time, gas fraction, and baryonic mass (Mbar), among others. We use spectroscopic data from the SDSS survey, and H i information from the ALFALFA survey to build a statistically significant sample of more than 5000 galaxies. Furthermore, we complement our analysis with data from the GASS and COLD GASS surveys, and with a sample of star-forming galaxies from the Virgo cluster. Additionally, we have compared our results with predictions from the EAGLE simulations, finding a very good agreement when using the high-resolution run. We explore in detail the Mbar–yeff relation, finding a bimodal trend that can be separated when the stellar age of galaxies is considered. On one hand, yeff increases with Mbar for young galaxies [log(tr) < 9.2 yr], while yeff shows an anticorrelation with Mbar for older galaxies [log(tr) > 9.4 yr]. While a correlation between Mbar and yeff has been observed and studied before, mainly for samples of dwarfs and irregular galaxies, their anticorrelated counterpart for massive galaxies has not been previously reported. The EAGLE simulations indicate that AGN feedback must have played an important role in their history by quenching their star formation rate, whereas low-mass galaxies would have been affected by a combination of outflows and infall of gas."	134766	0.5882353186607361	2	2.5880000591278076
153	1205.2695	118363519	WIMP-nucleus scattering in chiral effective theory		23026	0.5882353186607361	2	2.5880000591278076
154	1909.08226	202660937	Eigenvalues of Two-State Quantum Walks Induced by the Hadamard Walk	Existence of the eigenvalues of the discrete-time quantum walks is deeply related to localization of the walks. We revealed, for the first time, the distributions of the eigenvalues given by the splitted generating function method (the SGF method) of the space-inhomogeneous quantum walks in one dimension we had treated in our previous studies. Especially, we clarified the characteristic parameter dependence for the distributions of the eigenvalues with the aid of numerical simulation.	137944	1.7647058963775635	1	2.765000104904175
155	1907.09578	198179572	Information-Bottleneck Approach to Salient Region Discovery		127068	0.5882353186607361	3	3.5880000591278076
156	1412.6388	15426540	Distributed Decision Trees	Recently proposed budding tree is a decision tree algorithm in which every node is part internal node and part leaf. This allows representing every decision tree in a continuous parameter space, and therefore a budding tree can be jointly trained with backpropagation, like a neural network. Even though this continuity allows it to be used in hierarchical representation learning, the learned representations are local: Activation makes a soft selection among all root-to-leaf paths in a tree. In this work we extend the budding tree and propose the distributed tree where the children use different and independent splits and hence multiple paths in a tree can be traversed at the same time. This ability to combine multiple paths gives the power of a distributed representation, as in a traditional perceptron layer. We show that distributed trees perform comparably or better than budding and traditional hard trees on classification and regression tasks.	86502	0.0	3	3.0
157	1909.05829	202565422	Hierarchical Foresight: Self-Supervised Learning of Long-Horizon Tasks via Visual Subgoal Generation	Video prediction models combined with planning algorithms have shown promise in enabling robots to learn to perform many vision-based tasks through only self-supervision, reaching novel goals in cluttered scenes with unseen objects. However, due to the compounding uncertainty in long horizon video prediction and poor scalability of sampling-based planning optimizers, one significant limitation of these approaches is the ability to plan over long horizons to reach distant goals. To that end, we propose a framework for subgoal generation and planning, hierarchical visual foresight (HVF), which generates subgoal images conditioned on a goal image, and uses them for planning. The subgoal images are directly optimized to decompose the task into easy to plan segments, and as a result, we observe that the method naturally identifies semantically meaningful states as subgoals. Across three out of four simulated vision-based manipulation tasks, we find that our method achieves nearly a 200% performance improvement over planning without subgoals and model-free RL approaches. Further, our experiments illustrate that our approach extends to real, cluttered visual scenes. Project page: this https URL	136905	0.0	5	5.0
158	1403.3110	119278344	Exclusive Processes: Theory Introduction	We review the recent developments on the theoretical description of exclusive processes at medium and asymptotical energies. These are illustrated based on a few examples.	64057	0.5882353186607361	2	2.5880000591278076
159	1303.7274	14310935	Reputation and impact in academic careers	Significance Over a scientist’s career, a reputation is developed, a standing within a research community, based largely upon the quantity and quality of his/her publications. Here, we develop a framework for quantifying the influence author reputation has on a publication’s future impact. We find author reputation plays a key role in driving a paper’s citation count early in its citation life cycle, before a tipping point, after which reputation has much less influence relative to the paper’s citation count. In science, perceived quality, and decisions made based on those perceptions, is increasingly linked to citation counts. Shedding light on the complex mechanisms driving these quantitative measures facilitates not only better evaluation of scientific outputs but also a more transparent evaluation of the scientists producing them. Reputation is an important social construct in science, which enables informed quality assessments of both publications and careers of scientists in the absence of complete systemic information. However, the relation between reputation and career growth of an individual remains poorly understood, despite recent proliferation of quantitative research evaluation methods. Here, we develop an original framework for measuring how a publication’s citation rate Δc depends on the reputation of its central author i, in addition to its net citation count c. To estimate the strength of the reputation effect, we perform a longitudinal analysis on the careers of 450 highly cited scientists, using the total citations Ci of each scientist as his/her reputation measure. We find a citation crossover c×, which distinguishes the strength of the reputation effect. For publications with c < c×, the author’s reputation is found to dominate the annual citation rate. Hence, a new publication may gain a significant early advantage corresponding to roughly a 66% increase in the citation rate for each tenfold increase in Ci. However, the reputation effect becomes negligible for highly cited publications meaning that, for c ≥ c×, the citation rate measures scientific impact more transparently. In addition, we have developed a stochastic reputation model, which is found to reproduce numerous statistical observations for real careers, thus providing insight into the microscopic mechanisms underlying cumulative advantage in science.	39446	1.1764706373214722	5	6.176000118255615
160	1112.0222	111383879	Philosophy and problems of the definition of Extraterrestrial Life	"When we try to search for extraterrestrial life and intelligence, we have to follow some guidelines. The first step is to clarify what is to be meant by ""Life"" and ""intelligence"", i.e. an attempt to define these words. The word ""definition"" refers to two different situations. First, it means an arbitrary convention. On the other hand it also often designates an attempt to clarify the content of a pre-existing word for which we have some spontaneous preconceptions, whatever their grounds, and to catch an (illusory) ""essence"" of what is defined. It is then made use of pre-existing plain language words which carry an a priori pre-scientific content likely to introduce some confusion in the reader's mind. The complexity of the problem will be analyzed and we will show that some philosophical prejudice is unavoidable. There are two types of philosophy: ""Natural Philosophy"", seeking for some essence of things, and ""Critical (or analytical) Philosophy"", devoted to the analysis of the procedures by which we claim to construct a reality. An extension of Critical Philosophy, Epistemo-Analysis (i.e. the Psycho-Analysis of concepts) is presented and applied to the defintion of Life and to Astrobiology."	16787	0.0	3	3.0
161	1310.4924	18438352	Dark matter candidates, helicity effects and new affine gravity with torsion		53049	0.5882353186607361	2	2.5880000591278076
162	1811.04743	119194977	SAGE: finding IMBH in the black hole desert	SAGE (SagnAc interferometer for Gravitational wavE) is a project for a space observatory based on multiple 12-U CubeSats in geosynchronous equatorial orbit. The objective is a fast track mission which would fill the observational gap between LISA and ground based observatories. With albeit a lower sensitivity, it would allow early investigation of the nature and event rate of intermediate-mass black hole (IMBH) mergers, constraining our understanding of the universe formation by probing the building up of IMBH up to supermassive black holes (SMBH). Technically, the CubeSats would create a triangular Sagnac interferometer with 140.000 km roundtrip arm length, optimised to be sensitive to gravitational waves at frequencies between 10 mHz and 2 Hz. The nature of the Sagnac measurement makes it almost insensitive to position error, a feature enabling the use of spacecrafts in ballistic trajectories instead of perfect free fall. The light source and recombination units of the interferometer are based on compact fibered technologies without bulk optics. A peak sensitivity of 23 pm ()−1 is expected at 1 Hz assuming a 200 mW internal laser source and 10-centimeter diameter apertures. Because of the absence of a test mass, the main limitation would come from the non-gravitational forces applied on the spacecrafts. However, conditionally upon our ability to partially post-process the effect of solar wind and solar pressure, SAGE would allow detection of gravitational waves with strains as low as a few 10−19 within the 0.1 to 1 Hz range. Averaged over the entire sky, and including the antenna gain of the Sagnac interferometer, the SAGE observatory would sense equal mass black hole mergers in the 104 to 106 solar masses range up to a luminosity distance of 800 Mpc. Additionally, coalescence of stellar black holes (10 M) around SMBH (IMBH) forming extreme (intermediate) mass ratio inspirals could be detected within a sphere of radius 200 Mpc.	101674	1.1764706373214722	1	2.1760001182556152
163	1908.00692	199405289	Real Time Visual Tracking using Spatial-Aware Temporal Aggregation Network	More powerful feature representations derived from deep neural networks benefit visual tracking algorithms widely. However, the lack of exploitation on temporal information prevents tracking algorithms from adapting to appearances changing or resisting to drift. This paper proposes a correlation filter based tracking method which aggregates historical features in a spatial-aligned and scale-aware paradigm. The features of historical frames are sampled and aggregated to search frame according to a pixel-level alignment module based on deformable convolutions. In addition, we also use a feature pyramid structure to handle motion estimation at different scales, and address the different demands on feature granularity between tracking losses and deformation offset learning. By this design, the tracker, named as Spatial-Aware Temporal Aggregation network (SATA), is able to assemble appearances and motion contexts of various scales in a time period, resulting in better performance compared to a single static image. Our tracker achieves leading performance in OTB2013, OTB2015, VOT2015, VOT2016 and LaSOT, and operates at a real-time speed of 26 FPS, which indicates our method is effective and practical. Our code will be made publicly available at \href{this https URL}{this https URL}.	129188	0.0	3	3.0
164	1803.01690	822036	New Ideas for Brain Modelling 4		97392	0.0	4	4.0
165	1907.07198	197430837	RayTracer.jl: A Differentiable Renderer that supports Parameter Optimization for Scene Reconstruction	In this paper, we present RayTracer.jl, a renderer in Julia that is fully differentiable using source-to-source Automatic Differentiation (AD). This means that RayTracer not only renders 2D images from 3D scene parameters, but it can be used to optimize for model parameters that generate a target image in a Differentiable Programming (DP) pipeline. We interface our renderer with the deep learning library Flux for use in combination with neural networks. We demonstrate the use of this differentiable renderer in rendering tasks and in solving inverse graphics problems.	126026	8.235294342041016	5	13.234999656677246
166	1411.1076	15027589	A statistical model for tensor PCA	"We consider the Principal Component Analysis problem for large tensors of arbitrary order $k$ under a single-spike (or rank-one plus noise) model. On the one hand, we use information theory, and recent results in probability theory, to establish necessary and sufficient conditions under which the principal component can be estimated using unbounded computational resources. It turns out that this is possible as soon as the signal-to-noise ratio $\beta$ becomes larger than $C\sqrt{k\log k}$ (and in particular $\beta$ can remain bounded as the problem dimensions increase). 
On the other hand, we analyze several polynomial-time estimation algorithms, based on tensor unfolding, power iteration and message passing ideas from graphical models. We show that, unless the signal-to-noise ratio diverges in the system dimensions, none of these approaches succeeds. This is possibly related to a fundamental limitation of computationally tractable estimators for this problem. 
We discuss various initializations for tensor power iteration, and show that a tractable initialization based on the spectrum of the matricized tensor outperforms significantly baseline methods, statistically and computationally. Finally, we consider the case in which additional side information is available about the unknown signal. We characterize the amount of side information that allows the iterative algorithms to converge to a good estimate."	82661	0.0	3	3.0
167	1901.02001	57721216	Analogy-Based Preference Learning with Kernels		107320	0.0	5	5.0
168	1908.09063	202079740	How to Improve Functionals in Density Functional Theory? —Formalism and Benchmark Calculation—	We proposed in Ref. [J. Phys. B 39, 13120 (2019)] a way to improve energy density functionals in the density functional theory based on the combination of the inverse Kohn-Sham method and the density functional perturbation theory. In this proceeding, we mainly focus on the results for the Ar and Kr atoms.	133038	0.5882353186607361	2	2.5880000591278076
169	1308.4300	119281565	Chirally motivated separable potential model for ηN amplitudes		48930	1.1764706373214722	1	2.1760001182556152
170	1907.02092	195798720	Sampling the $$\mu \nu $$SSM for displaced decays of the tau left sneutrino LSP at the LHC		123771	1.7647058963775635	3	4.764999866485596
171	1910.01441	203641988	Can Sentiment Analysis Reveal Structure in a Plotless Novel?	Modernist novels are thought to break with traditional plot structure. In this paper, we test this theory by applying Sentiment Analysis to one of the most famous modernist novels, To the Lighthouse by Virginia Woolf. We first assess Sentiment Analysis in light of the critique that it cannot adequately account for literary language: we use a unique statistical comparison to demonstrate that even simple lexical approaches to Sentiment Analysis are surprisingly effective. We then use the Syuzhet.R package to explore similarities and differences across modeling methods. This comparative approach, when paired with literary close reading, can offer interpretive clues. To our knowledge, we are the first to undertake a hybrid model that fully leverages the strengths of both computational analysis and close reading. This hybrid model raises new questions for the literary critic, such as how to interpret relative versus absolute emotional valence and how to take into account subjective identification. Our finding is that while To the Lighthouse does not replicate a plot centered around a traditional hero, it does reveal an underlying emotional structure distributed between characters - what we term a distributed heroine model. This finding is innovative in the field of modernist and narrative studies and demonstrates that a hybrid method can yield significant discoveries.	141000	0.5882353186607361	3	3.5880000591278076
172	1901.02433	57721245	Learning with Collaborative Neural Network Group by Reflection	For the present engineering of neural systems, the preparing of extensive scale learning undertakings generally not just requires a huge neural system with a mind boggling preparing process yet additionally troublesome discover a clarification for genuine applications. In this paper, we might want to present the Collaborative Neural Network Group (CNNG). CNNG is a progression of neural systems that work cooperatively to deal with various errands independently in a similar learning framework. It is advanced from a solitary neural system by reflection. Along these lines, in light of various circumstances removed by the calculation, the CNNG can perform diverse techniques when handling the information. The examples of chose methodology can be seen by human to make profound adapting more reasonable. In our execution, the CNNG is joined by a few moderately little neural systems. We give a progression of examinations to assess the execution of CNNG contrasted with other learning strategies. The CNNG is able to get a higher accuracy with a much lower training cost. We can reduce the error rate by 74.5% and reached the accuracy of 99.45% in MNIST with three feedforward networks (4 layers) in one training epoch.	107513	0.5882353186607361	3	3.5880000591278076
173	1310.3725	116444847	Transforming High School Physics With Modeling And Computation	The Engage to Excel (PCAST) report, the National Research Council's Framework for K-12 Science Education, and the Next Generation Science Standards all call for transforming the physics classroom into an environment that teaches students real scientific practices. This work describes the early stages of one such attempt to transform a high school physics classroom. Specifically, a series of model-building and computational modeling exercises were piloted in a ninth grade Physics First classroom. Student use of computation was assessed using a proctored programming assignment, where the students produced and discussed a computational model of a baseball in motion via a high-level programming environment (VPython). Student views on computation and its link to mechanics was assessed with a written essay and a series of think-aloud interviews. This pilot study shows computation's ability for connecting scientific practice to the high school science classroom.	52654	2.3529412746429443	3	5.353000164031982
174	1909.06563	202577797	Multi-view and Multi-source Transfers in Neural Topic Modeling with Pretrained Topic and Word Embeddings	Though word embeddings and topics are complementary representations, several past works have only used pre-trained word embeddings in (neural) topic modeling to address data sparsity problem in short text or small collection of documents. However, no prior work has employed (pre-trained latent) topics in transfer learning paradigm. In this paper, we propose an approach to (1) perform knowledge transfer using latent topics obtained from a large source corpus, and (2) jointly transfer knowledge via the two representations (or views) in neural topic modeling to improve topic quality, better deal with polysemy and data sparsity issues in a target corpus. In doing so, we first accumulate topics and word representations from one or many source corpora to build a pool of topics and word vectors. Then, we identify one or multiple relevant source domain(s) and take advantage of corresponding topics and word features via the respective pools to guide meaningful learning in the sparse target domain. We quantify the quality of topic and document representations via generalization (perplexity), interpretability (topic coherence) and information retrieval (IR) using short-text, long-text, small and large document collections from news and medical domains. We have demonstrated the state-of-the-art results on topic modeling with the proposed framework.	137242	0.0	3	3.0
175	1402.5991	7602060	A predictive analytics approach to reducing avoidable hospital readmission	Hospital readmission has become a critical metric of quality and cost of healthcare. Medicare anticipates that nearly $17 billion is paid out on the 20% of patients who are readmitted within 30 days of discharge. Although several interventions such as transition care management and discharge reengineering have been practiced in recent years, the effectiveness and sustainability depends on how well they can identify and target patients at high risk of rehospitalization. Based on the literature, most current risk prediction models fail to reach an acceptable accuracy level; none of them considers patient's history of readmission and impacts of patient attribute changes over time; and they often do not discriminate between planned and unnecessary readmissions. Tackling such drawbacks, we develop a new readmission metric based on administrative data that can identify potentially avoidable readmissions from all other types of readmission. We further propose a tree based classification method to estimate the predicted probability of readmission that can directly incorporate patient's history of readmission and risk factors changes over time. The proposed methods are validated with 2011-12 Veterans Health Administration data from inpatients hospitalized for heart failure, acute myocardial infarction, pneumonia, or chronic obstructive pulmonary disease in the State of Michigan. Results shows improved discrimination power compared to the literature (c-statistics>80%) and good calibration.	62652	0.0	4	4.0
176	1909.11784	202888609	bamlss: A Lego Toolbox for Flexible Bayesian Regression (and Beyond)	"Over the last decades, the challenges in applied regression and in predictive modeling have been changing considerably: (1) More flexible model specifications are needed as big(ger) data become available, facilitated by more powerful computing infrastructure. (2) Full probabilistic modeling rather than predicting just means or expectations is crucial in many applications. (3) Interest in Bayesian inference has been increasing both as an appealing framework for regularizing or penalizing model estimation as well as a natural alternative to classical frequentist inference. However, while there has been a lot of research in all three areas, also leading to associated software packages, a modular software implementation that allows to easily combine all three aspects has not yet been available. For filling this gap, the R package bamlss is introduced for Bayesian additive models for location, scale, and shape (and beyond). At the core of the package are algorithms for highly-efficient Bayesian estimation and inference that can be applied to generalized additive models (GAMs) or generalized additive models for location, scale, and shape (GAMLSS), also known as distributional regression. However, its building blocks are designed as ""Lego bricks"" encompassing various distributions (exponential family, Cox, joint models, ...), regression terms (linear, splines, random effects, tensor products, spatial fields, ...), and estimators (MCMC, backfitting, gradient boosting, lasso, ...). It is demonstrated how these can be easily recombined to make classical models more flexible or create new custom models for specific modeling challenges."	139482	2.3529412746429443	2	4.353000164031982
177	1603.07593	53377931	Evaluating the Performance of Offensive Linemen in the NFL	How does one objectively measure the performance of an individual offensive lineman in the NFL? The existing literature proposes various measures that rely on subjective assessments of game film, but has yet to develop an objective methodology to evaluate performance. Using a variety of statistics related to an offensive lineman's performance, we develop a framework to objectively analyze the overall performance of an individual offensive lineman and determine specific linemen who are overvalued or undervalued relative to their salary. We identify eight players across the 2013-2014 and 2014-2015 NFL seasons that are considered to be overvalued or undervalued and corroborate the results with existing metrics that are based on subjective evaluation. To the best of our knowledge, the techniques set forth in this work have not been utilized in previous works to evaluate the performance of NFL players at any position, including offensive linemen.	91704	0.5882353186607361	3	3.5880000591278076
178	1901.03554	57825746	CSGAN: Cyclic-Synthesized Generative Adversarial Networks for Image-to-Image Transformation		108071	2.941176414489746	2	4.940999984741211
179	1906.00029	173990974	Human-Usable Password Schemas: Beyond Information-Theoretic Security	"Password users frequently employ passwords that are too simple, or they just reuse passwords for multiple websites. A common complaint is that utilizing secure passwords is too difficult. One possible solution to this problem is to use a password schema. Password schemas are deterministic functions which map challenges (typically the website name) to responses (passwords). Previous work has been done on developing and analyzing publishable schemas, but these analyses have been information-theoretic, not complexity-theoretic; they consider an adversary with infinite computing power. 
We perform an analysis with respect to adversaries having currently achievable computing capabilities, assessing the realistic practical security of such schemas. We prove for several specific schemas that a computer is no worse off than an infinite adversary and that it can successfully extract all information from leaked challenges and their respective responses, known as challenge-response pairs. We also show that any schema that hopes to be secure against adversaries with bounded computation should obscure information in a very specific way, by introducing many possible constraints with each challenge-response pair. These surprising results put the analyses of password schemas on a more solid and practical footing."	117393	1.1764706373214722	1	2.1760001182556152
180	1909.07727	202583904	An Image Based Visual Servo Approach with Deep Learning for Robotic Manipulation	Aiming at the difficulty of extracting image features and estimating the Jacobian matrix in image based visual servo, this paper proposes an image based visual servo approach with deep learning. With the powerful learning capabilities of convolutional neural networks(CNN), autonomous learning to extract features from images and fitting the nonlinear relationships from image space to task space is achieved, which can greatly facilitate the image based visual servo procedure. Based on the above ideas a two-stream network based on convolutional neural network is designed and the corresponding control scheme is proposed to realize the four degrees of freedom visual servo of the robot manipulator. Collecting images of observed target under different pose parameters of the manipulator as training samples for CNN, the trained network can be used to estimate the nonlinear relationship from 2D image space to 3D Cartesian space. The two-stream network takes the current image and the desirable image as inputs and makes them equal to guide the manipulator to the desirable pose. The effectiveness of the approach is verified with experimental results.	137712	1.7647058963775635	2	3.765000104904175
181	1906.11335	195699419	Enhancing Temporal Segmentation by Nonlocal Self-Similarity	Temporal segmentation of untrimmed videos and photo-streams is currently an active area of research in computer vision and image processing. This paper proposes a new approach to improve the temporal segmentation of photo-streams. The method consists in enhancing image representations by encoding long-range temporal dependencies. Our key contribution is to take advantage of the temporal station-arity assumption of photostreams for modeling each frame by its nonlocal self-similarity function. The proposed approach is put to test on the EDUB-Seg dataset, a standard benchmark for egocentric photostream temporal segmentation. Starting from seven different (CNN based) image features, the method yields consistent improvements in event segmentation quality, leading to an average increase of F-measure of 3.71% with respect to the state of the art.	122415	0.0	3	3.0
182	1909.03862	202537509	Out-of-Domain Detection for Natural Language Understanding in Dialog Systems	Natural Language Understanding (NLU) is a vital component of dialogue systems, and its ability to detect Out-of-Domain (OOD) inputs is critical in practical applications, since the acceptance of the OOD input that is unsupported by the current system may lead to catastrophic failure. However, most existing OOD detection methods rely heavily on manually labeled OOD samples and cannot take full advantage of unlabeled data. This limits the feasibility of these models in practical applications. In this paper, we propose a novel model to generate high-quality pseudo OOD samples that are akin to IN-Domain (IND) input utterances and thereby improves the performance of OOD detection. To this end, an autoencoder is trained to map an input utterance into a latent code. Moreover, the codes of IND and OOD samples are trained to be indistinguishable by utilizing a generative adversarial network. To provide more supervision signals, an auxiliary classifier is introduced to regularize the generated OOD samples to have indistinguishable intent labels. Experiments show that these pseudo OOD samples generated by our model can be used to effectively improve OOD detection in NLU. Besides, we also demonstrate that the effectiveness of these pseudo OOD data can be further improved by efficiently utilizing unlabeled data.	135988	0.5882353186607361	2	2.5880000591278076
183	1909.12939	203593763	A Weakly Supervised Adaptive Triplet Loss for Deep Metric Learning	We address the problem of distance metric learning in visual similarity search, defined as learning an image embedding model which projects images into Euclidean space where semantically and visually similar images are closer and dissimilar images are further from one another. We present a weakly supervised adaptive triplet loss (ATL) capable of capturing fine-grained semantic similarity that encourages the learned image embedding models to generalize well on cross-domain data. The method uses weakly labeled product description data to implicitly determine fine grained semantic classes, avoiding the need to annotate large amounts of training data. We evaluate on the Amazon fashion retrieval benchmark and DeepFashion in-shop retrieval data. The method boosts the performance of triplet loss baseline by 10.6% on cross-domain data and out-performs the state-of-art model on all evaluation metrics.	140035	0.5882353186607361	3	3.5880000591278076
184	1908.00160	199064657	Max–Min Fairness Design for MIMO Interference Channels: A Minorization–Maximization Approach	We address the problem of linear precoder (beamformer) design in a multiple-input multiple-output interference channel. The aim is to design the transmit covariance matrices in order to achieve max–min utility fairness for all users. The corresponding optimization problem is non-convex and NP-hard in general. We devise an efficient algorithm based on the minorization–maximization technique to obtain quality solutions to this problem. The proposed method solves a second-order cone convex program at each iteration. We prove that the devised method converges to stationary points of the problem. We also extend our algorithm to the case where there are uncertainties in the noise covariance matrices or channel state information. Simulation results show the effectiveness of the proposed method compared with its main competitor.	128926	1.1764706373214722	2	3.1760001182556152
185	1907.10925	198897896	On Uniform Equivalence of Epistemic Logic Programs	Abstract Epistemic Logic Programs (ELPs) extend Answer Set Programming (ASP) with epistemic negation and have received renewed interest in recent years. This led to the development of new research and efficient solving systems for ELPs. In practice, ELPs are often written in a modular way, where each module interacts with other modules by accepting sets of facts as input, and passing on sets of facts as output. An interesting question then presents itself: under which conditions can such a module be replaced by another one without changing the outcome, for any set of input facts? This problem is known as uniform equivalence, and has been studied extensively for ASP. For ELPs, however, such an investigation is, as of yet, missing. In this paper, we therefore propose a characterization of uniform equivalence that can be directly applied to the language of state-of-the-art ELP solvers. We also investigate the computational complexity of deciding uniform equivalence for two ELPs, and show that it is on the third level of the polynomial hierarchy.	127713	0.0	6	6.0
186	1510.06188	7470762	Learning-Based Compressive Subsampling	The problem of recovering a structured signal x ∈ C<sup>p</sup> from a set of dimensionality-reduced linear measurements b = Ax arises in a variety of applications, such as medical imaging, spectroscopy, Fourier optics, and computerized tomography. Due to computational and storage complexity or physical constraints imposed by the problem, the measurement matrix A ∈ C<sup>n×p</sup> is often of the form A = P<sub>Ω</sub>Ψ for some orthonormal basis matrix Ψ ∈ C<sup>P×P</sup> and subsampling operator P<sub>Ω</sub> : C<sup>p</sup> → C<sup>n</sup> that selects the rows indexed by Ω. This raises the fundamental question of how best to choose the index set Ω in order to optimize the recovery performance. Previous approaches to addressing this question rely on nonuniform random subsampling using application-specific knowledge of the structure of x. In this paper, we instead take a principled learning-based approach in which affixed index set is chosen based on a set of training signals x<sub>1</sub>, . . . , x<sub>m</sub>. We formulate combinatorial optimization problems seeking to maximize the energy captured in these signals in an average-case or worst-case sense, and we show that these can be efficiently solved either exactly or approximately via the identification of modularity and submodularity structures. We provide both deterministic and statistical theoretical guarantees showing how the resulting measurement matrices perform on signals differing from the training signals, and we provide numerical examples showing our approach to be effective on a variety of data sets.	91012	0.0	3	3.0
187	1910.03492	203902674	Neural Language Priors	The choice of sentence encoder architecture reflects assumptions about how a sentence's meaning is composed from its constituent words. We examine the contribution of these architectures by holding them randomly initialised and fixed, effectively treating them as as hand-crafted language priors, and evaluating the resulting sentence encoders on downstream language tasks. We find that even when encoders are presented with additional information that can be used to solve tasks, the corresponding priors do not leverage this information, except in an isolated case. We also find that apparently uninformative priors are just as good as seemingly informative priors on almost all tasks, indicating that learning is a necessary component to leverage information provided by architecture choice.	141868	0.0	3	3.0
188	1402.1294	119228393	A new scheme for color confinement due to violation of the non-Abelian Bianchi identities	A new scheme for color confinement in QCD due to violation of the non-Abelian Bianchi identities is discussed. The violation of the non-Abelian Bianchi identities (VNABI) $J_{\mu}$ is equal to Abelian-like monopole currents $k_{\mu}$ defined by the violation of the Abelian-like Bianchi identities. Although VNABI is an adjoint operator satisfying the covariant conservation rule $D_{\mu}J_{\mu}=0$, it gives us, at the same time, the Abelian-like conservation rule $\partial_{\mu}J_{\mu}=0$. The Abelian-like conservation rule $\partial_{\mu}J_{\mu}=0$ is also gauge-covariant. There are $N^2-1$ conserved magnetic charges in the case of color $SU(N)$. The charge of each component of VNABI is quantized \`{a} la Dirac. VNABI satisfying the Dirac quantization condition could be defined on lattice as lattice Abelian-like monopole currents without any gauge-fixing. Previous studies of the Abelian-like monopoles $k_{\mu}$ on lattice show that non-Abelian color confinement could be understood by the Abelian-like dual Meissner effect due to condensation of VNABI.	61253	1.1764706373214722	1	2.1760001182556152
189	1210.7234	9538834	Polaron formation: Ehrenfest dynamics vs. exact results.	We use a one-dimensional tight binding model with an impurity site characterized by electron-vibration coupling, to describe electron transfer and localization at zero temperature, aiming to examine the process of polaron formation in this system. In particular we focus on comparing a semiclassical approach that describes nuclear motion in this many vibronic-states system on the Ehrenfest dynamics level to a numerically exact fully quantum calculation based on the Bonca-Trugman method [J. Bonča and S. A. Trugman, Phys. Rev. Lett. 75, 2566 (1995)]. In both approaches, thermal relaxation in the nuclear subspace is implemented in equivalent approximate ways: In the Ehrenfest calculation the uncoupled (to the electronic subsystem) motion of the classical (harmonic) oscillator is simply damped as would be implied by coupling to a Markovian zero temperature bath. In the quantum calculation, thermal relaxation is implemented by augmenting the Liouville equation for the oscillator density matrix with kinetic terms that account for the same relaxation. In both cases we calculate the probability to trap the electron by forming a polaron and the probability that it escapes to infinity. Comparing these calculations, we find that while both result in similar long time yields for these processes, the Ehrenfest-dynamics based calculation fails to account for the correct time scale for the polaron formation. This failure results, as usual, from the fact that at the early stage of polaron formation the classical nuclear dynamics takes place on an unphysical average potential surface that reflects the distributed electronic population in the system, while the quantum calculation accounts fully for correlations between the electronic and vibrational subsystems.	31868	1.1764706373214722	2	3.1760001182556152
190	1010.0725	207372751	Link Prediction in Complex Networks: A Survey		4714	0.0	3	3.0
191	1810.04789	195873962	Applications of Graph Integration to Function Comparison and Malware Classification	We classify .NET files as either benign or malicious by examining directed graphs derived from the set of functions comprising the given file. Each graph is viewed probabilistically as a Markov chain where each node represents a code block of the corresponding function, and by computing the PageRank vector (Perron vector with transport), a probability measure can be defined over the nodes of the given graph. Each graph is vectorized by computing Lebesgue antiderivatives of hand-engineered functions defined on the vertex set of the given graph against the PageRank measure. Files are subsequently vectorized by aggregating the set of vectors corresponding to the set of graphs resulting from decompiling the given file. The result is a fast, intuitive, and easy-to-compute glass-box vectorization scheme, which can be leveraged for training a standalone classifier or to augment an existing feature space. We refer to this vectorization technique as PageRank Measure Integration Vectorization (PMIV). We demonstrate the efficacy of PMIV by training a vanilla random forest on 2.5 million samples of decompiled. NET, evenly split between benign and malicious, from our in-house corpus and compare this model to a baseline model which leverages a text-only feature space. The median time needed for decompilation and scoring was 24ms. 11Code available at https://github.com/gtownrocks/grafuple	100773	0.0	3	3.0
192	1703.08132	16046589	Weakly Supervised Action Learning with RNN Based Fine-to-Coarse Modeling	We present an approach for weakly supervised learning of human actions. Given a set of videos and an ordered list of the occurring actions, the goal is to infer start and end frames of the related action classes within the video and to train the respective action classifiers without any need for hand labeled frame boundaries. To address this task, we propose a combination of a discriminative representation of subactions, modeled by a recurrent neural network, and a coarse probabilistic model to allow for a temporal alignment and inference over long sequences. While this system alone already generates good results, we show that the performance can be further improved by approximating the number of subactions to the characteristics of the different action classes. To this end, we adapt the number of subaction classes by iterating realignment and reestimation during training. The proposed system is evaluated on two benchmark datasets, the Breakfast and the Hollywood extended dataset, showing a competitive performance on various weak learning tasks such as temporal action segmentation and action alignment.	93851	2.3529412746429443	3	5.353000164031982
193	1308.3182	13931603	Structural measures for multiplex networks.	Many real-world complex systems consist of a set of elementary units connected by relationships of different kinds. All such systems are better described in terms of multiplex networks, where the links at each layer represent a different type of interaction between the same set of nodes rather than in terms of (single-layer) networks. In this paper we present a general framework to describe and study multiplex networks, whose links are either unweighted or weighted. In particular, we propose a series of measures to characterize the multiplexicity of the systems in terms of (i) basic node and link properties such as the node degree, and the edge overlap and reinforcement, (ii) local properties such as the clustering coefficient and the transitivity, and (iii) global properties related to the navigability of the multiplex across the different layers. The measures we introduce are validated on a genuinely multiplex data set of Indonesian terrorists, where information among 78 individuals are recorded with respect to mutual trust, common operations, exchanged communications, and business relationships.	48593	0.5882353186607361	2	2.5880000591278076
194	1907.05552	196471197	Tiny-Inception-ResNet-v2: Using Deep Learning for Eliminating Bonded Labors of Brick Kilns in South Asia	"This paper proposes to employ a Inception-ResNet inspired deep learning architecture called Tiny-Inception-ResNet-v2 to eliminate bonded labor by identifying brick kilns within ""Brick-Kiln-Belt"" of South Asia. The framework is developed by training a network on the satellite imagery consisting of 11 different classes of South Asian region. The dataset developed during the process includes the geo-referenced images of brick kilns, houses, roads, tennis courts, farms, sparse trees, dense trees, orchards, parking lots, parks and barren lands. The dataset is made publicly available for further research. Our proposed network architecture with very fewer learning parameters outperforms all state-of-the-art architectures employed for recognition of brick kilns. Our proposed solution would enable regional monitoring and evaluation mechanisms for the Sustainable Development Goals."	125348	2.3529412746429443	6	8.352999687194824
195	1908.11447	201698167	Renormalized AdS gravity and holographic entanglement entropy of even-dimensional CFTs		134095	1.1764706373214722	1	2.1760001182556152
196	1909.09241	202712459	Nguyen’s tridents and the classification of semigraphical translators for mean curvature flow	Abstract We construct a one-parameter family of singly periodic translating solutions to mean curvature flow that converge as the period tends to 0 to the union of a grim reaper surface and a plane that bisects it lengthwise. The surfaces are semigraphical: they are properly embedded, and, after removing a discrete collection of vertical lines, they are graphs. We also provide a nearly complete classification of semigraphical translators.	138450	1.1764706373214722	2	3.1760001182556152
197	1901.06219	58029038	Red Blood Cell Image Generation for Data Augmentation Using Conditional Generative Adversarial Networks	In this paper, we describe how to apply image-to-image translation techniques to medical blood smear data to generate new data samples and meaningfully increase small datasets. Specifically, given the segmentation mask of the microscopy image, we are able to generate photorealistic images of blood cells which are further used alongside real data during the network training for segmentation and object detection tasks. This image data generation approach is based on conditional generative adversarial networks which have proven capabilities to high-quality image synthesis. In addition to synthesizing blood images, we synthesize segmentation mask as well which leads to a diverse variety of generated samples. The effectiveness of the technique is thoroughly analyzed and quantified through a number of experiments on a manually collected and annotated dataset of blood smear taken under a microscope.	109215	5.882352828979492	4	9.881999969482422
198	1901.02465	118597453	The fate of planetesimal discs in young open clusters: implications for 1I/’Oumuamua, the Kuiper belt, the Oort cloud, and more	"
 We perform N-body simulations of the early phases of open cluster evolution including a large population of planetesimals, initially arranged in Kuiper-belt like discs around each star. Using a new, fourth-order, and time-reversible N-body code on Graphics Processing Units (GPUs), we evolve the whole system under the stellar gravity, i.e. treating planetesimals as test particles, and consider two types of initial cluster models, similar to IC348 and the Hyades, respectively. In both cases, planetesimals can be dynamically excited, transferred between stars, or liberated to become free-floating (such as A/2017 U1 or ’Oumuamua) during the early cluster evolution. We find that planetesimals captured from another star are not necessarily dynamically distinct from those native to a star. After an encounter, both native and captured planetesimals can exhibit aligned periastrons, qualitatively similar to that seen in the Solar system and commonly thought to be the signature of Planet 9. We discuss the implications of our results for both our Solar system and exoplanetary systems."	107534	1.7647058963775635	1	2.765000104904175
199	1405.3117	6244108	Be In The Know: Connecting News Articles to Relevant Twitter Conversations	In the era of data-driven journalism, data analytics can deliver tools to support journalists in connecting to new and developing news stories, e.g., as echoed in micro-blogs such as Twitter, the new citizen-driven media. In this paper, we propose a framework for tracking and automatically connecting news articles to Twitter conversations as captured by Twitter hashtags. For example, such a system could alert journalists about news that get a lot of Twitter reaction, so that they can investigate those conversations for new developments in the story, promote their article to a set of interested consumers, or discover general sentiment towards the story. Mapping articles to appropriate hashtags is nevertheless very challenging, due to different language styles used in articles versus tweets, the streaming aspect of news and tweets, as well as the user behavior when marking certain tweet-terms as hashtags. As a case-study, we continuously track the RSS feeds of Irish Times news articles and a focused Twitter stream over a two months period, and present a system that assigns hashtags to each article, based on its Twitter echo. We propose a machine learning approach for classifying and ranking article-hashtag pairs. Our empirical study shows that our system delivers high precision for this task.	69075	2.941176414489746	4	6.940999984741211
