cd /work/k.church/semantic_scholar 

Semantic Scholar has lots of links like this:
https://www.semanticscholar.org/paper/Online-Speaker-Diarization-with-Graph-based-Label-Zhang-Lin/9d5644d416f2110f983acda61f75fd09b84e6d3e
https://www.semanticscholar.org/author/Ming-Li/2150652518

You actually don't need the full URL.  The following links will redirect to the ones above
https://www.semanticscholar.org/paper/9d5644d416f2110f983acda61f75fd09b84e6d3e
https://www.semanticscholar.org/author/2150652518

I will refer to the last bit of the URLs above as ids.  There are lots of different types of ids floating around

corpusid
paperid
authorid
PMID (pubmed id)
PMC
DOI
DBLP
MAG
ACL
ArXiv
...

If you have the DOI, then you can get more info from links like this:

http://doi.org/10.1038/425242a
https://sci-hub.se/10.1038/425242a

NOTE that DOI matching can be a bit tricky.  Spaces are sometimes permitted.  Case doesn't seem to matter (at least in some places):

These links all point to the same place as above:

https://sci-hub.se/10.1038/425242A
https://sci-hub.se/10.1038/425242%20A

If you have any of these ids, you can get many other fields with something like this:

echo 172192617 | /work/k.church/semantic_scholar/citations/graphs/src/fetch_from_semantic_scholar_api.py --fields externalIds
{'paperId': '38213de24ba58922a6084d5b82c2f8daf41984b4', 'externalIds': {'MAG': '2405552', 'CorpusId': 172192617}}
(gft) [k.church@c0176 ~]$ echo 38213de24ba58922a6084d5b82c2f8daf41984b4 | /work/k.church/semantic_scholar/citations/graphs/src/fetch_from_semantic_scholar_api.py --fields externalIds
{'paperId': '38213de24ba58922a6084d5b82c2f8daf41984b4', 'externalIds': {'MAG': '2405552', 'CorpusId': 172192617}}
echo 'DOI:10.1016/0006-2952(75)90029-5' | /work/k.church/semantic_scholar/citations/graphs/src/fetch_from_semantic_scholar_api.py --fields externalIds
{'paperId': '43125c5bd1870dccba5cd2e9f83f734302628b87', 'externalIds': {'MAG': '2028961430', 'DOI': '10.1016/0006-2952(75)90029-5', 'CorpusId': 46273216, 'PubMed': '8'}}
(gft) [k.church@c0176 ids]$ echo 'PMID:8' | /work/k.church/semantic_scholar/citations/graphs/src/fetch_from_semantic_scholar_api.py --fields externalIds
{'paperId': '43125c5bd1870dccba5cd2e9f83f734302628b87', 'externalIds': {'MAG': '2028961430', 'DOI': '10.1016/0006-2952(75)90029-5', 'CorpusId': 46273216, 'PubMed': '8'}}
(gft) [k.church@c0176 ids]$ echo 'PMID:8' | /work/k.church/semantic_scholar/citations/graphs/src/fetch_from_semantic_scholar_api.py --fields externalIds,title
{'paperId': '43125c5bd1870dccba5cd2e9f83f734302628b87', 'externalIds': {'MAG': '2028961430', 'DOI': '10.1016/0006-2952(75)90029-5', 'CorpusId': 46273216, 'PubMed': '8'}, 'title': 'Comparison between procaine and isocarboxazid metabolism in vitro by a liver microsomal amidase-esterase.'}


Instructions for how to crawl pubmed:
	     https://www.ncbi.nlm.nih.gov/books/NBK25497/
	     https://www.ncbi.nlm.nih.gov/books/NBK3837/
to get an API key, go to: https://www.ncbi.nlm.nih.gov/account/settings/

https://eutils.ncbi.nlm.nih.gov/entrez/eutils/einfo.fcgi
https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esummary.fcgi?db=pubmed&id=9007877 # returns xml
https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&id=9007877   # returns a json object

https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed_pubmed&id=9007877   # returns a json object

# These don't work... I'm just guessing how to use these tools
https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&query_key=PARALLEL%20GENETIC%20PROGRAMMING%20ON%20A%20NETWORK%20OF%20TRANSPUTERS&api_key=7c6aac71a1c6e0b44c10fabef49eff745909&webenv=kwc1
https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&query_key=GENETIC%20PROGRAMMING&api_key=7c6aac71a1c6e0b44c10fabef49eff745909&webenv=kwc1

# recommendations from semantic scholar
https://api.semanticscholar.org/recommendations/v1/papers/forpaper/9e2caa39ac534744a180972a30a320ad0ae41ea3?fields=url,abstract,authors


id=uid1,uid2,uid3&rettype=report_type&retmode=
data_mode


NLM_API_KEY=7c6aac71a1c6e0b44c10fabef49eff745909 

PubMed has links like these that make use of PMID (and PMC)
https://pubmed.ncbi.nlm.nih.gov/4732746
https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6943321/

https://pubmed.ncbi.nlm.nih.gov/9007877/
https://pubmed.ncbi.nlm.nih.gov/?size=50&linkname=pubmed_pubmed&from_uid=9007877   # find similar docs (by PMID)
https://pubmed.ncbi.nlm.nih.gov/?format=pmid&size=50&linkname=pubmed_pubmed&from_uid=9007877 # same as above, but easier for computer to parse
https://eutils.ncbi.nlm.nih.gov/entrez/eutils/elink.fcgi?dbfrom=pubmed&db=pubmed&id=9007877&cmd=neighbor_score

https://eutils.ncbi.nlm.nih.gov/entrez/eutils/elink.fcgi?dbfrom=pubmed&db=pubmed&id=10491450
https://eutils.ncbi.nlm.nih.gov/entrez/eutils/elink.fcgi?dbfrom=pubmed&db=pubmed&id=10491450&cmd=neighbor_score

https://eutils.ncbi.nlm.nih.gov/entrez/eutils/elink.fcgi?dbfrom=pubmed&db=pubmed&cmd=neighbor_score&id=10491450,21952578,19606921

https://www.ncbi.nlm.nih.gov/research/pubtator/?view=docsum&query=PMC14546 (PMC)
https://www.ncbi.nlm.nih.gov/research/pubtator/?view=docsum&query=26824844 (PMID)

https://www.wikidata.org/wiki/Q46268426
https://pubmed.ncbi.nlm.nih.gov/20571009/


https://archive.org/details/fatcat_snapshots_and_exports
 /work/k.church/semantic_scholar/papers/externalids/combined <==
CorpusId	DOI	DBLP	ArXiv	PubMed	PubMedCentral	MAG	ACL
172192617	None	None	None	None	None	2405552	None
179016946	None	None	None	None	None	1578390176	None
10350567	10.1109/CCDC.2009.5194854	None	None	None	None	2079661381	None
177434374	None	None	None	None	None	623219711	None
204644389	10.22034/JPIUT.2019.28311.2046	None	None	None	None	2973511042	None
186987721	None	None	None	None	None	2919236376	None
222525142	None	None	None	None	None	3028828807	None
172272523	None	None	None	None	None	150157994	None
180601888	10.1086/PBSA.45.2.24298675	None	None	None	None	2321103804	None

==> /work/k.church/pubmed/ftp.ncbi.nlm.nih.gov/pubmed/baseline/ids/combined <==
pubmed	doi	pii	pmc	mid
1	10.1016/0006-2944(75)90147-7	None	None	None
2	10.1016/0006-291x(75)90482-9	0006-291X(75)90482-9	None	None
3	10.1016/0006-291x(75)90498-2	0006-291X(75)90498-2	None	None
4	10.1016/0006-291x(75)90506-9	0006-291X(75)90506-9	None	None
5	10.1016/0006-291x(75)90508-2	0006-291X(75)90508-2	None	None
6	10.1016/0006-291x(75)90518-5	0006-291X(75)90518-5	None	None
7	10.1016/0006-2952(75)90020-9	0006-2952(75)90020-9	None	None
10885091	None	None	None	None
11106086	None	None	None	None

I'm working on better ways to join CorpusId (from semantic scholar) to PMID (from pubmed).
Some of that work is in: /work/k.church/ids

You should add this to your PATH
which find_lines
/work/k.church/semantic_scholar/citations/graphs/src/C/find_lines

This will convert semantic scholar CorpusId to a URL:

echo 10350567 | find_lines --input /work/k.church/semantic_scholar/papers/papers2url.V2/lines2html.V2
10350567	<a href="https://www.semanticscholar.org/paper/6056eef3a7267a329f36f19e517c0dcf1f8e863e">0: The analysis of main macroeconomic variable's shock effect to CPI</a>

The raw data for semantic scholar comes from 30 pieces in 7 directories like this:

find /work/k.church/semantic_scholar  -maxdepth 3 -name '*piece.001.gz'
/work/k.church/semantic_scholar/abstracts/abstracts.piece.001.gz
/work/k.church/semantic_scholar/authors/authors.piece.001.gz
/work/k.church/semantic_scholar/citations/citations.piece.001.gz
/work/k.church/semantic_scholar/embeddings/embeddings.piece.001.gz
/work/k.church/semantic_scholar/papers/papers.piece.001.gz
/work/k.church/semantic_scholar/s2orc/s2orc.piece.001.gz
/work/k.church/semantic_scholar/tldrs/tldrs.piece.001.gz

They refer to these 7 directories as databases.  They come from the
5/31/2022 snapshot of semantic scholar using the bulk download API
(see https://www.semanticscholar.org/product/api).

# Here is a one line description of each paper:

zcat /work/k.church/semantic_scholar/tldrs/tldrs.piece.001.gz | head
{"corpusid":13756819,"model":"tldr@v2.0.0","text":"It is obtained that the self-healing reaches a limit degree at the far field propagation domain, and that certain relatively small phase obstructions may produce a total damage on the beam."}
{"corpusid":13756928,"model":"tldr@v2.0.0","text":"An actively Q-switched Nd:Y VO4/YVO4 intracavity Raman laser at second-Stokes wavelength of 1313.6 nm is reported, which is capable of operating efficiently under pulse repetition frequency higher than 80 kHz."}
{"corpusid":13757302,"model":"tldr@v2.0.0","text":"Home-based, patient-centered care may improve early screening and detection of dementia and lead to early treatment which is beneficial to patients and the community."}
{"corpusid":13757621,"model":"tldr@v2.0.0","text":"The findings seem to suggest that, when coupled with the use of broth culture, U-ITI analysis may allow for more rapid microbial identification than biochemical methods, while no advantages in time to detect microbial growth were observed."}
{"corpusid":13757871,"model":"tldr@v2.0.0","text":"To test whether disordered and normal voice during vowel production is generalizable to connected speech, three speaking tasks were investigated and statistical differences were found between these tasks for certain amplitude and time based laryngeal function measures."}
{"corpusid":13758904,"model":"tldr@v2.0.0","text":"The mechanism by which active isopropyl quinoxaline-7-carboxylate 1,4-di-N-oxideQuinoxalines affect the parasite is described, indicating the induction of regulated necrosis."}
{"corpusid":13758907,"model":"tldr@v2.0.0","text":"These are the first data on placenta concentrations of methadone and metabolites after controlled drug administration, and will improve knowledge of the usefulness of this matrix for detecting in utero drug exposure and studying disposition of drugs in the maternal-fetal dyad."}
{"corpusid":13760675,"model":"tldr@v2.0.0","text":"Some observational studies of the general population showed that resting heart rate was associated with mortality, however, the relationship was unclear in dialysis patients, and the role in patients with kidney failure is still unclear."}
{"corpusid":13760756,"model":"tldr@v2.0.0","text":"The margins in the high speed group were more or less precisely as required, with less debris and no thermal necrosis, which illustrated the efficacy of a high speed osteotomy."}
{"corpusid":13761260,"model":"tldr@v2.0.0","text":"The platform is described: how the platform can be used to infer networks from a set of input genes, such as upregulated genes from an expression experiment, and how to avoid common pitfalls when evaluating the networks."}

The papers table has stuff about each corpusid.  There is a corpusid
for each of the nearly 250M papers in the system.  These papers come
from ACL, DBLP, ArXiv, MAG, etc.  Many of of these corpusids have one
or more external ids.  Warning: I am finding the same paper in many of
these external sources.  They report some (but not all) matches.  I'd
like to join them across more sources because some of these other
sources offer features (like similar papers) that are different from
the features in semantic scholar.

zcat /work/k.church/semantic_scholar/papers/papers.piece.001.gz | jq . | sed 30q
{
  "corpusid": 172192617,
  "externalids": {
    "ACL": null,
    "DBLP": null,
    "ArXiv": null,
    "MAG": "2405552",
    "CorpusId": "172192617",
    "PubMed": null,
    "DOI": null,
    "PubMedCentral": null
  },
  "url": "https://www.semanticscholar.org/paper/38213de24ba58922a6084d5b82c2f8daf41984b4",
  "title": "「現代日本女性の生き方意識に関する研究 : 特に宗教的意識および倫理的価値意識の側面から」(博士論文要旨・修士論文題目)",
  "authors": [
    {
      "authorId": "135715795",
      "name": "山縣 喜代"
    }
  ],
  "venue": "",
  "year": 1993,
  "referencecount": 0,
  "citationcount": 0,
  "influentialcitationcount": 0,
  "isopenaccess": false,
  "s2fieldsofstudy": null,
  "updated": "2022-01-24T18:57:32.578Z"
}

The authors table has info for each authorid.

zcat /work/k.church/semantic_scholar/authors/authors.piece.001.gz | jq . | sed 30q
{
  "authorid": "2106070277",
  "externalids": null,
  "url": "https://www.semanticscholar.org/author/2106070277",
  "name": "W. Davies",
  "aliases": [
    "W. Davies",
    "W. J. Davies"
  ],
  "affiliations": null,
  "homepage": null,
  "papercount": 4,
  "citationcount": 18,
  "hindex": 3,
  "updated": "2021-06-20T00:01:53.099Z"
}

The embedding table has specter embeddings for each corpusid.  That is a vector of 768 floats.

Those embeddings are used in the following scripts

# NOTE: some of these run slowly the first time because they mmap a lot
# of data.  But they will run quickly after you use them for a few
# queries

# The next 4 shell scripts take a corpusid as input

cd /work/k.church/githubs/scidocs/data/recomm
./line2specter.txt.sh 9558665
1.000000	9558665
0.994415	47390681
0.987289	35969603
0.805525	147470899
0.802543	16241602
0.801242	205485143
0.789194	3265361
0.779955	12563174
0.779713	2629140
0.778711	16105777

# Like above, but outputs a web page
./line2specter.sh 9558665

# Like above, but for a proposed alternative to specter
./line2proposed.txt.sh 9558665
1.000000	9558665
0.995231	47390681
0.991499	8754851
0.990136	6357507
0.989129	10685940
0.988744	10507844
0.987785	10782902
0.987721	10592782
0.987705	14655549
0.987696	643756

# Like above, but outputs a web page
./line2proposed.sh 9558665

I computed a citation matrix from the following table.  This table has
a bunch of fields that I didn't use.  Citations are classified as
influential (or not).  They also provide some context around each
citation.

zcat /work/k.church/semantic_scholar/citations/citations.piece.001.gz | jq . | sed 30q
{
  "citingcorpusid": "153224488",
  "citedcorpusid": "221965067",
  "isinfluential": false,
  "contexts": null,
  "intents": null,
  "updated": "2022-03-07T06:15:05.336Z"
}
{
  "citingcorpusid": "201330869",
  "citedcorpusid": "144756398",
  "isinfluential": false,
  "contexts": null,
  "intents": null,
  "updated": "2022-03-07T22:02:49.303Z"
}
{
  "citingcorpusid": "120832150",
  "citedcorpusid": "118090559",
  "isinfluential": false,
  "contexts": [
    "ly measurable level. Cosmic strings from GUTs [35], on the other hand, can generate gravity waves [36], which will be possibly measurable [24, 37, 38] by future experiments. Finally, domain walls are [33] catastrophic and GUTs should be cons\
tructed so that they avoid them (see e.g. Ref. [39]) or inﬂation should extinguish them. Note that, in some cases, more complex extended objects such as walls boun",
    "ation. Thisisthe second (potential) success oftheSBBmodel. Moreover,attheGUTphasetransition, topologically stableextended objects[31]suchasmagnetic monopoles [7], cosmic strings [32], or domain walls [33] can also be generated. Monopoles, wh\
ich exist in most GUTs, can lead into problems [8] which are, however, avoided by inﬂation [10, 11] (see below). This is a period of exponentially fast expansion o"
  ],
  "intents": [
    "background"
  ],
  "updated": "2022-03-07T23:06:12.877Z"
}
{

cd /work/k.church/semantic_scholar/citations/graphs
python
import numpy as np
import scipy.sparse
f='citations.G.shrink.new.T1.G2.npz'
M=scipy.sparse.load_npz(f)
M.shape


# N is only 108M (because I removed a bunch of papers without much connectivity, and changed the ids to a perfect hash)
>>> (108651994, 108651994)

# This is a boolean matrix
M.dtype
dtype('bool')


# There are 2B nonzero edges
M.count_nonzero()
2061048418

# The first two documents have 43 nonzero values (fanout)
M[1:3,:].count_nonzero()
43
>>> X,Y=M[1:3,:].nonzero()
>>> X
array([0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
      dtype=int32)
>>> Y
array([ 2449114, 28471223, 36655415,   547071,   696717,   930425,
        1168258,  1817441,  1851694,  3062882,  4154942,  4272272,
        4354626,  5605198,  6336377,  6650280,  6722670,  7076900,
        7548705,  8135630,  8353157,  9703034, 11133607, 13793282,
       13830920, 14118059, 14489972, 15151564, 15869412, 15951256,
       16404952, 17718904, 19144524, 20368615, 20423650, 21626102,
       25854411, 26700791, 29518716, 30523654, 30665538, 30767940,
       42775846], dtype=int32)

# The matrix is not symmetric
X,Y = M[:,1:3].nonzero()
>>> X
array([  2983397,   2999017,   3203139,   3754343,   4479066,   5510521,
         7718735,  10062192,  16058102,  23407146,  28242569,  32686543,
        36055605,  38049503,  39855393,  41534844,  87396775,  89769463,
        91774224,  95652634,  96303252,  96708670,  97978950,  98519796,
        98533602,  99653831, 104010403, 105448939, 105810538, 105949884],
      dtype=int32)
>>> Y
array([1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,
       0, 1, 1, 1, 1, 1, 1, 1], dtype=int32)

# I use this tool on the following annoy objects: /work/k.church/semantic_scholar/embeddings/M3/semantic_scholar_embedding_near.py

# The proposed method uses node2vec to construct embeddings from the citation graph (see ~kwc/final/morphology/dict_to_embedding.py)
# dict_to_embeddings.py uses https://www.ijcai.org/proceedings/2019/0594.pdf (nodevectors.ProNE from https://github.com/VHRanger/nodevectors)

# Annoy files make use of https://pypi.org/project/gensim/
/work/k.church/semantic_scholar/citations/graphs/K250/citations.G.shrink.new.T1.G2.npz.tasks.1.K250.ProNE.K250.T20.O5.w2v.bin.annoy

# Here is an annoy object for specter
/scratch/k.church/semantic_scholar/embeddings/embeddings.w2v.bin.annoy



#######
The following are notes for my memory; may not be so useful to others
#######

seff 29113748
sacct k.church

# constrint=ib (infinity band)
# sinfo -p short --Format=nodes,cpus,memory,features,nodelist
# https://vastdata.com/  (/work)
# gpft (/scratch) marina levi

wget https://api.semanticscholar.org/datasets/v1/release/2022-05-31

####

rel=2022-08-23
rel=2022-12-02
rel=2022-11-22
rel=2022-12-02
rel=2023-05-09
mkdir -p /work/k.church/semantic_scholar/releases/$rel
cd /work/k.church/semantic_scholar/releases/$rel
wget https://api.semanticscholar.org/datasets/v1/release/$rel
# curl -X GET https://api.semanticscholar.org/datasets/v1/release/$rel/dataset/citations -H "x-api-key: Us7RqgayhnaQkEKiEnbGH8EBneX5Jud14Mq3Uzpe" > citations.json
for d in abstracts papers authors citations embeddings papers s2orc tldrs
do
mkdir -p database/$d/todo
curl -X GET https://api.semanticscholar.org/datasets/v1/release/$rel/dataset/$d -H "x-api-key: Us7RqgayhnaQkEKiEnbGH8EBneX5Jud14Mq3Uzpe" > database/$d/todo/$d.json
done

for d in abstracts papers abstracts authors citations embeddings papers s2orc tldrs
do
cd /work/k.church/semantic_scholar/releases/$rel/database/$d/todo
python /work/k.church/semantic_scholar/json2files.py < $d.json |
awk '{f = sprintf("%s.todo.%03d.sh", d, NR);
printf "#!/bin/sh\ncurl --create-dirs \"%s\" -o %s.piece.%03d.gz\n", $0, d, NR > f; close(f)}' d=$d 
done

cd /work/k.church/semantic_scholar/releases/$rel/database/
for d in abstracts papers authors citations embeddings papers s2orc tldrs
do
cd /work/k.church/semantic_scholar/releases/$rel/database/$d
for f in /work/k.church/semantic_scholar/releases/$rel/database/$d/todo/*.todo.*.sh
do
# sbatch -p short  -e $f.err $f
sbatch -p express -t 59  -e $f.err $f
done
done


# for f in /work/k.church/semantic_scholar/releases/$rel/database/*/todo/*.todo.*.sh
# do
# echo $f
# sbatch -p short  -e $f.err $f
# done



#####
## WARNING: This needs to be done by hand
###
rel=2022-08-23
rel=2022-12-02
cd /work/k.church/semantic_scholar/releases/$rel/database/
for d in abstracts papers authors citations embeddings papers s2orc tldrs
do
mv ../"$d"*.gz $d
done

cd /work/k.church/semantic_scholar/releases/2022-12-02/database/tldrs/todo
for d in abstracts papers authors citations embeddings papers s2orc tldrs
do
mv "$d"*.gz /work/k.church/semantic_scholar/releases/2022-12-02/database/$d
done


curl -X GET https://api.semanticscholar.org/datasets/v1/release/2022-05-31/dataset/citations -H "x-api-key: Us7RqgayhnaQkEKiEnbGH8EBneX5Jud14Mq3Uzpe" > citations.json

https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html#cliv2-linux-install
curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"
./aws/install -i $HOME/local/aws-cli -b $HOME/bin

/home/k.church/bin/aws --version
https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-quickstart.html


Access Key ID:
AKIAVPBCSN6KSDSUIxxx
Secret Access Key:
azWI7PFXs9OH+I8PjdUR/LLK8vZMqKyhiNW7vxxx

cd /work/k.church/semantic_scholar

for f in $(curl https://s3-us-west-2.amazonaws.com/ai2-s2ag/samples/MANIFEST.txt)
  do curl --create-dirs "https://s3-us-west-2.amazonaws.com/ai2-s2ag/$f" -o $f
done

curl -X GET https://api.semanticscholar.org/datasets/v1/release/2022-05-31/dataset/citations -H "x-api-key: Us7RqgayhnaQkEKiEnbGH8EBneX5Jud14Mq3Uzpe" > citations.json
python json2files.py < citations.json |
awk 'BEGIN {print "#!/bin/sh"}
{printf "curl --create-dirs \"%s\" -o citations.piece.%03d\n", $0, NR}' > citations.todo.sh

cd /work/k.church/semantic_scholar
for d in abstracts # papers #  authors citations embeddings papers s2orc tldrs
do
curl -X GET https://api.semanticscholar.org/datasets/v1/release/2022-05-31/dataset/$d -H "x-api-key: Us7RqgayhnaQkEKiEnbGH8EBneX5Jud14Mq3Uzpe" > $d.json
done

curl -X GET https://api.semanticscholar.org/recommendations/v1/papers/forpaper/649def34f8be52c8b66281af98ae884c09aef38b -H "x-api-key: Us7RqgayhnaQkEKiEnbGH8EBneX5Jud14Mq3Uzpe"

for jj in j['recommendedPapers']: print(jj['paperId'] + '\t' + jj['title'])

# curl -X POST 
# {  "positivePaperIds": [    "649def34f8be52c8b66281af98ae884c09aef38b"  ],   "negativePaperIds": [    "ArXiv:1805.02262"  ]}


cd /work/k.church/semantic_scholar
for d in abstracts # papers # abstracts authors citations embeddings papers s2orc tldrs
do
python json2files.py < $d.json |
awk '{f = sprintf("%s.todo.%03d.sh", d, NR);
printf "#!/bin/sh\ncurl --create-dirs \"%s\" -o %s.piece.%03d.gz\n", $0, d, NR > f; close(f)}' d=$d 
done

for f in todo/*.sh
do
b=`basename $f .sh`
sbatch -p short -o $b.gz -e $b.err $f
done

cd /work/k.church/semantic_scholar/citations/graphs
for f in *.w2v # citations.G.shrink.new.T1.G2.npz.tasks.1.K*.ProNE.K*.T20.O5.w2v # *.w2v
do
if [ ! -s $f.annoy ]
then
echo working on $f
sbatch --mem=1000000 -t 1209 -p short -e $f.err /work/k.church/semantic_scholar/jobs/glove2annoy.py $f
fi
done

cd /work/k.church/semantic_scholar/citations/graphs
for f in citations.G.shrink.new.T1.G2.npz.K100.ProNE.K100.T20.O5.w2v # citations.G.shrink.new.T1.G2.npz.tasks.1.K200.ProNE.K200.T20.O5.w2v # embeddings/embeddings.piece.???.w2v
do
sbatch   --mem=500000 -t 1209 -p short -e $f.err /work/k.church/semantic_scholar/jobs/word2vec_to_annoy.sh $f
# sbatch --mem=100000 -t 59 -p express -e $f.err /work/k.church/semantic_scholar/jobs/word2vec_to_annoy.sh $f
# sbatch --mem=100000 -p debug -e $f.err /work/k.church/semantic_scholar/jobs/word2vec_to_annoy.sh $f
done

# cat citations.G.shrink.new.T1.G2.npz.tasks.1.K300.ProNE.K300.T20.O5.w2v | sed 300q | /work/k.church/semantic_scholar/jobs/w2v_to_npz.py /tmp/foobar
cd /work/k.church/semantic_scholar/citations/graphs
for f in *.w2v
do
sbatch -i $f --mem=1000G -t 1209 -p short /work/k.church/semantic_scholar/jobs/w2v_to_npz.py $f.kwc
done

# cat citations.G.shrink.new.T1.G2.npz.tasks.1.K300.ProNE.K300.T20.O5.w2v | sed 300q | /work/k.church/semantic_scholar/jobs/w2v_to_npz.py /tmp/foobar
# /work/k.church/semantic_scholar/jobs/kwc_to_npz.py /tmp/bar.kwc /tmp/bar
# cat *w2v | head  > /tmp/bar
# /work/k.church/semantic_scholar/jobs/w2v_to_kwc.sh /tmp/bar /tmp/bar.kwc
cd /work/k.church/semantic_scholar/citations/graphs
for f in *.w2v
do
sbatch -t 1209 -p short /work/k.church/semantic_scholar/jobs/w2v_to_kwc.sh $f $f.kwc
done



cd /work/k.church/semantic_scholar/citations/graphs
src=/work/k.church/semantic_scholar/citations/graphs/src
for f in *.w2v
do
out=/scratch/k.church/semantic_scholar/citations/graphs/`basename $f`.filtered
sbatch -e $out.err -i $f -o $out -t 1209 -p short -e $f.err $src/filter_w2v.py
done



cd /work/k.church/semantic_scholar/embeddings/M2
for f in x??
do
if [ -s $f.annoy ]
then
echo $f is already done
else
echo working on $f
sbatch -p short -e $f.err /work/k.church/semantic_scholar/jobs/glove2annoy.py $f
fi
done

cd /scratch/k.church/semantic_scholar/citations/graphs/
for piece in *.filtered
do
if [ ! -s $piece.annoy ]
then
echo working on $piece
sbatch -t 1209 --mem=1000G -p short -e $piece.err /work/k.church/semantic_scholar/jobs/glove2annoy.py $piece
fi
done




cd /work/k.church/semantic_scholar/embeddings/M3
for f in extra*.x?? embeddings.*.x??
do
if [ -s $f.annoy ]
then
echo $f is already done
else
echo working on $f
sbatch -p short -e $f.err /work/k.church/semantic_scholar/jobs/glove2annoy.py $f
fi
done


cd /work/k.church/semantic_scholar/embeddings/M3
for f in embeddings.piece.001.w2v.xaa.annoy
do
# python semantic_scholar_embedding_near.py -M $f --query_mode self -N 10 --random 200 > $f.sample
out=$f.sample.N50
sbatch -o $out -p short -e $out.err semantic_scholar_embedding_near.py -M $f --query_mode self -N 50 --random 200
done

dir=/work/k.church/semantic_scholar/embeddings/M3
cd /work/k.church/semantic_scholar/citations/graphs
for f in *filtered.annoy
do
out=$f.sample.N50
if [ ! -s $out ]
then
echo working on $out
sbatch -t 1209 --mem=100G -o $out -p short -e $out.err $dir/semantic_scholar_embedding_near.py -M $f --query_mode self -N 50 --random 400
fi
done

cd /work/k.church/semantic_scholar/embeddings/M3
for N in 50
do
for annoy in *.annoy
do
for f in extras.x?? embeddings.piece.???.w2v.x??
do
out=near/$N/$annoy/$f.out
err=near/$N/$annoy/$f.err
mkdir -p `dirname $out`
sbatch -p short -e $err -o $out -i $f semantic_scholar_embedding_near.py -M $annoy -N $N
done
done
done

cd /work/k.church/semantic_scholar/papers
mkdir -p papers2url
for f in papers.piece.???.gz
# for f in papers.piece.026.gz
do
out=papers2url/`basename $f .gz`.txt
err=papers2url/`basename $f .gz`.err
rm $out $err
sbatch -p express -e $err -o $out /work/k.church/semantic_scholar/papers/papers2url.py $f
done

cd /work/k.church/semantic_scholar/papers/papers2url
for f in papers.piece.???.txt
# for f in papers.piece.012.txt
# for f in papers.piece.017.txt
do
sbatch -p express /work/k.church/semantic_scholar/papers/index_lines.py $f
done

cd /work/k.church/semantic_scholar/papers/papers2url
cat *txt > all_papers
sbatch -p short /work/k.church/semantic_scholar/papers/index_lines.py all_papers


cd /work/k.church/semantic_scholar/embeddings
for f in *.w2v
do
sbatch -p short /work/k.church/semantic_scholar/jobs/split_job.sh $f
done



for f in embeddings/embeddings.piece.001.w2v
do
for n in 1 2 3 4 5 6 7 8 9 10
do
echo $n
nn=`echo $n| awk '{print $1 * 100000}'`
sed "$nn"q $f | tr ':	' '_ ' > $f.$n
done
done


for d in abstracts authors citations embeddings papers s2orc tldrs
do
python json2files.py < $d.json |
awk 'BEGIN {print "#!/bin/sh"}
{printf "curl --create-dirs \"%s\" -o %s.piece.%03d.gz\n", $0, d, NR}' d=$d > $d.todo.sh
done

for d in citations # abstracts authors embeddings papers s2orc tldrs # citations
do
sbatch -o $d.out -e $d.err -p short $d.todo.sh
done

for f in *piece.001.gz
 do
 echo $f
 zcat < $f | head | awk '{print substr($0, 1, 150), length($0)}'
 done

for f in embeddings.piece.???.gz
do
b=`basename $f .gz`
out=embeddings/$b.w2v
if [ -s $out ]
then
echo $f is already done
else
echo working on $f
sbatch -p short -o embeddings/$b.w2v -e embeddings/$b.err jobs/embedding2w2v_job.sh $f
fi
done

find /work/k.church/semantic_scholar/embeddings/M3/near -name *out -exec cat {} \; | head > /tmp/x

# This doesn't work, probably because all_papers is too big
# ALL_PAPERS=/work/k.church/semantic_scholar/papers/papers2url/all_papers
# sed 1q < /tmp/x | tr '_:\t' '\n' | egrep '^[0-9]*$' | /work/k.church/semantic_scholar/papers/find_lines.py $ALL_PAPERS 

# This works
sed 1q < /tmp/x | tr '_:\t' '\n' | egrep '^[0-9]*$' | /work/k.church/semantic_scholar/papers/find_lines.py /work/k.church/semantic_scholar/papers/papers2url/papers.piece.???.txt | cut -f1,3

echo corpusid_13612452 | python /work/k.church/semantic_scholar/src/similar_papers.py

echo 9558665 | /work/k.church/semantic_scholar/papers/find_lines.py /work/k.church/semantic_scholar/papers/papers2url/papers.piece.???.txt
9558665	https://www.semanticscholar.org/paper/9e2caa39ac534744a180972a30a320ad0ae41ea3	Word Association Norms, Mutual Information and Lexicography	2244184|145165877

echo corpusid_9558665 | python /work/k.church/semantic_scholar/src/similar_papers.py

echo '{"citingcorpusid":"52084418","citedcorpusid":"129509912","isinfluential":false,"contexts":null,"intents":null,"updated":"2020-12-08T00:09:38.430Z"}' | tr '"' '\n' | egrep '^[0-9]*$' | /work/k.church/semantic_scholar/papers/find_lines.py /work/k.church/semantic_scholar/papers/papers2url/papers.piece.???.txt
52084418	https://www.semanticscholar.org/paper/d3ce9f29a320dde8d2ce7b1c7157d2298dcbdbcb	Physical Mechanisms in Hyperspectral BRDF Data of Grass and Watercress	3186450|2107152052|66609723|49485805
129509912	https://www.semanticscholar.org/paper/d2c085aac5160943c1cb5ae867016f061aef7ea2	Quantifying reflectance anisotropy of photosynthetically active radiation in grasslands	49884705


243436692 is in papers.piece.012.gz
but it doesn't seem to make it to the next step (/work/k.church/semantic_scholar/papers/papers2url/)
{"corpusid":243436692,"externalids":{"ACL":null,"DBLP":null,"ArXiv":null,"MAG":null,"CorpusId":"243436692","PubMed":null,"DOI":"10.31234/osf.io/mb93p","PubMedCentral":null},"url":"https://www.semanticscholar.org/paper/78684904d544a561ed8b29fb775432e10b61820c","title":"The Small World of Words: English word association norms for over 12,000 cue words","authors":[{"authorId":"3224601","name":"S. De Deyne"},{"authorId":"145968398","name":"D. Navarro"},{"authorId":"2841005","name":"Amy Perfors"},{"authorId":"2993988","name":"M. Brysbaert"},{"authorId":"118038650","name":"G. Storms"}],"venue":"","year":2019,"referencecount":0,"citationcount":0,"influentialcitationcount":0,"isopenaccess":true,"s2fieldsofstudy":null,"updated":"2022-04-22T04:39:35.863Z"}

~/final/morphology/dict_to_embedding.py


cd /work/k.church/semantic_scholar/citations
for f in citations.piece.???.gz
do
b=`basename $f .gz`
out=graphs/$f.graph
err=graphs/$f.err
sbatch -p short -o $out -e $err /work/k.church/semantic_scholar/citations/citations2graph.py $f citations.piece.001.gz
done

cd /work/k.church/semantic_scholar/citations
for f in citations.piece.???.gz
do
b=`basename $f .gz`
mkdir -p contexts
out=contexts/$f.context
err=contexts/$f.err
sbatch -p express -o $out -e $err /work/k.church/semantic_scholar/citations/citations2citation_contexts.py $f citations.piece.001.gz
done


cd /work/k.church/semantic_scholar/citations/graphs
for f in citations.G.npz
do
sbatch -p short -e citations.G.shrink.err shrink_matrix.py -G $f -C citations.CC.npy -o citations.G.shrink
done

cd /work/k.church/semantic_scholar/citations/graphs
for f in citations.G.npz
do
T=100
sbatch -p short -e citations.G.shrink.T$T.err shrink_matrix.py -G $f -C citations.CC.npy -o citations.G.shrink.T$T -T $T
done

cd /work/k.church/semantic_scholar/citations/graphs
for f in citations.G.npz
do
T=1
sbatch -p short -e citations.G.shrink.T$T.err shrink_matrix.py -G $f -C citations.CC.npy -o citations.G.shrink.T$T -T $T
done

cd /work/k.church/semantic_scholar/citations/graphs
for f in citations.G.npz
do
T=2
sbatch -p short -e citations.G.shrink.new.T$T.err shrink_matrix.py -G $f -C citations.CC.npy -o citations.G.shrink.new.T$T -T $T
done


cd /work/k.church/semantic_scholar/citations/graphs
for f in citations.G.npz
do
T=3
sbatch -t 1209 -p short -e citations.G.shrink.new.T$T.err shrink_matrix.py -G $f -C citations.CC.npy -o citations.G.shrink.new.T$T -T $T
done

cd /work/k.church/semantic_scholar/citations/graphs
for f in citations.G.npz
do
T=1
sbatch -t 1209 -p short -e citations.G.shrink.new.T$T.err shrink_matrix.py -G $f -C citations.CC.npy -o citations.G.shrink.new.T$T -T $T
done

cd /work/k.church/semantic_scholar/citations/graphs
for f in citations.G.shrink.new.T1.G2.npz
do
sbatch -p short -t 1209 -e $f.prone.err --mem=1000000 $HOME/final/morphology/dict_to_embedding.py -G $f -o $f.4 -K 768
done

cd /work/k.church/semantic_scholar/citations/graphs
for f in citations.G.shrink.new.T1.G2.npz
do
for K in 768 10 100
do
sbatch -p short -t 1209 -e $f.prone.K$K.err --mem=1500000 $HOME/final/morphology/dict_to_embedding.py -G $f -o $f.K$K -K $K
done
done


cd /work/k.church/semantic_scholar/citations/graphs
for f in citations.G.shrink.new.T1.G2.npz
do
for K in 10 50 100 150 200
do
err=$f.prone.K$K.instrumented.err
rm $err
sbatch -p short -t 1209 -e $err --mem=1500000 $HOME/final/morphology/dict_to_embedding.py -G $f -o $f.instrumented.K$K -K $K
done
done

cd /work/k.church/semantic_scholar/citations/graphs/random_classes
for f in 6.npz
do
for K in 10 50 100 150 200 400 800 1600
do
err=$f.prone.K$K.instrumented.err
rm $err
sbatch -p short -t 1209 -e $err --mem=1500000 $HOME/final/morphology/dict_to_embedding.py -G $f -o $f.instrumented.K$K -K $K
done
done



cd /work/k.church/semantic_scholar/citations/graphs/random_classes
for f in 0.npz 
do
for K in 10 100 200
do
sbatch -p short -t 1209 -e with_symmetry/$f.prone.K$K.err --mem=500000 $HOME/final/morphology/dict_to_embedding.py -G $f -o with_symmetry/$f.K$K -K $K
done
done




cd /work/k.church/semantic_scholar/citations/graphs
mkdir -p random_classes
for f in citations.G.shrink.new.T1.G2.npz
do
for i in 0 1 2 3 4 5 6 7 8 9
do
out=random_classes/$i
sbatch -t 59 --mem=100000 -p express -e $out.err shrink_by_random_assignments.py -G $f -o random_classes/$i -n 1000000
done
done

cd /work/k.church/semantic_scholar/citations/graphs
mkdir -p random_classes/65k
for f in citations.G.shrink.new.T1.G2.npz
do
for i in 0 1 2 3 4 5 6 7 8 9
do
out=random_classes/65k/$i
sbatch -t 59 --mem=100000 -p express -e $out.err shrink_by_random_assignments.py -G $f -o $out -n 65000
done
done

cd /work/k.church/semantic_scholar/citations/graphs
mkdir -p random_classes/65k
for f in citations.G.shrink.new.T1.G2.npz
do
for i in 0 1 2 3 4 5 6 7 8 9
do
out=random_classes/65k/1$i
sbatch -t 59 --mem=100000 -p express -e $out.err shrink_by_random_assignments.py -G $f -o $out -n 65000
done
done



cd /work/k.church/semantic_scholar/citations/graphs/random_classes/65k
for K in 10 100 768
do
for f in [6-9].npz # [0-5].npz
do
err=$f.prone.K$K.err
rm $err
sbatch -p short -t 1209 -e $err  --mem=100000 $HOME/final/morphology/dict_to_embedding.py -G $f -o $f.K$K -K $K
done
done



cd /work/k.church/semantic_scholar/citations/graphs/random_classes
for K in 10
do
for f in 0.npz 4.npz 5.npz
do
err=$f.prone.K$K.err
rm $err
sbatch -p short -t 1209 -e $err  --mem=100000 $HOME/final/morphology/dict_to_embedding.py -G $f -o $f.K$K -K $K
done
done

cd /work/k.church/semantic_scholar/citations/graphs/random_classes
for K in 100 768
do
for f in 0.npz 4.npz 5.npz
do
err=$f.prone.K$K.err
rm $err
sbatch -p short -t 1209 -e $err  --mem=500000 $HOME/final/morphology/dict_to_embedding.py -G $f -o $f.K$K -K $K
done
done




cd /work/k.church/semantic_scholar/citations/graphs/random_classes
for K in 10
do
for f in 9.npz
do
err=$f.prone.K$K.err
rm $err
sbatch -p express -t 59 -e $err  --mem=100000 $HOME/final/morphology/dict_to_embedding.py -G $f -o $f.K$K -K $K
done
done



cd /work/k.church/semantic_scholar/citations/graphs
for f in citations.piece.???.gz.graph
do
sbatch -p short -t 1209 -e $f.prone.err --mem=1000000 $HOME/final/morphology/dict_to_embedding.py -G $f -o $f.2 -K 768
done

cd /work/k.church/semantic_scholar/citations/graphs
for f in citations.piece.?2?.gz.graph.G.npz
do
sbatch -p short -t 1209 -e $f.prone.err --mem=200000 $HOME/final/morphology/dict_to_embedding.py -G $f -o $f.3 -K 768
done

cd /work/k.church/semantic_scholar/citations/graphs
for f in citations.G.npz
do
sbatch -p short -t 1209 -e $f.prone.err --mem=500000 $HOME/final/morphology/dict_to_embedding.py -G $f -o $f.3 -K 768
done


cd /work/k.church/semantic_scholar/citations/graphs
for f in citations.piece.???.gz.graph
do
sbatch -p short -e $f.err3 -o $f.nodes nodes.sh $f
done

cd /work/k.church/semantic_scholar/citations/graphs
for f in citations.piece.???.gz.graph
do
sbatch -p short -e $f.err4 -o $f.node_freq node_freq.sh $f
done

sbatch -p short /work/k.church/semantic_scholar/citations/graphs/summarize_node_freq.sh



cd /work/k.church/semantic_scholar/citations/graphs
for f in citations.piece.???.gz.graph.node_freq
do
sbatch -p short -e $f.err4 -o $f.s /work/k.church/semantic_scholar/citations/graphs/sort_node_freq.sh $f
done

https://stackoverflow.com/questions/11173019/determining-the-byte-size-of-a-scipy-sparse-matrix
print(M.data.nbytes + M.indptr.nbytes + M.indices.nbytes)

cd /work/k.church/semantic_scholar/citations/graphs
for f in citations.piece.???.gz.graph
do
sbatch -p short -e $f.G.err graph2matrix.py -i $f -o $f.G
done

sbatch -p short /work/k.church/semantic_scholar/citations/graphs/sort_node_freq_job2.sh

split -l 5000000 citations.piece.011.gz.graph split.011/x
for f in split.011/x??
do
sbatch -p short -e $f.G.err graph2matrix.py -i $f -o $f.G
done

cd /work/k.church/semantic_scholar/citations/graphs/
sbatch -p short -e citations.G.err sum_matrices.py citations.G citations.piece.???.gz.graph.G.npz

CC = scipy.sparse.csgraph.connected_components(M)
>>> CC[0]
236038475
>>> CC[1][0:10]
array([0, 1, 2, 3, 4, 2, 2, 2, 2, 2], dtype=int32)
>>> f = np.bincount(CC[1])
>>> len(f)
236038475
>>> f[0:10]
array([       1,        1, 61490403,        1,        1,        1,
              1,        1,        1,        1])
>>> ff = np.bincount(f)
>>> ff[0:10]
array([        0, 234131978,   1536176,    259010,     69000,     23593,
            9476,      4304,      2142,      1143])
>>> len(ff)
61490404
>>> ff[0:100]
array([        0, 234131978,   1536176,    259010,     69000,     23593,
            9476,      4304,      2142,      1143,       643,       358,
             216,       152,        65,        67,        35,        31,
              21,        17,        11,        11,         2,         4,
               1,         3,         2,         1,         3,         1,
               0,         0,         3,         0,         0,         0,
               0,         1,         0,         0,         1,         0,
               0,         1,         0,         0,         0,         0,
               0,         0,         0,         0,         0,         0,
               0,         0,         1,         0,         0,         0,
               0,         0,         0,         0,         0,         0,
               0,         0,         0,         0,         0,         0,
               0,         0,         0,         0,         1,         0,
               0,         0,         0,         0,         0,         0,
               0,         0,         0,         0,         0,         0,
               0,         0,         0,         0,         0,         0,
               0,         0,         0,         0])
>>> res = [ (i, x) for i,x in enumerate(ff) if x > 0]
>>> len(res)
36
>>> res
[(1, 234131978), (2, 1536176), (3, 259010), (4, 69000), (5, 23593), (6, 9476), (7, 4304), (8, 2142), (9, 1143), (10, 643), (11, 358), (12, 216), (13, 152), (14, 65), (15, 67), (16, 35), (17, 31), (18, 21), (19, 17), (20, 11), (21, 11), (22, 2), (23, 4), (24, 1), (25, 3), (26, 2), (27, 1), (28, 3), (29, 1), (32, 3), (37, 1), (40, 1), (43, 1), (56, 1), (76, 1), (61490403, 1)]
>>> len(f) - 234131978
1906497


cd /work/k.church/semantic_scholar/citations/graphs/
sbatch -p short -e bak/citations.CC.err -o bak/citations.CC.out connected_components.py -G bak/citations.G.npz -C bak/citations.CC

cd /work/k.church/semantic_scholar/citations/graphs/
sbatch -p short -e citations.CC.err -o citations.CC.out connected_components.py -G citations.G.npz -C citations.CC

cd /work/k.church/semantic_scholar/citations/graphs/random_classes/65k/annoys
for f in ../*K100*w2v
do
b=`basename $f`
echo $b
awk 'NR == 1 {print; next}; {print "C" $0}' $f > $b
/work/k.church/semantic_scholar/jobs/glove2annoy.py $b
done

cd /work/k.church/semantic_scholar/citations/graphs/random_classes
mkdir annoys

cd /work/k.church/semantic_scholar/citations/graphs/random_classes/annoys
for f in ../*K100*w2v
do
b=`basename $f`
echo $b
awk 'NR == 1 {print; next}; {print "C" $0}' $f > $b
sbatch -p express /work/k.church/semantic_scholar/jobs/glove2annoy.py $b
done

cd /work/k.church/semantic_scholar/citations/graphs/random_classes/65k
for f in 10.npz # 1[0-9].npz
do
out=`basename $f .npz`.paths
err=$out.err
# sbatch --mem=500000 -e $err -t 59 -p express shortest_paths.py -G $f -o $out
sbatch --mem=500000 -e $err -t 1209 -p short shortest_paths.py -G $f -o $out
done

cd /work/k.church/semantic_scholar/citations/graphs/random_classes/65k
for f in 10.npz # 1[0-9].npz
do
out=`basename $f .npz`.laplacian
err=$out.err
rm $err
# sbatch --mem=500000 -e $err -t 59 -p express shortest_paths.py -G $f -o $out
sbatch --mem=500000 -e $err -t 1209 -p short laplacian.py -G $f -o $out
done



cd /work/k.church/semantic_scholar/citations/graphs/random_classes/65k/annoys
for f in ../*K768*w2v
do
b=`basename $f`
echo $b
awk 'NR == 1 {print; next}; {print "C" $0}' $f > $b
sbatch -p express /work/k.church/semantic_scholar/jobs/glove2annoy.py $b
done

cd /work/k.church/semantic_scholar/citations/graphs/random_classes/with_symmetry
for f in *.w2v
do
mkdir -p annoys
b=annoys/`basename $f`
echo $b
awk 'NR == 1 {print; next}; {print "C" $0}' $f > $b
sbatch -p express /work/k.church/semantic_scholar/jobs/glove2annoy.py $b
done


cd /work/k.church/semantic_scholar/citations/graphs/random_classes/65k/annoys
f=0.npz.K768.ProNE.K768.T20.O5.w2v
head $f | awk 'NR > 1' | /work/k.church/semantic_scholar/embeddings/M3/semantic_scholar_embedding_near.py -M $f.annoy
C0	C0:1.000	C10385:0.855	C5177:0.848	C58344:0.847	C34167:0.846	C27476:0.845	C25310:0.844	C35447:0.844	C43207:0.843	C36840:0.842
C1	C1:1.000	C10385:0.849	C20828:0.842	C30707:0.840	C960:0.840	C5177:0.840	C15929:0.840	C4197:0.837	C38397:0.837	C39954:0.837
C2	C2:1.000	C10385:0.859	C5177:0.846	C58679:0.846	C47678:0.844	C25873:0.843	C7184:0.843	C11080:0.842	C50521:0.842	C7773:0.842
C3	C3:1.000	C10385:0.860	C31336:0.853	C39850:0.851	C5177:0.849	C41686:0.849	C27558:0.848	C60433:0.848	C43622:0.848	C8977:0.848
C4	C4:1.000	C10385:0.862	C31336:0.851	C25064:0.848	C58728:0.845	C60450:0.845	C5177:0.845	C19887:0.844	C9561:0.843	C40283:0.843
C5	C5:1.000	C10385:0.871	C20628:0.856	C38880:0.854	C64107:0.854	C22228:0.853	C50455:0.852	C58448:0.852	C7184:0.851	C31336:0.851
C6	C6:1.000	C10385:0.856	C58268:0.850	C61860:0.847	C27764:0.846	C21790:0.845	C22228:0.845	C56409:0.844	C43622:0.844	C9561:0.844
C7	C7:1.000	C10385:0.849	C31336:0.840	C50706:0.838	C60416:0.838	C4197:0.836	C47678:0.836	C10709:0.836	C58785:0.836	C30400:0.836
C8	C8:1.000	C10385:0.860	C61269:0.848	C30453:0.848	C18329:0.844	C21931:0.844	C9561:0.844	C54536:0.843	C17541:0.843	C2138:0.842

head $f | awk 'NR > 1' | /work/k.church/semantic_scholar/embeddings/M3/semantic_scholar_embedding_near.py -M $f.annoy -N 500 | awk '{print $NF}'
C729:0.832
C42101:0.825
C18359:0.831
C16012:0.837
C22789:0.833
C37298:0.840
C51447:0.833
C35539:0.825
C17314:0.832


echo 9558665 | python /work/k.church/semantic_scholar/src/get_citations.py --verbose 0
echo 9558665 | python /work/k.church/semantic_scholar/src/get_citations.py

echo 9558665 | python /work/k.church/semantic_scholar/src/get_citations.py
9558665	https://www.semanticscholar.org/paper/9e2caa39ac534744a180972a30a320ad0ae41ea3	Word Association Norms, Mutual Information and Lexicography	2244184|145165877
--> 143660780 143660780	https://www.semanticscholar.org/paper/73ca80f87a5509a0f4cc62471b8f088f66facd0b	Frequency Analysis of English Usage: Lexicon and Grammar. By W. Nelson Francis and Henry Kučera with the assistance of Andrew W. Mackie. Boston: Houghton Mifflin. 1982. x + 561	11148235
--> 18476529 18476529	https://www.semanticscholar.org/paper/7ed08f16949c4cac198c87263318df655b7b250a	Book Reviews: Looking Up: An Account of the COBUILD PROJECT IN LEXICAL COMPUTING	1713574
--> 208978352 208978352	https://www.semanticscholar.org/paper/02bf83a75fc28d591392b7d0af831acb44393387	Parsing	145223398
--> 11945361 11945361	https://www.semanticscholar.org/paper/b2986b25f50babd536dd0ecf2237d9eabf5843c2	THE POPULATION FREQUENCIES OF SPECIES AND THE ESTIMATION OF POPULATION PARAMETERS	145179124
--> 146932957 146932957	https://www.semanticscholar.org/paper/91556a9ef3b88c8a3252a9d39630734047ee0c43	Word association norms	2274425|48566306
--> 60997669 60997669	https://www.semanticscholar.org/paper/e3a81d9b7464c0097e391c31fb4d84467533091c	Collins COBUILD English Language Dictionary	33190254
--> 3166885 063227786
--> 17307726 68|13109850
--> 112676098 112676098	https://www.semanticscholar.org/paper/cfd7ad5e1f48d938c65f807c795e91457210212f	Transmission of Information.	32549147|30021514
--> 5222302 5222302	https://www.semanticscholar.org/paper/d886320cc431c57f12027584fdce3434bf4caf9e	Deterministic Parsing of Syntactic Non-fluencies	21169546
--> 67110416 67110416	https://www.semanticscholar.org/paper/6b6dc1167d8f6f44f230309e9c27e4268578f1f7	Transmission of Information: A Statistical Theory of Communications.	50381258|30021514
--> 5923203 5923203	https://www.semanticscholar.org/paper/ff97fa9912e4e0517208bf45ef0f646f995e09d2	Parsing, Word Associations and Typical Predicate-Argument Relations	2244184|34938639|145165877|21169546
--> 61409897 61409897	https://www.semanticscholar.org/paper/c8306568c3d5f0c2257a1a735cbf10783d764731	Microcoding the Lexicon with Co-occurrence Knowledge	1801071
--> 60002365 662|3165918
--> 2567466 2567466	https://www.semanticscholar.org/paper/90535569dcd67a0851e35dc102ba18095c3af180	The nature of evidence.	3922811
--> 62735070 62735070	https://www.semanticscholar.org/paper/2638cf0a161f4510f1c16d8a76b4361faf5b4718	A Stochastic Parts Program and Noun Phrase Parser for Unrestricted Text	2244184
--> 208093066 NA

echo 9558665 | python /work/k.church/semantic_scholar/src/get_citations.py --verbose 0
9558665	143660780|18476529|208978352|11945361|146932957|60997669|3166885|17307726|112676098|5222302|67110416|5923203|61409897|60002365|2567466|62735070|208093066


sbatch -t 1209 --mem=500000 -p short /work/k.church/semantic_scholar/citations/graphs/cocitations.py


# These matrices become all True
cd /work/k.church/semantic_scholar/citations/graphs/random_classes/65k/
sbatch -e 0.2.dense.err -t 1209 --mem=500000 -p short co-citation.py -G 0.npz -o 0.2.dense -d
# sbatch -e 0.2.sparse.err -t 1209 --mem=500000 -p short co-citation.py -G 0.npz -o 0.2.sparse

cd /work/k.church/semantic_scholar/citations/graphs/random_classes
python
import numpy as np
import scipy.sparse
f='6.npz'
M=scipy.sparse.load_npz(f)
M.shape

cd /work/k.church/semantic_scholar/citations/graphs
python
import numpy as np
import scipy.sparse
f='citations.G.shrink.new.T1.G2.npz'
M=scipy.sparse.load_npz(f)
M.shape

PMI=9558665
_,PMI_fanout = M[PMI,:].nonzero()
PMI_fanout
# array([143660780,  18476529, 208978352,  11945361, 146932957,  60997669,
#          3166885,  17307726, 112676098,   5222302,  67110416,   5923203,
#         61409897,  60002365,   2567466,  62735070, 208093066], dtype=int32)
PMI_fanin,_ = M[:,PMI].nonzero()

near = M[np.array([PMI] + list(PMI_fanout[0:5])),:]
import sklearn.metrics.pairwise
sklearn.metrics.pairwise.cosine_similarity(near)

echo 9558665, 143660780, 18476529, 208978352, 11945361, 146932957 | tr , '\n' | /work/k.church/semantic_scholar/papers/find_lines.py /work/k.church/semantic_scholar/papers/papers2url/papers.piece.???.txt
9558665	https://www.semanticscholar.org/paper/9e2caa39ac534744a180972a30a320ad0ae41ea3	Word Association Norms, Mutual Information and Lexicography	2244184|145165877
143660780	https://www.semanticscholar.org/paper/73ca80f87a5509a0f4cc62471b8f088f66facd0b	Frequency Analysis of English Usage: Lexicon and Grammar. By W. Nelson Francis and Henry Kučera with the assistance of Andrew W. Mackie. Boston: Houghton Mifflin. 1982. x + 561	11148235
18476529	https://www.semanticscholar.org/paper/7ed08f16949c4cac198c87263318df655b7b250a	Book Reviews: Looking Up: An Account of the COBUILD PROJECT IN LEXICAL COMPUTING	1713574
208978352	https://www.semanticscholar.org/paper/02bf83a75fc28d591392b7d0af831acb44393387	Parsing	145223398
11945361	https://www.semanticscholar.org/paper/b2986b25f50babd536dd0ecf2237d9eabf5843c2	THE POPULATION FREQUENCIES OF SPECIES AND THE ESTIMATION OF POPULATION PARAMETERS	145179124
146932957	https://www.semanticscholar.org/paper/91556a9ef3b88c8a3252a9d39630734047ee0c43	Word association norms	2274425|48566306

# papers that cite the papers above
near = scipy.sparse.csr_matrix(M[:,np.array([PMI] + list(PMI_fanout[0:5]))], dtype=int)
(near.T @ near).toarray()
array([[4362,   31,   14,    2,   46,  239],
       [  31, 1273,    5,    2,   11,    3],
       [  14,    5,  139,    1,    5,    4],
       [   2,    2,    1,  159,    3,    2],
       [  46,   11,    5,    3, 3257,    5],
       [ 239,    3,    4,    2,    5,  512]])

np.set_printoptions(linewidth=200, precision=4)
np.round(sklearn.metrics.pairwise.cosine_similarity(near.T),3)
array([[1.    , 0.0132, 0.018 , 0.0024, 0.0122, 0.1599],
       [0.0132, 1.    , 0.0119, 0.0044, 0.0054, 0.0037],
       [0.018 , 0.0119, 1.    , 0.0067, 0.0074, 0.015 ],
       [0.0024, 0.0044, 0.0067, 1.    , 0.0042, 0.007 ],
       [0.0122, 0.0054, 0.0074, 0.0042, 1.    , 0.0039],
       [0.1599, 0.0037, 0.015 , 0.007 , 0.0039, 1.    ]])

s0 = np.sum(near, axis=0)
>>> s1 = np.sum(near, axis=1)
>>> s0
matrix([[4362, 1273,  139,  159, 3257,  512]])
>>> len(s1)
300000000

fan0 = np.sum(M, axis=0)
fan1 = np.sum(M, axis=1)

cd /work/k.church/semantic_scholar/citations/graphs
for G in citations.G.npz
do
for axis in 0 1
do
out=$G.fanin.axis$axis
sbatch -e $out.err -p debug fanin.py -G $G -X $axis -o $out
done
done


cd /work/k.church/semantic_scholar/citations/graphs
for G in citations.G.npz
do
for K in 20 # 50 # 100
do
out=$G.sketch.K$K
sbatch -e $out.err -p short -t  1209 sketchify.py -G $G -o $out

out=$out.transpose
sbatch -e $out.err -p short -t  1209 sketchify.py -G $G -o $out -T
done
done


cd /work/k.church/semantic_scholar/citations/graphs
for G in citations.G.npz
do
for K in 101 #  100 300 # 252 # 251 # 200 # 97 # 98 # 99 # 100 # 101 # 100 # 250 0 # 400 # 200 # 300 # 200 # 100 # 20 # 50 # 100
do
out=freq_rows/`basename $G .npz`/K$K/no_transpose/piece
mkdir -p `dirname $out`
rm $out.err
# sbatch -e $out.err -p short --mem=500000 -t  1209 freq_rows.py -G $G -o $out -K $K
# sbatch -e $out.err -p express --mem=100000 -t 59 freq_rows.py -G $G -o $out -K $K
sbatch -e $out.err -p short -t  1209 freq_rows.py -G $G -o $out -K $K

out=freq_rows/`basename $G .npz`/K$K/transpose/piece
mkdir -p `dirname $out`
rm $out.err
rm $out.err
# sbatch -e $out.err -p express --mem=100000 -t 59 freq_rows.py -G $G -o $out -K $K -T
# sbatch -e $out.err -p short --mem=500000 -t  1209 freq_rows.py -G $G -o $out -K $K -T
sbatch -e $out.err -p short -t  1209 freq_rows.py -G $G -o $out -K $K -T
# sbatch -e $out.err -p express -t  59 freq_rows.py -G $G -o $out -K $K -T
done
done

cd /work/k.church/semantic_scholar/citations/graphs
for G in citations.G.npz
do
out=citations.G.txt
sbatch -e $out.err -p short -t 1209 src/graph_tofile.py -G $G -o $out
done

cd /work/k.church/semantic_scholar/citations/graphs
for G in citations.G.npz
do
out=citations.G.transposed.txt
sbatch -e $out.err -p short -t 1209 src/graph_tofile.py -G $G -o $out -T
done

cd /work/k.church/semantic_scholar/citations/graphs
for G in citations.G.npz
do
out=citations.G
sbatch -e $out.err -p short -t 1209 src/graph_tofile.py -G $G -o $out
done

cd /work/k.church/semantic_scholar/citations/graphs
for G in citations.G.npz
do
out=citations.G.transposed
sbatch -e $out.err -p short -t 1209 src/graph_tofile.py -G $G -o $out -T
done

cd /work/k.church/semantic_scholar/citations/graphs
for G in citations.G.npz
do
out=citations.G.binary.txt
sbatch -e $out.err -p express -t 59 src/graph_tofile.py -G $G -o $out
done



cd /work/k.church/semantic_scholar/citations/graphs
for G in citations.G.binary
do
out=citations.G.from_binary
sbatch -e $out.err -p express -t 59 src/graph_fromfile.py -i $G -o $out -N 300000000
done



cd /work/k.church/semantic_scholar/citations/graphs
out=permutation
sbatch -e $out.err -p express -t 59 permutation_to_file.py -o $out -N 300000000


cd /work/k.church/semantic_scholar/citations/graphs
for G in citations.G.npz
do
for K in 99
do
dir=freq_rows/`basename $G .npz`/K$K/no_transpose
for piece in $dir/piece.pieces[0-9][0-9][0-9].npz # $dir/piece.[0-9][0-9][0-9].npz
do
echo working on $piece
# /work/k.church/semantic_scholar/citations/graphs/freq_rows/citations.G/K99/no_transpose/
sbatch -o $piece.out -e $piece.err -p express freq_row_piece.py -P $piece 
done
done
done

for piece in /work/k.church/semantic_scholar/citations/graphs/freq_rows/citations.G/*/transpose/piece.pieces???.npz
do
sbatch -o $piece.out -e $piece.err -t 59 -p express freq_row_piece.py -P $piece 
done


# put all the pieces back together again
cd /work/k.church/semantic_scholar/citations/graphs
for G in citations.G.npz
do
for K in 99
do
dir=freq_rows/`basename $G .npz`/K$K/no_transpose
echo $dir
sbatch -e $dir/big_sum.err -t 1209 -p short --mem=50000 sum_matrices.py $dir/big_sum $dir/piece.pieces???.sketch.npz $dir/piece.small.npz 
done
done

cd /work/k.church/semantic_scholar/citations/graphs/freq_rows/citations.G/C/pieces.200/sketch_threshold.100/citations.G.transposed
src=/work/k.church/semantic_scholar/citations/graphs/src
sbatch -e big_sum.err -t 1209 -p short --mem=50000 $src/sum_matrices.py big_sum sketch.???.big.trim.sketch.trim.npz sketch.small.trim.npz


cd /work/k.church/semantic_scholar/citations/graphs
for G in citations.G.npz
do
for K in 99
do
dir=freq_rows/`basename $G .npz`/K$K/transpose
for piece in $dir/piece.pieces[0-9][0-9][0-9].npz # $dir/piece.[0-9][0-9][0-9].npz
do
echo working on $piece
sbatch -o $piece.out -e $piece.err -p short -t 1209 freq_row_piece.py -P $piece -R 3
done
done
done






idx = np.arange(10)
P = np.random.permutation(10)
Pinv = np.zeros(10)
Pinv[P] = idx
idx[P][Pinv]
array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])

cd /work/k.church/semantic_scholar/citations/graphs
for f in citations.G.shrink.new.T1.G2.npz
do
sbatch -p short --cpus-per-task=56 -t 1209 -e $f.prone.tasks.56.err --mem=1000000 $HOME/final/morphology/dict_to_embedding.py -G $f -o $f.tasks.56 -K 768
done

cd /work/k.church/semantic_scholar/citations/graphs
for f in citations.G.shrink.new.T1.G2.npz
do
for K in 10 # 50 100 768
do
sbatch -p short --cpus-per-task=56 -t 1209 -e $f.prone.tasks.56.K$K.err --mem=500000 $HOME/final/morphology/dict_to_embedding.py -G $f -o $f.tasks.56.K$K -K $K
done
done


# first run of long queue
cd /work/k.church/semantic_scholar/citations/graphs
for f in citations.G.shrink.new.T1.G2.npz
do
for K in 100 768 # 20 50 # 10 # 50 100 768
do
sbatch -p long -t 6045 -e $f.prone.tasks.1.K$K.err --mem=500000 $HOME/final/morphology/dict_to_embedding.py -G $f -o $f.tasks.1.K$K -K $K
done
done

cd /work/k.church/semantic_scholar/citations/graphs
for f in citations.G.shrink.new.T1.G2.npz
do
for K in 250 290 #  300 # 300 # 200 768 # 100  20 50 # 10 # 50 100 768
do
sbatch -p long -t 7199 -e $f.prone.tasks.1.K$K.err --mem=1500000 $HOME/final/morphology/dict_to_embedding.py -G $f -o $f.tasks.1.K$K -K $K
done
done



cd /work/k.church/semantic_scholar/citations/graphs
for tasks in 56
do
for f in citations.G.shrink.new.T1.G2.npz
do
for K in 10 50 100 768
do
out=$f.K$K.transpose.tasks$tasks
sbatch -p short --cpus-per-task=56 -t 1209 -e $out.err --mem=500000 $HOME/final/morphology/dict_to_embedding.py -G $f -o $out -K $K --transpose
done
done
done



cd /work/k.church/semantic_scholar/citations/graphs
ls -lt citations.G.npz
python
import numpy as np
import scipy.sparse
f='citations.G.npz'
M=scipy.sparse.load_npz(f)
M.shape
fan0 = np.sum(M, axis=0)
fan1 = np.sum(M, axis=1)
fan0 = np.array(fan0).reshape(-1)
fan1 = np.array(fan1).reshape(-1)

cd /work/k.church/semantic_scholar/citations/graphs
python
import numpy as np
import scipy.sparse
f='citations.G.npz.freq_rows.K100.transpose.npz'
M=scipy.sparse.load_npz(f)
M.shape
# fan0 = np.sum(M, axis=0)
# fan1 = np.sum(M, axis=1)
# fan0 = np.array(fan0).reshape(-1)
# fan1 = np.array(fan1).reshape(-1)

N=M.shape[1]
idx = np.arange(N, dtype=np.int32)
P = np.random.permutation(N)
Pinv = np.zeros(N, dtype=np.int32)
Pinv[P] = idx
# This makes the matrix dense
# M2 = M[:,P]
# M3 = M2[:,Pinv]


# working on this on 06/28/2022
cd /work/k.church/semantic_scholar/citations/graphs
K=1000
pieces=100
in=citations.G.transposed
out=freq_rows/C/threshold.$K/pieces.$pieces/$in
mkdir -p `dirname $out`
sbatch -e $out.err -p short,express -t 59 ./src/C/freq_row.sh $in $out $pieces $K

cd /work/k.church/semantic_scholar/citations/graphs
K=1000
pieces=100
in=citations.G.transposed
dir=freq_rows/C/threshold.$K/pieces.$pieces
for piece in $dir/$in.???.big.X
do
p=`dirname $piece`/`basename $piece .X`
out=$p.trim
echo $p
sbatch -e $out.err -p short,express -t 59 ./src/C/freq_row_piece.sh $p $out permutation.P $K
done



cd /work/k.church/semantic_scholar/citations/graphs
K=1000
pieces=100
in=citations.G.transposed
dir=freq_rows/C/threshold.$K/pieces.$pieces

p=$dir/citations.G.transposed.small
# sbatch -e $p.err -p short -t 1209 ./src/graph_fromfile.py -i $p -o $p  -N 300000000


for piece in $dir/$in.???.big.X
do
p=`dirname $piece`/`basename $piece .X`.trim.sketch
# echo $p
sbatch -e $out.err -p debug,express,short -t 19 ./src/graph_fromfile.py -i $p -o $p  -N 300000000
done

sbatch -e freq_rows/C/threshold.$K/pieces.$pieces/big_sum.err -t 1209 -p short --mem=100000 src/sum_matrices.py freq_rows/C/threshold.$K/pieces.$pieces/big_sum freq_rows/C/threshold.$K/pieces.$pieces/$in.???.big.trim.sketch.npz freq_rows/C/threshold.$K/pieces.$pieces/$in.small.npz

# sbatch -e freq_rows/C/threshold.$K/pieces.$pieces/big_sum2.err -t 1209 -p short --mem=100000 src/sum_matrices.py freq_rows/C/threshold.$K/pieces.$pieces/big_sum2 freq_rows/C/threshold.$K/pieces.$pieces/$in.???.big.trim.sketch.npz freq_rows/C/threshold.$K/pieces.$pieces/$in.small.npz

cd /work/k.church/semantic_scholar/citations/graphs
sbatch -t 1209 --mem=500000 -p short src/cocitations.py -G freq_rows/C/threshold.$K/pieces.$pieces/big_sum.npz  -o freq_rows/C/threshold.$K/pieces.$pieces/cocitations.int.npz 


cd /work/k.church/semantic_scholar/citations/graphs
for G in freq_rows/C/threshold.$K/pieces.$pieces/big_sum
do
for axis in 0 1
do
out=$G.fanin.axis$axis
sbatch -e $out.err -p short src/fanin.py -G $G.npz -X $axis -o $out
done
done

cd /work/k.church/semantic_scholar/citations/graphs
for G in citations.G
do
for axis in 0 1
do
out=$G.fanin.axis$axis
sbatch -e $out.err -p short src/fanin.py -G $G.npz -X $axis -o $out -T
done
done

python
import numpy as np
import scipy.sparse
F=[np.load(x).reshape(-1) for x in ['citations.G.fanin.axis0.npy', 'citations.G.fanin.axis1.npy']]
f=[np.load(x).reshape(-1) for x in ['freq_rows/C/threshold.1000/pieces.100/big_sum.fanin.axis0.npy', 'freq_rows/C/threshold.1000/pieces.100/big_sum.fanin.axis1.npy']]

cd /work/k.church/semantic_scholar/citations/graphs
pieces=100
K=100
out=freq_rows/C/threshold.$K/pieces.$pieces/cocitations
sbatch -e $out.err -o $out.out -t 1209 --mem=500000 -p short src/cocitations.py -G freq_rows/C/threshold.$K/pieces.$pieces/big_sum.npz  -o $out.npz 
# end of 06/28/2022


# working on this on 06/28/2022 (2nd copy)
cd /work/k.church/semantic_scholar/citations/graphs
K=100
pieces=100
in=citations.G.transposed
out=freq_rows/C/threshold.$K/pieces.$pieces/$in
mkdir -p `dirname $out`
sbatch -e $out.err -p short,express -t 59 ./src/C/freq_row.sh $in $out $pieces $K

cd /work/k.church/semantic_scholar/citations/graphs
K=100
pieces=100
in=citations.G.transposed
dir=freq_rows/C/threshold.$K/pieces.$pieces
for piece in $dir/$in.???.big.X
do
p=`dirname $piece`/`basename $piece .X`
out=$p.trim
echo $p
sbatch -e $out.err -p short,express -t 59 ./src/C/freq_row_piece.sh $p $out permutation.P $K
done



cd /work/k.church/semantic_scholar/citations/graphs
K=100
pieces=100
in=citations.G.transposed
dir=freq_rows/C/threshold.$K/pieces.$pieces

p=$dir/citations.G.transposed.small
sbatch -e $p.err -p short -t 1209 ./src/graph_fromfile.py -i $p -o $p  -N 300000000


for piece in $dir/$in.???.big.X
do
p=`dirname $piece`/`basename $piece .X`.trim.sketch
# echo $p
sbatch -e $out.err -p debug,express,short -t 19 ./src/graph_fromfile.py -i $p -o $p  -N 300000000
done

sbatch -e freq_rows/C/threshold.$K/pieces.$pieces/big_sum.err -t 1209 -p short --mem=100000 src/sum_matrices.py freq_rows/C/threshold.$K/pieces.$pieces/big_sum freq_rows/C/threshold.$K/pieces.$pieces/$in.???.big.trim.sketch.npz freq_rows/C/threshold.$K/pieces.$pieces/$in.small.npz

# sbatch -e freq_rows/C/threshold.$K/pieces.$pieces/big_sum2.err -t 1209 -p short --mem=100000 src/sum_matrices.py freq_rows/C/threshold.$K/pieces.$pieces/big_sum2 freq_rows/C/threshold.$K/pieces.$pieces/$in.???.big.trim.sketch.npz freq_rows/C/threshold.$K/pieces.$pieces/$in.small.npz

sbatch -t 1209 --mem=500000 -p short src/cocitations.py -G freq_rows/C/threshold.$K/pieces.$pieces/big_sum.npz  -o freq_rows/C/threshold.$K/pieces.$pieces/cocitations.npz 


cd /work/k.church/semantic_scholar/citations/graphs
for G in freq_rows/C/threshold.$K/pieces.$pieces/big_sum
do
for axis in 0 1
do
out=$G.fanin.axis$axis
sbatch -e $out.err -p short src/fanin.py -G $G.npz -X $axis -o $out
done
done

cd /work/k.church/semantic_scholar/citations/graphs
for G in citations.G
do
for axis in 0 1
do
out=$G.fanin.axis$axis
sbatch -e $out.err -p short src/fanin.py -G $G.npz -X $axis -o $out -T
done
done

python
import numpy as np
import scipy.sparse
F=[np.load(x).reshape(-1) for x in ['citations.G.fanin.axis0.npy', 'citations.G.fanin.axis1.npy']]
f=[np.load(x).reshape(-1) for x in ['freq_rows/C/threshold.1000/pieces.100/big_sum.fanin.axis0.npy', 'freq_rows/C/threshold.1000/pieces.100/big_sum.fanin.axis1.npy']]


# end of 06/28/2022 (2nd copy) 



cd /work/k.church/semantic_scholar/citations/graphs
src/C/freq_row_piece freq_rows/citations.G/C/T300/K200/citations.G.binary.sketch.193.big  freq_rows/citations.G/C/T300/K200/citations.G.binary.sketch.193.big.sketch per*.P 300

cd /work/k.church/semantic_scholar/citations/graphs
for pieces in 200
do
for sketch_threshold in 1000 # 100
do
in=citations.G.transposed
out=freq_rows/citations.G/C/pieces.$pieces/sketch_threshold.$sketch_threshold/$in/sketch
mkdir -p `dirname $out`
sbatch -e $out.err -p short -t 1209 ./src/C/freq_row.sh $in $out $sketch_threshold $pieces
# ./src/C/freq_row.sh $in $out $sketch_threshold $pieces
done
done

cd /work/k.church/semantic_scholar/citations/graphs
piece=/work/k.church/semantic_scholar/citations/graphs/freq_rows/citations.G/C/pieces.200/sketch_threshold.100/citations.G.transposed/sketch.008.big
in=$piece
out=$piece.trim
./src/C/freq_row_piece $in $out permutation.P 100


cd /work/k.church/semantic_scholar/citations/graphs
# /work/k.church/semantic_scholar/citations/graphs/freq_rows/citations.G/C/pieces.200/sketch_threshold.100/citations.G.transposed/sketch.000.big.X #
for piece in /work/k.church/semantic_scholar/citations/graphs/freq_rows/citations.G/C/pieces.200/sketch_threshold.100/citations.G.transposed/sketch.???.big.X
do
in=`dirname $piece`/`basename $piece .X`
out=$in.trim
if [ ! -s $out.sketch.X ]
then
echo working on $out
sbatch -e $out.err -p express -t 59 ./src/C/freq_row_piece.sh $in $out permutation.P 100
fi
done

cd /work/k.church/semantic_scholar/citations/graphs
dir=/work/k.church/semantic_scholar/citations/graphs/freq_rows/citations.G/C/pieces.200/sketch_threshold.100/citations.G.transposed
for piece in $dir/sketch.small.X # $dir/sketch.???.big.trim.sketch.X
do
in=`dirname $piece`/`basename $piece .X`
out=$in.trim
sbatch -e $out.err -p express,short -t 59 ./src/graph_fromfile.py -i $in -o $out  -N 300000000
done

### 
cd /work/k.church/semantic_scholar/citations/graphs
# /work/k.church/semantic_scholar/citations/graphs/freq_rows/citations.G/C/pieces.200/sketch_threshold.100/citations.G.transposed/sketch.000.big.X #
threshold=1000
dir=/work/k.church/semantic_scholar/citations/graphs/freq_rows/citations.G/C/pieces.200/sketch_threshold.$threshold/citations.G.transposed

for piece in $dir/sketch.???.big.X
do
in=`dirname $piece`/`basename $piece .X`
out=$in.trim
if [ ! -s $out.sketch.X ]
then
echo working on $out
sbatch -e $out.err -p express,short,debug -t 19 ./src/C/freq_row_piece.sh $in $out permutation.P $threshold
fi
done

###

cd /work/k.church/semantic_scholar/citations/graphs
dir=/work/k.church/semantic_scholar/citations/graphs/freq_rows/citations.G/C/pieces.200/sketch_threshold.100/citations.G.transposed
for piece in $dir/sketch.small.X # $dir/sketch.???.big.trim.sketch.X
do
in=`dirname $piece`/`basename $piece .X`
out=$in.trim
sbatch -e $out.err -p express,short -t 59 ./src/graph_fromfile.py -i $in -o $out  -N 300000000
done





cd /work/k.church/semantic_scholar/citations/graphs
./src/C/check_order citations.G.binary.txt.[XY].int


# citations.G.shrink.new.T1.npz
cd /work/k.church/semantic_scholar/citations/graphs
for G in citations.G.shrink.new.T1.npz
do
for K in 32 # 64 # 128 # 256 768
do
# tasks=64
# tasks=56
tasks=16
# mode=SparseOTF
mode=FirstOrderUnweighted
out=`basename $G .npz`.K$K.tasks$tasks.emb
sbatch --cpus-per-task=$tasks -p short -t 1209 -n $tasks -o $out.out -e $out.err pecanpy --input $G --output $out.npz --mode $mode --extend --workers $tasks --verbose
done
done


cd /work/k.church/semantic_scholar/citations/graphs
for K in 64 # 128 # 256 768
do
# tasks=64
# tasks=56
tasks=16
# mode=SparseOTF
mode=FirstOrderUnweighted
out=citations.G.K$K.tasks$tasks.emb
sbatch --cpus-per-task=$tasks -p short -t 1209 -n $threads -o $out.out -e $out.err pecanpy --input citations.G.npz --output $out.npz --mode $mode --extend --workers $tasks --verbose
done


cd /work/k.church/semantic_scholar/citations/graphs
for K in 64 # 128 # 256 768
do
# tasks=64
tasks=56
# tasks=16
# mode=SparseOTF
in=citations.G.transposed
mode=SparseOTF
# mode=FirstOrderUnweighted
out=$in.K$K.tasks$tasks
sbatch -e $out.err --cpus-per-task=$tasks -p short -t 1209 -n $threads -o $out.out -e $out.err pecanpy --input $in.txt --output $out.npz --mode $mode --extend --workers $tasks --verbose
done


cd /work/k.church/semantic_scholar/citations/graphs
for K in 128 # 64 # 128 # 256 768
do
# tasks=64
tasks=56
# tasks=16
# mode=SparseOTF
in=citations.G.transposed
mode=SparseOTF
# mode=FirstOrderUnweighted
out=$in.K$K.tasks$tasks
sbatch -e $out.err --cpus-per-task=$tasks -p short -t 1209 -n $threads -o $out.out -e $out.err pecanpy --input $in.txt --output $out.npz --mode $mode --extend --workers $tasks --verbose
done

cd /work/k.church/semantic_scholar/citations/graphs
for K in 16 # 32 # 64 # 128 # 256 768
do
# tasks=64
# tasks=56
tasks=8
# mode=SparseOTF
# in=citations.G.transposed
in=citations.G.transpose
mode=SparseOTF
# mode=FirstOrderUnweighted
out=$in.K$K.tasks$tasks
# sbatch -e $out.err --cpus-per-task=$tasks -p short -t 1209 -n $tasks -o $out.out -e $out.err pecanpy --input $in.txt --output $out.npz --mode $mode --extend --workers $tasks --verbose
sbatch --mem=500000 -e $out.err -p short -t 1209 -n $tasks -o $out.out -e $out.err pecanpy --input $in.npz --output $out.npz --mode $mode --extend --workers $tasks --verbose
done

cd /work/k.church/semantic_scholar/citations/graphs
for K in 16 # 32 # 64 # 128 # 256 768
do
# tasks=64
# tasks=56
tasks=1
# mode=SparseOTF
# in=citations.G.transposed
in=citations.G.transpose
mode=SparseOTF
# mode=FirstOrderUnweighted
out=$in.K$K.tasks$tasks
# sbatch -e $out.err --cpus-per-task=$tasks -p short -t 1209 -n $tasks -o $out.out -e $out.err pecanpy --input $in.txt --output $out.npz --mode $mode --extend --workers $tasks --verbose
sbatch --mem=500000 -e $out.err -p short -t 1209 -n $tasks -o $out.out -e $out.err pecanpy --input $in.npz --output $out.npz --mode $mode --extend --workers $tasks --verbose
done




# cd /work/k.church/semantic_scholar/citations/graphs
# q=short
# out=threads.$q.txt
# err=threads.$q.err
# threads=4
# cores=4
# sbatch -o $out -e $err -p $q -n $threads -S $cores src/print_number_of_threads.py


cd /work/k.church/semantic_scholar/citations/graphs
q=short
for tasks in 16 # 56 64 128
do
out=threads.$q.tasks$tasks.txt
err=threads.$q.err
sbatch -o $out -e $err -p $q --cpus-per-task=$tasks src/print_number_of_threads.py
done


dir=/work/k.church/semantic_scholar/citations/graphs/freq_rows/citations.G/C/pieces.200/sketch_threshold.100/citations.G.transposed
python /work/k.church/semantic_scholar/citations/graphs/src/graph_fromfile.py -i $dir/sketch.000.big.trim.sketch -o $dir/sketch.000.big.trim.sketch  -N 300000000

import numpy as np
import scipy.sparse
# f='/work/k.church/semantic_scholar/citations/graphs/freq_rows/citations.G/C/pieces.200/sketch_threshold.100/citations.G.transposed/sketch.000.big.trim.sketch.npz'
f='/work/k.church/semantic_scholar/citations/graphs/freq_rows/C/threshold.1000/pieces.100/citations.G.transposed.073.big.trim.sketch.npz'
M=scipy.sparse.load_npz(f)

fan0 = np.array(np.sum(M, axis=0)).reshape(-1)
fan1 = np.array(np.sum(M, axis=1)).reshape(-1)
b0 = np.bincount(fan0)
b1 = np.bincount(fan1)


python
import numpy as np
import scipy.sparse
# f='demo/karate.int.npz'
big='/work/k.church/semantic_scholar/citations/graphs/freq_rows/C/threshold.100/pieces.100/big_sum.npz'
cocite='/work/k.church/semantic_scholar/citations/graphs/freq_rows/C/threshold.100/pieces.100/cocitations.npz'
# f='/work/k.church/semantic_scholar/citations/graphs/freq_rows/C/threshold.1000/pieces.100/citations.G.transposed.073.big.trim.sketch.npz'
f='freq_rows/C/threshold.1000/pieces.100/citations.G.transposed.small.npz'
Gbig=scipy.sparse.load_npz(big)
Gcocite=scipy.sparse.load_npz(cocite)
np.set_printoptions(linewidth=200)
print(M.todense())

NZ = [m.count_nonzero() for m in [G, M]]

cd /work/k.church/semantic_scholar/citations/graphs
for in in citations.G.transposed.[XY]
do
sbatch -p short -i $in -e $in.err -o $in.txt x_to_y.sh ia
done

sbatch -p short -o citations.G.transposed.txt -e citations.G.transposed.err src/paste.sh citations.G.transposed.[XY].txt

cd /work/k.church/semantic_scholar/citations/graphs
dir=/work/k.church/semantic_scholar/citations/graphs/freq_rows/C/threshold.100/pieces.100/
sbatch -o $dir/debug_cocitations.txt -e $dir/debug_cocitations.err -t 1209 --mem=500000 -p short src/debug_cocitations.py

PATH=$PATH:/work/k.church/semantic_scholar/citations/graphs/src/C/
XY_to_bigrams citations.G.transposed.000.big.trim.sketch.X 2>/dev/null | print_bigrams 2>/dev/null | head | unprint_bigrams | print_bigrams

XY_to_bigrams citations.G.transposed.???.big.trim.sketch.X citations.G.transposed.small.X > bigrams
sort_bigrams bigrams > bigrams.sorted
gram_matrix bigrams.sorted | uniq_bigrams_by_hashing > gram

PATH=$PATH:/work/k.church/semantic_scholar/citations/graphs/src/C/
cd /work/k.church/semantic_scholar/citations/graphs/freq_rows/C/threshold.100/pieces.100
sbatch --mem=1000000  -o gram -e gram.err -t 1209 -p short /work/k.church/semantic_scholar/citations/graphs/src/C/gram_matrix.sh bigrams.sorted
hist_bigrams 0 250000000 < bigrams > bigrams.0
hist_bigrams 1 250000000 < bigrams > bigrams.1

cd /work/k.church/semantic_scholar/citations/graphs/freq_rows/C/threshold.100/pieces.100
bincount_bigrams 200 < bigrams > bigrams.bincount

cd /work/k.church/semantic_scholar/citations/graphs/freq_rows/C/threshold.100/pieces.100
sbatch --mem=1000000 -o gram -e gram.foo.err -t 1209 -p short /work/k.church/semantic_scholar/citations/graphs/src/C/job.sh

PATH=$PATH:/work/k.church/semantic_scholar/citations/graphs/src/C/
cd /work/k.church/semantic_scholar/citations/graphs/freq_rows/C/threshold.100/pieces.100
N=250000000
for f in bigrams  bigrams.sorted    gram  gram.23019 
do
for axis in 0 1 -1
do
out=$f.axis.$axis
sbatch -i $f -o $out -e $out.err -p short hist_bigrams.sh $axis $N
done
done

egrep sum_all bigrams.err  bigrams.sorted.err    gram.err  gram.23019.err

sacct --starttime=2022-06-29 -o ElapsedRaw,JobName,State,CPUTimeRAW,AveCPU  | egrep job.sh

cd /work/k.church/semantic_scholar/citations/graphs/freq_rows/C/threshold.100/pieces.100
for obj in bigrams # gram.23019
do
pieces=256
prefix=$obj.pieces$pieces/x
mkdir -p `dirname $prefix`
sbatch -o $prefix.out -e $prefix.err -p short split_binary_records.sh -pieces $pieces -record_size 12 -filename $obj -prefix $prefix
done

cd /work/k.church/semantic_scholar/citations/graphs
awk '/tsvd_rand:/ {K[FILENAME]=$(NF-1)}; /total time/ {t[FILENAME]=$NF}; END {for(f in t) print K[f]+0, t[f]+0, f}' citations*shrink*prone*err | sort -n
10 18442.9 citations.G.shrink.new.T1.G2.npz.prone.tasks.56.K10.err
10 38934.6 citations.G.shrink.new.T1.G2.npz.prone.K10.instrumented.err
10 44302.2 citations.G.shrink.new.T1.G2.npz.prone.tasks.1.K10.err
20 59481.3 citations.G.shrink.new.T1.G2.npz.prone.tasks.1.K20.err
50 55246.4 citations.G.shrink.new.T1.G2.npz.prone.K50.instrumented.err

prefact total hidden_dimension filename
34.6683 77.7666 200 citations.G.shrink.new.T1.G2.npz.prone.tasks.1.K200.err
18.4647 43.9032 100 citations.G.shrink.new.T1.G2.npz.prone.tasks.1.K100.err
14.5139 22.5625  50 citations.G.shrink.new.T1.G2.npz.prone.tasks.1.K50.err
9.07111 16.5226  20 citations.G.shrink.new.T1.G2.npz.prone.tasks.1.K20.err
5.84917 12.3062  10 citations.G.shrink.new.T1.G2.npz.prone.tasks.1.K10.err

cd /work/k.church/semantic_scholar/citations/graphs
job1=`sbatch -p express  --array=100-103 src/C/simple_array_job.sh | awk '{print $NF}'`
# job2 will not start until after job1 is done
job2=`sbatch -p express  -d afterok:$job1 --array=200-203 src/C/simple_array_job.sh`


q=" -t 59 -p short,express "
npieces=256
pieces=/work/k.church/semantic_scholar/citations/graphs/freq_rows/C/threshold.100/pieces.100/bigrams.pieces$npieces/x
dir=`dirname $pieces`
mkdir -p $dir
cd $dir
job1=`sbatch $q                      --array=0-255 $src/C/sort_bigrams.sh  $pieces      | awk '{print $NF}'`
# job2=`sbatch $q -d afterany:$job1  --array=0-255 $src/C/check_bigrams.sh $pieces     | awk '{print $NF}'`
job3=`sbatch $q -d afterany:$job1    --array=0-127 $src/C/merge_bigrams.sh $pieces 0 128 | awk '{print $NF}'`
job4=`sbatch $q -d afterany:$job3    --array=0-63  $src/C/merge_bigrams.sh $pieces 1  64 | awk '{print $NF}'`
job5=`sbatch $q -d afterany:$job4    --array=0-31  $src/C/merge_bigrams.sh $pieces 2  32 | awk '{print $NF}'`
job6=`sbatch $q -d afterany:$job5    --array=0-15  $src/C/merge_bigrams.sh $pieces 3  16 | awk '{print $NF}'`
job7=`sbatch $q -d afterany:$job6    --array=0-7   $src/C/merge_bigrams.sh $pieces 4   8 | awk '{print $NF}'`
job8=`sbatch $q -d afterany:$job7    --array=0-3   $src/C/merge_bigrams.sh $pieces 5   4 | awk '{print $NF}'`
job9=`sbatch $q -d afterany:$job8    --array=0-1   $src/C/merge_bigrams.sh $pieces 6   2 | awk '{print $NF}'`
job10=`sbatch $q  -d afterany:$job9  --array=0     $src/C/merge_bigrams.sh $pieces 7   1 | awk '{print $NF}'`

# npieces=256
# cd /work/k.church/semantic_scholar/citations/graphs/freq_rows/C/threshold.100/pieces.100/bigrams.pieces$npieces/
# sbatch -p express  --array=0-127 $src/C/merge_bigrams.sh x 0
# sbatch -p express  --array=0-63 $src/C/merge_bigrams.sh x 1
# sbatch -p express  --array=0-31 $src/C/merge_bigrams.sh x 2
# sbatch -p express  --array=0-15 $src/C/merge_bigrams.sh x 3
# sbatch -p express  --array=0-7 $src/C/merge_bigrams.sh x 4

q=" -t 59 -p short,express "
npieces=256
pieces=/work/k.church/semantic_scholar/citations/graphs/freq_rows/C/threshold.100/pieces.100/bigrams.pieces$npieces/x
dir=`dirname $pieces`
mkdir -p $dir
cd $dir
job3=`sbatch $q --array=0-127 $src/C/merge_bigrams.sh $pieces 0 128 | awk '{print $NF}'`
job4=`sbatch $q --array=0-63  $src/C/merge_bigrams.sh $pieces 1  64 | awk '{print $NF}'`

lookup_bigram 61599	44390679 *merged.0 | awk '$1 > 0'
1.000000	61599	44390679	x.015.merged.1
2.000000	61599	44390679	x.031.merged.1
1.000000	61599	44390679	x.063.merged.1

lookup_bigram 61599	44390679 *sorted | awk '$1 > 0'
1.000000	61599	44390679	x.063.sorted

src=/work/k.church/semantic_scholar/citations/graphs/src
cd /work/k.church/semantic_scholar/citations/graphs/freq_rows/C/threshold.100/pieces.100/gram.23019.pieces200
pieces=x
job1=`sbatch -p short --array=0-199 $src/C/sort_bigrams.sh $pieces | awk '{print $NF}'`
job2=`sbatch -p express  -d afterok:$job1 --array=400-403 src/C/simple_array_job.sh`


pieces=/work/k.church/semantic_scholar/citations/graphs/freq_rows/C/threshold.100/pieces.100/bigrams.pieces200/x
job1=`sbatch -p short -e $pieces.err3 --array=0 $src/C/sort_bigrams.sh $pieces | awk '{print $NF}'`

cd /work/k.church/semantic_scholar/citations/graphs/freq_rows/C/threshold.100/pieces.100/bigrams.pieces256
# something is wrong
ls -l bigrams.sorted bigrams.pieces256/x.000.merged.7
-rw-rw----+ 1 k.church root   618153768 Jun 30 19:07 bigrams.pieces256/x.000.merged.7
-rw-rw----+ 1 k.church root 17583016800 Jun 29 19:12 bigrams.sorted


echo 61599 | x_to_y ai | extract_row x.000.merged.7 | print_bigrams 
calling memoize_index: x.000.merged.7
query = 61599, nrow = 3
28.00	61599	44390679
28.00	61599	207639404
28.00	61599	221691475
(gft) [k.church@c0167 bigrams.pieces256]$ echo 61599 | x_to_y ai | extract_row ../bigrams.sorted | print_bigrams 
calling memoize_index: ../bigrams.sorted
calling index_bigrams
matrix_shape --> 249150996
index_bigrams: n_index = 249150997
query = 61599, nrow = 14
1.00	61599	2219256
1.00	61599	2335972
1.00	61599	8411591
1.00	61599	9255369
1.00	61599	9311505
1.00	61599	11914979
1.00	61599	15990765
1.00	61599	20158621
1.00	61599	24136701
1.00	61599	29757967
1.00	61599	41951228
1.00	61599	44390679
1.00	61599	207639404
1.00	61599	221691475

cd /work/k.church/semantic_scholar/citations/graphs/freq_rows/C/threshold.100/pieces.100/bigrams.pieces256
sbatch -p debug,express,short --array=0-5 job.sh 

cd /work/k.church/semantic_scholar/citations/graphs/freq_rows/C/threshold.100/pieces.100/bigrams.pieces256
# sbatch -p debug,express,short --array=0-255 my_job.sh 
sbatch -p debug,express,short --array=0-255 my_job2.sh 

awk '/calling memoize_index:/ {fn=$NF}; /row/ && $NF != 0 {print fn, $NF}' slurm*out
x.062.sorted 11
x.063.sorted 3
x.000.merged.5 3
x.000.merged.6 3
x.000.merged.7 3
x.015.merged.1 3
x.015.merged.2 3
x.015.merged.3 3
x.001.merged.4 3
x.001.merged.5 3
x.001.merged.6 3
x.031.merged.0 3
x.031.merged.1 3
x.031.merged.2 3
x.003.merged.3 3
x.003.merged.4 3
x.003.merged.5 3
x.062.merged.0 11
x.062.merged.1 11
x.063.merged.0 3
x.063.merged.1 3
x.007.merged.2 3
x.007.merged.3 3
x.007.merged.4 3

echo 61599 | x_to_y ai | extract_row x.062.sorted| print_bigrams
calling memoize_index: x.062.sorted
query = 61599, nrow = 11
1.00	61599	2219256
1.00	61599	2335972
1.00	61599	8411591
1.00	61599	9255369
1.00	61599	9311505
1.00	61599	11914979
1.00	61599	15990765
1.00	61599	20158621
1.00	61599	24136701
1.00	61599	29757967
1.00	61599	41951228
(gft) [k.church@c0167 bigrams.pieces256]$ echo 61599 | x_to_y ai | extract_row x.063.sorted| print_bigrams
calling memoize_index: x.063.sorted
query = 61599, nrow = 3
1.00	61599	44390679
1.00	61599	207639404
1.00	61599	221691475

cd /work/k.church/semantic_scholar/citations/graphs/freq_rows/C/threshold.100/pieces.100
big_distributed_sort_bigrams.sh gram.23019 gram.23019.big
# rm -rf bigrams.big[23]
# big_distributed_sort_bigrams.sh bigrams.sorted bigrams.big5

mkdir -p /scratch/k.church/semantic_scholar/citations/graphs/freq_rows/C/threshold.100/pieces.100
ln -s /work/k.church/semantic_scholar/citations/graphs/freq_rows/C/threshold.100/pieces.100/gram.23019 /scratch/k.church/semantic_scholar/citations/graphs/freq_rows/C/threshold.100/pieces.100
ln -s /work/k.church/semantic_scholar/citations/graphs/freq_rows/C/threshold.100/pieces.100/bigrams /scratch/k.church/semantic_scholar/citations/graphs/freq_rows/C/threshold.100/pieces.100
ln -s /work/k.church/semantic_scholar/citations/graphs/freq_rows/C/threshold.100/pieces.100/bigrams.sorted /scratch/k.church/semantic_scholar/citations/graphs/freq_rows/C/threshold.100/pieces.100
cd /scratch/k.church/semantic_scholar/citations/graphs/freq_rows/C/threshold.100/pieces.100
big_distributed_sort_bigrams.sh gram.23019 gram.23019.big
# rm -rf bigrams.big[23]
# big_distributed_sort_bigrams.sh bigrams bigrams.big5


cd /scratch/k.church/semantic_scholar/citations/graphs/freq_rows/C/threshold.100/pieces.100/gram.23019.big
awk '$2 == "total_vals;" {x += $1}; END {print i, x}' i=start x.???.checked
awk '$2 == "total_vals;" {x += $1}; END {print i, x}' i=sorted x.???.sorted.checked
for i in 0 1 2 3 4 5 6 7
do
 awk '$2 == "total_vals;" {x += $1}; END {print i, x}' i=$i x.???.merged.$i.checked
 done



cd /work/k.church/semantic_scholar/citations/graphs
src=/work/k.church/semantic_scholar/citations/graphs/src
q=" -t 1209 -p short "
out=citations.G.page_rank.scikit
# networkx is crashing (perhaps because of memory); trying scikit instead
# $src/page_rank.py -G citations.G.npz -o $out.json
# sbatch --mem=100000 $q -e $out.err $src/page_rank.py -G citations.G.npz -o $out.json

# I probably want the transpose of this
sbatch $q -e $out.err $src/page_rank_scikit.py -G citations.G.npz -o $out.npy
sbatch $q -e $out.transpose.err $src/page_rank_scikit.py -G citations.G.transpose.npz -o $out.transpose.npy
sbatch $q -e $out.N100.transpose.err $src/page_rank_scikit.py -G citations.G.transpose.npz -o $out.N100.transpose.npy -N 100

# July 4th
# compute page rank again on sketches

cd /work/k.church/semantic_scholar/citations/graphs
src=/work/k.church/semantic_scholar/citations/graphs/src
q=" -t 1209 -p short "
out=citations.G.page_rank.scikit
# networkx is crashing (perhaps because of memory); trying scikit instead
# $src/page_rank.py -G citations.G.npz -o $out.json
# sbatch --mem=100000 $q -e $out.err $src/page_rank.py -G citations.G.npz -o $out.json

# July 4th
cd /work/k.church/semantic_scholar/citations/graphs/freq_rows/C/threshold.100/pieces.100
src=/work/k.church/semantic_scholar/citations/graphs/src
q=" -t 1209 -p short "
out=bigrams.page_rank
sbatch --mem=100000 $q -e $out.err $src/page_rank_scikit.py -G bigrams.npz -o $out.npy
sbatch --mem=100000 $q -e $out.transpose.err $src/page_rank_scikit.py -G bigrams.npz -o $out.transpose.npy -T
# sbatch $q -e $out.transpose.err $src/page_rank_scikit.py -G citations.G.transpose.npz -o $out.transpose.npy





cd /work/k.church/semantic_scholar/citations/graphs
src=/work/k.church/semantic_scholar/citations/graphs/src
q=" -t 1209 -p short "
out=citations.G.hits.V4
sbatch --mem=100000 $q -e $out.err $src/hits.py -G citations.G.npz -o $out.json



cd /work/k.church/semantic_scholar/citations/graphs/freq_rows/C/threshold.100/pieces.100
for f in */x.000.merged.7 gram gram.23019 # bigrams.sorted 
do
out=$f.checked
# mv $out $out.bak
sbatch -o $out -p express -t 59 `which check_bigrams.sh` $f
done

python /work/k.church/semantic_scholar/citations/graphs/src/page_rank_scikit.py -G /work/k.church/githubs/PecanPy.bak/demo/karate.npz -o /tmp/karate.npy 

# 
cd /work/k.church/semantic_scholar/citations/graphs/freq_rows/C/threshold.100/pieces.100
python
import numpy as np
V = [np.array(np.load(f)).reshape(-1) for f in ['/work/k.church/semantic_scholar/citations/graphs/citations.G.fanin.axis1.npy', \
'/work/k.church/semantic_scholar/citations/graphs/citations.G.page_rank.scikit.npy']]

s = (V[0] > 1000)
idx = np.arange(len(V[1]))
cite100 = np.array([idx[s], V[0][s], 1e11*V[1][s]], dtype=int)
np.savetxt("/home/k.church/to_go/cite100.txt", cite100.T, fmt='%d')

awk '{print $1}' $HOME/to_go/cite100_sample.txt | /work/k.church/semantic_scholar/papers/find_lines.py /work/k.church/semantic_scholar/papers/papers2url/papers.piece.???.txt > /tmp/foobar

awk 'BEGIN {while(getline < "/home/k.church/to_go/cite100_sample.txt" > 0) {cite[$1]=$2; pagerank[$1]=$3}; OFS=FS="\t"};
        {print cite[$1]+0, pagerank[$1]+0, $0}' /tmp/foobar | 
	sort -k2 -nr |
	awk 'BEGIN {print "<html><table><tr><th>citations</th><th>pagerank</th><th>id</th><th>paper</th></tr>"; FS=OFS="\t"};
	     END {print "</table></html>"};
	     {printf "<tr><td>%s</td><td>%s</td><td>%s</td><td><a href=\"%s\">%s</a></td></tr>\n", $1, $2, $3, $4, $5}' > /home/k.church/to_go/cite100_sample.html

sort -k3 -nr /home/k.church/to_go/cite100.txt | sed 200q | 
awk '{print $1}' | /work/k.church/semantic_scholar/papers/find_lines.py /work/k.church/semantic_scholar/papers/papers2url/papers.piece.???.txt > /tmp/foobar_high

sort -k3 -n /home/k.church/to_go/cite100.txt | egrep '^[0-9]' | sed 200q | 
awk '{print $1}' | /work/k.church/semantic_scholar/papers/find_lines.py /work/k.church/semantic_scholar/papers/papers2url/papers.piece.???.txt > /tmp/foobar_low

awk 'BEGIN {while(getline < "/home/k.church/to_go/cite100.txt" > 0) {cite[$1]=$2; pagerank[$1]=$3}; OFS=FS="\t"};
        {print cite[$1]+0, pagerank[$1]+0, $0}' /tmp/foobar_high |
	awk 'BEGIN {print "<html><table><tr><th>citations</th><th>pagerank</th><th>id</th><th>paper</th></tr>"; FS=OFS="\t"};
	     END {print "</table></html>"};
	     {printf "<tr><td>%s</td><td>%s</td><td>%s</td><td><a href=\"%s\">%s</a></td></tr>\n", $1, $2, $3, $4, $5}' > /home/k.church/to_go/cite1000_high_pagerank.html

awk 'BEGIN {while(getline < "/home/k.church/to_go/cite100.txt" > 0) {cite[$1]=$2; pagerank[$1]=$3}; OFS=FS="\t"};
        {print cite[$1]+0, pagerank[$1]+0, $0}' /tmp/foobar_low |
	awk 'BEGIN {print "<html><table><tr><th>citations</th><th>pagerank</th><th>id</th><th>paper</th></tr>"; FS=OFS="\t"};
	     END {print "</table></html>"};
	     {printf "<tr><td>%s</td><td>%s</td><td>%s</td><td><a href=\"%s\">%s</a></td></tr>\n", $1, $2, $3, $4, $5}' > /home/k.church/to_go/cite1000_low_pagerank.html




V = [np.array(np.load(f)).reshape(-1) for f in ['/work/k.church/semantic_scholar/citations/graphs/citations.G.fanin.axis0.npy', \
'/work/k.church/semantic_scholar/citations/graphs/citations.G.fanin.axis1.npy', \
'/work/k.church/semantic_scholar/citations/graphs/citations.G.page_rank.scikit.npy', \
'/work/k.church/semantic_scholar/citations/graphs/citations.G.page_rank.scikit.transpose.npy', \
'bigrams.page_rank.npy', \
'bigrams.page_rank.transpose.npy']]

q999 = [np.quantile(v, 0.999) for v in V]
q999
[226.0, 492.0, 4.5703595803861574e-08, 3.381538115170695e-08]
s = (V[1] > q999[1])
sample999 = np.array([v[s] for v in V]).T
np.savetxt("/home/k.church/to_go/sample999c.txt", sample999)

x = read.table("to_go/sample999.txt", header=T)

                                       citations.G.fanin.axis0 citations.G.page_rank.scikit.transpose bigrams.page_rank bigrams.page_rank.transpose citations.G.fanin.axis1 citations.G.page_rank.scikit
citations.G.fanin.axis0                                   1.00                                   0.76              0.75                       -0.21                    0.00                        -0.05
citations.G.page_rank.scikit.transpose                    0.76                                   1.00              0.99                       -0.09                   -0.01                        -0.02
bigrams.page_rank                                         0.75                                   0.99              1.00                       -0.11                   -0.01                        -0.02
bigrams.page_rank.transpose                              -0.21                                  -0.09             -0.11                        1.00                    0.01                         0.20
citations.G.fanin.axis1                                   0.00                                  -0.01             -0.01                        0.01                    1.00                         0.76
citations.G.page_rank.scikit                             -0.05                                  -0.02             -0.02                        0.20                    0.76                         1.00

> summary(x[,o])

> round(cor(x),2)
                                       citations.G.fanin.axis0 citations.G.fanin.axis1 citations.G.page_rank.scikit citations.G.page_rank.scikit.transpose
citations.G.fanin.axis0                                   1.00                    0.10                        -0.01                                   0.73
citations.G.fanin.axis1                                   0.10                    1.00                         0.73                                   0.03
citations.G.page_rank.scikit                             -0.01                    0.73                         1.00                                  -0.01
citations.G.page_rank.scikit.transpose                    0.73                    0.03                        -0.01                                   1.00

# large correlations are dominated by large values
# hypothesis: page rank will be more effective on sketches than on raw citation graph

> quantile(x[,3])
          0%          25%          50%          75%         100% 
    45.70371     55.70314     73.27208    115.46038 166051.96100 
> s = x[,3] < 115
> round(cor(x[s,]),2)
                                       citations.G.fanin.axis0 citations.G.fanin.axis1 citations.G.page_rank.scikit citations.G.page_rank.scikit.transpose
citations.G.fanin.axis0                                   1.00                    0.33                         0.00                                   0.74
citations.G.fanin.axis1                                   0.33                    1.00                         0.18                                   0.12
citations.G.page_rank.scikit                              0.00                    0.18                         1.00                                   0.00
citations.G.page_rank.scikit.transpose                    0.74                    0.12                         0.00                                   1.00
> quantile(x[,2])
    0%    25%    50%    75%   100% 
     1    152    390    905 679846 
> s = x[,2] < 905
> round(cor(x[s,]),2)
                                       citations.G.fanin.axis0 citations.G.fanin.axis1 citations.G.page_rank.scikit citations.G.page_rank.scikit.transpose
citations.G.fanin.axis0                                   1.00                    0.20                        -0.05                                   0.71
citations.G.fanin.axis1                                   0.20                    1.00                         0.06                                   0.05
citations.G.page_rank.scikit                             -0.05                    0.06                         1.00                                  -0.02
citations.G.page_rank.scikit.transpose                    0.71                    0.05                        -0.02                                   1.00


cd /work/k.church/semantic_scholar/citations/graphs/freq_rows/C/threshold.100/pieces.100
for t in bsr coo csc csr dia #  dok lil
do
echo $t
python /work/k.church/semantic_scholar/citations/graphs/src/bigrams_to_npz.py -B bigrams.1000 -o bigrams.1000.$t.npz -t $t
done

cd /work/k.church/semantic_scholar/citations/graphs/freq_rows/C/threshold.100/pieces.100
src=/work/k.church/semantic_scholar/citations/graphs/src
q=" -t 1209 -p short "
PATH=$PATH:$src:$src/C
sbatch -e bigrams.err $q $src/bigrams_to_npz.py -B bigrams -o bigrams.npz

cd /work/k.church/semantic_scholar/citations/graphs
for f in *.annoy
do
w2v=`dirname $f`/`basename $f .annoy`
if [ -s $w2v ]
then
echo working on $w2v
sbatch -e $w2v.gz.err $q $src/gzip.sh $w2v
# nohup nice gzip $w2v &
fi
done

import numpy as np
import scipy.sparse
f='bigrams.npz'
M=scipy.sparse.load_npz(f)


q=" -t 1209 -p short "
src=/work/k.church/semantic_scholar/citations/graphs/src
PATH=$PATH:$src:$src/C
cd /work/k.church/semantic_scholar/citations/graphs
for f in  *.annoy # citations.G.shrink.new.T1.G2.npz.instrumented.K10.ProNE.K10.T20.O5.w2v.annoy
do
sbatch -e $f.stats.err --mem=100000 $q annoy_stats.py -M $f -o $f.stats
done


q=" -t 1209 -p short "
src=/work/k.church/semantic_scholar/citations/graphs/src
PATH=$PATH:$src:$src/C
cd /work/k.church/semantic_scholar/citations/graphs
for f in  *.annoy # citations.G.shrink.new.T1.G2.npz.instrumented.K10.ProNE.K10.T20.O5.w2v.annoy
do
sbatch -e $f.stats.err --mem=100000 $q annoy_stats.py -M $f -o $f.stats
done

cd /work/k.church/semantic_scholar/embeddings/M3
for f in  *.annoy # citations.G.shrink.new.T1.G2.npz.instrumented.K10.ProNE.K10.T20.O5.w2v.annoy
do
sbatch -e $f.stats.err --mem=100000 $q annoy_stats.py -M $f -o $f.stats
done

mkdir -p /work/k.church/semantic_scholar/papers/papers2url.V2
cd /work/k.church/semantic_scholar/papers/papers2url.V2
sbatch --array 1-30 -p express -t 59 /work/k.church/semantic_scholar/papers/lines2html.py --input '../papers.piece.%03d.gz' --output 'lines2html.V2.%03d'

cd /work/k.church/semantic_scholar/papers/papers2url
# sbatch -p debug,express,short --array=1-30 ../lines2html.sh papers.piece txt
# cat papers.piece.???.txt.lines2html > lines2html
# sbatch -e lines2html.err -p express -t 59 --mem=500G ../index_lines.py lines2html
sbatch -e lines2html.err -p short -t 1209 --mem=500G ../index_lines.py lines2html


echo 95530365 | /work/k.church/semantic_scholar/papers/find_lines.py /work/k.church/semantic_scholar/papers/papers2url/lines2html

cd /work/k.church/semantic_scholar/embeddings/M3
tr '\t_' '\n' < embeddings.piece.001.w2v.xaa.annoy.sample | egrep '[0-9]' | cut -f1 -d: | sort -u | 
/work/k.church/semantic_scholar/papers/find_lines.py /work/k.church/semantic_scholar/papers/papers2url/lines2html > /tmp/l2html

cd /work/k.church/semantic_scholar/embeddings/M3
cat embeddings.piece.001.w2v.xaa.annoy.sample  embeddings.piece.001.w2v.xaa.annoy.sample.N50 | tr '\t_' '\n'  | egrep '[0-9]' | cut -f1 -d: | sort -u | 
/work/k.church/semantic_scholar/papers/find_lines.py /work/k.church/semantic_scholar/papers/papers2url/lines2html > /tmp/l2html

python sample_to_html.py /tmp/l2html < embeddings.piece.001.w2v.xaa.annoy.sample.N50 > embeddings.piece.001.w2v.xaa.annoy.sample.N50.html

cd /work/k.church/semantic_scholar/embeddings/M3
sbatch -p short -t 1209 --array=1-30 /work/k.church/semantic_scholar/embeddings/M3/index_job.sh embeddings.piece w2v.xaa
for f in extras.x??
do
sbatch -p short -t 1209 /work/k.church/semantic_scholar/embeddings/M3/index_job.sh $f
done

for f in *.xaa.bak
do
mv $f `basename $f .bak`
done

cd /work/k.church/semantic_scholar/embeddings/M3
head embeddings.piece.012.w2v.xaa | awk '{print $1}' | cut -f2 -d_ | python ids_to_vectors.py embeddings.piece.012.w2v.xaa.annoy > /tmp/x

dir=/work/k.church/semantic_scholar/embeddings/M3
random_bigrams /scratch/k.church/semantic_scholar/citations/graphs/freq_rows/C/threshold.100/pieces.100/bigrams 100 | print_bigrams | cut -f2- > /scratch/k.church/tmp/sample
tr '\t' '\n' < /scratch/k.church/tmp/sample > /scratch/k.church/tmp/sample.ids
sbatch -i /scratch/k.church/tmp/sample.ids -o /scratch/k.church/tmp/sample.vectors --array=1-42 -p debug,express $dir/ids_to_vectors.py 

/work/k.church/semantic_scholar/papers/find_lines.py /work/k.church/semantic_scholar/papers/papers2url/lines2html < /scratch/k.church/tmp/sample.ids > /scratch/k.church/tmp/sample.ids.html

python vectors_to_cosines.py < /scratch/k.church/tmp/sample.vectors | sort -nr > /scratch/k.church/tmp/sample.cos
awk 'BEGIN {SUBSEP=FS=OFS="\t"
    	    while(getline < "/scratch/k.church/tmp/sample" > 0) cite[$1, $2]=1
            while(getline < "/scratch/k.church/tmp/sample.ids.html" > 0) html[$1]=$2
     print("<html><table><tr><th>score</th><th>cite x y</th><th>cite y x</th><th>x</th><th>y</th></tr>")}
     END {print "</table></html>"}
      {score=$1; x=$2; y = $3;
       if(!(x in html)) html[x]=x
       if(!(y in html)) html[y]=y
       printf "<tr><td>%0.3f</td><td>%d</td><td>%d</td><td>%s</td><td>%s</td></tr>\n", score, cite[y,x], cite[x,y], html[x], html[y];}' /scratch/k.church/tmp/sample.cos |
# fix a stupid bug
sed 's/<href/<a href/g' > /scratch/k.church/tmp/sample.html





ln -s $dir/extras.xaa.annoy $dir/embeddings.piece.031.w2v.xaa.annoy
ln -s $dir/extras.xab.annoy $dir/embeddings.piece.032.w2v.xaa.annoy
ln -s $dir/extras.xac.annoy $dir/embeddings.piece.033.w2v.xaa.annoy
ln -s $dir/extras.xad.annoy $dir/embeddings.piece.034.w2v.xaa.annoy
ln -s $dir/extras.xae.annoy $dir/embeddings.piece.035.w2v.xaa.annoy
ln -s $dir/extras.xaf.annoy $dir/embeddings.piece.036.w2v.xaa.annoy
ln -s $dir/extras.xag.annoy $dir/embeddings.piece.037.w2v.xaa.annoy
ln -s $dir/extras.xah.annoy $dir/embeddings.piece.038.w2v.xaa.annoy
ln -s $dir/extras.xai.annoy $dir/embeddings.piece.039.w2v.xaa.annoy
ln -s $dir/extras.xaj.annoy $dir/embeddings.piece.040.w2v.xaa.annoy
ln -s $dir/extras.xak.annoy $dir/embeddings.piece.041.w2v.xaa.annoy
ln -s $dir/extras.xal.annoy $dir/embeddings.piece.042.w2v.xaa.annoy

ln -s $dir/extras.xaa.annoy.vectors.npy $dir/embeddings.piece.031.w2v.xaa.annoy.vectors.npy
ln -s $dir/extras.xab.annoy.vectors.npy $dir/embeddings.piece.032.w2v.xaa.annoy.vectors.npy
ln -s $dir/extras.xac.annoy.vectors.npy $dir/embeddings.piece.033.w2v.xaa.annoy.vectors.npy
ln -s $dir/extras.xad.annoy.vectors.npy $dir/embeddings.piece.034.w2v.xaa.annoy.vectors.npy
ln -s $dir/extras.xae.annoy.vectors.npy $dir/embeddings.piece.035.w2v.xaa.annoy.vectors.npy
ln -s $dir/extras.xaf.annoy.vectors.npy $dir/embeddings.piece.036.w2v.xaa.annoy.vectors.npy
ln -s $dir/extras.xag.annoy.vectors.npy $dir/embeddings.piece.037.w2v.xaa.annoy.vectors.npy
ln -s $dir/extras.xah.annoy.vectors.npy $dir/embeddings.piece.038.w2v.xaa.annoy.vectors.npy
ln -s $dir/extras.xai.annoy.vectors.npy $dir/embeddings.piece.039.w2v.xaa.annoy.vectors.npy
ln -s $dir/extras.xaj.annoy.vectors.npy $dir/embeddings.piece.040.w2v.xaa.annoy.vectors.npy
ln -s $dir/extras.xak.annoy.vectors.npy $dir/embeddings.piece.041.w2v.xaa.annoy.vectors.npy
ln -s $dir/extras.xal.annoy.vectors.npy $dir/embeddings.piece.042.w2v.xaa.annoy.vectors.npy

for piece in embeddings.piece.???.w2v.xaa
do
if [ ! -s $piece.annoy ]
then
echo working on $piece
sbatch --mem=100G -p short -e $piece.err /work/k.church/semantic_scholar/jobs/glove2annoy.py $piece
fi
done

155016327	<href="https://www.semanticscholar.org/paper/cafdc9671da4926bc2781128f305424c1a976c2d">É atrativo tornar-se professor do Ensino Médio no Brasil?: Evidências com base em decomposições paramétricas e não paramétricas</a>
57572515	<href="https://www.semanticscholar.org/paper/58526f48dcb02d091642e1ffa04aceaa37e933ea">Do Alternative Opportunities Matter? The Role of Female Labor Markets in the Decline of Teacher Quality</a>
154980312	<href="https://www.semanticscholar.org/paper/83d2f6f2fb386ba7c48637b91d4a9464b6c007df">Os professores públicos são mal remunerados nas escolas brasileiras? Uma análise da atratividade da carreira do magistério sob o aspecto da remuneração</a>
246586455	<href="https://www.semanticscholar.org/paper/df60bb15bde387ddce27608b6aed675ce86af14c">New Evidence on Teacher Pay</a>
150783626	<href="https://www.semanticscholar.org/paper/2b689075acda86a5c8a5d637d94626cae627c210">Os salários dos professores públicos são atrativos no Brasil</a>

import numpy as np
import scipy.sparse
f='citations.G.npz'
M=scipy.sparse.load_npz(f)
ids = [155016327,57572515,154980312,246586455,150783626]

import numpy as np
import scipy.sparse
f='citations.G.npz'
M=scipy.sparse.load_npz(f)
ids = np.array([155016327,57572515,154980312,246586455,150783626])
# This is too slow
D = scipy.sparse.csgraph.dijkstra(M, directed=False, indices=ids, limit=3)
# BFO = scipy.sparse.csgraph.breadth_first_order(M, ids[0], directed=False)

23176518	<href="https://www.semanticscholar.org/paper/8a4f7f4f8d3dc75ba4ed99a3b0ddc82db5b3f643">Headaches of Increasing Intensity for a Week After Using Crack Cocaine</a>
5041095	<href="https://www.semanticscholar.org/paper/445df80373b3f7210a2a5abc9972641a3b67888c">Trigeminocardiac reflex in neurosurgical practice: An observational prospective study</a>
80599679	<href="https://www.semanticscholar.org/paper/551d83c06dc38834cb43af8555306550b1985e8b">Diazepam Infusion in the Treatment of Tetanus</a>
57067469	<href="https://www.semanticscholar.org/paper/1dc88eaa1294514ce59a295fd9266085cd593153">Anesthesia and the Trigeminocardiac Reflex</a>
26990476	<href="https://www.semanticscholar.org/paper/ff4211a01b32064e89b71cc97d072835bbf0828f">Trigeminocardiac reflex during Le Fort I osteotomy: a case-crossover study.</a>



for id in 155016327 57572515 154980312 246586455 150783626 23176518 5041095 80599679 57067469 26990476
do
out=/scratch/k.church/semantic_scholar/citations/graphs/reachable/$id
mkdir -p `dirname $out`
sbatch -p debug,express,short -o $out -e $out.err $src/reachable.py -s $id -G /work/k.church/semantic_scholar/citations/graphs/citations.G.npz
done

$dir/semantic_scholar_embedding_near.py -M $f --query_mode self -N 5 --random 5 --map_node_names citations.G.shrink.new.T1.npz
23176518	23176518:1.000	5041095:0.925	80599679:0.924	57067469:0.920	26990476:0.919
56014693	56014693:1.000	236300278:0.993	3370250:0.991	150180823:0.991	232095754:0.990
245019715	245019715:1.000	3486842:0.952	174780325:0.936	247979153:0.931	80347388:0.925
124391102	124391102:1.000	93572633:0.886	96808493:0.870	124360434:0.858	121720145:0.851
155016327	155016327:1.000	57572515:0.988	154980312:0.985	246586455:0.984	150783626:0.981

f=/scratch/k.church/semantic_scholar/citations/graphs/citations.G.shrink.new.T1.G2.npz.tasks.1.K200.ProNE.K200.T20.O5.w2v.filtered.annoy
dir=/work/k.church/semantic_scholar/embeddings/M3
gdir=/work/k.church/semantic_scholar/citations/graphs
# $dir/semantic_scholar_embedding_near.py -M $f --query_mode self -N 5 --random 5 --map_node_names citations.G.shrink.new.T1.npz
$dir/semantic_scholar_embedding_near.py -M $f --query_mode pairwise_terms --map_node_names $gdir/citations.G.shrink.new.T1.npz < /tmp/terms > /tmp/terms.sim

in=/scratch/k.church/semantic_scholar/citations/graphs/experiment/terms
/work/k.church/semantic_scholar/papers/find_lines.py /work/k.church/semantic_scholar/papers/papers2url/lines2html < $in > $in.html

in=/scratch/k.church/semantic_scholar/citations/graphs/experiment/terms
rdir=/scratch/k.church/semantic_scholar/citations/graphs/experiment/reachable2
gdir=/work/k.church/semantic_scholar/citations/graphs
mkdir -p $rdir
for id in `cat $in`
do
sbatch -t 1209 -o $rdir/$id.out -e $rdir/$id.err -p short $src/reachable.py -s $id -G $gdir/citations.G.npz
done

in=/scratch/k.church/semantic_scholar/citations/graphs/experiment/terms
cd /scratch/k.church/semantic_scholar/citations/graphs/
cosdir=/scratch/k.church/semantic_scholar/citations/graphs/experiment/cos2
gdir=/work/k.church/semantic_scholar/citations/graphs
#map=/work/k.church/semantic_scholar/citations/graphs/citations.G.shrink.new.T1.G2.npz
map=$gdir/citations.G.shrink.new.T1.npz
mkdir -p $cosdir
for M in *filt*annoy
do
out=$cosdir/$M.out
err=$cosdir/$M.err
mkdir -p `dirname $out`
sbatch -t 59 -i $in -o $out -e $err -p express,short $dir/semantic_scholar_embedding_near.py -M $M --query_mode pairwise_terms --map_node_names $map
N=10
out=$cosdir/N.$N/$M.out
err=$cosdir/N.$N/$M.err
mkdir -p `dirname $out`
sbatch -t 59 -i $in -o $out -e $err -p express,short $dir/semantic_scholar_embedding_near.py -M $M --query_mode terms --map_node_names $map -N $N
done

cd /scratch/k.church/semantic_scholar/citations/graphs
awk '{for(i=2;i<=NF;i++) print $1, $i}' ./experiment/cos*/N.10/*.out | cut -f1 -d: | awk '$1 != $2' | sort -u > experiment/pairs2

dir=/work/k.church/semantic_scholar/embeddings/M3
in=/scratch/k.church/semantic_scholar/citations/graphs/experiment/pairs
cd /scratch/k.church/semantic_scholar/citations/graphs/
cosdir=/scratch/k.church/semantic_scholar/citations/graphs/experiment/cos2
gdir=/work/k.church/semantic_scholar/citations/graphs
#map=/work/k.church/semantic_scholar/citations/graphs/citations.G.shrink.new.T1.G2.npz
map=$gdir/citations.G.shrink.new.T1.npz
mkdir -p $cosdir
for M in *filt*annoy
do
out=$cosdir/pairs/$M.out
err=$cosdir/pairs/$M.err
mkdir -p `dirname $out`
sbatch -t 59 -i $in -o $out -e $err -p express,short $dir/semantic_scholar_embedding_near.py -M $M --query_mode pairs --map_node_names $map
done

dir=/work/k.church/semantic_scholar/embeddings/M3
in=/scratch/k.church/semantic_scholar/citations/graphs/experiment/pairs2
cd /scratch/k.church/semantic_scholar/citations/graphs/
cosdir=/scratch/k.church/semantic_scholar/citations/graphs/experiment/cos2
gdir=/work/k.church/semantic_scholar/citations/graphs
#map=/work/k.church/semantic_scholar/citations/graphs/citations.G.shrink.new.T1.G2.npz
map=$gdir/citations.G.shrink.new.T1.npz
mkdir -p $cosdir
for M in *filt*annoy
do
out=$cosdir/pairs2/$M.out
err=$cosdir/pairs2/$M.err
mkdir -p `dirname $out`
sbatch -t 59 -i $in -o $out -e $err -p express,short $dir/semantic_scholar_embedding_near.py -M $M --query_mode pairs --map_node_names $map
done

awk 'BEGIN {SUBSEP=OFS="\t"; 
while(getline < "/scratch/k.church/semantic_scholar/citations/graphs/experiment/pairs2" > 0) pairs[$1,$2]=0}; 
$2 == -1 {start=$1}; start SUBSEP $1 in pairs || $1 SUBSEP start in pairs {reachable[$1,start]=reachable[start,$1]=$2}; 
END {for(i in pairs) if(i in reachable) {print i, reachable[i]} else {print i, "NA"}}
  ' /scratch/k.church/semantic_scholar/citations/graphs/experiment/reachable2/*out | sort > /scratch/k.church/semantic_scholar/citations/graphs/experiment/reachable.txt

cd /scratch/k.church/semantic_scholar/citations/graphs/experiment
tr ' \t' '\n' < pairs2 | sort -u > terms2
/work/k.church/semantic_scholar/papers/find_lines.py /work/k.church/semantic_scholar/papers/papers2url/lines2html < terms2 |
# fix a stupid bug
sed 's/<href/<a href/g' > terms2.html

cd /scratch/k.church/semantic_scholar/citations/graphs/experiment
in=/scratch/k.church/semantic_scholar/citations/graphs/experiment/pairs2
paste $in.specter cos2/pairs2/*out /scratch/k.church/semantic_scholar/citations/graphs/experiment/reachable.txt | 
awk 'BEGIN {print "paper1\tpaper2\tSpecter\tpaper1\tpaper2\tK100\tpaper1\tpaper2\tK200\tpaper1\tpaper2\treachable"}; {print}' | 
paste - pairs2.citations > summary.txt

sed 1q summary.txt > summary2.txt
awk 'NR > 1' summary.txt | sort -k3 -nr >> summary2.txt

awk 'BEGIN {FS=OFS="\t"; 
while(getline < "terms2.html" > 0) link[$1]=$2; 
print "<html><table><tr><th>Specter</th><th>citations1</th><th>citations2</th><th>cocitations</th><th>K100</th><th>K200</th><th>reachable</th><th>x</th><th>y</th></tr>"}; 
END {print "</table></html>"}; 
NR == 1 {for(i=1;i<=NF;i++) header[$i]=i}
NR > 1 {printf("<tr><td>%s</td><td>%s</td><td>%s</td><td>%s</td><td>%s</td><td>%s</td><td>%s</td><td>%s</td><td>%s</td></tr>\n", 
$(header["Specter"]), $(header["citations1"]), $(header["citations2"]), $(header["cocitations"]),
$(header["K100"]), $(header["K200"]), $(header["reachable"]), 
link[$(header["paper1"])], link[$(header["paper2"])])}' summary2.txt > summary.html
cp summary.html $HOME/to_go

src=/work/k.church/semantic_scholar/citations/graphs/src

python $src/cocitations.py < /scratch/k.church/semantic_scholar/citations/graphs/experiment/pairs2 > /scratch/k.church/semantic_scholar/citations/graphs/experiment/pairs2.citations

src=/work/k.church/semantic_scholar/citations/graphs/src
in=/scratch/k.church/semantic_scholar/citations/graphs/experiment/pairs2
out=$in.citations2
err=$in.citations2.err
sbatch -p short -t 1209 -i $in -e $err -o $out $src/cocitations.py

src=/work/k.church/semantic_scholar/citations/graphs/src
d=/scratch/k.church/semantic_scholar/citations/graphs/tofile
G=/work/k.church/semantic_scholar/citations/graphs/citations.G.transpose.npz
mkdir -p $d
cd $d
out=citations.G.transpose
sbatch -e $out.err -p short -t 1209 $src/graph_tofile.py -G $G -o $out

echo 2 | /work/k.church/semantic_scholar/papers/find_lines.py /work/k.church/semantic_scholar/papers/papers2url/lines2html
2	<href="https://www.semanticscholar.org/paper/712fa8239d5705290225221fcadedece50bf46d7">Benchmarking state-of-the-art classification algorithms for credit scoring: An update of research</a>
(gft) [k.church@c0171 tofile]$ x_to_y ia < citations.G.transpose.Y.int | head | /work/k.church/semantic_scholar/papers/find_lines.py /work/k.church/semantic_scholar/papers/papers2url/lines2html
573045	<href="https://www.semanticscholar.org/paper/b8285e74b37b25e8527d2e87d1fae38fc55d41c4">Predicting ligand-dependent tumors from multi-dimensional signaling features</a>
3405605	<href="https://www.semanticscholar.org/paper/51c5cb5588e1b04bfbaa0b009ee97a44b6b424ef">A comparative study on base classifiers in ensemble methods for credit scoring</a>
3505367	<href="https://www.semanticscholar.org/paper/5be590420cdaa824b57a0f9d584569c41088fcd9">Late payment prediction models for fair allocation of customer contact lists to call center agents</a>
3523561	<href="https://www.semanticscholar.org/paper/46f0d066348d6747c99c8ca74140d677ae88ab1f">A Novel behavioral scoring model for estimating probability of default over time in peer-to-peer lending</a>
3756704	<href="https://www.semanticscholar.org/paper/942b8322127e601b6dd3fbe997aeda85d92e2607">Online Deep Learning: Growing RBM on the fly</a>
4230513	<href="https://www.semanticscholar.org/paper/6da16b8a8731c1e96352e47b9bc41aab755f66f8">Profit-Based Model Selection for Customer Retention Using Individual Customer Lifetime Values</a>
4498184	<href="https://www.semanticscholar.org/paper/9c61dd38b0999f23f737d1139952a4260ba7319a">Geographically Weighted Logistic Regression Applied to Credit Scoring Models *</a>
4536711	<href="https://www.semanticscholar.org/paper/ee49d94504a8061b1f17ffcdcf65ef687bb2444a">A Pruning Neural Network Model in Credit Classification Analysis</a>
4568438	<href="https://www.semanticscholar.org/paper/0575a0eb8e6d5bc106acbdac6306f4ff7073d7aa">Predicting mortgage default using convolutional neural networks</a>
4898505	<href="https://www.semanticscholar.org/paper/ddb63f66894a5638eb1bb414c330c82844d7282c">Dynamic ensemble classification for credit scoring using soft probability</a>

citedir=/scratch/k.church/semantic_scholar/citations/graphs/tofile
in=/scratch/k.church/semantic_scholar/citations/graphs/experiment/pairs2
$src/C/cocitations $citedir/citations.G.transpose.[XY].int  < $in > $in.citations

tr ' \t' '\n' < $in | sort -u > $in.terms
ddir=/work/k.church/semantic_scholar/embeddings/M3 
sbatch -i $in.terms -o $in.vectors --array=1-42 -p debug,express $ddir/ids_to_vectors.py 

# /work/k.church/semantic_scholar/embeddings/M3/cosines_for_pairs.py
python $ddir/cosines_for_pairs.py $in.vectors < $in > $in.specter

$HOME/final/morphology/dict_to_embedding.py -i /work/k.church/githubs/PecanPy/demo/karate.edg  -m ProNE_prefactorization -o /work/k.church/githubs/PecanPy/demo/karate.pre_factorization
$HOME/final/morphology/dict_to_embedding.py -i /work/k.church/githubs/PecanPy/demo/karate.edg  -m ProNE -o /work/k.church/githubs/PecanPy/demo/karate -F /work/k.church/githubs/PecanPy/demo/karate.pre_factorization.npy



cd /work/k.church/semantic_scholar/citations/graphs
for f in citations.G.shrink.new.T1.G2.npz
do
for K in 280 # 240 # 290 # 325 375 # 350 400 # 10 60 # 175 350 # 250 290 #  300 # 300 # 200 768 # 100  20 50 # 10 # 50 100 768
do
# do not need much time and memory for small K
sbatch -p long -t 7199 -e $f.ProNE.K$K.err --mem=1800G $HOME/final/morphology/dict_to_embedding.py -G $f -m ProNE -o $f.ProNE -K $K
# sbatch -p short -t 1209 -e $f.prone.pre.K$K.err --mem=750G $HOME/final/morphology/dict_to_embedding.py -G $f -m ProNE_prefactorization -o $f.pre -K $K
done
done

# jobs
# 30121640 (K=240)


cd /work/k.church/semantic_scholar/citations/graphs
for f in citations.G.shrink.new.T1.G2.npz
do
for K in 375
do
sbatch -p long -t 7199 -e $f.ProNE.K$K.err --mem=1800G $HOME/final/morphology/dict_to_embedding.py -G $f -m ProNE -o $f.ProNE -K $K
done
done


cd /work/k.church/semantic_scholar/embeddings
sbatch --array 1-30 -t 1209 -p short /work/k.church/semantic_scholar/jobs/embeddings_piece_to_kwc.py /work/k.church/semantic_scholar/embeddings/embeddings.piece /scratch/k.church/semantic_scholar/embeddings/embeddings.piece

cd /work/k.church/semantic_scholar/citations/graphs/K375
# python /work/k.church/semantic_scholar/jobs/npz_to_kwc_without_filtering.py citations.G.shrink.new.T1.G2.npz.ProNE.ProNE.K375.T20.O5.w2v.npz citations.G.shrink.new.T1.G2.npz.ProNE.ProNE.K375.T20.O5.w2v
sbatch --constraint=ib -p short -t 1209 --mem=500G /work/k.church/semantic_scholar/jobs/npz_to_kwc.py --input citations.G.shrink.new.T1.G2.npz.ProNE.ProNE.K375.T20.O5.w2v.npz --output citations.G.shrink.new.T1.G2.npz.ProNE.ProNE.K375.T20.O5.w2v.unfiltered
sbatch --constraint=ib -p short -t 1209 --mem=1000G /work/k.church/semantic_scholar/jobs/npz_to_kwc.py --input citations.G.shrink.new.T1.G2.npz.ProNE.ProNE.K375.T20.O5.w2v.npz --output citations.G.shrink.new.T1.G2.npz.ProNE.ProNE.K375.T20.O5.w2v.filtered --filter


python /work/k.church/semantic_scholar/jobs/npz_to_kwc.py PecanPy/demo/karate.ProNE.K32.T20.O5.w2v.npz PecanPy/demo/karate.kwc

python /work/k.church/semantic_scholar/jobs/kwc_to_binary_word2vec_format.py -i PecanPy/demo/karate.kwc -K 32 -o PecanPy/demo/karate.w2v.bin

cd /scratch/k.church/semantic_scholar/embeddings
sbatch -d afterany:29007789  --array 1-30 -t 1209 -p short /work/k.church/semantic_scholar/jobs/kwc_to_binary_word2vec_format.py -i embeddings.piece -K 768 -o embeddings.piece

python /work/k.church/semantic_scholar/simple_embedding2annoy.py --binary /work/k.church/githubs/PecanPy/demo/karate.w2v.bin


cd /scratch/k.church/semantic_scholar/embeddings
cat embeddings.piece.???.kwc.nodes.txt > embeddings.piece.kwc.nodes.txt
cat embeddings.piece.???.kwc.edges.f > embeddings.piece.kwc.edges.f
sbatch -t 1209 -p short /work/k.church/semantic_scholar/jobs/kwc_to_binary_word2vec_format.py -i embeddings.piece -K 768 -o embeddings.w2v.bin

sbatch  --mem=750G -p short  /work/k.church/semantic_scholar/simple_embedding2annoy.py --binary embeddings.w2v.bin


x = np.array(np.load('/work/k.church/semantic_scholar/citations/graphs/citations.G.fanin.axis1.npy')).reshape(-1)
x100 = np.arange(len(x))[x >= 100]
np.savetxt('/work/k.church/semantic_scholar/citations/graphs/citations.G.100+.txt', x100, fmt='%d')

y=np.load('/work/k.church/semantic_scholar/citations/graphs/citations.G.page_rank.scikit.npy')
y100 = y[x100]
np.savetxt('/work/k.church/semantic_scholar/citations/graphs/citations.G.100+.log10_page_rank.txt', np.log10(y100), fmt='%f')

freq = x[x100]
np.savetxt('/work/k.church/semantic_scholar/citations/graphs/citations.G.100+.freq.txt', freq, fmt='%d')

paste citations.G.100+.txt citations.G.100+.freq.txt citations.G.100+.log10_page_rank.txt > citations.G.100+.stats.txt
awk 'NR > 1 {print rand() "\t" $0}' citations.G.100+.stats.txt | sort | sed 1000q | cut -f2- > citations.G.100+.stats.txt.sample


edir=/work/k.church/semantic_scholar/embeddings
cd /work/k.church/semantic_scholar/citations/graphs
in=citations.G.100+.stats.txt.sample1
out=citations.G.100+.stats.txt.sample1.specter
sbatch --mem=400G -p short -t 1209 -o $out -e $out.err -i $in  $edir/M3/semantic_scholar_embedding_near.py -M $edir/embeddings.w2v.bin.annoy -q terms -N 100 --prefix X
out=citations.G.100+.stats.txt.sample1.ProNE.K200
sbatch --mem=100G -p short -t 1209 -o $out -e $out.err -i $in $edir/M3/semantic_scholar_embedding_near.py -M citations.G.shrink.new.T1.G2.npz.tasks.1.K200.ProNE.K200.T20.O5.w2v.filtered.annoy -q terms -N 100 --map_node_names citations.G.shrink.new.T1.npz

cd /work/k.church/semantic_scholar/citations/graphs
f=citations.G.shrink.new.T1.G2.npz.tasks.1.K250.ProNE.K250.T20.O5.w2v
sbatch -o $f.out -e $f.err --mem=1000G -p short -t 1209 /work/k.church/semantic_scholar/jobs/npz_to_annoy.sh $f.npz 250

egrep ^27747638 citations.G.100+.stats.txt.sample1.ProNE.K200 citations.G.100+.stats.txt.sample1.specter |
cut -f2- -d: | tr '\t' '\n' | cut -f1 -d: | tr -d X | sort -u | awk '{print 27747638 "\t" $1}' | awk '$1 != $2' > /scratch/k.church/semantic_scholar/citations/graphs/experiment2/pairs

gdir=/work/k.church/semantic_scholar/citations/graphs
edir=/work/k.church/semantic_scholar/embeddings
cd /scratch/k.church/semantic_scholar/citations/graphs/experiment2
in=pairs
out=$in.specter
sbatch --mem=400G -p short -t 1209 -o $out -e $out.err -i $in  $edir/M3/semantic_scholar_embedding_near.py -M $edir/embeddings.w2v.bin.annoy -q pairs --prefix X
out=$in.ProNE.K200
sbatch --mem=100G -p short -t 1209 -o $out -e $out.err -i $in $edir/M3/semantic_scholar_embedding_near.py -M $gdir/citations.G.shrink.new.T1.G2.npz.tasks.1.K200.ProNE.K200.T20.O5.w2v.filtered.annoy -q pairs --map_node_names $gdir/citations.G.shrink.new.T1.npz

gdir=/work/k.church/semantic_scholar/citations/graphs
edir=/work/k.church/semantic_scholar/embeddings
cd /scratch/k.church/semantic_scholar/citations/graphs/experiment2
in=pairs
out=$in.ProNE.K250
sbatch --mem=100G -p short -t 1209 -o $out -e $out.err -i $in $edir/M3/semantic_scholar_embedding_near.py -M $gdir/citations.G.shrink.new.T1.G2.npz.tasks.1.K250.ProNE.K250.T20.O5.w2v.bin.annoy -q pairs --map_node_names $gdir/citations.G.shrink.new.T1.npz --prefix X



gdir=/work/k.church/semantic_scholar/citations/graphs
edir=/work/k.church/semantic_scholar/embeddings
cd /work/k.church/semantic_scholar/citations/graphs
in=/scratch/k.church/semantic_scholar/citations/graphs/experiment3.kwc/papers.txt
out=$in.specter
sbatch --mem=400G -p short -t 1209 -o $out -e $out.err -i $in  $edir/M3/semantic_scholar_embedding_near.py -M $edir/embeddings.w2v.bin.annoy -q terms -N 100 --prefix X
out=$in.ProNE.K200
sbatch --mem=100G -p short -t 1209 -o $out -e $out.err -i $in $edir/M3/semantic_scholar_embedding_near.py -M citations.G.shrink.new.T1.G2.npz.tasks.1.K200.ProNE.K200.T20.O5.w2v.filtered.annoy -q terms -N 100 --map_node_names citations.G.shrink.new.T1.npz


gdir=/work/k.church/semantic_scholar/citations/graphs
edir=/work/k.church/semantic_scholar/embeddings
cd /work/k.church/semantic_scholar/citations/graphs
in=/scratch/k.church/semantic_scholar/citations/graphs/experiment3.kwc/papers.txt
out=$in.ProNE.K250.V3
sbatch --mem=200G -p short -t 1209 -o $out -e $out.err -i $in $edir/M3/semantic_scholar_embedding_near.py -M citations.G.shrink.new.T1.G2.npz.tasks.1.K250.ProNE.K250.T20.O5.w2v.bin.annoy -q terms -N 200 --map_node_names citations.G.shrink.new.T1.npz --prefix X

cd /scratch/k.church/semantic_scholar/citations/graphs/experiment3.kwc
awk '{for(i=2;i<=NF;i++) print $1 "\t" $i}' papers.txt.ProNE.K200 papers.txt.specter |
cut -f1 -d: |
 tr -d X | 
sort -u | 
awk '$1 != $2' > pairs

gdir=/work/k.church/semantic_scholar/citations/graphs
edir=/work/k.church/semantic_scholar/embeddings
cd /scratch/k.church/semantic_scholar/citations/graphs/experiment3.kwc
in=pairs
out=$in.specter
sbatch --mem=400G -p short -t 1209 -o $out -e $out.err -i $in  $edir/M3/semantic_scholar_embedding_near.py -M $edir/embeddings.w2v.bin.annoy -q pairs --prefix X
out=$in.ProNE.K200
sbatch --mem=100G -p short -t 1209 -o $out -e $out.err -i $in $edir/M3/semantic_scholar_embedding_near.py -M $gdir/citations.G.shrink.new.T1.G2.npz.tasks.1.K200.ProNE.K200.T20.O5.w2v.filtered.annoy -q pairs --map_node_names $gdir/citations.G.shrink.new.T1.npz

gdir=/work/k.church/semantic_scholar/citations/graphs
edir=/work/k.church/semantic_scholar/embeddings
cd /scratch/k.church/semantic_scholar/citations/graphs/experiment3.kwc
in=pairs
out=$in.ProNE.K250.V2
sbatch --mem=200G -p short -t 1209 -o $out -e $out.err -i $in $edir/M3/semantic_scholar_embedding_near.py -M $gdir/citations.G.shrink.new.T1.G2.npz.tasks.1.K250.ProNE.K250.T20.O5.w2v.bin.annoy -q pairs --map_node_names $gdir/citations.G.shrink.new.T1.npz --prefix X


echo 1203065 `egrep  1203065 papers*specter | cut -f1` |  tr -d X | tr ' \t' '\n'  | cut -f1 -d: | uniq | head | /work/k.church/semantic_scholar/papers/find_lines.py /work/k.church/semantic_scholar/papers/papers2url/lines2html
1203065	<href="https://www.semanticscholar.org/paper/8a3591b7af3bca0658fe5d9404b1a69d5dfa2918">Word sense disambiguation: why statistics when we have these numbers?</a>
131886	<href="https://www.semanticscholar.org/paper/0fa28d8a77e438d58b3d3670028b4e4f5380732b">One Sense Per Discourse</a>
17567112	<href="https://www.semanticscholar.org/paper/bf35ed0864ff6cf524a24f0a65aa6951f9d6f214">A method for disambiguating word senses in a large corpus</a>
2783947	<href="https://www.semanticscholar.org/paper/553fa529ba615e4bddea81e9a231ae19d5a870a4">Estimating Upper and Lower Bounds on the Performance of Word-Sense Disambiguation Programs</a>

cd /scratch/k.church/semantic_scholar/citations/graphs/experiment3.kwc
pr=/work/k.church/semantic_scholar/citations/graphs/citations.G.page_rank.scikit.npy
freq=/work/k.church/semantic_scholar/citations/graphs/citations.G.fanin.axis1.npy
aref=/work/k.church/semantic_scholar/jobs/aref.py
python $aref $pr $freq < pairs | 
awk '{print $1, $2, log($3)/log(10), $4, log($5)/log(10), $6}' OFS="\t" | 
paste pairs.specter pairs.ProNE.K200 - |
awk 'BEGIN {print "x	y	specter	x	y	ProNE	x	y	x.pagerank	x.cite	y.pagerank	y.cite"}; {print}' > pairs.combo


in=/work/k.church/semantic_scholar/citations/graphs/citations.G.100+.stats.txt.sample1

citedir=/scratch/k.church/semantic_scholar/citations/graphs/tofile
in=/scratch/k.church/semantic_scholar/citations/graphs/experiment3.kwc/pairs
$src/C/cocitations $citedir/citations.G.transpose.[XY].int  < $in > $in.citations

paste pairs.ProNE.K200 pairs.ProNE.K250.V2 | awk 'NF == 6 {print ($6 - $3)/($6 + $3) "\t" $0}' | sort -gr


# 250 stuff

gdir=/work/k.church/semantic_scholar/citations/graphs
edir=/work/k.church/semantic_scholar/embeddings
in=/scratch/k.church/semantic_scholar/citations/graphs/experiment3.kwc/pairs
out=$in.ProNE.K250.V4
annoy=$gdir/citations.G.shrink.new.T1.G2.npz.tasks.1.K250.ProNE.K250.T20.O5.w2v.bin.annoy
near=$edir/M3/semantic_scholar_embedding_near.py
map=$gdir/citations.G.shrink.new.T1.npz
sbatch --mem=200G -p short -t 1209 -o $out -e $out.err -i $in $near -M $annoy -q pairs --map_node_names $map --prefix X

gdir=/work/k.church/semantic_scholar/citations/graphs
edir=/work/k.church/semantic_scholar/embeddings
in=/scratch/k.church/semantic_scholar/citations/graphs/experiment3.kwc/papers.txt
out=$in.ProNE.K250.V4
sbatch --mem=200G -p short -t 1209 -o $out -e $out.err -i $in $near -M $annoy -q terms -N 200 --map_node_names $map --prefix X

# don't do this
# see /scratch/k.church/semantic_scholar/citations/graphs/experiment4/notes.txt
#
# cd /scratch/k.church/semantic_scholar/citations/graphs/experiment4/
# sbatch --mem=150G --array 0-99 -t 59 -p express,short near_job.sh sample1

# Killroy was HERE

cd /work/k.church/semantic_scholar/citations/graphs
# B=20
# B=40
B=20
for shard in `seq 30 79`
do
for K in 200 # 100 200 300
do
for f in *K$K*kwc.edges.f
do
out=/scratch/k.church/semantic_scholar/citations/graphs/K200/shard$shard/`basename $f .f`.$B.i
mkdir -p `dirname $out`
sbatch -p short -t 1209 -i $f -o $out $src/C/floats_to_random_bytes.sh $K $B
done
done
done

f=citations.G.shrink.new.T1.G2.npz.tasks.1.K100.ProNE.K100.T20.O5.w2v.kwc.edges.16.i
index_random_bytes $f 16  > $f.idx


citations.G.shrink.new.T1.G2.npz.tasks.1.K100.ProNE.K100.T20.O5.w2v.kwc.edges.20.i
citations.G.shrink.new.T1.G2.npz.tasks.1.K200.ProNE.K200.T20.O5.w2v.kwc.edges.20.i
citations.G.shrink.new.T1.G2.npz.tasks.1.K300.ProNE.K300.T20.O5.w2v.kwc.edges.20.i

cd /work/k.church/semantic_scholar/citations/graphs
for B in 40 # 16 20 # 40
do
for K in 200 # 100 200 # 300
do
for f in *K$K*kwc.edges.$B.i
do
out=$f.idx
# if [ ! -s $out ]
# then
echo working on $out
sbatch --array 21-40 -p express -t 59 $src/C/index_random_bytes.sh $f $B
# fi
done
done
done

# This is now covered by above
#
# cd /work/k.church/semantic_scholar/citations/graphs
# for B in 40 # 16 20 # 40
# do
# for K in 200 # 100 200 300
# do
# for f in *K$K*kwc.edges.$B.i
# do
# out=$f.idx
# echo working on $out
# sbatch --array 21-40 -p debug,express,short $src/C/print_random_bytes.sh $f $B
# done
# done
# done





ifile=citations.G.shrink.new.T1.G2.npz.tasks.1.K100.ProNE.K100.T20.O5.w2v.kwc.edges.20.i
ffile=citations.G.shrink.new.T1.G2.npz.tasks.1.K100.ProNE.K100.T20.O5.w2v.kwc.edges.f
calibrate_random_bytes_index $ifile $ifile.idx.1 20 $ffile 100

ifile=citations.G.shrink.new.T1.G2.npz.tasks.1.K200.ProNE.K200.T20.O5.w2v.kwc.edges.40.i
ffile=citations.G.shrink.new.T1.G2.npz.tasks.1.K200.ProNE.K200.T20.O5.w2v.kwc.edges.f
calibrate_random_bytes_index $ifile $ifile.idx.1 40 $ffile 200 --binary > $ifile.idx.1.calibrated


print_random_bytes_index citations.G.shrink.new.T1.G2.npz.tasks.1.K100.ProNE.K100.T20.O5.w2v.kwc.edges.16.i 16 | cut -c 8- | sed 1000000q | sort | uniq -c  | awk '{print $1}' | sort -n | uniq -c

print_random_bytes_index citations.G.shrink.new.T1.G2.npz.tasks.1.K100.ProNE.K100.T20.O5.w2v.kwc.edges.16.i 16 | awk 'NF == 2 {print $2}' | sed 10000000q | sort -n | uniq -c 
 121926 0
  58653 3
  57219 4
  57019 2
  54587 5
  51425 6
  50936 1
  47489 7
  44152 8
  40860 9

print_random_bytes_index $ifile $ifile.idx.pi 20  | sed 1000q | tail

ffile=citations.G.shrink.new.T1.G2.npz.tasks.1.K200.ProNE.K200.T20.O5.w2v.kwc.edges.f
ifile=citations.G.shrink.new.T1.G2.npz.tasks.1.K200.ProNE.K200.T20.O5.w2v.kwc.edges.20.i
print_random_bytes_index $ifile $ifile.idx.4 20 --binary > $ifile.idx.4.hamming_dist

cd /work/k.church/semantic_scholar/citations/graphs
for f in *.hamming_dist
do
header=`echo $f | awk -F. '{print "B" $(NF-4) "." $(NF-1)}'`
sbatch -p debug,express,short hist_bytes.py -i $f -o $f.txt -H $header -t uint8
done


# x and xinv are inverses of one another (in both directions)
import numpy as np
f=open('citations.G.shrink.new.T1.G2.npz.tasks.1.K200.ProNE.K200.T20.O5.w2v.kwc.edges.40.i.idx.1', 'rb')
x=np.frombuffer(f.read(),dtype=int)

f=open('citations.G.shrink.new.T1.G2.npz.tasks.1.K200.ProNE.K200.T20.O5.w2v.kwc.edges.40.i.idx.1.inv', 'rb')
xinv=np.frombuffer(f.read(),dtype=int)
xinv[x[np.arange(10)]]
array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])

x[xinv[np.arange(10)]]
array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])

cd /work/k.church/semantic_scholar/citations/graphs
for f in *.idx.[0-9][0-9]
do
if [ ! -s $f.inv ]
then
echo working on $f
sbatch -o $f.inv -p debug,express,short invert_permutation.sh $f
fi
done

cd /work/k.church/semantic_scholar/citations/graphs
map=citations.G.shrink.new.T1.npz
ifile=citations.G.shrink.new.T1.G2.npz.tasks.1.K200.ProNE.K200.T20.O5.w2v.kwc.edges.f
echo 246638887 | map_paper_ids.py $map old_to_new
echo 246638887 | map_paper_ids.py $map old_to_new | cut -f2 | near $ifile.idx.?

echo 9558665 | map_paper_ids.py $map old_to_new | cut -f2 | near $ifile.idx.? | 
cut -f2- |
tr ' \t' '\n' | cut -f1 -d: | map_paper_ids.py $map new_to_old  | cut -f2 | /work/k.church/semantic_scholar/papers/find_lines.py /work/k.church/semantic_scholar/papers/papers2url/lines2html

echo 16556807| map_paper_ids.py $map old_to_new | cut -f2 | near $ifile.idx.? | 
cut -f2- |
tr ' \t' '\n' | cut -f1 -d: | map_paper_ids.py $map new_to_old  | cut -f2 | /work/k.church/semantic_scholar/papers/find_lines.py /work/k.church/semantic_scholar/papers/papers2url/lines2html

# need to finish calibrate_random_bytes

echo '12380450	53943989' | calibrate_random_bytes $ifile 40 $ffile 200

head /scratch/k.church/semantic_scholar/citations/graphs/experiment4/pairs/pairs.045 | tr '\t' '\n' | map_paper_ids.py $map old_to_new > /tmp/x
cut -f2 /tmp/x |  calibrate_random_bytes $ifile 40 $ffile 200
57808715	56449758	17	0.957
57808715	8529251	35	0.796
57808715	70224204	20	0.956
57808715	70584573	14	0.966
57808715	70783952	15	0.960
57808715	70826530	16	0.953
57808715	70864147	28	0.860
57808715	70925652	16	0.963
57808715	70926363	18	0.955
57808715	71033426	28	0.869


cat /scratch/k.church/semantic_scholar/citations/graphs/experiment4/pairs/pairs.045 |
tr '\t' '\n' | 
map_paper_ids.py $map old_to_new |
cut -f2 |
calibrate_random_bytes $ifile 40 $ffile 200 > /scratch/k.church/semantic_scholar/citations/graphs/experiment4/pairs/pairs.045.calibrate_random_bytes

paste /scratch/k.church/semantic_scholar/citations/graphs/experiment4/pairs/pairs.045.K200.V2 /scratch/k.church/semantic_scholar/citations/graphs/experiment4/pairs/pairs.045.calibrate_random_bytes > /tmp/x


echo 57808715	71975321:25	70359900:18	71169284:10	71019421:25	91599778:31	70361220:19	32415215:18	71158183:22	71658488:20	27067210:58	70812206:12	70970954:15	70545941:19	89607944:26	68618361:25	72082482:21 | awk '{for(i=2;i<=NF;i++) print $1, $i}' | cut -f1 -d: | calibrate_random_bytes $ifile 40 $ffile 200 

cd /work/k.church/semantic_scholar/citations/graphs
ifile=citations.G.shrink.new.T1.G2.npz.tasks.1.K200.ProNE.K200.T20.O5.w2v.kwc.edges.40.i
ffile=citations.G.shrink.new.T1.G2.npz.tasks.1.K200.ProNE.K200.T20.O5.w2v.kwc.edges.f
echo '12380450	53943989' | calibrate_random_bytes 200 40 $ffile $ifile /scratch/k.church/semantic_scholar/citations/graphs/K200/shard*/*.i


f=/scratch/k.church/semantic_scholar/citations/graphs/experiment4/pairs/pairs.045.calibrate_random_bytes
cut -f1,2 < $f | 
calibrate_random_bytes 200 20 $ffile /scratch/k.church/semantic_scholar/citations/graphs/K200/shard*/*.i > $f.V3


cd /scratch/k.church/semantic_scholar/citations/graphs/K200
cbind_bytes 20 shard*/citations.G.shrink.new.T1.G2.npz.tasks.1.K200.ProNE.K200.T20.O5.w2v.kwc.edges.20.i > citations.G.shrink.new.T1.G2.npz.tasks.1.K200.ProNE.K200.T20.O5.w2v.kwc.edges.1600.i

job1=`sbatch -t 1209 -p short --array 0-39 cbind_bytes.sh 20 40 shard /citations.G.shrink.new.T1.G2.npz.tasks.1.K200.ProNE.K200.T20.O5.w2v.kwc.edges. | awk '{print $NF}'`
job2=`sbatch -t 1209 -d afterany:$job1 -p short --array 0-19 cbind_bytes.sh 40 20 shard /citations.G.shrink.new.T1.G2.npz.tasks.1.K200.ProNE.K200.T20.O5.w2v.kwc.edges. | awk '{print $NF}'`
job3=`sbatch -t 1209 -d afterany:$job2 -p short --array 0-9 cbind_bytes.sh 80 10 shard /citations.G.shrink.new.T1.G2.npz.tasks.1.K200.ProNE.K200.T20.O5.w2v.kwc.edges. | awk '{print $NF}'`
job4=`sbatch -t 1209 -d afterany:$job3 -p short --array 0-4 cbind_bytes.sh 160 5 shard /citations.G.shrink.new.T1.G2.npz.tasks.1.K200.ProNE.K200.T20.O5.w2v.kwc.edges. | awk '{print $NF}'`
job5=`sbatch -t 1209 -d afterany:$job4 -p short --array 0-1 cbind_bytes.sh 320 2 shard /citations.G.shrink.new.T1.G2.npz.tasks.1.K200.ProNE.K200.T20.O5.w2v.kwc.edges. | awk '{print $NF}'`
job6=`sbatch -t 1209 -d afterany:$job5 -p short --array 0   cbind_bytes.sh 640 1 shard /citations.G.shrink.new.T1.G2.npz.tasks.1.K200.ProNE.K200.T20.O5.w2v.kwc.edges. | awk '{print $NF}'`

# cd /work/k.church/semantic_scholar/citations/graphs
cd /scratch/k.church/semantic_scholar/citations/graphs/K200
for B in 20 40 80 160 320 
do
out=merged/10min/from/citations.G.shrink.new.T1.G2.npz.tasks.1.K200.ProNE.K200.T20.O5.w2v.kwc.edges.$B.i/1600.i
mkdir -p `dirname $out`
sbatch --constraint=ib -e $out.err -o $out -t 7199 -p long $src/C/cbind_bytes_without_array_jobs.sh $B /scratch/k.church/semantic_scholar/citations/graphs/K200/shard*/citations.G.shrink.new.T1.G2.npz.tasks.1.K200.ProNE.K200.T20.O5.w2v.kwc.edges.$B.i
done

# C program is here: /work/k.church/semantic_scholar/citations/graphs/src/C/cbind_bytes.c
# The binary is in the same directory (that is in my PATH)
# SLURM JOB IDs are in: find merged/10min/ -name '*.err'
src=/work/k.church/semantic_scholar/citations/graphs/src
cd /scratch/k.church/semantic_scholar/citations/graphs/K200
for B in 20 40 80 160 320 
do
out=merged/10min/from/citations.G.shrink.new.T1.G2.npz.tasks.1.K200.ProNE.K200.T20.O5.w2v.kwc.edges.$B.i/1600.i
mkdir -p `dirname $out`
sbatch --constraint=ib -e $out.err -o $out -t 10 -p debug $src/C/cbind_bytes_without_array_jobs.sh $B /scratch/k.church/semantic_scholar/citations/graphs/K200/shard*/citations.G.shrink.new.T1.G2.npz.tasks.1.K200.ProNE.K200.T20.O5.w2v.kwc.edges.$B.i
done

for B in 20 40 80 160 320 
do
echo $B `ls /scratch/k.church/semantic_scholar/citations/graphs/K200/shard*/citations.G.shrink.new.T1.G2.npz.tasks.1.K200.ProNE.K200.T20.O5.w2v.kwc.edges.$B.i | wc -l`
done


# C program is here: /work/k.church/semantic_scholar/citations/graphs/src/C/cbind_bytes.c
# The binary is in the same directory (that is in my PATH)
# SLURM JOB IDs are in: find merged/10min/ -name '*.err'
src=/work/k.church/semantic_scholar/citations/graphs/src
cd /scratch/k.church/semantic_scholar/citations/graphs/K200
for B in 20 40 80 160 320 
do
out=merged/10min/V5/from/citations.G.shrink.new.T1.G2.npz.tasks.1.K200.ProNE.K200.T20.O5.w2v.kwc.edges.$B.i/1600.i
mkdir -p `dirname $out`
sbatch --exclusive --constraint=ib,zen2 -e $out.err -o $out -t 10 -p debug,express,short,long $src/C/cbind_bytes_without_array_jobs.sh $B /scratch/k.church/semantic_scholar/citations/graphs/K200/shard*/citations.G.shrink.new.T1.G2.npz.tasks.1.K200.ProNE.K200.T20.O5.w2v.kwc.edges.$B.i
done


cd /scratch/k.church/semantic_scholar/citations/graphs/K200/shard0
for f in *.hamming_dist16
do
header=`echo $f | awk -F. '{print "B" $(NF-4) "." $(NF-1)}'`
sbatch -p debug,express,short hist_bytes.py -i $f -o $f.txt -H $header -t uint16
done




for B in 20 40 80 160 320 
do
echo $B `ls /scratch/k.church/semantic_scholar/citations/graphs/K200/shard*/citations.G.shrink.new.T1.G2.npz.tasks.1.K200.ProNE.K200.T20.O5.w2v.kwc.edges.$B.i | wc -l`
done






K=200
for B in 1280 # 640 # 20 40 80 160 320
do
f=/scratch/k.church/semantic_scholar/citations/graphs/experiment4/pairs/pairs.045.calibrate_random_bytes
ffile=/work/k.church/semantic_scholar/citations/graphs/K200/citations.G.shrink.new.T1.G2.npz.tasks.1.K200.ProNE.K200.T20.O5.w2v.kwc.edges.f
# ffile=/work/k.church/semantic_scholar/citations/graphs/citations.G.shrink.new.T1.G2.npz.tasks.1.K200.ProNE.K200.T20.O5.w2v.kwc.edges.f
cut -f1,2 < $f | 
calibrate_random_bytes $K $B $ffile /scratch/k.church/semantic_scholar/citations/graphs/K200/shard*/*".$B."i > $f.V6.$B &
done

for B in 20 40 80 160 320
do
ls -l /scratch/k.church/semantic_scholar/citations/graphs/K200/shard*/*.$B.i | awk '{x += $5}; END {print B, x/1e9, NR}' B=$B
done

# job5=`sbatch -t 1209  -p short --array 0-1 cbind_bytes.sh 320 2 shard /citations.G.shrink.new.T1.G2.npz.tasks.1.K200.ProNE.K200.T20.O5.w2v.kwc.edges. | awk '{print $NF}'`
# job6=`sbatch -t 1209 -d afterany:$job5 -p short --array 0   cbind_bytes.sh 640 1 shard /citations.G.shrink.new.T1.G2.npz.tasks.1.K200.ProNE.K200.T20.O5.w2v.kwc.edges. | awk '{print $NF}'`


cd /scratch/k.church/semantic_scholar/citations/graphs/K200/shard0
for f in *.320.i *.640.i *.1280.i
do
B=`echo $f | awk -F. '{print $(NF-1)}'`
echo working on B = $B and f = $f
sbatch --array 11-40 -p short -t 1209 $src/C/index_random_bytes.sh $f $B
done

cd /scratch/k.church/semantic_scholar/citations/graphs/K200/shard0
for f in *.20.i # *.320.i *.640.i *.1280.i
do
B=`echo $f | awk -F. '{print $(NF-1)}'`
echo working on B = $B and f = $f
# sbatch --array 11-40 -p short -t 1209 $src/C/index_random_bytes.sh $f $B
# sbatch --array 11 -p short -t 1209 $src/C/index_random_bytes.sh $f $B
sbatch --array 12 -p short -t 1209 $src/C/index_random_bytes.sh $f $B
done



cd /work/k.church/semantic_scholar/citations/graphs
map=/work/k.church/semantic_scholar/citations/graphs/citations.G.shrink.new.T1.npz

echo 16556807|  /work/k.church/semantic_scholar/papers/find_lines.py /work/k.church/semantic_scholar/papers/papers2url/lines2html


echo 111159811 |  /work/k.church/semantic_scholar/papers/find_lines.py /work/k.church/semantic_scholar/papers/papers2url/lines2html
111159811	<href="https://www.semanticscholar.org/paper/e55eed890cd9ba7fcbfb35062a0d47b370a8c699">Intermetallic compounds at aluminum-to-copper electrical interfaces: effect of temperature and electric current</a>

echo 111159811 | map_paper_ids.py $map old_to_new
111159811	57808715

ifile=citations.G.shrink.new.T1.G2.npz.tasks.1.K200.ProNE.K200.T20.O5.w2v.kwc.edges.160.i
echo 111159811 | map_paper_ids.py $map old_to_new | cut -f2 | near $ifile.idx.?? | 
cut -f2- |
tr ' \t' '\n' | cut -f1 -d: | map_paper_ids.py $map new_to_old  | cut -f2 | /work/k.church/semantic_scholar/papers/find_lines.py /work/k.church/semantic_scholar/papers/papers2url/lines2html

54206350	<href="https://www.semanticscholar.org/paper/ee7512178e7dbf6087d979a8dc074fc59a4021a0">On the Problem of Novel Composite Materials Development for Car Brake Rotor</a>
56225324	<href="https://www.semanticscholar.org/paper/e416e7c0e081655f0d4fd7e70694b5ec2b95f8a6">Interface characterization of Al/Cu 2-ply composites under various loading conditions</a>
140022435	<href="https://www.semanticscholar.org/paper/b110448df44dcb78c16a01c3ede8f0ab26b2dfd1">Laser Welding Characteristic of Dissimilar Metal for Aluminum to Steel</a>
105949362	<href="https://www.semanticscholar.org/paper/2c3a34d2c468c551181d0fc38a83c8ef2ac5284b">Structure and properties of detonation gun sprayed coatings from the synthesized FeAlSi/Al2O3 powder</a>
139603231	<href="https://www.semanticscholar.org/paper/20e7cfe1dfe600121177609784039ec0ebb12fc1">Specific Features of Wear of Oil-Scraper Piston Rings with Tin Coatings in Bench Tests for Friction and Wear</a>
10106501	<href="https://www.semanticscholar.org/paper/3488a6c9dc10c0316eb4396dc8b1c0cd5788b1e7">Creep behavior and in-depth microstructural characterization of dissimilar joints</a>
210800686	<href="https://www.semanticscholar.org/paper/868929a7a4025735792249949c3b4fc203341ccc">Effect of annealing method and applied stress on aging behavior of copper-aluminum bimetals</a>
135581678	<href="https://www.semanticscholar.org/paper/1e15926b10f3b9c191ce89ed1502e451de7db9f0">Investigations on microstructures and properties of B containing cast steel for wear resistance applications</a>
232832207	<href="https://www.semanticscholar.org/paper/1d81c4dee997d8fe81ba6d96f68d342ef9f28f83">Preparation of an Al–Ti–Mg Composite by Self-Propagating High-Temperature Synthesis</a>
54069140	<href="https://www.semanticscholar.org/paper/e60d09a5fe1e0c1885e365260b31eded22e9f2b6">Effect of Addition of Small Amount of Zinc on Microstructural Evolution and Thermal Shock Behavior in Low-Ag Sn-Ag-Cu Solder Joints during Thermal Cycling</a>
37400037	<href="https://www.semanticscholar.org/paper/546c6e6157076a8807a14c89ade6e1b971c090d9">Solid state reaction of Sn3.0Ag0.5Cu solder with Cu(Mn) under bump metallization</a>
10621232	<href="https://www.semanticscholar.org/paper/9bae737031c54531fdc6455ee092790118e073c9">Effect of Ni on the formation of Cu/sub 6/Sn/sub 5/ and Cu/sub 3/Sn intermetallics</a>
110078815	<href="https://www.semanticscholar.org/paper/d237ae9de575a8e132fc60cdf3b02cf674237fef">Effect of Neodymium Magnet Field on Operation of Inductive Watt-Hour Meters</a>
55381677	<href="https://www.semanticscholar.org/paper/c0b179246156663e02b206b2e9874a4f1aa9ce08">Effects of Fe—Al intermetallic compounds on interfacial bonding of clad materials</a>
97009033	<href="https://www.semanticscholar.org/paper/e74eab618990ef77cc69fe4b94f36343255fed6a">Mechanical Properties and Electrochemical Corrosion Behavior of Al/Sn-9Zn-xAg/Cu Joints</a>
213125395	<href="https://www.semanticscholar.org/paper/f1c118977c3d0616c91bb37f6a9a683a810d4ad4">Investigation of different failure modes in oil and natural gas pipeline steels</a>
1302086	<href="https://www.semanticscholar.org/paper/cd30783c414d57a5a1c545dd671650f62893e71d">Aberrant epigenetic regulation in clear cell sarcoma of the kidney featuring distinct DNA hypermethylation and EZH2 overexpression</a>
225219682	<href="https://www.semanticscholar.org/paper/edae1afa32c981e73dc018b4259e4cadbe0c0927">Experimental research of the effect of face milling strategy on the flatness deviations</a>
137139613	<href="https://www.semanticscholar.org/paper/137c01e89eef820f8a253a41bc43dc562ff05987">Sinterability and microstructure evolution during sintering of ferrous powder mixtures</a>
136926620	<href="https://www.semanticscholar.org/paper/86e3f7dfcf44aea15a07f79134b3efcf75b12a5c">Mechanical properties and microstructures of hybrid ultrasonic resistance brazing of WC-Co/BeCu</a>
94857184	<href="https://www.semanticscholar.org/paper/42ba237428bdc0dd145fd82d3466c1629879b676">Nanoindentation characterization of intermetallic compounds formed between Sn-Cu (-Ni) ball grid arrays and Cu substrates</a>
239663187	<href="https://www.semanticscholar.org/paper/e9ec4910d17e7fb8a26a434059434c23dfcb4589">RETRACTED ARTICLE: Modelling and simulation of surfacing welding remanufacturing for tunnel boring machine disc cutter</a>
32385133	<href="https://www.semanticscholar.org/paper/71eab00c869da8f31e64e4289167cd171d1471bc">Effect of in addition on the reaction and mechanical properties in Sn-Ag-Cu-In solder alloy</a>
136527661	<href="https://www.semanticscholar.org/paper/4da20d0a95b89e41f172e8cae67aa14765144ce8">Effects of intermetallic compound on the electrical and mechanical properties of friction welded Cu/Al bimetallic joints during annealing</a>
199408596	<href="https://www.semanticscholar.org/paper/c169b2161ae6c5acebaf85374585043b47af981f">Effect of a Zinc Interlayer on a Cu/Al Lap Joint in Ultrasound-Assisted Friction Stir Welding</a>
108995362	<href="https://www.semanticscholar.org/paper/adf3c17953a6454c4cd35f05fcea3c84dc33b712">Effect of electrical current on the morphology and kinetics of formation of intermetallic phases in bimetallic aluminum-copper joints</a>
96577960	<href="https://www.semanticscholar.org/paper/7f1f3a041eb21e1b26cfd5be519607bf99f3939e">Structural analysis of electron beam irradiated steel/molybdenum systems</a>
135805798	<href="https://www.semanticscholar.org/paper/e689d8e54af034665cad4d9b7a5200df5e660c1c">Brittleness study of intermetallic (Cu, Al) layers in copper-clad aluminium thin wires</a>
137348936	<href="https://www.semanticscholar.org/paper/181de479ef2a66dabffd5e46dfd8bfbef3ffffa5">An investigation of the wear behaviours of white cast irons under different compositions</a>
234407522	<href="https://www.semanticscholar.org/paper/883024172e98ebefec0bd6615e7d6da1fb5530dc">Gibbs-Thompson Effect as Driving Force for Liquid Film Migration</a>

echo 111159811 | map_paper_ids.py $map old_to_new | cut -f2 | near $ifile.idx.??  > /tmp/x

ffile=/work/k.church/semantic_scholar/citations/graphs/K200/citations.G.shrink.new.T1.G2.npz.tasks.1.K200.ProNE.K200.T20.O5.w2v.kwc.edges.f
# ffile=/work/k.church/semantic_scholar/citations/graphs/citations.G.shrink.new.T1.G2.npz.tasks.1.K200.ProNE.K200.T20.O5.w2v.kwc.edges.f
awk '{for(i=2;i<=NF;i++) print $1, $i}' /tmp/x | cut -f1 -d: | 
calibrate_random_bytes 200 160 $ffile $ifile.idx.??

cd /scratch/k.church/semantic_scholar/citations/graphs/K200/shard0
mkdir hists
for f in *.hamming_dist
do
header=`echo $f | awk -F. '{print "B" $(NF-4) "." $(NF-1)}'`
sbatch -p debug,express,short hist_bytes.py -i $f -o hists/$f.txt -H $header
done


cd /scratch/k.church/semantic_scholar/citations/graphs/K200/merged
for f in `find . -name '*.hamming_dist'`
do
header=`echo $f | awk -F. '{print "B" $(NF-4) "." $(NF-1)}'`
sbatch -p debug,express,short hist_bytes.py -i $f -o $f.txt -H $header
done


K=200
for B in  20 40 80 160 320 # 1280 # 640 #
do
f=/scratch/k.church/semantic_scholar/citations/graphs/experiment4/pairs/pairs.046
ffile=/work/k.church/semantic_scholar/citations/graphs/K200/citations.G.shrink.new.T1.G2.npz.tasks.1.K200.ProNE.K200.T20.O5.w2v.kwc.edges.f
#ffile=/work/k.church/semantic_scholar/citations/graphs/citations.G.shrink.new.T1.G2.npz.tasks.1.K200.ProNE.K200.T20.O5.w2v.kwc.edges.f
cut -f1,2 < $f | 
egrep -v NA |
tr '\t' '\n' | 
map_paper_ids.py $map old_to_new |
cut -f2 |
calibrate_random_bytes $K $B $ffile /scratch/k.church/semantic_scholar/citations/graphs/K200/shard*/*".$B."i > $f.calibrate_random_bytes.V6.$B &
done

K=200
map=/work/k.church/semantic_scholar/citations/graphs/citations.G.shrink.new.T1.npz
ffile=/work/k.church/semantic_scholar/citations/graphs/K200/citations.G.shrink.new.T1.G2.npz.tasks.1.K200.ProNE.K200.T20.O5.w2v.kwc.edges.f
# ffile=/work/k.church/semantic_scholar/citations/graphs/citations.G.shrink.new.T1.G2.npz.tasks.1.K200.ProNE.K200.T20.O5.w2v.kwc.edges.f
f=/scratch/k.church/semantic_scholar/citations/graphs/experiment4/pairs/pairs.046
cut -f1,2 < $f | 
egrep -v NA |
tr '\t' '\n' | 
map_paper_ids.py $map old_to_new > $f.old_to_new 
cut -f2 < $f.old_to_new  > $f.old_to_new1


for B in  80 160 320 # 1280 # 640 # 20 40 80 
do
ifile=/scratch/k.church/semantic_scholar/citations/graphs/K200/merged/from/citations.G.shrink.new.T1.G2.npz.tasks.1.K200.ProNE.K200.T20.O5.w2v.kwc.edges.$B.i/1600.i
in=$f.old_to_new1
out=/scratch/k.church/semantic_scholar/citations/graphs/K200/merged/from/$B/pairs.046
mkdir -p `dirname $out`
sbatch -p express -t 59 -o $out -i $in calibrate_random_bytes.sh $K $B $ffile $ifile
done

# cd /work/k.church/semantic_scholar/citations/graphs
cd /scratch/k.church/semantic_scholar/citations/graphs/K200
for B in 40 80 160 320 # 20
do
out=/scratch/k.church/semantic_scholar/citations/graphs/K200/merged/from/citations.G.shrink.new.T1.G2.npz.tasks.1.K200.ProNE.K200.T20.O5.w2v.kwc.edges.$B.i/pairs.046
mkdir -p `dirname $out`
sbatch -p express -t 59 -o $out -i $in calibrate_random_bytes.sh $K $B $ffile /scratch/k.church/semantic_scholar/citations/graphs/K200/shard*/citations.G.shrink.new.T1.G2.npz.tasks.1.K200.ProNE.K200.T20.O5.w2v.kwc.edges.$B.i
done

wc /scratch/k.church/semantic_scholar/citations/graphs/K200/merged/from/citations.G.shrink.new.T1.G2.npz.tasks.1.K200.ProNE.K200.T20.O5.w2v.kwc.edges.*.i/pairs.046
   369   4797  20510 /scratch/k.church/semantic_scholar/citations/graphs/K200/merged/from/citations.G.shrink.new.T1.G2.npz.tasks.1.K200.ProNE.K200.T20.O5.w2v.kwc.edges.160.i/pairs.046
   369  30627  78895 /scratch/k.church/semantic_scholar/citations/graphs/K200/merged/from/citations.G.shrink.new.T1.G2.npz.tasks.1.K200.ProNE.K200.T20.O5.w2v.kwc.edges.20.i/pairs.046
   369   2952  15204 /scratch/k.church/semantic_scholar/citations/graphs/K200/merged/from/citations.G.shrink.new.T1.G2.npz.tasks.1.K200.ProNE.K200.T20.O5.w2v.kwc.edges.320.i/pairs.046
   369  15867  49147 /scratch/k.church/semantic_scholar/citations/graphs/K200/merged/from/citations.G.shrink.new.T1.G2.npz.tasks.1.K200.ProNE.K200.T20.O5.w2v.kwc.edges.40.i/pairs.046
   369   8487  30763 /scratch/k.church/semantic_scholar/citations/graphs/K200/merged/from/citations.G.shrink.new.T1.G2.npz.tasks.1.K200.ProNE.K200.T20.O5.w2v.kwc.edges.80.i/pairs.046

for f in /scratch/k.church/semantic_scholar/citations/graphs/K200/merged/from/citations.G.shrink.new.T1.G2.npz.tasks.1.K200.ProNE.K200.T20.O5.w2v.kwc.edges.*.i/pairs.046
do
awk '{x=0; for(i=4;i<=NF;i++) x+=$i; print $1, $2, $3, x}' OFS="\t" $f > $f.summary
done

cd /scratch/k.church/semantic_scholar/citations/graphs/K200
for B in 40 80 160 320 
do
f=./merged/from/citations.G.shrink.new.T1.G2.npz.tasks.1.K200.ProNE.K200.T20.O5.w2v.kwc.edges.$B.i/1600.i
sbatch --constraint=ib,zen2 --mem=300G --array 11-40 -p short -t 1209 $src/C/index_random_bytes.sh $f 1600 200
done

cd /scratch/k.church/semantic_scholar/citations/graphs/K200/merged
find . -name '*.hamming_dist16' |
while read f
do
header=`echo $f | awk -F. '{print "B" $(NF-5) "." $(NF-1)}'`
sbatch -p debug,express,short hist_bytes.py -i $f -o $f.txt -H $header -t uint16
done

sacct -u k.church -o Submit,JobID,State,ExitCode,Elapsed,JobName -S 0720 | egrep index  | sort

cd /scratch/k.church/semantic_scholar/citations/graphs/K200/merged/from/citations.G.shrink.new.T1.G2.npz.tasks.1.K200.ProNE.K200.T20.O5.w2v.kwc.edges.320.i
ffile=/work/k.church/semantic_scholar/citations/graphs/K200/citations.G.shrink.new.T1.G2.npz.tasks.1.K200.ProNE.K200.T20.O5.w2v.kwc.edges.f
# ffile=/work/k.church/semantic_scholar/citations/graphs/citations.G.shrink.new.T1.G2.npz.tasks.1.K200.ProNE.K200.T20.O5.w2v.kwc.edges.f
K=200
B=1600
ifile=1600.i
cat ../../from/80/papers.046 | near 1600.i.idx.??
13272851	3257220:584
14934367	19709719:318
15964310	23596575:366
1639685	12783868:185
19102559	15683235:104
19366710	4949240:407
31426247	47630793:513
61335730	52559795:572
701105	17734378:329
9923524	12241196:287

cat ../../from/80/papers.046 | near 1600.i.idx.?? | cut -f1 -d: | 
calibrate_random_bytes $K $B $ffile $ifile

cat ../../from/80/papers.046 | near --threshold 400 1600.i.idx.?? | cut -f1 -d: | sort -u | 
calibrate_random_bytes $K $B $ffile $ifile

cat ../../from/80/papers.046 | near --threshold 1000 1600.i.idx.?? | cut -f1 -d: | sort -u | calibrate_random_bytes $K $B $ffile $ifile | sort -k3 -nr > /tmp/x

cd /scratch/k.church/semantic_scholar/citations/graphs/K200/merged/from/citations.G.shrink.new.T1.G2.npz.tasks.1.K200.ProNE.K200.T20.O5.w2v.kwc.edges.320.i
T=1000
map=/work/k.church/semantic_scholar/citations/graphs/citations.G.shrink.new.T1.npz
for f in experiment4/sample1.???
do
echo `date` $f
map_paper_ids.py $map old_to_new < $f | cut -f2 |  near --threshold $T 1600.i.idx.?? | cut -f1 -d: | sort -u | calibrate_random_bytes $K $B $ffile $ifile | sort -k3 -nr > $f.near.T$T
done

for day in 14 15 16 17 18 19 20
do
sacct -u k.church -S 07$day --partition long -o ElapsedRaw,JobID,JobName,Submit,Start | egrep dict 
done | sort -u -nr

cd /work/k.church/semantic_scholar/citations/graphs/K375
f=citations.G.shrink.new.T1.G2.npz.ProNE.ProNE.K375.T20.O5.w2v
sbatch --constraint=ib -o $f.out -e $f.err --mem=1000G -p short -t 1209 /work/k.church/semantic_scholar/jobs/npz_to_annoy.sh $f.npz 375

cd /scratch/k.church/semantic_scholar/citations/graphs/K200/merged/from/citations.G.shrink.new.T1.G2.npz.tasks.1.K200.ProNE.K200.T20.O5.w2v.kwc.edges.320.i
T=1000
ffile=/work/k.church/semantic_scholar/citations/graphs/K200/citations.G.shrink.new.T1.G2.npz.tasks.1.K200.ProNE.K200.T20.O5.w2v.kwc.edges.f
K=200
B=1600
ifile=1600.i
map=/work/k.church/semantic_scholar/citations/graphs/citations.G.shrink.new.T1.npz
for f in experiment4/sample1.???
do
echo `date` $f
# map_paper_ids.py $map old_to_new < $f | cut -f2 > $f.new
sbatch -p debug,express,short -i $f.new -o $f.near.details near.sh --show_details 1 ../*/1600.i.idx.??
done


cd /scratch/k.church/semantic_scholar/citations/graphs/K200/merged/from/citations.G.shrink.new.T1.G2.npz.tasks.1.K200.ProNE.K200.T20.O5.w2v.kwc.edges.320.i
T=1000
ffile=/work/k.church/semantic_scholar/citations/graphs/K200/citations.G.shrink.new.T1.G2.npz.tasks.1.K200.ProNE.K200.T20.O5.w2v.kwc.edges.f
K=200
B=1600
ifile=1600.i
map=/work/k.church/semantic_scholar/citations/graphs/citations.G.shrink.new.T1.npz
for B1 in 40 80 160 320 
do
for f in experiment4/sample1.???
do
echo `date` $f
# map_paper_ids.py $map old_to_new < $f | cut -f2 > $f.new
sbatch -p debug,express,short -i $f.new -o $f.near.best.B1.$B1 near.sh --show_details 0 --find_best 1  ../*w2v.kwc.edges.$B1.i/1600.i.idx.??
done
done

cd /scratch/k.church/semantic_scholar/citations/graphs/K200/merged/from/citations.G.shrink.new.T1.G2.npz.tasks.1.K200.ProNE.K200.T20.O5.w2v.kwc.edges.320.i
T=1000
ffile=/work/k.church/semantic_scholar/citations/graphs/K200/citations.G.shrink.new.T1.G2.npz.tasks.1.K200.ProNE.K200.T20.O5.w2v.kwc.edges.f
K=200
B=1600
ifile=1600.i
map=/work/k.church/semantic_scholar/citations/graphs/citations.G.shrink.new.T1.npz
for B1 in 40 80 160 320 
do
for f in experiment4/sample1.???
do
echo `date` $f
# map_paper_ids.py $map old_to_new < $f | cut -f2 > $f.new
sbatch -p debug,express,short -i $f.new -o $f.near.best_with_details.B1.$B1 near.sh --show_details 1 --find_best 1 ../*w2v.kwc.edges.$B1.i/1600.i.idx.??
done
done



awk '{for(i=2;i<=NF;i++) {n=split($i,x,":"); if(n == 2) printf "%s ", x[2]}; print ""}' experiment4/sample1.000.near.details > /tmp/x

mini = function(z) { return (min(((1:length(z)))[z == min(z)])) }

cd /scratch/k.church/semantic_scholar/citations/graphs/K200/merged/from/citations.G.shrink.new.T1.G2.npz.tasks.1.K200.ProNE.K200.T20.O5.w2v.kwc.edges.320.i
ls -s ../*/1600.i.idx.?? | head


# 320 and 160 look good, but not 40 and 80
# hypo: the problem involved hist files that needed to be recomputed
awk '/^[0-9]/ {print $NF}' sample1.???.near.best_with_details.B1.320 | cut -f2 -d: | awk '{n++; x+= $1}; END {print x/n, n}'
296.734 1000
(gft) [k.church@c2000 experiment4]$ awk '/^[0-9]/ {print $NF}' sample1.???.near.best_with_details.B1.160 | cut -f2 -d: | awk '{n++; x+= $1}; END {print x/n, n}'
304.272 1000
(gft) [k.church@c2000 experiment4]$ awk '/^[0-9]/ {print $NF}' sample1.???.near.best_with_details.B1.80 | cut -f2 -d: | awk '{n++; x+= $1}; END {print x/n, n}'
0 1000
(gft) [k.church@c2000 experiment4]$ awk '/^[0-9]/ {print $NF}' sample1.???.near.best_with_details.B1.40 | cut -f2 -d: | awk '{n++; x+= $1}; END {print x/n, n}'
0 1000

aref $map.old_to_new.i < sample1.000 | aref $map.new_to_old.i | paste - sample1.000 | awk '$1 != $2'


echo '15033963	19473827	foobar' | find_lines --input /work/k.church/semantic_scholar/papers/papers2url/lines2html --fields 'LL-'
parse_line found 3 pieces
lookup(piece: 15033963)
lookup(piece: 19473827)
<href="https://www.semanticscholar.org/paper/6ec684396fa54e6e6da44d1cdcbf1dc973e60436">Evidence of elevated glutamate in multiple sclerosis using magnetic resonance spectroscopy at 3 T.</a>	<href="https://www.semanticscholar.org/paper/e8a4a407aa4570c56b2c207a3280e7e9b2257c78">High Resolution Proton NMR Spectroscopy of Multiple Sclerosis Lesions</a>	foobar

pwd
/scratch/k.church/semantic_scholar/citations/graphs/K200/merged/from/citations.G.shrink.new.T1.G2.npz.tasks.1.K200.ProNE.K200.T20.O5.w2v.kwc.edges.320.i/experiment4a
(gft) [k.church@c2000 experiment4]$ awk 'NR == 1' sample1.000  | near --threshold 1000 --map $map ../../*/1600.i.idx.?? | sort -k2 -t: -n | uniq | sed 20q > /tmp/x
(gft) [k.church@c2000 experiment4]$ head /tmp/x
15033963	3175855:271
15033963	10490119:367
15033963	55337571:397
15033963	29679779:400
15033963	1082932:411
15033963	39986173:419
15033963	9100036:479
15033963	204956234:480
15033963	9979549:492
15033963	21686531:496

head /tmp/x | tr : '\t' | 
find_lines --input /work/k.church/semantic_scholar/papers/papers2url/lines2html --fields 'LL-' |
awk 'BEGIN {OFS="\t"; print "x", "y", "hamming"}; {print}' |
sed 's/<href/<a href/g' | 
tsv_to_html.sh > $HOME/to_go/x.html

echo 9558665 | near --threshold 1000 --map $map ../../*/1600.i.idx.?? | sort -k2 -t: -n | uniq | sed 20q > /tmp/y
cat /tmp/y | tr : '\t' | 
find_lines --input /work/k.church/semantic_scholar/papers/papers2url/lines2html --fields 'LL-' |
awk 'BEGIN {OFS="\t"; print "x", "y", "hamming"}; {print}' |
sed 's/<href/<a href/g' | 
tsv_to_html.sh > $HOME/to_go/y.html

# filtered floats are here for K375
ls -lt /scratch/k.church/npz_to_annoy/d0130/264613/citations.G.shrink.new.T1.G2.npz.ProNE.ProNE.K375.T20.O5.w2v.kwc.*
-rw-r--r-- 1 k.church users    971244288 Jul 22 21:50 /scratch/k.church/npz_to_annoy/d0130/264613/citations.G.shrink.new.T1.G2.npz.ProNE.ProNE.K375.T20.O5.w2v.kwc.nodes.txt
-rw-r--r-- 1 k.church users 162291514500 Jul 22 21:50 /scratch/k.church/npz_to_annoy/d0130/264613/citations.G.shrink.new.T1.G2.npz.ProNE.ProNE.K375.T20.O5.w2v.kwc.edges.f

dir=/work/k.church/semantic_scholar/jobs
cd /scratch/k.church/npz_to_annoy/d0130/264613/

tmp=/scratch/k.church/npz_to_annoy/d0130/264613/citations.G.shrink.new.T1.G2.npz.ProNE.ProNE.K375.T20.O5.w2v
K=375
python $dir/kwc_to_binary_word2vec_format.py -i $tmp -o $tmp.bin -K $K

cd /scratch/k.church/npz_to_annoy/d0130/264613/
B=40
for K in 375
do
for f in *K$K*kwc.edges.f
do
sbatch --constraint=ib,zen2 --array 10-59 -p short -t 1209 $src/C/floats_to_random_bytes.sh $K $B $f
done
done

cd /scratch/k.church/npz_to_annoy/d0130/264613/
sbatch -t 1209 -p short --mem=1000G /work/k.church/semantic_scholar/simple_embedding2annoy.py --binary *bin


# author ids

/work/k.church/semantic_scholar/papers/papers2url

head papers.piece.???.txt | cut -f1,4 | head
==> papers.piece.001.txt <==
corpusid	authors
172192617	135715795
179016946	48114705|69568932|69428299|145281356|144384687|69408861|32173274|66241453
10350567	2115868747
177434374	75170683
204644389	1397369554|135123622
186987721	1409525844
222525142	1996899031|1996899103|1996877567|1996864817
172272523	52038034

find /work/k.church/semantic_scholar/ -name '*.py' -or -name '*.c' -or -name 'notes.txt' -or -name '*.sh' > /work/k.church/semantic_scholar/src_files

ffile=/work/k.church/semantic_scholar/citations/graphs/K200/citations.G.shrink.new.T1.G2.npz.tasks.1.K200.ProNE.K200.T20.O5.w2v.kwc.edges.f
ifile=/scratch/k.church/semantic_scholar/citations/graphs/K200/merged/from/citations.G.shrink.new.T1.G2.npz.tasks.1.K200.ProNE.K200.T20.O5.w2v.kwc.edges.320.i/1600.i
idx=/scratch/k.church/semantic_scholar/citations/graphs/K200/merged/from/citations.G.shrink.new.T1.G2.npz.tasks.1.K200.ProNE.K200.T20.O5.w2v.kwc.edges.320.i/1600.i.idx.11

summarize_index --index $idx --random_bytes $ifile --npieces 2000000 --piece 0 --record_size 1600 --max_offset 5
summarize_index --floats $ffile --npieces 4000000 --piece 0 --record_size 200 --max_offset 10  --index $idx


ffile=/work/k.church/semantic_scholar/citations/graphs/K200/citations.G.shrink.new.T1.G2.npz.tasks.1.K200.ProNE.K200.T20.O5.w2v.kwc.edges.f
ifile=/scratch/k.church/semantic_scholar/citations/graphs/K200/merged/from/citations.G.shrink.new.T1.G2.npz.tasks.1.K200.ProNE.K200.T20.O5.w2v.kwc.edges.320.i/1600.i
idx=/scratch/k.church/semantic_scholar/citations/graphs/K200/merged/from/citations.G.shrink.new.T1.G2.npz.tasks.1.K200.ProNE.K200.T20.O5.w2v.kwc.edges.320.i/1600.i.idx.11
d=/scratch/k.church/semantic_scholar/citations/graphs/K200/merged/from/citations.G.shrink.new.T1.G2.npz.tasks.1.K200.ProNE.K200.T20.O5.w2v.kwc.edges.320.i/
mkdir -p $d/idx.11/cos $d/idx.11/hamming
# cd $d/idx.11/cos
# sbatch --array 0-999 -p express -t 59 $src/C/summarize_index.sh --floats $ffile --npieces 1000 --record_size 200 --max_offset 5  --index $idx

cd $d/idx.11/hamming
sbatch --array 0-999 -p express -t 59 $src/C/summarize_index.sh --random_bytes $ifile --npieces 1000 --record_size 1600 --max_offset 5  --index $idx


piece=12
ffile=/work/k.church/semantic_scholar/citations/graphs/K200/citations.G.shrink.new.T1.G2.npz.tasks.1.K200.ProNE.K200.T20.O5.w2v.kwc.edges.f
ifile=/scratch/k.church/semantic_scholar/citations/graphs/K200/merged/from/citations.G.shrink.new.T1.G2.npz.tasks.1.K200.ProNE.K200.T20.O5.w2v.kwc.edges.320.i/1600.i
idx=/scratch/k.church/semantic_scholar/citations/graphs/K200/merged/from/citations.G.shrink.new.T1.G2.npz.tasks.1.K200.ProNE.K200.T20.O5.w2v.kwc.edges.320.i/1600.i.idx.$piece
d=/scratch/k.church/semantic_scholar/citations/graphs/K200/merged/from/citations.G.shrink.new.T1.G2.npz.tasks.1.K200.ProNE.K200.T20.O5.w2v.kwc.edges.320.i/
mkdir -p $d/idx.$piece/cos $d/idx.$piece/hamming
cd $d/idx.$piece/cos
sbatch --array 0-99 -p short -t 1209 $src/C/summarize_index.sh --floats $ffile --npieces 100 --record_size 200 --max_offset 5  --index $idx

cd $d/idx.$piece/hamming
sbatch --array 0-99 -p short -t 1209  $src/C/summarize_index.sh --random_bytes $ifile --npieces 100 --record_size 1600 --max_offset 5  --index $idx


cosfile=/scratch/k.church/semantic_scholar/citations/graphs/K200/merged/from/citations.G.shrink.new.T1.G2.npz.tasks.1.K200.ProNE.K200.T20.O5.w2v.kwc.edges.320.i/idx.11/cos/summarize_index/c0162/29432845/0
hamfile=/scratch/k.church/semantic_scholar/citations/graphs/K200/merged/from/citations.G.shrink.new.T1.G2.npz.tasks.1.K200.ProNE.K200.T20.O5.w2v.kwc.edges.320.i/idx.11/hamming/summarize_index/0000

paste $cosfile $hamfile | head

sacct -u k.church -o Elapsed,UserCPU,TimelimitRaw,Partition,JobID,JobName,State | egrep summarize | egrep COMPLETED

pwd
/scratch/k.church/semantic_scholar/citations/graphs/K200/merged/from/citations.G.shrink.new.T1.G2.npz.tasks.1.K200.ProNE.K200.T20.O5.w2v.kwc.edges.320.i/idx.12/cos/summarize_index
(gft) [k.church@c0265 summarize_index]$ ls | cut -f3 -d. | awk '{print "sacct -j", $1, "-o Elapsed,UserCPU,TimelimitRaw,Partition,JobID,JobName,State"}' | sh | egrep summarize | egrep COMPL | sort -nr | head
  01:17:56  00:25.304       1209      short 29448640_23  summarize+  COMPLETED 
  01:15:51  00:36.263       1209      short 29448640_11  summarize+  COMPLETED 
  01:15:21  00:38.749       1209      short 29448640_10  summarize+  COMPLETED 
  01:09:06  00:23.251       1209      short 29448640_22  summarize+  COMPLETED 
  01:08:47  00:27.071       1209      short 29448640_9   summarize+  COMPLETED 
  01:05:53  00:30.694       1209      short 29448640_3   summarize+  COMPLETED 
  01:05:53  00:27.070       1209      short 29448640_2   summarize+  COMPLETED 
  01:01:38  00:49.698       1209      short 29448640_61  summarize+  COMPLETED 
  01:01:25  00:34.496       1209      short 29448640_0   summarize+  COMPLETED 
  01:00:35  00:37.585       1209      short 29448640_70  summarize+  COMPLETED 


cd /scratch/k.church/semantic_scholar/embeddings
# cat embeddings.piece.???.kwc.nodes.txt > specter.kwc.nodes.txt
# cat embeddings.piece.???.kwc.edges.f > specter.kwc.edges.f
K=768
B=20
f=specter.kwc.edges.f
# sbatch --mem=400G --array 10-59 -p short -t 1209 $src/C/floats_to_random_bytes.sh $K $B $f
sbatch --array 10-59 -p short -t 1209 $src/C/floats_to_random_bytes.sh $K $B $f

cd /scratch/k.church/semantic_scholar/embeddings
# cat embeddings.piece.???.kwc.nodes.txt > specter.kwc.nodes.txt
# cat embeddings.piece.???.kwc.edges.f > specter.kwc.edges.f
K=768
B=10
f=specter.kwc.edges.f
# sbatch --mem=400G --array 10-59 -p short -t 1209 $src/C/floats_to_random_bytes.sh $K $B $f
sbatch --array 10-99 -p short -t 1209 $src/C/floats_to_random_bytes.sh $K $B $f




summarize_index --floats $ffile --npieces 1000 --piece 502 --record_size 200 --max_offset 20  --index $idx | awk '{n++; for(i=2;i<=NF;i++) x[i]+= ($i > T)}; END{for(i in x) print i, x[i]/n, n}' T=0.95 | sort -k2 -nr

cd /scratch/k.church/semantic_scholar/citations/graphs/K200/merged/from/citations.G.shrink.new.T1.G2.npz.tasks.1.K200.ProNE.K200.T20.O5.w2v.kwc.edges.320.i/experiment4
T=1000
ffile=/work/k.church/semantic_scholar/citations/graphs/K200/citations.G.shrink.new.T1.G2.npz.tasks.1.K200.ProNE.K200.T20.O5.w2v.kwc.edges.f
K=200
B=1600
ifile=1600.i
map=/work/k.church/semantic_scholar/citations/graphs/citations.G.shrink.new.T1.npz
echo 9558665 | near --threshold 500 --map $map ../../*/1600.i.idx.11
echo 9558665 | near_with_floats --offset 5  --threshold 500 --map $map ../../*/1600.i.idx.11 --floats $ffile --record_size $K


echo 9558665 | near_with_floats --offset 5 --map $map --floats $ffile --record_size $K ../../*/1600.i.idx.?? | sort -nr -k3 -u > /tmp/x
cat /tmp/x | 
find_lines --input /work/k.church/semantic_scholar/papers/papers2url/lines2html --fields 'LL-' |
awk 'BEGIN {OFS="\t"; print "x", "y", "hamming"}; {print}' |
sed 's/<href/<a href/g' | 
tsv_to_html.sh > $HOME/to_go/x.html

sbatch -p short -t 1209 --array 1-30 authors2matrix.py --input 'papers.piece.%03d.authors' --output 'papers.piece.%03d.authors.npz' 

# This is too slow

cd /work/k.church/semantic_scholar/papers/papers2url/pieces
sbatch -p short -t 1209 --array 0-999 authors2matrix.py --input 'x.%03d' --output 'x.%03d.npz' 

authors2matrix --slurm 0 --input "x.%03d", --output "x.%03d.out" --paper_map /work/k.church/semantic_scholar/citations/graphs/citations.G.shrink.new.T1.npz --authors /work/k.church/semantic_scholar/authors/authors.id.L

cd /work/k.church/semantic_scholar/papers/papers2url/pieces
sbatch --array 0-999 -p express /work/k.church/semantic_scholar/papers/papers2url/pieces/authors2matrix.sh "x.%03d"

cd /work/k.church/semantic_scholar/papers/papers2url
sbatch -t 1209 --array 1-30 -p short /work/k.church/semantic_scholar/papers/papers2url/pieces/authors2matrix.sh "papers.piece.%03d.authors"

cat papers.piece.???.authors.citations.out | awk '$1 > 0 {print $1}' | x_to_y ai > papers.authors.new_paper_ids.i
cat papers.piece.???.authors.citations.out | awk '$1 > 0 {print $2}' | x_to_y ai > papers.authors.new_author_ids.i
cat papers.piece.???.authors.citations.out | awk '$1 > 0 {print $3}' | x_to_y ai > papers.authors.citations.i

sbatch --mem=20G -t 1208 -p short ints2matrix.py --shape 108651994,77585514 --X papers.authors.new_paper_ids.i --Y papers.authors.new_author_ids.i --vals papers.authors.citations.i --output papers.authors.npz --normalize

cd /work/k.church/semantic_scholar/citations/graphs
A=/work/k.church/semantic_scholar/papers/papers2url/papers.authors.normalize.npz
npz=/work/k.church/semantic_scholar/citations/graphs/K250/citations.G.shrink.new.T1.G2.npz.tasks.1.K250.ProNE.K250.T20.O5.w2v.npz
sbatch --constraint=ib -p short -t 1209 --mem=600G /work/k.church/semantic_scholar/jobs/npz_to_kwc.py -i $npz -o $npz.authors.norm --matrix $A


cd /work/k.church/semantic_scholar/citations/graphs
A=/work/k.church/semantic_scholar/papers/papers2url/papers.authors.normalize.npz
npz=/work/k.church/semantic_scholar/citations/graphs/K250/citations.G.shrink.new.T1.G2.npz.tasks.1.K250.ProNE.K250.T20.O5.w2v.npz
sbatch --constraint=ib -p short -t 1209 --mem=600G /work/k.church/semantic_scholar/jobs/npz_to_kwc.py -i $npz -o $npz.authors.norm --matrix $A

cd /scratch/k.church/semantic_scholar/citations/graphs/K250
B=20
for K in 250 # 100 200 300
do
for f in citations.G.shrink.new.T1.G2.npz.tasks.1.K250.ProNE.K250.T20.O5.w2v.npz.authors.norm.kwc.edges.f # *K$K*kwc.edges.f
do
# sbatch --mem=100G --constraint=ib,zen2 --array 10-59 -p short -t 1209 $src/C/floats_to_random_bytes.sh $K $B $f
sbatch --mem=100G --array 10-59 -p short -t 1209 $src/C/floats_to_random_bytes.sh $K $B $f
done
done

ls citations.G.shrink.new.T1.G2.npz.tasks.1.K250.ProNE.K250.T20.O5.w2v.npz.authors.norm.kwc.edges.f.*.??.B20.i | awk -F. '{printf "ln -s %s authors.norm.kwc.edges.f.%d.B20.i\n", $0, $(NF-2)}' | sh

cd /scratch/k.church/semantic_scholar/citations/graphs/K250
job1=`sbatch -t 1209 -p short --array 10-34 cbind_bytes.sh 20 25 authors.norm.kwc.edges.f. .B  | awk '{print $NF}'`

for B in 40
do
out=merged/authors.norm.kwc.edges.f.B1000.i
mkdir -p `dirname $out`
sbatch -d afterany:$job1 --constraint=ib,zen2 -e $out.err -o $out -t 1209 -p short $src/C/cbind_bytes_without_array_jobs.sh $B authors.norm.kwc.edges.f.??.B40.i
done

cd /scratch/k.church/semantic_scholar/citations/graphs/K250/merged
f=authors.norm.kwc.edges.f.B1000.i
# sbatch --constraint=ib,zen2 --mem=100G --array 11-40 -p short -t 1209 $src/C/index_random_bytes.sh $f 1000 250
sbatch --mem=100G --array 41-99 -p short -t 1209 $src/C/index_random_bytes.sh $f 1000 250

# check specter results tomorrow
# /scratch/k.church/semantic_scholar/embeddings

cd /scratch/k.church/semantic_scholar/citations/graphs/K250/merged
ffile=../citations.G.shrink.new.T1.G2.npz.tasks.1.K250.ProNE.K250.T20.O5.w2v.npz.authors.norm.kwc.edges.f
K=250
authors_map=/work/k.church/semantic_scholar/authors/authors.id.L
echo 2244184 | near_with_floats --new_map $authors_map --offset 5  --floats $ffile  --record_size $K authors.norm.kwc.edges.f.B1000.i.idx.?? | sort -k3 -nr -u | sed 50q 
echo 144444054 | near_with_floats --new_map $authors_map --offset 5  --floats $ffile  --record_size $K authors.norm.kwc.edges.f.B1000.i.idx.?? | sort -k3 -nr -u | sed 50q 
echo 1693517 | near_with_floats --new_map $authors_map --offset 5  --floats $ffile  --record_size $K authors.norm.kwc.edges.f.B1000.i.idx.?? | sort -k3 -nr -u | sed 50q 

cd /scratch/k.church/semantic_scholar/citations/graphs/K250/by_author
echo 1693517 | near_with_floats --new_map authors.id.L --offset 5  --floats authors.norm.kwc.edges.f --record_size 250 authors.norm.kwc.edges.f.B1000.i.idx.??

cat authors.piece.0??.id  | egrep 'href' | sort -t / -k5 -n > authors.url
index_lines authors.url

cd /scratch/k.church/semantic_scholar/citations/graphs/K250/by_author
echo 1693517 | near_with_floats --urls authors.url --new_map authors.id.L --offset 5  --floats authors.norm.kwc.edges.f --record_size 250 authors.norm.kwc.edges.f.B1000.i.idx.11


outdir=/home/k.church/to_go/pages
export outdir
cd /scratch/k.church/semantic_scholar/citations/graphs/K250/by_author
rm /tmp/y
for id in 145428168 1716907 1705919  2244184 2065645764 2007555891 1774515 9391905 120874496 2016914 144096985 1736049 3226331 2912454 7465342 1398035522 1695784 144883814 145259603 21308992 1738798 9215251 1714374 144173823 144270731 1784682 114531657 1751762 144768480 145366908 145115014 2708017 2393013 144783904 144418438 144365875 1699545 1982950 1688882 2360881 34938639 2474650 32538203 144716964 2472759 2803071 145322333 1755162 1702974 1699205 2508628 144179113 31489430 3103883 48408400 1710543 10080270 1752326 144547315 1679873
do
echo working on $id
echo $id |  near_with_floats --new_map authors.id.L --offset 5  --floats authors.norm.kwc.edges.f --record_size 250 authors.norm.kwc.edges.f.B1000.i.idx.?? | sort -nr | uniq | sed 50q > /tmp/x
awk '{print $2; print $3}' /tmp/x | sort -u | awk '{printf "./author_page.sh %d > $outdir/%d.html\n", $1, $1}' | sh
awk 'NR == 2 {printf("%d\t%s\n", id, $0)}' id=$id $outdir/$id.html >> /tmp/y
done


awk 'BEGIN {FS="\t"; print "<html><ol>"};
           {printf ("<li><a href=\"%s.html\">details</a> %s</li>\n", $1, $2)}
     END {print "</ol></html>"}' /tmp/y > $outdir/index.html



cd /work/k.church/semantic_scholar/papers/papers2url
export paper_map=/work/k.church/semantic_scholar/citations/graphs/citations.G.shrink.new.T1.npz
export author_map=/work/k.church/semantic_scholar/authors/authors.id.L
sbatch --array 1-30 -p debug,express /work/k.church/semantic_scholar/papers/papers2url/pieces/authors2matrix.sh "papers.piece.%03d.authors.citations"

cat papers.piece.???.authors.citations.out | awk '$1 > 0 {print $1}' | x_to_y ai > papers.authors.new_paper_ids.i
cat papers.piece.???.authors.citations.out | awk '$1 > 0 {print $2}' | x_to_y ai > papers.authors.new_author_ids.i
cat papers.piece.???.authors.citations.out | awk '$1 > 0 {print $3}' | x_to_y ai > papers.authors.citations.i

cd /work/k.church/semantic_scholar/papers/papers2url
# sbatch --mem=20G -t 1208 -p short ints2matrix.py --shape 108651994,77585514 --X papers.authors.new_paper_ids.i --Y papers.authors.new_author_ids.i --vals papers.authors.citations.i --output papers.authors.npz --normalize
# sbatch --mem=20G -t 59 -p express ints2matrix.py --shape 108651994,77585514 --X papers.authors.new_paper_ids.i --Y papers.authors.new_author_ids.i --vals papers.authors.citations.i --output papers.authors.npz --normalize
# do not normalize
sbatch --mem=20G -t 59 -p express ints2matrix.py --shape 108651994,77585514 --X papers.authors.new_paper_ids.i --Y papers.authors.new_author_ids.i --vals papers.authors.citations.i --output papers.authors.npz

cd /scratch/k.church/semantic_scholar/embeddings
ls specter.kwc.edges.f.*.??.B20.i | awk -F. '{printf "ln -s %s spector.kwc.edges.f.%d.B20.i\n", $0, $(NF-2)}' | sh

cd /scratch/k.church/semantic_scholar/embeddings
job1=`sbatch -t 1209 -p short --array 10-34 cbind_bytes.sh 20 25 spector.kwc.edges.f. .B  | awk '{print $NF}'`
# job2=`sbatch -d afterany:$job1 -t 1209 -p short --array 10-22 cbind_bytes.sh 40 12 spector.kwc.edges.f. .B  | awk '{print $NF}'`
# sbatch -d afterany:$job1 --constraint=ib,zen2 -e $out.err -o $out -t 1209 -p short $src/C/cbind_bytes_without_array_jobs.sh $B authors.norm.kwc.edges.f.??.B40.i
B=40
BB=`ls spector.kwc.edges.f.??.B40.i | awk 'END {print NR * 40}'`
out=merged/spector.kwc.edges.f.$BB.i
mkdir -p `dirname $out`
sbatch --constraint=ib,zen2 -e $out.err -o $out -t 1209 -p short cbind_bytes_without_array_jobs.sh $B spector.kwc.edges.f.??.B40.i

f=$out
sbatch -d afterany:29667057 --mem=150G --array 10-99 -p short -t 1209 $src/C/index_random_bytes.sh $f 960 250


# specter has a different number of papers in their embeddings
cd /work/k.church/semantic_scholar/papers/papers2url
for paper in papers.piece.???.authors.citations
do
ln -s $paper `basename $paper .citations`.specter.citations
done

cd /work/k.church/semantic_scholar/papers/papers2url
export paper_map=/scratch/k.church/semantic_scholar/embeddings/specter.kwc.nodes.txt
export author_map=/work/k.church/semantic_scholar/authors/authors.id.L
sbatch --array 1-30 -p debug,express /work/k.church/semantic_scholar/papers/papers2url/pieces/authors2matrix.sh "papers.piece.%03d.authors.specter.citations"

cd /work/k.church/semantic_scholar/papers/papers2url
cat papers.piece.???.authors.specter.citations.out | awk '$1 > 0 {print $1}' | x_to_y ai > papers.authors.specter.new_paper_ids.i
cat papers.piece.???.authors.specter.citations.out | awk '$1 > 0 {print $2}' | x_to_y ai > papers.authors.specter.new_author_ids.i
cat papers.piece.???.authors.specter.citations.out | awk '$1 > 0 {print $3}' | x_to_y ai > papers.authors.specter.citations.i

cd /work/k.church/semantic_scholar/papers/papers2url
# do not normalize
sbatch --mem=20G -t 59 -p express ints2matrix.py --shape 125312914,77585514 --X papers.authors.specter.new_paper_ids.i --Y papers.authors.specter.new_author_ids.i --vals papers.authors.specter.citations.i --output papers.authors.specter.npz


outdir=/home/k.church/to_go/specter/pages
mkdir -p $outdir
export outdir
cd /scratch/k.church/semantic_scholar/embeddings/merged
# invert_mapping.sh /scratch/k.church/semantic_scholar/embeddings/specter.kwc.nodes.txt
ffile=../specter.kwc.edges.f
# map=/work/k.church/semantic_scholar/citations/graphs/citations.G.shrink.new.T1.npz
map=../specter.kwc.nodes.txt
echo 246638887 | 
near_with_floats --map $map --offset 5  --floats $ffile --record_size 768 spector.kwc.edges.f.960.i spector.kwc.edges.f.960.i.idx.13

echo 9558665 | 
near_with_floats --map $map --offset 5  --floats $ffile --record_size 768 spector.kwc.edges.f.960.i spector.kwc.edges.f.960.i.idx.?? |
sort -nr -u | 
sed 50q | cut -f1,3 | find_lines --input /work/k.church/semantic_scholar/papers/papers2url/lines2html --fields '-L'

echo 246638887 | 
near_with_floats --map $map --offset 5  --floats $ffile --record_size 768 spector.kwc.edges.f.960.i spector.kwc.edges.f.960.i.idx.?? |
sort -nr -u | 
sed 50q | cut -f1,3 | find_lines --input /work/k.church/semantic_scholar/papers/papers2url/lines2html --fields '-L'

echo 144435865 |
near_with_floats --map $map --offset 5  --floats $ffile --record_size 768 spector.kwc.edges.f.960.i spector.kwc.edges.f.960.i.idx.?? |
sort -nr -u | 
sed 50q | cut -f1,3 | find_lines --input /work/k.church/semantic_scholar/papers/papers2url/lines2html --fields '-L'

cd /scratch/k.church/semantic_scholar/embeddings/merged
ffile=../specter.kwc.edges.f
echo 235959867 |
near_with_floats --map $map --offset 5  --floats $ffile --record_size 768 spector.kwc.edges.f.960.i spector.kwc.edges.f.960.i.idx.?? |
sort -nr -u | 
sed 50q | cut -f1,3 | find_lines --input /work/k.church/semantic_scholar/papers/papers2url/lines2html --fields '-L'


# for id in 145428168 1716907 1705919  2244184 2065645764 2007555891 1774515 9391905 120874496 2016914 144096985 1736049 3226331 2912454 7465342 1398035522 1695784 144883814 145259603 21308992 1738798 9215251 1714374 144173823 144270731 1784682 114531657 1751762 144768480 145366908 145115014 2708017 2393013 144783904 144418438 144365875 1699545 1982950 1688882 2360881 34938639 2474650 32538203 144716964 2472759 2803071 145322333 1755162 1702974 1699205 2508628 144179113 31489430 3103883 48408400 1710543 10080270 1752326 144547315 1679873
# do
# echo working on $id
# echo $id |  near_with_floats --new_map authors.id.L --offset 5  --floats authors.norm.kwc.edges.f --record_size 250 authors.norm.kwc.edges.f.B1000.i.idx.?? | sort -nr | uniq | sed 50q > /tmp/x
# awk '{print $2; print $3}' /tmp/x | sort -u | awk '{printf "./author_page.sh %d > $outdir/%d.html\n", $1, $1}' | sh
# awk 'NR == 2 {printf("%d\t%s\n", id, $0)}' id=$id $outdir/$id.html >> /tmp/y
# done


# awk 'BEGIN {FS="\t"; print "<html><ol>"};
#            {printf ("<li><a href=\"%s.html\">details</a> %s</li>\n", $1, $2)}
#      END {print "</ol></html>"}' /tmp/y > $outdir/index.html


cd /scratch/k.church/semantic_scholar/embeddings/merged
ffile=../specter.kwc.edges.f
echo 235959867 |
near_with_floats --map $map --offset 5  --floats $ffile --record_size 768 spector.kwc.edges.f.960.i spector.kwc.edges.f.960.i.idx.?? |
sort -nr -u | 
sed 50q | cut -f1,3 | find_lines --input /work/k.church/semantic_scholar/papers/papers2url/lines2html --fields '-L' > /tmp/alphafold.specter

cd /scratch/k.church/semantic_scholar/citations/graphs/K200/merged/from/citations.G.shrink.new.T1.G2.npz.tasks.1.K200.ProNE.K200.T20.O5.w2v.kwc.edges.320.i/experiment4
T=1000
ffile=/work/k.church/semantic_scholar/citations/graphs/K200/citations.G.shrink.new.T1.G2.npz.tasks.1.K200.ProNE.K200.T20.O5.w2v.kwc.edges.f
K=200
B=1600
ifile=1600.i

# *** check this out tomorrow July 31, 2022 ***
K=200
cd /scratch/k.church/semantic_scholar/citations/graphs/K200/merged/from/citations.G.shrink.new.T1.G2.npz.tasks.1.K200.ProNE.K200.T20.O5.w2v.kwc.edges.320.i
# f=/work/k.church/semantic_scholar/citations/graphs/K200/citations.G.shrink.new.T1.G2.npz.tasks.1.K200.ProNE.K200.T20.O5.w2v.kwc.edges.f
f=citations.kwc.edges.f
# sbatch -p short -t 1209 --array 300-359 $src/C/floats_to_idx.sh $K `basename $f`
# sbatch -p short -t 1209 --array 400-459 $src/C/floats_to_idx.sh $K `basename $f` 4
# sbatch -p short -t 1209 --array 800-859 $src/C/floats_to_idx.sh $K `basename $f` 8
sbatch -p short -t 1209 --array 900-949 $src/C/floats_to_idx.sh $K `basename $f` 12
sbatch -p short -t 1209 --array 950-999 $src/C/floats_to_idx.sh $K `basename $f` 16

# sbatch -p express -t 59 --array 500-699 $src/C/floats_to_idx.sh $K citations.kwc.edges.320.i

map=/work/k.church/semantic_scholar/citations/graphs/citations.G.shrink.new.T1.npz
echo 235959867 | 
near_with_floats --map $map --offset 5  --floats $ffile --record_size $K ../../*/1600.i.idx.?? |
sort -nr -u | 
sed 50q | cut -f1,3 | find_lines --input /work/k.church/semantic_scholar/papers/papers2url/lines2html --fields '-L' > /tmp/alphafold.citations


echo 235959867 | find_lines --input /work/k.church/semantic_scholar/papers/papers2url/lines2html

# cd /scratch/k.church/semantic_scholar/citations/graphs/K250/merged
# ffile=../citations.G.shrink.new.T1.G2.npz.tasks.1.K250.ProNE.K250.T20.O5.w2v.npz.authors.norm.kwc.edges.f
# K=250
# echo 235959867 |
# authors_map=/work/k.church/semantic_scholar/authors/authors.id.L
# echo 2244184 | near_with_floats --new_map $authors_map --offset 5  --floats $ffile  --record_size $K authors.norm.kwc.edges.f.B1000.i.idx.?? | sort -k3 -nr -u | sed 50q 

cd /work/k.church/semantic_scholar/citations/graphs
A=/work/k.church/semantic_scholar/papers/papers2url/papers.authors.npz
npz=/work/k.church/semantic_scholar/citations/graphs/K250/citations.G.shrink.new.T1.G2.npz.tasks.1.K250.ProNE.K250.T20.O5.w2v.npz
sbatch -p short -t 1209 --mem=1000G /work/k.church/semantic_scholar/jobs/npz_to_kwc.py -i $npz -o $npz.authors --matrix $A

cd /scratch/k.church/semantic_scholar/embeddings
ffile=/scratch/k.church/semantic_scholar/embeddings/specter.kwc.edges.f
K=768
A=/work/k.church/semantic_scholar/papers/papers2url/papers.authors.specter.npz
sbatch -p short -t 1209 --mem=1000G /work/k.church/semantic_scholar/jobs/project_kwc.py --input $ffile -K $K -o $ffile.authors --matrix $A

cd /work/k.church/semantic_scholar/papers/papers2url/
sbatch -p short -t 1209 --array 1-30 sort_titles.sh


for f in papers.piece.???.txt.titles
do
sort -m $f /work/k.church/dblp/title.sorted | common_prefix | 
awk -F'\t' '$1 > 20 && $1 > length($2) * 0.8  {print $0 "\t" prev3}; {prev3 = $3}'  |
egrep DBLP | egrep -v 'DBLP.*DBLP' > $f.matches
done

B=20
K=250
f=/work/k.church/semantic_scholar/citations/graphs/K250/citations.G.shrink.new.T1.G2.npz.tasks.1.K250.ProNE.K250.T20.O5.w2v.npz.authors.kwc.edges.f
sbatch --mem=100G --array 10-59 -p short -t 1209 $src/C/floats_to_random_bytes.sh $K $B $f

B=10
K=768
f=/scratch/k.church/semantic_scholar/embeddings/specter.kwc.edges.f.authors.kwc.edges.f
sbatch --mem=100G --array 10-99 -p short -t 1209 $src/C/floats_to_random_bytes.sh $K $B $f


cd /work/k.church/semantic_scholar/citations/graphs/K250
PWD=`pwd`
ls citations.G.shrink.new.T1.G2.npz.tasks.1.K250.ProNE.K250.T20.O5.w2v.npz.authors.kwc.edges.f.*.??.B20.i | awk -F. '{printf "ln -s $PWD/%s /scratch/k.church/semantic_scholar/citations/graphs/K250.V2/authors.kwc.edges.f.%s.B20.i \n", $0, $(NF-2)}' | sh

cd /scratch/k.church/semantic_scholar/citations/graphs/K250.V2
job1=`sbatch -t 1209 -p short --array 10-34 cbind_bytes.sh 20 25 authors.kwc.edges.f. .B  | awk '{print $NF}'`
B=40
BB=`ls authors.kwc.edges.f.??.B20.i | awk 'END {print NR * 20}'`
out=merged/authors.kwc.edges.f.$BB.i
mkdir -p `dirname $out`
sbatch -d afterany:$job1 --constraint=ib,zen2 -e $out.err -o $out -t 1209 -p short cbind_bytes_without_array_jobs.sh $B spector.kwc.edges.f.??.B40.i


# This is a better solution
# Embarrassingly parallel
# No need to keep random_bytes around
# Or to merge them into a long file and then permute them

dir=/scratch/k.church/semantic_scholar/citations/graphs/K250.V3
mkdir -p $dir
K=250
f=/work/k.church/semantic_scholar/citations/graphs/K250/citations.G.shrink.new.T1.G2.npz.tasks.1.K250.ProNE.K250.T20.O5.w2v.npz.authors.kwc.edges.f
ln -s $f $dir
ln -s /work/k.church/semantic_scholar/authors/authors.id.L $dir
cd $dir
sbatch -p short -t 1209 --array 10-99 $src/C/floats_to_idx.sh $K `basename $f`

dir=/scratch/k.church/semantic_scholar/embeddings/specter/K768/authors
mkdir -p $dir
K=768
f=/scratch/k.church/semantic_scholar/embeddings/specter.kwc.edges.f.authors.kwc.edges.f
ln -s $f $dir
ln -s /work/k.church/semantic_scholar/authors/authors.id.L $dir
cd $dir
sbatch -p short -t 1209 --array 10-99 $src/C/floats_to_idx.sh $K `basename $f`

cd /scratch/k.church/semantic_scholar/citations/graphs/K250.V3
ls -s slurm*out | awk '$1 > 0 {print "seff", substr($2, 7)}' | cut -f1 -d. | sh | egrep 'Wall.*time' | sort

Job Wall-clock time: 00:43:53
Job Wall-clock time: 00:43:56
Job Wall-clock time: 00:44:00
Job Wall-clock time: 00:45:52
Job Wall-clock time: 00:46:37
Job Wall-clock time: 00:48:19
Job Wall-clock time: 00:48:47
Job Wall-clock time: 00:49:51
Job Wall-clock time: 00:51:44
Job Wall-clock time: 00:56:18

dir=/scratch/k.church/semantic_scholar/citations/graphs/K250.V3
ln -s /work/k.church/semantic_scholar/authors/authors.id.L $dir
ln -s /work/k.church/semantic_scholar/authors/authors.url* $dir

dir=/scratch/k.church/semantic_scholar/embeddings/specter/K768/authors
ln -s /work/k.church/semantic_scholar/authors/authors.id.L $dir
ln -s /work/k.church/semantic_scholar/authors/authors.url* $dir

cd /scratch/k.church/semantic_scholar/citations/graphs/K250.V3
./author_page_top_level.sh pages25 145428168 1716907 1705919  2244184 2065645764 2007555891 1774515 9391905 120874496 2016914 144096985 1736049 3226331 2912454 7465342 1398035522 1695784 144883814 145259603 21308992 1738798 9215251 1714374 144173823 144270731 1784682 114531657 1751762 144768480 145366908 145115014 2708017 2393013 144783904 144418438 144365875 1699545 1982950 1688882 2360881 34938639 2474650 32538203 144716964 2472759 2803071 145322333 1755162 1702974 1699205 2508628 144179113 31489430 3103883 48408400 1710543 10080270 1752326 144547315 1679873


cd /scratch/k.church/semantic_scholar/citations/graphs/K375/authors
for f in *simple.idx.??.i
 do
#  echo $f
if [ ! -s $f.inv  ]
then
echo working on $f
#  invert_permutation $f > $f.inv
fi
 done


cd /scratch/k.church/semantic_scholar/citations/graphs/K375
npz=citations.npz
sbatch -p short -t 1209 --mem=600G /work/k.church/semantic_scholar/jobs/npz_to_kwc.py -i $npz -o $npz.papers
A=/work/k.church/semantic_scholar/papers/papers2url/papers.authors.normalize.npz
sbatch -p short -t 1209 --mem=900G /work/k.church/semantic_scholar/jobs/npz_to_kwc.py -i $npz -o $npz.papers.authors --matrix $A
job1=29911108
f=citations.npz.papers.kwc.edges.f
K=375
# f=/scratch/k.church/semantic_scholar/embeddings/specter.kwc.edges.f.authors.kwc.edges.f
# ln -s $f $dir
# ln -s /work/k.church/semantic_scholar/authors/authors.id.L $dir
# cd $dir
f=citations.npz.papers.authors.kwc.edges.f
sbatch -p short -t 1209 --array 10-99 $src/C/floats_to_idx.sh $K `basename $f`
job1=29911550
sbatch -d afterany:$job1 -p short -t 1209 --array 10-99 $src/C/floats_to_idx.sh $K `basename $f`

cd /scratch/k.church/semantic_scholar/citations/graphs/K375/papers
f=citations.npz.papers.kwc.edges.f
K=375
sbatch -p short -t 1209 --array 1-9 $src/C/floats_to_idx.sh $K `basename $f`
sbatch -p short -t 1209 --array 10-199 $src/C/floats_to_idx.sh $K `basename $f`
# sbatch -p short -t 1209 --array 100-199 $src/C/floats_to_idx.sh $K `basename $f`

# *** LOOK FOR: check this out tomorrow July 31, 2022 ***


# specter
cd /scratch/k.church/semantic_scholar/embeddings/merged
map=../specter.kwc.nodes.txt
echo 9558665 | 
near_with_floats --map $map --offset 5  --floats $ffile --record_size 768 spector.kwc.edges.f.960.i spector.kwc.edges.f.960.i.idx.?? |
sort -nr -u | 
sed 50q | cut -f1,3 | find_lines --input /work/k.church/semantic_scholar/papers/papers2url/lines2html --fields '-L'

cd /scratch/k.church/semantic_scholar/citations/graphs/K200/merged/from/citations.G.shrink.new.T1.G2.npz.tasks.1.K200.ProNE.K200.T20.O5.w2v.kwc.edges.320.i/experiment4
ffile=/work/k.church/semantic_scholar/citations/graphs/K200/citations.G.shrink.new.T1.G2.npz.tasks.1.K200.ProNE.K200.T20.O5.w2v.kwc.edges.f
K=200
ifile=1600.i
map=/work/k.church/semantic_scholar/citations/graphs/citations.G.shrink.new.T1.npz
echo 9558665 | 
near_with_floats  --floats $ffile --record_size $K --offset 5 --map $map ../../*/1600.i.idx.11 | 
sort -nr -u | 
sed 50q | cut -f1,3 | find_lines --input /work/k.church/semantic_scholar/papers/papers2url/lines2html --fields '-L'

echo 235959867 | 
near_with_floats  --floats $ffile --record_size $K --offset 5 --map $map ../../*/1600.i.idx.11 | 
sort -nr -u | 
sed 50q | cut -f1,3 | find_lines --input /work/k.church/semantic_scholar/papers/papers2url/lines2html --fields '-L'


cd /scratch/k.church/semantic_scholar/citations/graphs/K200/merged/from/citations.G.shrink.new.T1.G2.npz.tasks.1.K200.ProNE.K200.T20.O5.w2v.kwc.edges.320.i
map=/work/k.church/semantic_scholar/citations/graphs/citations.G.shrink.new.T1.npz
K=200
ffile=/work/k.church/semantic_scholar/citations/graphs/K200/citations.G.shrink.new.T1.G2.npz.tasks.1.K200.ProNE.K200.T20.O5.w2v.kwc.edges.f
echo 9558665 | 
near_with_floats  --floats $ffile --record_size $K --offset 5 --map $map citations.kwc.edges.f.simple.idx.???.i	1600.i.idx.?? | 
sort -nr -u | 
sed 50q | cut -f1,3 | find_lines --input /work/k.church/semantic_scholar/papers/papers2url/lines2html --fields '-L'

cd /scratch/k.church/semantic_scholar/citations/graphs/K200/merged/from/citations.G.shrink.new.T1.G2.npz.tasks.1.K200.ProNE.K200.T20.O5.w2v.kwc.edges.320.i
for f in citations.kwc.edges.f.simple.idx.???.i	1600.i.idx.?? 
do 
echo 9558665 | 
near_with_floats  --floats $ffile --record_size $K --offset 1 --map $map $f  |
awk '{print f "\t" $1}' f=$f
done > /tmp/x

cd /scratch/k.church/semantic_scholar/citations/graphs/K200/merged/from/citations.G.shrink.new.T1.G2.npz.tasks.1.K200.ProNE.K200.T20.O5.w2v.kwc.edges.320.i
ffile=/work/k.church/semantic_scholar/citations/graphs/K200/citations.G.shrink.new.T1.G2.npz.tasks.1.K200.ProNE.K200.T20.O5.w2v.kwc.edges.f
map=/work/k.church/semantic_scholar/citations/graphs/citations.G.shrink.new.T1.npz
K=200
for f in `ls -s *seed*.i | awk '$1 > 0 {print $2}'`
do
echo 9558665 | 
near_with_floats  --floats $ffile --record_size $K --offset 1 --map $map $f  |
awk '{print f "\t" $1}' f=$f
done > /tmp/x


cd /scratch/k.church/semantic_scholar/citations/graphs/K200/merged/from/citations.G.shrink.new.T1.G2.npz.tasks.1.K200.ProNE.K200.T20.O5.w2v.kwc.edges.320.i
ffile=/work/k.church/semantic_scholar/citations/graphs/K200/citations.G.shrink.new.T1.G2.npz.tasks.1.K200.ProNE.K200.T20.O5.w2v.kwc.edges.f
map=/work/k.church/semantic_scholar/citations/graphs/citations.G.shrink.new.T1.npz
K=200
echo 9558665 | 
near_with_floats  --floats $ffile --record_size $K --offset 5 --map $map `ls -s *seed*.i | awk '$1 > 0 {print $2}'` | 
sort -nr -u | 
sed 50q | cut -f1,3 | find_lines --input /work/k.church/semantic_scholar/papers/papers2url/lines2html --fields '-L'





cd /scratch/k.church/semantic_scholar/citations/graphs/K200/merged/from/citations.G.shrink.new.T1.G2.npz.tasks.1.K200.ProNE.K200.T20.O5.w2v.kwc.edges.320.i
ffile=/work/k.church/semantic_scholar/citations/graphs/K200/citations.G.shrink.new.T1.G2.npz.tasks.1.K200.ProNE.K200.T20.O5.w2v.kwc.edges.f
map=/work/k.church/semantic_scholar/citations/graphs/citations.G.shrink.new.T1.npz
K=200
for f in `ls -s *seed*???.i | awk '$1 > 0 {print $2}'`
do
echo 9558665 | 
near_with_floats  --floats $ffile --record_size $K --offset 1 --map $map $f  |
awk '{print f "\t" $1}' f=$f
done > /tmp/x


cd /scratch/k.church/semantic_scholar/citations/graphs/K200/merged/from/citations.G.shrink.new.T1.G2.npz.tasks.1.K200.ProNE.K200.T20.O5.w2v.kwc.edges.320.i
echo 9558665 | 
near_with_floats  --floats $ffile --record_size $K --offset 1 --map $map *seed*.B12.??.i  |
sort -nr -u | 
sed 50q | cut -f1,3 | find_lines --input /work/k.church/semantic_scholar/papers/papers2url/lines2html --fields '-L'

cd /scratch/k.church/semantic_scholar/citations/graphs/K200/merged/from/citations.G.shrink.new.T1.G2.npz.tasks.1.K200.ProNE.K200.T20.O5.w2v.kwc.edges.320.i
score_with_floats.sh $K $ffile 50 *.30?.i


echo 9558665 | 
near_with_floats  --floats $ffile --record_size $K --offset 5 --map $map *.B12.*idx*.i | 
sort -nr -u | 
sed 50q | cut -f1,3 | find_lines --input /work/k.church/semantic_scholar/papers/papers2url/lines2html --fields '-L'

echo 7375270 | brute_force_with_floats --record_size 200 --floats $ffile --threshold 0.985

cd /scratch/k.church/semantic_scholar/embeddings/new
ffile=specter.kwc.edges.f
K=768
sbatch -p short -t 1209 --array 100-199 $src/C/floats_to_idx.sh $K $ffile 8


cd /scratch/k.church/semantic_scholar/embeddings/new
ffile=specter.kwc.edges.f
K=768
map=/scratch/k.church/semantic_scholar/embeddings/specter.kwc.nodes.txt
echo 9558665 | near_with_floats  --floats $ffile --record_size $K --offset 1 --map $map *.i | 
sort -nr -u | 
sed 50q | cut -f1,3 | find_lines --input /work/k.church/semantic_scholar/papers/papers2url/lines2html --fields '-L'

cd /scratch/k.church/semantic_scholar/embeddings/new
sbatch -p short -t 1209 --mem=10G --array=100 idx_to_pairs.sh 'specter.kwc.edges.f.simple.seed100.K768.B8.idx.%d.i' 'specter.kwc.edges.f.simple.seed100.K768.B8.idx.%d.i.pairs' 5

for f in specter.kwc.edges.f.simple.seed*.K768.B8.idx.*.i
do
piece=`echo $f | awk -F. '{print $(NF-1)}'`
ln -s $f idx.$piece.i
done

sbatch -p short -t 1209 --mem=10G --array=100-197 idx_to_pairs.sh 'idx.%d.i' 'idx.%d.i.pairs' 5

cd /scratch/k.church/semantic_scholar/citations/graphs/K200/merged/from/citations.G.shrink.new.T1.G2.npz.tasks.1.K200.ProNE.K200.T20.O5.w2v.kwc.edges.320.i
K=200
ffile=/work/k.church/semantic_scholar/citations/graphs/K200/citations.G.shrink.new.T1.G2.npz.tasks.1.K200.ProNE.K200.T20.O5.w2v.kwc.edges.f
map=/work/k.church/semantic_scholar/citations/graphs/citations.G.shrink.new.T1.npz
echo 9558665 | 
near_with_floats  --floats $ffile --record_size $K --offset 5 --map $map *.B16.*.i | 
sort -nr -u | 
sed 50q | cut -f1,3 | find_lines --input /work/k.church/semantic_scholar/papers/papers2url/lines2html --fields '-L'

# score_with_floats.sh $K $ffile 50 *.B16.*.i

cd /scratch/k.church/semantic_scholar/citations/graphs/K200/merged/from/citations.G.shrink.new.T1.G2.npz.tasks.1.K200.ProNE.K200.T20.O5.w2v.kwc.edges.320.i
score_with_floats.sh $K $ffile 200 `cat /tmp/B` | sort -nr >  /tmp/B3.out

mkdir new
for f in *idx.9??.i
do
piece=`echo $f | awk -F. '{print $(NF-1)}'`
ln -s $f new/idx.$piece.i
done

cd /scratch/k.church/semantic_scholar/citations/graphs/K200/merged/from/citations.G.shrink.new.T1.G2.npz.tasks.1.K200.ProNE.K200.T20.O5.w2v.kwc.edges.320.i/new
sbatch -p short -t 1209 --mem=10G --array=900-999 idx_to_pairs.sh 'idx.%d.i' 'idx.%d.i.pairs' 5


sed 100000q /scratch/k.church/tmp/c0269/30194015/118/sort0JjC2e | pairs_to_cos --floats *.f  --record_size 768 | sort -nr | cut -c1-4 | uniq -c


cd /scratch/k.church/semantic_scholar/embeddings/new
K=768
ffile=specter.kwc.edges.f
sbatch --mem=400G -p short -t 1209 --array 0-999 pairs_to_cos.sh $K $ffile 'cos.%03d' idx.???.i.pairs

cd /scratch/k.church/semantic_scholar/embeddings/new
K=768
ffile=specter.kwc.edges.f
# sbatch -p debug --array 197 idx_to_cos.sh $K $ffile 'idx.%03d.i' 
sbatch --mem=400G -p short -t 1209 --array 100-197 idx_to_cos.sh $K $ffile 'idx.%03d.i' 


cd /scratch/k.church/semantic_scholar/citations/graphs/K200/merged/from/citations.G.shrink.new.T1.G2.npz.tasks.1.K200.ProNE.K200.T20.O5.w2v.kwc.edges.320.i/new
ffile=/work/k.church/semantic_scholar/citations/graphs/K200/citations.G.shrink.new.T1.G2.npz.tasks.1.K200.ProNE.K200.T20.O5.w2v.kwc.edges.f
sbatch -p short -t 1209 --mem=100G --array=900-999 idx_to_cos.sh 200 citations.kwc.edges.f 'idx.%03d.i' 


cd /scratch/k.church/semantic_scholar/citations/graphs/K200/merged/from/citations.G.shrink.new.T1.G2.npz.tasks.1.K200.ProNE.K200.T20.O5.w2v.kwc.edges.320.i
mkdir new
for f in `pwd`/*idx.9??.i
do
piece=`echo $f | awk -F. '{print $(NF-1)}'`
ln -s $f new/idx.$piece.i
done




mkdir $HOME/to_go/causality
cd /scratch/k.church/semantic_scholar/citations/graphs/K200/merged/from/citations.G.shrink.new.T1.G2.npz.tasks.1.K200.ProNE.K200.T20.O5.w2v.kwc.edges.320.i
K=200
ffile=/work/k.church/semantic_scholar/citations/graphs/K200/citations.G.shrink.new.T1.G2.npz.tasks.1.K200.ProNE.K200.T20.O5.w2v.kwc.edges.f
map=/work/k.church/semantic_scholar/citations/graphs/citations.G.shrink.new.T1.npz
query=704137
echo $query | 
near_with_floats  --floats $ffile --record_size $K --offset 5 --map $map *.B16.*.i | 
sort -nr -u | 
sed 50q | cut -f1,3 | find_lines --input /work/k.church/semantic_scholar/papers/papers2url.V2/lines2html.V2 --fields '-L' | 
awk 'BEGIN {print "score\tpaper"}; {print}' | 
sed 's/<href/<a href/g' |
tsv_to_html.sh > $HOME/to_go/causality/based_on_citations.html

query=704137
cd /scratch/k.church/semantic_scholar/embeddings/new
ffile=specter.kwc.edges.f
K=768
map=/scratch/k.church/semantic_scholar/embeddings/specter.kwc.nodes.txt
echo $query | near_with_floats  --floats $ffile --record_size $K --offset 1 --map $map specter.kwc.edges.f.simple.seed???.K768.B8.idx.???.i |
sort -nr -u | 
sed 50q | cut -f1,3 | find_lines --input /work/k.church/semantic_scholar/papers/papers2url.V2/lines2html.V2 --fields '-L' |
awk 'BEGIN {print "score\tpaper"}; {print}' | 
sed 's/<href/<a href/g' |
tsv_to_html.sh > $HOME/to_go/causality/based_on_specter.html

query=233186789
mkdir $HOME/to_go/causality.V2
cd /scratch/k.church/semantic_scholar/citations/graphs/K200/merged/from/citations.G.shrink.new.T1.G2.npz.tasks.1.K200.ProNE.K200.T20.O5.w2v.kwc.edges.320.i
K=200
ffile=/work/k.church/semantic_scholar/citations/graphs/K200/citations.G.shrink.new.T1.G2.npz.tasks.1.K200.ProNE.K200.T20.O5.w2v.kwc.edges.f
map=/work/k.church/semantic_scholar/citations/graphs/citations.G.shrink.new.T1.npz
echo $query | 
near_with_floats  --floats $ffile --record_size $K --offset 5 --map $map *.B16.*.i | 
sort -nr -u | 
sed 50q | cut -f1,3 | find_lines --input /work/k.church/semantic_scholar/papers/papers2url.V2/lines2html.V2 --fields '-L' | 
awk 'BEGIN {print "score\tpaper"}; {print}' | 
sed 's/<href/<a href/g' |
tsv_to_html.sh > $HOME/to_go/causality.V2/based_on_citations.html

cd /scratch/k.church/semantic_scholar/embeddings/new
ffile=specter.kwc.edges.f
K=768
map=/scratch/k.church/semantic_scholar/embeddings/specter.kwc.nodes.txt
echo $query | near_with_floats  --floats $ffile --record_size $K --offset 1 --map $map specter.kwc.edges.f.simple.seed???.K768.B8.idx.???.i |
sort -nr -u | 
sed 50q | cut -f1,3 | find_lines --input /work/k.church/semantic_scholar/papers/papers2url.V2/lines2html.V2 --fields '-L' |
awk 'BEGIN {print "score\tpaper"}; {print}' | 
sed 's/<href/<a href/g' |
tsv_to_html.sh > $HOME/to_go/causality.V2/based_on_specter.html


query=704137
mkdir $HOME/to_go/causality.V2
cd /scratch/k.church/semantic_scholar/citations/graphs/K200/merged/from/citations.G.shrink.new.T1.G2.npz.tasks.1.K200.ProNE.K200.T20.O5.w2v.kwc.edges.320.i
K=200
ffile=/work/k.church/semantic_scholar/citations/graphs/K200/citations.G.shrink.new.T1.G2.npz.tasks.1.K200.ProNE.K200.T20.O5.w2v.kwc.edges.f
map=/work/k.church/semantic_scholar/citations/graphs/citations.G.shrink.new.T1.npz
echo $query | 
near_with_floats  --floats $ffile --record_size $K --offset 5 --map $map *.B16.*.i | 
sort -nr -u | 
sed 50q | cut -f3 | find_lines --input /work/k.church/semantic_scholar/papers/papers2url.V2/papers.authors > /tmp/authors

# echo 68637217 | find_lines --input /work/k.church/semantic_scholar/papers/papers2url.V2/papers.authors
# 68637217	5609745|4060793|153519513|52117210|3997961|3513045|50456903|35543960|2556023|48816292|145361498|1764014|6928137

query=9558665
cd /scratch/k.church/semantic_scholar/citations/graphs/K200/merged/from/citations.G.shrink.new.T1.G2.npz.tasks.1.K200.ProNE.K200.T20.O5.w2v.kwc.edges.320.i
K=200
ffile=/work/k.church/semantic_scholar/citations/graphs/K200/citations.G.shrink.new.T1.G2.npz.tasks.1.K200.ProNE.K200.T20.O5.w2v.kwc.edges.f
map=/work/k.church/semantic_scholar/citations/graphs/citations.G.shrink.new.T1.npz
echo $query | 
near_with_floats  --floats $ffile --record_size $K --offset 5 --map $map *.B16.*.i | 
sort -nr -u | 
sed 200q | cut -f3 | find_lines --input /work/k.church/semantic_scholar/papers/papers2url.V2/papers.authors |
cut -f2 | tr '|' '\n' | sort | uniq -c | sort -nr  | awk '$1 > 1 {print $1 "\t" $2}' |
egrep -v NA | find_lines --input /work/k.church/semantic_scholar/authors/authors.url.V2  --fields '-L' 

cd /work/k.church/semantic_scholar/authors
paste authors.id authors.url > authors.url.V2
echo 2500077 | find_lines --input authors.url.V2

query=9558665
cd /scratch/k.church/semantic_scholar/embeddings/new
ffile=specter.kwc.edges.f
K=768
map=/scratch/k.church/semantic_scholar/embeddings/specter.kwc.nodes.txt
echo $query | near_with_floats  --floats $ffile --record_size $K --offset 1 --map $map specter.kwc.edges.f.simple.seed???.K768.B8.idx.???.i |
sort -nr -u | 
sed 200q |  cut -f3 | find_lines --input /work/k.church/semantic_scholar/papers/papers2url.V2/papers.authors |
cut -f2 | tr '|' '\n' | sort | uniq -c | sort -nr  | awk '$1 > 0 {print $1 "\t" $2}' |
egrep -v NA | find_lines --input /work/k.church/semantic_scholar/authors/authors.url.V2  --fields '-L' 

query=13188196
cd /scratch/k.church/semantic_scholar/embeddings/new
ffile=specter.kwc.edges.f
K=768
map=/scratch/k.church/semantic_scholar/embeddings/specter.kwc.nodes.txt
echo $query | near_with_floats  --floats $ffile --record_size $K --offset 1 --map $map specter.kwc.edges.f.simple.seed???.K768.B8.idx.???.i |
sort -nr -u | 
sed 200q |  cut -f3 | find_lines --input /work/k.church/semantic_scholar/papers/papers2url.V2/papers.authors |
cut -f2 | tr '|' '\n' | sort | uniq -c | sort -nr  | awk '$1 > 0 {print $1 "\t" $2}' |
egrep -v NA | find_lines --input /work/k.church/semantic_scholar/authors/authors.url.V2  --fields '-L' 

query=13188196
cd /scratch/k.church/semantic_scholar/citations/graphs/K200/merged/from/citations.G.shrink.new.T1.G2.npz.tasks.1.K200.ProNE.K200.T20.O5.w2v.kwc.edges.320.i
K=200
ffile=/work/k.church/semantic_scholar/citations/graphs/K200/citations.G.shrink.new.T1.G2.npz.tasks.1.K200.ProNE.K200.T20.O5.w2v.kwc.edges.f
map=/work/k.church/semantic_scholar/citations/graphs/citations.G.shrink.new.T1.npz
echo $query | 
near_with_floats  --floats $ffile --record_size $K --offset 5 --map $map *.B16.*.i | 
sort -nr -u | 
sed 200q | cut -f3 | find_lines --input /work/k.church/semantic_scholar/papers/papers2url.V2/papers.authors |
cut -f2 | tr '|' '\n' | sort | uniq -c | sort -nr  | awk '$1 > 1 {print $1 "\t" $2}' |
egrep -v NA | find_lines --input /work/k.church/semantic_scholar/authors/authors.url.V2  --fields '-L' 

query=2147973
cd /scratch/k.church/semantic_scholar/citations/graphs/K200/merged/from/citations.G.shrink.new.T1.G2.npz.tasks.1.K200.ProNE.K200.T20.O5.w2v.kwc.edges.320.i
K=200
ffile=/work/k.church/semantic_scholar/citations/graphs/K200/citations.G.shrink.new.T1.G2.npz.tasks.1.K200.ProNE.K200.T20.O5.w2v.kwc.edges.f
map=/work/k.church/semantic_scholar/citations/graphs/citations.G.shrink.new.T1.npz
echo $query | 
near_with_floats  --floats $ffile --record_size $K --offset 5 --map $map *.B16.*.i | 
sort -nr -u | 
sed 200q | cut -f3 | find_lines --input /work/k.church/semantic_scholar/papers/papers2url.V2/papers.authors |
cut -f2 | tr '|' '\n' | sort | uniq -c | sort -nr  | awk '$1 > 1 {print $1 "\t" $2}' |
egrep -v NA | find_lines --input /work/k.church/semantic_scholar/authors/authors.url.V2  --fields '-L' 

cd /work/k.church/semantic_scholar/papers
zcat *001.gz | jq -c '. | {corpusid,url}' | cut -f3,6 -d'"' | cut -c2- | tr -s ',"/' '	' | awk '{print $1 "\t" $NF}' | head

cd /work/k.church/semantic_scholar/papers/papers2url
sbatch -p express -t 59 --array=1-30 /work/k.church/semantic_scholar/papers/invert_paper_id_file.sh 'papers.piece.%03d.id'

sort -m papers.piece.???.id.inv -T . -S '90%' > papers.piece.id.inv


cd /work/k.church/githubs/scidocs/data/specter-embeddings
for f in *.id
do
find_in_sorted_lines --input /work/k.church/semantic_scholar/papers/papers2url/papers.id.inv < $f > $f.found &
done

cd /work/k.church/semantic_scholar/papers/papers2url
LC_ALL=C sort -T . -S '90%' -m papers.piece.???.txt.titles > papers.titles &

cd /work/k.church/githubs/scidocs/data/specter-embeddings/
for f in *.jsonl.f
do
out=$f.near
err=$out.err
sbatch -e $err -o $out --mem=400G -p short -t 1209 /work/k.church/semantic_scholar/embeddings/M3/semantic_scholar_embedding_near.py \
       --input $f \
       --query_mode binary_vectors \
       --embedding /scratch/k.church/semantic_scholar/embeddings/embeddings.w2v.bin.annoy 
done


curl -X GET https://api.semanticscholar.org/000f90380d768a85e2316225854fc377c079b5c4 -H "x-api-key: Us7RqgayhnaQkEKiEnbGH8EBneX5Jud14Mq3Uzpe" > /tmp/x

cd /work/k.church/githubs/scidocs/data/specter-embeddings
sbatch -e todo.err -o todo.out -p short -t 1209 run_to.sh 

cd /work/k.church/githubs/scidocs/data/specter-embeddings
sbatch -e todo.part2.err -o todo.part2.out -p long,short -t 1209 run_to.part2.sh 


mkdir -p /scratch/k.church/semantic_scholar/citations/graphs/K240
cd /scratch/k.church/semantic_scholar/citations/graphs/K240
npz=citations.npz
sbatch -p short -t 1209 --mem=600G /work/k.church/semantic_scholar/jobs/npz_to_kwc.py -i $npz -o $npz.papers
A=/work/k.church/semantic_scholar/papers/papers2url/papers.authors.normalize.npz
sbatch -p short -t 1209 --mem=900G /work/k.church/semantic_scholar/jobs/npz_to_kwc.py -i $npz -o $npz.papers.authors --matrix $A
job1=29911108
f=citations.npz.papers.kwc.edges.f
K=375
# f=/scratch/k.church/semantic_scholar/embeddings/specter.kwc.edges.f.authors.kwc.edges.f
# ln -s $f $dir
# ln -s /work/k.church/semantic_scholar/authors/authors.id.L $dir
# cd $dir
f=citations.npz.papers.authors.kwc.edges.f
sbatch -p short -t 1209 --array 10-99 $src/C/floats_to_idx.sh $K `basename $f`
job1=29911550
sbatch -d afterany:$job1 -p short -t 1209 --array 10-99 $src/C/floats_to_idx.sh $K `basename $f`


cd /scratch/k.church/semantic_scholar/citations/graphs/K240
mkdir papers authors slurm
cd papers
ln -s /work/k.church/semantic_scholar/citations/graphs/K240/citations.G.shrink.new.T1.G2.npz.ProNE.ProNE.K240.T20.O5.w2v.npz citations.npz
ln -s /work/k.church/semantic_scholar/citations/graphs/citations.G.shrink.new.T1.npz.new_to_old.i map.new_to_old.i
ln -s /work/k.church/semantic_scholar/citations/graphs/citations.G.shrink.new.T1.npz.old_to_new.i map.old_to_new.i
cd /scratch/k.church/semantic_scholar/citations/graphs/K240/papers
npz=citations.npz
K=240
B=6
# job1=`sbatch -p short -t 1209 --mem=300G /work/k.church/semantic_scholar/jobs/npz_to_kwc.py -i $npz -o $npz.papers | awk '{print $NF}'`
sbatch -p short -t 1209 --array 10-99 $src/C/floats_to_idx.sh $K citations.npz.papers.kwc.edges.f $B


cd /work/k.church/githubs/scidocs/data/recomm
python metadata_to_hashes.py < paper_metadata.json | LC_ALL=C sort > hashes.txt


sbatch -p express -t 59 --array 1-30 /work/k.church/semantic_scholar/abstracts/abstracts_to_hashes.sh

cd /work/k.church/githubs/scidocs/data/recomm
python metadata_to_abstract_hashes.py < paper_metadata.json  | LC_ALL=C sort -T . -S '90%' > abstract_hashes.txt

cd /work/k.church/semantic_scholar/abstracts
LC_ALL=C sort -m  -T . -S '90%' abstracts.piece.???.hashes  > abstracts.hashes

LC_ALL=C join -j1 /work/k.church/githubs/scidocs/data/recomm/abstract_hashes.txt /work/k.church/semantic_scholar/abstracts/abstracts.hashes > /work/k.church/githubs/scidocs/data/recomm/abstract_hashes.joined
LC_ALL=C join -j1 /work/k.church/githubs/scidocs/data/recomm/abstract_hashes.txt /work/k.church/semantic_scholar/abstracts/abstracts.hashes1 > /work/k.church/githubs/scidocs/data/recomm/abstract_hashes1.joined

# There are a lot of duplicate abstracts
uniq1.sh abstracts.hashes | awk '{x[NF]++}; END {for(i in x) if(i > 0) print i-1, x[i]}' | sort -n > abstracts.hashes.Nr

uniq1.sh abstracts.hashes | awk 'NF ==2'  > abstracts.hashes1

awk 'NR >1 {print $4}' todo | awk -F/ '{print $NF}' > id_src
tr '{}' '\n' < todo.out | awk -F'"' '/Not found/ {print "NA"; next}; /automatic/ {print $2}' | awk -F/ '{print $NF}' > id_forwarding


awk 'NR >1 {print $4}' todo.part2 | awk -F/ '{print $NF}' > id_src.part2
tr '{}' '\n' < todo.part2.out | awk -F'"' '/Not found/ {print "NA"; next}; /automatic/ {print $2}' | awk -F/ '{print $NF}' > id_forwarding.part2


query=2147973
query=246638887
query=61313353
query=14842425
query=9558665
query=6565772
query=52967399
query=118697759
query=102352962
cd /scratch/k.church/semantic_scholar/citations/graphs/K240/papers
# cd /scratch/k.church/semantic_scholar/citations/graphs/K200/merged/from/citations.G.shrink.new.T1.G2.npz.tasks.1.K200.ProNE.K200.T20.O5.w2v.kwc.edges.320.i
K=240
ffile=citations.npz.papers.kwc.edges.f
map=map
echo $query | 
near_with_floats  --floats $ffile --record_size $K --offset 5 --map $map citations.npz.papers.kwc.edges.f.simple.seed??.K240.B6.idx.??.i | 
sort -nr -u | 
sed 200q | 
cut -f1,3 |
find_lines --input /work/k.church/semantic_scholar/papers/papers2url.V2/lines2html.V2 --fields '-L'  | 
sort -k2 -nr -t'>' | 
sed 50q


query=102352962
cd /scratch/k.church/semantic_scholar/citations/graphs/K240/papers
# cd /scratch/k.church/semantic_scholar/citations/graphs/K200/merged/from/citations.G.shrink.new.T1.G2.npz.tasks.1.K200.ProNE.K200.T20.O5.w2v.kwc.edges.320.i
K=240
ffile=citations.npz.papers.kwc.edges.f
map=map
echo $query | 
near_with_floats  --floats $ffile --record_size $K --offset 5 --map $map citations.npz.papers.kwc.edges.f.simple.seed??.K240.B6.idx.??.i | 
sort -nr -u | 
sed 30q | 
cut -f1,3 |
find_lines --input /work/k.church/semantic_scholar/papers/papers2url.V2/lines2html.V2 --fields '-L' 

query=13188196
query=102352962
cd /scratch/k.church/semantic_scholar/embeddings/new
ffile=specter.kwc.edges.f
K=768
map=/scratch/k.church/semantic_scholar/embeddings/specter.kwc.nodes.txt
echo $query | near_with_floats  --floats $ffile --record_size $K --offset 1 --map $map specter.kwc.edges.f.simple.seed???.K768.B8.idx.???.i |
sort -nr -u | 
sed 50q |  cut -f1,3 | find_lines --input /work/k.church/semantic_scholar/papers/papers2url.V2/lines2html.V2 --fields '-L' 

query=102352962
cd /scratch/k.church/semantic_scholar/embeddings/new
ffile=specter.kwc.edges.f
K=768
map=/scratch/k.church/semantic_scholar/embeddings/specter.kwc.nodes.txt
echo $query | near_with_floats  --floats $ffile --record_size $K --offset 1 --map $map specter.kwc.edges.f.simple.seed???.K768.B8.idx.???.i |
sort -nr -u | 
sed 20q |  cut -f1,3 | find_lines --input /work/k.church/semantic_scholar/papers/papers2url.V2/lines2html.V2 --fields '-L' 

query=102352962
cd /scratch/k.church/semantic_scholar/citations/graphs/K240/papers
# cd /scratch/k.church/semantic_scholar/citations/graphs/K200/merged/from/citations.G.shrink.new.T1.G2.npz.tasks.1.K200.ProNE.K200.T20.O5.w2v.kwc.edges.320.i
K=240
ffile=citations.npz.papers.kwc.edges.f
map=map
echo $query | 
near_with_floats  --floats $ffile --record_size $K --offset 5 --map $map citations.npz.papers.kwc.edges.f.simple.seed??.K240.B6.idx.??.i | 
sort -nr -u | 
sed 20q | 
cut -f1,3 |
find_lines --input /work/k.church/semantic_scholar/papers/papers2url.V2/lines2html.V2 --fields '-L' 


query=$1
dir=/scratch/k.church/semantic_scholar/citations/graphs/K240/papers/
K=240
ffile=$dir/citations.npz.papers.kwc.edges.f
map=$dir/map
echo $query | 
near_with_floats  --floats $ffile --record_size $K --offset 5 --map $map $dir/citations.npz.papers.kwc.edges.f.simple.seed??.K240.B6.idx.1?.i

# returns similar articles
curl 'https://pubmed.ncbi.nlm.nih.gov/?format=pmid&linkname=pubmed_pubmed&from_uid=3945010'

# more info (and easier to parse)
curl 'https://pubmed.ncbi.nlm.nih.gov/?format=pubmed&size=50&linkname=pubmed_pubmed&from_uid=3945010'
curl 'https://pubmed.ncbi.nlm.nih.gov/?format=pubmed&size=50&linkname=pubmed_pubmed&from_uid=6210020'

# description of how similar articles are computed
https://pubmed.ncbi.nlm.nih.gov/help/#computation-of-similar-articles
https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2212667/

cd /work/k.church/semantic_scholar/papers
sbatch --array 1-30 -p short -t 1209 ./lines2ids.py --input 'papers.piece.%03d.gz' --output 'externalids/piece.%03d'


curl 'https://pubmed.ncbi.nlm.nih.gov/?format=pubmed&size=50&linkname=pubmed_pubmed&from_uid=6210020' | awk '$1 == "PMID-" {print $1}' | 
find_lines --input /work/k.church/semantic_scholar/papers/externalids/PubMedCentral | 
egrep -v NA | 
find_lines --input /work/k.church/semantic_scholar/papers/papers2url.V2/lines2html.V2 --fields '-L' 

https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4863348/

echo 6210020 | find_lines --input /work/k.church/semantic_scholar/papers/externalids/PubMedCentral | find_lines --input /work/k.church/semantic_scholar/papers/papers2url.V2/lines2html.V2 --fields '-L' 

K=280
wdir=/work/k.church/semantic_scholar/citations/graphs/K$K
dir=/scratch/k.church/semantic_scholar/citations/graphs/K$K
mkdir -p $dir
cd $dir
mkdir papers authors slurm
cd papers
ln -s $wdir/*.npz citations.npz
ln -s /work/k.church/semantic_scholar/citations/graphs/citations.G.shrink.new.T1.npz.new_to_old.i map.new_to_old.i
ln -s /work/k.church/semantic_scholar/citations/graphs/citations.G.shrink.new.T1.npz.old_to_new.i map.old_to_new.i
npz=citations.npz
B=6
job1=`sbatch -p short -t 1209 --mem=300G /work/k.church/semantic_scholar/jobs/npz_to_kwc.py -i $npz -o $npz.papers | awk '{print $NF}'`
sbatch -d afterany:$job1 -p short -t 1209 --array 10-99 $src/C/floats_to_idx.sh $K citations.npz.papers.kwc.edges.f $B


https://www.ncbi.nlm.nih.gov/research/pubtator/?view=docsum&query=PMC4732746

# The last number is a PMID
https://pubmed.ncbi.nlm.nih.gov/26824844/


https://www.ncbi.nlm.nih.gov/research/pubtator/?view=docsum&query=PMC19038500
https://pubmed.ncbi.nlm.nih.gov/12027509

https://pubmed.ncbi.nlm.nih.gov/?format=pubmed&size=50&linkname=pubmed_pubmed&from_uid=4732746
https://pubmed.ncbi.nlm.nih.gov/?size=50&linkname=pubmed_pubmed&from_uid=4732746

https://pubmed.ncbi.nlm.nih.gov/4732746
https://pubmed.ncbi.nlm.nih.gov/?size=50&linkname=pubmed_pubmed&from_uid=26824844
https://www.ncbi.nlm.nih.gov/research/pubtator/?view=docsum&query=PMC14546

cd /work/k.church/ids/PMIDs_to_crawl
for f in todo.??
do
sbatch -i $f -o $f.out -e $f.err -p short -t 1209 crawl_PMID_for_DOI.sh $f 
done


cd /work/k.church/ids/PMIDs_to_crawl
for f in todo.??
do
sbatch -i $f -o $f.json -e $f.json.err -p short -t 1209 crawl_PMID_with_fatcat.sh $f 
done

cd /work/k.church/ids/DOIs_to_crawl
for f in todo.??
do
sbatch -i $f -o $f.json -e $f.json.err -p short -t 1209 crawl_DOIs_with_fatcat.sh $f 
done

wget -r https://archive.org/download/refcat_2022-01-03

cd   /work/k.church/ids/refcat
mkdir -p refcat_2022-10-03
cd refcat_2022-10-03
for f in refcat-brefcombined-2022-01-03.json.zst refcat-doi-table-2022-01-03.json.zst refcat_2022-01-03_archive.torrent refcat_2022-01-03_files.xml refcat_2022-01-03_meta.sqlite refcat_2022-01-03_meta.xml
do
sbatch -p short -t 1209 -o $f -e $f.err /work/k.church/ids/refcat/wget.sh https://archive.org/download/refcat_2022-01-03/$f
done

cd /work/k.church/ids/PMIDs_to_crawl
jq -c '. | {ext_ids}' todo.vd.json | sed 50q

cd /work/k.church/ids/DOIs_to_crawl2
for f in todo.??
do
sbatch -i $f -o $f.json -e $f.json.err -p short -t 1209 crawl_DOIs_with_fatcat.sh $f 
done


cd /work/k.church/ids/DOIs_to_crawl2
for f in todo.????
do
sbatch -i $f -o $f.json -e $f.json.err -p short -t 1209 crawl_DOIs_with_fatcat.sh $f 
done


head /work/k.church/datasets/nips_reviewer_data/reviewers.txt
1	David Karger
2	Sally Goldman
3	Ramesh Gopinath
4	Bartlett Mel
5	David Andre
6	Dorin Comaniciu
7	Pierre Baldi
8	Shimon Edelman
9	Daphne Koller
10	Yoram Singer

sbatch -p debug,express,short --array 1-30 /work/k.church/datasets/nips_reviewer_data/kwc/get_authors.sh
cut -f2 ../reviewers > /tmp/reviewers2
awk '{print ">" $0 "<"}' /tmp/reviewers > /tmp/reviewers2
cat reviewers.piece.0??.txt | fgrep -f /tmp/reviewers2 | sort -k2 -t'>' > reviewers

http://api.semanticscholar.org/graph/v1/paper/search?query=literature+graph

cd /work/k.church/datasets/nips_reviewer_data/kwc
cut -f2- titles | cut -f1 -d/ | sed 's/ *$//' | tr 'A-Z' 'a-z' > /tmp/queries
fgrep -f /tmp/queries /work/k.church/semantic_scholar/papers/papers2url/papers.titles > title_matches

cd /work/k.church/semantic_scholar/papers/title_hashes
# sbatch -p debug,express,short --array '1-30' title_hashes.py --input ../papers.piece.%03d.gz --output titles.%03d
sbatch -p debug,express,short --array '14' title_hashes.py --input ../papers.piece.%03d.gz --output titles.%03d

cat titles.??? |
tr -d '\r' |  
awk -F'\t' 'length($2) == 32 && $3 ~ /^[ 0-9]*$/ && length($3) > 1' |
cut -f2,3 | sort -S '90%' -T . | uniq1.sh > title_hashes_with_corpusid

mkdir /work/k.church/semantic_scholar/tldrs/txt
cd /work/k.church/semantic_scholar/tldrs/txt
sbatch -p debug,express,short --array '1-30' ../tldr2txt.py  --input ../tldrs.piece.%03d.gz --output tldr.%03d

cd /work/k.church/datasets/nips_reviewer_data/kwc
cut -f3 < training.txt | awk 'match(tolower($0), /abstract/) {print substr($0, 1, RSTART-1); next}; match($0, /[.]/) {print substr($0, 1, RSTART-1); next}' | sed 's/[.] *$//' | cut -c1-200 | awk 'NF > 2' | sort -u > /tmp/titles
fgrep -i -f /tmp/titles < /work/k.church/semantic_scholar/papers/papers2url.V2/lines2html.V2 > training_matches

cd /work/k.church/semantic_scholar/papers/papers2url.V2/
tr 'A-Z' 'a-z' < lines2html.V2 | sort -t: -k3 -S '90%' -T . > lines2html.V2.sorted_by_titles


sbatch -p short -t 1209 --mem=40G /work/k.church/semantic_scholar/papers/papers2url.V2/sort_by_titles_job.sh


cat /tmp/titles |
awk '{print ":: " tolower($0)}'  | 
find_in_sorted_lines --multiple_matches --partial_matches --delimiter : --field 3  --input /work/k.church/semantic_scholar/papers/papers2url.V2/lines2html.V2.sorted_by_titles > /tmp/titles.out

egrep -v 'NA$' /tmp/titles.out | cut -f1,2 | sort | uniq1.sh > /tmp/titles.out2

awk 'BEGIN {FS=OFS="\t"; 
while(getline < "/tmp/titles.out2" > 0) 
if(NF == 2) found[$1]=$2
else if(NF == 3) found[$1]=$2 "|" $3;
else if(NF == 4) found[$1]=$2 "|" $3 "|" $4}
{key= ":: " tolower($3); 
if(match(key, /abstract/)) {key=substr(key, 1, RSTART-1)}; 
if(match(key, /[.] *$/)) {key=substr(key, 1, RSTART-1)}};
$1 == "NA" && key in found {print found[key] substr($0, 3); next}
{print}' training.txt > training2.txt

cd /work/k.church/datasets/nips_reviewer_data/kwc
cut -f3- training2.txt | 
awk 'match(tolower($0), /abstract/) {title = substr($0, 1, RSTART-1); abstract = substr($0, RSTART + RLENGTH); print title "\t" abstract; next}
     match($0, /[.]/) {title = substr($0, 1, RSTART-1); abstract = substr($0, RSTART + RLENGTH); print title "\t" abstract}' | 



echo ':: an introduction to contemporary' | find_in_sorted_lines --multiple_matches --partial_matches --delimiter : --field 3  --input /work/k.church/semantic_scholar/papers/papers2url.V2/lines2html.V2.sorted_by_titles  | head
:: an introduction to contemporary	73566639	<a href="https://www.semanticscholar.org/paper/297a9e7b35ce28dd888b71a70f93fe73e52ce166">1: an introduction to contemporary achievements in intelligent systems</a>
:: an introduction to contemporary	161911091	<a href="https://www.semanticscholar.org/paper/25304ba8534f4610b29553918fb8e6e18f64f706">12: an introduction to contemporary american fiction</a>
:: an introduction to contemporary	247289273	<a href="https://www.semanticscholar.org/paper/8e96f4df5ac7e37a841121f2e8d0ed3875a317df">0: an introduction to contemporary american fiction</a>
:: an introduction to contemporary	217531189	<a href="https://www.semanticscholar.org/paper/d238bae844c7b480aa8a62d1f3b1770e6746fe24">0: an introduction to contemporary archaeological theory : confronting dualisms</a>
:: an introduction to contemporary	140329986	<a href="https://www.semanticscholar.org/paper/31aa2f9f5d9a594a524d144497af4a5287c79492">4: an introduction to contemporary behaviorism</a>
:: an introduction to contemporary	166695371	<a href="https://www.semanticscholar.org/paper/8f42e54b7c4f142caee87f895ea69b2698db82e7">2: an introduction to contemporary business</a>
:: an introduction to contemporary	64172735	<a href="https://www.semanticscholar.org/paper/cd7aaa3d05e222ce94bf52fd8f85f9f709c489fa">2: an introduction to contemporary business</a>
:: an introduction to contemporary	195322705	<a href="https://www.semanticscholar.org/paper/a0147d3aa245c6fb8db60d4b034d345ccea6a05c">0: an introduction to contemporary chinese pulse diagnosis</a>
:: an introduction to contemporary	156922699	<a href="https://www.semanticscholar.org/paper/6f656ba874e2fd7689913fe3729d57ef47bfae64">0: an introduction to contemporary communitarianism</a>
:: an introduction to contemporary	59685077	<a href="https://www.semanticscholar.org/paper/f82340efc9e315920badd74c11a003dd8a9dda5e">1: an introduction to contemporary computing with xml in science and technology, natural language engineering & machine learning and self adaptive systems pervasive computing</a>

cd /work/k.church/datasets/nips_reviewer_data/kwc
sbatch  --mem=40G -p short -t 1209 /work/k.church/semantic_scholar/papers/papers2url.V2/run_titles_job.sh


cd /work/k.church/semantic_scholar/abstracts/txt
job1=`sbatch --array 1-30 ../abstracts_to_text.py --input '../abstracts.piece.%03d.gz' --output 'abstracts.piece.%03d.txt' | awk '{print $NF}'`
sbatch  --mem=40G -d afterany:$job1 -p short -t 1209 --array 1-30 ../sufsort.sh 'abstracts.piece.%03d.txt'

cd /work/k.church/semantic_scholar/abstracts/txt
sbatch  --mem=400G -p short -t 1209 --array 1-30 ../sufsort.sh 'abstracts.piece.%03d.txt'


sbatch  --mem=400G -p short -t 1209 /work/k.church/semantic_scholar/papers/papers2url.V2/run_titles_job2.sh

cd /work/k.church/semantic_scholar/papers/papers2url.V2/
awk '{split($0, x, ":"); s = length(x[1]) + length(x[2]) + 4; e = length($0) - s -4; print(substr($0, s, e) "\t" $1)}' lines2html.V2 > titles2ids
echo 'Hommage à Ronald Dworki' | find_lines_nonnumeric --input /work/k.church/semantic_scholar/papers/papers2url.V2/titles2ids

python $src/specter_embeddings.py --input /tmp/papers.json --output /tmp/papers.npy

cd /work/k.church/datasets/nips_reviewer_data/kwc
cut -f3- training2.txt | 
awk 'match(tolower($0), /abstract/) {title = substr($0, 1, RSTART-1); abstract = substr($0, RSTART + RLENGTH); print title "\t" abstract; next}
     # match($0, /[.]/) {title = substr($0, 1, RSTART-1); abstract = substr($0, RSTART + RLENGTH); print title "\t" abstract}
' | cut -c1-400 > training2.for_specter

cd /work/k.church/datasets/nips_reviewer_data/kwc/for_specter
egrep -i abstract ../training2.txt | cut -f1 > ids
split -l 10 ids ids.



python $src/specter_embeddings.py --input training2.for_specter --output training2.npy

cd /work/k.church/datasets/nips_reviewer_data/kwc
sbatch  --mem=400G -p short -t 1209 $src/specter_embeddings.py --input training2.for_specter --output training2.npy
	
cd /work/k.church/semantic_scholar/papers/papers2url.V2/
sbatch  --mem=40G -p short -t 1209 titles2ids.sh

cd /work/k.church/datasets/nips_reviewer_data/kwc/for_specter
for f in todo.??
do
sbatch -e $f.err  --mem=40G -p short -t 1209 $src/specter_embeddings.py --input $f --output $f.npy
done


cd /work/k.church/datasets/nips_reviewer_data/kwc/for_specter
mkdir -p near_specter
for f in *.npy
do
out=near_specter/`basename $f .npy`.near
err=$out.err
if [ ! -s $out ]
then
sbatch -e $err -o $out --mem=400G -p short -t 1209 /work/k.church/semantic_scholar/embeddings/M3/semantic_scholar_embedding_near.py \
       --input $f \
       --query_mode npy_vectors \
       --embedding /scratch/k.church/semantic_scholar/embeddings/embeddings.w2v.bin.annoy 
fi
done

find /work/k.church -maxdepth 5 -name '*.sh' > $HOME/many_sh.txt

cd /work/k.church/datasets/nips_reviewer_data/kwc/go_fetch
for f in todo.if
do
sbatch  -i $f -o $f.out -e $f.err -p short $src/fetch_specter_embedding_and_compare.py
done

echo 'CorpusId:10002984' | $src/fetch_similarities_from_semantic_scholar_api.py > /tmp/x.sim

mkdir /work/k.church/datasets/nips_reviewer_data/kwc/fetch_similarities_from_semantic_scholar_api
cd /work/k.church/datasets/nips_reviewer_data/kwc/fetch_similarities_from_semantic_scholar_api
cp ../go_fetch/todo.?? .

cd /work/k.church/datasets/nips_reviewer_data/kwc/fetch_similarities_from_semantic_scholar_api
for f in todo.b?
do
sbatch  -i $f -o $f.out -e $f.err -p express,short $src/fetch_similarities_from_semantic_scholar_api.py
done

cd /work/k.church/datasets/nips_reviewer_data/kwc/fetch_similarities_from_semantic_scholar_api
for f in todo.b?
do
cat $f | tr '|' '\n' | awk '{print "#!/bin/sh\n/work/k.church/githubs/scidocs/data/recomm/line2specter.txt.sh", $1}' > ../line2specter.txt/$f
cat $f | tr '|' '\n' | awk '{print "#!/bin/sh\n/work/k.church/githubs/scidocs/data/recomm/line2proposed.txt.sh", $1}' > ../line2proposed.txt/$f
done

for f in  /work/k.church/datasets/nips_reviewer_data/kwc/line2proposed.txt/todo.?? /work/k.church/datasets/nips_reviewer_data/kwc/line2specter.txt/todo.??
do
sbatch  -o $f.out -e $f.err -p short $f
done

cd /work/k.church/semantic_scholar/papers/papers2url.V2
split -l 1000000 titles2ids pieces/titles2ids.

cd /scratch/k.church/semantic_scholar/papers/papers2url.V2
for f in titles2ids.??
do
sbatch -p express,short /work/k.church/semantic_scholar/papers/papers2url.V2/sufsort_job.sh $f
done

cd /work/k.church/githubs/scidocs/data/recomm/externalIds
for f in ids.??
do
sbatch -p express,short -i $f -o $f.out -e $f.err $src/fetch_from_semantic_scholar_api.py externalIds
done

cd /work/k.church/githubs/scidocs/data/recomm
for f in ids2
do
sbatch -t 1209 -p short -i $f -o $f.externalIds.out -e $f.externalIds.err $src/fetch_from_semantic_scholar_api.py externalIds
done


/work/k.church/githubs/scidocs/data/recomm/PMID2sim.txt.sh 22230384 | find_lines --input /work/k.church/semantic_scholar/papers/papers2url.V2/lines2html.V2 --fields '-L'
PMID:21869730	<a href="https://www.semanticscholar.org/paper/2dc09525be01a2fb7adc4fb10ea14773a4e3b108">36: Effect of folic acid intervention on the change of serum folate level in hypertensive Chinese adults: do methylenetetrahydrofolate reductase and methionine synthase gene polymorphisms affect therapeutic responses?</a>
PMID:29371246	<a href="https://www.semanticscholar.org/paper/da65f740ce848b719d93b0369e508a7b85df3f36">18: MTHFR Gene and Serum Folate Interaction on Serum Homocysteine Lowering: Prospect for Precision Folic Acid Treatment</a>
PMID:28360116	<a href="https://www.semanticscholar.org/paper/59a0565036017cb163b6b527a6f1bc5826cac183">58: Homocysteine and Stroke Risk: Modifying Effect of Methylenetetrahydrofolate Reductase C677T Polymorphism and Folic Acid Intervention</a>
PMID:26337056	<a href="https://www.semanticscholar.org/paper/92806133e47139a46cfecd61be622ef832f75341">28: Joint associations of folate, homocysteine and MTHFR, MTR and MTRR gene polymorphisms with dyslipidemia in a Chinese hypertensive population: a cross-sectional study</a>
PMID:26266420	<a href="https://www.semanticscholar.org/paper/6cf6640e5f4006a860f5e3185fa00dada61c1c6d">53: Homocysteine Metabolism Gene Polymorphisms (MTHFR C677T, MTHFR A1298C, MTR A2756G and MTRR A66G) Jointly Elevate the Risk of Folate Deficiency</a>
PMID:19263808	<a href="https://www.semanticscholar.org/paper/c0681dc28dfe305fef92927a0a051c82db9e1959">30: Plasma homocysteine and gene polymorphisms associated with the risk of hyperlipidemia in northern Chinese subjects.</a>
PMID:22470444	<a href="https://www.semanticscholar.org/paper/4ecf0a40b5fc7b343c005f32820e9f68b1a15b3d">70: Polymorphisms in MTHFR, MS and CBS Genes and Homocysteine Levels in a Pakistani Population</a>
PMID:26991917	<a href="https://www.semanticscholar.org/paper/2a1cfa7055d444b14c3c5493aff602398699272e">23: Effect of folic acid supplementation on cancer risk among adults with hypertension in China: A randomized clinical trial</a>
PMID:21603981	<a href="https://www.semanticscholar.org/paper/e61dc1c8167e68797fb56c1bd8b45e6316a8ea13">26: Gene polymorphisms involved in folate and methionine metabolism and increased risk of sporadic colorectal adenocarcinoma</a>
PMID:24103477	<a href="https://www.semanticscholar.org/paper/c40c74e5fa8f584742c997c89f717dcffd295b0a">48: Methylenetetrahydrofolate reductase C677T and methionine synthase A2756G polymorphisms influence on leukocyte genomic DNA methylation level.</a>

/work/k.church/githubs/scidocs/data/recomm/PMID2sim.txt.sh 22230384 > /tmp/x

dir=/scratch/k.church/semantic_scholar/embeddings/new
K=768
ffile=$dir/specter.kwc.edges.f
map=/scratch/k.church/semantic_scholar/embeddings/specter.kwc.nodes.txt

awk 'NR == 1 {query=$2; next}; {print query "\t" $2}' /tmp/x | 
pairs_to_cos --record_size $K --map $map --floats $ffile | 
find_lines --input /work/k.church/semantic_scholar/papers/papers2url.V2/lines2html.V2 --fields '--L'


K=280
dir=/scratch/k.church/semantic_scholar/citations/graphs/K$K/papers/
ffile=$dir/citations.npz.papers.kwc.edges.f
map=$dir/map

awk 'NR == 1 {query=$2; next}; {print query "\t" $2}' /tmp/x | 
pairs_to_cos --record_size $K --map $map --floats $ffile | 
find_lines --input /work/k.church/semantic_scholar/papers/papers2url.V2/lines2html.V2 --fields '--L'

cd /work/k.church/githubs/scidocs/data/recomm
cat ids*out | tr "'" '"' | python ids2tsv.py | awk 'killroy[$0]++ < 1' > ids.tsv

cut -f4 ids.tsv | egrep -v '[A-Za-z]' | awk '{print "#!/bin/sh"; print "/work/k.church/githubs/scidocs/data/recomm/PMID2sim.txt.sh", $1}' | split -l 200 - train/pubmed/todo.


cd /work/k.church/githubs/scidocs/data/recomm/train/pubmed
prevjob=""
for f in todo.??
do
if [ $prevjob ]
then
dependency="-d afterany:$prevjob"
fi
prevjob=`sbatch $dependency -p short -t 1209 -o $f.out -e $f.err $f | awk '{print $NF}'`
done

cd /work/k.church/pubmed/ftp.ncbi.nlm.nih.gov/pubmed/baseline/ids/
query=30212113
for f in "`ls ../pubmed22n????.xml.gz | awk '{print substr($0, 1, 23-8) "?.xml.gz" }' | sort -u`"
do
sbatch  -p debug,express,short count_matches.sh $query $f
done


cd /work/k.church/pubmed/ftp.ncbi.nlm.nih.gov/pubmed/baseline/ids/
query=Household.Energy.Interventions 
ls ../pubmed22n????.xml.gz | awk '{print substr($0, 1, 23-8) "?.xml.gz" }' | sort -u | 
while read f
do
sbatch  -p debug,express,short count_matches.sh $query $f
done


cd /work/k.church/githubs/scidocs/data/recomm
cut -f4 ids.tsv | egrep -v '[A-Za-z]' | awk '{print "#!/bin/sh"; print "/work/k.church/githubs/scidocs/data/recomm/PMID2sim.txt.sh", $1}' | split -l 200 - train/pubmed/todo.

https://pypi.org/project/semanticscholar/


https://grobid.readthedocs.io/en/latest/Introduction/

cd /work/k.church/datasets/nips_reviewer_data/kwc
awk ' $1 != "NA" {next};  match(tolower($0), /abstract/) {print substr($0, 1, RSTART-1); next}; match($0, /[.] /) {print substr($0,1,RSTART-1)}' training3.txt | cut -f3 | sed 's/[.,] *$//' | sort -u | egrep ...  > titles2json/todo

cd /work/k.church/datasets/nips_reviewer_data/kwc
sbatch -p short -t 1209 -i titles2json/todo -o titles2json/todo.json titles2json.sh 

/work/k.church/githubs/scidocs/data/recomm/author2pages.sh 9215251 $HOME/to_go/Drago
/work/k.church/githubs/scidocs/data/recomm/author2pages.sh 1702139 $HOME/to_go/authors/Jure_Leskovec
/work/k.church/githubs/scidocs/data/recomm/author2pages.sh 1686223 $HOME/to_go/authors/Valia_Kordoni
/work/k.church/githubs/scidocs/data/recomm/author2pages.sh 144783904 $HOME/to_go/authors/Christopher_Manning
/work/k.church/githubs/scidocs/data/recomm/author2pages.sh 145419642 $HOME/to_go/authors/Percy_Liang

/work/k.church/githubs/scidocs/data/recomm/author2pages.sh 1840645 $HOME/to_go/authors/Silvio_Amir
/work/k.church/githubs/scidocs/data/recomm/author2pages.sh 49474944 $HOME/to_go/authors/Beth_Noveck



echo 239709636     | /work/k.church/semantic_scholar/citations/graphs/src/fetch_references_and_citations.py > /tmp/x

K=280
dir=/scratch/k.church/semantic_scholar/citations/graphs/K$K/papers/
ffile=$dir/citations.npz.papers.kwc.edges.f
map=$dir/map
awk '$1 == "reference"' /tmp/x | cut -f2-3 | egrep -v ERROR | pairs_to_cos --record_size $K --floats $ffile --map $map


dir=/scratch/k.church/semantic_scholar/embeddings/new
K=768
ffile=$dir/specter.kwc.edges.f
map=/scratch/k.church/semantic_scholar/embeddings/specter.kwc.nodes.txt

awk '$1 == "reference"' /tmp/x | cut -f2-3 | egrep -v ERROR | pairs_to_cos --record_size $K --floats $ffile --map $map | sort -nr | find_lines --input /work/k.church/semantic_scholar/papers/papers2url.V2/lines2html.V2 --fields '--L'
awk '$1 == "citation"' /tmp/x | cut -f2-3 | egrep -v ERROR | pairs_to_cos --record_size $K --floats $ffile --map $map | sort -nr | find_lines --input /work/k.church/semantic_scholar/papers/papers2url.V2/lines2html.V2 --fields '--L'

adir=/work/k.church/githubs/scidocs/data/recomm/
$dir/line2both.sh 239709636 280 > $HOME/to_go/foo.html

Who should I cite: learning literature search models from citation behavior
https://www.semanticscholar.org/paper/Who-should-I-cite%3A-learning-literature-search-from-Bethard-Jurafsky/ba2e4508a45303d22b01e4eade24b8137d55fd37
5963413

https://cs.stanford.edu/people/jure/pubs/citations-jcdl10.pdf
Citing for High Impact

echo 118344292 | $src/fetch_papers_from_authors.py

/work/k.church/semantic_scholar/citations/graphs/src/score_papers_and_authors.py --papers 221523184,14030641 --authors 118344292,1795513

echo $id | $src/fetch_references_and_citations.py > /tmp/x
dir=/scratch/k.church/semantic_scholar/embeddings/new
K=768
ffile=$dir/specter.kwc.edges.f
map=/scratch/k.church/semantic_scholar/embeddings/specter.kwc.nodes.txt

awk '$1 == "reference"' /tmp/x | cut -f2-3 | egrep -v ERROR | pairs_to_cos --record_size $K --floats $ffile --map $map | sort -nr | find_lines --input /work/k.church/semantic_scholar/papers/papers2url.V2/lines2html.V2 --fields '--L'
awk '$1 == "citation"' /tmp/x | cut -f2-3 | egrep -v ERROR | pairs_to_cos --record_size $K --floats $ffile --map $map | sort -nr | find_lines --input /work/k.church/semantic_scholar/papers/papers2url.V2/lines2html.V2 --fields '--L'


cd /work/k.church/semantic_scholar/papers
PATH=$PATH:$src:$src/C:$src/C/approx_string_match

in=/tmp/x 
zcat  papers.piece.010.gz | sed 10000q | jq -c '. | {title}' | cut -f4 -d'"' > $in

number_of_shingles=30
shingle_length=9

intern_strings --max_string_length 1000 --shingle_length $shingle_length --number_of_shingles $number_of_shingles --output $in.sketches < $in

# print_sketches --sketches $in.sketches --shingle_length $shingle_length --number_of_shingles $number_of_shingles < $in

intern_sketches --sketches $in.sketches --shingle_length $shingle_length --number_of_shingles $number_of_shingles --output_symbol_table $in.sketches.symbol_table > $in.sketches.interned


// TODO: make this work!!!!
invert_file --input $in.sketches.interned --output $in.sketches.interned.inverted --record_size $number_of_shingles


print_sketches --brief_mode --sketches $in.sketches --shingle_length $shingle_length --number_of_shingles $number_of_shingles | LC_ALL=C sort -u > $in.sketches.s

echo 4 20  foobar | score_sketches --postings $in.sketches.interned.inverted  --verbose --sketch_k 5
npostings = 300000, npointings_index = 300001
queries = 4, 20
post0 (23): 0 754 1002 2379 3238 4319 4707 5196 5209 5247 5848 6006 6413 6454 6678 6978 6979 7781 8126 8741 9010 9672 9848
post1 (7): 0 754 2684 2869 6673 8187 8741
last = 3238
post0 (5): 0 754 1002 2379 3238
post1 (4): 0 754 2684 2869
as = 2, bs = 3, cs = 2, ds = 3231
a = 4.357848, b = 18.642152, c = 2.642152, d = 299974.357848	4 20 foobar


# for i in `seq 0 99`
# do
# $src/random_permutation.py $shingle_length $in.sketches.perm.$i
# done

# # for f in /tmp/x.perm.?
# # do
# # x_to_y La < $f > $f.txt
# # done

# # paste /tmp/x.perm.?.txt

# for i in `seq 0 99`
# do
# echo i = $i
# index_sketches --sketches $in.sketches --permutation $in.sketches.perm.$i --shingle_length $shingle_length --number_of_shingles $number_of_shingles --output $in.sketches.idx.$i
# done

# for f in $in.sketches.idx.? $in.sketches.idx.??
# do
# x_to_y La < $f  | awk '{print NR, $0}'
# done

# paste /tmp/x.sketches.idx.?.txt

for f in $in.sketches.idx.? $in.sketches.idx.??
do
x_to_y La < $f | awk 'NR > 1 {print prev, $1}; {prev=$1}' 
done | sort -u | score_sketches --sketches $in.sketches --shingle_length $shingle_length --number_of_shingles $number_of_shingles | 
cut -f1 -d. | sort -n | uniq -c | sort -nr

echo `wc -l $in` | awk '{for(i=0;i<$1;i++) for(j=0;j<$1;j++) if(i < j) print i, j}' | score_sketches --sketches $in.sketches --shingle_length $shingle_length --number_of_shingles $number_of_shingles |
cut -f1 -d. | sort -n | uniq -c | sort -nr

# sed 1q $in > $in.1
# intern_strings --max_string_length 1000 --shingle_length $shingle_length --number_of_shingles $number_of_shingles --output $in.1.sketches --verbose < $in.1

id=250150926
src=/work/k.church/semantic_scholar/citations/graphs/src
echo $id | $src/fetch_references_and_citations.py > /tmp/x
awk '$1 == "reference"' /tmp/x | cut -f2-3 | egrep -v ERROR  | $src/score_pairs_of_papers.py > /tmp/x.scored
sort -nr /tmp/x.scored | find_lines --input /work/k.church/semantic_scholar/papers/papers2url.V2/lines2html.V2 --fields '--L'


adir=/work/k.church/githubs/scidocs/data/recomm/
$adir/line2both.sh $id 280 > $HOME/to_go/Muskan.html

adir=/work/k.church/githubs/scidocs/data/recomm/
id=13626864
$adir/line2both.sh $id 280 > $HOME/to_go/Choudhury.html

id=32730475
id=18742499
echo $id | $src/fetch_references_and_citations.py > /tmp/x


awk '$1 == "reference" && $4 > 100' /tmp/x | egrep -v ERROR | sort -k4 -nr | cut -f3 | /work/k.church/githubs/scidocs/data/recomm/ids2pages.sh $HOME/to_go/social_media_and_fake_news

awk '$4 > 100 && $4 ~ /[0-9]/' /tmp/x | egrep -v ERROR | cut -f3 | $src/fetch_from_semantic_scholar_api.py --fields authors > /tmp/x.authors

find_lines --input /work/k.church/semantic_scholar/authors/authors.url.V2 < /tmp/x.authors2 | sed 's/<[^>]*>//g' | awk '{printf "/work/k.church/githubs/scidocs/data/recomm/author2pages.sh %d $HOME/to_go/authors/social_media/%s\n", $1, $NF}'

egrep ACL ACL_papers.ids | cut -f2 | /work/k.church/githubs/scidocs/data/recomm/ids2pages.sh $HOME/to_go/ACL_responsible
egrep ACL ACL_papers.ids | cut -f2 | $src/fetch_references_and_citations.py >  $HOME/to_go/ACL_responsible/refrences_and_citations.txt
awk -F'\t' '$1 != "query" && $4 > 100' $HOME/to_go/ACL_responsible/refrences_and_citations.txt| egrep -v ERROR | cut -f3- | sort -u | sort -k2 -nr > $HOME/to_go/ACL_responsible/refrences_and_citations.100.txt
cat `find ACL_responsible -name '*.html'` | tr '"' '\n' | egrep https | sort -u | egrep paper | awk -F/ '{print $NF}' | $src/fetch_from_semantic_scholar_api.py --fields citationCount,externalIds,title,authors > ACL_responsible/papers.json

egrep  'reference.218971825' ACL_responsible/refrences_and_citations.txt | egrep -v ERROR | sort -k4 -nr | cut -f3 | /work/k.church/githubs/scidocs/data/recomm/ids2pages.sh $HOME/to_go/ACL_responsible_survey2
218971825

cut -f3 /home/k.church/to_go/ACL_responsible/refrences_and_citations.txt  | egrep '^[0-9]*$' | $src/fetch_from_semantic_scholar_api.py --fields externalIds,title,citationCount > /home/k.church/to_go/ACL_responsible/refrences_and_citations.json &

python ~/to_go/ACL_responsible/jsons2tsv.py < /home/k.church/to_go/ACL_responsible/refrences_and_citations.json > /home/k.church/to_go/ACL_responsible/refrences_and_citations.json.tsv
python ~/to_go/ACL_responsible/jsons2titles.py < /home/k.church/to_go/ACL_responsible/refrences_and_citations.json > /home/k.church/to_go/ACL_responsible/refrences_and_citations.json.title

egrep  'reference.218971825' $HOME/to_go/ACL_responsible/refrences_and_citations.txt | egrep -v ERROR | cut -f3  | egrep '^[0-9]*$' | $src/fetch_from_semantic_scholar_api.py --fields externalIds,title,citationCount,fieldsOfStudy,s2FieldsOfStudy > /home/k.church/to_go/ACL_responsible/survey_references.json &

egrep  'citation.218971825' $HOME/to_go/ACL_responsible/refrences_and_citations.txt | egrep -v ERROR | cut -f3  | egrep '^[0-9]*$' | $src/fetch_from_semantic_scholar_api.py --fields externalIds,title,citationCount,fieldsOfStudy,s2FieldsOfStudy > /home/k.church/to_go/ACL_responsible/survey_citations.json & 

    320 Computer Science
      1 Economics
      1 Engineering
      6 Mathematics
      9 Medicine
     66 NA
      1 Physics
      5 Psychology
      4 Sociology
(gft) [k.church@c0229 ~]$ python ~/to_go/ACL_responsible/jsons2fieldsOfStudy.py < /home/k.church/to_go/ACL_responsible/survey_references.json | tr '\t' '\n' | sort | uniq -c
0 errors

fan-in & fan-out & field \\
320 &   167 & Computer Science \\
66  &    5 & NA \\
9   &  10 & Medicine \\
6   &  22 & Mathematics \\
5   &  32 & Psychology \\
4   &  28 & Sociology \\
1   &   3 & Engineering \\
1 & 0 & Pysics \\
0   &   4 & Political Science \\
0   &   1 & Philosophy \\
0   &   1 & History \\
0   &   1 & Art \\ \hline
396 & 216 & Totals \\ \hline



fan-in & fan-out \\
citations & Field of Study & citations & Field of Study \\
5512 &	Computer Science & 10663	& Sociology \\
700  &	Computer Science & 10018	& Psychology, Medicine \\
282  &	Computer Science & 7740	& Philosophy \\
183  &	Computer Science & 2027	& Medicine \\
173  &	Computer Science & 1750	& Computer Science, Mathematics \\
162  &	Computer Science & 1443	& Computer Science, Medicine \\
100  &	Computer Science & 1069	& Psychology, Medicine \\
100  &	Computer Science & 989	& Engineering \\
89   &	Computer Science & 962	& Engineering \\
87   &	Computer Science & 788	& Sociology \\



sort -k3 -nr /tmp/foobar | awk '{print NR "\t" $0}' | sort -k3 -nr | awk  '{print NR "\t" $0}' | awk '$1 <= 5 || $2 <= 5'
1	1	0	0.723692	0.992347	86: Assessing Social and Intersectional Biases in Contextualized Word Representations
2	10	0	0.713732	0.976964	17: Quantifying Social Biases in Contextual Word Representations
3	7	0	0.708066	0.989925	0: Towards an Enhanced Understanding of Bias in Pre-trained Neural Language Models: A Survey with Special Emphasis on Affective Bias
4	16	0	0.703372	0.031126	0: Recipiency and peripheral participation in language brokering
5	11	0	0.699034	0.968625	0: How Pre-trained Language Models Capture Factual Knowledge? A Causal-Inspired Analysis
12	5	0	0.678026	0.990411	110: Identifying and Reducing Gender Bias in Word-Level Language Models
15	3	0	0.639074	0.991580	1: Richer Countries and Richer Representations
16	2	0	0.630936	0.991911	83: Social Bias in Elicited Natural Language Inferences
17	4	0	0.602719	0.991493	345: Gender Bias in Coreference Resolution: Evaluation and Debiasing Methods



1 & 1 & 86  & 101 & Assessing Social and Intersectional Biases in Contextualized Word Representations \\
2 & 10 & 17 18  &Quantifying Social Biases in Contextual Word Representations \\
3 & 7 & 0  & 1 & Towards an Enhanced Understanding of Bias in Pre-trained Neural Language Models: A Survey with Special Emphasis on Affective Bias \\
4 & 16 & 0 & 0  &Recipiency and peripheral participation in language brokering \\
5 & 11 & 0   &0 &  How Pre-trained Language Models Capture Factual Knowledge? A Causal-Inspired Analysis \\
12 & 5 & 110 & 124 & Identifying and Reducing Gender Bias in Word-Level Language Models \\
15 & 3 & 1  & 1 & Richer Countries and Richer Representations \\
16 & 2 & 83   & 91 &  Social Bias in Elicited Natural Language Inferences \\
17 & 4 & 345 &  384 & Gender Bias in Coreference Resolution: Evaluation and Debiasing Methods \\ \hline

cut -f1-2 -d: < /work/k.church/semantic_scholar/papers/papers2url.V2/lines2html.V2 | tr '>' ' ' | awk '{print $1 "\t" $NF}' > /work/k.church/semantic_scholar/papers/papers2url.V2/papers_citations.txt
cut -f2 < papers_citations.txt | sort -n | uniq -c > papers_citations.txt.Nr
awk '{x[$2]++}; END {for(i in x) print x[i] "\t" i}' papers_citations.txt | sort -k2 -n > papers_citations.txt.Nr.V2 &

cd /work/k.church/semantic_scholar/papers
mkdir -p years
for f in papers.piece.???.gz
do
sbatch -o years/`basename $f .gz`  -p short papers2year.py $f
done


cd /work/k.church/semantic_scholar/citations/graphs
import numpy as np
>>> X = np.load('citations.G.X.npy')
>>> X[0:10]
array([2, 2, 2, 2, 2, 2, 2, 2, 2, 2], dtype=int32)
>>> Y = np.load('citations.G.Y.npy')
>>> Y[0:10]
array([ 15455464, 221268337,   1100052,  12085723,   4430392,  11632848,
        20085438,  14092784,  10204914,  37465479], dtype=int32)

cd /work/k.church/semantic_scholar/papers/years
cat papers.piece.??? | egrep -iv none | sort  -T . -S '90%'  > years

awk 'NF == 2 && $0 !~ /[a-zA-Z]/' years | cut -f1 | awk '$1 > N {N=$1}; END {print N}'
249151006


cd /work/k.church/semantic_scholar/papers/years
awk 'NF == 2 && $0 !~ /[a-zA-Z]/' years | create_years_vector 250000000  > years.short

sbatch -p express -e years.err  $src/citation_years.py --graph /work/k.church/semantic_scholar/citations/graphs/citations.G.npz --years years.short -o years.npz

sbatch -p short -t 1209 $src/summarize_citation_years.py years.npz years_summary.npy

echo 9558665 | $src/fetch_from_semantic_scholar_api.py --citations --limit 1000 --fields contexts > /tmp/PMI
echo 220872254 | $src/fetch_from_semantic_scholar_api.py --citations --limit 1000 --fields contexts > /tmp/Wallace_survey.json

echo 73712 | $src/fetch_from_semantic_scholar_api.py --citations --limit 1000 --fields contexts > /tmp/Turing


cd /scratch/j.ortega/
awk '$2 < 0.1 {print $1}' may_august_comparison.txt | /work/k.church/semantic_scholar/citations/graphs/src/fetch_from_semantic_scholar_api.py --fields title,abstract > /work/k.church/semantic_scholar/j.ortega/zeros.json

cd /scratch/j.ortega/
awk '$2 < 0.999 && $2 > 0.1 {print $1}' may_august_comparison.txt | /work/k.church/semantic_scholar/citations/graphs/src/fetch_from_semantic_scholar_api.py --fields title,abstract > /work/k.church/semantic_scholar/j.ortega/small.json

cd /work/k.church/semantic_scholar/releases/2022-08-23/database/abstracts
sbatch -p express -t 59 --array 1-30 /work/k.church/semantic_scholar/abstracts/abstracts_to_hashes.sh

cd /work/k.church/semantic_scholar/releases/2022-12-02/database/abstracts
sbatch -p express -t 59 --array 1-30 /work/k.church/semantic_scholar/abstracts/abstracts_to_hashes.sh



cd /work/k.church/semantic_scholar/releases/2022-08-23/database/abstracts
cat abstracts.piece.???.hashes |
awk '{print $2 "\t" $1}' |
sort  -T . -S '90%'  > id.hashes.s &

cd /work/k.church/semantic_scholar/abstracts
cat abstracts.piece.???.hashes |
awk '{print $2 "\t" $1}' |
sort  -T . -S '90%'  > id.hashes.s &

cd /work/k.church/semantic_scholar/abstracts
comm id.hashes.s /work/k.church/semantic_scholar/releases/2022-08-23/database/abstracts/id.hashes.s |
awk -F'\t' 'NF != 4' | sed 's/^[	]*//' | 
uniq1.sh > /work/k.church/semantic_scholar/releases/2022-08-23/database/abstracts/id.hashes.mismatches_from_may &

cut -f1 /work/k.church/semantic_scholar/abstracts/id.hashes.s | uniq > /tmp/may.ids &
cut -f1 /work/k.church/semantic_scholar/releases/2022-08-23/database/abstracts/id.hashes.s | uniq > /tmp/aug.ids &

for f in /tmp/*.ids
do
sort -u  -T . -S '90%'  $f > $f.s &
done

comm /tmp/*.ids.s | awk -F'\t' '{x[NF]++}; END {for(i in x) print x[i] "\t" i}' | sort -nr
94385744	3
2753234	1
830980	2

comm -3 /tmp/*.ids.s > /work/k.church/semantic_scholar/releases/2022-08-23/database/abstracts/id.mismatches_may_aug

head /work/k.church/semantic_scholar/releases/2022-08-23/database/abstracts/id.mismatches_may_aug
100000068
100000169
100000192
	100000450
100000453
100000628
100000745
100001028
100001193
	100001650



I just got a CSV export of the whole IATE EU terminology database onto Discovery at /home/yue.r/terminology_management/IATE_export.csv.
 
It seems the file has no header, but it includes English term, target language, equivalent term in target language, and a reliability indicator that specifies if the provided term has been reviewed by a domain expert.
 
TermID|English term|Target language|Equivalent term|Term type|Reliability|
 
Two examples:
 
892670|financial institutions and credit|it|operatore in opzioni|Term|Minimum reliability|
3619545|political culture|fr|culture de l'effacement|Term|Reliable|
 
Should be a good source for cross-checking terminology from Scibert.
 
Richard

rel=2022-08-23
rel=2022-12-02
cd /work/k.church/semantic_scholar/releases/$rel/database/citations
mkdir -p graphs
for f in citations.piece.???.gz
do
b=`basename $f .gz`
out=graphs/$f.graph
err=graphs/$f.err
sbatch -p short -o $out -e $err /work/k.church/semantic_scholar/citations/citations2graph.py $f
done

rel=2022-08-23
rel=2022-12-02
cd /work/k.church/semantic_scholar/releases/$rel/database/citations/graphs
for f in citations.piece.???.gz.graph
do
export src
sbatch -p short /work/k.church/semantic_scholar/releases/2022-08-23/database/citations/graphs/graph2npz.sh $f
done

cd /work/k.church/semantic_scholar/releases/2022-08-23/database/citations/graphs
for S in 0.1
do
G=citations.G
sbatch -p short -t 1209 -e $G.S$S.err sample_edges.py -G $G.npz -o $G.S$S.npz -S $S
done

# killroy was here
rel=2022-08-23
rel=2022-12-02
cd /work/k.church/semantic_scholar/releases/$rel/database/citations/graphs
sbatch -p short -e citations.G.err sum_matrices.py citations.G citations.piece.???.gz.graph.npz



cd /work/k.church/semantic_scholar/releases/2022-08-23/database/citations/graphs
sbatch -p short -e citations.CC.err -o citations.CC.out connected_components.py -G citations.G.npz -C citations.CC

cd /work/k.church/semantic_scholar/releases/2022-08-23/database/citations/graphs
for S in 0.1
do
f=citations.CC.S.$S
sbatch -d afterany:32765246  -p short -e $f.err -o $f.out connected_components.py -G citations.G.S.$S.npz -C $f
done

cd /work/k.church/semantic_scholar/releases/2022-08-23/database/citations/graphs
f=citations.G.S.0.1.npz
T=1
S=0.1
sbatch --mem=100G -t 1209 -p short -e `basename $f .npz`.T$T.err shrink_matrix.py -G $f -C citations.CC.S.$S.npy -o citations.G.S.$S.shrink.T$T -T $T


# cd /work/k.church/semantic_scholar/releases/2022-08-23/database/citations/graphs
# for f in citations.G.npz
# do
# T=1
# sbatch --mem=100G -t 1209 -p short -e citations.G.shrink.T$T.err shrink_matrix.py -G $f -C citations.CC.npy -o citations.G.shrink.T$T -T $T
# done
# Submitted batch job 32422211


cd /work/k.church/semantic_scholar/releases/2022-08-23/database/citations/graphs
T=1
for f in citations.G.shrink.T$T.G2.npz # citations.G2.shrink.T$T.npz
do
for K in 270 290 # 280 300 # 100 250 # 290 #  300 # 300 # 200 768 # 100  20 50 # 10 # 50 100 768
do
sbatch -p long -t 7199 -e $f.prone.K$K.err --mem=1900G $HOME/final/morphology/dict_to_embedding.py -G $f -o $f.K$K -K $K
done
done

# cd /work/k.church/semantic_scholar/releases/2022-08-23/database/citations/graphs
# T=1
# for f in citations.G.shrink.T$T.G2.npz
# do
# for K in 150 
# do
# sbatch -p long -t 7199 -e $f.prone.K$K.err --mem=800G  $HOME/final/morphology/dict_to_embedding.py -G $f -o $f.K$K -K $K
# done
# done

# cd /work/k.church/semantic_scholar/releases/2022-08-23/database/citations/graphs
# T=1
# for f in citations.G.shrink.T$T.G2.npz
# do
# for K in 150 
# do
# sbatch -p debug -e $f.prone.K$K.debug.err  $HOME/final/morphology/dict_to_embedding.py -G $f -o $f.K$K.debug -K $K
# done
# done


# return_singluar_vectors=u --> no v.  That's what we want
https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.linalg.svds.html#scipy.sparse.linalg.svds

SVD_with_classes3.py -V 20  -n 10 -C 3 

http://publications.csail.mit.edu/lcs/pubs/pdf/MIT-LCS-TM-641.pdf
A Stream Algorithm for the SVD

Fast online SVD revisions for lightweight recommender systems

cd /work/k.church/semantic_scholar/authors
sbatch -p debug,express,short --array 1-30 ../author2id.py --hindex 10 --input 'authors.piece.%03d.gz' --output 'authors.piece.%03d.id.hindex10'

cd /work/k.church/semantic_scholar/papers
sbatch -p express,short --array 1-30 papers2ACL_authors.py --input 'papers.piece.%03d.gz' --output 'papers.piece.%03d.ACL_authors'

cat papers.piece.*.ACL_authors | sort -u > papers.ACL_authors
sort -u /work/k.church/semantic_scholar/authors/*hindex10* > /tmp/x
comm -12 /tmp/x papers.ACL_authors > papers.ACL_authors.hindex10

../author_select.py --input authors.piece.019.gz --output - --select ../papers/papers.ACL_authors.hindex10

cd /work/k.church/semantic_scholar/authors
sbatch -p debug,express,short --array 1-30 ../author_select.py --input 'authors.piece.%03d.gz' --output 'authors.piece.%03d.json.hindex10' --select ../papers/papers.ACL_authors.hindex10

python foo.py < authors.json.hindex10 > authors.json.hindex10.sorted

cd /work/k.church/semantic_scholar/releases/2022-08-23/database/citations/graphs
for S in 0.1
do
G=citations.G
sbatch -p short -t 1209 -e $G.S$S.err sample_edges.py -G $G.npz -o $G.S$S.npz -S $S
done

G=citations.G.S0.1.npz
outf=`basename $G .npz`.CC
sbatch -p express -e $outf.err -o $outf.out connected_components.py -G $G -C $outf

cd /work/k.church/semantic_scholar/releases/2022-08-23/database/citations/graphs
G=citations.G.S0.1.npz
CC=`basename $G .npz`.CC.npy
T=1
S=0.1
outf=citations.G.S$S.shrink.T$T
sbatch --mem=80G -t 1209 -p short -e `basename $outf .npz`.err shrink_matrix.py -G $G -C $CC -o $outf -T $T

cd /work/k.church/semantic_scholar/releases/2022-08-23/database/citations/graphs
T=1
for f in citations.G.S0.1.shrink.T1.G2.npz
do
for K in  300 # 200 60 100 # 50 # 250 # 290 #  300 # 300 # 200 768 # 100  20 50 # 10 # 50 100 768
do
sbatch -p short -t 1209 -e $f.prone.K$K.err --mem=800G $HOME/final/morphology/dict_to_embedding.py -G $f -o $f.K$K -K $K
done
done


cd /work/k.church/semantic_scholar/releases/2022-08-23/database/citations/graphs
for S in 0.5
do
G=citations.G
job1=`sbatch -p short -t 1209 -e $G.S$S.err sample_edges.py -G $G.npz -o $G.S$S.npz -S $S |  awk '{print $NF}'`
G=citations.G.S$S.npz
outf=`basename $G .npz`.CC
job2=`sbatch -d afterany:$job1 -p express -e $outf.err -o $outf.out connected_components.py -G $G -C $outf |  awk '{print $NF}'`

CC=`basename $G .npz`.CC.npy
T=1
outf=citations.G.S$S.shrink.T$T
sbatch -d afterany:$job2 --mem=80G -t 1209 -p short -e `basename $outf .npz`.err shrink_matrix.py -G $G -C $CC -o $outf -T $T
done

cd /work/k.church/semantic_scholar/releases/2022-08-23/database/citations/graphs
T=1
for f in citations.G.S0.5.shrink.T1.G2.npz
do
for K in 200 60 100 150 # 50 # 250 # 290 #  300 # 300 # 200 768 # 100  20 50 # 10 # 50 100 768
do
sbatch -p short -t 1209 -e $f.prone.K$K.err --mem=400G $HOME/final/morphology/dict_to_embedding.py -G $f -o $f.K$K -K $K
done
done


cd /work/k.church/semantic_scholar/releases/2022-08-23/database/citations/graphs
f=citations.G.S0.1.shrink.T1.G2.npz.K50.ProNE.K50.T20.O5.w2v
sbatch -e $f.err -p express /work/k.church/semantic_scholar/jobs/npz_to_kwc.py --input $f.npz  --output $f

cd /work/k.church/semantic_scholar/releases/2022-08-23/database/citations/graphs
f=citations.G.S0.5.shrink.T1.G2.npz.K50.ProNE.K50.T20.O5.w2v
sbatch -e $f.err -p express /work/k.church/semantic_scholar/jobs/npz_to_kwc.py --input $f.npz  --output $f


cd /work/k.church/semantic_scholar/releases/2022-08-23/database/citations/graphs
for f in citations.G.S0.*.shrink.*w2v*.npz
do
outf=`basename $f .npz`.kwc.edges.f
if [ -s $outf ]
then
echo $outf is already done
else
echo working on $outf
sbatch -e $f.err -p express /work/k.church/semantic_scholar/jobs/npz_to_kwc.py --input $f  --output `basename $f .npz`
fi
done




create_mapping_files.py -i citations.G.S0.1.shrink.T1.npz
create_mapping_files.py -i citations.G.S0.5.shrink.T1.npz

id=18742499
echo $id | $src/fetch_references_and_citations.py > /tmp/x

cd /work/k.church/semantic_scholar/releases/2022-08-23/database/citations/graphs
map=citations.G.S0.5.shrink.T1.npz
ffile=citations.G.S0.5.shrink.T1.G2.npz.K50.ProNE.K50.T20.O5.w2v.kwc.edges.f
K=50
awk '$2 ~ /^[0-9]*$/ && $3 ~ /^[0-9]*$/ {print $2 "\t" $3}' /tmp/x | pairs_to_cos --record_size $K --floats $ffile --map $map > /tmp/x.S50.K$K.out


cd /work/k.church/semantic_scholar/releases/2022-08-23/database/citations/graphs
map=citations.G.S0.1.shrink.T1.npz
ffile=citations.G.S0.1.shrink.T1.G2.npz.K50.ProNE.K50.T20.O5.w2v.kwc.edges.f
K=50
awk '$2 ~ /^[0-9]*$/ && $3 ~ /^[0-9]*$/ {print $2 "\t" $3}' /tmp/x | pairs_to_cos --record_size $K --floats $ffile --map $map > /tmp/x.S10.K$K.out

ffile=citations.G.S0.1.shrink.T1.G2.npz.K100.ProNE.K100.T20.O5.w2v.kwc.edges.f
K=100
awk '$2 ~ /^[0-9]*$/ && $3 ~ /^[0-9]*$/ {print $2 "\t" $3}' /tmp/x | pairs_to_cos --record_size $K --floats $ffile --map $map > /tmp/x.S10.K$K.out

ffile=citations.G.S0.1.shrink.T1.G2.npz.K60.ProNE.K60.T20.O5.w2v.kwc.edges.f
K=60
awk '$2 ~ /^[0-9]*$/ && $3 ~ /^[0-9]*$/ {print $2 "\t" $3}' /tmp/x | pairs_to_cos --record_size $K --floats $ffile --map $map > /tmp/x.S10.K$K.out

K=280
dir=/work/k.church/semantic_scholar/citations/graphs/K$K/papers
ffile=$dir/citations.npz.papers.kwc.edges.f
map=$dir/map
awk '$2 ~ /^[0-9]*$/ && $3 ~ /^[0-9]*$/ {print $2 "\t" $3}' /tmp/x | pairs_to_cos --record_size $K --floats $ffile --map $map > /tmp/x.S100.K280.out

echo `ls /tmp/x.S*out | cut -f2,3 -d.` > /tmp/xK
paste /tmp/x.S*out | cut -f1,4,7,10,13 >> /tmp/xK  
x  = read.table("/tmp/xK", header=T)
cor(x)

          K100      K280       K50       K60
K100 1.0000000 0.4277161 0.9340746 0.9274641
K280 0.4277161 1.0000000 0.3900238 0.3506334
K50  0.9340746 0.3900238 1.0000000 0.9203264
K60  0.9274641 0.3506334 0.9203264 1.0000000


mkdir -p /scratch/k.church/semantic_scholar/releases/2022-08-23/database/citations/graphs
cd /scratch/k.church/semantic_scholar/releases/2022-08-23/database/citations/graphs
sbatch -p short -t 1209 --mem=400G --array 60-99 /work/k.church/semantic_scholar/releases/2022-08-23/database/citations/graphs/sample_edges.sh 0.1 50

mkdir -p /scratch/k.church/semantic_scholar/releases/2022-08-23/database/citations/graphs
cd /scratch/k.church/semantic_scholar/releases/2022-08-23/database/citations/graphs
sbatch -p short -t 1439 --mem=400G --array 60-99 /work/k.church/semantic_scholar/releases/2022-08-23/database/citations/graphs/sample_edges.sh 0.45 60

mkdir -p /scratch/k.church/semantic_scholar/releases/2022-08-23/database/citations/graphs
cd /scratch/k.church/semantic_scholar/releases/2022-08-23/database/citations/graphs
sbatch -x d0130 -p short -t 1439 --mem=400G --array 60-99 /work/k.church/semantic_scholar/releases/2022-08-23/database/citations/graphs/sample_edges.sh 0.55 60

mkdir -p /scratch/k.church/semantic_scholar/releases/2022-08-23/database/citations/graphs
cd /scratch/k.church/semantic_scholar/releases/2022-08-23/database/citations/graphs
sbatch -x d0130 -p short -t 1439 --mem=400G --array 60-99 /work/k.church/semantic_scholar/releases/2022-08-23/database/citations/graphs/sample_edges.sh 0.65 60

mkdir -p /scratch/k.church/semantic_scholar/releases/2022-08-23/database/citations/graphs
cd /scratch/k.church/semantic_scholar/releases/2022-08-23/database/citations/graphs
# sbatch -p short -t 1439 --mem=500G --array 60-62 /work/k.church/semantic_scholar/releases/2022-08-23/database/citations/graphs/sample_edges.sh 0.20 100
for S in 0.50 0.40
do
for K in 80 100
do
sbatch -p short -t 1439 --mem=500G --array 60-62 /work/k.church/semantic_scholar/releases/2022-08-23/database/citations/graphs/sample_edges.sh $S $K
done
done

mkdir -p /scratch/k.church/semantic_scholar/releases/2022-08-23/database/citations/graphs
cd /scratch/k.church/semantic_scholar/releases/2022-08-23/database/citations/graphs
# sbatch -p short -t 1439 --mem=500G --array 60-62 /work/k.church/semantic_scholar/releases/2022-08-23/database/citations/graphs/sample_edges.sh 0.20 100
for S in 0.60
do
for K in 80
do
sbatch -p short -t 1439 --mem=800G --array 70-89 /work/k.church/semantic_scholar/releases/2022-08-23/database/citations/graphs/sample_edges.sh $S $K
done
done

rel=2022-08-23
rel=2022-12-02
mkdir -p /scratch/k.church/semantic_scholar/releases/$rel/database/citations/graphs
cd /scratch/k.church/semantic_scholar/releases/$rel/database/citations/graphs
for S in 0.40 0.50 0.57
do
for K in 50 80 
do
sbatch -p short -t 1439 --mem=600G --array 60-63 /work/k.church/semantic_scholar/releases/$rel/database/citations/graphs/sample_edges.sh $S $K
done
done

# March 28

rel=2022-08-23
rel=2022-12-02
mkdir -p /scratch/k.church/semantic_scholar/releases/$rel/database/citations/graphs
cd /scratch/k.church/semantic_scholar/releases/$rel/database/citations/graphs
for S in 0.70 0.80
do
for K in 50 80 100
do
sbatch -p short -t 1439 --mem=800G --array 60-63 /work/k.church/semantic_scholar/releases/$rel/database/citations/graphs/sample_edges.sh $S $K
done
done


rel=2022-08-23
rel=2022-12-02
mkdir -p /scratch/k.church/semantic_scholar/releases/$rel/database/citations/graphs
cd /scratch/k.church/semantic_scholar/releases/$rel/database/citations/graphs
for S in 0.01
do
for K in 10
do
sbatch -p short -t 1439  --array 11 /work/k.church/semantic_scholar/releases/$rel/database/citations/graphs/sample_edges.sh $S $K
done
done






for f in /scratch/k.church/semantic_scholar/releases/2022-08-23/database/citations/graphs/S0.1/K50/??/M.ProNE.K50*npz
do
outf=`dirname $f`/M
if [ ! -s $outf.kwc.edges.f ]
then
echo working on $outf
sbatch --mem=100GB -e $f.err -p short -t 1209 /work/k.church/semantic_scholar/jobs/npz_to_kwc.py --input $f  --output $outf
fi
done


cd /scratch/k.church/semantic_scholar/releases/2022-08-23/database/citations/graphs/S0.1/K50/

id=18742499
echo $id | $src/fetch_references_and_citations.py > refs_and_citations.txt
awk '$2 ~ /^[0-9]*$/ && $3 ~ /^[0-9]*$/ {print $2 "\t" $3}' refs_and_citations.txt > pairs.txt

for dir in /scratch/k.church/semantic_scholar/releases/2022-08-23/database/citations/graphs/S*/K60/[0-9][0-9]
do
# echo `date` working on $dir
K=60
ffile=$dir/M.kwc.edges.f
map=$dir/Gshrunk.npz
if [ ! -s $dir/pairs.out ]
then
sbatch -p express -e $dir/pairs.err -o $dir/pairs.out -i /scratch/k.church/semantic_scholar/releases/2022-08-23/database/citations/graphs/S0.1/K50/pairs.txt /scratch/k.church/semantic_scholar/releases/2022-08-23/database/citations/graphs/S0.1/K50/pairs_to_cos.sh --record_size $K --floats $ffile --map $map
fi
done

for f in /scratch/k.church/semantic_scholar/releases/2022-08-23/database/citations/graphs/S*/K*/??/pairs.out
do
if [ -s $f ]
then
cut -f1 $f > $f.1
fi
done

K=280
dir=/work/k.church/semantic_scholar/citations/graphs/K$K/papers
ffile=$dir/citations.npz.papers.kwc.edges.f
map=$dir/map
sbatch -p express -e pairs.err -o pairs.K280.out -i pairs.txt pairs_to_cos.sh --record_size $K --floats $ffile --map $map

do

cut -f1 pairs.K280.out > pairs.K280.out.1

id=18742499
K280=/scratch/k.church/semantic_scholar/releases/2022-08-23/database/citations/graphs/S0.1/K50/pairs.K280.out.1
for dir in /scratch/k.church/semantic_scholar/releases/2022-08-23/database/citations/graphs/S*/K*
do
paste $K280 $dir/??/pairs.out.1 > $dir/result.$id
done

x.S55.K60 = read.table("/scratch/k.church/semantic_scholar/releases/2022-08-23/database/citations/graphs/S0.55/K60/result.18742499")
x.S65.K60 = read.table("/scratch/k.church/semantic_scholar/releases/2022-08-23/database/citations/graphs/S0.65/K60/result.18742499")

x=x.S65.K60
ymax = apply(x[,2:23],1,max)
ymean = apply(x[,2:23],1,mean)
ymin = apply(x[,2:23],1,min)
ymedian = apply(x[,2:23],1,median)

g = glm(x[,1] ~ ymax + ymean + ymin + ymedian)

cor(cbind(x.S65.K60[,1:5],x.S55.K60[,1:5]))

mkdir -p /scratch/k.church/semantic_scholar/releases/2022-08-23/database/papers

# externalids
for rel in 2022-12-02 2022-11-22 2022-08-23
do
in="/work/k.church/semantic_scholar/releases/$rel/database/papers/papers.piece.%03d.gz"
out="/scratch/k.church/semantic_scholar/releases/$rel/database/external_ids/papers.piece.%03d.external_ids"
mkdir -p `dirname $out`
sbatch -t 59  --array 1-30 -p express /work/k.church/semantic_scholar/citations/graphs/src/semantic_scholar_zip_to_external_ids.py -i $in -o $out
done

for rel in  2022-11-22 2022-08-23 # 2022-12-02
do
for SRC in `ls /scratch/k.church/semantic_scholar/releases/$rel/database/external_ids/*001* | awk -F. '{print $NF}'`
do
mkdir -p /scratch/k.church/semantic_scholar/releases/$rel/database/external_ids/$SRC
ln -s /scratch/k.church/semantic_scholar/releases/$rel/database/external_ids/papers.piece.*.external_ids.$SRC /scratch/k.church/semantic_scholar/releases/$rel/database/external_ids/$SRC
done
done





# papers
rel=2022-11-22
rel=2022-12-02
in="/work/k.church/semantic_scholar/releases/$rel/database/papers/papers.piece.%03d.gz"
out="/scratch/k.church/semantic_scholar/releases/$rel/database/papers/papers.piece.%03d.ids"
mkdir -p `dirname $out`
sbatch -t 59  --array 1-30 -p express /work/k.church/semantic_scholar/citations/graphs/src/semantic_scholar_zip_to_ids.py -i $in -o $out

for f in embeddings # abstracts tldrs embeddings s2orc
do
in="/work/k.church/semantic_scholar/releases/$rel/database/$f/$f.piece.%03d.gz"
out="/scratch/k.church/semantic_scholar/releases/$rel/database/$f/$f.piece.%03d.ids"
mkdir -p `dirname $out`
sbatch -t 59 --array 1-30 -p express /work/k.church/semantic_scholar/citations/graphs/src/semantic_scholar_zip_to_ids.py -i $in -o $out
done

# citations
# citingcorpusid":"238686036","citedcorpusid
for f in citations
do
in="/work/k.church/semantic_scholar/releases/$rel/database/$f/$f.piece.%03d.gz"
for id in citingcorpusid citedcorpusid
do
out="/scratch/k.church/semantic_scholar/releases/$rel/database/$f/$id.piece.%03d.ids"
mkdir -p `dirname $out`
sbatch -t 1209 --array 1-30 -p short /work/k.church/semantic_scholar/citations/graphs/src/semantic_scholar_zip_to_ids.py -i $in -o $out --id $id
done
done


cd /scratch/k.church/semantic_scholar/releases
for f in `find . -name '*.ids'`
do
if [ ! -s $f.sorted_uniq ]
then 
echo $f
sbatch -i $f -o $f.sorted_uniq -t 59 -p express /work/k.church/semantic_scholar/jobs/sort_job.sh
fi
done

cd /scratch/k.church/semantic_scholar/releases
for rel in  2022-11-22 2022-08-23 2022-12-02
do
for SRC in ACL ArXiv CorpusId DBLP DOI MAG PubMed PubMedCentral
do
for f in /scratch/k.church/semantic_scholar/releases/$rel/database/external_ids/$SRC/*.$SRC
do
if [ ! -s $f.sorted_uniq ]
then 
echo $f
sbatch -e $f.err -i $f -o $f.sorted_uniq -t 59 -p express /work/k.church/semantic_scholar/jobs/sort_job.sh
fi
done
done
done







cd /scratch/k.church/semantic_scholar/releases
find . -name '*.ids' -exec wc -l {} \; > ids & 
find . -name '*.sorted_uniq' -exec wc -l {} \; > sorted_uniq &

cd /scratch/k.church/semantic_scholar/releases
for dir in `find /scratch/k.church/semantic_scholar/releases/*/database/external_ids/* -type d` ./2022-11-22/database/* ./2022-12*/database/*
do
if [ ! -s $dir/sorted_uniq ]
then
echo $dir
sbatch -o $dir/sorted_uniq -t 1209 -p short /work/k.church/semantic_scholar/jobs/sort_job.sh -m $dir/*.sorted_uniq
fi
done

cd /scratch/k.church/semantic_scholar/releases
for d in 2022-11-22 2022-08-23 2022-12-02 2022-05-31
do 
dir=$d/database/cited
mkdir -p $dir
sbatch -o $dir/sorted_uniq -t 1209 -p short /work/k.church/semantic_scholar/jobs/sort_job.sh -m $d/database/citations/citedcorpusid.piece.*.ids.sorted_uniq
dir=$d/database/citing
mkdir -p $dir
sbatch -o $dir/sorted_uniq -t 1209 -p short /work/k.church/semantic_scholar/jobs/sort_job.sh -m $d/database/citations/citingcorpusid.piece.*.ids.sorted_uniq
done

cd /work/k.church/semantic_scholar/releases/2022-08-23/database/citations/graphs
ls citations.G.shrink.T1.G2.npz.K*.ProNE.K*.T20.O5.w2v.npz
for f in citations.G.shrink.T1.G2.npz.K*.ProNE.K*.T20.O5.w2v.npz
do
sbatch -p short -t 1209 --mem=400G /work/k.church/semantic_scholar/jobs/npz_to_kwc.py -i $f -o `basename $f .npz`
done

cd /scratch/k.church/semantic_scholar/releases
mkdir -p venn5
ls */*/*/sorted_uniq | 
awk '{x[NR]=$0}; END {for(i in x) for(j in x) if(i > j) print  x[i], x[j]}' |
awk '{printf "sbatch -o venn5/%03d -p express -t 59 /work/k.church/semantic_scholar/jobs/comm_job.sh %s %s\n", NR, $1, $2}' | sh

cd /scratch/k.church/semantic_scholar/releases
awk '{printf "%0.2f\t%0.2f\t%0.2f\t%s\t%s\n", $1/1e6, $2/1e6, $3/1e6, $4, $5}' venn5/??? | sort -nr -k3 | sed 's-/sorted_uniq--g' | sed 's-/database--g'  > /tmp/v5

cd /scratch/k.church/semantic_scholar/releases
awk '{printf "%0.2f\t%0.2f\t%0.2f\t%s\t%s\n", $1/1e6, $2/1e6, $3/1e6, $4, $5}' venn2/?? | sort -nr -k3 | awk '($NF ~ /abs/ || $NF ~ /cit/) && ($(NF-1) ~ /abs/ || $(NF-1) ~ /cit/)'

cd /scratch/k.church/semantic_scholar/releases; awk '{printf "%0.2f\t%0.2f\t%0.2f\t%s\t%s\n", $1/1e6, $2/1e6, $3/1e6, $4, $5}' venn2/?? | sort -nr -k3 


cd /scratch/k.church/semantic_scholar/releases; awk '{printf "%0.2f\t%0.2f\t%0.2f\t%s\t%s\n", $1/1e6, $2/1e6, $3/1e6, $4, $5}' venn2/?? | sort -nr -k3 | awk '($NF ~ /abs/ || $NF ~ /emb/ || $NF ~ /paper/) && ($(NF-1) ~ /abs/ || $(NF-1) ~ /emb/ || $(NF-1) ~ /paper/)'

/scratch/j.ortega/paper_recommender/semantic_scholar_specter/120k_venn

cd /scratch/k.church/semantic_scholar/releases
venn  /scratch/k.church/semantic_scholar/releases/2022*/database/embeddings/sorted_uniq > 3way/venn/embeddings &
venn  /scratch/k.church/semantic_scholar/releases/2022*/database/papers/sorted_uniq > 3way/venn/papers &
venn  /scratch/k.church/semantic_scholar/releases/2022*/database/abstracts/sorted_uniq > 3way/venn/abstracts &


cd /scratch/k.church/semantic_scholar/releases
mkdir -p 4way/venn
venn  /scratch/k.church/semantic_scholar/releases/2022*/database/embeddings/sorted_uniq > 4way/venn/embeddings &
venn  /scratch/k.church/semantic_scholar/releases/2022*/database/papers/sorted_uniq > 4way/venn/papers &
venn  /scratch/k.church/semantic_scholar/releases/2022*/database/abstracts/sorted_uniq > 4way/venn/abstracts &


cd /scratch/k.church/semantic_scholar/releases
venn  2022*/database/papers/sorted_uniq 2022*/database/abstracts/sorted_uniq 2022*/database/embeddings/sorted_uniq > 4way/venn/papers.abstracts.embeddings &

cd /scratch/k.church/semantic_scholar/releases
venn 2022*/database/external_ids/*/sorted_uniq > 4way/venn/external_ids &
venn 2022*/database/papers/sorted_uniq 2022*/database/abstracts/sorted_uniq 2022*/database/embeddings/sorted_uniq 2022*/database/external_ids/*/sorted_uniq > 4way/venn/papers.abstracts.embeddings.external_ids &

cd /scratch/k.church/semantic_scholar/releases
for SRC in ACL ArXiv CorpusId DBLP DOI MAG PubMed PubMedCentral
do
venn 2022*/database/external_ids/$SRC/sorted_uniq > 4way/venn/$SRC &
done


cd /scratch/k.church/semantic_scholar/releases
venn  2022*/database/papers/sorted_uniq 2022*/database/abstracts/sorted_uniq 2022*/database/embeddings/sorted_uniq \
2022*/database/citations/sorted_uniq \
2022*/database/cited/sorted_uniq \
2022*/database/citing/sorted_uniq > 4way/venn/papers.abstracts.embeddings.citations.cited.citing &


cd /scratch/k.church/semantic_scholar/releases
venn `find .  -name 'sorted_uniq' | egrep cit | sort` > 4way/venn/cit &

cd /scratch/k.church/semantic_scholar/releases/4way/venn
for f in papers.abstracts.embeddings.citations.cited.citing # papers.abstracts.embeddings # papers abstracts embeddings
do
awk '/done/ {next}; 
     $1 == "#" {print $0; next};
     $2 ~ /^[0-9]*$/ {x[$2]++}; 
     END {for(i in x) printf "%0.6f\t%s\n", x[i]/1e6, i}' $f | sort -nr > $f.summary &
done

cd /scratch/k.church/semantic_scholar/releases/4way/venn
for f in `ls | egrep -v summary`
do
if [ ! -s $f.summary ]
then
echo $f
awk '/done/ {next}; 
     $1 == "#" {print $0; next};
     $2 ~ /^[0-9]*$/ {x[$2]++}; 
     END {for(i in x) printf "%0.6f\t%s\n", x[i]/1e6, i}' $f | sort -nr > $f.summary &
fi
done

cd /scratch/k.church/semantic_scholar/releases/4way/venn
for SRC in ACL ArXiv CorpusId DBLP DOI MAG PubMed PubMedCentral
do
echo `egrep 111$ $SRC.summary` $SRC
done | sort -nr

left=2022-12-02//database/external_ids
right=sorted_uniq
cd /scratch/k.church/semantic_scholar/releases/
venn $left/CorpusId/$right \
$left/MAG/$right \
$left/DOI/$right \
$left/PubMed/$right  \
$left/DBLP/$right  \
$left/PubMedCentral/$right \
$left/ArXiv/$right  \
$left/ACL/$right > /scratch/k.church/semantic_scholar/releases/4way/venn/external_ids.2022-12-02 &

left=2022-12-02//database/external_ids
right=sorted_uniq
cd /scratch/k.church/semantic_scholar/releases/
venn $left/CorpusId/$right \
$left/MAG/$right \
$left/DOI/$right \
$left/PubMed/$right  \
$left/DBLP/$right  \
$left/PubMedCentral/$right \
$left/ArXiv/$right  \
$left/ACL/$right \
2022-12*/database/papers/sorted_uniq \
2022-12*/database/abstracts/sorted_uniq \
2022-12*/database/citing/sorted_uniq \
2022-12*/database/cited/sorted_uniq > /scratch/k.church/semantic_scholar/releases/4way/venn/external_ids.more.2022-12-02 &

f=/scratch/k.church/semantic_scholar/releases/4way/venn/external_ids.more.2022-12-02
awk '/done/ {next}; 
     $1 == "#" {print $0; next};
     $2 ~ /^[0-9]*$/ {x[$2]++}; 
     END {for(i in x) printf "%0.6f\t%s\n", x[i]/1e6, i}' $f | sort -nr > $f.summary &


# cd /scratch/k.church/semantic_scholar/releases/2022-08-23/database/citations/graphs
cd /work/nlp/k.church/MAG240M
ls slurm*out | cut -f2 -d- | cut -f1 -d. | awk '{print "seff", $1}' |  sh > seff.out

awk -F'/' '/connected_compenents: my_load:/ {print substr(FILENAME, 7, length(FILENAME)-10), $(NF-3), $(NF-2)}' OFS="\t" slurm*out > slurm.summary
awk '/Array Job ID:/ {if(job) print job, state, mem, time; job=$NF; mem=time=state="NA"};  /State:/ {state=$2}; /Job Wall-clock time:/ {time=$NF}; /Memory Utilized:/ {mem=$(NF-1) " " $NF}; END {print job, state, mem, time}' OFS="\t" seff.out > seff.summary

awk 'BEGIN{while(getline < "seff.summary" > 0) x[$1]=$0}; {print x[$1], $2, $3}' OFS="\t" slurm.summary > summary.txt

awk -F'\t' 'NR == 1 {print "job", "GB", "minutes", "S", "K"}; 
/COMPLETED/ {split($3,mem," "); split($4,time,":"); print $1, mem[1], time[1]*60 + time[2] + (time[3]>30), substr($5,2), substr($6,2)}' OFS="\t" summary.txt > completed.txt

cd /scratch/k.church/semantic_scholar/releases/4way/venn
awk '$1 == "#" {z[$2+1]=$3; next}; {for(i=1;i<=length($2);i++) x[i] += $1 * substr($2,i,1)}; END {for(i in x) print x[i], z[i]}' external_ids.2022-12-02.summary | sort -nr
207.799 CorpusId
182.182 MAG
113.54 DOI
35.0349 PubMed
6.06365 DBLP
4.86428 PubMedCentral
2.14626 ArXiv
0.07672 ACL

R104451

module load gcc
pip install ogb torch_sparse torch_geometric torch_scatter

# httpsx://ogb.stanford.edu/docs/home/
# https://ogb.stanford.edu/docs/lsc/mag240m/#dataset_object
# from ogb.graphproppred import PygGraphPropPredDataset
# from ogb.graphproppred import GraphPropPredDataset
# from torch_geometric.data import DataLoader

# Download and process data at './dataset/ogbg_molhiv/'
# dataset = PygGraphPropPredDataset(name = "ogbg-molhiv", root = 'dataset/')
# dataset = GraphPropPredDataset(name = "ogbg-molhiv", root = '.dataset/')

# dataset = GraphPropPredDataset(name = "ogbg-molhiv", root = '.dataset/')
# print(dataset.num_papers) # number of paper nodesclasse
# print(dataset.num_authors) # number of author nodes
# print(dataset.num_institutions) # number of institution nodes
# print(dataset.num_paper_features) # dimensionality of paper features
# print(dataset.num_classes) # number of subject area classses


from ogb.lsc import MAG240MDataset
dataset = MAG240MDataset(root = '/work/nlp/')

print(dataset.num_papers) # number of paper nodes
print(dataset.num_authors) # number of author nodes
print(dataset.num_institutions) # number of institution nodes
print(dataset.num_paper_features) # dimensionality of paper features
print(dataset.num_classes) # number of subject area classes





/work/nlp
https://github.com/snap-stanford/ogb/tree/master/examples/lsc/mag240m
https://dgl-data.s3-accelerate.amazonaws.com/dataset/OGB-LSC/mag240m_kddcup2021.zip

https://learn.microsoft.com/en-us/academic-services/graph/reference-data-schema
https://www.microsoft.com/en-us/research/project/academic/articles/microsoft-academic-to-expand-horizons-with-community-driven-approach/


from ogb.lsc import MAG240MDataset
dataset = MAG240MDataset(root = '/work/nlp/')
import numpy as np
edge_index_cites = dataset.edge_index('paper', 'paper')
X = edge_index_cites[0,:]
Y = edge_index_cites[1,:]
fanout = np.bincount(X)
fanin = np.bincount(Y)
fanin_Nr = np.bincount(fanin)
fanout_Nr = np.bincount(fanout)
np.savetxt('/work/nlp/k.church/MAG240M/all_paper_label.txt', dataset.all_paper_label, fmt='%0.0f')
dataset.all_paper_label.astype(np.float32).tofile('/work/nlp/k.church/MAG240M/all_paper_label.f')

cd /work/nlp/k.church/MAG240M/
awk '/nan/ {next}; {print NR-1}' all_paper_label.txt | x_to_y ai > all_paper_id_with_label.i


cd /scratch/k.church/MAG240M
sbatch --mem=800G -p short -t 1209 /work/nlp/k.church/MAG240M/extract_feat.py


rel=2022-12-02
in="/work/k.church/semantic_scholar/releases/$rel/database/papers/papers.piece.%03d.gz"
out="/scratch/k.church/semantic_scholar/releases/$rel/database/papers/papers.piece.%03d.CorpusId.MAG"
mkdir -p `dirname $out`
sbatch -t 59  --array 1-30 -p express /work/k.church/semantic_scholar/citations/graphs/src/semantic_scholar_zip_to_ids2.py -i $in -o $out --id1 CorpusId --id2 MAG

MAG240M hides the names of things on purpose; bad idea since it makes it hard to show how broken the benchmark is
https://github.com/snap-stanford/ogb/discussions/259

cd /work/nlp/k.church/MAG240M
graph_fromfile.py -i edges -o G -d int64

mkdir -p /scratch/k.church/MAG240M
cd /work/nlp/k.church/MAG240M
for S in 0.8 0.9 0.7 # 0.60 # 0.30
do
for K in 100 150 #  80 50
do
sbatch -p short -t 1439 --mem=800G --array 60-63 /work/nlp/k.church/MAG240M/sample_edges.sh $S $K
done
done

mkdir -p /scratch/k.church/MAG240M
cd /work/nlp/k.church/MAG240M
for S in 0.1 0.2 0.4
do
for K in 10 20 30 40
do
sbatch -p short -t 1439 --mem=200G --array 60-61 /work/nlp/k.church/MAG240M/sample_edges.sh $S $K
done
done



# /scratch/k.church/MAG240M/graphs/S0.60/K50/60/M.kwc.edges.f
K=50
S=0.60
B=6
cd /scratch/k.church/MAG240M/graphs/S$S/K$K/60/
sbatch -p short -t 1209 --array 10-19 $src/C/floats_to_idx.sh $K M.kwc.edges.f $B

for K in 50 80 100
do
for S in 0.30 0.60 0.8
do
for f in /scratch/k.church/MAG240M/graphs/S$S/K$K/*
do
if [ -s $f/M.kwc.edges.f ]
then
echo working on $f
sbatch -p short -t 1209 --array 20-29 $src/C/floats_to_idx.sh $K $f/M.kwc.edges.f $B
fi
done
done
done


# mkdir -p $dir
# cd $dir
# mkdir papers authors slurm
# cd papers
# ln -s $wdir/*.npz citations.npz
# ln -s /work/k.church/semantic_scholar/citations/graphs/citations.G.shrink.new.T1.npz.new_to_old.i map.new_to_old.i
# ln -s /work/k.church/semantic_scholar/citations/graphs/citations.G.shrink.new.T1.npz.old_to_new.i map.old_to_new.i
# npz=citations.npz
# B=6
# job1=`sbatch -p short -t 1209 --mem=300G /work/k.church/semantic_scholar/jobs/npz_to_kwc.py -i $npz -o $npz.papers | awk '{print $NF}'`
# sbatch -d afterany:$job1 -p short -t 1209 --array 10-99 $src/C/floats_to_idx.sh $K citations.npz.papers.kwc.edges.f $B


ffile=/scratch/k.church/MAG240M/graphs/S0.60/K50/60/M.kwc.edges.f
map=/scratch/k.church/MAG240M/graphs/S0.60/K50/60/Gshrunk.npz
K=50
x_to_y ia < /work/nlp/k.church/MAG240M/all_paper_id_with_label.i | 
near_with_floats  --floats $ffile --record_size $K --offset 5 --map $map /scratch/k.church/MAG240M/graphs/S0.60/K50/60/*idx.??.i |
python /work/nlp/k.church/MAG240M/label_pairs.py | egrep -v nan > /scratch/k.church/MAG240M/graphs/S0.60/K50/60/predicted_labels.txt

# Try this tomorrow

ffile=/scratch/k.church/MAG240M/all_paper_feat.f
K=768
x_to_y ia < /work/nlp/k.church/MAG240M/all_paper_id_with_label.i | 
near_with_floats  --floats $ffile --record_size $K --offset 5 /scratch/k.church/MAG240M/*idx.??.i |
python /work/nlp/k.church/MAG240M/label_pairs.py | egrep -v nan > /scratch/k.church/MAG240M/predicted_labels.txt




cd /scratch/k.church/MAG240M/
sbatch -o predicted_labels_from_feat.txt -e predicted_labels_from_feat.err -p short -t 1439 /work/nlp/k.church/MAG240M/predict_label_from_feat_job.sh

dir=/scratch/k.church/MAG240M/
sbatch -o predicted_labels.txt -e predicted_labels.err -p short -t 1439 /work/nlp/k.church/MAG240M/predict_label_job.sh



awk 'NR == 1 {print "score\tid1\tid2\tlab1\tlab2"}; $2 != $3 && killroy[$2,$3]++ < 1' predicted_labels.txt | awk 'NF == 5' > /tmp/x
R
x = read.table("/tmp/x", header=T)
g = glm(lab1 == lab2 ~ score, data=x, family=binomial)

cd /scratch/k.church/MAG240M
B=6
K=768
ffile=all_paper_feat.f
sbatch -p short -t 1209 --array 20-29 $src/C/floats_to_idx.sh $K $ffile $B


cd /scratch/k.church/MAG240M
K=768
ffile=all_paper_feat.f
cat predicted_labels.txt | cut -f2- | pairs_to_cos --floats $ffile --record_size $K  | paste - predicted_labels.txt > predicted_labels2.txt

cd /scratch/k.church/MAG240M/graphs/S0.60/K50/60
K=768
ffile=/scratch/k.church/MAG240M/all_paper_feat.f
cat predicted_labels.txt | cut -f2,3 | pairs_to_cos --floats $ffile --record_size $K > predicted_labels3.txt



# sbatch -p express -e pairs.err -o pairs.K280.out -i pairs.txt 

cd /scratch/k.church/MAG240M/graphs/S0.60/K50/60
paste predicted_labels3.txt predicted_labels.txt | 
awk 'NR == 1 {print "BERT.score\tid1\tid2\tscore\tid1\tid2\tlab1\tlab2"}; 
$2 != $3 && killroy[$2,$3]++ < 1' | awk 'NF == 8'  > /tmp/x

cd /scratch/k.church/semantic_scholar/releases/4way/venn
awk '/#/ {next}; substr($2,11,1) == 1 && substr($2,14,1) == 1' papers.abstracts.embeddings.citations.cited.citing | cut -f1 > corpusid_with_embeddings_and_citations.txt

rel=2022-12-02
cd /work/k.church/semantic_scholar/releases/$rel/database/embeddings
indir=/work/k.church/semantic_scholar/releases/$rel/database/embeddings
outdir=/scratch/k.church/semantic_scholar/releases/$rel/database/embeddings
mkdir -p $outdir
sbatch --array 1-30 -t 1209 -p short /work/k.church/semantic_scholar/jobs/embeddings_piece_to_kwc.py $indir/embeddings.piece $outdir/embeddings.piece



drwxr-xr-x 2 k.church users 4096 Jan 22 01:30 /scratch/k.church/semantic_scholar/releases/2022-12-02/database/embeddings
drwxr-xr-x 2 k.church users 4096 Jan 22 01:29 /scratch/k.church/semantic_scholar/releases/2022-11-22/database/embeddings
drwxr-xr-x 2 k.church users 4096 Jan 22 01:29 /scratch/k.church/semantic_scholar/releases/2022-08-23/database/embeddings

# creating ffile files
for dir in in /scratch/k.church/semantic_scholar/releases/2022*
do
rel=`basename $dir`
cd /work/k.church/semantic_scholar/releases/$rel/database/embeddings
indir=/work/k.church/semantic_scholar/releases/$rel/database/embeddings
outdir=/scratch/k.church/semantic_scholar/releases/$rel/database/embeddings
mkdir -p $outdir
if [ -s $outdir/embeddings.piece.028.kwc.edges.f ]
then
echo $outdir is already done
else
echo working on $outdir
sbatch --array 1-30 -t 1209 -p short /work/k.church/semantic_scholar/jobs/embeddings_piece_to_kwc.py $indir/embeddings.piece $outdir/embeddings.piece
fi
done



# look for ids that have embeddings and citations
# found about 80M such ids
# a few of these won't be found for various reasons
cd /scratch/k.church/semantic_scholar/releases/4way/venn
awk '/#/ {next}; substr($2,11,1) == 1 && substr($2,14,1) == 1' papers.abstracts.embeddings.citations.cited.citing | cut -f1 > corpusid_with_embeddings_and_citations.txt

cd /scratch/k.church/semantic_scholar/to_go

cd /scratch/k.church/semantic_scholar/to_go/releases/2022-12-02/database/citations/graphs/
sbatch -e $out.err -p short -t 1209 $src/graph_tofile.py -G citations.G.npz -o citations.G


# This WAS NOT WORKING (but now it is) !!!!
cd /scratch/k.church/semantic_scholar/to_go/releases/2022-12-02/database/citations/graphs
nohup filter_edges /scratch/k.church/semantic_scholar/to_go/corpusid_with_embeddings_and_citations.txt.s.i citations.G.X.int citations.G.Y.int citations.G.X.int.filtered citations.G.Y.int.filtered & 

rel=2022-12-02
cd /scratch/k.church/semantic_scholar/releases/$rel/database/embeddings
cat embeddings.piece.???.kwc.nodes.txt > specter.kwc.nodes.txt
invert_mapping.sh specter.kwc.nodes.txt
cat embeddings.piece.???.kwc.edges.f > specter.kwc.edges.f

outdir=/scratch/k.church/semantic_scholar/to_go/releases/2022-12-02/database/embeddings
mkdir -p $outdir
indir=/scratch/k.church/semantic_scholar/releases/$rel/database/embeddings

x_to_y ia < /scratch/k.church/semantic_scholar/to_go/corpusid_with_embeddings_and_citations.txt.s.i  |
id_to_floats --binary_output --floats $indir/specter.kwc.edges.f --record_size 768 --map $indir/specter.kwc.nodes.txt > $outdir/specter.kwc.edges.filtered.f

cd /scratch/k.church/semantic_scholar/to_go/releases/2022-12-02/database/embeddings
sbatch -p short -t 1209 --mem=400G id_to_floats2.sh

mkdir $outdir/todo
cd $outdir
x_to_y ia < /scratch/k.church/semantic_scholar/to_go/corpusid_with_embeddings_and_citations.txt.s.i  |
split -l 50000 - $outdir/todo/x

outdir=/scratch/k.church/semantic_scholar/to_go/releases/2022-12-02/database/embeddings
for f in $outdir/todo/x*
do
outf=$outdir/done/`basename $f`
if [ ! -s $outf.filtered.f ]
then
echo working on $f
sbatch -i $f -e $outf.err -o $outf.filtered.f -p express,short -t 59 --mem=100G id_to_floats2.sh
fi
done

cd /scratch/k.church/semantic_scholar/to_go/releases/2022-12-02/database/embeddings
# nohup cat done/*f > specter.kwc.edges.filtered.f &
sbatch -e specter.kwc.edges.filtered.f.err --mem=800G  -p short -t 1439 $src/svd_floats.py -i specter.kwc.edges.filtered.f -o specter.kwc.edges.filtered.f.U.K50.npz -K 768,50
sbatch -e specter.kwc.edges.filtered.f.err.K100 --mem=800G  -p short -t 1439 $src/svd_floats.py -i specter.kwc.edges.filtered.f -o specter.kwc.edges.filtered.f.K100.U.npz -K 768,100
sbatch -e specter.kwc.edges.filtered.f.err.K20 --mem=100G  -p short -t 1439 $src/svd_floats.py -i specter.kwc.edges.filtered.f -o specter.kwc.edges.filtered.f.U.npz -K 768,20

sbatch -e specter.kwc.edges.filtered.f.err.K20.express --mem=100G  -p express -t 59  $src/svd_floats.py -i specter.kwc.edges.filtered.f -o specter.kwc.edges.filtered.f.K20.express.U.npz -K 768,20

K=10
sbatch -e specter.kwc.edges.filtered.f.err.K$K --mem=1800G  -p short -t 1439 $src/svd_floats.py -i specter.kwc.edges.filtered.f -o specter.kwc.edges.filtered.f.U.K$K.npz -K 768,$K

for K in 25 # 75
do
sbatch -e specter.kwc.edges.filtered.f.err.K$K --mem=1800G  -p long -t 7199 $src/svd_floats.py -i specter.kwc.edges.filtered.f -o specter.kwc.edges.filtered.f.U.K$K.npz -K 768,$K
done

for K in 20
do
sbatch -e specter.kwc.edges.filtered.f.err.K$K --mem=1800G  -p short -t 1439 $src/svd_floats.py -i specter.kwc.edges.filtered.f -o specter.kwc.edges.filtered.f.U.K$K.npz -K 768,$K
done

cd /scratch/k.church/semantic_scholar/to_go/releases/2022-12-02/database/embeddings
python $src/random_project_floats.py -K 768,20 < specter.kwc.edges.filtered.f > specter.kwc.edges.filtered.f.rand_proj.K20.f

K=101
sbatch -e specter.kwc.edges.filtered.f.rand_proj.K$K.err  -p short -t 1439 -i specter.kwc.edges.filtered.f -o specter.kwc.edges.filtered.f.rand_proj.K$K.f $src/random_project_floats.py -K 768,$K 

cd /scratch/k.church/semantic_scholar/to_go/releases/2022-12-02/database/embeddings
K=200
sbatch -e specter.kwc.edges.filtered.f.rand_proj.K$K.err  -p express -t 59  -i specter.kwc.edges.filtered.f -o specter.kwc.edges.filtered.f.rand_proj.K$K.f $src/random_project_floats.py -K 768,$K 



python $src/astype.py --dtypes float,float32 < specter.kwc.edges.filtered.f.rand_proj.K100.f.bak > specter.kwc.edges.filtered.f.rand_proj.K100.f

invert_mapping.sh /scratch/k.church/semantic_scholar/to_go/corpusid_with_embeddings_and_citations.txt.s

id=18742499
echo $id | $src/fetch_references_and_citations.py > refs_and_citations.txt
awk '$2 ~ /^[0-9]*$/ && $3 ~ /^[0-9]*$/ {print $2 "\t" $3}' refs_and_citations.txt > pairs.txt

map=/scratch/k.church/semantic_scholar/to_go/corpusid_with_embeddings_and_citations.txt.s
ffile=specter.kwc.edges.filtered.f
K=768
pairs_to_cos --map $map --floats $ffile --record_size $K  < pairs.txt | awk '$1 > -1' > pairs.baseline

ffile101=specter.kwc.edges.filtered.f.rand_proj.K101.f
ffile100=specter.kwc.edges.filtered.f.rand_proj.K100.f

cut -f2- < pairs.baseline | pairs_to_cos --map $map --floats $ffile101 --record_size 101  > pairs.K101
cut -f2- < pairs.baseline | pairs_to_cos --map $map --floats $ffile100 --record_size 100  > pairs.K100

ffile200=specter.kwc.edges.filtered.f.rand_proj.K200.f
cut -f2- < pairs.baseline | pairs_to_cos --map $map --floats $ffile200 --record_size 200  > pairs.K200

cd /scratch/k.church/semantic_scholar/to_go/releases/2022-12-02/database/embeddings
map=/scratch/k.church/semantic_scholar/to_go/corpusid_with_embeddings_and_citations.txt.s
cut -f2- < pairs.baseline | python $src/pairs_to_cos.py --embedding specter.npy --map $map > pairs.K200.f16

echo 'score.baseline	id1	id2	score.K100	id2	id2	score.K101	id1	id2	score.K200	id1	id2	score.K200.f16	id1	id2' > /tmp/x
paste pairs.baseline pairs.K* >> /tmp/x

python $src/astype.py --dtypes float32,float16 < specter.kwc.edges.filtered.f.rand_proj.K200.f > specter.kwc.edges.filtered.f.rand_proj.K200.f16
sbatch --mem=150G -p express -t 59 $src/array_fromfile.py -i specter.kwc.edges.filtered.f.rand_proj.K200.f16 -o specter.npy --dtype float16 -K 200


for f in /scratch/k.church/semantic_scholar/to_go/corpusid_with_embeddings_and_citations.txt.s.new_to_old.i /scratch/k.church/semantic_scholar/to_go/corpusid_with_embeddings_and_citations.txt.s.old_to_new.i
do
sbatch -p express -t 59 $src/array_fromfile.py -i $f -o $f.npy --dtype int32
done


cd /scratch/k.church/semantic_scholar/to_go/releases/2022-12-02/database/citations/graphs
ln -s citations.G.X.int.filtered citations.G.int.filtered.X 
ln -s citations.G.Y.int.filtered citations.G.int.filtered.Y 
sbatch -p short -t 1439 --mem=100G graph_fromfile.py -N 100000000 -i citations.G.int.filtered -o /scratch/k.church/semantic_scholar/to_go2/releases/2022-12-02/citations.npz

import numpy as np
import scipy.sparse
f='/scratch/k.church/semantic_scholar/to_go2/releases/2022-12-02/citations.npz'
M=scipy.sparse.load_npz(f)

cd /scratch/k.church/semantic_scholar/to_go2
aws s3 cp releases/ s3://jsalt2023kwc/V1/ --recursive --acl public-read
aws s3 ls s3://jsalt2023kwc/V1/ --recursive

# These don't work
# aws s3api put-object-acl --bucket DOC-EXAMPLE-BUCKET --key exampleobject --grant-full-control id="008exampleA45666666668889999008853" --grant-read uri=http://acs.amazonaws.com/groups/global/AllUsers
# aws s3api put-object-acl --bucket jsalt2023kwc --acl grant_read
# aws s3api put-object-acl --bucket s3://jsalt2023kwc/V1/ --key jsalt2023kwc --acl public-read

# But this does (I think)

{
    "Version": "2008-10-17",
    "Statement": [
        {
            "Sid": "AllowPublicRead",
            "Effect": "Allow",
            "Principal": {
                "AWS": "*"
            },
            "Action": "s3:GetObject",
            "Resource": "arn:aws:s3:::jsalt2023kwc/*"
        }
    ]
}

https://s3.console.aws.amazon.com/s3/buckets/jsalt2023kwc?region=us-west-2&tab=permissions

aws s3api get-object-acl --bucket s3://jsalt2023kwc/V1/ --key jsalt2023kwc

cd /scratch/k.church/semantic_scholar/to_go2
s3cmd put releases/ s3://jsalt2023kwc2/ --recursive --access_key=AKIAVPBCSN6KYQJXGxxx --secret_key=q8eceGyoVOZnJaO8RuY2Z4Rf9NHPXgyuSufZqxxx

cd /scratch/k.church/semantic_scholar/to_go2/releases/2022-12-02/
s3cmd put doc/ s3://jsalt2023kwc3/ --recursive --access_key=AKIAVPBCSN6KYQJXGxxx --secret_key=q8eceGyoVOZnJaO8RuY2Z4Rf9NHPXgyuSufZqxxx --region us-east-1

cd /scratch/k.church/semantic_scholar/to_go2/releases/2022-12-02/
s3cmd put doc/ s3://jsalt2023kwc3/ --recursive  --force


https://us-east-1.console.aws.amazon.com/settings/home?region=us-east-1#/

https://us-west-1.console.aws.amazon.com/settings/home?region=us-west-1#/

s3://jsalt2023kwc2/2022-12-02/context/
https://jsalt2023kwc2.s3.us-west-1.amazonaws.com/2022-12-02/context/

s3cmd ls s3://ai2-s2-datadog-logs --access_key=AKIAVPBCSN6KYQJXGxxx --secret_key=q8eceGyoVOZnJaO8RuY2Z4Rf9NHPXgyuSufZqxxx

/work/nlp/j.ortega/paper_recommender/semantic_scholar_specter/prone_benchmarks/get_prone_benchmarks.sh
/work/nlp/j.ortega/paper_recommender/semantic_scholar_specter/prone_benchmarks

cd /scratch/k.church/semantic_scholar/logs
sbatch -p express -t 59 /scratch/k.church/semantic_scholar/logs/job.sh

sbatch -p express -t 59 --array 1-30 /work/k.church/semantic_scholar/author2hindex.py -i /work/k.church/semantic_scholar/releases/2022-12-02/database/authors/authors.piece.%03d.gz -o /scratch/k.church/semantic_scholar/releases/2022-12-02/database/authors/authors.details.piece.%03d


cd /scratch/k.church/semantic_scholar/releases/2022-12-02/database/authors/
awk -F'\t' 'FNR > 1 && NF == 5 {x[$1]++}; END {for(i in x) print x[i] "\t" i}' authors.details.piece.??? | sort -k2 -n > hindex.txt

awk -F'\t' 'FNR > 1 && NF == 5 {x[$2]++}; END {for(i in x) print x[i] "\t" i}' authors.details.piece.??? | sort -k2 -n > papercount.txt & 
awk -F'\t' 'FNR > 1 && NF == 5 {x[$3]++}; END {for(i in x) print x[i] "\t" i}' authors.details.piece.??? | sort -k2 -n > citationcount.txt & 

/work/k.church/Scarpino/WHO

cd /work/k.church/semantic_scholar/releases/2022-12-02/database/

rel=2022-12-02
cd /work/k.church/semantic_scholar/releases/$rel/database/embeddings
indir=/work/k.church/semantic_scholar/releases/$rel/database/citations
outdir=/scratch/k.church/semantic_scholar/releases/$rel/database/citations/citing_sentences
mkdir -p $outdir
sbatch --array 1-30 -t 10 -p debug /work/k.church/semantic_scholar/citing_sentences_to_embeddings.py -i $indir/citations.piece.%03d.gz -o $outdir/citing_sentences.%03d

for f in `find /scratch/k.church/semantic_scholar/releases -name 'sorted_uniq'`
do
dir=`dirname $f`
wdir=`echo $dir | sed 's/scratch/work/'`
echo $f
echo $wdir
echo ""
mkdir -p $wdir
cp $f $wdir
done

rel=2022-12-02
indir=/work/k.church/semantic_scholar/releases/$rel/database/citations
outdir=/scratch/k.church/semantic_scholar/releases/$rel/database/citations/extractions/citing_sentences
mkdir -p $outdir
cd $outdir
sbatch --array 1-30 -p short -t 1439 /work/k.church/semantic_scholar/extract_citing_sentences.py -i $indir/citations.piece.%03d.gz -o $outdir/citing_sentences.%03d

cd /scratch/k.church/semantic_scholar/releases/2022-12-02/database/citations/extractions/citing_sentences
for f in citing_sentences.001 # citing_sentences.???
do
sbatch -i $f -p express -t 59 split_job.sh splits/`echo $f | cut -f2 -d.`
done

find splits -type f | awk 'BEGIN {i=j=0}; {printf "%03d %03d %s\n", i, j, $0; j++; if(j>=1000) {j=0;i++}}' > map

# sed 3000q map |
cat map | awk '$1 > 3' |
while read i j f
do
dir=pieces/$i
mkdir -p $dir
ln -s `pwd`/$f $dir/$j
done

cd /scratch/k.church/semantic_scholar/releases/2022-12-02/database/citations/extractions/citing_sentences
# sed 3000q map |
cat map | awk '$1 == 3' |
while read i j f
do
dir=pieces/$i
mkdir -p $dir
ln -s `pwd`/$f $dir/$j
done


dir=/scratch/k.church/semantic_scholar/releases/2022-12-02/database/citations/extractions/citing_sentences/pieces/000
sbatch -p express -t 59 --array 0-5 /work/k.church/semantic_scholar/extracted_citing_sentences_to_embedding.py -i $dir/%03d -o $dir/%03d

dir=/scratch/k.church/semantic_scholar/releases/2022-12-02/database/citations/extractions/citing_sentences/pieces/000
cd $dir
sbatch -p express -t 59 --array 6-999 /work/k.church/semantic_scholar/extracted_citing_sentences_to_embedding.py -i $dir/%03d -o $dir/%03d


cd /scratch/k.church/semantic_scholar/releases/2022-12-02/database/citations/extractions/citing_sentences/
find . -type f | while read f
 do
 awk '$0 == "0 errors" {print "rm", FILENAME}' $f
 done | head

cd /scratch/k.church/semantic_scholar/releases/2022-12-02/database/citations/extractions/citing_sentences/
for dir in /scratch/k.church/semantic_scholar/releases/2022-12-02/database/citations/extractions/citing_sentences/pieces/?[0-9][0-9] 
do
if [ ! -s $dir/999.kwc.nodes.i ]
then
echo working on $dir
# sbatch -p short -t 1409 --array 0-999 /work/k.church/semantic_scholar/extracted_citing_sentences_to_embedding.py -i $dir/%03d -o $dir/%03d
sbatch -p express -t 59 --array 0-999 /work/k.church/semantic_scholar/extracted_citing_sentences_to_embedding.py -i $dir/%03d -o $dir/%03d
else
echo $dir is done
fi
done

# for Jaimeng
cd /scratch/k.church/semantic_scholar/releases/2022-12-02/database/citations/extractions/citing_sentences/
for dir in /scratch/k.church/semantic_scholar/releases/2022-12-02/database/citations/extractions/citing_sentences/pieces/[0-9][789][0-9] 
do
if [ ! -s $dir/000.kwc.nodes.i ]
then
echo working on $dir
sbatch -p express -t 59 --array 0-999 /work/k.church/semantic_scholar/extracted_citing_sentences_to_embedding.py -i $dir/%03d -o $dir/%03d
else
echo $dir is done
fi
done

# for Annika
cd /scratch/k.church/semantic_scholar/releases/2022-12-02/database/citations/extractions/citing_sentences/
for dir in /scratch/k.church/semantic_scholar/releases/2022-12-02/database/citations/extractions/citing_sentences/pieces/[0-9][56][0-9] 
do
if [ ! -s $dir/000.kwc.nodes.i ]
then
echo working on $dir
sbatch -p express -t 59 --array 0-999 /work/k.church/semantic_scholar/extracted_citing_sentences_to_embedding.py -i $dir/%03d -o $dir/%03d
else
echo $dir is done
fi
done

# patch
cd /scratch/k.church/semantic_scholar/releases/2022-12-02/database/citations/extractions/citing_sentences/pieces/
find ??? -maxdepth 2 -name '*.i' -ls | egrep -v bak > x32000.V4
awk '{split($NF, x, "."); print "ls -l", x[1] ".kwc.edges.f"}' x32000.V4 | sh | awk '{print $5}' > x32000f.V4

find ??? -name '*.i' -ls | awk '{key=substr($NF, 1, 3); n[key]++; if($7 == 32000) x[key]++}; END {for(i in n) print x[i]/n[i], n[i], i}' > x

cd /scratch/k.church/semantic_scholar/releases/2022-12-02/database/citations/extractions/citing_sentences/pieces/
for dir in [0-9][0-9][0-9][0-9] # 01? # 000 # [0-9][0-9][0-9][0-9] #  `awk '$1 < 0.9 {print $NF}' x`         # `awk '$1 < 0.8 && $NF ~ /^0/ {print $NF}' x` # 02[5-9]
do
cd /scratch/k.church/semantic_scholar/releases/2022-12-02/database/citations/extractions/citing_sentences/pieces/$dir
mkdir -p bak2
for f in `ls -l *.i | awk '$5 != 32000 {print substr($NF, 1,3)}'`
do
echo working on $dir $f
mv $f.kwc* bak2
sbatch -p express -t 59 --array $f /work/k.church/semantic_scholar/extracted_citing_sentences_to_embedding.py -i %03d -o %03d
# sbatch -p short  --array $f /work/k.church/semantic_scholar/extracted_citing_sentences_to_embedding.py -i %03d -o %03d
done
done

# This should be done after the stuff above

cd /scratch/k.church/semantic_scholar/releases/2022-12-02/database/citations/extractions/citing_sentences/pieces
for dir in [0-9][0-9][0-9]
do
cd /scratch/k.church/semantic_scholar/releases/2022-12-02/database/citations/extractions/citing_sentences/pieces/$dir
for f in [0-9][0-9][0-9]
do
if [ ! -s $f.kwc.nodes.i ]
then
echo working on $dir $f
sbatch -p express -t 59 --array $f /work/k.church/semantic_scholar/extracted_citing_sentences_to_embedding.py -i %03d -o %03d
fi
done
done
# end patch

/work/nlp/j.ortega/paper_recommender/semantic_scholar_specter/prone_benchmarks/slurm-34944322_60.out



ls -lt /scratch/k.church/semantic_scholar/releases/2022-12-02/database/citations/extractions/citing_sentences/pieces/???/000.kwc.nodes.i | 
awk '{print $3}' | sort | uniq -c | sort -nr



cd /scratch/k.church/semantic_scholar/releases/2022-12-02/database/citations/extractions/citing_sentences/pieces
for dir in ???
do
sbatch -o $dir/hist_ids.out -e $dir/hist_ids.err -p short -t 1439 /work/k.church/semantic_scholar/hist_ids.sh $dir/???
done

cd /scratch/k.church/semantic_scholar/releases/2022-12-02/database/citations/extractions/citing_sentences/splits
find . -type f -name 'piece*' -exec ./remove_returns.sh {} \;  & 

cd /work/k.church/Scarpino/promed
split -d -a 5 -l 1000 for_BERT todo/x.

cd /work/k.church/Scarpino/promed/todo
mkdir -p ../done
sbatch -p short -t 1439 --array 41-533 /work/k.church/semantic_scholar/extracted_citing_sentences_to_embedding.py -i x.%05d -o ../done/x.%05d

# patch WHO
cd /work/k.church/Scarpino/promed/todo
for f in x.?????
do
if [ ! -s ../done/$f.kwc.nodes.i ]
then
echo working on $f
sbatch -p short -t 1439 /work/k.church/semantic_scholar/extracted_citing_sentences_to_embedding.py -i $f -o ../done/$f
fi
done

cd /work/k.church/Scarpino/promed/todo
sbatch -p short -t 1439 --array 1-3 /work/k.church/semantic_scholar/extracted_citing_sentences_to_embedding.py -i x.%05d -o ../done/x.%05d

cd /work/k.church/Scarpino/promed/done
cat `ls *i | awk -F. '$2 == 0 || $2 >= 4'` > x.kwc.nodes.i
cat `ls *f | awk -F. '$2 == 0 || $2 >= 4'` > x.kwc.edges.f

cd /work/k.church/Scarpino/promed/done
ffile=x.kwc.edges.f
B=6
K=768
sbatch -p short -t 1209 --array 50-99 $src/C/floats_to_idx.sh $K $ffile $B

x_to_y ia < x.kwc.nodes.i > x.kwc.nodes.txt
invert_mapping.sh x.kwc.nodes.txt


mkdir /work/k.church/Scarpino/promed/done2
for f in /work/k.church/Scarpino/promed/todo/x.*
do
outf=/work/k.church/Scarpino/WHO/done3/`basename $f`
sbatch -i $f -o $outf -p debug /work/k.church/Scarpino/promed/done/Scarpino_todo.sh
done

awk '$1 == "Query:" {query = $NF; print $0; next}; $1 > 0.9 {print query, $2, $0}' OFS="\t" /work/k.church/Scarpino/promed/done2/x.00000 | find_lines --input /work/k.church/Scarpino/promed/for_BERT  --fields '---L' > /work/k.church/Scarpino/promed/done2/x.00000.txt

cd /work/k.church/Scarpino/WHO
for f in [0-9][0-9][0-9][0-9]/Report_text
do
tr -d '\r' < $f | tr '\n' ' ' |
awk '{print 1000000 + substr(f, 1, 4) "\t" $0}' f=$f
done | 
split -d -a 3 -l 100 - todo/x.

cd /work/k.church/Scarpino/WHO/todo
mkdir -p ../done
sbatch -p short -t 1439 --array 0-27 /work/k.church/semantic_scholar/extracted_citing_sentences_to_embedding.py -i x.%03d -o ../done/x.%03d

cd /work/k.church/Scarpino/WHO/done
cat x.???.kwc.nodes.i | x_to_y ia | cat /work/k.church/Scarpino/promed/done/x.kwc.nodes.txt - > x.kwc.nodes.txt
invert_mapping.sh x.kwc.nodes.txt
cat /work/k.church/Scarpino/promed/done/x.kwc.edges.f x.???.kwc.edges.f > x.kwc.edges.f

cd /work/k.church/Scarpino/WHO/done
ffile=x.kwc.edges.f
B=6
K=768
sbatch -p short -t 1209 --array 50-99 $src/C/floats_to_idx.sh $K $ffile $B

mkdir -p /work/k.church/Scarpino/WHO/done3
cd /work/k.church/Scarpino/WHO/done3
for f in /work/k.church/Scarpino/WHO/todo/x.* /work/k.church/Scarpino/promed/todo/x.* 
do
outf=/work/k.church/Scarpino/WHO/done3/`basename $f`
if [ ! -s $outf ]
then
echo working on $f
sbatch -i $f -o $outf -p debug,short /work/k.church/Scarpino/promed/done/Scarpino_todo.sh
fi
done

cd /work/k.church/Scarpino/WHO/done3
cat /work/k.church/Scarpino/promed/for_BERT ../todo/x.* > ../for_BERT

awk '/Query/ {q=$2; next}; {print q "\t" $0}' x.000 | awk '$2 > 0.95 && killroy[$1]++ < 10 {print $0 "\t" $NF}' | find_lines --input ../for_BERT --fields '---L' > x.000.txt

awk '/Query/ {q=$2; next}; {print q "\t" $0}' x.000 | awk '$2 > 0.95 && killroy[$1]++ < 10 {print $0 "\t" $NF}' | find_lines --input ../URLs --fields '---L' > x.000.URLs.txt

awk '/Query/ {q=$2; next}; {print q "\t" $0}' x.00000 | awk '$2 > 0.95 && killroy[$1]++ < 10 {print $0 "\t" $NF}' | find_lines --input ../URLs --fields '---L' > x.00000.URLs.txt




cd /work/k.church/Scarpino/promed
python unpack_URLs.py | awk '{printf "%07d\thttps://promedmail.org/promed-post/?id=%s\n", NR, $NF}'  > unpack_URLs

cd /work/k.church/Scarpino/promed/unpacked/003
awk 'FNR == 1 && $1 == "<header" {id[FILENAME] = substr($2,4,length($2)-4)}
$1 == "Subject:" {subject[FILENAME]=$0}; 
$1 == "Archive" && $2 == "Number:" {AN[FILENAME]=$NF}; 
END {for(fn in id) print fn, id[fn], AN[fn], subject[fn]}' OFS="\t" *


cd /work/k.church/Scarpino/promed/unpacked
mkdir -p summaries
for dir in [0-9][0-9][0-9]
do
awk 'FNR == 1 && $1 == "<header" {id[FILENAME] = substr($2,4,length($2)-4)}
$1 == "Subject:" {subject[FILENAME]=$0}; 
$1 == "Archive" && $2 == "Number:" {AN[FILENAME]=$NF}; 
END {for(fn in id) print substr(fn,5), id[fn], AN[fn], subject[fn]}' OFS="\t" $dir/[0-9][0-9][0-9][0-9][0-9][0-9]
done > summaries/ids


cd /work/k.church/Scarpino/promed/unpacked
for dir in [0-9][0-9][0-9]
do
awk '/Archive Number:/ {AN=$NF}; 
/See Also/ {go=1; next}; 
$1 == "</body>" {go=0}; 
go > 0 && $NF !~ /[a-zA-Z]/ && split($NF, x, ".") == 2 {print substr(FILENAME,5), AN, $NF}' OFS="\t" $dir/* 
done > summaries/see_also

cd /work/k.church/Scarpino/promed/unpacked
for dir in [0-9][0-9][0-9]
do
awk '/Archive Number:/ {AN=$NF; subid="NA"; go=0}; 
$1 ~ /^[\[][0-9][\]]$/  || $1 ~ /^[\[][0-9][0-9][\]]$/ {subid = $1; go=1; next};
go > 0 && ($1 ~ /http:/ || $1 ~ /https:/) {print substr(FILENAME, 5), AN, subid, $0; go=0};
# NF == 0 {go=0}
$1 == "Source:" || /full report/ {go=1}' OFS="\t" $dir/* 
done > summaries/sources

https://promedmail.org/promed-post/?id=20070321.1000

cd /work/k.church/Scarpino/WHO/done3
for f in x.[0-9][0-9][0-9][0-9][0-9]
do
awk '/Query/ {q=$2; next}; {print q "\t" $0}' $f | awk '$2 > 0.95 && killroy[$1]++ < 10 {print $0 "\t" $NF}' | find_lines --input ../URLs --fields '---L' > $f.URLs.txt
done

awk -F'\t' '$2 == 1 {q=$4}; /who/ {print $0 "\t" q}' x.00000.URLs.txt |  
awk -F'\t' '{nx=split($(NF-1), x, "/"); ny=split($NF, y, "/"); 
tx = mktime(substr(x[nx],1,4) " " substr(x[nx],6,2) " " substr(x[nx], 9,2) " 00 00 00" )
ty = mktime(substr(y[ny], 5, 4) " " substr(y[ny], 9, 2) " " substr(y[ny], 11, 2) " 00 00 00" )
print tx+0, ty+0, tx - ty, $0, x[nx], y[ny], substr(x[nx],1,4), substr(x[nx],6,2), substr(x[nx], 9,2), substr(y[ny], 5, 4), substr(y[ny], 9, 2), substr(y[ny], 11, 2)}' OFS="\t" | head

cd /work/k.church/Scarpino/WHO/done3
awk -F'\t' '$2 == 1 {q=$4}; /who/ {print $0 "\t" q}' x.[0-9][0-9][0-9][0-9][0-9].URLs.txt |  
awk -F'\t' '{nx=split($(NF-1), x, "/"); ny=split($NF, y, "/"); 
tx = mktime(substr(x[nx],1,4) " " substr(x[nx],6,2) " " substr(x[nx], 9,2) " 00 00 00" )
ty = mktime(substr(y[ny], 5, 4) " " substr(y[ny], 9, 2) " " substr(y[ny], 11, 2) " 00 00 00" )
delta = (ty - tx)/(60*60 *24)
print delta, $0}' OFS="\t" > deltas

awk -F'\t' '$1 >= -60 && $1 <= 60 {printf "%0.0f\t%0.3f\t<a href=\"%s\">%s</a>\t<a href=\"%s\">%s</a>\n", $1, $3, $5, $2, $6, $4}' deltas | sort -n |
awk 'NR == 1 {printf "delta days\tmatch score\tWHO\tProMed\n"}; {print}' | tsv_to_html.sh > $HOME/to_go/deltas.html

awk '{printf "%0.0f\n", $1}' deltas | sort | uniq -c | sort -nr | sed 20q


cd /work/nlp/j.ortega/paper_recommender/semantic_scholar_specter/prone_benchmarks
/home/j.ortega/anaconda3/lib/python3.9/site-packages/nodevectors/prone.py

cd /work/k.church/Scarpino/WHO
awk -F'\t' '{nx = split($2, x, "="); print x[nx], $1}' OFS="\t" URLs | sort | uniq1.sh | egrep . > archive_number_to_ids

awk 'BEGIN {while(getline < "/work/k.church/Scarpino/WHO/archive_number_to_ids" > 0) x[$1]=$0}; {print x[$2] "|" x[$3]}' see_also | sed 50q

python $src/array_fromfile.py --input /work/k.church/Scarpino/WHO/done/x.kwc.edges.f --output /work/k.church/Scarpino/WHO/done/x.kwc.edges.npy --dtype float32 -K 768
python $src/array_fromfile.py --input /work/k.church/Scarpino/WHO/done/x.kwc.nodes.txt.old_to_new.i --output /work/k.church/Scarpino/WHO/done/x.kwc.nodes.txt.old_to_new.npy --dtype int32
python $src/array_fromfile.py --input /work/k.church/Scarpino/WHO/done/x.kwc.nodes.txt.new_to_old.i --output /work/k.church/Scarpino/WHO/done/x.kwc.nodes.txt.new_to_old.npy --dtype int32


0.81960905	20181026.6113040	20180909.6018232
0.88892597	20170813.5246606	20161115.4630368

https://promedmail.org/promed-post/?id=20181026.6113040	
https://promedmail.org/promed-post/?id=20180909.6018232

python /work/k.church/Scarpino/WHO/done/see_also_cosines.py --random_control 50 | sort -n
cut -f2- see_also | python /work/k.church/Scarpino/WHO/done/see_also_cosines.py > see_also.scored.txt

cd /work/k.church/semantic_scholar/releases/2022-12-02
cd /work/k.church/semantic_scholar/releases/2022-12-02/database/papers
python /work/k.church/semantic_scholar/papers/papers2field_of_study.py papers.piece.001.gz | head

cd /work/k.church/semantic_scholar/releases/2022-12-02/database/papers
mkdir -p field_of_study.V2
for f in papers.piece.???.gz
do
b=`basename $f .gz`
sbatch -t 1439 -p short -o field_of_study.V2/$b.out -e field_of_study.V2/$b.err /work/k.church/semantic_scholar/papers/papers2field_of_study.py $f
done

cd /work/k.church/semantic_scholar/releases/2022-12-02/database/papers/field_of_study.V2
cut -d'|' -f1 *out | cut -f2 | awk '{x[$0]++}; END {for(i in x) print x[i] "\t" i}' | sort -nr > summary.txt
awk -F'|' 'NF == 1 {print rand() "\t" $0}' papers.piece.001.out | sort | awk -F'\t' 'killroy[$NF]++ < 500' | cut -f2- > sample
awk -F'\t' 'killroy[$NF]++ < 50' sample | sort -k2 > sample50

cd /work/k.church/semantic_scholar/releases/2022-12-02/database/papers/field_of_study.V2
K=280
dir=/work/k.church/semantic_scholar/citations/graphs/K$K/papers/
ffile=$dir/citations.npz.papers.kwc.edges.f
map=$dir/map
cut -f1 < sample50 | id_to_floats --floats $ffile --record_size $K --map $map --binary_output > sample50.K$K.f

cd /work/k.church/semantic_scholar/releases/2022-12-02/database/papers/field_of_study.V2
dir=/work/k.church/semantic_scholar/releases/2022-12-02/database/embeddings
K=768
ffile=$dir/specter.kwc.edges.f
map=$dir/specter.kwc.nodes.txt
cut -f1 < sample50 | id_to_floats --floats $ffile --record_size $K --map $map --binary_output > sample50.specter.K$K.f

python floats_to_cosines.py  --input sample50.K280.f --output sample50.K280.cos.npy -K 280 -S sample50 > sample50.K280.cos.txt
python floats_to_cosines.py  --input sample50.specter.K768.f --output sample50.specter.K768.cos.npy -K 768  -S sample50 > sample50.specter.K768.cos.txt

cd /work/k.church/semantic_scholar/releases/2022-12-02/database/papers/field_of_study.V2
cut -f1 < sample50 | $src/fetch_from_semantic_scholar_api.py --fields externalIds,fieldsOfStudy,embedding > sample50.json

/work/TabularData/ChandraExperiments/arXiv/srcExperiments/analyzeArXivData

<li><a href="desc2023.gz"> desc2023.gz</a></li>
<li><a href="desc2023.xml"> desc2023.xml</a></li>
<li><a href="desc2023.zip"> desc2023.zip</a></li>
<li><a href="pa2023.xml"> pa2023.xml</a></li>
<li><a href="qual2023.xml"> qual2023.xml</a></li>
<li><a href="supp2023.gz"> supp2023.gz</a></li>
<li><a href="supp2023.xml"> supp2023.xml</a></li>
<li><a href="supp2023.zip"> supp2023.zip</a></li>

cd /work/k.church/pubmed
wget -r -l 2 "https://nlmpubs.nlm.nih.gov/projects/mesh/MESH_FILES/xmlmesh/"

# for f in desc2023.gz desc2023.xml desc2023.zip pa2023.xml qual2023.xml supp2023.gz supp2023.xml supp2023.zip 
# do
# wget "https://nlmpubs.nlm.nih.gov/projects/mesh/MESH_FILES/xmlmesh/" $f
# done


https://nam12.safelinks.protection.outlook.com/?url=https%3A%2F%2Food.discovery.neu.edu%2Fpun%2Fsys%2Fdashboard%2Ffiles%2Ffs%2Fhome%2Fw.saba%2Fdata%2Fpromed_splits_out.json&data=05%7C01%7Ck.church%40northeastern.edu%7C9a934d9693884b34e90c08db0def6de6%7Ca8eec281aaa34daeac9b9a398b9215e7%7C0%7C0%7C638119094205316262%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&sdata=9B6G9kULzgG5ZziE9N7IA7UpD6X%2F3%2F7oTD3a6kwdkq8%3D&reserved=0

/home/w.saba/data/promed_splits_out.json


https://promedmail.org/promed-post/?id=20180408.5732766

cd /work/k.church/Walid/unpacked
awk '$1 == "parent_archive_number" {print $2 "\t" FILENAME}'  */* | sort | uniq1.sh | egrep . > summaries/magic_key_decoder_ring

cd /work/k.church/Scarpino/promed/unpacked
for pat in "[1]" "[2]" "[3]" "[4]"
do
for f in [0-9][0-9][0-9]
do
fgrep -c "$pat" $f/* 
done | 
awk -F: '$2 > 0 {x++}; END {print x, NR, pat}' pat="$pat" > summaries/patterns
done


cd /work/k.church/Scarpino/promed/unpacked
for f in [0-9][0-9][0-9]
do
awk '/Archive Number:/ {AN=$NF; x[AN]="NA"}; 
    $1 ~ /^[\[][0-9][0-9]*[\]]$/ {x[AN]=$1}; 
    END {for(i in x) print i "\t" x[i]}' $f/*
done > summaries/items_per_archive_number


find /work/k.church/semantic_scholar/ -maxdepth 7 -name 'citat*.K*.f'
/work/k.church/semantic_scholar/citations/graphs/K10/K100/citations.G.shrink.new.T1.G2.npz.tasks.1.K100.ProNE.K100.T20.O5.w2v.kwc.edges.f
/work/k.church/semantic_scholar/citations/graphs/K200/citations.G.shrink.new.T1.G2.npz.tasks.1.K200.ProNE.K200.T20.O5.w2v.kwc.edges.f
/work/k.church/semantic_scholar/citations/graphs/K250/citations.G.shrink.new.T1.G2.npz.tasks.1.K250.ProNE.K250.T20.O5.w2v.npz.authors.kwc.edges.f
/work/k.church/semantic_scholar/citations/graphs/K250/citations.G.shrink.new.T1.G2.npz.tasks.1.K250.ProNE.K250.T20.O5.w2v.npz.authors.norm.kwc.edges.f
/work/k.church/semantic_scholar/citations/graphs/K300/citations.G.shrink.new.T1.G2.npz.tasks.1.K300.ProNE.K300.T20.O5.w2v.kwc.edges.f
/work/k.church/semantic_scholar/releases/2022-08-23/database/citations/graphs/citations.G.S0.1.shrink.T1.G2.npz.K100.ProNE.K100.T20.O5.w2v.kwc.edges.f
/work/k.church/semantic_scholar/releases/2022-08-23/database/citations/graphs/citations.G.S0.1.shrink.T1.G2.npz.K50.ProNE.K50.T20.O5.w2v.kwc.edges.f
/work/k.church/semantic_scholar/releases/2022-08-23/database/citations/graphs/citations.G.S0.1.shrink.T1.G2.npz.K60.ProNE.K60.T20.O5.w2v.kwc.edges.f
/work/k.church/semantic_scholar/releases/2022-08-23/database/citations/graphs/citations.G.S0.5.shrink.T1.G2.npz.K50.ProNE.K50.T20.O5.w2v.kwc.edges.f
/work/k.church/semantic_scholar/releases/2022-08-23/database/citations/graphs/citations.G.shrink.T1.G2.npz.K100.ProNE.K100.T20.O5.w2v.kwc.edges.f
/work/k.church/semantic_scholar/releases/2022-08-23/database/citations/graphs/citations.G.shrink.T1.G2.npz.K150.ProNE.K150.T20.O5.w2v.kwc.edges.f
/work/k.church/semantic_scholar/releases/2022-08-23/database/citations/graphs/citations.G.shrink.T1.G2.npz.K250.ProNE.K250.T20.O5.w2v.kwc.edges.f
(gft) [k.church@c3085 ~]$ find /work/k.church/semantic_scholar/ -maxdepth 7 -name 'spec*.f'
/work/k.church/semantic_scholar/embeddings/new/specter.kwc.edges.f
/work/k.church/semantic_scholar/embeddings/specter/K768/authors/specter.kwc.edges.f.authors.kwc.edges.f
/work/k.church/semantic_scholar/embeddings/specter.kwc.edges.f
/work/k.church/semantic_scholar/embeddings/specter.kwc.edges.f.authors.kwc.edges.f
/work/k.church/semantic_scholar/releases/2022-12-02/database/embeddings/specter.kwc.edges.f

ls -ltd /work/k.church/semantic_scholar/releases/202*
drwxrwx---+ 2 k.church users 4096 Jan 16 12:26 /work/k.church/semantic_scholar/releases/2022-05-31
drwxrwx---+ 2 k.church users 4096 Dec  8 18:21 /work/k.church/semantic_scholar/releases/2022-12-02
drwxrwx---+ 2 k.church users 4096 Dec  7 20:15 /work/k.church/semantic_scholar/releases/2022-11-22
drwxrwx---+ 2 k.church nlp   4096 Aug 29 18:11 /work/k.church/semantic_scholar/releases/2022-08-23

PATH=$PATH:~k.church/final/suffix
cd /work/k.church/Scarpino/promed/conc
concord.sh pubdoc 'Adult worker bees will remove these dead larvae' | head

170448950     47117	y are curled at the base of uncapped brood cells. 	Adult worker bees will remove these dead larvae le
109334119     30267	y are curled at the base of uncapped brood cells. 	Adult worker bees will remove these dead larvae le
 43693032     12099	y are curled at the base of uncapped brood cells. 	Adult worker bees will remove these dead larvae le
 80410499     22223	y are curled at the base of uncapped brood cells. 	Adult worker bees will remove these dead larvae le
425650782    117713	y are curled at the base of uncapped brood cells. 	Adult worker bees will remove these dead larvae le
 10193277      2869	y are curled at the base of uncapped brood cells. 	Adult worker bees will remove these dead larvae le
  3981864      1137	y are curled at the base of uncapped brood cells. 	Adult worker bees will remove these dead larvae le
443532414    122847	y are curled at the base of uncapped brood cells. 	Adult worker bees will remove these dead larvae le
291841562     80699	y are curled at the base of uncapped brood cells. 	Adult worker bees will remove these dead larvae le
448005759    124105	y are curled at the base of uncapped brood cells. 	Adult worker bees will remove these dead larvae le


2008-12-16 2009-01-14 2009-01-26 2009-02-05 2009-02-09
2009-03-02 2009-03-10 2009-03-11 2009-03-23 2009-03-30
2009-04-08 2009-04-17 2009-04-21 2009-04-23 2009-05-06
2009-05-15 2009-05-22 2009-05-28 2009-06-02 2009-07-01
2009-08-11 2009-08-31 2009-09-24 2009-11-20 2009-11-27
2009-12-21 2010-01-28

 [1,] "2009-01-14" "29"
 [2,] "2009-01-26" "12"
 [3,] "2009-02-05" "10"
 [4,] "2009-02-09" "4" 
 [5,] "2009-03-02" "21"
 [6,] "2009-03-10" "8" 
 [7,] "2009-03-11" "1" 
 [8,] "2009-03-23" "12"
 [9,] "2009-03-30" "7" 
[10,] "2009-04-08" "9" 
[11,] "2009-04-17" "9" 
[12,] "2009-04-21" "4" 
[13,] "2009-04-23" "2" 
[14,] "2009-05-06" "13"
[15,] "2009-05-15" "9" 
[16,] "2009-05-22" "7" 
[17,] "2009-05-28" "6" 
[18,] "2009-06-02" "5" 
[19,] "2009-07-01" "29"
[20,] "2009-08-11" "41"
[21,] "2009-08-31" "20"
[22,] "2009-09-24" "24"
[23,] "2009-11-20" "57"
[24,] "2009-11-27" "7" 
[25,] "2009-12-21" "24"
[26,] "2010-01-28" "38"


I made two versions of the crawler, one works on PDFs and the other grabs EN and FR abstracts directly from the page description. PDF crawler still needs some regex refining. 40 abstracts available in FR and EN at /home/yue.r/cardiac_arrest.
 
Richard

wget 'https://api.archives-ouvertes.fr/search/?q=le&wt=xml&rows=10000'

cd /work/k.church/Richard_Yue
# sbatch -t 1409 -p short hal_crawler2.py
# sbatch -t 1409 -p short hal_crawler2.py -N 20000 -q cancer -O cancer
sbatch -t 1409 -p short hal_crawler2.py -N 20000 -q arrêt -O arrêt
sbatch -t 1409 -p short hal_crawler2.py -N 20000 -q 'faux ami' -O faux_ami


awk '/ null/ {next}; {print FILENAME}' */*json > good.txt
python summarize.py `cat good.txt` > summary.txt

cd /work/k.church/Scarpino/promed/unpacked
egrep -ic egypt [0-9]??/* | awk -F: '$2 > 0 {print $1}'  > /tmp/egypt
egrep -ic avian [0-9]??/* | awk -F: '$2 > 0 {print $1}'  > /tmp/avian
comm -12 /tmp/egypt /tmp/avian | cut -f2 -d/ > /tmp/egypt_avian

mkdir -p /work/k.church/openalex
cd /work/k.church/openalex
# aws s3 ls --summarize --human-readable --no-sign-request --recursive "s3://openalex/"
# aws s3 sync "s3://openalex" "openalex-snapshot" --no-sign-request
sbatch -p short -t 1409 /work/k.church/openalex/openalex_sync_job.sh

cd /work/k.church/Walid
# python unpack.py -j results_out.json -d results_out
python unpack.py -j results_out_lax.json -d results_out_lax

for f in /work/k.church/Walid/results_out /work/k.church/Walid/results_out_lax
do
cd $f
mkdir -p summaries
awk '$1 == "parent_archive_number" {print $2 "\t" FILENAME}'  */* | sort | uniq1.sh | egrep . > summaries/magic_key_decoder_ring
done

cd /work/k.church/Scarpino/promed/unpacked
mv summaries/items_per_archive_number summaries/items_per_archive_number.bak
for f in [0-9][0-9][0-9]
do
awk '/Archive Number:/ {AN=$NF; x[AN]="NA"}; 
    length($1) < 5 && $1 ~ /^[\[][0-9][0-9]*[\]]$/ && substr($1,2,length($1)-2) <= 17 {x[AN]=$1}; 
    END {for(i in x) print i "\t" x[i]}' $f/*
done > summaries/items_per_archive_number

cd /home/k.church/to_go/reviewing/ACL/ACL-2023/submissions
for f in *.pdf
do
if [ ! -s $f.json ]
then
echo working on $f
pdfx -j $f > $f.json &
fi
done

for f in *.pdf.json
do
python extract_references.py $f | $src/fetch_corpusId.py > $f.with_corpusIds.txt &
done

export SPECTER_API_KEY="Us7RqgayhnaQkEKiEnbGH8EBneX5Jud14Mq3Uzpe"
cd /home/k.church/to_go/reviewing/ACL/ACL-2023/submissions
for f in *.with_corpusIds.txt
do
if [ ! -s $f.authors ]
then
echo working on $f
cut -f1 < $f | $src/fetch_from_semantic_scholar_api.py --fields externalIds,authors,title > $f.authors
fi
done

sort -u *.authors | python extract_authors.py | sort -u > /tmp/a
cat /tmp/a | $src/fetch_from_semantic_scholar_api.py --API author --fields name,hIndex > authors.json

export SPECTER_API_KEY="Us7RqgayhnaQkEKiEnbGH8EBneX5Jud14Mq3Uzpe"
cd /home/k.church/to_go/reviewing/ACL/ACL-2023/submissions
for f in 1207*.authors 1642*.authors 2012*.authors 3401*.authors 3891*.authors 
do
sort -u $f | python extract_authors.py | sort -u | $src/fetch_from_semantic_scholar_api.py --API author --fields name,hIndex > $f.json &
done

python extract_references.py *json | $src/fetch_corpusId.py > papers_with_corpusIds.txt


@InProceedings{pmlr-v97-song19d,
  title =	 {{MASS}: Masked Sequence to Sequence Pre-training for Language Generation},
  author =       {Song, Kaitao and Tan, Xu and Qin, Tao and Lu, Jianfeng and Liu, Tie-Yan},
  booktitle = 	  {Proceedings of the 36th International Conference on Machine Learning},
  pages =    {5926--5936},
  year =      {2019},
  editor =     {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume =      {97},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	  {09--15 Jun},
  publisher =    {PMLR},
  pdf =      {http://proceedings.mlr.press/v97/song19d/song19d.pdf},
  url =       {https://proceedings.mlr.press/v97/song19d.html},
  abstract =   {Pre-training and fine-tuning, e.g., BERT&nbsp;\citep{devlin2018bert}, have achieved great success in language understanding by transferring knowledge from rich-resource pre-training task to the low/zero-resource downstream tasks. Inspired by the success of BERT, we propose MAsked Sequence to Sequence pre-training (MASS) for the encoder-decoder based language generation tasks. MASS adopts the encoder-decoder framework to reconstruct a sentence fragment given the remaining part of the sentence: its encoder takes a sentence with randomly masked fragment (several consecutive tokens) as input, and its decoder tries to predict this masked fragment. In this way, MASS can jointly train the encoder and decoder to develop the capability of representation extraction and language modeling. By further fine-tuning on a variety of zero/low-resource language generation tasks, including neural machine translation, text summarization and conversational response generation (3 tasks and totally 8 datasets), MASS achieves significant improvements over the baselines without pre-training or with other pre-training methods. Especially, we achieve the state-of-the-art accuracy (30.02 in terms of BLEU score) on the unsupervised English-French translation, even beating the early attention-based supervised model&nbsp;\citep{bahdanau2015neural}.}
}

rel=2022-12-02
cd /work/k.church/semantic_scholar/releases/$rel/database/papers

job1=`sbatch -p short --array 1-30 /work/k.church/semantic_scholar/papers/papers_to_titles.py --input papers.piece.%03d.gz --output titles.piece.%03d | awk '{print $NF}'`
sbatch -d afterany:$job1 -p short --array 1-30 /work/k.church/semantic_scholar/papers/sort.sh titles.piece.%03d titles.piece.%03d.s

LC_ALL=C sort  -T . -S '90%'  titles.piece.???.s > titles.txt

# This isn't working
echo 'WHAT WERE THE' | find_in_sorted_lines --partial_matches --multiple_matches --delimiter '\n' --input titles.txt

echo 'Poly-encoders: Architectures and Pre-training Strategies for Fast and Accurate Multi-sentence Scoring' | find_in_sorted_lines  --delimiter '\n' --input /work/k.church/semantic_scholar/releases/2022-12-02/database/papers/titles.txt 

cd /home/k.church/to_go/reviewing/ACL/ACL-2023/submissions
awk '$2 == "NA"' papers_with_corpusIds.txt  | cut -f3- | $src/urls_to_titles.py > papers_with_corpusIds.titles

awk '$1 != "NA"' papers_with_corpusIds.titles | cut -f1 | awk -F'|' 'NF > 1 {print $1}' | sed 's/ *$//' | sed 's/^ *//g' | sed 's/  */ /g' | 
find_in_sorted_lines  --delimiter '\n' --input /work/k.church/semantic_scholar/releases/2022-12-02/database/papers/titles.txt > papers_with_corpusIds.titles2


awk '$1 != "NA"' papers_with_corpusIds.titles | 
awk -F'\t' '{n = split($1,x,"|"); title=x[1]; gsub(/^ */, "", title); gsub(/ *$/, "", title); gsub(/  */, " ", title); 
gsub(/ [\-] [A-Za-z0-9 ]*$/, "", title); 
print title, $2, $3}' OFS="\t" > /tmp/x
cut -f1 < /tmp/x | find_in_sorted_lines  --delimiter '\n' --input /work/k.church/semantic_scholar/releases/2022-12-02/database/papers/titles.txt > papers_with_corpusIds.titles3

# hand edit papers_with_corpusIds.titles3
cut -f2 /tmp/x | paste papers_with_corpusIds.titles3 - > papers_with_corpusIds.titles4

awk -F'\t' 'BEGIN {while(getline < "papers_with_corpusIds.titles4" > 0) x[$NF] = $(NF-1)}; 
$2 != "NA" {print; next}; $(NF-1) in x {print $1, x[$(NF-1)], $3, $4; next}; {print}' OFS="\t" papers_with_corpusIds.txt > papers_with_corpusIds.corrected

https://github.com/jsvine/pdfplumber

from PyPDF2 import PdfReader
f='4249_file_Paper.pdf'
>>> reader = PdfReader(f)
>>> len(reader.pages)
12
>>> text = [ p.extract_text() for p in reader.pages ]
>>> text[0]

cd /home/k.church/to_go/reviewing/ACL/ACL-2023/submissions
awk '$2 != "NA" {print $NF "\t" $2}' papers_with_corpusIds.corrected | sort -u | uniq1.sh | egrep .  > papers2corpusids

cat reviewers.txt | awk -F/ '{print $NF}' | $src/fetch_papers_from_authors.py > reviewers.out


AuthorInfo	1698689	41	6899	191	{'DBLP': ['Nebojsa Jojic']}	https://www.semanticscholar.org/author/1698689	N. Jojic	[]
AuthorInfo	47977519	36	5105	216	{'DBLP': ['Ajay Divakaran']}	https://www.semanticscholar.org/author/47977519	Ajay Divakaran	[]
AuthorInfo	48822754	22	2543	500	{'DBLP': ['Adarsh Kumar', 'Anil Kumar', 'Anil Kumar 0001', 'Anil Kumar 0009', 'Anil Kumar 0010']}	https://www.semanticscholar.org/author/48822754	Adarsh Kumar	[]
AuthorInfo	145262461	22	1925	111	{'DBLP': ['Trung Bui', 'Tuan-Dung Bui']}	https://www.semanticscholar.org/author/145262461	Trung Bui	[]
AuthorInfo	2191455	16	1645	37	{'DBLP': ['Sheng Shen']}	https://www.semanticscholar.org/author/2191455	Sheng Shen	['University of California, Berkeley']
AuthorInfo	2909575	13	1281	106	{'DBLP': ['Amrita Saha']}	https://www.semanticscholar.org/author/2909575	Amrita Saha	[]
AuthorInfo	1785372925	13	2207	28	{'DBLP': ['Tianlu Wang']}	https://www.semanticscholar.org/author/1785372925	Tianlu Wang	[]
AuthorInfo	38531701	11	359	57	{'DBLP': ['Saurav Sahay']}	https://www.semanticscholar.org/author/38531701	Saurav Sahay	[]
AuthorInfo	2634203	11	984	22	{'DBLP': ['Shubham Toshniwal']}	https://www.semanticscholar.org/author/2634203	Shubham Toshniwal	[]
AuthorInfo	24025563	11	612	22	{'DBLP': ['Vikas Raunak']}	https://www.semanticscholar.org/author/24025563	Vikas Raunak	[]
AuthorInfo	51183248	10	3378	16	{'DBLP': ['Ledell Wu', 'Ledell Yu Wu', 'Yu Wu']}	https://www.semanticscholar.org/author/51183248	Ledell Yu Wu	['Facebook AI Research']
AuthorInfo	3466801	10	342	25	{'DBLP': ['Prafulla Choubey', 'Prafulla K. Choubey', 'Prafulla Kumar Choubey']}	https://www.semanticscholar.org/author/3466801	Prafulla Kumar Choubey	['Texas A&M Univeristy']
AuthorInfo	146452866	10	320	15	{}	https://www.semanticscholar.org/author/146452866	Chujie Zheng	['Tsinghua University']
AuthorInfo	33842085	9	276	38	{'DBLP': ['Adit Krishnan', 'Aditya Krishnan', 'Aditya Krishnan 0001']}	https://www.semanticscholar.org/author/33842085	A. Krishnan	[]
AuthorInfo	31773000	9	621	15	{'DBLP': ['Saiful Bari'], 'ORCID': '0000-0001-8795-8040'}	https://www.semanticscholar.org/author/31773000	M Saiful Bari	['Nanyang Technological University']
AuthorInfo	2604621	9	222	27	{'DBLP': ['Meriem Beloucif']}	https://www.semanticscholar.org/author/2604621	Meriem Beloucif	[]
AuthorInfo	67284811	8	441	15	{'DBLP': ['Mengzhou Xia']}	https://www.semanticscholar.org/author/67284811	M. Xia	[]
AuthorInfo	47827264	8	256	12	{'DBLP': ['Yang Zhao']}	https://www.semanticscholar.org/author/47827264	Yang Zhao	['IBM Research - Tokyo']
AuthorInfo	2118204633	8	487	13	{'DBLP': ['Christy Li', 'Christy Y. Li', 'Christy Yuan Li', 'Yuan Li']}	https://www.semanticscholar.org/author/2118204633	Yuan Li	[]
AuthorInfo	143733211	8	656	20	{'DBLP': ['Julián Salazar'], 'ORCID': '0000-0003-0943-7594'}	https://www.semanticscholar.org/author/143733211	Julian Salazar	['Amazon AWS AI']
AuthorInfo	37374479	6	76	14	{'DBLP': ['He Bai', 'He Bai 0002'], 'ORCID': '0000-0002-8933-647X'}	https://www.semanticscholar.org/author/37374479	He Bai	['University of Waterloo']
AuthorInfo	95439281	5	127	14	{'DBLP': ['Yanjie Gou']}	https://www.semanticscholar.org/author/95439281	Yanjie Gou	[]
AuthorInfo	68974453	5	237	8	{'DBLP': ['Asa Cooper Stickland']}	https://www.semanticscholar.org/author/68974453	Asa Cooper Stickland	[]
AuthorInfo	33906968	5	126	24	{'DBLP': ['Shashank Mujumdar']}	https://www.semanticscholar.org/author/33906968	Shashank Mujumdar	[]
AuthorInfo	27526892	5	76	23	{'DBLP': ['Balaji Ganesan']}	https://www.semanticscholar.org/author/27526892	Balaji Ganesan	[]
AuthorInfo	26339093	5	1566	7	{'DBLP': ['Yige Xu 0001'], 'ORCID': '0000-0002-6718-1251'}	https://www.semanticscholar.org/author/26339093	Yige Xu	['Nanyang Technological University']
AuthorInfo	2146380510	5	243	9	{'DBLP': ['Qian Chen']}	https://www.semanticscholar.org/author/2146380510	Qian Chen	[]
AuthorInfo	2027599235	5	174	8	{}	https://www.semanticscholar.org/author/2027599235	Mukai Li	[]
AuthorInfo	145790135	5	474	5	{}	https://www.semanticscholar.org/author/145790135	Erik Jones	[]
AuthorInfo	120419790	5	589	7	{'ORCID': '0000-0001-5707-2134'}	https://www.semanticscholar.org/author/120419790	Jos Rozen	['Naver Labs Europe']
AuthorInfo	82742429	4	51	15	{'DBLP': ['Abhilash Nandy']}	https://www.semanticscholar.org/author/82742429	Abhilash Nandy	['Indian Institute of Technology, Kharagpur', 'L3S Research Center, Leibniz Universität Hannover']
AuthorInfo	2123247219	4	31	9	{'DBLP': ['Alexander Hanbo Li']}	https://www.semanticscholar.org/author/2123247219	A. Li	[]
AuthorInfo	2117786875	4	108	13	{}	https://www.semanticscholar.org/author/2117786875	Yao Fu	[]
AuthorInfo	46177458	3	167	6	{'DBLP': ['Liliang Ren']}	https://www.semanticscholar.org/author/46177458	Liliang Ren	['University of Illinois Urbana Champaign']
AuthorInfo	2139767217	3	70	7	{'DBLP': ['Yanis Labrak']}	https://www.semanticscholar.org/author/2139767217	Yanis Labrak	[]
AuthorInfo	2112127876	3	58	7	{'DBLP': ['Siddharth Deepak Mishra']}	https://www.semanticscholar.org/author/2112127876	S. Mishra	[]


120419790	Jos Rozen	239009562	364	Multitask Prompted Training Enables Zero-Shot Task Generalization
2191455	Sheng Shen	239009562	364	Multitask Prompted Training Enables Zero-Shot Task Generalization
31773000	M Saiful Bari	239009562	364	Multitask Prompted Training Enables Zero-Shot Task Generalization
120419790	Jos Rozen	249538544	157	Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models
24025563	Vikas Raunak	249538544	157	Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models
2634203	Shubham Toshniwal	249538544	157	Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models
2191455	Sheng Shen	253264914	12	Crosslingual Generalization through Multitask Finetuning
31773000	M Saiful Bari	253264914	12	Crosslingual Generalization through Multitask Finetuning
120419790	Jos Rozen	253420279	61	BLOOM: A 176B-Parameter Open-Access Multilingual Language Model
2139767217	Yanis Labrak	253420279	61	BLOOM: A 176B-Parameter Open-Access Multilingual Language Model
2191455	Sheng Shen	253420279	61	BLOOM: A 176B-Parameter Open-Access Multilingual Language Model
24025563	Vikas Raunak	253420279	61	BLOOM: A 176B-Parameter Open-Access Multilingual Language Model
31773000	M Saiful Bari	253420279	61	BLOOM: A 176B-Parameter Open-Access Multilingual Language Model

cd /home/k.church/to_go/reviewing/ACL/ACL-2023/submissions
cut -f2- < papers2corpusids | tr '\t' '\n' | sort -u | awk '$1 < 254017244' > papers1
egrep -v AuthorInfo reviewers.out | cut -f3 | sort -u | awk '$1 < 254017244' > papers2

cd /home/k.church/to_go/reviewing/ACL/ACL-2023/submissions
cat papers1 papers2 | sort -u | awk '$1 < 254017244' > papers3

dir=/work/k.church/semantic_scholar/releases/2022-12-02/database/embeddings
K=768
ffile=$dir/specter.kwc.edges.f
map=$dir/specter.kwc.nodes.txt
for p in papers1 papers2 papers3
do
id_to_floats --floats $ffile --record_size $K --map $map --binary_output < $p > $p.specter.K$K.f
$src/array_fromfile.py -i $p.specter.K$K.f -o $p.specter.K$K.npy -K $K -d float32
done

K=280
dir=/work/k.church/semantic_scholar/citations/graphs/K$K/papers/
ffile=$dir/citations.npz.papers.kwc.edges.f
map=$dir/map
for p in papers1 papers2 papers3
do
id_to_floats --floats $ffile --record_size $K --map $map --binary_output < $p > $p.proposed.K$K.f
$src/array_fromfile.py -i $p.proposed.K$K.f -o $p.proposed.K$K.npy -K $K -d float32
done

$src/my_cos.py -X papers1.specter.K768.npy -Y papers2.specter.K768.npy 
$src/my_cos.py -X papers1.proposed.K280.npy -Y papers2.proposed.K280.npy 

paste papers1 papers1.best papers1.best_scores | head

dir=/work/k.church/semantic_scholar/releases/2022-12-02/database/embeddings
K=768
ffile=$dir/specter.kwc.edges.f
map=$dir/specter.kwc.nodes.txt
paste papers1 papers1.best | head | $src/C/score_pairs --record_size $K --floats $ffile --map $map 

$src/my_cos.py -X papers1.proposed.K280.npy -Y papers2.proposed.K280.npy 
--Return--
> /work/k.church/semantic_scholar/citations/graphs/src/my_cos.py(32)<module>()->None
-> pdb.set_trace()
(Pdb) a1 = np.argmax(sim, axis=1)
(Pdb) p1 = np.loadtxt('papers1', dtype=int)
(Pdb) p2 = np.loadtxt('papers2', dtype=int)
(Pdb) m1 = np.max(sim, axis=1)
(Pdb) np.savetxt('papers1.proposed.best_scores', m1, fmt='%0.3f')
(Pdb) np.savetxt('papers1.proposed.best', p2[a1], fmt='%d')
(Pdb) quit()

K=280
dir=/work/k.church/semantic_scholar/citations/graphs/K$K/papers/
ffile=$dir/citations.npz.papers.kwc.edges.f
map=$dir/map
paste papers1 papers1.proposed.best | head | $src/C/score_pairs --record_size $K --floats $ffile --map $map 

adir=/work/k.church/githubs/scidocs/data/recomm/
$adir/line2both.sh $id 280 > $HOME/to_go/Muskan.html

cd /home/k.church/to_go/reviewing/ACL/ACL-2023/submissions
mkdir -p pages
adir=/work/k.church/githubs/scidocs/data/recomm/
cat papers1 |
while read p
do
sbatch -p debug -o pages/$p.html $adir/line2both.sh $p 
done

openalex ids
/scratch/sun.jiam


cd /work/k.church/openalex/openalex-snapshot/data/works
for dir in *
do
for f in $dir/*.gz
do
outf=$mydir/$dir/$f.out
mkdir -p `dirname $outf`
sbatch -o $outf -p short ~k.church/openalex $f
done

$src/array_tofile.py -i citations.G.X.npy -o citations.G.X.i
$src/array_tofile.py -i citations.G.Y.npy -o citations.G.Y.i

cd /work/k.church/semantic_scholar/citations/graphs
sbatch -o walk2.txt -p short $src/random_walk.py -G citations.G.npz -D 100 -N 1000000

$src/random_walk.py -G citations.G.npz > walk.txt

cd /work/k.church/semantic_scholar/citations/graphs
K=280
dir=/work/k.church/semantic_scholar/citations/graphs/K$K/papers/
ffile=$dir/citations.npz.papers.kwc.edges.f
map=$dir/map
# cut -f1,2 < walk2.txt | pairs_to_cos --map $map --floats $ffile --record_size $K > /work/k.church/semantic_scholar/citations/graphs/walk2.proposed
export K
export ffile
export map
sbatch -i walk2.txt -o walk2.proposed.V2 -p short -t 1409 $src/C/simple_pairs_to_cos.sh -i walk2.txt -o walk2.proposed.V2 $src/C/simple_pairs_to_cos.sh

cd /work/k.church/semantic_scholar/citations/graphs
dir=/work/k.church/semantic_scholar/releases/2022-12-02/database/embeddings
K=768
ffile=$dir/specter.kwc.edges.f
map=$dir/specter.kwc.nodes.txt
# cut -f1,2 < walk2.txt | pairs_to_cos --map $map --floats $ffile --record_size $K > /work/k.church/semantic_scholar/citations/graphs/walk2.specter
export K
export ffile
export map
sbatch -i walk2.txt -o walk2.specter.V2 -p short -t 1409 $src/C/simple_pairs_to_cos.sh

/work/k.church/pubmed/ftp.ncbi.nlm.nih.gov/pubmed/baseline
awk '/<Abstract>/ {go++}; /<\/Abstract>/ {go=0}; /VernacularTitle/ {print}; /<OtherAbstract Type="Publisher" Language="fre">/ {go++}; /<\/OtherAbstract>/ {go=0}; go > 0' /tmp/y >/tmp/zz

cd /work/k.church/pubmed/ftp.ncbi.nlm.nih.gov/pubmed/baseline
for f in *gz
do
echo $f `zcat $f | egrep -c '<OtherAbstract.* Language="fre"'`
done

cd /work/k.church/pubmed/ftp.ncbi.nlm.nih.gov/pubmed/baseline
ls *gz | awk '{x[NR] = $0}; END {for(i=NR;i>0; i--) print x[i]}' | 
while read f
do
echo $f `zcat $f | egrep -c '<OtherAbstract.* Language="fre"'`
done

https://github.com/boxiangliu/ParaMed
https://aclanthology.org/W19-5403.pdf
https://aclanthology.org/volumes/W19-54/


['252716960', '247762169', '247997008', '232352485', '46960659', '211126663', '117626862', '2272000', '355118']



cd $HOME/to_go
cut -f1 election.json.txt | /work/k.church/semantic_scholar/citations/graphs/src/urls_to_titles.py > election.json.titles

sed 's/ [:\-\|] .*$//g' /tmp/z | sed 's/ [:\-\|] .*$//g' | uniq | egrep . | sed 's/^ *//g' > /tmp/zz

tr '|' ':' < /tmp/z |
awk -F' - ' '{print $1}' |
awk -F' : ' '{print $1}' |
uniq | 
awk 'NF > 1' | 
sed 's/^ *//g' > /tmp/zz

cat /tmp/zz | find_in_sorted_lines --multiple_matches --delimiter '\n' --input /work/k.church/semantic_scholar/releases/2022-12-02/database/papers/titles.txt > /tmp/zzz
cut -f1 < /tmp/zzz  | uniq -c | awk '$1 > 100'

# sorted list of 203M MAG ids
# /scratch/sun.jiam/merged_sorted_output.mag.sorted
# /work/k.church/semantic_scholar/releases/2022-12-02/database/external_ids/MAG/sorted_uniq
cd /work/k.church/semantic_scholar/releases/4way/venn
venn /scratch/sun.jiam/merged_sorted_output.mag.sorted /work/k.church/semantic_scholar/releases/*/database/external_ids/MAG/sorted_uniq > MAG
awk 'NF ==1 {print; next}; {x[$2]++}; END {for(i in x) print x[i] "\t" i}' MAG | sort -nr > MAG.summary2

# Jiameng: here is what I did over the weekend


export SPECTER_API_KEY="Us7RqgayhnaQkEKiEnbGH8EBneX5Jud14Mq3Uzpe"
cd /work/k.church/openalex/openalex-snapshot/data/works/updated_date=2022-02-13
python $HOME/openalex_kwc.py out part_000.gz 
awk '{print "MAG:" $1}' out.mag | $src/fetch_corpusId.py > out.mag.semantic_scholar 

cd /work/k.church/openalex/openalex-snapshot/data/works
for dir in updated_date*
do
outf=$dir/refs.out
errf=$dir/refs.err
if [ ! -s $outf ]
then
sbatch -e $errf -o $outf -p short /work/k.church/openalex/src/openalex_extract_references.py $dir/*gz
fi
done

cd /work/k.church/openalex/openalex-snapshot/data/works
for dir in updated_date*
do
outf=$dir/open_access.out
errf=$dir/open_access.err
if [ ! -s  $outf ]
then
sbatch -e $errf -o $outf -p short /work/k.church/openalex/src/openalex_extract_pdf.py $dir/*gz
fi
done

cd /work/k.church/openalex/openalex-snapshot/data/works
for dir in updated*
do
outf=$dir/ids.out
errf=$dir/ids.err
mkdir -p `dirname $outf`
sbatch -e $errf -p short /work/k.church/openalex/src/openalex_kwc.py $outf $dir/*.gz
done

cd /work/k.church/openalex/openalex-snapshot/data/works
for dir in updated_date*
do
outf=$dir/id.out
errf=$dir/id.err
if [ ! -s  $outf ]
then
sbatch -e $errf -o $outf -p short /work/k.church/openalex/src/openalex_extract_ids.sh $dir/*gz
fi
done

cd /work/k.church/openalex/openalex-snapshot/data/works
for dir in updated_date*
do
outf=$dir/title.out
errf=$dir/title.err
if [ ! -s  $outf ]
then
sbatch -e $errf -o $outf -p short /work/k.church/openalex/src/openalex_extract_title.py $dir/*gz
fi
done




cd /work/k.church/openalex/openalex-snapshot/data/works
for f in in updated_date*/refs.out
do
outf=$f.npz
errf=$outf.err
if [ ! -s  $outf ]
then
sbatch -i $f -e $errf -p short /work/k.church/openalex/src/openalex_refs_to_matrix.py --output $outf --map id.out.L
fi
done

cd /work/k.church/openalex/openalex-snapshot/data/works
sbatch --mem=200G -p short -e citations.G.err sum_matrices.py citations.G.npz updated_date*/refs.out.npz

python
import scipy.sparse
f='/work/k.church/openalex/openalex-snapshot/data/works/citations.G.npz'
M=scipy.sparse.load_npz(f)
GB = (M.data.nbytes + M.indices.nbytes + M.indptr.nbytes)/1e9

import numpy as np
fanout0 = np.sum(M, axis=0)
np.save('fanout0.npy', fanout0)
fanout1 = np.sum(M, axis=1)
np.save('fanout1.npy', fanout0)

$outdir/`basename $f .gz`.out

cd /scratch/k.church/semantic_scholar/releases/2022-12-02/database/citations/extractions/citing_sentences

We've done all but 29 of the 183 directories

ls -l ???/000.kwc.nodes.i | awk '{print $3}' | sort | uniq -c | sort -nr
     62 k.church
     36 chandra
     29 sun.jiam
     20 j.ortega
      7 a.schoene

These are the ones that aren't done yet

026	      114	    131		  137		178
100	      116	    132		  138		179
110	      117	    133		  139		180
111	      118	    134		  140		181
112	      119	    135		  176		182
113	      130	    136		  177


adir=/work/k.church/githubs/scidocs/data/recomm/
cd /work/k.church/hints/Beth
awk 'NF == 1' hints.txt | egrep -v auth | 
while read p
do
sbatch -p debug -o hints/$p.html $adir/line2both.sh $p 
done

adir=/work/k.church/githubs/scidocs/data/recomm/
cd /work/k.church/Beth
mkdir -p hints/refs
awk 'NF == 1' hints.txt | egrep -v auth | 
while read p
do
echo $p | $src/fetch_references_and_citations.py > hints/refs/$p.txt &
done

cd /scratch/sun.jiam
comm merged_sorted_ss.PubMedCentral merged_sorted_oa.pmcid.bytesorted | awk -F'\t' 'rand() < 50/8e6 {print NF "\thttps://www.ncbi.nlm.nih.gov/pmc/articles/PMC" $NF "/"}' > /tmp/x

cd /scratch/k.church/semantic_scholar/releases/2022-12-02/database/citations/extractions/citing_sentences/pieces
for dir in ???
do
if [ ! -s $dir/000.kwc.nodes.i ] 
then
echo $dir
fi
done

cd /work/k.church/semantic_scholar/citations/graphs
for f in walks/todo/x.d[a-z]
do
sbatch --mem=20G -p short -t 1409 -i $f -o $f.out -e $f.err $src/BFS.py -G citations.G.npz -D 10 --quick
done


cd /work/k.church/semantic_scholar/citations/graphs
for f in walks/todo/x.??
do
sbatch --mem=20G -p short -t 1409 -i $f -o $f.out.V3 -e $f.err $src/C/BFS.sh citations.G
done

cd /work/k.church/semantic_scholar/citations/graphs
awk 'BEGIN {FS=OFS="\t"; 
            while(getline < "walk2.proposed.V2" > 0) proposed[$2,$3]=$1; 
            while(getline < "walk2.specter.V2" > 0) specter[$2,$3]=$1
	    print "bestDist", "steps", "src", "dest", "walkDist", "specter", "proposed"};
  {key = $3 SUBSEP $4}; 
  !(key in specter) {specter[key]="NA"}
  !(key in proposed) {proposed[key]="NA"}
  $1 > 0 && killroy[key]++ < 1 {print $0, specter[key], proposed[key]}' walk2.V3 > walk2.V3.merged

/work/nlp/sun.jiam/ss_id_time_dict_kwc.py
/work/nlp/sun.jiam/openalex_ids_years_dict_kwc.py


cd /work/k.church/openalex/openalex-snapshot/data/works
outdir=/scratch/k.church/openalex/id_and_year
for dir in in updated_date*
do
outf=$outdir/`basename $dir`.txt
errf=$outf.err
if [ ! -s  $outf ]
then
sbatch -e $errf -p short /work/k.church/openalex/openalex_extract_id_and_year.py $outf $dir/*gz
fi
done

cd /work/k.church/semantic_scholar/citations/graphs
$src/index_X.py -X citations.G.X.i -O citations.G.X.idx.i

cd /work/k.church/sun.jiam/scirepeval_data/test/scidocs/view_cite_read/cite
awk '{print $1; print $3}' *qrel | sort -u > ids.txt
$src/fetch_corpusId.py < ids.txt > ids.out
mkdir todo
split -l 500 ids.txt todo/x.
for f in todo/x.??
do
sbatch -p express -i $f -o $f.out $src/fetch_corpusId.py
done

[7:32 AM] Ortega, John
./slurm-33607388_60.out:The above exception was the direct cause of the following exception:
./slurm-33607388_61.out:numpy.core._exceptions._ArrayMemoryError: Unable to allocate 33.3 GiB for an array with shape (74392238, 60) and data type float64

[7:33 AM] Ortega, John
./slurm-33712670_990.out:PermissionError: [Errno 13] Permission denied: '/scratch/k.church/semantic_scholar/releases/2022-12-02/database/citations/extractions/citing_sentences/pieces/10[1-9]/990.kwc.nodes.i

[7:33 AM] Ortega, John
/work/nlp/j.ortega/paper_recommender/semantic_scholar_specter/prone_benchmarks/perm_denied_error

[7:33 AM] Ortega, John
that is where all of the "errors" are at...

[7:34 AM] Ortega, John
On a positive note

[7:34 AM] Ortega, John
there are about 100 more than before (originally we had 60 runs now we have 167 good ones)

[7:34 AM] Ortega, John
/work/nlp/j.ortega/paper_recommender/semantic_scholar_specter/prone_benchmarks <-- the good ones are here

[7:35 AM] Ortega, John
My script is there also: get_prone_benchmarks.sh (there are two more scripts in there for getting the edges)

[7:35 AM] Ortega, John
I will get back to this later today

[7:36 AM] Ortega, John
going to head out for lunch around 11:30 or so to meet a friend by Northeastern


cd /work/nlp/j.ortega/paper_recommender/semantic_scholar_specter/prone_benchmarks
for f in */slurm*out
do
awk 'NR == 1 {if(/sample_edges/) good++}; END { print(FILENAME, (good ? "starts with sample_edges" : "does not start with sample_edges"), (NR > 5))}' $f
done | cut -f2- -d ' ' | sort | uniq -c


   1000 does not start with sample_edges 0
    317 starts with sample_edges 1

cd /work/nlp/j.ortega/paper_recommender/semantic_scholar_specter/prone_benchmarks
egrep sample_edges.py *out | awk '{print $1, $(NF-2)}' > /tmp/S
cat /tmp/S | awk '{print "seff", substr($1, 7, 11)}' | sh | egrep 'Memory Utilized' > /tmp/S.memory
paste /tmp/S /tmp/S.memory | awk '{print $2, $5}' >  $HOME/to_go/S
egrep 'tsvd_rand.*bytes' *out > /tmp/S.bytes
paste /tmp/S /tmp/S.memory /tmp/S.bytes | awk 'BEGIN {print "S GB.total GB.svd K"}; {print $2, $5, $(NF-1), $(NF-6)}' >  $HOME/to_go/S

cd /work/k.church/sun.jiam/scirepeval_data/test/scidocs/view_cite_read/cite
dir=/work/k.church/semantic_scholar/releases/2022-12-02/database/embeddings
K=768
ffile=$dir/specter.kwc.edges.f
map=$dir/specter.kwc.nodes.txt
# cut -f1,2 < walk2.txt | pairs_to_cos --map $map --floats $ffile --record_size $K > /work/k.church/semantic_scholar/citations/graphs/walk2.specter
export K
export ffile
export map
# sbatch -i walk2.txt -o walk2.specter.V2 -p short -t 1409 $src/C/simple_pairs_to_cos.sh
cut -f1,2 < test.qrel.corpusId | egrep -v NA | pairs_to_cos --map $map --floats $ffile --record_size $K > test.qrel.corpusId.specter

cd /work/k.church/sun.jiam/scirepeval_data/test/scidocs/view_cite_read/cite
K=280
dir=/work/k.church/semantic_scholar/citations/graphs/K$K/papers/
ffile=$dir/citations.npz.papers.kwc.edges.f
map=$dir/map
# cut -f1,2 < walk2.txt | pairs_to_cos --map $map --floats $ffile --record_size $K > /work/k.church/semantic_scholar/citations/graphs/walk2.proposed
export K
export ffile
export map
# sbatch -i walk2.txt -o walk2.proposed.V2 -p short -t 1409 $src/C/simple_pairs_to_cos.sh -i walk2.txt -o walk2.proposed.V2 $src/C/simple_pairs_to_cos.sh
cut -f1,2 < test.qrel.corpusId | egrep -v NA | pairs_to_cos --map $map --floats $ffile --record_size $K > test.qrel.corpusId.proposed

egrep -v NA test.qrel.corpusId | paste test.qrel.corpusId.specter test.qrel.corpusId.proposed - > test.qrel.corpusId.combo

/scratch/sun.jiam/PubMed_analysis


cd /work/k.church/sun.jiam/scirepeval_data/test/scidocs/view_cite_read/cite
egrep -v NA ids2.out | cut -f1 > ids2.out.ss.txt
egrep -v NA ids2.out | cut -f2 > ids2.out.corpusId.txt

dir=/work/k.church/semantic_scholar/releases/2022-12-02/database/embeddings
K=768
ffile=$dir/specter.kwc.edges.f
map=$dir/specter.kwc.nodes.txt
id_to_floats --map $map --floats $ffile --record_size $K --binary_output < ids2.out.corpusId.txt > ids2.out.corpusId.specter.f

cd /work/k.church/sun.jiam/scirepeval_data/test/scidocs/view_cite_read/cite
K=280
dir=/work/k.church/semantic_scholar/citations/graphs/K$K/papers/
ffile=$dir/citations.npz.papers.kwc.edges.f
map=$dir/map
id_to_floats --map $map --floats $ffile --record_size $K --binary_output < ids2.out.corpusId.txt > ids2.out.corpusId.proposed.f


cd /scratch/k.church/semantic_scholar/releases/2022-12-02/database/citations/extractions/citing_sentences/pieces
for dir in [0-9][0-9][0-9]
do
sbatch -p express -t 59 /work/k.church/semantic_scholar/stupid_distinct_ids.sh $dir
done
LC_ALL=C sort -m -u  -T . -S '90%' ???/distinct_corpusIds.txt > distinct_corpusIds.txt &

For the random samples of ACL venn outputs, please check
/scratch/sun.jiam/ACL_analysis/ ACL.sample
 
For the random samples for PubMed venn outputs, please check /scratch/sun.jiam/PubMed_analysis/ PMID.sample

cd /work/k.church/sun.jiam/scirepeval_data/test/scidocs/view_cite_read/cite
awk 'NR == 1 {FS=OFS="\t"; print "fit", $0; next}; {print -4.586628  + $4 *  7.830600 , $0}' /home/k.church/to_go/test.qrel.corpusId.combo > /home/k.church/to_go/test.qrel.corpusId.combo.fit


partition=scratch
# out of space on work
# partition=work
mkdir -p  /$partition/k.church/semantic_scholar/releases/2022-12-02/database/citations/extractions/citing_sentences/
cd /scratch/k.church/semantic_scholar/releases/2022-12-02/database/citations/extractions/citing_sentences/pieces/
for dir in [0-9][0-9][0-9]
do
cat $dir/[0-9][0-9][0-9].kwc.nodes.i
done  > /$partition/k.church/semantic_scholar/releases/2022-12-02/database/citations/extractions/citing_sentences/citing_sentences.kwc.nodes.i

cd /scratch/k.church/semantic_scholar/releases/2022-12-02/database/citations/extractions/citing_sentences/pieces/
for dir in [0-9][0-9][0-9]
do
cat $dir/[0-9][0-9][0-9].kwc.edges.f 
done > /$partition/k.church/semantic_scholar/releases/2022-12-02/database/citations/extractions/citing_sentences/citing_sentences.kwc.edges.f

ls -l /$partition/k.church/semantic_scholar/releases/2022-12-02/database/citations/extractions/citing_sentences/citing_sentences.kwc.*.[if] |
 awk '{x[NR]=$5}; END {print x[2]/x[3]}'

ls -l /$partition/k.church/semantic_scholar/releases/2022-12-02/database/citations/extractions/citing_sentences/citing_sentences.kwc.*.[if] |
 awk '{x[NR]=$5}; END {print x[1]/x[2]}'

# cd /$partition/k.church/semantic_scholar/releases/2022-12-02/database/citations/extractions/citing_sentences
# ffile=citing_sentences.kwc.edges.f
# B=6
# K=768
# sbatch --mem=800G -p short -t 1209 --array 50-99 $src/C/floats_to_idx.sh $K $ffile $B

partition=scratch
# out of space on work
# partition=work
cd /$partition/k.church/semantic_scholar/releases/2022-12-02/database/citations/extractions/citing_sentences
ffile=citing_sentences.kwc.edges.f
B=6
K=768
sbatch --mem=100G -p short -t 1209 --array 10-20 $src/C/floats_to_idx.sh $K $ffile $B

partition=scratch
# out of space on work
# partition=work
cd /$partition/k.church/semantic_scholar/releases/2022-12-02/database/citations/extractions/citing_sentences
ffile=citing_sentences.kwc.edges.f
B=4
K=768
sbatch --mem=5G -p short -t 1409 --array 41-60 $src/C/floats_to_idx.sh $K $ffile $B

partition=scratch
# out of space on work
# partition=work
cd /scratch/k.church/semantic_scholar/releases/2022-12-02/database/citations/extractions/citing_sentences/pieces/
for dir in [0-9][0-9][0-9] # [0-9][0-9][5-9] #  [0-9][0-9][0-9]
do
ffile=$dir/citing_sentences.kwc.edges.f
# cat $dir/[0-9][0-9][0-9].kwc.edges.f > $ffile
B=6
K=768
if [ ! -s $dir/citing_sentences.kwc.edges.f.*.50.B6.i ]
then
sbatch -p short -t 1409 --array 50-59 $src/C/floats_to_random_bytes.sh $K $B $ffile
fi
done


partition=scratch
# out of space on work
# partition=work
cd /scratch/k.church/semantic_scholar/releases/2022-12-02/database/citations/extractions/citing_sentences/pieces/
for dir in [0-9][0-9][0-9] # [0-9][0-9][5789] # [0-9][0-9][0-4] #  [0-9][0-9][0-9]
do
ffile=$dir/citing_sentences.kwc.edges.f
# cat $dir/[0-9][0-9][0-9].kwc.edges.f > $ffile
B=6
K=768
sbatch -p express -t 59 --array 31-33 $src/C/floats_to_random_bytes.sh $K $B $ffile
done

# April 1
cd /scratch/k.church/semantic_scholar/releases/2022-12-02/database/citations/extractions/citing_sentences/pieces/
for dir in `awk '$2 != $3 {print $1}' /tmp/ffiles*sum*`
do
rm $dir/citing_sentences.kwc.edges.f*
ffile=$dir/citing_sentences.kwc.edges.f
B=6
K=768
job1=`sbatch -p short create_citing_sentences.sh $dir | awk '{print $NF}'`
sbatch -d afterany:$job1 -p express --array 10-19,30-33 $src/C/floats_to_random_bytes.sh $K $B $ffile
done


partition=scratch
# out of space on work
# partition=work
cd /scratch/k.church/semantic_scholar/releases/2022-12-02/database/citations/extractions/citing_sentences/pieces/
for dir in [0-9][0-9][0-9]
do
if [ ! -s $dir/citing_sentences.kwc.edges.f ]
then
sbatch -p short create_citing_sentences.sh $dir
fi
done

cd /scratch/k.church/semantic_scholar/releases/2022-12-02/database/citations/extractions/citing_sentences/pieces/
cat citing_sentences_files.V2 | 
awk '{key = substr($NF,1,3); keys[key]++}; 
/citing_sent/ {V2 = 0; if($0 ~ /V2/) V2++; x[key, V2] += $7; next};
{y[key] += $7};
 END {for(key in keys) print key, y[key]+0, x[key,0]+0, x[key,1]+0}' | awk '$2 != $4' | wc

pip install S2search

cd /scratch/k.church/semantic_scholar/releases/2022-12-02/database/citations/extractions/citing_sentences/pieces/
for dir in [0-9][0-9][0-9]
do
for i in 10 11 12 13 14 15 16 17 18 19
do
if [ ! -s $dir/citing_sentences.kwc.edges.f.*.$i.B6.i ]
then
echo working on $dir $i
ffile=$dir/citing_sentences.kwc.edges.f
B=6
K=768
sbatch -p express -t 59 --array $i $src/C/floats_to_random_bytes.sh $K $B $ffile
fi
done
done

cd /scratch/k.church/semantic_scholar/releases/2022-12-02/database/citations/extractions/citing_sentences/pieces/
find [0-9][0-9][0-9] -name '*B6.i' -ls > /tmp/B6.V3 &
find [0-9][0-9][0-9] -name '*.kwc.edges.f' -ls > /tmp/ffiles.V3

awk '{key = substr($NF,1,3)}; 
/B6.i$/ {B6[key] += $7; next}; 
/citing/ {citing[key] += $7; next}; 
END {for(i in citing) print i, citing[i]+0, B6[i]+0}' /tmp/B6.V3  /tmp/ffiles.V3 | sort > /tmp/B6.V3.summary

awk '{key=substr($NF,1,3)}; 
/bak/ {next}
/citing/ {citing[key]=$7; next}; 
{x[key] += $7}; 
END {for(key in x) print key, x[key]+0, citing[key]}' OFS="\t" /tmp/ffiles.V3 |
sort  > /tmp/ffiles.V3.summary


cd /scratch/k.church/semantic_scholar/releases/2022-12-02/database/citations/extractions/citing_sentences/pieces
cat [0-9][0-9][0-9]/citing_sentences.kwc.nodes.i > citing_sentences.kwc.nodes.i & 
cat [0-9][0-9][0-9]/citing_sentences.kwc.edges.f > citing_sentences.kwc.edges.f & 

cd /scratch/k.church/semantic_scholar/releases/2022-12-02/database/citations/extractions/citing_sentences/pieces
x_to_y ia < citing_sentences.kwc.nodes.i > citing_sentences.kwc.nodes.txt
invert_mapping.sh citing_sentences.kwc.nodes.txt


partition=scratch
# out of space on work
# partition=work
cd /scratch/k.church/semantic_scholar/releases/2022-12-02/database/citations/extractions/citing_sentences/pieces/
for dir in [0-9][0-9][0-9]
do
sbatch -p short create_citing_sentences.sh $dir
done

sbatch --mem=30G -p short -t 1409 --array 10-19,30-33 /scratch/k.church/semantic_scholar/releases/2022-12-02/database/citations/extractions/citing_sentences/pieces/create_B6.sh

dir=/work/k.church/Scarpino/WHO/done
K=768
ffile=$dir/x.kwc.edges.f
map=$dir/x.kwc.nodes.txt
for idx in $dir/x.kwc.edges.f.simple.seed5?.K768.B6.idx.5?.i
do
x_to_y La < $idx | awk '{print prev, $1; prev=$1}' OFS="\t" | awk 'NF == 2 && rand() < 0.1' | 
pairs_to_cos --map $map --floats $ffile --record_size $K > $idx.cos
done

import numpy as np
x = np.random.choice(533195, 53709*2)
np.savetxt('random_baseline.txt', x, fmt='%d')

idx=random_baseline
pairs_to_cos --map $map --floats $ffile --record_size $K < $idx.txt > $idx.cos

dir=/work/k.church/Scarpino/WHO/done
B=6
K=768
ffile=$dir/x.kwc.edges.f
map=$dir/x.kwc.nodes.txt
sbatch -p express -t 59 --array 50 $src/C/floats_to_random_bytes.sh $K $B $ffile


ls -l x.kwc.edges.f.35057620.50.B6.i *B6.idx.50.i
-rw-rw----+ 1 k.church users 3199170 Apr  3 15:12 x.kwc.edges.f.35057620.50.B6.i
-rw-rw----+ 1 k.church users 4265560 Jan 25 17:37 x.kwc.edges.f.simple.seed50.K768.B6.idx.50.i

idx=x.kwc.edges.f.simple.seed50.K768.B6.idx.50.i
randombytes=x.kwc.edges.f.35057620.50.B6.i
# calibrate_random_bytes_index random_bytes random_bytes.idx [N (number of bytes per record)] floats K [--stdin|--ascii|--binary] > report
calibrate_random_bytes_index $randombytes $idx $B $ffile $K --ascii > $randombytes.calibration 

/home/yue.r/acronym_linear_regression

/work/k.church/githubs/scirepeval
https://huggingface.co/datasets/allenai/scirepeval

cd /scratch/k.church/semantic_scholar/releases/2022-12-02/database/abstracts
ln -s /work/k.church/semantic_scholar/releases/2022-12-02/database/abstracts/*gz .

cd /scratch/k.church/semantic_scholar/releases/2022-12-02/database/abstracts
sbatch -p express -t 59 --array 1-30 /work/k.church/semantic_scholar/abstracts/abstracts_to_text.sh

rel=2022-12-02
cd /work/k.church/semantic_scholar/releases/$rel/database/papers
awk -F'\t' '{print $2 "\t" $1}' titles.piece.??? > corpusId_to_title

cd /scratch/k.church/semantic_scholar/releases/2022-12-02/database/abstracts
for dir in abstracts.piece.???
do
for model in allenai/specter allenai/specter2 malteos/scincl
do
mkdir -p $dir/$model
sbatch -p express -t 59 --array 0-9 /work/k.church/semantic_scholar/extracted_citing_sentences_to_embedding.py -i $dir/abstracts.%03d.with_title -o $dir/$model/%03d --model $model
done
done

cd /scratch/k.church/semantic_scholar/releases/2022-12-02/database/abstracts
mkdir -p todo
for model in allenai/specter allenai/specter2 malteos/scincl
do
for dir in abstracts.piece.???
do
mkdir -p $dir/$model
for f in $dir/abstracts.V2.????.with_title
do
i=`basename $f | cut -f3 -d.`
echo /work/k.church/semantic_scholar/extracted_citing_sentences_to_embedding.py -i $f -o $dir/$model/$i --model $model
done
done
done | 
egrep -v '[?]' |
awk '{print "#!/bin/sh"; print}' |
split -l 40 -d -a 5 - todo/x.

cd /scratch/k.church/semantic_scholar/releases/2022-12-02/database/abstracts
for f in todo/x.11[789]??
do
sbatch -e $f.err -o $f.out -p short -t 1409 $f
done


# stuff to do
cd /scratch/k.church/semantic_scholar/releases/2022-12-02/database/abstracts
mkdir -p todo
for model in allenai/specter2 # michiyasunaga/LinkBERT-large # malteos/scincl # allenai/specter # 
do
for dir in abstracts.piece.002
do
sbatch --array 0-999 -p express -t 59 /work/k.church/semantic_scholar/extracted_citing_sentences_to_embedding.py -i $dir/abstracts.V2.1%3d.with_title -o $dir/$model/1%03d --model $model
done
done

cd /scratch/k.church/semantic_scholar/releases/2022-12-02/database/abstracts
mkdir -p todo
for model in michiyasunaga/LinkBERT-large # malteos/scincl # allenai/specter #  allenai/specter2 #
do
for dir in abstracts.piece.001
do
mkdir -p $dir/$model
sbatch --array 0 -p debug -t 19 /work/k.church/semantic_scholar/extracted_citing_sentences_to_embedding.py -i $dir/abstracts.V2.%04d.with_title -o $dir/$model/%04d --model $model
done
done




for config in fos mesh_descriptors cite_count pub_year cite_prediction cite_prediction_new \
    high_influence_cite same_author search biomimicry drsm feeds_1 feeds_m feeds_title \
    peer_review_score_hIndex trec_covid tweet_mentions scidocs_mag_mesh scidocs_view_cite_read paper_reviewer_matching

cd /work/k.church/githubs/scirepeval/kwc
awk '{printf "gft_summary --data H:allenai/scirepeval,%s > gft_summary_output/%s.txt\n", $1, $1}' configs.txt | sh

curl -X GET "https://datasets-server.huggingface.co/splits?dataset=allenai%2Fscirepeval" | jq
{
  "splits": [
    {
      "dataset": "allenai/scirepeval",
      "config": "biomimicry",
      "split": "evaluation"
    },
    {
      "dataset": "allenai/scirepeval",
      "config": "cite_count",
      "split": "evaluation"
    },
    {
      "dataset": "allenai/scirepeval",
      "config": "cite_count",
      "split": "train"
    },
    {
      "dataset": "allenai/scirepeval",
      "config": "cite_count",
      "split": "validation"
    },
    {
      "dataset": "allenai/scirepeval",
      "config": "cite_prediction",
      "split": "train"
    },
    {
      "dataset": "allenai/scirepeval",
      "config": "cite_prediction",
      "split": "validation"
    },
    {
      "dataset": "allenai/scirepeval",
      "config": "cite_prediction_new",
      "split": "train"
    },
    {
      "dataset": "allenai/scirepeval",
      "config": "cite_prediction_new",
      "split": "validation"
    },
    {
      "dataset": "allenai/scirepeval",
      "config": "drsm",
      "split": "evaluation"
    },
    {
      "dataset": "allenai/scirepeval",
      "config": "feeds_1",
      "split": "evaluation"
    },
    {
      "dataset": "allenai/scirepeval",
      "config": "feeds_m",
      "split": "evaluation"
    },
    {
      "dataset": "allenai/scirepeval",
      "config": "feeds_title",
      "split": "evaluation"
    },
    {
      "dataset": "allenai/scirepeval",
      "config": "fos",
      "split": "evaluation"
    },
    {
      "dataset": "allenai/scirepeval",
      "config": "fos",
      "split": "train"
    },
    {
      "dataset": "allenai/scirepeval",
      "config": "fos",
      "split": "validation"
    },
    {
      "dataset": "allenai/scirepeval",
      "config": "high_influence_cite",
      "split": "evaluation"
    },
    {
      "dataset": "allenai/scirepeval",
      "config": "high_influence_cite",
      "split": "train"
    },
    {
      "dataset": "allenai/scirepeval",
      "config": "high_influence_cite",
      "split": "validation"
    },
    {
      "dataset": "allenai/scirepeval",
      "config": "mesh_descriptors",
      "split": "evaluation"
    },
    {
      "dataset": "allenai/scirepeval",
      "config": "mesh_descriptors",
      "split": "train"
    },
    {
      "dataset": "allenai/scirepeval",
      "config": "mesh_descriptors",
      "split": "validation"
    },
    {
      "dataset": "allenai/scirepeval",
      "config": "paper_reviewer_matching",
      "split": "evaluation"
    },
    {
      "dataset": "allenai/scirepeval",
      "config": "peer_review_score_hIndex",
      "split": "evaluation"
    },
    {
      "dataset": "allenai/scirepeval",
      "config": "pub_year",
      "split": "evaluation"
    },
    {
      "dataset": "allenai/scirepeval",
      "config": "pub_year",
      "split": "train"
    },
    {
      "dataset": "allenai/scirepeval",
      "config": "pub_year",
      "split": "validation"
    },
    {
      "dataset": "allenai/scirepeval",
      "config": "same_author",
      "split": "evaluation"
    },
    {
      "dataset": "allenai/scirepeval",
      "config": "same_author",
      "split": "train"
    },
    {
      "dataset": "allenai/scirepeval",
      "config": "same_author",
      "split": "validation"
    },
    {
      "dataset": "allenai/scirepeval",
      "config": "scidocs_mag_mesh",
      "split": "evaluation"
    },
    {
      "dataset": "allenai/scirepeval",
      "config": "scidocs_view_cite_read",
      "split": "evaluation"
    },
    {
      "dataset": "allenai/scirepeval",
      "config": "search",
      "split": "evaluation"
    },
    {
      "dataset": "allenai/scirepeval",
      "config": "search",
      "split": "train"
    },
    {
      "dataset": "allenai/scirepeval",
      "config": "search",
      "split": "validation"
    },
    {
      "dataset": "allenai/scirepeval",
      "config": "trec_covid",
      "split": "evaluation"
    },
    {
      "dataset": "allenai/scirepeval",
      "config": "tweet_mentions",
      "split": "evaluation"
    }
  ]
}

https://api.semanticscholar.org/api-docs/recommendations
https://api.semanticscholar.org/recommendations/v1/papers/forpaper/a3e4ceb42cbcd2c807d53aff90a8cb1f5ee3f031?fields=title,url,externalIds,citationCount&limit=10

id=9558665
echo $id | /work/k.church/semantic_scholar/citations/graphs/src/fetch_semantic_scholar_recommendations.py > /tmp/x
cut -f1,2 < /tmp/x | /work/k.church/githubs/scidocs/data/recomm/pairs2proposed.txt.sh > /tmp/x.proposed
cut -f1,2 < /tmp/x | /work/k.church/githubs/scidocs/data/recomm/pairs2specter.txt.sh > /tmp/x.specter
id=9558665
/work/k.church/githubs/scidocs/data/recomm/line2both.sh $id > /tmp/x.html

https://api.semanticscholar.org/graph/v1/paper/649def34f8be52c8b66281af98ae884c09aef38b?fields=title,citations.authors
https://api.semanticscholar.org/graph/v1/paper/CorpusID:9558665?fields=title,embedding

id=147470899
echo $id | /work/k.church/semantic_scholar/citations/graphs/src/fetch_semantic_scholar_recommendations.py > /tmp/x
/work/k.church/githubs/scidocs/data/recomm/line2both.sh $id > /tmp/y.html


import matplotlib.pyplot as plt
from matplotlib_venn import venn2

# Use the venn2 function
venn2(subsets = (10, 5, 2), set_labels = ('Group A', 'Group B'))
plt.show()

cd /scratch/k.church/semantic_scholar/releases
awk '/2022-12/ && $2 <= 14 {select[$2]=1; print}; $1 == "#" {next}; {key=""; for(i=0;i<length($2);i++) if(i in select) key = key substr($2, i+1, 1); x[key]+=$1}; END {
printf "%0.1f, ", x["100"]+0
printf "%0.1f, ", x["010"]+0
printf "%0.1f, ", x["110"]+0
printf "%0.1f, ", x["001"]+0
printf "%0.1f, ", x["011"]+0
printf "%0.1f, ", x["101"]+0
printf "%0.1f, ", x["111"]+0
}' 4way/venn/papers.abstracts.embeddings.citations.cited.citing.summary 

# 3 2022-12-02/database/papers/sorted_uniq
# 7 2022-12-02/database/abstracts/sorted_uniq
# 14 2022-12-02/database/citations/sorted_uniq
63.7345 100
33.7424 110
45.6279 101
64.6943 111
8.97767 000
0.220925 001

venn3(subsets = (63.7, 0, 45.6, 0, 33.7, 0, 9.0))

100
010
110
001
011
101
111

plt.clf()
v = venn3(subsets= (63.7, 0.0, 33.7, 0.2, 0.0, 45.6, 64.7), set_labels=('Papers', 'Abstracts', 'Citations'), alpha=0.5)
v.get_label_by_id('100').set_size(16)
v.get_label_by_id('010').set_size(16)
v.get_label_by_id('001').set_size(16)
v.get_label_by_id('110').set_size(16)
v.get_label_by_id('101').set_size(16)
v.get_label_by_id('011').set_size(16)
v.get_label_by_id('111').set_size(16)
v.hide_zeroes()
plt.show()

cd /scratch/k.church/semantic_scholar/releases
awk '/2022-12/ && $2 <= 14 && $2 > 3 {select[$2]=1; print}; $1 == "#" {next}; {key=""; for(i=0;i<length($2);i++) if(i in select) key = key substr($2, i+1, 1); x[key]+=$1}; END {
printf "%0.1f, ", x["10"]+0
printf "%0.1f, ", x["01"]+0
printf "%0.1f, ", x["11"]+0
}' 4way/venn/papers.abstracts.embeddings.citations.cited.citing.summary 


plt.clf()


import matplotlib.pyplot as plt
from matplotlib_venn import venn2

plt.clf()
plt.rcParams.update({'font.size': 24})
v = venn2(subsets= (34, 46, 65), set_labels=('Abstracts', 'Citation Graph'), alpha=0.5)
v.get_label_by_id('10').set_size(20)
v.get_label_by_id('01').set_size(20)
v.get_label_by_id('11').set_size(20)
plt.savefig('abstracts.citation_graph.png', bbox_inches='tight', dpi=1200)


import matplotlib.pyplot as plt
from matplotlib_venn import venn2,venn3

plt.clf()
plt.rcParams.update({'font.size': 24})
v = venn2(subsets= (34, 46, 65), set_labels=('Abstracts', 'Citation Graph'), alpha=0.5)
v.get_label_by_id('10').set_text('34M\n(16%)')
v.get_label_by_id('01').set_text('46M\n(22%)')
v.get_label_by_id('11').set_text('65M\n(31%)')
plt.savefig('abstracts.citation_graph.png', bbox_inches='tight', dpi=1200)

import matplotlib.pyplot as plt
from matplotlib_venn import venn2,venn3

plt.clf()
plt.rcParams.update({'font.size': 24})
v = venn2(subsets= (34, 46, 65), set_labels=('Abstracts', 'Citation Graph'), alpha=0.5)
v.get_label_by_id('10').set_text('34M\n(16%)')
v.get_label_by_id('01').set_text('46M\n(22%)')
v.get_label_by_id('11').set_text('65M\n(31%)')
plt.savefig('abstracts.citation_graph.400.png', bbox_inches='tight', dpi=400)





v = venn2(subsets= (34, 46, 65), set_labels=('Abstracts', 'Citaton Graph'), alpha=0.5)
v.get_label_by_id('10').set_size(20)
v.get_label_by_id('01').set_size(20)
v.get_label_by_id('11').set_size(20)
plt.show()

awk '{
select[1] = "M"
select[2] = "D"
select[3] = "P"
select[4] = "D"
select[5] = "P"
# select[6] = "A"
# select[7] = "A"
}
$1 == "#" {next}; 
{delete(killroy); 
key=""; for(i=0;i<length($2);i++) if(i in select && (substr($2, i+1, 1) == "1")) killroy[select[i]]++; 
for(i in killroy) key = key i;
x[key]+=$1};
END {for(i in x) print x[i], i}' /work/k.church/semantic_scholar/releases/4way/venn/external_ids.more.2022-12-02.summary | sort -nr


awk '{
select[1] = "MAG "
select[2] = "DOI "
select[3] = "misc "
select[4] = "misc "
select[5] = "misc "
select[6] = "misc "
select[7] = "misc "
}
$1 == "#" {next}; 
{delete(killroy); 
key=""; for(i=0;i<length($2);i++) if(i in select && (substr($2, i+1, 1) == "1")) killroy[select[i]]++; 
for(i in killroy) key = key i;
x[key]+=$1};
END {for(i in x) print x[i], i}' /work/k.church/semantic_scholar/releases/4way/venn/external_ids.more.2022-12-02.summary | sort -nr


awk '{
select[1] = "MAG "
select[2] = "DOI "
select[3] = "misc "
select[4] = "misc "
select[5] = "misc "
select[6] = "misc "
select[7] = "misc "
}
$1 == "#" {next}; 
{delete(killroy); 
key=""; for(i=0;i<length($2);i++) if(i in select && (substr($2, i+1, 1) == "1")) killroy[select[i]]++; 
for(i in killroy) key = key i;
x[key]+=$1};
END {

printf "%0.1f, ", x["MAG "]+0
printf "%0.1f, ", x["DOI "]+0
printf "%0.1f, ", x["MAG DOI "]+0
printf "%0.1f, ", x["misc "]+0
printf "%0.1f, ", x["misc DOI "]+0
printf "%0.1f, ", x["MAG misc "]+0
printf "%0.1f, ", x["MAG misc DOI "]+0
}' /work/k.church/semantic_scholar/releases/4way/venn/external_ids.more.2022-12-02.summary | sort -nr


plt.clf()
v = venn3(subsets= (79.2, 12.0, 68.4, 3.4, 4.6, 6.1, 28.6), set_labels=('MAG', 'DOI', 'misc'), alpha=0.5)
v.get_label_by_id('100').set_size(16)
v.get_label_by_id('010').set_size(16)
v.get_label_by_id('001').set_size(16)
v.get_label_by_id('110').set_size(16)
v.get_label_by_id('101').set_size(16)
v.get_label_by_id('011').set_size(16)
v.get_label_by_id('111').set_size(16)
v.hide_zeroes()
plt.show()

indir=/work/k.church/semantic_scholar/releases/2022-12-02/database/papers
outdir=/scratch/k.church/semantic_scholar/releases/2022-12-02/database/papers/fos/
mkdir -p $outdir
sbatch -p debug -t 19 --array 1-30 /work/k.church/semantic_scholar/papers2fos.py $indir/papers.piece.%03d.gz $outdir/fos.piece.%03d

sbatch -p debug -t 19 --array 1-30 /scratch/k.church/semantic_scholar/releases/2022-12-02/database/papers/fos/job.sh $outdir/fos.piece.%03d $outdir/fos.piece.%03d.ids

cd /scratch/k.church/semantic_scholar/releases/2022-12-02/database/papers/fos/
mkdir -p merged/external
for fos in `ls */external/*.s | awk -F/ '{print $NF}' | cut -f1 -d . | sort -u`
do
echo working on $fos
LC_ALL=C sort -m -T . -S '90%' */external/$fos.s > merged/external/$fos
done

indir=/work/k.church/semantic_scholar/releases/2022-12-02/database/papers
outdir=/scratch/k.church/semantic_scholar/releases/2022-12-02/database/papers/openaccess
mkdir -p $outdir
sbatch -p debug -t 19 --array 1-30 /work/k.church/semantic_scholar/openaccess.py $indir/papers.piece.%03d.gz $outdir/openaccess.piece.%03d

cat /scratch/k.church/semantic_scholar/releases/2022-12-02/database/papers/openaccess/big_venn.summary |
awk '$1 == "#" { header[1+$2] = $NF; next};
     {for(i in header) if(substr($NF, i, 1) == "1") { n[i] += $1; if(substr($NF, length($NF), 1) == "1") x[i] += $1}}
     END {for(i in n) print x[i]/n[i], n[i], header[i]}' OFS="\t" |
sort -k2 -nr > /scratch/k.church/semantic_scholar/releases/2022-12-02/database/papers/openaccess/big_venn.summary.openaccess

cat /scratch/k.church/semantic_scholar/releases/2022-12-02/database/papers/openaccess/big_venn.summary |
awk '$1 == "#" { header[1+$2] = $NF; next};
     {for(i in header) if(substr($NF, i, 1) == "1") { n[i] += $1; if(substr($NF, 1, 1) == "1") x[i] += $1}}
     END {for(i in n) print x[i]/n[i], n[i], header[i]}' OFS="\t" |
sort -k2 -nr > /scratch/k.church/semantic_scholar/releases/2022-12-02/database/papers/openaccess/big_venn.summary.abstract

cat /scratch/k.church/semantic_scholar/releases/2022-12-02/database/papers/openaccess/big_venn.summary |
awk '$1 == "#" { header[1+$2] = $NF; next};
     {for(i in header) if(substr($NF, i, 1) == "1") { n[i] += $1; if(substr($NF, 2, 1) == "1" || substr($2, 3, 1) == "1") x[i] += $1}}
     END {for(i in n) print x[i]/n[i], n[i], header[i]}' OFS="\t" |
sort -k2 -nr > /scratch/k.church/semantic_scholar/releases/2022-12-02/database/papers/openaccess/big_venn.summary.cite

paste big_venn.summary.abstract big_venn.summary.cite big_venn.summary.openaccess |
awk '{printf "%0.2f %0.2f %0.2f %0.1f %s\n", $1, $4, $7, $8/1e6, $9}'

Pr(abstract)	Pr(cite)	Pr(openaccess)	N		case
0.47	0.53	0.12	207.8	in Semantic Scholar
0.47	0.53	0.13	182.2	in MAG
0.76	0.67	0.16	129.9	has Specter Embedding
0.48	0.72	0.23	113.5	has DOI
1.00	0.66	0.18	98.4	has abstract
0.56	1.00	0.21	87.4	has citations
0.63	1.00	0.22	82.0	has references
0.54	0.81	0.25	35.0	in PubMed
0.69	0.81	1.00	25.9	open access
0.66	0.93	0.22	6.1	in DBLP
0.68	0.93	0.53	4.9	in PubMedCentral
0.81	0.98	0.53	2.1	in arXiv
0.96	0.96	0.38	0.1	in ACL Anthology


cd /scratch/k.church/semantic_scholar/releases/2022-12-02/database/abstracts
venn  /work/k.church/semantic_scholar/releases/2022-12-02/database/abstracts/sorted_uniq /work/k.church/semantic_scholar/releases/2022-12-02/database/cite/sorted_uniq /scratch/k.church/semantic_scholar/releases/2022-12-02/database/papers/openaccess/sorted_uniq > /work/k.church/semantic_scholar/releases/2022-12-02/database/cite/venn

venn /work/k.church/semantic_scholar/releases/2022-12-02/database/embeddings/sorted_uniq /work/k.church/semantic_scholar/releases/2022-12-02/database/cite/sorted_uniq /scratch/k.church/semantic_scholar/releases/2022-12-02/database/papers/openaccess/sorted_uniq > /work/k.church/semantic_scholar/releases/2022-12-02/database/cite/venn

awk '{x[$2]=$1}; END {
printf "%0.1f, ", x["100"]+0
printf "%0.1f, ", x["010"]+0
printf "%0.1f, ", x["110"]+0
printf "%0.1f, ", x["001"]+0
printf "%0.1f, ", x["011"]+0
printf "%0.1f, ", x["101"]+0
printf "%0.1f, ", x["111"]+0
}' /tmp/y
> > > > > > > > 41.0, 22.0, 67.6, 2.8, 1.7, 2.1, 19.3, (gft) [k.church@c3085 abstracts]$ 

plt.clf()
v = venn3(subsets= (41.0, 22.0, 67.6, 2.8, 1.7, 2.1, 19.3), set_labels=('Specter', 'Cite', ' Open Access'), alpha=0.5)
v.get_label_by_id('100').set_size(16)
v.get_label_by_id('010').set_size(16)
v.get_label_by_id('001').set_size(16)
v.get_label_by_id('110').set_size(16)
v.get_label_by_id('101').set_size(16)
v.get_label_by_id('011').set_size(16)
v.get_label_by_id('111').set_size(16)
v.hide_zeroes()
plt.show()

awk '{x[substr($2,1,2)]+=$1}; END {
printf "%0.1f, ", x["10"]+0
printf "%0.1f, ", x["01"]+0
printf "%0.1f, ", x["11"]+0
}' /tmp/y


plt.clf()
v = venn2(subsets= (43.1, 23.7, 86.8), set_labels=('Specter', 'Cite'), alpha=0.5)
v.get_label_by_id('10').set_size(16)
v.get_label_by_id('01').set_size(16)
v.get_label_by_id('11').set_size(16)
plt.show()

cd /work/k.church/sun.jiam/scirepeval_data/test/scidocs/view_cite_read/cite
awk 'BEGIN {while(getline < "ids2.out" > 0) id[$1]=$2}; $1 in id && $3 in id {print id[$1], id[$3], $NF, $0}' test.qrel > test.qrel.sanity

cd /work/k.church/semantic_scholar/citations/graphs
egrep -v NA /work/k.church/sun.jiam/scirepeval_data/test/scidocs/view_cite_read/cite/test.qrel.corpusId | 
$src/C/BFS.sh citations.G -max_depth 1 > /work/k.church/sun.jiam/scirepeval_data/test/scidocs/view_cite_read/cite/test.qrel.corpusId.BFS
awk '{x=$1; if(x != 1) x=0; print x, $NF}' /work/k.church/sun.jiam/scirepeval_data/test/scidocs/view_cite_read/cite/test.qrel.corpusId.BFS | sort | uniq -c | sort -nr

awk 'BEGIN {printf "query\tcandidate\torig.y\tcorrected.y\n"}; {printf "%d\t%d\t%d\t%d\n", $3, $4, $5, ($1 == 1)}' /work/k.church/sun.jiam/scirepeval_data/test/scidocs/view_cite_read/cite/test.qrel.corpusId.BFS > /work/k.church/sun.jiam/scirepeval_data/test/scidocs/view_cite_read/cite/test.qrel.corpusId.corrected


mkdir -p /work/k.church/sun.jiam/scirepeval_data/test/scidocs/view_cite_read/cite/new_test_set/to_do
cut -f1 /work/k.church/sun.jiam/scirepeval_data/test/scidocs/view_cite_read/cite/test.qrel.corpusId | 
uniq | 
awk '{print "/work/k.church/semantic_scholar/citations/graphs/src/C/BFS5.sh", $1}' |
egrep -v NA |
awk '{print "#!/bin/sh"; print}' |
split -d -a 3 -l 100 - /work/k.church/sun.jiam/scirepeval_data/test/scidocs/view_cite_read/cite/new_test_set/to_do/x.

for f in /work/k.church/sun.jiam/scirepeval_data/test/scidocs/view_cite_read/cite/new_test_set/to_do/x.???
do
sbatch -e $f.err -o $f.done -p debug -t 19 $f
done

cat /work/k.church/sun.jiam/scirepeval_data/test/scidocs/view_cite_read/cite/new_test_set/to_do/x.???.done | awk '{print $2, $3, $1}' OFS="\t" > /work/k.church/sun.jiam/scirepeval_data/test/scidocs/view_cite_read/cite/new_test_set/random_walk

awk '{print $1; print $2}' /work/k.church/sun.jiam/scirepeval_data/test/scidocs/view_cite_read/cite/new_test_set/random_walk | 
sort  -u > /work/k.church/sun.jiam/scirepeval_data/test/scidocs/view_cite_read/cite/new_test_set/random_walk.ids

ids=/work/k.church/sun.jiam/scirepeval_data/test/scidocs/view_cite_read/cite/new_test_set/random_walk.ids

dir=/work/k.church/semantic_scholar/releases/2022-12-02/database/embeddings
K=768
ffile=$dir/specter.kwc.edges.f
map=$dir/specter.kwc.nodes.txt
id_to_floats --map $map --floats $ffile --record_size $K --binary_output < $ids > $ids.specter.f

ids=/work/k.church/sun.jiam/scirepeval_data/test/scidocs/view_cite_read/cite/new_test_set/random_walk.ids
cd /work/k.church/sun.jiam/scirepeval_data/test/scidocs/view_cite_read/cite
K=280
dir=/work/k.church/semantic_scholar/citations/graphs/K$K/papers/
ffile=$dir/citations.npz.papers.kwc.edges.f
map=$dir/map
id_to_floats --map $map --floats $ffile --record_size $K --binary_output < $ids > $ids.proposed.f

pairs=/work/k.church/sun.jiam/scirepeval_data/test/scidocs/view_cite_read/cite/new_test_set/random_walk
dir=/work/k.church/semantic_scholar/releases/2022-12-02/database/embeddings
K=768
ffile=$dir/specter.kwc.edges.f
map=$dir/specter.kwc.nodes.txt
export K
export ffile
export map
cut -f1,2 < $pairs | egrep -v NA | pairs_to_cos --map $map --floats $ffile --record_size $K > $pairs.specter

pairs=/work/k.church/sun.jiam/scirepeval_data/test/scidocs/view_cite_read/cite/new_test_set/random_walk
K=280
dir=/work/k.church/semantic_scholar/citations/graphs/K$K/papers/
ffile=$dir/citations.npz.papers.kwc.edges.f
map=$dir/map
export K
export ffile
export map
cut -f1,2 < $pairs | egrep -v NA | pairs_to_cos --map $map --floats $ffile --record_size $K > $pairs.proposed

cd /work/k.church/sun.jiam/scirepeval_data/test/scidocs/view_cite_read/cite/new_test_set/
$src/array_fromfile.py -i random_walk.ids.proposed.f -K 280 -d float32 -o random_walk.ids.proposed.npy
$src/array_fromfile.py -i random_walk.ids.specter.f -K 768 -d float32 -o random_walk.ids.specter.npy

cd /work/k.church/sun.jiam/scirepeval_data/test/scidocs/view_cite_read/cite/new_test_set/
sbatch -p short -t 1409 -i random_walk -o random_walk.BFS -e random_walk.BFS.err $src/C/BFS.sh $citedir/citations.G.symmetric -max_depth 5

dir=/work/k.church/semantic_scholar/citations/graphs
$src/graph_tofile.py -G $dir/citations.G.npz -o $dir/citations.G.symmetric --make_symmetric

citedir=/work/k.church/semantic_scholar/citations/graphs/
dir=/work/k.church/semantic_scholar/citations/graphs
$src/graph_tofile.py -G $dir/citations.G.transpose.npz -o $dir/citations.G.transpose

citedir=/work/k.church/semantic_scholar/citations/graphs/
cut -f2- /tmp/both.52967399.21302.2 | $src/C/cocitations $citedir/citations.G.transpose.[XY].i  

citedir=/work/k.church/semantic_scholar/citations/graphs/
cut -f2- /tmp/both.52967399.21302.2 | $src/C/BFS $citedir/citations.G -max_depth 5


sbatch -p debug --mem=30G $HOME/to_go/foo.sh

citedir=/work/k.church/semantic_scholar/citations/graphs/
cd /work/k.church/sun.jiam/scirepeval_data/test/scidocs/view_cite_read/cite/new_test_set/to_do2
for f in x.???
do
sbatch -p express -t 59 -i $f -o $f.BFS -e $f.err $src/C/BFS.sh $citedir/citations.G.symmetric -max_depth 6
done

pairs=/work/k.church/sun.jiam/scirepeval_data/test/scidocs/view_cite_read/cite/new_test_set/random_walk
K=280
dir=/work/k.church/semantic_scholar/citations/graphs/K$K/papers/
ffile=$dir/citations.npz.papers.kwc.edges.f
map=$dir/map
echo 9558665 | $src/fetch_references_and_citations.py > /tmp/PMI
awk '/ERROR/ {next}; $1 == "reference" {print $2; print $3}' /tmp/PMI  | awk 'killroy[$1]++ < 1' |
id_to_floats --floats $ffile --record_size $K --map $map > /tmp/PMI.floats


echo 9558665 | $src/fetch_references_and_citations.py > /tmp/PMI
(gft) [k.church@c3085 abstracts]$ awk '$1 == "reference" /tmp/PMI | egrep -v ERROR
>   C-c C-c
(gft) [k.church@c3085 abstracts]$ awk '$1 == "reference"' /tmp/PMI | egrep -v ERROR
reference	9558665	18476529	167	Book Reviews: Looking Up: An Account of the COBUILD PROJECT IN LEXICAL COMPUTING
reference	9558665	5923203	130	Parsing, Word Associations and Typical Predicate-Argument Relations
reference	9558665	61409897	32	Microcoding the Lexicon with Co-occurrence Knowledge
reference	9558665	3166885	1077	A Stochastic Parts Program and Noun Phrase Parser for Unrestricted Text
reference	9558665	62735070	349	A Stochastic Parts Program and Noun Phrase Parser for Unrestricted Text
reference	9558665	17307726	394	On the Recognition of Printed Characters of Any Font and Size
reference	9558665	60997669	486	Collins COBUILD English Language Dictionary
reference	9558665	60002365	294	Looking up : an account of the COBUILD Project in lexical computing and the development of the Collins COBUILD English Language Dictionary
reference	9558665	143660780	1271	Frequency Analysis of English Usage: Lexicon and Grammar. By W. Nelson Francis and Henry Kučera with the assistance of Andrew W. Mackie. Boston: Houghton Mifflin. 1982. x + 561
reference	9558665	5222302	195	Deterministic Parsing of Syntactic Non-fluencies
reference	9558665	208978352	164	Parsing
reference	9558665	2567466	70	The nature of evidence.
reference	9558665	146932957	526	Word association norms
reference	9558665	112676098	261	Transmission of Information.
reference	9558665	67110416	993	Transmission of Information: A Statistical Theory of Communications.
reference	9558665	208093066	2043	A Synopsis of Linguistic Theory, 1930-1955
reference	9558665	11945361	3400	THE POPULATION FREQUENCIES OF SPECIES AND THE ESTIMATION OF POPULATION PARAMETERS
(gft) [k.church@c3085 abstracts]$ awk '/ERROR/ {next}; $1 == "reference" {print $2; print $3}' /tmp/PMI | sort -n -u
2567466
3166885
5222302
5923203
9558665
11945361
17307726
18476529
60002365
60997669
61409897
62735070
67110416
112676098
143660780
146932957
208093066
208978352
(gft) [k.church@c3085 abstracts]$ awk '/ERROR/ {next}; $1 == "reference" {print $2; print $3}' /tmp/PMI  | awk 'killroy[$1]++ < 1' 
9558665
18476529
5923203
61409897
3166885
62735070
17307726
60997669
60002365
143660780
5222302
208978352
2567466
146932957
112676098
67110416
208093066
11945361
(gft) [k.church@c3085 abstracts]$ which cos_to_pairs
/usr/bin/which: no cos_to_pairs in (/shared/centos7/jq/1.6/bin:/home/k.church/venv/gft/bin:/shared/centos7/python/3.8.1/bin:/usr/lib64/qt-3.3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/opt/dell/srvadmin/sbin:/home/k.church/public_github/gft/gft:/work/k.church/githubs/zstd/programs:/home/k.church/final/suffix:/work/k.church/semantic_scholar/citations/graphs/src:/work/k.church/semantic_scholar/citations/graphs/src/C:/work/k.church/semantic_scholar/jobs:/work/k.church/sqlite/bld)
(gft) [k.church@c3085 abstracts]$ which pairs_to_cos
/work/k.church/semantic_scholar/citations/graphs/src/C/pairs_to_cos
(gft) [k.church@c3085 abstracts]$ head /work/k.church/sun.jiam/scirepeval_data/test/scidocs/view_cite_read/cite/test.qrel.corpusId.corrected
query	candidate	orig.y	corrected.y
14253651	2469510	1	1
14253651	32072941	1	1
14253651	26282848	1	0
14253651	16708577	1	0
14253651	8766892	1	0
14253651	10107843	0	0
14253651	14174498	0	0
14253651	181655	0	0
14253651	10533922	0	0
(gft) [k.church@c3085 abstracts]$ cut -f3- /work/k.church/sun.jiam/scirepeval_data/test/scidocs/view_cite_read/cite/test.qrel.corpusId.corrected | sort | uniq -c | sort -nr
  24863 0	0
   3121 1	1
   1755 1	0
      2 0	1
      1 orig.y	corrected.y
(gft) [k.church@c3085 abstracts]$ echo 2000 | awk '{print $1/30000}'
0.0666667
(gft) [k.church@c3085 abstracts]$ wc /work/k.church/sun.jiam/scirepeval_data/test/scidocs/view_cite_read/cite/test.qrel.corpusId.corrected
 29742 118968 630651 /work/k.church/sun.jiam/scirepeval_data/test/scidocs/view_cite_read/cite/test.qrel.corpusId.corrected
(gft) [k.church@c3085 abstracts]$ pairs=/work/k.church/sun.jiam/scirepeval_data/test/scidocs/view_cite_read/cite/new_test_set/random_walk
K=280
dir=/work/k.church/semantic_scholar/citations/graphs/K$K/papers/
ffile=$dir/citations.npz.papers.kwc.edges.f
map=$dir/map
echo 9558665 | $src/fetch_references_and_citations.py > /tmp/PMI
awk '/ERROR/ {next}; $1 == "reference" {print $2; print $3}' /tmp/PMI  | awk 'killroy[$1]++ < 1' |
id_to_floats --record_size $K --map $map > /tmp/PMI.floats

(gft) [k.church@c3085 abstracts]$ (gft) [k.church@c3085 abstracts]$ (gft) [k.church@c3085 abstracts]$ (gft) [k.church@c3085 abstracts]$ (gft) [k.church@c3085 abstracts]$ 
(gft) [k.church@c3085 abstracts]$ > fatal: bad input: --floats
(gft) [k.church@c3085 abstracts]$ (gft) [k.church@c3085 abstracts]$ (gft) [k.church@c3085 abstracts]$ pairs=/work/k.church/sun.jiam/scirepeval_data/test/scidocs/view_cite_read/cite/new_test_set/random_walk
K=280
dir=/work/k.church/semantic_scholar/citations/graphs/K$K/papers/
ffile=$dir/citations.npz.papers.kwc.edges.f
map=$dir/map
echo 9558665 | $src/fetch_references_and_citations.py > /tmp/PMI
awk '/ERROR/ {next}; $1 == "reference" {print $2; print $3}' /tmp/PMI  | awk 'killroy[$1]++ < 1' |
id_to_floats --floats $ffile --record_size $K --map $map > /tmp/PMI.floats


(gft) [k.church@c3085 abstracts]$ (gft) [k.church@c3085 abstracts]$ (gft) [k.church@c3085 abstracts]$ (gft) [k.church@c3085 abstracts]$ (gft) [k.church@c3085 abstracts]$ 
(gft) [k.church@c3085 abstracts]$ > (gft) [k.church@c3085 abstracts]$ (gft) [k.church@c3085 abstracts]$ (gft) [k.church@c3085 abstracts]$ (gft) [k.church@c3085 abstracts]$ 
(gft) [k.church@c3085 abstracts]$ head /tmp/PMI.floats | cut -c1-150
9558665 7375270 -0.005832 -0.009479 0.096270 -0.008587 0.053805 -0.027838 0.008056 0.163126 -0.019716 0.002598 -0.056923 -0.159065 -0.015683 -0.328421
18476529 13590301 -0.005343 -0.013477 0.148791 0.077103 0.036645 -0.040040 -0.037058 0.035487 -0.005327 0.076330 -0.065475 -0.206235 0.057193 -0.23896
5923203 4616579 -0.007358 -0.009744 0.105219 -0.020875 0.066796 -0.034207 0.009853 0.186826 -0.019956 0.013077 -0.069972 -0.156230 -0.033545 -0.340386
61409897 37464445 -0.026157 -0.010471 0.127623 0.020820 0.058844 -0.039798 -0.011295 0.127667 -0.015199 0.040211 -0.068864 -0.184988 -0.000279 -0.3181
3166885 2590733 -0.007539 -0.008309 0.101237 -0.038806 0.077734 -0.036057 0.012675 0.223664 -0.021502 -0.001672 -0.068078 -0.150469 -0.045438 -0.35977
62735070 37999954 -0.005953 -0.008373 0.103422 -0.032262 0.075427 -0.035600 0.009858 0.214929 -0.020406 0.004812 -0.069797 -0.155476 -0.043005 -0.3604
17307726 12866000 -0.023546 -0.012902 0.153582 -0.193843 0.193910 -0.117938 -0.017247 0.295962 -0.018059 -0.051936 -0.039596 -0.171239 0.089396 -0.288
60997669 37336668 -0.007343 -0.017228 0.211819 0.137531 0.042260 -0.069731 -0.065154 -0.000190 0.000430 0.091965 -0.065827 -0.222690 0.075050 -0.21471
60002365 37022768 -0.006946 -0.012952 0.146904 0.092054 0.029375 -0.043431 -0.042906 0.004605 -0.000612 0.082824 -0.056114 -0.202092 0.067349 -0.19586
143660780 74092823 -0.004640 -0.004258 0.090431 0.030644 0.031021 -0.001708 -0.012431 0.067101 -0.006486 0.094485 -0.067785 -0.103569 0.026994 -0.1117
(gft) [k.church@c3085 abstracts]$ awk '{print NF}' /tmp/PMI.floats | uniq -c
     18 282
(gft) [k.church@c3085 abstracts]$ python
Python 3.8.1 (default, Dec 19 2019, 15:08:16) 
[GCC 4.8.5 20150623 (Red Hat 4.8.5-28)] on linux
Type "help", "copyright", "credits" or "license" for more information.
>>> import numpy as np
>>> x = np.loadtxt("/tmp/PMI.floats")
>>> from sklearn.metrics.pairwise import cosine_similarity
>>> scores = cosine_similarity(x[:,2:])
>>> scores.shape
(18, 18)
>>> scores1 = scores.reshape(-1)
>>> scores1.shape
(324,)
>>> np.min(scores1)
0.1007180071350608
>>> np.max(scores1)
1.0000000000000007
>>> np.set_printoptions(precision=2,linewidth=200)
>>> scores
array([[1.  , 0.71, 0.94, 0.9 , 0.88, 0.88, 0.24, 0.59, 0.6 , 0.35, 0.66, 0.59, 0.55, 0.8 , 0.37, 0.5 , 0.93, 0.28],
       [0.71, 1.  , 0.79, 0.89, 0.72, 0.76, 0.21, 0.97, 0.98, 0.64, 0.89, 0.74, 0.92, 0.72, 0.25, 0.34, 0.77, 0.21],
       [0.94, 0.79, 1.  , 0.96, 0.97, 0.97, 0.33, 0.68, 0.68, 0.44, 0.82, 0.76, 0.61, 0.81, 0.42, 0.53, 0.84, 0.35],
       [0.9 , 0.89, 0.96, 1.  , 0.89, 0.91, 0.27, 0.81, 0.81, 0.57, 0.88, 0.79, 0.72, 0.83, 0.36, 0.46, 0.85, 0.29],
       [0.88, 0.72, 0.97, 0.89, 1.  , 1.  , 0.4 , 0.6 , 0.61, 0.36, 0.81, 0.77, 0.54, 0.72, 0.4 , 0.52, 0.76, 0.37],
       [0.88, 0.76, 0.97, 0.91, 1.  , 1.  , 0.39, 0.64, 0.65, 0.41, 0.84, 0.79, 0.57, 0.74, 0.4 , 0.52, 0.78, 0.36],
       [0.24, 0.21, 0.33, 0.27, 0.4 , 0.39, 1.  , 0.17, 0.17, 0.12, 0.31, 0.34, 0.14, 0.23, 0.25, 0.3 , 0.16, 0.16],
       [0.59, 0.97, 0.68, 0.81, 0.6 , 0.64, 0.17, 1.  , 0.99, 0.61, 0.84, 0.69, 0.95, 0.61, 0.19, 0.26, 0.66, 0.17],
       [0.6 , 0.98, 0.68, 0.81, 0.61, 0.65, 0.17, 0.99, 1.  , 0.64, 0.86, 0.69, 0.94, 0.63, 0.19, 0.27, 0.68, 0.17],
       [0.35, 0.64, 0.44, 0.57, 0.36, 0.41, 0.12, 0.61, 0.64, 1.  , 0.69, 0.54, 0.48, 0.72, 0.17, 0.19, 0.4 , 0.1 ],
       [0.66, 0.89, 0.82, 0.88, 0.81, 0.84, 0.31, 0.84, 0.86, 0.69, 1.  , 0.84, 0.73, 0.7 , 0.3 , 0.38, 0.65, 0.27],
       [0.59, 0.74, 0.76, 0.79, 0.77, 0.79, 0.34, 0.69, 0.69, 0.54, 0.84, 1.  , 0.58, 0.61, 0.33, 0.38, 0.55, 0.3 ],
       [0.55, 0.92, 0.61, 0.72, 0.54, 0.57, 0.14, 0.95, 0.94, 0.48, 0.73, 0.58, 1.  , 0.52, 0.16, 0.24, 0.64, 0.15],
       [0.8 , 0.72, 0.81, 0.83, 0.72, 0.74, 0.23, 0.61, 0.63, 0.72, 0.7 , 0.61, 0.52, 1.  , 0.36, 0.43, 0.75, 0.23],
       [0.37, 0.25, 0.42, 0.36, 0.4 , 0.4 , 0.25, 0.19, 0.19, 0.17, 0.3 , 0.33, 0.16, 0.36, 1.  , 0.96, 0.26, 0.33],
       [0.5 , 0.34, 0.53, 0.46, 0.52, 0.52, 0.3 , 0.26, 0.27, 0.19, 0.38, 0.38, 0.24, 0.43, 0.96, 1.  , 0.39, 0.35],
       [0.93, 0.77, 0.84, 0.85, 0.76, 0.78, 0.16, 0.66, 0.68, 0.4 , 0.65, 0.55, 0.64, 0.75, 0.26, 0.39, 1.  , 0.22],
       [0.28, 0.21, 0.35, 0.29, 0.37, 0.36, 0.16, 0.17, 0.17, 0.1 , 0.27, 0.3 , 0.15, 0.23, 0.33, 0.35, 0.22, 1.  ]])
>>> from sklearn.cluster import AgglomerativeClustering
>>> model = AgglomerativeClustering(distance_threshold=0, n_clusters=None)
>>> model = model.fit(scores)
>>> quit()
(gft) [k.church@c3085 abstracts]$ find_lines --input /work/k.church/semantic_scholar/papers/papers2url.V2/lines2html.V2 < /tmp/new_ids.txt 
112676098	<a href="https://www.semanticscholar.org/paper/cfd7ad5e1f48d938c65f807c795e91457210212f">264: Transmission of Information.</a>
208978352	<a href="https://www.semanticscholar.org/paper/02bf83a75fc28d591392b7d0af831acb44393387">159: Parsing</a>
67110416	<a href="https://www.semanticscholar.org/paper/6b6dc1167d8f6f44f230309e9c27e4268578f1f7">940: Transmission of Information: A Statistical Theory of Communications.</a>
5222302	<a href="https://www.semanticscholar.org/paper/d886320cc431c57f12027584fdce3434bf4caf9e">193: Deterministic Parsing of Syntactic Non-fluencies</a>
208093066	<a href="https://www.semanticscholar.org/paper/88b3959b6f5333e5358eac43970a5fa29b54642c">1908: A Synopsis of Linguistic Theory, 1930-1955</a>
60997669	<a href="https://www.semanticscholar.org/paper/e3a81d9b7464c0097e391c31fb4d84467533091c">479: Collins COBUILD English Language Dictionary</a>
2567466	<a href="https://www.semanticscholar.org/paper/90535569dcd67a0851e35dc102ba18095c3af180">63: The nature of evidence.</a>
11945361	<a href="https://www.semanticscholar.org/paper/b2986b25f50babd536dd0ecf2237d9eabf5843c2">3258: THE POPULATION FREQUENCIES OF SPECIES AND THE ESTIMATION OF POPULATION PARAMETERS</a>
60002365	<a href="https://www.semanticscholar.org/paper/cd3ec563d8f0fd495873a1b275fa184331b5da36">296: Looking up : an account of the COBUILD Project in lexical computing and the development of the Collins COBUILD English Language Dictionary</a>
143660780	<a href="https://www.semanticscholar.org/paper/73ca80f87a5509a0f4cc62471b8f088f66facd0b">1273: Frequency Analysis of English Usage: Lexicon and Grammar. By W. Nelson Francis and Henry Kučera with the assistance of Andrew W. Mackie. Boston: Houghton Mifflin. 1982. x + 561</a>
146932957	<a href="https://www.semanticscholar.org/paper/91556a9ef3b88c8a3252a9d39630734047ee0c43">512: Word association norms</a>
61409897	<a href="https://www.semanticscholar.org/paper/c8306568c3d5f0c2257a1a735cbf10783d764731">36: Microcoding the Lexicon with Co-occurrence Knowledge</a>
62735070	<a href="https://www.semanticscholar.org/paper/2638cf0a161f4510f1c16d8a76b4361faf5b4718">368: A Stochastic Parts Program and Noun Phrase Parser for Unrestricted Text</a>
3166885	<a href="https://www.semanticscholar.org/paper/a7e084fe51a40eeaaf79bf0b78e837d5bc4a8e10">1059: A stochastic parts program and noun phrase parser for unrestricted text</a>
17307726	<a href="https://www.semanticscholar.org/paper/0de98bd19065702bb44b8aa5ad43ef6923fdacb0">393: On the Recognition of Printed Characters of Any Font and Size</a>
18476529	<a href="https://www.semanticscholar.org/paper/7ed08f16949c4cac198c87263318df655b7b250a">139: Book Reviews: Looking Up: An Account of the COBUILD PROJECT IN LEXICAL COMPUTING</a>
5923203	<a href="https://www.semanticscholar.org/paper/ff97fa9912e4e0517208bf45ef0f646f995e09d2">130: Parsing, Word Associations and Typical Predicate-Argument Relations</a>
9558665	<a href="https://www.semanticscholar.org/paper/9e2caa39ac534744a180972a30a320ad0ae41ea3">4362: Word Association Norms, Mutual Information and Lexicography</a>
(gft) [k.church@c3085 abstracts]$ 

python
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np
x = np.loadtxt("PMI.floats")
np.set_printoptions(precision=2,linewidth=200)
scores = cosine_similarity(x[:,2:])
model = AgglomerativeClustering(distance_threshold=0, n_clusters=None)
model = model.fit(scores)
from matplotlib import pyplot as plt
from scipy.cluster.hierarchy import dendrogram
from sklearn.datasets import load_iris
from sklearn.cluster import AgglomerativeClustering
from matplotlib import pyplot as plt
model.distances_
s = model.labels_
scores[s,:][:,s]
x[:,0].astype(int)
np.savetxt("new_ids.txt", x[:,0].astype(int)[s], fmt="%d")

$src/cluster_paper.sh 3662564
scores: 0.8516 +- 0.228, shape: (22, 22)
quantiles: [0.1  0.21 0.64 0.74 0.8  0.84 0.86 0.91 0.93 0.94 0.95 0.96 0.97 0.98 0.99 0.99 1.   1.   1.   1.  ]
[[1.   0.92 0.94 1.   1.   0.61 0.88 0.97 0.84 1.   0.13 0.94 0.91 0.93 0.93 0.91 0.99 0.99 0.96 1.   1.   0.78]
 [0.92 1.   1.   0.93 0.93 0.74 0.99 0.98 0.78 0.9  0.22 1.   0.99 1.   1.   0.99 0.95 0.95 0.99 0.93 0.94 0.86]
 [0.94 1.   1.   0.95 0.95 0.73 0.99 0.99 0.79 0.92 0.26 1.   0.99 1.   0.99 0.99 0.96 0.96 0.98 0.95 0.95 0.86]
 [1.   0.93 0.95 1.   1.   0.64 0.9  0.98 0.86 0.99 0.14 0.94 0.93 0.93 0.93 0.92 0.99 0.99 0.96 1.   1.   0.81]
 [1.   0.93 0.95 1.   1.   0.63 0.89 0.98 0.84 1.   0.15 0.95 0.92 0.94 0.94 0.92 1.   1.   0.96 1.   1.   0.79]
 [0.61 0.74 0.73 0.64 0.63 1.   0.74 0.7  0.48 0.6  0.12 0.73 0.8  0.72 0.72 0.72 0.69 0.69 0.71 0.64 0.65 0.94]
 [0.88 0.99 0.99 0.9  0.89 0.74 1.   0.97 0.74 0.86 0.3  0.99 0.99 0.99 0.99 0.99 0.92 0.91 0.96 0.89 0.9  0.86]
 [0.97 0.98 0.99 0.98 0.98 0.7  0.97 1.   0.82 0.96 0.2  0.99 0.98 0.99 0.98 0.98 0.98 0.98 0.99 0.98 0.98 0.85]
 [0.84 0.78 0.79 0.86 0.84 0.48 0.74 0.82 1.   0.84 0.11 0.79 0.76 0.77 0.76 0.76 0.83 0.83 0.81 0.84 0.84 0.64]
 [1.   0.9  0.92 0.99 1.   0.6  0.86 0.96 0.84 1.   0.1  0.92 0.89 0.91 0.91 0.89 0.99 0.99 0.95 0.99 1.   0.77]
 [0.13 0.22 0.26 0.14 0.15 0.12 0.3  0.2  0.11 0.1  1.   0.24 0.23 0.27 0.27 0.28 0.16 0.11 0.13 0.14 0.14 0.12]
 [0.94 1.   1.   0.94 0.95 0.73 0.99 0.99 0.79 0.92 0.24 1.   0.99 1.   0.99 0.99 0.96 0.96 0.99 0.94 0.95 0.86]
 [0.91 0.99 0.99 0.93 0.92 0.8  0.99 0.98 0.76 0.89 0.23 0.99 1.   0.99 0.99 0.98 0.95 0.94 0.97 0.92 0.93 0.91]
 [0.93 1.   1.   0.93 0.94 0.72 0.99 0.99 0.77 0.91 0.27 1.   0.99 1.   1.   1.   0.96 0.95 0.98 0.93 0.94 0.85]
 [0.93 1.   0.99 0.93 0.94 0.72 0.99 0.98 0.76 0.91 0.27 0.99 0.99 1.   1.   1.   0.96 0.95 0.98 0.93 0.94 0.84]
 [0.91 0.99 0.99 0.92 0.92 0.72 0.99 0.98 0.76 0.89 0.28 0.99 0.98 1.   1.   1.   0.94 0.93 0.97 0.91 0.92 0.84]
 [0.99 0.95 0.96 0.99 1.   0.69 0.92 0.98 0.83 0.99 0.16 0.96 0.95 0.96 0.96 0.94 1.   1.   0.98 0.99 1.   0.83]
 [0.99 0.95 0.96 0.99 1.   0.69 0.91 0.98 0.83 0.99 0.11 0.96 0.94 0.95 0.95 0.93 1.   1.   0.98 0.99 1.   0.84]
 [0.96 0.99 0.98 0.96 0.96 0.71 0.96 0.99 0.81 0.95 0.13 0.99 0.97 0.98 0.98 0.97 0.98 0.98 1.   0.96 0.97 0.86]
 [1.   0.93 0.95 1.   1.   0.64 0.89 0.98 0.84 0.99 0.14 0.94 0.92 0.93 0.93 0.91 0.99 0.99 0.96 1.   1.   0.81]
 [1.   0.94 0.95 1.   1.   0.65 0.9  0.98 0.84 1.   0.14 0.95 0.93 0.94 0.94 0.92 1.   1.   0.97 1.   1.   0.8 ]
 [0.78 0.86 0.86 0.81 0.79 0.94 0.86 0.85 0.64 0.77 0.12 0.86 0.91 0.85 0.84 0.84 0.83 0.84 0.86 0.81 0.8  1.  ]]
26501419	<a href="https://www.semanticscholar.org/paper/f010affab57b5fcf1cd6be23df79d8ec98c7289c">867: TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension</a>
14915449	<a href="https://www.semanticscholar.org/paper/35b91b365ceb016fb3e022577cec96fb9b445dc5">530: The Goldilocks Principle: Reading Children's Books with Explicit Memory Representations</a>
12501880	<a href="https://www.semanticscholar.org/paper/b798cfd967e1a9ca5e7bc995d33a907bf65d1c7f">572: Gated Self-Matching Networks for Reading Comprehension and Question Answering</a>
11816014	<a href="https://www.semanticscholar.org/paper/05dd7254b632376973f3a1b4d39485da17814df5">4168: SQuAD: 100,000+ Questions for Machine Comprehension of Text</a>
11606382	<a href="https://www.semanticscholar.org/paper/3adff57fd09965224506a1bacc0579d9d3c8c11e">303: SearchQA: A New Q&A Dataset Augmented with Context from a Search Engine</a>
11080756	<a href="https://www.semanticscholar.org/paper/d7da009f457917aa381619facfa5ffae9329a6e9">16360: Bleu: a Method for Automatic Evaluation of Machine Translation</a>
9205021	<a href="https://www.semanticscholar.org/paper/c6e5df6322659276da6133f9b734a389d7a255e8">361: Attention-over-Attention Neural Networks for Reading Comprehension</a>
8535316	<a href="https://www.semanticscholar.org/paper/3a7b63b50c64f4ec3358477790e84cbd6be2a0b4">1714: Bidirectional Attention Flow for Machine Comprehension</a>
7228830	<a href="https://www.semanticscholar.org/paper/ffb949d3493c3b2f3c9acf9c75cb03938933ddf0">1059: Adversarial Examples for Evaluating Reading Comprehension Systems</a>
6826032	<a href="https://www.semanticscholar.org/paper/636a79420d838eabe4af7fb25d6437de45ab64e8">685: RACE: Large-scale ReAding Comprehension Dataset From Examinations</a>
6628106	<a href="https://www.semanticscholar.org/paper/a6cb366736791bcccc5c8639de5a8f9636bf87e8">87690: Adam: A Method for Stochastic Optimization</a>
6360322	<a href="https://www.semanticscholar.org/paper/b1e20420982a4f923c08652941666b189b11b7fe">491: A Thorough Examination of the CNN/Daily Mail Reading Comprehension Task</a>
6203757	<a href="https://www.semanticscholar.org/paper/d1505c6123c102e53eb19dff312cb25cea840b72">2389: Teaching Machines to Read and Comprehend</a>
5592690	<a href="https://www.semanticscholar.org/paper/ff1861b71eaedba46cb679bbe2c585dbe18f9b19">538: Machine Comprehension Using Match-LSTM and Answer Pointer</a>
3933075	<a href="https://www.semanticscholar.org/paper/f92272e33b11a0d2f47b5b65446c0f1a913cfd17">75: Consensus Attention-based Neural Networks for Chinese Reading Comprehension</a>
3714278	<a href="https://www.semanticscholar.org/paper/e978d832a4d86571e1b52aa1685dc32ccb250f50">613: Dynamic Coattention Networks For Question Answering</a>
3662564	<a href="https://www.semanticscholar.org/paper/995b7affd684b910d5a1c520c3af00fd20cc39b0">167: DuReader: a Chinese Machine Reading Comprehension Dataset from Real-world Applications</a>
2593903	<a href="https://www.semanticscholar.org/paper/d91043f0d48b9b2c8ff7ee321abb8fd7efafff7a">338: The NarrativeQA Reading Comprehension Challenge</a>
2100831	<a href="https://www.semanticscholar.org/paper/564257469fa44cdb57e4272f85253efb9acfd69d">589: MCTest: A Challenge Dataset for the Open-Domain Machine Comprehension of Text</a>
1289517	<a href="https://www.semanticscholar.org/paper/a69cf45d44a9d806d2487a1ffb9eca71ee73c2ee">1032: MS MARCO: A Human Generated MAchine Reading COmprehension Dataset</a>
1167588	<a href="https://www.semanticscholar.org/paper/3eda43078ae1f4741f09be08c4ecab6229046a5c">579: NewsQA: A Machine Comprehension Dataset</a>
964287	<a href="https://www.semanticscholar.org/paper/60b05f32c32519a809f21642ef1eb3eaf3848008">6795: ROUGE: A Package for Automatic Evaluation of Summaries</a>

$src/cluster_paper.sh 1055111 proposed
$src/cluster_paper.sh 1055111 specter

cd /work/k.church/sun.jiam/scirepeval_data/test/scidocs/view_cite_read/cite/cluster_fit
awk '{print "#!/bin/sh"; printf "$src/cluster_paper_features.sh %d proposed | $src/cluster_paper2.py\n", $1}' ids | split -d -a 3 -l 20 - todo/x. 

cd /work/k.church/sun.jiam/scirepeval_data/test/scidocs/view_cite_read/cite/cluster_fit
for f in todo/x.???
do
sbatch -o $f.out -e $f.err -p debug -t 19 $f
done

# for f in todo/x.???
# do
# sed 's/proposed/specter/g' < $f > todo.specter/`basename $f`
# done

cd /work/k.church/sun.jiam/scirepeval_data/test/scidocs/view_cite_read/cite/cluster_fit
for f in todo.specter/x.???
do
sbatch -o $f.out -e $f.err -p debug -t 19 $f
done

/work/nlp/sun.jiam/prone_benchmarks/new_prone_bench_2.csv

SKLEARN_METRICS=/work/k.church/githubs/gft/gft/gft_internals/sklearn_metrics


cd /scratch/k.church/semantic_scholar/tmp
/work/k.church/semantic_scholar/releases/2022-08-23/database/citations/graphs/sample_edges_part1.sh 0.01 5 10
$src/make_graph_symmetric.py -G Gshrunk.G2.npz -O Gshrunk.G2.symmetric.npz

$src/prefactor_graph.py -G Gshrunk.G2.symmetric.npz -O Gshrunk.G2.U -K 5

job0=`sbatch --mem=10G -p debug $src/ProNE_chebyshev.py -G Gshrunk.G2.symmetric.npz -U Gshrunk.G2.U.npy --temp_file_prefix Gshrunk.G2 --iteration 0 | awk '{print $NF}'`
job1=`sbatch --mem=10G -d afterany:$job0 -p debug $src/ProNE_chebyshev.py -G Gshrunk.G2.symmetric.npz -U Gshrunk.G2.U.npy --temp_file_prefix Gshrunk.G2 --iteration 1 | awk '{print $NF}'`
job2=`sbatch --mem=10G -d afterany:$job1 -p debug $src/ProNE_chebyshev.py -G Gshrunk.G2.symmetric.npz -U Gshrunk.G2.U.npy --temp_file_prefix Gshrunk.G2 --iteration 2 | awk '{print $NF}'`
job3=`sbatch --mem=10G -d afterany:$job2 -p debug $src/ProNE_chebyshev.py -G Gshrunk.G2.symmetric.npz -U Gshrunk.G2.U.npy --temp_file_prefix Gshrunk.G2 --iteration 3 | awk '{print $NF}'`
job4=`sbatch --mem=10G -d afterany:$job3 -p debug $src/ProNE_chebyshev.py -G Gshrunk.G2.symmetric.npz -U Gshrunk.G2.U.npy --temp_file_prefix Gshrunk.G2 --iteration 4 | awk '{print $NF}'`
job5=`sbatch --mem=10G -d afterany:$job4 -p debug $src/ProNE_chebyshev.py -G Gshrunk.G2.symmetric.npz -U Gshrunk.G2.U.npy --temp_file_prefix Gshrunk.G2 --iteration 5 | awk '{print $NF}'`
job6=`sbatch --mem=10G -d afterany:$job5 -p debug $src/ProNE_chebyshev.py -G Gshrunk.G2.symmetric.npz -U Gshrunk.G2.U.npy --temp_file_prefix Gshrunk.G2 --iteration 6 | awk '{print $NF}'`
job7=`sbatch --mem=10G -d afterany:$job6 -p debug $src/ProNE_chebyshev.py -G Gshrunk.G2.symmetric.npz -U Gshrunk.G2.U.npy --temp_file_prefix Gshrunk.G2 --iteration 7 | awk '{print $NF}'`
job8=`sbatch --mem=10G -d afterany:$job7 -p debug $src/ProNE_chebyshev.py -G Gshrunk.G2.symmetric.npz -U Gshrunk.G2.U.npy --temp_file_prefix Gshrunk.G2 --iteration 8 | awk '{print $NF}'`
job9=`sbatch --mem=10G -d afterany:$job8 -p debug $src/ProNE_chebyshev.py -G Gshrunk.G2.symmetric.npz -U Gshrunk.G2.U.npy --temp_file_prefix Gshrunk.G2 --iteration 9 | awk '{print $NF}'`

# cd /scratch/k.church/semantic_scholar/tmp
# sbatch -p express -t 59 $src/prefactor_graph.py -G Gshrunk.G2.npz -O Gshrunk.G2.U.V2 -K 5 -S 

# cd /scratch/k.church/semantic_scholar/tmp
# $src/ProNE_chebyshev.py --help

# $src/ProNE_chebyshev.py -G Gshrunk.G2.symmetric.npz -U Gshrunk.G2.U.npy --temp_file_prefix Gshrunk.G2 --iteration 0
# $src/ProNE_chebyshev.py -G Gshrunk.G2.symmetric.npz -U Gshrunk.G2.U.npy --temp_file_prefix Gshrunk.G2 --iteration 1
# $src/ProNE_chebyshev.py -G Gshrunk.G2.symmetric.npz -U Gshrunk.G2.U.npy --temp_file_prefix Gshrunk.G2 --iteration 2

ls sl*out | tr '\-' . | cut -f2 -d. | awk '{print "seff", $0}' | sh | egrep "Memory Utilized"

ls sl*out | tr '\-' . | cut -f2 -d. | awk '{printf "sacct --format JobID,MaxRSSNode -j %s | tail -1 \n", $0}' | sh
35838757.ex+      c2200 
35838758.ex+      c2000 
35838759.ex+      c2000 
35838760.ex+      c2000 
35838761.ex+      c2000 
35838762.ex+      c2000 
35838763.ex+      c2000 
35838766.ex+      c2000 
35838768.ex+      c0315 
35838769.ex+      c0315 

cd /work/k.church/Walid/

sh /work/nlp/sun.jiam/abstract_script/abstract_script_2.sh

/home/yue.r/frequency/SF_LF_frequencies.csv

cd /scratch/k.church/semantic_scholar/releases/2022-12-02/database/citations/extractions/citing_sentences/pieces
cp *idx* /work/k.church/semantic_scholar/releases/2022-12-02/database/citations/extractions/citing_sentences

cd /scratch/k.church/semantic_scholar/releases/2022-12-02/database/citations/extractions/citing_sentences/pieces
rsync -L -r 000 /work/k.church/semantic_scholar/releases/2022-12-02/database/citations/extractions/citing_sentences/pieces

df -h .
Filesystem                              Size  Used Avail Use% Mounted on
vast1-mghpcc-eth.neu.edu:/work_project   21T   18T  2.9T  86% /work

cd /work/k.church/semantic_scholar/releases/2022-12-02/database/citations/extractions/citing_sentences/pieces/
sbatch --array 2-182 -p express -t 59 /work/k.church/semantic_scholar/releases/2022-12-02/database/citations/extractions/citing_sentences/pieces/copy_job.sh

cd /work/k.church/semantic_scholar/releases/2022-12-02/database
for f in */*.001.gz 
do
echo $f
zcat $f | sed 1q | jq  | cut -c1-200
done

nohup sh $HOME/job.sh >> $HOME/job.out5 &

cd /work/k.church/JSALT-2023/semantic_scholar/embeddings/specter
K=280
f=embedding
sbatch -e $f.K$K.debug.err -p debug -t 19  -i $f.f -o $f.K$K.debug.f $JSALTsrc/random_project_floats.py -K 768,$K 

cd /work/k.church/JSALT-2023/semantic_scholar/embeddings/specter
for seed in 1 2 3 4 5 6 7 8 9
do
K=280
f=embedding
sbatch -e $f.K$K.err -p express -t 59  -i $f.f -o $f.K$K.seed.$seed.f $JSALTsrc/random_project_floats.py -K 768,$K  --seed $seed
done

cd /work/k.church/JSALT-2023/semantic_scholar/embeddings/specter.K280
for seed in 1 2 3 4 5 6 7 8 9
do
sbatch -p express -t 59 /work/k.church/githubs/JSALT_Better_Together/src/create_rotation_matrix.py -i ../specter.K280,../proposed -N 10000000 --seed $seed -o rot.seed.$seed.V3
done

/work/nlp/sun.jiam/FusionScholar/cocite.ids

rel=2023-05-09
cd /work/k.church/semantic_scholar/releases/$rel/database/citations
mkdir -p graphs
for f in citations.piece.???.gz
do
b=`basename $f .gz`
err=graphs/$f.err
sbatch -p express -t 59 -e $err $JSALTsrc/citations2graph.py $f
done

rel=2023-05-09
cd /work/k.church/semantic_scholar/releases/$rel/database/citations/graphs
for f in citations.piece.???.gz.X.txt
do
err=$f.err
in=`basename $f .X.txt`
out=`basename $f .gz.X.txt`
sbatch -p debug -t 19 -e $err $JSALTsrc/graph_fromfile.py -o $out -T -N 270000000 -i $in
done


cd /work/k.church/JSALT-2023/semantic_scholar/embeddings/specter.K280
# R=rot.seed.1.npy,rot.seed.2.npy,rot.seed.3.npy,rot.seed.4.npy,rot.seed.5.npy,rot.seed.6.npy,rot.seed.7.npy,rot.seed.8.npy,rot.seed.9.npy
R=rot.seed.1.V2.npy,rot.seed.2.V2.npy,rot.seed.3.V2.npy,rot.seed.4.V2.npy,rot.seed.5.V2.npy,rot.seed.6.V2.npy,rot.seed.7.V2.npy,rot.seed.8.V2.npy,rot.seed.9.V2.npy
python /work/k.church/githubs/JSALT_Better_Together/src/test_rotation_matrices.py -i ../specter.K280,../proposed -N 1000 --seed 1 -R $R | sort -n > /tmp/x1.V2

cd /work/k.church/JSALT-2023/semantic_scholar/embeddings/specter.K280
seed=0
/work/k.church/githubs/JSALT_Better_Together/src/create_rotation_matrix.py -i ../specter.K280,../proposed -N 100000 --seed $seed -o rot.seed.$seed.Vjunk

cd /scratch/k.church/semantic_scholar/releases/2022-12-02/database/abstracts
for f in todo/x.12[012]??
do
sbatch -e $f.err -o $f.out -p short -t 1409 $f
done

rel=2023-05-09
cd /work/k.church/semantic_scholar/releases/$rel/database/citations/graphs
sbatch --mem=100G -p express  -t 59 -e citations.G.err sum_matrices.py citations.G citations.piece.???.npz

rel=2023-05-09
T=1
cd /work/k.church/semantic_scholar/releases/$rel/database/citations/graphs
job2=`sbatch -p long -e citations.CC.err -o citations.CC.out connected_components.py -G citations.G.npz -C citations.CC | awk '{print $NF}'`
sbatch -d afterany:$job2 --mem=80G -t 1209 -p long -e `basename $outf .npz`.err shrink_matrix.py -G $G -C citations.CC.npy -o $outf -T $T


job0=`sbatch --mem=10G -p long $src/ProNE_chebyshev.py -G Gshrunk.G2.symmetric.npz -U Gshrunk.G2.U.npy --temp_file_prefix Gshrunk.G2 --iteration 0 | awk '{print $NF}'`
job1=`sbatch --mem=10G -d afterany:$job0 -p long $src/ProNE_chebyshev.py -G Gshrunk.G2.symmetric.npz -U Gshrunk.G2.U.npy --temp_file_prefix Gshrunk.G2 --iteration 1 | awk '{print $NF}'`
job2=`sbatch --mem=10G -d afterany:$job1 -p long $src/ProNE_chebyshev.py -G Gshrunk.G2.symmetric.npz -U Gshrunk.G2.U.npy --temp_file_prefix Gshrunk.G2 --iteration 2 | awk '{print $NF}'`
job3=`sbatch --mem=10G -d afterany:$job2 -p long $src/ProNE_chebyshev.py -G Gshrunk.G2.symmetric.npz -U Gshrunk.G2.U.npy --temp_file_prefix Gshrunk.G2 --iteration 3 | awk '{print $NF}'`
job4=`sbatch --mem=10G -d afterany:$job3 -p long $src/ProNE_chebyshev.py -G Gshrunk.G2.symmetric.npz -U Gshrunk.G2.U.npy --temp_file_prefix Gshrunk.G2 --iteration 4 | awk '{print $NF}'`
job5=`sbatch --mem=10G -d afterany:$job4 -p long $src/ProNE_chebyshev.py -G Gshrunk.G2.symmetric.npz -U Gshrunk.G2.U.npy --temp_file_prefix Gshrunk.G2 --iteration 5 | awk '{print $NF}'`
job6=`sbatch --mem=10G -d afterany:$job5 -p long $src/ProNE_chebyshev.py -G Gshrunk.G2.symmetric.npz -U Gshrunk.G2.U.npy --temp_file_prefix Gshrunk.G2 --iteration 6 | awk '{print $NF}'`
job7=`sbatch --mem=10G -d afterany:$job6 -p long $src/ProNE_chebyshev.py -G Gshrunk.G2.symmetric.npz -U Gshrunk.G2.U.npy --temp_file_prefix Gshrunk.G2 --iteration 7 | awk '{print $NF}'`
job8=`sbatch --mem=10G -d afterany:$job7 -p long $src/ProNE_chebyshev.py -G Gshrunk.G2.symmetric.npz -U Gshrunk.G2.U.npy --temp_file_prefix Gshrunk.G2 --iteration 8 | awk '{print $NF}'`
job9=`sbatch --mem=10G -d afterany:$job8 -p long $src/ProNE_chebyshev.py -G Gshrunk.G2.symmetric.npz -U Gshrunk.G2.U.npy --temp_file_prefix Gshrunk.G2 --iteration 9 | awk '{print $NF}'`
