{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2f6b882-2c2e-42c1-9fcf-243a596a2674",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, os, sys, matplotlib.pyplot as plt, seaborn as sns, pandas, orjson\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f018977-516c-4579-9a77-0d8563c66a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dict_of_arrays(exp_model_dict, read_directory='../log_calculations/unified_outputs/', fname_to_read='pppl.npy',convert_for_plot=True):\n",
    "    np_arrays = {}\n",
    "    \n",
    "    for exp, models in exp_model_dict.items():\n",
    "        for model in models:\n",
    "            file_name = os.path.join(read_directory, exp, model, fname_to_read)\n",
    "            if convert_for_plot:\n",
    "                np_arrays[model] = get_xy(np.load(file_name))\n",
    "            else:\n",
    "                np_arrays[model] = np.load(file_name)\n",
    "\n",
    "    return np_arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "393a4ab4-98de-40d7-b517-c69f1644dea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dict_of_df(df_dict, read_directory='../log_calculations/unified_outputs/', fname_to_read='filtered_df.csv'):\n",
    "    \n",
    "    dfs = {}\n",
    "    read_dirs = []\n",
    "    for exp, models in df_dict.items():\n",
    "        for model in models:\n",
    "            file_name = os.path.join(read_directory, exp, model, fname_to_read)\n",
    "            read_dirs.append(os.path.join(read_directory, exp, model))\n",
    "            dfs[model] = pandas.read_csv(file_name)\n",
    "    return dfs, read_dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b4c0f47-c264-475c-b13a-62252817ca43",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    # 'every_5_5000' : ['allenai/scibert_scivocab_cased', 'bert-base-cased', 'roberta-base', 'xlm-roberta-base']\n",
    "    'every_5_5000' : ['bert-large-cased']\n",
    "}\n",
    "\n",
    "dfs, read_dirs = get_dict_of_df(models)\n",
    "model_nps = get_dict_of_arrays(models, fname_to_read='filtered_log_probs.npy', convert_for_plot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d13b0e9-a53e-400e-bfa4-ac705377997f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 21/21 [00:33<00:00,  1.60s/it]\n"
     ]
    }
   ],
   "source": [
    "bin_samples_dir = '/projects/abeb4417/jsalt/lm_perplexity/sampling/bin_samples/'\n",
    "\n",
    "cids_to_check = pandas.concat([df[['bin', 'corpusid']] for _, df in dfs.items()], axis=0).drop_duplicates()\n",
    "bins = cids_to_check['bin'].unique()\n",
    "\n",
    "external_data = {}\n",
    "   \n",
    "for bin in tqdm(bins):\n",
    "    cids = cids_to_check[cids_to_check['bin'] == bin]['corpusid'].values\n",
    "    seen_cids = set()\n",
    "    with open(os.path.join(bin_samples_dir, f'{bin:03d}')) as input_sample:\n",
    "        for line in input_sample:\n",
    "            dat = orjson.loads(line)\n",
    "            cid = dat['corpusid']\n",
    "            if cid in cids:\n",
    "                if cid not in external_data:\n",
    "                    external_data[cid] = dat['openaccessinfo']['externalids']\n",
    "                seen_cids.add(cid)\n",
    "            if seen_cids == set(cids):\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aaf188ba-50ac-41b2-b340-c0d64aaf51b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MAG': '2129968670',\n",
       " 'ACL': None,\n",
       " 'DOI': '10.1002/PRAC.19231050116',\n",
       " 'PubMedCentral': None,\n",
       " 'ArXiv': None}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(cids_to_check['corpusid'] == 98785756).sum()\n",
    "external_data[98785756]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5fcc809-7878-415a-83a8-124a922067d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ACL', 'ArXiv', 'DOI', 'MAG', 'PubMedCentral'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = set()\n",
    "for _,x in external_data.items():\n",
    "    for l in x:\n",
    "        k.add(l)\n",
    "\n",
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2fa3940e-2d72-4da1-b846-b2569eb5e5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['has_pubmed'] = df['corpusid'].apply(lambda cid: True if (cid in external_ids) and (('PubMed' in external_ids[cid]['externalIds']) or ('PubMedCentral' in external_ids[cid]['externalIds'])) else False)\n",
    "\n",
    "for _, df in dfs.items():\n",
    "    for external_source in ['MAG', 'ACL', 'DOI', 'PubMedCentral', 'ArXiv']:\n",
    "        df[f'has_{external_source}'.lower()] = df['corpusid'].apply(lambda cid: True if (external_source in external_data[cid]) and (external_data[cid][external_source] is not None) else False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc62b180-886c-473b-8c88-a15b7eb9cd66",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Need to query semantic scholar for pubmed\n",
    "from tqdm import trange\n",
    "import requests\n",
    "def fetch_semantic_scholar_info(corpus_ids = [], paper_ids = [], fields=\"title,abstract,externalIds,corpusId\", max_batch_size=500, output_file = None, verbose=False):\n",
    "\n",
    "\tprocessed_ids = processed_ids = ['CorpusId:{}'.format(id) for id in corpus_ids] + paper_ids\n",
    "\n",
    "\treturned_objects = {}\n",
    "\n",
    "\tfor i in trange(0,len(corpus_ids)+1,max_batch_size):\n",
    "\t\tto_send = processed_ids[i:i+max_batch_size]\n",
    "\n",
    "\t\tif len(to_send) == 0:\n",
    "\t\t\tcontinue\n",
    "\t\n",
    "\t\tif verbose:\n",
    "\t\t\tprint('Sending following IDS to Semantic Scholar (len {}): {}'.format(len(processed_ids), processed_ids))\n",
    "\n",
    "\t\tapikey=os.environ.get('SPECTER_API_KEY')\n",
    "\t\tr = requests.post(\n",
    "\t\t\t'https://api.semanticscholar.org/graph/v1/paper/batch',\n",
    "\t\t\tparams={'fields': fields},\n",
    "\t\t\theaders={\"x-api-key\" : apikey},\n",
    "\t\t\tjson={\"ids\" : to_send}\n",
    "\t\t)\n",
    "\n",
    "\t\tfor resp in r.json():\n",
    "\t\t\tif resp is not None and isinstance(resp, dict):\n",
    "\t\t\t\treturned_objects[resp['corpusId']] = resp\n",
    "\n",
    "\n",
    "\tif output_file:\n",
    "\t\twith open(output_file, 'w') as f:\n",
    "\t\t\tjson.dump(r.json(), f, indent=2)\n",
    "\n",
    "\treturn returned_objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8ae4ff0c-4909-4be3-a839-568c22c2048c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.45it/s]\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "with open('/projects/abeb4417/jsalt/lm_perplexity/log_calculations/pickles/every_5_5000/external_ids.pickle', 'rb') as f:\n",
    "    ss_queries = pickle.load(f)\n",
    "\n",
    "need_to_fetch = []\n",
    "for cid in cids_to_check['corpusid']:\n",
    "    if cid not in ss_queries:\n",
    "        need_to_fetch.append(cid)\n",
    "\n",
    "ss_new_resps = fetch_semantic_scholar_info(need_to_fetch, fields='corpusId,externalIds')\n",
    "\n",
    "ss_pubmed = ss_queries | ss_new_resps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "21f2df0b-963b-466a-9f46-88413e2febaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model, df in dfs.items():\n",
    "    df['has_pubmed'] = df['corpusid'].apply(lambda cid: True if (cid in ss_pubmed) and (('PubMed' in ss_pubmed[cid]['externalIds'])) else False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8b5c6a48-b706-43ec-8344-888d64f08308",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20747"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(df['has_pubmed'] == True).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1ef3e56a-b835-4f66-8176-fad76d5b78aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../log_calculations/unified_outputs/every_5_5000/bert-large-cased\n",
      "bert-large-cased\n"
     ]
    }
   ],
   "source": [
    "for (model, df), dir in zip(dfs.items(), read_dirs):\n",
    "    print(dir)\n",
    "    print(model)\n",
    "    df.to_csv(os.path.join(dir, 'filtered_df_with_source.tsv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4264cf41-6ff5-4baf-9151-bd1058b66b11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jsalt_env",
   "language": "python",
   "name": "jsalt_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
