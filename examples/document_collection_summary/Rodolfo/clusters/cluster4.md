## Cluster 4 Papers

Total papers: 14
<html><table><tr>
<th>Paper Link</th>
</tr>
<tr>
<td><a href=https://www.semanticscholar.org/paper/e3ae64a5aaf39fbd7fcd5830c47364cfd0928c1d>Knowledge-Rich BERT Embeddings for Readability Assessment</a></td>
</tr>
<tr>
<td><a href=https://www.semanticscholar.org/paper/f206088d952b44e0c0b32ca82946fc5dce2ab469>COfEE: A Comprehensive Ontology for Event Extraction from text, with an online annotation tool</a></td>
</tr>
<tr>
<td><a href=https://www.semanticscholar.org/paper/29b972f985c53d960b96a5bf9916cb132c138923>Context-Dependent Domain Adversarial Neural Network for Multimodal Emotion Recognition</a></td>
</tr>
<tr>
<td><a href=https://www.semanticscholar.org/paper/fe2b861be3e24955fb5cc9d3c7cab1af6f17d9b7>OPI@LT-EDI-ACL2022: Detecting Signs of Depression from Social Media Text using RoBERTa Pre-trained Language Models</a></td>
</tr>
<tr>
<td><a href=https://www.semanticscholar.org/paper/ab224d59d524300b850bd9475147d1f90294ceb7>Leveraging Transfer learning techniques- BERT, RoBERTa, ALBERT and DistilBERT for Fake Review Detection</a></td>
</tr>
<tr>
<td><a href=https://www.semanticscholar.org/paper/2c39775d7428277a502f22117264f6376e1ea2ab>ZYJ@LT-EDI-EACL2021:XLM-RoBERTa-Based Model with Attention for Hope Speech Detection</a></td>
</tr>
<tr>
<td><a href=https://www.semanticscholar.org/paper/5ce4fc0590b08a2747b3c58bd1722b716662d82e>An improved aspect-category sentiment analysis model for text sentiment analysis based on RoBERTa</a></td>
</tr>
<tr>
<td><a href=https://www.semanticscholar.org/paper/1e652ee651cd84809b6120203781901d1a562ee4>NIT_COVID-19 at WNUT-2020 Task 2: Deep Learning Model RoBERTa for Identify Informative COVID-19 English Tweets</a></td>
</tr>
<tr>
<td><a href=https://www.semanticscholar.org/paper/dc49923910b64eef93fe0bceb63a9cd48d135b87>NIT COVID-19 at WNUT-2020 Task 2: Deep Learning Model RoBERTa for Identify Informative COVID-19 English Tweets</a></td>
</tr>
<tr>
<td><a href=https://www.semanticscholar.org/paper/57786ebee8ae46fc30213160f7cd8ce22090f152>KPWr n82 NER model (on Polish RoBERTa base)</a></td>
</tr>
<tr>
<td><a href=https://www.semanticscholar.org/paper/31bca1b59da98c68e778a64abe2f495cda998b78>Identifying Personal Experience Tweets of Medication Effects Using Pre-trained RoBERTa Language Model and Its Updating</a></td>
</tr>
<tr>
<td><a href=https://www.semanticscholar.org/paper/56874f9aef515902c5a49d84d10f629f8dcd5f40>Differentially Private Fine-tuning of Language Models</a></td>
</tr>
<tr>
<td><a href=https://www.semanticscholar.org/paper/b3cba8c2aa400ddaf066afac18ff5365826a1341>Investigating COVID-19 News Across Four Nations: A Topic Modeling and Sentiment Analysis Approach</a></td>
</tr>
<tr>
<td><a href=https://www.semanticscholar.org/paper/fa7b8acd47631bada5b66049824bfd335ac6bf8f>Towards Improving Adversarial Training of NLP Models</a></td>
</tr>
</table></html>
