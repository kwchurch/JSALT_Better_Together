## Cluster 6 Papers

Total papers: 124
<html><table><tr>
<th>Paper Link</th>
</tr>
<tr>
<td><a href=https://www.semanticscholar.org/paper/a82fb4b5de7844d28c190f9a37ba2606b81f4070>Dual Contrastive Learning: Text Classification via Label-Aware Data Augmentation</a></td>
</tr>
<tr>
<td><a href=https://www.semanticscholar.org/paper/df25192774efe751e8be1f12067e6a6ac60bffb1>LightNER: A Lightweight Generative Framework with Prompt-guided Attention for Low-resource NER</a></td>
</tr>
<tr>
<td><a href=https://www.semanticscholar.org/paper/38d3657ee15f2612330eb5e036bbc38d9137f75a>ZmBART: An Unsupervised Cross-lingual Transfer Framework for Language Generation</a></td>
</tr>
<tr>
<td><a href=https://www.semanticscholar.org/paper/caaa757d433613e62664daa021065be67bd877c9>Unsupervised Text Style Transfer with Padded Masked Language Models</a></td>
</tr>
<tr>
<td><a href=https://www.semanticscholar.org/paper/095277b3963c6d8d1b2475ff818f52f27f9e4946>Self-supervised and Supervised Joint Training for Resource-rich Machine Translation</a></td>
</tr>
<tr>
<td><a href=https://www.semanticscholar.org/paper/a87f3935d73032d333e1740f2916e687a0f2a12a>BERT memorisation and pitfalls in low-resource scenarios</a></td>
</tr>
<tr>
<td><a href=https://www.semanticscholar.org/paper/b955cc5f57f2fbba9e4e00002b969760fa4c7b73>Legal Transformer Models May Not Always Help</a></td>
</tr>
<tr>
<td><a href=https://www.semanticscholar.org/paper/9ddcc9f2073a0ca19729a6fd4242605e5a8e348d>An Empirical Study of Cross-Lingual Transferability in Generative Dialogue State Tracker</a></td>
</tr>
<tr>
<td><a href=https://www.semanticscholar.org/paper/32fc867c053c3733a9d173a2a342a6561a259f04>Discovering Representation Sprachbund For Multilingual Pre-Training</a></td>
</tr>
<tr>
<td><a href=https://www.semanticscholar.org/paper/0e047dafc97c9fca3ce1635e68efb2b9a9a5923e>What Have Been Learned & What Should Be Learned? An Empirical Study of How to Selectively Augment Text for Classification</a></td>
</tr>
<tr>
<td><a href=https://www.semanticscholar.org/paper/f7979c6690562c5f8bf700e3fd184c4d1df0a54c>Zero-shot Neural Transfer for Cross-lingual Entity Linking</a></td>
</tr>
<tr>
<td><a href=https://www.semanticscholar.org/paper/39e632b948115736a035d59f3041bbc8a63ca417>A Systematic Comparison of Architectures for Document-Level Sentiment Classification</a></td>
</tr>
<tr>
<td><a href=https://www.semanticscholar.org/paper/9211b04598dec29c8c519a5927c80db181c416b2>Modern Variational Methods for Semi-Supervised Text Classification</a></td>
</tr>
<tr>
<td><a href=https://www.semanticscholar.org/paper/77d70173d4d30d52dee29985459cd4275099058c>Deep Learning for the Soft Cutoff Problem</a></td>
</tr>
<tr>
<td><a href=https://www.semanticscholar.org/paper/077f8329a7b6fa3b7c877a57b81eb6c18b5f87de>RoBERTa: A Robustly Optimized BERT Pretraining Approach</a></td>
</tr>
<tr>
<td><a href=https://www.semanticscholar.org/paper/cdd947572a03c1f15a2d5110b22456411488d504>An ensemble model for classifying idioms and literal texts using BERT and RoBERTa</a></td>
</tr>
<tr>
<td><a href=https://www.semanticscholar.org/paper/0b7b605a2d6e0cb1e53d1fa03311278e95f99260>Does syntax matter? A strong baseline for Aspect-based Sentiment Analysis with RoBERTa</a></td>
</tr>
<tr>
<td><a href=https://www.semanticscholar.org/paper/0672f88d5dc762002b515ca4a0a9f101017fea35>Probing Across Time: What Does RoBERTa Know and When?</a></td>
</tr>
<tr>
<td><a href=https://www.semanticscholar.org/paper/0934952763f1549e75b142261e73f2b27a2f495b>RobeCzech: Czech RoBERTa, a monolingual contextualized language representation model</a></td>
</tr>
<tr>
<td><a href=https://www.semanticscholar.org/paper/634e8ee7e86f253c4b6c722a3bb7c32b7aa3892b>RobBERT: a Dutch RoBERTa-based Language Model</a></td>
</tr>
<tr>
<td><a href=https://www.semanticscholar.org/paper/bbaf0ef9cf240d764dd20afbeb5ecef5c1d115f6>EmoBERTa: Speaker-Aware Emotion Recognition in Conversation with RoBERTa</a></td>
</tr>
<tr>
<td><a href=https://www.semanticscholar.org/paper/4d952c130b3ce772e0797d7e32e6bd8b59772016>DeepSCC: Source Code Classification Based on Fine-Tuned RoBERTa</a></td>
</tr>
<tr>
<td><a href=https://www.semanticscholar.org/paper/e372972a62f828306796fe36595857828454fbe0>GCDH@LT-EDI-EACL2021: XLM-RoBERTa for Hope Speech Detection in English, Malayalam, and Tamil</a></td>
</tr>
<tr>
<td><a href=https://www.semanticscholar.org/paper/898d3560f7d36dc84d8355ed7439c78c32068380>NLytics at CheckThat! 2021: Multi-class fake news detection of news articles and domain identification with RoBERTa - a baseline model</a></td>
</tr>
<tr>
<td><a href=https://www.semanticscholar.org/paper/b147f4df4768d6aaeadcdf5c4ac6dcbbfec85fc9>Cross-Lingual Named Entity Recognition Using Parallel Corpus: A New Approach Using XLM-RoBERTa Alignment</a></td>
</tr>
<tr>
<td><a href=https://www.semanticscholar.org/paper/07df21fb90f3ab8d51da8cb967f67c87efaa4145>JUST-BLUE at SemEval-2021 Task 1: Predicting Lexical Complexity using BERT and RoBERTa Pre-trained Language Models</a></td>
</tr>
<tr>
<td><a href=https://www.semanticscholar.org/paper/055fac05cd424e7b1bdcd359ff7980ca8d938ef3>Learning Which Features Matter: RoBERTa Acquires a Preference for Linguistic Generalizations (Eventually)</a></td>
</tr>
<tr>
<td><a href=https://www.semanticscholar.org/paper/15d9c0e078104c16232ed525b1f3b9ed2ceedd1a>ZYJ123@DravidianLangTech-EACL2021: Offensive Language Identification based on XLM-RoBERTa with DPCNN</a></td>
</tr>
<tr>
<td><a href=https://www.semanticscholar.org/paper/a9693b7b57f203940889de6d3f979c70c09202ed>Shuffled-token Detection for Refining Pre-trained RoBERTa</a></td>
</tr>
<tr>
<td><a href=https://www.semanticscholar.org/paper/5feeb14ede859d123a2b2a846d90abd8d8934b24>SarcasmDet at SemEval-2021 Task 7: Detect Humor and Offensive based on Demographic Factors using RoBERTa Pre-trained Model</a></td>
</tr>
<tr>
<td><a href=https://www.semanticscholar.org/paper/84c4a26baec0b6414a2738c8ac9b702a3792d217>Trusting RoBERTa over BERT: Insights from CheckListing the Natural Language Inference Task</a></td>
</tr>
<tr>
<td><a href=https://www.semanticscholar.org/paper/62637f5ac7c12a968f7ed63ed94313c9abb9dfe9>A RoBERTa-based model on measuring the severity of the signs of depression</a></td>
</tr>
<tr>
<td><a href=https://www.semanticscholar.org/paper/310213842f3646d5fc0a5dbbd4e0ef3d0130b91c>TorontoCL at CMCL 2021 Shared Task: RoBERTa with Multi-Stage Fine-Tuning for Eye-Tracking Prediction</a></td>
</tr>
<tr>
<td><a href=https://www.semanticscholar.org/paper/4b0fc45308ef5fddd75a5cd4fbf732386199a6ec>Comparative Analyses of Bert, Roberta, Distilbert, and Xlnet for Text-Based Emotion Recognition</a></td>
</tr>
<tr>
<td><a href=https://www.semanticscholar.org/paper/91aca4acc06348c67df00180191d02297c563d9f>Empathetic Dialogue Generation with Pre-trained RoBERTa-GPT2 and External Knowledge</a></td>
</tr>
<tr>
<td><a href=https://www.semanticscholar.org/paper/2829f1532fc28d437f1f854771a88340b8de66da>RG PA at SemEval-2021 Task 1: A Contextual Attention-based Model with RoBERTa for Lexical Complexity Prediction</a></td>
</tr>
<tr>
<td><a href=https://www.semanticscholar.org/paper/5d63495e669a410770373c5017a6738feec74141>Exploring Transformers in Emotion Recognition: a comparison of BERT, DistillBERT, RoBERTa, XLNet and ELECTRA</a></td>
</tr>
<tr>
<td><a href=https://www.semanticscholar.org/paper/5ce9b363be3af959592521a1aa5a8170111f2451>Research on Named Entity Recognition of Electronic Medical Records Based on RoBERTa and Radical-Level Feature</a></td>
</tr>
<tr>
<td><a href=https://www.semanticscholar.org/paper/59e7925f029fd1da2741d00dcea4b11bfa36b318>Classification of Tweets Self-reporting Adverse Pregnancy Outcomes and Potential COVID-19 Cases Using RoBERTa Transformers</a></td>
</tr>
<tr>
<td><a href=https://www.semanticscholar.org/paper/0e00bea2bcd3ab894a37be0d7c26d88d371c6551>AStarTwice at SemEval-2021 Task 5: Toxic Span Detection Using RoBERTa-CRF, Domain Specific Pre-Training and Self-Training</a></td>
</tr>
<tr>
<td><a href=https://www.semanticscholar.org/paper/0468f9995b896885a778f45a2e87b5afc5c85b1a>PALI at SemEval-2021 Task 2: Fine-Tune XLM-RoBERTa for Word in Context Disambiguation</a></td>
</tr>
<tr>
<td><a href=https://www.semanticscholar.org/paper/5d8abd55853e0aa17f71141da31a859a01391da9>Fine-Grained Chinese Named Entity Recognition Based on RoBERTa-WWM-BiLSTM-CRF Model</a></td>
</tr>
<tr>
<td><a href=https://www.semanticscholar.org/paper/79c70f0998276829b03e3400216b7ec01db18839>An Empirical Comparison of BERT, RoBERTa, and Electra for Fact Verification</a></td>
</tr>
<tr>
<td><a href=https://www.semanticscholar.org/paper/17801995b7352f212d2841ad7b59482379de50a5>Emotion Classification for Spanish with XLM-RoBERTa and TextCNN</a></td>
</tr>
<tr>
<td><a href=https://www.semanticscholar.org/paper/f595084ccdc21c1f297d1669a2526a00bc07e9fa>RoBERTa-wwm-ext Fine-Tuning for Chinese Text Classification</a></td>
</tr>
<tr>
<td><a href=https://www.semanticscholar.org/paper/bc19c03ff225083d6e8eba25a9300a474f3515bc>HamiltonDinggg at SemEval-2021 Task 5: Investigating Toxic Span Detection using RoBERTa Pre-training</a></td>
</tr>
<tr>
<td><a href=https://www.semanticscholar.org/paper/873f58e26ff8987b2a77e3e89e3419213011059c>Ensemble ALBERT and RoBERTa for Span Prediction in Question Answering</a></td>
</tr>
<tr>
<td><a href=https://www.semanticscholar.org/paper/1329e7867315e6c95774b6b6b675cac9afaa8be1>ApplicaAI at SemEval-2020 Task 11: On RoBERTa-CRF, Span CLS and Whether Self-Training Helps Them</a></td>
</tr>
<tr>
<td><a href=https://www.semanticscholar.org/paper/a9dc87b20993002fa84b4d4f818babb77ba18fc6>EfficientQA : a RoBERTa Based Phrase-Indexed Question-Answering System</a></td>
</tr>
<tr>
<td><a href=https://www.semanticscholar.org/paper/8b96a8de2a9fdc27ac2ed21d8e8374597d422277>Aggression Identification in English, Hindi and Bangla Text using BERT, RoBERTa and SVM</a></td>
</tr>
<tr>
<td><a href=https://www.semanticscholar.org/paper/fc6ec78f98e9a8c1f49a298bafcd50784200b66d>Aschern at SemEval-2020 Task 11: It Takes Three to Tango: RoBERTa, CRF, and Transfer Learning</a></td>
</tr>
<tr>
<td><a href=https://www.semanticscholar.org/paper/260cce438595c708433719a75c72889fefa5f731>Compositional and Lexical Semantics in RoBERTa, BERT and DistilBERT: A Case Study on CoQA</a></td>
</tr>
<tr>
<td><a href=https://www.semanticscholar.org/paper/877835df469d8b2160bd40162acde70270acb108>Classification of mental illnesses on social media using RoBERTa</a></td>
</tr>
<tr>
<td><a href=https://www.semanticscholar.org/paper/fbcb97b1ce3cb1d121b8c0a1acade49ed789c98b>Contextual word representations: RoBERTa</a></td>
</tr>
<tr>
<td><a href=https://www.semanticscholar.org/paper/11e310b95da8108b1c2cc35e18b323ef9cbe30c0>Understanding RoBERTa's Mood: The Role of Contextual-Embeddings as User-Representations for Depression Prediction</a></td>
</tr>
<tr>
<td><a href=https://www.semanticscholar.org/paper/bcafdf8ca05a49cb15ccfbe35f0c6103b8cb8849>From Universal Language Model to Downstream Task: Improving RoBERTa-Based Vietnamese Hate Speech Detection</a></td>
</tr>
<tr>
<td><a href=https://www.semanticscholar.org/paper/65ccfdf770f5f3480549047ee7f474d2877e88e2>YNU@Dravidian-CodeMix-FIRE2020: XLM-RoBERTa for Multi-language Sentiment Analysis</a></td>
</tr>
<tr>
<td><a href=https://www.semanticscholar.org/paper/55c426916aaed21a80b143c83cec35664553beca>YNU_OXZ @ HaSpeeDe 2 and AMI : XLM-RoBERTa with Ordered Neurons LSTM for Classification Task at EVALITA 2020</a></td>
</tr>
<tr>
<td><a href=https://www.semanticscholar.org/paper/483a39b7953a88494d2ec65f53acaea91ae76eb0>Rhetorical Role Labelling for Legal Judgements Using ROBERTA</a></td>
</tr>
<tr>
<td><a href=https://www.semanticscholar.org/paper/fa566d45380b0e70ec05fbfed9c4874c6d37e54b>Fake News Detection in the Urdu Language using CharCNN-RoBERTa</a></td>
</tr>
<tr>
<td><a href=https://www.semanticscholar.org/paper/aa33cc2ebbdc43d7ab70afef366894d7e399af48>DSC-IIT ISM at WNUT-2020 Task 2: Detection of COVID-19 informative tweets using RoBERTa</a></td>
</tr>
<tr>
<td><a href=https://www.semanticscholar.org/paper/9a9b691be8868b58d9fae0a1a5e4dacda83d78f1>A Hybrid Neural Network RBERT-C Based on Pre-trained RoBERTa and CNN for User Intent Classification</a></td>
</tr>
<tr>
<td><a href=https://www.semanticscholar.org/paper/925e0cecc7f44b55b96015ed38a9ecb80d4de3b2>YNU_OXZ at HASOC 2020: Multilingual Hate Speech and Offensive Content Identification based on XLM-RoBERTa</a></td>
</tr>
<tr>
<td><a href=https://www.semanticscholar.org/paper/33462c976dd960548bcc400e5f62a5d6253fe543>Data Agnostic RoBERTa-based Natural Language to SQL Query Generation</a></td>
</tr>
<tr>
<td><a href=https://www.semanticscholar.org/paper/bd4a8bdd6bf02a28f16b015363d3fa5f7f7e1546>Fine-Grained Chinese Named Entity Recognition with RoBERTa and Convolutional Attention Networks</a></td>
</tr>
<tr>
<td><a href=https://www.semanticscholar.org/paper/b350f88858d37ca0ee0fbd5c3e613b56211f5eb7>Masked Reasoner at SemEval-2020 Task 4: Fine-Tuning RoBERTa for Commonsense Reasoning</a></td>
</tr>
<tr>
<td><a href=https://www.semanticscholar.org/paper/832e497718201b3c841d850bfe2744b2fca96a53>TheNorth at SemEval-2020 Task 12: Hate Speech Detection Using RoBERTa</a></td>
</tr>
<tr>
<td><a href=https://www.semanticscholar.org/paper/7a951b4339fe7ab5beac4decd61c911fba4004f6>Want to Identify, Extract and Normalize Adverse Drug Reactions in Tweets? Use RoBERTa</a></td>
</tr>
<tr>
<td><a href=https://www.semanticscholar.org/paper/c218dbfb82aba4d67a1fd5faa7d4fe661a212ad4>FBK@SMM4H2020: RoBERTa for Detecting Medications on Twitter</a></td>
</tr>
<tr>
<td><a href=https://www.semanticscholar.org/paper/2bc3db214291ce277e541a8b37dcec9adcd3b92b>HR@JUST Team at SemEval-2020 Task 4: The Impact of RoBERTa Transformer for Evaluation Common Sense Understanding</a></td>
</tr>
<tr>
<td><a href=https://www.semanticscholar.org/paper/fa3fb07fe6ddf9ac1831799f189c47a74441c9c5>NLP@JUST at SemEval-2020 Task 4: Ensemble Technique for BERT and Roberta to Evaluate Commonsense Validation</a></td>
</tr>
<tr>
<td><a href=https://www.semanticscholar.org/paper/395de0bd3837fdf4b4b5e5f04835bcc69c279481>BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension</a></td>
</tr>
<tr>
<td><a href=https://www.semanticscholar.org/paper/93d63ec754f29fa22572615320afe0521f7ec66d>Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks</a></td>
</tr>
<tr>
<td><a href=https://www.semanticscholar.org/paper/71b6394ad5654f5cd0fba763768ba4e523f7bbca>Longformer: The Long-Document Transformer</a></td>
</tr>
<tr>
<td><a href=https://www.semanticscholar.org/paper/05f5f8b2065a520846d89771ebaea2bb1534e9c6>DeBERTa: Decoding-enhanced BERT with Disentangled Attention</a></td>
</tr>
<tr>
<td><a href=https://www.semanticscholar.org/paper/002c58077a1f1b296468b117230a1199e91f35c2>Black-Box Tuning for Language-Model-as-a-Service</a></td>
</tr>
<tr>
<td><a href=https://www.semanticscholar.org/paper/2ff41a463a374b138bb5a012e5a32bc4beefec20>Pre-Training With Whole Word Masking for Chinese BERT</a></td>
</tr>
<tr>
<td><a href=https://www.semanticscholar.org/paper/72cdd6ebe0221fb568ef20534f44ba5b35190a56>BERTweet: A pre-trained language model for English Tweets</a></td>
</tr>
<tr>
<td><a href=https://www.semanticscholar.org/paper/520bd2331cca8d5a9c032c186a2a0f7704ead6ff>R-Drop: Regularized Dropout for Neural Networks</a></td>
</tr>
<tr>
<td><a href=https://www.semanticscholar.org/paper/789b5441743c2e38cf4c38749ed820c0671d81b1>Muppet: Massive Multi-task Representations with Pre-Finetuning</a></td>
</tr>
<tr>
<td><a href=https://www.semanticscholar.org/paper/27b6acf6bbb8fcad1f6bf1b90331b49489bd5ff1>PromptBERT: Improving BERT Sentence Embeddings with Prompts</a></td>
</tr>
<tr>
<td><a href=https://www.semanticscholar.org/paper/063f8b1ecf2394ca776ac61869734de9c1953808>AdapterHub: A Framework for Adapting Transformers</a></td>
</tr>
<tr>
<td><a href=https://www.semanticscholar.org/paper/4f03e69963b9649950ba29ae864a0de8c14f1f86>K-Adapter: Infusing Knowledge into Pre-Trained Models with Adapters</a></td>
</tr>
<tr>
<td><a href=https://www.semanticscholar.org/paper/6afe0fb12ceacadbbfed7202d430770a3f344731>Revisiting Pre-Trained Models for Chinese Natural Language Processing</a></td>
</tr>
<tr>
<td><a href=https://www.semanticscholar.org/paper/35dffa79dc5c511d7cda7b6c25c0562eba3a5e0c>Supervised Contrastive Learning for Pre-trained Language Model Fine-tuning</a></td>
</tr>
<tr>
<td><a href=https://www.semanticscholar.org/paper/28459083ba624020c8f1c1ed7c3a075f48b4e709>KLUE: Korean Language Understanding Evaluation</a></td>
</tr>
<tr>
<td><a href=https://www.semanticscholar.org/paper/512f34906ddaefe885af2e5eec9b2b3b50ffd377>Deep entity matching with pre-trained language models</a></td>
</tr>
<tr>
<td><a href=https://www.semanticscholar.org/paper/eb606d9ce65139754232cee62f6ab77f3e0c665f>Leveraging Pre-trained Checkpoints for Sequence Generation Tasks</a></td>
</tr>
<tr>
<td><a href=https://www.semanticscholar.org/paper/2435c04832d486975304a094e55ecbab8acf8a5f>Fast, Effective, and Self-Supervised: Transforming Masked Language Models into Universal Lexical and Sentence Encoders</a></td>
</tr>
<tr>
<td><a href=https://www.semanticscholar.org/paper/8b9d77d5e52a70af37451d3db3d32781b83ea054>On the Stability of Fine-tuning BERT: Misconceptions, Explanations, and Strong Baselines</a></td>
</tr>
<tr>
<td><a href=https://www.semanticscholar.org/paper/d01fa0311e8e15b8b874b376123530c815f52852>FreeLB: Enhanced Adversarial Training for Natural Language Understanding</a></td>
</tr>
<tr>
<td><a href=https://www.semanticscholar.org/paper/1c332cfa211400fc6f56983fb01a6692046116dd>DynaBERT: Dynamic BERT with Adaptive Width and Depth</a></td>
</tr>
<tr>
<td><a href=https://www.semanticscholar.org/paper/8b2cbb2f101b025c16e12d0d7628f65e5378e10d>GraPPa: Grammar-Augmented Pre-Training for Table Semantic Parsing</a></td>
</tr>
<tr>
<td><a href=https://www.semanticscholar.org/paper/d1ccffb8eb1b7a99cd586723074b82fa5399bdd2>Transfer Learning Approaches for Building Cross-Language Dense Retrieval Models</a></td>
</tr>
<tr>
<td><a href=https://www.semanticscholar.org/paper/0cee58946a13a5c2845647b4af8b9d2bf52a8b6b>BOND: BERT-Assisted Open-Domain Named Entity Recognition with Distant Supervision</a></td>
</tr>
<tr>
<td><a href=https://www.semanticscholar.org/paper/9e594ae4ae9c38b6495810a8872f513ae19be29c>Intermediate-Task Transfer Learning with Pretrained Language Models: When and Why Does It Work?</a></td>
</tr>
<tr>
<td><a href=https://www.semanticscholar.org/paper/52f47e781852a77abedada48cfa971b24c919dde>Calibration of Pre-trained Transformers</a></td>
</tr>
<tr>
<td><a href=https://www.semanticscholar.org/paper/4099c4d272c12081b562392606e6d567e4ae7031>Masked Language Model Scoring</a></td>
</tr>
<tr>
<td><a href=https://www.semanticscholar.org/paper/9be1d1bf82f6ca6a7bf6a7d92f8f37b647e493d0>Probing Pretrained Language Models for Lexical Semantics</a></td>
</tr>
<tr>
<td><a href=https://www.semanticscholar.org/paper/77e09c4b9ffb3024af6af3002728ab254fb479b1>L3Cube-MahaHate: A Tweet-based Marathi Hate Speech Detection Dataset and BERT Models</a></td>
</tr>
<tr>
<td><a href=https://www.semanticscholar.org/paper/d4fb836846b79d8692df8bf54d20d1a9d02ffe7d>Debiasing Pre-Trained Language Models via Efficient Fine-Tuning</a></td>
</tr>
<tr>
<td><a href=https://www.semanticscholar.org/paper/5733233ea20498eba4afbe085038a067d4582f21>Syntax-BERT: Improving Pre-trained Transformers with Syntax Trees</a></td>
</tr>
<tr>
<td><a href=https://www.semanticscholar.org/paper/c553280c1fc1d0bc7b94683bb75910e309b0d579>Larger-Scale Transformers for Multilingual Masked Language Modeling</a></td>
</tr>
<tr>
<td><a href=https://www.semanticscholar.org/paper/faca71d01e3dd3379f4027176e8cf1f02d31c03c>NSP-BERT: A Prompt-based Few-Shot Learner through an Original Pre-training Task —— Next Sentence Prediction</a></td>
</tr>
<tr>
<td><a href=https://www.semanticscholar.org/paper/e54ffc76d805c48660bb0fd20019ca82ac94ba0d>Intrinsic Dimensionality Explains the Effectiveness of Language Model Fine-Tuning</a></td>
</tr>
<tr>
<td><a href=https://www.semanticscholar.org/paper/96fb0630238edd4a40e3497236bce711cf66b3be>Mono vs Multilingual BERT for Hate Speech Detection and Text Classification: A Case Study in Marathi</a></td>
</tr>
<tr>
<td><a href=https://www.semanticscholar.org/paper/09171a3f87fcb94456eaabefc65731683374f983>Outliers Dimensions that Disrupt Transformers Are Driven by Frequency</a></td>
</tr>
<tr>
<td><a href=https://www.semanticscholar.org/paper/5e0cffc51e8b64a8f11326f955fa4b4f1803e3be>oLMpics-On What Language Model Pre-training Captures</a></td>
</tr>
<tr>
<td><a href=https://www.semanticscholar.org/paper/15c9bbc6de95fbff176b6cb76530785146da81eb>A Computational Approach to Understanding Empathy Expressed in Text-Based Mental Health Support</a></td>
</tr>
<tr>
<td><a href=https://www.semanticscholar.org/paper/465491b0507e107f248a8277ac17248c2ff8f915>BERT is to NLP what AlexNet is to CV: Can Pre-Trained Language Models Identify Analogies?</a></td>
</tr>
<tr>
<td><a href=https://www.semanticscholar.org/paper/64452abf42e272b64a45c9469775b728a72c6157>Hyperbolic Relevance Matching for Neural Keyphrase Extraction</a></td>
</tr>
<tr>
<td><a href=https://www.semanticscholar.org/paper/b1c0307b49e91fd639d65f0d2370eb4bc1f5bd5f>Nozza@LT-EDI-ACL2022: Ensemble Modeling for Homophobia and Transphobia Detection</a></td>
</tr>
<tr>
<td><a href=https://www.semanticscholar.org/paper/3b451fa663704f927e1ec602d7c0845a9826922d>Evaluating the Robustness of Neural Language Models to Input Perturbations</a></td>
</tr>
<tr>
<td><a href=https://www.semanticscholar.org/paper/d56c1fc337fb07ec004dc846f80582c327af717c>StructBERT: Incorporating Language Structures into Pre-training for Deep Language Understanding</a></td>
</tr>
<tr>
<td><a href=https://www.semanticscholar.org/paper/bec60a7f7d6e72a23a6d3d3c90a6936103c7873c>An Empirical Study on the Usage of BERT Models for Code Completion</a></td>
</tr>
<tr>
<td><a href=https://www.semanticscholar.org/paper/0de580957d23dd65e31b6c95e6bc5d15bc15c57d>Impact of Tokenization on Language Models: An Analysis for Turkish</a></td>
</tr>
<tr>
<td><a href=https://www.semanticscholar.org/paper/94c35b9784d1a37bf1cd5ae65db9315345ebfcb7>RETRACTED ARTICLE: An ensemble deep learning classifier for sentiment analysis on code-mix Hindi–English data</a></td>
</tr>
<tr>
<td><a href=https://www.semanticscholar.org/paper/a321d1ec561a23512a5aa687c0d89c971bb5687b>Self-training Improves Pre-training for Natural Language Understanding</a></td>
</tr>
<tr>
<td><a href=https://www.semanticscholar.org/paper/e8d84c7c098f087eb935ff5eee87e0e585d6c758>MultiEURLEX - A multi-lingual and multi-label legal document classification dataset for zero-shot cross-lingual transfer</a></td>
</tr>
<tr>
<td><a href=https://www.semanticscholar.org/paper/dfd104dd0ff28b1bde2fbd4c4d6d3ccb4761f639>Learning Compact Metrics for MT</a></td>
</tr>
<tr>
<td><a href=https://www.semanticscholar.org/paper/016760dc4a05489ddf5dbb48aecbb49e214e1b71>Birds Have Four Legs?! NumerSense: Probing Numerical Commonsense Knowledge of Pre-trained Language Models</a></td>
</tr>
<tr>
<td><a href=https://www.semanticscholar.org/paper/2b9955bc08fc5f4ddba73082ddabcfaabdbb4416>Poor Man's BERT: Smaller and Faster Transformer Models</a></td>
</tr>
<tr>
<td><a href=https://www.semanticscholar.org/paper/4221152ca44ed9686496531685bc022bd521fd11>Deep Continuous Prompt for Contrastive Learning of Sentence Embeddings</a></td>
</tr>
<tr>
<td><a href=https://www.semanticscholar.org/paper/82159bec7a4f9e399af1b632d56bb7f98439810e>KOLD: Korean Offensive Language Dataset</a></td>
</tr>
</table></html>
