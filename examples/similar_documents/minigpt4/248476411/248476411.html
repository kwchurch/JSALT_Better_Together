<html><table><tr>
<th>Method</th>
<th>cosS</th>
<th>cosP</th>
<th>paper</th>
</tr>
<tr>
<td>Specter</td>
<td>1.000</td>
<td>1.000</td>
<td><a href="https://www.semanticscholar.org/paper/cd71c96e05068b26e8f83e6c61a6a239685e943a">7: Flamingo: a Visual Language Model for Few-Shot Learning</a></td>
</tr>
<tr>
<td>Specter</td>
<td>0.841</td>
<td>0.863</td>
<td><a href="https://www.semanticscholar.org/paper/616e0ed02ca024a8c1d4b86167f7486ea92a13d9">8: VisualGPT: Data-efficient Adaptation of Pretrained Language Models for Image Captioning</a></td>
</tr>
<tr>
<td>Specter</td>
<td>0.840</td>
<td>0.516</td>
<td><a href="https://www.semanticscholar.org/paper/16aac81ae033f7295d82e5b679400d105170a3e1">8: Oracle Performance for Visual Captioning</a></td>
</tr>
<tr>
<td>Specter</td>
<td>0.839</td>
<td>0.482</td>
<td><a href="https://www.semanticscholar.org/paper/117aae1dc5b3aee679a690f7dab84e9a23add930">0: AGE AND VIDEO CAPTIONING</a></td>
</tr>
<tr>
<td>Specter</td>
<td>0.837</td>
<td>0.940</td>
<td><a href="https://www.semanticscholar.org/paper/c04067f03fba2df0c14ea51a170f213eb2983708">20: CLIP-Adapter: Better Vision-Language Models with Feature Adapters</a></td>
</tr>
<tr>
<td>Specter</td>
<td>0.833</td>
<td>0.761</td>
<td><a href="https://www.semanticscholar.org/paper/5b492d4ad0a779091d531446e1ec384280d235d0">0: Fusion Models for Improved Visual Captioning</a></td>
</tr>
<tr>
<td>Specter</td>
<td>0.833</td>
<td>0.960</td>
<td><a href="https://www.semanticscholar.org/paper/dab79db635f83640a528c36b854094acad1afdc1">5: IGLUE: A Benchmark for Transfer Learning across Modalities, Tasks, and Languages</a></td>
</tr>
<tr>
<td>Specter</td>
<td>0.833</td>
<td>0.981</td>
<td><a href="https://www.semanticscholar.org/paper/b7231a746850d9cb98b7762ba151a456404a54e0">3: A Survey of Vision-Language Pre-Trained Models</a></td>
</tr>
<tr>
<td>Specter</td>
<td>0.831</td>
<td>0.902</td>
<td><a href="https://www.semanticscholar.org/paper/9405cc0d6169988371b2755e573cc28650d14dfe">5985: Language Models are Unsupervised Multitask Learners</a></td>
</tr>
<tr>
<td>Specter</td>
<td>0.829</td>
<td>0.845</td>
<td><a href="https://www.semanticscholar.org/paper/e3b100e5a8847c150985af5ccc838220b2d30e35">2: AdapterHub Playground: Simple and Flexible Few-Shot Learning with Adapters</a></td>
</tr>
<tr>
<td>Specter</td>
<td>0.829</td>
<td>0.948</td>
<td><a href="https://www.semanticscholar.org/paper/63c74d15940af1af9b386b5762e4445e54c73719">131: VinVL: Revisiting Visual Representations in Vision-Language Models</a></td>
</tr>
<tr>
<td>Specter</td>
<td>0.826</td>
<td>0.817</td>
<td><a href="https://www.semanticscholar.org/paper/e4c95438e8668f19e6d00fa794aad67c60b364ef">1: Auto-Parsing Network for Image Captioning and Visual Question Answering</a></td>
</tr>
<tr>
<td>Specter</td>
<td>0.824</td>
<td>0.847</td>
<td><a href="https://www.semanticscholar.org/paper/17c0df95e608f91c2385e42c629a65f095988c10">26: Shaping Visual Representations with Language for Few-Shot Classification</a></td>
</tr>
<tr>
<td>Specter</td>
<td>0.823</td>
<td>0.639</td>
<td><a href="https://www.semanticscholar.org/paper/c2ce1912727a3e8c88b4af65c9ca088b3c8eb1a0">0: Vision and Language Learning: From Image Captioning and Visual Question Answering towards Embodied Agents</a></td>
</tr>
<tr>
<td>Specter</td>
<td>0.822</td>
<td>0.960</td>
<td><a href="https://www.semanticscholar.org/paper/c05cd00ae61f3c1c39be2603a2f96fdfe0c59dd8">5: UFO: A UniFied TransfOrmer for Vision-Language Representation Learning</a></td>
</tr>
<tr>
<td>Specter</td>
<td>0.821</td>
<td>0.875</td>
<td><a href="https://www.semanticscholar.org/paper/9de403a58395a1b56bfceee6e009788c43db6d08">270: End-to-End Learning of Visual Representations From Uncurated Instructional Videos</a></td>
</tr>
<tr>
<td>Specter</td>
<td>0.819</td>
<td>0.950</td>
<td><a href="https://www.semanticscholar.org/paper/403fcde5c36533036e7f29705221376d80dd1dba">0: Super-Prompting: Utilizing Model-Independent Contextual Data to Reduce Data Annotation Required in Visual Commonsense Tasks</a></td>
</tr>
<tr>
<td>Specter</td>
<td>0.810</td>
<td>0.917</td>
<td><a href="https://www.semanticscholar.org/paper/474736f4f97671f6ab604fa940ffc23d41cbc4b6">1: PSP: Pre-trained Soft Prompts for Few-Shot Abstractive Summarization</a></td>
</tr>
<tr>
<td>Specter</td>
<td>0.807</td>
<td>0.786</td>
<td><a href="https://www.semanticscholar.org/paper/1e1bd132613866c176a8fc780cb1b9f9aa43feeb">17: Video Question Answering with Spatio-Temporal Reasoning</a></td>
</tr>
<tr>
<td>Specter</td>
<td>0.807</td>
<td>0.579</td>
<td><a href="https://www.semanticscholar.org/paper/cfe95e9dca1f07db5f0672c65943f22aace00e04">16: CaptionNet: Automatic End-to-End Siamese Difference Captioning Model With Attention</a></td>
</tr>
<tr>
<td>Proposed</td>
<td>1.000</td>
<td>1.000</td>
<td><a href="https://www.semanticscholar.org/paper/cd71c96e05068b26e8f83e6c61a6a239685e943a">7: Flamingo: a Visual Language Model for Few-Shot Learning</a></td>
</tr>
<tr>
<td>Proposed</td>
<td>0.735</td>
<td>0.994</td>
<td><a href="https://www.semanticscholar.org/paper/2fd6f77540c1cc8e70b96208ccf9971b4251fc02">16: FLAVA: A Foundational Language And Vision Alignment Model</a></td>
</tr>
<tr>
<td>Proposed</td>
<td>0.835</td>
<td>0.990</td>
<td><a href="https://www.semanticscholar.org/paper/55a19318cc93714802c7ac59e07651789749b20c">7: VL-Adapter: Parameter-Efficient Transfer Learning for Vision-and-Language Tasks</a></td>
</tr>
<tr>
<td>Proposed</td>
<td>0.817</td>
<td>0.989</td>
<td><a href="https://www.semanticscholar.org/paper/400d619cbabeb669115bb7281a889ab869829ef5">6: MERLOT Reserve: Neural Script Knowledge through Vision and Language and Sound</a></td>
</tr>
<tr>
<td>Proposed</td>
<td>0.850</td>
<td>0.989</td>
<td><a href="https://www.semanticscholar.org/paper/7f71875f8214dffa4f3276da123c4990a6d437cc">1: Enabling Multimodal Generation on CLIP via Vision-Language Knowledge Distillation</a></td>
</tr>
<tr>
<td>Proposed</td>
<td>0.788</td>
<td>0.989</td>
<td><a href="https://www.semanticscholar.org/paper/43bc25ccd3ae452672a253016cebda5f9f7550fd">0: Distilled Dual-Encoder Model for Vision-Language Understanding</a></td>
</tr>
<tr>
<td>Proposed</td>
<td>0.806</td>
<td>0.989</td>
<td><a href="https://www.semanticscholar.org/paper/234f61268f3019f7d9e2fffae1fabd039a70ceec">16: VLMo: Unified Vision-Language Pre-Training with Mixture-of-Modality-Experts</a></td>
</tr>
<tr>
<td>Proposed</td>
<td>0.836</td>
<td>0.988</td>
<td><a href="https://www.semanticscholar.org/paper/a3b42a83669998f65df60d7c065a70d07ca95e99">13: BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation</a></td>
</tr>
<tr>
<td>Proposed</td>
<td>0.766</td>
<td>0.988</td>
<td><a href="https://www.semanticscholar.org/paper/94ff111c4d81bd03f159321728ceec8b4711c89d">16: An Empirical Study of Training End-to-End Vision-and-Language Transformers</a></td>
</tr>
<tr>
<td>Proposed</td>
<td>0.732</td>
<td>0.988</td>
<td><a href="https://www.semanticscholar.org/paper/91dc75f94da13452a54ad5c03fab2c5fda87e9ba">5: Uni-Perceiver: Pre-training Unified Architecture for Generic Perception for Zero-shot and Few-shot Tasks</a></td>
</tr>
<tr>
<td>Proposed</td>
<td>0.774</td>
<td>0.986</td>
<td><a href="https://www.semanticscholar.org/paper/e10251e7a50307744068120c73a4e8faf76c4e9f">0: Zero and R2D2: A Large-scale Chinese Cross-modal Benchmark and A Vision-Language Framework</a></td>
</tr>
<tr>
<td>Proposed</td>
<td>0.797</td>
<td>0.986</td>
<td><a href="https://www.semanticscholar.org/paper/4d42fc9c9dbdeadc8772ba3a00ac9231bfbbe2fb">0: Multimodal Adaptive Distillation for Leveraging Unimodal Encoders for Vision-Language Tasks</a></td>
</tr>
<tr>
<td>Proposed</td>
<td>0.773</td>
<td>0.986</td>
<td><a href="https://www.semanticscholar.org/paper/5e5fbc41106db9acaaf3a365801051e477f0e984">69: UNIMO: Towards Unified-Modal Understanding and Generation via Cross-Modal Contrastive Learning</a></td>
</tr>
<tr>
<td>Proposed</td>
<td>0.844</td>
<td>0.986</td>
<td><a href="https://www.semanticscholar.org/paper/80e2886a8c581dc9e628b11d89be95b3cc29204c">0: Language Models with Image Descriptors are Strong Few-Shot Video-Language Learners</a></td>
</tr>
<tr>
<td>Proposed</td>
<td>0.892</td>
<td>0.986</td>
<td><a href="https://www.semanticscholar.org/paper/01b5412f3d17e90e09226d7c40ad4d4468a1414d">62: Multimodal Few-Shot Learning with Frozen Language Models</a></td>
</tr>
<tr>
<td>Proposed</td>
<td>0.746</td>
<td>0.986</td>
<td><a href="https://www.semanticscholar.org/paper/e75ef4de1117db773b02b901f6d19a2ffd36a84a">2: Understanding Chinese Video and Language via Contrastive Multimodal Pre-Training</a></td>
</tr>
<tr>
<td>Proposed</td>
<td>0.771</td>
<td>0.986</td>
<td><a href="https://www.semanticscholar.org/paper/f675c62abfa788ea0be85d3124eba15a14d5e9d6">24: FILIP: Fine-grained Interactive Language-Image Pre-Training</a></td>
</tr>
<tr>
<td>Proposed</td>
<td>0.809</td>
<td>0.986</td>
<td><a href="https://www.semanticscholar.org/paper/6065fe83bbab89763e1637a16d64676bbda6b6bd">2: Unified Multimodal Pre-training and Prompt-based Tuning for Vision-Language Understanding and Generation</a></td>
</tr>
<tr>
<td>Proposed</td>
<td>0.699</td>
<td>0.985</td>
<td><a href="https://www.semanticscholar.org/paper/ae28b4edea7b3db37ffd0ebba7c54478a9b5e3ab">0: LoopITR: Combining Dual and Cross Encoder Architectures for Image-Text Retrieval</a></td>
</tr>
<tr>
<td>Proposed</td>
<td>0.831</td>
<td>0.985</td>
<td><a href="https://www.semanticscholar.org/paper/26ed89bff49545f8c6ddfd05ba9a9c59c926d322">5: MAGMA - Multimodal Augmentation of Generative Models through Adapter-based Finetuning</a></td>
</tr>
</table></html>
