<html><table><tr>
<th>Method</th>
<th>cosS</th>
<th>cosP</th>
<th>paper</th>
</tr>
<tr>
<td>Specter</td>
<td>1.000</td>
<td>1.000</td>
<td><a href="https://www.semanticscholar.org/paper/d5caec8107da41ec71fc0bb36d60fc2d8834846e">226: Meshed-Memory Transformer for Image Captioning</a></td>
</tr>
<tr>
<td>Specter</td>
<td>0.984</td>
<td>0.975</td>
<td><a href="https://www.semanticscholar.org/paper/261570379fd841b426a4c51e8004f2cf9f1df771">13: M2: Meshed-Memory Transformer for Image Captioning</a></td>
</tr>
<tr>
<td>Specter</td>
<td>0.836</td>
<td>0.807</td>
<td><a href="https://www.semanticscholar.org/paper/f15f92647a02e526c02c1b61bf8bcc5d6b206e99">7: Adaptive Syncretic Attention for Constrained Image Captioning</a></td>
</tr>
<tr>
<td>Specter</td>
<td>0.834</td>
<td>0.910</td>
<td><a href="https://www.semanticscholar.org/paper/e5c6987ca61d770b5b576f1c40ab7da7baac2c8d">0: Enhanced Modality Transition for Image Captioning</a></td>
</tr>
<tr>
<td>Specter</td>
<td>0.833</td>
<td>0.856</td>
<td><a href="https://www.semanticscholar.org/paper/48f17b2b08aebd16e711f5c7ca9e773fe6639dc3">5: Reference-based model using multimodal gated recurrent units for image captioning</a></td>
</tr>
<tr>
<td>Specter</td>
<td>0.812</td>
<td>0.853</td>
<td><a href="https://www.semanticscholar.org/paper/1fac7ac22af5db06edde0fbda2bc61a97c7c9625">4: Cross-Lingual Image Caption Generation Based on Visual Attention Model</a></td>
</tr>
<tr>
<td>Specter</td>
<td>0.807</td>
<td>0.868</td>
<td><a href="https://www.semanticscholar.org/paper/50f6dd2aa07074d2904f153a0e489285499436c1">6: Unifying Multimodal Transformer for Bi-directional Image and Text Generation</a></td>
</tr>
<tr>
<td>Specter</td>
<td>0.804</td>
<td>0.983</td>
<td><a href="https://www.semanticscholar.org/paper/cc03c6a4160ad133faeb1b6c49d24432c7a896aa">26: CPTR: Full Transformer Network for Image Captioning</a></td>
</tr>
<tr>
<td>Specter</td>
<td>0.800</td>
<td>0.839</td>
<td><a href="https://www.semanticscholar.org/paper/a5ff02a42c1ace31b9ab8d42a2e8e0c678c1e8d6">3: SibNet: Sibling Convolutional Encoder for Video Captioning</a></td>
</tr>
<tr>
<td>Specter</td>
<td>0.799</td>
<td>0.665</td>
<td><a href="https://www.semanticscholar.org/paper/e1162f3f69917fcc13a0b5c4aec556e893021d11">0: OctaNLP: A Benchmark for Evaluating Multitask Generalization of Transformer-Based Pre-trained Language Models</a></td>
</tr>
<tr>
<td>Specter</td>
<td>0.798</td>
<td>0.856</td>
<td><a href="https://www.semanticscholar.org/paper/11a8cc3fa18ab4f7158447cc1fc8800489e82f9c">0: AttResNet: Attention-based ResNet for Image Captioning</a></td>
</tr>
<tr>
<td>Specter</td>
<td>0.797</td>
<td>0.863</td>
<td><a href="https://www.semanticscholar.org/paper/01c4cbf23457b8c650d3487096a90f695b9177cf">3: Multi-Pass Transformer for Machine Translation</a></td>
</tr>
<tr>
<td>Specter</td>
<td>0.796</td>
<td>0.755</td>
<td><a href="https://www.semanticscholar.org/paper/a3053db1016d039757d072f1ae85735d8a21478f">20: Order embeddings and character-level convolutions for multimodal alignment</a></td>
</tr>
<tr>
<td>Specter</td>
<td>0.794</td>
<td>0.828</td>
<td><a href="https://www.semanticscholar.org/paper/61257bd6109ebf218135cf69c7f53904782afd40">0: Towards Efficient and Elastic Visual Question Answering with Doubly Slimmable Transformer</a></td>
</tr>
<tr>
<td>Specter</td>
<td>0.793</td>
<td>0.946</td>
<td><a href="https://www.semanticscholar.org/paper/f257fd73880dfaa119b474be4ef05856ee698d6f">0: Backpropagation-Based Decoding for Multimodal Machine Translation</a></td>
</tr>
<tr>
<td>Specter</td>
<td>0.793</td>
<td>0.898</td>
<td><a href="https://www.semanticscholar.org/paper/50282fd5ceb8bed77b176bd4f2bfd957a2c024be">0: Switchable Novel Object Captioner.</a></td>
</tr>
<tr>
<td>Specter</td>
<td>0.791</td>
<td>0.950</td>
<td><a href="https://www.semanticscholar.org/paper/0574dc64c8275b09ed587dc3977f4d3c990bd4df">73: Context-Aware Visual Policy Network for Sequence-Level Image Captioning</a></td>
</tr>
<tr>
<td>Specter</td>
<td>0.790</td>
<td>0.884</td>
<td><a href="https://www.semanticscholar.org/paper/20888a7aebaf77a306c0886f165bd0d468db806d">91: Spatio-Temporal Dynamics and Semantic Attribute Enriched Visual Encoding for Video Captioning</a></td>
</tr>
<tr>
<td>Specter</td>
<td>0.789</td>
<td>0.945</td>
<td><a href="https://www.semanticscholar.org/paper/c0c571007d8c0fabf5aec16fbe8105f036fd1252">0: Label-Attention Transformer with Geometrically Coherent Objects for Image Captioning</a></td>
</tr>
<tr>
<td>Specter</td>
<td>0.784</td>
<td>0.819</td>
<td><a href="https://www.semanticscholar.org/paper/0f4a3b7f835737c7e004073f4a873e356c0063b4">29: Incorporating Source Syntax into Transformer-Based Neural Machine Translation</a></td>
</tr>
<tr>
<td>Proposed</td>
<td>1.000</td>
<td>1.000</td>
<td><a href="https://www.semanticscholar.org/paper/d5caec8107da41ec71fc0bb36d60fc2d8834846e">226: Meshed-Memory Transformer for Image Captioning</a></td>
</tr>
<tr>
<td>Proposed</td>
<td>0.755</td>
<td>0.996</td>
<td><a href="https://www.semanticscholar.org/paper/4c163d4942117179d3e97182e1b280027d7d60a9">292: Attention on Attention for Image Captioning</a></td>
</tr>
<tr>
<td>Proposed</td>
<td>0.726</td>
<td>0.996</td>
<td><a href="https://www.semanticscholar.org/paper/b499228aa74b59be32711c3926e44de208d6b636">136: Image Captioning: Transforming Objects into Words</a></td>
</tr>
<tr>
<td>Proposed</td>
<td>0.762</td>
<td>0.993</td>
<td><a href="https://www.semanticscholar.org/paper/833560cd68a3e3d1be1bc650756dd6c679798551">48: Normalized and Geometry-Aware Self-Attention Network for Image Captioning</a></td>
</tr>
<tr>
<td>Proposed</td>
<td>0.831</td>
<td>0.992</td>
<td><a href="https://www.semanticscholar.org/paper/7c4530882cfcef1d2b4aa2996f494dfac626b5d9">105: Entangled Transformer for Image Captioning</a></td>
</tr>
<tr>
<td>Proposed</td>
<td>0.734</td>
<td>0.992</td>
<td><a href="https://www.semanticscholar.org/paper/4adfa7b83342b77c830f2b0f6fc1b784c21e7ed0">142: X-Linear Attention Networks for Image Captioning</a></td>
</tr>
<tr>
<td>Proposed</td>
<td>0.724</td>
<td>0.989</td>
<td><a href="https://www.semanticscholar.org/paper/b1293397b43fe1a97957132c245199bdb61016cb">40: Aligning Visual Regions and Textual Concepts for Semantic-Grounded Image Representations</a></td>
</tr>
<tr>
<td>Proposed</td>
<td>0.822</td>
<td>0.989</td>
<td><a href="https://www.semanticscholar.org/paper/625489eb6c1878dbde50f2ff9776c2e8152b11df">0: CaMEL: Mean Teacher Learning for Image Captioning</a></td>
</tr>
<tr>
<td>Proposed</td>
<td>0.776</td>
<td>0.988</td>
<td><a href="https://www.semanticscholar.org/paper/ae7e5a4de962ca4face3bb52b36dfd09db5451d8">18: Dual-Level Collaborative Transformer for Image Captioning</a></td>
</tr>
<tr>
<td>Proposed</td>
<td>0.606</td>
<td>0.986</td>
<td><a href="https://www.semanticscholar.org/paper/5cac47b57b9ba4d92e5579b3f53f6eda88678b86">1: “I’ve Seen Things You People Wouldn’t Believe”: Hallucinating Entities in GuessWhat?!</a></td>
</tr>
<tr>
<td>Proposed</td>
<td>0.793</td>
<td>0.986</td>
<td><a href="https://www.semanticscholar.org/paper/7217b5d8d0fb753532026cc36b0aaa056960c6f8">19: Improving Image Captioning by Leveraging Intra- and Inter-layer Global Representation in Transformer Network</a></td>
</tr>
<tr>
<td>Proposed</td>
<td>0.731</td>
<td>0.986</td>
<td><a href="https://www.semanticscholar.org/paper/a82c1d1ccaa3a3d1d6ee6677de0eed2e93ddb6e8">2205: Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering</a></td>
</tr>
<tr>
<td>Proposed</td>
<td>0.796</td>
<td>0.986</td>
<td><a href="https://www.semanticscholar.org/paper/1e1cb3a5d3d8b80c2a523189246f711686dda669">1: Semi-Autoregressive Transformer for Image Captioning</a></td>
</tr>
<tr>
<td>Proposed</td>
<td>0.684</td>
<td>0.985</td>
<td><a href="https://www.semanticscholar.org/paper/4921243268c81d0d6db99053a9d004852225a622">97: Object Hallucination in Image Captioning</a></td>
</tr>
<tr>
<td>Proposed</td>
<td>0.738</td>
<td>0.983</td>
<td><a href="https://www.semanticscholar.org/paper/17e695d7b00600e0fd6599e1d7703d9f76e796e8">7: Towards Accurate Text-based Image Captioning with Content Diversity Exploration</a></td>
</tr>
<tr>
<td>Proposed</td>
<td>0.804</td>
<td>0.983</td>
<td><a href="https://www.semanticscholar.org/paper/cc03c6a4160ad133faeb1b6c49d24432c7a896aa">26: CPTR: Full Transformer Network for Image Captioning</a></td>
</tr>
<tr>
<td>Proposed</td>
<td>0.722</td>
<td>0.982</td>
<td><a href="https://www.semanticscholar.org/paper/01da70bed0df540e2ae10645855b33fd468128e3">0: Question-controlled Text-aware Image Captioning</a></td>
</tr>
<tr>
<td>Proposed</td>
<td>0.780</td>
<td>0.982</td>
<td><a href="https://www.semanticscholar.org/paper/59d65f0719287512f3f605615f64b7eda27db97b">1: Modeling Local and Global Contexts for Image Captioning</a></td>
</tr>
<tr>
<td>Proposed</td>
<td>0.811</td>
<td>0.982</td>
<td><a href="https://www.semanticscholar.org/paper/23903b4c42fdbea0b7b35e3157b48d8dfd18e1a5">0: End-to-End Transformer Based Model for Image Captioning</a></td>
</tr>
<tr>
<td>Proposed</td>
<td>0.749</td>
<td>0.980</td>
<td><a href="https://www.semanticscholar.org/paper/1ba0be712e5fa5fae0b7821909ecb5f5588accc4">0: Supplementary Material for “ Semi-Autoregressive Transformer for Image Captioning ” 1</a></td>
</tr>
</table></html>
