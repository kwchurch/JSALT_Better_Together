<html><table><tr>
<th>Method</th>
<th>cosS</th>
<th>cosP</th>
<th>paper</th>
</tr>
<tr>
<td>Specter</td>
<td>1.000</td>
<td>1.000</td>
<td><a href="https://www.semanticscholar.org/paper/a3b42a83669998f65df60d7c065a70d07ca95e99">13: BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation</a></td>
</tr>
<tr>
<td>Specter</td>
<td>0.907</td>
<td>0.985</td>
<td><a href="https://www.semanticscholar.org/paper/6648b4db5f12c30941ea78c695e77aded19672bb">330: Unified Vision-Language Pre-Training for Image Captioning and VQA</a></td>
</tr>
<tr>
<td>Specter</td>
<td>0.873</td>
<td>0.891</td>
<td><a href="https://www.semanticscholar.org/paper/0594bf579eae7f8a69842896b7d8f945a8ee77b0">0: Unsupervised Prompt Learning for Vision-Language Models</a></td>
</tr>
<tr>
<td>Specter</td>
<td>0.864</td>
<td>0.992</td>
<td><a href="https://www.semanticscholar.org/paper/7f71875f8214dffa4f3276da123c4990a6d437cc">1: Enabling Multimodal Generation on CLIP via Vision-Language Knowledge Distillation</a></td>
</tr>
<tr>
<td>Specter</td>
<td>0.850</td>
<td>0.980</td>
<td><a href="https://www.semanticscholar.org/paper/8f167ec1149921fac63b1ea855443de109bb013a">56: How Much Can CLIP Benefit Vision-and-Language Tasks?</a></td>
</tr>
<tr>
<td>Specter</td>
<td>0.843</td>
<td>0.984</td>
<td><a href="https://www.semanticscholar.org/paper/ac13afe6c5f456a1f250e03e3258f5cfecc99373">0: Bridging the Gap between Recognition-level Pre-training and Commonsensical Vision-language Tasks</a></td>
</tr>
<tr>
<td>Specter</td>
<td>0.838</td>
<td>0.863</td>
<td><a href="https://www.semanticscholar.org/paper/b0181353f32b1ad3ac6bc59838c69b0e5c64137a">51: Learning Visual Representations with Caption Annotations</a></td>
</tr>
<tr>
<td>Specter</td>
<td>0.828</td>
<td>0.973</td>
<td><a href="https://www.semanticscholar.org/paper/f5aaf9f38ba6e00f1645beab6b83176452322cef">0: PEVL: Position-enhanced Pre-training and Prompt Tuning for Vision-language Models</a></td>
</tr>
<tr>
<td>Specter</td>
<td>0.823</td>
<td>0.529</td>
<td><a href="https://www.semanticscholar.org/paper/848602d3de1b1ab9a06146e8b8f3f836cacbce91">28: Neural Image Caption Generation with Weighted Training and Reference</a></td>
</tr>
<tr>
<td>Specter</td>
<td>0.817</td>
<td>0.592</td>
<td><a href="https://www.semanticscholar.org/paper/2df61fcd01b6a70a94dff2b25d6ed8dc4c16e422">3: The role of image representations in vision to language tasks</a></td>
</tr>
<tr>
<td>Specter</td>
<td>0.809</td>
<td>0.739</td>
<td><a href="https://www.semanticscholar.org/paper/f96e4732cd4eb66b0dc0d6e912b0c894d6bf0e35">0: Saliency-based Multi-View Mixed Language Training for Zero-shot Cross-lingual Classification</a></td>
</tr>
<tr>
<td>Specter</td>
<td>0.808</td>
<td>0.848</td>
<td><a href="https://www.semanticscholar.org/paper/181473adfd2e977a5ac25d9f716f88cb08c85db2">1: Towards Language-guided Visual Recognition via Dynamic Convolutions</a></td>
</tr>
<tr>
<td>Specter</td>
<td>0.801</td>
<td>0.918</td>
<td><a href="https://www.semanticscholar.org/paper/c0731ec306b3182834a55f85b712c9d6e1538529">0: Probing Multi-modal Machine Translation with Pre-trained Language Model</a></td>
</tr>
<tr>
<td>Specter</td>
<td>0.798</td>
<td>0.527</td>
<td><a href="https://www.semanticscholar.org/paper/d76ac69266185152274dcafc0b5b7d6f31a8965a">2: Parallel Image Captioning Using 2D Masked Convolution</a></td>
</tr>
<tr>
<td>Specter</td>
<td>0.797</td>
<td>-0.021</td>
<td><a href="https://www.semanticscholar.org/paper/16e77ec987bf29a11d2acf80f307a5ba94d7c807">3: Map and Relabel: Towards Almost-Zero Resource Speech Recognition</a></td>
</tr>
<tr>
<td>Specter</td>
<td>0.796</td>
<td>0.838</td>
<td><a href="https://www.semanticscholar.org/paper/077c713bccd9d2c7fde68d4cbde06ab0f07a6855">66: ConSERT: A Contrastive Framework for Self-Supervised Sentence Representation Transfer</a></td>
</tr>
<tr>
<td>Specter</td>
<td>0.794</td>
<td>0.777</td>
<td><a href="https://www.semanticscholar.org/paper/a89cd9056c0fb037d659215b121686ff3b454fd5">38: Align2Ground: Weakly Supervised Phrase Grounding Guided by Image-Caption Alignment</a></td>
</tr>
<tr>
<td>Specter</td>
<td>0.792</td>
<td>0.793</td>
<td><a href="https://www.semanticscholar.org/paper/3977c0056755c2911811509ac38e0cef532004b0">1: Self-Distillation for Few-Shot Image Captioning</a></td>
</tr>
<tr>
<td>Specter</td>
<td>0.791</td>
<td>0.840</td>
<td><a href="https://www.semanticscholar.org/paper/10efdde1ae3a9d359ac1aae0bd5ef7bfd68810dd">9: DICT-MLM: Improved Multilingual Pre-Training using Bilingual Dictionaries</a></td>
</tr>
<tr>
<td>Specter</td>
<td>0.791</td>
<td>0.653</td>
<td><a href="https://www.semanticscholar.org/paper/dec69765fc6c188897b09c8282d32db788e2c261">14: Improving Pre-Trained Multilingual Model with Vocabulary Expansion</a></td>
</tr>
<tr>
<td>Proposed</td>
<td>1.000</td>
<td>1.000</td>
<td><a href="https://www.semanticscholar.org/paper/a3b42a83669998f65df60d7c065a70d07ca95e99">13: BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation</a></td>
</tr>
<tr>
<td>Proposed</td>
<td>0.854</td>
<td>0.997</td>
<td><a href="https://www.semanticscholar.org/paper/b7231a746850d9cb98b7762ba151a456404a54e0">3: A Survey of Vision-Language Pre-Trained Models</a></td>
</tr>
<tr>
<td>Proposed</td>
<td>0.811</td>
<td>0.995</td>
<td><a href="https://www.semanticscholar.org/paper/a9fd5511b42206a27748f373e0fdb7eb76a23055">94: ImageBERT: Cross-modal Pre-training with Large-scale Weak-supervised Image-Text Data</a></td>
</tr>
<tr>
<td>Proposed</td>
<td>0.573</td>
<td>0.995</td>
<td><a href="https://www.semanticscholar.org/paper/5e00596fa946670d894b1bdaeff5a98e3867ef13">72: SimVLM: Simple Visual Language Model Pretraining with Weak Supervision</a></td>
</tr>
<tr>
<td>Proposed</td>
<td>0.801</td>
<td>0.995</td>
<td><a href="https://www.semanticscholar.org/paper/94ff111c4d81bd03f159321728ceec8b4711c89d">16: An Empirical Study of Training End-to-End Vision-and-Language Transformers</a></td>
</tr>
<tr>
<td>Proposed</td>
<td>0.670</td>
<td>0.995</td>
<td><a href="https://www.semanticscholar.org/paper/aa8c4d9b9df3b5978cc8588f948c1341f2dd3614">1: CommerceMM: Large-Scale Commerce MultiModal Representation Learning with Omni Retrieval</a></td>
</tr>
<tr>
<td>Proposed</td>
<td>0.738</td>
<td>0.995</td>
<td><a href="https://www.semanticscholar.org/paper/598a2ee223e2949c3b28389e922c1892b4717d2a">133: Pixel-BERT: Aligning Image Pixels with Text by Deep Multi-Modal Transformers</a></td>
</tr>
<tr>
<td>Proposed</td>
<td>0.857</td>
<td>0.994</td>
<td><a href="https://www.semanticscholar.org/paper/3a7a6f5db53df223b5ae719209805a9b04164c07">1: VLP: A Survey on Vision-Language Pre-training</a></td>
</tr>
<tr>
<td>Proposed</td>
<td>0.836</td>
<td>0.994</td>
<td><a href="https://www.semanticscholar.org/paper/2bc1c8bd00bbf7401afcb5460277840fd8bab029">365: Unicoder-VL: A Universal Encoder for Vision and Language by Cross-modal Pre-training</a></td>
</tr>
<tr>
<td>Proposed</td>
<td>0.834</td>
<td>0.994</td>
<td><a href="https://www.semanticscholar.org/paper/d8a305b9366608d54452ac30459ee57b4f5cf1c9">534: UNITER: UNiversal Image-TExt Representation Learning</a></td>
</tr>
<tr>
<td>Proposed</td>
<td>0.830</td>
<td>0.993</td>
<td><a href="https://www.semanticscholar.org/paper/357ea206504e48d1416aa12e84f44c9902bd4686">0: COTS: Collaborative Two-Stream Vision-Language Pre-Training Model for Cross-Modal Retrieval</a></td>
</tr>
<tr>
<td>Proposed</td>
<td>0.845</td>
<td>0.993</td>
<td><a href="https://www.semanticscholar.org/paper/0839722fb5369c0abaff8515bfc08299efc790a1">148: ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision</a></td>
</tr>
<tr>
<td>Proposed</td>
<td>0.843</td>
<td>0.993</td>
<td><a href="https://www.semanticscholar.org/paper/5fa8f2a40ca9daa4063af8a27518bb7b74f890f2">0: mPLUG: Effective and Efficient Vision-Language Learning by Cross-modal Skip-connections</a></td>
</tr>
<tr>
<td>Proposed</td>
<td>0.801</td>
<td>0.993</td>
<td><a href="https://www.semanticscholar.org/paper/899a4e362cdaf6c98af59aa3a267c7e0abf01c98">0: MLP Architectures for Vision-and-Language Modeling: An Empirical Study</a></td>
</tr>
<tr>
<td>Proposed</td>
<td>0.819</td>
<td>0.992</td>
<td><a href="https://www.semanticscholar.org/paper/54416048772b921720f19869ed11c2a360589d03">270: UNITER: Learning UNiversal Image-TExt Representations</a></td>
</tr>
<tr>
<td>Proposed</td>
<td>0.747</td>
<td>0.992</td>
<td><a href="https://www.semanticscholar.org/paper/91dc75f94da13452a54ad5c03fab2c5fda87e9ba">5: Uni-Perceiver: Pre-training Unified Architecture for Generic Perception for Zero-shot and Few-shot Tasks</a></td>
</tr>
<tr>
<td>Proposed</td>
<td>0.864</td>
<td>0.992</td>
<td><a href="https://www.semanticscholar.org/paper/7f71875f8214dffa4f3276da123c4990a6d437cc">1: Enabling Multimodal Generation on CLIP via Vision-Language Knowledge Distillation</a></td>
</tr>
<tr>
<td>Proposed</td>
<td>0.793</td>
<td>0.992</td>
<td><a href="https://www.semanticscholar.org/paper/2527626c11a84f15709e943fbfa2356e19930e3b">673: VL-BERT: Pre-training of Generic Visual-Linguistic Representations</a></td>
</tr>
<tr>
<td>Proposed</td>
<td>0.818</td>
<td>0.992</td>
<td><a href="https://www.semanticscholar.org/paper/80e2886a8c581dc9e628b11d89be95b3cc29204c">0: Language Models with Image Descriptors are Strong Few-Shot Video-Language Learners</a></td>
</tr>
<tr>
<td>Proposed</td>
<td>0.804</td>
<td>0.992</td>
<td><a href="https://www.semanticscholar.org/paper/5aec474c31a2f4b74703c6f786c0a8ff85c450da">592: VisualBERT: A Simple and Performant Baseline for Vision and Language</a></td>
</tr>
</table></html>
